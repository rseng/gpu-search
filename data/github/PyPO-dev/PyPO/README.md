# https://github.com/PyPO-dev/PyPO

```console
setup.py:        "Environment :: GPU :: NVIDIA CUDA :: 11",
docs/navtreeindex0.js:"index.html#cuda":[0,1,1],
docs/search/files_7.js:  ['interfacecuda_2eh_6',['InterfaceCUDA.h',['../InterfaceCUDA_8h.html',1,'']]],
docs/search/all_0.js:  ['_5farrc1tocudac_8',['_arrC1ToCUDAC',['../Kernelsf_8cu.html#a1536f5d7be6659c3b4c717597cbb621b',1,'Kernelsf.cu']]],
docs/search/all_0.js:  ['_5farrc3tocudac_9',['_arrC3ToCUDAC',['../Kernelsf_8cu.html#a0a8dc68cd14e31c1262b3f001eeb7374',1,'Kernelsf.cu']]],
docs/search/all_0.js:  ['_5farrcudactoc1_10',['_arrCUDACToC1',['../Kernelsf_8cu.html#a92b9063166d7f6bfe6dffdba614af933',1,'Kernelsf.cu']]],
docs/search/all_0.js:  ['_5farrcudactoc3_11',['_arrCUDACToC3',['../Kernelsf_8cu.html#a06b277d10f9f9a4e6b233f487c5d6f48',1,'Kernelsf.cu']]],
docs/search/all_0.js:  ['_5finitcuda_17',['_initCUDA',['../KernelsRTf_8cu.html#a0b636357371c5f35156792eaac8862ba',1,'KernelsRTf.cu']]],
docs/search/functions_d.js:  ['pypo_5fgpuf_66',['PyPO_GPUf',['../BindGPU_8py.html#a5f9e94f0296112e66d7137a2bb790ace',1,'PyPO::BindGPU']]]
docs/search/all_7.js:  ['gpu_30',['Using PyPO On GPU',['../index.html#cuda',1,'']]],
docs/search/all_7.js:  ['gpuassert_31',['gpuAssert',['../MemUtils_8h.html#ab3e90881a2476fd461eb2bcfcaa7cf63',1,'MemUtils.h']]],
docs/search/all_1.js:  ['and_20cuda_20source_44',['C/C++ And CUDA Source',['../contribguide.html#cppcuda_docs',1,'']]],
docs/search/all_f.js:  ['pypo_20on_20gpu_90',['Using PyPO On GPU',['../index.html#cuda',1,'']]],
docs/search/all_f.js:  ['pypo_5fgpuf_94',['PyPO_GPUf',['../BindGPU_8py.html#a5f9e94f0296112e66d7137a2bb790ace',1,'PyPO::BindGPU']]],
docs/search/all_14.js:  ['using_20pypo_20on_20gpu_4',['Using PyPO On GPU',['../index.html#cuda',1,'']]],
docs/search/all_11.js:  ['rt_5fgpuf_50',['RT_GPUf',['../BindGPU_8py.html#a92029cfca01428d30363840d07dbcc57',1,'PyPO::BindGPU']]],
docs/search/all_8.js:  ['has_5fcuda_1',['has_CUDA',['../Checks_8py.html#a34775586534cfcede50a1717460ed921',1,'PyPO::Checks']]],
docs/search/functions_8.js:  ['has_5fcuda_1',['has_CUDA',['../Checks_8py.html#a34775586534cfcede50a1717460ed921',1,'PyPO::Checks']]],
docs/search/all_2.js:  ['bindgpu_2epy_3',['BindGPU.py',['../BindGPU_8py.html',1,'']]],
docs/search/functions_7.js:  ['gpuassert_21',['gpuAssert',['../MemUtils_8h.html#ab3e90881a2476fd461eb2bcfcaa7cf63',1,'MemUtils.h']]],
docs/search/functions_0.js:  ['_5farrc1tocudac_8',['_arrC1ToCUDAC',['../Kernelsf_8cu.html#a1536f5d7be6659c3b4c717597cbb621b',1,'Kernelsf.cu']]],
docs/search/functions_0.js:  ['_5farrc3tocudac_9',['_arrC3ToCUDAC',['../Kernelsf_8cu.html#a0a8dc68cd14e31c1262b3f001eeb7374',1,'Kernelsf.cu']]],
docs/search/functions_0.js:  ['_5farrcudactoc1_10',['_arrCUDACToC1',['../Kernelsf_8cu.html#a92b9063166d7f6bfe6dffdba614af933',1,'Kernelsf.cu']]],
docs/search/functions_0.js:  ['_5farrcudactoc3_11',['_arrCUDACToC3',['../Kernelsf_8cu.html#a06b277d10f9f9a4e6b233f487c5d6f48',1,'Kernelsf.cu']]],
docs/search/functions_0.js:  ['_5finitcuda_17',['_initCUDA',['../KernelsRTf_8cu.html#a0b636357371c5f35156792eaac8862ba',1,'KernelsRTf.cu']]],
docs/search/functions_9.js:  ['initcuda_0',['initCUDA',['../Kernelsf_8cu.html#a570d4df3c5990feaab80053b0c1a8a6e',1,'Kernelsf.cu']]],
docs/search/functions_e.js:  ['rt_5fgpuf_15',['RT_GPUf',['../BindGPU_8py.html#a92029cfca01428d30363840d07dbcc57',1,'PyPO::BindGPU']]],
docs/search/files_1.js:  ['bindgpu_2epy_3',['BindGPU.py',['../BindGPU_8py.html',1,'']]],
docs/search/all_3.js:  ['c_20and_20cuda_20source_0',['C/C++ And CUDA Source',['../contribguide.html#cppcuda_docs',1,'']]],
docs/search/all_3.js:  ['c_20c_20and_20cuda_20source_1',['C/C++ And CUDA Source',['../contribguide.html#cppcuda_docs',1,'']]],
docs/search/all_3.js:  ['c_20c_20cuda_2',['C/C++/CUDA',['../contribguide.html#runtests_cpp',1,'']]],
docs/search/all_3.js:  ['c_20cuda_3',['C/C++/CUDA',['../contribguide.html#runtests_cpp',1,'']]],
docs/search/all_3.js:  ['callkernelf_5feh_37',['callKernelf_EH',['../InterfaceCUDA_8h.html#a4e2449511d21878cdc3d746583b7e711',1,'callKernelf_EH(c2Bundlef *res, reflparamsf source, reflparamsf target, reflcontainerf *cs, reflcontainerf *ct, c2Bundlef *currents, float k, float epsilon, float t_direction, int nBlocks, int nThreads):&#160;Kernelsf.cu'],['../Kernelsf_8cu.html#adcd1cf7913ba1e02151a13465527e7ae',1,'callKernelf_EH(c2Bundlef *res, reflparamsf source, reflparamsf target, reflcontainerf *cs, reflcontainerf *ct, c2Bundlef *currents, float k, float epsilon, float t_direction, int nBlocks, int nThreads):&#160;Kernelsf.cu']]],
docs/search/all_3.js:  ['callkernelf_5fehp_38',['callKernelf_EHP',['../InterfaceCUDA_8h.html#a07ab0b2854c12a2c1de63e828ca7e59a',1,'callKernelf_EHP(c2rBundlef *res, reflparamsf source, reflparamsf target, reflcontainerf *cs, reflcontainerf *ct, c2Bundlef *currents, float k, float epsilon, float t_direction, int nBlocks, int nThreads):&#160;Kernelsf.cu'],['../Kernelsf_8cu.html#a782e4edb3dd2bfd699c89f502642722e',1,'callKernelf_EHP(c2rBundlef *res, reflparamsf source, reflparamsf target, reflcontainerf *cs, reflcontainerf *ct, c2Bundlef *currents, float k, float epsilon, float t_direction, int nBlocks, int nThreads):&#160;Kernelsf.cu']]],
docs/search/all_3.js:  ['callkernelf_5fff_39',['callKernelf_FF',['../InterfaceCUDA_8h.html#aa0f2b77b579f659939a1636073e5d30c',1,'callKernelf_FF(c2Bundlef *res, reflparamsf source, reflparamsf target, reflcontainerf *cs, reflcontainerf *ct, c2Bundlef *currents, float k, float epsilon, float t_direction, int nBlocks, int nThreads):&#160;Kernelsf.cu'],['../Kernelsf_8cu.html#a9513614b1aaf7c6a869edc7693d1d013',1,'callKernelf_FF(c2Bundlef *res, reflparamsf source, reflparamsf target, reflcontainerf *cs, reflcontainerf *ct, c2Bundlef *currents, float k, float epsilon, float t_direction, int nBlocks, int nThreads):&#160;Kernelsf.cu']]],
docs/search/all_3.js:  ['callkernelf_5fjm_40',['callKernelf_JM',['../InterfaceCUDA_8h.html#a44cb81d90e9cacc61fc2ef985870153e',1,'callKernelf_JM(c2Bundlef *res, reflparamsf source, reflparamsf target, reflcontainerf *cs, reflcontainerf *ct, c2Bundlef *currents, float k, float epsilon, float t_direction, int nBlocks, int nThreads):&#160;Kernelsf.cu'],['../Kernelsf_8cu.html#a369d792b232ead83b2b9db7ae8373a77',1,'callKernelf_JM(c2Bundlef *res, reflparamsf source, reflparamsf target, reflcontainerf *cs, reflcontainerf *ct, c2Bundlef *currents, float k, float epsilon, float t_direction, int nBlocks, int nThreads):&#160;Kernelsf.cu']]],
docs/search/all_3.js:  ['callkernelf_5fjmeh_41',['callKernelf_JMEH',['../InterfaceCUDA_8h.html#adf0167d1974376ab00066fcaeff27a8d',1,'callKernelf_JMEH(c4Bundlef *res, reflparamsf source, reflparamsf target, reflcontainerf *cs, reflcontainerf *ct, c2Bundlef *currents, float k, float epsilon, float t_direction, int nBlocks, int nThreads):&#160;Kernelsf.cu'],['../Kernelsf_8cu.html#a107c1d18b23494ca80715743cdca3816',1,'callKernelf_JMEH(c4Bundlef *res, reflparamsf source, reflparamsf target, reflcontainerf *cs, reflcontainerf *ct, c2Bundlef *currents, float k, float epsilon, float t_direction, int nBlocks, int nThreads):&#160;Kernelsf.cu']]],
docs/search/all_3.js:  ['callkernelf_5fscalar_42',['callKernelf_scalar',['../InterfaceCUDA_8h.html#a7563acdb1e3f2a9cfe6ecbe62a4ebbba',1,'callKernelf_scalar(arrC1f *res, reflparamsf source, reflparamsf target, reflcontainerf *cs, reflcontainerf *ct, arrC1f *inp, float k, float epsilon, float t_direction, int nBlocks, int nThreads):&#160;Kernelsf.cu'],['../Kernelsf_8cu.html#ad2b2bebbbbcea7a3eb5eadb3522871a3',1,'callKernelf_scalar(arrC1f *res, reflparamsf source, reflparamsf target, reflcontainerf *cs, reflcontainerf *ct, arrC1f *inp, float k, float epsilon, float t_direction, int nBlocks, int nThreads):&#160;Kernelsf.cu']]],
docs/search/all_3.js:  ['callrtkernel_43',['callRTKernel',['../InterfaceCUDA_8h.html#afd99cd13f9977d3429332c22dc81cf28',1,'callRTKernel(reflparamsf ctp, cframef *fr_in, cframef *fr_out, float epsilon, float t0, int nBlocks, int nThreads):&#160;KernelsRTf.cu'],['../KernelsRTf_8cu.html#aff4d46be8cc9115e8360df4bc0a40b75',1,'callRTKernel(reflparamsf ctp, cframef *fr_in, cframef *fr_out, float epsilon, float t0, int nBlocks, int nThreads):&#160;KernelsRTf.cu']]],
docs/search/all_3.js:  ['cuda_101',['C/C++/CUDA',['../contribguide.html#runtests_cpp',1,'']]],
docs/search/all_3.js:  ['cuda_20source_102',['C/C++ And CUDA Source',['../contribguide.html#cppcuda_docs',1,'']]],
docs/search/functions_a.js:  ['loadgpulib_2',['loadGPUlib',['../BindGPU_8py.html#a40a3fc036f857cb8842034a8c0f2d18d',1,'PyPO::BindGPU']]],
docs/search/functions_3.js:  ['callkernelf_5feh_27',['callKernelf_EH',['../InterfaceCUDA_8h.html#a4e2449511d21878cdc3d746583b7e711',1,'callKernelf_EH(c2Bundlef *res, reflparamsf source, reflparamsf target, reflcontainerf *cs, reflcontainerf *ct, c2Bundlef *currents, float k, float epsilon, float t_direction, int nBlocks, int nThreads):&#160;Kernelsf.cu'],['../Kernelsf_8cu.html#adcd1cf7913ba1e02151a13465527e7ae',1,'callKernelf_EH(c2Bundlef *res, reflparamsf source, reflparamsf target, reflcontainerf *cs, reflcontainerf *ct, c2Bundlef *currents, float k, float epsilon, float t_direction, int nBlocks, int nThreads):&#160;Kernelsf.cu']]],
docs/search/functions_3.js:  ['callkernelf_5fehp_28',['callKernelf_EHP',['../InterfaceCUDA_8h.html#a07ab0b2854c12a2c1de63e828ca7e59a',1,'callKernelf_EHP(c2rBundlef *res, reflparamsf source, reflparamsf target, reflcontainerf *cs, reflcontainerf *ct, c2Bundlef *currents, float k, float epsilon, float t_direction, int nBlocks, int nThreads):&#160;Kernelsf.cu'],['../Kernelsf_8cu.html#a782e4edb3dd2bfd699c89f502642722e',1,'callKernelf_EHP(c2rBundlef *res, reflparamsf source, reflparamsf target, reflcontainerf *cs, reflcontainerf *ct, c2Bundlef *currents, float k, float epsilon, float t_direction, int nBlocks, int nThreads):&#160;Kernelsf.cu']]],
docs/search/functions_3.js:  ['callkernelf_5fff_29',['callKernelf_FF',['../InterfaceCUDA_8h.html#aa0f2b77b579f659939a1636073e5d30c',1,'callKernelf_FF(c2Bundlef *res, reflparamsf source, reflparamsf target, reflcontainerf *cs, reflcontainerf *ct, c2Bundlef *currents, float k, float epsilon, float t_direction, int nBlocks, int nThreads):&#160;Kernelsf.cu'],['../Kernelsf_8cu.html#a9513614b1aaf7c6a869edc7693d1d013',1,'callKernelf_FF(c2Bundlef *res, reflparamsf source, reflparamsf target, reflcontainerf *cs, reflcontainerf *ct, c2Bundlef *currents, float k, float epsilon, float t_direction, int nBlocks, int nThreads):&#160;Kernelsf.cu']]],
docs/search/functions_3.js:  ['callkernelf_5fjm_30',['callKernelf_JM',['../InterfaceCUDA_8h.html#a44cb81d90e9cacc61fc2ef985870153e',1,'callKernelf_JM(c2Bundlef *res, reflparamsf source, reflparamsf target, reflcontainerf *cs, reflcontainerf *ct, c2Bundlef *currents, float k, float epsilon, float t_direction, int nBlocks, int nThreads):&#160;Kernelsf.cu'],['../Kernelsf_8cu.html#a369d792b232ead83b2b9db7ae8373a77',1,'callKernelf_JM(c2Bundlef *res, reflparamsf source, reflparamsf target, reflcontainerf *cs, reflcontainerf *ct, c2Bundlef *currents, float k, float epsilon, float t_direction, int nBlocks, int nThreads):&#160;Kernelsf.cu']]],
docs/search/functions_3.js:  ['callkernelf_5fjmeh_31',['callKernelf_JMEH',['../InterfaceCUDA_8h.html#adf0167d1974376ab00066fcaeff27a8d',1,'callKernelf_JMEH(c4Bundlef *res, reflparamsf source, reflparamsf target, reflcontainerf *cs, reflcontainerf *ct, c2Bundlef *currents, float k, float epsilon, float t_direction, int nBlocks, int nThreads):&#160;Kernelsf.cu'],['../Kernelsf_8cu.html#a107c1d18b23494ca80715743cdca3816',1,'callKernelf_JMEH(c4Bundlef *res, reflparamsf source, reflparamsf target, reflcontainerf *cs, reflcontainerf *ct, c2Bundlef *currents, float k, float epsilon, float t_direction, int nBlocks, int nThreads):&#160;Kernelsf.cu']]],
docs/search/functions_3.js:  ['callkernelf_5fscalar_32',['callKernelf_scalar',['../InterfaceCUDA_8h.html#a7563acdb1e3f2a9cfe6ecbe62a4ebbba',1,'callKernelf_scalar(arrC1f *res, reflparamsf source, reflparamsf target, reflcontainerf *cs, reflcontainerf *ct, arrC1f *inp, float k, float epsilon, float t_direction, int nBlocks, int nThreads):&#160;Kernelsf.cu'],['../Kernelsf_8cu.html#ad2b2bebbbbcea7a3eb5eadb3522871a3',1,'callKernelf_scalar(arrC1f *res, reflparamsf source, reflparamsf target, reflcontainerf *cs, reflcontainerf *ct, arrC1f *inp, float k, float epsilon, float t_direction, int nBlocks, int nThreads):&#160;Kernelsf.cu']]],
docs/search/functions_3.js:  ['callrtkernel_33',['callRTKernel',['../InterfaceCUDA_8h.html#afd99cd13f9977d3429332c22dc81cf28',1,'callRTKernel(reflparamsf ctp, cframef *fr_in, cframef *fr_out, float epsilon, float t0, int nBlocks, int nThreads):&#160;KernelsRTf.cu'],['../KernelsRTf_8cu.html#aff4d46be8cc9115e8360df4bc0a40b75',1,'callRTKernel(reflparamsf ctp, cframef *fr_in, cframef *fr_out, float epsilon, float t0, int nBlocks, int nThreads):&#160;KernelsRTf.cu']]],
docs/search/all_b.js:  ['loadgpulib_6',['loadGPUlib',['../BindGPU_8py.html#a40a3fc036f857cb8842034a8c0f2d18d',1,'PyPO::BindGPU']]],
docs/search/all_9.js:  ['initcuda_15',['initCUDA',['../Kernelsf_8cu.html#a570d4df3c5990feaab80053b0c1a8a6e',1,'Kernelsf.cu']]],
docs/search/all_9.js:  ['interfacecuda_2eh_62',['InterfaceCUDA.h',['../InterfaceCUDA_8h.html',1,'']]],
docs/search/all_e.js:  ['on_20gpu_5',['Using PyPO On GPU',['../index.html#cuda',1,'']]],
docs/search/all_12.js:  ['source_52',['Source',['../contribguide.html#cppcuda_docs',1,'C/C++ And CUDA Source'],['../index.html#sdist',1,'Install From Source'],['../contribguide.html#python_docs',1,'Python Source']]],
docs/index.js:      [ "Using PyPO On GPU", "index.html#cuda", null ],
tests/test_SystemPO_RT.py:Tests for checking if CPU and GPU operations in PyPO are correct
tests/test_SystemPO_RT.py:import PyPO.BindGPU as gpulibs
tests/test_SystemPO_RT.py:        self.GPU_flag = True
tests/test_SystemPO_RT.py:            lib = gpulibs.loadGPUlib()
tests/test_SystemPO_RT.py:            self.GPU_flag = False
tests/test_SystemPO_RT.py:    def test_loadGPUlib(self):
tests/test_SystemPO_RT.py:        if self.GPU_flag:
tests/test_SystemPO_RT.py:            lib = gpulibs.loadGPUlib()
tests/test_SystemPO_RT.py:            print("No GPU libraries found... Not testing GPU functionalities.")
tests/test_SystemPO_RT.py:    @params(("CPU", System.runPO), ("GPU", System.runPO), ("CPU", System.runGUIPO))
tests/test_SystemPO_RT.py:        if dev == "GPU" and not self.GPU_flag:
tests/test_SystemPO_RT.py:    @params(("CPU", System.runPO), ("GPU", System.runPO), ("CPU", System.runGUIPO))
tests/test_SystemPO_RT.py:        if dev == "GPU" and not self.GPU_flag:
tests/test_SystemPO_RT.py:    @params(("CPU", System.runPO), ("GPU", System.runPO), ("CPU", System.runGUIPO))
tests/test_SystemPO_RT.py:        if dev == "GPU" and not self.GPU_flag:
tests/test_SystemPO_RT.py:            ("GPU", True, System.runPO, System.runHybridPropagation, FieldComponents.Ex, None), 
tests/test_SystemPO_RT.py:        if dev == "GPU" and not self.GPU_flag:
tests/test_SystemPO_RT.py:    @params(("CPU", System.runPO), ("GPU", System.runPO), ("CPU", System.runGUIPO))
tests/test_SystemPO_RT.py:        if dev == "GPU" and not self.GPU_flag:
tests/test_SystemPO_RT.py:    @params(("CPU", System.runPO), ("GPU", System.runPO), ("CPU", System.runGUIPO))
tests/test_SystemPO_RT.py:        if dev == "GPU" and not self.GPU_flag:
tests/test_SystemPO_RT.py:    @params(("CPU", System.runRayTracer), ("GPU", System.runRayTracer), ("CPU", System.runGUIRayTracer))
tests/test_SystemPO_RT.py:        if dev == "GPU" and not self.GPU_flag:
CMakeLists.txt:check_language(CUDA)
CMakeLists.txt:if(CMAKE_CUDA_COMPILER)
CMakeLists.txt:    enable_language(CUDA)
CMakeLists.txt:    set(CMAKE_CUDA_STANDARD 11)
CMakeLists.txt:    file(GLOB CUDAfiles src/CUDA/*.cu)
CMakeLists.txt:    add_library(pypogpu SHARED ${CUDAfiles})
CMakeLists.txt:    target_include_directories(pypogpu PRIVATE src/CUDA)
CMakeLists.txt:    target_link_libraries(pypogpu PRIVATE pyporefl)
CMakeLists.txt:    set_target_properties(pypogpu PROPERTIES CUDA_SEPARABLE_COMPILATION ON)
CMakeLists.txt:    set_property(TARGET pypogpu PROPERTY CUDA_ARCHITECTURES OFF)
CMakeLists.txt:    message(STATUS "WARNING: no CUDA compiler detected. Not building CUDA libraries")
src/GUI/ParameterForms/formData.py:    sublist_dev = ["CPU", "GPU"]
src/GUI/ParameterForms/formData.py:    sublist_dev = ["CPU", "GPU"]
src/GUI/ParameterForms/formData.py:    sublist_dev = ["CPU", "GPU"]
src/GUI/ParameterForms/formData.py:    sublist_dev = ["CPU", "GPU"]
src/CUDA/KernelsRTf.cu:#include "InterfaceCUDA.h"
src/CUDA/KernelsRTf.cu:    \brief Kernel for CUDA RT calculations.
src/CUDA/KernelsRTf.cu: * Initialize CUDA.
src/CUDA/KernelsRTf.cu:__host__ std::array<dim3, 2> _initCUDA(reflparamsf ctp, float epsilon, float t0,
src/CUDA/KernelsRTf.cu:    gpuErrchk( cudaMemcpyToSymbol(conrt, &_conrt, CSIZERT * sizeof(float)) );
src/CUDA/KernelsRTf.cu:    gpuErrchk( cudaMemcpyToSymbol(mat, ctp.transf, 16 * sizeof(float)) );
src/CUDA/KernelsRTf.cu:    gpuErrchk( cudaMemcpyToSymbol(nTot, &_nTot, sizeof(int)) );
src/CUDA/KernelsRTf.cu:    gpuErrchk( cudaMemcpyToSymbol(cflip, &iflip, sizeof(int)) );
src/CUDA/KernelsRTf.cu:    gpuErrchk( cudaMemcpyToSymbol(ctype, &(ctp.type), sizeof(int)) );
src/CUDA/KernelsRTf.cu:  @param nBlocks Number of blocks in GPU grid.
src/CUDA/KernelsRTf.cu:    BT = _initCUDA(ctp, epsilon, t0, fr_in->size, nBlocks, nThreads);
src/CUDA/KernelsRTf.cu:    gpuErrchk( cudaDeviceSynchronize() );
src/CUDA/KernelsRTf.cu:    gpuErrchk( cudaDeviceReset() );
src/CUDA/InterfaceCUDA.h:#ifndef __InterfaceCUDA_h
src/CUDA/InterfaceCUDA.h:#define __InterfaceCUDA_h
src/CUDA/InterfaceCUDA.h:/*! \file InterfaceCUDA.h
src/CUDA/InterfaceCUDA.h:    \brief Declarations of PO and RT library for GPU.
src/CUDA/InterfaceCUDA.h:    Provides double and single precision interface for NVIDIA GPUs running CUDA.
src/CUDA/Debug.h:    \brief Methods for printing complex or real arrays of length 3 for GPU.
src/CUDA/Kernelsf.cu:#include "InterfaceCUDA.h"
src/CUDA/Kernelsf.cu:    \brief Kernels for CUDA PO calculations.
src/CUDA/Kernelsf.cu: * Initialize CUDA.
src/CUDA/Kernelsf.cu: __host__ std::array<dim3, 2> initCUDA(float k, float epsilon, int gt, int gs, float t_direction, int nBlocks, int nThreads)
src/CUDA/Kernelsf.cu:     gpuErrchk( cudaMemcpyToSymbol(g_s, &gs, sizeof(int)) );
src/CUDA/Kernelsf.cu:     gpuErrchk( cudaMemcpyToSymbol(g_t, &gt, sizeof(int)) );
src/CUDA/Kernelsf.cu:     gpuErrchk( cudaMemcpyToSymbol(eye, &_eye, sizeof(_eye)) );
src/CUDA/Kernelsf.cu:     gpuErrchk( cudaMemcpyToSymbol(con, &_con, CSIZE * sizeof(cuFloatComplex)) );
src/CUDA/Kernelsf.cu:__host__ void _arrC1ToCUDAC(float *rarr, float *iarr, cuFloatComplex* carr,  int size)
src/CUDA/Kernelsf.cu:__host__ void _arrC3ToCUDAC(float *r1arr, float *r2arr, float *r3arr,
src/CUDA/Kernelsf.cu:__host__ void _arrCUDACToC1(cuFloatComplex* carr, float *rarr, float *iarr, int size)
src/CUDA/Kernelsf.cu:__host__ void _arrCUDACToC3(cuFloatComplex* c1arr, cuFloatComplex* c2arr, cuFloatComplex* c3arr,
src/CUDA/Kernelsf.cu: * Calculate J, M currents on a target surface using CUDA.
src/CUDA/Kernelsf.cu: * @param nBlocks Number of blocks in GPU grid.
src/CUDA/Kernelsf.cu:    BT = initCUDA(k, epsilon, ct->size, cs->size, t_direction, nBlocks, nThreads);
src/CUDA/Kernelsf.cu:    cudaFuncSetCacheConfig(GpropagateBeam_0, cudaFuncCachePreferL1);
src/CUDA/Kernelsf.cu:    _arrC3ToCUDAC(currents->r1x, currents->r1y, currents->r1z,
src/CUDA/Kernelsf.cu:    _arrC3ToCUDAC(currents->r2x, currents->r2y, currents->r2z,
src/CUDA/Kernelsf.cu:    gpuErrchk( cudaPeekAtLastError() );
src/CUDA/Kernelsf.cu:    gpuErrchk( cudaDeviceSynchronize() );
src/CUDA/Kernelsf.cu:    gpuErrchk( cudaDeviceReset() );
src/CUDA/Kernelsf.cu:    _arrCUDACToC3(vec_hout[0], vec_hout[1], vec_hout[2], res->r1x, res->r1y, res->r1z, res->i1x, res->i1y, res->i1z, ct->size);
src/CUDA/Kernelsf.cu:    _arrCUDACToC3(vec_hout[3], vec_hout[4], vec_hout[5], res->r2x, res->r2y, res->r2z, res->i2x, res->i2y, res->i2z, ct->size);
src/CUDA/Kernelsf.cu: * Calculate E, H fields on a target surface using CUDA.
src/CUDA/Kernelsf.cu: * @param nBlocks Number of blocks in GPU grid.
src/CUDA/Kernelsf.cu:    BT = initCUDA(k, epsilon, ct->size, cs->size, t_direction, nBlocks, nThreads);
src/CUDA/Kernelsf.cu:    cudaFuncSetCacheConfig(GpropagateBeam_1, cudaFuncCachePreferL1);
src/CUDA/Kernelsf.cu:    _arrC3ToCUDAC(currents->r1x, currents->r1y, currents->r1z,
src/CUDA/Kernelsf.cu:    _arrC3ToCUDAC(currents->r2x, currents->r2y, currents->r2z,
src/CUDA/Kernelsf.cu:    gpuErrchk( cudaPeekAtLastError() );
src/CUDA/Kernelsf.cu:    gpuErrchk( cudaDeviceSynchronize() );
src/CUDA/Kernelsf.cu:    gpuErrchk( cudaDeviceReset() );
src/CUDA/Kernelsf.cu:    _arrCUDACToC3(vec_hout[0], vec_hout[1], vec_hout[2], res->r1x, res->r1y, res->r1z, res->i1x, res->i1y, res->i1z, ct->size);
src/CUDA/Kernelsf.cu:    _arrCUDACToC3(vec_hout[3], vec_hout[4], vec_hout[5], res->r2x, res->r2y, res->r2z, res->i2x, res->i2y, res->i2z, ct->size);
src/CUDA/Kernelsf.cu: * Calculate J, M currents and E, H fields on a target surface using CUDA.
src/CUDA/Kernelsf.cu: * @param nBlocks Number of blocks in GPU grid.
src/CUDA/Kernelsf.cu:    BT = initCUDA(k, epsilon, ct->size, cs->size, t_direction, nBlocks, nThreads);
src/CUDA/Kernelsf.cu:    cudaFuncSetCacheConfig(GpropagateBeam_2, cudaFuncCachePreferL1);
src/CUDA/Kernelsf.cu:    _arrC3ToCUDAC(currents->r1x, currents->r1y, currents->r1z,
src/CUDA/Kernelsf.cu:    _arrC3ToCUDAC(currents->r2x, currents->r2y, currents->r2z,
src/CUDA/Kernelsf.cu:    gpuErrchk( cudaPeekAtLastError() );
src/CUDA/Kernelsf.cu:    gpuErrchk( cudaDeviceSynchronize() );
src/CUDA/Kernelsf.cu:    gpuErrchk( cudaDeviceReset() );
src/CUDA/Kernelsf.cu:    _arrCUDACToC3(vec_hout[0], vec_hout[1], vec_hout[2], res->r1x, res->r1y, res->r1z, res->i1x, res->i1y, res->i1z, ct->size);
src/CUDA/Kernelsf.cu:    _arrCUDACToC3(vec_hout[3], vec_hout[4], vec_hout[5], res->r2x, res->r2y, res->r2z, res->i2x, res->i2y, res->i2z, ct->size);
src/CUDA/Kernelsf.cu:    _arrCUDACToC3(vec_hout[6], vec_hout[7], vec_hout[8], res->r3x, res->r3y, res->r3z, res->i3x, res->i3y, res->i3z, ct->size);
src/CUDA/Kernelsf.cu:    _arrCUDACToC3(vec_hout[9], vec_hout[10], vec_hout[11], res->r4x, res->r4y, res->r4z, res->i4x, res->i4y, res->i4z, ct->size);
src/CUDA/Kernelsf.cu: * Calculate reflected E, H fields and P, the reflected Poynting vectorfield, on a target surface using CUDA.
src/CUDA/Kernelsf.cu: * @param nBlocks Number of blocks in GPU grid.
src/CUDA/Kernelsf.cu:    BT = initCUDA(k, epsilon, ct->size, cs->size, t_direction, nBlocks, nThreads);
src/CUDA/Kernelsf.cu:    cudaFuncSetCacheConfig(GpropagateBeam_3, cudaFuncCachePreferL1);
src/CUDA/Kernelsf.cu:    _arrC3ToCUDAC(currents->r1x, currents->r1y, currents->r1z,
src/CUDA/Kernelsf.cu:    _arrC3ToCUDAC(currents->r2x, currents->r2y, currents->r2z,
src/CUDA/Kernelsf.cu:    gpuErrchk( cudaDeviceReset() );
src/CUDA/Kernelsf.cu:    _arrCUDACToC3(vec_hout[0], vec_hout[1], vec_hout[2], res->r1x, res->r1y, res->r1z, res->i1x, res->i1y, res->i1z, ct->size);
src/CUDA/Kernelsf.cu:    _arrCUDACToC3(vec_hout[3], vec_hout[4], vec_hout[5], res->r2x, res->r2y, res->r2z, res->i2x, res->i2y, res->i2z, ct->size);
src/CUDA/Kernelsf.cu: * Calculate E, H fields on a far-field target surface using CUDA.
src/CUDA/Kernelsf.cu: * @param nBlocks Number of blocks in GPU grid.
src/CUDA/Kernelsf.cu:    BT = initCUDA(k, epsilon, ct->size, cs->size, t_direction, nBlocks, nThreads);
src/CUDA/Kernelsf.cu:    cudaFuncSetCacheConfig(GpropagateBeam_4, cudaFuncCachePreferL1);
src/CUDA/Kernelsf.cu:    _arrC3ToCUDAC(currents->r1x, currents->r1y, currents->r1z,
src/CUDA/Kernelsf.cu:    _arrC3ToCUDAC(currents->r2x, currents->r2y, currents->r2z,
src/CUDA/Kernelsf.cu:    gpuErrchk( cudaPeekAtLastError() );
src/CUDA/Kernelsf.cu:    gpuErrchk( cudaDeviceSynchronize() );
src/CUDA/Kernelsf.cu:    gpuErrchk( cudaDeviceReset() );
src/CUDA/Kernelsf.cu:    _arrCUDACToC3(vec_hout[0], vec_hout[1], vec_hout[2], res->r1x, res->r1y, res->r1z, res->i1x, res->i1y, res->i1z, ct->size);
src/CUDA/Kernelsf.cu:    _arrCUDACToC3(vec_hout[3], vec_hout[4], vec_hout[5], res->r2x, res->r2y, res->r2z, res->i2x, res->i2y, res->i2z, ct->size);
src/CUDA/Kernelsf.cu: * Calculate scalar field on a target surface using CUDA.
src/CUDA/Kernelsf.cu: * @param nBlocks Number of blocks in GPU grid.
src/CUDA/Kernelsf.cu:    BT = initCUDA(k, epsilon, ct->size, cs->size, t_direction, nBlocks, nThreads);
src/CUDA/Kernelsf.cu:    cudaFuncSetCacheConfig(GpropagateBeam_5, cudaFuncCachePreferL1);
src/CUDA/Kernelsf.cu:    _arrC1ToCUDAC(inp->x, inp->y, h_sfs, cs->size);
src/CUDA/Kernelsf.cu:    gpuErrchk( cudaMalloc((void**)&d_sfs, cs->size * sizeof(cuFloatComplex)) );
src/CUDA/Kernelsf.cu:    gpuErrchk( cudaMemcpy(d_sfs, h_sfs, cs->size * sizeof(cuFloatComplex), cudaMemcpyHostToDevice) );
src/CUDA/Kernelsf.cu:    gpuErrchk( cudaMalloc((void**)&d_sft, ct->size * sizeof(cuFloatComplex)) );
src/CUDA/Kernelsf.cu:    gpuErrchk( cudaPeekAtLastError() );
src/CUDA/Kernelsf.cu:    gpuErrchk( cudaDeviceSynchronize() );
src/CUDA/Kernelsf.cu:    gpuErrchk( cudaMemcpy(h_sft, d_sft, ct->size * sizeof(cuFloatComplex), cudaMemcpyDeviceToHost) );
src/CUDA/Kernelsf.cu:    gpuErrchk( cudaDeviceReset() );
src/CUDA/Kernelsf.cu:    _arrCUDACToC1(h_sft, res->x, res->y, ct->size);
src/include/GUtils.h:#include <cuda.h>
src/include/GUtils.h:    \brief Linear algebra functions for the CUDA version of PyPO. 
src/include/GUtils.h:        For the CUDA complex valued linear algebra, we employ the cuComplex.h library.
src/include/RTRefls.h:    Simple, bare-bones definitions of reflectors for CPU/GPU ray-tracing.
src/include/MemUtils.h:#include <cuda.h>
src/include/MemUtils.h:#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }
src/include/MemUtils.h:    \brief Utility class for CUDA memory allocations.
src/include/MemUtils.h: * Check CUDA API error status of call.
src/include/MemUtils.h: * Wrapper for finding errors in CUDA API calls.
src/include/MemUtils.h:inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort = true)
src/include/MemUtils.h:   if (code != cudaSuccess)
src/include/MemUtils.h:      fprintf(stderr,"GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line);
src/include/MemUtils.h: * Utility class for memory allocations/copies between CUDA and host.
src/include/MemUtils.h: * Allocate memory for floats on GPU and return pointers.
src/include/MemUtils.h: * @param n Number of pointers to allocate on GPU.
src/include/MemUtils.h: * @return out Vector containing GPU-allocated pointers.
src/include/MemUtils.h:        gpuErrchk( cudaMalloc((void**)&p, size * sizeof(float)) );
src/include/MemUtils.h: * Allocate memory for cuFloatComplex on GPU and return pointers.
src/include/MemUtils.h: * @param n Number of pointers to allocate on GPU.
src/include/MemUtils.h: * @return out Vector containing GPU-allocated pointers.
src/include/MemUtils.h:        gpuErrchk( cudaMalloc((void**)&p, size * sizeof(cuFloatComplex)) );
src/include/MemUtils.h: * @param n Number of pointers to allocate on GPU.
src/include/MemUtils.h: * Copy local arrays of floats to allocated memory on GPU.
src/include/MemUtils.h:        if(H2D) {gpuErrchk( cudaMemcpy(vecFloat[i], vecData[i], size * sizeof(float), cudaMemcpyHostToDevice) );}
src/include/MemUtils.h:        else {gpuErrchk( cudaMemcpy(vecFloat[i], vecData[i], size * sizeof(float), cudaMemcpyDeviceToHost) );}
src/include/MemUtils.h: * Copy local arrays of cuFloatComplex to allocated memory on GPU.
src/include/MemUtils.h:        if(H2D) {gpuErrchk( cudaMemcpy(vecFloat[i], vecData[i], size * sizeof(cuFloatComplex), cudaMemcpyHostToDevice) );}
src/include/MemUtils.h:        else {gpuErrchk( cudaMemcpy(vecFloat[i], vecData[i], size * sizeof(cuFloatComplex), cudaMemcpyDeviceToHost) );}
src/CMakeLists.txt:check_language(CUDA)
src/CMakeLists.txt:if(CMAKE_CUDA_COMPILER)
src/CMakeLists.txt:    add_executable(runCUDATests include/tests/test_GUtils.cpp)
src/CMakeLists.txt:    set_target_properties(runCUDATests PROPERTIES RUNTIME_OUTPUT_DIRECTORY "${CMAKE_CURRENT_SOURCE_DIR}/bin")
src/CMakeLists.txt:    target_link_libraries(runCUDATests ${GTEST_LIBRARIES} GTest::gtest_main)
src/CMakeLists.txt:        target_link_libraries(runCUDATests pthread)
src/CMakeLists.txt:    gtest_discover_tests(runCUDATests)
src/CMakeLists.txt:    message(STATUS "WARNING: no CUDA compiler detected. Not building CUDA tests")
src/PyPO/Threadmgr.py:    This class is only used to spawn calls to the C++/CUDA backend inside a daemon thread so that Python keeps control over the process.
src/PyPO/Threadmgr.py:    This allows users to Ctrl-c a running calculation in C++/CUDA from Python.
src/PyPO/System.py:import PyPO.BindGPU as BGPU
src/PyPO/System.py:        elif _runPODict["device"] == "GPU":
src/PyPO/System.py:            self.clog.work(f"Hardware: running {_runPODict['nThreads']} CUDA threads per block.")
src/PyPO/System.py:            out = BGPU.PyPO_GPUf(source, target, _runPODict)
src/PyPO/System.py:        elif _runRTDict["device"] == "GPU":
src/PyPO/System.py:            self.clog.work(f"Hardware: running {_runRTDict['nThreads']} CUDA threads per block.")
src/PyPO/System.py:            frameObj = BGPU.RT_GPUf(_runRTDict)
src/PyPO/System.py:        elif _runPODict["device"] == "GPU":
src/PyPO/System.py:            out = BGPU.PyPO_GPUf(source, target, _runPODict)
src/PyPO/System.py:        elif _runRTDict["device"] == "GPU":
src/PyPO/System.py:            frameObj = BGPU.RT_GPUf(_runRTDict)
src/PyPO/Templates.py:        "nThreads"      : "Number of CPU/GPU threads (int)",
src/PyPO/Templates.py:        "nThreads"  : "Number of CPU/GPU threads (int)",
src/PyPO/Templates.py:        "nThreads"  : "Number of CPU/GPU threads (int)",
src/PyPO/BindGPU.py:These bindings are concerned with propagations for the ray-tracer and the physical optics on the GPU.
src/PyPO/BindGPU.py:def loadGPUlib():
src/PyPO/BindGPU.py:    Load the pypogpu shared library. Will detect the operating system and link the library accordingly.
src/PyPO/BindGPU.py:        lib = ctypes.CDLL(os.path.join(path_cur, "libpypogpu.dll"))
src/PyPO/BindGPU.py:            lib = ctypes.CDLL(os.path.join(path_cur, "libpypogpu.so"))
src/PyPO/BindGPU.py:            lib = ctypes.CDLL(os.path.join(path_cur, "libpypogpu.dylib"))
src/PyPO/BindGPU.py:def PyPO_GPUf(source, target, runPODict):
src/PyPO/BindGPU.py:    Perform a PO propagation on the GPU.
src/PyPO/BindGPU.py:    Note that the calculations are always done in single precision for the GPU.
src/PyPO/BindGPU.py:    lib = loadGPUlib()
src/PyPO/BindGPU.py:def RT_GPUf(runRTDict):
src/PyPO/BindGPU.py:    Perform an RT propagation on the GPU.
src/PyPO/BindGPU.py:    Note that the calculations are always done in single precision for the GPU.
src/PyPO/BindGPU.py:    lib = loadGPUlib()
src/PyPO/Structs.py:The structures come in double format for CPU and single format for GPU.
src/PyPO/Checks.py:def has_CUDA():
src/PyPO/Checks.py:    Check if the CUDA dynamically linked libraries exist.
src/PyPO/Checks.py:    win_cuda = os.path.exists(os.path.join(path_cur, "pypogpu.dll"))
src/PyPO/Checks.py:    nix_cuda = os.path.exists(os.path.join(path_cur, "libpypogpu.so"))
src/PyPO/Checks.py:    mac_cuda = os.path.exists(os.path.join(path_cur, "libpypogpu.dylib"))
src/PyPO/Checks.py:    has = win_cuda or nix_cuda or mac_cuda
src/PyPO/Checks.py:    cuda = has_CUDA()
src/PyPO/Checks.py:        if runRTDict["device"] != "CPU" and runRTDict["device"] != "GPU":
src/PyPO/Checks.py:        if runRTDict["device"] == "GPU" and not cuda:
src/PyPO/Checks.py:            clog.warning(f"No PyPO CUDA libraries found. Defaulting to CPU.")
src/PyPO/Checks.py:        elif runRTDict["device"] == "GPU":
src/PyPO/Checks.py:    cuda = has_CUDA()
src/PyPO/Checks.py:        if cuda:
src/PyPO/Checks.py:            runPODict["device"] = "GPU"
src/PyPO/Checks.py:        if runPODict["device"] != "CPU" and runPODict["device"] != "GPU":
src/PyPO/Checks.py:        if runPODict["device"] == "GPU" and not cuda:
src/PyPO/Checks.py:            clog.warning(f"No PyPO CUDA libraries found. Defaulting to CPU.")
src/PyPO/Checks.py:        elif runPODict["device"] == "GPU":
src/SetupGTest.py:    Create build environment for C++/CUDA unittest using Google test.
src/SetupGTest.py:    parser = argparse.ArgumentParser(description="setup for making C++/CUDA unittests")
src/SetupGTest.py:            exePath = os.path.join(binPath, "runCUDATests")
doxy/Doxyfile:INPUT                  = doxy src/common src/CPU src/CUDA src/include src/PyPO src/GUI src/GUI/ParameterForms
doxy/mainpage.dox:`PyPO` is optical simulation software, aimed at engineers/students in the field of (quasi)-optical design that have access to CUDA-capable GPUs.
doxy/mainpage.dox:The software can be run multi-threaded, but the real power shines through when running `PyPO` on the GPU.
doxy/mainpage.dox:\subsection cuda Using PyPO On GPU
doxy/mainpage.dox:The full power of `PyPO` shines through when running on the GPU. 
doxy/mainpage.dox:In order to use the GPU accelerated libraries for `PyPO`, a working CUDA installation should be present.
doxy/mainpage.dox:For Linux, [this article](https://linuxconfig.org/how-to-install-cuda-on-ubuntu-20-04-focal-fossa-linux) describes nicely how to install CUDA through the apt repository.
doxy/mainpage.dox:For Windows, see the [official instructions](https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html) on the Nvidia website.
doxy/mainpage.dox:Unfortunately, [Apple has stopped supporting Nvidia cards since 2015](https://appleinsider.com/articles/19/01/18/apples-management-doesnt-want-nvidia-support-in-macos-and-thats-a-bad-sign-for-the-mac-pro) and therefore, the CUDA libraries for `PyPO` cannot be installed on MacOS currently.
doxy/mainpage.dox:After installing CUDA, `PyPO` will detect this and, upon building, will build the CUDA libraries.
doxy/mainpage.dox:It is therefore important to re-install `PyPO` if you installed it before installing CUDA, otherwise the libraries will not be built.
doxy/mainpage.dox:- the second part forms a reference for all internal Python, C/C++ and CUDA code.
doxy/BasicTut1.dox:The first section will describe the libraries powering `PyPO`. Even though these libraries are written in C/C++/CUDA, we will try to keep the explanation accesible.
doxy/BasicTut1.dox:`PyPO` is powered by libraries written in C/C++/CUDA. Because the physical optics calculations are quite heavy, these are written in a compiled language.
doxy/BasicTut1.dox:Because of the powerful computational abilities of current day GPUs, we have also written the libraries in CUDA. 
doxy/BasicTut1.dox:This version is orders of magnitude faster for PO than the parallel CPU version and we highly recommend running `PyPO` on a system which has acces to an Nvidia card.
doxy/BasicTut1.dox:This does necessitate an Nvidia card and GPU implementations in other frameworks (e.g. Metal for Apple) are always a welcome addition.
doxy/BasicTut1.dox:However, because of the way CUDA memory operations work, we could not help but use raw pointers every now and then in the CUDA code.
doxy/BasicTut1.dox:So the narrative that `PyPO` handles all (de-)allocation of memory is not strictly true since the CUDA code does some (de-)allocating of its own, in its own scope.
doxy/BasicTut1.dox:So, for the sake of simplicity and because the CUDA (de-)allocations happen in the local CUDA scope, we will continue as if `PyPO` takes care of all (de-)allocations.
doxy/InstDev.dox:For the C/C++/CUDA code, this is not true. These scripts need to be compiled into libraries again (which happens when the `pip install -e .` command is run)
doxy/InstDev.dox:\subsubsection cppcuda_docs C/C++ And CUDA Source
doxy/InstDev.dox:The C/C++/CUDA scripts use the so-called [Javadoc](https://en.wikipedia.org/wiki/Javadoc) style
doxy/InstDev.dox:\subsubsection runtests_cpp C/C++/CUDA
doxy/InstDev.dox:A small suite of unittests for the C/C++/CUDA backend is also present and is tested using [googletest](https://google.github.io/googletest/). 
doxy/InstDev.dox:Note that most backend functionality is implicitly tested through the (larger) Python unittesting suite, and if not developing in the C/C++/CUDA backend, the Python unittesting suite should be sufficient to test correct functioning of `PyPO`.

```
