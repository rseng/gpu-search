# https://github.com/yogi-tud/space_gpu

```console
visualize_exp2_only.py:    # draw line for GPU bandwith
visualize_exp2_only.py:    gpu_bandwith = 1448
visualize_exp2_only.py:        axes[pos].hlines(xmin=0.01, xmax=0.99, y=gpu_bandwith, linewidth=2, color='grey', linestyles="dotted",
visualize_exp2_only.py:                         label="Bandwith: " + str("{:1d}".format(gpu_bandwith)) + "[GiB/s]")
visualize_exp2_only.py:    #draw line for GPU bandwith
visualize_exp2_only.py:    #gpu_bandwith=1448
visualize_exp2_only.py:     #   axes[pos][pos2].hlines(xmin=0.01, xmax=0.99, y=gpu_bandwith, linewidth=2, color='grey', linestyles="dotted",
visualize_exp2_only.py:      #                   label="Bandwith: " + str("{:1d}".format(gpu_bandwith)) + "[GiB/s]")
visualize_exp2_only.py:     #   axes[pos].hlines(xmin=0.01, xmax=0.99, y=gpu_bandwith, linewidth=2, color='grey', linestyles="dotted",
visualize_exp2_only.py:      #               label="Bandwith: " + str("{:1d}".format(gpu_bandwith))+"[GiB/s]")
paper.bib:    author = {{NVIDIA CUB library}},
paper.bib:  title={Efficient algorithms for stream compaction on GPUs},
paper.bib:    title={Accelerating Parallel Operation for Compacting Selected Elements on GPUs},
paper.bib:    howpublished = {\url{https://github.com/yogi-tud/space_gpu/blob/main/Euro_Pars_2022_Compaction_CRC.pdf}}
paper.bib:    howpublished = {\url{https://github.com/yogi-tud/space_gpu/blob/main/overview.pdf}}
run_full.py:    #cmd = './build/gpu_compressstore2 '+run
run_full.py:    cmd = './gpu_compressstore2 ' + run
visualize_results.py:    # draw line for GPU bandwith
visualize_results.py:    gpu_bandwith = 1448
visualize_results.py:        axes[pos].hlines(xmin=0.01, xmax=0.99, y=gpu_bandwith, linewidth=2, color='grey', linestyles="dotted",
visualize_results.py:                         label="Bandwith: " + str("{:1d}".format(gpu_bandwith)) + "[GiB/s]")
visualize_results.py:    #draw line for GPU bandwith
visualize_results.py:    #gpu_bandwith=1448
visualize_results.py:     #   axes[pos][pos2].hlines(xmin=0.01, xmax=0.99, y=gpu_bandwith, linewidth=2, color='grey', linestyles="dotted",
visualize_results.py:      #                   label="Bandwith: " + str("{:1d}".format(gpu_bandwith)) + "[GiB/s]")
visualize_results.py:     #   axes[pos].hlines(xmin=0.01, xmax=0.99, y=gpu_bandwith, linewidth=2, color='grey', linestyles="dotted",
visualize_results.py:      #               label="Bandwith: " + str("{:1d}".format(gpu_bandwith))+"[GiB/s]")
deps/cub/tune/tune_device_reduce.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/tune/tune_device_reduce.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/tune/tune_device_reduce.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/tune/tune_device_reduce.cu:// Ensure printing of CUDA runtime errors to console
deps/cub/tune/tune_device_reduce.cu:        if (g_verify) CubDebugExit(cudaMemset(d_out, 0, sizeof(T)));
deps/cub/tune/tune_device_reduce.cu:        if (g_verify) CubDebugExit(cudaDeviceSynchronize());
deps/cub/tune/tune_device_reduce.cu:        GpuTimer gpu_timer;
deps/cub/tune/tune_device_reduce.cu:            gpu_timer.Start();
deps/cub/tune/tune_device_reduce.cu:            gpu_timer.Stop();
deps/cub/tune/tune_device_reduce.cu:            elapsed_millis += gpu_timer.ElapsedMillis();
deps/cub/tune/tune_device_reduce.cu:        CubDebugExit(cudaDeviceSynchronize());
deps/cub/tune/tune_device_reduce.cu:    CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(T) * g_max_items, cudaMemcpyHostToDevice));
deps/cub/LICENSE.TXT:Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/LICENSE.TXT:   *  Neither the name of the NVIDIA CORPORATION nor the
deps/cub/LICENSE.TXT:DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/experimental/histogram_compare.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/experimental/histogram_compare.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/experimental/histogram_compare.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/experimental/histogram_compare.cu:// Ensure printing of CUDA runtime errors to console
deps/cub/experimental/histogram_compare.cu:    // Copy data to gpu
deps/cub/experimental/histogram_compare.cu:    CubDebugExit(cudaMemcpy(d_pixels, h_pixels, pixel_bytes, cudaMemcpyHostToDevice));
deps/cub/experimental/histogram_compare.cu:    // Allocate results arrays on cpu/gpu
deps/cub/experimental/histogram_compare.cu:    // Get GPU device bandwidth (GB/s)
deps/cub/experimental/histogram_compare.cu:    CubDebugExit(cudaGetDevice(&device_ordinal));
deps/cub/experimental/histogram_compare.cu:    CubDebugExit(cudaDeviceGetAttribute(&bus_width, cudaDevAttrGlobalMemoryBusWidth, device_ordinal));
deps/cub/experimental/histogram_compare.cu:    CubDebugExit(cudaDeviceGetAttribute(&mem_clock_khz, cudaDevAttrMemoryClockRate, device_ordinal));
deps/cub/experimental/histogram_compare.cu:    CubDebugExit(cudaDeviceSynchronize());
deps/cub/experimental/histogram/histogram_cub.h: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/experimental/histogram/histogram_cub.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/experimental/histogram/histogram_cub.h: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/experimental/histogram/histogram_cub.h:        (cudaStream_t) 0,
deps/cub/experimental/histogram/histogram_cub.h:    cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/experimental/histogram/histogram_cub.h:    GpuTimer gpu_timer;
deps/cub/experimental/histogram/histogram_cub.h:    gpu_timer.Start();
deps/cub/experimental/histogram/histogram_cub.h:        (cudaStream_t) 0,
deps/cub/experimental/histogram/histogram_cub.h:    gpu_timer.Stop();
deps/cub/experimental/histogram/histogram_cub.h:    float elapsed_millis = gpu_timer.ElapsedMillis();
deps/cub/experimental/histogram/histogram_cub.h:    cudaFree(d_temp_storage);
deps/cub/experimental/histogram/histogram_gmem_atomics.h: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/experimental/histogram/histogram_gmem_atomics.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/experimental/histogram/histogram_gmem_atomics.h: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/experimental/histogram/histogram_gmem_atomics.h:    cudaDeviceProp props;
deps/cub/experimental/histogram/histogram_gmem_atomics.h:    cudaGetDeviceProperties(&props, 0);
deps/cub/experimental/histogram/histogram_gmem_atomics.h:    cudaMalloc(&d_part_hist, total_blocks * NUM_PARTS * sizeof(unsigned int));
deps/cub/experimental/histogram/histogram_gmem_atomics.h:    GpuTimer gpu_timer;
deps/cub/experimental/histogram/histogram_gmem_atomics.h:    gpu_timer.Start();
deps/cub/experimental/histogram/histogram_gmem_atomics.h:    gpu_timer.Stop();
deps/cub/experimental/histogram/histogram_gmem_atomics.h:    float elapsed_millis = gpu_timer.ElapsedMillis();
deps/cub/experimental/histogram/histogram_gmem_atomics.h:    cudaFree(d_part_hist);
deps/cub/experimental/histogram/histogram_smem_atomics.h: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/experimental/histogram/histogram_smem_atomics.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/experimental/histogram/histogram_smem_atomics.h: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/experimental/histogram/histogram_smem_atomics.h:    cudaDeviceProp props;
deps/cub/experimental/histogram/histogram_smem_atomics.h:    cudaGetDeviceProperties(&props, 0);
deps/cub/experimental/histogram/histogram_smem_atomics.h:    cudaMalloc(&d_part_hist, total_blocks * NUM_PARTS * sizeof(unsigned int));
deps/cub/experimental/histogram/histogram_smem_atomics.h:    GpuTimer gpu_timer;
deps/cub/experimental/histogram/histogram_smem_atomics.h:    gpu_timer.Start();
deps/cub/experimental/histogram/histogram_smem_atomics.h:    gpu_timer.Stop();
deps/cub/experimental/histogram/histogram_smem_atomics.h:    float elapsed_millis = gpu_timer.ElapsedMillis();
deps/cub/experimental/histogram/histogram_smem_atomics.h:    cudaFree(d_part_hist);
deps/cub/experimental/defunct/test_device_seg_reduce.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/experimental/defunct/test_device_seg_reduce.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/experimental/defunct/test_device_seg_reduce.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/experimental/defunct/test_device_seg_reduce.cu:// Ensure printing of CUDA runtime errors to console
deps/cub/experimental/defunct/test_device_seg_reduce.cu: * \brief BlockSegReduceTiles implements a stateful abstraction of CUDA thread blocks for participating in device-wide segmented reduction.
deps/cub/experimental/defunct/test_device_seg_reduce.cu: * \brief BlockSegReduceRegionByKey implements a stateful abstraction of CUDA thread blocks for participating in device-wide reduce-value-by-key.
deps/cub/experimental/defunct/test_device_seg_reduce.cu:    static cudaError_t Dispatch(
deps/cub/experimental/defunct/test_device_seg_reduce.cu:        cudaStream_t                    stream,                                 ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/experimental/defunct/test_device_seg_reduce.cu:        return CubDebug(cudaErrorNotSupported );
deps/cub/experimental/defunct/test_device_seg_reduce.cu:        cudaError error = cudaSuccess;
deps/cub/experimental/defunct/test_device_seg_reduce.cu:            if (CubDebug(error = cudaGetDevice(&device_ordinal))) break;
deps/cub/experimental/defunct/test_device_seg_reduce.cu:            if (CubDebug(error = cudaDeviceGetAttribute (&sm_count, cudaDevAttrMultiProcessorCount, device_ordinal))) break;
deps/cub/experimental/defunct/test_device_seg_reduce.cu:                return cudaSuccess;
deps/cub/experimental/defunct/test_device_seg_reduce.cu:            if (CubDebug(error = cudaDeviceSetSharedMemConfig(cudaSharedMemBankSizeEightByte))) break;
deps/cub/experimental/defunct/test_device_seg_reduce.cu:    static cudaError_t Dispatch(
deps/cub/experimental/defunct/test_device_seg_reduce.cu:        cudaStream_t                    stream,                                 ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/experimental/defunct/test_device_seg_reduce.cu:        cudaError error = cudaSuccess;
deps/cub/experimental/defunct/test_device_seg_reduce.cu:    static cudaError_t Reduce(
deps/cub/experimental/defunct/test_device_seg_reduce.cu:        cudaStream_t            stream              = 0,                ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/experimental/defunct/test_device_seg_reduce.cu:    static cudaError_t Sum(
deps/cub/experimental/defunct/test_device_seg_reduce.cu:        cudaStream_t            stream              = 0,                ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/experimental/defunct/test_device_seg_reduce.cu:    CubDebugExit(cudaMemcpy(d_values, h_values, sizeof(Value) * num_values, cudaMemcpyHostToDevice));
deps/cub/experimental/defunct/test_device_seg_reduce.cu:    CubDebugExit(cudaMemcpy(d_segment_offsets, h_segment_offsets, sizeof(OffsetT) * (num_segments + 1), cudaMemcpyHostToDevice));
deps/cub/experimental/defunct/test_device_seg_reduce.cu:    CubDebugExit(cudaMemset(d_output, 0, sizeof(Value) * num_segments));
deps/cub/experimental/defunct/test_device_seg_reduce.cu:    GpuTimer gpu_timer;
deps/cub/experimental/defunct/test_device_seg_reduce.cu:    gpu_timer.Start();
deps/cub/experimental/defunct/test_device_seg_reduce.cu:    gpu_timer.Stop();
deps/cub/experimental/defunct/test_device_seg_reduce.cu:    float elapsed_millis = gpu_timer.ElapsedMillis();
deps/cub/experimental/defunct/example_coo_spmv.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/experimental/defunct/example_coo_spmv.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/experimental/defunct/example_coo_spmv.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/experimental/defunct/example_coo_spmv.cu:// Ensure printing of CUDA runtime errors to console
deps/cub/experimental/defunct/example_coo_spmv.cu:    // Texture type to actually use (e.g., because CUDA doesn't load doubles as texture items)
deps/cub/experimental/defunct/example_coo_spmv.cu:    typedef texture<CastType, cudaTextureType1D, cudaReadModeElementType> TexRef;
deps/cub/experimental/defunct/example_coo_spmv.cu:        cudaChannelFormatDesc tex_desc = cudaCreateChannelDesc<CastType>();
deps/cub/experimental/defunct/example_coo_spmv.cu:            CubDebugExit(cudaBindTexture(&offset, ref, d_in, tex_desc, bytes));
deps/cub/experimental/defunct/example_coo_spmv.cu:        CubDebugExit(cudaUnbindTexture(ref));
deps/cub/experimental/defunct/example_coo_spmv.cu:    // Get CUDA properties
deps/cub/experimental/defunct/example_coo_spmv.cu:    CubDebugExit(cudaMemcpy(d_rows,     h_rows,     sizeof(VertexId) * num_edges,       cudaMemcpyHostToDevice));
deps/cub/experimental/defunct/example_coo_spmv.cu:    CubDebugExit(cudaMemcpy(d_columns,  h_columns,  sizeof(VertexId) * num_edges,       cudaMemcpyHostToDevice));
deps/cub/experimental/defunct/example_coo_spmv.cu:    CubDebugExit(cudaMemcpy(d_values,   h_values,   sizeof(Value) * num_edges,          cudaMemcpyHostToDevice));
deps/cub/experimental/defunct/example_coo_spmv.cu:    CubDebugExit(cudaMemcpy(d_vector,   h_vector,   sizeof(Value) * coo_graph.col_dim,  cudaMemcpyHostToDevice));
deps/cub/experimental/defunct/example_coo_spmv.cu:    CubDebugExit(cudaDeviceSetSharedMemConfig(cudaSharedMemBankSizeEightByte));
deps/cub/experimental/defunct/example_coo_spmv.cu:    GpuTimer gpu_timer;
deps/cub/experimental/defunct/example_coo_spmv.cu:        gpu_timer.Start();
deps/cub/experimental/defunct/example_coo_spmv.cu:        CubDebugExit(cudaMemset(d_result, 0, coo_graph.row_dim * sizeof(Value)));
deps/cub/experimental/defunct/example_coo_spmv.cu:        gpu_timer.Stop();
deps/cub/experimental/defunct/example_coo_spmv.cu:            elapsed_millis += gpu_timer.ElapsedMillis();
deps/cub/experimental/defunct/example_coo_spmv.cu:    CubDebugExit(cudaCudaSynchronize());
deps/cub/experimental/defunct/example_coo_spmv.cu:    // Run GPU version
deps/cub/experimental/spmv_compare.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/experimental/spmv_compare.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/experimental/spmv_compare.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/experimental/spmv_compare.cu:// Ensure printing of CUDA runtime errors to console
deps/cub/experimental/spmv_compare.cu:// GPU I/O proxy
deps/cub/experimental/spmv_compare.cu: * Run GPU I/O proxy
deps/cub/experimental/spmv_compare.cu:float TestGpuCsrIoProxy(
deps/cub/experimental/spmv_compare.cu:    CubDebugExit(cudaGetDevice(&device_ordinal));
deps/cub/experimental/spmv_compare.cu:    CubDebugExit(cudaPeekAtLastError());
deps/cub/experimental/spmv_compare.cu:    GpuTimer timer;
deps/cub/experimental/spmv_compare.cu:    cudaDeviceSynchronize();
deps/cub/experimental/spmv_compare.cu:    CubDebugExit(cudaMemcpy(params.d_vector_y, vector_y_in, sizeof(float) * params.num_rows, cudaMemcpyHostToDevice));
deps/cub/experimental/spmv_compare.cu:    GpuTimer timer;
deps/cub/experimental/spmv_compare.cu:    cudaDeviceSynchronize();
deps/cub/experimental/spmv_compare.cu:    CubDebugExit(cudaMemcpy(params.d_vector_y, vector_y_in, sizeof(float) * params.num_rows, cudaMemcpyHostToDevice));
deps/cub/experimental/spmv_compare.cu:    GpuTimer timer;
deps/cub/experimental/spmv_compare.cu:    CubDebugExit(cudaMemcpy(params.d_vector_y, vector_y_in, sizeof(float) * params.num_rows, cudaMemcpyHostToDevice));
deps/cub/experimental/spmv_compare.cu:    GpuTimer timer;
deps/cub/experimental/spmv_compare.cu:    CubDebugExit(cudaMemcpy(params.d_vector_y, vector_y_in, sizeof(float) * params.num_rows, cudaMemcpyHostToDevice));
deps/cub/experimental/spmv_compare.cu:    GpuTimer timer;
deps/cub/experimental/spmv_compare.cu:// GPU Merge-based SpMV
deps/cub/experimental/spmv_compare.cu:float TestGpuMergeCsrmv(
deps/cub/experimental/spmv_compare.cu:        (cudaStream_t) 0, false));
deps/cub/experimental/spmv_compare.cu:    CubDebugExit(cudaMemcpy(params.d_vector_y, vector_y_in, sizeof(ValueT) * params.num_rows, cudaMemcpyHostToDevice));
deps/cub/experimental/spmv_compare.cu:        (cudaStream_t) 0, !g_quiet));
deps/cub/experimental/spmv_compare.cu:    GpuTimer timer;
deps/cub/experimental/spmv_compare.cu:            (cudaStream_t) 0, false));
deps/cub/experimental/spmv_compare.cu:    // Get GPU device bandwidth (GB/s)
deps/cub/experimental/spmv_compare.cu:    // Allocate and initialize GPU problem
deps/cub/experimental/spmv_compare.cu:    CubDebugExit(cudaMemcpy(params.d_values,            csr_matrix.values,          sizeof(ValueT) * csr_matrix.num_nonzeros, cudaMemcpyHostToDevice));
deps/cub/experimental/spmv_compare.cu:    CubDebugExit(cudaMemcpy(params.d_row_end_offsets,   csr_matrix.row_offsets,     sizeof(OffsetT) * (csr_matrix.num_rows + 1), cudaMemcpyHostToDevice));
deps/cub/experimental/spmv_compare.cu:    CubDebugExit(cudaMemcpy(params.d_column_indices,    csr_matrix.column_indices,  sizeof(OffsetT) * csr_matrix.num_nonzeros, cudaMemcpyHostToDevice));
deps/cub/experimental/spmv_compare.cu:    CubDebugExit(cudaMemcpy(params.d_vector_x,          vector_x,                   sizeof(ValueT) * csr_matrix.num_cols, cudaMemcpyHostToDevice));
deps/cub/experimental/spmv_compare.cu:    printf("GPU CSR I/O Prox, "); fflush(stdout);
deps/cub/experimental/spmv_compare.cu:    avg_millis = TestGpuCsrIoProxy(params, timing_iterations);
deps/cub/experimental/spmv_compare.cu:        avg_millis = TestGpuMergeCsrmv(vector_y_in, vector_y_out, params, timing_iterations);
deps/cub/experimental/spmv_compare.cu:    CubDebugExit(cudaDeviceSynchronize());
deps/cub/experimental/sparse_matrix.h: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/experimental/sparse_matrix.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/experimental/sparse_matrix.h: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/test/test_device_select_if.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/test/test_device_select_if.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/test/test_device_select_if.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/test/test_device_select_if.cu:// Ensure printing of CUDA runtime errors to console
deps/cub/test/test_device_select_if.cu:    CDP,        // GPU-based (dynamic parallelism) dispatch to CUB method
deps/cub/test/test_device_select_if.cu:cudaError_t Dispatch(
deps/cub/test/test_device_select_if.cu:    cudaError_t*                /*d_cdp_error*/,
deps/cub/test/test_device_select_if.cu:    cudaStream_t                stream,
deps/cub/test/test_device_select_if.cu:    cudaError_t error = cudaSuccess;
deps/cub/test/test_device_select_if.cu:cudaError_t Dispatch(
deps/cub/test/test_device_select_if.cu:    cudaError_t*                /*d_cdp_error*/,
deps/cub/test/test_device_select_if.cu:    cudaStream_t                stream,
deps/cub/test/test_device_select_if.cu:    cudaError_t error = cudaSuccess;
deps/cub/test/test_device_select_if.cu:cudaError_t Dispatch(
deps/cub/test/test_device_select_if.cu:    cudaError_t*                /*d_cdp_error*/,
deps/cub/test/test_device_select_if.cu:    cudaStream_t                stream,
deps/cub/test/test_device_select_if.cu:    cudaError_t error = cudaSuccess;
deps/cub/test/test_device_select_if.cu:cudaError_t Dispatch(
deps/cub/test/test_device_select_if.cu:    cudaError_t*                /*d_cdp_error*/,
deps/cub/test/test_device_select_if.cu:    cudaStream_t                stream,
deps/cub/test/test_device_select_if.cu:    cudaError_t error = cudaSuccess;
deps/cub/test/test_device_select_if.cu:// CUDA Nested Parallelism Test Kernel
deps/cub/test/test_device_select_if.cu:    cudaError_t*                d_cdp_error,
deps/cub/test/test_device_select_if.cu:    *d_cdp_error = cudaErrorNotSupported;
deps/cub/test/test_device_select_if.cu:cudaError_t Dispatch(
deps/cub/test/test_device_select_if.cu:    cudaError_t*                d_cdp_error,
deps/cub/test/test_device_select_if.cu:    cudaStream_t                stream,
deps/cub/test/test_device_select_if.cu:    CubDebugExit(cudaMemcpy(&temp_storage_bytes, d_temp_storage_bytes, sizeof(size_t) * 1, cudaMemcpyDeviceToHost));
deps/cub/test/test_device_select_if.cu:    cudaError_t retval;
deps/cub/test/test_device_select_if.cu:    CubDebugExit(cudaMemcpy(&retval, d_cdp_error, sizeof(cudaError_t) * 1, cudaMemcpyDeviceToHost));
deps/cub/test/test_device_select_if.cu:    cudaError_t*    d_cdp_error = NULL;
deps/cub/test/test_device_select_if.cu:    CubDebugExit(g_allocator.DeviceAllocate((void**)&d_cdp_error,           sizeof(cudaError_t) * 1));
deps/cub/test/test_device_select_if.cu:    CubDebugExit(cudaMemcpy(d_flags, h_flags, sizeof(FlagT) * num_items, cudaMemcpyHostToDevice));
deps/cub/test/test_device_select_if.cu:    CubDebugExit(cudaMemset(d_out, 0, sizeof(T) * num_items));
deps/cub/test/test_device_select_if.cu:    CubDebugExit(cudaMemset(d_num_selected_out, 0, sizeof(int)));
deps/cub/test/test_device_select_if.cu:    GpuTimer gpu_timer;
deps/cub/test/test_device_select_if.cu:    gpu_timer.Start();
deps/cub/test/test_device_select_if.cu:    gpu_timer.Stop();
deps/cub/test/test_device_select_if.cu:    float elapsed_millis = gpu_timer.ElapsedMillis();
deps/cub/test/test_device_select_if.cu:    CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(T) * num_items, cudaMemcpyHostToDevice));
deps/cub/test/test_warp_merge_sort.cu: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/cub/test/test_warp_merge_sort.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/test/test_warp_merge_sort.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/test/test_warp_merge_sort.cu:// Ensure printing of CUDA runtime errors to console
deps/cub/test/test_warp_merge_sort.cu:  CubDebugExit(cudaPeekAtLastError());
deps/cub/test/test_warp_merge_sort.cu:  CubDebugExit(cudaDeviceSynchronize());
deps/cub/test/test_warp_merge_sort.cu:  CubDebugExit(cudaPeekAtLastError());
deps/cub/test/test_warp_merge_sort.cu:  CubDebugExit(cudaDeviceSynchronize());
deps/cub/test/test_temporary_storage_layout.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/test/test_temporary_storage_layout.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/test/test_temporary_storage_layout.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/test/test_temporary_storage_layout.cu:// Ensure printing of CUDA runtime errors to console
deps/cub/test/test_util.h: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/test/test_util.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/test/test_util.h: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/test/test_util.h:    cudaDeviceProp              deviceProp;
deps/cub/test/test_util.h:    cudaError_t DeviceInit(int dev = -1)
deps/cub/test/test_util.h:        cudaError_t error = cudaSuccess;
deps/cub/test/test_util.h:            error = CubDebug(cudaGetDeviceCount(&deviceCount));
deps/cub/test/test_util.h:                fprintf(stderr, "No devices supporting CUDA.\n");
deps/cub/test/test_util.h:            error = CubDebug(cudaSetDevice(dev));
deps/cub/test/test_util.h:            CubDebugExit(cudaMemGetInfo(&device_free_physmem, &device_total_physmem));
deps/cub/test/test_util.h:            error = CubDebug(cudaGetDeviceProperties(&deviceProp, dev));
deps/cub/test/test_util.h:                fprintf(stderr, "Device does not support CUDA.\n");
deps/cub/test/test_util.h:    CubDebugExit(cudaGetDevice(&device));
deps/cub/test/test_util.h:    CubDebugExit(cudaMemGetInfo(&free_mem, &total_mem));
deps/cub/test/test_util.h: * Comparison and ostream operators for CUDA vector types
deps/cub/test/test_util.h:    cudaMemcpy(h_data, d_data, sizeof(T) * num_items, cudaMemcpyDeviceToHost);
deps/cub/test/test_util.h:    cudaMemcpy(h_reference, d_reference, sizeof(T) * num_items, cudaMemcpyDeviceToHost);
deps/cub/test/test_util.h:    cudaMemcpy(h_data, d_data, sizeof(T) * num_items, cudaMemcpyDeviceToHost);
deps/cub/test/test_util.h:    cudaMemcpy(h_data, d_data, sizeof(T) * num_items, cudaMemcpyDeviceToHost);
deps/cub/test/test_util.h:struct GpuTimer
deps/cub/test/test_util.h:    cudaEvent_t start;
deps/cub/test/test_util.h:    cudaEvent_t stop;
deps/cub/test/test_util.h:    GpuTimer()
deps/cub/test/test_util.h:        cudaEventCreate(&start);
deps/cub/test/test_util.h:        cudaEventCreate(&stop);
deps/cub/test/test_util.h:    ~GpuTimer()
deps/cub/test/test_util.h:        cudaEventDestroy(start);
deps/cub/test/test_util.h:        cudaEventDestroy(stop);
deps/cub/test/test_util.h:        cudaEventRecord(start, 0);
deps/cub/test/test_util.h:        cudaEventRecord(stop, 0);
deps/cub/test/test_util.h:        cudaEventSynchronize(stop);
deps/cub/test/test_util.h:        cudaEventElapsedTime(&elapsed, start, stop);
deps/cub/test/test_thread_sort.cu: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/cub/test/test_thread_sort.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/test/test_thread_sort.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/test/test_warp_mask.cu: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/cub/test/test_warp_mask.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/test/test_warp_mask.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/test/test_warp_mask.cu:// Ensure printing of CUDA runtime errors to console
deps/cub/test/test_warp_reduce.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/test/test_warp_reduce.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/test/test_warp_reduce.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/test/test_warp_reduce.cu:// Ensure printing of CUDA runtime errors to console
deps/cub/test/test_warp_reduce.cu:    CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(T) * BLOCK_THREADS, cudaMemcpyHostToDevice));
deps/cub/test/test_warp_reduce.cu:    CubDebugExit(cudaMemset(d_out, 0, sizeof(T) * BLOCK_THREADS));
deps/cub/test/test_warp_reduce.cu:    CubDebugExit(cudaPeekAtLastError());
deps/cub/test/test_warp_reduce.cu:    CubDebugExit(cudaDeviceSynchronize());
deps/cub/test/test_warp_reduce.cu:    CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(T) * BLOCK_THREADS, cudaMemcpyHostToDevice));
deps/cub/test/test_warp_reduce.cu:    CubDebugExit(cudaMemcpy(d_flags, h_flags, sizeof(int) * BLOCK_THREADS, cudaMemcpyHostToDevice));
deps/cub/test/test_warp_reduce.cu:    CubDebugExit(cudaMemset(d_head_out, 0, sizeof(T) * BLOCK_THREADS));
deps/cub/test/test_warp_reduce.cu:    CubDebugExit(cudaMemset(d_tail_out, 0, sizeof(T) * BLOCK_THREADS));
deps/cub/test/test_warp_reduce.cu:    CubDebugExit(cudaPeekAtLastError());
deps/cub/test/test_warp_reduce.cu:    CubDebugExit(cudaDeviceSynchronize());
deps/cub/test/test_warp_reduce.cu:    CubDebugExit(cudaPeekAtLastError());
deps/cub/test/test_warp_reduce.cu:    CubDebugExit(cudaDeviceSynchronize());
deps/cub/test/test_device_segmented_sort.cu: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/cub/test/test_device_segmented_sort.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/test/test_device_segmented_sort.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/test/test_device_segmented_sort.cu:// Ensure printing of CUDA runtime errors to console
deps/cub/test/test_device_segmented_sort.cu:  (__CUDACC_VER_MAJOR__ >= 9 || CUDA_VERSION >= 9000) && !_NVHPC_CUDA
deps/cub/test/test_device_segmented_sort.cu:  (__CUDACC_VER_MAJOR__ >= 11 || CUDA_VERSION >= 11000) && !_NVHPC_CUDA
deps/cub/test/test_device_segmented_sort.cu:#include <cuda_fp16.h>
deps/cub/test/test_device_segmented_sort.cu:#include <cuda_bf16.h>
deps/cub/test/test_device_segmented_sort.cu:    cudaMemcpy(keys_output,
deps/cub/test/test_device_segmented_sort.cu:               cudaMemcpyDeviceToHost);
deps/cub/test/test_device_segmented_sort.cu:    cudaMemcpy(values_output,
deps/cub/test/test_device_segmented_sort.cu:               cudaMemcpyDeviceToHost);
deps/cub/test/test_device_segmented_sort.cu:  CUB_RUNTIME_FUNCTION __forceinline__ cudaError_t Invoke()
deps/cub/test/test_device_segmented_sort.cu:    return cudaSuccess;
deps/cub/test/test_device_segmented_sort.cu:  CUB_RUNTIME_FUNCTION __forceinline__ cudaError_t Invoke()
deps/cub/test/test_device_segmented_sort.cu:    return cudaSuccess;
deps/cub/test/test_device_segmented_sort.cu:    CubDebugExit(cudaDeviceSynchronize());
deps/cub/test/test_device_segmented_sort.cu:    CubDebugExit(cudaPeekAtLastError());
deps/cub/test/test_device_merge_sort.cu: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/cub/test/test_device_merge_sort.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/test/test_device_merge_sort.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/test/test_device_merge_sort.cu:// Ensure printing of CUDA runtime errors to console
deps/cub/test/test_device_radix_sort.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/test/test_device_radix_sort.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/test/test_device_radix_sort.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/test/test_device_radix_sort.cu:// Ensure printing of CUDA runtime errors to console
deps/cub/test/test_device_radix_sort.cu:#if (__CUDACC_VER_MAJOR__ >= 9 || CUDA_VERSION >= 9000) && !_NVHPC_CUDA
deps/cub/test/test_device_radix_sort.cu:    #include <cuda_fp16.h>
deps/cub/test/test_device_radix_sort.cu:#if (__CUDACC_VER_MAJOR__ >= 11 || CUDA_VERSION >= 11000) && !_NVHPC_CUDA
deps/cub/test/test_device_radix_sort.cu:    #include <cuda_bf16.h>
deps/cub/test/test_device_radix_sort.cu:    CDP,                        // GPU-based (dynamic parallelism) dispatch to CUB method
deps/cub/test/test_device_radix_sort.cu:cudaError_t Dispatch(
deps/cub/test/test_device_radix_sort.cu:    cudaError_t             */*d_cdp_error*/,
deps/cub/test/test_device_radix_sort.cu:    cudaStream_t            stream,
deps/cub/test/test_device_radix_sort.cu:cudaError_t Dispatch(
deps/cub/test/test_device_radix_sort.cu:    cudaError_t                 */*d_cdp_error*/,
deps/cub/test/test_device_radix_sort.cu:    cudaStream_t            stream,
deps/cub/test/test_device_radix_sort.cu:    cudaError_t retval = DeviceRadixSort::SortPairs(
deps/cub/test/test_device_radix_sort.cu:cudaError_t Dispatch(
deps/cub/test/test_device_radix_sort.cu:    cudaError_t             */*d_cdp_error*/,
deps/cub/test/test_device_radix_sort.cu:    cudaStream_t            stream,
deps/cub/test/test_device_radix_sort.cu:cudaError_t Dispatch(
deps/cub/test/test_device_radix_sort.cu:    cudaError_t                 */*d_cdp_error*/,
deps/cub/test/test_device_radix_sort.cu:    cudaStream_t            stream,
deps/cub/test/test_device_radix_sort.cu:    cudaError_t retval = DeviceRadixSort::SortPairsDescending(
deps/cub/test/test_device_radix_sort.cu:cudaError_t Dispatch(
deps/cub/test/test_device_radix_sort.cu:    cudaError_t             */*d_cdp_error*/,
deps/cub/test/test_device_radix_sort.cu:    cudaStream_t            stream,
deps/cub/test/test_device_radix_sort.cu:cudaError_t Dispatch(
deps/cub/test/test_device_radix_sort.cu:    cudaError_t                             */*d_cdp_error*/,
deps/cub/test/test_device_radix_sort.cu:    cudaStream_t            stream,
deps/cub/test/test_device_radix_sort.cu:    cudaError_t retval = DeviceSegmentedRadixSort::SortPairs(
deps/cub/test/test_device_radix_sort.cu:cudaError_t Dispatch(
deps/cub/test/test_device_radix_sort.cu:    cudaError_t             */*d_cdp_error*/,
deps/cub/test/test_device_radix_sort.cu:    cudaStream_t            stream,
deps/cub/test/test_device_radix_sort.cu:cudaError_t Dispatch(
deps/cub/test/test_device_radix_sort.cu:    cudaError_t                             */*d_cdp_error*/,
deps/cub/test/test_device_radix_sort.cu:    cudaStream_t            stream,
deps/cub/test/test_device_radix_sort.cu:    cudaError_t retval = DeviceSegmentedRadixSort::SortPairsDescending(
deps/cub/test/test_device_radix_sort.cu:// CUDA Nested Parallelism Test Kernel
deps/cub/test/test_device_radix_sort.cu:    cudaError_t             *d_cdp_error,
deps/cub/test/test_device_radix_sort.cu:    *d_cdp_error            = cudaErrorNotSupported;
deps/cub/test/test_device_radix_sort.cu:cudaError_t Dispatch(
deps/cub/test/test_device_radix_sort.cu:    cudaError_t             *d_cdp_error,
deps/cub/test/test_device_radix_sort.cu:    cudaStream_t            stream,
deps/cub/test/test_device_radix_sort.cu:    CubDebugExit(cudaMemcpy(&d_keys.selector, d_selector, sizeof(int) * 1, cudaMemcpyDeviceToHost));
deps/cub/test/test_device_radix_sort.cu:    CubDebugExit(cudaMemcpy(&temp_storage_bytes, d_temp_storage_bytes, sizeof(size_t) * 1, cudaMemcpyDeviceToHost));
deps/cub/test/test_device_radix_sort.cu:    cudaError_t retval;
deps/cub/test/test_device_radix_sort.cu:    CubDebugExit(cudaMemcpy(&retval, d_cdp_error, sizeof(cudaError_t) * 1, cudaMemcpyDeviceToHost));
deps/cub/test/test_device_radix_sort.cu:#if (__CUDACC_VER_MAJOR__ >= 9 || CUDA_VERSION >= 9000) && !_NVHPC_CUDA
deps/cub/test/test_device_radix_sort.cu:#if (__CUDACC_VER_MAJOR__ >= 11 || CUDA_VERSION >= 11000) && !_NVHPC_CUDA
deps/cub/test/test_device_radix_sort.cu:    cudaError_t             *d_cdp_error;
deps/cub/test/test_device_radix_sort.cu:    CubDebugExit(g_allocator.DeviceAllocate((void**)&d_cdp_error, sizeof(cudaError_t) * 1));
deps/cub/test/test_device_radix_sort.cu:    CubDebugExit(cudaMemcpy(d_keys.d_buffers[0], h_keys, sizeof(KeyT) * num_items, cudaMemcpyHostToDevice));
deps/cub/test/test_device_radix_sort.cu:    CubDebugExit(cudaMemset(d_keys.d_buffers[1], 0, sizeof(KeyT) * num_items));
deps/cub/test/test_device_radix_sort.cu:        CubDebugExit(cudaMemcpy(d_values.d_buffers[0], h_values, sizeof(ValueT) * num_items, cudaMemcpyHostToDevice));
deps/cub/test/test_device_radix_sort.cu:        CubDebugExit(cudaMemset(d_values.d_buffers[1], 0, sizeof(ValueT) * num_items));
deps/cub/test/test_device_radix_sort.cu:    GpuTimer gpu_timer;
deps/cub/test/test_device_radix_sort.cu:        CubDebugExit(cudaMemcpy(d_keys.d_buffers[d_keys.selector], h_keys, sizeof(KeyT) * num_items, cudaMemcpyHostToDevice));
deps/cub/test/test_device_radix_sort.cu:        CubDebugExit(cudaMemset(d_keys.d_buffers[d_keys.selector ^ 1], 0, sizeof(KeyT) * num_items));
deps/cub/test/test_device_radix_sort.cu:            CubDebugExit(cudaMemcpy(d_values.d_buffers[d_values.selector], h_values, sizeof(ValueT) * num_items, cudaMemcpyHostToDevice));
deps/cub/test/test_device_radix_sort.cu:            CubDebugExit(cudaMemset(d_values.d_buffers[d_values.selector ^ 1], 0, sizeof(ValueT) * num_items));
deps/cub/test/test_device_radix_sort.cu:        gpu_timer.Start();
deps/cub/test/test_device_radix_sort.cu:        gpu_timer.Stop();
deps/cub/test/test_device_radix_sort.cu:        elapsed_millis += gpu_timer.ElapsedMillis();
deps/cub/test/test_device_radix_sort.cu:    CubDebugExit(cudaMemcpy(d_segment_offsets, h_segment_offsets, sizeof(std::size_t) * (num_segments + 1), cudaMemcpyHostToDevice));
deps/cub/test/test_device_radix_sort.cu:    CubDebugExit(cudaMemcpy(d_segment_offsets, h_segment_offsets, sizeof(std::size_t) * (num_segments + 1), cudaMemcpyHostToDevice));
deps/cub/test/test_device_radix_sort.cu:#if (__CUDACC_VER_MAJOR__ >= 9 || CUDA_VERSION >= 9000) && !_NVHPC_CUDA
deps/cub/test/test_device_radix_sort.cu:#if (__CUDACC_VER_MAJOR__ >= 11 || CUDA_VERSION >= 11000) && !_NVHPC_CUDA
deps/cub/test/test_block_reduce.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/test/test_block_reduce.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/test/test_block_reduce.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/test/test_block_reduce.cu:// Ensure printing of CUDA runtime errors to console
deps/cub/test/test_block_reduce.cu:#include <cuda_runtime_api.h>
deps/cub/test/test_block_reduce.cu:    CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(T) * num_items, cudaMemcpyHostToDevice));
deps/cub/test/test_block_reduce.cu:    CubDebugExit(cudaMemset(d_out, 0, sizeof(T) * 1));
deps/cub/test/test_block_reduce.cu:    CubDebugExit(cudaPeekAtLastError());
deps/cub/test/test_block_reduce.cu:    CubDebugExit(cudaDeviceSynchronize());
deps/cub/test/test_block_reduce.cu:    CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(T) * num_items, cudaMemcpyHostToDevice));
deps/cub/test/test_block_reduce.cu:    CubDebugExit(cudaMemset(d_out, 0, sizeof(T) * 1));
deps/cub/test/test_block_reduce.cu:    CubDebugExit(cudaPeekAtLastError());
deps/cub/test/test_block_reduce.cu:    CubDebugExit(cudaDeviceSynchronize());
deps/cub/test/test_warp_load.cu: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/cub/test/test_warp_load.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/test/test_warp_load.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/test/test_warp_load.cu:// Ensure printing of CUDA runtime errors to console
deps/cub/test/test_warp_load.cu:  CubDebugExit(cudaPeekAtLastError());
deps/cub/test/test_warp_load.cu:  CubDebugExit(cudaDeviceSynchronize());
deps/cub/test/test_warp_load.cu:  CubDebugExit(cudaPeekAtLastError());
deps/cub/test/test_warp_load.cu:  CubDebugExit(cudaDeviceSynchronize());
deps/cub/test/test_device_reduce_by_key.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/test/test_device_reduce_by_key.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/test/test_device_reduce_by_key.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/test/test_device_reduce_by_key.cu:// Ensure printing of CUDA runtime errors to console
deps/cub/test/test_device_reduce_by_key.cu:    CDP,        // GPU-based (dynamic parallelism) dispatch to CUB method
deps/cub/test/test_device_reduce_by_key.cu:cudaError_t Dispatch(
deps/cub/test/test_device_reduce_by_key.cu:    cudaError_t                 */*d_cdp_error*/,
deps/cub/test/test_device_reduce_by_key.cu:    cudaStream_t                stream,
deps/cub/test/test_device_reduce_by_key.cu:    cudaError_t error = cudaSuccess;
deps/cub/test/test_device_reduce_by_key.cu:// CUDA Nested Parallelism Test Kernel
deps/cub/test/test_device_reduce_by_key.cu:    cudaError_t                 *d_cdp_error,
deps/cub/test/test_device_reduce_by_key.cu:    cudaStream_t                stream,
deps/cub/test/test_device_reduce_by_key.cu:    *d_cdp_error = cudaErrorNotSupported;
deps/cub/test/test_device_reduce_by_key.cu:cudaError_t Dispatch(
deps/cub/test/test_device_reduce_by_key.cu:    cudaError_t                 *d_cdp_error,
deps/cub/test/test_device_reduce_by_key.cu:    cudaStream_t                stream,
deps/cub/test/test_device_reduce_by_key.cu:    CubDebugExit(cudaMemcpy(&temp_storage_bytes, d_temp_storage_bytes, sizeof(size_t) * 1, cudaMemcpyDeviceToHost));
deps/cub/test/test_device_reduce_by_key.cu:    cudaError_t retval;
deps/cub/test/test_device_reduce_by_key.cu:    CubDebugExit(cudaMemcpy(&retval, d_cdp_error, sizeof(cudaError_t) * 1, cudaMemcpyDeviceToHost));
deps/cub/test/test_device_reduce_by_key.cu:    cudaError_t     *d_cdp_error = NULL;
deps/cub/test/test_device_reduce_by_key.cu:    CubDebugExit(g_allocator.DeviceAllocate((void**)&d_cdp_error,           sizeof(cudaError_t) * 1));
deps/cub/test/test_device_reduce_by_key.cu:    CubDebugExit(cudaMemset(d_keys_out, 0, sizeof(KeyT) * num_items));
deps/cub/test/test_device_reduce_by_key.cu:    CubDebugExit(cudaMemset(d_values_out, 0, sizeof(ValueT) * num_items));
deps/cub/test/test_device_reduce_by_key.cu:    CubDebugExit(cudaMemset(d_num_runs, 0, sizeof(int)));
deps/cub/test/test_device_reduce_by_key.cu:    GpuTimer gpu_timer;
deps/cub/test/test_device_reduce_by_key.cu:    gpu_timer.Start();
deps/cub/test/test_device_reduce_by_key.cu:    gpu_timer.Stop();
deps/cub/test/test_device_reduce_by_key.cu:    float elapsed_millis = gpu_timer.ElapsedMillis();
deps/cub/test/test_device_reduce_by_key.cu:    CubDebugExit(cudaMemcpy(d_keys_in, h_keys_in, sizeof(KeyT) * num_items, cudaMemcpyHostToDevice));
deps/cub/test/test_device_reduce_by_key.cu:    CubDebugExit(cudaMemcpy(d_values_in, h_values_in, sizeof(ValueT) * num_items, cudaMemcpyHostToDevice));
deps/cub/test/test_device_reduce_by_key.cu:    CubDebugExit(cudaMemcpy(d_keys_in, h_keys_in, sizeof(KeyT) * num_items, cudaMemcpyHostToDevice));
deps/cub/test/test_device_adjacent_difference.cu: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/cub/test/test_device_adjacent_difference.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/test/test_device_adjacent_difference.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/test/test_device_adjacent_difference.cu:// Ensure printing of CUDA runtime errors to console
deps/cub/test/test_warp_store.cu: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/cub/test/test_warp_store.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/test/test_warp_store.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/test/test_warp_store.cu:// Ensure printing of CUDA runtime errors to console
deps/cub/test/test_warp_store.cu:  CubDebugExit(cudaPeekAtLastError());
deps/cub/test/test_warp_store.cu:  CubDebugExit(cudaDeviceSynchronize());
deps/cub/test/test_warp_store.cu:  CubDebugExit(cudaPeekAtLastError());
deps/cub/test/test_warp_store.cu:  CubDebugExit(cudaDeviceSynchronize());
deps/cub/test/test_device_scan.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/test/test_device_scan.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/test/test_device_scan.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/test/test_device_scan.cu:// Ensure printing of CUDA runtime errors to console
deps/cub/test/test_device_scan.cu:    CDP,        // GPU-based (dynamic parallelism) dispatch to CUB method
deps/cub/test/test_device_scan.cu:cudaError_t Dispatch(
deps/cub/test/test_device_scan.cu:    cudaError_t         */*d_cdp_error*/,
deps/cub/test/test_device_scan.cu:    cudaStream_t        stream,
deps/cub/test/test_device_scan.cu:    cudaError_t error = cudaSuccess;
deps/cub/test/test_device_scan.cu:cudaError_t Dispatch(
deps/cub/test/test_device_scan.cu:    cudaError_t         */*d_cdp_error*/,
deps/cub/test/test_device_scan.cu:    cudaStream_t        stream,
deps/cub/test/test_device_scan.cu:    cudaError_t error = cudaSuccess;
deps/cub/test/test_device_scan.cu:cudaError_t Dispatch(
deps/cub/test/test_device_scan.cu:    cudaError_t         */*d_cdp_error*/,
deps/cub/test/test_device_scan.cu:    cudaStream_t        stream,
deps/cub/test/test_device_scan.cu:    cudaError_t error = cudaSuccess;
deps/cub/test/test_device_scan.cu:cudaError_t Dispatch(
deps/cub/test/test_device_scan.cu:    cudaError_t         */*d_cdp_error*/,
deps/cub/test/test_device_scan.cu:    cudaStream_t        stream,
deps/cub/test/test_device_scan.cu:    cudaError_t error = cudaSuccess;
deps/cub/test/test_device_scan.cu:// CUDA Nested Parallelism Test Kernel
deps/cub/test/test_device_scan.cu:    cudaError_t         *d_cdp_error,
deps/cub/test/test_device_scan.cu:    *d_cdp_error = cudaErrorNotSupported;
deps/cub/test/test_device_scan.cu:cudaError_t Dispatch(
deps/cub/test/test_device_scan.cu:    cudaError_t         *d_cdp_error,
deps/cub/test/test_device_scan.cu:    cudaStream_t        stream,
deps/cub/test/test_device_scan.cu:    CubDebugExit(cudaMemcpy(&temp_storage_bytes, d_temp_storage_bytes, sizeof(size_t) * 1, cudaMemcpyDeviceToHost));
deps/cub/test/test_device_scan.cu:    cudaError_t retval;
deps/cub/test/test_device_scan.cu:    CubDebugExit(cudaMemcpy(&retval, d_cdp_error, sizeof(cudaError_t) * 1, cudaMemcpyDeviceToHost));
deps/cub/test/test_device_scan.cu:    cudaError_t     *d_cdp_error = NULL;
deps/cub/test/test_device_scan.cu:    CubDebugExit(g_allocator.DeviceAllocate((void**)&d_cdp_error,   sizeof(cudaError_t) * 1));
deps/cub/test/test_device_scan.cu:    CubDebugExit(cudaMemset(d_out, 0, sizeof(OutputT) * num_items));
deps/cub/test/test_device_scan.cu:    GpuTimer gpu_timer;
deps/cub/test/test_device_scan.cu:    gpu_timer.Start();
deps/cub/test/test_device_scan.cu:    gpu_timer.Stop();
deps/cub/test/test_device_scan.cu:    float elapsed_millis = gpu_timer.ElapsedMillis();
deps/cub/test/test_device_scan.cu:    CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(InputT) * num_items, cudaMemcpyHostToDevice));
deps/cub/test/test_block_histogram.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/test/test_block_histogram.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/test/test_block_histogram.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/test/test_block_histogram.cu:// Ensure printing of CUDA runtime errors to console
deps/cub/test/test_block_histogram.cu:    CubDebugExit(cudaMemcpy(d_samples, h_samples, sizeof(SampleT) * num_samples, cudaMemcpyHostToDevice));
deps/cub/test/test_block_histogram.cu:    CubDebugExit(cudaMemset(d_histogram, 0, sizeof(int) * BINS));
deps/cub/test/test_block_histogram.cu:    CubDebugExit(cudaPeekAtLastError());
deps/cub/test/test_block_histogram.cu:    CubDebugExit(cudaDeviceSynchronize());
deps/cub/test/test_device_reduce.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/test/test_device_reduce.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/test/test_device_reduce.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/test/test_device_reduce.cu:// Ensure printing of CUDA runtime errors to console
deps/cub/test/test_device_reduce.cu:    CUB_CDP,        // GPU-based (dynamic parallelism) dispatch to CUB method
deps/cub/test/test_device_reduce.cu:cudaError_t Dispatch(
deps/cub/test/test_device_reduce.cu:    cudaError_t         */*d_cdp_error*/,
deps/cub/test/test_device_reduce.cu:    cudaStream_t        stream,
deps/cub/test/test_device_reduce.cu:    cudaError_t error = cudaSuccess;
deps/cub/test/test_device_reduce.cu:cudaError_t Dispatch(
deps/cub/test/test_device_reduce.cu:    cudaError_t         */*d_cdp_error*/,
deps/cub/test/test_device_reduce.cu:    cudaStream_t        stream,
deps/cub/test/test_device_reduce.cu:    cudaError_t error = cudaSuccess;
deps/cub/test/test_device_reduce.cu:cudaError_t Dispatch(
deps/cub/test/test_device_reduce.cu:    cudaError_t         */*d_cdp_error*/,
deps/cub/test/test_device_reduce.cu:    cudaStream_t        stream,
deps/cub/test/test_device_reduce.cu:    cudaError_t error = cudaSuccess;
deps/cub/test/test_device_reduce.cu:cudaError_t Dispatch(
deps/cub/test/test_device_reduce.cu:    cudaError_t         */*d_cdp_error*/,
deps/cub/test/test_device_reduce.cu:    cudaStream_t        stream,
deps/cub/test/test_device_reduce.cu:    cudaError_t error = cudaSuccess;
deps/cub/test/test_device_reduce.cu:cudaError_t Dispatch(
deps/cub/test/test_device_reduce.cu:    cudaError_t         */*d_cdp_error*/,
deps/cub/test/test_device_reduce.cu:    cudaStream_t        stream,
deps/cub/test/test_device_reduce.cu:    cudaError_t error = cudaSuccess;
deps/cub/test/test_device_reduce.cu:cudaError_t Dispatch(
deps/cub/test/test_device_reduce.cu:    cudaError_t         */*d_cdp_error*/,
deps/cub/test/test_device_reduce.cu:    cudaStream_t        stream,
deps/cub/test/test_device_reduce.cu:    cudaError_t error = cudaSuccess;
deps/cub/test/test_device_reduce.cu:cudaError_t Dispatch(
deps/cub/test/test_device_reduce.cu:    cudaError_t         */*d_cdp_error*/,
deps/cub/test/test_device_reduce.cu:    cudaStream_t        stream,
deps/cub/test/test_device_reduce.cu:    cudaError_t error = cudaSuccess;
deps/cub/test/test_device_reduce.cu:cudaError_t Dispatch(
deps/cub/test/test_device_reduce.cu:    cudaError_t         */*d_cdp_error*/,
deps/cub/test/test_device_reduce.cu:    cudaStream_t        stream,
deps/cub/test/test_device_reduce.cu:    cudaError_t error = cudaSuccess;
deps/cub/test/test_device_reduce.cu:cudaError_t Dispatch(
deps/cub/test/test_device_reduce.cu:    cudaError_t         */*d_cdp_error*/,
deps/cub/test/test_device_reduce.cu:    cudaStream_t        stream,
deps/cub/test/test_device_reduce.cu:    cudaError_t error = cudaSuccess;
deps/cub/test/test_device_reduce.cu:cudaError_t Dispatch(
deps/cub/test/test_device_reduce.cu:    cudaError_t         */*d_cdp_error*/,
deps/cub/test/test_device_reduce.cu:    cudaStream_t        stream,
deps/cub/test/test_device_reduce.cu:    cudaError_t error = cudaSuccess;
deps/cub/test/test_device_reduce.cu:cudaError_t Dispatch(
deps/cub/test/test_device_reduce.cu:    cudaError_t         */*d_cdp_error*/,
deps/cub/test/test_device_reduce.cu:    cudaStream_t        stream,
deps/cub/test/test_device_reduce.cu:    cudaError_t error = cudaSuccess;
deps/cub/test/test_device_reduce.cu:cudaError_t Dispatch(
deps/cub/test/test_device_reduce.cu:    cudaError_t         */*d_cdp_error*/,
deps/cub/test/test_device_reduce.cu:    cudaStream_t        stream,
deps/cub/test/test_device_reduce.cu:    cudaError_t error = cudaSuccess;
deps/cub/test/test_device_reduce.cu:// CUDA nested-parallelism test kernel
deps/cub/test/test_device_reduce.cu:    cudaError_t         *d_cdp_error,
deps/cub/test/test_device_reduce.cu:    *d_cdp_error = cudaErrorNotSupported;
deps/cub/test/test_device_reduce.cu:cudaError_t Dispatch(
deps/cub/test/test_device_reduce.cu:    cudaError_t         *d_cdp_error,
deps/cub/test/test_device_reduce.cu:    cudaStream_t        stream,
deps/cub/test/test_device_reduce.cu:    CubDebugExit(cudaMemcpy(&temp_storage_bytes, d_temp_storage_bytes, sizeof(size_t) * 1, cudaMemcpyDeviceToHost));
deps/cub/test/test_device_reduce.cu:    cudaError_t retval;
deps/cub/test/test_device_reduce.cu:    CubDebugExit(cudaMemcpy(&retval, d_cdp_error, sizeof(cudaError_t) * 1, cudaMemcpyDeviceToHost));
deps/cub/test/test_device_reduce.cu:    cudaError_t     *d_cdp_error = NULL;
deps/cub/test/test_device_reduce.cu:    CubDebugExit(g_allocator.DeviceAllocate((void**)&d_cdp_error,           sizeof(cudaError_t) * 1));
deps/cub/test/test_device_reduce.cu:        GpuTimer gpu_timer;
deps/cub/test/test_device_reduce.cu:        gpu_timer.Start();
deps/cub/test/test_device_reduce.cu:        gpu_timer.Stop();
deps/cub/test/test_device_reduce.cu:        float elapsed_millis = gpu_timer.ElapsedMillis();
deps/cub/test/test_device_reduce.cu:    CubDebugExit(cudaMemset(d_out, 0, sizeof(OutputT) * num_segments));
deps/cub/test/test_device_reduce.cu:    CubDebugExit(cudaMemcpy(d_in,               h_in,                   sizeof(InputT) * num_items, cudaMemcpyHostToDevice));
deps/cub/test/test_device_reduce.cu:    CubDebugExit(cudaMemcpy(d_segment_offsets,  h_segment_offsets,      sizeof(OffsetT) * (num_segments + 1), cudaMemcpyHostToDevice));
deps/cub/test/test_device_reduce.cu:    CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(InputT) * num_items, cudaMemcpyHostToDevice));
deps/cub/test/test_device_reduce.cu:        CubDebugExit(cudaMemcpy(d_segment_offsets, h_segment_offsets, sizeof(OffsetT) * (num_segments + 1), cudaMemcpyHostToDevice));
deps/cub/test/test_device_reduce.cu:    cudaError_t Invoke()
deps/cub/test/test_device_reduce.cu:        return cudaSuccess;
deps/cub/test/test_warp_scan.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/test/test_warp_scan.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/test/test_warp_scan.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/test/test_warp_scan.cu:// Ensure printing of CUDA runtime errors to console
deps/cub/test/test_warp_scan.cu:    CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(T) * TOTAL_ITEMS, cudaMemcpyHostToDevice));
deps/cub/test/test_warp_scan.cu:    CubDebugExit(cudaMemset(d_out, 0, sizeof(T) * (TOTAL_ITEMS + 1)));
deps/cub/test/test_warp_scan.cu:    CubDebugExit(cudaMemset(d_aggregate, 0, sizeof(T) * TOTAL_ITEMS));
deps/cub/test/test_warp_scan.cu:    CubDebugExit(cudaPeekAtLastError());
deps/cub/test/test_warp_scan.cu:    CubDebugExit(cudaDeviceSynchronize());
deps/cub/test/test_warp_scan.cu:    CubDebugExit(cudaGetDevice(&device_ordinal));
deps/cub/test/test_device_run_length_encode.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/test/test_device_run_length_encode.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/test/test_device_run_length_encode.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/test/test_device_run_length_encode.cu:// Ensure printing of CUDA runtime errors to console
deps/cub/test/test_device_run_length_encode.cu:    CDP,        // GPU-based (dynamic parallelism) dispatch to CUB method
deps/cub/test/test_device_run_length_encode.cu:cudaError_t Dispatch(
deps/cub/test/test_device_run_length_encode.cu:    cudaError_t                 */*d_cdp_error*/,
deps/cub/test/test_device_run_length_encode.cu:    cudaStream_t                stream,
deps/cub/test/test_device_run_length_encode.cu:    cudaError_t error = cudaSuccess;
deps/cub/test/test_device_run_length_encode.cu:cudaError_t Dispatch(
deps/cub/test/test_device_run_length_encode.cu:    cudaError_t                 */*d_cdp_error*/,
deps/cub/test/test_device_run_length_encode.cu:    cudaStream_t                stream,
deps/cub/test/test_device_run_length_encode.cu:    cudaError_t error = cudaSuccess;
deps/cub/test/test_device_run_length_encode.cu:// CUDA Nested Parallelism Test Kernel
deps/cub/test/test_device_run_length_encode.cu:    cudaError_t                 *d_cdp_error,
deps/cub/test/test_device_run_length_encode.cu:    cudaStream_t                stream,
deps/cub/test/test_device_run_length_encode.cu:    *d_cdp_error = cudaErrorNotSupported;
deps/cub/test/test_device_run_length_encode.cu:cudaError_t Dispatch(
deps/cub/test/test_device_run_length_encode.cu:    cudaError_t                 *d_cdp_error,
deps/cub/test/test_device_run_length_encode.cu:    cudaStream_t                stream,
deps/cub/test/test_device_run_length_encode.cu:    CubDebugExit(cudaMemcpy(&temp_storage_bytes, d_temp_storage_bytes, sizeof(size_t) * 1, cudaMemcpyDeviceToHost));
deps/cub/test/test_device_run_length_encode.cu:    cudaError_t retval;
deps/cub/test/test_device_run_length_encode.cu:    CubDebugExit(cudaMemcpy(&retval, d_cdp_error, sizeof(cudaError_t) * 1, cudaMemcpyDeviceToHost));
deps/cub/test/test_device_run_length_encode.cu:    cudaError_t*     d_cdp_error = NULL;
deps/cub/test/test_device_run_length_encode.cu:    CubDebugExit(g_allocator.DeviceAllocate((void**)&d_cdp_error,           sizeof(cudaError_t) * 1));
deps/cub/test/test_device_run_length_encode.cu:        CubDebugExit(cudaMemset(d_unique_out,   0, sizeof(T) * num_items));
deps/cub/test/test_device_run_length_encode.cu:        CubDebugExit(cudaMemset(d_offsets_out,  0, sizeof(OffsetT) * num_items));
deps/cub/test/test_device_run_length_encode.cu:    CubDebugExit(cudaMemset(d_lengths_out,  0, sizeof(LengthT) * num_items));
deps/cub/test/test_device_run_length_encode.cu:    CubDebugExit(cudaMemset(d_num_runs,     0, sizeof(int)));
deps/cub/test/test_device_run_length_encode.cu:    GpuTimer gpu_timer;
deps/cub/test/test_device_run_length_encode.cu:    gpu_timer.Start();
deps/cub/test/test_device_run_length_encode.cu:    gpu_timer.Stop();
deps/cub/test/test_device_run_length_encode.cu:    float elapsed_millis = gpu_timer.ElapsedMillis();
deps/cub/test/test_device_run_length_encode.cu:    CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(T) * num_items, cudaMemcpyHostToDevice));
deps/cub/test/test_grid_barrier.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/test/test_grid_barrier.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/test/test_grid_barrier.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/test/test_grid_barrier.cu:// Ensure printing of CUDA runtime errors to console
deps/cub/test/test_grid_barrier.cu:    cudaError_t retval = cudaSuccess;
deps/cub/test/test_grid_barrier.cu:    CubDebugExit(cudaGetDevice(&device_ordinal));
deps/cub/test/test_grid_barrier.cu:    CubDebugExit(cudaDeviceGetAttribute(&sm_count, cudaDevAttrMultiProcessorCount, device_ordinal));
deps/cub/test/test_grid_barrier.cu:    CubDebugExit(cudaDeviceGetAttribute(&max_block_threads, cudaDevAttrMaxThreadsPerBlock, device_ordinal));
deps/cub/test/test_grid_barrier.cu:    GpuTimer gpu_timer;
deps/cub/test/test_grid_barrier.cu:    gpu_timer.Start();
deps/cub/test/test_grid_barrier.cu:    gpu_timer.Stop();
deps/cub/test/test_grid_barrier.cu:    retval = CubDebug(cudaDeviceSynchronize());
deps/cub/test/test_grid_barrier.cu:    float avg_elapsed = gpu_timer.ElapsedMillis() / float(iterations);
deps/cub/test/test_grid_barrier.cu:        gpu_timer.ElapsedMillis(),
deps/cub/test/test_device_spmv.cu: * Copyright (c) 2021, NVIDIA CORPORATION.  All rights reserved.
deps/cub/test/test_device_spmv.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/test/test_device_spmv.cu: * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/test/test_device_spmv.cu:// Ensure printing of CUDA runtime errors to console
deps/cub/test/test_block_run_length_decode.cu: * Copyright (c) 2011-2021, NVIDIA CORPORATION. All rights reserved.
deps/cub/test/test_block_run_length_decode.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/test/test_block_run_length_decode.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/test/test_block_run_length_decode.cu:  cudaStream_t stream;
deps/cub/test/test_block_run_length_decode.cu:  cudaStreamCreate(&stream);
deps/cub/test/test_block_run_length_decode.cu:  cudaEvent_t cuda_evt_timers[NUM_TIMERS];
deps/cub/test/test_block_run_length_decode.cu:    cudaEventCreate(&cuda_evt_timers[i]);
deps/cub/test/test_block_run_length_decode.cu:  CubDebugExit(cudaMalloc(&temp_storage, temp_storage_bytes));
deps/cub/test/test_block_run_length_decode.cu:  CubDebugExit(cudaMalloc(&d_decoded_sizes, num_blocks * sizeof(*d_decoded_sizes)));
deps/cub/test/test_block_run_length_decode.cu:  CubDebugExit(cudaMalloc(&d_decoded_offsets, (num_blocks + 1) * sizeof(*d_decoded_offsets)));
deps/cub/test/test_block_run_length_decode.cu:  CubDebugExit(cudaMallocHost(&h_num_decoded_total, sizeof(*h_num_decoded_total)));
deps/cub/test/test_block_run_length_decode.cu:  CubDebugExit(cudaEventRecord(cuda_evt_timers[TIMER_SIZE_BEGIN], stream));
deps/cub/test/test_block_run_length_decode.cu:  CubDebugExit(cudaEventRecord(cuda_evt_timers[TIMER_SIZE_END], stream));
deps/cub/test/test_block_run_length_decode.cu:  CubDebugExit(cudaMemsetAsync(d_decoded_offsets, 0, sizeof(d_decoded_offsets[0]), stream));
deps/cub/test/test_block_run_length_decode.cu:  CubDebugExit(cudaMemcpyAsync(h_num_decoded_total,
deps/cub/test/test_block_run_length_decode.cu:                               cudaMemcpyDeviceToHost,
deps/cub/test/test_block_run_length_decode.cu:  // Ensure the total decoded size has been copied from GPU to CPU
deps/cub/test/test_block_run_length_decode.cu:  CubDebugExit(cudaStreamSynchronize(stream));
deps/cub/test/test_block_run_length_decode.cu:  CubDebugExit(cudaMallocHost(&h_decoded_out, (*h_num_decoded_total) * sizeof(RunItemT)));
deps/cub/test/test_block_run_length_decode.cu:  CubDebugExit(cudaMalloc(&d_decoded_out, (*h_num_decoded_total) * sizeof(RunItemT)));
deps/cub/test/test_block_run_length_decode.cu:    CubDebugExit(cudaMalloc(&d_relative_offsets, (*h_num_decoded_total) * sizeof(RunLengthT)));
deps/cub/test/test_block_run_length_decode.cu:    CubDebugExit(cudaMallocHost(&h_relative_offsets, (*h_num_decoded_total) * sizeof(RunLengthT)));
deps/cub/test/test_block_run_length_decode.cu:  CubDebugExit(cudaEventRecord(cuda_evt_timers[TIMER_DECODE_BEGIN], stream));
deps/cub/test/test_block_run_length_decode.cu:  CubDebugExit(cudaEventRecord(cuda_evt_timers[TIMER_DECODE_END], stream));
deps/cub/test/test_block_run_length_decode.cu:  CubDebugExit(cudaMemcpyAsync(h_decoded_out,
deps/cub/test/test_block_run_length_decode.cu:                               cudaMemcpyDeviceToHost,
deps/cub/test/test_block_run_length_decode.cu:    CubDebugExit(cudaMemcpyAsync(h_relative_offsets,
deps/cub/test/test_block_run_length_decode.cu:                                 cudaMemcpyDeviceToHost,
deps/cub/test/test_block_run_length_decode.cu:  CubDebugExit(cudaStreamSynchronize(stream));
deps/cub/test/test_block_run_length_decode.cu:  cudaEventElapsedTime(&duration_size, cuda_evt_timers[TIMER_SIZE_BEGIN], cuda_evt_timers[TIMER_SIZE_END]);
deps/cub/test/test_block_run_length_decode.cu:  cudaEventElapsedTime(&duration_decode, cuda_evt_timers[TIMER_DECODE_BEGIN], cuda_evt_timers[TIMER_DECODE_END]);
deps/cub/test/test_block_run_length_decode.cu:      std::cout << "Mismatch at #" << i << ": CPU item: " << host_golden[i].first << ", GPU: " << h_decoded_out[i]
deps/cub/test/test_block_run_length_decode.cu:                  << ", GPU: " << h_decoded_out[i] << "; relative offsets: CPU: " << host_golden[i].second
deps/cub/test/test_block_run_length_decode.cu:                  << ", GPU: " << h_relative_offsets[i] << "\n";
deps/cub/test/test_block_run_length_decode.cu:  CubDebugExit(cudaFree(temp_storage));
deps/cub/test/test_block_run_length_decode.cu:  CubDebugExit(cudaFree(d_decoded_sizes));
deps/cub/test/test_block_run_length_decode.cu:  CubDebugExit(cudaFree(d_decoded_offsets));
deps/cub/test/test_block_run_length_decode.cu:  CubDebugExit(cudaFree(d_decoded_out));
deps/cub/test/test_block_run_length_decode.cu:  CubDebugExit(cudaFreeHost(h_num_decoded_total));
deps/cub/test/test_block_run_length_decode.cu:  CubDebugExit(cudaFreeHost(h_decoded_out));
deps/cub/test/test_block_run_length_decode.cu:    CubDebugExit(cudaFree(d_relative_offsets));
deps/cub/test/test_block_run_length_decode.cu:    CubDebugExit(cudaFreeHost(h_relative_offsets));
deps/cub/test/test_block_run_length_decode.cu:    CubDebugExit(cudaEventDestroy(cuda_evt_timers[i]));
deps/cub/test/test_block_run_length_decode.cu:  CubDebugExit(cudaStreamDestroy(stream));
deps/cub/test/CMakeLists.txt:#   testing/vector.cu will be "vector", and testing/cuda/copy.cu will be
deps/cub/test/CMakeLists.txt:#   "cuda.copy".
deps/cub/test/CMakeLists.txt:    cub_enable_rdc_for_cuda_target(${test_target})
deps/cub/test/test_block_shuffle.cu: * Copyright (c) 2011-2021, NVIDIA CORPORATION. All rights reserved.
deps/cub/test/test_block_shuffle.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/test/test_block_shuffle.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/test/test_block_shuffle.cu:// Ensure printing of CUDA runtime errors to console
deps/cub/test/test_block_shuffle.cu:    CubDebugExit(cudaPeekAtLastError());
deps/cub/test/test_block_shuffle.cu:    CubDebugExit(cudaDeviceSynchronize());
deps/cub/test/test_block_shuffle.cu:    CubDebugExit(cudaPeekAtLastError());
deps/cub/test/test_block_shuffle.cu:    CubDebugExit(cudaDeviceSynchronize());
deps/cub/test/test_block_shuffle.cu:    CubDebugExit(cudaMemcpy(h_output, d_output, num_items * sizeof (DataType), cudaMemcpyDeviceToHost));
deps/cub/test/test_device_histogram.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/test/test_device_histogram.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/test/test_device_histogram.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/test/test_device_histogram.cu:// Ensure printing of CUDA runtime errors to console
deps/cub/test/test_device_histogram.cu:    CDP,        // GPU-based (dynamic parallelism) dispatch to CUB method
deps/cub/test/test_device_histogram.cu:    static cudaError_t Range(
deps/cub/test/test_device_histogram.cu:        cudaError_t             */*d_cdp_error*/,
deps/cub/test/test_device_histogram.cu:        cudaStream_t        stream,
deps/cub/test/test_device_histogram.cu:        cudaError_t error = cudaSuccess;
deps/cub/test/test_device_histogram.cu:    static cudaError_t Even(
deps/cub/test/test_device_histogram.cu:        cudaError_t             */*d_cdp_error*/,
deps/cub/test/test_device_histogram.cu:        cudaStream_t        stream,
deps/cub/test/test_device_histogram.cu:        cudaError_t error = cudaSuccess;
deps/cub/test/test_device_histogram.cu:    static cudaError_t Range(
deps/cub/test/test_device_histogram.cu:        cudaError_t             */*d_cdp_error*/,
deps/cub/test/test_device_histogram.cu:        cudaStream_t        stream,
deps/cub/test/test_device_histogram.cu:        cudaError_t error = cudaSuccess;
deps/cub/test/test_device_histogram.cu:    static cudaError_t Even(
deps/cub/test/test_device_histogram.cu:        cudaError_t             */*d_cdp_error*/,
deps/cub/test/test_device_histogram.cu:        cudaStream_t        stream,
deps/cub/test/test_device_histogram.cu:        cudaError_t error = cudaSuccess;
deps/cub/test/test_device_histogram.cu:// CUDA nested-parallelism test kernel
deps/cub/test/test_device_histogram.cu:    cudaError_t         *d_cdp_error,
deps/cub/test/test_device_histogram.cu:    *d_cdp_error = cudaErrorNotSupported;
deps/cub/test/test_device_histogram.cu:cudaError_t Dispatch(
deps/cub/test/test_device_histogram.cu:    cudaError_t         *d_cdp_error,
deps/cub/test/test_device_histogram.cu:    cudaStream_t        stream,
deps/cub/test/test_device_histogram.cu:    CubDebugExit(cudaMemcpy(&temp_storage_bytes, d_temp_storage_bytes, sizeof(size_t) * 1, cudaMemcpyDeviceToHost));
deps/cub/test/test_device_histogram.cu:    cudaError_t retval;
deps/cub/test/test_device_histogram.cu:    CubDebugExit(cudaMemcpy(&retval, d_cdp_error, sizeof(cudaError_t) * 1, cudaMemcpyDeviceToHost));
deps/cub/test/test_device_histogram.cu:        CubDebugExit(cudaMemset(d_histogram[channel], 0, sizeof(CounterT) * (num_levels[channel] - 1)));
deps/cub/test/test_device_histogram.cu:    cudaError_t     *d_cdp_error = NULL;
deps/cub/test/test_device_histogram.cu:    CubDebugExit(g_allocator.DeviceAllocate((void**)&d_cdp_error,           sizeof(cudaError_t) * 1));
deps/cub/test/test_device_histogram.cu:    CubDebugExit(cudaMemset(d_temp_storage, canary_token, temp_storage_bytes + (canary_bytes * 2)));
deps/cub/test/test_device_histogram.cu:    CubDebugExit(cudaPeekAtLastError());
deps/cub/test/test_device_histogram.cu:    CubDebugExit(cudaDeviceSynchronize());
deps/cub/test/test_device_histogram.cu:    GpuTimer gpu_timer;
deps/cub/test/test_device_histogram.cu:    gpu_timer.Start();
deps/cub/test/test_device_histogram.cu:    gpu_timer.Stop();
deps/cub/test/test_device_histogram.cu:    float elapsed_millis = gpu_timer.ElapsedMillis();
deps/cub/test/test_device_histogram.cu:    CubDebugExit(cudaMemcpy(d_samples, h_samples, sizeof(SampleT) * total_samples, cudaMemcpyHostToDevice));
deps/cub/test/test_device_histogram.cu:    CubDebugExit(cudaMemcpy(d_samples, h_samples, sizeof(SampleT) * total_samples, cudaMemcpyHostToDevice));
deps/cub/test/test_device_histogram.cu:        CubDebugExit(cudaMemcpy(d_levels[channel], levels[channel],         sizeof(LevelT) * num_levels[channel], cudaMemcpyHostToDevice));
deps/cub/test/test_device_histogram.cu:        CubDebugExit(cudaMemset(d_histogram[channel], 0,                        sizeof(CounterT) * bins));
deps/cub/test/test_device_histogram.cu:    cudaError_t     *d_cdp_error = NULL;
deps/cub/test/test_device_histogram.cu:    CubDebugExit(g_allocator.DeviceAllocate((void**)&d_cdp_error,           sizeof(cudaError_t) * 1));
deps/cub/test/test_device_histogram.cu:    CubDebugExit(cudaMemset(d_temp_storage, canary_token, temp_storage_bytes + (canary_bytes * 2)));
deps/cub/test/test_device_histogram.cu:    CubDebugExit(cudaPeekAtLastError());
deps/cub/test/test_device_histogram.cu:    CubDebugExit(cudaDeviceSynchronize());
deps/cub/test/test_device_histogram.cu:    GpuTimer gpu_timer;
deps/cub/test/test_device_histogram.cu:    gpu_timer.Start();
deps/cub/test/test_device_histogram.cu:    gpu_timer.Stop();
deps/cub/test/test_device_histogram.cu:    float elapsed_millis = gpu_timer.ElapsedMillis();
deps/cub/test/test_allocator.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/test/test_allocator.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/test/test_allocator.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/test/test_allocator.cu:// Ensure printing of CUDA runtime errors to console
deps/cub/test/test_allocator.cu:    // Get number of GPUs and current GPU
deps/cub/test/test_allocator.cu:    int num_gpus;
deps/cub/test/test_allocator.cu:    int initial_gpu;
deps/cub/test/test_allocator.cu:    if (CubDebug(cudaGetDeviceCount(&num_gpus))) exit(1);
deps/cub/test/test_allocator.cu:    if (CubDebug(cudaGetDevice(&initial_gpu))) exit(1);
deps/cub/test/test_allocator.cu:    // Create default allocator (caches up to 6MB in device allocations per GPU)
deps/cub/test/test_allocator.cu:    printf("Running single-gpu tests...\n"); fflush(stdout);
deps/cub/test/test_allocator.cu:    cudaStream_t other_stream;
deps/cub/test/test_allocator.cu:    CubDebugExit(cudaStreamCreate(&other_stream));
deps/cub/test/test_allocator.cu:    // Allocate 999 bytes on the current gpu in stream0
deps/cub/test/test_allocator.cu:    // Check that that we have 1 live block on the initial GPU
deps/cub/test/test_allocator.cu:    // Check that that we have no cached block on the initial GPU
deps/cub/test/test_allocator.cu:    // Allocate 999 bytes on the current gpu in other_stream
deps/cub/test/test_allocator.cu:    // Check that that we have 1 live blocks on the initial GPU (that we allocated a new one because d_999B_stream0_b is only available for stream 0 until it becomes idle)
deps/cub/test/test_allocator.cu:    // Check that that we have one cached block on the initial GPU
deps/cub/test/test_allocator.cu:    CubDebugExit(cudaDeviceSynchronize());
deps/cub/test/test_allocator.cu:    // Check that that we have 2 live blocks on the initial GPU
deps/cub/test/test_allocator.cu:    // Check that that we have no cached block on the initial GPU
deps/cub/test/test_allocator.cu:    CubDebugExit(cudaDeviceSynchronize());
deps/cub/test/test_allocator.cu:    // Check that that we have 2 live blocks on the initial GPU
deps/cub/test/test_allocator.cu:    // Check that that we have no cached block on the initial GPU
deps/cub/test/test_allocator.cu:    CubDebugExit(cudaDeviceSynchronize());
deps/cub/test/test_allocator.cu:    CubDebugExit(cudaStreamDestroy(other_stream));
deps/cub/test/test_allocator.cu:    // Check that that we have 2 live blocks on the initial GPU
deps/cub/test/test_allocator.cu:    // Check that that we have no cached block on the initial GPU
deps/cub/test/test_allocator.cu:    // Allocate 5 bytes on the current gpu
deps/cub/test/test_allocator.cu:    // Check that that we have zero free bytes cached on the initial GPU
deps/cub/test/test_allocator.cu:    AssertEquals(allocator.cached_bytes[initial_gpu].free, 0);
deps/cub/test/test_allocator.cu:    // Check that that we have 1 live block on the initial GPU
deps/cub/test/test_allocator.cu:    // Allocate 4096 bytes on the current gpu
deps/cub/test/test_allocator.cu:    // Check that that we have 2 live blocks on the initial GPU
deps/cub/test/test_allocator.cu:    // Check that that we have min_bin_bytes free bytes cached on the initial gpu
deps/cub/test/test_allocator.cu:    AssertEquals(allocator.cached_bytes[initial_gpu].free, allocator.min_bin_bytes);
deps/cub/test/test_allocator.cu:    // Check that that we have 1 live block on the initial GPU
deps/cub/test/test_allocator.cu:    // Check that that we have 1 cached block on the initial GPU
deps/cub/test/test_allocator.cu:    // Check that that we have the 4096 + min_bin free bytes cached on the initial gpu
deps/cub/test/test_allocator.cu:    AssertEquals(allocator.cached_bytes[initial_gpu].free, allocator.min_bin_bytes + 4096);
deps/cub/test/test_allocator.cu:    // Check that that we have 0 live block on the initial GPU
deps/cub/test/test_allocator.cu:    // Check that that we have 2 cached block on the initial GPU
deps/cub/test/test_allocator.cu:    // Allocate 768 bytes on the current gpu
deps/cub/test/test_allocator.cu:    // Check that that we have the min_bin free bytes cached on the initial gpu (4096 was reused)
deps/cub/test/test_allocator.cu:    AssertEquals(allocator.cached_bytes[initial_gpu].free, allocator.min_bin_bytes);
deps/cub/test/test_allocator.cu:    // Check that that we have 1 live block on the initial GPU
deps/cub/test/test_allocator.cu:    // Check that that we have 1 cached block on the initial GPU
deps/cub/test/test_allocator.cu:    // Allocate max_cached_bytes on the current gpu
deps/cub/test/test_allocator.cu:    // Check that that we have the min_bin free bytes cached on the initial gpu (max cached was not returned because we went over)
deps/cub/test/test_allocator.cu:    AssertEquals(allocator.cached_bytes[initial_gpu].free, allocator.min_bin_bytes);
deps/cub/test/test_allocator.cu:    // Check that that we have 1 live block on the initial GPU
deps/cub/test/test_allocator.cu:    // Check that that we still have 1 cached block on the initial GPU
deps/cub/test/test_allocator.cu:    // Free all cached blocks on all GPUs
deps/cub/test/test_allocator.cu:    // Check that that we have 0 bytes cached on the initial GPU
deps/cub/test/test_allocator.cu:    AssertEquals(allocator.cached_bytes[initial_gpu].free, 0);
deps/cub/test/test_allocator.cu:    // Check that that we have 0 cached blocks across all GPUs
deps/cub/test/test_allocator.cu:    // Check that that still we have 1 live block across all GPUs
deps/cub/test/test_allocator.cu:    // Allocate max cached bytes + 1 on the current gpu
deps/cub/test/test_allocator.cu:    // Check that that we have 4096 free bytes cached on the initial gpu
deps/cub/test/test_allocator.cu:    AssertEquals(allocator.cached_bytes[initial_gpu].free, rounded_bytes);
deps/cub/test/test_allocator.cu:    // Check that that we have 1 cached blocks across all GPUs
deps/cub/test/test_allocator.cu:    // Check that that still we have 0 live block across all GPUs
deps/cub/test/test_allocator.cu:    // BUG: find out why these tests fail when one GPU is CDP compliant and the other is not
deps/cub/test/test_allocator.cu:    if (num_gpus > 1)
deps/cub/test/test_allocator.cu:        printf("\nRunning multi-gpu tests...\n"); fflush(stdout);
deps/cub/test/test_allocator.cu:        // Allocate 768 bytes on the next gpu
deps/cub/test/test_allocator.cu:        int next_gpu = (initial_gpu + 1) % num_gpus;
deps/cub/test/test_allocator.cu:        CubDebugExit(allocator.DeviceAllocate(next_gpu, (void **) &d_768B_2, 768));
deps/cub/test/test_allocator.cu:        // DeviceFree d_768B on the next gpu
deps/cub/test/test_allocator.cu:        CubDebugExit(allocator.DeviceFree(next_gpu, d_768B_2));
deps/cub/test/test_allocator.cu:        // Re-allocate 768 bytes on the next gpu
deps/cub/test/test_allocator.cu:        CubDebugExit(allocator.DeviceAllocate(next_gpu, (void **) &d_768B_2, 768));
deps/cub/test/test_allocator.cu:        // Re-free d_768B on the next gpu
deps/cub/test/test_allocator.cu:        CubDebugExit(allocator.DeviceFree(next_gpu, d_768B_2));
deps/cub/test/test_allocator.cu:        // Check that that we have 4096 free bytes cached on the initial gpu
deps/cub/test/test_allocator.cu:        AssertEquals(allocator.cached_bytes[initial_gpu].free, rounded_bytes);
deps/cub/test/test_allocator.cu:        // Check that that we have 4096 free bytes cached on the second gpu
deps/cub/test/test_allocator.cu:        AssertEquals(allocator.cached_bytes[next_gpu].free, rounded_bytes);
deps/cub/test/test_allocator.cu:        // Check that that we have 2 cached blocks across all GPUs
deps/cub/test/test_allocator.cu:        // Check that that still we have 0 live block across all GPUs
deps/cub/test/test_allocator.cu:    // CUDA
deps/cub/test/test_allocator.cu:        CubDebugExit(cudaMalloc((void **) &d_1024MB, timing_bytes));
deps/cub/test/test_allocator.cu:        CubDebugExit(cudaFree(d_1024MB));
deps/cub/test/test_allocator.cu:    float cuda_malloc_elapsed_millis = cpu_timer.ElapsedMillis();
deps/cub/test/test_allocator.cu:    printf("\t CUB CachingDeviceAllocator allocation CPU speedup: %.2f (avg cudaMalloc %.4f ms vs. avg DeviceAllocate %.4f ms)\n",
deps/cub/test/test_allocator.cu:        cuda_malloc_elapsed_millis / cub_calloc_elapsed_millis,
deps/cub/test/test_allocator.cu:        cuda_malloc_elapsed_millis / timing_iterations,
deps/cub/test/test_allocator.cu:    // GPU performance comparisons.  Allocate and free a 1MB block 2000 times
deps/cub/test/test_allocator.cu:    GpuTimer gpu_timer;
deps/cub/test/test_allocator.cu:    printf("\nGPU Performance (%d timing iterations, %d bytes):\n", timing_iterations, timing_bytes);
deps/cub/test/test_allocator.cu:    gpu_timer.Start();
deps/cub/test/test_allocator.cu:    gpu_timer.Stop();
deps/cub/test/test_allocator.cu:    float cuda_empty_elapsed_millis = gpu_timer.ElapsedMillis();
deps/cub/test/test_allocator.cu:    // CUDA
deps/cub/test/test_allocator.cu:    gpu_timer.Start();
deps/cub/test/test_allocator.cu:        CubDebugExit(cudaMalloc((void **) &d_1024MB, timing_bytes));
deps/cub/test/test_allocator.cu:        CubDebugExit(cudaFree(d_1024MB));
deps/cub/test/test_allocator.cu:    gpu_timer.Stop();
deps/cub/test/test_allocator.cu:    cuda_malloc_elapsed_millis = gpu_timer.ElapsedMillis() - cuda_empty_elapsed_millis;
deps/cub/test/test_allocator.cu:    gpu_timer.Start();
deps/cub/test/test_allocator.cu:    gpu_timer.Stop();
deps/cub/test/test_allocator.cu:    cub_calloc_elapsed_millis = gpu_timer.ElapsedMillis() - cuda_empty_elapsed_millis;
deps/cub/test/test_allocator.cu:    printf("\t CUB CachingDeviceAllocator allocation GPU speedup: %.2f (avg cudaMalloc %.4f ms vs. avg DeviceAllocate %.4f ms)\n",
deps/cub/test/test_allocator.cu:        cuda_malloc_elapsed_millis / cub_calloc_elapsed_millis,
deps/cub/test/test_allocator.cu:        cuda_malloc_elapsed_millis / timing_iterations,
deps/cub/test/test_device_select_unique.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/test/test_device_select_unique.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/test/test_device_select_unique.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/test/test_device_select_unique.cu:// Ensure printing of CUDA runtime errors to console
deps/cub/test/test_device_select_unique.cu:    CDP,        // GPU-based (dynamic parallelism) dispatch to CUB method
deps/cub/test/test_device_select_unique.cu:cudaError_t Dispatch(
deps/cub/test/test_device_select_unique.cu:    cudaError_t                 */*d_cdp_error*/,
deps/cub/test/test_device_select_unique.cu:    cudaStream_t                stream,
deps/cub/test/test_device_select_unique.cu:    cudaError_t error = cudaSuccess;
deps/cub/test/test_device_select_unique.cu:// CUDA Nested Parallelism Test Kernel
deps/cub/test/test_device_select_unique.cu:    cudaError_t                 *d_cdp_error,
deps/cub/test/test_device_select_unique.cu:    *d_cdp_error = cudaErrorNotSupported;
deps/cub/test/test_device_select_unique.cu:cudaError_t Dispatch(
deps/cub/test/test_device_select_unique.cu:    cudaError_t                 *d_cdp_error,
deps/cub/test/test_device_select_unique.cu:    cudaStream_t                stream,
deps/cub/test/test_device_select_unique.cu:    CubDebugExit(cudaMemcpy(&temp_storage_bytes, d_temp_storage_bytes, sizeof(size_t) * 1, cudaMemcpyDeviceToHost));
deps/cub/test/test_device_select_unique.cu:    cudaError_t retval;
deps/cub/test/test_device_select_unique.cu:    CubDebugExit(cudaMemcpy(&retval, d_cdp_error, sizeof(cudaError_t) * 1, cudaMemcpyDeviceToHost));
deps/cub/test/test_device_select_unique.cu:    cudaError_t     *d_cdp_error = NULL;
deps/cub/test/test_device_select_unique.cu:    CubDebugExit(g_allocator.DeviceAllocate((void**)&d_cdp_error,           sizeof(cudaError_t) * 1));
deps/cub/test/test_device_select_unique.cu:    CubDebugExit(cudaMemset(d_out, 0, sizeof(T) * num_items));
deps/cub/test/test_device_select_unique.cu:    CubDebugExit(cudaMemset(d_num_selected_out, 0, sizeof(int)));
deps/cub/test/test_device_select_unique.cu:    GpuTimer gpu_timer;
deps/cub/test/test_device_select_unique.cu:    gpu_timer.Start();
deps/cub/test/test_device_select_unique.cu:    gpu_timer.Stop();
deps/cub/test/test_device_select_unique.cu:    float elapsed_millis = gpu_timer.ElapsedMillis();
deps/cub/test/test_device_select_unique.cu:    CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(T) * num_items, cudaMemcpyHostToDevice));
deps/cub/test/test_block_scan.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/test/test_block_scan.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/test/test_block_scan.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/test/test_block_scan.cu:// Ensure printing of CUDA runtime errors to console
deps/cub/test/test_block_scan.cu:    CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(T) * TILE_SIZE, cudaMemcpyHostToDevice));
deps/cub/test/test_block_scan.cu:    CubDebugExit(cudaMemset(d_out, 0, sizeof(T) * (TILE_SIZE + 1)));
deps/cub/test/test_block_scan.cu:    CubDebugExit(cudaMemset(d_aggregate, 0, sizeof(T) * BLOCK_THREADS));
deps/cub/test/test_block_scan.cu:    CubDebugExit(cudaPeekAtLastError());
deps/cub/test/test_block_scan.cu:    CubDebugExit(cudaDeviceSynchronize());
deps/cub/test/half.h: * Copyright (c) 2011-2019, NVIDIA CORPORATION.  All rights reserved.
deps/cub/test/half.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/test/half.h: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/test/half.h: * Utilities for interacting with the opaque CUDA __half type
deps/cub/test/half.h:#include <cuda_fp16.h>
deps/cub/test/test_warp_exchange.cu: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/cub/test/test_warp_exchange.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/test/test_warp_exchange.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/test/test_warp_exchange.cu:  cudaDeviceSynchronize();
deps/cub/test/test_warp_exchange.cu:  cudaDeviceSynchronize();
deps/cub/test/test_block_load_store.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/test/test_block_load_store.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/test/test_block_load_store.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/test/test_block_load_store.cu:// Ensure printing of CUDA runtime errors to console
deps/cub/test/test_block_load_store.cu:    CubDebugExit(cudaPeekAtLastError());
deps/cub/test/test_block_load_store.cu:    CubDebugExit(cudaDeviceSynchronize());
deps/cub/test/test_block_load_store.cu:    CubDebugExit(cudaMemset(d_out_unguarded, 0, sizeof(T) * unguarded_elements));
deps/cub/test/test_block_load_store.cu:    CubDebugExit(cudaMemset(d_out_guarded, 0, sizeof(T) * guarded_elements));
deps/cub/test/test_block_load_store.cu:    CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(T) * unguarded_elements, cudaMemcpyHostToDevice));
deps/cub/test/test_block_load_store.cu:    CubDebugExit(cudaMemset(d_out_unguarded, 0, sizeof(T) * unguarded_elements));
deps/cub/test/test_block_load_store.cu:    CubDebugExit(cudaMemset(d_out_guarded, 0, sizeof(T) * guarded_elements));
deps/cub/test/test_block_load_store.cu:    CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(T) * unguarded_elements, cudaMemcpyHostToDevice));
deps/cub/test/cmake/test_install/CMakeLists.txt:project(CubTestInstall CXX CUDA)
deps/cub/test/cmake/CMakeLists.txt:      -D "CMAKE_CUDA_COMPILER=${CMAKE_CUDA_COMPILER}"
deps/cub/test/test_iterator.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/test/test_iterator.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/test/test_iterator.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/test/test_iterator.cu:// Ensure printing of CUDA runtime errors to console
deps/cub/test/test_iterator.cu:    CDP,        // GPU-based (dynamic parallelism) dispatch to CUB method
deps/cub/test/test_iterator.cu:    CubDebugExit(cudaPeekAtLastError());
deps/cub/test/test_iterator.cu:    CubDebugExit(cudaDeviceSynchronize());
deps/cub/test/test_iterator.cu:    CubDebugExit(cudaMemcpy(d_data, h_data, sizeof(T) * TEST_VALUES, cudaMemcpyHostToDevice));
deps/cub/test/test_iterator.cu:    CubDebugExit(cudaMemcpy(d_data, h_data, sizeof(T) * TEST_VALUES, cudaMemcpyHostToDevice));
deps/cub/test/test_iterator.cu:    CubDebugExit(cudaMemcpy(d_data, h_data, sizeof(T) * TEST_VALUES, cudaMemcpyHostToDevice));
deps/cub/test/test_iterator.cu:    CubDebugExit(cudaMemcpy(d_dummy, h_data + DUMMY_OFFSET, sizeof(T) * DUMMY_TEST_VALUES, cudaMemcpyHostToDevice));
deps/cub/test/test_iterator.cu:#if CUDART_VERSION >= 5050
deps/cub/test/test_iterator.cu:    CubDebugExit(cudaMemcpy(d_data, h_data, sizeof(T) * TEST_VALUES, cudaMemcpyHostToDevice));
deps/cub/test/test_iterator.cu:    CubDebugExit(cudaMemcpy(d_dummy, h_data + DUMMY_OFFSET, sizeof(T) * DUMMY_TEST_VALUES, cudaMemcpyHostToDevice));
deps/cub/test/test_iterator.cu:    CubDebugExit(cudaMemcpy(d_data, h_data, sizeof(T) * TEST_VALUES, cudaMemcpyHostToDevice));
deps/cub/test/test_iterator.cu:#endif  // CUDART_VERSION
deps/cub/test/test_iterator.cu:    // Test tex-obj iterators if CUDA dynamic parallelism enabled
deps/cub/test/test_iterator.cu:#if CUDART_VERSION >= 5050
deps/cub/test/test_iterator.cu:    // Test tex-ref iterators for CUDA 5.5
deps/cub/test/test_iterator.cu:#endif  // CUDART_VERSION
deps/cub/test/test_device_three_way_partition.cu: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/cub/test/test_device_three_way_partition.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/test/test_device_three_way_partition.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/test/test_device_three_way_partition.cu:// Ensure printing of CUDA runtime errors to console
deps/cub/test/test_block_radix_sort.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/test/test_block_radix_sort.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/test/test_block_radix_sort.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/test/test_block_radix_sort.cu:// Ensure printing of CUDA runtime errors to console
deps/cub/test/test_block_radix_sort.cu:    cudaSharedMemConfig SMEM_CONFIG,
deps/cub/test/test_block_radix_sort.cu:    cudaSharedMemConfig     SMEM_CONFIG,
deps/cub/test/test_block_radix_sort.cu:    CubDebugExit(cudaMemcpy(d_keys, h_keys, sizeof(Key) * TILE_SIZE, cudaMemcpyHostToDevice));
deps/cub/test/test_block_radix_sort.cu:    CubDebugExit(cudaMemcpy(d_values, h_values, sizeof(Value) * TILE_SIZE, cudaMemcpyHostToDevice));
deps/cub/test/test_block_radix_sort.cu:    cudaDeviceSetSharedMemConfig(SMEM_CONFIG);
deps/cub/test/test_block_radix_sort.cu:    CubDebugExit(cudaPeekAtLastError());
deps/cub/test/test_block_radix_sort.cu:    CubDebugExit(cudaDeviceSynchronize());
deps/cub/test/test_block_radix_sort.cu:    cudaSharedMemConfig     SMEM_CONFIG,
deps/cub/test/test_block_radix_sort.cu:    cudaSharedMemConfig     SMEM_CONFIG,
deps/cub/test/test_block_radix_sort.cu:    cudaSharedMemConfig     SMEM_CONFIG,
deps/cub/test/test_block_radix_sort.cu:    Test<BLOCK_THREADS, ITEMS_PER_THREAD, RADIX_BITS, MEMOIZE_OUTER_SCAN, INNER_SCAN_ALGORITHM, cudaSharedMemBankSizeFourByte, Key, NullType>();    // Keys-only (4-byte smem bank config)
deps/cub/test/test_block_radix_sort.cu:    Test<BLOCK_THREADS, ITEMS_PER_THREAD, RADIX_BITS, MEMOIZE_OUTER_SCAN, INNER_SCAN_ALGORITHM, cudaSharedMemBankSizeEightByte, Key, NullType>();   // Keys-only (8-byte smem bank config)
deps/cub/test/test_block_radix_sort.cu:    Test<BLOCK_THREADS, ITEMS_PER_THREAD, RADIX_BITS, MEMOIZE_OUTER_SCAN, INNER_SCAN_ALGORITHM, cudaSharedMemBankSizeFourByte, Key, char>();        // With small-values
deps/cub/test/test_block_radix_sort.cu:    Test<BLOCK_THREADS, ITEMS_PER_THREAD, RADIX_BITS, MEMOIZE_OUTER_SCAN, INNER_SCAN_ALGORITHM, cudaSharedMemBankSizeFourByte, Key, Key>();         // With same-values
deps/cub/test/test_block_radix_sort.cu:    Test<BLOCK_THREADS, ITEMS_PER_THREAD, RADIX_BITS, MEMOIZE_OUTER_SCAN, INNER_SCAN_ALGORITHM, cudaSharedMemBankSizeFourByte, Key, TestFoo>();     // With large values
deps/cub/test/test_device_scan_by_key.cu: * Copyright (c) 2021 NVIDIA CORPORATION.  All rights reserved.
deps/cub/test/test_device_scan_by_key.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/test/test_device_scan_by_key.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/test/test_device_scan_by_key.cu:// Ensure printing of CUDA runtime errors to console
deps/cub/test/test_device_scan_by_key.cu:    CDP,        // GPU-based (dynamic parallelism) dispatch to CUB method
deps/cub/test/test_device_scan_by_key.cu:cudaError_t Dispatch(
deps/cub/test/test_device_scan_by_key.cu:    cudaError_t           */*d_cdp_error*/,
deps/cub/test/test_device_scan_by_key.cu:    cudaStream_t          stream,
deps/cub/test/test_device_scan_by_key.cu:    cudaError_t error = cudaSuccess;
deps/cub/test/test_device_scan_by_key.cu:cudaError_t Dispatch(
deps/cub/test/test_device_scan_by_key.cu:    cudaError_t           */*d_cdp_error*/,
deps/cub/test/test_device_scan_by_key.cu:    cudaStream_t          stream,
deps/cub/test/test_device_scan_by_key.cu:    cudaError_t error = cudaSuccess;
deps/cub/test/test_device_scan_by_key.cu:cudaError_t Dispatch(
deps/cub/test/test_device_scan_by_key.cu:    cudaError_t           */*d_cdp_error*/,
deps/cub/test/test_device_scan_by_key.cu:    cudaStream_t          stream,
deps/cub/test/test_device_scan_by_key.cu:    cudaError_t error = cudaSuccess;
deps/cub/test/test_device_scan_by_key.cu:cudaError_t Dispatch(
deps/cub/test/test_device_scan_by_key.cu:    cudaError_t           */*d_cdp_error*/,
deps/cub/test/test_device_scan_by_key.cu:    cudaStream_t          stream,
deps/cub/test/test_device_scan_by_key.cu:    cudaError_t error = cudaSuccess;
deps/cub/test/test_device_scan_by_key.cu:// CUDA Nested Parallelism Test Kernel
deps/cub/test/test_device_scan_by_key.cu:    cudaError_t           *d_cdp_error,
deps/cub/test/test_device_scan_by_key.cu:    *d_cdp_error = cudaErrorNotSupported;
deps/cub/test/test_device_scan_by_key.cu:cudaError_t Dispatch(
deps/cub/test/test_device_scan_by_key.cu:    cudaError_t           *d_cdp_error,
deps/cub/test/test_device_scan_by_key.cu:    cudaStream_t          stream,
deps/cub/test/test_device_scan_by_key.cu:    CubDebugExit(cudaMemcpy(&temp_storage_bytes, d_temp_storage_bytes, sizeof(size_t) * 1, cudaMemcpyDeviceToHost));
deps/cub/test/test_device_scan_by_key.cu:    cudaError_t retval;
deps/cub/test/test_device_scan_by_key.cu:    CubDebugExit(cudaMemcpy(&retval, d_cdp_error, sizeof(cudaError_t) * 1, cudaMemcpyDeviceToHost));
deps/cub/test/test_device_scan_by_key.cu:    cudaError_t     *d_cdp_error = NULL;
deps/cub/test/test_device_scan_by_key.cu:    CubDebugExit(g_allocator.DeviceAllocate((void**)&d_cdp_error,   sizeof(cudaError_t) * 1));
deps/cub/test/test_device_scan_by_key.cu:    CubDebugExit(cudaMemset(d_values_out, 0, sizeof(OutputT) * num_items));
deps/cub/test/test_device_scan_by_key.cu:    GpuTimer gpu_timer;
deps/cub/test/test_device_scan_by_key.cu:    gpu_timer.Start();
deps/cub/test/test_device_scan_by_key.cu:    gpu_timer.Stop();
deps/cub/test/test_device_scan_by_key.cu:    float elapsed_millis = gpu_timer.ElapsedMillis();
deps/cub/test/test_device_scan_by_key.cu:    CubDebugExit(cudaMemcpy(d_keys_in, h_keys_in, sizeof(KeyT) * num_items, cudaMemcpyHostToDevice));
deps/cub/test/test_device_scan_by_key.cu:    CubDebugExit(cudaMemcpy(d_values_in, h_values_in, sizeof(InputT) * num_items, cudaMemcpyHostToDevice));
deps/cub/test/test_block_merge_sort.cu: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/cub/test/test_block_merge_sort.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/test/test_block_merge_sort.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/test/test_block_merge_sort.cu:// Ensure printing of CUDA runtime errors to console
deps/cub/test/test_block_merge_sort.cu:  CubDebugExit(cudaPeekAtLastError());
deps/cub/test/test_block_merge_sort.cu:  CubDebugExit(cudaDeviceSynchronize());
deps/cub/test/test_block_merge_sort.cu:  CubDebugExit(cudaPeekAtLastError());
deps/cub/test/test_block_merge_sort.cu:  CubDebugExit(cudaDeviceSynchronize());
deps/cub/test/test_block_adjacent_difference.cu: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/cub/test/test_block_adjacent_difference.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/test/test_block_adjacent_difference.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/test/test_block_adjacent_difference.cu:// Ensure printing of CUDA runtime errors to console
deps/cub/test/test_block_adjacent_difference.cu:  CubDebugExit(cudaPeekAtLastError());
deps/cub/test/test_block_adjacent_difference.cu:  CubDebugExit(cudaDeviceSynchronize());
deps/cub/test/test_block_adjacent_difference.cu:  CubDebugExit(cudaPeekAtLastError());
deps/cub/test/test_block_adjacent_difference.cu:  CubDebugExit(cudaDeviceSynchronize());
deps/cub/test/test_block_adjacent_difference.cu:  CubDebugExit(cudaPeekAtLastError());
deps/cub/test/test_block_adjacent_difference.cu:  CubDebugExit(cudaDeviceSynchronize());
deps/cub/test/test_block_adjacent_difference.cu:  CubDebugExit(cudaPeekAtLastError());
deps/cub/test/test_block_adjacent_difference.cu:  CubDebugExit(cudaDeviceSynchronize());
deps/cub/test/test_block_adjacent_difference.cu:  CubDebugExit(cudaPeekAtLastError());
deps/cub/test/test_block_adjacent_difference.cu:  CubDebugExit(cudaDeviceSynchronize());
deps/cub/test/test_block_adjacent_difference.cu:  CubDebugExit(cudaPeekAtLastError());
deps/cub/test/test_block_adjacent_difference.cu:  CubDebugExit(cudaDeviceSynchronize());
deps/cub/test/bfloat16.h: * Copyright (c) 2021, NVIDIA CORPORATION.  All rights reserved.
deps/cub/test/bfloat16.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/test/bfloat16.h: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/test/bfloat16.h: * Utilities for interacting with the opaque CUDA __nv_bfloat16 type
deps/cub/test/bfloat16.h:#include <cuda_bf16.h>
deps/cub/CHANGELOG.md:each segment and utilizes the GPU more efficiently.
deps/cub/CHANGELOG.md:device-scope algorithms when invoked via CUDA Dynamic Parallelism (CDP).**
deps/cub/CHANGELOG.md:- NVIDIA/cub#305: The template parameters of `cub::DispatchScan` have changed to
deps/cub/CHANGELOG.md:- NVIDIA/cub#377: Remove broken `operator->()` from
deps/cub/CHANGELOG.md:- NVIDIA/cub#305: Add overloads to `cub::DeviceScan` algorithms that allow the
deps/cub/CHANGELOG.md:- NVIDIA/cub#354: Add `cub::BlockRunLengthDecode` algorithm. Thanks to Elias
deps/cub/CHANGELOG.md:- NVIDIA/cub#357: Add `cub::DeviceSegmentedSort`, an optimized version
deps/cub/CHANGELOG.md:- NVIDIA/cub#376: Add "by key" overloads to `cub::DeviceScan`. Thanks to Xiang
deps/cub/CHANGELOG.md:- NVIDIA/cub#349: Doxygen and unused variable fixes.
deps/cub/CHANGELOG.md:- NVIDIA/cub#363: Maintenance updates for the new `cub::DeviceMergeSort`
deps/cub/CHANGELOG.md:- NVIDIA/cub#382: Fix several `-Wconversion` warnings. Thanks to Matt Stack
deps/cub/CHANGELOG.md:- NVIDIA/cub#388: Fix debug assertion on MSVC when using
deps/cub/CHANGELOG.md:- NVIDIA/cub#395: Support building with `__CUDA_NO_HALF_CONVERSIONS__`. Thanks
deps/cub/CHANGELOG.md:# CUB 1.14.0 (NVIDIA HPC SDK 21.9)
deps/cub/CHANGELOG.md:CUB 1.14.0 is a major release accompanying the NVIDIA HPC SDK 21.9.
deps/cub/CHANGELOG.md:- NVIDIA/cub#350: When the `CUB_NS_[PRE|POST]FIX` macros are set,
deps/cub/CHANGELOG.md:- NVIDIA/cub#322: Ported the merge sort algorithm from Thrust:
deps/cub/CHANGELOG.md:- NVIDIA/cub#326: Simplify the namespace wrapper macros, and detect when
deps/cub/CHANGELOG.md:- NVIDIA/cub#160, NVIDIA/cub#163, NVIDIA/cub#352: Fixed several bugs in
deps/cub/CHANGELOG.md:- NVIDIA/cub#328: Fixed error handling bug and incorrect debugging output in
deps/cub/CHANGELOG.md:- NVIDIA/cub#335: Fixed a compile error affecting clang and NVRTC. Thanks to
deps/cub/CHANGELOG.md:- NVIDIA/cub#351: Fixed some errors in the `cub::DeviceHistogram` documentation.
deps/cub/CHANGELOG.md:- NVIDIA/cub#348: Add an example that demonstrates how to use dynamic shared
deps/cub/CHANGELOG.md:# CUB 1.13.1 (CUDA Toolkit 11.5)
deps/cub/CHANGELOG.md:CUB 1.13.1 is a minor release accompanying the CUDA Toolkit 11.5.
deps/cub/CHANGELOG.md:NVIDIA/thrust#1401.
deps/cub/CHANGELOG.md:- NVIDIA/cub#326: Add `THRUST_CUB_WRAPPED_NAMESPACE` hooks.
deps/cub/CHANGELOG.md:# CUB 1.13.0 (NVIDIA HPC SDK 21.7)
deps/cub/CHANGELOG.md:CUB 1.13.0 is the major release accompanying the NVIDIA HPC SDK 21.7 release.
deps/cub/CHANGELOG.md:- NVIDIA/cub#320: Deprecated `cub::TexRefInputIterator<T, UNIQUE_ID>`. Use
deps/cub/CHANGELOG.md:- NVIDIA/cub#274: Add `BLOCK_LOAD_STRIPED` and `BLOCK_STORE_STRIPED`
deps/cub/CHANGELOG.md:- NVIDIA/cub#291: `cub::DeviceSegmentedRadixSort` and
deps/cub/CHANGELOG.md:- NVIDIA/cub#306: Add `bfloat16` support to `cub::DeviceRadixSort`. Thanks to
deps/cub/CHANGELOG.md:- NVIDIA/cub#320: Introduce a new `CUB_IGNORE_DEPRECATED_API` macro that
deps/cub/CHANGELOG.md:- NVIDIA/cub#277: Fixed sanitizer warnings in `RadixSortScanBinsKernels`. Thanks
deps/cub/CHANGELOG.md:- NVIDIA/cub#287: `cub::DeviceHistogram` now correctly handles cases
deps/cub/CHANGELOG.md:- NVIDIA/cub#311: Fixed several bugs and added tests for the `cub::BlockShuffle`
deps/cub/CHANGELOG.md:- NVIDIA/cub#312: Eliminate unnecessary kernel instantiations when
deps/cub/CHANGELOG.md:- NVIDIA/cub#319: Fixed out-of-bounds memory access on debugging builds
deps/cub/CHANGELOG.md:- NVIDIA/cub#320: Fixed harmless missing return statement warning in
deps/cub/CHANGELOG.md:    - NVIDIA/cub#275: Fixed comments describing the `cub::If` and `cub::Equals`
deps/cub/CHANGELOG.md:    - NVIDIA/cub#290: Documented that `cub::DeviceSegmentedReduce` will produce
deps/cub/CHANGELOG.md:    - NVIDIA/cub#298: `CONTRIBUTING.md` now refers to Thrust's build
deps/cub/CHANGELOG.md:    - NVIDIA/cub#301: Expand `cub::DeviceScan` documentation to include in-place
deps/cub/CHANGELOG.md:    - NVIDIA/cub#307: Expand `cub::DeviceRadixSort` and `cub::BlockRadixSort`
deps/cub/CHANGELOG.md:    - NVIDIA/cub#316: Move `WARP_TIME_SLICING` documentation to the correct
deps/cub/CHANGELOG.md:    - NVIDIA/cub#321: Update URLs from deprecated github.com to preferred
deps/cub/CHANGELOG.md:# CUB 1.12.1 (CUDA Toolkit 11.4)
deps/cub/CHANGELOG.md:# CUB 1.12.0 (NVIDIA HPC SDK 21.3)
deps/cub/CHANGELOG.md:CUB 1.12.0 is a bugfix release accompanying the NVIDIA HPC SDK 21.3 and
deps/cub/CHANGELOG.md:the CUDA Toolkit 11.4.
deps/cub/CHANGELOG.md:- NVIDIA/cub#256: Deprecate Clang < 7 and MSVC < 2019.
deps/cub/CHANGELOG.md:- NVIDIA/cub#218: Radix sort now treats -0.0 and +0.0 as equivalent for floating
deps/cub/CHANGELOG.md:- NVIDIA/cub#247: Suppress newly triggered warnings in Clang. Thanks to Andrew
deps/cub/CHANGELOG.md:- NVIDIA/cub#249: Enable stricter warning flags. This fixes a number of
deps/cub/CHANGELOG.md:  - NVIDIA/cub#221: Overflow in `temp_storage_bytes` when `num_items` close to
deps/cub/CHANGELOG.md:  - NVIDIA/cub#228: CUB uses non-standard C++ extensions that break strict
deps/cub/CHANGELOG.md:  - NVIDIA/cub#257: Warning when compiling `GridEvenShare` with unsigned
deps/cub/CHANGELOG.md:- NVIDIA/cub#258: Use correct `OffsetT` in `DispatchRadixSort::InitPassConfig`.
deps/cub/CHANGELOG.md:- NVIDIA/cub#259: Remove some problematic `__forceinline__` annotations.
deps/cub/CHANGELOG.md:- NVIDIA/cub#123: Fix incorrect issue number in changelog. Thanks to Peet
deps/cub/CHANGELOG.md:# CUB 1.11.0 (CUDA Toolkit 11.3)
deps/cub/CHANGELOG.md:CUB 1.11.0 is a major release accompanying the CUDA Toolkit 11.3 release,
deps/cub/CHANGELOG.md:- NVIDIA/cub#201: The intermediate accumulator type used when `DeviceScan` is
deps/cub/CHANGELOG.md:- NVIDIA/cub#204: Faster `DeviceRadixSort`, up to 2x performance increase for
deps/cub/CHANGELOG.md:  1.5-2x on Clang CUDA. Thanks to Justin Lebar for this contribution.
deps/cub/CHANGELOG.md:- NVIDIA/cub#200: Allow CUB to be added to CMake projects via `add_subdirectory`.
deps/cub/CHANGELOG.md:- NVIDIA/cub#214: Optionally add install rules when included with
deps/cub/CHANGELOG.md:- NVIDIA/cub#215: Fix integer truncation in `AgentReduceByKey`, `AgentScan`,
deps/cub/CHANGELOG.md:- NVIDIA/cub#225: Fix compile-time regression when defining `CUB_NS_PREFIX`
deps/cub/CHANGELOG.md:- NVIDIA/cub#210: Fix some edge cases in `DeviceScan`:
deps/cub/CHANGELOG.md:- NVIDIA/cub#217: Fix and add test for cmake package install rules. Thanks to
deps/cub/CHANGELOG.md:- NVIDIA/cub#170, NVIDIA/cub#233: Update CUDA version checks to behave on Clang
deps/cub/CHANGELOG.md:  CUDA and `nvc++`. Thanks to Artem Belevich, Andrew Corrigan, and David Olsen
deps/cub/CHANGELOG.md:- NVIDIA/cub#220, NVIDIA/cub#216: Various fixes for Clang CUDA. Thanks to Andrew
deps/cub/CHANGELOG.md:- NVIDIA/cub#231: Fix signedness mismatch warnings in unit tests.
deps/cub/CHANGELOG.md:- NVIDIA/cub#231: Suppress GPU deprecation warnings.
deps/cub/CHANGELOG.md:- NVIDIA/cub#214: Use semantic versioning rules for our CMake package's
deps/cub/CHANGELOG.md:- NVIDIA/cub#214: Use `FindPackageHandleStandardArgs` to print standard status
deps/cub/CHANGELOG.md:- NVIDIA/cub#207: Fix `CubDebug` usage
deps/cub/CHANGELOG.md:- NVIDIA/cub#213: Remove tuning policies for unsupported hardware (<SM35).
deps/cub/CHANGELOG.md:  - Github's `thrust/cub` repository is now `NVIDIA/cub`
deps/cub/CHANGELOG.md:# CUB 1.10.0 (NVIDIA HPC SDK 20.9, CUDA Toolkit 11.2)
deps/cub/CHANGELOG.md:CUB 1.10.0 is the major release accompanying the NVIDIA HPC SDK 20.9 release
deps/cub/CHANGELOG.md:  and the CUDA Toolkit 11.2 release.
deps/cub/CHANGELOG.md:https://github.com/NVIDIA/cub/blob/main/CODE_OF_CONDUCT.md
deps/cub/CHANGELOG.md:- NVIDIA/thrust#1244: Check for macro collisions with system headers during
deps/cub/CHANGELOG.md:- NVIDIA/thrust#1153: Switch to placement new instead of assignment to
deps/cub/CHANGELOG.md:- NVIDIA/cub#38: Fix `cub::DeviceHistogram` for `size_t` `OffsetT`s.
deps/cub/CHANGELOG.md:- NVIDIA/cub#35: Fix GCC-5 maybe-uninitialized warning.
deps/cub/CHANGELOG.md:- NVIDIA/cub#36: Qualify namespace for `va_printf` in `_CubLog`.
deps/cub/CHANGELOG.md:# CUB 1.9.10-1 (NVIDIA HPC SDK 20.7, CUDA Toolkit 11.1)
deps/cub/CHANGELOG.md:CUB 1.9.10-1 is the minor release accompanying the NVIDIA HPC SDK 20.7 release
deps/cub/CHANGELOG.md:  and the CUDA Toolkit 11.1 release.
deps/cub/CHANGELOG.md:- NVIDIA/thrust#1217: Move static local in cub::DeviceCount to a separate
deps/cub/CHANGELOG.md:# CUB 1.9.10 (NVIDIA HPC SDK 20.5)
deps/cub/CHANGELOG.md:Thrust 1.9.10 is the release accompanying the NVIDIA HPC SDK 20.5 release.
deps/cub/CHANGELOG.md:    (ex: `cmake -DCUB_DIR=/usr/local/cuda/include/cub/cmake/ .`) and then you
deps/cub/CHANGELOG.md:# CUB 1.9.9 (CUDA 11.0)
deps/cub/CHANGELOG.md:CUB 1.9.9 is the release accompanying the CUDA Toolkit 11.0 release.
deps/cub/CHANGELOG.md:  GPU-accelerated C++17 Parallel Algorithms.
deps/cub/CHANGELOG.md:    `cudaGetDeviceCount`.
deps/cub/CHANGELOG.md:- Lazily initialize the per-device CUDAattribute caches, because CUDA context
deps/cub/CHANGELOG.md:    creation is expensive and adds up with large CUDA binaries on machines with
deps/cub/CHANGELOG.md:    many GPUs.
deps/cub/CHANGELOG.md:  Thanks to the NVIDIA PyTorch team for bringing this to our attention.
deps/cub/CHANGELOG.md:# CUB 1.9.8-1 (NVIDIA HPC SDK 20.3)
deps/cub/CHANGELOG.md:CUB 1.9.8-1 is a variant of 1.9.8 accompanying the NVIDIA HPC SDK 20.3 release.
deps/cub/CHANGELOG.md:  GPU-accelerated C++17 Parallel Algorithms.
deps/cub/CHANGELOG.md:# CUB 1.9.8 (CUDA 11.0 Early Access)
deps/cub/CHANGELOG.md:  in the CUDA Toolkit.
deps/cub/CHANGELOG.md:When compiling CUB in C++11 mode, CUB now caches calls to CUDA attribute query
deps/cub/CHANGELOG.md:- (C++11 or later) Cache calls to `cudaFuncGetAttributes` and
deps/cub/CHANGELOG.md:    `cudaDeviceGetAttribute` within `cub::PtxVersion` and `cub::SmVersion`.
deps/cub/CHANGELOG.md:    These CUDA APIs acquire locks to CUDA driver/runtime mutex and perform
deps/cub/CHANGELOG.md:- #115: `cub::WarpReduce` segmented reduction is broken in CUDA 9 for logical
deps/cub/CHANGELOG.md:CUB 1.7.1 delivers improved radix sort performance on SM7x (Volta) GPUs and a
deps/cub/CHANGELOG.md:- #104: `uint64_t` `cub::WarpReduce` broken for CUB 1.7.0 on CUDA 8 and older.
deps/cub/CHANGELOG.md:- #103: Can't mix Thrust from CUDA 9.0 and CUB.
deps/cub/CHANGELOG.md:- #98: cuda-memcheck: --tool initcheck failed with lineOfSight.
deps/cub/CHANGELOG.md:CUB 1.7.0 brings support for CUDA 9.0 and SM7x (Volta) GPUs.
deps/cub/CHANGELOG.md:  However, SM1x devices are now deprecated in CUDA, and the interfaces of these
deps/cub/CHANGELOG.md:    SM7x and newer GPUs which have independent thread scheduling.
deps/cub/CHANGELOG.md:  (Pascal) GPUs.
deps/cub/CHANGELOG.md:- Restore fence work-around for scan (reduce-by-key, etc.) hangs in CUDA 8.5.
deps/cub/CHANGELOG.md:  and enhances radix sort performance for SM6x (Pascal) GPUs.
deps/cub/CHANGELOG.md:- Radix sort tuning policies updated for SM6x (Pascal) GPUs - 6.2B 4 byte
deps/cub/CHANGELOG.md:  GPUs.
deps/cub/CHANGELOG.md:- Radix sort tuning policies updated for SM6x (Pascal) GPUs.
deps/cub/CHANGELOG.md:- Issue #47: `cub::CachingDeviceAllocator` needs to clean up CUDA global error
deps/cub/CHANGELOG.md:- Fix for generic-type reduce-by-key `cub::WarpScan` for SM3x and newer GPUs.
deps/cub/CHANGELOG.md:    SM52 (Mawell) GPUs.
deps/cub/CHANGELOG.md:- Fix CUDA 7.5 issues on SM52 GPUs with SHFL-based warp-scan and
deps/cub/CHANGELOG.md:- Fix minor CUDA 7.0 performance regressions in `cub::DeviceScan` and
deps/cub/CHANGELOG.md:    when invoking CUB device-wide rountines using CUDA dynamic parallelism.
deps/cub/CHANGELOG.md:  GPUs.
deps/cub/CHANGELOG.md:- Support and performance tuning for SM5x (Maxwell) GPUs.
deps/cub/CHANGELOG.md:- Workaround for a benign WAW race warning reported by cuda-memcheck
deps/cub/CHANGELOG.md:    SM3x (Kepler) GPUs.
deps/cub/CHANGELOG.md:    compiled for SM1x is run on newer GPUs of higher compute-capability: the
deps/cub/CHANGELOG.md:  `cub::DeviceReduce::RunLengthEncode` and support for CUDA 6.0.
deps/cub/CHANGELOG.md:  - Explain that iterator (in)compatibilities with CUDA 5.0 (and older) and
deps/cub/CHANGELOG.md:- Added workaround to make `cub::TexRefInputIteratorT` work with CUDA 6.0.
deps/cub/CHANGELOG.md:Additionally, scan and sort performance for older GPUs has been improved and
deps/cub/CHANGELOG.md:    GPUs (SM1x to SM3x).
deps/cub/CHANGELOG.md:- CUDA Dynamic Parallelism (CDP, e.g. device-callable) versions of device-wide
deps/cub/CHANGELOG.md:- Updates to accommodate CUDA 5.5 dynamic parallelism.
deps/cub/CHANGELOG.md:  warp-level, and thread-level primitives for CUDA kernel programming.
deps/cub/cub/util_macro.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/util_macro.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/util_macro.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/block/block_raking_layout.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/block/block_raking_layout.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/block/block_raking_layout.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/block/block_raking_layout.cuh: * This type facilitates a shared memory usage pattern where a block of CUDA
deps/cub/cub/block/block_store.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/block/block_store.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/block/block_store.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/block/block_store.cuh: * Operations for writing linear segments of data from the CUDA thread block
deps/cub/cub/block/block_store.cuh: * which is the default starting offset returned by \p cudaMalloc()
deps/cub/cub/block/block_store.cuh: *   - The data type \p T is not a built-in primitive or CUDA vector type (e.g., \p short, \p int2, \p double, \p float2, etc.)
deps/cub/cub/block/block_store.cuh:        // Maximum CUDA vector size is 4 elements
deps/cub/cub/block/block_store.cuh: * \brief cub::BlockStoreAlgorithm enumerates alternative algorithms for cub::BlockStore to write a blocked arrangement of items across a CUDA thread block to a linear segment of memory.
deps/cub/cub/block/block_store.cuh:     * to memory using CUDA's built-in vectorized stores as a coalescing optimization.
deps/cub/cub/block/block_store.cuh:     *   - The data type \p T is not a built-in primitive or CUDA vector type (e.g., \p short, \p int2, \p double, \p float2, etc.)
deps/cub/cub/block/block_store.cuh: * \brief The BlockStore class provides [<em>collective</em>](index.html#sec0) data movement methods for writing a [<em>blocked arrangement</em>](index.html#sec5sec3) of items partitioned across a CUDA thread block to a linear segment of memory.  ![](block_store_logo.png)
deps/cub/cub/block/block_store.cuh: *      of data is written directly to memory using CUDA's built-in vectorized stores as a
deps/cub/cub/block/specializations/block_scan_raking.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/block/specializations/block_scan_raking.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/block/specializations/block_scan_raking.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/block/specializations/block_scan_raking.cuh: * cub::BlockScanRaking provides variants of raking-based parallel prefix scan across a CUDA thread block.
deps/cub/cub/block/specializations/block_scan_raking.cuh: * \brief BlockScanRaking provides variants of raking-based parallel prefix scan across a CUDA thread block.
deps/cub/cub/block/specializations/block_histogram_sort.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/block/specializations/block_histogram_sort.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/block/specializations/block_histogram_sort.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/block/specializations/block_histogram_sort.cuh: * The cub::BlockHistogramSort class provides sorting-based methods for constructing block-wide histograms from data samples partitioned across a CUDA thread block.
deps/cub/cub/block/specializations/block_histogram_sort.cuh: * \brief The BlockHistogramSort class provides sorting-based methods for constructing block-wide histograms from data samples partitioned across a CUDA thread block.
deps/cub/cub/block/specializations/block_histogram_sort.cuh:            cudaSharedMemBankSizeFourByte,
deps/cub/cub/block/specializations/block_reduce_raking.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/block/specializations/block_reduce_raking.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/block/specializations/block_reduce_raking.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/block/specializations/block_reduce_raking.cuh: * cub::BlockReduceRaking provides raking-based methods of parallel reduction across a CUDA thread block.  Supports non-commutative reduction operators.
deps/cub/cub/block/specializations/block_reduce_raking.cuh: * \brief BlockReduceRaking provides raking-based methods of parallel reduction across a CUDA thread block.  Supports non-commutative reduction operators.
deps/cub/cub/block/specializations/block_reduce_warp_reductions.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/block/specializations/block_reduce_warp_reductions.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/block/specializations/block_reduce_warp_reductions.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/block/specializations/block_reduce_warp_reductions.cuh: * cub::BlockReduceWarpReductions provides variants of warp-reduction-based parallel reduction across a CUDA thread block.  Supports non-commutative reduction operators.
deps/cub/cub/block/specializations/block_reduce_warp_reductions.cuh: * \brief BlockReduceWarpReductions provides variants of warp-reduction-based parallel reduction across a CUDA thread block.  Supports non-commutative reduction operators.
deps/cub/cub/block/specializations/block_histogram_atomic.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/block/specializations/block_histogram_atomic.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/block/specializations/block_histogram_atomic.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/block/specializations/block_histogram_atomic.cuh: * The cub::BlockHistogramAtomic class provides atomic-based methods for constructing block-wide histograms from data samples partitioned across a CUDA thread block.
deps/cub/cub/block/specializations/block_histogram_atomic.cuh: * \brief The BlockHistogramAtomic class provides atomic-based methods for constructing block-wide histograms from data samples partitioned across a CUDA thread block.
deps/cub/cub/block/specializations/block_scan_warp_scans2.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/block/specializations/block_scan_warp_scans2.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/block/specializations/block_scan_warp_scans2.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/block/specializations/block_scan_warp_scans2.cuh: * cub::BlockScanWarpscans provides warpscan-based variants of parallel prefix scan across a CUDA thread block.
deps/cub/cub/block/specializations/block_scan_warp_scans2.cuh: * \brief BlockScanWarpScans provides warpscan-based variants of parallel prefix scan across a CUDA thread block.
deps/cub/cub/block/specializations/block_scan_warp_scans.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/block/specializations/block_scan_warp_scans.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/block/specializations/block_scan_warp_scans.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/block/specializations/block_scan_warp_scans.cuh: * cub::BlockScanWarpscans provides warpscan-based variants of parallel prefix scan across a CUDA thread block.
deps/cub/cub/block/specializations/block_scan_warp_scans.cuh: * \brief BlockScanWarpScans provides warpscan-based variants of parallel prefix scan across a CUDA thread block.
deps/cub/cub/block/specializations/block_scan_warp_scans3.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/block/specializations/block_scan_warp_scans3.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/block/specializations/block_scan_warp_scans3.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/block/specializations/block_scan_warp_scans3.cuh: * cub::BlockScanWarpscans provides warpscan-based variants of parallel prefix scan across a CUDA thread block.
deps/cub/cub/block/specializations/block_scan_warp_scans3.cuh: * \brief BlockScanWarpScans provides warpscan-based variants of parallel prefix scan across a CUDA thread block.
deps/cub/cub/block/specializations/block_reduce_raking_commutative_only.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/block/specializations/block_reduce_raking_commutative_only.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/block/specializations/block_reduce_raking_commutative_only.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/block/specializations/block_reduce_raking_commutative_only.cuh: * cub::BlockReduceRakingCommutativeOnly provides raking-based methods of parallel reduction across a CUDA thread block.  Does not support non-commutative reduction operators.
deps/cub/cub/block/specializations/block_reduce_raking_commutative_only.cuh: * \brief BlockReduceRakingCommutativeOnly provides raking-based methods of parallel reduction across a CUDA thread block.  Does not support non-commutative reduction operators.  Does not support block sizes that are not a multiple of the warp size.
deps/cub/cub/block/block_radix_sort.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/block/block_radix_sort.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/block/block_radix_sort.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/block/block_radix_sort.cuh: * The cub::BlockRadixSort class provides [<em>collective</em>](index.html#sec0) methods for radix sorting of items partitioned across a CUDA thread block.
deps/cub/cub/block/block_radix_sort.cuh: * \brief The BlockRadixSort class provides [<em>collective</em>](index.html#sec0) methods for sorting items partitioned across a CUDA thread block using a radix sorting method.  ![](sorting_logo.png)
deps/cub/cub/block/block_radix_sort.cuh: * \tparam SMEM_CONFIG          <b>[optional]</b> Shared memory bank mode (default: \p cudaSharedMemBankSizeFourByte)
deps/cub/cub/block/block_radix_sort.cuh: * (<tt>unsigned char</tt>, \p int, \p double, etc.) as well as CUDA's \p __half
deps/cub/cub/block/block_radix_sort.cuh:    cudaSharedMemConfig     SMEM_CONFIG             = cudaSharedMemBankSizeFourByte,
deps/cub/cub/block/block_adjacent_difference.cuh: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/block/block_adjacent_difference.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/block/block_adjacent_difference.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/block/block_adjacent_difference.cuh: * of adjacent elements partitioned across a CUDA thread block.
deps/cub/cub/block/block_adjacent_difference.cuh: *        differences of adjacent elements partitioned across a CUDA thread
deps/cub/cub/block/block_adjacent_difference.cuh: *   the elements partitioned across a CUDA thread block. Because the binary
deps/cub/cub/block/block_adjacent_difference.cuh:     *        partitioned across a CUDA thread block.
deps/cub/cub/block/block_adjacent_difference.cuh:     *        partitioned across a CUDA thread block.
deps/cub/cub/block/block_adjacent_difference.cuh:     * @brief Subtracts the left element of each adjacent pair of elements partitioned across a CUDA thread block.
deps/cub/cub/block/block_adjacent_difference.cuh:     *        partitioned across a CUDA thread block.
deps/cub/cub/block/block_adjacent_difference.cuh:     *        partitioned across a CUDA thread block.
deps/cub/cub/block/block_adjacent_difference.cuh:     *        elements partitioned across a CUDA thread block.
deps/cub/cub/block/block_load.cuh: * Copyright (c) 2011-2016, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/block/block_load.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/block/block_load.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/block/block_load.cuh: * Operations for reading linear tiles of data into the CUDA thread block.
deps/cub/cub/block/block_load.cuh: *   - The data type \p T is not a built-in primitive or CUDA vector type (e.g., \p short, \p int2, \p double, \p float2, etc.)
deps/cub/cub/block/block_load.cuh: * \brief cub::BlockLoadAlgorithm enumerates alternative algorithms for cub::BlockLoad to read a linear segment of data from memory into a blocked arrangement across a CUDA thread block.
deps/cub/cub/block/block_load.cuh:     * from memory using CUDA's built-in vectorized loads as a coalescing optimization.
deps/cub/cub/block/block_load.cuh:     *   - The data type \p T is not a built-in primitive or CUDA vector type
deps/cub/cub/block/block_load.cuh: * \brief The BlockLoad class provides [<em>collective</em>](index.html#sec0) data movement methods for loading a linear segment of items from memory into a [<em>blocked arrangement</em>](index.html#sec5sec3) across a CUDA thread block.  ![](block_load_logo.png)
deps/cub/cub/block/block_load.cuh: *      of data is read directly from memory using CUDA's built-in vectorized loads as a
deps/cub/cub/block/block_discontinuity.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/block/block_discontinuity.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/block/block_discontinuity.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/block/block_discontinuity.cuh: * The cub::BlockDiscontinuity class provides [<em>collective</em>](index.html#sec0) methods for flagging discontinuities within an ordered set of items partitioned across a CUDA thread block.
deps/cub/cub/block/block_discontinuity.cuh: * \brief The BlockDiscontinuity class provides [<em>collective</em>](index.html#sec0) methods for flagging discontinuities within an ordered set of items partitioned across a CUDA thread block. ![](discont_logo.png)
deps/cub/cub/block/block_radix_rank.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/block/block_radix_rank.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/block/block_radix_rank.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/block/block_radix_rank.cuh: * cub::BlockRadixRank provides operations for ranking unsigned integer types within a CUDA thread block
deps/cub/cub/block/block_radix_rank.cuh: * \brief BlockRadixRank provides operations for ranking unsigned integer types within a CUDA thread block.
deps/cub/cub/block/block_radix_rank.cuh: * \tparam SMEM_CONFIG          <b>[optional]</b> Shared memory bank mode (default: \p cudaSharedMemBankSizeFourByte)
deps/cub/cub/block/block_radix_rank.cuh:    cudaSharedMemConfig     SMEM_CONFIG             = cudaSharedMemBankSizeFourByte,
deps/cub/cub/block/block_radix_rank.cuh:      cub::detail::conditional_t<SMEM_CONFIG == cudaSharedMemBankSizeEightByte,
deps/cub/cub/block/block_scan.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/block/block_scan.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/block/block_scan.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/block/block_scan.cuh: * The cub::BlockScan class provides [<em>collective</em>](index.html#sec0) methods for computing a parallel prefix sum/scan of items partitioned across a CUDA thread block.
deps/cub/cub/block/block_scan.cuh: * \brief BlockScanAlgorithm enumerates alternative algorithms for cub::BlockScan to compute a parallel prefix scan across a CUDA thread block.
deps/cub/cub/block/block_scan.cuh:     *   GPU is under-occupied, it can often provide higher overall throughput
deps/cub/cub/block/block_scan.cuh:     *   across the GPU when suitably occupied.
deps/cub/cub/block/block_scan.cuh:     *   GPU because due to a heavy reliance on inefficient warpscans, it can
deps/cub/cub/block/block_scan.cuh:     *   often provide lower turnaround latencies when the GPU is under-occupied.
deps/cub/cub/block/block_scan.cuh: * \brief The BlockScan class provides [<em>collective</em>](index.html#sec0) methods for computing a parallel prefix sum/scan of items partitioned across a CUDA thread block. ![](block_scan_logo.png)
deps/cub/cub/block/block_run_length_decode.cuh: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/block/block_run_length_decode.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/block/block_run_length_decode.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/block/block_merge_sort.cuh: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/block/block_merge_sort.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/block/block_merge_sort.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/block/block_merge_sort.cuh:   * @brief Sorts items partitioned across a CUDA thread block using
deps/cub/cub/block/block_merge_sort.cuh:   * @brief Sorts items partitioned across a CUDA thread block using
deps/cub/cub/block/block_merge_sort.cuh:   * @brief Sorts items partitioned across a CUDA thread block using a merge sorting method.
deps/cub/cub/block/block_merge_sort.cuh:   * @brief Sorts items partitioned across a CUDA thread block using
deps/cub/cub/block/block_merge_sort.cuh:   * @brief Sorts items partitioned across a CUDA thread block using
deps/cub/cub/block/block_merge_sort.cuh:   * @brief Sorts items partitioned across a CUDA thread block using
deps/cub/cub/block/block_merge_sort.cuh:   * @brief Sorts items partitioned across a CUDA thread block using
deps/cub/cub/block/block_merge_sort.cuh:   * @brief Sorts items partitioned across a CUDA thread block using
deps/cub/cub/block/block_merge_sort.cuh: *        partitioned across a CUDA thread block using a merge sorting method.
deps/cub/cub/block/block_shuffle.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/block/block_shuffle.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/block/block_shuffle.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/block/block_shuffle.cuh: * The cub::BlockShuffle class provides [<em>collective</em>](index.html#sec0) methods for shuffling data partitioned across a CUDA thread block.
deps/cub/cub/block/block_shuffle.cuh: * \brief The BlockShuffle class provides [<em>collective</em>](index.html#sec0) methods for shuffling data partitioned across a CUDA thread block.
deps/cub/cub/block/radix_rank_sort_operations.cuh: * Copyright (c) 2011-2020, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/block/radix_rank_sort_operations.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/block/radix_rank_sort_operations.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/block/block_exchange.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/block/block_exchange.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/block/block_exchange.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/block/block_exchange.cuh: * The cub::BlockExchange class provides [<em>collective</em>](index.html#sec0) methods for rearranging data partitioned across a CUDA thread block.
deps/cub/cub/block/block_exchange.cuh: * \brief The BlockExchange class provides [<em>collective</em>](index.html#sec0) methods for rearranging data partitioned across a CUDA thread block. ![](transpose_logo.png)
deps/cub/cub/block/block_histogram.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/block/block_histogram.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/block/block_histogram.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/block/block_histogram.cuh: * The cub::BlockHistogram class provides [<em>collective</em>](index.html#sec0) methods for constructing block-wide histograms from data samples partitioned across a CUDA thread block.
deps/cub/cub/block/block_histogram.cuh: * \brief The BlockHistogram class provides [<em>collective</em>](index.html#sec0) methods for constructing block-wide histograms from data samples partitioned across a CUDA thread block. ![](histogram_logo.png)
deps/cub/cub/block/block_reduce.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/block/block_reduce.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/block/block_reduce.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/block/block_reduce.cuh: * The cub::BlockReduce class provides [<em>collective</em>](index.html#sec0) methods for computing a parallel reduction of items partitioned across a CUDA thread block.
deps/cub/cub/block/block_reduce.cuh: * reduction across a CUDA thread block.
deps/cub/cub/block/block_reduce.cuh:     *   throughput across the GPU when suitably occupied.  However, turn-around latency may be
deps/cub/cub/block/block_reduce.cuh:     *   when the GPU is under-occupied.
deps/cub/cub/block/block_reduce.cuh:     *   throughput across the GPU when suitably occupied.  However, turn-around latency may be
deps/cub/cub/block/block_reduce.cuh:     *   when the GPU is under-occupied.
deps/cub/cub/block/block_reduce.cuh:     *   throughput across the GPU.  However turn-around latency may be lower and
deps/cub/cub/block/block_reduce.cuh:     *   thus useful when the GPU is under-occupied.
deps/cub/cub/block/block_reduce.cuh: * \brief The BlockReduce class provides [<em>collective</em>](index.html#sec0) methods for computing a parallel reduction of items partitioned across a CUDA thread block. ![](reduce_logo.png)
deps/cub/cub/detail/device_double_buffer.cuh: *  Copyright 2021 NVIDIA Corporation
deps/cub/cub/detail/temporary_storage.cuh:*  Copyright 2021 NVIDIA Corporation
deps/cub/cub/detail/temporary_storage.cuh:      // `cudaMalloc` will return `nullptr`. This fact makes it impossible to
deps/cub/cub/detail/temporary_storage.cuh:  __host__ __device__ cudaError_t map_to_buffer(void *d_temp_storage,
deps/cub/cub/detail/temporary_storage.cuh:      return cudaErrorAlreadyMapped;
deps/cub/cub/detail/temporary_storage.cuh:    cudaError_t error = cudaSuccess;
deps/cub/cub/detail/type_traits.cuh: * Copyright (c) 2021, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/detail/type_traits.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/detail/type_traits.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/detail/device_synchronize.cuh: *  Copyright 2021 NVIDIA Corporation
deps/cub/cub/detail/device_synchronize.cuh:#include <cuda_runtime_api.h>
deps/cub/cub/detail/device_synchronize.cuh: * Call `cudaDeviceSynchronize()` using the proper API for the current CUB and
deps/cub/cub/detail/device_synchronize.cuh: * CUDA configuration.
deps/cub/cub/detail/device_synchronize.cuh:CUB_RUNTIME_FUNCTION inline cudaError_t device_synchronize()
deps/cub/cub/detail/device_synchronize.cuh:  cudaError_t result = cudaErrorUnknown;
deps/cub/cub/detail/device_synchronize.cuh:    result = cudaDeviceSynchronize();
deps/cub/cub/detail/device_synchronize.cuh:    // Device code with the CUDA runtime.
deps/cub/cub/detail/device_synchronize.cuh:#if defined(__CUDACC__) &&                                                     \
deps/cub/cub/detail/device_synchronize.cuh:  ((__CUDACC_VER_MAJOR__ > 11) ||                                              \
deps/cub/cub/detail/device_synchronize.cuh:   ((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 6)))
deps/cub/cub/detail/device_synchronize.cuh:    // CUDA >= 11.6
deps/cub/cub/detail/device_synchronize.cuh:    result = __cudaDeviceSynchronizeDeprecationAvoidance();
deps/cub/cub/detail/device_synchronize.cuh:#else // CUDA < 11.6
deps/cub/cub/detail/device_synchronize.cuh:    result = cudaDeviceSynchronize();
deps/cub/cub/detail/device_synchronize.cuh:#else // Device code without the CUDA runtime.
deps/cub/cub/detail/device_synchronize.cuh:    // Device side CUDA API calls are not supported in this configuration.
deps/cub/cub/detail/device_synchronize.cuh:    result = cudaErrorInvalidConfiguration;
deps/cub/cub/detail/exec_check_disable.cuh:*  Copyright 2021 NVIDIA Corporation
deps/cub/cub/detail/exec_check_disable.cuh:#if defined(__CUDACC__) && \
deps/cub/cub/detail/exec_check_disable.cuh:    !defined(_NVHPC_CUDA) && \
deps/cub/cub/detail/exec_check_disable.cuh:    !(defined(__CUDA__) && defined(__clang__))
deps/cub/cub/util_allocator.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/util_allocator.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/util_allocator.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/util_allocator.cuh:        cudaStream_t    associated_stream;  // Associated associated_stream
deps/cub/cub/util_allocator.cuh:        cudaEvent_t     ready_event;        // Signal when associated stream has run to the point at which this block was freed
deps/cub/cub/util_allocator.cuh:    typedef std::map<int, TotalBytes> GpuCachedBytes;
deps/cub/cub/util_allocator.cuh:    const bool      skip_cleanup;       /// Whether or not to skip a call to FreeAllCached() when destructor is called.  (The CUDA runtime may have already shut down for statically declared allocators)
deps/cub/cub/util_allocator.cuh:    GpuCachedBytes  cached_bytes;       /// Map of device ordinal to aggregate cached bytes on that device
deps/cub/cub/util_allocator.cuh:    cudaError_t SetMaxCachedBytes(size_t max_cached_bytes_)
deps/cub/cub/util_allocator.cuh:        return cudaSuccess;
deps/cub/cub/util_allocator.cuh:    cudaError_t DeviceAllocate(
deps/cub/cub/util_allocator.cuh:        cudaStream_t    active_stream = 0)  ///< [in] The stream to be associated with this allocation
deps/cub/cub/util_allocator.cuh:        cudaError_t error               = cudaSuccess;
deps/cub/cub/util_allocator.cuh:            if (CubDebug(error = cudaGetDevice(&entrypoint_device))) return error;
deps/cub/cub/util_allocator.cuh:                    const cudaError_t event_status = cudaEventQuery(block_itr->ready_event);
deps/cub/cub/util_allocator.cuh:                    if(event_status != cudaErrorNotReady)
deps/cub/cub/util_allocator.cuh:                if (CubDebug(error = cudaGetDevice(&entrypoint_device))) return error;
deps/cub/cub/util_allocator.cuh:                if (CubDebug(error = cudaSetDevice(device))) return error;
deps/cub/cub/util_allocator.cuh:            if (CubDebug(error = cudaMalloc(&search_key.d_ptr, search_key.bytes)) == cudaErrorMemoryAllocation)
deps/cub/cub/util_allocator.cuh:                error = cudaSuccess;    // Reset the error we will return
deps/cub/cub/util_allocator.cuh:                cudaGetLastError();     // Reset CUDART's error
deps/cub/cub/util_allocator.cuh:                    // No need to worry about synchronization with the device: cudaFree is
deps/cub/cub/util_allocator.cuh:                    if (CubDebug(error = cudaFree(block_itr->d_ptr))) break;
deps/cub/cub/util_allocator.cuh:                    if (CubDebug(error = cudaEventDestroy(block_itr->ready_event))) break;
deps/cub/cub/util_allocator.cuh:                if (CubDebug(error = cudaMalloc(&search_key.d_ptr, search_key.bytes))) return error;
deps/cub/cub/util_allocator.cuh:            if (CubDebug(error = cudaEventCreateWithFlags(&search_key.ready_event, cudaEventDisableTiming)))
deps/cub/cub/util_allocator.cuh:                if (CubDebug(error = cudaSetDevice(entrypoint_device))) return error;
deps/cub/cub/util_allocator.cuh:    cudaError_t DeviceAllocate(
deps/cub/cub/util_allocator.cuh:        cudaStream_t    active_stream = 0)  ///< [in] The stream to be associated with this allocation
deps/cub/cub/util_allocator.cuh:    cudaError_t DeviceFree(
deps/cub/cub/util_allocator.cuh:        cudaError_t error               = cudaSuccess;
deps/cub/cub/util_allocator.cuh:            if (CubDebug(error = cudaGetDevice(&entrypoint_device)))
deps/cub/cub/util_allocator.cuh:            if (CubDebug(error = cudaGetDevice(&entrypoint_device))) return error;
deps/cub/cub/util_allocator.cuh:            if (CubDebug(error = cudaSetDevice(device))) return error;
deps/cub/cub/util_allocator.cuh:            if (CubDebug(error = cudaEventRecord(search_key.ready_event, search_key.associated_stream))) return error;
deps/cub/cub/util_allocator.cuh:            if (CubDebug(error = cudaFree(d_ptr))) return error;
deps/cub/cub/util_allocator.cuh:            if (CubDebug(error = cudaEventDestroy(search_key.ready_event))) return error;
deps/cub/cub/util_allocator.cuh:            if (CubDebug(error = cudaSetDevice(entrypoint_device))) return error;
deps/cub/cub/util_allocator.cuh:    cudaError_t DeviceFree(
deps/cub/cub/util_allocator.cuh:    cudaError_t FreeAllCached()
deps/cub/cub/util_allocator.cuh:        cudaError_t error         = cudaSuccess;
deps/cub/cub/util_allocator.cuh:                if (CubDebug(error = cudaGetDevice(&entrypoint_device))) break;
deps/cub/cub/util_allocator.cuh:                if (CubDebug(error = cudaSetDevice(begin->device))) break;
deps/cub/cub/util_allocator.cuh:            if (CubDebug(error = cudaFree(begin->d_ptr))) break;
deps/cub/cub/util_allocator.cuh:            if (CubDebug(error = cudaEventDestroy(begin->ready_event))) break;
deps/cub/cub/util_allocator.cuh:            if (CubDebug(error = cudaSetDevice(entrypoint_device))) return error;
deps/cub/cub/util_math.cuh: * Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/util_math.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/util_math.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/host/mutex.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/host/mutex.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/host/mutex.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/util_compiler.cuh: * Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/util_compiler.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/util_compiler.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/util_compiler.cuh:#if defined(__CUDACC__) || defined(_NVHPC_CUDA)
deps/cub/cub/util_compiler.cuh:// CUDA-capable clang should behave similar to NVCC.
deps/cub/cub/util_compiler.cuh:#  if defined(__CUDA__)
deps/cub/cub/iterator/discard_output_iterator.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/iterator/discard_output_iterator.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/iterator/discard_output_iterator.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/iterator/constant_input_iterator.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/iterator/constant_input_iterator.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/iterator/constant_input_iterator.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/iterator/arg_index_input_iterator.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/iterator/arg_index_input_iterator.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/iterator/arg_index_input_iterator.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/iterator/tex_obj_input_iterator.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/iterator/tex_obj_input_iterator.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/iterator/tex_obj_input_iterator.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/iterator/tex_obj_input_iterator.cuh:    cudaTextureObject_t tex_obj;
deps/cub/cub/iterator/tex_obj_input_iterator.cuh:    cudaError_t BindTexture(
deps/cub/cub/iterator/tex_obj_input_iterator.cuh:        QualifiedT      *ptr,               ///< Native pointer to wrap that is aligned to cudaDeviceProp::textureAlignment
deps/cub/cub/iterator/tex_obj_input_iterator.cuh:        cudaChannelFormatDesc   channel_desc = cudaCreateChannelDesc<TextureWord>();
deps/cub/cub/iterator/tex_obj_input_iterator.cuh:        cudaResourceDesc        res_desc;
deps/cub/cub/iterator/tex_obj_input_iterator.cuh:        cudaTextureDesc         tex_desc;
deps/cub/cub/iterator/tex_obj_input_iterator.cuh:        memset(&res_desc, 0, sizeof(cudaResourceDesc));
deps/cub/cub/iterator/tex_obj_input_iterator.cuh:        memset(&tex_desc, 0, sizeof(cudaTextureDesc));
deps/cub/cub/iterator/tex_obj_input_iterator.cuh:        res_desc.resType                = cudaResourceTypeLinear;
deps/cub/cub/iterator/tex_obj_input_iterator.cuh:        tex_desc.readMode               = cudaReadModeElementType;
deps/cub/cub/iterator/tex_obj_input_iterator.cuh:        return CubDebug(cudaCreateTextureObject(&tex_obj, &res_desc, &tex_desc, NULL));
deps/cub/cub/iterator/tex_obj_input_iterator.cuh:    cudaError_t UnbindTexture()
deps/cub/cub/iterator/tex_obj_input_iterator.cuh:        return CubDebug(cudaDestroyTextureObject(tex_obj));
deps/cub/cub/iterator/transform_input_iterator.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/iterator/transform_input_iterator.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/iterator/transform_input_iterator.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/iterator/cache_modified_output_iterator.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/iterator/cache_modified_output_iterator.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/iterator/cache_modified_output_iterator.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/iterator/cache_modified_input_iterator.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/iterator/cache_modified_input_iterator.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/iterator/cache_modified_input_iterator.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/iterator/counting_input_iterator.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/iterator/counting_input_iterator.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/iterator/counting_input_iterator.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/iterator/tex_ref_input_iterator.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/iterator/tex_ref_input_iterator.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/iterator/tex_ref_input_iterator.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/iterator/tex_ref_input_iterator.cuh:#if (CUDART_VERSION >= 5050) || defined(DOXYGEN_ACTIVE)  // This iterator is compatible with CUDA 5.5 and newer
deps/cub/cub/iterator/tex_ref_input_iterator.cuh:// This class uses the deprecated cudaBindTexture / cudaUnbindTexture APIs.
deps/cub/cub/iterator/tex_ref_input_iterator.cuh:// See issue NVIDIA/cub#191.
deps/cub/cub/iterator/tex_ref_input_iterator.cuh:        static cudaError_t BindTexture(void *d_in, size_t &bytes, size_t &offset)
deps/cub/cub/iterator/tex_ref_input_iterator.cuh:                cudaChannelFormatDesc tex_desc = cudaCreateChannelDesc<TextureWord>();
deps/cub/cub/iterator/tex_ref_input_iterator.cuh:                return (CubDebug(cudaBindTexture(&offset, ref, d_in, bytes)));
deps/cub/cub/iterator/tex_ref_input_iterator.cuh:            return cudaSuccess;
deps/cub/cub/iterator/tex_ref_input_iterator.cuh:        static cudaError_t UnbindTexture()
deps/cub/cub/iterator/tex_ref_input_iterator.cuh:            return CubDebug(cudaUnbindTexture(ref));
deps/cub/cub/iterator/tex_ref_input_iterator.cuh: * \deprecated [Since 1.13.0] The CUDA texture management APIs used by
deps/cub/cub/iterator/tex_ref_input_iterator.cuh:// This class uses the deprecated cudaBindTexture / cudaUnbindTexture APIs.
deps/cub/cub/iterator/tex_ref_input_iterator.cuh:// See issue NVIDIA/cub#191.
deps/cub/cub/iterator/tex_ref_input_iterator.cuh:    cudaError_t BindTexture(
deps/cub/cub/iterator/tex_ref_input_iterator.cuh:        QualifiedT      *ptr,                   ///< Native pointer to wrap that is aligned to cudaDeviceProp::textureAlignment
deps/cub/cub/iterator/tex_ref_input_iterator.cuh:        cudaError_t retval = TexId::BindTexture(this->ptr + tex_offset, bytes, offset);
deps/cub/cub/iterator/tex_ref_input_iterator.cuh:    cudaError_t UnbindTexture()
deps/cub/cub/iterator/tex_ref_input_iterator.cuh:#endif // CUDART_VERSION
deps/cub/cub/device/device_merge_sort.cuh: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/device/device_merge_sort.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/device/device_merge_sort.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/device/device_merge_sort.cuh: * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_merge_sort.cuh:   * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_merge_sort.cuh:   *   **[optional]** CUDA stream to launch kernels within. Default is
deps/cub/cub/device/device_merge_sort.cuh:  CUB_RUNTIME_FUNCTION static cudaError_t
deps/cub/cub/device/device_merge_sort.cuh:            cudaStream_t stream    = 0,
deps/cub/cub/device/device_merge_sort.cuh:   * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_merge_sort.cuh:   *   **[optional]** CUDA stream to launch kernels within. Default is
deps/cub/cub/device/device_merge_sort.cuh:  CUB_RUNTIME_FUNCTION static cudaError_t
deps/cub/cub/device/device_merge_sort.cuh:                cudaStream_t stream    = 0,
deps/cub/cub/device/device_merge_sort.cuh:   * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_merge_sort.cuh:   *   **[optional]** CUDA stream to launch kernels within. Default is
deps/cub/cub/device/device_merge_sort.cuh:  CUB_RUNTIME_FUNCTION static cudaError_t
deps/cub/cub/device/device_merge_sort.cuh:           cudaStream_t stream    = 0,
deps/cub/cub/device/device_merge_sort.cuh:   * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_merge_sort.cuh:   *   **[optional]** CUDA stream to launch kernels within. Default is
deps/cub/cub/device/device_merge_sort.cuh:  CUB_RUNTIME_FUNCTION static cudaError_t
deps/cub/cub/device/device_merge_sort.cuh:               cudaStream_t stream    = 0,
deps/cub/cub/device/device_merge_sort.cuh:   * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_merge_sort.cuh:   *   **[optional]** CUDA stream to launch kernels within. Default is
deps/cub/cub/device/device_merge_sort.cuh:  CUB_RUNTIME_FUNCTION static cudaError_t
deps/cub/cub/device/device_merge_sort.cuh:                  cudaStream_t stream    = 0,
deps/cub/cub/device/device_merge_sort.cuh:   * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_merge_sort.cuh:   *   **[optional]** CUDA stream to launch kernels within. Default is
deps/cub/cub/device/device_merge_sort.cuh:  CUB_RUNTIME_FUNCTION static cudaError_t
deps/cub/cub/device/device_merge_sort.cuh:                 cudaStream_t stream    = 0,
deps/cub/cub/device/device_select.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/device/device_select.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/device/device_select.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/device/device_select.cuh: * performance across different CUDA architectures for \p int32 items,
deps/cub/cub/device/device_select.cuh: * performance across different CUDA architectures for \p int32 items
deps/cub/cub/device/device_select.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_select.cuh:    static cudaError_t Flagged(
deps/cub/cub/device/device_select.cuh:        cudaStream_t                stream             = 0,         ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/device_select.cuh:     * CUDA architectures for \p int32 and \p int64 items, respectively.  Items are
deps/cub/cub/device/device_select.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_select.cuh:    static cudaError_t If(
deps/cub/cub/device/device_select.cuh:        cudaStream_t                stream             = 0,         ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/device_select.cuh:     * CUDA architectures for \p int32 and \p int64 items, respectively.  Segments have
deps/cub/cub/device/device_select.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_select.cuh:    static cudaError_t Unique(
deps/cub/cub/device/device_select.cuh:        cudaStream_t                stream             = 0,         ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/device_scan.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/device/device_scan.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/device/device_scan.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/device/device_scan.cuh: * [1] [Duane Merrill and Michael Garland.  "Single-pass Parallel Prefix Scan with Decoupled Look-back", <em>NVIDIA Technical Report NVR-2016-002</em>, 2016.](https://research.nvidia.com/publication/single-pass-parallel-prefix-scan-decoupled-look-back)
deps/cub/cub/device/device_scan.cuh: * performance across different CUDA architectures for \p int32 keys.
deps/cub/cub/device/device_scan.cuh:     *   (e.g., addition of floating point types) on the same GPU device.
deps/cub/cub/device/device_scan.cuh:     * CUDA architectures for \p int32 and \p int64 items, respectively.
deps/cub/cub/device/device_scan.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_scan.cuh:    static cudaError_t ExclusiveSum(
deps/cub/cub/device/device_scan.cuh:        cudaStream_t    stream              = 0,            ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/device_scan.cuh:     *   (e.g., addition of floating point types) on the same GPU device.
deps/cub/cub/device/device_scan.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_scan.cuh:    static cudaError_t ExclusiveScan(
deps/cub/cub/device/device_scan.cuh:        cudaStream_t    stream              = 0,            ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/device_scan.cuh:    static cudaError_t ExclusiveScan(
deps/cub/cub/device/device_scan.cuh:        cudaStream_t                            stream              = 0,            ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/device_scan.cuh:     *   (e.g., addition of floating point types) on the same GPU device.
deps/cub/cub/device/device_scan.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_scan.cuh:    static cudaError_t InclusiveSum(
deps/cub/cub/device/device_scan.cuh:        cudaStream_t        stream             = 0,         ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/device_scan.cuh:     *   (e.g., addition of floating point types) on the same GPU device.
deps/cub/cub/device/device_scan.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_scan.cuh:    static cudaError_t InclusiveScan(
deps/cub/cub/device/device_scan.cuh:        cudaStream_t    stream             = 0,             ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/device_scan.cuh:     *   (e.g., addition of floating point types) on the same GPU device.
deps/cub/cub/device/device_scan.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_scan.cuh:    static cudaError_t ExclusiveSumByKey(
deps/cub/cub/device/device_scan.cuh:        cudaStream_t          stream=0,                     ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/device_scan.cuh:     *   (e.g., addition of floating point types) on the same GPU device.
deps/cub/cub/device/device_scan.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_scan.cuh:    static cudaError_t ExclusiveScanByKey(
deps/cub/cub/device/device_scan.cuh:        cudaStream_t          stream=0,                     ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/device_scan.cuh:     *   (e.g., addition of floating point types) on the same GPU device.
deps/cub/cub/device/device_scan.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_scan.cuh:    static cudaError_t InclusiveSumByKey(
deps/cub/cub/device/device_scan.cuh:        cudaStream_t          stream=0,                     ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/device_scan.cuh:     *   (e.g., addition of floating point types) on the same GPU device.
deps/cub/cub/device/device_scan.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_scan.cuh:    static cudaError_t InclusiveScanByKey(
deps/cub/cub/device/device_scan.cuh:        cudaStream_t          stream=0,                     ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/dispatch/dispatch_radix_sort.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/device/dispatch/dispatch_radix_sort.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/device/dispatch/dispatch_radix_sort.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/device/dispatch/dispatch_radix_sort.cuh:#include <thrust/system/cuda/detail/core/triple_chevron_launch.h>
deps/cub/cub/device/dispatch/dispatch_radix_sort.cuh:    cudaStream_t            stream;                 ///< [in] CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/dispatch/dispatch_radix_sort.cuh:        cudaStream_t            stream,
deps/cub/cub/device/dispatch/dispatch_radix_sort.cuh:    cudaError_t InvokeSingleTile(
deps/cub/cub/device/dispatch/dispatch_radix_sort.cuh:        return CubDebug(cudaErrorNotSupported );
deps/cub/cub/device/dispatch/dispatch_radix_sort.cuh:        cudaError error = cudaSuccess;
deps/cub/cub/device/dispatch/dispatch_radix_sort.cuh:            THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/cub/cub/device/dispatch/dispatch_radix_sort.cuh:            if (CubDebug(error = cudaPeekAtLastError())) break;
deps/cub/cub/device/dispatch/dispatch_radix_sort.cuh:    cudaError_t InvokePass(
deps/cub/cub/device/dispatch/dispatch_radix_sort.cuh:        cudaError error = cudaSuccess;
deps/cub/cub/device/dispatch/dispatch_radix_sort.cuh:            THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/cub/cub/device/dispatch/dispatch_radix_sort.cuh:            if (CubDebug(error = cudaPeekAtLastError())) break;
deps/cub/cub/device/dispatch/dispatch_radix_sort.cuh:            THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/cub/cub/device/dispatch/dispatch_radix_sort.cuh:            if (CubDebug(error = cudaPeekAtLastError())) break;
deps/cub/cub/device/dispatch/dispatch_radix_sort.cuh:            THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/cub/cub/device/dispatch/dispatch_radix_sort.cuh:            if (CubDebug(error = cudaPeekAtLastError())) break;
deps/cub/cub/device/dispatch/dispatch_radix_sort.cuh:        cudaError_t InitPassConfig(
deps/cub/cub/device/dispatch/dispatch_radix_sort.cuh:            cudaError error = cudaSuccess;
deps/cub/cub/device/dispatch/dispatch_radix_sort.cuh:    cudaError_t InvokeOnesweep()
deps/cub/cub/device/dispatch/dispatch_radix_sort.cuh:        cudaError_t error = cudaSuccess;
deps/cub/cub/device/dispatch/dispatch_radix_sort.cuh:            if (CubDebug(error = cudaMemsetAsync(
deps/cub/cub/device/dispatch/dispatch_radix_sort.cuh:            if (CubDebug(error = cudaMemsetAsync
deps/cub/cub/device/dispatch/dispatch_radix_sort.cuh:            if (CubDebug(error = cudaGetDevice(&device))) break;
deps/cub/cub/device/dispatch/dispatch_radix_sort.cuh:            if (CubDebug(error = cudaDeviceGetAttribute(
deps/cub/cub/device/dispatch/dispatch_radix_sort.cuh:                   &num_sms, cudaDevAttrMultiProcessorCount, device))) break;
deps/cub/cub/device/dispatch/dispatch_radix_sort.cuh:            if (CubDebug(error = cudaOccupancyMaxActiveBlocksPerMultiprocessor(
deps/cub/cub/device/dispatch/dispatch_radix_sort.cuh:            if (CubDebug(error = cudaPeekAtLastError())) break;
deps/cub/cub/device/dispatch/dispatch_radix_sort.cuh:            if (CubDebug(error = cudaPeekAtLastError())) break;
deps/cub/cub/device/dispatch/dispatch_radix_sort.cuh:                    if (CubDebug(error = cudaMemsetAsync(
deps/cub/cub/device/dispatch/dispatch_radix_sort.cuh:                    if (CubDebug(error = cudaPeekAtLastError())) break;
deps/cub/cub/device/dispatch/dispatch_radix_sort.cuh:    cudaError_t InvokePasses(
deps/cub/cub/device/dispatch/dispatch_radix_sort.cuh:        return CubDebug(cudaErrorNotSupported );
deps/cub/cub/device/dispatch/dispatch_radix_sort.cuh:        cudaError error = cudaSuccess;
deps/cub/cub/device/dispatch/dispatch_radix_sort.cuh:            if (CubDebug(error = cudaGetDevice(&device_ordinal))) break;
deps/cub/cub/device/dispatch/dispatch_radix_sort.cuh:            if (CubDebug(error = cudaDeviceGetAttribute (&sm_count, cudaDevAttrMultiProcessorCount, device_ordinal))) break;
deps/cub/cub/device/dispatch/dispatch_radix_sort.cuh:                return cudaSuccess;
deps/cub/cub/device/dispatch/dispatch_radix_sort.cuh:    cudaError_t InvokeManyTiles(Int2Type<false>)
deps/cub/cub/device/dispatch/dispatch_radix_sort.cuh:    cudaError_t InvokeManyTiles(Int2Type<true>)
deps/cub/cub/device/dispatch/dispatch_radix_sort.cuh:    cudaError_t Invoke()
deps/cub/cub/device/dispatch/dispatch_radix_sort.cuh:          return cudaSuccess;
deps/cub/cub/device/dispatch/dispatch_radix_sort.cuh:    static cudaError_t Dispatch(
deps/cub/cub/device/dispatch/dispatch_radix_sort.cuh:        cudaStream_t            stream,                 ///< [in] CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/dispatch/dispatch_radix_sort.cuh:        cudaError_t error;
deps/cub/cub/device/dispatch/dispatch_radix_sort.cuh:    cudaStream_t            stream;                 ///< [in] CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/dispatch/dispatch_radix_sort.cuh:        cudaStream_t            stream,
deps/cub/cub/device/dispatch/dispatch_radix_sort.cuh:    cudaError_t InvokePass(
deps/cub/cub/device/dispatch/dispatch_radix_sort.cuh:        cudaError error = cudaSuccess;
deps/cub/cub/device/dispatch/dispatch_radix_sort.cuh:            THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/cub/cub/device/dispatch/dispatch_radix_sort.cuh:            if (CubDebug(error = cudaPeekAtLastError())) break;
deps/cub/cub/device/dispatch/dispatch_radix_sort.cuh:        cudaError_t InitPassConfig(SegmentedKernelT segmented_kernel)
deps/cub/cub/device/dispatch/dispatch_radix_sort.cuh:    cudaError_t InvokePasses(
deps/cub/cub/device/dispatch/dispatch_radix_sort.cuh:        return CubDebug(cudaErrorNotSupported );
deps/cub/cub/device/dispatch/dispatch_radix_sort.cuh:        cudaError error = cudaSuccess;
deps/cub/cub/device/dispatch/dispatch_radix_sort.cuh:                return cudaSuccess;
deps/cub/cub/device/dispatch/dispatch_radix_sort.cuh:    cudaError_t Invoke()
deps/cub/cub/device/dispatch/dispatch_radix_sort.cuh:          return cudaSuccess;
deps/cub/cub/device/dispatch/dispatch_radix_sort.cuh:    static cudaError_t Dispatch(
deps/cub/cub/device/dispatch/dispatch_radix_sort.cuh:        cudaStream_t            stream,                 ///< [in] CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/dispatch/dispatch_radix_sort.cuh:        cudaError_t error;
deps/cub/cub/device/dispatch/dispatch_scan.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/device/dispatch/dispatch_scan.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/device/dispatch/dispatch_scan.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/device/dispatch/dispatch_scan.cuh:#include <thrust/system/cuda/detail/core/triple_chevron_launch.h>
deps/cub/cub/device/dispatch/dispatch_scan.cuh:    cudaStream_t    stream;                 ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/dispatch/dispatch_scan.cuh:        cudaStream_t    stream,                 ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/dispatch/dispatch_scan.cuh:    cudaError_t Invoke(InitKernel init_kernel, ScanKernel scan_kernel)
deps/cub/cub/device/dispatch/dispatch_scan.cuh:        return CubDebug(cudaErrorNotSupported);
deps/cub/cub/device/dispatch/dispatch_scan.cuh:        cudaError error = cudaSuccess;
deps/cub/cub/device/dispatch/dispatch_scan.cuh:            if (CubDebug(error = cudaGetDevice(&device_ordinal))) break;
deps/cub/cub/device/dispatch/dispatch_scan.cuh:            THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/cub/cub/device/dispatch/dispatch_scan.cuh:            if (CubDebug(error = cudaPeekAtLastError())) break;
deps/cub/cub/device/dispatch/dispatch_scan.cuh:            if (CubDebug(error = cudaDeviceGetAttribute(&max_dim_x, cudaDevAttrMaxGridDimX, device_ordinal))) break;;
deps/cub/cub/device/dispatch/dispatch_scan.cuh:                THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/cub/cub/device/dispatch/dispatch_scan.cuh:                if (CubDebug(error = cudaPeekAtLastError())) break;
deps/cub/cub/device/dispatch/dispatch_scan.cuh:    cudaError_t Invoke()
deps/cub/cub/device/dispatch/dispatch_scan.cuh:    static cudaError_t Dispatch(
deps/cub/cub/device/dispatch/dispatch_scan.cuh:        cudaStream_t    stream,                 ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/dispatch/dispatch_scan.cuh:        cudaError_t error;
deps/cub/cub/device/dispatch/dispatch_merge_sort.cuh: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/device/dispatch/dispatch_merge_sort.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/device/dispatch/dispatch_merge_sort.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/device/dispatch/dispatch_merge_sort.cuh:#include <thrust/system/cuda/detail/core/triple_chevron_launch.h>
deps/cub/cub/device/dispatch/dispatch_merge_sort.cuh:                        THRUST_NS_QUALIFIER::cuda_cub::core::make_load_iterator(ActivePolicyT(), keys_in),
deps/cub/cub/device/dispatch/dispatch_merge_sort.cuh:                        THRUST_NS_QUALIFIER::cuda_cub::core::make_load_iterator(ActivePolicyT(), items_in),
deps/cub/cub/device/dispatch/dispatch_merge_sort.cuh:    THRUST_NS_QUALIFIER::cuda_cub::core::make_load_iterator(ActivePolicyT(), keys_ping),
deps/cub/cub/device/dispatch/dispatch_merge_sort.cuh:    THRUST_NS_QUALIFIER::cuda_cub::core::make_load_iterator(ActivePolicyT(), items_ping),
deps/cub/cub/device/dispatch/dispatch_merge_sort.cuh:    THRUST_NS_QUALIFIER::cuda_cub::core::make_load_iterator(ActivePolicyT(), keys_pong),
deps/cub/cub/device/dispatch/dispatch_merge_sort.cuh:    THRUST_NS_QUALIFIER::cuda_cub::core::make_load_iterator(ActivePolicyT(), items_pong),
deps/cub/cub/device/dispatch/dispatch_merge_sort.cuh:#if defined(_NVHPC_CUDA)
deps/cub/cub/device/dispatch/dispatch_merge_sort.cuh:  cudaStream_t stream;
deps/cub/cub/device/dispatch/dispatch_merge_sort.cuh:                    cudaStream_t stream,
deps/cub/cub/device/dispatch/dispatch_merge_sort.cuh:    THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/cub/cub/device/dispatch/dispatch_merge_sort.cuh:  cudaStream_t stream;
deps/cub/cub/device/dispatch/dispatch_merge_sort.cuh:                                                     cudaStream_t stream,
deps/cub/cub/device/dispatch/dispatch_merge_sort.cuh:    THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/cub/cub/device/dispatch/dispatch_merge_sort.cuh:  /// CUDA stream to launch kernels within. Default is stream<sub>0</sub>.
deps/cub/cub/device/dispatch/dispatch_merge_sort.cuh:  cudaStream_t stream;
deps/cub/cub/device/dispatch/dispatch_merge_sort.cuh:                    cudaStream_t stream,
deps/cub/cub/device/dispatch/dispatch_merge_sort.cuh:  CUB_RUNTIME_FUNCTION __forceinline__ cudaError_t Invoke()
deps/cub/cub/device/dispatch/dispatch_merge_sort.cuh:    cudaError error = cudaSuccess;
deps/cub/cub/device/dispatch/dispatch_merge_sort.cuh:      if (CubDebug(error = cudaGetDevice(&device_ordinal)))
deps/cub/cub/device/dispatch/dispatch_merge_sort.cuh:              error = cudaDeviceGetAttribute(&max_shmem,
deps/cub/cub/device/dispatch/dispatch_merge_sort.cuh:                                             cudaDevAttrMaxSharedMemoryPerBlock,
deps/cub/cub/device/dispatch/dispatch_merge_sort.cuh:      if (CubDebug(error = cudaPeekAtLastError()))
deps/cub/cub/device/dispatch/dispatch_merge_sort.cuh:        THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/cub/cub/device/dispatch/dispatch_merge_sort.cuh:        if (CubDebug(error = cudaPeekAtLastError()))
deps/cub/cub/device/dispatch/dispatch_merge_sort.cuh:        if (CubDebug(error = cudaPeekAtLastError()))
deps/cub/cub/device/dispatch/dispatch_merge_sort.cuh:  CUB_RUNTIME_FUNCTION __forceinline__ static cudaError_t
deps/cub/cub/device/dispatch/dispatch_merge_sort.cuh:           cudaStream_t stream,
deps/cub/cub/device/dispatch/dispatch_merge_sort.cuh:    cudaError error = cudaSuccess;
deps/cub/cub/device/dispatch/dispatch_rle.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/device/dispatch/dispatch_rle.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/device/dispatch/dispatch_rle.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/device/dispatch/dispatch_rle.cuh:#include <thrust/system/cuda/detail/core/triple_chevron_launch.h>
deps/cub/cub/device/dispatch/dispatch_rle.cuh:    static cudaError_t Dispatch(
deps/cub/cub/device/dispatch/dispatch_rle.cuh:        cudaStream_t                stream,                         ///< [in] CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/dispatch/dispatch_rle.cuh:        return CubDebug(cudaErrorNotSupported);
deps/cub/cub/device/dispatch/dispatch_rle.cuh:        cudaError error = cudaSuccess;
deps/cub/cub/device/dispatch/dispatch_rle.cuh:            if (CubDebug(error = cudaGetDevice(&device_ordinal))) break;
deps/cub/cub/device/dispatch/dispatch_rle.cuh:            THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/cub/cub/device/dispatch/dispatch_rle.cuh:            if (CubDebug(error = cudaPeekAtLastError())) break;
deps/cub/cub/device/dispatch/dispatch_rle.cuh:            if (CubDebug(error = cudaDeviceGetAttribute(&max_dim_x, cudaDevAttrMaxGridDimX, device_ordinal))) break;;
deps/cub/cub/device/dispatch/dispatch_rle.cuh:            THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/cub/cub/device/dispatch/dispatch_rle.cuh:            if (CubDebug(error = cudaPeekAtLastError())) break;
deps/cub/cub/device/dispatch/dispatch_rle.cuh:    static cudaError_t Dispatch(
deps/cub/cub/device/dispatch/dispatch_rle.cuh:        cudaStream_t                stream,                         ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/dispatch/dispatch_rle.cuh:        cudaError error = cudaSuccess;
deps/cub/cub/device/dispatch/dispatch_histogram.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/device/dispatch/dispatch_histogram.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/device/dispatch/dispatch_histogram.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/device/dispatch/dispatch_histogram.cuh:#include <thrust/system/cuda/detail/core/triple_chevron_launch.h>
deps/cub/cub/device/dispatch/dispatch_histogram.cuh:    static cudaError_t InitConfigs(
deps/cub/cub/device/dispatch/dispatch_histogram.cuh:        cudaError_t result = cudaErrorNotSupported;
deps/cub/cub/device/dispatch/dispatch_histogram.cuh:        cudaError_t Init()
deps/cub/cub/device/dispatch/dispatch_histogram.cuh:            return cudaSuccess;
deps/cub/cub/device/dispatch/dispatch_histogram.cuh:    static cudaError_t PrivatizedDispatch(
deps/cub/cub/device/dispatch/dispatch_histogram.cuh:        cudaStream_t                        stream,                                         ///< [in] CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/dispatch/dispatch_histogram.cuh:        return CubDebug(cudaErrorNotSupported);
deps/cub/cub/device/dispatch/dispatch_histogram.cuh:        cudaError error = cudaSuccess;
deps/cub/cub/device/dispatch/dispatch_histogram.cuh:            if (CubDebug(error = cudaGetDevice(&device_ordinal))) break;
deps/cub/cub/device/dispatch/dispatch_histogram.cuh:            if (CubDebug(error = cudaDeviceGetAttribute (&sm_count, cudaDevAttrMultiProcessorCount, device_ordinal))) break;
deps/cub/cub/device/dispatch/dispatch_histogram.cuh:            THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/cub/cub/device/dispatch/dispatch_histogram.cuh:            THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/cub/cub/device/dispatch/dispatch_histogram.cuh:            if (CubDebug(error = cudaPeekAtLastError())) break;
deps/cub/cub/device/dispatch/dispatch_histogram.cuh:    static cudaError_t DispatchRange(
deps/cub/cub/device/dispatch/dispatch_histogram.cuh:        cudaStream_t        stream,                                     ///< [in] CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/dispatch/dispatch_histogram.cuh:        cudaError error = cudaSuccess;
deps/cub/cub/device/dispatch/dispatch_histogram.cuh:    static cudaError_t DispatchRange(
deps/cub/cub/device/dispatch/dispatch_histogram.cuh:        cudaStream_t        stream,                                     ///< [in] CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/dispatch/dispatch_histogram.cuh:        cudaError error = cudaSuccess;
deps/cub/cub/device/dispatch/dispatch_histogram.cuh:    static cudaError_t DispatchEven(
deps/cub/cub/device/dispatch/dispatch_histogram.cuh:        cudaStream_t        stream,                                     ///< [in] CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/dispatch/dispatch_histogram.cuh:        cudaError error = cudaSuccess;
deps/cub/cub/device/dispatch/dispatch_histogram.cuh:    static cudaError_t DispatchEven(
deps/cub/cub/device/dispatch/dispatch_histogram.cuh:        cudaStream_t        stream,                                     ///< [in] CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/dispatch/dispatch_histogram.cuh:        cudaError error = cudaSuccess;
deps/cub/cub/device/dispatch/dispatch_adjacent_difference.cuh: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/device/dispatch/dispatch_adjacent_difference.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/device/dispatch/dispatch_adjacent_difference.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/device/dispatch/dispatch_adjacent_difference.cuh:#include <thrust/system/cuda/detail/core/triple_chevron_launch.h>
deps/cub/cub/device/dispatch/dispatch_adjacent_difference.cuh:  cudaStream_t stream;
deps/cub/cub/device/dispatch/dispatch_adjacent_difference.cuh:                             cudaStream_t stream,
deps/cub/cub/device/dispatch/dispatch_adjacent_difference.cuh:  CUB_RUNTIME_FUNCTION __forceinline__ cudaError_t Invoke()
deps/cub/cub/device/dispatch/dispatch_adjacent_difference.cuh:    cudaError error = cudaSuccess;
deps/cub/cub/device/dispatch/dispatch_adjacent_difference.cuh:        THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(init_grid_size,
deps/cub/cub/device/dispatch/dispatch_adjacent_difference.cuh:        if (CubDebug(error = cudaPeekAtLastError()))
deps/cub/cub/device/dispatch/dispatch_adjacent_difference.cuh:      THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/cub/cub/device/dispatch/dispatch_adjacent_difference.cuh:      if (CubDebug(error = cudaPeekAtLastError()))
deps/cub/cub/device/dispatch/dispatch_adjacent_difference.cuh:  static cudaError_t Dispatch(void *d_temp_storage,
deps/cub/cub/device/dispatch/dispatch_adjacent_difference.cuh:                              cudaStream_t stream,
deps/cub/cub/device/dispatch/dispatch_adjacent_difference.cuh:    cudaError error = cudaSuccess;
deps/cub/cub/device/dispatch/dispatch_segmented_sort.cuh: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/device/dispatch/dispatch_segmented_sort.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/device/dispatch/dispatch_segmented_sort.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/device/dispatch/dispatch_segmented_sort.cuh:#include <thrust/system/cuda/detail/core/triple_chevron_launch.h>
deps/cub/cub/device/dispatch/dispatch_segmented_sort.cuh:  /// CUDA stream to launch kernels within.
deps/cub/cub/device/dispatch/dispatch_segmented_sort.cuh:  cudaStream_t stream;
deps/cub/cub/device/dispatch/dispatch_segmented_sort.cuh:                        cudaStream_t stream,
deps/cub/cub/device/dispatch/dispatch_segmented_sort.cuh:  CUB_RUNTIME_FUNCTION __forceinline__ cudaError_t Invoke()
deps/cub/cub/device/dispatch/dispatch_segmented_sort.cuh:    cudaError error = cudaSuccess;
deps/cub/cub/device/dispatch/dispatch_segmented_sort.cuh:  CUB_RUNTIME_FUNCTION __forceinline__ static cudaError_t
deps/cub/cub/device/dispatch/dispatch_segmented_sort.cuh:           cudaStream_t stream,
deps/cub/cub/device/dispatch/dispatch_segmented_sort.cuh:    cudaError error = cudaSuccess;
deps/cub/cub/device/dispatch/dispatch_segmented_sort.cuh:  CUB_RUNTIME_FUNCTION __forceinline__ cudaError_t
deps/cub/cub/device/dispatch/dispatch_segmented_sort.cuh:    cudaError_t error = cudaSuccess;
deps/cub/cub/device/dispatch/dispatch_segmented_sort.cuh:      if (CubDebug(error = cudaMemcpyAsync(
deps/cub/cub/device/dispatch/dispatch_segmented_sort.cuh:                     cudaMemcpyDeviceToHost,
deps/cub/cub/device/dispatch/dispatch_segmented_sort.cuh:      THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/cub/cub/device/dispatch/dispatch_segmented_sort.cuh:      if (CubDebug(error = cudaPeekAtLastError()))
deps/cub/cub/device/dispatch/dispatch_segmented_sort.cuh:      THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/cub/cub/device/dispatch/dispatch_segmented_sort.cuh:      if (CubDebug(error = cudaPeekAtLastError()))
deps/cub/cub/device/dispatch/dispatch_segmented_sort.cuh:  CUB_RUNTIME_FUNCTION __forceinline__ cudaError_t SortWithoutPartitioning(
deps/cub/cub/device/dispatch/dispatch_segmented_sort.cuh:    cudaError_t error = cudaSuccess;
deps/cub/cub/device/dispatch/dispatch_segmented_sort.cuh:    THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(blocks_in_grid,
deps/cub/cub/device/dispatch/dispatch_segmented_sort.cuh:    if (CubDebug(error = cudaPeekAtLastError()))
deps/cub/cub/device/dispatch/dispatch_scan_by_key.cuh: * Copyright (c) 2021, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/device/dispatch/dispatch_scan_by_key.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/device/dispatch/dispatch_scan_by_key.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/device/dispatch/dispatch_scan_by_key.cuh:#include <thrust/system/cuda/detail/core/triple_chevron_launch.h>
deps/cub/cub/device/dispatch/dispatch_scan_by_key.cuh:    cudaStream_t          stream;                 ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/dispatch/dispatch_scan_by_key.cuh:        cudaStream_t          stream,                 ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/dispatch/dispatch_scan_by_key.cuh:    cudaError_t Invoke(InitKernel init_kernel, ScanKernel scan_kernel)
deps/cub/cub/device/dispatch/dispatch_scan_by_key.cuh:        return CubDebug(cudaErrorNotSupported);
deps/cub/cub/device/dispatch/dispatch_scan_by_key.cuh:        cudaError error = cudaSuccess;
deps/cub/cub/device/dispatch/dispatch_scan_by_key.cuh:            if (CubDebug(error = cudaGetDevice(&device_ordinal))) break;
deps/cub/cub/device/dispatch/dispatch_scan_by_key.cuh:            THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/cub/cub/device/dispatch/dispatch_scan_by_key.cuh:            if (CubDebug(error = cudaPeekAtLastError())) break;
deps/cub/cub/device/dispatch/dispatch_scan_by_key.cuh:            if (CubDebug(error = cudaDeviceGetAttribute(&max_dim_x, cudaDevAttrMaxGridDimX, device_ordinal))) break;
deps/cub/cub/device/dispatch/dispatch_scan_by_key.cuh:                THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/cub/cub/device/dispatch/dispatch_scan_by_key.cuh:                if (CubDebug(error = cudaPeekAtLastError())) break;
deps/cub/cub/device/dispatch/dispatch_scan_by_key.cuh:    cudaError_t Invoke()
deps/cub/cub/device/dispatch/dispatch_scan_by_key.cuh:    static cudaError_t Dispatch(
deps/cub/cub/device/dispatch/dispatch_scan_by_key.cuh:        cudaStream_t          stream,                 ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/dispatch/dispatch_scan_by_key.cuh:        cudaError_t error;
deps/cub/cub/device/dispatch/dispatch_reduce_by_key.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/device/dispatch/dispatch_reduce_by_key.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/device/dispatch/dispatch_reduce_by_key.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/device/dispatch/dispatch_reduce_by_key.cuh:#include <thrust/system/cuda/detail/core/triple_chevron_launch.h>
deps/cub/cub/device/dispatch/dispatch_reduce_by_key.cuh:    static cudaError_t Dispatch(
deps/cub/cub/device/dispatch/dispatch_reduce_by_key.cuh:        cudaStream_t                stream,                     ///< [in] CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/dispatch/dispatch_reduce_by_key.cuh:        return CubDebug(cudaErrorNotSupported);
deps/cub/cub/device/dispatch/dispatch_reduce_by_key.cuh:        cudaError error = cudaSuccess;
deps/cub/cub/device/dispatch/dispatch_reduce_by_key.cuh:            if (CubDebug(error = cudaGetDevice(&device_ordinal))) break;
deps/cub/cub/device/dispatch/dispatch_reduce_by_key.cuh:            THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/cub/cub/device/dispatch/dispatch_reduce_by_key.cuh:            if (CubDebug(error = cudaPeekAtLastError())) break;
deps/cub/cub/device/dispatch/dispatch_reduce_by_key.cuh:            if (CubDebug(error = cudaDeviceGetAttribute(&max_dim_x, cudaDevAttrMaxGridDimX, device_ordinal))) break;;
deps/cub/cub/device/dispatch/dispatch_reduce_by_key.cuh:                THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/cub/cub/device/dispatch/dispatch_reduce_by_key.cuh:                if (CubDebug(error = cudaPeekAtLastError())) break;
deps/cub/cub/device/dispatch/dispatch_reduce_by_key.cuh:    static cudaError_t Dispatch(
deps/cub/cub/device/dispatch/dispatch_reduce_by_key.cuh:        cudaStream_t                stream,                         ///< [in] CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/dispatch/dispatch_reduce_by_key.cuh:        cudaError error = cudaSuccess;
deps/cub/cub/device/dispatch/dispatch_select_if.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/device/dispatch/dispatch_select_if.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/device/dispatch/dispatch_select_if.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/device/dispatch/dispatch_select_if.cuh:#include <thrust/system/cuda/detail/core/triple_chevron_launch.h>
deps/cub/cub/device/dispatch/dispatch_select_if.cuh:    static cudaError_t Dispatch(
deps/cub/cub/device/dispatch/dispatch_select_if.cuh:        cudaStream_t                stream,                         ///< [in] CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/dispatch/dispatch_select_if.cuh:        return CubDebug(cudaErrorNotSupported);
deps/cub/cub/device/dispatch/dispatch_select_if.cuh:        cudaError error = cudaSuccess;
deps/cub/cub/device/dispatch/dispatch_select_if.cuh:            if (CubDebug(error = cudaGetDevice(&device_ordinal))) break;
deps/cub/cub/device/dispatch/dispatch_select_if.cuh:            THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/cub/cub/device/dispatch/dispatch_select_if.cuh:            if (CubDebug(error = cudaPeekAtLastError())) break;
deps/cub/cub/device/dispatch/dispatch_select_if.cuh:            if (CubDebug(error = cudaDeviceGetAttribute(&max_dim_x, cudaDevAttrMaxGridDimX, device_ordinal))) break;;
deps/cub/cub/device/dispatch/dispatch_select_if.cuh:            THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/cub/cub/device/dispatch/dispatch_select_if.cuh:            if (CubDebug(error = cudaPeekAtLastError())) break;
deps/cub/cub/device/dispatch/dispatch_select_if.cuh:    static cudaError_t Dispatch(
deps/cub/cub/device/dispatch/dispatch_select_if.cuh:        cudaStream_t                stream,                         ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/dispatch/dispatch_select_if.cuh:        cudaError error = cudaSuccess;
deps/cub/cub/device/dispatch/dispatch_reduce.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/device/dispatch/dispatch_reduce.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/device/dispatch/dispatch_reduce.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/device/dispatch/dispatch_reduce.cuh:#include <thrust/system/cuda/detail/core/triple_chevron_launch.h>
deps/cub/cub/device/dispatch/dispatch_reduce.cuh:    cudaStream_t        stream;                         ///< [in] CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/dispatch/dispatch_reduce.cuh:        cudaStream_t            stream,
deps/cub/cub/device/dispatch/dispatch_reduce.cuh:    cudaError_t InvokeSingleTile(
deps/cub/cub/device/dispatch/dispatch_reduce.cuh:        return CubDebug(cudaErrorNotSupported );
deps/cub/cub/device/dispatch/dispatch_reduce.cuh:        cudaError error = cudaSuccess;
deps/cub/cub/device/dispatch/dispatch_reduce.cuh:            THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/cub/cub/device/dispatch/dispatch_reduce.cuh:            if (CubDebug(error = cudaPeekAtLastError())) break;
deps/cub/cub/device/dispatch/dispatch_reduce.cuh:    cudaError_t InvokePasses(
deps/cub/cub/device/dispatch/dispatch_reduce.cuh:        return CubDebug(cudaErrorNotSupported );
deps/cub/cub/device/dispatch/dispatch_reduce.cuh:        cudaError error = cudaSuccess;
deps/cub/cub/device/dispatch/dispatch_reduce.cuh:            if (CubDebug(error = cudaGetDevice(&device_ordinal))) break;
deps/cub/cub/device/dispatch/dispatch_reduce.cuh:            if (CubDebug(error = cudaDeviceGetAttribute (&sm_count, cudaDevAttrMultiProcessorCount, device_ordinal))) break;
deps/cub/cub/device/dispatch/dispatch_reduce.cuh:                return cudaSuccess;
deps/cub/cub/device/dispatch/dispatch_reduce.cuh:            THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/cub/cub/device/dispatch/dispatch_reduce.cuh:            if (CubDebug(error = cudaPeekAtLastError())) break;
deps/cub/cub/device/dispatch/dispatch_reduce.cuh:            THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/cub/cub/device/dispatch/dispatch_reduce.cuh:            if (CubDebug(error = cudaPeekAtLastError())) break;
deps/cub/cub/device/dispatch/dispatch_reduce.cuh:    cudaError_t Invoke()
deps/cub/cub/device/dispatch/dispatch_reduce.cuh:    static cudaError_t Dispatch(
deps/cub/cub/device/dispatch/dispatch_reduce.cuh:        cudaStream_t    stream,                             ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/dispatch/dispatch_reduce.cuh:        cudaError error = cudaSuccess;
deps/cub/cub/device/dispatch/dispatch_reduce.cuh:    cudaStream_t         stream;                 ///< [in] CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/dispatch/dispatch_reduce.cuh:        cudaStream_t            stream,
deps/cub/cub/device/dispatch/dispatch_reduce.cuh:    cudaError_t InvokePasses(
deps/cub/cub/device/dispatch/dispatch_reduce.cuh:        return CubDebug(cudaErrorNotSupported );
deps/cub/cub/device/dispatch/dispatch_reduce.cuh:        cudaError error = cudaSuccess;
deps/cub/cub/device/dispatch/dispatch_reduce.cuh:                return cudaSuccess;
deps/cub/cub/device/dispatch/dispatch_reduce.cuh:            THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/cub/cub/device/dispatch/dispatch_reduce.cuh:            if (CubDebug(error = cudaPeekAtLastError())) break;
deps/cub/cub/device/dispatch/dispatch_reduce.cuh:    cudaError_t Invoke()
deps/cub/cub/device/dispatch/dispatch_reduce.cuh:    static cudaError_t Dispatch(
deps/cub/cub/device/dispatch/dispatch_reduce.cuh:        cudaStream_t         stream,                             ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/dispatch/dispatch_reduce.cuh:            return cudaSuccess;
deps/cub/cub/device/dispatch/dispatch_reduce.cuh:        cudaError error = cudaSuccess;
deps/cub/cub/device/dispatch/dispatch_spmv_orig.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/device/dispatch/dispatch_spmv_orig.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/device/dispatch/dispatch_spmv_orig.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/device/dispatch/dispatch_spmv_orig.cuh:#include <thrust/system/cuda/detail/core/triple_chevron_launch.h>
deps/cub/cub/device/dispatch/dispatch_spmv_orig.cuh:    static cudaError_t Dispatch(
deps/cub/cub/device/dispatch/dispatch_spmv_orig.cuh:        cudaStream_t            stream,                             ///< [in] CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/dispatch/dispatch_spmv_orig.cuh:        return CubDebug(cudaErrorNotSupported );
deps/cub/cub/device/dispatch/dispatch_spmv_orig.cuh:        cudaError error = cudaSuccess;
deps/cub/cub/device/dispatch/dispatch_spmv_orig.cuh:              return cudaErrorInvalidValue;
deps/cub/cub/device/dispatch/dispatch_spmv_orig.cuh:                THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/cub/cub/device/dispatch/dispatch_spmv_orig.cuh:                if (CubDebug(error = cudaPeekAtLastError())) break;
deps/cub/cub/device/dispatch/dispatch_spmv_orig.cuh:            if (CubDebug(error = cudaGetDevice(&device_ordinal))) break;
deps/cub/cub/device/dispatch/dispatch_spmv_orig.cuh:            if (CubDebug(error = cudaDeviceGetAttribute (&sm_count, cudaDevAttrMultiProcessorCount, device_ordinal))) break;
deps/cub/cub/device/dispatch/dispatch_spmv_orig.cuh:            if (CubDebug(error = cudaDeviceGetAttribute(&max_dim_x, cudaDevAttrMaxGridDimX, device_ordinal))) break;
deps/cub/cub/device/dispatch/dispatch_spmv_orig.cuh:                THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/cub/cub/device/dispatch/dispatch_spmv_orig.cuh:                if (CubDebug(error = cudaPeekAtLastError())) break;
deps/cub/cub/device/dispatch/dispatch_spmv_orig.cuh:            THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/cub/cub/device/dispatch/dispatch_spmv_orig.cuh:            if (CubDebug(error = cudaPeekAtLastError())) break;
deps/cub/cub/device/dispatch/dispatch_spmv_orig.cuh:                THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/cub/cub/device/dispatch/dispatch_spmv_orig.cuh:                if (CubDebug(error = cudaPeekAtLastError())) break;
deps/cub/cub/device/dispatch/dispatch_spmv_orig.cuh:    static cudaError_t Dispatch(
deps/cub/cub/device/dispatch/dispatch_spmv_orig.cuh:        cudaStream_t            stream                  = 0,        ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/dispatch/dispatch_spmv_orig.cuh:        cudaError error = cudaSuccess;
deps/cub/cub/device/dispatch/dispatch_three_way_partition.cuh: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/device/dispatch/dispatch_three_way_partition.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/device/dispatch/dispatch_three_way_partition.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/device/dispatch/dispatch_three_way_partition.cuh:#include <thrust/system/cuda/detail/core/triple_chevron_launch.h>
deps/cub/cub/device/dispatch/dispatch_three_way_partition.cuh:  CUB_RUNTIME_FUNCTION __forceinline__ static cudaError_t
deps/cub/cub/device/dispatch/dispatch_three_way_partition.cuh:           cudaStream_t stream,
deps/cub/cub/device/dispatch/dispatch_three_way_partition.cuh:    cudaError error = cudaSuccess;
deps/cub/cub/device/dispatch/dispatch_three_way_partition.cuh:      if (CubDebug(error = cudaGetDevice(&device_ordinal)))
deps/cub/cub/device/dispatch/dispatch_three_way_partition.cuh:      THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/cub/cub/device/dispatch/dispatch_three_way_partition.cuh:      if (CubDebug(error = cudaPeekAtLastError()))
deps/cub/cub/device/dispatch/dispatch_three_way_partition.cuh:      if (CubDebug(error = cudaDeviceGetAttribute(&max_dim_x,
deps/cub/cub/device/dispatch/dispatch_three_way_partition.cuh:                                                  cudaDevAttrMaxGridDimX,
deps/cub/cub/device/dispatch/dispatch_three_way_partition.cuh:      THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/cub/cub/device/dispatch/dispatch_three_way_partition.cuh:      if (CubDebug(error = cudaPeekAtLastError()))
deps/cub/cub/device/dispatch/dispatch_three_way_partition.cuh:  static cudaError_t Dispatch(
deps/cub/cub/device/dispatch/dispatch_three_way_partition.cuh:    cudaStream_t                stream,
deps/cub/cub/device/dispatch/dispatch_three_way_partition.cuh:    cudaError error = cudaSuccess;
deps/cub/cub/device/device_spmv.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/device/device_spmv.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/device/device_spmv.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/device/device_spmv.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_spmv.cuh:    static cudaError_t CsrMV(
deps/cub/cub/device/device_spmv.cuh:        cudaStream_t        stream                  = 0,        ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/device_segmented_radix_sort.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/device/device_segmented_radix_sort.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/device/device_segmented_radix_sort.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/device/device_segmented_radix_sort.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_segmented_radix_sort.cuh:    static cudaError_t SortPairs(
deps/cub/cub/device/device_segmented_radix_sort.cuh:        cudaStream_t        stream              = 0,                ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/device_segmented_radix_sort.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_segmented_radix_sort.cuh:    static cudaError_t SortPairs(
deps/cub/cub/device/device_segmented_radix_sort.cuh:        cudaStream_t            stream              = 0,                ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/device_segmented_radix_sort.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_segmented_radix_sort.cuh:    static cudaError_t SortPairsDescending(
deps/cub/cub/device/device_segmented_radix_sort.cuh:        cudaStream_t        stream              = 0,                ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/device_segmented_radix_sort.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_segmented_radix_sort.cuh:    static cudaError_t SortPairsDescending(
deps/cub/cub/device/device_segmented_radix_sort.cuh:        cudaStream_t            stream              = 0,                ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/device_segmented_radix_sort.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_segmented_radix_sort.cuh:    static cudaError_t SortKeys(
deps/cub/cub/device/device_segmented_radix_sort.cuh:        cudaStream_t        stream              = 0,                ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/device_segmented_radix_sort.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_segmented_radix_sort.cuh:    static cudaError_t SortKeys(
deps/cub/cub/device/device_segmented_radix_sort.cuh:        cudaStream_t        stream              = 0,                ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/device_segmented_radix_sort.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_segmented_radix_sort.cuh:    static cudaError_t SortKeysDescending(
deps/cub/cub/device/device_segmented_radix_sort.cuh:        cudaStream_t        stream              = 0,                ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/device_segmented_radix_sort.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_segmented_radix_sort.cuh:    static cudaError_t SortKeysDescending(
deps/cub/cub/device/device_segmented_radix_sort.cuh:        cudaStream_t        stream              = 0,                ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/device_run_length_encode.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/device/device_run_length_encode.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/device/device_run_length_encode.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/device/device_run_length_encode.cuh: * different CUDA architectures for \p int32 items.
deps/cub/cub/device/device_run_length_encode.cuh:     * CUDA architectures for \p int32 and \p int64 items, respectively.  Segments have
deps/cub/cub/device/device_run_length_encode.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_run_length_encode.cuh:    static cudaError_t Encode(
deps/cub/cub/device/device_run_length_encode.cuh:        cudaStream_t                stream             = 0,         ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/device_run_length_encode.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_run_length_encode.cuh:    static cudaError_t NonTrivialRuns(
deps/cub/cub/device/device_run_length_encode.cuh:        cudaStream_t            stream             = 0,         ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/device_segmented_sort.cuh: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/device/device_segmented_sort.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/device/device_segmented_sort.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/device/device_segmented_sort.cuh: * (`unsigned char`, `int`, `double`, etc.) as well as CUDA's `__half` and
deps/cub/cub/device/device_segmented_sort.cuh: * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_segmented_sort.cuh:   * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_segmented_sort.cuh:   *   <b>[optional]</b> CUDA stream to launch kernels within. Default is
deps/cub/cub/device/device_segmented_sort.cuh:  CUB_RUNTIME_FUNCTION static cudaError_t
deps/cub/cub/device/device_segmented_sort.cuh:           cudaStream_t stream    = 0,
deps/cub/cub/device/device_segmented_sort.cuh:   * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_segmented_sort.cuh:   *   <b>[optional]</b> CUDA stream to launch kernels within. Default is
deps/cub/cub/device/device_segmented_sort.cuh:  CUB_RUNTIME_FUNCTION static cudaError_t
deps/cub/cub/device/device_segmented_sort.cuh:                     cudaStream_t stream    = 0,
deps/cub/cub/device/device_segmented_sort.cuh:   * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_segmented_sort.cuh:   *   <b>[optional]</b> CUDA stream to launch kernels within. Default is
deps/cub/cub/device/device_segmented_sort.cuh:  CUB_RUNTIME_FUNCTION static cudaError_t
deps/cub/cub/device/device_segmented_sort.cuh:           cudaStream_t stream    = 0,
deps/cub/cub/device/device_segmented_sort.cuh:   * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_segmented_sort.cuh:   *   <b>[optional]</b> CUDA stream to launch kernels within. Default is
deps/cub/cub/device/device_segmented_sort.cuh:  CUB_RUNTIME_FUNCTION static cudaError_t
deps/cub/cub/device/device_segmented_sort.cuh:                     cudaStream_t stream    = 0,
deps/cub/cub/device/device_segmented_sort.cuh:   * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_segmented_sort.cuh:   *   <b>[optional]</b> CUDA stream to launch kernels within. Default is
deps/cub/cub/device/device_segmented_sort.cuh:  CUB_RUNTIME_FUNCTION static cudaError_t
deps/cub/cub/device/device_segmented_sort.cuh:                 cudaStream_t stream    = 0,
deps/cub/cub/device/device_segmented_sort.cuh:   * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_segmented_sort.cuh:   *   <b>[optional]</b> CUDA stream to launch kernels within. Default is
deps/cub/cub/device/device_segmented_sort.cuh:  CUB_RUNTIME_FUNCTION static cudaError_t
deps/cub/cub/device/device_segmented_sort.cuh:                           cudaStream_t stream    = 0,
deps/cub/cub/device/device_segmented_sort.cuh:   * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_segmented_sort.cuh:   *   <b>[optional]</b> CUDA stream to launch kernels within. Default is
deps/cub/cub/device/device_segmented_sort.cuh:  CUB_RUNTIME_FUNCTION static cudaError_t
deps/cub/cub/device/device_segmented_sort.cuh:                 cudaStream_t stream    = 0,
deps/cub/cub/device/device_segmented_sort.cuh:   * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_segmented_sort.cuh:   *   <b>[optional]</b> CUDA stream to launch kernels within. Default is
deps/cub/cub/device/device_segmented_sort.cuh:  CUB_RUNTIME_FUNCTION static cudaError_t
deps/cub/cub/device/device_segmented_sort.cuh:                           cudaStream_t stream    = 0,
deps/cub/cub/device/device_segmented_sort.cuh:   * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_segmented_sort.cuh:   *   <b>[optional]</b> CUDA stream to launch kernels within. Default is
deps/cub/cub/device/device_segmented_sort.cuh:  CUB_RUNTIME_FUNCTION static cudaError_t
deps/cub/cub/device/device_segmented_sort.cuh:           cudaStream_t stream = 0,
deps/cub/cub/device/device_segmented_sort.cuh:   * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_segmented_sort.cuh:   *   <b>[optional]</b> CUDA stream to launch kernels within. Default is
deps/cub/cub/device/device_segmented_sort.cuh:  CUB_RUNTIME_FUNCTION static cudaError_t
deps/cub/cub/device/device_segmented_sort.cuh:                      cudaStream_t stream    = 0,
deps/cub/cub/device/device_segmented_sort.cuh:   * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_segmented_sort.cuh:   *   <b>[optional]</b> CUDA stream to launch kernels within. Default is
deps/cub/cub/device/device_segmented_sort.cuh:  CUB_RUNTIME_FUNCTION static cudaError_t
deps/cub/cub/device/device_segmented_sort.cuh:            cudaStream_t stream    = 0,
deps/cub/cub/device/device_segmented_sort.cuh:   * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_segmented_sort.cuh:   *   <b>[optional]</b> CUDA stream to launch kernels within. Default is
deps/cub/cub/device/device_segmented_sort.cuh:  CUB_RUNTIME_FUNCTION static cudaError_t
deps/cub/cub/device/device_segmented_sort.cuh:                      cudaStream_t stream    = 0,
deps/cub/cub/device/device_segmented_sort.cuh:   * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_segmented_sort.cuh:   *   <b>[optional]</b> CUDA stream to launch kernels within. Default is
deps/cub/cub/device/device_segmented_sort.cuh:  CUB_RUNTIME_FUNCTION static cudaError_t
deps/cub/cub/device/device_segmented_sort.cuh:                  cudaStream_t stream    = 0,
deps/cub/cub/device/device_segmented_sort.cuh:   * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_segmented_sort.cuh:   *   <b>[optional]</b> CUDA stream to launch kernels within. Default is
deps/cub/cub/device/device_segmented_sort.cuh:  CUB_RUNTIME_FUNCTION static cudaError_t
deps/cub/cub/device/device_segmented_sort.cuh:                            cudaStream_t stream    = 0,
deps/cub/cub/device/device_segmented_sort.cuh:   * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_segmented_sort.cuh:   *   <b>[optional]</b> CUDA stream to launch kernels within. Default is
deps/cub/cub/device/device_segmented_sort.cuh:  CUB_RUNTIME_FUNCTION static cudaError_t
deps/cub/cub/device/device_segmented_sort.cuh:                  cudaStream_t stream    = 0,
deps/cub/cub/device/device_segmented_sort.cuh:   * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_segmented_sort.cuh:   *   <b>[optional]</b> CUDA stream to launch kernels within. Default is
deps/cub/cub/device/device_segmented_sort.cuh:  CUB_RUNTIME_FUNCTION static cudaError_t
deps/cub/cub/device/device_segmented_sort.cuh:                            cudaStream_t stream    = 0,
deps/cub/cub/device/device_histogram.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/device/device_histogram.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/device/device_histogram.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/device/device_histogram.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_histogram.cuh:    static cudaError_t HistogramEven(
deps/cub/cub/device/device_histogram.cuh:        cudaStream_t        stream                  = 0,                ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/device_histogram.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_histogram.cuh:    static cudaError_t HistogramEven(
deps/cub/cub/device/device_histogram.cuh:        cudaStream_t        stream                  = 0,                ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/device_histogram.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_histogram.cuh:    static cudaError_t MultiHistogramEven(
deps/cub/cub/device/device_histogram.cuh:        cudaStream_t        stream                  = 0,                ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/device_histogram.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_histogram.cuh:    static cudaError_t MultiHistogramEven(
deps/cub/cub/device/device_histogram.cuh:        cudaStream_t        stream                  = 0,                ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/device_histogram.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_histogram.cuh:    static cudaError_t HistogramRange(
deps/cub/cub/device/device_histogram.cuh:        cudaStream_t        stream              = 0,                ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/device_histogram.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_histogram.cuh:    static cudaError_t HistogramRange(
deps/cub/cub/device/device_histogram.cuh:        cudaStream_t        stream              = 0,                ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/device_histogram.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_histogram.cuh:    static cudaError_t MultiHistogramRange(
deps/cub/cub/device/device_histogram.cuh:        cudaStream_t        stream              = 0,                ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/device_histogram.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_histogram.cuh:    static cudaError_t MultiHistogramRange(
deps/cub/cub/device/device_histogram.cuh:        cudaStream_t        stream              = 0,                ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/device_radix_sort.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/device/device_radix_sort.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/device/device_radix_sort.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/device/device_radix_sort.cuh: * (`unsigned char`, `int`, `double`, etc.) as well as CUDA's `__half`
deps/cub/cub/device/device_radix_sort.cuh: * performance across different CUDA architectures for uniform-random \p uint32 keys.
deps/cub/cub/device/device_radix_sort.cuh:     * CUDA architectures for uniform-random <tt>uint32,uint32</tt> and
deps/cub/cub/device/device_radix_sort.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_radix_sort.cuh:    static cudaError_t SortPairs(
deps/cub/cub/device/device_radix_sort.cuh:        cudaStream_t        stream              = 0,                ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/device_radix_sort.cuh:     * CUDA architectures for uniform-random <tt>uint32,uint32</tt> and
deps/cub/cub/device/device_radix_sort.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_radix_sort.cuh:    static cudaError_t SortPairs(
deps/cub/cub/device/device_radix_sort.cuh:        cudaStream_t            stream              = 0,                ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/device_radix_sort.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_radix_sort.cuh:    static cudaError_t SortPairsDescending(
deps/cub/cub/device/device_radix_sort.cuh:        cudaStream_t        stream              = 0,                ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/device_radix_sort.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_radix_sort.cuh:    static cudaError_t SortPairsDescending(
deps/cub/cub/device/device_radix_sort.cuh:        cudaStream_t            stream              = 0,                ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/device_radix_sort.cuh:     * CUDA architectures for uniform-random \p uint32 and \p uint64 keys, respectively.
deps/cub/cub/device/device_radix_sort.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_radix_sort.cuh:    static cudaError_t SortKeys(
deps/cub/cub/device/device_radix_sort.cuh:        cudaStream_t        stream              = 0,                ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/device_radix_sort.cuh:     * CUDA architectures for uniform-random \p uint32 and \p uint64 keys, respectively.
deps/cub/cub/device/device_radix_sort.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_radix_sort.cuh:    static cudaError_t SortKeys(
deps/cub/cub/device/device_radix_sort.cuh:        cudaStream_t        stream              = 0,                ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/device_radix_sort.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_radix_sort.cuh:    static cudaError_t SortKeysDescending(
deps/cub/cub/device/device_radix_sort.cuh:        cudaStream_t        stream              = 0,                ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/device_radix_sort.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_radix_sort.cuh:    static cudaError_t SortKeysDescending(
deps/cub/cub/device/device_radix_sort.cuh:        cudaStream_t        stream              = 0,                ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/device_partition.cuh: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/device/device_partition.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/device/device_partition.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/device/device_partition.cuh: * performance across different CUDA architectures for @p int32 items,
deps/cub/cub/device/device_partition.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_partition.cuh:     *   **[optional]** CUDA stream to launch kernels within.
deps/cub/cub/device/device_partition.cuh:    CUB_RUNTIME_FUNCTION __forceinline__ static cudaError_t
deps/cub/cub/device/device_partition.cuh:            cudaStream_t stream    = 0,
deps/cub/cub/device/device_partition.cuh:     * different CUDA architectures for @p int32 and @p int64 items,
deps/cub/cub/device/device_partition.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_partition.cuh:     *   **[optional]** CUDA stream to launch kernels within.
deps/cub/cub/device/device_partition.cuh:    CUB_RUNTIME_FUNCTION __forceinline__ static cudaError_t
deps/cub/cub/device/device_partition.cuh:       cudaStream_t stream    = 0,
deps/cub/cub/device/device_partition.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_partition.cuh:    CUB_RUNTIME_FUNCTION __forceinline__ static cudaError_t
deps/cub/cub/device/device_partition.cuh:       cudaStream_t stream    = 0,
deps/cub/cub/device/device_segmented_reduce.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/device/device_segmented_reduce.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/device/device_segmented_reduce.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/device/device_segmented_reduce.cuh:     *   (e.g., addition of floating point types) on the same GPU device.
deps/cub/cub/device/device_segmented_reduce.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_segmented_reduce.cuh:    static cudaError_t Reduce(
deps/cub/cub/device/device_segmented_reduce.cuh:        cudaStream_t         stream              = 0,            ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/device_segmented_reduce.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_segmented_reduce.cuh:    static cudaError_t Sum(
deps/cub/cub/device/device_segmented_reduce.cuh:        cudaStream_t          stream              = 0,            ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/device_segmented_reduce.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_segmented_reduce.cuh:    static cudaError_t Min(
deps/cub/cub/device/device_segmented_reduce.cuh:        cudaStream_t          stream              = 0,            ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/device_segmented_reduce.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_segmented_reduce.cuh:    static cudaError_t ArgMin(
deps/cub/cub/device/device_segmented_reduce.cuh:        cudaStream_t         stream              = 0,            ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/device_segmented_reduce.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_segmented_reduce.cuh:    static cudaError_t Max(
deps/cub/cub/device/device_segmented_reduce.cuh:        cudaStream_t         stream              = 0,            ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/device_segmented_reduce.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_segmented_reduce.cuh:    static cudaError_t ArgMax(
deps/cub/cub/device/device_segmented_reduce.cuh:        cudaStream_t         stream              = 0,            ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/device_reduce.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/device/device_reduce.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/device/device_reduce.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/device/device_reduce.cuh: * performance across different CUDA architectures for \p int32 keys.
deps/cub/cub/device/device_reduce.cuh: * performance across different CUDA architectures for \p fp32
deps/cub/cub/device/device_reduce.cuh:     *   (e.g., addition of floating point types) on the same GPU device.
deps/cub/cub/device/device_reduce.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_reduce.cuh:    static cudaError_t Reduce(
deps/cub/cub/device/device_reduce.cuh:        cudaStream_t                stream              = 0,            ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/device_reduce.cuh:     *   (e.g., addition of floating point types) on the same GPU device.
deps/cub/cub/device/device_reduce.cuh:     * CUDA architectures for \p int32 and \p int64 items, respectively.
deps/cub/cub/device/device_reduce.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_reduce.cuh:    static cudaError_t Sum(
deps/cub/cub/device/device_reduce.cuh:        cudaStream_t                stream              = 0,            ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/device_reduce.cuh:     *   (e.g., addition of floating point types) on the same GPU device.
deps/cub/cub/device/device_reduce.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_reduce.cuh:    static cudaError_t Min(
deps/cub/cub/device/device_reduce.cuh:        cudaStream_t                stream              = 0,            ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/device_reduce.cuh:     *   (e.g., addition of floating point types) on the same GPU device.
deps/cub/cub/device/device_reduce.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_reduce.cuh:    static cudaError_t ArgMin(
deps/cub/cub/device/device_reduce.cuh:        cudaStream_t                stream              = 0,            ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/device_reduce.cuh:     *   (e.g., addition of floating point types) on the same GPU device.
deps/cub/cub/device/device_reduce.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_reduce.cuh:    static cudaError_t Max(
deps/cub/cub/device/device_reduce.cuh:        cudaStream_t                stream              = 0,            ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/device_reduce.cuh:     *   (e.g., addition of floating point types) on the same GPU device.
deps/cub/cub/device/device_reduce.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_reduce.cuh:    static cudaError_t ArgMax(
deps/cub/cub/device/device_reduce.cuh:        cudaStream_t                stream              = 0,            ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/device_reduce.cuh:     *   (e.g., addition of floating point types) on the same GPU device.
deps/cub/cub/device/device_reduce.cuh:     * different CUDA architectures for \p fp32 and \p fp64 values, respectively.  Segments
deps/cub/cub/device/device_reduce.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_reduce.cuh:    static cudaError_t ReduceByKey(
deps/cub/cub/device/device_reduce.cuh:        cudaStream_t                stream             = 0,         ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/cub/cub/device/device_adjacent_difference.cuh: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/device/device_adjacent_difference.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/device/device_adjacent_difference.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/device/device_adjacent_difference.cuh: * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_adjacent_difference.cuh:  static CUB_RUNTIME_FUNCTION cudaError_t
deps/cub/cub/device/device_adjacent_difference.cuh:                     cudaStream_t stream,
deps/cub/cub/device/device_adjacent_difference.cuh:   * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_adjacent_difference.cuh:   *   <b>[optional]</b> CUDA stream to launch kernels within. Default is
deps/cub/cub/device/device_adjacent_difference.cuh:  static CUB_RUNTIME_FUNCTION cudaError_t
deps/cub/cub/device/device_adjacent_difference.cuh:                   cudaStream_t stream         = 0,
deps/cub/cub/device/device_adjacent_difference.cuh:   * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_adjacent_difference.cuh:   *   <b>[optional]</b> CUDA stream to launch kernels within. Default is
deps/cub/cub/device/device_adjacent_difference.cuh:  static CUB_RUNTIME_FUNCTION cudaError_t
deps/cub/cub/device/device_adjacent_difference.cuh:               cudaStream_t stream         = 0,
deps/cub/cub/device/device_adjacent_difference.cuh:   * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_adjacent_difference.cuh:   *   <b>[optional]</b> CUDA stream to launch kernels within. Default is
deps/cub/cub/device/device_adjacent_difference.cuh:  static CUB_RUNTIME_FUNCTION cudaError_t
deps/cub/cub/device/device_adjacent_difference.cuh:                    cudaStream_t stream         = 0,
deps/cub/cub/device/device_adjacent_difference.cuh:   * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/cub/cub/device/device_adjacent_difference.cuh:   *   <b>[optional]</b> CUDA stream to launch kernels within. Default is
deps/cub/cub/device/device_adjacent_difference.cuh:  static CUB_RUNTIME_FUNCTION cudaError_t
deps/cub/cub/device/device_adjacent_difference.cuh:                cudaStream_t stream         = 0,
deps/cub/cub/util_device.cuh: * Copyright (c) 2011-2020, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/util_device.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/util_device.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/util_device.cuh: * Properties of a given CUDA device and the corresponding PTX bundle
deps/cub/cub/util_device.cuh:cudaError_t AliasTemporaries(
deps/cub/cub/util_device.cuh:        return cudaSuccess;
deps/cub/cub/util_device.cuh:        return CubDebug(cudaErrorInvalidValue);
deps/cub/cub/util_device.cuh:    return cudaSuccess;
deps/cub/cub/util_device.cuh:#if defined(CUB_RUNTIME_ENABLED) // Host code or device code with the CUDA runtime.
deps/cub/cub/util_device.cuh:    if (CubDebug(cudaGetDevice(&device))) return -1;
deps/cub/cub/util_device.cuh:#else // Device code without the CUDA runtime.
deps/cub/cub/util_device.cuh:            CubDebug(cudaSetDevice(new_device));
deps/cub/cub/util_device.cuh:            CubDebug(cudaSetDevice(old_device));
deps/cub/cub/util_device.cuh: * \brief Returns the number of CUDA devices available or -1 if an error
deps/cub/cub/util_device.cuh:#if defined(CUB_RUNTIME_ENABLED) // Host code or device code with the CUDA runtime.
deps/cub/cub/util_device.cuh:    if (CubDebug(cudaGetDeviceCount(&count)))
deps/cub/cub/util_device.cuh:        // CUDA makes no guarantees about the state of the output parameter if
deps/cub/cub/util_device.cuh:        // `cudaGetDeviceCount` fails; in practice, they don't, but out of
deps/cub/cub/util_device.cuh:#else // Device code without the CUDA runtime.
deps/cub/cub/util_device.cuh: * \brief Returns the number of CUDA devices available.
deps/cub/cub/util_device.cuh: * \brief Per-device cache for a CUDA attribute value; the attribute is queried
deps/cub/cub/util_device.cuh:        cudaError_t error;
deps/cub/cub/util_device.cuh:            return DevicePayload{0, cudaErrorInvalidDevice};
deps/cub/cub/util_device.cuh:                    // Clear the global CUDA error state which may have been
deps/cub/cub/util_device.cuh:                    cudaGetLastError();
deps/cub/cub/util_device.cuh:CUB_RUNTIME_FUNCTION inline cudaError_t PtxVersionUncached(int& ptx_version)
deps/cub/cub/util_device.cuh:    cudaError_t result = cudaSuccess;
deps/cub/cub/util_device.cuh:            cudaFuncAttributes empty_kernel_attrs;
deps/cub/cub/util_device.cuh:            result = cudaFuncGetAttributes(&empty_kernel_attrs,
deps/cub/cub/util_device.cuh:__host__ inline cudaError_t PtxVersionUncached(int& ptx_version, int device)
deps/cub/cub/util_device.cuh:__host__ inline cudaError_t PtxVersion(int& ptx_version, int device)
deps/cub/cub/util_device.cuh:CUB_RUNTIME_FUNCTION inline cudaError_t PtxVersion(int& ptx_version)
deps/cub/cub/util_device.cuh:    cudaError_t result = cudaErrorUnknown;
deps/cub/cub/util_device.cuh:CUB_RUNTIME_FUNCTION inline cudaError_t SmVersionUncached(int& sm_version, int device = CurrentDevice())
deps/cub/cub/util_device.cuh:#if defined(CUB_RUNTIME_ENABLED) // Host code or device code with the CUDA runtime.
deps/cub/cub/util_device.cuh:    cudaError_t error = cudaSuccess;
deps/cub/cub/util_device.cuh:        if (CubDebug(error = cudaDeviceGetAttribute(&major, cudaDevAttrComputeCapabilityMajor, device))) break;
deps/cub/cub/util_device.cuh:        if (CubDebug(error = cudaDeviceGetAttribute(&minor, cudaDevAttrComputeCapabilityMinor, device))) break;
deps/cub/cub/util_device.cuh:#else // Device code without the CUDA runtime.
deps/cub/cub/util_device.cuh:    // CUDA API calls are not supported from this device.
deps/cub/cub/util_device.cuh:    return CubDebug(cudaErrorInvalidConfiguration);
deps/cub/cub/util_device.cuh:CUB_RUNTIME_FUNCTION inline cudaError_t SmVersion(int& sm_version, int device = CurrentDevice())
deps/cub/cub/util_device.cuh:    cudaError_t result = cudaErrorUnknown;
deps/cub/cub/util_device.cuh:CUB_RUNTIME_FUNCTION inline cudaError_t SyncStream(cudaStream_t stream)
deps/cub/cub/util_device.cuh:    cudaError_t result = cudaErrorUnknown;
deps/cub/cub/util_device.cuh:            result = CubDebug(cudaStreamSynchronize(stream));
deps/cub/cub/util_device.cuh:            #if defined(CUB_RUNTIME_ENABLED) // Device code with the CUDA runtime.
deps/cub/cub/util_device.cuh:            #else // Device code without the CUDA runtime.
deps/cub/cub/util_device.cuh:                // CUDA API calls are not supported from this device.
deps/cub/cub/util_device.cuh:                result = CubDebug(cudaErrorInvalidConfiguration);
deps/cub/cub/util_device.cuh:cudaError_t MaxSmOccupancy(
deps/cub/cub/util_device.cuh:    // CUDA API calls not supported from this device
deps/cub/cub/util_device.cuh:    return CubDebug(cudaErrorInvalidConfiguration);
deps/cub/cub/util_device.cuh:    return CubDebug(cudaOccupancyMaxActiveBlocksPerMultiprocessor(
deps/cub/cub/util_device.cuh:    cudaError_t Init(KernelPtrT kernel_ptr)
deps/cub/cub/util_device.cuh:        cudaError_t retval   = MaxSmOccupancy(sm_occupancy, kernel_ptr, block_threads);
deps/cub/cub/util_device.cuh:  static cudaError_t Invoke(int ptx_version, FunctorT& op)
deps/cub/cub/util_device.cuh:    static cudaError_t Invoke(int /*ptx_version*/, FunctorT& op) {
deps/cub/cub/util_debug.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/util_debug.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/util_debug.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/util_debug.cuh: * \brief If \p CUB_STDERR is defined and \p error is not \p cudaSuccess, the corresponding error message is printed to \p stderr (or \p stdout in device code) along with the supplied source context.
deps/cub/cub/util_debug.cuh: * \return The CUDA error.
deps/cub/cub/util_debug.cuh:__host__ __device__ __forceinline__ cudaError_t Debug(
deps/cub/cub/util_debug.cuh:    cudaError_t     error,
deps/cub/cub/util_debug.cuh:    // Clear the global CUDA error state which may have been set by the last
deps/cub/cub/util_debug.cuh:    cudaGetLastError();
deps/cub/cub/util_debug.cuh:                fprintf(stderr, "CUDA error %d [%s, %d]: %s\n", error, filename, line, cudaGetErrorString(error));
deps/cub/cub/util_debug.cuh:                printf("CUDA error %d [block (%d,%d,%d) thread (%d,%d,%d), %s, %d]\n", error, blockIdx.z, blockIdx.y, blockIdx.x, threadIdx.z, threadIdx.y, threadIdx.x, filename, line);
deps/cub/cub/util_debug.cuh:    #define CubDebug(e) CUB_NS_QUALIFIER::Debug((cudaError_t) (e), __FILE__, __LINE__)
deps/cub/cub/util_debug.cuh:    #define CubDebugExit(e) if (CUB_NS_QUALIFIER::Debug((cudaError_t) (e), __FILE__, __LINE__)) { exit(1); }
deps/cub/cub/util_debug.cuh:    #if defined(_NVHPC_CUDA)
deps/cub/cub/util_debug.cuh:    #elif !(defined(__clang__) && defined(__CUDA__))
deps/cub/cub/util_debug.cuh:        #ifdef __CUDA_ARCH__
deps/cub/cub/util_debug.cuh:        #ifndef __CUDA_ARCH__
deps/cub/cub/config.cuh: * Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/config.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/config.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/agent/agent_reduce_by_key.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/agent/agent_reduce_by_key.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/agent/agent_reduce_by_key.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/agent/agent_reduce_by_key.cuh: * cub::AgentReduceByKey implements a stateful abstraction of CUDA thread blocks for participating in device-wide reduce-value-by-key.
deps/cub/cub/agent/agent_reduce_by_key.cuh: * \brief AgentReduceByKey implements a stateful abstraction of CUDA thread blocks for participating in device-wide reduce-value-by-key
deps/cub/cub/agent/agent_merge_sort.cuh: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/agent/agent_merge_sort.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/agent/agent_merge_sort.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/agent/agent_merge_sort.cuh:#include <thrust/system/cuda/detail/core/util.h>
deps/cub/cub/agent/agent_merge_sort.cuh:  using KeysLoadIt  = typename THRUST_NS_QUALIFIER::cuda_cub::core::LoadIterator<Policy, KeyInputIteratorT>::type;
deps/cub/cub/agent/agent_merge_sort.cuh:  using ItemsLoadIt = typename THRUST_NS_QUALIFIER::cuda_cub::core::LoadIterator<Policy, ValueInputIteratorT>::type;
deps/cub/cub/agent/agent_merge_sort.cuh:  using KeysLoadPingIt  = typename THRUST_NS_QUALIFIER::cuda_cub::core::LoadIterator<Policy, KeyIteratorT>::type;
deps/cub/cub/agent/agent_merge_sort.cuh:  using ItemsLoadPingIt = typename THRUST_NS_QUALIFIER::cuda_cub::core::LoadIterator<Policy, ValueIteratorT>::type;
deps/cub/cub/agent/agent_merge_sort.cuh:  using KeysLoadPongIt  = typename THRUST_NS_QUALIFIER::cuda_cub::core::LoadIterator<Policy, KeyT *>::type;
deps/cub/cub/agent/agent_merge_sort.cuh:  using ItemsLoadPongIt = typename THRUST_NS_QUALIFIER::cuda_cub::core::LoadIterator<Policy, ValueT *>::type;
deps/cub/cub/agent/agent_radix_sort_upsweep.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/agent/agent_radix_sort_upsweep.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/agent/agent_radix_sort_upsweep.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/agent/agent_radix_sort_upsweep.cuh: * AgentRadixSortUpsweep implements a stateful abstraction of CUDA thread blocks for participating in device-wide radix sort upsweep .
deps/cub/cub/agent/agent_radix_sort_upsweep.cuh: * \brief AgentRadixSortUpsweep implements a stateful abstraction of CUDA thread blocks for participating in device-wide radix sort upsweep .
deps/cub/cub/agent/agent_adjacent_difference.cuh: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/agent/agent_adjacent_difference.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/agent/agent_adjacent_difference.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/agent/agent_adjacent_difference.cuh:#include <thrust/system/cuda/detail/core/util.h>
deps/cub/cub/agent/agent_adjacent_difference.cuh:  using LoadIt = typename THRUST_NS_QUALIFIER::cuda_cub::core::LoadIterator<Policy, InputIteratorT>::type;
deps/cub/cub/agent/agent_adjacent_difference.cuh:          THRUST_NS_QUALIFIER::cuda_cub::core::make_load_iterator(Policy(),
deps/cub/cub/agent/agent_scan_by_key.cuh: * Copyright (c) 2021, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/agent/agent_scan_by_key.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/agent/agent_scan_by_key.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/agent/agent_scan_by_key.cuh: * AgentScanByKey implements a stateful abstraction of CUDA thread blocks for participating in device-wide prefix scan by key.
deps/cub/cub/agent/agent_scan_by_key.cuh: * \brief AgentScanByKey implements a stateful abstraction of CUDA thread blocks for participating in device-wide prefix scan by key.
deps/cub/cub/agent/agent_scan_by_key.cuh:        Int2Type<false> /* is_incclusive */)
deps/cub/cub/agent/agent_histogram.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/agent/agent_histogram.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/agent/agent_histogram.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/agent/agent_histogram.cuh: * cub::AgentHistogram implements a stateful abstraction of CUDA thread blocks for participating in device-wide histogram .
deps/cub/cub/agent/agent_histogram.cuh: * \brief AgentHistogram implements a stateful abstraction of CUDA thread blocks for participating in device-wide histogram .
deps/cub/cub/agent/agent_reduce.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/agent/agent_reduce.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/agent/agent_reduce.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/agent/agent_reduce.cuh: * cub::AgentReduce implements a stateful abstraction of CUDA thread blocks for participating in device-wide reduction .
deps/cub/cub/agent/agent_reduce.cuh: * \brief AgentReduce implements a stateful abstraction of CUDA thread blocks for participating in device-wide reduction .
deps/cub/cub/agent/agent_scan.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/agent/agent_scan.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/agent/agent_scan.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/agent/agent_scan.cuh: * cub::AgentScan implements a stateful abstraction of CUDA thread blocks for participating in device-wide prefix scan .
deps/cub/cub/agent/agent_scan.cuh: * \brief AgentScan implements a stateful abstraction of CUDA thread blocks for participating in device-wide prefix scan .
deps/cub/cub/agent/agent_sub_warp_merge_sort.cuh: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/agent/agent_sub_warp_merge_sort.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/agent/agent_sub_warp_merge_sort.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/agent/agent_sub_warp_merge_sort.cuh:#include <thrust/system/cuda/detail/core/util.h>
deps/cub/cub/agent/agent_sub_warp_merge_sort.cuh:#if defined(__CUDA_FP16_TYPES_EXIST__) && (CUB_PTX_ARCH < 530)
deps/cub/cub/agent/agent_sub_warp_merge_sort.cuh:#if defined(__CUDA_FP16_TYPES_EXIST__) && (CUB_PTX_ARCH < 530)
deps/cub/cub/agent/agent_sub_warp_merge_sort.cuh:  using KeysLoadItT = typename THRUST_NS_QUALIFIER::cuda_cub::core::
deps/cub/cub/agent/agent_sub_warp_merge_sort.cuh:  using ItemsLoadItT = typename THRUST_NS_QUALIFIER::cuda_cub::core::
deps/cub/cub/agent/agent_radix_sort_downsweep.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/agent/agent_radix_sort_downsweep.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/agent/agent_radix_sort_downsweep.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/agent/agent_radix_sort_downsweep.cuh: * AgentRadixSortDownsweep implements a stateful abstraction of CUDA thread
deps/cub/cub/agent/agent_radix_sort_downsweep.cuh: * \brief AgentRadixSortDownsweep implements a stateful abstraction of CUDA thread blocks for participating in device-wide radix sort downsweep .
deps/cub/cub/agent/agent_segment_fixup.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/agent/agent_segment_fixup.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/agent/agent_segment_fixup.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/agent/agent_segment_fixup.cuh: * cub::AgentSegmentFixup implements a stateful abstraction of CUDA thread blocks for participating in device-wide reduce-value-by-key.
deps/cub/cub/agent/agent_segment_fixup.cuh: * \brief AgentSegmentFixup implements a stateful abstraction of CUDA thread blocks for participating in device-wide reduce-value-by-key
deps/cub/cub/agent/agent_segmented_radix_sort.cuh: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/agent/agent_segmented_radix_sort.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/agent/agent_segmented_radix_sort.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/agent/agent_segmented_radix_sort.cuh: * https://github.com/NVIDIA/cub/issues/383 is addressed.
deps/cub/cub/agent/agent_rle.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/agent/agent_rle.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/agent/agent_rle.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/agent/agent_rle.cuh: * cub::AgentRle implements a stateful abstraction of CUDA thread blocks for participating in device-wide run-length-encode.
deps/cub/cub/agent/agent_rle.cuh: * \brief AgentRle implements a stateful abstraction of CUDA thread blocks for participating in device-wide run-length-encode 
deps/cub/cub/agent/agent_select_if.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/agent/agent_select_if.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/agent/agent_select_if.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/agent/agent_select_if.cuh: * cub::AgentSelectIf implements a stateful abstraction of CUDA thread blocks for participating in device-wide select.
deps/cub/cub/agent/agent_select_if.cuh: * \brief AgentSelectIf implements a stateful abstraction of CUDA thread blocks for participating in device-wide selection
deps/cub/cub/agent/agent_three_way_partition.cuh: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/agent/agent_three_way_partition.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/agent/agent_three_way_partition.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/agent/agent_radix_sort_histogram.cuh: * Copyright (c) 2011-2020, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/agent/agent_radix_sort_histogram.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/agent/agent_radix_sort_histogram.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/agent/agent_radix_sort_histogram.cuh: * agent_radix_sort_histogram.cuh implements a stateful abstraction of CUDA
deps/cub/cub/agent/agent_radix_sort_histogram.cuh:                // Using cuda::atomic<> results in lower performance on GP100,
deps/cub/cub/agent/agent_radix_sort_histogram.cuh:                    // Using cuda::atomic<> here would also require using it in
deps/cub/cub/agent/agent_radix_sort_histogram.cuh:                    // cuda::atomic_ref<> becomes available.
deps/cub/cub/agent/single_pass_scan_operators.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/agent/single_pass_scan_operators.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/agent/single_pass_scan_operators.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/agent/single_pass_scan_operators.cuh:    cudaError_t Init(
deps/cub/cub/agent/single_pass_scan_operators.cuh:        return cudaSuccess;
deps/cub/cub/agent/single_pass_scan_operators.cuh:    static cudaError_t AllocationSize(
deps/cub/cub/agent/single_pass_scan_operators.cuh:        return cudaSuccess;
deps/cub/cub/agent/single_pass_scan_operators.cuh:    cudaError_t Init(
deps/cub/cub/agent/single_pass_scan_operators.cuh:        cudaError_t error = cudaSuccess;
deps/cub/cub/agent/single_pass_scan_operators.cuh:    static cudaError_t AllocationSize(
deps/cub/cub/agent/single_pass_scan_operators.cuh:    cudaError_t Init(
deps/cub/cub/agent/single_pass_scan_operators.cuh:        return cudaSuccess;
deps/cub/cub/agent/single_pass_scan_operators.cuh:    static cudaError_t AllocationSize(
deps/cub/cub/agent/single_pass_scan_operators.cuh:        return cudaSuccess;
deps/cub/cub/agent/agent_radix_sort_onesweep.cuh: * Copyright (c) 2011-2020, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/agent/agent_radix_sort_onesweep.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/agent/agent_radix_sort_onesweep.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/agent/agent_radix_sort_onesweep.cuh: * agent_radix_sort_onesweep.cuh implements a stateful abstraction of CUDA
deps/cub/cub/agent/agent_spmv_orig.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/agent/agent_spmv_orig.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/agent/agent_spmv_orig.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/agent/agent_spmv_orig.cuh: * cub::AgentSpmv implements a stateful abstraction of CUDA thread blocks for participating in device-wide SpMV.
deps/cub/cub/agent/agent_spmv_orig.cuh: * \brief AgentSpmv implements a stateful abstraction of CUDA thread blocks for participating in device-wide SpMV.
deps/cub/cub/util_arch.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/util_arch.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/util_arch.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/util_arch.cuh:#if ((__CUDACC_VER_MAJOR__ >= 9) || defined(_NVHPC_CUDA) ||            \
deps/cub/cub/util_arch.cuh:     CUDA_VERSION >= 9000) &&                                                  \
deps/cub/cub/util_arch.cuh:    #if defined(_NVHPC_CUDA)
deps/cub/cub/util_arch.cuh:        // __NVCOMPILER_CUDA_ARCH__ is the target PTX version, and is defined
deps/cub/cub/util_arch.cuh:        #define CUB_PTX_ARCH __NVCOMPILER_CUDA_ARCH__
deps/cub/cub/util_arch.cuh:    #elif !defined(__CUDA_ARCH__)
deps/cub/cub/util_arch.cuh:        #define CUB_PTX_ARCH __CUDA_ARCH__
deps/cub/cub/util_arch.cuh:    #if defined(_NVHPC_CUDA)
deps/cub/cub/util_arch.cuh:/// Whether or not the source targeted by the active compiler pass is allowed to  invoke device kernels or methods from the CUDA runtime API.
deps/cub/cub/util_arch.cuh:    #if !defined(__CUDA_ARCH__) || (__CUDA_ARCH__>= 350 && defined(__CUDACC_RDC__))
deps/cub/cub/util_cpp_dialect.cuh: * Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/util_cpp_dialect.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/util_cpp_dialect.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/thread/thread_operators.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/thread/thread_operators.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/thread/thread_operators.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/thread/thread_search.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/thread/thread_search.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/thread/thread_search.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/thread/thread_sort.cuh: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/thread/thread_sort.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/thread/thread_sort.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/thread/thread_load.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/thread/thread_load.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/thread/thread_load.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/thread/thread_store.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/thread/thread_store.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/thread/thread_store.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/thread/thread_scan.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/thread/thread_scan.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/thread/thread_scan.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/thread/thread_reduce.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/thread/thread_reduce.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/thread/thread_reduce.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/util_ptx.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/util_ptx.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/util_ptx.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/util_type.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/util_type.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/util_type.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/util_type.cuh:#if (__CUDACC_VER_MAJOR__ >= 9 || CUDA_VERSION >= 9000) && !_NVHPC_CUDA
deps/cub/cub/util_type.cuh:    #include <cuda_fp16.h>
deps/cub/cub/util_type.cuh:#if (__CUDACC_VER_MAJOR__ >= 11 || CUDA_VERSION >= 11000) && !_NVHPC_CUDA
deps/cub/cub/util_type.cuh:    #include <cuda_bf16.h>
deps/cub/cub/util_type.cuh:        /// The "true CUDA" alignment of T in bytes
deps/cub/cub/util_type.cuh: * \brief Exposes a member typedef \p Type that names the corresponding CUDA vector type if one exists.  Otherwise \p Type refers to the CubVector structure itself, which will wrap the corresponding \p x, \p y, etc. vector fields.
deps/cub/cub/util_type.cuh:    /// The maximum number of elements in CUDA vector types
deps/cub/cub/util_type.cuh:// Expand CUDA vector types for built-in primitives
deps/cub/cub/util_type.cuh:#if (__CUDACC_VER_MAJOR__ >= 9 || CUDA_VERSION >= 9000) && !_NVHPC_CUDA
deps/cub/cub/util_type.cuh:#if (__CUDACC_VER_MAJOR__ >= 11 || CUDA_VERSION >= 11000) && !_NVHPC_CUDA
deps/cub/cub/util_type.cuh:#if (__CUDACC_VER_MAJOR__ >= 9 || CUDA_VERSION >= 9000) && !_NVHPC_CUDA
deps/cub/cub/util_type.cuh:#if (__CUDACC_VER_MAJOR__ >= 11 || CUDA_VERSION >= 11000) && !_NVHPC_CUDA
deps/cub/cub/util_deprecated.cuh: * Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/util_deprecated.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/util_deprecated.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/version.cuh: * Copyright (c) 2011-2020, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/version.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/version.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/cmake/cub-config.cmake:  # 3) nvcc will automatically check the CUDA Toolkit include path *before* the
deps/cub/cub/util_namespace.cuh: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/util_namespace.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/util_namespace.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/cub.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/cub.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/cub.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/warp/warp_load.cuh: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/warp/warp_load.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/warp/warp_load.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/warp/warp_load.cuh: * Operations for reading linear tiles of data into the CUDA warp.
deps/cub/cub/warp/warp_load.cuh: *        a CUDA warp.
deps/cub/cub/warp/warp_load.cuh:   * from memory using CUDA's built-in vectorized loads as a coalescing optimization.
deps/cub/cub/warp/warp_load.cuh:   *   - The data type @p T is not a built-in primitive or CUDA vector type
deps/cub/cub/warp/warp_load.cuh: *        across a CUDA thread block.
deps/cub/cub/warp/warp_load.cuh: *   targeted CUDA compute-capability (e.g., 32 threads for SM86). Must be a
deps/cub/cub/warp/warp_load.cuh: *      of data is read directly from memory using CUDA's built-in vectorized
deps/cub/cub/warp/warp_merge_sort.cuh: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/warp/warp_merge_sort.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/warp/warp_merge_sort.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/warp/warp_merge_sort.cuh: *        across a CUDA warp using a merge sorting method.
deps/cub/cub/warp/warp_merge_sort.cuh: *   targeted CUDA compute-capability (e.g., 32 threads for SM86). Must be a
deps/cub/cub/warp/warp_scan.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/warp/warp_scan.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/warp/warp_scan.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/warp/warp_scan.cuh: * The cub::WarpScan class provides [<em>collective</em>](index.html#sec0) methods for computing a parallel prefix scan of items partitioned across a CUDA thread warp.
deps/cub/cub/warp/warp_scan.cuh: * \brief The WarpScan class provides [<em>collective</em>](index.html#sec0) methods for computing a parallel prefix scan of items partitioned across a CUDA thread warp.  ![](warp_scan_logo.png)
deps/cub/cub/warp/warp_scan.cuh: * \tparam LOGICAL_WARP_THREADS     <b>[optional]</b> The number of threads per "logical" warp (may be less than the number of hardware warp threads).  Default is the warp size associated with the CUDA Compute Capability targeted by the compiler (e.g., 32 threads for SM20).
deps/cub/cub/warp/specializations/warp_reduce_shfl.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/warp/specializations/warp_reduce_shfl.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/warp/specializations/warp_reduce_shfl.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/warp/specializations/warp_reduce_shfl.cuh: * cub::WarpReduceShfl provides SHFL-based variants of parallel reduction of items partitioned across a CUDA thread warp.
deps/cub/cub/warp/specializations/warp_reduce_shfl.cuh: * \brief WarpReduceShfl provides SHFL-based variants of parallel reduction of items partitioned across a CUDA thread warp.
deps/cub/cub/warp/specializations/warp_scan_shfl.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/warp/specializations/warp_scan_shfl.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/warp/specializations/warp_scan_shfl.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/warp/specializations/warp_scan_shfl.cuh: * cub::WarpScanShfl provides SHFL-based variants of parallel prefix scan of items partitioned across a CUDA thread warp.
deps/cub/cub/warp/specializations/warp_scan_shfl.cuh: * \brief WarpScanShfl provides SHFL-based variants of parallel prefix scan of items partitioned across a CUDA thread warp.
deps/cub/cub/warp/specializations/warp_scan_smem.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/warp/specializations/warp_scan_smem.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/warp/specializations/warp_scan_smem.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/warp/specializations/warp_scan_smem.cuh: * cub::WarpScanSmem provides smem-based variants of parallel prefix scan of items partitioned across a CUDA thread warp.
deps/cub/cub/warp/specializations/warp_scan_smem.cuh: * \brief WarpScanSmem provides smem-based variants of parallel prefix scan of items partitioned across a CUDA thread warp.
deps/cub/cub/warp/specializations/warp_reduce_smem.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/warp/specializations/warp_reduce_smem.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/warp/specializations/warp_reduce_smem.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/warp/specializations/warp_reduce_smem.cuh: * cub::WarpReduceSmem provides smem-based variants of parallel reduction of items partitioned across a CUDA thread warp.
deps/cub/cub/warp/specializations/warp_reduce_smem.cuh: * \brief WarpReduceSmem provides smem-based variants of parallel reduction of items partitioned across a CUDA thread warp.
deps/cub/cub/warp/warp_exchange.cuh: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/warp/warp_exchange.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/warp/warp_exchange.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/warp/warp_exchange.cuh: * methods for rearranging data partitioned across a CUDA warp.
deps/cub/cub/warp/warp_exchange.cuh: *        methods for rearranging data partitioned across a CUDA warp.
deps/cub/cub/warp/warp_exchange.cuh: *   targeted CUDA compute-capability (e.g., 32 threads for SM86). Must be a
deps/cub/cub/warp/warp_reduce.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/warp/warp_reduce.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/warp/warp_reduce.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/warp/warp_reduce.cuh: * The cub::WarpReduce class provides [<em>collective</em>](index.html#sec0) methods for computing a parallel reduction of items partitioned across a CUDA thread warp.
deps/cub/cub/warp/warp_reduce.cuh: * \brief The WarpReduce class provides [<em>collective</em>](index.html#sec0) methods for computing a parallel reduction of items partitioned across a CUDA thread warp. ![](warp_reduce_logo.png)
deps/cub/cub/warp/warp_reduce.cuh: * \tparam LOGICAL_WARP_THREADS     <b>[optional]</b> The number of threads per "logical" warp (may be less than the number of hardware warp threads).  Default is the warp size of the targeted CUDA compute-capability (e.g., 32 threads for SM20).
deps/cub/cub/warp/warp_store.cuh: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/warp/warp_store.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/warp/warp_store.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/warp/warp_store.cuh: * Operations for writing linear segments of data from the CUDA warp
deps/cub/cub/warp/warp_store.cuh: *        cub::WarpStore to write a blocked arrangement of items across a CUDA
deps/cub/cub/warp/warp_store.cuh:   * directly to memory using CUDA's built-in vectorized stores as a coalescing
deps/cub/cub/warp/warp_store.cuh:   *   - The data type @p T is not a built-in primitive or CUDA vector type
deps/cub/cub/warp/warp_store.cuh: *        of items partitioned across a CUDA warp to a linear segment of memory.
deps/cub/cub/warp/warp_store.cuh: *   targeted CUDA compute-capability (e.g., 32 threads for SM86). Must be a
deps/cub/cub/warp/warp_store.cuh: *      of data is written directly to memory using CUDA's built-in vectorized
deps/cub/cub/grid/grid_even_share.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/grid/grid_even_share.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/grid/grid_even_share.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/grid/grid_even_share.cuh: * cub::GridEvenShare is a descriptor utility for distributing input among CUDA thread blocks in an "even-share" fashion.  Each thread block gets roughly the same number of fixed-size work units (grains).
deps/cub/cub/grid/grid_even_share.cuh: * CUDA thread blocks in an "even-share" fashion.  Each thread block gets roughly
deps/cub/cub/grid/grid_barrier.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/grid/grid_barrier.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/grid/grid_barrier.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/grid/grid_barrier.cuh: * cub::GridBarrier implements a software global barrier among thread blocks within a CUDA grid
deps/cub/cub/grid/grid_barrier.cuh: * \brief GridBarrier implements a software global barrier among thread blocks within a CUDA grid
deps/cub/cub/grid/grid_barrier.cuh:    cudaError_t HostReset()
deps/cub/cub/grid/grid_barrier.cuh:        cudaError_t retval = cudaSuccess;
deps/cub/cub/grid/grid_barrier.cuh:            CubDebug(retval = cudaFree(d_sync));
deps/cub/cub/grid/grid_barrier.cuh:    cudaError_t Setup(int sweep_grid_size)
deps/cub/cub/grid/grid_barrier.cuh:        cudaError_t retval = cudaSuccess;
deps/cub/cub/grid/grid_barrier.cuh:                    if (CubDebug(retval = cudaFree(d_sync))) break;
deps/cub/cub/grid/grid_barrier.cuh:                if (CubDebug(retval = cudaMalloc((void**) &d_sync, sync_bytes))) break;
deps/cub/cub/grid/grid_barrier.cuh:                if (CubDebug(retval = cudaMemset(d_sync, 0, new_sync_bytes))) break;
deps/cub/cub/grid/grid_mapping.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/grid/grid_mapping.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/grid/grid_mapping.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/grid/grid_mapping.cuh: * cub::GridMappingStrategy enumerates alternative strategies for mapping constant-sized tiles of device-wide data onto a grid of CUDA thread blocks.
deps/cub/cub/grid/grid_mapping.cuh: * \brief cub::GridMappingStrategy enumerates alternative strategies for mapping constant-sized tiles of device-wide data onto a grid of CUDA thread blocks.
deps/cub/cub/grid/grid_queue.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/cub/grid/grid_queue.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/cub/grid/grid_queue.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/cub/grid/grid_queue.cuh:    __host__ __device__ __forceinline__ cudaError_t FillAndResetDrain(
deps/cub/cub/grid/grid_queue.cuh:        cudaStream_t stream = 0)
deps/cub/cub/grid/grid_queue.cuh:        cudaError_t result = cudaErrorUnknown;
deps/cub/cub/grid/grid_queue.cuh:                result = cudaSuccess;
deps/cub/cub/grid/grid_queue.cuh:                result = CubDebug(cudaMemcpyAsync(d_counters, counters, sizeof(OffsetT) * 2, cudaMemcpyHostToDevice, stream));
deps/cub/cub/grid/grid_queue.cuh:    __host__ __device__ __forceinline__ cudaError_t ResetDrain(cudaStream_t stream = 0)
deps/cub/cub/grid/grid_queue.cuh:        cudaError_t result = cudaErrorUnknown;
deps/cub/cub/grid/grid_queue.cuh:                result = cudaSuccess;
deps/cub/cub/grid/grid_queue.cuh:                result = CubDebug(cudaMemsetAsync(d_counters + DRAIN, 0, sizeof(OffsetT), stream));
deps/cub/cub/grid/grid_queue.cuh:    __host__ __device__ __forceinline__ cudaError_t ResetFill(cudaStream_t stream = 0)
deps/cub/cub/grid/grid_queue.cuh:        cudaError_t result = cudaErrorUnknown;
deps/cub/cub/grid/grid_queue.cuh:                result = cudaSuccess;
deps/cub/cub/grid/grid_queue.cuh:                result = CubDebug(cudaMemsetAsync(d_counters + FILL, 0, sizeof(OffsetT), stream));
deps/cub/cub/grid/grid_queue.cuh:    __host__ __device__ __forceinline__ cudaError_t FillSize(
deps/cub/cub/grid/grid_queue.cuh:        cudaStream_t stream = 0)
deps/cub/cub/grid/grid_queue.cuh:        cudaError_t result = cudaErrorUnknown;
deps/cub/cub/grid/grid_queue.cuh:                result = cudaSuccess;
deps/cub/cub/grid/grid_queue.cuh:                result = CubDebug(cudaMemcpyAsync(&fill_size, d_counters + FILL, sizeof(OffsetT), cudaMemcpyDeviceToHost, stream));
deps/cub/cub/grid/grid_queue.cuh:    /// Drain \p num_items from the queue.  Returns offset from which to read items.  To be called from CUDA kernel.
deps/cub/cub/grid/grid_queue.cuh:    /// Fill \p num_items into the queue.  Returns offset from which to write items.    To be called from CUDA kernel.
deps/cub/CODE_OF_CONDUCT.md:This document defines the Code of Conduct followed and enforced for NVIDIA C++
deps/cub/CODE_OF_CONDUCT.md:  reported by contacting [cpp-conduct@nvidia.com](mailto:cpp-conduct@nvidia.com).
deps/cub/CODE_OF_CONDUCT.md:This Code of Conduct was taken from the [NVIDIA RAPIDS] project, which was
deps/cub/CODE_OF_CONDUCT.md:Please email [cpp-conduct@nvidia.com] for any Code of Conduct related matters.
deps/cub/CODE_OF_CONDUCT.md:[cpp-conduct@nvidia.com]: mailto:cpp-conduct@nvidia.com
deps/cub/CODE_OF_CONDUCT.md:[NVIDIA RAPIDS]: https://docs.rapids.ai/resources/conduct/
deps/cub/README.md:of the CUDA programming model:
deps/cub/README.md:  - Compatible with CUDA dynamic parallelism
deps/cub/README.md:![Orientation of collective primitives within the CUDA software stack](http://nvlabs.github.io/cub/cub_overview.png)
deps/cub/README.md:CUB is included in the NVIDIA HPC SDK and the CUDA Toolkit.
deps/cub/README.md:// Block-sorting CUDA kernel
deps/cub/README.md:CUB is distributed with the NVIDIA HPC SDK and the CUDA Toolkit in addition
deps/cub/README.md:| 1.14.0                    | NVIDIA HPC SDK 21.9                     |
deps/cub/README.md:| 1.13.1                    | CUDA Toolkit 11.5                       |
deps/cub/README.md:| 1.13.0                    | NVIDIA HPC SDK 21.7                     |
deps/cub/README.md:| 1.12.1                    | CUDA Toolkit 11.4                       |
deps/cub/README.md:| 1.12.0                    | NVIDIA HPC SDK 21.3                     |
deps/cub/README.md:| 1.11.0                    | CUDA Toolkit 11.3                       |
deps/cub/README.md:| 1.10.0                    | NVIDIA HPC SDK 20.9 & CUDA Toolkit 11.2 |
deps/cub/README.md:| 1.9.10-1                  | NVIDIA HPC SDK 20.7 & CUDA Toolkit 11.1 |
deps/cub/README.md:| 1.9.10                    | NVIDIA HPC SDK 20.5                     |
deps/cub/README.md:| 1.9.9                     | CUDA Toolkit 11.0                       |
deps/cub/README.md:| 1.9.8-1                   | NVIDIA HPC SDK 20.3                     |
deps/cub/README.md:| 1.9.8                     | CUDA Toolkit 11.0 Early Access          |
deps/cub/README.md:| 1.9.8                     | CUDA 11.0 Early Access                  |
deps/cub/README.md:git clone --recursive https://github.com/NVIDIA/thrust.git
deps/cub/README.md:Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/README.md:   *  Neither the name of the NVIDIA CORPORATION nor the
deps/cub/README.md:DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/CMakeLists.txt:# 3.18.3 for C++17 + CUDA.
deps/cub/CMakeLists.txt:# Remove this when we use the new CUDA_ARCHITECTURES properties.
deps/cub/CMakeLists.txt:# need the install rules. See GH issue NVIDIA/thrust#1211.
deps/cub/CMakeLists.txt:include(cmake/CubCudaConfig.cmake)
deps/cub/common.mk:# * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/common.mk:# *	 * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/common.mk:# * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/common.mk:    NVCCFLAGS += -rdc=true -lcudadevrt
deps/cub/common.mk:# [abi=<0|1>] CUDA ABI option (enabled by default)
deps/cub/common.mk:NVCCFLAGS += $(SM_DEF) -Xptxas -v -Xcudafe -\#
deps/cub/common.mk:	CUDART_CYG = "$(shell dirname $(NVCC))/../lib/Win32/cudart.lib"
deps/cub/common.mk:	CUDART_CYG = "$(shell dirname $(NVCC))/../lib/x64/cudart.lib"
deps/cub/common.mk:	CUDART = "$(shell cygpath -w $(CUDART_CYG))"
deps/cub/common.mk:    CUDART = "$(shell dirname $(NVCC))/../lib/libcudart_static.a"
deps/cub/common.mk:    CUDART = "$(shell dirname $(NVCC))/../lib64/libcudart_static.a"
deps/cub/cmake/CubCudaConfig.cmake:enable_language(CUDA)
deps/cub/cmake/CubCudaConfig.cmake:# Thrust sets up the architecture flags in CMAKE_CUDA_FLAGS already. Just
deps/cub/cmake/CubCudaConfig.cmake:# reuse them if possible. After we transition to CMake 3.18 CUDA_ARCHITECTURE
deps/cub/cmake/CubCudaConfig.cmake:  set(CMAKE_CUDA_FLAGS "${THRUST_CUDA_FLAGS_BASE} ${THRUST_CUDA_FLAGS_NO_RDC}")
deps/cub/cmake/CubCudaConfig.cmake:  if ("NVCXX" STREQUAL "${CMAKE_CUDA_COMPILER_ID}")
deps/cub/cmake/CubCudaConfig.cmake:      if ("NVCXX" STREQUAL "${CMAKE_CUDA_COMPILER_ID}")
deps/cub/cmake/CubCudaConfig.cmake:        set(arch_flag "-gpu=cc${arch}")
deps/cub/cmake/CubCudaConfig.cmake:  if (NOT "NVCXX" STREQUAL "${CMAKE_CUDA_COMPILER_ID}")
deps/cub/cmake/CubCudaConfig.cmake:  # TODO Once CMake 3.18 is required, use the CUDA_ARCHITECTURE target props
deps/cub/cmake/CubCudaConfig.cmake:  string(APPEND CMAKE_CUDA_FLAGS "${arch_flags}")
deps/cub/cmake/CubCudaConfig.cmake:if ("NVCXX" STREQUAL "${CMAKE_CUDA_COMPILER_ID}")
deps/cub/cmake/CubBuildTargetList.cmake:        CUDA_STANDARD ${dialect}
deps/cub/cmake/CubBuildTargetList.cmake:    # CMake still emits errors about empty CUDA_ARCHITECTURES when CMP0104
deps/cub/cmake/CubBuildTargetList.cmake:          CUDA_ARCHITECTURES OFF
deps/cub/cmake/CubBuildTargetList.cmake:    if ("NVCXX" STREQUAL "${CMAKE_CUDA_COMPILER_ID}")
deps/cub/cmake/CubBuildTargetList.cmake:        CUDA_RESOLVE_DEVICE_SYMBOLS OFF
deps/cub/cmake/CubBuildTargetList.cmake:    thrust_create_target(cub.thrust HOST CPP DEVICE CUDA)
deps/cub/cmake/CubCompilerHacks.cmake:# `CMAKE_CUDA_COMPILER_ID=NVCXX` and `CMAKE_CUDA_COMPILER_FORCED=ON`.
deps/cub/cmake/CubCompilerHacks.cmake:if ("NVCXX" STREQUAL "${CMAKE_CUDA_COMPILER_ID}")
deps/cub/cmake/CubCompilerHacks.cmake:    message(FATAL_ERROR "You are using NVC++ as your CUDA C++ compiler, but have"
deps/cub/cmake/CubCompilerHacks.cmake:  # We don't set CMAKE_CUDA_HOST_COMPILER for NVC++; if we do, CMake tries to
deps/cub/cmake/CubCompilerHacks.cmake:  # pass `-ccbin ${CMAKE_CUDA_HOST_COMPILER}` to NVC++, which it doesn't
deps/cub/cmake/CubCompilerHacks.cmake:  if (NOT "${CMAKE_CUDA_HOST_COMPILER}" STREQUAL "")
deps/cub/cmake/CubCompilerHacks.cmake:    unset(CMAKE_CUDA_HOST_COMPILER CACHE)
deps/cub/cmake/CubCompilerHacks.cmake:    message(FATAL_ERROR "You are using NVC++ as your CUDA C++ compiler, but have"
deps/cub/cmake/CubCompilerHacks.cmake:      " please unset the CMAKE_CUDA_HOST_COMPILER variable."
deps/cub/cmake/CubCompilerHacks.cmake:  set(CMAKE_CXX_COMPILER "${CMAKE_CUDA_COMPILER}")
deps/cub/cmake/CubCompilerHacks.cmake:  set(CMAKE_CUDA_FLAGS "${CMAKE_CUDA_FLAGS} -stdpar")
deps/cub/cmake/CubCompilerHacks.cmake:  set(CMAKE_CUDA_HOST_LINK_LAUNCHER "${CMAKE_CUDA_COMPILER}")
deps/cub/cmake/CubCompilerHacks.cmake:  set(CMAKE_CUDA_LINK_EXECUTABLE
deps/cub/cmake/CubCompilerHacks.cmake:    "<CMAKE_CUDA_HOST_LINK_LAUNCHER> <FLAGS> <LINK_FLAGS> <OBJECTS> -o <TARGET> <LINK_LIBRARIES>")
deps/cub/cmake/CubCompilerHacks.cmake:# We don't set CMAKE_CUDA_HOST_COMPILER for NVC++; if we do, CMake tries to
deps/cub/cmake/CubCompilerHacks.cmake:# pass `-ccbin ${CMAKE_CUDA_HOST_COMPILER}` to NVC++, which it doesn't
deps/cub/cmake/CubCompilerHacks.cmake:if ((NOT "NVCXX" STREQUAL "${CMAKE_CUDA_COMPILER_ID}"))
deps/cub/cmake/CubCompilerHacks.cmake:  if (NOT ("${CMAKE_CUDA_HOST_COMPILER}" STREQUAL "" OR
deps/cub/cmake/CubCompilerHacks.cmake:    "${CMAKE_CUDA_HOST_COMPILER}" STREQUAL "${CMAKE_CXX_COMPILER}"))
deps/cub/cmake/CubCompilerHacks.cmake:    set(tmp "${CMAKE_CUDA_HOST_COMPILER}")
deps/cub/cmake/CubCompilerHacks.cmake:    unset(CMAKE_CUDA_HOST_COMPILER CACHE)
deps/cub/cmake/CubCompilerHacks.cmake:      "CUDA host compiler. Refusing to overwrite specified "
deps/cub/cmake/CubCompilerHacks.cmake:      "CMAKE_CUDA_HOST_COMPILER -- please reconfigure without setting this "
deps/cub/cmake/CubCompilerHacks.cmake:      "CMAKE_CUDA_HOST_COMPILER=${tmp}"
deps/cub/cmake/CubCompilerHacks.cmake:  set(CMAKE_CUDA_HOST_COMPILER "${CMAKE_CXX_COMPILER}")
deps/cub/cmake/CubCompilerHacks.cmake:# `CMAKE_CUDA_COMPILER_ID=NVCXX` and `CMAKE_CUDA_COMPILER_FORCED=ON`.
deps/cub/cmake/CubCompilerHacks.cmake:if ("NVCXX" STREQUAL "${CMAKE_CUDA_COMPILER_ID}")
deps/cub/cmake/CubCompilerHacks.cmake:  set(CMAKE_CUDA_STANDARD_DEFAULT 03)
deps/cub/cmake/CubCompilerHacks.cmake:  set(CMAKE_CUDA03_STANDARD_COMPILE_OPTION "-std=c++03")
deps/cub/cmake/CubCompilerHacks.cmake:  set(CMAKE_CUDA03_EXTENSION_COMPILE_OPTION "-std=c++03")
deps/cub/cmake/CubCompilerHacks.cmake:  set(CMAKE_CUDA03_STANDARD__HAS_FULL_SUPPORT TRUE)
deps/cub/cmake/CubCompilerHacks.cmake:  set_property(GLOBAL PROPERTY CMAKE_CUDA03_KNOWN_FEATURES)
deps/cub/cmake/CubCompilerHacks.cmake:  set(CMAKE_CUDA11_STANDARD_COMPILE_OPTION "-std=c++11")
deps/cub/cmake/CubCompilerHacks.cmake:  set(CMAKE_CUDA11_EXTENSION_COMPILE_OPTION "-std=c++11")
deps/cub/cmake/CubCompilerHacks.cmake:  set(CMAKE_CUDA11_STANDARD__HAS_FULL_SUPPORT TRUE)
deps/cub/cmake/CubCompilerHacks.cmake:  set_property(GLOBAL PROPERTY CMAKE_CUDA11_KNOWN_FEATURES)
deps/cub/cmake/CubCompilerHacks.cmake:  set(CMAKE_CUDA14_STANDARD_COMPILE_OPTION "-std=c++14")
deps/cub/cmake/CubCompilerHacks.cmake:  set(CMAKE_CUDA14_EXTENSION_COMPILE_OPTION "-std=c++14")
deps/cub/cmake/CubCompilerHacks.cmake:  set(CMAKE_CUDA14_STANDARD__HAS_FULL_SUPPORT TRUE)
deps/cub/cmake/CubCompilerHacks.cmake:  set_property(GLOBAL PROPERTY CMAKE_CUDA14_KNOWN_FEATURES)
deps/cub/cmake/CubCompilerHacks.cmake:  set(CMAKE_CUDA17_STANDARD_COMPILE_OPTION "-std=c++17")
deps/cub/cmake/CubCompilerHacks.cmake:  set(CMAKE_CUDA17_EXTENSION_COMPILE_OPTION "-std=c++17")
deps/cub/cmake/CubCompilerHacks.cmake:  set(CMAKE_CUDA17_STANDARD__HAS_FULL_SUPPORT TRUE)
deps/cub/cmake/CubCompilerHacks.cmake:  set_property(GLOBAL PROPERTY CMAKE_CUDA17_KNOWN_FEATURES)
deps/cub/cmake/CubCompilerHacks.cmake:  cmake_record_cuda_compile_features()
deps/cub/cmake/CubCompilerHacks.cmake:  set(CMAKE_CUDA_COMPILE_FEATURES
deps/cub/cmake/CubCompilerHacks.cmake:    ${CMAKE_CUDA03_COMPILE_FEATURES}
deps/cub/cmake/CubCompilerHacks.cmake:    ${CMAKE_CUDA11_COMPILE_FEATURES}
deps/cub/cmake/CubCompilerHacks.cmake:    ${CMAKE_CUDA14_COMPILE_FEATURES}
deps/cub/cmake/CubCompilerHacks.cmake:    ${CMAKE_CUDA17_COMPILE_FEATURES}
deps/cub/cmake/CubCompilerHacks.cmake:    ${CMAKE_CUDA20_COMPILE_FEATURES}
deps/cub/cmake/CubUtilities.cmake:# Enable RDC for a CUDA target. Encapsulates compiler hacks:
deps/cub/cmake/CubUtilities.cmake:function(cub_enable_rdc_for_cuda_target target_name)
deps/cub/cmake/CubUtilities.cmake:  if ("NVCXX" STREQUAL "${CMAKE_CUDA_COMPILER_ID}")
deps/cub/cmake/CubUtilities.cmake:      COMPILE_FLAGS "-gpu=rdc"
deps/cub/cmake/CubUtilities.cmake:      CUDA_SEPARABLE_COMPILATION ON
deps/cub/cmake/CubBuildCompilerTargets.cmake:    # The CUDA `host_runtime.h` header emits this for
deps/cub/cmake/CubBuildCompilerTargets.cmake:    # `__cudaUnregisterBinaryUtil`.
deps/cub/cmake/CubBuildCompilerTargets.cmake:    # This complains about functions in CUDA system headers when used with nvcc.
deps/cub/cmake/CubBuildCompilerTargets.cmake:    # CUB uses deprecated texture functions (cudaBindTexture, etc). These
deps/cub/cmake/CubBuildCompilerTargets.cmake:    # This can be removed once NVIDIA/cub#191 is fixed.
deps/cub/cmake/CubBuildCompilerTargets.cmake:  if ("NVCXX" STREQUAL "${CMAKE_CUDA_COMPILER_ID}")
deps/cub/cmake/CubBuildCompilerTargets.cmake:    # * NVCC accepts CUDA C++ in .cu files but not .cpp files.
deps/cub/cmake/CubBuildCompilerTargets.cmake:    # * NVC++ accepts CUDA C++ in .cpp files but not .cu files.
deps/cub/cmake/CubBuildCompilerTargets.cmake:      $<$<COMPILE_LANG_AND_ID:CUDA,NVCXX>:${cxx_option}>
deps/cub/cmake/CubBuildCompilerTargets.cmake:      # if (using CUDA and CUDA_COMPILER is NVCC) add -Xcompiler=opt:
deps/cub/cmake/CubBuildCompilerTargets.cmake:      $<$<COMPILE_LANG_AND_ID:CUDA,NVIDIA>:-Xcompiler=${cxx_option}>
deps/cub/cmake/CubBuildCompilerTargets.cmake:  # Add these for both CUDA and CXX targets:
deps/cub/cmake/CubBuildCompilerTargets.cmake:    # If using CUDA w/ NVCC...
deps/cub/cmake/CubBuildCompilerTargets.cmake:    $<$<COMPILE_LANG_AND_ID:CUDA,NVIDIA>:-Xcudafe=--display_error_number>
deps/cub/cmake/CubBuildCompilerTargets.cmake:    $<$<COMPILE_LANG_AND_ID:CUDA,NVIDIA>:-Xcudafe=--promote_warnings>
deps/cub/cmake/CubBuildCompilerTargets.cmake:    # Don't complain about deprecated GPU targets.
deps/cub/cmake/CubBuildCompilerTargets.cmake:    $<$<COMPILE_LANG_AND_ID:CUDA,NVIDIA>:-Wno-deprecated-gpu-targets>
deps/cub/cmake/CubBuildCompilerTargets.cmake:    # TexRefInputIterator uses deprecated CUDART APIs, see NVIDIA/cub#191.
deps/cub/cmake/CubBuildCompilerTargets.cmake:    $<$<AND:$<COMPILE_LANG_AND_ID:CUDA,NVIDIA>,$<VERSION_LESS:$<CUDA_COMPILER_VERSION>,11.5>>:-Wno-deprecated-declarations>
deps/cub/examples/block/example_block_radix_sort.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/examples/block/example_block_radix_sort.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/examples/block/example_block_radix_sort.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/examples/block/example_block_radix_sort.cu: *   nvcc -arch=sm_XX example_block_radix_sort.cu -I../.. -lcudart -O3
deps/cub/examples/block/example_block_radix_sort.cu:// Ensure printing of CUDA runtime errors to console (define before including cub.h)
deps/cub/examples/block/example_block_radix_sort.cu:    CubDebugExit(cudaMalloc((void**)&d_in,          sizeof(Key) * TILE_SIZE * g_grid_size));
deps/cub/examples/block/example_block_radix_sort.cu:    CubDebugExit(cudaMalloc((void**)&d_out,         sizeof(Key) * TILE_SIZE * g_grid_size));
deps/cub/examples/block/example_block_radix_sort.cu:    CubDebugExit(cudaMalloc((void**)&d_elapsed,     sizeof(clock_t) * g_grid_size));
deps/cub/examples/block/example_block_radix_sort.cu:    CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(Key) * TILE_SIZE * g_grid_size, cudaMemcpyHostToDevice));
deps/cub/examples/block/example_block_radix_sort.cu:    CubDebugExit(cudaPeekAtLastError());
deps/cub/examples/block/example_block_radix_sort.cu:    CubDebugExit(cudaDeviceSynchronize());
deps/cub/examples/block/example_block_radix_sort.cu:    GpuTimer            timer;
deps/cub/examples/block/example_block_radix_sort.cu:        CubDebugExit(cudaMemcpy(h_elapsed, d_elapsed, sizeof(clock_t) * g_grid_size, cudaMemcpyDeviceToHost));
deps/cub/examples/block/example_block_radix_sort.cu:    CubDebugExit(cudaDeviceSynchronize());
deps/cub/examples/block/example_block_radix_sort.cu:    if (d_in) CubDebugExit(cudaFree(d_in));
deps/cub/examples/block/example_block_radix_sort.cu:    if (d_out) CubDebugExit(cudaFree(d_out));
deps/cub/examples/block/example_block_radix_sort.cu:    if (d_elapsed) CubDebugExit(cudaFree(d_elapsed));
deps/cub/examples/block/example_block_reduce_dyn_smem.cu: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/cub/examples/block/example_block_reduce_dyn_smem.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/examples/block/example_block_reduce_dyn_smem.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/examples/block/example_block_reduce_dyn_smem.cu: *   nvcc -arch=sm_XX example_block_reduce_dyn_smem.cu -I../.. -lcudart -O3 -std=c++14
deps/cub/examples/block/example_block_reduce_dyn_smem.cu:// Ensure printing of CUDA runtime errors to console (define before including cub.h)
deps/cub/examples/block/example_block_reduce_dyn_smem.cu:    cudaMalloc((void**)&d_in,          sizeof(int) * BLOCK_THREADS);
deps/cub/examples/block/example_block_reduce_dyn_smem.cu:    cudaMalloc((void**)&d_out,         sizeof(int) * BLOCK_THREADS);
deps/cub/examples/block/example_block_reduce_dyn_smem.cu:    cudaMemcpy(d_in, h_in, sizeof(int) * BLOCK_THREADS, cudaMemcpyHostToDevice);
deps/cub/examples/block/example_block_reduce_dyn_smem.cu:    cudaStream_t stream = NULL;
deps/cub/examples/block/example_block_reduce_dyn_smem.cu:    CubDebugExit(cudaPeekAtLastError());
deps/cub/examples/block/example_block_reduce_dyn_smem.cu:    CubDebugExit(cudaDeviceSynchronize());
deps/cub/examples/block/example_block_reduce_dyn_smem.cu:    if (d_in) cudaFree(d_in);
deps/cub/examples/block/example_block_reduce_dyn_smem.cu:    if (d_out) cudaFree(d_out);
deps/cub/examples/block/.gitignore:/cuda55.sdf
deps/cub/examples/block/.gitignore:/cuda55.suo
deps/cub/examples/block/.gitignore:/cuda60.sdf
deps/cub/examples/block/.gitignore:/cuda60.suo
deps/cub/examples/block/example_block_reduce.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/examples/block/example_block_reduce.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/examples/block/example_block_reduce.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/examples/block/example_block_reduce.cu: *   nvcc -arch=sm_XX example_block_reduce.cu -I../.. -lcudart -O3
deps/cub/examples/block/example_block_reduce.cu:// Ensure printing of CUDA runtime errors to console (define before including cub.h)
deps/cub/examples/block/example_block_reduce.cu:    int *h_gpu          = new int[TILE_SIZE + 1];
deps/cub/examples/block/example_block_reduce.cu:    cudaMalloc((void**)&d_in,          sizeof(int) * TILE_SIZE);
deps/cub/examples/block/example_block_reduce.cu:    cudaMalloc((void**)&d_out,         sizeof(int) * 1);
deps/cub/examples/block/example_block_reduce.cu:    cudaMalloc((void**)&d_elapsed,     sizeof(clock_t));
deps/cub/examples/block/example_block_reduce.cu:    cudaMemcpy(d_in, h_in, sizeof(int) * TILE_SIZE, cudaMemcpyHostToDevice);
deps/cub/examples/block/example_block_reduce.cu:    GpuTimer    timer;
deps/cub/examples/block/example_block_reduce.cu:        cudaMemcpy(d_in, h_in, sizeof(int) * TILE_SIZE, cudaMemcpyHostToDevice);
deps/cub/examples/block/example_block_reduce.cu:        CubDebugExit(cudaMemcpy(&clocks, d_elapsed, sizeof(clock_t), cudaMemcpyDeviceToHost));
deps/cub/examples/block/example_block_reduce.cu:    CubDebugExit(cudaPeekAtLastError());
deps/cub/examples/block/example_block_reduce.cu:    CubDebugExit(cudaDeviceSynchronize());
deps/cub/examples/block/example_block_reduce.cu:    if (h_gpu) delete[] h_gpu;
deps/cub/examples/block/example_block_reduce.cu:    if (d_in) cudaFree(d_in);
deps/cub/examples/block/example_block_reduce.cu:    if (d_out) cudaFree(d_out);
deps/cub/examples/block/example_block_reduce.cu:    if (d_elapsed) cudaFree(d_elapsed);
deps/cub/examples/block/example_block_scan.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/examples/block/example_block_scan.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/examples/block/example_block_scan.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/examples/block/example_block_scan.cu: *   nvcc -arch=sm_XX example_block_scan.cu -I../.. -lcudart -O3
deps/cub/examples/block/example_block_scan.cu:// Ensure printing of CUDA runtime errors to console (define before including cub.h)
deps/cub/examples/block/example_block_scan.cu:    int *h_gpu          = new int[TILE_SIZE + 1];
deps/cub/examples/block/example_block_scan.cu:    cudaMalloc((void**)&d_in,          sizeof(int) * TILE_SIZE);
deps/cub/examples/block/example_block_scan.cu:    cudaMalloc((void**)&d_out,         sizeof(int) * (TILE_SIZE + 1));
deps/cub/examples/block/example_block_scan.cu:    cudaMalloc((void**)&d_elapsed,     sizeof(clock_t));
deps/cub/examples/block/example_block_scan.cu:    cudaMemcpy(d_in, h_in, sizeof(int) * TILE_SIZE, cudaMemcpyHostToDevice);
deps/cub/examples/block/example_block_scan.cu:    GpuTimer    timer;
deps/cub/examples/block/example_block_scan.cu:        cudaMemcpy(d_in, h_in, sizeof(int) * TILE_SIZE, cudaMemcpyHostToDevice);
deps/cub/examples/block/example_block_scan.cu:        CubDebugExit(cudaMemcpy(&clocks, d_elapsed, sizeof(clock_t), cudaMemcpyDeviceToHost));
deps/cub/examples/block/example_block_scan.cu:    CubDebugExit(cudaPeekAtLastError());
deps/cub/examples/block/example_block_scan.cu:    CubDebugExit(cudaDeviceSynchronize());
deps/cub/examples/block/example_block_scan.cu:    if (h_gpu) delete[] h_gpu;
deps/cub/examples/block/example_block_scan.cu:    if (d_in) cudaFree(d_in);
deps/cub/examples/block/example_block_scan.cu:    if (d_out) cudaFree(d_out);
deps/cub/examples/block/example_block_scan.cu:    if (d_elapsed) cudaFree(d_elapsed);
deps/cub/examples/device/example_device_select_unique.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/examples/device/example_device_select_unique.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/examples/device/example_device_select_unique.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/examples/device/example_device_select_unique.cu: *   nvcc -arch=sm_XX example_device_select_unique.cu -I../.. -lcudart -O3
deps/cub/examples/device/example_device_select_unique.cu:// Ensure printing of CUDA runtime errors to console
deps/cub/examples/device/example_device_select_unique.cu:    CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(int) * num_items, cudaMemcpyHostToDevice));
deps/cub/examples/device/example_device_select_if.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/examples/device/example_device_select_if.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/examples/device/example_device_select_if.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/examples/device/example_device_select_if.cu: *   nvcc -arch=sm_XX example_device_select_if.cu -I../.. -lcudart -O3
deps/cub/examples/device/example_device_select_if.cu:// Ensure printing of CUDA runtime errors to console
deps/cub/examples/device/example_device_select_if.cu:    CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(int) * num_items, cudaMemcpyHostToDevice));
deps/cub/examples/device/example_device_partition_if.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/examples/device/example_device_partition_if.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/examples/device/example_device_partition_if.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/examples/device/example_device_partition_if.cu: *   nvcc -arch=sm_XX example_device_select_if.cu -I../.. -lcudart -O3
deps/cub/examples/device/example_device_partition_if.cu:// Ensure printing of CUDA runtime errors to console
deps/cub/examples/device/example_device_partition_if.cu:    CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(int) * num_items, cudaMemcpyHostToDevice));
deps/cub/examples/device/example_device_reduce.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/examples/device/example_device_reduce.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/examples/device/example_device_reduce.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/examples/device/example_device_reduce.cu: *   nvcc -arch=sm_XX example_device_reduce.cu -I../.. -lcudart -O3
deps/cub/examples/device/example_device_reduce.cu:// Ensure printing of CUDA runtime errors to console
deps/cub/examples/device/example_device_reduce.cu:    CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(int) * num_items, cudaMemcpyHostToDevice));
deps/cub/examples/device/example_device_radix_sort.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/examples/device/example_device_radix_sort.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/examples/device/example_device_radix_sort.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/examples/device/example_device_radix_sort.cu: *   nvcc -arch=sm_XX example_device_radix_sort.cu -I../.. -lcudart -O3
deps/cub/examples/device/example_device_radix_sort.cu:// Ensure printing of CUDA runtime errors to console
deps/cub/examples/device/example_device_radix_sort.cu:    CubDebugExit(cudaMemcpy(d_keys.d_buffers[d_keys.selector], h_keys, sizeof(float) * num_items, cudaMemcpyHostToDevice));
deps/cub/examples/device/example_device_radix_sort.cu:    CubDebugExit(cudaMemcpy(d_values.d_buffers[d_values.selector], h_values, sizeof(int) * num_items, cudaMemcpyHostToDevice));
deps/cub/examples/device/example_device_scan.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/examples/device/example_device_scan.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/examples/device/example_device_scan.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/examples/device/example_device_scan.cu: *   nvcc -arch=sm_XX example_device_scan.cu -I../.. -lcudart -O3
deps/cub/examples/device/example_device_scan.cu:// Ensure printing of CUDA runtime errors to console
deps/cub/examples/device/example_device_scan.cu:    CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(int) * num_items, cudaMemcpyHostToDevice));
deps/cub/examples/device/example_device_partition_flagged.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/examples/device/example_device_partition_flagged.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/examples/device/example_device_partition_flagged.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/examples/device/example_device_partition_flagged.cu: *   nvcc -arch=sm_XX example_device_partition_flagged.cu -I../.. -lcudart -O3
deps/cub/examples/device/example_device_partition_flagged.cu:// Ensure printing of CUDA runtime errors to console
deps/cub/examples/device/example_device_partition_flagged.cu:    CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(int) * num_items, cudaMemcpyHostToDevice));
deps/cub/examples/device/example_device_partition_flagged.cu:    CubDebugExit(cudaMemcpy(d_flags, h_flags, sizeof(unsigned char) * num_items, cudaMemcpyHostToDevice));
deps/cub/examples/device/example_device_select_flagged.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/examples/device/example_device_select_flagged.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/examples/device/example_device_select_flagged.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/examples/device/example_device_select_flagged.cu: *   nvcc -arch=sm_XX example_device_select_flagged.cu -I../.. -lcudart -O3
deps/cub/examples/device/example_device_select_flagged.cu:// Ensure printing of CUDA runtime errors to console
deps/cub/examples/device/example_device_select_flagged.cu:    CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(int) * num_items, cudaMemcpyHostToDevice));
deps/cub/examples/device/example_device_select_flagged.cu:    CubDebugExit(cudaMemcpy(d_flags, h_flags, sizeof(unsigned char) * num_items, cudaMemcpyHostToDevice));
deps/cub/examples/device/example_device_sort_find_non_trivial_runs.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/cub/examples/device/example_device_sort_find_non_trivial_runs.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/cub/examples/device/example_device_sort_find_non_trivial_runs.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/cub/examples/device/example_device_sort_find_non_trivial_runs.cu: *   nvcc -arch=sm_XX example_device_sort_find_non_trivial_runs.cu -I../.. -lcudart -O3
deps/cub/examples/device/example_device_sort_find_non_trivial_runs.cu:// Ensure printing of CUDA runtime errors to console
deps/cub/examples/device/example_device_sort_find_non_trivial_runs.cu:    GpuTimer gpu_timer;
deps/cub/examples/device/example_device_sort_find_non_trivial_runs.cu:    GpuTimer gpu_rle_timer;
deps/cub/examples/device/example_device_sort_find_non_trivial_runs.cu:        CubDebugExit(cudaMemcpy(d_keys.d_buffers[d_keys.selector], h_keys, sizeof(float) * num_items, cudaMemcpyHostToDevice));
deps/cub/examples/device/example_device_sort_find_non_trivial_runs.cu:        CubDebugExit(cudaMemcpy(d_values.d_buffers[d_values.selector], h_values, sizeof(int) * num_items, cudaMemcpyHostToDevice));
deps/cub/examples/device/example_device_sort_find_non_trivial_runs.cu:        gpu_timer.Start();
deps/cub/examples/device/example_device_sort_find_non_trivial_runs.cu:        gpu_rle_timer.Start();
deps/cub/examples/device/example_device_sort_find_non_trivial_runs.cu:        gpu_timer.Stop();
deps/cub/examples/device/example_device_sort_find_non_trivial_runs.cu:        gpu_rle_timer.Stop();
deps/cub/examples/device/example_device_sort_find_non_trivial_runs.cu:            elapsed_millis += gpu_timer.ElapsedMillis();
deps/cub/examples/device/example_device_sort_find_non_trivial_runs.cu:            elapsed_rle_millis += gpu_rle_timer.ElapsedMillis();
deps/cub/examples/device/example_device_sort_find_non_trivial_runs.cu:        // GPU cleanup
deps/cub/examples/device/.gitignore:/cuda55.sdf
deps/cub/examples/device/.gitignore:/cuda55.suo
deps/cub/examples/device/.gitignore:/cuda60.sdf
deps/cub/examples/device/.gitignore:/cuda60.suo
deps/cub/examples/CMakeLists.txt:#   instance, examples/vector.cu will be "vector", and examples/cuda/copy.cu
deps/cub/examples/CMakeLists.txt:#   would be "cuda.copy".
deps/cub/examples/CMakeLists.txt:    cub_enable_rdc_for_cuda_target(${example_target})
deps/cub/examples/cmake/CMakeLists.txt:    -D "CMAKE_CUDA_COMPILER=${CMAKE_CUDA_COMPILER}"
deps/cub/examples/cmake/add_subdir/CMakeLists.txt:# Silence warnings about empty CUDA_ARCHITECTURES properties on example targets:
deps/cub/examples/cmake/add_subdir/CMakeLists.txt:project(CubAddSubDirExample CUDA)
deps/cub/CONTRIBUTING.md:[CONTRIBUTING.md](https://github.com/NVIDIA/thrust/blob/main/CONTRIBUTING.md).
deps/cub/CONTRIBUTING.md:  - Controls the targeted CUDA architecture(s)
deps/cub/CONTRIBUTING.md:  - Multiple options may be selected when using NVCC as the CUDA compiler.
deps/cub/CONTRIBUTING.md:  - If enabled, CUDA objects will target the most recent virtual architecture
deps/cub/CONTRIBUTING.md:[here](https://github.com/NVIDIA/thrust/blob/main/CONTRIBUTING.md#development-model).
deps/thrust/ci/local/build.bash:# Copyright (c) 2018-2020 NVIDIA Corporation
deps/thrust/ci/local/build.bash:  echo "Build and test your local repository using a gpuCI Docker image."
deps/thrust/ci/local/build.bash:  echo "-d, --disable-gpus"
deps/thrust/ci/local/build.bash:  echo "  Don't start the container with the NVIDIA runtime and GPUs attached."
deps/thrust/ci/local/build.bash:IMAGE="gpuci/cccl:cuda11.3.1-devel-ubuntu20.04-gcc7"
deps/thrust/ci/local/build.bash:BUILD_TYPE="gpu"
deps/thrust/ci/local/build.bash:  --disable-gpus) BUILD_TYPE="cpu" ;;
deps/thrust/ci/local/build.bash:# failure on Debian: https://github.com/NVIDIA/nvidia-docker/issues/1399
deps/thrust/ci/local/build.bash:# GPU - Setup GPUs.
deps/thrust/ci/local/build.bash:if [[ "${BUILD_TYPE}" == "gpu" ]]; then
deps/thrust/ci/local/build.bash:  # Limit GPUs available to the container based on ${CUDA_VISIBLE_DEVICES}.
deps/thrust/ci/local/build.bash:  if [[ -z "${CUDA_VISIBLE_DEVICES}" ]]; then
deps/thrust/ci/local/build.bash:    VISIBLE_DEVICES="${CUDA_VISIBLE_DEVICES}"
deps/thrust/ci/local/build.bash:  GPU_OPTS="--gpus device=${VISIBLE_DEVICES}"
deps/thrust/ci/local/build.bash:    GPU_OPTS="--runtime=nvidia -e NVIDIA_VISIBLE_DEVICES='${VISIBLE_DEVICES}'"
deps/thrust/ci/local/build.bash:NVIDIA_DOCKER_INSTALLED=$(docker info 2>&1 | grep -i runtime | grep -c nvidia)
deps/thrust/ci/local/build.bash:if [[ "${NVIDIA_DOCKER_INSTALLED}" == 0 ]]; then
deps/thrust/ci/local/build.bash:  echo "NVIDIA Docker not found, please install it: https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#installing-docker-ce"
deps/thrust/ci/local/build.bash:docker run --rm -it ${GPU_OPTS} \
deps/thrust/ci/axis/cpu.yml:# Copyright (c) 2018-2020 NVIDIA Corporation
deps/thrust/ci/axis/cpu.yml:  - cuda
deps/thrust/ci/axis/cpu.yml:  - 21.5-devel-cuda11.3
deps/thrust/ci/axis/cpu.yml:    SDK_TYPE: cuda
deps/thrust/ci/axis/cpu.yml:  - SDK_TYPE: cuda
deps/thrust/ci/axis/cpu.yml:    SDK_VER: 21.5-devel-cuda11.3
deps/thrust/ci/axis/gpu.yml:# Copyright (c) 2018-2020 NVIDIA Corporation
deps/thrust/ci/axis/gpu.yml:  - cuda
deps/thrust/ci/cpu/build.bash:# Copyright (c) 2018-2020 NVIDIA Corporation
deps/thrust/ci/cpu/build.bash:# Thrust and CUB build script for gpuCI (CPU-only)
deps/thrust/ci/common/determine_build_parallelism.bash:# Copyright (c) 2018-2020 NVIDIA Corporation
deps/thrust/ci/common/build.bash:# Copyright (c) 2018-2020 NVIDIA Corporation
deps/thrust/ci/common/build.bash:# Thrust and CUB build script for gpuCI
deps/thrust/ci/common/build.bash:# Prints out ${args[*]} with a gpuCI log prefix and a newline before and after.
deps/thrust/ci/common/build.bash:# Get the variables the Docker container set up for us: ${CXX}, ${CUDACXX}, etc.
deps/thrust/ci/common/build.bash:export PATH=/usr/local/cuda/bin:${PATH}
deps/thrust/ci/common/build.bash:# The Docker image sets up `${CXX}` and `${CUDACXX}`.
deps/thrust/ci/common/build.bash:append CMAKE_FLAGS "-DCMAKE_CUDA_COMPILER='${CUDACXX}'"
deps/thrust/ci/common/build.bash:  append CMAKE_FLAGS "-DCMAKE_CUDA_COMPILER_FORCED=ON"
deps/thrust/ci/common/build.bash:  append CMAKE_FLAGS "-DCMAKE_CUDA_COMPILER_ID=NVCXX"
deps/thrust/ci/common/build.bash:  # We use NVC++ "slim" image which only contain a single CUDA toolkit version.
deps/thrust/ci/common/build.bash:  # When using NVC++ in an environment without GPUs (like our CPU-only
deps/thrust/ci/common/build.bash:  # builders) it unfortunately defaults to the oldest CUDA toolkit version it
deps/thrust/ci/common/build.bash:  # explicitly tell NVC++ it which CUDA toolkit version to use.
deps/thrust/ci/common/build.bash:  CUDA_VER=$(echo ${SDK_VER} | sed 's/.*\(cuda[0-9]\+\.[0-9]\+\)/\1/')
deps/thrust/ci/common/build.bash:  append CMAKE_FLAGS "-DCMAKE_CUDA_FLAGS=-gpu=${CUDA_VER}"
deps/thrust/ci/common/build.bash:    append CMAKE_FLAGS "-DCMAKE_CUDA_FLAGS=-allow-unsupported-compiler"
deps/thrust/ci/common/build.bash:  # in gpuCI if one was not set.
deps/thrust/ci/common/build.bash:  elif [[ "${BUILD_TYPE}" == "gpu" ]]; then
deps/thrust/ci/common/build.bash:    # Pre- and post-commit GPU CI builds.
deps/thrust/ci/common/build.bash:    append CMAKE_FLAGS "-DTHRUST_MULTICONFIG_ENABLE_SYSTEM_CUDA=ON"
deps/thrust/ci/common/build.bash:    append CMAKE_FLAGS "-DTHRUST_MULTICONFIG_ENABLE_SYSTEM_CUDA=ON"
deps/thrust/ci/common/build.bash:    append CMAKE_FLAGS "-DTHRUST_MULTICONFIG_ENABLE_SYSTEM_CUDA=ON"
deps/thrust/ci/common/build.bash:      # If no GPU is automatically detected, NVC++ insists that you explicitly
deps/thrust/ci/common/build.bash:  CTEST_EXCLUSION_REGEXES+=("^cub" "^thrust.*cuda")
deps/thrust/ci/common/build.bash:${CUDACXX} --version 2>&1 | sed -Ez '$ s/\n*$/\n/'
deps/thrust/ci/common/build.bash:if [[ "${BUILD_TYPE}" == "gpu" ]]; then
deps/thrust/ci/common/build.bash:  nvidia-smi 2>&1 | sed -Ez '$ s/\n*$/\n/'
deps/thrust/ci/gpu/build.bash:# Copyright (c) 2018-2020 NVIDIA Corporation
deps/thrust/ci/gpu/build.bash:# Thrust and CUB build script for gpuCI (heterogeneous)
deps/thrust/internal/rename_cub_namespace.sh:# Run this in //sw/gpgpu/thrust/thrust/system/cuda/detail/cub to add a THRUST_
deps/thrust/internal/test/unittest.lst:TestAdjacentDifferenceCudaStreams
deps/thrust/internal/test/unittest.lst:TestAllOfCudaStreams
deps/thrust/internal/test/unittest.lst:TestAnyOfCudaStreams
deps/thrust/internal/test/unittest.lst:TestCountCudaStreams
deps/thrust/internal/test/unittest.lst:TestCudaMallocResultAligned
deps/thrust/internal/test/unittest.lst:TestCudaReduceIntervals
deps/thrust/internal/test/unittest.lst:TestCudaReduceIntervalsSimple
deps/thrust/internal/test/unittest.lst:TestEqualCudaStreams
deps/thrust/internal/test/unittest.lst:TestExclusiveScanByKeyCudaStreams
deps/thrust/internal/test/unittest.lst:TestFillCudaStreams
deps/thrust/internal/test/unittest.lst:TestFindCudaStreams
deps/thrust/internal/test/unittest.lst:TestForEachCudaStreams
deps/thrust/internal/test/unittest.lst:TestGatherCudaStreams
deps/thrust/internal/test/unittest.lst:TestGatherIfCudaStreams
deps/thrust/internal/test/unittest.lst:TestGenerateCudaStreams
deps/thrust/internal/test/unittest.lst:TestGenerateNCudaStreams
deps/thrust/internal/test/unittest.lst:TestInclusiveScanByKeyCudaStreams
deps/thrust/internal/test/unittest.lst:TestInnerProductCudaStreams
deps/thrust/internal/test/unittest.lst:TestIsPartitionedCudaStreams
deps/thrust/internal/test/unittest.lst:TestIsSortedCudaStreams
deps/thrust/internal/test/unittest.lst:TestIsSortedUntilCudaStreams
deps/thrust/internal/test/unittest.lst:TestMaxElementCudaStreams
deps/thrust/internal/test/unittest.lst:TestMergeByKeyCudaStreams
deps/thrust/internal/test/unittest.lst:TestMergeCudaStreams
deps/thrust/internal/test/unittest.lst:TestMinElementCudaStreams
deps/thrust/internal/test/unittest.lst:TestMinMaxElementCudaStreams
deps/thrust/internal/test/unittest.lst:TestMismatchCudaStreams
deps/thrust/internal/test/unittest.lst:TestNoneOfCudaStreams
deps/thrust/internal/test/unittest.lst:TestPartitionCudaStreams
deps/thrust/internal/test/unittest.lst:TestPartitionPointCudaStreams
deps/thrust/internal/test/unittest.lst:TestReduceByKeyCudaStreams
deps/thrust/internal/test/unittest.lst:TestReduceCudaStreams
deps/thrust/internal/test/unittest.lst:TestRemoveCopyCudaStreams
deps/thrust/internal/test/unittest.lst:TestRemoveCopyIfCudaStreams
deps/thrust/internal/test/unittest.lst:TestRemoveCopyIfStencilCudaStreams
deps/thrust/internal/test/unittest.lst:TestRemoveCudaStreams
deps/thrust/internal/test/unittest.lst:TestRemoveIfCudaStreams
deps/thrust/internal/test/unittest.lst:TestRemoveIfStencilCudaStreams
deps/thrust/internal/test/unittest.lst:TestReplaceCudaStreams
deps/thrust/internal/test/unittest.lst:TestReverseCudaStreams
deps/thrust/internal/test/unittest.lst:TestScanCudaStreams
deps/thrust/internal/test/unittest.lst:TestScatterCudaStreams
deps/thrust/internal/test/unittest.lst:TestScatterIfCudaStreams
deps/thrust/internal/test/unittest.lst:TestSelectSystemCudaToCpp
deps/thrust/internal/test/unittest.lst:TestSequenceCudaStreams
deps/thrust/internal/test/unittest.lst:TestSetDifferenceByKeyCudaStreams
deps/thrust/internal/test/unittest.lst:TestSetDifferenceCudaStreams
deps/thrust/internal/test/unittest.lst:TestSetIntersectionByKeyCudaStreams
deps/thrust/internal/test/unittest.lst:TestSetIntersectionCudaStreams
deps/thrust/internal/test/unittest.lst:TestSetSymmetricDifferenceByKeyCudaStreams
deps/thrust/internal/test/unittest.lst:TestSetSymmetricDifferenceCudaStreams
deps/thrust/internal/test/unittest.lst:TestSetUnionByKeyCudaStreams
deps/thrust/internal/test/unittest.lst:TestSetUnionCudaStreams
deps/thrust/internal/test/unittest.lst:TestSortByKeyCudaStreams
deps/thrust/internal/test/unittest.lst:TestSortCudaStreams
deps/thrust/internal/test/unittest.lst:TestSwapRangesCudaStreams
deps/thrust/internal/test/unittest.lst:TestTabulateCudaStreams
deps/thrust/internal/test/unittest.lst:TestTransformBinaryCudaStreams
deps/thrust/internal/test/unittest.lst:TestTransformReduceCudaStreams
deps/thrust/internal/test/unittest.lst:TestTransformScanCudaStreams
deps/thrust/internal/test/unittest.lst:TestTransformUnaryCudaStreams
deps/thrust/internal/test/unittest.lst:TestUninitializedCopyCudaStreams
deps/thrust/internal/test/unittest.lst:TestUninitializedCopyNCudaStreams
deps/thrust/internal/test/unittest.lst:TestUninitializedFillCudaStreams
deps/thrust/internal/test/unittest.lst:TestUninitializedFillNCudaStreams
deps/thrust/internal/test/unittest.lst:TestUniqueByKeyCudaStreams
deps/thrust/internal/test/unittest.lst:TestUniqueCopyByKeyCudaStreams
deps/thrust/internal/test/unittest.lst:TestUniqueCopyCudaStreams
deps/thrust/internal/test/unittest.lst:TestUniqueCudaStreams
deps/thrust/internal/test/warningstester.cu://#include "cuda_runtime_api.h"
deps/thrust/internal/test/thrust_nightly.pl:# Copyright (c) 2018 NVIDIA Corporation
deps/thrust/internal/test/thrust_nightly.pl:    printf("CUDA DVS BASIC SANITY SCORE : %.1f\n", $dvs_score);
deps/thrust/internal/scripts/refresh_from_github2.sh:git clone -q git://github.com/NVIDIA/thrust.git -b ${branch} /tmp/thrust-${branch}
deps/thrust/internal/scripts/eris_perf.py:# Copyright (c) 2018 NVIDIA Corporation
deps/thrust/internal/scripts/eris_perf.py:    "CUDA Eris driver script: runs a benchmark suite multiple times, combines "
deps/thrust/internal/scripts/eris_perf.py:    "the results, and outputs them in the CUDA Eris performance result format."
deps/thrust/internal/build/testframework.mk:CUSRC := unittest/cuda/testframework.cu
deps/thrust/internal/build/testframework.mk:$(CUSRC).CUDACC_FLAGS    := -I$(ROOTDIR)/thrust/testing/cuda/
deps/thrust/internal/build/common_build.mk:  CUDACC_FLAGS += -Xcompiler "/bigobj"
deps/thrust/internal/build/common_build.mk:      CUDACC_FLAGS += -Xcompiler "-mthumb"
deps/thrust/internal/build/common_build.mk:  # CUDA includes
deps/thrust/internal/build/common_build.mk:    INCLUDES_ABSPATH += $(VULCAN_INSTALL_DIR)/cuda/include
deps/thrust/internal/build/common_build.mk:    INCLUDES_ABSPATH += $(VULCAN_INSTALL_DIR)/cuda/_internal/cudart
deps/thrust/internal/build/common_build.mk:    INCLUDES_ABSPATH += $(ROOTDIR)/cuda/inc
deps/thrust/internal/build/common_build.mk:    INCLUDES_ABSPATH += $(ROOTDIR)/cuda/tools/cudart
deps/thrust/internal/build/common_build.mk:  # CUDA, CUB, and Thrust includes
deps/thrust/internal/build/common_build.mk:  INCLUDES_ABSPATH += $(GPGPU_COMPILER_EXPORT)/include
deps/thrust/internal/build/common_build.mk:    LIBDIRS_ABSPATH += $(GPGPU_COMPILER_EXPORT)/lib32
deps/thrust/internal/build/common_build.mk:    LIBDIRS_ABSPATH += $(GPGPU_COMPILER_EXPORT)/lib64
deps/thrust/internal/build/common_build.mk:USES_CUDA_DRIVER_HEADERS := 1
deps/thrust/internal/build/common_compiler.mk:    CUDACC_FLAGS += -Xcompiler "-Wall -Wextra -Werror"
deps/thrust/internal/build/common_compiler.mk:      CUDACC_FLAGS += -Xcompiler "-Wno-unused-parameter"
deps/thrust/internal/build/common_compiler.mk:      # that are defined as static inline in cuda_fp16.h. Disable this warning
deps/thrust/internal/build/common_compiler.mk:      CUDACC_FLAGS += -Xcompiler "-Wno-unused-function"
deps/thrust/internal/build/common_compiler.mk:      CUDACC_FLAGS += -Xcompiler "-Winit-self -Woverloaded-virtual -Wno-cast-align -Wcast-qual -Wno-long-long -Wno-variadic-macros -Wno-unused-function"
deps/thrust/internal/build/common_compiler.mk:        CUDACC_FLAGS += -Xcompiler "-Wno-unused-parameter"
deps/thrust/internal/build/common_compiler.mk:        CUDACC_FLAGS += -Xcompiler "-Wno-unneeded-internal-declaration"
deps/thrust/internal/build/common_compiler.mk:          CUDACC_FLAGS += -Xcompiler "-Wno-noexcept-type"
deps/thrust/internal/build/common_compiler.mk:            CUDACC_FLAGS += -DTHRUST_IGNORE_DEPRECATED_CPP_DIALECT
deps/thrust/internal/build/common_compiler.mk:            CUDACC_FLAGS += -Xcompiler "-Wno-noexcept-type"
deps/thrust/internal/build/common_compiler.mk:            CUDACC_FLAGS += -Xcompiler "-Wno-error=class-memaccess"
deps/thrust/internal/build/common_compiler.mk:  CUDACC_FLAGS += -Xcompiler "/WX"
deps/thrust/internal/build/common_compiler.mk:  CUDACC_FLAGS += -Xcompiler "/wd4244 /wd4267"
deps/thrust/internal/build/common_compiler.mk:  CUDACC_FLAGS += -Xcompiler "/wd4800"
deps/thrust/internal/build/common_compiler.mk:  CUDACC_FLAGS += -Xcompiler "/wd4146"
deps/thrust/internal/build/common_compiler.mk:  CUDACC_FLAGS += -Xcompiler "/wd4494"
deps/thrust/internal/build/common_compiler.mk:  CUDACC_FLAGS += -Xcompiler "/bigobj"
deps/thrust/internal/build/common_compiler.mk:CUDACC_FLAGS += -Werror all-warnings
deps/thrust/internal/build/common_compiler.mk:# Print warning numbers with cudafe diagnostics
deps/thrust/internal/build/common_compiler.mk:CUDACC_FLAGS += -Xcudafe --display_error_number
deps/thrust/internal/build/warningstester.mk:INCLUDES += $(VULCAN_INSTALL_DIR)/cuda/include
deps/thrust/internal/build/warningstester.mk:INCLUDES += $(VULCAN_INSTALL_DIR)/cuda/_internal/cudart
deps/thrust/internal/build/warningstester.mk:INCLUDES += ../../../cuda/tools/cudart
deps/thrust/internal/build/warningstester.mk:CUDACC_FLAGS += -I$(GENERATED_SOURCES)
deps/thrust/internal/build/warningstester.mk:	$(PYTHON) $(SRC_CWD)/warningstester_create_uber_header.py $(VULCAN_INSTALL_DIR)/cuda/targets/ppc64le-linux/include > $@
deps/thrust/internal/build/warningstester.mk:	$(PYTHON) $(SRC_CWD)/warningstester_create_uber_header.py $(VULCAN_INSTALL_DIR)/cuda/include > $@
deps/thrust/internal/reverse_rename_cub_namespace.sh:# Run this in //sw/gpgpu/thrust/thrust/system/cuda/detail/cub to undo the
deps/thrust/internal/benchmark/compare_benchmark_results.py:# Copyright (c) 2018 NVIDIA Corporation
deps/thrust/internal/benchmark/CMakeLists.txt:  # Skip non cpp.cuda targets:
deps/thrust/internal/benchmark/CMakeLists.txt:      NOT config_device STREQUAL "CUDA")
deps/thrust/internal/benchmark/combine_benchmark_results.py:# Copyright (c) 2018 NVIDIA Corporation
deps/thrust/internal/benchmark/bench.cu:#if THRUST_DEVICE_SYSTEM == THRUST_DEVICE_SYSTEM_CUDA
deps/thrust/internal/benchmark/bench.cu:  #include <thrust/system/cuda/error.h> // For `thrust::cuda_category`
deps/thrust/internal/benchmark/bench.cu:      #if THRUST_DEVICE_SYSTEM == THRUST_DEVICE_SYSTEM_CUDA
deps/thrust/internal/benchmark/bench.cu:        cudaError_t err = cudaDeviceSynchronize();
deps/thrust/internal/benchmark/bench.cu:        if (err != cudaSuccess)
deps/thrust/internal/benchmark/bench.cu:          throw thrust::error_code(err, thrust::cuda_category());
deps/thrust/internal/benchmark/bench.cu:      #if THRUST_DEVICE_SYSTEM == THRUST_DEVICE_SYSTEM_CUDA
deps/thrust/internal/benchmark/bench.cu:        cudaError_t err = cudaDeviceSynchronize();
deps/thrust/internal/benchmark/bench.cu:        if (err != cudaSuccess)
deps/thrust/internal/benchmark/bench.cu:          throw thrust::error_code(err, thrust::cuda_category());
deps/thrust/internal/benchmark/bench.cu:      #if THRUST_DEVICE_SYSTEM == THRUST_DEVICE_SYSTEM_CUDA
deps/thrust/internal/benchmark/bench.cu:        cudaError_t err = cudaDeviceSynchronize();
deps/thrust/internal/benchmark/bench.cu:        if (err != cudaSuccess)
deps/thrust/internal/benchmark/bench.cu:          throw thrust::error_code(err, thrust::cuda_category());
deps/thrust/internal/benchmark/bench.cu:      #if THRUST_DEVICE_SYSTEM == THRUST_DEVICE_SYSTEM_CUDA
deps/thrust/internal/benchmark/bench.cu:        cudaError_t err = cudaDeviceSynchronize();
deps/thrust/internal/benchmark/bench.cu:        if (err != cudaSuccess)
deps/thrust/internal/benchmark/bench.cu:          throw thrust::error_code(err, thrust::cuda_category());
deps/thrust/internal/benchmark/bench.cu:      #if THRUST_DEVICE_SYSTEM == THRUST_DEVICE_SYSTEM_CUDA
deps/thrust/internal/benchmark/bench.cu:        cudaError_t err = cudaDeviceSynchronize();
deps/thrust/internal/benchmark/bench.cu:        if (err != cudaSuccess)
deps/thrust/internal/benchmark/bench.cu:          throw thrust::error_code(err, thrust::cuda_category());
deps/thrust/internal/benchmark/bench.cu:  #if THRUST_DEVICE_SYSTEM == THRUST_DEVICE_SYSTEM_CUDA
deps/thrust/internal/benchmark/bench.cu:    // Set the CUDA device to use for the benchmark - `0` by default.
deps/thrust/internal/benchmark/bench.cu:    cudaSetDevice(device);
deps/thrust/internal/benchmark/timer.h:#  define CUDA_SAFE_CALL_NO_SYNC( call) do {                                 \
deps/thrust/internal/benchmark/timer.h:    cudaError err = call;                                                    \
deps/thrust/internal/benchmark/timer.h:    if( cudaSuccess != err) {                                                \
deps/thrust/internal/benchmark/timer.h:        fprintf(stderr, "CUDA error in file '%s' in line %i : %s.\n",        \
deps/thrust/internal/benchmark/timer.h:                __FILE__, __LINE__, cudaGetErrorString( err) );              \
deps/thrust/internal/benchmark/timer.h:#  define CUDA_SAFE_CALL( call) do {                                         \
deps/thrust/internal/benchmark/timer.h:    CUDA_SAFE_CALL_NO_SYNC(call);                                            \
deps/thrust/internal/benchmark/timer.h:    cudaError err = cudaDeviceSynchronize();                                 \
deps/thrust/internal/benchmark/timer.h:    if( cudaSuccess != err) {                                                \
deps/thrust/internal/benchmark/timer.h:        fprintf(stderr, "CUDA error in file '%s' in line %i : %s.\n",        \
deps/thrust/internal/benchmark/timer.h:                __FILE__, __LINE__, cudaGetErrorString( err) );              \
deps/thrust/internal/benchmark/timer.h:class cuda_timer
deps/thrust/internal/benchmark/timer.h:    cudaEvent_t start_;
deps/thrust/internal/benchmark/timer.h:    cudaEvent_t stop_;
deps/thrust/internal/benchmark/timer.h:    cuda_timer()
deps/thrust/internal/benchmark/timer.h:        CUDA_SAFE_CALL(cudaEventCreate(&start_));
deps/thrust/internal/benchmark/timer.h:        CUDA_SAFE_CALL(cudaEventCreate(&stop_));
deps/thrust/internal/benchmark/timer.h:    ~cuda_timer()
deps/thrust/internal/benchmark/timer.h:        CUDA_SAFE_CALL(cudaEventDestroy(start_));
deps/thrust/internal/benchmark/timer.h:        CUDA_SAFE_CALL(cudaEventDestroy(stop_));
deps/thrust/internal/benchmark/timer.h:        CUDA_SAFE_CALL(cudaEventRecord(start_, 0));
deps/thrust/internal/benchmark/timer.h:        CUDA_SAFE_CALL(cudaEventRecord(stop_, 0));
deps/thrust/internal/benchmark/timer.h:        CUDA_SAFE_CALL(cudaEventSynchronize(stop_));
deps/thrust/internal/benchmark/timer.h:        CUDA_SAFE_CALL(cudaEventElapsedTime(&elapsed_time, start_, stop_));
deps/thrust/internal/racecheck.sh:MEMCHECK=/work/nightly/memcheck/bin/x86_64_Linux_release/cuda-memcheck 
deps/thrust/generate_mk.py:tests_cu,  dependencies_cu  = generate_test_mk(mk_path, "testing/cuda/", "test.cuda", REL_DIR)
deps/thrust/generate_mk.py:examples_cuda = generate_example_mk(mk_path, "examples/cuda/", "example.cuda", REL_DIR)
deps/thrust/generate_mk.py:examples_all.extend(examples_cuda)
deps/thrust/CHANGELOG.md:**A future version of Thrust will remove support for CUDA Dynamic Parallelism
deps/thrust/CHANGELOG.md:This will only affect calls to Thrust algorithms made from CUDA device-side code
deps/thrust/CHANGELOG.md:on the calling GPU thread instead of launching a device-wide kernel.
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1507: Allow `thrust::sequence` to work with non-numeric types.
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1509: Avoid macro collision when calling `max()` on MSVC. Thanks
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1514: Initialize all members in `counting_iterator`'s default
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1518: Fix `std::allocator_traits` on MSVC + C++17.
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1530: Fix several `-Wconversion` warnings. Thanks to Matt
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1539: Fixed typo in `thrust::for_each` documentation. Thanks to
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1548: Avoid name collision with `B0` macro in termios.h system
deps/thrust/CHANGELOG.md:# Thrust 1.14.0 (NVIDIA HPC SDK 21.9)
deps/thrust/CHANGELOG.md:Thrust 1.14.0 is a major release accompanying the NVIDIA HPC SDK 21.9.
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1464: Add preprocessor hooks that allow `thrust::` to be wrapped
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1457: Support cv-qualified types in `thrust::tuple_size` and
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1471: Fixed excessive memory allocation in `scan_by_key`. Thanks
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1476: Removed dead code from the `expand` example. Thanks to
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1488: Fixed the path to the installed CUB headers in the CMake
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1491: Fallback to `std::iterator_traits` when no
deps/thrust/CHANGELOG.md:# Thrust 1.13.1 (CUDA Toolkit 11.5)
deps/thrust/CHANGELOG.md:Thrust 1.13.1 is a minor release accompanying the CUDA Toolkit 11.5.
deps/thrust/CHANGELOG.md:those reported in NVIDIA/thrust#1401.
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1464: Add `THRUST_CUB_WRAPPED_NAMESPACE` hooks.
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1488: Fix path to installed CUB in Thrust's CMake config files.
deps/thrust/CHANGELOG.md:# Thrust 1.13.0 (NVIDIA HPC SDK 21.7)
deps/thrust/CHANGELOG.md:Thrust 1.13.0 is the major release accompanying the NVIDIA HPC SDK 21.7 release.
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1459: Remove deprecated aliases `thrust::host_space_tag` and
deps/thrust/CHANGELOG.md:- NVIDIA/cub#306: Add radix-sort support for `bfloat16` in `thrust::sort`.
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1423: `thrust::transform_iterator` now supports non-copyable
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1459: Introduce a new `THRUST_IGNORE_DEPRECATED_API` macro that
deps/thrust/CHANGELOG.md:- NVIDIA/cub#277: Fixed sanitizer warnings when `thrust::sort` calls
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1442: Reduce extraneous comparisons in `thrust::sort`'s merge
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1447: Fix memory leak and avoid overallocation when
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1405: Update links to standard C++ documentations from sgi to
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1432: Updated build instructions in `CONTRIBUTING.md` to include
deps/thrust/CHANGELOG.md:# Thrust 1.12.1 (CUDA Toolkit 11.4)
deps/thrust/CHANGELOG.md:# Thrust 1.12.0 (NVIDIA HPC SDK 21.3)
deps/thrust/CHANGELOG.md:Thrust 1.12.0 is the major release accompanying the NVIDIA HPC SDK 21.3
deps/thrust/CHANGELOG.md:and the CUDA Toolkit 11.4.
deps/thrust/CHANGELOG.md:CUDA's unified memory with Thrust.
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1372: Deprecate Clang < 7 and MSVC < 2019.
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1376: Standardize `thrust::scan_by_key` functors / accumulator
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1251: Add two new `thrust::async::` algorithms: `inclusive_scan`
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1334: Add `thrust::universal_vector`, `universal_ptr`,
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1347: Qualify calls to `make_reverse_iterator`.
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1359: Enable stricter warning flags. This fixes several
deps/thrust/CHANGELOG.md:  - NVIDIA/cub#221: Overflow in `temp_storage_bytes` when `num_items` close to
deps/thrust/CHANGELOG.md:  - NVIDIA/cub#228: CUB uses non-standard C++ extensions that break strict
deps/thrust/CHANGELOG.md:  - NVIDIA/cub#257: Warning when compiling `GridEvenShare` with unsigned
deps/thrust/CHANGELOG.md:  - NVIDIA/thrust#974: Conversion warnings in `thrust::transform_reduce`.
deps/thrust/CHANGELOG.md:  - NVIDIA/thrust#1091: Conversion warnings in `thrust::counting_iterator`.
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1373: Fix compilation error when a standard library type is
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1388: Fix `signbit(double)` implementation on MSVC.
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1389: Support building Thrust tests without CUDA enabled.
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1304: Use `cub::DeviceScan` to implement
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1362, NVIDIA/thrust#1370: Update smoke test naming.
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1380: Fix typos in `set_operation` documentation. Thanks to
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1383: Include FreeBSD license in LICENSE.md for
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1384: Add missing precondition to `thrust::gather`
deps/thrust/CHANGELOG.md:# Thrust 1.11.0 (CUDA Toolkit 11.3)
deps/thrust/CHANGELOG.md:- NVIDIA/cub#204: New implementation for `thrust::sort` on CUDA when using
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1310, NVIDIA/thrust#1312: Various tuple-related APIs have been
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1297: Optionally add install rules when included with
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1309: Fix `thrust::shuffle` to produce better quality random
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1337: Fix compile-time regression in `transform_inclusive_scan`
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1306: Fix binary search `middle` calculation to avoid overflows.
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1314: Use `size_t` for the index type parameter
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1329: Fix runtime error when copying an
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1323: Fix and add test for cmake package install rules. Thanks
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1338: Fix GCC version checks in `thrust::detail::is_pod`
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1289: Partial fixes for Clang 10 as host/c++ compiler. Exposed
deps/thrust/CHANGELOG.md:  an nvcc bug that will be fixed in a future version of the CUDA Toolkit (NVBug
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1272: Fix ambiguous `iter_swap` call when
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1281: Update our bundled `FindTBB.cmake` module to support
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1298: Use semantic versioning rules for our CMake package's
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1300: Use `FindPackageHandleStandardArgs` to print standard
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1320: Use feature-testing instead of a language dialect check
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1319: Suppress GPU deprecation warnings.
deps/thrust/CHANGELOG.md:- NVIDIA/cub#213: Removed some tuning policies for unsupported hardware (<SM35).
deps/thrust/CHANGELOG.md:  - Github's `thrust/cub` repository is now `NVIDIA/cub`
deps/thrust/CHANGELOG.md:# Thrust 1.10.0 (NVIDIA HPC SDK 20.9, CUDA Toolkit 11.2)
deps/thrust/CHANGELOG.md:Thrust 1.10.0 is the major release accompanying the NVIDIA HPC SDK 20.9 release
deps/thrust/CHANGELOG.md:  and the CUDA Toolkit 11.2 release.
deps/thrust/CHANGELOG.md:https://github.com/NVIDIA/thrust/blob/main/CODE_OF_CONDUCT.md
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1159: CMake multi-config support, which allows multiple
deps/thrust/CHANGELOG.md:  More details can be found here: https://github.com/NVIDIA/thrust/blob/main/CONTRIBUTING.md#multi-config-cmake-options
deps/thrust/CHANGELOG.md:      with the Thrust source root (see NVIDIA/thrust#976).
deps/thrust/CHANGELOG.md:      https://github.com/NVIDIA/thrust/blob/main/examples/cmake/add_subdir/CMakeLists.txt
deps/thrust/CHANGELOG.md:    Logic that modified `CMAKE_CXX_STANDARD` and `CMAKE_CUDA_STANDARD` has been
deps/thrust/CHANGELOG.md:  - CUDA configuration CMake code has been moved to to `cmake/ThrustCUDAConfig.cmake`.
deps/thrust/CHANGELOG.md:- Contributor documentation: https://github.com/NVIDIA/thrust/blob/main/CONTRIBUTING.md
deps/thrust/CHANGELOG.md:- Code of Conduct: https://github.com/NVIDIA/thrust/blob/main/CODE_OF_CONDUCT.md.
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1221: Allocator and vector classes have been replaced with
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1186: Use placeholder expressions to simplify the definitions
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1170: More conforming semantics for scan algorithms:
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1255: Always use `cudaStreamSynchronize` instead of
deps/thrust/CHANGELOG.md:    `cudaDeviceSynchronize` if the execution policy has a stream attached to it.
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1201: Tests for correct handling of legacy and per-thread
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1260: Fix `thrust::transform_inclusive_scan` with heterogeneous
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1258, NVC++ FS #28463: Ensure the CUDA radix sort backend
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1264: Evaluate `CUDA_CUB_RET_IF_FAIL` macro argument only once.
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1262: Add missing `<stdexcept>` header.
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1250: Restore some `THRUST_DECLTYPE_RETURNS` macros in async
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1249: Use `std::iota` in `CUDATestDriver::target_devices`.
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1244: Check for macro collisions with system headers during
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1224: Remove unnecessary SFINAE contexts from asynchronous
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1190: Make `out_of_memory_recovery` test trigger faster.
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1187: Elminate superfluous iterators specific to the CUDA
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1181: Various fixes for GoUDA.
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1178, NVIDIA/thrust#1229: Use transparent functionals in
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1153: Switch to placement new instead of assignment to
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1050: Fix compilation of asynchronous algorithms when RDC is
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1042: Correct return type of
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#1009: Avoid returning uninitialized allocators.
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#990: Add missing `<thrust/system/cuda/memory.h>` include to
deps/thrust/CHANGELOG.md:    `<thrust/system/cuda/detail/malloc_and_free.h>`.
deps/thrust/CHANGELOG.md:- NVIDIA/thrust#966: Fix spurious MSVC conversion with loss of data warning in
deps/thrust/CHANGELOG.md:- Add missing header caught by `tbb.cuda` configs.
deps/thrust/CHANGELOG.md:# Thrust 1.9.10-1 (NVIDIA HPC SDK 20.7, CUDA Toolkit 11.1)
deps/thrust/CHANGELOG.md:Thrust 1.9.10-1 is the minor release accompanying the NVIDIA HPC SDK 20.7 release
deps/thrust/CHANGELOG.md:  and the CUDA Toolkit 11.1 release.
deps/thrust/CHANGELOG.md:# Thrust 1.9.10 (NVIDIA HPC SDK 20.5)
deps/thrust/CHANGELOG.md:Thrust 1.9.10 is the release accompanying the NVIDIA HPC SDK 20.5 release.
deps/thrust/CHANGELOG.md:    migrating away from the legacy `FindThrust.cmake` [here](https://github.com/NVIDIA/thrust/blob/main/thrust/cmake/README.md)
deps/thrust/CHANGELOG.md:    which gives a 2x speedup on large multi-GPU systems such as V100 and A100
deps/thrust/CHANGELOG.md:    DGX where `cudaMalloc` is very slow.
deps/thrust/CHANGELOG.md:- #1128: Respect `CUDA_API_PER_THREAD_DEFAULT_STREAM`.
deps/thrust/CHANGELOG.md:# Thrust 1.9.9 (CUDA Toolkit 11.0)
deps/thrust/CHANGELOG.md:  GPU-accelerated C++17 Parallel Algorithms.
deps/thrust/CHANGELOG.md:  The most significant change is in how we use `__CUDA_ARCH__`.
deps/thrust/CHANGELOG.md:- #1068: `thrust::system::cuda::managed_memory_pointer`, a universal memory
deps/thrust/CHANGELOG.md:- #1077: Remove `__device__` from CUDA MR-based device allocators to fix
deps/thrust/CHANGELOG.md:- #1103: Fix regression of `thrust::detail::temporary_allocator` with non-CUDA
deps/thrust/CHANGELOG.md:# Thrust 1.9.8-1 (NVIDIA HPC SDK 20.3)
deps/thrust/CHANGELOG.md:Thrust 1.9.8-1 is a variant of 1.9.8 accompanying the NVIDIA HPC SDK 20.3
deps/thrust/CHANGELOG.md:  GPU-accelerated C++17 Parallel Algorithms when using the CUDA Toolkit 11.0
deps/thrust/CHANGELOG.md:# Thrust 1.9.8 (CUDA Toolkit 11.0 Early Access)
deps/thrust/CHANGELOG.md:Thrust 1.9.8, which is included in the CUDA Toolkit 11.0 release, removes
deps/thrust/CHANGELOG.md:Additionally, CUB is now included as a first class citizen in the CUDA toolkit.
deps/thrust/CHANGELOG.md:- #1020: After making a CUDA API call, always clear the global CUDA error state
deps/thrust/CHANGELOG.md:    by calling `cudaGetLastError`.
deps/thrust/CHANGELOG.md:- #1046: Actually throw `thrust::bad_alloc` when `thrust::system::cuda::malloc`
deps/thrust/CHANGELOG.md:- Add missing move operations to `thrust::system::cuda::vector`.
deps/thrust/CHANGELOG.md:- #1015: Check that the backend is CUDA before using CUDA-specifics in
deps/thrust/CHANGELOG.md:- Remove unused functions from the CUDA backend which call slow CUDA attribute
deps/thrust/CHANGELOG.md:# Thrust 1.9.7-1 (CUDA Toolkit 10.2 for Tegra)
deps/thrust/CHANGELOG.md:Thrust 1.9.7-1 is a minor release accompanying the CUDA Toolkit 10.2 release
deps/thrust/CHANGELOG.md:# Thrust 1.9.7 (CUDA Toolkit 10.2)
deps/thrust/CHANGELOG.md:Thrust 1.9.7 is a minor release accompanying the CUDA Toolkit 10.2 release.
deps/thrust/CHANGELOG.md:  for stream acquisition in `thrust::future`) was not included in the CUDA
deps/thrust/CHANGELOG.md:The tag `cuda-10.2aarch64sbsa` contains the exact version of Thrust present
deps/thrust/CHANGELOG.md:  in the CUDA Toolkit 10.2 preview release for AArch64 SBSA.
deps/thrust/CHANGELOG.md:- #967, NVBug 2448170: Fix the CUDA backend `thrust::for_each` so that it
deps/thrust/CHANGELOG.md:  - Not present in the CUDA Toolkit 10.2 preview release for AArch64 SBSA.
deps/thrust/CHANGELOG.md:# Thrust 1.9.6-1 (NVIDIA HPC SDK 20.3)
deps/thrust/CHANGELOG.md:Thrust 1.9.6-1 is a variant of 1.9.6 accompanying the NVIDIA HPC SDK 20.3
deps/thrust/CHANGELOG.md:  GPU-accelerated C++17 Parallel Algorithms when using the CUDA Toolkit 10.1
deps/thrust/CHANGELOG.md:# Thrust 1.9.6 (CUDA Toolkit 10.1 Update 2)
deps/thrust/CHANGELOG.md:Thrust 1.9.6 is a minor release accompanying the CUDA Toolkit 10.1 Update 2
deps/thrust/CHANGELOG.md:- NVBug 200488234: CUDA header files contain Unicode characters which leads
deps/thrust/CHANGELOG.md:# Thrust 1.9.5 (CUDA Toolkit 10.1 Update 1)
deps/thrust/CHANGELOG.md:Thrust 1.9.5 is a minor release accompanying the CUDA Toolkit 10.1 Update 1
deps/thrust/CHANGELOG.md:# Thrust 1.9.4 (CUDA Toolkit 10.1)
deps/thrust/CHANGELOG.md:  - Currently, these primitives are only implemented for the CUDA backend and
deps/thrust/CHANGELOG.md:    - Asynchronous algorithms are currently only implemented for the CUDA
deps/thrust/CHANGELOG.md:  - The fast path is now enabled when copying CUDA `__half` and vector types with
deps/thrust/CHANGELOG.md:- All Thrust synchronous algorithms for the CUDA backend now actually
deps/thrust/CHANGELOG.md:    unlike `cudaMalloc`/`cudaFree`. So, now `thrust::for_each`,
deps/thrust/CHANGELOG.md:      accessed from both the host and device (e.g. `cudaMallocManaged`).
deps/thrust/CHANGELOG.md:      host memory (e.g. `cudaMallocHost`).
deps/thrust/CHANGELOG.md:  - The test driver now synchronizes with CUDA devices and check for errors
deps/thrust/CHANGELOG.md:      to disable CUDA-specific code with the preprocessor.
deps/thrust/CHANGELOG.md:  - `thrust::system_error` in the CUDA backend now print out its `cudaError_t`
deps/thrust/CHANGELOG.md:- #924, NVBug 2096679, NVBug 2315990: Fix dispatch for the CUDA backend's
deps/thrust/CHANGELOG.md:    a regression with device compilation that started in CUDA Toolkit 9.2.
deps/thrust/CHANGELOG.md:- NVBug 2289115: Remove flaky `simple_cuda_streams` example.
deps/thrust/CHANGELOG.md:# Thrust 1.9.3 (CUDA Toolkit 10.0)
deps/thrust/CHANGELOG.md:Thrust 1.9.3 unifies and integrates CUDA Thrust and GitHub Thrust.
deps/thrust/CHANGELOG.md:    refactor temporary memory allocation in the CUDA backend to be exception
deps/thrust/CHANGELOG.md:- #899: Make `thrust::cuda::experimental::pinned_allocator`'s comparison
deps/thrust/CHANGELOG.md:- NVBug 2092152: Remove all includes of `<cuda.h>`.
deps/thrust/CHANGELOG.md:# Thrust 1.9.2 (CUDA Toolkit 9.2)
deps/thrust/CHANGELOG.md:# Thrust 1.9.1-2 (CUDA Toolkit 9.1)
deps/thrust/CHANGELOG.md:Thrust 1.9.1-2 integrates version 1.7.4 of CUB and introduces a new CUDA backend
deps/thrust/CHANGELOG.md:# Thrust 1.9.0-5 (CUDA Toolkit 9.0)
deps/thrust/CHANGELOG.md:Thrust 1.9.0-5 replaces the original CUDA backend (bulk) with a new one
deps/thrust/CHANGELOG.md:  written using CUB, a high performance CUDA collectives library.
deps/thrust/CHANGELOG.md:This brings a substantial performance improvement to the CUDA backend across
deps/thrust/CHANGELOG.md:- Any code depending on CUDA backend implementation details will likely be
deps/thrust/CHANGELOG.md:- New CUDA backend based on CUB which delivers substantially higher performance.
deps/thrust/CHANGELOG.md:# Thrust 1.8.3 (CUDA Toolkit 8.0)
deps/thrust/CHANGELOG.md:# Thrust 1.8.2 (CUDA Toolkit 7.5)
deps/thrust/CHANGELOG.md:- #632: Fix an error in `thrust::set_intersection_by_key` with the CUDA backend.
deps/thrust/CHANGELOG.md:    with streams attached, i.e. `thrust::::cuda::par.on(stream)`.
deps/thrust/CHANGELOG.md:- #628: `thrust::reduce_by_key` for the CUDA backend fails for Compute
deps/thrust/CHANGELOG.md:# Thrust 1.8.1 (CUDA Toolkit 7.0)
deps/thrust/CHANGELOG.md:- #628: `thrust::reduce_by_key` for the CUDA backend fails for Compute
deps/thrust/CHANGELOG.md:Thrust 1.8.0 introduces support for algorithm invocation from CUDA device
deps/thrust/CHANGELOG.md:  code, support for CUDA streams, and algorithm performance improvements.
deps/thrust/CHANGELOG.md:Users may now invoke Thrust algorithms from CUDA device code, providing a
deps/thrust/CHANGELOG.md:  parallel algorithms library to CUDA programmers authoring custom kernels, as
deps/thrust/CHANGELOG.md:  available to individual CUDA threads.
deps/thrust/CHANGELOG.md:The `.on(stream)` syntax allows users to request a CUDA stream for kernels
deps/thrust/CHANGELOG.md:Finally, new CUDA algorithm implementations provide substantial performance
deps/thrust/CHANGELOG.md:- Algorithms in CUDA Device Code:
deps/thrust/CHANGELOG.md:    - Thrust algorithms may now be invoked from CUDA `__device__` and
deps/thrust/CHANGELOG.md:      The following execution policies are supported in CUDA __device__ code:
deps/thrust/CHANGELOG.md:      - `thrust::cuda::par`
deps/thrust/CHANGELOG.md:      - `thrust::device`, when THRUST_DEVICE_SYSTEM == THRUST_DEVICE_SYSTEM_CUDA.
deps/thrust/CHANGELOG.md:  - Device-side algorithm execution may not be parallelized unless CUDA Dynamic
deps/thrust/CHANGELOG.md:  - CUDA Streams
deps/thrust/CHANGELOG.md:    - The `thrust::cuda::par.on(stream)` syntax allows users to request that
deps/thrust/CHANGELOG.md:        CUDA kernels launched during algorithm execution should occur on a given
deps/thrust/CHANGELOG.md:    - Algorithms executed with a CUDA stream in this manner may still
deps/thrust/CHANGELOG.md:- simple_cuda_streams demonstrates how to request a CUDA stream during
deps/thrust/CHANGELOG.md:- CUDA sort performance for user-defined types is 300% faster on Tesla K20c for
deps/thrust/CHANGELOG.md:- CUDA merge performance is 200% faster on Tesla K20c for large problem sizes.
deps/thrust/CHANGELOG.md:- CUDA sort performance for primitive types is 50% faster on Tesla K20c for
deps/thrust/CHANGELOG.md:- CUDA reduce_by_key performance is 25% faster on Tesla K20c for large problem
deps/thrust/CHANGELOG.md:- CUDA scan performance is 15% faster on Tesla K20c for large problem sizes.
deps/thrust/CHANGELOG.md:- #371: Do not redefine `__CUDA_ARCH__`.
deps/thrust/CHANGELOG.md:- The CUDA implementation of thrust::reduce_by_key incorrectly outputs the last
deps/thrust/CHANGELOG.md:- Thanks to Sean Baxter for contributing faster CUDA reduce, merge, and scan
deps/thrust/CHANGELOG.md:- Thanks to Duane Merrill for contributing a faster CUDA radix sort implementation.
deps/thrust/CHANGELOG.md:# Thrust 1.7.2 (CUDA Toolkit 6.5)
deps/thrust/CHANGELOG.md:# Thrust 1.7.1 (CUDA Toolkit 6.0)
deps/thrust/CHANGELOG.md:- Eliminate unused variable warning in CUDA `reduce_by_key` implementation.
deps/thrust/CHANGELOG.md:# Thrust 1.7.0 (CUDA Toolkit 5.5)
deps/thrust/CHANGELOG.md:For 32b types, new CUDA merge and set operations provide 2-15x faster
deps/thrust/CHANGELOG.md:  performance while a new CUDA comparison sort provides 1.3-4x faster
deps/thrust/CHANGELOG.md:      thrust::cuda::execution_policy) instead of the tag struct (e.g.
deps/thrust/CHANGELOG.md:      thrust::cuda::tag). Otherwise, algorithm specializations will silently go
deps/thrust/CHANGELOG.md:      examples/cuda/fallback_allocator.cu for usage examples.
deps/thrust/CHANGELOG.md:    For example, instead of wrapping raw pointers allocated by cudaMalloc with
deps/thrust/CHANGELOG.md:      an argument to an algorithm invocation to enable CUDA execution.
deps/thrust/CHANGELOG.md:    - `thrust::cuda::par`
deps/thrust/CHANGELOG.md:- CUDA merge performance is 2-15x faster.
deps/thrust/CHANGELOG.md:- CUDA comparison sort performance is 1.3-4x faster.
deps/thrust/CHANGELOG.md:- CUDA set operation performance is 1.5-15x faster.
deps/thrust/CHANGELOG.md:- Simplified the cuda/custom_temporary_allocation example.
deps/thrust/CHANGELOG.md:- Simplified the cuda/fallback_allocator example.
deps/thrust/CHANGELOG.md:- #231, #209: Fix set operation failures with CUDA.
deps/thrust/CHANGELOG.md:- #187: Fix incorrect occupancy calculation with CUDA.
deps/thrust/CHANGELOG.md:- #153: Fix broken multi GPU behavior with CUDA.
deps/thrust/CHANGELOG.md:- #16: Fix compilation error when sorting bool with CUDA.
deps/thrust/CHANGELOG.md:    cuda/custom_temporary_allocation.
deps/thrust/CHANGELOG.md:    a faster merge implementation for CUDA.
deps/thrust/CHANGELOG.md:    for CUDA.
deps/thrust/CHANGELOG.md:  such as CUDA and OpenMP to coexist within a single program.
deps/thrust/CHANGELOG.md:- The header <thrust/experimental/cuda/pinned_allocator.h> has been moved to
deps/thrust/CHANGELOG.md:    <thrust/system/cuda/experimental/pinned_allocator.h>
deps/thrust/CHANGELOG.md:- thrust::experimental::cuda::pinned_allocator has been moved to
deps/thrust/CHANGELOG.md:    thrust::cuda::experimental::pinned_allocator
deps/thrust/CHANGELOG.md:- The macro THRUST_DEVICE_BACKEND_CUDA has been renamed THRUST_DEVICE_SYSTEM_CUDA
deps/thrust/CHANGELOG.md:- `cuda/custom_temporary_allocation`
deps/thrust/CHANGELOG.md:- `cuda/fallback_allocator`
deps/thrust/CHANGELOG.md:- All CUDA algorithms now support large data types.
deps/thrust/CHANGELOG.md:# Thrust 1.5.3 (CUDA Toolkit 5.0)
deps/thrust/CHANGELOG.md:# Thrust 1.5.2 (CUDA Toolkit 4.2)
deps/thrust/CHANGELOG.md:# Thrust 1.5.1 (CUDA Toolkit 4.1)
deps/thrust/CHANGELOG.md:- Sorting data referenced by permutation_iterators on CUDA produces invalid results
deps/thrust/CHANGELOG.md:A new CUDA `reduce_by_key` implementation provides 2-3x faster
deps/thrust/CHANGELOG.md:- CUDA scan algorithms are 10-40% faster
deps/thrust/CHANGELOG.md:- out-of-memory exceptions now provide detailed information from CUDART
deps/thrust/CHANGELOG.md:# Thrust 1.4.0 (CUDA Toolkit 4.0)
deps/thrust/CHANGELOG.md:Thrust 1.4.0 is the first release of Thrust to be included in the CUDA Toolkit.
deps/thrust/CHANGELOG.md:  - `thrust/experimental/cuda/ogl_interop_allocator.h` and the functionality
deps/thrust/CHANGELOG.md:      is CUDA.
deps/thrust/CHANGELOG.md:  - Compute Capability 2.1 GPUs.
deps/thrust/CHANGELOG.md:- Thanks to Nathan Whitehead for help with CUDA Toolkit integration.
deps/thrust/CHANGELOG.md:Thrust 1.3.0 provides support for CUDA Toolkit 3.2 in addition to many feature
deps/thrust/CHANGELOG.md:CUDA errors are now converted to runtime exceptions using the system_error
deps/thrust/CHANGELOG.md:  - `thrust::experimental::cuda::ogl_interop_allocator`.
deps/thrust/CHANGELOG.md:  - GF104-based GPUs.
deps/thrust/CHANGELOG.md:- Thanks to Duane Merrill for contributing a fast CUDA radix sort implementation
deps/thrust/CHANGELOG.md:Small fixes for compatibility for the CUDA Toolkit 3.1.
deps/thrust/CHANGELOG.md:Lastly, improvements to the robustness of the CUDA backend ensure correctness
deps/thrust/CHANGELOG.md:  - Fermi-class GPUs.
deps/thrust/CHANGELOG.md:- #51 thrust::experimental::arch functions gracefully handle unrecognized GPUs
deps/thrust/CHANGELOG.md:- #68 allow built-in CUDA vector types to work with device_vector in pure C++
deps/thrust/CHANGELOG.md:- Thanks to Gregory Diamos for contributing a CUDA implementation of
deps/thrust/CHANGELOG.md:Small fixes for compatibility with CUDA Toolkit 2.3a and Mac OSX Snow Leopard.
deps/thrust/CHANGELOG.md:- Scan and reduce use cudaFuncGetAttributes to determine grid size.
deps/thrust/CHANGELOG.md:    used with large types with the CUDA Toolkit 3.1.
deps/thrust/Makefile:# Copyright 2010-2020 NVIDIA Corporation.
deps/thrust/Makefile:  CREATE_DVS_PACKAGE = $(ZIP) -r built/CUDA-thrust-package.zip bin thrust/internal/test thrust/internal/scripts thrust/internal/benchmark $(DVS_COMMON_TEST_PACKAGE_FILES)
deps/thrust/Makefile:  APPEND_H_DVS_PACKAGE = $(ZIP) -rg built/CUDA-thrust-package.zip thrust -9 -i *.h
deps/thrust/Makefile:  APPEND_INL_DVS_PACKAGE = $(ZIP) -rg built/CUDA-thrust-package.zip thrust -9 -i *.inl
deps/thrust/Makefile:  APPEND_CUH_DVS_PACKAGE = $(ZIP) -rg built/CUDA-thrust-package.zip thrust -9 -i *.cuh
deps/thrust/Makefile:  MAKE_DVS_PACKAGE = tar -I lbzip2 -chvf built/CUDA-thrust-package.tar.bz2 $(TAR_FILES)
deps/thrust/Makefile:# Build the CUDA Runtime in GVS, because GVS has no CUDA Runtime component.
deps/thrust/Makefile:# This is a temporary workaround until the Tegra team adds a CUDA Runtime
deps/thrust/Makefile:	$(MAKE) $(DVS_OPTIONS) -s -C ../cuda $(THRUST_DVS_BUILD)
deps/thrust/doc/thrust.dox:                         "cuda_cub=system::cuda"
deps/thrust/CODE_OF_CONDUCT.md:This document defines the Code of Conduct followed and enforced for NVIDIA C++
deps/thrust/CODE_OF_CONDUCT.md:  reported by contacting [cpp-conduct@nvidia.com](mailto:cpp-conduct@nvidia.com).
deps/thrust/CODE_OF_CONDUCT.md:This Code of Conduct was taken from the [NVIDIA RAPIDS] project, which was
deps/thrust/CODE_OF_CONDUCT.md:Please email [cpp-conduct@nvidia.com] for any Code of Conduct related matters.
deps/thrust/CODE_OF_CONDUCT.md:[cpp-conduct@nvidia.com]: mailto:cpp-conduct@nvidia.com
deps/thrust/CODE_OF_CONDUCT.md:[NVIDIA RAPIDS]: https://docs.rapids.ai/resources/conduct/
deps/thrust/README.md:<a href='https://gpuci.gpuopenanalytics.com/job/nvidia/job/thrust/job/branch/job/thrust-cpu-build/CXX_TYPE=gcc,CXX_VER=9,OS_TYPE=ubuntu,OS_VER=20.04,SDK_TYPE=cuda,SDK_VER=11.3.1-devel'><img src='https://gpuci.gpuopenanalytics.com/job/nvidia/job/thrust/job/branch/job/thrust-cpu-build/CXX_TYPE=gcc,CXX_VER=9,OS_TYPE=ubuntu,OS_VER=20.04,SDK_TYPE=cuda,SDK_VER=11.3.1-devel/badge/icon'></a>
deps/thrust/README.md:GPUs and multicore CPUs. **Interoperability** with established technologies
deps/thrust/README.md:(such as CUDA, TBB, and OpenMP) facilitates integration with existing
deps/thrust/README.md:Thrust is included in the NVIDIA HPC SDK and the CUDA Toolkit.
deps/thrust/README.md:The CUDA Toolkit provides a recent release of the Thrust source code in
deps/thrust/README.md:git clone --recursive https://github.com/NVIDIA/thrust.git
deps/thrust/README.md:- The CUB include path, if using the CUDA device system (`-I<thrust repo root>/dependencies/cub/`)
deps/thrust/README.md:- By default, the CPP host system and CUDA device system are used.
deps/thrust/README.md:    `CPP`, `OMP`, `TBB`, or `CUDA` (default).
deps/thrust/README.md:- [Examples](https://github.com/NVIDIA/thrust/tree/main/examples)
deps/thrust/README.md:- [User Support](https://github.com/NVIDIA/thrust/discussions)
deps/thrust/README.md:<a href='https://gpuci.gpuopenanalytics.com/job/nvidia/job/thrust/job/branch/job/thrust-gpu-build/CXX_TYPE=gcc,CXX_VER=7,OS_TYPE=ubuntu,OS_VER=20.04,SDK_TYPE=cuda,SDK_VER=11.3.1-devel/'><img src='https://gpuci.gpuopenanalytics.com/job/nvidia/job/thrust/job/branch/job/thrust-gpu-build/CXX_TYPE=gcc,CXX_VER=7,OS_TYPE=ubuntu,OS_VER=20.04,SDK_TYPE=cuda,SDK_VER=11.3.1-devel/badge/icon?subject=NVCC%2011.3.1%20%2B%20GCC%207%20build%20and%20device%20tests'></a>
deps/thrust/README.md:<a href='https://gpuci.gpuopenanalytics.com/job/nvidia/job/thrust/job/branch/job/thrust-cpu-build/CXX_TYPE=gcc,CXX_VER=10,OS_TYPE=ubuntu,OS_VER=20.04,SDK_TYPE=cuda,SDK_VER=11.3.1-devel/'><img src='https://gpuci.gpuopenanalytics.com/job/nvidia/job/thrust/job/branch/job/thrust-cpu-build/CXX_TYPE=gcc,CXX_VER=10,OS_TYPE=ubuntu,OS_VER=20.04,SDK_TYPE=cuda,SDK_VER=11.3.1-devel/badge/icon?subject=NVCC%2011.3.1%20%2B%20GCC%2010%20build%20and%20host%20tests'></a>
deps/thrust/README.md:<a href='https://gpuci.gpuopenanalytics.com/job/nvidia/job/thrust/job/branch/job/thrust-cpu-build/CXX_TYPE=gcc,CXX_VER=9,OS_TYPE=ubuntu,OS_VER=20.04,SDK_TYPE=cuda,SDK_VER=11.3.1-devel/'><img src='https://gpuci.gpuopenanalytics.com/job/nvidia/job/thrust/job/branch/job/thrust-cpu-build/CXX_TYPE=gcc,CXX_VER=9,OS_TYPE=ubuntu,OS_VER=20.04,SDK_TYPE=cuda,SDK_VER=11.3.1-devel/badge/icon?subject=NVCC%2011.3.1%20%2B%20GCC%209%20build%20and%20host%20tests'></a>
deps/thrust/README.md:<a href='https://gpuci.gpuopenanalytics.com/job/nvidia/job/thrust/job/branch/job/thrust-cpu-build/CXX_TYPE=gcc,CXX_VER=8,OS_TYPE=ubuntu,OS_VER=20.04,SDK_TYPE=cuda,SDK_VER=11.3.1-devel/'><img src='https://gpuci.gpuopenanalytics.com/job/nvidia/job/thrust/job/branch/job/thrust-cpu-build/CXX_TYPE=gcc,CXX_VER=8,OS_TYPE=ubuntu,OS_VER=20.04,SDK_TYPE=cuda,SDK_VER=11.3.1-devel/badge/icon?subject=NVCC%2011.3.1%20%2B%20GCC%208%20build%20and%20host%20tests'></a>
deps/thrust/README.md:<a href='https://gpuci.gpuopenanalytics.com/job/nvidia/job/thrust/job/branch/job/thrust-cpu-build/CXX_TYPE=gcc,CXX_VER=7,OS_TYPE=ubuntu,OS_VER=20.04,SDK_TYPE=cuda,SDK_VER=11.3.1-devel/'><img src='https://gpuci.gpuopenanalytics.com/job/nvidia/job/thrust/job/branch/job/thrust-cpu-build/CXX_TYPE=gcc,CXX_VER=7,OS_TYPE=ubuntu,OS_VER=20.04,SDK_TYPE=cuda,SDK_VER=11.3.1-devel/badge/icon?subject=NVCC%2011.3.1%20%2B%20GCC%207%20build%20and%20host%20tests'></a>
deps/thrust/README.md:<a href='https://gpuci.gpuopenanalytics.com/job/nvidia/job/thrust/job/branch/job/thrust-cpu-build/CXX_TYPE=gcc,CXX_VER=6,OS_TYPE=ubuntu,OS_VER=20.04,SDK_TYPE=cuda,SDK_VER=11.3.1-devel/'><img src='https://gpuci.gpuopenanalytics.com/job/nvidia/job/thrust/job/branch/job/thrust-cpu-build/CXX_TYPE=gcc,CXX_VER=6,OS_TYPE=ubuntu,OS_VER=20.04,SDK_TYPE=cuda,SDK_VER=11.3.1-devel/badge/icon?subject=NVCC%2011.3.1%20%2B%20GCC%206%20build%20and%20host%20tests'></a>
deps/thrust/README.md:<a href='https://gpuci.gpuopenanalytics.com/job/nvidia/job/thrust/job/branch/job/thrust-cpu-build/CXX_TYPE=gcc,CXX_VER=5,OS_TYPE=ubuntu,OS_VER=20.04,SDK_TYPE=cuda,SDK_VER=11.3.1-devel/'><img src='https://gpuci.gpuopenanalytics.com/job/nvidia/job/thrust/job/branch/job/thrust-cpu-build/CXX_TYPE=gcc,CXX_VER=5,OS_TYPE=ubuntu,OS_VER=20.04,SDK_TYPE=cuda,SDK_VER=11.3.1-devel/badge/icon?subject=NVCC%2011.3.1%20%2B%20GCC%205%20build%20and%20host%20tests'></a>
deps/thrust/README.md:<a href='https://gpuci.gpuopenanalytics.com/job/nvidia/job/thrust/job/branch/job/thrust-cpu-build/CXX_TYPE=clang,CXX_VER=11,OS_TYPE=ubuntu,OS_VER=20.04,SDK_TYPE=cuda,SDK_VER=11.3.1-devel/'><img src='https://gpuci.gpuopenanalytics.com/job/nvidia/job/thrust/job/branch/job/thrust-cpu-build/CXX_TYPE=clang,CXX_VER=11,OS_TYPE=ubuntu,OS_VER=20.04,SDK_TYPE=cuda,SDK_VER=11.3.1-devel/badge/icon?subject=NVCC%2011.3.1%20%2B%20Clang%2011%20build%20and%20host%20tests'></a>
deps/thrust/README.md:<a href='https://gpuci.gpuopenanalytics.com/job/nvidia/job/thrust/job/branch/job/thrust-cpu-build/CXX_TYPE=clang,CXX_VER=10,OS_TYPE=ubuntu,OS_VER=20.04,SDK_TYPE=cuda,SDK_VER=11.3.1-devel/'><img src='https://gpuci.gpuopenanalytics.com/job/nvidia/job/thrust/job/branch/job/thrust-cpu-build/CXX_TYPE=clang,CXX_VER=10,OS_TYPE=ubuntu,OS_VER=20.04,SDK_TYPE=cuda,SDK_VER=11.3.1-devel/badge/icon?subject=NVCC%2011.3.1%20%2B%20Clang%2010%20build%20and%20host%20tests'></a>
deps/thrust/README.md:<a href='https://gpuci.gpuopenanalytics.com/job/nvidia/job/thrust/job/branch/job/thrust-cpu-build/CXX_TYPE=clang,CXX_VER=9,OS_TYPE=ubuntu,OS_VER=20.04,SDK_TYPE=cuda,SDK_VER=11.3.1-devel/'><img src='https://gpuci.gpuopenanalytics.com/job/nvidia/job/thrust/job/branch/job/thrust-cpu-build/CXX_TYPE=clang,CXX_VER=9,OS_TYPE=ubuntu,OS_VER=20.04,SDK_TYPE=cuda,SDK_VER=11.3.1-devel/badge/icon?subject=NVCC%2011.3.1%20%2B%20Clang%209%20build%20and%20host%20tests'></a>
deps/thrust/README.md:<a href='https://gpuci.gpuopenanalytics.com/job/nvidia/job/thrust/job/branch/job/thrust-cpu-build/CXX_TYPE=clang,CXX_VER=8,OS_TYPE=ubuntu,OS_VER=20.04,SDK_TYPE=cuda,SDK_VER=11.3.1-devel/'><img src='https://gpuci.gpuopenanalytics.com/job/nvidia/job/thrust/job/branch/job/thrust-cpu-build/CXX_TYPE=clang,CXX_VER=8,OS_TYPE=ubuntu,OS_VER=20.04,SDK_TYPE=cuda,SDK_VER=11.3.1-devel/badge/icon?subject=NVCC%2011.3.1%20%2B%20Clang%208%20build%20and%20host%20tests'></a>
deps/thrust/README.md:<a href='https://gpuci.gpuopenanalytics.com/job/nvidia/job/thrust/job/branch/job/thrust-cpu-build/CXX_TYPE=clang,CXX_VER=7,OS_TYPE=ubuntu,OS_VER=20.04,SDK_TYPE=cuda,SDK_VER=11.3.1-devel/'><img src='https://gpuci.gpuopenanalytics.com/job/nvidia/job/thrust/job/branch/job/thrust-cpu-build/CXX_TYPE=clang,CXX_VER=7,OS_TYPE=ubuntu,OS_VER=20.04,SDK_TYPE=cuda,SDK_VER=11.3.1-devel/badge/icon?subject=NVCC%2011.3.1%20%2B%20Clang%207%20build%20and%20host%20tests'></a>
deps/thrust/README.md:<a href='https://gpuci.gpuopenanalytics.com/job/nvidia/job/thrust/job/branch/job/thrust-cpu-build/CXX_TYPE=icc,CXX_VER=latest,OS_TYPE=ubuntu,OS_VER=20.04,SDK_TYPE=cuda,SDK_VER=11.3.1-devel/'><img src='https://gpuci.gpuopenanalytics.com/job/nvidia/job/thrust/job/branch/job/thrust-cpu-build/CXX_TYPE=icc,CXX_VER=latest,OS_TYPE=ubuntu,OS_VER=20.04,SDK_TYPE=cuda,SDK_VER=11.3.1-devel/badge/icon?subject=NVCC%2011.3.1%20%2B%20ICC%20build%20and%20host%20tests'></a>
deps/thrust/README.md:<a href='https://gpuci.gpuopenanalytics.com/job/nvidia/job/thrust/job/branch/job/thrust-cpu-build/CXX_TYPE=nvcxx,CXX_VER=21.5,OS_TYPE=ubuntu,OS_VER=20.04,SDK_TYPE=nvhpc,SDK_VER=21.5-devel-cuda11.3/'><img src='https://gpuci.gpuopenanalytics.com/job/nvidia/job/thrust/job/branch/job/thrust-cpu-build/CXX_TYPE=nvcxx,CXX_VER=21.5,OS_TYPE=ubuntu,OS_VER=20.04,SDK_TYPE=nvhpc,SDK_VER=21.5-devel-cuda11.3/badge/icon?subject=NVC%2B%2B%2021.5%20build%20and%20host%20tests'></a>
deps/thrust/README.md:Thrust is distributed with the NVIDIA HPC SDK and the CUDA Toolkit in addition
deps/thrust/README.md:| 1.14.0            | NVIDIA HPC SDK 21.9                     |
deps/thrust/README.md:| 1.13.1            | CUDA Toolkit 11.5                       |
deps/thrust/README.md:| 1.13.0            | NVIDIA HPC SDK 21.7                     |
deps/thrust/README.md:| 1.12.1            | CUDA Toolkit 11.4                       |
deps/thrust/README.md:| 1.12.0            | NVIDIA HPC SDK 21.3                     |
deps/thrust/README.md:| 1.11.0            | CUDA Toolkit 11.3                       |
deps/thrust/README.md:| 1.10.0            | NVIDIA HPC SDK 20.9 & CUDA Toolkit 11.2 |
deps/thrust/README.md:| 1.9.10-1          | NVIDIA HPC SDK 20.7 & CUDA Toolkit 11.1 |
deps/thrust/README.md:| 1.9.10            | NVIDIA HPC SDK 20.5                     |
deps/thrust/README.md:| 1.9.9             | CUDA Toolkit 11.0                       |
deps/thrust/README.md:| 1.9.8-1           | NVIDIA HPC SDK 20.3                     |
deps/thrust/README.md:| 1.9.8             | CUDA Toolkit 11.0 Early Access          |
deps/thrust/README.md:| 1.9.7-1           | CUDA Toolkit 10.2 for Tegra             |
deps/thrust/README.md:| 1.9.7             | CUDA Toolkit 10.2                       |
deps/thrust/README.md:| 1.9.6-1           | NVIDIA HPC SDK 20.3                     |
deps/thrust/README.md:| 1.9.6             | CUDA Toolkit 10.1 Update 2              |
deps/thrust/README.md:| 1.9.5             | CUDA Toolkit 10.1 Update 1              |
deps/thrust/README.md:| 1.9.4             | CUDA Toolkit 10.1                       |
deps/thrust/README.md:| 1.9.3             | CUDA Toolkit 10.0                       |
deps/thrust/README.md:| 1.9.2             | CUDA Toolkit 9.2                        |
deps/thrust/README.md:| 1.9.1-2           | CUDA Toolkit 9.1                        |
deps/thrust/README.md:| 1.9.0-5           | CUDA Toolkit 9.0                        |
deps/thrust/README.md:| 1.8.3             | CUDA Toolkit 8.0                        |
deps/thrust/README.md:| 1.8.2             | CUDA Toolkit 7.5                        |
deps/thrust/README.md:| 1.8.1             | CUDA Toolkit 7.0                        |
deps/thrust/README.md:| 1.7.2             | CUDA Toolkit 6.5                        |
deps/thrust/README.md:| 1.7.1             | CUDA Toolkit 6.0                        |
deps/thrust/README.md:| 1.7.0             | CUDA Toolkit 5.5                        |
deps/thrust/README.md:| 1.5.3             | CUDA Toolkit 5.0                        |
deps/thrust/README.md:| 1.5.2             | CUDA Toolkit 4.2                        |
deps/thrust/README.md:| 1.5.1             | CUDA Toolkit 4.1                        |
deps/thrust/README.md:| 1.4.0             | CUDA Toolkit 4.0                        |
deps/thrust/README.md:git clone --recursive https://github.com/NVIDIA/thrust.git
deps/thrust/README.md:By default, a serial `CPP` host system, `CUDA` accelerated device system, and
deps/thrust/testing/regression/nvbug_1965743__unnecessary_static_on_get_occ_device_properties.cu:#include <thrust/system/cuda/detail/core/util.h>
deps/thrust/testing/regression/gh_911__merge_by_key_wrong_element_type_default_comparator.cu:  // does NOT compile in CUDA 9.1 (compiles fine in CUDA 8)
deps/thrust/testing/inner_product.cu:{ // Regression test for NVIDIA/thrust#1178
deps/thrust/testing/zip_iterator_scan.cu:#if THRUST_DEVICE_SYSTEM == THRUST_DEVICE_SYSTEM_CUDA
deps/thrust/testing/zip_iterator_scan.cu:#include <unittest/cuda/testframework.h>
deps/thrust/testing/count.cu:    size_t gpu_result = thrust::count(d_data.begin(), d_data.end(), T(5));
deps/thrust/testing/count.cu:    ASSERT_EQUAL(cpu_result, gpu_result);
deps/thrust/testing/count.cu:    size_t gpu_result = thrust::count_if(d_data.begin(), d_data.end(), greater_than_five<T>());
deps/thrust/testing/count.cu:    ASSERT_EQUAL(cpu_result, gpu_result);
deps/thrust/testing/find.cu:{ // Regression test for NVIDIA/thrust#1229
deps/thrust/testing/complex_transform.cu:#if THRUST_DEVICE_SYSTEM == THRUST_DEVICE_SYSTEM_CUDA
deps/thrust/testing/complex_transform.cu:#include <unittest/cuda/testframework.h>
deps/thrust/testing/dereference.cu:#if THRUST_DEVICE_SYSTEM == THRUST_DEVICE_SYSTEM_CUDA
deps/thrust/testing/dereference.cu:#if THRUST_DEVICE_SYSTEM == THRUST_DEVICE_SYSTEM_CUDA
deps/thrust/testing/uninitialized_copy.cu:#if __CUDA_ARCH__
deps/thrust/testing/pair_scan.cu:#if THRUST_DEVICE_SYSTEM == THRUST_DEVICE_SYSTEM_CUDA
deps/thrust/testing/pair_scan.cu:#include <unittest/cuda/testframework.h>
deps/thrust/testing/async_sort.cu:#if (THRUST_HOST_COMPILER == THRUST_HOST_COMPILER_MSVC) && defined(__CUDACC__)
deps/thrust/testing/async_sort.cu:#if (__CUDACC_VER_MAJOR__ < 11) || (__CUDACC_VER_MAJOR__ == 11 && __CUDACC_VER_MINOR__ < 1)
deps/thrust/testing/allocator.cu:#if !__CUDA_ARCH__
deps/thrust/testing/transform_scan.cu:// Regression test for https://github.com/NVIDIA/thrust/issues/1332
deps/thrust/testing/vector.cu:#if defined(__CUDACC__) && CUDART_VERSION==3000
deps/thrust/testing/vector.cu:      // reset the CUDA error
deps/thrust/testing/vector.cu:      cudaGetLastError();
deps/thrust/testing/vector.cu:#endif // defined(__CUDACC__) && CUDART_VERSION==3000
deps/thrust/testing/vector.cu:#if defined(__CUDACC__) && CUDART_VERSION==3000
deps/thrust/testing/vector.cu:#endif // defined(__CUDACC__) && CUDART_VERSION==3000
deps/thrust/testing/uninitialized_fill.cu:#if __CUDA_ARCH__
deps/thrust/testing/CMakeLists.txt:# Update flags to reflect RDC options. See note in ThrustCudaConfig.cmake --
deps/thrust/testing/CMakeLists.txt:  set(CMAKE_CUDA_FLAGS "${THRUST_CUDA_FLAGS_BASE} ${THRUST_CUDA_FLAGS_RDC}")
deps/thrust/testing/CMakeLists.txt:  set(CMAKE_CUDA_FLAGS "${THRUST_CUDA_FLAGS_BASE} ${THRUST_CUDA_FLAGS_NO_RDC}")
deps/thrust/testing/CMakeLists.txt:# Async/future/event tests only support the CUDA backend:
deps/thrust/testing/CMakeLists.txt:thrust_declare_test_restrictions(async_copy        CPP.CUDA OMP.CUDA TBB.CUDA)
deps/thrust/testing/CMakeLists.txt:thrust_declare_test_restrictions(async_for_each    CPP.CUDA OMP.CUDA TBB.CUDA)
deps/thrust/testing/CMakeLists.txt:thrust_declare_test_restrictions(async_reduce      CPP.CUDA OMP.CUDA TBB.CUDA)
deps/thrust/testing/CMakeLists.txt:thrust_declare_test_restrictions(async_reduce_into CPP.CUDA OMP.CUDA TBB.CUDA)
deps/thrust/testing/CMakeLists.txt:thrust_declare_test_restrictions(async_sort        CPP.CUDA OMP.CUDA TBB.CUDA)
deps/thrust/testing/CMakeLists.txt:thrust_declare_test_restrictions(async_transform   CPP.CUDA OMP.CUDA TBB.CUDA)
deps/thrust/testing/CMakeLists.txt:thrust_declare_test_restrictions(event             CPP.CUDA OMP.CUDA TBB.CUDA)
deps/thrust/testing/CMakeLists.txt:thrust_declare_test_restrictions(future            CPP.CUDA OMP.CUDA TBB.CUDA)
deps/thrust/testing/CMakeLists.txt:# for CUDA.
deps/thrust/testing/CMakeLists.txt:thrust_declare_test_restrictions(unittest_static_assert CPP.CPP CPP.CUDA)
deps/thrust/testing/CMakeLists.txt:#   testing/vector.cu will be "vector", and testing/cuda/copy.cu will be
deps/thrust/testing/CMakeLists.txt:#   "cuda.copy".
deps/thrust/testing/CMakeLists.txt:  # Wrap the .cu file in .cpp for non-CUDA backends
deps/thrust/testing/CMakeLists.txt:  if ("CUDA" STREQUAL "${config_device}")
deps/thrust/testing/CMakeLists.txt:    if (THRUST_ENABLE_TESTS_WITH_RDC AND ("CUDA" STREQUAL "${config_device}"))
deps/thrust/testing/CMakeLists.txt:      thrust_enable_rdc_for_cuda_target(${test_target})
deps/thrust/testing/CMakeLists.txt:add_subdirectory(cuda)
deps/thrust/testing/async_transform.cu:, cudaStream_t stream_;
deps/thrust/testing/async_transform.cu:, thrust::cuda_cub::throw_on_error(
deps/thrust/testing/async_transform.cu:    cudaStreamCreateWithFlags(&stream_, cudaStreamNonBlocking)
deps/thrust/testing/async_transform.cu:, thrust::cuda_cub::throw_on_error(
deps/thrust/testing/async_transform.cu:    cudaStreamDestroy(stream_)
deps/thrust/testing/async_transform.cu:, cudaStream_t stream_;
deps/thrust/testing/async_transform.cu:, thrust::cuda_cub::throw_on_error(
deps/thrust/testing/async_transform.cu:    cudaStreamCreateWithFlags(&stream_, cudaStreamNonBlocking)
deps/thrust/testing/async_transform.cu:, thrust::cuda_cub::throw_on_error(
deps/thrust/testing/async_transform.cu:    cudaStreamDestroy(stream_)
deps/thrust/testing/unittest/system.h:#if __GNUC__ && !_NVHPC_CUDA
deps/thrust/testing/unittest/testframework.cu:#if THRUST_DEVICE_SYSTEM == THRUST_DEVICE_SYSTEM_CUDA
deps/thrust/testing/unittest/testframework.cu:#include <unittest/cuda/testframework.h>
deps/thrust/testing/unittest/CMakeLists.txt:  if ("CUDA" STREQUAL "${config_device}")
deps/thrust/testing/unittest/CMakeLists.txt:      cuda/testframework.cu
deps/thrust/testing/unittest/CMakeLists.txt:    # Wrap the cu file inside a .cpp file for non-CUDA builds
deps/thrust/testing/unittest/runtime_static_assert.h:#if THRUST_DEVICE_SYSTEM == THRUST_DEVICE_SYSTEM_CUDA
deps/thrust/testing/unittest/runtime_static_assert.h:        ::cudaMemcpyToSymbol(unittest::detail::device_exception, &raw_ptr, sizeof(ex_t*)); \
deps/thrust/testing/unittest/runtime_static_assert.h:        ::cudaMemcpyToSymbol(unittest::detail::device_exception, &raw_ptr, sizeof(ex_t*)); \
deps/thrust/testing/unittest/runtime_static_assert.h:#ifdef __CUDA_ARCH__
deps/thrust/testing/unittest/cuda/testframework.h:#include <thrust/system/cuda/memory.h>
deps/thrust/testing/unittest/cuda/testframework.h:class CUDATestDriver
deps/thrust/testing/unittest/cuda/testframework.h:    bool check_cuda_error(bool concise);
deps/thrust/testing/unittest/cuda/testframework.h:UnitTestDriver &driver_instance(thrust::system::cuda::tag);
deps/thrust/testing/unittest/cuda/testframework.cu:#include <unittest/cuda/testframework.h>
deps/thrust/testing/unittest/cuda/testframework.cu:#include <thrust/system/cuda/memory.h>
deps/thrust/testing/unittest/cuda/testframework.cu:#include <cuda_runtime.h>
deps/thrust/testing/unittest/cuda/testframework.cu:  cudaFuncAttributes attr;
deps/thrust/testing/unittest/cuda/testframework.cu:  cudaError_t error = cudaFuncGetAttributes(&attr, dummy_kernel);
deps/thrust/testing/unittest/cuda/testframework.cu:  // clear the CUDA global error state if we just set it, so that
deps/thrust/testing/unittest/cuda/testframework.cu:  // check_cuda_error doesn't complain
deps/thrust/testing/unittest/cuda/testframework.cu:  if (cudaSuccess != error) (void)cudaGetLastError();
deps/thrust/testing/unittest/cuda/testframework.cu:  return cudaSuccess == error;
deps/thrust/testing/unittest/cuda/testframework.cu:  cudaGetDeviceCount(&deviceCount);
deps/thrust/testing/unittest/cuda/testframework.cu:    std::cout << "There is no device supporting CUDA" << std::endl;
deps/thrust/testing/unittest/cuda/testframework.cu:  cudaGetDevice(&selected_device);
deps/thrust/testing/unittest/cuda/testframework.cu:    cudaDeviceProp deviceProp;
deps/thrust/testing/unittest/cuda/testframework.cu:    cudaGetDeviceProperties(&deviceProp, dev);
deps/thrust/testing/unittest/cuda/testframework.cu:        std::cout << "There is no device supporting CUDA." << std::endl;
deps/thrust/testing/unittest/cuda/testframework.cu:        std::cout << "There is 1 device supporting CUDA" << std:: endl;
deps/thrust/testing/unittest/cuda/testframework.cu:        std::cout << "There are " << deviceCount <<  " devices supporting CUDA" << std:: endl;
deps/thrust/testing/unittest/cuda/testframework.cu:std::vector<int> CUDATestDriver::target_devices(const ArgumentMap &kwargs)
deps/thrust/testing/unittest/cuda/testframework.cu:    cudaGetDeviceCount(&count);
deps/thrust/testing/unittest/cuda/testframework.cu:bool CUDATestDriver::check_cuda_error(bool concise)
deps/thrust/testing/unittest/cuda/testframework.cu:  cudaError_t const error = cudaGetLastError();
deps/thrust/testing/unittest/cuda/testframework.cu:  if(cudaSuccess != error)
deps/thrust/testing/unittest/cuda/testframework.cu:      std::cout << "[ERROR] CUDA error detected before running tests: ["
deps/thrust/testing/unittest/cuda/testframework.cu:                << std::string(cudaGetErrorName(error))
deps/thrust/testing/unittest/cuda/testframework.cu:                << std::string(cudaGetErrorString(error))
deps/thrust/testing/unittest/cuda/testframework.cu:  return cudaSuccess != error;
deps/thrust/testing/unittest/cuda/testframework.cu:bool CUDATestDriver::post_test_smoke_check(const UnitTest &test, bool concise)
deps/thrust/testing/unittest/cuda/testframework.cu:  cudaError_t const error = cudaDeviceSynchronize();
deps/thrust/testing/unittest/cuda/testframework.cu:  if(cudaSuccess != error)
deps/thrust/testing/unittest/cuda/testframework.cu:      std::cout << "\t[ERROR] CUDA error detected after running " << test.name << ": ["
deps/thrust/testing/unittest/cuda/testframework.cu:                << std::string(cudaGetErrorName(error))
deps/thrust/testing/unittest/cuda/testframework.cu:                << std::string(cudaGetErrorString(error))
deps/thrust/testing/unittest/cuda/testframework.cu:  return cudaSuccess == error;
deps/thrust/testing/unittest/cuda/testframework.cu:bool CUDATestDriver::run_tests(const ArgumentSet &args, const ArgumentMap &kwargs)
deps/thrust/testing/unittest/cuda/testframework.cu:  if(check_cuda_error(concise)) return false;
deps/thrust/testing/unittest/cuda/testframework.cu:    cudaDeviceSynchronize();
deps/thrust/testing/unittest/cuda/testframework.cu:    cudaSetDevice(*device);
deps/thrust/testing/unittest/cuda/testframework.cu:      cudaDeviceProp deviceProp;
deps/thrust/testing/unittest/cuda/testframework.cu:      cudaGetDeviceProperties(&deviceProp, *device);
deps/thrust/testing/unittest/cuda/testframework.cu:      cudaDeviceProp deviceProp;
deps/thrust/testing/unittest/cuda/testframework.cu:      cudaGetDeviceProperties(&deviceProp, *device);
deps/thrust/testing/unittest/cuda/testframework.cu:    if(check_cuda_error(concise)) return false;
deps/thrust/testing/unittest/cuda/testframework.cu:int CUDATestDriver::current_device_architecture() const
deps/thrust/testing/unittest/cuda/testframework.cu:  cudaGetDevice(&current);
deps/thrust/testing/unittest/cuda/testframework.cu:  cudaDeviceProp deviceProp;
deps/thrust/testing/unittest/cuda/testframework.cu:  cudaGetDeviceProperties(&deviceProp, current);
deps/thrust/testing/unittest/cuda/testframework.cu:UnitTestDriver &driver_instance(thrust::system::cuda::tag)
deps/thrust/testing/unittest/cuda/testframework.cu:  static CUDATestDriver s_instance;
deps/thrust/testing/allocator_aware_policies.cu:#if THRUST_DEVICE_SYSTEM == THRUST_DEVICE_SYSTEM_CUDA
deps/thrust/testing/allocator_aware_policies.cu:#include <thrust/system/cuda/detail/par.h>
deps/thrust/testing/allocator_aware_policies.cu:#if THRUST_DEVICE_SYSTEM == THRUST_DEVICE_SYSTEM_CUDA
deps/thrust/testing/allocator_aware_policies.cu:    thrust::system::cuda::detail::par_t,
deps/thrust/testing/allocator_aware_policies.cu:    thrust::cuda_cub::execute_on_stream_base
deps/thrust/testing/allocator_aware_policies.cu:> cuda_par_info;
deps/thrust/testing/allocator_aware_policies.cu:#if THRUST_DEVICE_SYSTEM == THRUST_DEVICE_SYSTEM_CUDA
deps/thrust/testing/allocator_aware_policies.cu:        cuda_par_info,
deps/thrust/testing/async_reduce.cu:, cudaStream_t stream_;
deps/thrust/testing/async_reduce.cu:, thrust::cuda_cub::throw_on_error(
deps/thrust/testing/async_reduce.cu:    cudaStreamCreateWithFlags(&stream_, cudaStreamNonBlocking)
deps/thrust/testing/async_reduce.cu:, thrust::cuda_cub::throw_on_error(
deps/thrust/testing/async_reduce.cu:    cudaStreamDestroy(stream_)
deps/thrust/testing/async_reduce.cu:, cudaStream_t stream_;
deps/thrust/testing/async_reduce.cu:, thrust::cuda_cub::throw_on_error(
deps/thrust/testing/async_reduce.cu:    cudaStreamCreateWithFlags(&stream_, cudaStreamNonBlocking)
deps/thrust/testing/async_reduce.cu:, thrust::cuda_cub::throw_on_error(
deps/thrust/testing/async_reduce.cu:    cudaStreamDestroy(stream_)
deps/thrust/testing/async_reduce.cu:, cudaStream_t stream_;
deps/thrust/testing/async_reduce.cu:, thrust::cuda_cub::throw_on_error(
deps/thrust/testing/async_reduce.cu:    cudaStreamCreateWithFlags(&stream_, cudaStreamNonBlocking)
deps/thrust/testing/async_reduce.cu:, thrust::cuda_cub::throw_on_error(
deps/thrust/testing/async_reduce.cu:    cudaStreamDestroy(stream_)
deps/thrust/testing/async_reduce.cu:, cudaStream_t stream_;
deps/thrust/testing/async_reduce.cu:, thrust::cuda_cub::throw_on_error(
deps/thrust/testing/async_reduce.cu:    cudaStreamCreateWithFlags(&stream_, cudaStreamNonBlocking)
deps/thrust/testing/async_reduce.cu:, thrust::cuda_cub::throw_on_error(
deps/thrust/testing/async_reduce.cu:    cudaStreamDestroy(stream_)
deps/thrust/testing/async_reduce.cu:, cudaStream_t stream_;
deps/thrust/testing/async_reduce.cu:, thrust::cuda_cub::throw_on_error(
deps/thrust/testing/async_reduce.cu:    cudaStreamCreateWithFlags(&stream_, cudaStreamNonBlocking)
deps/thrust/testing/async_reduce.cu:, thrust::cuda_cub::throw_on_error(
deps/thrust/testing/async_reduce.cu:    cudaStreamDestroy(stream_)
deps/thrust/testing/async_reduce.cu:, cudaStream_t stream_;
deps/thrust/testing/async_reduce.cu:, thrust::cuda_cub::throw_on_error(
deps/thrust/testing/async_reduce.cu:    cudaStreamCreateWithFlags(&stream_, cudaStreamNonBlocking)
deps/thrust/testing/async_reduce.cu:, thrust::cuda_cub::throw_on_error(
deps/thrust/testing/async_reduce.cu:    cudaStreamDestroy(stream_)
deps/thrust/testing/async_reduce.cu:, cudaStream_t stream_;
deps/thrust/testing/async_reduce.cu:, thrust::cuda_cub::throw_on_error(
deps/thrust/testing/async_reduce.cu:    cudaStreamCreateWithFlags(&stream_, cudaStreamNonBlocking)
deps/thrust/testing/async_reduce.cu:, thrust::cuda_cub::throw_on_error(
deps/thrust/testing/async_reduce.cu:    cudaStreamDestroy(stream_)
deps/thrust/testing/async_reduce.cu:, cudaStream_t stream_;
deps/thrust/testing/async_reduce.cu:, thrust::cuda_cub::throw_on_error(
deps/thrust/testing/async_reduce.cu:    cudaStreamCreateWithFlags(&stream_, cudaStreamNonBlocking)
deps/thrust/testing/async_reduce.cu:, thrust::cuda_cub::throw_on_error(
deps/thrust/testing/async_reduce.cu:    cudaStreamDestroy(stream_)
deps/thrust/testing/async_reduce.cu:    cudaStream_t stream;
deps/thrust/testing/async_reduce.cu:    thrust::cuda_cub::throw_on_error(
deps/thrust/testing/async_reduce.cu:      cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking)
deps/thrust/testing/async_reduce.cu:    thrust::cuda_cub::throw_on_error(
deps/thrust/testing/async_reduce.cu:      cudaStreamDestroy(stream)
deps/thrust/testing/async_reduce.cu:    cudaStream_t stream0;
deps/thrust/testing/async_reduce.cu:    thrust::cuda_cub::throw_on_error(
deps/thrust/testing/async_reduce.cu:      cudaStreamCreateWithFlags(&stream0, cudaStreamNonBlocking)
deps/thrust/testing/async_reduce.cu:    cudaStream_t stream1;
deps/thrust/testing/async_reduce.cu:    thrust::cuda_cub::throw_on_error(
deps/thrust/testing/async_reduce.cu:      cudaStreamCreateWithFlags(&stream1, cudaStreamNonBlocking)
deps/thrust/testing/async_reduce.cu:    thrust::cuda_cub::throw_on_error(cudaStreamDestroy(stream0));
deps/thrust/testing/async_reduce.cu:    thrust::cuda_cub::throw_on_error(cudaStreamDestroy(stream1));
deps/thrust/testing/unittest_static_assert.cmake:# See https://github.com/NVIDIA/thrust/issues/1397
deps/thrust/testing/unittest_static_assert.cmake:if ("NVCXX" STREQUAL "${CMAKE_CUDA_COMPILER_ID}")
deps/thrust/testing/device_delete.cu:#ifdef __CUDA_ARCH__
deps/thrust/testing/async_reduce_into.cu:, cudaStream_t stream_;
deps/thrust/testing/async_reduce_into.cu:, thrust::cuda_cub::throw_on_error(
deps/thrust/testing/async_reduce_into.cu:    cudaStreamCreateWithFlags(&stream_, cudaStreamNonBlocking)
deps/thrust/testing/async_reduce_into.cu:, thrust::cuda_cub::throw_on_error(
deps/thrust/testing/async_reduce_into.cu:    cudaStreamDestroy(stream_)
deps/thrust/testing/async_reduce_into.cu:, cudaStream_t stream_;
deps/thrust/testing/async_reduce_into.cu:, thrust::cuda_cub::throw_on_error(
deps/thrust/testing/async_reduce_into.cu:    cudaStreamCreateWithFlags(&stream_, cudaStreamNonBlocking)
deps/thrust/testing/async_reduce_into.cu:, thrust::cuda_cub::throw_on_error(
deps/thrust/testing/async_reduce_into.cu:    cudaStreamDestroy(stream_)
deps/thrust/testing/async_reduce_into.cu:, cudaStream_t stream_;
deps/thrust/testing/async_reduce_into.cu:, thrust::cuda_cub::throw_on_error(
deps/thrust/testing/async_reduce_into.cu:    cudaStreamCreateWithFlags(&stream_, cudaStreamNonBlocking)
deps/thrust/testing/async_reduce_into.cu:, thrust::cuda_cub::throw_on_error(
deps/thrust/testing/async_reduce_into.cu:    cudaStreamDestroy(stream_)
deps/thrust/testing/async_reduce_into.cu:, cudaStream_t stream_;
deps/thrust/testing/async_reduce_into.cu:, thrust::cuda_cub::throw_on_error(
deps/thrust/testing/async_reduce_into.cu:    cudaStreamCreateWithFlags(&stream_, cudaStreamNonBlocking)
deps/thrust/testing/async_reduce_into.cu:, thrust::cuda_cub::throw_on_error(
deps/thrust/testing/async_reduce_into.cu:    cudaStreamDestroy(stream_)
deps/thrust/testing/async_reduce_into.cu:, cudaStream_t stream_;
deps/thrust/testing/async_reduce_into.cu:, thrust::cuda_cub::throw_on_error(
deps/thrust/testing/async_reduce_into.cu:    cudaStreamCreateWithFlags(&stream_, cudaStreamNonBlocking)
deps/thrust/testing/async_reduce_into.cu:, thrust::cuda_cub::throw_on_error(
deps/thrust/testing/async_reduce_into.cu:    cudaStreamDestroy(stream_)
deps/thrust/testing/async_reduce_into.cu:, cudaStream_t stream_;
deps/thrust/testing/async_reduce_into.cu:, thrust::cuda_cub::throw_on_error(
deps/thrust/testing/async_reduce_into.cu:    cudaStreamCreateWithFlags(&stream_, cudaStreamNonBlocking)
deps/thrust/testing/async_reduce_into.cu:, thrust::cuda_cub::throw_on_error(
deps/thrust/testing/async_reduce_into.cu:    cudaStreamDestroy(stream_)
deps/thrust/testing/async_reduce_into.cu:, cudaStream_t stream_;
deps/thrust/testing/async_reduce_into.cu:, thrust::cuda_cub::throw_on_error(
deps/thrust/testing/async_reduce_into.cu:    cudaStreamCreateWithFlags(&stream_, cudaStreamNonBlocking)
deps/thrust/testing/async_reduce_into.cu:, thrust::cuda_cub::throw_on_error(
deps/thrust/testing/async_reduce_into.cu:    cudaStreamDestroy(stream_)
deps/thrust/testing/async_reduce_into.cu:, cudaStream_t stream_;
deps/thrust/testing/async_reduce_into.cu:, thrust::cuda_cub::throw_on_error(
deps/thrust/testing/async_reduce_into.cu:    cudaStreamCreateWithFlags(&stream_, cudaStreamNonBlocking)
deps/thrust/testing/async_reduce_into.cu:, thrust::cuda_cub::throw_on_error(
deps/thrust/testing/async_reduce_into.cu:    cudaStreamDestroy(stream_)
deps/thrust/testing/reduce.cu:        T gpu_result = thrust::reduce(d_data.begin(), d_data.end(), init, plus_mod_10<T>());
deps/thrust/testing/reduce.cu:        ASSERT_EQUAL(cpu_result, gpu_result);
deps/thrust/testing/async/CMakeLists.txt:# The async tests only support CUDA enabled configs. Create a list of valid
deps/thrust/testing/async/CMakeLists.txt:set(cuda_configs)
deps/thrust/testing/async/CMakeLists.txt:  if (config_device STREQUAL CUDA)
deps/thrust/testing/async/CMakeLists.txt:    list(APPEND cuda_configs ${thrust_target})
deps/thrust/testing/async/CMakeLists.txt:list(LENGTH cuda_configs num_cuda_configs)
deps/thrust/testing/async/CMakeLists.txt:if (num_cuda_configs EQUAL 0)
deps/thrust/testing/async/CMakeLists.txt:  foreach(thrust_target IN LISTS cuda_configs)
deps/thrust/testing/async/CMakeLists.txt:        thrust_enable_rdc_for_cuda_target(${test_target})
deps/thrust/testing/async/test_policy_overloads.h:      ASSERT_NOT_EQUAL(thrust::cuda_cub::default_stream(),
deps/thrust/testing/async/test_policy_overloads.h:    thrust::system::cuda::detail::unique_stream test_stream{};
deps/thrust/testing/async/test_policy_overloads.h:    ASSERT_NOT_EQUAL_QUIET(thrust::cuda_cub::default_stream(), stream_a);
deps/thrust/testing/tuple_scan.cu:#if THRUST_DEVICE_SYSTEM == THRUST_DEVICE_SYSTEM_CUDA
deps/thrust/testing/tuple_scan.cu:#include <unittest/cuda/testframework.h>
deps/thrust/testing/cmake/test_install/CMakeLists.txt:project(ThrustTestInstall CXX CUDA)
deps/thrust/testing/cmake/test_install/CMakeLists.txt:  find_package(Thrust CONFIG COMPONENTS CPP CUDA)
deps/thrust/testing/cmake/test_install/CMakeLists.txt:  assert_target(Thrust::CUDA::Device)
deps/thrust/testing/cmake/test_install/CMakeLists.txt:  assert_boolean(THRUST_CUDA_FOUND TRUE)
deps/thrust/testing/cmake/CMakeLists.txt:if ("NVCXX" STREQUAL "${CMAKE_CUDA_COMPILER_ID}")
deps/thrust/testing/cmake/CMakeLists.txt:    -D "CMAKE_CUDA_COMPILER_ID=${CMAKE_CUDA_COMPILER_ID}"
deps/thrust/testing/cmake/CMakeLists.txt:    -D "CMAKE_CUDA_COMPILER_FORCED=${CMAKE_CUDA_COMPILER_FORCED}"
deps/thrust/testing/cmake/CMakeLists.txt:if (THRUST_CPP_FOUND AND THRUST_CUDA_FOUND)
deps/thrust/testing/cmake/CMakeLists.txt:      -D "CMAKE_CUDA_COMPILER=${CMAKE_CUDA_COMPILER}"
deps/thrust/testing/cuda/remove.mk:CUDACC_FLAGS += -rdc=true
deps/thrust/testing/cuda/adjacent_difference.mk:CUDACC_FLAGS += -rdc=true
deps/thrust/testing/cuda/binary_search.cu:  cudaStream_t stream = 0;
deps/thrust/testing/cuda/binary_search.cu:  result_t result = thrust::equal_range(thrust::cuda::par.on(stream),
deps/thrust/testing/cuda/sequence.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/sequence.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/sequence.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/sequence.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/sequence.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/sequence.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/sequence.cu:void TestSequenceCudaStreams()
deps/thrust/testing/cuda/sequence.cu:  cudaStream_t s;
deps/thrust/testing/cuda/sequence.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/sequence.cu:  thrust::sequence(thrust::cuda::par.on(s), v.begin(), v.end());
deps/thrust/testing/cuda/sequence.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/sequence.cu:  thrust::sequence(thrust::cuda::par.on(s), v.begin(), v.end(), 10);
deps/thrust/testing/cuda/sequence.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/sequence.cu:  thrust::sequence(thrust::cuda::par.on(s), v.begin(), v.end(), 10, 2);
deps/thrust/testing/cuda/sequence.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/sequence.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/sequence.cu:DECLARE_UNITTEST(TestSequenceCudaStreams);
deps/thrust/testing/cuda/unique_by_key.mk:CUDACC_FLAGS += -rdc=true
deps/thrust/testing/cuda/reduce.mk:CUDACC_FLAGS += -rdc=true
deps/thrust/testing/cuda/set_symmetric_difference_by_key.cu:  cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/set_symmetric_difference_by_key.cu:  ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/set_symmetric_difference_by_key.cu:void TestSetSymmetricDifferenceByKeyCudaStreams()
deps/thrust/testing/cuda/set_symmetric_difference_by_key.cu:  cudaStream_t s;
deps/thrust/testing/cuda/set_symmetric_difference_by_key.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/set_symmetric_difference_by_key.cu:    thrust::set_symmetric_difference_by_key(thrust::cuda::par.on(s),
deps/thrust/testing/cuda/set_symmetric_difference_by_key.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/set_symmetric_difference_by_key.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/set_symmetric_difference_by_key.cu:DECLARE_UNITTEST(TestSetSymmetricDifferenceByKeyCudaStreams);
deps/thrust/testing/cuda/is_sorted.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/is_sorted.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/is_sorted.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/is_sorted.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/is_sorted.cu:void TestIsSortedCudaStreams()
deps/thrust/testing/cuda/is_sorted.cu:  cudaStream_t s;
deps/thrust/testing/cuda/is_sorted.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/is_sorted.cu:  ASSERT_EQUAL(thrust::is_sorted(thrust::cuda::par.on(s), v.begin(), v.begin() + 0), true);
deps/thrust/testing/cuda/is_sorted.cu:  ASSERT_EQUAL(thrust::is_sorted(thrust::cuda::par.on(s), v.begin(), v.begin() + 1), true);
deps/thrust/testing/cuda/is_sorted.cu:  ASSERT_EQUAL(thrust::is_sorted(thrust::cuda::par.on(s), v.begin(), v.begin() + 2), true);
deps/thrust/testing/cuda/is_sorted.cu:  ASSERT_EQUAL(thrust::is_sorted(thrust::cuda::par.on(s), v.begin(), v.begin() + 3), true);
deps/thrust/testing/cuda/is_sorted.cu:  ASSERT_EQUAL(thrust::is_sorted(thrust::cuda::par.on(s), v.begin(), v.begin() + 4), false);
deps/thrust/testing/cuda/is_sorted.cu:  ASSERT_EQUAL(thrust::is_sorted(thrust::cuda::par.on(s), v.begin(), v.begin() + 3, thrust::less<int>()),    true);
deps/thrust/testing/cuda/is_sorted.cu:  ASSERT_EQUAL(thrust::is_sorted(thrust::cuda::par.on(s), v.begin(), v.begin() + 1, thrust::greater<int>()), true);
deps/thrust/testing/cuda/is_sorted.cu:  ASSERT_EQUAL(thrust::is_sorted(thrust::cuda::par.on(s), v.begin(), v.begin() + 4, thrust::greater<int>()), false);
deps/thrust/testing/cuda/is_sorted.cu:  ASSERT_EQUAL(thrust::is_sorted(thrust::cuda::par.on(s), v.begin(), v.end()), false);
deps/thrust/testing/cuda/is_sorted.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/is_sorted.cu:DECLARE_UNITTEST(TestIsSortedCudaStreams);
deps/thrust/testing/cuda/uninitialized_fill.mk:CUDACC_FLAGS += -rdc=true
deps/thrust/testing/cuda/unique_by_key.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/unique_by_key.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/unique_by_key.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/unique_by_key.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/unique_by_key.cu:  TestUniqueByKeyDevice(thrust::cuda::par_nosync);
deps/thrust/testing/cuda/unique_by_key.cu:void TestUniqueByKeyCudaStreams(ExecutionPolicy policy)
deps/thrust/testing/cuda/unique_by_key.cu:  cudaStream_t s;
deps/thrust/testing/cuda/unique_by_key.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/unique_by_key.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/unique_by_key.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/unique_by_key.cu:void TestUniqueByKeyCudaStreamsSync()
deps/thrust/testing/cuda/unique_by_key.cu:  TestUniqueByKeyCudaStreams(thrust::cuda::par);
deps/thrust/testing/cuda/unique_by_key.cu:DECLARE_UNITTEST(TestUniqueByKeyCudaStreamsSync);
deps/thrust/testing/cuda/unique_by_key.cu:void TestUniqueByKeyCudaStreamsNoSync()
deps/thrust/testing/cuda/unique_by_key.cu:  TestUniqueByKeyCudaStreams(thrust::cuda::par_nosync);
deps/thrust/testing/cuda/unique_by_key.cu:DECLARE_UNITTEST(TestUniqueByKeyCudaStreamsNoSync);
deps/thrust/testing/cuda/unique_by_key.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/unique_by_key.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/unique_by_key.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/unique_by_key.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/unique_by_key.cu:  TestUniqueCopyByKeyDevice(thrust::cuda::par_nosync);
deps/thrust/testing/cuda/unique_by_key.cu:void TestUniqueCopyByKeyCudaStreams(ExecutionPolicy policy)
deps/thrust/testing/cuda/unique_by_key.cu:  cudaStream_t s;
deps/thrust/testing/cuda/unique_by_key.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/unique_by_key.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/unique_by_key.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/unique_by_key.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/unique_by_key.cu:void TestUniqueCopyByKeyCudaStreamsSync()
deps/thrust/testing/cuda/unique_by_key.cu:  TestUniqueCopyByKeyCudaStreams(thrust::cuda::par);
deps/thrust/testing/cuda/unique_by_key.cu:DECLARE_UNITTEST(TestUniqueCopyByKeyCudaStreamsSync);
deps/thrust/testing/cuda/unique_by_key.cu:void TestUniqueCopyByKeyCudaStreamsNoSync()
deps/thrust/testing/cuda/unique_by_key.cu:  TestUniqueCopyByKeyCudaStreams(thrust::cuda::par_nosync);
deps/thrust/testing/cuda/unique_by_key.cu:DECLARE_UNITTEST(TestUniqueCopyByKeyCudaStreamsNoSync);
deps/thrust/testing/cuda/pair_sort_by_key.mk:CUDACC_FLAGS += -rdc=true
deps/thrust/testing/cuda/merge.mk:CUDACC_FLAGS += -rdc=true
deps/thrust/testing/cuda/is_partitioned.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/is_partitioned.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/is_partitioned.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/is_partitioned.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/is_partitioned.cu:void TestIsPartitionedCudaStreams()
deps/thrust/testing/cuda/is_partitioned.cu:  cudaStream_t s;
deps/thrust/testing/cuda/is_partitioned.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/is_partitioned.cu:  ASSERT_EQUAL_QUIET(true, thrust::is_partitioned(thrust::cuda::par.on(s), v.begin(), v.begin(), thrust::identity<int>()));
deps/thrust/testing/cuda/is_partitioned.cu:  ASSERT_EQUAL_QUIET(true, thrust::is_partitioned(thrust::cuda::par.on(s), v.begin(), v.begin() + 1, thrust::identity<int>()));
deps/thrust/testing/cuda/is_partitioned.cu:  ASSERT_EQUAL_QUIET(true, thrust::is_partitioned(thrust::cuda::par.on(s), v.begin(), v.begin() + 2, thrust::identity<int>()));
deps/thrust/testing/cuda/is_partitioned.cu:  ASSERT_EQUAL_QUIET(true, thrust::is_partitioned(thrust::cuda::par.on(s), v.begin(), v.end(), thrust::identity<int>()));
deps/thrust/testing/cuda/is_partitioned.cu:  ASSERT_EQUAL_QUIET(true, thrust::is_partitioned(thrust::cuda::par.on(s), v.begin() + 3, v.end(), thrust::identity<int>()));
deps/thrust/testing/cuda/is_partitioned.cu:  ASSERT_EQUAL_QUIET(false, thrust::is_partitioned(thrust::cuda::par.on(s), v.begin(), v.end(), thrust::identity<int>()));
deps/thrust/testing/cuda/is_partitioned.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/is_partitioned.cu:DECLARE_UNITTEST(TestIsPartitionedCudaStreams);
deps/thrust/testing/cuda/sort_by_key.cu:#if (__CUDA_ARCH__ >= 200)
deps/thrust/testing/cuda/sort_by_key.cu:  cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/sort_by_key.cu:  ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/sort_by_key.cu:void TestComparisonSortByKeyCudaStreams()
deps/thrust/testing/cuda/sort_by_key.cu:  cudaStream_t s;
deps/thrust/testing/cuda/sort_by_key.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/sort_by_key.cu:  thrust::sort_by_key(thrust::cuda::par.on(s), keys.begin(), keys.end(), vals.begin(), my_less<int>());
deps/thrust/testing/cuda/sort_by_key.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/sort_by_key.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/sort_by_key.cu:DECLARE_UNITTEST(TestComparisonSortByKeyCudaStreams);
deps/thrust/testing/cuda/sort_by_key.cu:void TestSortByKeyCudaStreams()
deps/thrust/testing/cuda/sort_by_key.cu:  cudaStream_t s;
deps/thrust/testing/cuda/sort_by_key.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/sort_by_key.cu:  thrust::sort_by_key(thrust::cuda::par.on(s), keys.begin(), keys.end(), vals.begin());
deps/thrust/testing/cuda/sort_by_key.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/sort_by_key.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/sort_by_key.cu:DECLARE_UNITTEST(TestSortByKeyCudaStreams);
deps/thrust/testing/cuda/mismatch.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/mismatch.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/mismatch.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/mismatch.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/mismatch.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/mismatch.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/mismatch.cu:void TestMismatchCudaStreams()
deps/thrust/testing/cuda/mismatch.cu:  cudaStream_t s;
deps/thrust/testing/cuda/mismatch.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/mismatch.cu:  ASSERT_EQUAL(thrust::mismatch(thrust::cuda::par.on(s), a.begin(), a.end(), b.begin()).first  - a.begin(), 2);
deps/thrust/testing/cuda/mismatch.cu:  ASSERT_EQUAL(thrust::mismatch(thrust::cuda::par.on(s), a.begin(), a.end(), b.begin()).second - b.begin(), 2);
deps/thrust/testing/cuda/mismatch.cu:  ASSERT_EQUAL(thrust::mismatch(thrust::cuda::par.on(s), a.begin(), a.end(), b.begin()).first  - a.begin(), 3);
deps/thrust/testing/cuda/mismatch.cu:  ASSERT_EQUAL(thrust::mismatch(thrust::cuda::par.on(s), a.begin(), a.end(), b.begin()).second - b.begin(), 3);
deps/thrust/testing/cuda/mismatch.cu:  ASSERT_EQUAL(thrust::mismatch(thrust::cuda::par.on(s), a.begin(), a.end(), b.begin()).first  - a.begin(), 4);
deps/thrust/testing/cuda/mismatch.cu:  ASSERT_EQUAL(thrust::mismatch(thrust::cuda::par.on(s), a.begin(), a.end(), b.begin()).second - b.begin(), 4);
deps/thrust/testing/cuda/mismatch.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/mismatch.cu:DECLARE_UNITTEST(TestMismatchCudaStreams);
deps/thrust/testing/cuda/max_element.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/max_element.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/max_element.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/max_element.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/max_element.cu:  TestMaxElementDevice(thrust::cuda::par_nosync);
deps/thrust/testing/cuda/max_element.cu:void TestMaxElementCudaStreams(ExecutionPolicy policy)
deps/thrust/testing/cuda/max_element.cu:  cudaStream_t s;
deps/thrust/testing/cuda/max_element.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/max_element.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/max_element.cu:void TestMaxElementCudaStreamsSync(){
deps/thrust/testing/cuda/max_element.cu:  TestMaxElementCudaStreams(thrust::cuda::par);
deps/thrust/testing/cuda/max_element.cu:DECLARE_UNITTEST(TestMaxElementCudaStreamsSync);
deps/thrust/testing/cuda/max_element.cu:void TestMaxElementCudaStreamsNoSync(){
deps/thrust/testing/cuda/max_element.cu:  TestMaxElementCudaStreams(thrust::cuda::par_nosync);
deps/thrust/testing/cuda/max_element.cu:DECLARE_UNITTEST(TestMaxElementCudaStreamsNoSync);
deps/thrust/testing/cuda/inner_product.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/inner_product.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/inner_product.cu:void TestInnerProductCudaStreams()
deps/thrust/testing/cuda/inner_product.cu:  cudaStream_t s;
deps/thrust/testing/cuda/inner_product.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/inner_product.cu:  int result = thrust::inner_product(thrust::cuda::par.on(s), v1.begin(), v1.end(), v2.begin(), init);
deps/thrust/testing/cuda/inner_product.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/inner_product.cu:DECLARE_UNITTEST(TestInnerProductCudaStreams);
deps/thrust/testing/cuda/reverse.cu:  cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/reverse.cu:  ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/reverse.cu:  cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/reverse.cu:  ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/reverse.cu:void TestReverseCudaStreams()
deps/thrust/testing/cuda/reverse.cu:  cudaStream_t s;
deps/thrust/testing/cuda/reverse.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/reverse.cu:  thrust::reverse(thrust::cuda::par.on(s), data.begin(), data.end());
deps/thrust/testing/cuda/reverse.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/reverse.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/reverse.cu:DECLARE_UNITTEST(TestReverseCudaStreams);
deps/thrust/testing/cuda/reverse.cu:void TestReverseCopyCudaStreams()
deps/thrust/testing/cuda/reverse.cu:  cudaStream_t s;
deps/thrust/testing/cuda/reverse.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/reverse.cu:  thrust::reverse_copy(thrust::cuda::par.on(s), data.begin(), data.end(), result.begin());
deps/thrust/testing/cuda/reverse.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/reverse.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/reverse.cu:DECLARE_UNITTEST(TestReverseCopyCudaStreams);
deps/thrust/testing/cuda/mismatch.mk:CUDACC_FLAGS += -rdc=true
deps/thrust/testing/cuda/stream_per_thread.cu:#include <thrust/system/cuda/detail/util.h>
deps/thrust/testing/cuda/stream_per_thread.cu:  auto stream = thrust::cuda_cub::stream(exec);
deps/thrust/testing/cuda/stream_per_thread.cu:  ASSERT_EQUAL(stream, cudaStreamPerThread);
deps/thrust/testing/cuda/partition_point.cu:  cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/partition_point.cu:  ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/partition_point.cu:void TestPartitionPointCudaStreams()
deps/thrust/testing/cuda/partition_point.cu:  cudaStream_t s;
deps/thrust/testing/cuda/partition_point.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/partition_point.cu:  ASSERT_EQUAL_QUIET(ref, thrust::partition_point(thrust::cuda::par.on(s), first, last, thrust::identity<T>()));
deps/thrust/testing/cuda/partition_point.cu:  ASSERT_EQUAL_QUIET(ref, thrust::partition_point(thrust::cuda::par.on(s), first, last, thrust::identity<T>()));
deps/thrust/testing/cuda/partition_point.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/partition_point.cu:DECLARE_UNITTEST(TestPartitionPointCudaStreams);
deps/thrust/testing/cuda/unique.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/unique.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/unique.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/unique.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/unique.cu:  TestUniqueDevice(thrust::cuda::par_nosync);
deps/thrust/testing/cuda/unique.cu:void TestUniqueCudaStreams(ExecutionPolicy policy)
deps/thrust/testing/cuda/unique.cu:  cudaStream_t s;
deps/thrust/testing/cuda/unique.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/unique.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/unique.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/unique.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/unique.cu:void TestUniqueCudaStreamsSync()
deps/thrust/testing/cuda/unique.cu:  TestUniqueCudaStreams(thrust::cuda::par);
deps/thrust/testing/cuda/unique.cu:DECLARE_UNITTEST(TestUniqueCudaStreamsSync);
deps/thrust/testing/cuda/unique.cu:void TestUniqueCudaStreamsNoSync()
deps/thrust/testing/cuda/unique.cu:  TestUniqueCudaStreams(thrust::cuda::par_nosync);
deps/thrust/testing/cuda/unique.cu:DECLARE_UNITTEST(TestUniqueCudaStreamsNoSync);
deps/thrust/testing/cuda/unique.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/unique.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/unique.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/unique.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/unique.cu:  TestUniqueCopyDevice(thrust::cuda::par_nosync);
deps/thrust/testing/cuda/unique.cu:void TestUniqueCopyCudaStreams(ExecutionPolicy policy)
deps/thrust/testing/cuda/unique.cu:  cudaStream_t s;
deps/thrust/testing/cuda/unique.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/unique.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/unique.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/unique.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/unique.cu:void TestUniqueCopyCudaStreamsSync()
deps/thrust/testing/cuda/unique.cu:  TestUniqueCopyCudaStreams(thrust::cuda::par);
deps/thrust/testing/cuda/unique.cu:DECLARE_UNITTEST(TestUniqueCopyCudaStreamsSync);
deps/thrust/testing/cuda/unique.cu:void TestUniqueCopyCudaStreamsNoSync()
deps/thrust/testing/cuda/unique.cu:  TestUniqueCopyCudaStreams(thrust::cuda::par_nosync);
deps/thrust/testing/cuda/unique.cu:DECLARE_UNITTEST(TestUniqueCopyCudaStreamsNoSync);
deps/thrust/testing/cuda/generate.mk:CUDACC_FLAGS += -rdc=true
deps/thrust/testing/cuda/reduce_by_key.mk:CUDACC_FLAGS += -rdc=true
deps/thrust/testing/cuda/count.cu:  cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/count.cu:  ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/count.cu:  cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/count.cu:  ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/count.cu:void TestCountCudaStreams()
deps/thrust/testing/cuda/count.cu:  cudaStream_t s;
deps/thrust/testing/cuda/count.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/count.cu:  ASSERT_EQUAL(thrust::count(thrust::cuda::par.on(s), data.begin(), data.end(), 0), 2);
deps/thrust/testing/cuda/count.cu:  ASSERT_EQUAL(thrust::count(thrust::cuda::par.on(s), data.begin(), data.end(), 1), 3);
deps/thrust/testing/cuda/count.cu:  ASSERT_EQUAL(thrust::count(thrust::cuda::par.on(s), data.begin(), data.end(), 2), 0);
deps/thrust/testing/cuda/count.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/count.cu:DECLARE_UNITTEST(TestCountCudaStreams);
deps/thrust/testing/cuda/fill.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/fill.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/fill.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/fill.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/fill.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/fill.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/fill.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/fill.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/fill.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/fill.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/fill.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/fill.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/fill.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/fill.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/fill.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/fill.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/fill.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/fill.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/fill.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/fill.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/fill.cu:void TestFillCudaStreams()
deps/thrust/testing/cuda/fill.cu:  cudaStream_t s;
deps/thrust/testing/cuda/fill.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/fill.cu:  thrust::fill(thrust::cuda::par.on(s), v.begin() + 1, v.begin() + 4, 7);
deps/thrust/testing/cuda/fill.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/fill.cu:  thrust::fill(thrust::cuda::par.on(s), v.begin() + 0, v.begin() + 3, 8);
deps/thrust/testing/cuda/fill.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/fill.cu:  thrust::fill(thrust::cuda::par.on(s), v.begin() + 2, v.end(), 9);
deps/thrust/testing/cuda/fill.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/fill.cu:  thrust::fill(thrust::cuda::par.on(s), v.begin(), v.end(), 1);
deps/thrust/testing/cuda/fill.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/fill.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/fill.cu:DECLARE_UNITTEST(TestFillCudaStreams);
deps/thrust/testing/cuda/find.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/find.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/find.cu:      cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/find.cu:      ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/find.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/find.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/find.cu:      cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/find.cu:      ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/find.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/find.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/find.cu:      cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/find.cu:      ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/find.cu:void TestFindCudaStreams()
deps/thrust/testing/cuda/find.cu:  cudaStream_t s;
deps/thrust/testing/cuda/find.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/find.cu:  ASSERT_EQUAL(thrust::find(thrust::cuda::par.on(s), vec.begin(), vec.end(), 0) - vec.begin(), 5);
deps/thrust/testing/cuda/find.cu:  ASSERT_EQUAL(thrust::find(thrust::cuda::par.on(s), vec.begin(), vec.end(), 1) - vec.begin(), 0);
deps/thrust/testing/cuda/find.cu:  ASSERT_EQUAL(thrust::find(thrust::cuda::par.on(s), vec.begin(), vec.end(), 2) - vec.begin(), 1);
deps/thrust/testing/cuda/find.cu:  ASSERT_EQUAL(thrust::find(thrust::cuda::par.on(s), vec.begin(), vec.end(), 3) - vec.begin(), 2);
deps/thrust/testing/cuda/find.cu:  ASSERT_EQUAL(thrust::find(thrust::cuda::par.on(s), vec.begin(), vec.end(), 4) - vec.begin(), 5);
deps/thrust/testing/cuda/find.cu:  ASSERT_EQUAL(thrust::find(thrust::cuda::par.on(s), vec.begin(), vec.end(), 5) - vec.begin(), 4);
deps/thrust/testing/cuda/find.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/find.cu:DECLARE_UNITTEST(TestFindCudaStreams);
deps/thrust/testing/cuda/sort_by_key.mk:CUDACC_FLAGS += -rdc=true
deps/thrust/testing/cuda/transform_reduce.mk:CUDACC_FLAGS += -rdc=true
deps/thrust/testing/cuda/sequence.mk:CUDACC_FLAGS += -rdc=true
deps/thrust/testing/cuda/equal.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/equal.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/equal.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/equal.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/equal.cu:      cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/equal.cu:      ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/equal.cu:      cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/equal.cu:      ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/equal.cu:      cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/equal.cu:      ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/equal.cu:void TestEqualCudaStreams()
deps/thrust/testing/cuda/equal.cu:  cudaStream_t s;
deps/thrust/testing/cuda/equal.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/equal.cu:  ASSERT_EQUAL(thrust::equal(thrust::cuda::par.on(s), v1.begin(), v1.end(), v1.begin()), true);
deps/thrust/testing/cuda/equal.cu:  ASSERT_EQUAL(thrust::equal(thrust::cuda::par.on(s), v1.begin(), v1.end(), v2.begin()), false);
deps/thrust/testing/cuda/equal.cu:  ASSERT_EQUAL(thrust::equal(thrust::cuda::par.on(s), v2.begin(), v2.end(), v2.begin()), true);
deps/thrust/testing/cuda/equal.cu:  ASSERT_EQUAL(thrust::equal(thrust::cuda::par.on(s), v1.begin(), v1.begin() + 0, v1.begin()), true);
deps/thrust/testing/cuda/equal.cu:  ASSERT_EQUAL(thrust::equal(thrust::cuda::par.on(s), v1.begin(), v1.begin() + 1, v1.begin()), true);
deps/thrust/testing/cuda/equal.cu:  ASSERT_EQUAL(thrust::equal(thrust::cuda::par.on(s), v1.begin(), v1.begin() + 3, v2.begin()), true);
deps/thrust/testing/cuda/equal.cu:  ASSERT_EQUAL(thrust::equal(thrust::cuda::par.on(s), v1.begin(), v1.begin() + 4, v2.begin()), false);
deps/thrust/testing/cuda/equal.cu:  ASSERT_EQUAL(thrust::equal(thrust::cuda::par.on(s), v1.begin(), v1.end(), v2.begin(), thrust::less_equal<int>()), true);
deps/thrust/testing/cuda/equal.cu:  ASSERT_EQUAL(thrust::equal(thrust::cuda::par.on(s), v1.begin(), v1.end(), v2.begin(), thrust::greater<int>()),    false);
deps/thrust/testing/cuda/equal.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/equal.cu:DECLARE_UNITTEST(TestEqualCudaStreams);
deps/thrust/testing/cuda/set_symmetric_difference.cu:  cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/set_symmetric_difference.cu:  ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/set_symmetric_difference.cu:void TestSetSymmetricDifferenceCudaStreams()
deps/thrust/testing/cuda/set_symmetric_difference.cu:  cudaStream_t s;
deps/thrust/testing/cuda/set_symmetric_difference.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/set_symmetric_difference.cu:  Iterator end = thrust::set_symmetric_difference(thrust::cuda::par.on(s),
deps/thrust/testing/cuda/set_symmetric_difference.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/set_symmetric_difference.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/set_symmetric_difference.cu:DECLARE_UNITTEST(TestSetSymmetricDifferenceCudaStreams);
deps/thrust/testing/cuda/set_intersection.cu:  cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/set_intersection.cu:  ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/set_intersection.cu:  TestSetIntersectionDevice(thrust::cuda::par_nosync);
deps/thrust/testing/cuda/set_intersection.cu:void TestSetIntersectionCudaStreams(ExecutionPolicy policy)
deps/thrust/testing/cuda/set_intersection.cu:  cudaStream_t s;
deps/thrust/testing/cuda/set_intersection.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/set_intersection.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/set_intersection.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/set_intersection.cu:void TestSetIntersectionCudaStreamsSync()
deps/thrust/testing/cuda/set_intersection.cu:  TestSetIntersectionCudaStreams(thrust::cuda::par);
deps/thrust/testing/cuda/set_intersection.cu:DECLARE_UNITTEST(TestSetIntersectionCudaStreamsSync);
deps/thrust/testing/cuda/set_intersection.cu:void TestSetIntersectionCudaStreamsNoSync()
deps/thrust/testing/cuda/set_intersection.cu:  TestSetIntersectionCudaStreams(thrust::cuda::par_nosync);
deps/thrust/testing/cuda/set_intersection.cu:DECLARE_UNITTEST(TestSetIntersectionCudaStreamsNoSync);
deps/thrust/testing/cuda/gather.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/gather.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/gather.cu:void TestGatherCudaStreams()
deps/thrust/testing/cuda/gather.cu:  cudaStream_t s;
deps/thrust/testing/cuda/gather.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/gather.cu:  thrust::gather(thrust::cuda::par.on(s), map.begin(), map.end(), src.begin(), dst.begin());
deps/thrust/testing/cuda/gather.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/gather.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/gather.cu:DECLARE_UNITTEST(TestGatherCudaStreams);
deps/thrust/testing/cuda/gather.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/gather.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/gather.cu:void TestGatherIfCudaStreams(void)
deps/thrust/testing/cuda/gather.cu:  cudaStream_t s;
deps/thrust/testing/cuda/gather.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/gather.cu:  thrust::gather_if(thrust::cuda::par.on(s), map.begin(), map.end(), flg.begin(), src.begin(), dst.begin());
deps/thrust/testing/cuda/gather.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/gather.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/gather.cu:DECLARE_UNITTEST(TestGatherIfCudaStreams);
deps/thrust/testing/cuda/set_union.cu:  cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/set_union.cu:  ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/set_union.cu:void TestSetUnionCudaStreams()
deps/thrust/testing/cuda/set_union.cu:  cudaStream_t s;
deps/thrust/testing/cuda/set_union.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/set_union.cu:  Iterator end = thrust::set_union(thrust::cuda::par.on(s),
deps/thrust/testing/cuda/set_union.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/set_union.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/set_union.cu:DECLARE_UNITTEST(TestSetUnionCudaStreams);
deps/thrust/testing/cuda/min_element.mk:CUDACC_FLAGS += -rdc=true
deps/thrust/testing/cuda/scan.mk:CUDACC_FLAGS += -rdc=true
deps/thrust/testing/cuda/fill.mk:CUDACC_FLAGS += -rdc=true
deps/thrust/testing/cuda/gather.mk:CUDACC_FLAGS += -rdc=true
deps/thrust/testing/cuda/cudart.mk:CUDACC_FLAGS += -rdc=true
deps/thrust/testing/cuda/partition.mk:CUDACC_FLAGS += -rdc=true
deps/thrust/testing/cuda/pinned_allocator.mk:CUDACC_FLAGS += -rdc=true
deps/thrust/testing/cuda/set_difference.cu:  cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/set_difference.cu:  ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/set_difference.cu:void TestSetDifferenceCudaStreams()
deps/thrust/testing/cuda/set_difference.cu:  cudaStream_t s;
deps/thrust/testing/cuda/set_difference.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/set_difference.cu:  Iterator end = thrust::set_difference(thrust::cuda::par.on(s), a.begin(), a.end(), b.begin(), b.end(), result.begin());
deps/thrust/testing/cuda/set_difference.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/set_difference.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/set_difference.cu:DECLARE_UNITTEST(TestSetDifferenceCudaStreams);
deps/thrust/testing/cuda/reduce_by_key.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/reduce_by_key.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/reduce_by_key.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/reduce_by_key.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/reduce_by_key.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/reduce_by_key.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/reduce_by_key.cu:  TestReduceByKeyDevice(thrust::cuda::par_nosync);
deps/thrust/testing/cuda/reduce_by_key.cu:void TestReduceByKeyCudaStreams(ExecutionPolicy policy)
deps/thrust/testing/cuda/reduce_by_key.cu:  cudaStream_t s;
deps/thrust/testing/cuda/reduce_by_key.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/reduce_by_key.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/reduce_by_key.cu:void TestReduceByKeyCudaStreamsSync()
deps/thrust/testing/cuda/reduce_by_key.cu:  TestReduceByKeyCudaStreams(thrust::cuda::par);
deps/thrust/testing/cuda/reduce_by_key.cu:DECLARE_UNITTEST(TestReduceByKeyCudaStreamsSync);
deps/thrust/testing/cuda/reduce_by_key.cu:void TestReduceByKeyCudaStreamsNoSync()
deps/thrust/testing/cuda/reduce_by_key.cu:  TestReduceByKeyCudaStreams(thrust::cuda::par_nosync);
deps/thrust/testing/cuda/reduce_by_key.cu:DECLARE_UNITTEST(TestReduceByKeyCudaStreamsNoSync);
deps/thrust/testing/cuda/stream_per_thread.cmake:    $<$<AND:$<COMPILE_LANGUAGE:CUDA>,$<CUDA_COMPILER_ID:NVIDIA>>:--default-stream=per-thread>
deps/thrust/testing/cuda/stream_per_thread.cmake:if (CMAKE_CUDA_COMPILER_ID STREQUAL "Feta")
deps/thrust/testing/cuda/merge_by_key.cu:  cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/merge_by_key.cu:  ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/merge_by_key.cu:void TestMergeByKeyCudaStreams()
deps/thrust/testing/cuda/merge_by_key.cu:  cudaStream_t s;
deps/thrust/testing/cuda/merge_by_key.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/merge_by_key.cu:    thrust::merge_by_key(thrust::cuda::par.on(s),
deps/thrust/testing/cuda/merge_by_key.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/merge_by_key.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/merge_by_key.cu:DECLARE_UNITTEST(TestMergeByKeyCudaStreams);
deps/thrust/testing/cuda/complex.mk:CUDACC_FLAGS += -rdc=true
deps/thrust/testing/cuda/transform_scan.mk:CUDACC_FLAGS += -rdc=true
deps/thrust/testing/cuda/uninitialized_copy.cu:  cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/uninitialized_copy.cu:  ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/uninitialized_copy.cu:void TestUninitializedCopyCudaStreams()
deps/thrust/testing/cuda/uninitialized_copy.cu:  cudaStream_t s;
deps/thrust/testing/cuda/uninitialized_copy.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/uninitialized_copy.cu:  thrust::uninitialized_copy(thrust::cuda::par.on(s), v1.begin(), v1.end(), v2.begin());
deps/thrust/testing/cuda/uninitialized_copy.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/uninitialized_copy.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/uninitialized_copy.cu:DECLARE_UNITTEST(TestUninitializedCopyCudaStreams);
deps/thrust/testing/cuda/uninitialized_copy.cu:  cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/uninitialized_copy.cu:  ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/uninitialized_copy.cu:void TestUninitializedCopyNCudaStreams()
deps/thrust/testing/cuda/uninitialized_copy.cu:  cudaStream_t s;
deps/thrust/testing/cuda/uninitialized_copy.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/uninitialized_copy.cu:  thrust::uninitialized_copy_n(thrust::cuda::par.on(s), v1.begin(), v1.size(), v2.begin());
deps/thrust/testing/cuda/uninitialized_copy.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/uninitialized_copy.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/uninitialized_copy.cu:DECLARE_UNITTEST(TestUninitializedCopyNCudaStreams);
deps/thrust/testing/cuda/is_partitioned.mk:CUDACC_FLAGS += -rdc=true
deps/thrust/testing/cuda/scan.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/scan.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/scan.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/scan.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/scan.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/scan.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/scan.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/scan.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/scan.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/scan.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/scan.cu:void TestScanCudaStreams()
deps/thrust/testing/cuda/scan.cu:  cudaStream_t s;
deps/thrust/testing/cuda/scan.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/scan.cu:  iter = thrust::inclusive_scan(thrust::cuda::par.on(s), input.begin(), input.end(), output.begin());
deps/thrust/testing/cuda/scan.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/scan.cu:  iter = thrust::exclusive_scan(thrust::cuda::par.on(s), input.begin(), input.end(), output.begin(), 0);
deps/thrust/testing/cuda/scan.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/scan.cu:  iter = thrust::exclusive_scan(thrust::cuda::par.on(s), input.begin(), input.end(), output.begin(), 3);
deps/thrust/testing/cuda/scan.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/scan.cu:  iter = thrust::inclusive_scan(thrust::cuda::par.on(s), input.begin(), input.end(), output.begin(), thrust::plus<T>());
deps/thrust/testing/cuda/scan.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/scan.cu:  iter = thrust::exclusive_scan(thrust::cuda::par.on(s), input.begin(), input.end(), output.begin(), 3, thrust::plus<T>());
deps/thrust/testing/cuda/scan.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/scan.cu:  iter = thrust::inclusive_scan(thrust::cuda::par.on(s), input.begin(), input.end(), input.begin());
deps/thrust/testing/cuda/scan.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/scan.cu:  iter = thrust::exclusive_scan(thrust::cuda::par.on(s), input.begin(), input.end(), input.begin(), 3);
deps/thrust/testing/cuda/scan.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/scan.cu:  iter = thrust::exclusive_scan(thrust::cuda::par.on(s), input.begin(), input.end(), input.begin());
deps/thrust/testing/cuda/scan.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/scan.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/scan.cu:DECLARE_UNITTEST(TestScanCudaStreams);
deps/thrust/testing/cuda/complex.cu:#include <cuda_fp16.h>
deps/thrust/testing/cuda/scatter.mk:CUDACC_FLAGS += -rdc=true
deps/thrust/testing/cuda/equal.mk:CUDACC_FLAGS += -rdc=true
deps/thrust/testing/cuda/set_symmetric_difference_by_key.mk:CUDACC_FLAGS += -rdc=true
deps/thrust/testing/cuda/generate.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/generate.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/generate.cu:void TestGenerateCudaStreams()
deps/thrust/testing/cuda/generate.cu:  cudaStream_t s;
deps/thrust/testing/cuda/generate.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/generate.cu:  thrust::generate(thrust::cuda::par.on(s), result.begin(), result.end(), f);
deps/thrust/testing/cuda/generate.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/generate.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/generate.cu:DECLARE_UNITTEST(TestGenerateCudaStreams);
deps/thrust/testing/cuda/generate.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/generate.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/generate.cu:void TestGenerateNCudaStreams()
deps/thrust/testing/cuda/generate.cu:  cudaStream_t s;
deps/thrust/testing/cuda/generate.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/generate.cu:  thrust::generate_n(thrust::cuda::par.on(s), result.begin(), result.size(), f);
deps/thrust/testing/cuda/generate.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/generate.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/generate.cu:DECLARE_UNITTEST(TestGenerateNCudaStreams);
deps/thrust/testing/cuda/copy_if.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/copy_if.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/copy_if.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/copy_if.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/copy_if.cu:  TestCopyIfDevice(thrust::cuda::par_nosync);
deps/thrust/testing/cuda/copy_if.cu:void TestCopyIfCudaStreams(ExecutionPolicy policy)
deps/thrust/testing/cuda/copy_if.cu:  cudaStream_t s;
deps/thrust/testing/cuda/copy_if.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/copy_if.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/copy_if.cu:void TestCopyIfCudaStreamsSync(){
deps/thrust/testing/cuda/copy_if.cu:  TestCopyIfCudaStreams(thrust::cuda::par);
deps/thrust/testing/cuda/copy_if.cu:DECLARE_UNITTEST(TestCopyIfCudaStreamsSync);
deps/thrust/testing/cuda/copy_if.cu:void TestCopyIfCudaStreamsNoSync(){
deps/thrust/testing/cuda/copy_if.cu:  TestCopyIfCudaStreams(thrust::cuda::par_nosync);
deps/thrust/testing/cuda/copy_if.cu:DECLARE_UNITTEST(TestCopyIfCudaStreamsNoSync);
deps/thrust/testing/cuda/copy_if.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/copy_if.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/copy_if.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/copy_if.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/copy_if.cu:  TestCopyIfStencilDevice(thrust::cuda::par_nosync);
deps/thrust/testing/cuda/copy_if.cu:void TestCopyIfStencilCudaStreams(ExecutionPolicy policy)
deps/thrust/testing/cuda/copy_if.cu:  cudaStream_t s;
deps/thrust/testing/cuda/copy_if.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/copy_if.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/copy_if.cu:void TestCopyIfStencilCudaStreamsSync()
deps/thrust/testing/cuda/copy_if.cu:  TestCopyIfStencilCudaStreams(thrust::cuda::par);
deps/thrust/testing/cuda/copy_if.cu:DECLARE_UNITTEST(TestCopyIfStencilCudaStreamsSync);
deps/thrust/testing/cuda/copy_if.cu:void TestCopyIfStencilCudaStreamsNoSync()
deps/thrust/testing/cuda/copy_if.cu:  TestCopyIfStencilCudaStreams(thrust::cuda::par_nosync);
deps/thrust/testing/cuda/copy_if.cu:DECLARE_UNITTEST(TestCopyIfStencilCudaStreamsNoSync);
deps/thrust/testing/cuda/find.mk:CUDACC_FLAGS += -rdc=true
deps/thrust/testing/cuda/pinned_allocator.cu:#include <thrust/system/cuda/experimental/pinned_allocator.h>
deps/thrust/testing/cuda/pinned_allocator.cu:  typedef thrust::host_vector<T, thrust::cuda::experimental::pinned_allocator<T> > Vector;
deps/thrust/testing/cuda/set_intersection_by_key.mk:CUDACC_FLAGS += -rdc=true
deps/thrust/testing/cuda/merge_by_key.mk:CUDACC_FLAGS += -rdc=true
deps/thrust/testing/cuda/count.mk:CUDACC_FLAGS += -rdc=true
deps/thrust/testing/cuda/scatter.cu:  cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/scatter.cu:  ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/scatter.cu:  cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/scatter.cu:  ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/scatter.cu:void TestScatterCudaStreams()
deps/thrust/testing/cuda/scatter.cu:  cudaStream_t s;
deps/thrust/testing/cuda/scatter.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/scatter.cu:  thrust::scatter(thrust::cuda::par.on(s), src.begin(), src.end(), map.begin(), dst.begin());
deps/thrust/testing/cuda/scatter.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/scatter.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/scatter.cu:DECLARE_UNITTEST(TestScatterCudaStreams);
deps/thrust/testing/cuda/scatter.cu:void TestScatterIfCudaStreams()
deps/thrust/testing/cuda/scatter.cu:  cudaStream_t s;
deps/thrust/testing/cuda/scatter.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/scatter.cu:  thrust::scatter_if(thrust::cuda::par.on(s), src.begin(), src.end(), map.begin(), flg.begin(), dst.begin());
deps/thrust/testing/cuda/scatter.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/scatter.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/scatter.cu:DECLARE_UNITTEST(TestScatterIfCudaStreams);
deps/thrust/testing/cuda/remove.cu:  cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/remove.cu:  ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/remove.cu:  cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/remove.cu:  ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/remove.cu:  cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/remove.cu:  ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/remove.cu:  cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/remove.cu:  ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/remove.cu:  cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/remove.cu:  ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/remove.cu:  cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/remove.cu:  ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/remove.cu:void TestRemoveCudaStreams()
deps/thrust/testing/cuda/remove.cu:  cudaStream_t s;
deps/thrust/testing/cuda/remove.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/remove.cu:  Vector::iterator end = thrust::remove(thrust::cuda::par.on(s),
deps/thrust/testing/cuda/remove.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/remove.cu:DECLARE_UNITTEST(TestRemoveCudaStreams);
deps/thrust/testing/cuda/remove.cu:void TestRemoveCopyCudaStreams()
deps/thrust/testing/cuda/remove.cu:  cudaStream_t s;
deps/thrust/testing/cuda/remove.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/remove.cu:  Vector::iterator end = thrust::remove_copy(thrust::cuda::par.on(s),
deps/thrust/testing/cuda/remove.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/remove.cu:DECLARE_UNITTEST(TestRemoveCopyCudaStreams);
deps/thrust/testing/cuda/remove.cu:void TestRemoveIfCudaStreams()
deps/thrust/testing/cuda/remove.cu:  cudaStream_t s;
deps/thrust/testing/cuda/remove.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/remove.cu:  Vector::iterator end = thrust::remove_if(thrust::cuda::par.on(s),
deps/thrust/testing/cuda/remove.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/remove.cu:DECLARE_UNITTEST(TestRemoveIfCudaStreams);
deps/thrust/testing/cuda/remove.cu:void TestRemoveIfStencilCudaStreams()
deps/thrust/testing/cuda/remove.cu:  cudaStream_t s;
deps/thrust/testing/cuda/remove.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/remove.cu:  Vector::iterator end = thrust::remove_if(thrust::cuda::par.on(s),
deps/thrust/testing/cuda/remove.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/remove.cu:DECLARE_UNITTEST(TestRemoveIfStencilCudaStreams);
deps/thrust/testing/cuda/remove.cu:void TestRemoveCopyIfCudaStreams()
deps/thrust/testing/cuda/remove.cu:  cudaStream_t s;
deps/thrust/testing/cuda/remove.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/remove.cu:  Vector::iterator end = thrust::remove_copy_if(thrust::cuda::par.on(s),
deps/thrust/testing/cuda/remove.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/remove.cu:DECLARE_UNITTEST(TestRemoveCopyIfCudaStreams);
deps/thrust/testing/cuda/remove.cu:void TestRemoveCopyIfStencilCudaStreams()
deps/thrust/testing/cuda/remove.cu:  cudaStream_t s;
deps/thrust/testing/cuda/remove.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/remove.cu:  Vector::iterator end = thrust::remove_copy_if(thrust::cuda::par.on(s),
deps/thrust/testing/cuda/remove.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/remove.cu:DECLARE_UNITTEST(TestRemoveCopyIfStencilCudaStreams);
deps/thrust/testing/cuda/minmax_element.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/minmax_element.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/minmax_element.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/minmax_element.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/minmax_element.cu:void TestMinMaxElementCudaStreams()
deps/thrust/testing/cuda/minmax_element.cu:  cudaStream_t s;
deps/thrust/testing/cuda/minmax_element.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/minmax_element.cu:  ASSERT_EQUAL( *thrust::minmax_element(thrust::cuda::par.on(s), data.begin(), data.end()).first,  1);
deps/thrust/testing/cuda/minmax_element.cu:  ASSERT_EQUAL( *thrust::minmax_element(thrust::cuda::par.on(s), data.begin(), data.end()).second, 5);
deps/thrust/testing/cuda/minmax_element.cu:  ASSERT_EQUAL(  thrust::minmax_element(thrust::cuda::par.on(s), data.begin(), data.end()).first  - data.begin(), 2);
deps/thrust/testing/cuda/minmax_element.cu:  ASSERT_EQUAL(  thrust::minmax_element(thrust::cuda::par.on(s), data.begin(), data.end()).second - data.begin(), 1);
deps/thrust/testing/cuda/minmax_element.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/minmax_element.cu:DECLARE_UNITTEST(TestMinMaxElementCudaStreams);
deps/thrust/testing/cuda/scan_by_key.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/scan_by_key.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/scan_by_key.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/scan_by_key.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/scan_by_key.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/scan_by_key.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/scan_by_key.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/scan_by_key.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/scan_by_key.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/scan_by_key.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/scan_by_key.cu:void TestInclusiveScanByKeyCudaStreams()
deps/thrust/testing/cuda/scan_by_key.cu:  cudaStream_t s;
deps/thrust/testing/cuda/scan_by_key.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/scan_by_key.cu:  Iterator iter = thrust::inclusive_scan_by_key(thrust::cuda::par.on(s), keys.begin(), keys.end(), vals.begin(), output.begin());
deps/thrust/testing/cuda/scan_by_key.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/scan_by_key.cu:  thrust::inclusive_scan_by_key(thrust::cuda::par.on(s), keys.begin(), keys.end(), vals.begin(), output.begin(), thrust::equal_to<T>(), thrust::multiplies<T>());
deps/thrust/testing/cuda/scan_by_key.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/scan_by_key.cu:  thrust::inclusive_scan_by_key(thrust::cuda::par.on(s), keys.begin(), keys.end(), vals.begin(), output.begin(), thrust::equal_to<T>());
deps/thrust/testing/cuda/scan_by_key.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/scan_by_key.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/scan_by_key.cu:DECLARE_UNITTEST(TestInclusiveScanByKeyCudaStreams);
deps/thrust/testing/cuda/scan_by_key.cu:void TestExclusiveScanByKeyCudaStreams()
deps/thrust/testing/cuda/scan_by_key.cu:  cudaStream_t s;
deps/thrust/testing/cuda/scan_by_key.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/scan_by_key.cu:  Iterator iter = thrust::exclusive_scan_by_key(thrust::cuda::par.on(s), keys.begin(), keys.end(), vals.begin(), output.begin());
deps/thrust/testing/cuda/scan_by_key.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/scan_by_key.cu:  thrust::exclusive_scan_by_key(thrust::cuda::par.on(s), keys.begin(), keys.end(), vals.begin(), output.begin(), T(10));
deps/thrust/testing/cuda/scan_by_key.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/scan_by_key.cu:  thrust::exclusive_scan_by_key(thrust::cuda::par.on(s), keys.begin(), keys.end(), vals.begin(), output.begin(), T(10), thrust::equal_to<T>(), thrust::multiplies<T>());
deps/thrust/testing/cuda/scan_by_key.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/scan_by_key.cu:  thrust::exclusive_scan_by_key(thrust::cuda::par.on(s), keys.begin(), keys.end(), vals.begin(), output.begin(), T(10), thrust::equal_to<T>());
deps/thrust/testing/cuda/scan_by_key.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/scan_by_key.cu:DECLARE_UNITTEST(TestExclusiveScanByKeyCudaStreams);
deps/thrust/testing/cuda/replace.mk:CUDACC_FLAGS += -rdc=true
deps/thrust/testing/cuda/is_sorted.mk:CUDACC_FLAGS += -rdc=true
deps/thrust/testing/cuda/binary_search.mk:CUDACC_FLAGS += -rdc=true
deps/thrust/testing/cuda/set_symmetric_difference.mk:CUDACC_FLAGS += -rdc=true
deps/thrust/testing/cuda/transform_scan.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/transform_scan.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/transform_scan.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/transform_scan.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/transform_scan.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/transform_scan.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/transform_scan.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/transform_scan.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/transform_scan.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/transform_scan.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/transform_scan.cu:void TestTransformScanCudaStreams()
deps/thrust/testing/cuda/transform_scan.cu:  cudaStream_t s;
deps/thrust/testing/cuda/transform_scan.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/transform_scan.cu:  iter = thrust::transform_inclusive_scan(thrust::cuda::par.on(s), input.begin(), input.end(), output.begin(), thrust::negate<T>(), thrust::plus<T>());
deps/thrust/testing/cuda/transform_scan.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/transform_scan.cu:  iter = thrust::transform_exclusive_scan(thrust::cuda::par.on(s), input.begin(), input.end(), output.begin(), thrust::negate<T>(), 0, thrust::plus<T>());
deps/thrust/testing/cuda/transform_scan.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/transform_scan.cu:  iter = thrust::transform_exclusive_scan(thrust::cuda::par.on(s), input.begin(), input.end(), output.begin(), thrust::negate<T>(), 3, thrust::plus<T>());
deps/thrust/testing/cuda/transform_scan.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/transform_scan.cu:  iter = thrust::transform_inclusive_scan(thrust::cuda::par.on(s), input.begin(), input.end(), input.begin(), thrust::negate<T>(), thrust::plus<T>());
deps/thrust/testing/cuda/transform_scan.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/transform_scan.cu:  iter = thrust::transform_exclusive_scan(thrust::cuda::par.on(s), input.begin(), input.end(), input.begin(), thrust::negate<T>(), 3, thrust::plus<T>());
deps/thrust/testing/cuda/transform_scan.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/transform_scan.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/transform_scan.cu:DECLARE_UNITTEST(TestTransformScanCudaStreams);
deps/thrust/testing/cuda/uninitialized_fill.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/uninitialized_fill.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/uninitialized_fill.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/uninitialized_fill.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/uninitialized_fill.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/uninitialized_fill.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/uninitialized_fill.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/uninitialized_fill.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/uninitialized_fill.cu:void TestUninitializedFillCudaStreams()
deps/thrust/testing/cuda/uninitialized_fill.cu:  cudaStream_t s;
deps/thrust/testing/cuda/uninitialized_fill.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/uninitialized_fill.cu:  thrust::uninitialized_fill(thrust::cuda::par.on(s), v.begin(), v.end(), exemplar);
deps/thrust/testing/cuda/uninitialized_fill.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/uninitialized_fill.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/uninitialized_fill.cu:DECLARE_UNITTEST(TestUninitializedFillCudaStreams);
deps/thrust/testing/cuda/uninitialized_fill.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/uninitialized_fill.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/uninitialized_fill.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/uninitialized_fill.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/uninitialized_fill.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/uninitialized_fill.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/uninitialized_fill.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/uninitialized_fill.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/uninitialized_fill.cu:void TestUninitializedFillNCudaStreams()
deps/thrust/testing/cuda/uninitialized_fill.cu:  cudaStream_t s;
deps/thrust/testing/cuda/uninitialized_fill.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/uninitialized_fill.cu:  thrust::uninitialized_fill_n(thrust::cuda::par.on(s), v.begin(), v.size(), exemplar);
deps/thrust/testing/cuda/uninitialized_fill.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/uninitialized_fill.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/uninitialized_fill.cu:DECLARE_UNITTEST(TestUninitializedFillNCudaStreams);
deps/thrust/testing/cuda/pair_sort_by_key.cu:#if (__CUDA_ARCH__ >= 200)
deps/thrust/testing/cuda/pair_sort_by_key.cu:  cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/pair_sort_by_key.cu:  ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/replace.cu:  cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/replace.cu:  ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/replace.cu:  cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/replace.cu:  ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/replace.cu:  cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/replace.cu:  ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/replace.cu:  cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/replace.cu:  ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/replace.cu:  cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/replace.cu:  ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/replace.cu:  cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/replace.cu:  ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/replace.cu:void TestReplaceCudaStreams()
deps/thrust/testing/cuda/replace.cu:  cudaStream_t s;
deps/thrust/testing/cuda/replace.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/replace.cu:  thrust::replace(thrust::cuda::par.on(s), data.begin(), data.end(), (T) 1, (T) 4);
deps/thrust/testing/cuda/replace.cu:  thrust::replace(thrust::cuda::par.on(s), data.begin(), data.end(), (T) 2, (T) 5);
deps/thrust/testing/cuda/replace.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/replace.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/replace.cu:DECLARE_UNITTEST(TestReplaceCudaStreams);
deps/thrust/testing/cuda/partition.cu:  cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/partition.cu:  ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/partition.cu:  TestPartitionDevice(thrust::cuda::par_nosync);
deps/thrust/testing/cuda/partition.cu:  cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/partition.cu:  ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/partition.cu:  TestPartitionStencilDevice(thrust::cuda::par_nosync);
deps/thrust/testing/cuda/partition.cu:  cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/partition.cu:  ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/partition.cu:  TestPartitionCopyDevice(thrust::cuda::par_nosync);
deps/thrust/testing/cuda/partition.cu:  cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/partition.cu:  ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/partition.cu:  TestPartitionCopyStencilDevice(thrust::cuda::par_nosync);
deps/thrust/testing/cuda/partition.cu:#if (__CUDA_ARCH__ >= 200)
deps/thrust/testing/cuda/partition.cu:  cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/partition.cu:  ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/partition.cu:  TestStablePartitionDevice(thrust::cuda::par_nosync);
deps/thrust/testing/cuda/partition.cu:#if (__CUDA_ARCH__ >= 200)
deps/thrust/testing/cuda/partition.cu:  cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/partition.cu:  ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/partition.cu:  TestStablePartitionStencilDevice(thrust::cuda::par_nosync);
deps/thrust/testing/cuda/partition.cu:  cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/partition.cu:  ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/partition.cu:  TestStablePartitionCopyDevice(thrust::cuda::par_nosync);
deps/thrust/testing/cuda/partition.cu:  cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/partition.cu:  ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/partition.cu:  TestStablePartitionCopyStencilDevice(thrust::cuda::par_nosync);
deps/thrust/testing/cuda/partition.cu:void TestPartitionCudaStreams(ExecutionPolicy policy)
deps/thrust/testing/cuda/partition.cu:  cudaStream_t s;
deps/thrust/testing/cuda/partition.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/partition.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/partition.cu:void TestPartitionCudaStreamsSync()
deps/thrust/testing/cuda/partition.cu:  TestPartitionCudaStreams(thrust::cuda::par);
deps/thrust/testing/cuda/partition.cu:DECLARE_UNITTEST(TestPartitionCudaStreamsSync);
deps/thrust/testing/cuda/partition.cu:void TestPartitionCudaStreamsNoSync()
deps/thrust/testing/cuda/partition.cu:  TestPartitionCudaStreams(thrust::cuda::par_nosync);
deps/thrust/testing/cuda/partition.cu:DECLARE_UNITTEST(TestPartitionCudaStreamsNoSync);
deps/thrust/testing/cuda/scan_by_key.mk:CUDACC_FLAGS += -rdc=true
deps/thrust/testing/cuda/CMakeLists.txt:# compatible. See note in ThrustCudaConfig.cmake.
deps/thrust/testing/cuda/CMakeLists.txt:set(CMAKE_CUDA_FLAGS "${THRUST_CUDA_FLAGS_BASE} ${THRUST_CUDA_FLAGS_RDC}")
deps/thrust/testing/cuda/CMakeLists.txt:  if (NOT config_device STREQUAL "CUDA")
deps/thrust/testing/cuda/CMakeLists.txt:    string(PREPEND test_name "cuda.")
deps/thrust/testing/cuda/CMakeLists.txt:    # All in testing/cuda will test device-side launch (aka calling parallel
deps/thrust/testing/cuda/CMakeLists.txt:    # algorithms from device code), which requires the CUDA device-side runtime,
deps/thrust/testing/cuda/CMakeLists.txt:    thrust_enable_rdc_for_cuda_target(${test_target})
deps/thrust/testing/cuda/set_difference_by_key.cu:  cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/set_difference_by_key.cu:  ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/set_difference_by_key.cu:void TestSetDifferenceByKeyCudaStreams()
deps/thrust/testing/cuda/set_difference_by_key.cu:  cudaStream_t s;
deps/thrust/testing/cuda/set_difference_by_key.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/set_difference_by_key.cu:    thrust::set_difference_by_key(thrust::cuda::par.on(s),
deps/thrust/testing/cuda/set_difference_by_key.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/set_difference_by_key.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/set_difference_by_key.cu:DECLARE_UNITTEST(TestSetDifferenceByKeyCudaStreams);
deps/thrust/testing/cuda/tabulate.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/tabulate.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/tabulate.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/tabulate.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/tabulate.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/tabulate.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/tabulate.cu:void TestTabulateCudaStreams()
deps/thrust/testing/cuda/tabulate.cu:  cudaStream_t s;
deps/thrust/testing/cuda/tabulate.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/tabulate.cu:  thrust::tabulate(thrust::cuda::par.on(s), v.begin(), v.end(), thrust::identity<T>());
deps/thrust/testing/cuda/tabulate.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/tabulate.cu:  thrust::tabulate(thrust::cuda::par.on(s), v.begin(), v.end(), -_1);
deps/thrust/testing/cuda/tabulate.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/tabulate.cu:  thrust::tabulate(thrust::cuda::par.on(s), v.begin(), v.end(), _1 * _1 * _1);
deps/thrust/testing/cuda/tabulate.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/tabulate.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/tabulate.cu:DECLARE_UNITTEST(TestTabulateCudaStreams);
deps/thrust/testing/cuda/max_element.mk:CUDACC_FLAGS += -rdc=true
deps/thrust/testing/cuda/set_union_by_key.mk:CUDACC_FLAGS += -rdc=true
deps/thrust/testing/cuda/copy.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/copy.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/copy.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/copy.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/pair_sort.mk:CUDACC_FLAGS += -rdc=true
deps/thrust/testing/cuda/set_intersection.mk:CUDACC_FLAGS += -rdc=true
deps/thrust/testing/cuda/uninitialized_copy.mk:CUDACC_FLAGS += -rdc=true
deps/thrust/testing/cuda/memory.mk:CUDACC_FLAGS += -rdc=true
deps/thrust/testing/cuda/transform.cu:  cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/transform.cu:  ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/transform.cu:  cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/transform.cu:  ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/transform.cu:  cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/transform.cu:  ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/transform.cu:  cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/transform.cu:  ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/transform.cu:  cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/transform.cu:  ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/transform.cu:void TestTransformUnaryCudaStreams()
deps/thrust/testing/cuda/transform.cu:  cudaStream_t s;
deps/thrust/testing/cuda/transform.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/transform.cu:  iter = thrust::transform(thrust::cuda::par.on(s), input.begin(), input.end(), output.begin(), thrust::negate<T>());
deps/thrust/testing/cuda/transform.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/transform.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/transform.cu:DECLARE_UNITTEST(TestTransformUnaryCudaStreams);
deps/thrust/testing/cuda/transform.cu:void TestTransformBinaryCudaStreams()
deps/thrust/testing/cuda/transform.cu:  cudaStream_t s;
deps/thrust/testing/cuda/transform.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/transform.cu:  iter = thrust::transform(thrust::cuda::par.on(s), input1.begin(), input1.end(), input2.begin(), output.begin(), thrust::minus<T>());
deps/thrust/testing/cuda/transform.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/transform.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/transform.cu:DECLARE_UNITTEST(TestTransformBinaryCudaStreams);
deps/thrust/testing/cuda/merge_sort.mk:CUDACC_FLAGS += -rdc=true
deps/thrust/testing/cuda/logical.mk:CUDACC_FLAGS += -rdc=true
deps/thrust/testing/cuda/set_difference.mk:CUDACC_FLAGS += -rdc=true
deps/thrust/testing/cuda/is_sorted_until.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/is_sorted_until.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/is_sorted_until.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/is_sorted_until.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/is_sorted_until.cu:void TestIsSortedUntilCudaStreams()
deps/thrust/testing/cuda/is_sorted_until.cu:  cudaStream_t s;
deps/thrust/testing/cuda/is_sorted_until.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/is_sorted_until.cu:  ASSERT_EQUAL_QUIET(ref, thrust::is_sorted_until(thrust::cuda::par.on(s), first, last));
deps/thrust/testing/cuda/is_sorted_until.cu:  ASSERT_EQUAL_QUIET(ref, thrust::is_sorted_until(thrust::cuda::par.on(s), first, last));
deps/thrust/testing/cuda/is_sorted_until.cu:  ASSERT_EQUAL_QUIET(ref, thrust::is_sorted_until(thrust::cuda::par.on(s), first, last));
deps/thrust/testing/cuda/is_sorted_until.cu:  ASSERT_EQUAL_QUIET(ref, thrust::is_sorted_until(thrust::cuda::par.on(s), first, last));
deps/thrust/testing/cuda/is_sorted_until.cu:  ASSERT_EQUAL_QUIET(ref, thrust::is_sorted_until(thrust::cuda::par.on(s), first, last));
deps/thrust/testing/cuda/is_sorted_until.cu:  ASSERT_EQUAL_QUIET(ref, thrust::is_sorted_until(thrust::cuda::par.on(s), first, last, thrust::less<T>()));
deps/thrust/testing/cuda/is_sorted_until.cu:  ASSERT_EQUAL_QUIET(ref, thrust::is_sorted_until(thrust::cuda::par.on(s), first, last, thrust::less<T>()));
deps/thrust/testing/cuda/is_sorted_until.cu:  ASSERT_EQUAL_QUIET(ref, thrust::is_sorted_until(thrust::cuda::par.on(s), first, last, thrust::greater<T>()));
deps/thrust/testing/cuda/is_sorted_until.cu:  ASSERT_EQUAL_QUIET(ref, thrust::is_sorted_until(thrust::cuda::par.on(s), first, last, thrust::greater<T>()));
deps/thrust/testing/cuda/is_sorted_until.cu:  ASSERT_EQUAL_QUIET(ref, thrust::is_sorted_until(thrust::cuda::par.on(s), first, last, thrust::greater<T>()));
deps/thrust/testing/cuda/is_sorted_until.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/is_sorted_until.cu:DECLARE_UNITTEST(TestIsSortedUntilCudaStreams);
deps/thrust/testing/cuda/adjacent_difference.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/adjacent_difference.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/adjacent_difference.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/adjacent_difference.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/adjacent_difference.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/adjacent_difference.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/adjacent_difference.cu:void TestAdjacentDifferenceCudaStreams()
deps/thrust/testing/cuda/adjacent_difference.cu:  cudaStream_t s;
deps/thrust/testing/cuda/adjacent_difference.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/adjacent_difference.cu:  thrust::adjacent_difference(thrust::cuda::par.on(s), input.begin(), input.end(), output.begin());
deps/thrust/testing/cuda/adjacent_difference.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/adjacent_difference.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/adjacent_difference.cu:DECLARE_UNITTEST(TestAdjacentDifferenceCudaStreams);
deps/thrust/testing/cuda/merge.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/merge.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/merge.cu:void TestMergeCudaStreams()
deps/thrust/testing/cuda/merge.cu:  cudaStream_t s;
deps/thrust/testing/cuda/merge.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/merge.cu:  Iterator end = thrust::merge(thrust::cuda::par.on(s),
deps/thrust/testing/cuda/merge.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/merge.cu:DECLARE_UNITTEST(TestMergeCudaStreams);
deps/thrust/testing/cuda/merge_sort.cu:    thrust::cuda_bulk::tag cuda_tag;
deps/thrust/testing/cuda/merge_sort.cu:    thrust::system::cuda_bulk::detail::detail::stable_merge_sort(cuda_tag, unsorted_keys.begin(), unsorted_keys.end(), thrust::less<T>());
deps/thrust/testing/cuda/merge_sort.cu:    thrust::cuda_bulk::tag cuda_tag;
deps/thrust/testing/cuda/merge_sort.cu:    thrust::system::cuda_bulk::detail::detail::stable_merge_sort_by_key(cuda_tag, unsorted_keys.begin(), unsorted_keys.end(), unsorted_values.begin(), thrust::less<T>());
deps/thrust/testing/cuda/merge_sort.cu:    thrust::cuda_bulk::tag cuda_tag;
deps/thrust/testing/cuda/merge_sort.cu:    thrust::system::cuda_bulk::detail::detail::stable_merge_sort(cuda_tag, unsorted_keys.begin(), unsorted_keys.end(), less_div_10<T>());
deps/thrust/testing/cuda/merge_sort.cu:    thrust::cuda_bulk::tag cuda_tag;
deps/thrust/testing/cuda/merge_sort.cu:    thrust::system::cuda_bulk::detail::detail::stable_merge_sort(cuda_tag, d_data.begin(), d_data.end(), thrust::greater<int>());
deps/thrust/testing/cuda/merge_sort.cu:    thrust::cuda_bulk::tag cuda_tag;
deps/thrust/testing/cuda/merge_sort.cu:    thrust::system::cuda_bulk::detail::detail::stable_merge_sort_by_key(cuda_tag, d_keys.begin(), d_keys.end(), d_values.begin(), thrust::less<T>());
deps/thrust/testing/cuda/merge_sort.cu:    thrust::cuda_bulk::tag cuda_tag;
deps/thrust/testing/cuda/merge_sort.cu:    thrust::system::cuda_bulk::detail::detail::stable_merge_sort_by_key(cuda_tag, d_keys.begin(), d_keys.end(), d_values.begin(), thrust::greater<int>());
deps/thrust/testing/cuda/merge_sort.cu:  thrust::cuda_bulk::tag cuda_tag;
deps/thrust/testing/cuda/merge_sort.cu:  thrust::system::cuda_bulk::detail::detail::stable_merge_sort(cuda_tag, d_data.begin(), d_data.end(), thrust::less<T>());
deps/thrust/testing/cuda/reverse.mk:CUDACC_FLAGS += -rdc=true
deps/thrust/testing/cuda/swap_ranges.mk:CUDACC_FLAGS += -rdc=true
deps/thrust/testing/cuda/reduce.cu:  cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/reduce.cu:  ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/reduce.cu:    TestReduceDevice<T>(thrust::cuda::par_nosync, n);
deps/thrust/testing/cuda/reduce.cu:void TestReduceCudaStreams(ExecutionPolicy policy)
deps/thrust/testing/cuda/reduce.cu:  cudaStream_t s;
deps/thrust/testing/cuda/reduce.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/reduce.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/reduce.cu:void TestReduceCudaStreamsSync()
deps/thrust/testing/cuda/reduce.cu:  TestReduceCudaStreams(thrust::cuda::par);
deps/thrust/testing/cuda/reduce.cu:DECLARE_UNITTEST(TestReduceCudaStreamsSync);
deps/thrust/testing/cuda/reduce.cu:void TestReduceCudaStreamsNoSync()
deps/thrust/testing/cuda/reduce.cu:  TestReduceCudaStreams(thrust::cuda::par_nosync);
deps/thrust/testing/cuda/reduce.cu:DECLARE_UNITTEST(TestReduceCudaStreamsNoSync);
deps/thrust/testing/cuda/sort.cu:#if (__CUDA_ARCH__ >= 200)
deps/thrust/testing/cuda/sort.cu:  cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/sort.cu:  ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/sort.cu:void TestSortCudaStreams()
deps/thrust/testing/cuda/sort.cu:  cudaStream_t s;
deps/thrust/testing/cuda/sort.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/sort.cu:  thrust::sort(thrust::cuda::par.on(s), keys.begin(), keys.end());
deps/thrust/testing/cuda/sort.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/sort.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/sort.cu:DECLARE_UNITTEST(TestSortCudaStreams);
deps/thrust/testing/cuda/sort.cu:void TestComparisonSortCudaStreams()
deps/thrust/testing/cuda/sort.cu:  cudaStream_t s;
deps/thrust/testing/cuda/sort.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/sort.cu:  thrust::sort(thrust::cuda::par.on(s), keys.begin(), keys.end(), my_less<int>());
deps/thrust/testing/cuda/sort.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/sort.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/sort.cu:DECLARE_UNITTEST(TestComparisonSortCudaStreams);
deps/thrust/testing/cuda/memory.cu:#include <thrust/system/cuda/memory.h>
deps/thrust/testing/cuda/memory.cu:void TestSelectSystemCudaToCpp()
deps/thrust/testing/cuda/memory.cu:  thrust::cuda::tag cuda_tag;
deps/thrust/testing/cuda/memory.cu:  thrust::cuda_cub::cross_system<thrust::cuda::tag,thrust::cpp::tag> cuda_to_cpp(cuda_tag, cpp_tag);
deps/thrust/testing/cuda/memory.cu:  // select_system(cuda::tag, thrust::host_system_tag) should return cuda_to_cpp
deps/thrust/testing/cuda/memory.cu:  bool is_cuda_to_cpp = are_same_type(cuda_to_cpp, select_system(cuda_tag, cpp_tag));
deps/thrust/testing/cuda/memory.cu:  ASSERT_EQUAL(true, is_cuda_to_cpp);
deps/thrust/testing/cuda/memory.cu:DECLARE_UNITTEST(TestSelectSystemCudaToCpp);
deps/thrust/testing/cuda/memory.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/memory.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/memory.cu:      cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/memory.cu:      ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/memory.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/memory.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/memory.cu:      cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/memory.cu:      ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/for_each.mk:CUDACC_FLAGS += -rdc=true
deps/thrust/testing/cuda/stream_per_thread.mk:CUDACC_FLAGS += --default-stream per-thread
deps/thrust/testing/cuda/partition_point.mk:CUDACC_FLAGS += -rdc=true
deps/thrust/testing/cuda/set_union.mk:CUDACC_FLAGS += -rdc=true
deps/thrust/testing/cuda/cudart.cu:#include <cuda_runtime_api.h>
deps/thrust/testing/cuda/cudart.cu:void TestCudaMallocResultAligned(const std::size_t n)
deps/thrust/testing/cuda/cudart.cu:  cudaMalloc(&ptr, n * sizeof(T));
deps/thrust/testing/cuda/cudart.cu:  cudaFree(ptr);
deps/thrust/testing/cuda/cudart.cu:DECLARE_VARIABLE_UNITTEST(TestCudaMallocResultAligned);
deps/thrust/testing/cuda/managed_memory_pointer.mk:CUDACC_FLAGS += -rdc=true
deps/thrust/testing/cuda/is_sorted_until.mk:CUDACC_FLAGS += -rdc=true
deps/thrust/testing/cuda/tabulate.mk:CUDACC_FLAGS += -rdc=true
deps/thrust/testing/cuda/logical.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/logical.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/logical.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/logical.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/logical.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/logical.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/logical.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/logical.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/logical.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/logical.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/logical.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/logical.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/logical.cu:void TestAllOfCudaStreams()
deps/thrust/testing/cuda/logical.cu:  cudaStream_t s;
deps/thrust/testing/cuda/logical.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/logical.cu:  ASSERT_EQUAL(thrust::all_of(thrust::cuda::par.on(s), v.begin(), v.end(), thrust::identity<T>()), true);
deps/thrust/testing/cuda/logical.cu:  ASSERT_EQUAL(thrust::all_of(thrust::cuda::par.on(s), v.begin(), v.end(), thrust::identity<T>()), false);
deps/thrust/testing/cuda/logical.cu:  ASSERT_EQUAL(thrust::all_of(thrust::cuda::par.on(s), v.begin() + 0, v.begin() + 0, thrust::identity<T>()), true);
deps/thrust/testing/cuda/logical.cu:  ASSERT_EQUAL(thrust::all_of(thrust::cuda::par.on(s), v.begin() + 0, v.begin() + 1, thrust::identity<T>()), true);
deps/thrust/testing/cuda/logical.cu:  ASSERT_EQUAL(thrust::all_of(thrust::cuda::par.on(s), v.begin() + 0, v.begin() + 2, thrust::identity<T>()), false);
deps/thrust/testing/cuda/logical.cu:  ASSERT_EQUAL(thrust::all_of(thrust::cuda::par.on(s), v.begin() + 1, v.begin() + 2, thrust::identity<T>()), false);
deps/thrust/testing/cuda/logical.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/logical.cu:DECLARE_UNITTEST(TestAllOfCudaStreams);
deps/thrust/testing/cuda/logical.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/logical.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/logical.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/logical.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/logical.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/logical.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/logical.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/logical.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/logical.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/logical.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/logical.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/logical.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/logical.cu:void TestAnyOfCudaStreams()
deps/thrust/testing/cuda/logical.cu:  cudaStream_t s;
deps/thrust/testing/cuda/logical.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/logical.cu:  ASSERT_EQUAL(thrust::any_of(thrust::cuda::par.on(s), v.begin(), v.end(), thrust::identity<T>()), true);
deps/thrust/testing/cuda/logical.cu:  ASSERT_EQUAL(thrust::any_of(thrust::cuda::par.on(s), v.begin(), v.end(), thrust::identity<T>()), true);
deps/thrust/testing/cuda/logical.cu:  ASSERT_EQUAL(thrust::any_of(thrust::cuda::par.on(s), v.begin() + 0, v.begin() + 0, thrust::identity<T>()), false);
deps/thrust/testing/cuda/logical.cu:  ASSERT_EQUAL(thrust::any_of(thrust::cuda::par.on(s), v.begin() + 0, v.begin() + 1, thrust::identity<T>()), true);
deps/thrust/testing/cuda/logical.cu:  ASSERT_EQUAL(thrust::any_of(thrust::cuda::par.on(s), v.begin() + 0, v.begin() + 2, thrust::identity<T>()), true);
deps/thrust/testing/cuda/logical.cu:  ASSERT_EQUAL(thrust::any_of(thrust::cuda::par.on(s), v.begin() + 1, v.begin() + 2, thrust::identity<T>()), false);
deps/thrust/testing/cuda/logical.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/logical.cu:DECLARE_UNITTEST(TestAnyOfCudaStreams);
deps/thrust/testing/cuda/logical.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/logical.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/logical.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/logical.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/logical.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/logical.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/logical.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/logical.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/logical.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/logical.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/logical.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/logical.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/logical.cu:void TestNoneOfCudaStreams()
deps/thrust/testing/cuda/logical.cu:  cudaStream_t s;
deps/thrust/testing/cuda/logical.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/logical.cu:  ASSERT_EQUAL(thrust::none_of(thrust::cuda::par.on(s), v.begin(), v.end(), thrust::identity<T>()), false);
deps/thrust/testing/cuda/logical.cu:  ASSERT_EQUAL(thrust::none_of(thrust::cuda::par.on(s), v.begin(), v.end(), thrust::identity<T>()), false);
deps/thrust/testing/cuda/logical.cu:  ASSERT_EQUAL(thrust::none_of(thrust::cuda::par.on(s), v.begin() + 0, v.begin() + 0, thrust::identity<T>()), true);
deps/thrust/testing/cuda/logical.cu:  ASSERT_EQUAL(thrust::none_of(thrust::cuda::par.on(s), v.begin() + 0, v.begin() + 1, thrust::identity<T>()), false);
deps/thrust/testing/cuda/logical.cu:  ASSERT_EQUAL(thrust::none_of(thrust::cuda::par.on(s), v.begin() + 0, v.begin() + 2, thrust::identity<T>()), false);
deps/thrust/testing/cuda/logical.cu:  ASSERT_EQUAL(thrust::none_of(thrust::cuda::par.on(s), v.begin() + 1, v.begin() + 2, thrust::identity<T>()), true);
deps/thrust/testing/cuda/logical.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/logical.cu:DECLARE_UNITTEST(TestNoneOfCudaStreams);
deps/thrust/testing/cuda/for_each.cu:  cudaGetDevice(&current_device);
deps/thrust/testing/cuda/for_each.cu:  cudaDeviceProp prop;
deps/thrust/testing/cuda/for_each.cu:  cudaGetDeviceProperties(&prop, current_device);
deps/thrust/testing/cuda/for_each.cu:  cudaGetDevice(&current_device);
deps/thrust/testing/cuda/for_each.cu:  cudaDeviceProp prop;
deps/thrust/testing/cuda/for_each.cu:  cudaGetDeviceProperties(&prop, current_device);
deps/thrust/testing/cuda/for_each.cu:  cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/for_each.cu:  ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/for_each.cu:    cudaError_t const err = cudaGetLastError();
deps/thrust/testing/cuda/for_each.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/for_each.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/for_each.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/for_each.cu:  cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/for_each.cu:  ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/for_each.cu:  cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/for_each.cu:  ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/for_each.cu:void TestForEachCudaStreams()
deps/thrust/testing/cuda/for_each.cu:  cudaStream_t s;
deps/thrust/testing/cuda/for_each.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/for_each.cu:  thrust::for_each(thrust::cuda::par.on(s), input.begin(), input.end(), f);
deps/thrust/testing/cuda/for_each.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/for_each.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/for_each.cu:DECLARE_UNITTEST(TestForEachCudaStreams);
deps/thrust/testing/cuda/copy_if.mk:CUDACC_FLAGS += -rdc=true
deps/thrust/testing/cuda/min_element.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/min_element.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/min_element.cu:    cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/min_element.cu:    ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/min_element.cu:void TestMinElementCudaStreams()
deps/thrust/testing/cuda/min_element.cu:  cudaStream_t s;
deps/thrust/testing/cuda/min_element.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/min_element.cu:  ASSERT_EQUAL( *thrust::min_element(thrust::cuda::par.on(s), data.begin(), data.end()), 1);
deps/thrust/testing/cuda/min_element.cu:  ASSERT_EQUAL( thrust::min_element(thrust::cuda::par.on(s), data.begin(), data.end()) - data.begin(), 2);
deps/thrust/testing/cuda/min_element.cu:  ASSERT_EQUAL( *thrust::min_element(thrust::cuda::par.on(s), data.begin(), data.end(), thrust::greater<T>()), 5);
deps/thrust/testing/cuda/min_element.cu:  ASSERT_EQUAL( thrust::min_element(thrust::cuda::par.on(s), data.begin(), data.end(), thrust::greater<T>()) - data.begin(), 1);
deps/thrust/testing/cuda/min_element.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/min_element.cu:DECLARE_UNITTEST(TestMinElementCudaStreams);
deps/thrust/testing/cuda/stream_legacy.cu:#include <thrust/system/cuda/detail/util.h>
deps/thrust/testing/cuda/stream_legacy.cu:  auto stream = thrust::cuda_cub::stream(exec);
deps/thrust/testing/cuda/stream_legacy.cu:  ASSERT_EQUAL(stream, cudaStreamLegacy);
deps/thrust/testing/cuda/inner_product.mk:CUDACC_FLAGS += -rdc=true
deps/thrust/testing/cuda/set_union_by_key.cu:  cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/set_union_by_key.cu:  ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/set_union_by_key.cu:void TestSetUnionByKeyCudaStreams()
deps/thrust/testing/cuda/set_union_by_key.cu:  cudaStream_t s;
deps/thrust/testing/cuda/set_union_by_key.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/set_union_by_key.cu:    thrust::set_union_by_key(thrust::cuda::par.on(s),
deps/thrust/testing/cuda/set_union_by_key.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/set_union_by_key.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/set_union_by_key.cu:DECLARE_UNITTEST(TestSetUnionByKeyCudaStreams);
deps/thrust/testing/cuda/transform_reduce.cu:  cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/transform_reduce.cu:  ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/transform_reduce.cu:void TestTransformReduceCudaStreams()
deps/thrust/testing/cuda/transform_reduce.cu:  cudaStream_t s;
deps/thrust/testing/cuda/transform_reduce.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/transform_reduce.cu:  T result = thrust::transform_reduce(thrust::cuda::par.on(s), data.begin(), data.end(), thrust::negate<T>(), init, thrust::plus<T>());
deps/thrust/testing/cuda/transform_reduce.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/transform_reduce.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/transform_reduce.cu:DECLARE_UNITTEST(TestTransformReduceCudaStreams);
deps/thrust/testing/cuda/pair_sort.cu:#if (__CUDA_ARCH__ >= 200)
deps/thrust/testing/cuda/pair_sort.cu:  cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/pair_sort.cu:  ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/sort.mk:CUDACC_FLAGS += -rdc=true
deps/thrust/testing/cuda/set_intersection_by_key.cu:  cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/set_intersection_by_key.cu:  ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/set_intersection_by_key.cu:  TestSetIntersectionByKeyDevice(thrust::cuda::par_nosync);
deps/thrust/testing/cuda/set_intersection_by_key.cu:void TestSetIntersectionByKeyCudaStreams(ExecutionPolicy policy)
deps/thrust/testing/cuda/set_intersection_by_key.cu:  cudaStream_t s;
deps/thrust/testing/cuda/set_intersection_by_key.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/set_intersection_by_key.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/set_intersection_by_key.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/set_intersection_by_key.cu:void TestSetIntersectionByKeyCudaStreamsSync()
deps/thrust/testing/cuda/set_intersection_by_key.cu:  TestSetIntersectionByKeyCudaStreams(thrust::cuda::par);
deps/thrust/testing/cuda/set_intersection_by_key.cu:DECLARE_UNITTEST(TestSetIntersectionByKeyCudaStreamsSync);
deps/thrust/testing/cuda/set_intersection_by_key.cu:void TestSetIntersectionByKeyCudaStreamsNoSync()
deps/thrust/testing/cuda/set_intersection_by_key.cu:  TestSetIntersectionByKeyCudaStreams(thrust::cuda::par_nosync);
deps/thrust/testing/cuda/set_intersection_by_key.cu:DECLARE_UNITTEST(TestSetIntersectionByKeyCudaStreamsNoSync);
deps/thrust/testing/cuda/set_difference_by_key.mk:CUDACC_FLAGS += -rdc=true
deps/thrust/testing/cuda/swap_ranges.cu:  cudaError_t const err = cudaDeviceSynchronize();
deps/thrust/testing/cuda/swap_ranges.cu:  ASSERT_EQUAL(cudaSuccess, err);
deps/thrust/testing/cuda/swap_ranges.cu:void TestSwapRangesCudaStreams()
deps/thrust/testing/cuda/swap_ranges.cu:  cudaStream_t s;
deps/thrust/testing/cuda/swap_ranges.cu:  cudaStreamCreate(&s);
deps/thrust/testing/cuda/swap_ranges.cu:  thrust::swap_ranges(thrust::cuda::par.on(s), v1.begin(), v1.end(), v2.begin());
deps/thrust/testing/cuda/swap_ranges.cu:  cudaStreamSynchronize(s);
deps/thrust/testing/cuda/swap_ranges.cu:  cudaStreamDestroy(s);
deps/thrust/testing/cuda/swap_ranges.cu:DECLARE_UNITTEST(TestSwapRangesCudaStreams);
deps/thrust/testing/cuda/transform.mk:CUDACC_FLAGS += -rdc=true
deps/thrust/testing/cuda/unique.mk:CUDACC_FLAGS += -rdc=true
deps/thrust/testing/cuda/minmax_element.mk:CUDACC_FLAGS += -rdc=true
deps/thrust/testing/cuda/copy.mk:CUDACC_FLAGS += -rdc=true
deps/thrust/testing/out_of_memory_recovery.cu:// 5. Launching that parallel algorithm fails because of the prior CUDA out of memory error.
deps/thrust/testing/stable_sort_by_key_large.cu:    // so cuda::stable_merge_sort_by_key() is called
deps/thrust/testing/transform_reduce.cu:    T gpu_result = thrust::transform_reduce(d_data.begin(), d_data.end(), thrust::negate<T>(), init, thrust::plus<T>());
deps/thrust/testing/transform_reduce.cu:    ASSERT_ALMOST_EQUAL(cpu_result, gpu_result);
deps/thrust/testing/transform_reduce.cu:    T gpu_result = thrust::transform_reduce(d_data.cbegin(), d_data.cend(), thrust::negate<T>(), init, thrust::plus<T>());
deps/thrust/testing/transform_reduce.cu:    ASSERT_ALMOST_EQUAL(cpu_result, gpu_result);
deps/thrust/testing/zip_iterator_reduce_by_key.cu:#if THRUST_DEVICE_SYSTEM == THRUST_DEVICE_SYSTEM_CUDA
deps/thrust/testing/zip_iterator_reduce_by_key.cu:#include <unittest/cuda/testframework.h>
deps/thrust/testing/zip_iterator_reduce_by_key.cu:#if THRUST_DEVICE_SYSTEM == THRUST_DEVICE_SYSTEM_CUDA
deps/thrust/testing/zip_iterator_reduce_by_key.cu:    if(const CUDATestDriver *driver = dynamic_cast<const CUDATestDriver*>(&UnitTestDriver::s_driver()))
deps/thrust/testing/dependencies_aware_policies.cu:#if THRUST_DEVICE_SYSTEM == THRUST_DEVICE_SYSTEM_CUDA
deps/thrust/testing/dependencies_aware_policies.cu:#  include <thrust/system/cuda/detail/par.h>
deps/thrust/testing/dependencies_aware_policies.cu:#if THRUST_DEVICE_SYSTEM == THRUST_DEVICE_SYSTEM_CUDA
deps/thrust/testing/dependencies_aware_policies.cu:    thrust::system::cuda::detail::par_t,
deps/thrust/testing/dependencies_aware_policies.cu:    thrust::cuda_cub::execute_on_stream_base
deps/thrust/testing/dependencies_aware_policies.cu:> cuda_par_info;
deps/thrust/testing/dependencies_aware_policies.cu:#if THRUST_DEVICE_SYSTEM == THRUST_DEVICE_SYSTEM_CUDA
deps/thrust/testing/dependencies_aware_policies.cu:        cuda_par_info
deps/thrust/thrust/device_allocator.h: *  Copyright 2008-2018 NVIDIA Corporation
deps/thrust/thrust/mr/allocator.h: *  Copyright 2018 NVIDIA Corporation
deps/thrust/thrust/mr/polymorphic_adaptor.h: *  Copyright 2018-2019 NVIDIA Corporation
deps/thrust/thrust/mr/pool_options.h: *  Copyright 2018 NVIDIA Corporation
deps/thrust/thrust/mr/disjoint_sync_pool.h: *  Copyright 2018 NVIDIA Corporation
deps/thrust/thrust/mr/disjoint_tls_pool.h: *  Copyright 2018 NVIDIA Corporation
deps/thrust/thrust/mr/device_memory_resource.h: *  Copyright 2018-2020 NVIDIA Corporation
deps/thrust/thrust/mr/fancy_pointer_resource.h: *  Copyright 2018 NVIDIA Corporation
deps/thrust/thrust/mr/sync_pool.h: *  Copyright 2018 NVIDIA Corporation
deps/thrust/thrust/mr/pool.h: *  Copyright 2018 NVIDIA Corporation
deps/thrust/thrust/mr/pool.h: *      meaning that the non-managed CUDA resource, returning a device-tagged pointer, will work, but will be much less
deps/thrust/thrust/mr/disjoint_pool.h: *  Copyright 2018 NVIDIA Corporation
deps/thrust/thrust/mr/disjoint_pool.h: *      information in memory obtained from \p Upstream; for instance, \p Upstream can be a CUDA non-managed memory
deps/thrust/thrust/mr/disjoint_pool.h: *      resource, or a CUDA managed memory resource whose memory we would prefer to not migrate back and forth between
deps/thrust/thrust/mr/new.h: *  Copyright 2018 NVIDIA Corporation
deps/thrust/thrust/mr/memory_resource.h: *  Copyright 2018 NVIDIA Corporation
deps/thrust/thrust/mr/host_memory_resource.h: *  Copyright 2018-2020 NVIDIA Corporation
deps/thrust/thrust/mr/tls_pool.h: *  Copyright 2018 NVIDIA Corporation
deps/thrust/thrust/mr/validator.h: *  Copyright 2018 NVIDIA Corporation
deps/thrust/thrust/mr/universal_memory_resource.h: *  Copyright 2018-2020 NVIDIA Corporation
deps/thrust/thrust/mismatch.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/shuffle.h: *  Copyright 2008-2020 NVIDIA Corporation
deps/thrust/thrust/device_ptr.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/adjacent_difference.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/caching_allocator.h: *  Copyright 2020 NVIDIA Corporation
deps/thrust/thrust/detail/device_new.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/gather.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/reference_forward_declaration.h: *  Copyright 2008-2020 NVIDIA Corporation
deps/thrust/thrust/detail/execute_with_allocator_fwd.h: *  Copyright 2008-2018 NVIDIA Corporation
deps/thrust/thrust/detail/execute_with_dependencies.h: *  Copyright 2018 NVIDIA Corporation
deps/thrust/thrust/detail/distance.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/cpp11_required.h: *  Copyright 2018 NVIDIA Corporation
deps/thrust/thrust/detail/extrema.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/device_ptr.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/range/tail_flags.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/range/tail_flags.h:  // XXX WAR cudafe bug
deps/thrust/thrust/detail/range/head_flags.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/range/head_flags.h:  // XXX WAR cudafe issue
deps/thrust/thrust/detail/range/head_flags.h:  // XXX WAR cudafe issue
deps/thrust/thrust/detail/sequence.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/logical.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/reduce.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/tuple_algorithms.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/scatter.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/device_delete.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/contiguous_storage.inl: *  Copyright 2008-2018 NVIDIA Corporation
deps/thrust/thrust/detail/config/debug.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/config/compiler.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/config/compiler.h:#if defined(__CUDACC__) || defined(_NVHPC_CUDA)
deps/thrust/thrust/detail/config/compiler.h:// CUDA-capable clang should behave similar to NVCC.
deps/thrust/thrust/detail/config/compiler.h:#if defined(__CUDA__)
deps/thrust/thrust/detail/config/compiler.h:#if (THRUST_HOST_COMPILER == THRUST_HOST_COMPILER_MSVC) && !defined(__CUDA_ARCH__)
deps/thrust/thrust/detail/config/compiler.h:#if (THRUST_HOST_COMPILER == THRUST_HOST_COMPILER_CLANG) && !defined(__CUDA_ARCH__)
deps/thrust/thrust/detail/config/compiler.h:#if (THRUST_HOST_COMPILER == THRUST_HOST_COMPILER_GCC) && !defined(__CUDA_ARCH__)
deps/thrust/thrust/detail/config/simple_defines.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/config/global_workarounds.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/config/global_workarounds.h:#  if defined(__NVCC__) && (CUDART_VERSION >= 6000)
deps/thrust/thrust/detail/config/global_workarounds.h:#  endif // nvcc & cuda 6+
deps/thrust/thrust/detail/config/config.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/config/cpp_dialect.h: *  Copyright 2020 NVIDIA Corporation
deps/thrust/thrust/detail/config/device_system.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/config/device_system.h:#define THRUST_DEVICE_SYSTEM_CUDA    1
deps/thrust/thrust/detail/config/device_system.h:#define THRUST_DEVICE_SYSTEM THRUST_DEVICE_SYSTEM_CUDA
deps/thrust/thrust/detail/config/device_system.h:#define THRUST_DEVICE_BACKEND_CUDA THRUST_DEVICE_SYSTEM_CUDA
deps/thrust/thrust/detail/config/device_system.h:#if THRUST_DEVICE_SYSTEM == THRUST_DEVICE_SYSTEM_CUDA
deps/thrust/thrust/detail/config/device_system.h:#define __THRUST_DEVICE_SYSTEM_NAMESPACE cuda
deps/thrust/thrust/detail/config/host_system.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/config/cpp_compatibility.h: *  Copyright 2008-2018 NVIDIA Corporation
deps/thrust/thrust/detail/config/cpp_compatibility.h:#if defined(__CUDA_ARCH__) || defined(_NVHPC_CUDA)
deps/thrust/thrust/detail/config/cpp_compatibility.h:#if defined(_NVHPC_CUDA)
deps/thrust/thrust/detail/config/cpp_compatibility.h:#elif defined(__CUDA_ARCH__)
deps/thrust/thrust/detail/config/memory_resource.h: *  Copyright 2018 NVIDIA Corporation
deps/thrust/thrust/detail/config/deprecated.h: *  Copyright 2018-2020 NVIDIA Corporation
deps/thrust/thrust/detail/config/compiler_fence.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/config/host_device.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/config/exec_check_disable.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/config/exec_check_disable.h:#if defined(__CUDACC__) && !defined(_NVHPC_CUDA) && \
deps/thrust/thrust/detail/config/exec_check_disable.h:    !(defined(__CUDA__) && defined(__clang__))
deps/thrust/thrust/detail/config/forceinline.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/config/forceinline.h:#if defined(__CUDACC__) || defined(_NVHPC_CUDA)
deps/thrust/thrust/detail/config/namespace.h: *  Copyright 2021 NVIDIA Corporation
deps/thrust/thrust/detail/function.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/pair.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/complex/csinh.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/complex/csqrtf.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/complex/clog.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/complex/cexpf.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/complex/catrig.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/complex/csinhf.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/complex/stream.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/complex/ccosh.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/complex/ctanh.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/complex/ccoshf.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/complex/cproj.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/complex/cproj.h:    // std::numeric_limits<T>::infinity() doesn't run on the GPU
deps/thrust/thrust/detail/complex/cproj.h:    // std::numeric_limits<T>::infinity() doesn't run on the GPU
deps/thrust/thrust/detail/complex/complex.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/complex/csqrt.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/complex/arithmetic.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/complex/cexp.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/complex/cpow.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/complex/ctanhf.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/complex/clogf.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/complex/c99math.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/complex/c99math.h:#  if defined(__CUDACC__) && !(defined(__CUDA__) && defined(__clang__)) && !defined(_NVHPC_CUDA)
deps/thrust/thrust/detail/complex/c99math.h:#  endif // __CUDACC__
deps/thrust/thrust/detail/complex/c99math.h:#if !defined(__CUDACC__) && !defined(_NVHPC_CUDA)
deps/thrust/thrust/detail/complex/c99math.h:#endif // __CUDACC__
deps/thrust/thrust/detail/complex/catrigf.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/complex/math_private.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/tuple_transform.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/uninitialized_fill.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/memory_wrapper.h: *  Copyright 2020 NVIDIA Corporation
deps/thrust/thrust/detail/uninitialized_copy.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/config.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/algorithm_wrapper.h: *  Copyright 2020 NVIDIA Corporation
deps/thrust/thrust/detail/mpl/math.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/select_system.h: *  Copyright 2008-2018 NVIDIA Corporation
deps/thrust/thrust/detail/set_operations.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/swap.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/shuffle.inl: *  Copyright 2008-2020 NVIDIA Corporation
deps/thrust/thrust/detail/memory_algorithms.h:// Copyright (c) 2018 NVIDIA Corporation
deps/thrust/thrust/detail/memory_algorithms.h:  #if !__CUDA_ARCH__ // No exceptions in CUDA.
deps/thrust/thrust/detail/memory_algorithms.h:  #if !__CUDA_ARCH__ // No exceptions in CUDA.
deps/thrust/thrust/detail/memory_algorithms.h:  #if !__CUDA_ARCH__ // No exceptions in CUDA.
deps/thrust/thrust/detail/memory_algorithms.h:  #if !__CUDA_ARCH__ // No exceptions in CUDA.
deps/thrust/thrust/detail/memory_algorithms.h:  #if !__CUDA_ARCH__ // No exceptions in CUDA.
deps/thrust/thrust/detail/memory_algorithms.h:  #if !__CUDA_ARCH__ // No exceptions in CUDA.
deps/thrust/thrust/detail/memory_algorithms.h:  #if !__CUDA_ARCH__ // No exceptions in CUDA.
deps/thrust/thrust/detail/memory_algorithms.h:  #if !__CUDA_ARCH__ // No exceptions in CUDA.
deps/thrust/thrust/detail/type_deduction.h:// Copyright (c)      2018 NVIDIA Corporation
deps/thrust/thrust/detail/temporary_array.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/execution_policy.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/swap.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/cpp14_required.h: *  Copyright 2018 NVIDIA Corporation
deps/thrust/thrust/detail/numeric_traits.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/pointer.h: *  Copyright 2008-2018 NVIDIA Corporation
deps/thrust/thrust/detail/util/align.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/integer_traits.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/generate.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/transform_reduce.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/copy_if.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/modern_gcc_required.h: *  Copyright 2018 NVIDIA Corporation
deps/thrust/thrust/detail/static_assert.h: *  Copyright 2008-2018 NVIDIA Corporation
deps/thrust/thrust/detail/numeric_wrapper.h: *  Copyright 2021 NVIDIA Corporation
deps/thrust/thrust/detail/execute_with_allocator.h: *  Copyright 2008-2018 NVIDIA Corporation
deps/thrust/thrust/detail/scan.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/event_error.h: *  Copyright 2008-2018 NVIDIA Corporation
deps/thrust/thrust/detail/cstdint.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/get_iterator_value.h: *  Copyright 2008-2016 NVIDIA Corporation
deps/thrust/thrust/detail/raw_pointer_cast.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/reverse.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/copy_if.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/find.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/tuple.inl: *  Copyright 2008-2018 NVIDIA Corporation
deps/thrust/thrust/detail/contiguous_storage.h: *  Copyright 2008-2018 NVIDIA Corporation
deps/thrust/thrust/detail/raw_reference_cast.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/temporary_buffer.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/malloc_and_free.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/malloc_and_free.h:#if CUDART_VERSION < 5000
deps/thrust/thrust/detail/malloc_and_free.h:// cudafe generates unqualified calls to free(int *volatile)
deps/thrust/thrust/detail/malloc_and_free.h:#endif // CUDART_VERSION
deps/thrust/thrust/detail/vector_base.h: *  Copyright 2008-2018 NVIDIA Corporation
deps/thrust/thrust/detail/static_map.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/internal_functional.h: *  Copyright 2008-2018 NVIDIA Corporation
deps/thrust/thrust/detail/device_free.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/mismatch.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/transform_scan.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/overlapped_copy.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/reference.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/functional.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/replace.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/pointer.inl: *  Copyright 2008-2018 NVIDIA Corporation
deps/thrust/thrust/detail/pointer.inl:// e.g. cuda's managed_memory_pointer
deps/thrust/thrust/detail/binary_search.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/temporary_array.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/temporary_array.h:// Forward declare temporary_array, as it's used by the CUDA copy backend, which
deps/thrust/thrust/detail/use_default.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/vector_base.inl: *  Copyright 2008-2018 NVIDIA Corporation
deps/thrust/thrust/detail/tabulate.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/merge.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/seq.h: *  Copyright 2008-2018 NVIDIA Corporation
deps/thrust/thrust/detail/remove.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/swap_ranges.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/allocator_aware_execution_policy.h: *  Copyright 2018 NVIDIA Corporation
deps/thrust/thrust/detail/dependencies_aware_execution_policy.h: *  Copyright 2018 NVIDIA Corporation
deps/thrust/thrust/detail/integer_math.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/allocator/temporary_allocator.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/allocator/copy_construct_range.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/allocator/fill_construct_range.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/allocator/allocator_traits.h: *  Copyright 2008-2018 NVIDIA Corporation
deps/thrust/thrust/detail/allocator/destroy_range.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/allocator/fill_construct_range.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/allocator/allocator_traits.inl: *  Copyright 2008-2018 NVIDIA Corporation
deps/thrust/thrust/detail/allocator/tagged_allocator.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/allocator/temporary_allocator.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/allocator/temporary_allocator.inl:#if (defined(_NVHPC_CUDA) || defined(__CUDA_ARCH__)) && \
deps/thrust/thrust/detail/allocator/temporary_allocator.inl:    THRUST_DEVICE_SYSTEM == THRUST_DEVICE_SYSTEM_CUDA
deps/thrust/thrust/detail/allocator/temporary_allocator.inl:#include <thrust/system/cuda/detail/terminate.h>
deps/thrust/thrust/detail/allocator/temporary_allocator.inl:      #if THRUST_INCLUDE_DEVICE_CODE && THRUST_DEVICE_SYSTEM == THRUST_DEVICE_SYSTEM_CUDA
deps/thrust/thrust/detail/allocator/temporary_allocator.inl:        thrust::system::cuda::detail::terminate_with_message("temporary_buffer::allocate: get_temporary_buffer failed");
deps/thrust/thrust/detail/allocator/no_throw_allocator.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/allocator/no_throw_allocator.h:#ifndef __CUDA_ARCH__
deps/thrust/thrust/detail/allocator/malloc_allocator.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/allocator/default_construct_range.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/allocator/copy_construct_range.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/allocator/default_construct_range.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/allocator/malloc_allocator.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/allocator/tagged_allocator.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/allocator/destroy_range.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/type_traits.h: *  Copyright 2008-2018 NVIDIA Corporation
deps/thrust/thrust/detail/type_traits.h:#if defined(__CUDA_ARCH__)
deps/thrust/thrust/detail/type_traits.h:#  if (__CUDA_ARCH__ < 130)
deps/thrust/thrust/detail/trivial_sequence.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/functional/argument.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/functional/actor.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/functional/operators/relational_operators.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/functional/operators/compound_assignment_operators.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/functional/operators/operator_adaptors.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/functional/operators/assignment_operator.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/functional/operators/logical_operators.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/functional/operators/bitwise_operators.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/functional/operators/arithmetic_operators.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/functional/placeholder.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/functional/operators.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/functional/value.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/functional/actor.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/functional/composite.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/advance.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/type_traits/function_traits.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/type_traits/has_trivial_assign.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/type_traits/has_member_function.h: *  Copyright 2008-2021 NVIDIA Corporation
deps/thrust/thrust/detail/type_traits/is_call_possible.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/type_traits/minimum_type.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/type_traits/iterator/is_output_iterator.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/type_traits/iterator/is_discard_iterator.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/type_traits/pointer_traits.h: *  Copyright 2008-2020 NVIDIA Corporation
deps/thrust/thrust/detail/type_traits/result_of_adaptable_function.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/type_traits/is_metafunction_defined.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/type_traits/has_nested_type.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/inner_product.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/copy.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/for_each.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/sort.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/preprocessor.h:// Copyright (c) 2017-2018 NVIDIA Corporation
deps/thrust/thrust/detail/minmax.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/unique.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/equal.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/fill.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/alignment.h: *  Copyright 2017 NVIDIA Corporation
deps/thrust/thrust/detail/transform.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/count.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/copy.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/adjacent_difference.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/tuple_meta_transform.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/device_malloc.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/detail/partition.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/gather.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/generate.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/partition.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/device_make_unique.h: *  Copyright 2008-2018 NVIDIA Corporation
deps/thrust/thrust/addressof.h:// Copyright (c) 2018 NVIDIA Corporation
deps/thrust/thrust/device_malloc_allocator.h: *  Copyright 2008-2018 NVIDIA Corporation
deps/thrust/thrust/uninitialized_copy.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/device_delete.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/random.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/tuple.h: *  Copyright 2008-2018 NVIDIA Corporation
deps/thrust/thrust/transform.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/tabulate.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/device_reference.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/execution_policy.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/execution_policy.h: *  \p thrust::device_vector or to avoid wrapping e.g. raw pointers allocated by the CUDA API with types
deps/thrust/thrust/execution_policy.h: *  cannot be dereferenced by a GPU. For this reason, raw pointers allocated by host APIs should not be mixed
deps/thrust/thrust/execution_policy.h: *  with a \p thrust::device algorithm invocation when the device backend is CUDA.
deps/thrust/thrust/device_new.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/swap.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/merge.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/sequence.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/universal_allocator.h: *  Copyright 2008-2020 NVIDIA Corporation
deps/thrust/thrust/device_free.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/iterator/counting_iterator.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/iterator/detail/transform_output_iterator.inl: *  Copyright 2008-2016 NVIDIA Corporation
deps/thrust/thrust/iterator/detail/reverse_iterator.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/iterator/detail/iterator_traits.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/iterator/detail/join_iterator.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/iterator/detail/permutation_iterator_base.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/iterator/detail/is_iterator_category.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/iterator/detail/normal_iterator.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/iterator/detail/iterator_traversal_tags.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/iterator/detail/device_system_tag.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/iterator/detail/universal_categories.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/iterator/detail/reverse_iterator_base.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/iterator/detail/tagged_iterator.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/iterator/detail/iterator_category_to_system.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/iterator/detail/transform_iterator.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/iterator/detail/retag.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/iterator/detail/host_system_tag.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/iterator/detail/minimum_category.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/iterator/detail/iterator_facade_category.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/iterator/detail/iterator_adaptor_base.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/iterator/detail/any_assign.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/iterator/detail/discard_iterator_base.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/iterator/detail/constant_iterator_base.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/iterator/detail/zip_iterator_base.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/iterator/detail/iterator_category_with_system_and_traversal.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/iterator/detail/counting_iterator.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/iterator/detail/any_system_tag.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/iterator/detail/iterator_category_to_traversal.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/iterator/detail/distance_from_result.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/iterator/detail/minimum_system.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/iterator/detail/tuple_of_iterator_references.h: *  Copyright 2008-2018 NVIDIA Corporation
deps/thrust/thrust/iterator/detail/zip_iterator.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/iterator/detail/transform_input_output_iterator.inl: *  Copyright 2020 NVIDIA Corporation
deps/thrust/thrust/iterator/transform_iterator.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/iterator/transform_output_iterator.h: *  Copyright 2008-2018 NVIDIA Corporation
deps/thrust/thrust/iterator/iterator_categories.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/iterator/constant_iterator.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/iterator/retag.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/iterator/discard_iterator.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/iterator/reverse_iterator.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/iterator/iterator_facade.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/iterator/iterator_adaptor.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/iterator/iterator_traits.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/iterator/transform_input_output_iterator.h: *  Copyright 2020 NVIDIA Corporation
deps/thrust/thrust/iterator/zip_iterator.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/iterator/permutation_iterator.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/inner_product.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/advance.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/count.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/complex.h: *  Copyright 2008-2019 NVIDIA Corporation
deps/thrust/thrust/reduce.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/binary_search.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/pair.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/limits.h:// Copyright (c) 2018 NVIDIA Corporation
deps/thrust/thrust/system/detail/error_code.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/internal/decompose.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/adl/mismatch.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/adl/mismatch.h:#include <thrust/system/cuda/detail/mismatch.h>
deps/thrust/thrust/system/detail/adl/swap_ranges.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/adl/swap_ranges.h:#include <thrust/system/cuda/detail/swap_ranges.h>
deps/thrust/thrust/system/detail/adl/adjacent_difference.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/adl/adjacent_difference.h:#include <thrust/system/cuda/detail/adjacent_difference.h>
deps/thrust/thrust/system/detail/adl/gather.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/adl/gather.h:#include <thrust/system/cuda/detail/gather.h>
deps/thrust/thrust/system/detail/adl/generate.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/adl/generate.h:#include <thrust/system/cuda/detail/generate.h>
deps/thrust/thrust/system/detail/adl/partition.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/adl/partition.h:#include <thrust/system/cuda/detail/partition.h>
deps/thrust/thrust/system/detail/adl/uninitialized_copy.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/adl/uninitialized_copy.h:#include <thrust/system/cuda/detail/uninitialized_copy.h>
deps/thrust/thrust/system/detail/adl/transform.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/adl/transform.h:#include <thrust/system/cuda/detail/transform.h>
deps/thrust/thrust/system/detail/adl/tabulate.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/adl/tabulate.h:#include <thrust/system/cuda/detail/tabulate.h>
deps/thrust/thrust/system/detail/adl/merge.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/adl/merge.h:#include <thrust/system/cuda/detail/merge.h>
deps/thrust/thrust/system/detail/adl/sequence.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/adl/sequence.h:#include <thrust/system/cuda/detail/sequence.h>
deps/thrust/thrust/system/detail/adl/assign_value.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/adl/assign_value.h:#include <thrust/system/cuda/detail/assign_value.h>
deps/thrust/thrust/system/detail/adl/inner_product.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/adl/inner_product.h:#include <thrust/system/cuda/detail/inner_product.h>
deps/thrust/thrust/system/detail/adl/count.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/adl/count.h:#include <thrust/system/cuda/detail/count.h>
deps/thrust/thrust/system/detail/adl/reduce.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/adl/reduce.h:#include <thrust/system/cuda/detail/reduce.h>
deps/thrust/thrust/system/detail/adl/binary_search.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/adl/binary_search.h:#include <thrust/system/cuda/detail/binary_search.h>
deps/thrust/thrust/system/detail/adl/copy_if.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/adl/copy_if.h:#include <thrust/system/cuda/detail/copy_if.h>
deps/thrust/thrust/system/detail/adl/remove.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/adl/remove.h:#include <thrust/system/cuda/detail/remove.h>
deps/thrust/thrust/system/detail/adl/temporary_buffer.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/adl/temporary_buffer.h:#include <thrust/system/cuda/detail/temporary_buffer.h>
deps/thrust/thrust/system/detail/adl/malloc_and_free.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/adl/malloc_and_free.h:#include <thrust/system/cuda/detail/malloc_and_free.h>
deps/thrust/thrust/system/detail/adl/replace.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/adl/replace.h:#include <thrust/system/cuda/detail/replace.h>
deps/thrust/thrust/system/detail/adl/find.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/adl/find.h:#include <thrust/system/cuda/detail/find.h>
deps/thrust/thrust/system/detail/adl/scan_by_key.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/adl/scan_by_key.h:#include <thrust/system/cuda/detail/scan_by_key.h>
deps/thrust/thrust/system/detail/adl/iter_swap.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/adl/iter_swap.h:#include <thrust/system/cuda/detail/iter_swap.h>
deps/thrust/thrust/system/detail/adl/for_each.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/adl/for_each.h:#include <thrust/system/cuda/detail/for_each.h>
deps/thrust/thrust/system/detail/adl/unique_by_key.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/adl/unique_by_key.h:#include <thrust/system/cuda/detail/unique_by_key.h>
deps/thrust/thrust/system/detail/adl/get_value.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/adl/get_value.h:#include <thrust/system/cuda/detail/get_value.h>
deps/thrust/thrust/system/detail/adl/transform_scan.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/adl/transform_scan.h:#include <thrust/system/cuda/detail/transform_scan.h>
deps/thrust/thrust/system/detail/adl/transform_reduce.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/adl/transform_reduce.h:#include <thrust/system/cuda/detail/transform_reduce.h>
deps/thrust/thrust/system/detail/adl/extrema.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/adl/extrema.h:#include <thrust/system/cuda/detail/extrema.h>
deps/thrust/thrust/system/detail/adl/async/transform.h: *  Copyright 2008-2018 NVIDIA Corporation
deps/thrust/thrust/system/detail/adl/async/reduce.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/adl/async/for_each.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/adl/async/sort.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/adl/async/scan.h: *  Copyright 2008-2020 NVIDIA Corporation
deps/thrust/thrust/system/detail/adl/async/copy.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/adl/logical.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/adl/logical.h:#include <thrust/system/cuda/detail/logical.h>
deps/thrust/thrust/system/detail/adl/set_operations.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/adl/set_operations.h:#include <thrust/system/cuda/detail/set_operations.h>
deps/thrust/thrust/system/detail/adl/per_device_resource.h: *  Copyright 2018 NVIDIA Corporation
deps/thrust/thrust/system/detail/adl/per_device_resource.h:#include <thrust/system/cuda/detail/per_device_resource.h>
deps/thrust/thrust/system/detail/adl/equal.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/adl/equal.h:#include <thrust/system/cuda/detail/equal.h>
deps/thrust/thrust/system/detail/adl/unique.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/adl/unique.h:#include <thrust/system/cuda/detail/unique.h>
deps/thrust/thrust/system/detail/adl/sort.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/adl/sort.h:#include <thrust/system/cuda/detail/sort.h>
deps/thrust/thrust/system/detail/adl/reduce_by_key.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/adl/reduce_by_key.h:#include <thrust/system/cuda/detail/reduce_by_key.h>
deps/thrust/thrust/system/detail/adl/fill.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/adl/fill.h:#include <thrust/system/cuda/detail/fill.h>
deps/thrust/thrust/system/detail/adl/reverse.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/adl/reverse.h:#include <thrust/system/cuda/detail/reverse.h>
deps/thrust/thrust/system/detail/adl/scan.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/adl/scan.h:#include <thrust/system/cuda/detail/scan.h>
deps/thrust/thrust/system/detail/adl/copy.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/adl/copy.h:#include <thrust/system/cuda/detail/copy.h>
deps/thrust/thrust/system/detail/adl/scatter.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/adl/scatter.h:#include <thrust/system/cuda/detail/scatter.h>
deps/thrust/thrust/system/detail/adl/uninitialized_fill.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/adl/uninitialized_fill.h:#include <thrust/system/cuda/detail/uninitialized_fill.h>
deps/thrust/thrust/system/detail/bad_alloc.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/sequential/mismatch.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/sequential/swap_ranges.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/sequential/adjacent_difference.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/sequential/gather.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/sequential/generate.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/sequential/partition.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/sequential/uninitialized_copy.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/sequential/transform.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/sequential/tabulate.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/sequential/stable_radix_sort.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/sequential/execution_policy.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/sequential/copy_backward.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/sequential/merge.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/sequential/sequence.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/sequential/stable_merge_sort.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/sequential/stable_merge_sort.inl:      // avoid recursion in CUDA threads
deps/thrust/thrust/system/detail/sequential/stable_merge_sort.inl:      // avoid recursion in CUDA threads
deps/thrust/thrust/system/detail/sequential/assign_value.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/sequential/inner_product.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/sequential/trivial_copy.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/sequential/count.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/sequential/reduce.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/sequential/binary_search.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/sequential/stable_radix_sort.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/sequential/stable_merge_sort.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/sequential/copy_if.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/sequential/remove.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/sequential/temporary_buffer.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/sequential/malloc_and_free.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/sequential/replace.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/sequential/find.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/sequential/scan_by_key.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/sequential/iter_swap.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/sequential/stable_primitive_sort.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/sequential/stable_primitive_sort.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/sequential/insertion_sort.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/sequential/for_each.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/sequential/merge.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/sequential/unique_by_key.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/sequential/get_value.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/sequential/general_copy.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/sequential/transform_scan.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/sequential/transform_reduce.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/sequential/extrema.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/sequential/logical.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/sequential/set_operations.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/sequential/per_device_resource.h: *  Copyright 2018 NVIDIA Corporation
deps/thrust/thrust/system/detail/sequential/equal.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/sequential/unique.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/sequential/sort.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/sequential/copy.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/sequential/reduce_by_key.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/sequential/fill.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/sequential/sort.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/sequential/sort.inl:  // the compilation time of stable_primitive_sort is too expensive to use within a single CUDA thread
deps/thrust/thrust/system/detail/sequential/sort.inl:#ifndef __CUDA_ARCH__
deps/thrust/thrust/system/detail/sequential/sort.inl:  // the compilation time of stable_primitive_sort_by_key is too expensive to use within a single CUDA thread
deps/thrust/thrust/system/detail/sequential/sort.inl:#ifndef __CUDA_ARCH__
deps/thrust/thrust/system/detail/sequential/reverse.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/sequential/scan.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/sequential/copy.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/sequential/scatter.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/sequential/uninitialized_fill.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/errno.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/error_category.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/error_condition.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/system_error.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/mismatch.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/shuffle.h: *  Copyright 2008-2020 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/gather.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/swap_ranges.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/adjacent_difference.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/distance.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/extrema.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/select_system_exists.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/gather.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/sequence.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/generate.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/tag.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/reduce.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/scatter.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/partition.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/uninitialized_fill.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/uninitialized_copy.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/uninitialized_copy.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/select_system.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/scalar/binary_search.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/scalar/binary_search.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/set_operations.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/shuffle.inl: *  Copyright 2008-20120 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/transform.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/tabulate.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/merge.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/sequence.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/generate.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/transform_reduce.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/copy_if.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/inner_product.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/advance.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/count.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/reduce.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/binary_search.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/scan.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/reverse.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/copy_if.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/find.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/reduce_by_key.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/remove.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/temporary_buffer.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/distance.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/replace.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/find.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/scan_by_key.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/mismatch.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/transform_scan.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/memory.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/replace.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/binary_search.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/for_each.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/tabulate.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/merge.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/unique_by_key.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/remove.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/swap_ranges.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/temporary_buffer.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/scan_by_key.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/scan_by_key.inl:    //    S. Sengupta, M. Harris, and M. Garland. "Efficient parallel scan algorithms for GPUs"
deps/thrust/thrust/system/detail/generic/scan_by_key.inl:    //    NVIDIA Technical Report NVR-2008-003, December 2008
deps/thrust/thrust/system/detail/generic/scan_by_key.inl:    //    S. Sengupta, M. Harris, and M. Garland. "Efficient parallel scan algorithms for GPUs"
deps/thrust/thrust/system/detail/generic/scan_by_key.inl:    //    NVIDIA Technical Report NVR-2008-003, December 2008
deps/thrust/thrust/system/detail/generic/transform_scan.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/transform_reduce.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/select_system.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/advance.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/extrema.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/logical.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/inner_product.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/set_operations.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/per_device_resource.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/equal.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/unique.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/sort.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/copy.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/reduce_by_key.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/fill.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/sort.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/unique_by_key.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/unique.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/reverse.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/equal.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/transform.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/count.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/scan.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/copy.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/adjacent_difference.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/scatter.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/partition.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/memory.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/detail/generic/uninitialized_fill.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/tbb/detail/mismatch.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/tbb/detail/swap_ranges.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/tbb/detail/adjacent_difference.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/tbb/detail/gather.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/tbb/detail/generate.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/tbb/detail/reduce.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/tbb/detail/partition.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/tbb/detail/uninitialized_copy.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/tbb/detail/transform.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/tbb/detail/tabulate.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/tbb/detail/execution_policy.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/tbb/detail/merge.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/tbb/detail/sequence.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/tbb/detail/assign_value.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/tbb/detail/copy_if.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/tbb/detail/inner_product.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/tbb/detail/count.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/tbb/detail/reduce.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/tbb/detail/binary_search.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/tbb/detail/scan.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/tbb/detail/copy_if.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/tbb/detail/reduce_by_key.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/tbb/detail/reduce_intervals.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/tbb/detail/remove.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/tbb/detail/temporary_buffer.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/tbb/detail/malloc_and_free.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/tbb/detail/replace.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/tbb/detail/find.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/tbb/detail/scan_by_key.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/tbb/detail/iter_swap.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/tbb/detail/for_each.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/tbb/detail/merge.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/tbb/detail/unique_by_key.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/tbb/detail/remove.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/tbb/detail/get_value.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/tbb/detail/transform_scan.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/tbb/detail/transform_reduce.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/tbb/detail/par.h: *  Copyright 2008-2018 NVIDIA Corporation
deps/thrust/thrust/system/tbb/detail/extrema.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/tbb/detail/logical.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/tbb/detail/set_operations.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/tbb/detail/per_device_resource.h: *  Copyright 2018 NVIDIA Corporation
deps/thrust/thrust/system/tbb/detail/equal.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/tbb/detail/unique.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/tbb/detail/sort.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/tbb/detail/copy.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/tbb/detail/reduce_by_key.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/tbb/detail/for_each.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/tbb/detail/fill.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/tbb/detail/sort.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/tbb/detail/unique_by_key.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/tbb/detail/unique.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/tbb/detail/reverse.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/tbb/detail/scan.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/tbb/detail/copy.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/tbb/detail/scatter.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/tbb/detail/partition.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/tbb/detail/memory.inl: *  Copyright 2008-2018 NVIDIA Corporation
deps/thrust/thrust/system/tbb/detail/uninitialized_fill.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/tbb/execution_policy.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/tbb/pointer.h: *  Copyright 2008-2020 NVIDIA Corporation
deps/thrust/thrust/system/tbb/memory.h: *  Copyright 2008-2018 NVIDIA Corporation
deps/thrust/thrust/system/tbb/memory_resource.h: *  Copyright 2018-2020 NVIDIA Corporation
deps/thrust/thrust/system/tbb/vector.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/error_code.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/omp/detail/mismatch.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/omp/detail/swap_ranges.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/omp/detail/adjacent_difference.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/omp/detail/gather.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/omp/detail/generate.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/omp/detail/reduce.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/omp/detail/partition.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/omp/detail/uninitialized_copy.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/omp/detail/transform.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/omp/detail/tabulate.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/omp/detail/execution_policy.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/omp/detail/merge.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/omp/detail/sequence.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/omp/detail/default_decomposition.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/omp/detail/assign_value.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/omp/detail/copy_if.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/omp/detail/inner_product.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/omp/detail/count.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/omp/detail/reduce.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/omp/detail/binary_search.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/omp/detail/copy_if.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/omp/detail/reduce_by_key.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/omp/detail/reduce_intervals.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/omp/detail/remove.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/omp/detail/temporary_buffer.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/omp/detail/malloc_and_free.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/omp/detail/replace.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/omp/detail/find.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/omp/detail/scan_by_key.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/omp/detail/iter_swap.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/omp/detail/pragma_omp.h:* Copyright (c) 2021, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/thrust/system/omp/detail/pragma_omp.h:*     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/thrust/system/omp/detail/pragma_omp.h:* ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/thrust/system/omp/detail/for_each.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/omp/detail/default_decomposition.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/omp/detail/unique_by_key.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/omp/detail/remove.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/omp/detail/get_value.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/omp/detail/transform_scan.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/omp/detail/transform_reduce.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/omp/detail/reduce_intervals.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/omp/detail/par.h: *  Copyright 2008-2018 NVIDIA Corporation
deps/thrust/thrust/system/omp/detail/extrema.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/omp/detail/logical.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/omp/detail/set_operations.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/omp/detail/per_device_resource.h: *  Copyright 2018 NVIDIA Corporation
deps/thrust/thrust/system/omp/detail/equal.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/omp/detail/unique.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/omp/detail/sort.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/omp/detail/copy.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/omp/detail/reduce_by_key.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/omp/detail/for_each.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/omp/detail/fill.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/omp/detail/sort.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/omp/detail/unique_by_key.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/omp/detail/unique.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/omp/detail/reverse.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/omp/detail/scan.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/omp/detail/copy.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/omp/detail/scatter.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/omp/detail/partition.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/omp/detail/memory.inl: *  Copyright 2008-2018 NVIDIA Corporation
deps/thrust/thrust/system/omp/detail/uninitialized_fill.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/omp/execution_policy.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/omp/pointer.h: *  Copyright 2008-2020 NVIDIA Corporation
deps/thrust/thrust/system/omp/memory.h: *  Copyright 2008-2018 NVIDIA Corporation
deps/thrust/thrust/system/omp/memory_resource.h: *  Copyright 2018-2020 NVIDIA Corporation
deps/thrust/thrust/system/omp/vector.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/system_error.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/system_error.h: *  such as the CUDA runtime.
deps/thrust/thrust/system/cuda/error.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/cuda/error.h:/*! \file thrust/system/cuda/error.h
deps/thrust/thrust/system/cuda/error.h: *  \brief CUDA-specific error reporting
deps/thrust/thrust/system/cuda/error.h:#include <thrust/system/cuda/detail/guarded_driver_types.h>
deps/thrust/thrust/system/cuda/error.h:namespace cuda
deps/thrust/thrust/system/cuda/error.h:// To construct an error_code after a CUDA Runtime error:
deps/thrust/thrust/system/cuda/error.h://   error_code(::cudaGetLastError(), cuda_category())
deps/thrust/thrust/system/cuda/error.h:/*! Namespace for CUDA Runtime errors.
deps/thrust/thrust/system/cuda/error.h:/*! \p errc_t enumerates the kinds of CUDA Runtime errors.
deps/thrust/thrust/system/cuda/error.h:  // from cuda/include/driver_types.h
deps/thrust/thrust/system/cuda/error.h:  success                            = cudaSuccess,
deps/thrust/thrust/system/cuda/error.h:  missing_configuration              = cudaErrorMissingConfiguration,
deps/thrust/thrust/system/cuda/error.h:  memory_allocation                  = cudaErrorMemoryAllocation,
deps/thrust/thrust/system/cuda/error.h:  initialization_error               = cudaErrorInitializationError,
deps/thrust/thrust/system/cuda/error.h:  launch_failure                     = cudaErrorLaunchFailure,
deps/thrust/thrust/system/cuda/error.h:  prior_launch_failure               = cudaErrorPriorLaunchFailure,
deps/thrust/thrust/system/cuda/error.h:  launch_timeout                     = cudaErrorLaunchTimeout,
deps/thrust/thrust/system/cuda/error.h:  launch_out_of_resources            = cudaErrorLaunchOutOfResources,
deps/thrust/thrust/system/cuda/error.h:  invalid_device_function            = cudaErrorInvalidDeviceFunction,
deps/thrust/thrust/system/cuda/error.h:  invalid_configuration              = cudaErrorInvalidConfiguration,
deps/thrust/thrust/system/cuda/error.h:  invalid_device                     = cudaErrorInvalidDevice,
deps/thrust/thrust/system/cuda/error.h:  invalid_value                      = cudaErrorInvalidValue,
deps/thrust/thrust/system/cuda/error.h:  invalid_pitch_value                = cudaErrorInvalidPitchValue,
deps/thrust/thrust/system/cuda/error.h:  invalid_symbol                     = cudaErrorInvalidSymbol,
deps/thrust/thrust/system/cuda/error.h:  map_buffer_object_failed           = cudaErrorMapBufferObjectFailed,
deps/thrust/thrust/system/cuda/error.h:  unmap_buffer_object_failed         = cudaErrorUnmapBufferObjectFailed,
deps/thrust/thrust/system/cuda/error.h:  invalid_host_pointer               = cudaErrorInvalidHostPointer,
deps/thrust/thrust/system/cuda/error.h:  invalid_device_pointer             = cudaErrorInvalidDevicePointer,
deps/thrust/thrust/system/cuda/error.h:  invalid_texture                    = cudaErrorInvalidTexture,
deps/thrust/thrust/system/cuda/error.h:  invalid_texture_binding            = cudaErrorInvalidTextureBinding,
deps/thrust/thrust/system/cuda/error.h:  invalid_channel_descriptor         = cudaErrorInvalidChannelDescriptor,
deps/thrust/thrust/system/cuda/error.h:  invalid_memcpy_direction           = cudaErrorInvalidMemcpyDirection,
deps/thrust/thrust/system/cuda/error.h:  address_of_constant_error          = cudaErrorAddressOfConstant,
deps/thrust/thrust/system/cuda/error.h:  texture_fetch_failed               = cudaErrorTextureFetchFailed,
deps/thrust/thrust/system/cuda/error.h:  texture_not_bound                  = cudaErrorTextureNotBound,
deps/thrust/thrust/system/cuda/error.h:  synchronization_error              = cudaErrorSynchronizationError,
deps/thrust/thrust/system/cuda/error.h:  invalid_filter_setting             = cudaErrorInvalidFilterSetting,
deps/thrust/thrust/system/cuda/error.h:  invalid_norm_setting               = cudaErrorInvalidNormSetting,
deps/thrust/thrust/system/cuda/error.h:  mixed_device_execution             = cudaErrorMixedDeviceExecution,
deps/thrust/thrust/system/cuda/error.h:  cuda_runtime_unloading             = cudaErrorCudartUnloading,
deps/thrust/thrust/system/cuda/error.h:  unknown                            = cudaErrorUnknown,
deps/thrust/thrust/system/cuda/error.h:  not_yet_implemented                = cudaErrorNotYetImplemented,
deps/thrust/thrust/system/cuda/error.h:  memory_value_too_large             = cudaErrorMemoryValueTooLarge,
deps/thrust/thrust/system/cuda/error.h:  invalid_resource_handle            = cudaErrorInvalidResourceHandle,
deps/thrust/thrust/system/cuda/error.h:  not_ready                          = cudaErrorNotReady,
deps/thrust/thrust/system/cuda/error.h:  insufficient_driver                = cudaErrorInsufficientDriver,
deps/thrust/thrust/system/cuda/error.h:  set_on_active_process_error        = cudaErrorSetOnActiveProcess,
deps/thrust/thrust/system/cuda/error.h:  no_device                          = cudaErrorNoDevice,
deps/thrust/thrust/system/cuda/error.h:  ecc_uncorrectable                  = cudaErrorECCUncorrectable,
deps/thrust/thrust/system/cuda/error.h:#if CUDART_VERSION >= 4020
deps/thrust/thrust/system/cuda/error.h:  shared_object_symbol_not_found     = cudaErrorSharedObjectSymbolNotFound,
deps/thrust/thrust/system/cuda/error.h:  shared_object_init_failed          = cudaErrorSharedObjectInitFailed,
deps/thrust/thrust/system/cuda/error.h:  unsupported_limit                  = cudaErrorUnsupportedLimit,
deps/thrust/thrust/system/cuda/error.h:  duplicate_variable_name            = cudaErrorDuplicateVariableName,
deps/thrust/thrust/system/cuda/error.h:  duplicate_texture_name             = cudaErrorDuplicateTextureName,
deps/thrust/thrust/system/cuda/error.h:  duplicate_surface_name             = cudaErrorDuplicateSurfaceName,
deps/thrust/thrust/system/cuda/error.h:  devices_unavailable                = cudaErrorDevicesUnavailable,
deps/thrust/thrust/system/cuda/error.h:  invalid_kernel_image               = cudaErrorInvalidKernelImage,
deps/thrust/thrust/system/cuda/error.h:  no_kernel_image_for_device         = cudaErrorNoKernelImageForDevice,
deps/thrust/thrust/system/cuda/error.h:  incompatible_driver_context        = cudaErrorIncompatibleDriverContext,
deps/thrust/thrust/system/cuda/error.h:  peer_access_already_enabled        = cudaErrorPeerAccessAlreadyEnabled,
deps/thrust/thrust/system/cuda/error.h:  peer_access_not_enabled            = cudaErrorPeerAccessNotEnabled,
deps/thrust/thrust/system/cuda/error.h:  device_already_in_use              = cudaErrorDeviceAlreadyInUse,
deps/thrust/thrust/system/cuda/error.h:  profiler_disabled                  = cudaErrorProfilerDisabled,
deps/thrust/thrust/system/cuda/error.h:  assert_triggered                   = cudaErrorAssert,
deps/thrust/thrust/system/cuda/error.h:  too_many_peers                     = cudaErrorTooManyPeers,
deps/thrust/thrust/system/cuda/error.h:  host_memory_already_registered     = cudaErrorHostMemoryAlreadyRegistered,
deps/thrust/thrust/system/cuda/error.h:  host_memory_not_registered         = cudaErrorHostMemoryNotRegistered,
deps/thrust/thrust/system/cuda/error.h:  operating_system_error             = cudaErrorOperatingSystem,
deps/thrust/thrust/system/cuda/error.h:#if CUDART_VERSION >= 5000
deps/thrust/thrust/system/cuda/error.h:  peer_access_unsupported            = cudaErrorPeerAccessUnsupported,
deps/thrust/thrust/system/cuda/error.h:  launch_max_depth_exceeded          = cudaErrorLaunchMaxDepthExceeded,
deps/thrust/thrust/system/cuda/error.h:  launch_file_scoped_texture_used    = cudaErrorLaunchFileScopedTex,
deps/thrust/thrust/system/cuda/error.h:  launch_file_scoped_surface_used    = cudaErrorLaunchFileScopedSurf,
deps/thrust/thrust/system/cuda/error.h:  sync_depth_exceeded                = cudaErrorSyncDepthExceeded,
deps/thrust/thrust/system/cuda/error.h:  attempted_operation_not_permitted  = cudaErrorNotPermitted,
deps/thrust/thrust/system/cuda/error.h:  attempted_operation_not_supported  = cudaErrorNotSupported,
deps/thrust/thrust/system/cuda/error.h:  startup_failure                    = cudaErrorStartupFailure
deps/thrust/thrust/system/cuda/error.h:} // end namespace cuda_cub
deps/thrust/thrust/system/cuda/error.h: *        return a pointer to the string <tt>"cuda"</tt>. The object's
deps/thrust/thrust/system/cuda/error.h: *        If the argument <tt>ev</tt> corresponds to a CUDA error value, the function
deps/thrust/thrust/system/cuda/error.h: *        shall return <tt>error_condition(ev,cuda_category())</tt>.
deps/thrust/thrust/system/cuda/error.h:inline const error_category &cuda_category(void);
deps/thrust/thrust/system/cuda/error.h:// XXX N3000 prefers is_error_code_enum<cuda::errc>
deps/thrust/thrust/system/cuda/error.h:/*! Specialization of \p is_error_code_enum for \p cuda::errc::errc_t
deps/thrust/thrust/system/cuda/error.h:template<> struct is_error_code_enum<cuda::errc::errc_t> : thrust::detail::true_type {};
deps/thrust/thrust/system/cuda/error.h:// XXX replace cuda::errc::errc_t with cuda::errc upon c++0x
deps/thrust/thrust/system/cuda/error.h:/*! \return <tt>error_code(static_cast<int>(e), cuda::error_category())</tt>
deps/thrust/thrust/system/cuda/error.h:inline error_code make_error_code(cuda::errc::errc_t e);
deps/thrust/thrust/system/cuda/error.h:// XXX replace cuda::errc::errc_t with cuda::errc upon c++0x
deps/thrust/thrust/system/cuda/error.h:/*! \return <tt>error_condition(static_cast<int>(e), cuda::error_category())</tt>.
deps/thrust/thrust/system/cuda/error.h:inline error_condition make_error_condition(cuda::errc::errc_t e);
deps/thrust/thrust/system/cuda/error.h:namespace cuda_cub
deps/thrust/thrust/system/cuda/error.h:namespace errc = system::cuda::errc;
deps/thrust/thrust/system/cuda/error.h:} // end cuda_cub
deps/thrust/thrust/system/cuda/error.h:namespace cuda
deps/thrust/thrust/system/cuda/error.h:// XXX replace with using system::cuda_errc upon c++0x
deps/thrust/thrust/system/cuda/error.h:namespace errc = system::cuda::errc;
deps/thrust/thrust/system/cuda/error.h:} // end cuda
deps/thrust/thrust/system/cuda/error.h:using system::cuda_category;
deps/thrust/thrust/system/cuda/error.h:#include <thrust/system/cuda/detail/error.inl>
deps/thrust/thrust/system/cuda/detail/mismatch.h: * Copyright (c) 2016, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/thrust/system/cuda/detail/mismatch.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/thrust/system/cuda/detail/mismatch.h: * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/thrust/system/cuda/detail/mismatch.h:#include <thrust/system/cuda/config.h>
deps/thrust/thrust/system/cuda/detail/mismatch.h:#include <thrust/system/cuda/detail/execution_policy.h>
deps/thrust/thrust/system/cuda/detail/mismatch.h:namespace cuda_cub {
deps/thrust/thrust/system/cuda/detail/mismatch.h:} // namespace cuda_
deps/thrust/thrust/system/cuda/detail/mismatch.h:#include <thrust/system/cuda/detail/find.h>
deps/thrust/thrust/system/cuda/detail/mismatch.h:namespace cuda_cub {
deps/thrust/thrust/system/cuda/detail/mismatch.h:  transform_t result = cuda_cub::find_if_not(policy,
deps/thrust/thrust/system/cuda/detail/mismatch.h:  return cuda_cub::mismatch(policy,
deps/thrust/thrust/system/cuda/detail/mismatch.h:} // namespace cuda_cub
deps/thrust/thrust/system/cuda/detail/future.inl:// Copyright (c) 2018 NVIDIA Corporation
deps/thrust/thrust/system/cuda/detail/future.inl:#include <thrust/system/cuda/memory.h>
deps/thrust/thrust/system/cuda/detail/future.inl:#include <thrust/system/cuda/future.h>
deps/thrust/thrust/system/cuda/detail/future.inl:#include <thrust/system/cuda/detail/util.h>
deps/thrust/thrust/system/cuda/detail/future.inl:#include <thrust/system/cuda/detail/get_value.h>
deps/thrust/thrust/system/cuda/detail/future.inl:namespace system { namespace cuda { namespace detail
deps/thrust/thrust/system/cuda/detail/future.inl:      thrust::cuda_cub::throw_on_error(cudaEventDestroy(e));
deps/thrust/thrust/system/cuda/detail/future.inl:    thrust::cuda_cub::throw_on_error(
deps/thrust/thrust/system/cuda/detail/future.inl:      cudaEventCreateWithFlags(&e, cudaEventDisableTiming)
deps/thrust/thrust/system/cuda/detail/future.inl:    cudaError_t const err = cudaEventQuery(handle_.get());
deps/thrust/thrust/system/cuda/detail/future.inl:    if (cudaErrorNotReady == err)
deps/thrust/thrust/system/cuda/detail/future.inl:    thrust::cuda_cub::throw_on_error(err);
deps/thrust/thrust/system/cuda/detail/future.inl:    thrust::cuda_cub::throw_on_error(cudaEventSynchronize(handle_.get()));
deps/thrust/thrust/system/cuda/detail/future.inl:      thrust::cuda_cub::throw_on_error(cudaStreamDestroy(s));
deps/thrust/thrust/system/cuda/detail/future.inl:      thrust::cuda_cub::throw_on_error(cudaStreamDestroy(s));
deps/thrust/thrust/system/cuda/detail/future.inl:    thrust::cuda_cub::throw_on_error(
deps/thrust/thrust/system/cuda/detail/future.inl:      cudaStreamCreateWithFlags(&s, cudaStreamNonBlocking)
deps/thrust/thrust/system/cuda/detail/future.inl:  // GCC 10 complains if this is defaulted. See NVIDIA/thrust#1269.
deps/thrust/thrust/system/cuda/detail/future.inl:    cudaError_t const err = cudaStreamQuery(handle_.get());
deps/thrust/thrust/system/cuda/detail/future.inl:    if (cudaErrorNotReady == err)
deps/thrust/thrust/system/cuda/detail/future.inl:    thrust::cuda_cub::throw_on_error(err);
deps/thrust/thrust/system/cuda/detail/future.inl:    thrust::cuda_cub::throw_on_error(
deps/thrust/thrust/system/cuda/detail/future.inl:      cudaStreamSynchronize(handle_.get())
deps/thrust/thrust/system/cuda/detail/future.inl:    thrust::cuda_cub::throw_on_error(
deps/thrust/thrust/system/cuda/detail/future.inl:      cudaStreamWaitEvent(handle_.get(), e.get(), 0)
deps/thrust/thrust/system/cuda/detail/future.inl:    thrust::cuda_cub::throw_on_error(cudaEventRecord(e.get(), handle_.get()));
deps/thrust/thrust/system/cuda/detail/future.inl:// Precondition: `device` is the current CUDA device.
deps/thrust/thrust/system/cuda/detail/future.inl:// Precondition: `device` is the current CUDA device.
deps/thrust/thrust/system/cuda/detail/future.inl:// Precondition: `device` is the current CUDA device.
deps/thrust/thrust/system/cuda/detail/future.inl:// Precondition: `device` is the current CUDA device.
deps/thrust/thrust/system/cuda/detail/future.inl:// Precondition: `device` is the current CUDA device.
deps/thrust/thrust/system/cuda/detail/future.inl:// Precondition: `device` is the current CUDA device.
deps/thrust/thrust/system/cuda/detail/future.inl:  thrust::system::cuda::detail::make_dependent_future(
deps/thrust/thrust/system/cuda/detail/future.inl:    thrust::cuda_cub::throw_on_error(cudaGetDevice(&device_));
deps/thrust/thrust/system/cuda/detail/future.inl:  thrust::system::cuda::detail::try_acquire_stream(
deps/thrust/thrust/system/cuda/detail/future.inl:  thrust::system::cuda::detail::make_dependent_event(
deps/thrust/thrust/system/cuda/detail/future.inl:    thrust::cuda_cub::throw_on_error(cudaGetDevice(&device_));
deps/thrust/thrust/system/cuda/detail/future.inl:  thrust::system::cuda::detail::try_acquire_stream(
deps/thrust/thrust/system/cuda/detail/future.inl:  thrust::system::cuda::detail::make_dependent_future(
deps/thrust/thrust/system/cuda/detail/future.inl:  thrust::cuda_cub::throw_on_error(cudaGetDevice(&device_id));
deps/thrust/thrust/system/cuda/detail/future.inl:  thrust::cuda_cub::throw_on_error(cudaGetDevice(&device_id));
deps/thrust/thrust/system/cuda/detail/future.inl:}} // namespace system::cuda
deps/thrust/thrust/system/cuda/detail/swap_ranges.h: * Copyright (c) 2016, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/thrust/system/cuda/detail/swap_ranges.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/thrust/system/cuda/detail/swap_ranges.h: * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/thrust/system/cuda/detail/swap_ranges.h:#include <thrust/system/cuda/detail/transform.h>
deps/thrust/thrust/system/cuda/detail/swap_ranges.h:#include <thrust/system/cuda/detail/par_to_seq.h>
deps/thrust/thrust/system/cuda/detail/swap_ranges.h:#include <thrust/system/cuda/detail/parallel_for.h>
deps/thrust/thrust/system/cuda/detail/swap_ranges.h:namespace cuda_cub {
deps/thrust/thrust/system/cuda/detail/swap_ranges.h:  cuda_cub::parallel_for(policy,
deps/thrust/thrust/system/cuda/detail/swap_ranges.h:  cuda_cub::throw_on_error(
deps/thrust/thrust/system/cuda/detail/swap_ranges.h:    cuda_cub::synchronize_optional(policy)
deps/thrust/thrust/system/cuda/detail/swap_ranges.h:}    // namespace cuda_cub
deps/thrust/thrust/system/cuda/detail/adjacent_difference.h: * Copyright (c) 2016, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/thrust/system/cuda/detail/adjacent_difference.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/thrust/system/cuda/detail/adjacent_difference.h: * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/thrust/system/cuda/detail/adjacent_difference.h:#include <thrust/system/cuda/config.h>
deps/thrust/thrust/system/cuda/detail/adjacent_difference.h:#include <thrust/system/cuda/detail/util.h>
deps/thrust/thrust/system/cuda/detail/adjacent_difference.h:#include <thrust/system/cuda/detail/core/agent_launcher.h>
deps/thrust/thrust/system/cuda/detail/adjacent_difference.h:#include <thrust/system/cuda/detail/par_to_seq.h>
deps/thrust/thrust/system/cuda/detail/adjacent_difference.h:#include <thrust/system/cuda/detail/dispatch.h>
deps/thrust/thrust/system/cuda/detail/adjacent_difference.h:namespace cuda_cub {
deps/thrust/thrust/system/cuda/detail/adjacent_difference.h:  cudaError_t THRUST_RUNTIME_FUNCTION
deps/thrust/thrust/system/cuda/detail/adjacent_difference.h:            cudaStream_t stream,
deps/thrust/thrust/system/cuda/detail/adjacent_difference.h:      return cudaSuccess;
deps/thrust/thrust/system/cuda/detail/adjacent_difference.h:    cudaError_t status = cudaSuccess;
deps/thrust/thrust/system/cuda/detail/adjacent_difference.h:    CUDA_CUB_RET_IF_FAIL(status);
deps/thrust/thrust/system/cuda/detail/adjacent_difference.h:    CUDA_CUB_RET_IF_FAIL(cudaPeekAtLastError());
deps/thrust/thrust/system/cuda/detail/adjacent_difference.h:    CUDA_CUB_RET_IF_FAIL(cudaPeekAtLastError());
deps/thrust/thrust/system/cuda/detail/adjacent_difference.h:    cudaStream_t stream       = cuda_cub::stream(policy);
deps/thrust/thrust/system/cuda/detail/adjacent_difference.h:    cudaError_t status;
deps/thrust/thrust/system/cuda/detail/adjacent_difference.h:    cuda_cub::throw_on_error(status, "adjacent_difference failed on 1st step");
deps/thrust/thrust/system/cuda/detail/adjacent_difference.h:    cuda_cub::throw_on_error(status, "adjacent_difference failed on 2nd step");
deps/thrust/thrust/system/cuda/detail/adjacent_difference.h:    status = cuda_cub::synchronize_optional(policy);
deps/thrust/thrust/system/cuda/detail/adjacent_difference.h:    cuda_cub::throw_on_error(status, "adjacent_difference failed to synchronize");
deps/thrust/thrust/system/cuda/detail/adjacent_difference.h:  if (__THRUST_HAS_CUDART__)
deps/thrust/thrust/system/cuda/detail/adjacent_difference.h:#if !__THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/adjacent_difference.h:  return cuda_cub::adjacent_difference(policy,
deps/thrust/thrust/system/cuda/detail/adjacent_difference.h:} // namespace cuda_cub
deps/thrust/thrust/system/cuda/detail/gather.h: * Copyright (c) 2016, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/thrust/system/cuda/detail/gather.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/thrust/system/cuda/detail/gather.h: * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/thrust/system/cuda/detail/gather.h:#include <thrust/system/cuda/detail/transform.h>
deps/thrust/thrust/system/cuda/detail/gather.h:namespace cuda_cub {
deps/thrust/thrust/system/cuda/detail/gather.h:  return cuda_cub::transform(policy,
deps/thrust/thrust/system/cuda/detail/gather.h:  return cuda_cub::transform_if(policy,
deps/thrust/thrust/system/cuda/detail/gather.h:  return cuda_cub::gather_if(policy,
deps/thrust/thrust/system/cuda/detail/gather.h:} // namespace cuda_cub
deps/thrust/thrust/system/cuda/detail/generate.h: * Copyright (c) 2016, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/thrust/system/cuda/detail/generate.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/thrust/system/cuda/detail/generate.h: * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/thrust/system/cuda/detail/generate.h:#include <thrust/system/cuda/config.h>
deps/thrust/thrust/system/cuda/detail/generate.h:#include <thrust/system/cuda/detail/for_each.h>
deps/thrust/thrust/system/cuda/detail/generate.h:namespace cuda_cub {
deps/thrust/thrust/system/cuda/detail/generate.h:  return cuda_cub::for_each_n(policy,
deps/thrust/thrust/system/cuda/detail/generate.h:  cuda_cub::generate_n(policy, first, thrust::distance(first, last), generator);
deps/thrust/thrust/system/cuda/detail/generate.h:}    // namespace cuda_cub
deps/thrust/thrust/system/cuda/detail/internal/copy_cross_system.h: * Copyright (c) 2016, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/thrust/system/cuda/detail/internal/copy_cross_system.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/thrust/system/cuda/detail/internal/copy_cross_system.h: * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/thrust/system/cuda/detail/internal/copy_cross_system.h:// but must be part of include in thrust/system/cuda/detail/copy.h
deps/thrust/thrust/system/cuda/detail/internal/copy_cross_system.h:#include <thrust/system/cuda/config.h>
deps/thrust/thrust/system/cuda/detail/internal/copy_cross_system.h:#include <thrust/system/cuda/detail/uninitialized_copy.h>
deps/thrust/thrust/system/cuda/detail/internal/copy_cross_system.h:#include <thrust/system/cuda/detail/util.h>
deps/thrust/thrust/system/cuda/detail/internal/copy_cross_system.h:namespace cuda_cub {
deps/thrust/thrust/system/cuda/detail/internal/copy_cross_system.h:                      thrust::cuda_cub::execution_policy<D>& device_s,
deps/thrust/thrust/system/cuda/detail/internal/copy_cross_system.h:    cudaError status;
deps/thrust/thrust/system/cuda/detail/internal/copy_cross_system.h:    status = cuda_cub::trivial_copy_to_device(dst,
deps/thrust/thrust/system/cuda/detail/internal/copy_cross_system.h:                                              cuda_cub::stream(device_s));
deps/thrust/thrust/system/cuda/detail/internal/copy_cross_system.h:    cuda_cub::throw_on_error(status, "__copy::trivial_device_copy H->D: failed");
deps/thrust/thrust/system/cuda/detail/internal/copy_cross_system.h:  trivial_device_copy(thrust::cuda_cub::execution_policy<D>& device_s,
deps/thrust/thrust/system/cuda/detail/internal/copy_cross_system.h:    cudaError status;
deps/thrust/thrust/system/cuda/detail/internal/copy_cross_system.h:    status = cuda_cub::trivial_copy_from_device(dst,
deps/thrust/thrust/system/cuda/detail/internal/copy_cross_system.h:                                                cuda_cub::stream(device_s));
deps/thrust/thrust/system/cuda/detail/internal/copy_cross_system.h:    cuda_cub::throw_on_error(status, "trivial_device_copy D->H failed");
deps/thrust/thrust/system/cuda/detail/internal/copy_cross_system.h:                      thrust::cuda_cub::execution_policy<D>& device_s,
deps/thrust/thrust/system/cuda/detail/internal/copy_cross_system.h:    cudaError status = cuda_cub::trivial_copy_to_device(d_in_ptr.data().get(),
deps/thrust/thrust/system/cuda/detail/internal/copy_cross_system.h:                                                        cuda_cub::stream(device_s));
deps/thrust/thrust/system/cuda/detail/internal/copy_cross_system.h:    cuda_cub::throw_on_error(status, "__copy:: H->D: failed");
deps/thrust/thrust/system/cuda/detail/internal/copy_cross_system.h:    OutputIt ret = cuda_cub::copy_n(device_s, d_in_ptr.data(), num_items, result);
deps/thrust/thrust/system/cuda/detail/internal/copy_cross_system.h:  cross_system_copy_n(thrust::cuda_cub::execution_policy<D>& device_s,
deps/thrust/thrust/system/cuda/detail/internal/copy_cross_system.h:    cuda_cub::uninitialized_copy_n(device_s, first, num_items, d_in_ptr.data());
deps/thrust/thrust/system/cuda/detail/internal/copy_cross_system.h:    cudaError status;
deps/thrust/thrust/system/cuda/detail/internal/copy_cross_system.h:    status = cuda_cub::trivial_copy_from_device(temp.data().get(),
deps/thrust/thrust/system/cuda/detail/internal/copy_cross_system.h:                                                cuda_cub::stream(device_s));
deps/thrust/thrust/system/cuda/detail/internal/copy_cross_system.h:    cuda_cub::throw_on_error(status, "__copy:: D->H: failed");
deps/thrust/thrust/system/cuda/detail/internal/copy_cross_system.h:} // namespace cuda_cub
deps/thrust/thrust/system/cuda/detail/internal/copy_device_to_device.h: * Copyright (c) 2016, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/thrust/system/cuda/detail/internal/copy_device_to_device.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/thrust/system/cuda/detail/internal/copy_device_to_device.h: * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/thrust/system/cuda/detail/internal/copy_device_to_device.h:#include <thrust/system/cuda/config.h>
deps/thrust/thrust/system/cuda/detail/internal/copy_device_to_device.h:#include <thrust/system/cuda/detail/execution_policy.h>
deps/thrust/thrust/system/cuda/detail/internal/copy_device_to_device.h:#include <thrust/system/cuda/detail/transform.h>
deps/thrust/thrust/system/cuda/detail/internal/copy_device_to_device.h:namespace cuda_cub {
deps/thrust/thrust/system/cuda/detail/internal/copy_device_to_device.h:    return cuda_cub::transform(policy,
deps/thrust/thrust/system/cuda/detail/internal/copy_device_to_device.h:}    // namespace cuda_cub
deps/thrust/thrust/system/cuda/detail/partition.h: * Copyright (c) 2016, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/thrust/system/cuda/detail/partition.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/thrust/system/cuda/detail/partition.h: * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/thrust/system/cuda/detail/partition.h:#include <thrust/system/cuda/config.h>
deps/thrust/thrust/system/cuda/detail/partition.h:#include <thrust/system/cuda/detail/util.h>
deps/thrust/thrust/system/cuda/detail/partition.h:#include <thrust/system/cuda/detail/reverse.h>
deps/thrust/thrust/system/cuda/detail/partition.h:#include <thrust/system/cuda/detail/find.h>
deps/thrust/thrust/system/cuda/detail/partition.h:#include <thrust/system/cuda/detail/uninitialized_copy.h>
deps/thrust/thrust/system/cuda/detail/partition.h:#include <thrust/system/cuda/detail/core/agent_launcher.h>
deps/thrust/thrust/system/cuda/detail/partition.h:#include <thrust/system/cuda/detail/par_to_seq.h>
deps/thrust/thrust/system/cuda/detail/partition.h:namespace cuda_cub {
deps/thrust/thrust/system/cuda/detail/partition.h:  static cudaError_t THRUST_RUNTIME_FUNCTION
deps/thrust/thrust/system/cuda/detail/partition.h:            cudaStream_t     stream,
deps/thrust/thrust/system/cuda/detail/partition.h:    cudaError_t status = cudaSuccess;
deps/thrust/thrust/system/cuda/detail/partition.h:    CUDA_CUB_RET_IF_FAIL(status);
deps/thrust/thrust/system/cuda/detail/partition.h:    CUDA_CUB_RET_IF_FAIL(status);
deps/thrust/thrust/system/cuda/detail/partition.h:    CUDA_CUB_RET_IF_FAIL(status);
deps/thrust/thrust/system/cuda/detail/partition.h:    CUDA_CUB_RET_IF_FAIL(cudaPeekAtLastError());
deps/thrust/thrust/system/cuda/detail/partition.h:    CUDA_CUB_RET_IF_FAIL(cudaPeekAtLastError());
deps/thrust/thrust/system/cuda/detail/partition.h:    cudaStream_t stream             = cuda_cub::stream(policy);
deps/thrust/thrust/system/cuda/detail/partition.h:    cudaError_t status;
deps/thrust/thrust/system/cuda/detail/partition.h:    cuda_cub::throw_on_error(status, "partition failed on 1st step");
deps/thrust/thrust/system/cuda/detail/partition.h:    cuda_cub::throw_on_error(status, "partition failed on 1st alias_storage");
deps/thrust/thrust/system/cuda/detail/partition.h:    cuda_cub::throw_on_error(status, "partition failed on 2nd alias_storage");
deps/thrust/thrust/system/cuda/detail/partition.h:    cuda_cub::throw_on_error(status, "partition failed on 2nd step");
deps/thrust/thrust/system/cuda/detail/partition.h:    status = cuda_cub::synchronize(policy);
deps/thrust/thrust/system/cuda/detail/partition.h:    cuda_cub::throw_on_error(status, "partition failed to synchronize");
deps/thrust/thrust/system/cuda/detail/partition.h:    cuda_cub::uninitialized_copy(policy, first, last, tmp.begin());
deps/thrust/thrust/system/cuda/detail/partition.h:  if (__THRUST_HAS_CUDART__)
deps/thrust/thrust/system/cuda/detail/partition.h:#if !__THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/partition.h:  if (__THRUST_HAS_CUDART__)
deps/thrust/thrust/system/cuda/detail/partition.h:#if !__THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/partition.h:  if (__THRUST_HAS_CUDART__)
deps/thrust/thrust/system/cuda/detail/partition.h:#if !__THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/partition.h:  if (__THRUST_HAS_CUDART__)
deps/thrust/thrust/system/cuda/detail/partition.h:#if !__THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/partition.h:  if (__THRUST_HAS_CUDART__)
deps/thrust/thrust/system/cuda/detail/partition.h:#if !__THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/partition.h:  if (__THRUST_HAS_CUDART__)
deps/thrust/thrust/system/cuda/detail/partition.h:#if !__THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/partition.h:  if (__THRUST_HAS_CUDART__)
deps/thrust/thrust/system/cuda/detail/partition.h:    cuda_cub::reverse(policy, result, last);
deps/thrust/thrust/system/cuda/detail/partition.h:#if !__THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/partition.h:  if (__THRUST_HAS_CUDART__)
deps/thrust/thrust/system/cuda/detail/partition.h:    cuda_cub::reverse(policy, result, last);
deps/thrust/thrust/system/cuda/detail/partition.h:#if !__THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/partition.h:  ItemsIt boundary = cuda_cub::find_if_not(policy, first, last, predicate);
deps/thrust/thrust/system/cuda/detail/partition.h:  ItemsIt end      = cuda_cub::find_if(policy,boundary,last,predicate);
deps/thrust/thrust/system/cuda/detail/partition.h:}    // namespace cuda_cub
deps/thrust/thrust/system/cuda/detail/uninitialized_copy.h: * Copyright (c) 2016, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/thrust/system/cuda/detail/uninitialized_copy.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/thrust/system/cuda/detail/uninitialized_copy.h: * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/thrust/system/cuda/detail/uninitialized_copy.h:#include <thrust/system/cuda/detail/execution_policy.h>
deps/thrust/thrust/system/cuda/detail/uninitialized_copy.h:#include <thrust/system/cuda/detail/util.h>
deps/thrust/thrust/system/cuda/detail/uninitialized_copy.h:#include <thrust/system/cuda/detail/parallel_for.h>
deps/thrust/thrust/system/cuda/detail/uninitialized_copy.h:namespace cuda_cub {
deps/thrust/thrust/system/cuda/detail/uninitialized_copy.h:#if defined(__CUDA__) && defined(__clang__)
deps/thrust/thrust/system/cuda/detail/uninitialized_copy.h:  cuda_cub::parallel_for(policy,
deps/thrust/thrust/system/cuda/detail/uninitialized_copy.h:  cuda_cub::throw_on_error(
deps/thrust/thrust/system/cuda/detail/uninitialized_copy.h:    cuda_cub::synchronize_optional(policy)
deps/thrust/thrust/system/cuda/detail/uninitialized_copy.h:  return cuda_cub::uninitialized_copy_n(policy,
deps/thrust/thrust/system/cuda/detail/uninitialized_copy.h:}    // namespace cuda_
deps/thrust/thrust/system/cuda/detail/error.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/cuda/detail/error.inl:#include <thrust/system/cuda/error.h>
deps/thrust/thrust/system/cuda/detail/error.inl:#include <thrust/system/cuda/detail/guarded_cuda_runtime_api.h>
deps/thrust/thrust/system/cuda/detail/error.inl:error_code make_error_code(cuda::errc::errc_t e)
deps/thrust/thrust/system/cuda/detail/error.inl:  return error_code(static_cast<int>(e), cuda_category());
deps/thrust/thrust/system/cuda/detail/error.inl:error_condition make_error_condition(cuda::errc::errc_t e)
deps/thrust/thrust/system/cuda/detail/error.inl:  return error_condition(static_cast<int>(e), cuda_category());
deps/thrust/thrust/system/cuda/detail/error.inl:namespace cuda_cub
deps/thrust/thrust/system/cuda/detail/error.inl:class cuda_error_category
deps/thrust/thrust/system/cuda/detail/error.inl:    inline cuda_error_category(void) {}
deps/thrust/thrust/system/cuda/detail/error.inl:      return "cuda";
deps/thrust/thrust/system/cuda/detail/error.inl:      char const* const unknown_name = "cudaErrorUnknown";
deps/thrust/thrust/system/cuda/detail/error.inl:      char const* c_str  = ::cudaGetErrorString(static_cast<cudaError_t>(ev));
deps/thrust/thrust/system/cuda/detail/error.inl:      char const* c_name = ::cudaGetErrorName(static_cast<cudaError_t>(ev));
deps/thrust/thrust/system/cuda/detail/error.inl:      using namespace cuda::errc;
deps/thrust/thrust/system/cuda/detail/error.inl:      if(ev < ::cudaErrorApiFailureBase)
deps/thrust/thrust/system/cuda/detail/error.inl:}; // end cuda_error_category
deps/thrust/thrust/system/cuda/detail/error.inl:} // end namespace cuda_cub
deps/thrust/thrust/system/cuda/detail/error.inl:const error_category &cuda_category(void)
deps/thrust/thrust/system/cuda/detail/error.inl:  static const thrust::system::cuda_cub::detail::cuda_error_category result;
deps/thrust/thrust/system/cuda/detail/guarded_cuda_runtime_api.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/cuda/detail/guarded_cuda_runtime_api.h:// and to undefine them before entering cuda_runtime_api.h (which will redefine them)
deps/thrust/thrust/system/cuda/detail/guarded_cuda_runtime_api.h:// we only try to do this stuff if cuda/include/host_defines.h has been included
deps/thrust/thrust/system/cuda/detail/guarded_cuda_runtime_api.h:#include <cuda_runtime_api.h>
deps/thrust/thrust/system/cuda/detail/transform.h: * Copyright (c) 2016, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/thrust/system/cuda/detail/transform.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/thrust/system/cuda/detail/transform.h: * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/thrust/system/cuda/detail/transform.h:#include <thrust/system/cuda/config.h>
deps/thrust/thrust/system/cuda/detail/transform.h:#include <thrust/system/cuda/detail/util.h>
deps/thrust/thrust/system/cuda/detail/transform.h:#include <thrust/system/cuda/detail/parallel_for.h>
deps/thrust/thrust/system/cuda/detail/transform.h:namespace cuda_cub {
deps/thrust/thrust/system/cuda/detail/transform.h:    cuda_cub::parallel_for(policy,
deps/thrust/thrust/system/cuda/detail/transform.h:    cuda_cub::throw_on_error(
deps/thrust/thrust/system/cuda/detail/transform.h:      cuda_cub::synchronize_optional(policy)
deps/thrust/thrust/system/cuda/detail/transform.h:    cuda_cub::parallel_for(policy,
deps/thrust/thrust/system/cuda/detail/transform.h:    cuda_cub::throw_on_error(
deps/thrust/thrust/system/cuda/detail/transform.h:      cuda_cub::synchronize_optional(policy)
deps/thrust/thrust/system/cuda/detail/transform.h:  return cuda_cub::transform_if(policy,
deps/thrust/thrust/system/cuda/detail/transform.h:  return cuda_cub::transform_if(policy,
deps/thrust/thrust/system/cuda/detail/transform.h:  return cuda_cub::transform_if(policy,
deps/thrust/thrust/system/cuda/detail/transform.h:}    // namespace cuda_cub
deps/thrust/thrust/system/cuda/detail/tabulate.h: * Copyright (c) 2016, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/thrust/system/cuda/detail/tabulate.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/thrust/system/cuda/detail/tabulate.h: * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/thrust/system/cuda/detail/tabulate.h:#include <thrust/system/cuda/config.h>
deps/thrust/thrust/system/cuda/detail/tabulate.h:#include <thrust/system/cuda/execution_policy.h>
deps/thrust/thrust/system/cuda/detail/tabulate.h:#include <thrust/system/cuda/detail/parallel_for.h>
deps/thrust/thrust/system/cuda/detail/tabulate.h:namespace cuda_cub {
deps/thrust/thrust/system/cuda/detail/tabulate.h:  cuda_cub::parallel_for(policy,
deps/thrust/thrust/system/cuda/detail/tabulate.h:  cuda_cub::throw_on_error(
deps/thrust/thrust/system/cuda/detail/tabulate.h:    cuda_cub::synchronize_optional(policy)
deps/thrust/thrust/system/cuda/detail/tabulate.h:}    // namespace cuda_cub
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h: * Copyright (c) 2016, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h: * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:#include <thrust/system/cuda/detail/core/alignment.h>
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:#include <thrust/system/cuda/detail/guarded_cuda_runtime_api.h>
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:namespace cuda_cub {
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:    cudaStream_t const stream;
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:                   cudaStream_t stream_     = 0)
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:    cudaError_t __host__
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:      return cudaPeekAtLastError();
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:    cudaError_t __host__
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:      return cudaPeekAtLastError();
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:    cudaError_t __host__
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:      return cudaPeekAtLastError();
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:    cudaError_t __host__
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:      return cudaPeekAtLastError();
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:    cudaError_t __host__
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:      return cudaPeekAtLastError();
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:    cudaError_t __host__
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:      return cudaPeekAtLastError();
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:    cudaError_t __host__
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:      return cudaPeekAtLastError();
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:    cudaError_t __host__
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:      return cudaPeekAtLastError();
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:    cudaError_t __host__
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:      return cudaPeekAtLastError();
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:    cudaError_t __host__
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:      return cudaPeekAtLastError();
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:    cudaError_t __host__
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:      return cudaPeekAtLastError();
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:    cudaError_t __host__
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:      return cudaPeekAtLastError();
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:    cudaError_t __host__
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:      return cudaPeekAtLastError();
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:    cudaError_t __host__
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:      return cudaPeekAtLastError();
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:    cudaError_t __host__
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:      return cudaPeekAtLastError();
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:    cudaError_t __host__
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:      return cudaPeekAtLastError();
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:    cudaError_t __host__
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:      return cudaPeekAtLastError();
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:    cudaError_t __device__
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:      cudaError_t status = cudaErrorNotSupported;
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:#if __THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:      void *param_buffer = cudaGetParameterBuffer(64,size);
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:    cudaError_t __device__
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:      cudaError_t status = cudaErrorNotSupported;
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:#if __THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:      void *param_buffer = cudaGetParameterBuffer(64,size);
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:    cudaError_t __device__
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:      cudaError_t status = cudaErrorNotSupported;
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:#if __THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:      void *param_buffer = cudaGetParameterBuffer(64,size);
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:    cudaError_t __device__
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:      cudaError_t status = cudaErrorNotSupported;
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:#if __THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:      void *param_buffer = cudaGetParameterBuffer(64,size);
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:    cudaError_t __device__
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:      cudaError_t status = cudaErrorNotSupported;
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:#if __THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:      void *param_buffer = cudaGetParameterBuffer(64,size);
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:    cudaError_t __device__
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:      cudaError_t status = cudaErrorNotSupported;
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:#if __THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:      void *param_buffer = cudaGetParameterBuffer(64,size);
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:    cudaError_t __device__
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:      cudaError_t status = cudaErrorNotSupported;
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:#if __THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:      void *param_buffer = cudaGetParameterBuffer(64,size);
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:    cudaError_t __device__
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:      cudaError_t status = cudaErrorNotSupported;
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:#if __THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:      void *param_buffer = cudaGetParameterBuffer(64,size);
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:    cudaError_t __device__
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:      cudaError_t status = cudaErrorNotSupported;
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:#if __THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:      void *param_buffer = cudaGetParameterBuffer(64,size);
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:    cudaError_t __device__
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:      cudaError_t status = cudaErrorNotSupported;
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:#if __THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:      void *param_buffer = cudaGetParameterBuffer(64,size);
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:    cudaError_t __device__
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:      cudaError_t status = cudaErrorNotSupported;
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:#if __THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:      void *param_buffer = cudaGetParameterBuffer(64,size);
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:    cudaError_t __device__
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:      cudaError_t status = cudaErrorNotSupported;
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:#if __THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:      void *param_buffer = cudaGetParameterBuffer(64,size);
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:    cudaError_t __device__
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:      cudaError_t status = cudaErrorNotSupported;
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:#if __THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:      void *param_buffer = cudaGetParameterBuffer(64,size);
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:    cudaError_t __device__
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:      cudaError_t status = cudaErrorNotSupported;
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:#if __THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:      void *param_buffer = cudaGetParameterBuffer(64,size);
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:    cudaError_t __device__
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:      cudaError_t status = cudaErrorNotSupported;
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:#if __THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:      void *param_buffer = cudaGetParameterBuffer(64,size);
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:    cudaError_t __device__
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:      cudaError_t status = cudaErrorNotSupported;
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:#if __THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:      void *param_buffer = cudaGetParameterBuffer(64,size);
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:    cudaError_t __device__
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:      cudaError_t status = cudaErrorNotSupported;
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:#if __THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:      void *param_buffer = cudaGetParameterBuffer(64,size);
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:    cudaError_t __device__
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:#if __THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:      return cudaLaunchDevice((void*)k,
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:      return cudaErrorNotSupported;
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:#if defined(_NVHPC_CUDA)
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:#elif defined(__CUDA_ARCH__)
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:    cudaError_t THRUST_FUNCTION
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:    cudaError_t THRUST_FUNCTION
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:    cudaError_t THRUST_FUNCTION
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:    cudaError_t THRUST_FUNCTION
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:    cudaError_t THRUST_FUNCTION
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:    cudaError_t THRUST_FUNCTION
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:    cudaError_t THRUST_FUNCTION
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:    cudaError_t THRUST_FUNCTION
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:    cudaError_t THRUST_FUNCTION
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:    cudaError_t THRUST_FUNCTION
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:    cudaError_t THRUST_FUNCTION
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:    cudaError_t THRUST_FUNCTION
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:    cudaError_t THRUST_FUNCTION
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:    cudaError_t THRUST_FUNCTION
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:    cudaError_t THRUST_FUNCTION
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:    cudaError_t THRUST_FUNCTION
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:    cudaError_t THRUST_FUNCTION
deps/thrust/thrust/system/cuda/detail/core/triple_chevron_launch.h:}    // namespace cuda_
deps/thrust/thrust/system/cuda/detail/core/agent_launcher.h: * Copyright (c) 2016, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/thrust/system/cuda/detail/core/agent_launcher.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/thrust/system/cuda/detail/core/agent_launcher.h: * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/thrust/system/cuda/detail/core/agent_launcher.h:#include <thrust/system/cuda/detail/guarded_cuda_runtime_api.h>
deps/thrust/thrust/system/cuda/detail/core/agent_launcher.h:#include <thrust/system/cuda/detail/core/triple_chevron_launch.h>
deps/thrust/thrust/system/cuda/detail/core/agent_launcher.h:#include <thrust/system/cuda/detail/core/util.h>
deps/thrust/thrust/system/cuda/detail/core/agent_launcher.h:namespace cuda_cub {
deps/thrust/thrust/system/cuda/detail/core/agent_launcher.h:#if defined(__CUDA_ARCH__) || defined(_NVHPC_CUDA)
deps/thrust/thrust/system/cuda/detail/core/agent_launcher.h:    cudaStream_t    stream;
deps/thrust/thrust/system/cuda/detail/core/agent_launcher.h:                  cudaStream_t stream_,
deps/thrust/thrust/system/cuda/detail/core/agent_launcher.h:                  cudaStream_t stream_,
deps/thrust/thrust/system/cuda/detail/core/agent_launcher.h:                  cudaStream_t stream_,
deps/thrust/thrust/system/cuda/detail/core/agent_launcher.h:                  cudaStream_t stream_,
deps/thrust/thrust/system/cuda/detail/core/agent_launcher.h:    AgentPlan static get_plan(cudaStream_t s, void* d_ptr = 0)
deps/thrust/thrust/system/cuda/detail/core/agent_launcher.h:#ifdef __CUDACC_RDC__
deps/thrust/thrust/system/cuda/detail/core/agent_launcher.h:      core::cuda_optional<int> ptx_version = core::get_ptx_version();
deps/thrust/thrust/system/cuda/detail/core/agent_launcher.h:      //CUDA_CUB_RET_IF_FAIL(ptx_version.status());
deps/thrust/thrust/system/cuda/detail/core/agent_launcher.h:    typename core::get_plan<Agent>::type static get_plan(cudaStream_t , void* d_ptr = 0)
deps/thrust/thrust/system/cuda/detail/core/agent_launcher.h:      core::cuda_optional<int> ptx_version = core::get_ptx_version();
deps/thrust/thrust/system/cuda/detail/core/agent_launcher.h:            cudaStreamSynchronize(stream);
deps/thrust/thrust/system/cuda/detail/core/agent_launcher.h:    static cuda_optional<int> THRUST_RUNTIME_FUNCTION
deps/thrust/thrust/system/cuda/detail/core/agent_launcher.h:      cudaError_t status = cub::MaxSmOccupancy(occ, k, block_threads);
deps/thrust/thrust/system/cuda/detail/core/agent_launcher.h:      return cuda_optional<int>(status == cudaSuccess ? occ : -1, status);
deps/thrust/thrust/system/cuda/detail/core/agent_launcher.h:    cuda_optional<int> THRUST_RUNTIME_FUNCTION
deps/thrust/thrust/system/cuda/detail/core/agent_launcher.h:        cuda_optional<int> occ = max_sm_occupancy(k);
deps/thrust/thrust/system/cuda/detail/core/agent_launcher.h:        core::cuda_optional<int> ptx_version = core::get_ptx_version();
deps/thrust/thrust/system/cuda/detail/core/agent_launcher.h:    static cuda_optional<int> THRUST_RUNTIME_FUNCTION
deps/thrust/thrust/system/cuda/detail/core/agent_launcher.h:    static cuda_optional<int> THRUST_RUNTIME_FUNCTION
deps/thrust/thrust/system/cuda/detail/core/agent_launcher.h:    static cuda_optional<int> THRUST_RUNTIME_FUNCTION
deps/thrust/thrust/system/cuda/detail/core/agent_launcher.h:    static cuda_optional<int> THRUST_RUNTIME_FUNCTION
deps/thrust/thrust/system/cuda/detail/core/agent_launcher.h:    static cuda_optional<int> THRUST_RUNTIME_FUNCTION
deps/thrust/thrust/system/cuda/detail/core/agent_launcher.h:    static cuda_optional<int> THRUST_RUNTIME_FUNCTION
deps/thrust/thrust/system/cuda/detail/core/agent_launcher.h:    static cuda_optional<int> THRUST_RUNTIME_FUNCTION
deps/thrust/thrust/system/cuda/detail/core/agent_launcher.h:    static cuda_optional<int> THRUST_RUNTIME_FUNCTION
deps/thrust/thrust/system/cuda/detail/core/agent_launcher.h:    static cuda_optional<int> THRUST_RUNTIME_FUNCTION
deps/thrust/thrust/system/cuda/detail/core/agent_launcher.h:    static cuda_optional<int> THRUST_RUNTIME_FUNCTION
deps/thrust/thrust/system/cuda/detail/core/agent_launcher.h:    static cuda_optional<int> THRUST_RUNTIME_FUNCTION
deps/thrust/thrust/system/cuda/detail/core/agent_launcher.h:    static cuda_optional<int> THRUST_RUNTIME_FUNCTION
deps/thrust/thrust/system/cuda/detail/core/agent_launcher.h:    static cuda_optional<int> THRUST_RUNTIME_FUNCTION
deps/thrust/thrust/system/cuda/detail/core/agent_launcher.h:    static cuda_optional<int> THRUST_RUNTIME_FUNCTION
deps/thrust/thrust/system/cuda/detail/core/agent_launcher.h:    static cuda_optional<int> THRUST_RUNTIME_FUNCTION
deps/thrust/thrust/system/cuda/detail/core/agent_launcher.h:    static cuda_optional<int> THRUST_RUNTIME_FUNCTION
deps/thrust/thrust/system/cuda/detail/core/agent_launcher.h:#ifdef __CUDA_ARCH__
deps/thrust/thrust/system/cuda/detail/core/alignment.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/cuda/detail/core/alignment.h:#include <thrust/system/cuda/detail/util.h>
deps/thrust/thrust/system/cuda/detail/core/alignment.h:namespace cuda_cub {
deps/thrust/thrust/system/cuda/detail/core/alignment.h:// __align__ is CUDA-specific, so guard it
deps/thrust/thrust/system/cuda/detail/core/alignment.h:}    // end cuda_
deps/thrust/thrust/system/cuda/detail/core/util.h: * Copyright (c) 2016, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/thrust/system/cuda/detail/core/util.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/thrust/system/cuda/detail/core/util.h: * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/thrust/system/cuda/detail/core/util.h:#include <cuda_occupancy.h>
deps/thrust/thrust/system/cuda/detail/core/util.h:#include <thrust/system/cuda/config.h>
deps/thrust/thrust/system/cuda/detail/core/util.h:#include <thrust/system/cuda/detail/util.h>
deps/thrust/thrust/system/cuda/detail/core/util.h:namespace cuda_cub {
deps/thrust/thrust/system/cuda/detail/core/util.h:#ifdef _NVHPC_CUDA
deps/thrust/thrust/system/cuda/detail/core/util.h:#  if (__NVCOMPILER_CUDA_ARCH__ >= 600)
deps/thrust/thrust/system/cuda/detail/core/util.h:#  elif (__NVCOMPILER_CUDA_ARCH__ >= 520)
deps/thrust/thrust/system/cuda/detail/core/util.h:#  elif (__NVCOMPILER_CUDA_ARCH__ >= 350)
deps/thrust/thrust/system/cuda/detail/core/util.h:#  if (__CUDA_ARCH__ >= 600)
deps/thrust/thrust/system/cuda/detail/core/util.h:#  elif (__CUDA_ARCH__ >= 520)
deps/thrust/thrust/system/cuda/detail/core/util.h:#  elif (__CUDA_ARCH__ >= 350)
deps/thrust/thrust/system/cuda/detail/core/util.h:#  elif (__CUDA_ARCH__ >= 300)
deps/thrust/thrust/system/cuda/detail/core/util.h:#  elif !defined (__CUDA_ARCH__)
deps/thrust/thrust/system/cuda/detail/core/util.h:      #ifdef _NVHPC_CUDA
deps/thrust/thrust/system/cuda/detail/core/util.h:        #ifdef __THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/core/util.h:        #if (CUB_PTX_ARCH > 0) && defined(__THRUST_HAS_CUDART__)
deps/thrust/thrust/system/cuda/detail/core/util.h:// TODO: since we are unable to afford kernel launch + cudaMemcpy ON EVERY
deps/thrust/thrust/system/cuda/detail/core/util.h://       first invocation will invoke kernel launch + cudaMemcpy, but
deps/thrust/thrust/system/cuda/detail/core/util.h:  xget_agent_plan_impl(F f, cudaStream_t s, void* d_ptr)
deps/thrust/thrust/system/cuda/detail/core/util.h:#ifdef __CUDA_ARCH__
deps/thrust/thrust/system/cuda/detail/core/util.h:      cudaGetSymbolAddress(&d_ptr, agent_plan_device);
deps/thrust/thrust/system/cuda/detail/core/util.h:    cudaMemcpyAsync((void*)&plan,
deps/thrust/thrust/system/cuda/detail/core/util.h:                    cudaMemcpyDeviceToHost,
deps/thrust/thrust/system/cuda/detail/core/util.h:    cudaStreamSynchronize(s);
deps/thrust/thrust/system/cuda/detail/core/util.h:  get_agent_plan(cudaStream_t s = 0, void *ptr = 0)
deps/thrust/thrust/system/cuda/detail/core/util.h:    cuda_cub::throw_on_error(cudaGetDevice(&dev_id),
deps/thrust/thrust/system/cuda/detail/core/util.h:                             "failed to cudaGetDevice");
deps/thrust/thrust/system/cuda/detail/core/util.h:    cudaError_t status;
deps/thrust/thrust/system/cuda/detail/core/util.h:    status = cudaDeviceGetAttribute(&i32value,
deps/thrust/thrust/system/cuda/detail/core/util.h:                                    cudaDevAttrMultiProcessorCount,
deps/thrust/thrust/system/cuda/detail/core/util.h:    cuda_cub::throw_on_error(status,
deps/thrust/thrust/system/cuda/detail/core/util.h:    cuda_cub::throw_on_error(cudaGetDevice(&dev_id),
deps/thrust/thrust/system/cuda/detail/core/util.h:                             "failed to cudaGetDevice");
deps/thrust/thrust/system/cuda/detail/core/util.h:    cudaError_t status;
deps/thrust/thrust/system/cuda/detail/core/util.h:    status = cudaDeviceGetAttribute(&i32value,
deps/thrust/thrust/system/cuda/detail/core/util.h:                                    cudaDevAttrMaxSharedMemoryPerBlock,
deps/thrust/thrust/system/cuda/detail/core/util.h:    cuda_cub::throw_on_error(status,
deps/thrust/thrust/system/cuda/detail/core/util.h:  // cuda_optional
deps/thrust/thrust/system/cuda/detail/core/util.h:  // used for function that return cudaError_t along with the result
deps/thrust/thrust/system/cuda/detail/core/util.h:  class cuda_optional
deps/thrust/thrust/system/cuda/detail/core/util.h:    cudaError_t status_;
deps/thrust/thrust/system/cuda/detail/core/util.h:    cuda_optional() : status_(cudaSuccess) {}
deps/thrust/thrust/system/cuda/detail/core/util.h:    cuda_optional(T v, cudaError_t status = cudaSuccess) : status_(status), value_(v) {}
deps/thrust/thrust/system/cuda/detail/core/util.h:    isValid() const { return cudaSuccess == status_; }
deps/thrust/thrust/system/cuda/detail/core/util.h:    cudaError_t __host__ __device__
deps/thrust/thrust/system/cuda/detail/core/util.h:  cuda_optional<int> THRUST_RUNTIME_FUNCTION
deps/thrust/thrust/system/cuda/detail/core/util.h:    cudaError_t status = cub::PtxVersion(ptx_version);
deps/thrust/thrust/system/cuda/detail/core/util.h:    return cuda_optional<int>(ptx_version, status);
deps/thrust/thrust/system/cuda/detail/core/util.h:  cudaError_t THRUST_RUNTIME_FUNCTION
deps/thrust/thrust/system/cuda/detail/core/util.h:  sync_stream(cudaStream_t stream)
deps/thrust/thrust/system/cuda/detail/core/util.h:#define CUDA_CUB_RET_IF_FAIL(e) \
deps/thrust/thrust/system/cuda/detail/core/util.h:    inline cuda_optional<size_t> get_max_shared_memory_per_block()
deps/thrust/thrust/system/cuda/detail/core/util.h:      cudaError_t status = cudaSuccess;
deps/thrust/thrust/system/cuda/detail/core/util.h:      status             = cudaGetDevice(&dev_id);
deps/thrust/thrust/system/cuda/detail/core/util.h:      if (status != cudaSuccess) return cuda_optional<size_t>(0, status);
deps/thrust/thrust/system/cuda/detail/core/util.h:      status        = cudaDeviceGetAttribute(&max_shmem,
deps/thrust/thrust/system/cuda/detail/core/util.h:                                      cudaDevAttrMaxSharedMemoryPerBlock,
deps/thrust/thrust/system/cuda/detail/core/util.h:      if (status != cudaSuccess) return cuda_optional<size_t>(0, status);
deps/thrust/thrust/system/cuda/detail/core/util.h:      return cuda_optional<size_t>(max_shmem, status);
deps/thrust/thrust/system/cuda/detail/core/util.h:  THRUST_RUNTIME_FUNCTION cudaError_t
deps/thrust/thrust/system/cuda/detail/core/util.h:} // namespace cuda_
deps/thrust/thrust/system/cuda/detail/execution_policy.h: * Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/thrust/system/cuda/detail/execution_policy.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/thrust/system/cuda/detail/execution_policy.h: * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/thrust/system/cuda/detail/execution_policy.h:#include <thrust/system/cuda/config.h>
deps/thrust/thrust/system/cuda/detail/execution_policy.h:namespace cuda_cub
deps/thrust/thrust/system/cuda/detail/execution_policy.h:, thrust::detail::allocator_aware_execution_policy<cuda_cub::execution_policy>
deps/thrust/thrust/system/cuda/detail/execution_policy.h:, thrust::detail::dependencies_aware_execution_policy<cuda_cub::execution_policy>
deps/thrust/thrust/system/cuda/detail/execution_policy.h:} // namespace cuda_cub
deps/thrust/thrust/system/cuda/detail/execution_policy.h:namespace system { namespace cuda { namespace detail
deps/thrust/thrust/system/cuda/detail/execution_policy.h:using thrust::cuda_cub::tag;
deps/thrust/thrust/system/cuda/detail/execution_policy.h:using thrust::cuda_cub::execution_policy;
deps/thrust/thrust/system/cuda/detail/execution_policy.h:}}} // namespace system::cuda::detail
deps/thrust/thrust/system/cuda/detail/execution_policy.h:namespace system { namespace cuda
deps/thrust/thrust/system/cuda/detail/execution_policy.h:using thrust::cuda_cub::tag;
deps/thrust/thrust/system/cuda/detail/execution_policy.h:using thrust::cuda_cub::execution_policy;
deps/thrust/thrust/system/cuda/detail/execution_policy.h:}} // namespace system::cuda
deps/thrust/thrust/system/cuda/detail/execution_policy.h:namespace cuda
deps/thrust/thrust/system/cuda/detail/execution_policy.h:using thrust::cuda_cub::tag;
deps/thrust/thrust/system/cuda/detail/execution_policy.h:using thrust::cuda_cub::execution_policy;
deps/thrust/thrust/system/cuda/detail/execution_policy.h:} // namespace cuda
deps/thrust/thrust/system/cuda/detail/merge.h:j * Copyright (c) 2016, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/thrust/system/cuda/detail/merge.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/thrust/system/cuda/detail/merge.h: * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/thrust/system/cuda/detail/merge.h:#include <thrust/system/cuda/detail/util.h>
deps/thrust/thrust/system/cuda/detail/merge.h:#include <thrust/system/cuda/detail/execution_policy.h>
deps/thrust/thrust/system/cuda/detail/merge.h:#include <thrust/system/cuda/detail/util.h>
deps/thrust/thrust/system/cuda/detail/merge.h:#include <thrust/system/cuda/detail/core/agent_launcher.h>
deps/thrust/thrust/system/cuda/detail/merge.h:#include <thrust/system/cuda/detail/core/util.h>
deps/thrust/thrust/system/cuda/detail/merge.h:#include <thrust/system/cuda/detail/par_to_seq.h>
deps/thrust/thrust/system/cuda/detail/merge.h:namespace cuda_cub {
deps/thrust/thrust/system/cuda/detail/merge.h:  cudaError_t THRUST_RUNTIME_FUNCTION
deps/thrust/thrust/system/cuda/detail/merge.h:            cudaStream_t  stream,
deps/thrust/thrust/system/cuda/detail/merge.h:      return cudaErrorNotSupported;
deps/thrust/thrust/system/cuda/detail/merge.h:    cudaError_t status = cudaSuccess;
deps/thrust/thrust/system/cuda/detail/merge.h:    CUDA_CUB_RET_IF_FAIL(status);
deps/thrust/thrust/system/cuda/detail/merge.h:      CUDA_CUB_RET_IF_FAIL(cudaPeekAtLastError());
deps/thrust/thrust/system/cuda/detail/merge.h:    CUDA_CUB_RET_IF_FAIL(cudaPeekAtLastError());
deps/thrust/thrust/system/cuda/detail/merge.h:    cudaStream_t stream       = cuda_cub::stream(policy);
deps/thrust/thrust/system/cuda/detail/merge.h:    cudaError_t status;
deps/thrust/thrust/system/cuda/detail/merge.h:    cuda_cub::throw_on_error(status, "merge: failed on 1st step");
deps/thrust/thrust/system/cuda/detail/merge.h:    cuda_cub::throw_on_error(status, "merge: failed on 2nd step");
deps/thrust/thrust/system/cuda/detail/merge.h:    status = cuda_cub::synchronize_optional(policy);
deps/thrust/thrust/system/cuda/detail/merge.h:    cuda_cub::throw_on_error(status, "merge: failed to synchronize");
deps/thrust/thrust/system/cuda/detail/merge.h:  if (__THRUST_HAS_CUDART__)
deps/thrust/thrust/system/cuda/detail/merge.h:#if !__THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/merge.h:  return cuda_cub::merge(policy,
deps/thrust/thrust/system/cuda/detail/merge.h:  if (__THRUST_HAS_CUDART__)
deps/thrust/thrust/system/cuda/detail/merge.h:#if !__THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/merge.h:  return cuda_cub::merge_by_key(policy,
deps/thrust/thrust/system/cuda/detail/merge.h:}    // namespace cuda_cub
deps/thrust/thrust/system/cuda/detail/sequence.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/cuda/detail/assign_value.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/cuda/detail/assign_value.h:#include <thrust/system/cuda/config.h>
deps/thrust/thrust/system/cuda/detail/assign_value.h:#include <thrust/system/cuda/detail/execution_policy.h>
deps/thrust/thrust/system/cuda/detail/assign_value.h:#include <thrust/system/cuda/detail/copy.h>
deps/thrust/thrust/system/cuda/detail/assign_value.h:namespace cuda_cub {
deps/thrust/thrust/system/cuda/detail/assign_value.h:  void assign_value(thrust::cuda::execution_policy<DerivedPolicy> &exec, Pointer1 dst, Pointer2 src)
deps/thrust/thrust/system/cuda/detail/assign_value.h:    __host__ inline static void host_path(thrust::cuda::execution_policy<DerivedPolicy> &exec, Pointer1 dst, Pointer2 src)
deps/thrust/thrust/system/cuda/detail/assign_value.h:      cuda_cub::copy(exec, src, src + 1, dst);
deps/thrust/thrust/system/cuda/detail/assign_value.h:    __device__ inline static void device_path(thrust::cuda::execution_policy<DerivedPolicy> &, Pointer1 dst, Pointer2 src)
deps/thrust/thrust/system/cuda/detail/assign_value.h:      cuda_cub::copy(rotated_systems, src, src + 1, dst);
deps/thrust/thrust/system/cuda/detail/assign_value.h:      // XXX forward the true cuda::execution_policy inside systems here
deps/thrust/thrust/system/cuda/detail/assign_value.h:      thrust::cuda::tag cuda_tag;
deps/thrust/thrust/system/cuda/detail/assign_value.h:      thrust::cuda_cub::assign_value(cuda_tag, dst, src);
deps/thrust/thrust/system/cuda/detail/assign_value.h:} // end cuda_cub
deps/thrust/thrust/system/cuda/detail/parallel_for.h: * Copyright (c) 2016, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/thrust/system/cuda/detail/parallel_for.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/thrust/system/cuda/detail/parallel_for.h: * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/thrust/system/cuda/detail/parallel_for.h:#include <thrust/system/cuda/config.h>
deps/thrust/thrust/system/cuda/detail/parallel_for.h:#include <thrust/system/cuda/detail/util.h>
deps/thrust/thrust/system/cuda/detail/parallel_for.h:#include <thrust/system/cuda/detail/par_to_seq.h>
deps/thrust/thrust/system/cuda/detail/parallel_for.h:#include <thrust/system/cuda/detail/core/agent_launcher.h>
deps/thrust/thrust/system/cuda/detail/parallel_for.h:#include <thrust/system/cuda/detail/par_to_seq.h>
deps/thrust/thrust/system/cuda/detail/parallel_for.h:namespace cuda_cub {
deps/thrust/thrust/system/cuda/detail/parallel_for.h:  THRUST_RUNTIME_FUNCTION cudaError_t
deps/thrust/thrust/system/cuda/detail/parallel_for.h:               cudaStream_t stream)
deps/thrust/thrust/system/cuda/detail/parallel_for.h:      return cudaSuccess;
deps/thrust/thrust/system/cuda/detail/parallel_for.h:    CUDA_CUB_RET_IF_FAIL(cudaPeekAtLastError());
deps/thrust/thrust/system/cuda/detail/parallel_for.h:    return cudaSuccess;
deps/thrust/thrust/system/cuda/detail/parallel_for.h:  if (__THRUST_HAS_CUDART__)
deps/thrust/thrust/system/cuda/detail/parallel_for.h:    cudaStream_t stream = cuda_cub::stream(policy);
deps/thrust/thrust/system/cuda/detail/parallel_for.h:    cudaError_t  status = __parallel_for::parallel_for(count, f, stream);
deps/thrust/thrust/system/cuda/detail/parallel_for.h:    cuda_cub::throw_on_error(status, "parallel_for failed");
deps/thrust/thrust/system/cuda/detail/parallel_for.h:#if !__THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/parallel_for.h:}    // namespace cuda_cub
deps/thrust/thrust/system/cuda/detail/inner_product.h: * Copyright (c) 2016, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/thrust/system/cuda/detail/inner_product.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/thrust/system/cuda/detail/inner_product.h: * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/thrust/system/cuda/detail/inner_product.h:#include <thrust/system/cuda/detail/reduce.h>
deps/thrust/thrust/system/cuda/detail/inner_product.h:namespace cuda_cub {
deps/thrust/thrust/system/cuda/detail/inner_product.h:  return cuda_cub::reduce_n(policy,
deps/thrust/thrust/system/cuda/detail/inner_product.h:  return cuda_cub::inner_product(policy,
deps/thrust/thrust/system/cuda/detail/inner_product.h:}    // namespace cuda_cub
deps/thrust/thrust/system/cuda/detail/dispatch.h: *  Copyright 2018 NVIDIA Corporation
deps/thrust/thrust/system/cuda/detail/count.h: * Copyright (c) 2016, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/thrust/system/cuda/detail/count.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/thrust/system/cuda/detail/count.h: * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/thrust/system/cuda/detail/count.h:#include <thrust/system/cuda/config.h>
deps/thrust/thrust/system/cuda/detail/count.h:#include <thrust/system/cuda/detail/util.h>
deps/thrust/thrust/system/cuda/detail/count.h:#include <thrust/system/cuda/detail/reduce.h>
deps/thrust/thrust/system/cuda/detail/count.h:namespace cuda_cub {
deps/thrust/thrust/system/cuda/detail/count.h:  return cuda_cub::reduce_n(policy,
deps/thrust/thrust/system/cuda/detail/count.h:  return cuda_cub::count_if(policy,
deps/thrust/thrust/system/cuda/detail/count.h:} // namespace cuda_cub
deps/thrust/thrust/system/cuda/detail/reduce.h: * Copyright (c) 2016, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/thrust/system/cuda/detail/reduce.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/thrust/system/cuda/detail/reduce.h: * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/thrust/system/cuda/detail/reduce.h:#include <thrust/system/cuda/config.h>
deps/thrust/thrust/system/cuda/detail/reduce.h:#include <thrust/system/cuda/detail/util.h>
deps/thrust/thrust/system/cuda/detail/reduce.h:#include <thrust/system/cuda/detail/par_to_seq.h>
deps/thrust/thrust/system/cuda/detail/reduce.h:#include <thrust/system/cuda/detail/get_value.h>
deps/thrust/thrust/system/cuda/detail/reduce.h:#include <thrust/system/cuda/detail/dispatch.h>
deps/thrust/thrust/system/cuda/detail/reduce.h:#include <thrust/system/cuda/detail/make_unsigned_special.h>
deps/thrust/thrust/system/cuda/detail/reduce.h:#include <thrust/system/cuda/detail/core/agent_launcher.h>
deps/thrust/thrust/system/cuda/detail/reduce.h:namespace cuda_cub {
deps/thrust/thrust/system/cuda/detail/reduce.h:  cudaError_t THRUST_RUNTIME_FUNCTION
deps/thrust/thrust/system/cuda/detail/reduce.h:            cudaStream_t stream,
deps/thrust/thrust/system/cuda/detail/reduce.h:    using core::cuda_optional;
deps/thrust/thrust/system/cuda/detail/reduce.h:      return cudaErrorNotSupported;
deps/thrust/thrust/system/cuda/detail/reduce.h:    cudaError_t status = cudaSuccess;
deps/thrust/thrust/system/cuda/detail/reduce.h:      CUDA_CUB_RET_IF_FAIL(cudaPeekAtLastError());
deps/thrust/thrust/system/cuda/detail/reduce.h:      cuda_optional<int> sm_count = core::get_sm_count();
deps/thrust/thrust/system/cuda/detail/reduce.h:      CUDA_CUB_RET_IF_FAIL(sm_count.status());
deps/thrust/thrust/system/cuda/detail/reduce.h:      cuda_optional<int> max_blocks_per_sm =
deps/thrust/thrust/system/cuda/detail/reduce.h:      CUDA_CUB_RET_IF_FAIL(max_blocks_per_sm.status());
deps/thrust/thrust/system/cuda/detail/reduce.h:      CUDA_CUB_RET_IF_FAIL(status);
deps/thrust/thrust/system/cuda/detail/reduce.h:        CUDA_CUB_RET_IF_FAIL(cudaPeekAtLastError());
deps/thrust/thrust/system/cuda/detail/reduce.h:        CUDA_CUB_RET_IF_FAIL(cudaErrorNotSupported);
deps/thrust/thrust/system/cuda/detail/reduce.h:      CUDA_CUB_RET_IF_FAIL(cudaPeekAtLastError());
deps/thrust/thrust/system/cuda/detail/reduce.h:      CUDA_CUB_RET_IF_FAIL(cudaPeekAtLastError());
deps/thrust/thrust/system/cuda/detail/reduce.h:    cudaStream_t stream             = cuda_cub::stream(policy);
deps/thrust/thrust/system/cuda/detail/reduce.h:    cudaError_t status;
deps/thrust/thrust/system/cuda/detail/reduce.h:    cuda_cub::throw_on_error(status, "reduce failed on 1st step");
deps/thrust/thrust/system/cuda/detail/reduce.h:    cuda_cub::throw_on_error(status, "reduce failed on 1st alias_storage");
deps/thrust/thrust/system/cuda/detail/reduce.h:    cuda_cub::throw_on_error(status, "reduce failed on 2nd alias_storage");
deps/thrust/thrust/system/cuda/detail/reduce.h:    cuda_cub::throw_on_error(status, "reduce failed on 2nd step");
deps/thrust/thrust/system/cuda/detail/reduce.h:    status = cuda_cub::synchronize(policy);
deps/thrust/thrust/system/cuda/detail/reduce.h:    cuda_cub::throw_on_error(status, "reduce failed to synchronize");
deps/thrust/thrust/system/cuda/detail/reduce.h:    T result = cuda_cub::get_value(policy, d_result);
deps/thrust/thrust/system/cuda/detail/reduce.h:  cudaStream_t stream = cuda_cub::stream(policy);
deps/thrust/thrust/system/cuda/detail/reduce.h:  cudaError_t status;
deps/thrust/thrust/system/cuda/detail/reduce.h:  cuda_cub::throw_on_error(status, "after reduction step 1");
deps/thrust/thrust/system/cuda/detail/reduce.h:  // aligned for any type of data. `malloc`/`cudaMalloc`/`new`/`std::allocator`
deps/thrust/thrust/system/cuda/detail/reduce.h:  cuda_cub::throw_on_error(status, "after reduction step 2");
deps/thrust/thrust/system/cuda/detail/reduce.h:  status = cuda_cub::synchronize(policy);
deps/thrust/thrust/system/cuda/detail/reduce.h:  cuda_cub::throw_on_error(status, "reduce failed to synchronize");
deps/thrust/thrust/system/cuda/detail/reduce.h:  // aligned for any type of data. `malloc`/`cudaMalloc`/`new`/`std::allocator`
deps/thrust/thrust/system/cuda/detail/reduce.h:  return thrust::cuda_cub::get_value(policy,
deps/thrust/thrust/system/cuda/detail/reduce.h:  if (__THRUST_HAS_CUDART__)
deps/thrust/thrust/system/cuda/detail/reduce.h:    return thrust::cuda_cub::detail::reduce_n_impl(
deps/thrust/thrust/system/cuda/detail/reduce.h:  #if !__THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/reduce.h:  return cuda_cub::reduce_n(policy, first, num_items, init, binary_op);
deps/thrust/thrust/system/cuda/detail/reduce.h:  return cuda_cub::reduce(policy, first, last, init, plus<T>());
deps/thrust/thrust/system/cuda/detail/reduce.h:  return cuda_cub::reduce(policy, first, last, value_type(0));
deps/thrust/thrust/system/cuda/detail/reduce.h:} // namespace cuda_cub
deps/thrust/thrust/system/cuda/detail/binary_search.h:*  Copyright 2021 NVIDIA Corporation
deps/thrust/thrust/system/cuda/detail/copy_if.h: * Copyright (c) 2016, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/thrust/system/cuda/detail/copy_if.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/thrust/system/cuda/detail/copy_if.h: * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/thrust/system/cuda/detail/copy_if.h:#include <thrust/system/cuda/config.h>
deps/thrust/thrust/system/cuda/detail/copy_if.h:#include <thrust/system/cuda/detail/util.h>
deps/thrust/thrust/system/cuda/detail/copy_if.h:#include <thrust/system/cuda/detail/core/agent_launcher.h>
deps/thrust/thrust/system/cuda/detail/copy_if.h:#include <thrust/system/cuda/detail/core/util.h>
deps/thrust/thrust/system/cuda/detail/copy_if.h:#include <thrust/system/cuda/detail/par_to_seq.h>
deps/thrust/thrust/system/cuda/detail/copy_if.h:namespace cuda_cub {
deps/thrust/thrust/system/cuda/detail/copy_if.h:  static cudaError_t THRUST_RUNTIME_FUNCTION
deps/thrust/thrust/system/cuda/detail/copy_if.h:            cudaStream_t     stream,
deps/thrust/thrust/system/cuda/detail/copy_if.h:      return cudaSuccess;
deps/thrust/thrust/system/cuda/detail/copy_if.h:    cudaError_t status = cudaSuccess;
deps/thrust/thrust/system/cuda/detail/copy_if.h:    CUDA_CUB_RET_IF_FAIL(status);
deps/thrust/thrust/system/cuda/detail/copy_if.h:    CUDA_CUB_RET_IF_FAIL(status);
deps/thrust/thrust/system/cuda/detail/copy_if.h:    CUDA_CUB_RET_IF_FAIL(status);
deps/thrust/thrust/system/cuda/detail/copy_if.h:    CUDA_CUB_RET_IF_FAIL(cudaPeekAtLastError());
deps/thrust/thrust/system/cuda/detail/copy_if.h:    CUDA_CUB_RET_IF_FAIL(cudaPeekAtLastError());
deps/thrust/thrust/system/cuda/detail/copy_if.h:    cudaStream_t stream             = cuda_cub::stream(policy);
deps/thrust/thrust/system/cuda/detail/copy_if.h:    cudaError_t status;
deps/thrust/thrust/system/cuda/detail/copy_if.h:    cuda_cub::throw_on_error(status, "copy_if failed on 1st step");
deps/thrust/thrust/system/cuda/detail/copy_if.h:    cuda_cub::throw_on_error(status, "copy_if failed on 1st alias_storage");
deps/thrust/thrust/system/cuda/detail/copy_if.h:    cuda_cub::throw_on_error(status, "copy_if failed on 2nd alias_storage");
deps/thrust/thrust/system/cuda/detail/copy_if.h:    cuda_cub::throw_on_error(status, "copy_if failed on 2nd step");
deps/thrust/thrust/system/cuda/detail/copy_if.h:    status = cuda_cub::synchronize(policy);
deps/thrust/thrust/system/cuda/detail/copy_if.h:    cuda_cub::throw_on_error(status, "copy_if failed to synchronize");
deps/thrust/thrust/system/cuda/detail/copy_if.h:  if (__THRUST_HAS_CUDART__)
deps/thrust/thrust/system/cuda/detail/copy_if.h:#if !__THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/copy_if.h:  if (__THRUST_HAS_CUDART__)
deps/thrust/thrust/system/cuda/detail/copy_if.h:#if !__THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/copy_if.h:}    // namespace cuda_cub
deps/thrust/thrust/system/cuda/detail/cross_system.h: * Copyright (c) 2016, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/thrust/system/cuda/detail/cross_system.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/thrust/system/cuda/detail/cross_system.h: * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/thrust/system/cuda/detail/cross_system.h:#include <thrust/system/cuda/detail/guarded_cuda_runtime_api.h>
deps/thrust/thrust/system/cuda/detail/cross_system.h:#include <thrust/system/cuda/detail/execution_policy.h>
deps/thrust/thrust/system/cuda/detail/cross_system.h:namespace cuda_cub {
deps/thrust/thrust/system/cuda/detail/cross_system.h:    thrust::system::cuda::execution_policy<Sys1> const&
deps/thrust/thrust/system/cuda/detail/cross_system.h:      cudaMemcpyKind, cudaMemcpyDeviceToHost
deps/thrust/thrust/system/cuda/detail/cross_system.h:  , thrust::system::cuda::execution_policy<Sys2> const&
deps/thrust/thrust/system/cuda/detail/cross_system.h:      cudaMemcpyKind, cudaMemcpyHostToDevice
deps/thrust/thrust/system/cuda/detail/cross_system.h:    thrust::system::cuda::execution_policy<Sys1> const&
deps/thrust/thrust/system/cuda/detail/cross_system.h:  , thrust::system::cuda::execution_policy<Sys2> const&
deps/thrust/thrust/system/cuda/detail/cross_system.h:      cudaMemcpyKind, cudaMemcpyDeviceToDevice
deps/thrust/thrust/system/cuda/detail/cross_system.h:      cudaMemcpyKind, cudaMemcpyDeviceToDevice
deps/thrust/thrust/system/cuda/detail/cross_system.h:        bool, cudaMemcpyDeviceToHost == Direction::value
deps/thrust/thrust/system/cuda/detail/cross_system.h:        bool, cudaMemcpyDeviceToHost == Direction::value
deps/thrust/thrust/system/cuda/detail/cross_system.h:        bool, cudaMemcpyHostToDevice == Direction::value
deps/thrust/thrust/system/cuda/detail/cross_system.h:        bool, cudaMemcpyHostToDevice == Direction::value
deps/thrust/thrust/system/cuda/detail/cross_system.h:        bool, cudaMemcpyDeviceToDevice == Direction::value
deps/thrust/thrust/system/cuda/detail/cross_system.h:        bool, cudaMemcpyDeviceToDevice == Direction::value
deps/thrust/thrust/system/cuda/detail/cross_system.h:  select_device_system(thrust::cuda::execution_policy<Sys1> &sys1,
deps/thrust/thrust/system/cuda/detail/cross_system.h:  select_device_system(thrust::cuda::execution_policy<Sys1> const &sys1,
deps/thrust/thrust/system/cuda/detail/cross_system.h:                       thrust::cuda::execution_policy<Sys2> &sys2)
deps/thrust/thrust/system/cuda/detail/cross_system.h:                       thrust::cuda::execution_policy<Sys2> const &sys2)
deps/thrust/thrust/system/cuda/detail/cross_system.h:  select_device_system(thrust::cuda::execution_policy<Sys1> &sys1,
deps/thrust/thrust/system/cuda/detail/cross_system.h:                       thrust::cuda::execution_policy<Sys2> &)
deps/thrust/thrust/system/cuda/detail/cross_system.h:  select_device_system(thrust::cuda::execution_policy<Sys1> const &sys1,
deps/thrust/thrust/system/cuda/detail/cross_system.h:                       thrust::cuda::execution_policy<Sys2> const &)
deps/thrust/thrust/system/cuda/detail/cross_system.h:  select_host_system(thrust::cuda::execution_policy<Sys1> &,
deps/thrust/thrust/system/cuda/detail/cross_system.h:  select_host_system(thrust::cuda::execution_policy<Sys1> const &,
deps/thrust/thrust/system/cuda/detail/cross_system.h:                     thrust::cuda::execution_policy<Sys2> &)
deps/thrust/thrust/system/cuda/detail/cross_system.h:                     thrust::cuda::execution_policy<Sys2> const &)
deps/thrust/thrust/system/cuda/detail/cross_system.h:} // namespace cuda_cub
deps/thrust/thrust/system/cuda/detail/remove.h: * Copyright (c) 2016, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/thrust/system/cuda/detail/remove.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/thrust/system/cuda/detail/remove.h: * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/thrust/system/cuda/detail/remove.h:#include <thrust/system/cuda/detail/copy_if.h>
deps/thrust/thrust/system/cuda/detail/remove.h:namespace cuda_cub {
deps/thrust/thrust/system/cuda/detail/remove.h:  return cuda_cub::copy_if(policy, first, last, stencil, first,
deps/thrust/thrust/system/cuda/detail/remove.h:  return cuda_cub::copy_if(policy, first, last, first,
deps/thrust/thrust/system/cuda/detail/remove.h:  return cuda_cub::remove_if(policy, first, last, _1 == value);
deps/thrust/thrust/system/cuda/detail/remove.h:  return cuda_cub::copy_if(policy, first, last, stencil, result,
deps/thrust/thrust/system/cuda/detail/remove.h:  return cuda_cub::copy_if(policy, first, last, result,
deps/thrust/thrust/system/cuda/detail/remove.h:  return cuda_cub::remove_copy_if(policy, first, last, result, pred);
deps/thrust/thrust/system/cuda/detail/remove.h:}    // namespace cuda_cub
deps/thrust/thrust/system/cuda/detail/temporary_buffer.h: *  Copyright 2008-2016 NVIDIA Corporation
deps/thrust/thrust/system/cuda/detail/malloc_and_free.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/cuda/detail/malloc_and_free.h:#include <thrust/system/cuda/detail/guarded_cuda_runtime_api.h>
deps/thrust/thrust/system/cuda/detail/malloc_and_free.h:#include <thrust/system/cuda/config.h>
deps/thrust/thrust/system/cuda/detail/malloc_and_free.h:#include <thrust/system/cuda/detail/util.h>
deps/thrust/thrust/system/cuda/detail/malloc_and_free.h:namespace cuda_cub {
deps/thrust/thrust/system/cuda/detail/malloc_and_free.h:#ifndef __CUDA_ARCH__
deps/thrust/thrust/system/cuda/detail/malloc_and_free.h:// depending on the heavyweight thrust/system/cuda/memory.h header
deps/thrust/thrust/system/cuda/detail/malloc_and_free.h:        cudaError_t status = alloc.DeviceAllocate(&result, n);
deps/thrust/thrust/system/cuda/detail/malloc_and_free.h:        cudaError_t status = cudaMalloc(&result, n);
deps/thrust/thrust/system/cuda/detail/malloc_and_free.h:      if(status != cudaSuccess)
deps/thrust/thrust/system/cuda/detail/malloc_and_free.h:        cudaGetLastError(); // Clear global CUDA error state.
deps/thrust/thrust/system/cuda/detail/malloc_and_free.h:        throw thrust::system::detail::bad_alloc(thrust::cuda_category().message(status).c_str());
deps/thrust/thrust/system/cuda/detail/malloc_and_free.h:        cudaError_t status = alloc.DeviceFree(thrust::raw_pointer_cast(ptr));
deps/thrust/thrust/system/cuda/detail/malloc_and_free.h:        cudaError_t status = cudaFree(thrust::raw_pointer_cast(ptr));
deps/thrust/thrust/system/cuda/detail/malloc_and_free.h:      cuda_cub::throw_on_error(status, "device free failed");
deps/thrust/thrust/system/cuda/detail/malloc_and_free.h:}    // namespace cuda_cub
deps/thrust/thrust/system/cuda/detail/replace.h: * Copyright (c) 2016, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/thrust/system/cuda/detail/replace.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/thrust/system/cuda/detail/replace.h: * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/thrust/system/cuda/detail/replace.h:#include <thrust/system/cuda/detail/transform.h>
deps/thrust/thrust/system/cuda/detail/replace.h:namespace cuda_cub {
deps/thrust/thrust/system/cuda/detail/replace.h:  cuda_cub::transform_if(policy,
deps/thrust/thrust/system/cuda/detail/replace.h:  cuda_cub::transform_if(policy,
deps/thrust/thrust/system/cuda/detail/replace.h:  cuda_cub::transform_if(policy,
deps/thrust/thrust/system/cuda/detail/replace.h:  return cuda_cub::transform(policy,
deps/thrust/thrust/system/cuda/detail/replace.h:  return cuda_cub::transform(policy,
deps/thrust/thrust/system/cuda/detail/replace.h:  return cuda_cub::replace_copy_if(policy,
deps/thrust/thrust/system/cuda/detail/replace.h:}    // namespace cuda_cub
deps/thrust/thrust/system/cuda/detail/find.h: * Copyright (c) 2016, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/thrust/system/cuda/detail/find.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/thrust/system/cuda/detail/find.h: * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/thrust/system/cuda/detail/find.h:#include <thrust/system/cuda/config.h>
deps/thrust/thrust/system/cuda/detail/find.h:#include <thrust/system/cuda/detail/execution_policy.h>
deps/thrust/thrust/system/cuda/detail/find.h:namespace cuda_cub {
deps/thrust/thrust/system/cuda/detail/find.h:}; // namespace cuda_cub
deps/thrust/thrust/system/cuda/detail/find.h:#include <thrust/system/cuda/detail/reduce.h>
deps/thrust/thrust/system/cuda/detail/find.h:namespace cuda_cub {
deps/thrust/thrust/system/cuda/detail/find.h:  return cuda_cub::find_if_n(policy, first, thrust::distance(first,last), predicate);
deps/thrust/thrust/system/cuda/detail/find.h:  return cuda_cub::find_if(policy, first, last, thrust::detail::not1(predicate));
deps/thrust/thrust/system/cuda/detail/find.h:  return cuda_cub::find_if(policy,
deps/thrust/thrust/system/cuda/detail/find.h:} // namespace cuda_cub
deps/thrust/thrust/system/cuda/detail/scan_by_key.h: * Copyright (c) 2016, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/thrust/system/cuda/detail/scan_by_key.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/thrust/system/cuda/detail/scan_by_key.h: * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/thrust/system/cuda/detail/scan_by_key.h:#include <thrust/system/cuda/detail/util.h>
deps/thrust/thrust/system/cuda/detail/scan_by_key.h:#include <thrust/system/cuda/execution_policy.h>
deps/thrust/thrust/system/cuda/detail/scan_by_key.h:#include <thrust/system/cuda/detail/par_to_seq.h>
deps/thrust/thrust/system/cuda/detail/scan_by_key.h:#include <thrust/system/cuda/detail/core/agent_launcher.h>
deps/thrust/thrust/system/cuda/detail/scan_by_key.h:namespace cuda_cub {
deps/thrust/thrust/system/cuda/detail/scan_by_key.h:                thrust::detail::false_type /* is_incclusive */)
deps/thrust/thrust/system/cuda/detail/scan_by_key.h:  THRUST_RUNTIME_FUNCTION cudaError_t
deps/thrust/thrust/system/cuda/detail/scan_by_key.h:            cudaStream_t   stream,
deps/thrust/thrust/system/cuda/detail/scan_by_key.h:    cudaError_t status = cudaSuccess;
deps/thrust/thrust/system/cuda/detail/scan_by_key.h:      return cudaErrorNotSupported;
deps/thrust/thrust/system/cuda/detail/scan_by_key.h:    CUDA_CUB_RET_IF_FAIL(status);
deps/thrust/thrust/system/cuda/detail/scan_by_key.h:    CUDA_CUB_RET_IF_FAIL(status);
deps/thrust/thrust/system/cuda/detail/scan_by_key.h:    CUDA_CUB_RET_IF_FAIL(status);
deps/thrust/thrust/system/cuda/detail/scan_by_key.h:    CUDA_CUB_RET_IF_FAIL(cudaPeekAtLastError());
deps/thrust/thrust/system/cuda/detail/scan_by_key.h:    CUDA_CUB_RET_IF_FAIL(cudaPeekAtLastError());
deps/thrust/thrust/system/cuda/detail/scan_by_key.h:    cudaStream_t stream       = cuda_cub::stream(policy);
deps/thrust/thrust/system/cuda/detail/scan_by_key.h:    cudaError_t status;
deps/thrust/thrust/system/cuda/detail/scan_by_key.h:    cuda_cub::throw_on_error(status, "scan_by_key: failed on 1st step");
deps/thrust/thrust/system/cuda/detail/scan_by_key.h:    cuda_cub::throw_on_error(status, "scan_by_key: failed on 2nd step");
deps/thrust/thrust/system/cuda/detail/scan_by_key.h:    status = cuda_cub::synchronize_optional(policy);
deps/thrust/thrust/system/cuda/detail/scan_by_key.h:    cuda_cub::throw_on_error(status, "scan_by_key: failed to synchronize");
deps/thrust/thrust/system/cuda/detail/scan_by_key.h:  if (__THRUST_HAS_CUDART__)
deps/thrust/thrust/system/cuda/detail/scan_by_key.h:#if !__THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/scan_by_key.h:  return cuda_cub::inclusive_scan_by_key(policy,
deps/thrust/thrust/system/cuda/detail/scan_by_key.h:  return cuda_cub::inclusive_scan_by_key(policy,
deps/thrust/thrust/system/cuda/detail/scan_by_key.h:  if (__THRUST_HAS_CUDART__)
deps/thrust/thrust/system/cuda/detail/scan_by_key.h:#if !__THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/scan_by_key.h:  return cuda_cub::exclusive_scan_by_key(policy,
deps/thrust/thrust/system/cuda/detail/scan_by_key.h:  return cuda_cub::exclusive_scan_by_key(policy,
deps/thrust/thrust/system/cuda/detail/scan_by_key.h:  return cuda_cub::exclusive_scan_by_key(policy,
deps/thrust/thrust/system/cuda/detail/scan_by_key.h:}    // namespace cuda_cub
deps/thrust/thrust/system/cuda/detail/iter_swap.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/cuda/detail/iter_swap.h:#include <thrust/system/cuda/config.h>
deps/thrust/thrust/system/cuda/detail/iter_swap.h:#include <thrust/system/cuda/detail/execution_policy.h>
deps/thrust/thrust/system/cuda/detail/iter_swap.h:namespace cuda_cub {
deps/thrust/thrust/system/cuda/detail/iter_swap.h:void iter_swap(thrust::cuda::execution_policy<DerivedPolicy> &, Pointer1 a, Pointer2 b)
deps/thrust/thrust/system/cuda/detail/iter_swap.h:} // end cuda_cub
deps/thrust/thrust/system/cuda/detail/guarded_driver_types.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/cuda/detail/for_each.h: * Copyright (c) 2016, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/thrust/system/cuda/detail/for_each.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/thrust/system/cuda/detail/for_each.h: * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/thrust/system/cuda/detail/for_each.h:#include <thrust/system/cuda/config.h>
deps/thrust/thrust/system/cuda/detail/for_each.h:#include <thrust/system/cuda/detail/util.h>
deps/thrust/thrust/system/cuda/detail/for_each.h:#include <thrust/system/cuda/detail/parallel_for.h>
deps/thrust/thrust/system/cuda/detail/for_each.h:namespace cuda_cub {
deps/thrust/thrust/system/cuda/detail/for_each.h:    cuda_cub::parallel_for(policy,
deps/thrust/thrust/system/cuda/detail/for_each.h:    cuda_cub::throw_on_error(
deps/thrust/thrust/system/cuda/detail/for_each.h:      cuda_cub::synchronize_optional(policy)
deps/thrust/thrust/system/cuda/detail/for_each.h:    return cuda_cub::for_each_n(policy, first,  count, op);
deps/thrust/thrust/system/cuda/detail/for_each.h:}    // namespace cuda_cub
deps/thrust/thrust/system/cuda/detail/unique_by_key.h: * Copyright (c) 2016, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/thrust/system/cuda/detail/unique_by_key.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/thrust/system/cuda/detail/unique_by_key.h: * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/thrust/system/cuda/detail/unique_by_key.h:#include <thrust/system/cuda/config.h>
deps/thrust/thrust/system/cuda/detail/unique_by_key.h:#include <thrust/system/cuda/detail/util.h>
deps/thrust/thrust/system/cuda/detail/unique_by_key.h:#include <thrust/system/cuda/detail/core/agent_launcher.h>
deps/thrust/thrust/system/cuda/detail/unique_by_key.h:#include <thrust/system/cuda/detail/get_value.h>
deps/thrust/thrust/system/cuda/detail/unique_by_key.h:#include <thrust/system/cuda/detail/par_to_seq.h>
deps/thrust/thrust/system/cuda/detail/unique_by_key.h:namespace cuda_cub {
deps/thrust/thrust/system/cuda/detail/unique_by_key.h:  static cudaError_t THRUST_RUNTIME_FUNCTION
deps/thrust/thrust/system/cuda/detail/unique_by_key.h:            cudaStream_t     stream,
deps/thrust/thrust/system/cuda/detail/unique_by_key.h:    cudaError_t status = cudaSuccess;
deps/thrust/thrust/system/cuda/detail/unique_by_key.h:    CUDA_CUB_RET_IF_FAIL(status);
deps/thrust/thrust/system/cuda/detail/unique_by_key.h:    CUDA_CUB_RET_IF_FAIL(status);
deps/thrust/thrust/system/cuda/detail/unique_by_key.h:    CUDA_CUB_RET_IF_FAIL(status);
deps/thrust/thrust/system/cuda/detail/unique_by_key.h:    CUDA_CUB_RET_IF_FAIL(cudaPeekAtLastError());
deps/thrust/thrust/system/cuda/detail/unique_by_key.h:    CUDA_CUB_RET_IF_FAIL(cudaPeekAtLastError());
deps/thrust/thrust/system/cuda/detail/unique_by_key.h:    cudaStream_t stream             = cuda_cub::stream(policy);
deps/thrust/thrust/system/cuda/detail/unique_by_key.h:    cudaError_t status;
deps/thrust/thrust/system/cuda/detail/unique_by_key.h:    cuda_cub::throw_on_error(status, "unique_by_key: failed on 1st step");
deps/thrust/thrust/system/cuda/detail/unique_by_key.h:    cuda_cub::throw_on_error(status, "unique_by_key failed on 1st alias_storage");
deps/thrust/thrust/system/cuda/detail/unique_by_key.h:    cuda_cub::throw_on_error(status, "unique_by_key failed on 2nd alias_storage");
deps/thrust/thrust/system/cuda/detail/unique_by_key.h:    cuda_cub::throw_on_error(status, "unique_by_key: failed on 2nd step");
deps/thrust/thrust/system/cuda/detail/unique_by_key.h:    status = cuda_cub::synchronize(policy);
deps/thrust/thrust/system/cuda/detail/unique_by_key.h:    cuda_cub::throw_on_error(status, "unique_by_key: failed to synchronize");
deps/thrust/thrust/system/cuda/detail/unique_by_key.h:  if (__THRUST_HAS_CUDART__)
deps/thrust/thrust/system/cuda/detail/unique_by_key.h:#if !__THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/unique_by_key.h:  return cuda_cub::unique_by_key_copy(policy,
deps/thrust/thrust/system/cuda/detail/unique_by_key.h:  if (__THRUST_HAS_CUDART__)
deps/thrust/thrust/system/cuda/detail/unique_by_key.h:    ret = cuda_cub::unique_by_key_copy(policy,
deps/thrust/thrust/system/cuda/detail/unique_by_key.h:#if !__THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/unique_by_key.h:  return cuda_cub::unique_by_key(policy,
deps/thrust/thrust/system/cuda/detail/unique_by_key.h:}    // namespace cuda_cub
deps/thrust/thrust/system/cuda/detail/get_value.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/cuda/detail/get_value.h:#include <thrust/system/cuda/config.h>
deps/thrust/thrust/system/cuda/detail/get_value.h:#include <thrust/system/cuda/detail/cross_system.h>
deps/thrust/thrust/system/cuda/detail/get_value.h:namespace cuda_cub {
deps/thrust/thrust/system/cuda/detail/get_value.h:  #ifdef _NVHPC_CUDA
deps/thrust/thrust/system/cuda/detail/get_value.h:    #ifndef __CUDA_ARCH__
deps/thrust/thrust/system/cuda/detail/get_value.h:    #endif // __CUDA_ARCH__
deps/thrust/thrust/system/cuda/detail/get_value.h:} // end cuda_cub
deps/thrust/thrust/system/cuda/detail/terminate.h: * Copyright (c) 2016, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/thrust/system/cuda/detail/terminate.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/thrust/system/cuda/detail/terminate.h: * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/thrust/system/cuda/detail/terminate.h:#include <thrust/system/cuda/detail/util.h>
deps/thrust/thrust/system/cuda/detail/terminate.h:namespace cuda
deps/thrust/thrust/system/cuda/detail/terminate.h:  thrust::cuda_cub::terminate();
deps/thrust/thrust/system/cuda/detail/terminate.h:  thrust::cuda_cub::terminate();
deps/thrust/thrust/system/cuda/detail/terminate.h:} // end cuda
deps/thrust/thrust/system/cuda/detail/transform_scan.h: * Copyright (c) 2016, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/thrust/system/cuda/detail/transform_scan.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/thrust/system/cuda/detail/transform_scan.h: * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/thrust/system/cuda/detail/transform_scan.h:#include <thrust/system/cuda/detail/scan.h>
deps/thrust/thrust/system/cuda/detail/transform_scan.h:namespace cuda_cub {
deps/thrust/thrust/system/cuda/detail/transform_scan.h:  return cuda_cub::inclusive_scan_n(policy,
deps/thrust/thrust/system/cuda/detail/transform_scan.h:  return cuda_cub::exclusive_scan_n(policy,
deps/thrust/thrust/system/cuda/detail/transform_scan.h:}    // namespace cuda_cub
deps/thrust/thrust/system/cuda/detail/transform_reduce.h: * Copyright (c) 2016, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/thrust/system/cuda/detail/transform_reduce.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/thrust/system/cuda/detail/transform_reduce.h: * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/thrust/system/cuda/detail/transform_reduce.h:#include <thrust/system/cuda/detail/reduce.h>
deps/thrust/thrust/system/cuda/detail/transform_reduce.h:namespace cuda_cub {
deps/thrust/thrust/system/cuda/detail/transform_reduce.h:  return cuda_cub::reduce_n(policy,
deps/thrust/thrust/system/cuda/detail/transform_reduce.h:}    // namespace cuda_cub
deps/thrust/thrust/system/cuda/detail/par.h: * Copyright (c) 2016-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/thrust/system/cuda/detail/par.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/thrust/system/cuda/detail/par.h: * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/thrust/system/cuda/detail/par.h:#include <thrust/system/cuda/detail/guarded_cuda_runtime_api.h>
deps/thrust/thrust/system/cuda/detail/par.h:#include <thrust/system/cuda/detail/execution_policy.h>
deps/thrust/thrust/system/cuda/detail/par.h:#include <thrust/system/cuda/detail/util.h>
deps/thrust/thrust/system/cuda/detail/par.h:namespace cuda_cub {
deps/thrust/thrust/system/cuda/detail/par.h:  cudaStream_t stream;
deps/thrust/thrust/system/cuda/detail/par.h:  execute_on_stream_base(cudaStream_t stream_ = default_stream())
deps/thrust/thrust/system/cuda/detail/par.h:  on(cudaStream_t const &s) const
deps/thrust/thrust/system/cuda/detail/par.h:  cudaStream_t
deps/thrust/thrust/system/cuda/detail/par.h:  cudaStream_t stream;
deps/thrust/thrust/system/cuda/detail/par.h:  execute_on_stream_nosync_base(cudaStream_t stream_ = default_stream())
deps/thrust/thrust/system/cuda/detail/par.h:  on(cudaStream_t const &s) const
deps/thrust/thrust/system/cuda/detail/par.h:  cudaStream_t
deps/thrust/thrust/system/cuda/detail/par.h:  execute_on_stream(cudaStream_t stream) 
deps/thrust/thrust/system/cuda/detail/par.h:  execute_on_stream_nosync(cudaStream_t stream) 
deps/thrust/thrust/system/cuda/detail/par.h:  on(cudaStream_t const &stream) const
deps/thrust/thrust/system/cuda/detail/par.h:  on(cudaStream_t const &stream) const
deps/thrust/thrust/system/cuda/detail/par.h:  //this function is defined to allow non-blocking calls on the default_stream() with thrust::cuda::par_nosync
deps/thrust/thrust/system/cuda/detail/par.h:  //without explicitly using thrust::cuda::par_nosync.on(default_stream())
deps/thrust/thrust/system/cuda/detail/par.h:/*! \p thrust::cuda::par_nosync is a parallel execution policy targeting Thrust's CUDA device backend.
deps/thrust/thrust/system/cuda/detail/par.h: *  Similar to \p thrust::cuda::par it allows execution of Thrust algorithms in a specific CUDA stream.
deps/thrust/thrust/system/cuda/detail/par.h: *  \p thrust::cuda::par_nosync indicates that an algorithm is free to avoid any synchronization of the 
deps/thrust/thrust/system/cuda/detail/par.h: *  The following code snippet demonstrates how to use \p thrust::cuda::par_nosync :
deps/thrust/thrust/system/cuda/detail/par.h: *        cudaStream_t stream;
deps/thrust/thrust/system/cuda/detail/par.h: *        cudaStreamCreate(&stream);
deps/thrust/thrust/system/cuda/detail/par.h: *        auto nosync_policy = thrust::cuda::par_nosync.on(stream);
deps/thrust/thrust/system/cuda/detail/par.h: *        cudaStreamSynchronize(stream);
deps/thrust/thrust/system/cuda/detail/par.h: *        cudaStreamSynchronize(stream);
deps/thrust/thrust/system/cuda/detail/par.h: *        cudaStreamDestroy(stream);
deps/thrust/thrust/system/cuda/detail/par.h:}    // namespace cuda_
deps/thrust/thrust/system/cuda/detail/par.h:namespace cuda {
deps/thrust/thrust/system/cuda/detail/par.h:  using thrust::cuda_cub::par;
deps/thrust/thrust/system/cuda/detail/par.h:  using thrust::cuda_cub::par_nosync;
deps/thrust/thrust/system/cuda/detail/par.h:    using thrust::cuda_cub::par_t;
deps/thrust/thrust/system/cuda/detail/par.h:    using thrust::cuda_cub::par_nosync_t;
deps/thrust/thrust/system/cuda/detail/par.h:} // namesapce cuda
deps/thrust/thrust/system/cuda/detail/par.h:namespace cuda {
deps/thrust/thrust/system/cuda/detail/par.h:using thrust::cuda_cub::par;
deps/thrust/thrust/system/cuda/detail/par.h:using thrust::cuda_cub::par_nosync;
deps/thrust/thrust/system/cuda/detail/par.h:} // namespace cuda
deps/thrust/thrust/system/cuda/detail/par_to_seq.h: * Copyright (c) 2016, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/thrust/system/cuda/detail/par_to_seq.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/thrust/system/cuda/detail/par_to_seq.h: * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/thrust/system/cuda/detail/par_to_seq.h:#include <thrust/system/cuda/detail/par.h>
deps/thrust/thrust/system/cuda/detail/par_to_seq.h:namespace cuda_cub {
deps/thrust/thrust/system/cuda/detail/par_to_seq.h:#if __THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/par_to_seq.h:#define THRUST_CUDART_DISPATCH par
deps/thrust/thrust/system/cuda/detail/par_to_seq.h:#define THRUST_CUDART_DISPATCH seq
deps/thrust/thrust/system/cuda/detail/par_to_seq.h:} // namespace cuda_
deps/thrust/thrust/system/cuda/detail/extrema.h: * Copyright (c) 2016, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/thrust/system/cuda/detail/extrema.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/thrust/system/cuda/detail/extrema.h: * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/thrust/system/cuda/detail/extrema.h:#include <thrust/system/cuda/config.h>
deps/thrust/thrust/system/cuda/detail/extrema.h:#include <thrust/system/cuda/detail/reduce.h>
deps/thrust/thrust/system/cuda/detail/extrema.h:namespace cuda_cub {
deps/thrust/thrust/system/cuda/detail/extrema.h:  cudaError_t THRUST_RUNTIME_FUNCTION
deps/thrust/thrust/system/cuda/detail/extrema.h:            cudaStream_t stream,
deps/thrust/thrust/system/cuda/detail/extrema.h:    using core::cuda_optional;
deps/thrust/thrust/system/cuda/detail/extrema.h:      return cudaErrorNotSupported;
deps/thrust/thrust/system/cuda/detail/extrema.h:    cudaError_t status = cudaSuccess;
deps/thrust/thrust/system/cuda/detail/extrema.h:      CUDA_CUB_RET_IF_FAIL(cudaPeekAtLastError());
deps/thrust/thrust/system/cuda/detail/extrema.h:      cuda_optional<int> sm_count = core::get_sm_count();
deps/thrust/thrust/system/cuda/detail/extrema.h:      CUDA_CUB_RET_IF_FAIL(sm_count.status());
deps/thrust/thrust/system/cuda/detail/extrema.h:      cuda_optional<int> max_blocks_per_sm =
deps/thrust/thrust/system/cuda/detail/extrema.h:      CUDA_CUB_RET_IF_FAIL(max_blocks_per_sm.status());
deps/thrust/thrust/system/cuda/detail/extrema.h:      CUDA_CUB_RET_IF_FAIL(status);
deps/thrust/thrust/system/cuda/detail/extrema.h:        CUDA_CUB_RET_IF_FAIL(cudaPeekAtLastError());
deps/thrust/thrust/system/cuda/detail/extrema.h:        CUDA_CUB_RET_IF_FAIL(cudaErrorNotSupported);
deps/thrust/thrust/system/cuda/detail/extrema.h:      CUDA_CUB_RET_IF_FAIL(cudaPeekAtLastError());
deps/thrust/thrust/system/cuda/detail/extrema.h:      CUDA_CUB_RET_IF_FAIL(cudaPeekAtLastError());
deps/thrust/thrust/system/cuda/detail/extrema.h:    cudaStream_t stream             = cuda_cub::stream(policy);
deps/thrust/thrust/system/cuda/detail/extrema.h:    cudaError_t status;
deps/thrust/thrust/system/cuda/detail/extrema.h:    cuda_cub::throw_on_error(status, "extrema failed on 1st step");
deps/thrust/thrust/system/cuda/detail/extrema.h:    cuda_cub::throw_on_error(status, "extrema failed on 1st alias storage");
deps/thrust/thrust/system/cuda/detail/extrema.h:    cuda_cub::throw_on_error(status, "extrema failed on 2nd alias storage");
deps/thrust/thrust/system/cuda/detail/extrema.h:    cuda_cub::throw_on_error(status, "extrema failed on 2nd step");
deps/thrust/thrust/system/cuda/detail/extrema.h:    status = cuda_cub::synchronize(policy);
deps/thrust/thrust/system/cuda/detail/extrema.h:    cuda_cub::throw_on_error(status, "extrema failed to synchronize");
deps/thrust/thrust/system/cuda/detail/extrema.h:    T result = cuda_cub::get_value(policy, d_result);
deps/thrust/thrust/system/cuda/detail/extrema.h:  if (__THRUST_HAS_CUDART__)
deps/thrust/thrust/system/cuda/detail/extrema.h:#if !__THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/extrema.h:  return cuda_cub::min_element(policy, first, last, less<value_type>());
deps/thrust/thrust/system/cuda/detail/extrema.h:  if (__THRUST_HAS_CUDART__)
deps/thrust/thrust/system/cuda/detail/extrema.h:#if !__THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/extrema.h:  return cuda_cub::max_element(policy, first, last, less<value_type>());
deps/thrust/thrust/system/cuda/detail/extrema.h:  if (__THRUST_HAS_CUDART__)
deps/thrust/thrust/system/cuda/detail/extrema.h:#if !__THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/extrema.h:  return cuda_cub::minmax_element(policy, first, last, less<value_type>());
deps/thrust/thrust/system/cuda/detail/extrema.h:} // namespace cuda_cub
deps/thrust/thrust/system/cuda/detail/async/customization.h: * Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/thrust/system/cuda/detail/async/customization.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/thrust/system/cuda/detail/async/customization.h: * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/thrust/system/cuda/detail/async/customization.h:// TODO: Move into system::cuda
deps/thrust/thrust/system/cuda/detail/async/customization.h:#include <thrust/system/cuda/config.h>
deps/thrust/thrust/system/cuda/detail/async/customization.h:#include <thrust/system/cuda/memory_resource.h>
deps/thrust/thrust/system/cuda/detail/async/customization.h:namespace system { namespace cuda { namespace detail
deps/thrust/thrust/system/cuda/detail/async/customization.h:    thrust::system::cuda::memory_resource
deps/thrust/thrust/system/cuda/detail/async/customization.h:    thrust::system::cuda::universal_host_pinned_memory_resource
deps/thrust/thrust/system/cuda/detail/async/customization.h:}}} // namespace system::cuda::detail
deps/thrust/thrust/system/cuda/detail/async/inclusive_scan.h: * Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/thrust/system/cuda/detail/async/inclusive_scan.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/thrust/system/cuda/detail/async/inclusive_scan.h: * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/thrust/system/cuda/detail/async/inclusive_scan.h:#include <thrust/system/cuda/config.h>
deps/thrust/thrust/system/cuda/detail/async/inclusive_scan.h:#include <thrust/system/cuda/detail/async/customization.h>
deps/thrust/thrust/system/cuda/detail/async/inclusive_scan.h:#include <thrust/system/cuda/detail/util.h>
deps/thrust/thrust/system/cuda/detail/async/inclusive_scan.h:#include <thrust/system/cuda/future.h>
deps/thrust/thrust/system/cuda/detail/async/inclusive_scan.h:namespace cuda
deps/thrust/thrust/system/cuda/detail/async/inclusive_scan.h:  cudaError_t status;
deps/thrust/thrust/system/cuda/detail/async/inclusive_scan.h:    thrust::cuda_cub::throw_on_error(status,
deps/thrust/thrust/system/cuda/detail/async/inclusive_scan.h:  cudaStream_t const user_raw_stream = thrust::cuda_cub::stream(policy);
deps/thrust/thrust/system/cuda/detail/async/inclusive_scan.h:  if (thrust::cuda_cub::default_stream() != user_raw_stream)
deps/thrust/thrust/system/cuda/detail/async/inclusive_scan.h:    thrust::cuda_cub::throw_on_error(status,
deps/thrust/thrust/system/cuda/detail/async/inclusive_scan.h:}}} // namespace system::cuda::detail
deps/thrust/thrust/system/cuda/detail/async/inclusive_scan.h:namespace cuda_cub
deps/thrust/thrust/system/cuda/detail/async/inclusive_scan.h:  thrust::system::cuda::detail::async_inclusive_scan_n(
deps/thrust/thrust/system/cuda/detail/async/inclusive_scan.h:} // namespace cuda_cub
deps/thrust/thrust/system/cuda/detail/async/transform.h: * Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/thrust/system/cuda/detail/async/transform.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/thrust/system/cuda/detail/async/transform.h: * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/thrust/system/cuda/detail/async/transform.h:// TODO: Move into system::cuda
deps/thrust/thrust/system/cuda/detail/async/transform.h:#include <thrust/system/cuda/config.h>
deps/thrust/thrust/system/cuda/detail/async/transform.h:#include <thrust/system/cuda/detail/async/customization.h>
deps/thrust/thrust/system/cuda/detail/async/transform.h:#include <thrust/system/cuda/detail/parallel_for.h>
deps/thrust/thrust/system/cuda/detail/async/transform.h:#include <thrust/system/cuda/future.h>
deps/thrust/thrust/system/cuda/detail/async/transform.h:namespace system { namespace cuda { namespace detail
deps/thrust/thrust/system/cuda/detail/async/transform.h:  cudaStream_t const user_raw_stream = thrust::cuda_cub::stream(policy);
deps/thrust/thrust/system/cuda/detail/async/transform.h:  if (thrust::cuda_cub::default_stream() != user_raw_stream)
deps/thrust/thrust/system/cuda/detail/async/transform.h:  thrust::cuda_cub::throw_on_error(
deps/thrust/thrust/system/cuda/detail/async/transform.h:    thrust::cuda_cub::__parallel_for::parallel_for(
deps/thrust/thrust/system/cuda/detail/async/transform.h:}}} // namespace system::cuda::detail
deps/thrust/thrust/system/cuda/detail/async/transform.h:namespace cuda_cub
deps/thrust/thrust/system/cuda/detail/async/transform.h:  thrust::system::cuda::detail::async_transform_n(
deps/thrust/thrust/system/cuda/detail/async/transform.h:} // cuda_cub
deps/thrust/thrust/system/cuda/detail/async/reduce.h: * Copyright (c) 2016, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/thrust/system/cuda/detail/async/reduce.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/thrust/system/cuda/detail/async/reduce.h: * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/thrust/system/cuda/detail/async/reduce.h:// TODO: Move into system::cuda
deps/thrust/thrust/system/cuda/detail/async/reduce.h:#include <thrust/system/cuda/config.h>
deps/thrust/thrust/system/cuda/detail/async/reduce.h:#include <thrust/system/cuda/detail/async/customization.h>
deps/thrust/thrust/system/cuda/detail/async/reduce.h:#include <thrust/system/cuda/detail/reduce.h>
deps/thrust/thrust/system/cuda/detail/async/reduce.h:#include <thrust/system/cuda/future.h>
deps/thrust/thrust/system/cuda/detail/async/reduce.h:namespace system { namespace cuda { namespace detail
deps/thrust/thrust/system/cuda/detail/async/reduce.h:  thrust::cuda_cub::throw_on_error(
deps/thrust/thrust/system/cuda/detail/async/reduce.h:  // aligned for any type of data. `malloc`/`cudaMalloc`/`new`/`std::allocator`
deps/thrust/thrust/system/cuda/detail/async/reduce.h:  cudaStream_t const user_raw_stream = thrust::cuda_cub::stream(policy);
deps/thrust/thrust/system/cuda/detail/async/reduce.h:  if (thrust::cuda_cub::default_stream() != user_raw_stream)
deps/thrust/thrust/system/cuda/detail/async/reduce.h:  thrust::cuda_cub::throw_on_error(
deps/thrust/thrust/system/cuda/detail/async/reduce.h:}}} // namespace system::cuda::detail
deps/thrust/thrust/system/cuda/detail/async/reduce.h:namespace cuda_cub
deps/thrust/thrust/system/cuda/detail/async/reduce.h:  thrust::system::cuda::detail::async_reduce_n(
deps/thrust/thrust/system/cuda/detail/async/reduce.h:} // cuda_cub
deps/thrust/thrust/system/cuda/detail/async/reduce.h:namespace system { namespace cuda { namespace detail
deps/thrust/thrust/system/cuda/detail/async/reduce.h:  thrust::cuda_cub::throw_on_error(
deps/thrust/thrust/system/cuda/detail/async/reduce.h:  // aligned for any type of data. `malloc`/`cudaMalloc`/`new`/`std::allocator`
deps/thrust/thrust/system/cuda/detail/async/reduce.h:  cudaStream_t const user_raw_stream = thrust::cuda_cub::stream(policy);
deps/thrust/thrust/system/cuda/detail/async/reduce.h:  if (thrust::cuda_cub::default_stream() != user_raw_stream)
deps/thrust/thrust/system/cuda/detail/async/reduce.h:  thrust::cuda_cub::throw_on_error(
deps/thrust/thrust/system/cuda/detail/async/reduce.h:}}} // namespace system::cuda::detail
deps/thrust/thrust/system/cuda/detail/async/reduce.h:namespace cuda_cub
deps/thrust/thrust/system/cuda/detail/async/reduce.h:  thrust::system::cuda::detail::async_reduce_into_n(
deps/thrust/thrust/system/cuda/detail/async/reduce.h:} // cuda_cub
deps/thrust/thrust/system/cuda/detail/async/for_each.h: * Copyright (c) 2016, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/thrust/system/cuda/detail/async/for_each.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/thrust/system/cuda/detail/async/for_each.h: * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/thrust/system/cuda/detail/async/for_each.h:// TODO: Move into system::cuda
deps/thrust/thrust/system/cuda/detail/async/for_each.h:#include <thrust/system/cuda/config.h>
deps/thrust/thrust/system/cuda/detail/async/for_each.h:#include <thrust/system/cuda/detail/async/customization.h>
deps/thrust/thrust/system/cuda/detail/async/for_each.h:#include <thrust/system/cuda/detail/parallel_for.h>
deps/thrust/thrust/system/cuda/detail/async/for_each.h:#include <thrust/system/cuda/future.h>
deps/thrust/thrust/system/cuda/detail/async/for_each.h:namespace system { namespace cuda { namespace detail
deps/thrust/thrust/system/cuda/detail/async/for_each.h:  cudaStream_t const user_raw_stream = thrust::cuda_cub::stream(policy);
deps/thrust/thrust/system/cuda/detail/async/for_each.h:  if (thrust::cuda_cub::default_stream() != user_raw_stream)
deps/thrust/thrust/system/cuda/detail/async/for_each.h:  thrust::cuda_cub::throw_on_error(
deps/thrust/thrust/system/cuda/detail/async/for_each.h:    thrust::cuda_cub::__parallel_for::parallel_for(
deps/thrust/thrust/system/cuda/detail/async/for_each.h:}}} // namespace system::cuda::detail
deps/thrust/thrust/system/cuda/detail/async/for_each.h:namespace cuda_cub
deps/thrust/thrust/system/cuda/detail/async/for_each.h:  thrust::system::cuda::detail::async_for_each_n(
deps/thrust/thrust/system/cuda/detail/async/for_each.h:} // cuda_cub
deps/thrust/thrust/system/cuda/detail/async/sort.h: * Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/thrust/system/cuda/detail/async/sort.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/thrust/system/cuda/detail/async/sort.h: * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/thrust/system/cuda/detail/async/sort.h:// TODO: Move into system::cuda
deps/thrust/thrust/system/cuda/detail/async/sort.h:#include <thrust/system/cuda/config.h>
deps/thrust/thrust/system/cuda/detail/async/sort.h:#include <thrust/system/cuda/detail/async/customization.h>
deps/thrust/thrust/system/cuda/detail/async/sort.h:#include <thrust/system/cuda/detail/async/copy.h>
deps/thrust/thrust/system/cuda/detail/async/sort.h:#include <thrust/system/cuda/detail/sort.h>
deps/thrust/thrust/system/cuda/detail/async/sort.h:#include <thrust/system/cuda/future.h>
deps/thrust/thrust/system/cuda/detail/async/sort.h:namespace system { namespace cuda { namespace detail
deps/thrust/thrust/system/cuda/detail/async/sort.h:  thrust::cuda_cub::throw_on_error(
deps/thrust/thrust/system/cuda/detail/async/sort.h:    thrust::cuda_cub::__merge_sort::doit_step<
deps/thrust/thrust/system/cuda/detail/async/sort.h:  // aligned for any type of data. `malloc`/`cudaMalloc`/`new`/`std::allocator`
deps/thrust/thrust/system/cuda/detail/async/sort.h:  cudaStream_t const user_raw_stream = thrust::cuda_cub::stream(policy);
deps/thrust/thrust/system/cuda/detail/async/sort.h:  if (thrust::cuda_cub::default_stream() != user_raw_stream)
deps/thrust/thrust/system/cuda/detail/async/sort.h:  thrust::cuda_cub::throw_on_error(
deps/thrust/thrust/system/cuda/detail/async/sort.h:    thrust::cuda_cub::__merge_sort::doit_step<
deps/thrust/thrust/system/cuda/detail/async/sort.h:, cudaError_t
deps/thrust/thrust/system/cuda/detail/async/sort.h:  cudaStream_t          stream
deps/thrust/thrust/system/cuda/detail/async/sort.h:, cudaError_t
deps/thrust/thrust/system/cuda/detail/async/sort.h:  cudaStream_t          stream
deps/thrust/thrust/system/cuda/detail/async/sort.h:  thrust::cuda_cub::throw_on_error(
deps/thrust/thrust/system/cuda/detail/async/sort.h:  // aligned for any type of data. `malloc`/`cudaMalloc`/`new`/`std::allocator`
deps/thrust/thrust/system/cuda/detail/async/sort.h:  cudaStream_t const user_raw_stream = thrust::cuda_cub::stream(policy);
deps/thrust/thrust/system/cuda/detail/async/sort.h:  if (thrust::cuda_cub::default_stream() != user_raw_stream)
deps/thrust/thrust/system/cuda/detail/async/sort.h:  thrust::cuda_cub::throw_on_error(
deps/thrust/thrust/system/cuda/detail/async/sort.h:}}} // namespace system::cuda::detail
deps/thrust/thrust/system/cuda/detail/async/sort.h:namespace cuda_cub
deps/thrust/thrust/system/cuda/detail/async/sort.h:  thrust::system::cuda::detail::async_stable_sort_n(
deps/thrust/thrust/system/cuda/detail/async/sort.h:} // cuda_cub
deps/thrust/thrust/system/cuda/detail/async/scan.h: * Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/thrust/system/cuda/detail/async/scan.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/thrust/system/cuda/detail/async/scan.h: * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/thrust/system/cuda/detail/async/scan.h:#include <thrust/system/cuda/detail/async/exclusive_scan.h>
deps/thrust/thrust/system/cuda/detail/async/scan.h:#include <thrust/system/cuda/detail/async/inclusive_scan.h>
deps/thrust/thrust/system/cuda/detail/async/exclusive_scan.h: * Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/thrust/system/cuda/detail/async/exclusive_scan.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/thrust/system/cuda/detail/async/exclusive_scan.h: * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/thrust/system/cuda/detail/async/exclusive_scan.h:#include <thrust/system/cuda/config.h>
deps/thrust/thrust/system/cuda/detail/async/exclusive_scan.h:#include <thrust/system/cuda/detail/async/customization.h>
deps/thrust/thrust/system/cuda/detail/async/exclusive_scan.h:#include <thrust/system/cuda/detail/util.h>
deps/thrust/thrust/system/cuda/detail/async/exclusive_scan.h:#include <thrust/system/cuda/future.h>
deps/thrust/thrust/system/cuda/detail/async/exclusive_scan.h:namespace cuda
deps/thrust/thrust/system/cuda/detail/async/exclusive_scan.h:  cudaError_t status;
deps/thrust/thrust/system/cuda/detail/async/exclusive_scan.h:    thrust::cuda_cub::throw_on_error(status,
deps/thrust/thrust/system/cuda/detail/async/exclusive_scan.h:  cudaStream_t const user_raw_stream = thrust::cuda_cub::stream(policy);
deps/thrust/thrust/system/cuda/detail/async/exclusive_scan.h:  if (thrust::cuda_cub::default_stream() != user_raw_stream)
deps/thrust/thrust/system/cuda/detail/async/exclusive_scan.h:    thrust::cuda_cub::throw_on_error(status,
deps/thrust/thrust/system/cuda/detail/async/exclusive_scan.h:}}} // namespace system::cuda::detail
deps/thrust/thrust/system/cuda/detail/async/exclusive_scan.h:namespace cuda_cub
deps/thrust/thrust/system/cuda/detail/async/exclusive_scan.h:  thrust::system::cuda::detail::async_exclusive_scan_n(
deps/thrust/thrust/system/cuda/detail/async/exclusive_scan.h:} // namespace cuda_cub
deps/thrust/thrust/system/cuda/detail/async/copy.h: * Copyright (c) 2016, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/thrust/system/cuda/detail/async/copy.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/thrust/system/cuda/detail/async/copy.h: * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/thrust/system/cuda/detail/async/copy.h:// TODO: Move into system::cuda
deps/thrust/thrust/system/cuda/detail/async/copy.h:#include <thrust/system/cuda/config.h>
deps/thrust/thrust/system/cuda/detail/async/copy.h:#include <thrust/system/cuda/detail/async/customization.h>
deps/thrust/thrust/system/cuda/detail/async/copy.h:#include <thrust/system/cuda/detail/async/transform.h>
deps/thrust/thrust/system/cuda/detail/async/copy.h:#include <thrust/system/cuda/detail/cross_system.h>
deps/thrust/thrust/system/cuda/detail/async/copy.h:#include <thrust/system/cuda/future.h>
deps/thrust/thrust/system/cuda/detail/async/copy.h:namespace system { namespace cuda { namespace detail
deps/thrust/thrust/system/cuda/detail/async/copy.h:  cudaStream_t const user_raw_stream = thrust::cuda_cub::stream(
deps/thrust/thrust/system/cuda/detail/async/copy.h:  if (thrust::cuda_cub::default_stream() != user_raw_stream)
deps/thrust/thrust/system/cuda/detail/async/copy.h:  thrust::cuda_cub::throw_on_error(
deps/thrust/thrust/system/cuda/detail/async/copy.h:    cudaMemcpyAsync(
deps/thrust/thrust/system/cuda/detail/async/copy.h:  thrust::cuda::execution_policy<FromPolicy>& from_exec
deps/thrust/thrust/system/cuda/detail/async/copy.h:, thrust::cuda::execution_policy<ToPolicy>&   to_exec
deps/thrust/thrust/system/cuda/detail/async/copy.h:void async_copy_n_compile_failure_no_cuda_to_non_contiguous_output()
deps/thrust/thrust/system/cuda/detail/async/copy.h:  , "copying to non-ContiguousIterators in another system from the CUDA system "
deps/thrust/thrust/system/cuda/detail/async/copy.h:  async_copy_n_compile_failure_no_cuda_to_non_contiguous_output<OutputIt>();
deps/thrust/thrust/system/cuda/detail/async/copy.h:, thrust::cuda::execution_policy<ToPolicy>& to_exec
deps/thrust/thrust/system/cuda/detail/async/copy.h:    , thrust::cuda::execution_policy<ToPolicy>
deps/thrust/thrust/system/cuda/detail/async/copy.h:  thrust::cuda::execution_policy<FromPolicy>& from_exec
deps/thrust/thrust/system/cuda/detail/async/copy.h:      thrust::cuda::execution_policy<FromPolicy>
deps/thrust/thrust/system/cuda/detail/async/copy.h:    "the CUDA system; use `THRUST_PROCLAIM_TRIVIALLY_RELOCATABLE(T)` to "
deps/thrust/thrust/system/cuda/detail/async/copy.h:  // TODO: We could do more here with cudaHostRegister.
deps/thrust/thrust/system/cuda/detail/async/copy.h:}}} // namespace system::cuda::detail
deps/thrust/thrust/system/cuda/detail/async/copy.h:namespace cuda_cub
deps/thrust/thrust/system/cuda/detail/async/copy.h:  thrust::cuda::execution_policy<FromPolicy>&         from_exec
deps/thrust/thrust/system/cuda/detail/async/copy.h:  thrust::system::cuda::detail::async_copy_n(
deps/thrust/thrust/system/cuda/detail/async/copy.h:, thrust::cuda::execution_policy<ToPolicy>&  to_exec
deps/thrust/thrust/system/cuda/detail/async/copy.h:  thrust::system::cuda::detail::async_copy_n(
deps/thrust/thrust/system/cuda/detail/async/copy.h:  thrust::cuda::execution_policy<FromPolicy>& from_exec
deps/thrust/thrust/system/cuda/detail/async/copy.h:, thrust::cuda::execution_policy<ToPolicy>&   to_exec
deps/thrust/thrust/system/cuda/detail/async/copy.h:  thrust::system::cuda::detail::async_copy_n(
deps/thrust/thrust/system/cuda/detail/async/copy.h:} // cuda_cub
deps/thrust/thrust/system/cuda/detail/logical.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/cuda/detail/set_operations.h: * Copyright (c) 2016, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/thrust/system/cuda/detail/set_operations.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/thrust/system/cuda/detail/set_operations.h: * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/thrust/system/cuda/detail/set_operations.h:#include <thrust/system/cuda/detail/util.h>
deps/thrust/thrust/system/cuda/detail/set_operations.h:#include <thrust/system/cuda/detail/execution_policy.h>
deps/thrust/thrust/system/cuda/detail/set_operations.h:#include <thrust/system/cuda/detail/core/agent_launcher.h>
deps/thrust/thrust/system/cuda/detail/set_operations.h:#include <thrust/system/cuda/detail/par_to_seq.h>
deps/thrust/thrust/system/cuda/detail/set_operations.h:#include <thrust/system/cuda/detail/get_value.h>
deps/thrust/thrust/system/cuda/detail/set_operations.h:namespace cuda_cub {
deps/thrust/thrust/system/cuda/detail/set_operations.h:  cudaError_t THRUST_RUNTIME_FUNCTION
deps/thrust/thrust/system/cuda/detail/set_operations.h:            cudaStream_t   stream,
deps/thrust/thrust/system/cuda/detail/set_operations.h:      return cudaErrorNotSupported;
deps/thrust/thrust/system/cuda/detail/set_operations.h:    cudaError_t status = cudaSuccess;
deps/thrust/thrust/system/cuda/detail/set_operations.h:    CUDA_CUB_RET_IF_FAIL(status);
deps/thrust/thrust/system/cuda/detail/set_operations.h:    CUDA_CUB_RET_IF_FAIL(status);
deps/thrust/thrust/system/cuda/detail/set_operations.h:    CUDA_CUB_RET_IF_FAIL(status);
deps/thrust/thrust/system/cuda/detail/set_operations.h:    CUDA_CUB_RET_IF_FAIL(cudaPeekAtLastError());
deps/thrust/thrust/system/cuda/detail/set_operations.h:    CUDA_CUB_RET_IF_FAIL(cudaPeekAtLastError());
deps/thrust/thrust/system/cuda/detail/set_operations.h:    CUDA_CUB_RET_IF_FAIL(cudaPeekAtLastError());
deps/thrust/thrust/system/cuda/detail/set_operations.h:    cudaStream_t stream             = cuda_cub::stream(policy);
deps/thrust/thrust/system/cuda/detail/set_operations.h:    cudaError_t status;
deps/thrust/thrust/system/cuda/detail/set_operations.h:    cuda_cub::throw_on_error(status, "set_operations failed on 1st step");
deps/thrust/thrust/system/cuda/detail/set_operations.h:    cuda_cub::throw_on_error(status, "set_operations failed on 1st alias_storage");
deps/thrust/thrust/system/cuda/detail/set_operations.h:    cuda_cub::throw_on_error(status, "set_operations failed on 2nd alias_storage");
deps/thrust/thrust/system/cuda/detail/set_operations.h:    cuda_cub::throw_on_error(status, "set_operations failed on 2nd step");
deps/thrust/thrust/system/cuda/detail/set_operations.h:    status = cuda_cub::synchronize(policy);
deps/thrust/thrust/system/cuda/detail/set_operations.h:    cuda_cub::throw_on_error(status, "set_operations failed to synchronize");
deps/thrust/thrust/system/cuda/detail/set_operations.h:    std::size_t output_count = cuda_cub::get_value(policy, d_output_count);
deps/thrust/thrust/system/cuda/detail/set_operations.h:  if (__THRUST_HAS_CUDART__)
deps/thrust/thrust/system/cuda/detail/set_operations.h:#if !__THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/set_operations.h:  return cuda_cub::set_difference(policy,
deps/thrust/thrust/system/cuda/detail/set_operations.h:  if (__THRUST_HAS_CUDART__)
deps/thrust/thrust/system/cuda/detail/set_operations.h:#if !__THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/set_operations.h:  return cuda_cub::set_intersection(policy,
deps/thrust/thrust/system/cuda/detail/set_operations.h:  if (__THRUST_HAS_CUDART__)
deps/thrust/thrust/system/cuda/detail/set_operations.h:#if !__THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/set_operations.h:  return cuda_cub::set_symmetric_difference(policy,
deps/thrust/thrust/system/cuda/detail/set_operations.h:  if (__THRUST_HAS_CUDART__)
deps/thrust/thrust/system/cuda/detail/set_operations.h:#if !__THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/set_operations.h:  return cuda_cub::set_union(policy,
deps/thrust/thrust/system/cuda/detail/set_operations.h:  if (__THRUST_HAS_CUDART__)
deps/thrust/thrust/system/cuda/detail/set_operations.h:#if !__THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/set_operations.h:  return cuda_cub::set_difference_by_key(policy,
deps/thrust/thrust/system/cuda/detail/set_operations.h:  if (__THRUST_HAS_CUDART__)
deps/thrust/thrust/system/cuda/detail/set_operations.h:#if !__THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/set_operations.h:  return cuda_cub::set_intersection_by_key(policy,
deps/thrust/thrust/system/cuda/detail/set_operations.h:  if (__THRUST_HAS_CUDART__)
deps/thrust/thrust/system/cuda/detail/set_operations.h:#if !__THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/set_operations.h:  return cuda_cub::set_symmetric_difference_by_key(policy,
deps/thrust/thrust/system/cuda/detail/set_operations.h:  if (__THRUST_HAS_CUDART__)
deps/thrust/thrust/system/cuda/detail/set_operations.h:#if !__THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/set_operations.h:  return cuda_cub::set_union_by_key(policy,
deps/thrust/thrust/system/cuda/detail/set_operations.h:}    // namespace cuda_cub
deps/thrust/thrust/system/cuda/detail/per_device_resource.h: * Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/thrust/system/cuda/detail/per_device_resource.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/thrust/system/cuda/detail/per_device_resource.h: * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/thrust/system/cuda/detail/per_device_resource.h:#include <thrust/system/cuda/config.h>
deps/thrust/thrust/system/cuda/detail/per_device_resource.h:#include <thrust/system/cuda/detail/util.h>
deps/thrust/thrust/system/cuda/detail/per_device_resource.h:#include <thrust/system/cuda/detail/execution_policy.h>
deps/thrust/thrust/system/cuda/detail/per_device_resource.h:#include <thrust/system/cuda/detail/util.h>
deps/thrust/thrust/system/cuda/detail/per_device_resource.h:namespace cuda_cub
deps/thrust/thrust/system/cuda/detail/per_device_resource.h:    thrust::cuda_cub::throw_on_error(cudaGetDevice(&device_id));
deps/thrust/thrust/system/cuda/detail/equal.h: * Copyright (c) 2016, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/thrust/system/cuda/detail/equal.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/thrust/system/cuda/detail/equal.h: * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/thrust/system/cuda/detail/equal.h:#include <thrust/system/cuda/config.h>
deps/thrust/thrust/system/cuda/detail/equal.h:#include <thrust/system/cuda/detail/mismatch.h>
deps/thrust/thrust/system/cuda/detail/equal.h:namespace cuda_cub {
deps/thrust/thrust/system/cuda/detail/equal.h:  return cuda_cub::mismatch(policy, first1, last1, first2, binary_pred).first == last1;
deps/thrust/thrust/system/cuda/detail/equal.h:  return cuda_cub::equal(policy,
deps/thrust/thrust/system/cuda/detail/equal.h:} // namespace cuda_cub
deps/thrust/thrust/system/cuda/detail/unique.h: * Copyright (c) 2016, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/thrust/system/cuda/detail/unique.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/thrust/system/cuda/detail/unique.h: * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/thrust/system/cuda/detail/unique.h:#include <thrust/system/cuda/config.h>
deps/thrust/thrust/system/cuda/detail/unique.h:#include <thrust/system/cuda/detail/core/agent_launcher.h>
deps/thrust/thrust/system/cuda/detail/unique.h:#include <thrust/system/cuda/detail/par_to_seq.h>
deps/thrust/thrust/system/cuda/detail/unique.h:#include <thrust/system/cuda/detail/util.h>
deps/thrust/thrust/system/cuda/detail/unique.h:#include <thrust/system/cuda/detail/get_value.h>
deps/thrust/thrust/system/cuda/detail/unique.h:namespace cuda_cub {
deps/thrust/thrust/system/cuda/detail/unique.h:  static cudaError_t THRUST_RUNTIME_FUNCTION
deps/thrust/thrust/system/cuda/detail/unique.h:            cudaStream_t     stream,
deps/thrust/thrust/system/cuda/detail/unique.h:    cudaError_t status = cudaSuccess;
deps/thrust/thrust/system/cuda/detail/unique.h:    CUDA_CUB_RET_IF_FAIL(status);
deps/thrust/thrust/system/cuda/detail/unique.h:    CUDA_CUB_RET_IF_FAIL(status);
deps/thrust/thrust/system/cuda/detail/unique.h:    CUDA_CUB_RET_IF_FAIL(status);
deps/thrust/thrust/system/cuda/detail/unique.h:    CUDA_CUB_RET_IF_FAIL(cudaPeekAtLastError());
deps/thrust/thrust/system/cuda/detail/unique.h:    CUDA_CUB_RET_IF_FAIL(cudaPeekAtLastError());
deps/thrust/thrust/system/cuda/detail/unique.h:    cudaStream_t stream             = cuda_cub::stream(policy);
deps/thrust/thrust/system/cuda/detail/unique.h:    cudaError_t status;
deps/thrust/thrust/system/cuda/detail/unique.h:    cuda_cub::throw_on_error(status, "unique: failed on 1st step");
deps/thrust/thrust/system/cuda/detail/unique.h:    cuda_cub::throw_on_error(status, "unique: failed on 1st step");
deps/thrust/thrust/system/cuda/detail/unique.h:    cuda_cub::throw_on_error(status, "unique: failed on 2nd step");
deps/thrust/thrust/system/cuda/detail/unique.h:    cuda_cub::throw_on_error(status, "unique: failed on 2nd step");
deps/thrust/thrust/system/cuda/detail/unique.h:    status = cuda_cub::synchronize(policy);
deps/thrust/thrust/system/cuda/detail/unique.h:    cuda_cub::throw_on_error(status, "unique: failed to synchronize");
deps/thrust/thrust/system/cuda/detail/unique.h:  if (__THRUST_HAS_CUDART__)
deps/thrust/thrust/system/cuda/detail/unique.h:#if !__THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/unique.h:  return cuda_cub::unique_copy(policy, first, last, result, equal_to<input_type>());
deps/thrust/thrust/system/cuda/detail/unique.h:  if (__THRUST_HAS_CUDART__)
deps/thrust/thrust/system/cuda/detail/unique.h:    ret = cuda_cub::unique_copy(policy, first, last, first, binary_pred);
deps/thrust/thrust/system/cuda/detail/unique.h:#if !__THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/unique.h:  return cuda_cub::unique(policy, first, last, equal_to<input_type>());
deps/thrust/thrust/system/cuda/detail/unique.h:}    // namespace cuda_cub
deps/thrust/thrust/system/cuda/detail/sort.h: * Copyright (c) 2016, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/thrust/system/cuda/detail/sort.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/thrust/system/cuda/detail/sort.h: * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/thrust/system/cuda/detail/sort.h:#include <thrust/system/cuda/detail/util.h>
deps/thrust/thrust/system/cuda/detail/sort.h:#include <thrust/system/cuda/config.h>
deps/thrust/thrust/system/cuda/detail/sort.h:#include <thrust/system/cuda/detail/core/agent_launcher.h>
deps/thrust/thrust/system/cuda/detail/sort.h:#include <thrust/system/cuda/detail/core/util.h>
deps/thrust/thrust/system/cuda/detail/sort.h:#include <thrust/system/cuda/detail/execution_policy.h>
deps/thrust/thrust/system/cuda/detail/sort.h:#include <thrust/system/cuda/detail/par_to_seq.h>
deps/thrust/thrust/system/cuda/detail/sort.h:namespace cuda_cub {
deps/thrust/thrust/system/cuda/detail/sort.h:  THRUST_RUNTIME_FUNCTION cudaError_t
deps/thrust/thrust/system/cuda/detail/sort.h:            cudaStream_t stream,
deps/thrust/thrust/system/cuda/detail/sort.h:  THRUST_RUNTIME_FUNCTION cudaError_t
deps/thrust/thrust/system/cuda/detail/sort.h:            cudaStream_t stream,
deps/thrust/thrust/system/cuda/detail/sort.h:  THRUST_RUNTIME_FUNCTION cudaError_t
deps/thrust/thrust/system/cuda/detail/sort.h:            cudaStream_t stream,
deps/thrust/thrust/system/cuda/detail/sort.h:      return cudaSuccess;
deps/thrust/thrust/system/cuda/detail/sort.h:    cudaStream_t stream       = cuda_cub::stream(policy);
deps/thrust/thrust/system/cuda/detail/sort.h:    cudaError_t status;
deps/thrust/thrust/system/cuda/detail/sort.h:    cuda_cub::throw_on_error(status, "merge_sort: failed on 1st step");
deps/thrust/thrust/system/cuda/detail/sort.h:    cuda_cub::throw_on_error(status, "merge_sort: failed on 2nd step");
deps/thrust/thrust/system/cuda/detail/sort.h:    status = cuda_cub::synchronize_optional(policy);
deps/thrust/thrust/system/cuda/detail/sort.h:    cuda_cub::throw_on_error(status, "merge_sort: failed to synchronize");
deps/thrust/thrust/system/cuda/detail/sort.h:    THRUST_RUNTIME_FUNCTION static cudaError_t
deps/thrust/thrust/system/cuda/detail/sort.h:         cudaStream_t             stream,
deps/thrust/thrust/system/cuda/detail/sort.h:    THRUST_RUNTIME_FUNCTION static cudaError_t
deps/thrust/thrust/system/cuda/detail/sort.h:         cudaStream_t             stream,
deps/thrust/thrust/system/cuda/detail/sort.h:    THRUST_RUNTIME_FUNCTION static cudaError_t
deps/thrust/thrust/system/cuda/detail/sort.h:         cudaStream_t             stream,
deps/thrust/thrust/system/cuda/detail/sort.h:    THRUST_RUNTIME_FUNCTION static cudaError_t
deps/thrust/thrust/system/cuda/detail/sort.h:         cudaStream_t             stream,
deps/thrust/thrust/system/cuda/detail/sort.h:    cudaStream_t stream             = cuda_cub::stream(policy);
deps/thrust/thrust/system/cuda/detail/sort.h:    cudaError_t status;
deps/thrust/thrust/system/cuda/detail/sort.h:    cuda_cub::throw_on_error(status, "radix_sort: failed on 1st step");
deps/thrust/thrust/system/cuda/detail/sort.h:    cuda_cub::throw_on_error(status, "radix_sort: failed on 2nd step");
deps/thrust/thrust/system/cuda/detail/sort.h:      cuda_cub::copy_n(policy, temp_ptr, keys_count, keys);
deps/thrust/thrust/system/cuda/detail/sort.h:        cuda_cub::copy_n(policy, temp_ptr, items_count, items);
deps/thrust/thrust/system/cuda/detail/sort.h:        cuda_cub::copy(policy, values.begin(), values.end(), items_first);
deps/thrust/thrust/system/cuda/detail/sort.h:      cuda_cub::copy(policy, keys.begin(), keys.end(), keys_first);
deps/thrust/thrust/system/cuda/detail/sort.h:    cuda_cub::throw_on_error(
deps/thrust/thrust/system/cuda/detail/sort.h:      cuda_cub::synchronize_optional(policy),
deps/thrust/thrust/system/cuda/detail/sort.h:  if (__THRUST_HAS_CUDART__)
deps/thrust/thrust/system/cuda/detail/sort.h:#if !__THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/sort.h:  if (__THRUST_HAS_CUDART__)
deps/thrust/thrust/system/cuda/detail/sort.h:#if !__THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/sort.h:  if (__THRUST_HAS_CUDART__)
deps/thrust/thrust/system/cuda/detail/sort.h:#if !__THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/sort.h:  if (__THRUST_HAS_CUDART__)
deps/thrust/thrust/system/cuda/detail/sort.h:#if !__THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/sort.h:  cuda_cub::sort(policy, first, last, less<item_type>());
deps/thrust/thrust/system/cuda/detail/sort.h:  cuda_cub::stable_sort(policy, first, last, less<item_type>());
deps/thrust/thrust/system/cuda/detail/sort.h:  cuda_cub::sort_by_key(policy, keys_first, keys_last, values, less<key_type>());
deps/thrust/thrust/system/cuda/detail/sort.h:  cuda_cub::stable_sort_by_key(policy, keys_first, keys_last, values, less<key_type>());
deps/thrust/thrust/system/cuda/detail/sort.h:}    // namespace cuda_cub
deps/thrust/thrust/system/cuda/detail/reduce_by_key.h: * Copyright (c) 2016, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/thrust/system/cuda/detail/reduce_by_key.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/thrust/system/cuda/detail/reduce_by_key.h: * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/thrust/system/cuda/detail/reduce_by_key.h:#include <thrust/system/cuda/config.h>
deps/thrust/thrust/system/cuda/detail/reduce_by_key.h:#include <thrust/system/cuda/detail/util.h>
deps/thrust/thrust/system/cuda/detail/reduce_by_key.h:#include <thrust/system/cuda/detail/par_to_seq.h>
deps/thrust/thrust/system/cuda/detail/reduce_by_key.h:#include <thrust/system/cuda/detail/core/agent_launcher.h>
deps/thrust/thrust/system/cuda/detail/reduce_by_key.h:#include <thrust/system/cuda/detail/get_value.h>
deps/thrust/thrust/system/cuda/detail/reduce_by_key.h:namespace cuda_cub {
deps/thrust/thrust/system/cuda/detail/reduce_by_key.h:  THRUST_RUNTIME_FUNCTION cudaError_t
deps/thrust/thrust/system/cuda/detail/reduce_by_key.h:            cudaStream_t    stream,
deps/thrust/thrust/system/cuda/detail/reduce_by_key.h:    cudaError_t status = cudaSuccess;
deps/thrust/thrust/system/cuda/detail/reduce_by_key.h:      return cudaErrorNotSupported;
deps/thrust/thrust/system/cuda/detail/reduce_by_key.h:    CUDA_CUB_RET_IF_FAIL(status);
deps/thrust/thrust/system/cuda/detail/reduce_by_key.h:    CUDA_CUB_RET_IF_FAIL(status);
deps/thrust/thrust/system/cuda/detail/reduce_by_key.h:    CUDA_CUB_RET_IF_FAIL(status);
deps/thrust/thrust/system/cuda/detail/reduce_by_key.h:    CUDA_CUB_RET_IF_FAIL(cudaPeekAtLastError());
deps/thrust/thrust/system/cuda/detail/reduce_by_key.h:    CUDA_CUB_RET_IF_FAIL(cudaPeekAtLastError());
deps/thrust/thrust/system/cuda/detail/reduce_by_key.h:    cudaStream_t stream             = cuda_cub::stream(policy);
deps/thrust/thrust/system/cuda/detail/reduce_by_key.h:    cudaError_t status;
deps/thrust/thrust/system/cuda/detail/reduce_by_key.h:    cuda_cub::throw_on_error(status, "reduce_by_key failed on 1st step");
deps/thrust/thrust/system/cuda/detail/reduce_by_key.h:    cuda_cub::throw_on_error(status, "reduce failed on 1st alias_storage");
deps/thrust/thrust/system/cuda/detail/reduce_by_key.h:    cuda_cub::throw_on_error(status, "reduce failed on 2nd alias_storage");
deps/thrust/thrust/system/cuda/detail/reduce_by_key.h:    cuda_cub::throw_on_error(status, "reduce_by_key failed on 2nd step");
deps/thrust/thrust/system/cuda/detail/reduce_by_key.h:    status = cuda_cub::synchronize(policy);
deps/thrust/thrust/system/cuda/detail/reduce_by_key.h:    cuda_cub::throw_on_error(status, "reduce_by_key: failed to synchronize");
deps/thrust/thrust/system/cuda/detail/reduce_by_key.h:    int num_runs_out = cuda_cub::get_value(policy, d_num_runs_out);
deps/thrust/thrust/system/cuda/detail/reduce_by_key.h:  if (__THRUST_HAS_CUDART__)
deps/thrust/thrust/system/cuda/detail/reduce_by_key.h:#if !__THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/reduce_by_key.h:  return cuda_cub::reduce_by_key(policy,
deps/thrust/thrust/system/cuda/detail/reduce_by_key.h:  return cuda_cub::reduce_by_key(policy,
deps/thrust/thrust/system/cuda/detail/reduce_by_key.h:} // namespace cuda_
deps/thrust/thrust/system/cuda/detail/fill.h: * Copyright (c) 2016, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/thrust/system/cuda/detail/fill.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/thrust/system/cuda/detail/fill.h: * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/thrust/system/cuda/detail/fill.h:#include <thrust/system/cuda/detail/util.h>
deps/thrust/thrust/system/cuda/detail/fill.h:#include <thrust/system/cuda/detail/parallel_for.h>
deps/thrust/thrust/system/cuda/detail/fill.h:namespace cuda_cub {
deps/thrust/thrust/system/cuda/detail/fill.h:  cuda_cub::parallel_for(policy,
deps/thrust/thrust/system/cuda/detail/fill.h:  cuda_cub::throw_on_error(
deps/thrust/thrust/system/cuda/detail/fill.h:    cuda_cub::synchronize_optional(policy)
deps/thrust/thrust/system/cuda/detail/fill.h:  cuda_cub::fill_n(policy, first, thrust::distance(first,last), value);
deps/thrust/thrust/system/cuda/detail/fill.h:} // namespace cuda_cub
deps/thrust/thrust/system/cuda/detail/reverse.h: * Copyright (c) 2016, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/thrust/system/cuda/detail/reverse.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/thrust/system/cuda/detail/reverse.h: * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/thrust/system/cuda/detail/reverse.h:#include <thrust/system/cuda/detail/execution_policy.h>
deps/thrust/thrust/system/cuda/detail/reverse.h:namespace cuda_cub {
deps/thrust/thrust/system/cuda/detail/reverse.h:}    // namespace cuda_cub
deps/thrust/thrust/system/cuda/detail/reverse.h:#include <thrust/system/cuda/detail/swap_ranges.h>
deps/thrust/thrust/system/cuda/detail/reverse.h:#include <thrust/system/cuda/detail/copy.h>
deps/thrust/thrust/system/cuda/detail/reverse.h:namespace cuda_cub {
deps/thrust/thrust/system/cuda/detail/reverse.h:  return cuda_cub::copy(policy,
deps/thrust/thrust/system/cuda/detail/reverse.h:  cuda_cub::swap_ranges(policy, first, mid, thrust::make_reverse_iterator(last));
deps/thrust/thrust/system/cuda/detail/reverse.h:}    // namespace cuda_cub
deps/thrust/thrust/system/cuda/detail/util.h: * Copyright (c) 2016, NVIDIA CORPORATION.  All rights meserved.
deps/thrust/thrust/system/cuda/detail/util.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/thrust/system/cuda/detail/util.h: * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/thrust/system/cuda/detail/util.h:#include <thrust/system/cuda/detail/execution_policy.h>
deps/thrust/thrust/system/cuda/detail/util.h:#include <thrust/system/cuda/error.h>
deps/thrust/thrust/system/cuda/detail/util.h:namespace cuda_cub {
deps/thrust/thrust/system/cuda/detail/util.h:cudaStream_t
deps/thrust/thrust/system/cuda/detail/util.h:#ifdef CUDA_API_PER_THREAD_DEFAULT_STREAM
deps/thrust/thrust/system/cuda/detail/util.h:  return cudaStreamPerThread;
deps/thrust/thrust/system/cuda/detail/util.h:  return cudaStreamLegacy;
deps/thrust/thrust/system/cuda/detail/util.h:cudaStream_t
deps/thrust/thrust/system/cuda/detail/util.h:__host__ __device__ cudaStream_t
deps/thrust/thrust/system/cuda/detail/util.h:cudaError_t
deps/thrust/thrust/system/cuda/detail/util.h:  cudaError_t result;
deps/thrust/thrust/system/cuda/detail/util.h:      cudaStreamSynchronize(stream(policy));
deps/thrust/thrust/system/cuda/detail/util.h:      result = cudaGetLastError();
deps/thrust/thrust/system/cuda/detail/util.h:      #if __THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/util.h:        result = cudaGetLastError();
deps/thrust/thrust/system/cuda/detail/util.h:        result = cudaSuccess;
deps/thrust/thrust/system/cuda/detail/util.h:cudaError_t
deps/thrust/thrust/system/cuda/detail/util.h:cudaError_t
deps/thrust/thrust/system/cuda/detail/util.h:  cudaError_t result;
deps/thrust/thrust/system/cuda/detail/util.h:        cudaStreamSynchronize(stream(policy));
deps/thrust/thrust/system/cuda/detail/util.h:        result = cudaGetLastError();
deps/thrust/thrust/system/cuda/detail/util.h:        result = cudaSuccess;
deps/thrust/thrust/system/cuda/detail/util.h:      #if __THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/util.h:          result = cudaGetLastError();
deps/thrust/thrust/system/cuda/detail/util.h:          result = cudaSuccess;
deps/thrust/thrust/system/cuda/detail/util.h:        result = cudaSuccess;
deps/thrust/thrust/system/cuda/detail/util.h:cudaError_t
deps/thrust/thrust/system/cuda/detail/util.h:THRUST_HOST_FUNCTION cudaError_t
deps/thrust/thrust/system/cuda/detail/util.h:                         cudaStream_t stream)
deps/thrust/thrust/system/cuda/detail/util.h:  cudaError status = cudaSuccess;
deps/thrust/thrust/system/cuda/detail/util.h:  status = ::cudaMemcpyAsync(dst,
deps/thrust/thrust/system/cuda/detail/util.h:                             cudaMemcpyDeviceToHost,
deps/thrust/thrust/system/cuda/detail/util.h:  cudaStreamSynchronize(stream);
deps/thrust/thrust/system/cuda/detail/util.h:THRUST_HOST_FUNCTION cudaError_t
deps/thrust/thrust/system/cuda/detail/util.h:                       cudaStream_t stream)
deps/thrust/thrust/system/cuda/detail/util.h:  cudaError status = cudaSuccess;
deps/thrust/thrust/system/cuda/detail/util.h:  status = ::cudaMemcpyAsync(dst,
deps/thrust/thrust/system/cuda/detail/util.h:                             cudaMemcpyHostToDevice,
deps/thrust/thrust/system/cuda/detail/util.h:  cudaStreamSynchronize(stream);
deps/thrust/thrust/system/cuda/detail/util.h:__host__ __device__ cudaError_t
deps/thrust/thrust/system/cuda/detail/util.h:  cudaError_t  status = cudaSuccess;
deps/thrust/thrust/system/cuda/detail/util.h:  cudaStream_t stream = cuda_cub::stream(policy);
deps/thrust/thrust/system/cuda/detail/util.h:  status = ::cudaMemcpyAsync(dst,
deps/thrust/thrust/system/cuda/detail/util.h:                             cudaMemcpyDeviceToDevice,
deps/thrust/thrust/system/cuda/detail/util.h:  cuda_cub::synchronize(policy);
deps/thrust/thrust/system/cuda/detail/util.h:inline void throw_on_error(cudaError_t status)
deps/thrust/thrust/system/cuda/detail/util.h:#if __THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/util.h:  // Clear the global CUDA error state which may have been set by the last
deps/thrust/thrust/system/cuda/detail/util.h:  cudaGetLastError();
deps/thrust/thrust/system/cuda/detail/util.h:  if (cudaSuccess != status)
deps/thrust/thrust/system/cuda/detail/util.h:        throw thrust::system_error(status, thrust::cuda_category());
deps/thrust/thrust/system/cuda/detail/util.h:        #if __THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/util.h:          printf("Thrust CUDA backend error: %s: %s\n",
deps/thrust/thrust/system/cuda/detail/util.h:                 cudaGetErrorName(status),
deps/thrust/thrust/system/cuda/detail/util.h:                 cudaGetErrorString(status));
deps/thrust/thrust/system/cuda/detail/util.h:          printf("Thrust CUDA backend error: %d\n",
deps/thrust/thrust/system/cuda/detail/util.h:        cuda_cub::terminate();
deps/thrust/thrust/system/cuda/detail/util.h:inline void throw_on_error(cudaError_t status, char const *msg)
deps/thrust/thrust/system/cuda/detail/util.h:#if __THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/util.h:  // Clear the global CUDA error state which may have been set by the last
deps/thrust/thrust/system/cuda/detail/util.h:  cudaGetLastError();
deps/thrust/thrust/system/cuda/detail/util.h:  if (cudaSuccess != status)
deps/thrust/thrust/system/cuda/detail/util.h:        throw thrust::system_error(status, thrust::cuda_category(), msg);
deps/thrust/thrust/system/cuda/detail/util.h:        #if __THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/util.h:          printf("Thrust CUDA backend error: %s: %s: %s\n",
deps/thrust/thrust/system/cuda/detail/util.h:                 cudaGetErrorName(status),
deps/thrust/thrust/system/cuda/detail/util.h:                 cudaGetErrorString(status),
deps/thrust/thrust/system/cuda/detail/util.h:          printf("Thrust CUDA backend error: %d: %s \n",
deps/thrust/thrust/system/cuda/detail/util.h:        cuda_cub::terminate();
deps/thrust/thrust/system/cuda/detail/util.h:}    // cuda_
deps/thrust/thrust/system/cuda/detail/make_unsigned_special.h: *  Copyright 2019 NVIDIA Corporation
deps/thrust/thrust/system/cuda/detail/make_unsigned_special.h:namespace cuda_cub {
deps/thrust/thrust/system/cuda/detail/make_unsigned_special.h:    // this is special, because CUDA's atomicAdd doesn't have an overload
deps/thrust/thrust/system/cuda/detail/scan.h: * Copyright (c) 2016, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/thrust/system/cuda/detail/scan.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/thrust/system/cuda/detail/scan.h: * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/thrust/system/cuda/detail/scan.h:#include <thrust/system/cuda/config.h>
deps/thrust/thrust/system/cuda/detail/scan.h:#include <thrust/system/cuda/detail/dispatch.h>
deps/thrust/thrust/system/cuda/detail/scan.h:namespace cuda_cub
deps/thrust/thrust/system/cuda/detail/scan.h:OutputIt inclusive_scan_n_impl(thrust::cuda_cub::execution_policy<Derived> &policy,
deps/thrust/thrust/system/cuda/detail/scan.h:  cudaStream_t stream = thrust::cuda_cub::stream(policy);
deps/thrust/thrust/system/cuda/detail/scan.h:  cudaError_t status;
deps/thrust/thrust/system/cuda/detail/scan.h:    thrust::cuda_cub::throw_on_error(status,
deps/thrust/thrust/system/cuda/detail/scan.h:    thrust::cuda_cub::throw_on_error(status,
deps/thrust/thrust/system/cuda/detail/scan.h:    thrust::cuda_cub::throw_on_error(thrust::cuda_cub::synchronize_optional(policy),
deps/thrust/thrust/system/cuda/detail/scan.h:OutputIt exclusive_scan_n_impl(thrust::cuda_cub::execution_policy<Derived> &policy,
deps/thrust/thrust/system/cuda/detail/scan.h:  cudaStream_t stream = thrust::cuda_cub::stream(policy);
deps/thrust/thrust/system/cuda/detail/scan.h:  cudaError_t status;
deps/thrust/thrust/system/cuda/detail/scan.h:    thrust::cuda_cub::throw_on_error(status,
deps/thrust/thrust/system/cuda/detail/scan.h:    thrust::cuda_cub::throw_on_error(status,
deps/thrust/thrust/system/cuda/detail/scan.h:    thrust::cuda_cub::throw_on_error(thrust::cuda_cub::synchronize_optional(policy),
deps/thrust/thrust/system/cuda/detail/scan.h:OutputIt inclusive_scan_n(thrust::cuda_cub::execution_policy<Derived> &policy,
deps/thrust/thrust/system/cuda/detail/scan.h:  if (__THRUST_HAS_CUDART__)
deps/thrust/thrust/system/cuda/detail/scan.h:    ret = thrust::cuda_cub::detail::inclusive_scan_n_impl(policy,
deps/thrust/thrust/system/cuda/detail/scan.h:#if !__THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/scan.h:OutputIt inclusive_scan(thrust::cuda_cub::execution_policy<Derived> &policy,
deps/thrust/thrust/system/cuda/detail/scan.h:  return thrust::cuda_cub::inclusive_scan_n(policy,
deps/thrust/thrust/system/cuda/detail/scan.h:OutputIt inclusive_scan(thrust::cuda_cub::execution_policy<Derived> &policy,
deps/thrust/thrust/system/cuda/detail/scan.h:  return thrust::cuda_cub::inclusive_scan(policy,
deps/thrust/thrust/system/cuda/detail/scan.h:OutputIt exclusive_scan_n(thrust::cuda_cub::execution_policy<Derived> &policy,
deps/thrust/thrust/system/cuda/detail/scan.h:  if (__THRUST_HAS_CUDART__)
deps/thrust/thrust/system/cuda/detail/scan.h:    ret = thrust::cuda_cub::detail::exclusive_scan_n_impl(policy,
deps/thrust/thrust/system/cuda/detail/scan.h:#if !__THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/scan.h:OutputIt exclusive_scan(thrust::cuda_cub::execution_policy<Derived> &policy,
deps/thrust/thrust/system/cuda/detail/scan.h:  return thrust::cuda_cub::exclusive_scan_n(policy,
deps/thrust/thrust/system/cuda/detail/scan.h:OutputIt exclusive_scan(thrust::cuda_cub::execution_policy<Derived> &policy,
deps/thrust/thrust/system/cuda/detail/scan.h:  return thrust::cuda_cub::exclusive_scan(policy,
deps/thrust/thrust/system/cuda/detail/scan.h:OutputIt exclusive_scan(thrust::cuda_cub::execution_policy<Derived> &policy,
deps/thrust/thrust/system/cuda/detail/scan.h:  return cuda_cub::exclusive_scan(policy, first, last, result, init_type{});
deps/thrust/thrust/system/cuda/detail/scan.h:} // namespace cuda_cub
deps/thrust/thrust/system/cuda/detail/copy.h: * Copyright (c) 2016, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/thrust/system/cuda/detail/copy.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/thrust/system/cuda/detail/copy.h: * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/thrust/system/cuda/detail/copy.h:#include <thrust/system/cuda/config.h>
deps/thrust/thrust/system/cuda/detail/copy.h:#include <thrust/system/cuda/detail/execution_policy.h>
deps/thrust/thrust/system/cuda/detail/copy.h:#include <thrust/system/cuda/detail/cross_system.h>
deps/thrust/thrust/system/cuda/detail/copy.h:namespace cuda_cub {
deps/thrust/thrust/system/cuda/detail/copy.h:}    // namespace cuda_
deps/thrust/thrust/system/cuda/detail/copy.h:#include <thrust/system/cuda/detail/internal/copy_device_to_device.h>
deps/thrust/thrust/system/cuda/detail/copy.h:#include <thrust/system/cuda/detail/internal/copy_cross_system.h>
deps/thrust/thrust/system/cuda/detail/copy.h:#include <thrust/system/cuda/detail/par_to_seq.h>
deps/thrust/thrust/system/cuda/detail/copy.h:namespace cuda_cub {
deps/thrust/thrust/system/cuda/detail/copy.h:  if (__THRUST_HAS_CUDART__)
deps/thrust/thrust/system/cuda/detail/copy.h:#if !__THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/copy.h:  if (__THRUST_HAS_CUDART__)
deps/thrust/thrust/system/cuda/detail/copy.h:#if !__THRUST_HAS_CUDART__
deps/thrust/thrust/system/cuda/detail/copy.h:}    // namespace cuda_cub
deps/thrust/thrust/system/cuda/detail/scatter.h: * Copyright (c) 2016, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/thrust/system/cuda/detail/scatter.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/thrust/system/cuda/detail/scatter.h: * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/thrust/system/cuda/detail/scatter.h:#include <thrust/system/cuda/detail/transform.h>
deps/thrust/thrust/system/cuda/detail/scatter.h:namespace cuda_cub {
deps/thrust/thrust/system/cuda/detail/scatter.h:  cuda_cub::transform(policy,
deps/thrust/thrust/system/cuda/detail/scatter.h:  cuda_cub::transform_if(policy,
deps/thrust/thrust/system/cuda/detail/scatter.h:  cuda_cub::scatter_if(policy,
deps/thrust/thrust/system/cuda/detail/scatter.h:} // namespace cuda_cub
deps/thrust/thrust/system/cuda/detail/memory.inl: *  Copyright 2008-2018 NVIDIA Corporation
deps/thrust/thrust/system/cuda/detail/memory.inl:#include <thrust/system/cuda/memory.h>
deps/thrust/thrust/system/cuda/detail/memory.inl:#include <thrust/system/cuda/detail/malloc_and_free.h>
deps/thrust/thrust/system/cuda/detail/memory.inl:namespace cuda_cub
deps/thrust/thrust/system/cuda/detail/memory.inl:  tag cuda_tag;
deps/thrust/thrust/system/cuda/detail/memory.inl:  return pointer<void>(thrust::cuda_cub::malloc(cuda_tag, n));
deps/thrust/thrust/system/cuda/detail/memory.inl:  pointer<void> raw_ptr = thrust::cuda_cub::malloc(sizeof(T) * n);
deps/thrust/thrust/system/cuda/detail/memory.inl:  tag cuda_tag;
deps/thrust/thrust/system/cuda/detail/memory.inl:  return thrust::cuda_cub::free(cuda_tag, ptr.get());
deps/thrust/thrust/system/cuda/detail/memory.inl:} // end cuda_cub
deps/thrust/thrust/system/cuda/detail/uninitialized_fill.h: * Copyright (c) 2016, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/thrust/system/cuda/detail/uninitialized_fill.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/thrust/system/cuda/detail/uninitialized_fill.h: * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/thrust/system/cuda/detail/uninitialized_fill.h:#include <thrust/system/cuda/detail/execution_policy.h>
deps/thrust/thrust/system/cuda/detail/uninitialized_fill.h:#include <thrust/system/cuda/detail/util.h>
deps/thrust/thrust/system/cuda/detail/uninitialized_fill.h:#include <thrust/system/cuda/detail/parallel_for.h>
deps/thrust/thrust/system/cuda/detail/uninitialized_fill.h:namespace cuda_cub {
deps/thrust/thrust/system/cuda/detail/uninitialized_fill.h:#if defined(__CUDA__) && defined(__clang__)
deps/thrust/thrust/system/cuda/detail/uninitialized_fill.h:      // XXX unsafe. cuda-clang is seemingly unable to call ::new in device code
deps/thrust/thrust/system/cuda/detail/uninitialized_fill.h:  cuda_cub::parallel_for(policy,
deps/thrust/thrust/system/cuda/detail/uninitialized_fill.h:  cuda_cub::throw_on_error(
deps/thrust/thrust/system/cuda/detail/uninitialized_fill.h:    cuda_cub::synchronize_optional(policy)
deps/thrust/thrust/system/cuda/detail/uninitialized_fill.h:  cuda_cub::uninitialized_fill_n(policy,
deps/thrust/thrust/system/cuda/detail/uninitialized_fill.h:}    // namespace cuda_cub
deps/thrust/thrust/system/cuda/config.h: * Copyright (c) 2016, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/thrust/system/cuda/config.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/thrust/system/cuda/config.h: * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/thrust/system/cuda/config.h:#if defined(__CUDACC__) || defined(_NVHPC_CUDA)
deps/thrust/thrust/system/cuda/config.h:#  if !defined(__CUDA_ARCH__) || (__CUDA_ARCH__>= 350 && defined(__CUDACC_RDC__))
deps/thrust/thrust/system/cuda/config.h:#    define __THRUST_HAS_CUDART__ 1
deps/thrust/thrust/system/cuda/config.h:#    define __THRUST_HAS_CUDART__ 0
deps/thrust/thrust/system/cuda/config.h:#  define __THRUST_HAS_CUDART__ 0
deps/thrust/thrust/system/cuda/config.h:#ifdef __CUDA_ARCH__
deps/thrust/thrust/system/cuda/config.h:#error The version of CUB in your include path is not compatible with this release of Thrust. CUB is now included in the CUDA Toolkit, so you no longer need to use your own checkout of CUB. Define THRUST_IGNORE_CUB_VERSION_CHECK to ignore this.
deps/thrust/thrust/system/cuda/execution_policy.h: * Copyright (c) 2016, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/thrust/system/cuda/execution_policy.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/thrust/system/cuda/execution_policy.h: * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/thrust/system/cuda/execution_policy.h:#include <thrust/system/cuda/detail/execution_policy.h>
deps/thrust/thrust/system/cuda/execution_policy.h:#include <thrust/system/cuda/detail/par.h>
deps/thrust/thrust/system/cuda/pointer.h: *  Copyright 2008-2020 NVIDIA Corporation
deps/thrust/thrust/system/cuda/pointer.h:/*! \file thrust/system/cuda/memory.h
deps/thrust/thrust/system/cuda/pointer.h:#include <thrust/system/cuda/detail/execution_policy.h>
deps/thrust/thrust/system/cuda/pointer.h:namespace cuda_cub
deps/thrust/thrust/system/cuda/pointer.h:/*! \p cuda::pointer stores a pointer to an object allocated in memory
deps/thrust/thrust/system/cuda/pointer.h: *  accessible by the \p cuda system. This type provides type safety when
deps/thrust/thrust/system/cuda/pointer.h: *  dispatching algorithms on ranges resident in \p cuda memory.
deps/thrust/thrust/system/cuda/pointer.h: *  \p cuda::pointer has pointer semantics: it may be dereferenced and
deps/thrust/thrust/system/cuda/pointer.h: *  \p cuda::pointer can be created with the function \p cuda::malloc, or by
deps/thrust/thrust/system/cuda/pointer.h: *  The raw pointer encapsulated by a \p cuda::pointer may be obtained by eiter
deps/thrust/thrust/system/cuda/pointer.h: *  \note \p cuda::pointer is not a "smart" pointer; it is the programmer's
deps/thrust/thrust/system/cuda/pointer.h: *        responsibility to deallocate memory pointed to by \p cuda::pointer.
deps/thrust/thrust/system/cuda/pointer.h: *  \see cuda::malloc
deps/thrust/thrust/system/cuda/pointer.h: *  \see cuda::free
deps/thrust/thrust/system/cuda/pointer.h:  thrust::cuda_cub::tag,
deps/thrust/thrust/system/cuda/pointer.h:  thrust::tagged_reference<T, thrust::cuda_cub::tag>
deps/thrust/thrust/system/cuda/pointer.h:/*! \p cuda::universal_pointer stores a pointer to an object allocated in
deps/thrust/thrust/system/cuda/pointer.h: *  memory accessible by the \p cuda system and host systems.
deps/thrust/thrust/system/cuda/pointer.h: *  \p cuda::universal_pointer has pointer semantics: it may be dereferenced
deps/thrust/thrust/system/cuda/pointer.h: *  \p cuda::universal_pointer can be created with \p cuda::universal_allocator
deps/thrust/thrust/system/cuda/pointer.h: *  The raw pointer encapsulated by a \p cuda::universal_pointer may be
deps/thrust/thrust/system/cuda/pointer.h: *  \note \p cuda::universal_pointer is not a "smart" pointer; it is the
deps/thrust/thrust/system/cuda/pointer.h: *        \p cuda::universal_pointer.
deps/thrust/thrust/system/cuda/pointer.h: *  \see cuda::universal_allocator
deps/thrust/thrust/system/cuda/pointer.h:  thrust::cuda_cub::tag,
deps/thrust/thrust/system/cuda/pointer.h:/*! \p cuda::reference is a wrapped reference to an object stored in memory
deps/thrust/thrust/system/cuda/pointer.h: *  accessible by the \p cuda system. \p cuda::reference is the type of the
deps/thrust/thrust/system/cuda/pointer.h: *  result of dereferencing a \p cuda::pointer.
deps/thrust/thrust/system/cuda/pointer.h: *  \see cuda::pointer
deps/thrust/thrust/system/cuda/pointer.h:using reference = thrust::tagged_reference<T, thrust::cuda_cub::tag>;
deps/thrust/thrust/system/cuda/pointer.h:} // namespace cuda_cub
deps/thrust/thrust/system/cuda/pointer.h:/*! \namespace thrust::system::cuda
deps/thrust/thrust/system/cuda/pointer.h: *  \brief \p thrust::system::cuda is the namespace containing functionality
deps/thrust/thrust/system/cuda/pointer.h: *  CUDA backend system. The identifiers are provided in a separate namespace
deps/thrust/thrust/system/cuda/pointer.h: *  aliased in the top-level <tt>thrust::cuda</tt> namespace for easy access.
deps/thrust/thrust/system/cuda/pointer.h:namespace system { namespace cuda
deps/thrust/thrust/system/cuda/pointer.h:using thrust::cuda_cub::pointer;
deps/thrust/thrust/system/cuda/pointer.h:using thrust::cuda_cub::universal_pointer;
deps/thrust/thrust/system/cuda/pointer.h:using thrust::cuda_cub::reference;
deps/thrust/thrust/system/cuda/pointer.h:}} // namespace system::cuda
deps/thrust/thrust/system/cuda/pointer.h:/*! \namespace thrust::cuda
deps/thrust/thrust/system/cuda/pointer.h: *  \brief \p thrust::cuda is a top-level alias for \p thrust::system::cuda.
deps/thrust/thrust/system/cuda/pointer.h:namespace cuda
deps/thrust/thrust/system/cuda/pointer.h:using thrust::cuda_cub::pointer;
deps/thrust/thrust/system/cuda/pointer.h:using thrust::cuda_cub::universal_pointer;
deps/thrust/thrust/system/cuda/pointer.h:using thrust::cuda_cub::reference;
deps/thrust/thrust/system/cuda/pointer.h:} // namespace cuda
deps/thrust/thrust/system/cuda/experimental/pinned_allocator.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/cuda/experimental/pinned_allocator.h:/*! \file thrust/system/cuda/experimental/pinned_allocator.h
deps/thrust/thrust/system/cuda/experimental/pinned_allocator.h: *  \brief An allocator which creates new elements in "pinned" memory with \p cudaMallocHost
deps/thrust/thrust/system/cuda/experimental/pinned_allocator.h:#include <thrust/system/cuda/detail/guarded_cuda_runtime_api.h>
deps/thrust/thrust/system/cuda/experimental/pinned_allocator.h:#include <thrust/system/cuda/error.h>
deps/thrust/thrust/system/cuda/experimental/pinned_allocator.h:namespace cuda
deps/thrust/thrust/system/cuda/experimental/pinned_allocator.h:/*! \p pinned_allocator is a CUDA-specific host memory allocator
deps/thrust/thrust/system/cuda/experimental/pinned_allocator.h: *  that employs \c cudaMallocHost for allocation.
deps/thrust/thrust/system/cuda/experimental/pinned_allocator.h:      cudaError_t error = cudaMallocHost(reinterpret_cast<void**>(&result), cnt * sizeof(value_type));
deps/thrust/thrust/system/cuda/experimental/pinned_allocator.h:        cudaGetLastError(); // Clear global CUDA error state.
deps/thrust/thrust/system/cuda/experimental/pinned_allocator.h:      cudaError_t error = cudaFreeHost(p);
deps/thrust/thrust/system/cuda/experimental/pinned_allocator.h:      cudaGetLastError(); // Clear global CUDA error state.
deps/thrust/thrust/system/cuda/experimental/pinned_allocator.h:        cudaGetLastError(); // Clear global CUDA error state.
deps/thrust/thrust/system/cuda/experimental/pinned_allocator.h:        throw thrust::system_error(error, thrust::cuda_category());
deps/thrust/thrust/system/cuda/experimental/pinned_allocator.h:} // end cuda
deps/thrust/thrust/system/cuda/experimental/pinned_allocator.h:// alias cuda's members at top-level
deps/thrust/thrust/system/cuda/experimental/pinned_allocator.h:namespace cuda
deps/thrust/thrust/system/cuda/experimental/pinned_allocator.h:using thrust::system::cuda::experimental::pinned_allocator;
deps/thrust/thrust/system/cuda/experimental/pinned_allocator.h:} // end cuda
deps/thrust/thrust/system/cuda/memory.h: *  Copyright 2008-2018 NVIDIA Corporation
deps/thrust/thrust/system/cuda/memory.h:/*! \file thrust/system/cuda/memory.h
deps/thrust/thrust/system/cuda/memory.h: *  \brief Managing memory associated with Thrust's CUDA system.
deps/thrust/thrust/system/cuda/memory.h:#include <thrust/system/cuda/memory_resource.h>
deps/thrust/thrust/system/cuda/memory.h:namespace cuda_cub
deps/thrust/thrust/system/cuda/memory.h:/*! Allocates an area of memory available to Thrust's <tt>cuda</tt> system.
deps/thrust/thrust/system/cuda/memory.h: *  \return A <tt>cuda::pointer<void></tt> pointing to the beginning of the newly
deps/thrust/thrust/system/cuda/memory.h: *          allocated memory. A null <tt>cuda::pointer<void></tt> is returned if
deps/thrust/thrust/system/cuda/memory.h: *  \note The <tt>cuda::pointer<void></tt> returned by this function must be
deps/thrust/thrust/system/cuda/memory.h: *        deallocated with \p cuda::free.
deps/thrust/thrust/system/cuda/memory.h: *  \see cuda::free
deps/thrust/thrust/system/cuda/memory.h:/*! Allocates a typed area of memory available to Thrust's <tt>cuda</tt> system.
deps/thrust/thrust/system/cuda/memory.h: *  \return A <tt>cuda::pointer<T></tt> pointing to the beginning of the newly
deps/thrust/thrust/system/cuda/memory.h: *          allocated elements. A null <tt>cuda::pointer<T></tt> is returned if
deps/thrust/thrust/system/cuda/memory.h: *  \note The <tt>cuda::pointer<T></tt> returned by this function must be
deps/thrust/thrust/system/cuda/memory.h: *        deallocated with \p cuda::free.
deps/thrust/thrust/system/cuda/memory.h: *  \see cuda::free
deps/thrust/thrust/system/cuda/memory.h:/*! Deallocates an area of memory previously allocated by <tt>cuda::malloc</tt>.
deps/thrust/thrust/system/cuda/memory.h: *  \param ptr A <tt>cuda::pointer<void></tt> pointing to the beginning of an area
deps/thrust/thrust/system/cuda/memory.h: *         of memory previously allocated with <tt>cuda::malloc</tt>.
deps/thrust/thrust/system/cuda/memory.h: *  \see cuda::malloc
deps/thrust/thrust/system/cuda/memory.h:/*! \p cuda::allocator is the default allocator used by the \p cuda system's
deps/thrust/thrust/system/cuda/memory.h: *  containers such as <tt>cuda::vector</tt> if no user-specified allocator is
deps/thrust/thrust/system/cuda/memory.h: *  provided. \p cuda::allocator allocates (deallocates) storage with \p
deps/thrust/thrust/system/cuda/memory.h: *  cuda::malloc (\p cuda::free).
deps/thrust/thrust/system/cuda/memory.h:  T, thrust::system::cuda::memory_resource
deps/thrust/thrust/system/cuda/memory.h:/*! \p cuda::universal_allocator allocates memory that can be used by the \p cuda
deps/thrust/thrust/system/cuda/memory.h:  T, thrust::system::cuda::universal_memory_resource
deps/thrust/thrust/system/cuda/memory.h:} // namespace cuda_cub
deps/thrust/thrust/system/cuda/memory.h:namespace system { namespace cuda
deps/thrust/thrust/system/cuda/memory.h:using thrust::cuda_cub::malloc;
deps/thrust/thrust/system/cuda/memory.h:using thrust::cuda_cub::free;
deps/thrust/thrust/system/cuda/memory.h:using thrust::cuda_cub::allocator;
deps/thrust/thrust/system/cuda/memory.h:using thrust::cuda_cub::universal_allocator;
deps/thrust/thrust/system/cuda/memory.h:}} // namespace system::cuda
deps/thrust/thrust/system/cuda/memory.h:/*! \namespace thrust::cuda
deps/thrust/thrust/system/cuda/memory.h: *  \brief \p thrust::cuda is a top-level alias for \p thrust::system::cuda.
deps/thrust/thrust/system/cuda/memory.h:namespace cuda
deps/thrust/thrust/system/cuda/memory.h:using thrust::cuda_cub::malloc;
deps/thrust/thrust/system/cuda/memory.h:using thrust::cuda_cub::free;
deps/thrust/thrust/system/cuda/memory.h:using thrust::cuda_cub::allocator;
deps/thrust/thrust/system/cuda/memory.h:using thrust::cuda_cub::universal_allocator;
deps/thrust/thrust/system/cuda/memory.h:} // namespace cuda
deps/thrust/thrust/system/cuda/memory.h:#include <thrust/system/cuda/detail/memory.inl>
deps/thrust/thrust/system/cuda/memory_resource.h: *  Copyright 2018-2020 NVIDIA Corporation
deps/thrust/thrust/system/cuda/memory_resource.h:/*! \file cuda/memory_resource.h
deps/thrust/thrust/system/cuda/memory_resource.h: *  \brief Memory resources for the CUDA system.
deps/thrust/thrust/system/cuda/memory_resource.h:#include <thrust/system/cuda/detail/guarded_cuda_runtime_api.h>
deps/thrust/thrust/system/cuda/memory_resource.h:#include <thrust/system/cuda/pointer.h>
deps/thrust/thrust/system/cuda/memory_resource.h:#include <thrust/system/cuda/error.h>
deps/thrust/thrust/system/cuda/memory_resource.h:#include <thrust/system/cuda/detail/util.h>
deps/thrust/thrust/system/cuda/memory_resource.h:namespace cuda
deps/thrust/thrust/system/cuda/memory_resource.h:    typedef cudaError_t (CUDARTAPI *allocation_fn)(void **, std::size_t);
deps/thrust/thrust/system/cuda/memory_resource.h:    typedef cudaError_t (CUDARTAPI *deallocation_fn)(void *);
deps/thrust/thrust/system/cuda/memory_resource.h:    class cuda_memory_resource final : public mr::memory_resource<Pointer>
deps/thrust/thrust/system/cuda/memory_resource.h:            cudaError_t status = Alloc(&ret, bytes);
deps/thrust/thrust/system/cuda/memory_resource.h:            if (status != cudaSuccess)
deps/thrust/thrust/system/cuda/memory_resource.h:                cudaGetLastError(); // Clear the CUDA global error state.
deps/thrust/thrust/system/cuda/memory_resource.h:                throw thrust::system::detail::bad_alloc(thrust::cuda_category().message(status).c_str());
deps/thrust/thrust/system/cuda/memory_resource.h:            cudaError_t status = Dealloc(thrust::detail::pointer_traits<Pointer>::get(p));
deps/thrust/thrust/system/cuda/memory_resource.h:            if (status != cudaSuccess)
deps/thrust/thrust/system/cuda/memory_resource.h:                thrust::cuda_cub::throw_on_error(status, "CUDA free failed");
deps/thrust/thrust/system/cuda/memory_resource.h:    inline cudaError_t CUDARTAPI cudaMallocManaged(void ** ptr, std::size_t bytes)
deps/thrust/thrust/system/cuda/memory_resource.h:        return ::cudaMallocManaged(ptr, bytes, cudaMemAttachGlobal);
deps/thrust/thrust/system/cuda/memory_resource.h:    typedef detail::cuda_memory_resource<cudaMalloc, cudaFree,
deps/thrust/thrust/system/cuda/memory_resource.h:        thrust::cuda::pointer<void> >
deps/thrust/thrust/system/cuda/memory_resource.h:    typedef detail::cuda_memory_resource<detail::cudaMallocManaged, cudaFree,
deps/thrust/thrust/system/cuda/memory_resource.h:        thrust::cuda::universal_pointer<void> >
deps/thrust/thrust/system/cuda/memory_resource.h:    typedef detail::cuda_memory_resource<cudaMallocHost, cudaFreeHost,
deps/thrust/thrust/system/cuda/memory_resource.h:        thrust::cuda::universal_pointer<void> >
deps/thrust/thrust/system/cuda/memory_resource.h:/*! The memory resource for the CUDA system. Uses <tt>cudaMalloc</tt> and wraps
deps/thrust/thrust/system/cuda/memory_resource.h: *  the result with \p cuda::pointer.
deps/thrust/thrust/system/cuda/memory_resource.h:/*! The universal memory resource for the CUDA system. Uses
deps/thrust/thrust/system/cuda/memory_resource.h: *  <tt>cudaMallocManaged</tt> and wraps the result with
deps/thrust/thrust/system/cuda/memory_resource.h: *  \p cuda::universal_pointer.
deps/thrust/thrust/system/cuda/memory_resource.h:/*! The host pinned memory resource for the CUDA system. Uses
deps/thrust/thrust/system/cuda/memory_resource.h: *  <tt>cudaMallocHost</tt> and wraps the result with \p
deps/thrust/thrust/system/cuda/memory_resource.h: *  cuda::universal_pointer.
deps/thrust/thrust/system/cuda/memory_resource.h:} // end cuda
deps/thrust/thrust/system/cuda/memory_resource.h:namespace cuda
deps/thrust/thrust/system/cuda/memory_resource.h:using thrust::system::cuda::memory_resource;
deps/thrust/thrust/system/cuda/memory_resource.h:using thrust::system::cuda::universal_memory_resource;
deps/thrust/thrust/system/cuda/memory_resource.h:using thrust::system::cuda::universal_host_pinned_memory_resource;
deps/thrust/thrust/system/cuda/future.h:// Copyright (c) 2018 NVIDIA Corporation
deps/thrust/thrust/system/cuda/future.h:#include <thrust/system/cuda/pointer.h>
deps/thrust/thrust/system/cuda/future.h:#include <thrust/system/cuda/detail/execution_policy.h>
deps/thrust/thrust/system/cuda/future.h:namespace system { namespace cuda
deps/thrust/thrust/system/cuda/future.h:}} // namespace system::cuda
deps/thrust/thrust/system/cuda/future.h:namespace cuda
deps/thrust/thrust/system/cuda/future.h:using thrust::system::cuda::ready_event;
deps/thrust/thrust/system/cuda/future.h:using thrust::system::cuda::ready_future;
deps/thrust/thrust/system/cuda/future.h:using thrust::system::cuda::unique_eager_event;
deps/thrust/thrust/system/cuda/future.h:using thrust::system::cuda::unique_eager_future;
deps/thrust/thrust/system/cuda/future.h:using thrust::system::cuda::when_all;
deps/thrust/thrust/system/cuda/future.h:} // namespace cuda
deps/thrust/thrust/system/cuda/future.h:thrust::cuda::unique_eager_event
deps/thrust/thrust/system/cuda/future.h:  thrust::cuda::execution_policy<DerivedPolicy> const&
deps/thrust/thrust/system/cuda/future.h:thrust::cuda::unique_eager_future<T>
deps/thrust/thrust/system/cuda/future.h:  thrust::cuda::execution_policy<DerivedPolicy> const&
deps/thrust/thrust/system/cuda/future.h:#include <thrust/system/cuda/detail/future.inl>
deps/thrust/thrust/system/cuda/vector.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/cuda/vector.h:/*! \file thrust/system/cuda/vector.h
deps/thrust/thrust/system/cuda/vector.h: *         Thrust's CUDA system.
deps/thrust/thrust/system/cuda/vector.h:#include <thrust/system/cuda/memory.h>
deps/thrust/thrust/system/cuda/vector.h:namespace cuda_cub
deps/thrust/thrust/system/cuda/vector.h:/*! \p cuda::vector is a container that supports random access to elements,
deps/thrust/thrust/system/cuda/vector.h: *  elements in a \p cuda::vector may vary dynamically; memory management is
deps/thrust/thrust/system/cuda/vector.h: *  automatic. The elements contained in a \p cuda::vector reside in memory
deps/thrust/thrust/system/cuda/vector.h: *  accessible by the \p cuda system.
deps/thrust/thrust/system/cuda/vector.h: *  \tparam T The element type of the \p cuda::vector.
deps/thrust/thrust/system/cuda/vector.h: *  \tparam Allocator The allocator type of the \p cuda::vector.
deps/thrust/thrust/system/cuda/vector.h: *          Defaults to \p cuda::allocator.
deps/thrust/thrust/system/cuda/vector.h: *                   shared by \p cuda::vector
deps/thrust/thrust/system/cuda/vector.h:template <typename T, typename Allocator = thrust::system::cuda::allocator<T>>
deps/thrust/thrust/system/cuda/vector.h:/*! \p cuda::universal_vector is a container that supports random access to
deps/thrust/thrust/system/cuda/vector.h: *  number of elements in a \p cuda::universal_vector may vary dynamically;
deps/thrust/thrust/system/cuda/vector.h: *  \p cuda::universal_vector reside in memory accessible by the \p cuda system
deps/thrust/thrust/system/cuda/vector.h: *  \tparam T The element type of the \p cuda::universal_vector.
deps/thrust/thrust/system/cuda/vector.h: *  \tparam Allocator The allocator type of the \p cuda::universal_vector.
deps/thrust/thrust/system/cuda/vector.h: *          Defaults to \p cuda::universal_allocator.
deps/thrust/thrust/system/cuda/vector.h: *                   shared by \p cuda::universal_vector
deps/thrust/thrust/system/cuda/vector.h:template <typename T, typename Allocator = thrust::system::cuda::universal_allocator<T>>
deps/thrust/thrust/system/cuda/vector.h:} // namespace cuda_cub
deps/thrust/thrust/system/cuda/vector.h:namespace system { namespace cuda
deps/thrust/thrust/system/cuda/vector.h:using thrust::cuda_cub::vector;
deps/thrust/thrust/system/cuda/vector.h:using thrust::cuda_cub::universal_vector;
deps/thrust/thrust/system/cuda/vector.h:namespace cuda
deps/thrust/thrust/system/cuda/vector.h:using thrust::cuda_cub::vector;
deps/thrust/thrust/system/cuda/vector.h:using thrust::cuda_cub::universal_vector;
deps/thrust/thrust/system/cpp/detail/mismatch.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/cpp/detail/swap_ranges.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/cpp/detail/adjacent_difference.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/cpp/detail/gather.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/cpp/detail/generate.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/cpp/detail/partition.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/cpp/detail/uninitialized_copy.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/cpp/detail/transform.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/cpp/detail/tabulate.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/cpp/detail/execution_policy.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/cpp/detail/merge.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/cpp/detail/sequence.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/cpp/detail/assign_value.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/cpp/detail/inner_product.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/cpp/detail/count.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/cpp/detail/reduce.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/cpp/detail/binary_search.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/cpp/detail/copy_if.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/cpp/detail/remove.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/cpp/detail/temporary_buffer.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/cpp/detail/malloc_and_free.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/cpp/detail/replace.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/cpp/detail/find.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/cpp/detail/scan_by_key.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/cpp/detail/iter_swap.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/cpp/detail/for_each.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/cpp/detail/unique_by_key.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/cpp/detail/get_value.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/cpp/detail/transform_scan.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/cpp/detail/transform_reduce.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/cpp/detail/par.h: *  Copyright 2008-2018 NVIDIA Corporation
deps/thrust/thrust/system/cpp/detail/extrema.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/cpp/detail/logical.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/cpp/detail/set_operations.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/cpp/detail/per_device_resource.h: *  Copyright 2018 NVIDIA Corporation
deps/thrust/thrust/system/cpp/detail/equal.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/cpp/detail/unique.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/cpp/detail/sort.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/cpp/detail/reduce_by_key.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/cpp/detail/fill.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/cpp/detail/reverse.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/cpp/detail/vector.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/cpp/detail/scan.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/cpp/detail/copy.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/cpp/detail/scatter.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/cpp/detail/memory.inl: *  Copyright 2008-2018 NVIDIA Corporation
deps/thrust/thrust/system/cpp/detail/uninitialized_fill.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/cpp/execution_policy.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system/cpp/pointer.h: *  Copyright 2008-2020 NVIDIA Corporation
deps/thrust/thrust/system/cpp/memory.h: *  Copyright 2008-2018 NVIDIA Corporation
deps/thrust/thrust/system/cpp/memory_resource.h: *  Copyright 2018-2020 NVIDIA Corporation
deps/thrust/thrust/system/cpp/vector.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/remove.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/distance.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/replace.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/find.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/memory.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/host_vector.h: *  Copyright 2008-2018 NVIDIA Corporation
deps/thrust/thrust/for_each.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/for_each.h: *      // code compiled for a GPU with compute capability 2.0 or
deps/thrust/thrust/for_each.h: *      // code compiled for a GPU with compute capability 2.0 or
deps/thrust/thrust/for_each.h: *      // code compiled for a GPU with compute capability 2.0 or
deps/thrust/thrust/for_each.h: *      // code compiled for a GPU with compute capability 2.0 or
deps/thrust/thrust/universal_vector.h: *  Copyright 2008-2020 NVIDIA Corporation
deps/thrust/thrust/version.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/event.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/device_new_allocator.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system_error.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/system_error.h: *         low-level application program interfaces such as the CUDA runtime.
deps/thrust/thrust/transform_scan.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/future.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/random/uniform_int_distribution.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/random/detail/uniform_real_distribution.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/random/detail/uniform_int_distribution.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/random/detail/subtract_with_carry_engine.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/random/detail/linear_feedback_shift_engine.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/random/detail/random_core_access.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/random/detail/normal_distribution.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/random/detail/linear_congruential_engine_discard.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/random/detail/mod.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/random/detail/xor_combine_engine_max.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/random/detail/discard_block_engine.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/random/detail/xor_combine_engine.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/random/detail/linear_feedback_shift_engine_wordmask.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/random/detail/normal_distribution_base.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/random/detail/normal_distribution_base.h:#if THRUST_DEVICE_COMPILER == THRUST_DEVICE_COMPILER_NVCC && !defined(_NVHPC_CUDA)
deps/thrust/thrust/random/detail/linear_congruential_engine.inl: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/random/normal_distribution.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/random/linear_congruential_engine.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/random/xor_combine_engine.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/random/uniform_real_distribution.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/random/discard_block_engine.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/random/subtract_with_carry_engine.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/random/linear_feedback_shift_engine.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/transform_reduce.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/universal_ptr.h: *  Copyright 2008-2020 NVIDIA Corporation
deps/thrust/thrust/device_vector.h: *  Copyright 2008-2018 NVIDIA Corporation
deps/thrust/thrust/extrema.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/async/transform.h: *  Copyright 2008-2018 NVIDIA Corporation
deps/thrust/thrust/async/reduce.h: *  Copyright 2008-2018 NVIDIA Corporation
deps/thrust/thrust/async/for_each.h: *  Copyright 2008-2018 NVIDIA Corporation
deps/thrust/thrust/async/sort.h: *  Copyright 2008-2018 NVIDIA Corporation
deps/thrust/thrust/async/scan.h: *  Copyright 2008-2020 NVIDIA Corporation
deps/thrust/thrust/async/copy.h: *  Copyright 2008-2018 NVIDIA Corporation
deps/thrust/thrust/logical.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/type_traits/logical_metafunctions.h://  Copyright (c)      2018 NVIDIA Corporation
deps/thrust/thrust/type_traits/is_trivially_relocatable.h://  Copyright (c)      2018 NVIDIA Corporation
deps/thrust/thrust/type_traits/is_trivially_relocatable.h:#if THRUST_DEVICE_SYSTEM == THRUST_DEVICE_SYSTEM_CUDA
deps/thrust/thrust/type_traits/is_trivially_relocatable.h:#include <thrust/system/cuda/detail/guarded_cuda_runtime_api.h>
deps/thrust/thrust/type_traits/void_t.h: *  Copyright 2018 NVIDIA Corporation
deps/thrust/thrust/type_traits/is_operator_plus_function_object.h: *  Copyright 2008-2018 NVIDIA Corporation
deps/thrust/thrust/type_traits/integer_sequence.h://  Copyright (c)      2018 NVIDIA Corporation
deps/thrust/thrust/type_traits/is_operator_less_or_greater_function_object.h: *  Copyright 2008-2018 NVIDIA Corporation
deps/thrust/thrust/type_traits/remove_cvref.h: *  Copyright 2018 NVIDIA Corporation
deps/thrust/thrust/type_traits/is_execution_policy.h: *  Copyright 2018 NVIDIA Corporation
deps/thrust/thrust/type_traits/is_contiguous_iterator.h: *  Copyright 2008-2018 NVIDIA Corporation
deps/thrust/thrust/cmake/thrust-config.cmake:# Provided by NVIDIA under the same license as the associated Thrust library.
deps/thrust/thrust/cmake/thrust-config.cmake:# # Create default target with: HOST=CPP DEVICE=CUDA
deps/thrust/thrust/cmake/thrust-config.cmake:# thrust_create_target(ThrustWithMyCUB DEVICE CUDA)
deps/thrust/thrust/cmake/thrust-config.cmake:# # Create target with HOST=CPP DEVICE=CUDA and some advanced flags set
deps/thrust/thrust/cmake/thrust-config.cmake:# thrust_is_cuda_system_found(<var_name>)
deps/thrust/thrust/cmake/thrust-config.cmake:  CUDA CPP OMP TBB
deps/thrust/thrust/cmake/thrust-config.cmake:  _thrust_set_if_undefined(TCT_DEVICE CUDA)
deps/thrust/thrust/cmake/thrust-config.cmake:    if (("${TCT_HOST}" STREQUAL "CUDA" OR "${TCT_DEVICE}" STREQUAL "CUDA") AND
deps/thrust/thrust/cmake/thrust-config.cmake:        "CUB is now included in the CUDA Toolkit, so you no longer need to use your own checkout of CUB. "
deps/thrust/thrust/cmake/thrust-config.cmake:function(thrust_is_cuda_system_found var_name)
deps/thrust/thrust/cmake/thrust-config.cmake:  thrust_is_system_found(CUDA ${var_name})
deps/thrust/thrust/cmake/thrust-config.cmake:  thrust_is_system_found(CUDA THRUST_CUDA_FOUND)
deps/thrust/thrust/cmake/thrust-config.cmake:  _thrust_debug_backend_targets(CUDA "CUB ${THRUST_CUB_VERSION}")
deps/thrust/thrust/cmake/thrust-config.cmake:  # 3) nvcc will automatically check the CUDA Toolkit include path *before* the
deps/thrust/thrust/cmake/thrust-config.cmake:# Use the provided cub_target for the CUDA backend. If Thrust::CUDA already
deps/thrust/thrust/cmake/thrust-config.cmake:  if (NOT TARGET Thrust::CUDA)
deps/thrust/thrust/cmake/thrust-config.cmake:    _thrust_declare_interface_alias(Thrust::CUDA _Thrust_CUDA)
deps/thrust/thrust/cmake/thrust-config.cmake:    target_link_libraries(_Thrust_CUDA INTERFACE Thrust::Thrust ${cub_target})
deps/thrust/thrust/cmake/thrust-config.cmake:    thrust_debug_target(Thrust::CUDA "CUB ${THRUST_CUB_VERSION}" internal)
deps/thrust/thrust/cmake/thrust-config.cmake:    _thrust_setup_system(CUDA)
deps/thrust/thrust/cmake/thrust-config.cmake:macro(_thrust_find_CUDA required)
deps/thrust/thrust/cmake/thrust-config.cmake:  if (NOT TARGET Thrust::CUDA)
deps/thrust/thrust/cmake/thrust-config.cmake:# Wrap the OpenMP flags for CUDA targets
deps/thrust/thrust/cmake/thrust-config.cmake:      $<$<AND:$<COMPILE_LANGUAGE:CUDA>,$<CUDA_COMPILER_ID:NVIDIA>>:-Xcompiler=${CMAKE_MATCH_1}>
deps/thrust/thrust/cmake/thrust-config.cmake:  elseif ("${backend}" STREQUAL "CUDA")
deps/thrust/thrust/cmake/thrust-config.cmake:    _thrust_find_CUDA("${required}")
deps/thrust/thrust/cmake/README.md:#### Default Configuration (CUDA)
deps/thrust/thrust/cmake/README.md:`thrust_create_target` will configure its result to use CUDA acceleration.
deps/thrust/thrust/cmake/README.md:target will default to using CPP for host and/or CUDA for device.
deps/thrust/thrust/cmake/README.md:| `DEVICE`            | `CUDA`                  | Default device system           |
deps/thrust/thrust/cmake/README.md:thrust_is_cuda_system_found(<var_name>)
deps/thrust/thrust/cmake/README.md:# Generic version that takes a component name from CUDA, CPP, TBB, OMP:
deps/thrust/thrust/cmake/README.md:Each backend system (`CPP`, `CUDA`, `TBB`, `OMP`) is described by multiple
deps/thrust/thrust/cmake/README.md:  - For example, the `Thrust::CUDA` target is an interface
deps/thrust/thrust/set_operations.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/per_device_resource.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/equal.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/unique.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/sort.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/allocate_unique.h:// Copyright (c) 2018 NVIDIA Corporation
deps/thrust/thrust/fill.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/reverse.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/device_malloc.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/scan.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/functional.h: *  Copyright 2008-2018 NVIDIA Corporation
deps/thrust/thrust/copy.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/scatter.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/thrust/uninitialized_fill.h: *  Copyright 2008-2013 NVIDIA Corporation
deps/thrust/CMakeLists.txt:# 3.18.3 for C++17 + CUDA
deps/thrust/CMakeLists.txt:# Remove this when we use the new CUDA_ARCHITECTURES properties with both
deps/thrust/CMakeLists.txt:option(THRUST_INCLUDE_CUB_CMAKE "Build CUB tests and examples. (Requires CUDA)." "OFF")
deps/thrust/CMakeLists.txt:# bench.cu from CMake for NVIDIA's internal builds.
deps/thrust/CMakeLists.txt:# need the install rules. See GH issue NVIDIA/thrust#1211.
deps/thrust/CMakeLists.txt:if (THRUST_CUDA_FOUND)
deps/thrust/CMakeLists.txt:  include(cmake/ThrustCudaConfig.cmake)
deps/thrust/CMakeLists.txt:message(STATUS "CUDA system found? ${THRUST_CUDA_FOUND}")
deps/thrust/CMakeLists.txt:if (THRUST_INCLUDE_CUB_CMAKE AND THRUST_CUDA_FOUND)
deps/thrust/dependencies/cub/tune/tune_device_reduce.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/tune/tune_device_reduce.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/tune/tune_device_reduce.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/tune/tune_device_reduce.cu:// Ensure printing of CUDA runtime errors to console
deps/thrust/dependencies/cub/tune/tune_device_reduce.cu:        if (g_verify) CubDebugExit(cudaMemset(d_out, 0, sizeof(T)));
deps/thrust/dependencies/cub/tune/tune_device_reduce.cu:        if (g_verify) CubDebugExit(cudaDeviceSynchronize());
deps/thrust/dependencies/cub/tune/tune_device_reduce.cu:        GpuTimer gpu_timer;
deps/thrust/dependencies/cub/tune/tune_device_reduce.cu:            gpu_timer.Start();
deps/thrust/dependencies/cub/tune/tune_device_reduce.cu:            gpu_timer.Stop();
deps/thrust/dependencies/cub/tune/tune_device_reduce.cu:            elapsed_millis += gpu_timer.ElapsedMillis();
deps/thrust/dependencies/cub/tune/tune_device_reduce.cu:        CubDebugExit(cudaDeviceSynchronize());
deps/thrust/dependencies/cub/tune/tune_device_reduce.cu:    CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(T) * g_max_items, cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/LICENSE.TXT:Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/LICENSE.TXT:   *  Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/LICENSE.TXT:DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/experimental/histogram_compare.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/experimental/histogram_compare.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/experimental/histogram_compare.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/experimental/histogram_compare.cu:// Ensure printing of CUDA runtime errors to console
deps/thrust/dependencies/cub/experimental/histogram_compare.cu:    // Copy data to gpu
deps/thrust/dependencies/cub/experimental/histogram_compare.cu:    CubDebugExit(cudaMemcpy(d_pixels, h_pixels, pixel_bytes, cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/experimental/histogram_compare.cu:    // Allocate results arrays on cpu/gpu
deps/thrust/dependencies/cub/experimental/histogram_compare.cu:    // Get GPU device bandwidth (GB/s)
deps/thrust/dependencies/cub/experimental/histogram_compare.cu:    CubDebugExit(cudaGetDevice(&device_ordinal));
deps/thrust/dependencies/cub/experimental/histogram_compare.cu:    CubDebugExit(cudaDeviceGetAttribute(&bus_width, cudaDevAttrGlobalMemoryBusWidth, device_ordinal));
deps/thrust/dependencies/cub/experimental/histogram_compare.cu:    CubDebugExit(cudaDeviceGetAttribute(&mem_clock_khz, cudaDevAttrMemoryClockRate, device_ordinal));
deps/thrust/dependencies/cub/experimental/histogram_compare.cu:    CubDebugExit(cudaDeviceSynchronize());
deps/thrust/dependencies/cub/experimental/histogram/histogram_cub.h: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/experimental/histogram/histogram_cub.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/experimental/histogram/histogram_cub.h: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/experimental/histogram/histogram_cub.h:        (cudaStream_t) 0,
deps/thrust/dependencies/cub/experimental/histogram/histogram_cub.h:    cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/experimental/histogram/histogram_cub.h:    GpuTimer gpu_timer;
deps/thrust/dependencies/cub/experimental/histogram/histogram_cub.h:    gpu_timer.Start();
deps/thrust/dependencies/cub/experimental/histogram/histogram_cub.h:        (cudaStream_t) 0,
deps/thrust/dependencies/cub/experimental/histogram/histogram_cub.h:    gpu_timer.Stop();
deps/thrust/dependencies/cub/experimental/histogram/histogram_cub.h:    float elapsed_millis = gpu_timer.ElapsedMillis();
deps/thrust/dependencies/cub/experimental/histogram/histogram_cub.h:    cudaFree(d_temp_storage);
deps/thrust/dependencies/cub/experimental/histogram/histogram_gmem_atomics.h: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/experimental/histogram/histogram_gmem_atomics.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/experimental/histogram/histogram_gmem_atomics.h: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/experimental/histogram/histogram_gmem_atomics.h:    cudaDeviceProp props;
deps/thrust/dependencies/cub/experimental/histogram/histogram_gmem_atomics.h:    cudaGetDeviceProperties(&props, 0);
deps/thrust/dependencies/cub/experimental/histogram/histogram_gmem_atomics.h:    cudaMalloc(&d_part_hist, total_blocks * NUM_PARTS * sizeof(unsigned int));
deps/thrust/dependencies/cub/experimental/histogram/histogram_gmem_atomics.h:    GpuTimer gpu_timer;
deps/thrust/dependencies/cub/experimental/histogram/histogram_gmem_atomics.h:    gpu_timer.Start();
deps/thrust/dependencies/cub/experimental/histogram/histogram_gmem_atomics.h:    gpu_timer.Stop();
deps/thrust/dependencies/cub/experimental/histogram/histogram_gmem_atomics.h:    float elapsed_millis = gpu_timer.ElapsedMillis();
deps/thrust/dependencies/cub/experimental/histogram/histogram_gmem_atomics.h:    cudaFree(d_part_hist);
deps/thrust/dependencies/cub/experimental/histogram/histogram_smem_atomics.h: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/experimental/histogram/histogram_smem_atomics.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/experimental/histogram/histogram_smem_atomics.h: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/experimental/histogram/histogram_smem_atomics.h:    cudaDeviceProp props;
deps/thrust/dependencies/cub/experimental/histogram/histogram_smem_atomics.h:    cudaGetDeviceProperties(&props, 0);
deps/thrust/dependencies/cub/experimental/histogram/histogram_smem_atomics.h:    cudaMalloc(&d_part_hist, total_blocks * NUM_PARTS * sizeof(unsigned int));
deps/thrust/dependencies/cub/experimental/histogram/histogram_smem_atomics.h:    GpuTimer gpu_timer;
deps/thrust/dependencies/cub/experimental/histogram/histogram_smem_atomics.h:    gpu_timer.Start();
deps/thrust/dependencies/cub/experimental/histogram/histogram_smem_atomics.h:    gpu_timer.Stop();
deps/thrust/dependencies/cub/experimental/histogram/histogram_smem_atomics.h:    float elapsed_millis = gpu_timer.ElapsedMillis();
deps/thrust/dependencies/cub/experimental/histogram/histogram_smem_atomics.h:    cudaFree(d_part_hist);
deps/thrust/dependencies/cub/experimental/defunct/test_device_seg_reduce.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/experimental/defunct/test_device_seg_reduce.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/experimental/defunct/test_device_seg_reduce.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/experimental/defunct/test_device_seg_reduce.cu:// Ensure printing of CUDA runtime errors to console
deps/thrust/dependencies/cub/experimental/defunct/test_device_seg_reduce.cu: * \brief BlockSegReduceTiles implements a stateful abstraction of CUDA thread blocks for participating in device-wide segmented reduction.
deps/thrust/dependencies/cub/experimental/defunct/test_device_seg_reduce.cu: * \brief BlockSegReduceRegionByKey implements a stateful abstraction of CUDA thread blocks for participating in device-wide reduce-value-by-key.
deps/thrust/dependencies/cub/experimental/defunct/test_device_seg_reduce.cu:    static cudaError_t Dispatch(
deps/thrust/dependencies/cub/experimental/defunct/test_device_seg_reduce.cu:        cudaStream_t                    stream,                                 ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/experimental/defunct/test_device_seg_reduce.cu:        return CubDebug(cudaErrorNotSupported );
deps/thrust/dependencies/cub/experimental/defunct/test_device_seg_reduce.cu:        cudaError error = cudaSuccess;
deps/thrust/dependencies/cub/experimental/defunct/test_device_seg_reduce.cu:            if (CubDebug(error = cudaGetDevice(&device_ordinal))) break;
deps/thrust/dependencies/cub/experimental/defunct/test_device_seg_reduce.cu:            if (CubDebug(error = cudaDeviceGetAttribute (&sm_count, cudaDevAttrMultiProcessorCount, device_ordinal))) break;
deps/thrust/dependencies/cub/experimental/defunct/test_device_seg_reduce.cu:                return cudaSuccess;
deps/thrust/dependencies/cub/experimental/defunct/test_device_seg_reduce.cu:            if (CubDebug(error = cudaDeviceSetSharedMemConfig(cudaSharedMemBankSizeEightByte))) break;
deps/thrust/dependencies/cub/experimental/defunct/test_device_seg_reduce.cu:    static cudaError_t Dispatch(
deps/thrust/dependencies/cub/experimental/defunct/test_device_seg_reduce.cu:        cudaStream_t                    stream,                                 ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/experimental/defunct/test_device_seg_reduce.cu:        cudaError error = cudaSuccess;
deps/thrust/dependencies/cub/experimental/defunct/test_device_seg_reduce.cu:    static cudaError_t Reduce(
deps/thrust/dependencies/cub/experimental/defunct/test_device_seg_reduce.cu:        cudaStream_t            stream              = 0,                ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/experimental/defunct/test_device_seg_reduce.cu:    static cudaError_t Sum(
deps/thrust/dependencies/cub/experimental/defunct/test_device_seg_reduce.cu:        cudaStream_t            stream              = 0,                ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/experimental/defunct/test_device_seg_reduce.cu:    CubDebugExit(cudaMemcpy(d_values, h_values, sizeof(Value) * num_values, cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/experimental/defunct/test_device_seg_reduce.cu:    CubDebugExit(cudaMemcpy(d_segment_offsets, h_segment_offsets, sizeof(OffsetT) * (num_segments + 1), cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/experimental/defunct/test_device_seg_reduce.cu:    CubDebugExit(cudaMemset(d_output, 0, sizeof(Value) * num_segments));
deps/thrust/dependencies/cub/experimental/defunct/test_device_seg_reduce.cu:    GpuTimer gpu_timer;
deps/thrust/dependencies/cub/experimental/defunct/test_device_seg_reduce.cu:    gpu_timer.Start();
deps/thrust/dependencies/cub/experimental/defunct/test_device_seg_reduce.cu:    gpu_timer.Stop();
deps/thrust/dependencies/cub/experimental/defunct/test_device_seg_reduce.cu:    float elapsed_millis = gpu_timer.ElapsedMillis();
deps/thrust/dependencies/cub/experimental/defunct/example_coo_spmv.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/experimental/defunct/example_coo_spmv.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/experimental/defunct/example_coo_spmv.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/experimental/defunct/example_coo_spmv.cu:// Ensure printing of CUDA runtime errors to console
deps/thrust/dependencies/cub/experimental/defunct/example_coo_spmv.cu:    // Texture type to actually use (e.g., because CUDA doesn't load doubles as texture items)
deps/thrust/dependencies/cub/experimental/defunct/example_coo_spmv.cu:    typedef texture<CastType, cudaTextureType1D, cudaReadModeElementType> TexRef;
deps/thrust/dependencies/cub/experimental/defunct/example_coo_spmv.cu:        cudaChannelFormatDesc tex_desc = cudaCreateChannelDesc<CastType>();
deps/thrust/dependencies/cub/experimental/defunct/example_coo_spmv.cu:            CubDebugExit(cudaBindTexture(&offset, ref, d_in, tex_desc, bytes));
deps/thrust/dependencies/cub/experimental/defunct/example_coo_spmv.cu:        CubDebugExit(cudaUnbindTexture(ref));
deps/thrust/dependencies/cub/experimental/defunct/example_coo_spmv.cu:    // Get CUDA properties
deps/thrust/dependencies/cub/experimental/defunct/example_coo_spmv.cu:    CubDebugExit(cudaMemcpy(d_rows,     h_rows,     sizeof(VertexId) * num_edges,       cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/experimental/defunct/example_coo_spmv.cu:    CubDebugExit(cudaMemcpy(d_columns,  h_columns,  sizeof(VertexId) * num_edges,       cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/experimental/defunct/example_coo_spmv.cu:    CubDebugExit(cudaMemcpy(d_values,   h_values,   sizeof(Value) * num_edges,          cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/experimental/defunct/example_coo_spmv.cu:    CubDebugExit(cudaMemcpy(d_vector,   h_vector,   sizeof(Value) * coo_graph.col_dim,  cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/experimental/defunct/example_coo_spmv.cu:    CubDebugExit(cudaDeviceSetSharedMemConfig(cudaSharedMemBankSizeEightByte));
deps/thrust/dependencies/cub/experimental/defunct/example_coo_spmv.cu:    GpuTimer gpu_timer;
deps/thrust/dependencies/cub/experimental/defunct/example_coo_spmv.cu:        gpu_timer.Start();
deps/thrust/dependencies/cub/experimental/defunct/example_coo_spmv.cu:        CubDebugExit(cudaMemset(d_result, 0, coo_graph.row_dim * sizeof(Value)));
deps/thrust/dependencies/cub/experimental/defunct/example_coo_spmv.cu:        gpu_timer.Stop();
deps/thrust/dependencies/cub/experimental/defunct/example_coo_spmv.cu:            elapsed_millis += gpu_timer.ElapsedMillis();
deps/thrust/dependencies/cub/experimental/defunct/example_coo_spmv.cu:    CubDebugExit(cudaCudaSynchronize());
deps/thrust/dependencies/cub/experimental/defunct/example_coo_spmv.cu:    // Run GPU version
deps/thrust/dependencies/cub/experimental/spmv_compare.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/experimental/spmv_compare.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/experimental/spmv_compare.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/experimental/spmv_compare.cu:// Ensure printing of CUDA runtime errors to console
deps/thrust/dependencies/cub/experimental/spmv_compare.cu:// GPU I/O proxy
deps/thrust/dependencies/cub/experimental/spmv_compare.cu: * Run GPU I/O proxy
deps/thrust/dependencies/cub/experimental/spmv_compare.cu:float TestGpuCsrIoProxy(
deps/thrust/dependencies/cub/experimental/spmv_compare.cu:    CubDebugExit(cudaGetDevice(&device_ordinal));
deps/thrust/dependencies/cub/experimental/spmv_compare.cu:    CubDebugExit(cudaPeekAtLastError());
deps/thrust/dependencies/cub/experimental/spmv_compare.cu:    GpuTimer timer;
deps/thrust/dependencies/cub/experimental/spmv_compare.cu:    cudaDeviceSynchronize();
deps/thrust/dependencies/cub/experimental/spmv_compare.cu:    CubDebugExit(cudaMemcpy(params.d_vector_y, vector_y_in, sizeof(float) * params.num_rows, cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/experimental/spmv_compare.cu:    GpuTimer timer;
deps/thrust/dependencies/cub/experimental/spmv_compare.cu:    cudaDeviceSynchronize();
deps/thrust/dependencies/cub/experimental/spmv_compare.cu:    CubDebugExit(cudaMemcpy(params.d_vector_y, vector_y_in, sizeof(float) * params.num_rows, cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/experimental/spmv_compare.cu:    GpuTimer timer;
deps/thrust/dependencies/cub/experimental/spmv_compare.cu:    CubDebugExit(cudaMemcpy(params.d_vector_y, vector_y_in, sizeof(float) * params.num_rows, cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/experimental/spmv_compare.cu:    GpuTimer timer;
deps/thrust/dependencies/cub/experimental/spmv_compare.cu:    CubDebugExit(cudaMemcpy(params.d_vector_y, vector_y_in, sizeof(float) * params.num_rows, cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/experimental/spmv_compare.cu:    GpuTimer timer;
deps/thrust/dependencies/cub/experimental/spmv_compare.cu:// GPU Merge-based SpMV
deps/thrust/dependencies/cub/experimental/spmv_compare.cu:float TestGpuMergeCsrmv(
deps/thrust/dependencies/cub/experimental/spmv_compare.cu:        (cudaStream_t) 0, false));
deps/thrust/dependencies/cub/experimental/spmv_compare.cu:    CubDebugExit(cudaMemcpy(params.d_vector_y, vector_y_in, sizeof(ValueT) * params.num_rows, cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/experimental/spmv_compare.cu:        (cudaStream_t) 0, !g_quiet));
deps/thrust/dependencies/cub/experimental/spmv_compare.cu:    GpuTimer timer;
deps/thrust/dependencies/cub/experimental/spmv_compare.cu:            (cudaStream_t) 0, false));
deps/thrust/dependencies/cub/experimental/spmv_compare.cu:    // Get GPU device bandwidth (GB/s)
deps/thrust/dependencies/cub/experimental/spmv_compare.cu:    // Allocate and initialize GPU problem
deps/thrust/dependencies/cub/experimental/spmv_compare.cu:    CubDebugExit(cudaMemcpy(params.d_values,            csr_matrix.values,          sizeof(ValueT) * csr_matrix.num_nonzeros, cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/experimental/spmv_compare.cu:    CubDebugExit(cudaMemcpy(params.d_row_end_offsets,   csr_matrix.row_offsets,     sizeof(OffsetT) * (csr_matrix.num_rows + 1), cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/experimental/spmv_compare.cu:    CubDebugExit(cudaMemcpy(params.d_column_indices,    csr_matrix.column_indices,  sizeof(OffsetT) * csr_matrix.num_nonzeros, cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/experimental/spmv_compare.cu:    CubDebugExit(cudaMemcpy(params.d_vector_x,          vector_x,                   sizeof(ValueT) * csr_matrix.num_cols, cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/experimental/spmv_compare.cu:    printf("GPU CSR I/O Prox, "); fflush(stdout);
deps/thrust/dependencies/cub/experimental/spmv_compare.cu:    avg_millis = TestGpuCsrIoProxy(params, timing_iterations);
deps/thrust/dependencies/cub/experimental/spmv_compare.cu:        avg_millis = TestGpuMergeCsrmv(vector_y_in, vector_y_out, params, timing_iterations);
deps/thrust/dependencies/cub/experimental/spmv_compare.cu:    CubDebugExit(cudaDeviceSynchronize());
deps/thrust/dependencies/cub/experimental/sparse_matrix.h: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/experimental/sparse_matrix.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/experimental/sparse_matrix.h: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/test/test_device_select_if.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/test/test_device_select_if.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/test/test_device_select_if.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/test/test_device_select_if.cu:// Ensure printing of CUDA runtime errors to console
deps/thrust/dependencies/cub/test/test_device_select_if.cu:    CDP,        // GPU-based (dynamic parallelism) dispatch to CUB method
deps/thrust/dependencies/cub/test/test_device_select_if.cu:cudaError_t Dispatch(
deps/thrust/dependencies/cub/test/test_device_select_if.cu:    cudaError_t*                /*d_cdp_error*/,
deps/thrust/dependencies/cub/test/test_device_select_if.cu:    cudaStream_t                stream,
deps/thrust/dependencies/cub/test/test_device_select_if.cu:    cudaError_t error = cudaSuccess;
deps/thrust/dependencies/cub/test/test_device_select_if.cu:cudaError_t Dispatch(
deps/thrust/dependencies/cub/test/test_device_select_if.cu:    cudaError_t*                /*d_cdp_error*/,
deps/thrust/dependencies/cub/test/test_device_select_if.cu:    cudaStream_t                stream,
deps/thrust/dependencies/cub/test/test_device_select_if.cu:    cudaError_t error = cudaSuccess;
deps/thrust/dependencies/cub/test/test_device_select_if.cu:cudaError_t Dispatch(
deps/thrust/dependencies/cub/test/test_device_select_if.cu:    cudaError_t*                /*d_cdp_error*/,
deps/thrust/dependencies/cub/test/test_device_select_if.cu:    cudaStream_t                stream,
deps/thrust/dependencies/cub/test/test_device_select_if.cu:    cudaError_t error = cudaSuccess;
deps/thrust/dependencies/cub/test/test_device_select_if.cu:cudaError_t Dispatch(
deps/thrust/dependencies/cub/test/test_device_select_if.cu:    cudaError_t*                /*d_cdp_error*/,
deps/thrust/dependencies/cub/test/test_device_select_if.cu:    cudaStream_t                stream,
deps/thrust/dependencies/cub/test/test_device_select_if.cu:    cudaError_t error = cudaSuccess;
deps/thrust/dependencies/cub/test/test_device_select_if.cu:// CUDA Nested Parallelism Test Kernel
deps/thrust/dependencies/cub/test/test_device_select_if.cu:    cudaError_t*                d_cdp_error,
deps/thrust/dependencies/cub/test/test_device_select_if.cu:    *d_cdp_error = cudaErrorNotSupported;
deps/thrust/dependencies/cub/test/test_device_select_if.cu:cudaError_t Dispatch(
deps/thrust/dependencies/cub/test/test_device_select_if.cu:    cudaError_t*                d_cdp_error,
deps/thrust/dependencies/cub/test/test_device_select_if.cu:    cudaStream_t                stream,
deps/thrust/dependencies/cub/test/test_device_select_if.cu:    CubDebugExit(cudaMemcpy(&temp_storage_bytes, d_temp_storage_bytes, sizeof(size_t) * 1, cudaMemcpyDeviceToHost));
deps/thrust/dependencies/cub/test/test_device_select_if.cu:    cudaError_t retval;
deps/thrust/dependencies/cub/test/test_device_select_if.cu:    CubDebugExit(cudaMemcpy(&retval, d_cdp_error, sizeof(cudaError_t) * 1, cudaMemcpyDeviceToHost));
deps/thrust/dependencies/cub/test/test_device_select_if.cu:    cudaError_t*    d_cdp_error = NULL;
deps/thrust/dependencies/cub/test/test_device_select_if.cu:    CubDebugExit(g_allocator.DeviceAllocate((void**)&d_cdp_error,           sizeof(cudaError_t) * 1));
deps/thrust/dependencies/cub/test/test_device_select_if.cu:    CubDebugExit(cudaMemcpy(d_flags, h_flags, sizeof(FlagT) * num_items, cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/test/test_device_select_if.cu:    CubDebugExit(cudaMemset(d_out, 0, sizeof(T) * num_items));
deps/thrust/dependencies/cub/test/test_device_select_if.cu:    CubDebugExit(cudaMemset(d_num_selected_out, 0, sizeof(int)));
deps/thrust/dependencies/cub/test/test_device_select_if.cu:    GpuTimer gpu_timer;
deps/thrust/dependencies/cub/test/test_device_select_if.cu:    gpu_timer.Start();
deps/thrust/dependencies/cub/test/test_device_select_if.cu:    gpu_timer.Stop();
deps/thrust/dependencies/cub/test/test_device_select_if.cu:    float elapsed_millis = gpu_timer.ElapsedMillis();
deps/thrust/dependencies/cub/test/test_device_select_if.cu:    CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(T) * num_items, cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/test/test_warp_merge_sort.cu: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/test/test_warp_merge_sort.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/test/test_warp_merge_sort.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/test/test_warp_merge_sort.cu:// Ensure printing of CUDA runtime errors to console
deps/thrust/dependencies/cub/test/test_warp_merge_sort.cu:  CubDebugExit(cudaPeekAtLastError());
deps/thrust/dependencies/cub/test/test_warp_merge_sort.cu:  CubDebugExit(cudaDeviceSynchronize());
deps/thrust/dependencies/cub/test/test_warp_merge_sort.cu:  CubDebugExit(cudaPeekAtLastError());
deps/thrust/dependencies/cub/test/test_warp_merge_sort.cu:  CubDebugExit(cudaDeviceSynchronize());
deps/thrust/dependencies/cub/test/test_temporary_storage_layout.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/test/test_temporary_storage_layout.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/test/test_temporary_storage_layout.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/test/test_temporary_storage_layout.cu:// Ensure printing of CUDA runtime errors to console
deps/thrust/dependencies/cub/test/test_util.h: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/test/test_util.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/test/test_util.h: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/test/test_util.h:    cudaDeviceProp              deviceProp;
deps/thrust/dependencies/cub/test/test_util.h:    cudaError_t DeviceInit(int dev = -1)
deps/thrust/dependencies/cub/test/test_util.h:        cudaError_t error = cudaSuccess;
deps/thrust/dependencies/cub/test/test_util.h:            error = CubDebug(cudaGetDeviceCount(&deviceCount));
deps/thrust/dependencies/cub/test/test_util.h:                fprintf(stderr, "No devices supporting CUDA.\n");
deps/thrust/dependencies/cub/test/test_util.h:            error = CubDebug(cudaSetDevice(dev));
deps/thrust/dependencies/cub/test/test_util.h:            CubDebugExit(cudaMemGetInfo(&device_free_physmem, &device_total_physmem));
deps/thrust/dependencies/cub/test/test_util.h:            error = CubDebug(cudaGetDeviceProperties(&deviceProp, dev));
deps/thrust/dependencies/cub/test/test_util.h:                fprintf(stderr, "Device does not support CUDA.\n");
deps/thrust/dependencies/cub/test/test_util.h:    CubDebugExit(cudaGetDevice(&device));
deps/thrust/dependencies/cub/test/test_util.h:    CubDebugExit(cudaMemGetInfo(&free_mem, &total_mem));
deps/thrust/dependencies/cub/test/test_util.h: * Comparison and ostream operators for CUDA vector types
deps/thrust/dependencies/cub/test/test_util.h:    cudaMemcpy(h_data, d_data, sizeof(T) * num_items, cudaMemcpyDeviceToHost);
deps/thrust/dependencies/cub/test/test_util.h:    cudaMemcpy(h_reference, d_reference, sizeof(T) * num_items, cudaMemcpyDeviceToHost);
deps/thrust/dependencies/cub/test/test_util.h:    cudaMemcpy(h_data, d_data, sizeof(T) * num_items, cudaMemcpyDeviceToHost);
deps/thrust/dependencies/cub/test/test_util.h:    cudaMemcpy(h_data, d_data, sizeof(T) * num_items, cudaMemcpyDeviceToHost);
deps/thrust/dependencies/cub/test/test_util.h:struct GpuTimer
deps/thrust/dependencies/cub/test/test_util.h:    cudaEvent_t start;
deps/thrust/dependencies/cub/test/test_util.h:    cudaEvent_t stop;
deps/thrust/dependencies/cub/test/test_util.h:    GpuTimer()
deps/thrust/dependencies/cub/test/test_util.h:        cudaEventCreate(&start);
deps/thrust/dependencies/cub/test/test_util.h:        cudaEventCreate(&stop);
deps/thrust/dependencies/cub/test/test_util.h:    ~GpuTimer()
deps/thrust/dependencies/cub/test/test_util.h:        cudaEventDestroy(start);
deps/thrust/dependencies/cub/test/test_util.h:        cudaEventDestroy(stop);
deps/thrust/dependencies/cub/test/test_util.h:        cudaEventRecord(start, 0);
deps/thrust/dependencies/cub/test/test_util.h:        cudaEventRecord(stop, 0);
deps/thrust/dependencies/cub/test/test_util.h:        cudaEventSynchronize(stop);
deps/thrust/dependencies/cub/test/test_util.h:        cudaEventElapsedTime(&elapsed, start, stop);
deps/thrust/dependencies/cub/test/test_thread_sort.cu: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/test/test_thread_sort.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/test/test_thread_sort.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/test/test_warp_mask.cu: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/test/test_warp_mask.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/test/test_warp_mask.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/test/test_warp_mask.cu:// Ensure printing of CUDA runtime errors to console
deps/thrust/dependencies/cub/test/test_warp_reduce.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/test/test_warp_reduce.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/test/test_warp_reduce.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/test/test_warp_reduce.cu:// Ensure printing of CUDA runtime errors to console
deps/thrust/dependencies/cub/test/test_warp_reduce.cu:    CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(T) * BLOCK_THREADS, cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/test/test_warp_reduce.cu:    CubDebugExit(cudaMemset(d_out, 0, sizeof(T) * BLOCK_THREADS));
deps/thrust/dependencies/cub/test/test_warp_reduce.cu:    CubDebugExit(cudaPeekAtLastError());
deps/thrust/dependencies/cub/test/test_warp_reduce.cu:    CubDebugExit(cudaDeviceSynchronize());
deps/thrust/dependencies/cub/test/test_warp_reduce.cu:    CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(T) * BLOCK_THREADS, cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/test/test_warp_reduce.cu:    CubDebugExit(cudaMemcpy(d_flags, h_flags, sizeof(int) * BLOCK_THREADS, cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/test/test_warp_reduce.cu:    CubDebugExit(cudaMemset(d_head_out, 0, sizeof(T) * BLOCK_THREADS));
deps/thrust/dependencies/cub/test/test_warp_reduce.cu:    CubDebugExit(cudaMemset(d_tail_out, 0, sizeof(T) * BLOCK_THREADS));
deps/thrust/dependencies/cub/test/test_warp_reduce.cu:    CubDebugExit(cudaPeekAtLastError());
deps/thrust/dependencies/cub/test/test_warp_reduce.cu:    CubDebugExit(cudaDeviceSynchronize());
deps/thrust/dependencies/cub/test/test_warp_reduce.cu:    CubDebugExit(cudaPeekAtLastError());
deps/thrust/dependencies/cub/test/test_warp_reduce.cu:    CubDebugExit(cudaDeviceSynchronize());
deps/thrust/dependencies/cub/test/test_device_segmented_sort.cu: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/test/test_device_segmented_sort.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/test/test_device_segmented_sort.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/test/test_device_segmented_sort.cu:// Ensure printing of CUDA runtime errors to console
deps/thrust/dependencies/cub/test/test_device_segmented_sort.cu:  (__CUDACC_VER_MAJOR__ >= 9 || CUDA_VERSION >= 9000) && !_NVHPC_CUDA
deps/thrust/dependencies/cub/test/test_device_segmented_sort.cu:  (__CUDACC_VER_MAJOR__ >= 11 || CUDA_VERSION >= 11000) && !_NVHPC_CUDA
deps/thrust/dependencies/cub/test/test_device_segmented_sort.cu:#include <cuda_fp16.h>
deps/thrust/dependencies/cub/test/test_device_segmented_sort.cu:#include <cuda_bf16.h>
deps/thrust/dependencies/cub/test/test_device_segmented_sort.cu:    cudaMemcpy(keys_output,
deps/thrust/dependencies/cub/test/test_device_segmented_sort.cu:               cudaMemcpyDeviceToHost);
deps/thrust/dependencies/cub/test/test_device_segmented_sort.cu:    cudaMemcpy(values_output,
deps/thrust/dependencies/cub/test/test_device_segmented_sort.cu:               cudaMemcpyDeviceToHost);
deps/thrust/dependencies/cub/test/test_device_segmented_sort.cu:  CUB_RUNTIME_FUNCTION __forceinline__ cudaError_t Invoke()
deps/thrust/dependencies/cub/test/test_device_segmented_sort.cu:    return cudaSuccess;
deps/thrust/dependencies/cub/test/test_device_segmented_sort.cu:  CUB_RUNTIME_FUNCTION __forceinline__ cudaError_t Invoke()
deps/thrust/dependencies/cub/test/test_device_segmented_sort.cu:    return cudaSuccess;
deps/thrust/dependencies/cub/test/test_device_segmented_sort.cu:    CubDebugExit(cudaDeviceSynchronize());
deps/thrust/dependencies/cub/test/test_device_segmented_sort.cu:    CubDebugExit(cudaPeekAtLastError());
deps/thrust/dependencies/cub/test/test_device_merge_sort.cu: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/test/test_device_merge_sort.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/test/test_device_merge_sort.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/test/test_device_merge_sort.cu:// Ensure printing of CUDA runtime errors to console
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:// Ensure printing of CUDA runtime errors to console
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:#if (__CUDACC_VER_MAJOR__ >= 9 || CUDA_VERSION >= 9000) && !_NVHPC_CUDA
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:    #include <cuda_fp16.h>
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:#if (__CUDACC_VER_MAJOR__ >= 11 || CUDA_VERSION >= 11000) && !_NVHPC_CUDA
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:    #include <cuda_bf16.h>
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:    CDP,                        // GPU-based (dynamic parallelism) dispatch to CUB method
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:cudaError_t Dispatch(
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:    cudaError_t             */*d_cdp_error*/,
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:    cudaStream_t            stream,
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:cudaError_t Dispatch(
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:    cudaError_t                 */*d_cdp_error*/,
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:    cudaStream_t            stream,
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:    cudaError_t retval = DeviceRadixSort::SortPairs(
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:cudaError_t Dispatch(
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:    cudaError_t             */*d_cdp_error*/,
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:    cudaStream_t            stream,
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:cudaError_t Dispatch(
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:    cudaError_t                 */*d_cdp_error*/,
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:    cudaStream_t            stream,
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:    cudaError_t retval = DeviceRadixSort::SortPairsDescending(
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:cudaError_t Dispatch(
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:    cudaError_t             */*d_cdp_error*/,
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:    cudaStream_t            stream,
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:cudaError_t Dispatch(
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:    cudaError_t                             */*d_cdp_error*/,
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:    cudaStream_t            stream,
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:    cudaError_t retval = DeviceSegmentedRadixSort::SortPairs(
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:cudaError_t Dispatch(
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:    cudaError_t             */*d_cdp_error*/,
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:    cudaStream_t            stream,
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:cudaError_t Dispatch(
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:    cudaError_t                             */*d_cdp_error*/,
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:    cudaStream_t            stream,
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:    cudaError_t retval = DeviceSegmentedRadixSort::SortPairsDescending(
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:// CUDA Nested Parallelism Test Kernel
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:    cudaError_t             *d_cdp_error,
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:    *d_cdp_error            = cudaErrorNotSupported;
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:cudaError_t Dispatch(
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:    cudaError_t             *d_cdp_error,
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:    cudaStream_t            stream,
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:    CubDebugExit(cudaMemcpy(&d_keys.selector, d_selector, sizeof(int) * 1, cudaMemcpyDeviceToHost));
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:    CubDebugExit(cudaMemcpy(&temp_storage_bytes, d_temp_storage_bytes, sizeof(size_t) * 1, cudaMemcpyDeviceToHost));
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:    cudaError_t retval;
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:    CubDebugExit(cudaMemcpy(&retval, d_cdp_error, sizeof(cudaError_t) * 1, cudaMemcpyDeviceToHost));
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:#if (__CUDACC_VER_MAJOR__ >= 9 || CUDA_VERSION >= 9000) && !_NVHPC_CUDA
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:#if (__CUDACC_VER_MAJOR__ >= 11 || CUDA_VERSION >= 11000) && !_NVHPC_CUDA
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:    cudaError_t             *d_cdp_error;
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:    CubDebugExit(g_allocator.DeviceAllocate((void**)&d_cdp_error, sizeof(cudaError_t) * 1));
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:    CubDebugExit(cudaMemcpy(d_keys.d_buffers[0], h_keys, sizeof(KeyT) * num_items, cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:    CubDebugExit(cudaMemset(d_keys.d_buffers[1], 0, sizeof(KeyT) * num_items));
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:        CubDebugExit(cudaMemcpy(d_values.d_buffers[0], h_values, sizeof(ValueT) * num_items, cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:        CubDebugExit(cudaMemset(d_values.d_buffers[1], 0, sizeof(ValueT) * num_items));
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:    GpuTimer gpu_timer;
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:        CubDebugExit(cudaMemcpy(d_keys.d_buffers[d_keys.selector], h_keys, sizeof(KeyT) * num_items, cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:        CubDebugExit(cudaMemset(d_keys.d_buffers[d_keys.selector ^ 1], 0, sizeof(KeyT) * num_items));
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:            CubDebugExit(cudaMemcpy(d_values.d_buffers[d_values.selector], h_values, sizeof(ValueT) * num_items, cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:            CubDebugExit(cudaMemset(d_values.d_buffers[d_values.selector ^ 1], 0, sizeof(ValueT) * num_items));
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:        gpu_timer.Start();
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:        gpu_timer.Stop();
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:        elapsed_millis += gpu_timer.ElapsedMillis();
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:    CubDebugExit(cudaMemcpy(d_segment_offsets, h_segment_offsets, sizeof(std::size_t) * (num_segments + 1), cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:    CubDebugExit(cudaMemcpy(d_segment_offsets, h_segment_offsets, sizeof(std::size_t) * (num_segments + 1), cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:#if (__CUDACC_VER_MAJOR__ >= 9 || CUDA_VERSION >= 9000) && !_NVHPC_CUDA
deps/thrust/dependencies/cub/test/test_device_radix_sort.cu:#if (__CUDACC_VER_MAJOR__ >= 11 || CUDA_VERSION >= 11000) && !_NVHPC_CUDA
deps/thrust/dependencies/cub/test/test_block_reduce.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/test/test_block_reduce.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/test/test_block_reduce.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/test/test_block_reduce.cu:// Ensure printing of CUDA runtime errors to console
deps/thrust/dependencies/cub/test/test_block_reduce.cu:#include <cuda_runtime_api.h>
deps/thrust/dependencies/cub/test/test_block_reduce.cu:    CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(T) * num_items, cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/test/test_block_reduce.cu:    CubDebugExit(cudaMemset(d_out, 0, sizeof(T) * 1));
deps/thrust/dependencies/cub/test/test_block_reduce.cu:    CubDebugExit(cudaPeekAtLastError());
deps/thrust/dependencies/cub/test/test_block_reduce.cu:    CubDebugExit(cudaDeviceSynchronize());
deps/thrust/dependencies/cub/test/test_block_reduce.cu:    CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(T) * num_items, cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/test/test_block_reduce.cu:    CubDebugExit(cudaMemset(d_out, 0, sizeof(T) * 1));
deps/thrust/dependencies/cub/test/test_block_reduce.cu:    CubDebugExit(cudaPeekAtLastError());
deps/thrust/dependencies/cub/test/test_block_reduce.cu:    CubDebugExit(cudaDeviceSynchronize());
deps/thrust/dependencies/cub/test/test_warp_load.cu: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/test/test_warp_load.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/test/test_warp_load.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/test/test_warp_load.cu:// Ensure printing of CUDA runtime errors to console
deps/thrust/dependencies/cub/test/test_warp_load.cu:  CubDebugExit(cudaPeekAtLastError());
deps/thrust/dependencies/cub/test/test_warp_load.cu:  CubDebugExit(cudaDeviceSynchronize());
deps/thrust/dependencies/cub/test/test_warp_load.cu:  CubDebugExit(cudaPeekAtLastError());
deps/thrust/dependencies/cub/test/test_warp_load.cu:  CubDebugExit(cudaDeviceSynchronize());
deps/thrust/dependencies/cub/test/test_device_reduce_by_key.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/test/test_device_reduce_by_key.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/test/test_device_reduce_by_key.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/test/test_device_reduce_by_key.cu:// Ensure printing of CUDA runtime errors to console
deps/thrust/dependencies/cub/test/test_device_reduce_by_key.cu:    CDP,        // GPU-based (dynamic parallelism) dispatch to CUB method
deps/thrust/dependencies/cub/test/test_device_reduce_by_key.cu:cudaError_t Dispatch(
deps/thrust/dependencies/cub/test/test_device_reduce_by_key.cu:    cudaError_t                 */*d_cdp_error*/,
deps/thrust/dependencies/cub/test/test_device_reduce_by_key.cu:    cudaStream_t                stream,
deps/thrust/dependencies/cub/test/test_device_reduce_by_key.cu:    cudaError_t error = cudaSuccess;
deps/thrust/dependencies/cub/test/test_device_reduce_by_key.cu:// CUDA Nested Parallelism Test Kernel
deps/thrust/dependencies/cub/test/test_device_reduce_by_key.cu:    cudaError_t                 *d_cdp_error,
deps/thrust/dependencies/cub/test/test_device_reduce_by_key.cu:    cudaStream_t                stream,
deps/thrust/dependencies/cub/test/test_device_reduce_by_key.cu:    *d_cdp_error = cudaErrorNotSupported;
deps/thrust/dependencies/cub/test/test_device_reduce_by_key.cu:cudaError_t Dispatch(
deps/thrust/dependencies/cub/test/test_device_reduce_by_key.cu:    cudaError_t                 *d_cdp_error,
deps/thrust/dependencies/cub/test/test_device_reduce_by_key.cu:    cudaStream_t                stream,
deps/thrust/dependencies/cub/test/test_device_reduce_by_key.cu:    CubDebugExit(cudaMemcpy(&temp_storage_bytes, d_temp_storage_bytes, sizeof(size_t) * 1, cudaMemcpyDeviceToHost));
deps/thrust/dependencies/cub/test/test_device_reduce_by_key.cu:    cudaError_t retval;
deps/thrust/dependencies/cub/test/test_device_reduce_by_key.cu:    CubDebugExit(cudaMemcpy(&retval, d_cdp_error, sizeof(cudaError_t) * 1, cudaMemcpyDeviceToHost));
deps/thrust/dependencies/cub/test/test_device_reduce_by_key.cu:    cudaError_t     *d_cdp_error = NULL;
deps/thrust/dependencies/cub/test/test_device_reduce_by_key.cu:    CubDebugExit(g_allocator.DeviceAllocate((void**)&d_cdp_error,           sizeof(cudaError_t) * 1));
deps/thrust/dependencies/cub/test/test_device_reduce_by_key.cu:    CubDebugExit(cudaMemset(d_keys_out, 0, sizeof(KeyT) * num_items));
deps/thrust/dependencies/cub/test/test_device_reduce_by_key.cu:    CubDebugExit(cudaMemset(d_values_out, 0, sizeof(ValueT) * num_items));
deps/thrust/dependencies/cub/test/test_device_reduce_by_key.cu:    CubDebugExit(cudaMemset(d_num_runs, 0, sizeof(int)));
deps/thrust/dependencies/cub/test/test_device_reduce_by_key.cu:    GpuTimer gpu_timer;
deps/thrust/dependencies/cub/test/test_device_reduce_by_key.cu:    gpu_timer.Start();
deps/thrust/dependencies/cub/test/test_device_reduce_by_key.cu:    gpu_timer.Stop();
deps/thrust/dependencies/cub/test/test_device_reduce_by_key.cu:    float elapsed_millis = gpu_timer.ElapsedMillis();
deps/thrust/dependencies/cub/test/test_device_reduce_by_key.cu:    CubDebugExit(cudaMemcpy(d_keys_in, h_keys_in, sizeof(KeyT) * num_items, cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/test/test_device_reduce_by_key.cu:    CubDebugExit(cudaMemcpy(d_values_in, h_values_in, sizeof(ValueT) * num_items, cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/test/test_device_reduce_by_key.cu:    CubDebugExit(cudaMemcpy(d_keys_in, h_keys_in, sizeof(KeyT) * num_items, cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/test/test_device_adjacent_difference.cu: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/test/test_device_adjacent_difference.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/test/test_device_adjacent_difference.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/test/test_device_adjacent_difference.cu:// Ensure printing of CUDA runtime errors to console
deps/thrust/dependencies/cub/test/test_warp_store.cu: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/test/test_warp_store.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/test/test_warp_store.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/test/test_warp_store.cu:// Ensure printing of CUDA runtime errors to console
deps/thrust/dependencies/cub/test/test_warp_store.cu:  CubDebugExit(cudaPeekAtLastError());
deps/thrust/dependencies/cub/test/test_warp_store.cu:  CubDebugExit(cudaDeviceSynchronize());
deps/thrust/dependencies/cub/test/test_warp_store.cu:  CubDebugExit(cudaPeekAtLastError());
deps/thrust/dependencies/cub/test/test_warp_store.cu:  CubDebugExit(cudaDeviceSynchronize());
deps/thrust/dependencies/cub/test/test_device_scan.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/test/test_device_scan.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/test/test_device_scan.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/test/test_device_scan.cu:// Ensure printing of CUDA runtime errors to console
deps/thrust/dependencies/cub/test/test_device_scan.cu:    CDP,        // GPU-based (dynamic parallelism) dispatch to CUB method
deps/thrust/dependencies/cub/test/test_device_scan.cu:cudaError_t Dispatch(
deps/thrust/dependencies/cub/test/test_device_scan.cu:    cudaError_t         */*d_cdp_error*/,
deps/thrust/dependencies/cub/test/test_device_scan.cu:    cudaStream_t        stream,
deps/thrust/dependencies/cub/test/test_device_scan.cu:    cudaError_t error = cudaSuccess;
deps/thrust/dependencies/cub/test/test_device_scan.cu:cudaError_t Dispatch(
deps/thrust/dependencies/cub/test/test_device_scan.cu:    cudaError_t         */*d_cdp_error*/,
deps/thrust/dependencies/cub/test/test_device_scan.cu:    cudaStream_t        stream,
deps/thrust/dependencies/cub/test/test_device_scan.cu:    cudaError_t error = cudaSuccess;
deps/thrust/dependencies/cub/test/test_device_scan.cu:cudaError_t Dispatch(
deps/thrust/dependencies/cub/test/test_device_scan.cu:    cudaError_t         */*d_cdp_error*/,
deps/thrust/dependencies/cub/test/test_device_scan.cu:    cudaStream_t        stream,
deps/thrust/dependencies/cub/test/test_device_scan.cu:    cudaError_t error = cudaSuccess;
deps/thrust/dependencies/cub/test/test_device_scan.cu:cudaError_t Dispatch(
deps/thrust/dependencies/cub/test/test_device_scan.cu:    cudaError_t         */*d_cdp_error*/,
deps/thrust/dependencies/cub/test/test_device_scan.cu:    cudaStream_t        stream,
deps/thrust/dependencies/cub/test/test_device_scan.cu:    cudaError_t error = cudaSuccess;
deps/thrust/dependencies/cub/test/test_device_scan.cu:// CUDA Nested Parallelism Test Kernel
deps/thrust/dependencies/cub/test/test_device_scan.cu:    cudaError_t         *d_cdp_error,
deps/thrust/dependencies/cub/test/test_device_scan.cu:    *d_cdp_error = cudaErrorNotSupported;
deps/thrust/dependencies/cub/test/test_device_scan.cu:cudaError_t Dispatch(
deps/thrust/dependencies/cub/test/test_device_scan.cu:    cudaError_t         *d_cdp_error,
deps/thrust/dependencies/cub/test/test_device_scan.cu:    cudaStream_t        stream,
deps/thrust/dependencies/cub/test/test_device_scan.cu:    CubDebugExit(cudaMemcpy(&temp_storage_bytes, d_temp_storage_bytes, sizeof(size_t) * 1, cudaMemcpyDeviceToHost));
deps/thrust/dependencies/cub/test/test_device_scan.cu:    cudaError_t retval;
deps/thrust/dependencies/cub/test/test_device_scan.cu:    CubDebugExit(cudaMemcpy(&retval, d_cdp_error, sizeof(cudaError_t) * 1, cudaMemcpyDeviceToHost));
deps/thrust/dependencies/cub/test/test_device_scan.cu:    cudaError_t     *d_cdp_error = NULL;
deps/thrust/dependencies/cub/test/test_device_scan.cu:    CubDebugExit(g_allocator.DeviceAllocate((void**)&d_cdp_error,   sizeof(cudaError_t) * 1));
deps/thrust/dependencies/cub/test/test_device_scan.cu:    CubDebugExit(cudaMemset(d_out, 0, sizeof(OutputT) * num_items));
deps/thrust/dependencies/cub/test/test_device_scan.cu:    GpuTimer gpu_timer;
deps/thrust/dependencies/cub/test/test_device_scan.cu:    gpu_timer.Start();
deps/thrust/dependencies/cub/test/test_device_scan.cu:    gpu_timer.Stop();
deps/thrust/dependencies/cub/test/test_device_scan.cu:    float elapsed_millis = gpu_timer.ElapsedMillis();
deps/thrust/dependencies/cub/test/test_device_scan.cu:    CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(InputT) * num_items, cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/test/test_block_histogram.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/test/test_block_histogram.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/test/test_block_histogram.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/test/test_block_histogram.cu:// Ensure printing of CUDA runtime errors to console
deps/thrust/dependencies/cub/test/test_block_histogram.cu:    CubDebugExit(cudaMemcpy(d_samples, h_samples, sizeof(SampleT) * num_samples, cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/test/test_block_histogram.cu:    CubDebugExit(cudaMemset(d_histogram, 0, sizeof(int) * BINS));
deps/thrust/dependencies/cub/test/test_block_histogram.cu:    CubDebugExit(cudaPeekAtLastError());
deps/thrust/dependencies/cub/test/test_block_histogram.cu:    CubDebugExit(cudaDeviceSynchronize());
deps/thrust/dependencies/cub/test/test_device_reduce.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/test/test_device_reduce.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/test/test_device_reduce.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/test/test_device_reduce.cu:// Ensure printing of CUDA runtime errors to console
deps/thrust/dependencies/cub/test/test_device_reduce.cu:    CUB_CDP,        // GPU-based (dynamic parallelism) dispatch to CUB method
deps/thrust/dependencies/cub/test/test_device_reduce.cu:cudaError_t Dispatch(
deps/thrust/dependencies/cub/test/test_device_reduce.cu:    cudaError_t         */*d_cdp_error*/,
deps/thrust/dependencies/cub/test/test_device_reduce.cu:    cudaStream_t        stream,
deps/thrust/dependencies/cub/test/test_device_reduce.cu:    cudaError_t error = cudaSuccess;
deps/thrust/dependencies/cub/test/test_device_reduce.cu:cudaError_t Dispatch(
deps/thrust/dependencies/cub/test/test_device_reduce.cu:    cudaError_t         */*d_cdp_error*/,
deps/thrust/dependencies/cub/test/test_device_reduce.cu:    cudaStream_t        stream,
deps/thrust/dependencies/cub/test/test_device_reduce.cu:    cudaError_t error = cudaSuccess;
deps/thrust/dependencies/cub/test/test_device_reduce.cu:cudaError_t Dispatch(
deps/thrust/dependencies/cub/test/test_device_reduce.cu:    cudaError_t         */*d_cdp_error*/,
deps/thrust/dependencies/cub/test/test_device_reduce.cu:    cudaStream_t        stream,
deps/thrust/dependencies/cub/test/test_device_reduce.cu:    cudaError_t error = cudaSuccess;
deps/thrust/dependencies/cub/test/test_device_reduce.cu:cudaError_t Dispatch(
deps/thrust/dependencies/cub/test/test_device_reduce.cu:    cudaError_t         */*d_cdp_error*/,
deps/thrust/dependencies/cub/test/test_device_reduce.cu:    cudaStream_t        stream,
deps/thrust/dependencies/cub/test/test_device_reduce.cu:    cudaError_t error = cudaSuccess;
deps/thrust/dependencies/cub/test/test_device_reduce.cu:cudaError_t Dispatch(
deps/thrust/dependencies/cub/test/test_device_reduce.cu:    cudaError_t         */*d_cdp_error*/,
deps/thrust/dependencies/cub/test/test_device_reduce.cu:    cudaStream_t        stream,
deps/thrust/dependencies/cub/test/test_device_reduce.cu:    cudaError_t error = cudaSuccess;
deps/thrust/dependencies/cub/test/test_device_reduce.cu:cudaError_t Dispatch(
deps/thrust/dependencies/cub/test/test_device_reduce.cu:    cudaError_t         */*d_cdp_error*/,
deps/thrust/dependencies/cub/test/test_device_reduce.cu:    cudaStream_t        stream,
deps/thrust/dependencies/cub/test/test_device_reduce.cu:    cudaError_t error = cudaSuccess;
deps/thrust/dependencies/cub/test/test_device_reduce.cu:cudaError_t Dispatch(
deps/thrust/dependencies/cub/test/test_device_reduce.cu:    cudaError_t         */*d_cdp_error*/,
deps/thrust/dependencies/cub/test/test_device_reduce.cu:    cudaStream_t        stream,
deps/thrust/dependencies/cub/test/test_device_reduce.cu:    cudaError_t error = cudaSuccess;
deps/thrust/dependencies/cub/test/test_device_reduce.cu:cudaError_t Dispatch(
deps/thrust/dependencies/cub/test/test_device_reduce.cu:    cudaError_t         */*d_cdp_error*/,
deps/thrust/dependencies/cub/test/test_device_reduce.cu:    cudaStream_t        stream,
deps/thrust/dependencies/cub/test/test_device_reduce.cu:    cudaError_t error = cudaSuccess;
deps/thrust/dependencies/cub/test/test_device_reduce.cu:cudaError_t Dispatch(
deps/thrust/dependencies/cub/test/test_device_reduce.cu:    cudaError_t         */*d_cdp_error*/,
deps/thrust/dependencies/cub/test/test_device_reduce.cu:    cudaStream_t        stream,
deps/thrust/dependencies/cub/test/test_device_reduce.cu:    cudaError_t error = cudaSuccess;
deps/thrust/dependencies/cub/test/test_device_reduce.cu:cudaError_t Dispatch(
deps/thrust/dependencies/cub/test/test_device_reduce.cu:    cudaError_t         */*d_cdp_error*/,
deps/thrust/dependencies/cub/test/test_device_reduce.cu:    cudaStream_t        stream,
deps/thrust/dependencies/cub/test/test_device_reduce.cu:    cudaError_t error = cudaSuccess;
deps/thrust/dependencies/cub/test/test_device_reduce.cu:cudaError_t Dispatch(
deps/thrust/dependencies/cub/test/test_device_reduce.cu:    cudaError_t         */*d_cdp_error*/,
deps/thrust/dependencies/cub/test/test_device_reduce.cu:    cudaStream_t        stream,
deps/thrust/dependencies/cub/test/test_device_reduce.cu:    cudaError_t error = cudaSuccess;
deps/thrust/dependencies/cub/test/test_device_reduce.cu:cudaError_t Dispatch(
deps/thrust/dependencies/cub/test/test_device_reduce.cu:    cudaError_t         */*d_cdp_error*/,
deps/thrust/dependencies/cub/test/test_device_reduce.cu:    cudaStream_t        stream,
deps/thrust/dependencies/cub/test/test_device_reduce.cu:    cudaError_t error = cudaSuccess;
deps/thrust/dependencies/cub/test/test_device_reduce.cu:// CUDA nested-parallelism test kernel
deps/thrust/dependencies/cub/test/test_device_reduce.cu:    cudaError_t         *d_cdp_error,
deps/thrust/dependencies/cub/test/test_device_reduce.cu:    *d_cdp_error = cudaErrorNotSupported;
deps/thrust/dependencies/cub/test/test_device_reduce.cu:cudaError_t Dispatch(
deps/thrust/dependencies/cub/test/test_device_reduce.cu:    cudaError_t         *d_cdp_error,
deps/thrust/dependencies/cub/test/test_device_reduce.cu:    cudaStream_t        stream,
deps/thrust/dependencies/cub/test/test_device_reduce.cu:    CubDebugExit(cudaMemcpy(&temp_storage_bytes, d_temp_storage_bytes, sizeof(size_t) * 1, cudaMemcpyDeviceToHost));
deps/thrust/dependencies/cub/test/test_device_reduce.cu:    cudaError_t retval;
deps/thrust/dependencies/cub/test/test_device_reduce.cu:    CubDebugExit(cudaMemcpy(&retval, d_cdp_error, sizeof(cudaError_t) * 1, cudaMemcpyDeviceToHost));
deps/thrust/dependencies/cub/test/test_device_reduce.cu:    cudaError_t     *d_cdp_error = NULL;
deps/thrust/dependencies/cub/test/test_device_reduce.cu:    CubDebugExit(g_allocator.DeviceAllocate((void**)&d_cdp_error,           sizeof(cudaError_t) * 1));
deps/thrust/dependencies/cub/test/test_device_reduce.cu:        GpuTimer gpu_timer;
deps/thrust/dependencies/cub/test/test_device_reduce.cu:        gpu_timer.Start();
deps/thrust/dependencies/cub/test/test_device_reduce.cu:        gpu_timer.Stop();
deps/thrust/dependencies/cub/test/test_device_reduce.cu:        float elapsed_millis = gpu_timer.ElapsedMillis();
deps/thrust/dependencies/cub/test/test_device_reduce.cu:    CubDebugExit(cudaMemset(d_out, 0, sizeof(OutputT) * num_segments));
deps/thrust/dependencies/cub/test/test_device_reduce.cu:    CubDebugExit(cudaMemcpy(d_in,               h_in,                   sizeof(InputT) * num_items, cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/test/test_device_reduce.cu:    CubDebugExit(cudaMemcpy(d_segment_offsets,  h_segment_offsets,      sizeof(OffsetT) * (num_segments + 1), cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/test/test_device_reduce.cu:    CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(InputT) * num_items, cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/test/test_device_reduce.cu:        CubDebugExit(cudaMemcpy(d_segment_offsets, h_segment_offsets, sizeof(OffsetT) * (num_segments + 1), cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/test/test_device_reduce.cu:    cudaError_t Invoke()
deps/thrust/dependencies/cub/test/test_device_reduce.cu:        return cudaSuccess;
deps/thrust/dependencies/cub/test/test_warp_scan.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/test/test_warp_scan.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/test/test_warp_scan.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/test/test_warp_scan.cu:// Ensure printing of CUDA runtime errors to console
deps/thrust/dependencies/cub/test/test_warp_scan.cu:    CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(T) * TOTAL_ITEMS, cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/test/test_warp_scan.cu:    CubDebugExit(cudaMemset(d_out, 0, sizeof(T) * (TOTAL_ITEMS + 1)));
deps/thrust/dependencies/cub/test/test_warp_scan.cu:    CubDebugExit(cudaMemset(d_aggregate, 0, sizeof(T) * TOTAL_ITEMS));
deps/thrust/dependencies/cub/test/test_warp_scan.cu:    CubDebugExit(cudaPeekAtLastError());
deps/thrust/dependencies/cub/test/test_warp_scan.cu:    CubDebugExit(cudaDeviceSynchronize());
deps/thrust/dependencies/cub/test/test_warp_scan.cu:    CubDebugExit(cudaGetDevice(&device_ordinal));
deps/thrust/dependencies/cub/test/test_device_run_length_encode.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/test/test_device_run_length_encode.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/test/test_device_run_length_encode.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/test/test_device_run_length_encode.cu:// Ensure printing of CUDA runtime errors to console
deps/thrust/dependencies/cub/test/test_device_run_length_encode.cu:    CDP,        // GPU-based (dynamic parallelism) dispatch to CUB method
deps/thrust/dependencies/cub/test/test_device_run_length_encode.cu:cudaError_t Dispatch(
deps/thrust/dependencies/cub/test/test_device_run_length_encode.cu:    cudaError_t                 */*d_cdp_error*/,
deps/thrust/dependencies/cub/test/test_device_run_length_encode.cu:    cudaStream_t                stream,
deps/thrust/dependencies/cub/test/test_device_run_length_encode.cu:    cudaError_t error = cudaSuccess;
deps/thrust/dependencies/cub/test/test_device_run_length_encode.cu:cudaError_t Dispatch(
deps/thrust/dependencies/cub/test/test_device_run_length_encode.cu:    cudaError_t                 */*d_cdp_error*/,
deps/thrust/dependencies/cub/test/test_device_run_length_encode.cu:    cudaStream_t                stream,
deps/thrust/dependencies/cub/test/test_device_run_length_encode.cu:    cudaError_t error = cudaSuccess;
deps/thrust/dependencies/cub/test/test_device_run_length_encode.cu:// CUDA Nested Parallelism Test Kernel
deps/thrust/dependencies/cub/test/test_device_run_length_encode.cu:    cudaError_t                 *d_cdp_error,
deps/thrust/dependencies/cub/test/test_device_run_length_encode.cu:    cudaStream_t                stream,
deps/thrust/dependencies/cub/test/test_device_run_length_encode.cu:    *d_cdp_error = cudaErrorNotSupported;
deps/thrust/dependencies/cub/test/test_device_run_length_encode.cu:cudaError_t Dispatch(
deps/thrust/dependencies/cub/test/test_device_run_length_encode.cu:    cudaError_t                 *d_cdp_error,
deps/thrust/dependencies/cub/test/test_device_run_length_encode.cu:    cudaStream_t                stream,
deps/thrust/dependencies/cub/test/test_device_run_length_encode.cu:    CubDebugExit(cudaMemcpy(&temp_storage_bytes, d_temp_storage_bytes, sizeof(size_t) * 1, cudaMemcpyDeviceToHost));
deps/thrust/dependencies/cub/test/test_device_run_length_encode.cu:    cudaError_t retval;
deps/thrust/dependencies/cub/test/test_device_run_length_encode.cu:    CubDebugExit(cudaMemcpy(&retval, d_cdp_error, sizeof(cudaError_t) * 1, cudaMemcpyDeviceToHost));
deps/thrust/dependencies/cub/test/test_device_run_length_encode.cu:    cudaError_t*     d_cdp_error = NULL;
deps/thrust/dependencies/cub/test/test_device_run_length_encode.cu:    CubDebugExit(g_allocator.DeviceAllocate((void**)&d_cdp_error,           sizeof(cudaError_t) * 1));
deps/thrust/dependencies/cub/test/test_device_run_length_encode.cu:        CubDebugExit(cudaMemset(d_unique_out,   0, sizeof(T) * num_items));
deps/thrust/dependencies/cub/test/test_device_run_length_encode.cu:        CubDebugExit(cudaMemset(d_offsets_out,  0, sizeof(OffsetT) * num_items));
deps/thrust/dependencies/cub/test/test_device_run_length_encode.cu:    CubDebugExit(cudaMemset(d_lengths_out,  0, sizeof(LengthT) * num_items));
deps/thrust/dependencies/cub/test/test_device_run_length_encode.cu:    CubDebugExit(cudaMemset(d_num_runs,     0, sizeof(int)));
deps/thrust/dependencies/cub/test/test_device_run_length_encode.cu:    GpuTimer gpu_timer;
deps/thrust/dependencies/cub/test/test_device_run_length_encode.cu:    gpu_timer.Start();
deps/thrust/dependencies/cub/test/test_device_run_length_encode.cu:    gpu_timer.Stop();
deps/thrust/dependencies/cub/test/test_device_run_length_encode.cu:    float elapsed_millis = gpu_timer.ElapsedMillis();
deps/thrust/dependencies/cub/test/test_device_run_length_encode.cu:    CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(T) * num_items, cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/test/test_grid_barrier.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/test/test_grid_barrier.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/test/test_grid_barrier.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/test/test_grid_barrier.cu:// Ensure printing of CUDA runtime errors to console
deps/thrust/dependencies/cub/test/test_grid_barrier.cu:    cudaError_t retval = cudaSuccess;
deps/thrust/dependencies/cub/test/test_grid_barrier.cu:    CubDebugExit(cudaGetDevice(&device_ordinal));
deps/thrust/dependencies/cub/test/test_grid_barrier.cu:    CubDebugExit(cudaDeviceGetAttribute(&sm_count, cudaDevAttrMultiProcessorCount, device_ordinal));
deps/thrust/dependencies/cub/test/test_grid_barrier.cu:    CubDebugExit(cudaDeviceGetAttribute(&max_block_threads, cudaDevAttrMaxThreadsPerBlock, device_ordinal));
deps/thrust/dependencies/cub/test/test_grid_barrier.cu:    GpuTimer gpu_timer;
deps/thrust/dependencies/cub/test/test_grid_barrier.cu:    gpu_timer.Start();
deps/thrust/dependencies/cub/test/test_grid_barrier.cu:    gpu_timer.Stop();
deps/thrust/dependencies/cub/test/test_grid_barrier.cu:    retval = CubDebug(cudaDeviceSynchronize());
deps/thrust/dependencies/cub/test/test_grid_barrier.cu:    float avg_elapsed = gpu_timer.ElapsedMillis() / float(iterations);
deps/thrust/dependencies/cub/test/test_grid_barrier.cu:        gpu_timer.ElapsedMillis(),
deps/thrust/dependencies/cub/test/test_device_spmv.cu: * Copyright (c) 2021, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/test/test_device_spmv.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/test/test_device_spmv.cu: * ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/test/test_device_spmv.cu:// Ensure printing of CUDA runtime errors to console
deps/thrust/dependencies/cub/test/test_block_run_length_decode.cu: * Copyright (c) 2011-2021, NVIDIA CORPORATION. All rights reserved.
deps/thrust/dependencies/cub/test/test_block_run_length_decode.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/test/test_block_run_length_decode.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/test/test_block_run_length_decode.cu:  cudaStream_t stream;
deps/thrust/dependencies/cub/test/test_block_run_length_decode.cu:  cudaStreamCreate(&stream);
deps/thrust/dependencies/cub/test/test_block_run_length_decode.cu:  cudaEvent_t cuda_evt_timers[NUM_TIMERS];
deps/thrust/dependencies/cub/test/test_block_run_length_decode.cu:    cudaEventCreate(&cuda_evt_timers[i]);
deps/thrust/dependencies/cub/test/test_block_run_length_decode.cu:  CubDebugExit(cudaMalloc(&temp_storage, temp_storage_bytes));
deps/thrust/dependencies/cub/test/test_block_run_length_decode.cu:  CubDebugExit(cudaMalloc(&d_decoded_sizes, num_blocks * sizeof(*d_decoded_sizes)));
deps/thrust/dependencies/cub/test/test_block_run_length_decode.cu:  CubDebugExit(cudaMalloc(&d_decoded_offsets, (num_blocks + 1) * sizeof(*d_decoded_offsets)));
deps/thrust/dependencies/cub/test/test_block_run_length_decode.cu:  CubDebugExit(cudaMallocHost(&h_num_decoded_total, sizeof(*h_num_decoded_total)));
deps/thrust/dependencies/cub/test/test_block_run_length_decode.cu:  CubDebugExit(cudaEventRecord(cuda_evt_timers[TIMER_SIZE_BEGIN], stream));
deps/thrust/dependencies/cub/test/test_block_run_length_decode.cu:  CubDebugExit(cudaEventRecord(cuda_evt_timers[TIMER_SIZE_END], stream));
deps/thrust/dependencies/cub/test/test_block_run_length_decode.cu:  CubDebugExit(cudaMemsetAsync(d_decoded_offsets, 0, sizeof(d_decoded_offsets[0]), stream));
deps/thrust/dependencies/cub/test/test_block_run_length_decode.cu:  CubDebugExit(cudaMemcpyAsync(h_num_decoded_total,
deps/thrust/dependencies/cub/test/test_block_run_length_decode.cu:                               cudaMemcpyDeviceToHost,
deps/thrust/dependencies/cub/test/test_block_run_length_decode.cu:  // Ensure the total decoded size has been copied from GPU to CPU
deps/thrust/dependencies/cub/test/test_block_run_length_decode.cu:  CubDebugExit(cudaStreamSynchronize(stream));
deps/thrust/dependencies/cub/test/test_block_run_length_decode.cu:  CubDebugExit(cudaMallocHost(&h_decoded_out, (*h_num_decoded_total) * sizeof(RunItemT)));
deps/thrust/dependencies/cub/test/test_block_run_length_decode.cu:  CubDebugExit(cudaMalloc(&d_decoded_out, (*h_num_decoded_total) * sizeof(RunItemT)));
deps/thrust/dependencies/cub/test/test_block_run_length_decode.cu:    CubDebugExit(cudaMalloc(&d_relative_offsets, (*h_num_decoded_total) * sizeof(RunLengthT)));
deps/thrust/dependencies/cub/test/test_block_run_length_decode.cu:    CubDebugExit(cudaMallocHost(&h_relative_offsets, (*h_num_decoded_total) * sizeof(RunLengthT)));
deps/thrust/dependencies/cub/test/test_block_run_length_decode.cu:  CubDebugExit(cudaEventRecord(cuda_evt_timers[TIMER_DECODE_BEGIN], stream));
deps/thrust/dependencies/cub/test/test_block_run_length_decode.cu:  CubDebugExit(cudaEventRecord(cuda_evt_timers[TIMER_DECODE_END], stream));
deps/thrust/dependencies/cub/test/test_block_run_length_decode.cu:  CubDebugExit(cudaMemcpyAsync(h_decoded_out,
deps/thrust/dependencies/cub/test/test_block_run_length_decode.cu:                               cudaMemcpyDeviceToHost,
deps/thrust/dependencies/cub/test/test_block_run_length_decode.cu:    CubDebugExit(cudaMemcpyAsync(h_relative_offsets,
deps/thrust/dependencies/cub/test/test_block_run_length_decode.cu:                                 cudaMemcpyDeviceToHost,
deps/thrust/dependencies/cub/test/test_block_run_length_decode.cu:  CubDebugExit(cudaStreamSynchronize(stream));
deps/thrust/dependencies/cub/test/test_block_run_length_decode.cu:  cudaEventElapsedTime(&duration_size, cuda_evt_timers[TIMER_SIZE_BEGIN], cuda_evt_timers[TIMER_SIZE_END]);
deps/thrust/dependencies/cub/test/test_block_run_length_decode.cu:  cudaEventElapsedTime(&duration_decode, cuda_evt_timers[TIMER_DECODE_BEGIN], cuda_evt_timers[TIMER_DECODE_END]);
deps/thrust/dependencies/cub/test/test_block_run_length_decode.cu:      std::cout << "Mismatch at #" << i << ": CPU item: " << host_golden[i].first << ", GPU: " << h_decoded_out[i]
deps/thrust/dependencies/cub/test/test_block_run_length_decode.cu:                  << ", GPU: " << h_decoded_out[i] << "; relative offsets: CPU: " << host_golden[i].second
deps/thrust/dependencies/cub/test/test_block_run_length_decode.cu:                  << ", GPU: " << h_relative_offsets[i] << "\n";
deps/thrust/dependencies/cub/test/test_block_run_length_decode.cu:  CubDebugExit(cudaFree(temp_storage));
deps/thrust/dependencies/cub/test/test_block_run_length_decode.cu:  CubDebugExit(cudaFree(d_decoded_sizes));
deps/thrust/dependencies/cub/test/test_block_run_length_decode.cu:  CubDebugExit(cudaFree(d_decoded_offsets));
deps/thrust/dependencies/cub/test/test_block_run_length_decode.cu:  CubDebugExit(cudaFree(d_decoded_out));
deps/thrust/dependencies/cub/test/test_block_run_length_decode.cu:  CubDebugExit(cudaFreeHost(h_num_decoded_total));
deps/thrust/dependencies/cub/test/test_block_run_length_decode.cu:  CubDebugExit(cudaFreeHost(h_decoded_out));
deps/thrust/dependencies/cub/test/test_block_run_length_decode.cu:    CubDebugExit(cudaFree(d_relative_offsets));
deps/thrust/dependencies/cub/test/test_block_run_length_decode.cu:    CubDebugExit(cudaFreeHost(h_relative_offsets));
deps/thrust/dependencies/cub/test/test_block_run_length_decode.cu:    CubDebugExit(cudaEventDestroy(cuda_evt_timers[i]));
deps/thrust/dependencies/cub/test/test_block_run_length_decode.cu:  CubDebugExit(cudaStreamDestroy(stream));
deps/thrust/dependencies/cub/test/CMakeLists.txt:#   testing/vector.cu will be "vector", and testing/cuda/copy.cu will be
deps/thrust/dependencies/cub/test/CMakeLists.txt:#   "cuda.copy".
deps/thrust/dependencies/cub/test/CMakeLists.txt:    cub_enable_rdc_for_cuda_target(${test_target})
deps/thrust/dependencies/cub/test/test_block_shuffle.cu: * Copyright (c) 2011-2021, NVIDIA CORPORATION. All rights reserved.
deps/thrust/dependencies/cub/test/test_block_shuffle.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/test/test_block_shuffle.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/test/test_block_shuffle.cu:// Ensure printing of CUDA runtime errors to console
deps/thrust/dependencies/cub/test/test_block_shuffle.cu:    CubDebugExit(cudaPeekAtLastError());
deps/thrust/dependencies/cub/test/test_block_shuffle.cu:    CubDebugExit(cudaDeviceSynchronize());
deps/thrust/dependencies/cub/test/test_block_shuffle.cu:    CubDebugExit(cudaPeekAtLastError());
deps/thrust/dependencies/cub/test/test_block_shuffle.cu:    CubDebugExit(cudaDeviceSynchronize());
deps/thrust/dependencies/cub/test/test_block_shuffle.cu:    CubDebugExit(cudaMemcpy(h_output, d_output, num_items * sizeof (DataType), cudaMemcpyDeviceToHost));
deps/thrust/dependencies/cub/test/test_device_histogram.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/test/test_device_histogram.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/test/test_device_histogram.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/test/test_device_histogram.cu:// Ensure printing of CUDA runtime errors to console
deps/thrust/dependencies/cub/test/test_device_histogram.cu:    CDP,        // GPU-based (dynamic parallelism) dispatch to CUB method
deps/thrust/dependencies/cub/test/test_device_histogram.cu:    static cudaError_t Range(
deps/thrust/dependencies/cub/test/test_device_histogram.cu:        cudaError_t             */*d_cdp_error*/,
deps/thrust/dependencies/cub/test/test_device_histogram.cu:        cudaStream_t        stream,
deps/thrust/dependencies/cub/test/test_device_histogram.cu:        cudaError_t error = cudaSuccess;
deps/thrust/dependencies/cub/test/test_device_histogram.cu:    static cudaError_t Even(
deps/thrust/dependencies/cub/test/test_device_histogram.cu:        cudaError_t             */*d_cdp_error*/,
deps/thrust/dependencies/cub/test/test_device_histogram.cu:        cudaStream_t        stream,
deps/thrust/dependencies/cub/test/test_device_histogram.cu:        cudaError_t error = cudaSuccess;
deps/thrust/dependencies/cub/test/test_device_histogram.cu:    static cudaError_t Range(
deps/thrust/dependencies/cub/test/test_device_histogram.cu:        cudaError_t             */*d_cdp_error*/,
deps/thrust/dependencies/cub/test/test_device_histogram.cu:        cudaStream_t        stream,
deps/thrust/dependencies/cub/test/test_device_histogram.cu:        cudaError_t error = cudaSuccess;
deps/thrust/dependencies/cub/test/test_device_histogram.cu:    static cudaError_t Even(
deps/thrust/dependencies/cub/test/test_device_histogram.cu:        cudaError_t             */*d_cdp_error*/,
deps/thrust/dependencies/cub/test/test_device_histogram.cu:        cudaStream_t        stream,
deps/thrust/dependencies/cub/test/test_device_histogram.cu:        cudaError_t error = cudaSuccess;
deps/thrust/dependencies/cub/test/test_device_histogram.cu:// CUDA nested-parallelism test kernel
deps/thrust/dependencies/cub/test/test_device_histogram.cu:    cudaError_t         *d_cdp_error,
deps/thrust/dependencies/cub/test/test_device_histogram.cu:    *d_cdp_error = cudaErrorNotSupported;
deps/thrust/dependencies/cub/test/test_device_histogram.cu:cudaError_t Dispatch(
deps/thrust/dependencies/cub/test/test_device_histogram.cu:    cudaError_t         *d_cdp_error,
deps/thrust/dependencies/cub/test/test_device_histogram.cu:    cudaStream_t        stream,
deps/thrust/dependencies/cub/test/test_device_histogram.cu:    CubDebugExit(cudaMemcpy(&temp_storage_bytes, d_temp_storage_bytes, sizeof(size_t) * 1, cudaMemcpyDeviceToHost));
deps/thrust/dependencies/cub/test/test_device_histogram.cu:    cudaError_t retval;
deps/thrust/dependencies/cub/test/test_device_histogram.cu:    CubDebugExit(cudaMemcpy(&retval, d_cdp_error, sizeof(cudaError_t) * 1, cudaMemcpyDeviceToHost));
deps/thrust/dependencies/cub/test/test_device_histogram.cu:        CubDebugExit(cudaMemset(d_histogram[channel], 0, sizeof(CounterT) * (num_levels[channel] - 1)));
deps/thrust/dependencies/cub/test/test_device_histogram.cu:    cudaError_t     *d_cdp_error = NULL;
deps/thrust/dependencies/cub/test/test_device_histogram.cu:    CubDebugExit(g_allocator.DeviceAllocate((void**)&d_cdp_error,           sizeof(cudaError_t) * 1));
deps/thrust/dependencies/cub/test/test_device_histogram.cu:    CubDebugExit(cudaMemset(d_temp_storage, canary_token, temp_storage_bytes + (canary_bytes * 2)));
deps/thrust/dependencies/cub/test/test_device_histogram.cu:    CubDebugExit(cudaPeekAtLastError());
deps/thrust/dependencies/cub/test/test_device_histogram.cu:    CubDebugExit(cudaDeviceSynchronize());
deps/thrust/dependencies/cub/test/test_device_histogram.cu:    GpuTimer gpu_timer;
deps/thrust/dependencies/cub/test/test_device_histogram.cu:    gpu_timer.Start();
deps/thrust/dependencies/cub/test/test_device_histogram.cu:    gpu_timer.Stop();
deps/thrust/dependencies/cub/test/test_device_histogram.cu:    float elapsed_millis = gpu_timer.ElapsedMillis();
deps/thrust/dependencies/cub/test/test_device_histogram.cu:    CubDebugExit(cudaMemcpy(d_samples, h_samples, sizeof(SampleT) * total_samples, cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/test/test_device_histogram.cu:    CubDebugExit(cudaMemcpy(d_samples, h_samples, sizeof(SampleT) * total_samples, cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/test/test_device_histogram.cu:        CubDebugExit(cudaMemcpy(d_levels[channel], levels[channel],         sizeof(LevelT) * num_levels[channel], cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/test/test_device_histogram.cu:        CubDebugExit(cudaMemset(d_histogram[channel], 0,                        sizeof(CounterT) * bins));
deps/thrust/dependencies/cub/test/test_device_histogram.cu:    cudaError_t     *d_cdp_error = NULL;
deps/thrust/dependencies/cub/test/test_device_histogram.cu:    CubDebugExit(g_allocator.DeviceAllocate((void**)&d_cdp_error,           sizeof(cudaError_t) * 1));
deps/thrust/dependencies/cub/test/test_device_histogram.cu:    CubDebugExit(cudaMemset(d_temp_storage, canary_token, temp_storage_bytes + (canary_bytes * 2)));
deps/thrust/dependencies/cub/test/test_device_histogram.cu:    CubDebugExit(cudaPeekAtLastError());
deps/thrust/dependencies/cub/test/test_device_histogram.cu:    CubDebugExit(cudaDeviceSynchronize());
deps/thrust/dependencies/cub/test/test_device_histogram.cu:    GpuTimer gpu_timer;
deps/thrust/dependencies/cub/test/test_device_histogram.cu:    gpu_timer.Start();
deps/thrust/dependencies/cub/test/test_device_histogram.cu:    gpu_timer.Stop();
deps/thrust/dependencies/cub/test/test_device_histogram.cu:    float elapsed_millis = gpu_timer.ElapsedMillis();
deps/thrust/dependencies/cub/test/test_allocator.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/test/test_allocator.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/test/test_allocator.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/test/test_allocator.cu:// Ensure printing of CUDA runtime errors to console
deps/thrust/dependencies/cub/test/test_allocator.cu:    // Get number of GPUs and current GPU
deps/thrust/dependencies/cub/test/test_allocator.cu:    int num_gpus;
deps/thrust/dependencies/cub/test/test_allocator.cu:    int initial_gpu;
deps/thrust/dependencies/cub/test/test_allocator.cu:    if (CubDebug(cudaGetDeviceCount(&num_gpus))) exit(1);
deps/thrust/dependencies/cub/test/test_allocator.cu:    if (CubDebug(cudaGetDevice(&initial_gpu))) exit(1);
deps/thrust/dependencies/cub/test/test_allocator.cu:    // Create default allocator (caches up to 6MB in device allocations per GPU)
deps/thrust/dependencies/cub/test/test_allocator.cu:    printf("Running single-gpu tests...\n"); fflush(stdout);
deps/thrust/dependencies/cub/test/test_allocator.cu:    cudaStream_t other_stream;
deps/thrust/dependencies/cub/test/test_allocator.cu:    CubDebugExit(cudaStreamCreate(&other_stream));
deps/thrust/dependencies/cub/test/test_allocator.cu:    // Allocate 999 bytes on the current gpu in stream0
deps/thrust/dependencies/cub/test/test_allocator.cu:    // Check that that we have 1 live block on the initial GPU
deps/thrust/dependencies/cub/test/test_allocator.cu:    // Check that that we have no cached block on the initial GPU
deps/thrust/dependencies/cub/test/test_allocator.cu:    // Allocate 999 bytes on the current gpu in other_stream
deps/thrust/dependencies/cub/test/test_allocator.cu:    // Check that that we have 1 live blocks on the initial GPU (that we allocated a new one because d_999B_stream0_b is only available for stream 0 until it becomes idle)
deps/thrust/dependencies/cub/test/test_allocator.cu:    // Check that that we have one cached block on the initial GPU
deps/thrust/dependencies/cub/test/test_allocator.cu:    CubDebugExit(cudaDeviceSynchronize());
deps/thrust/dependencies/cub/test/test_allocator.cu:    // Check that that we have 2 live blocks on the initial GPU
deps/thrust/dependencies/cub/test/test_allocator.cu:    // Check that that we have no cached block on the initial GPU
deps/thrust/dependencies/cub/test/test_allocator.cu:    CubDebugExit(cudaDeviceSynchronize());
deps/thrust/dependencies/cub/test/test_allocator.cu:    // Check that that we have 2 live blocks on the initial GPU
deps/thrust/dependencies/cub/test/test_allocator.cu:    // Check that that we have no cached block on the initial GPU
deps/thrust/dependencies/cub/test/test_allocator.cu:    CubDebugExit(cudaDeviceSynchronize());
deps/thrust/dependencies/cub/test/test_allocator.cu:    CubDebugExit(cudaStreamDestroy(other_stream));
deps/thrust/dependencies/cub/test/test_allocator.cu:    // Check that that we have 2 live blocks on the initial GPU
deps/thrust/dependencies/cub/test/test_allocator.cu:    // Check that that we have no cached block on the initial GPU
deps/thrust/dependencies/cub/test/test_allocator.cu:    // Allocate 5 bytes on the current gpu
deps/thrust/dependencies/cub/test/test_allocator.cu:    // Check that that we have zero free bytes cached on the initial GPU
deps/thrust/dependencies/cub/test/test_allocator.cu:    AssertEquals(allocator.cached_bytes[initial_gpu].free, 0);
deps/thrust/dependencies/cub/test/test_allocator.cu:    // Check that that we have 1 live block on the initial GPU
deps/thrust/dependencies/cub/test/test_allocator.cu:    // Allocate 4096 bytes on the current gpu
deps/thrust/dependencies/cub/test/test_allocator.cu:    // Check that that we have 2 live blocks on the initial GPU
deps/thrust/dependencies/cub/test/test_allocator.cu:    // Check that that we have min_bin_bytes free bytes cached on the initial gpu
deps/thrust/dependencies/cub/test/test_allocator.cu:    AssertEquals(allocator.cached_bytes[initial_gpu].free, allocator.min_bin_bytes);
deps/thrust/dependencies/cub/test/test_allocator.cu:    // Check that that we have 1 live block on the initial GPU
deps/thrust/dependencies/cub/test/test_allocator.cu:    // Check that that we have 1 cached block on the initial GPU
deps/thrust/dependencies/cub/test/test_allocator.cu:    // Check that that we have the 4096 + min_bin free bytes cached on the initial gpu
deps/thrust/dependencies/cub/test/test_allocator.cu:    AssertEquals(allocator.cached_bytes[initial_gpu].free, allocator.min_bin_bytes + 4096);
deps/thrust/dependencies/cub/test/test_allocator.cu:    // Check that that we have 0 live block on the initial GPU
deps/thrust/dependencies/cub/test/test_allocator.cu:    // Check that that we have 2 cached block on the initial GPU
deps/thrust/dependencies/cub/test/test_allocator.cu:    // Allocate 768 bytes on the current gpu
deps/thrust/dependencies/cub/test/test_allocator.cu:    // Check that that we have the min_bin free bytes cached on the initial gpu (4096 was reused)
deps/thrust/dependencies/cub/test/test_allocator.cu:    AssertEquals(allocator.cached_bytes[initial_gpu].free, allocator.min_bin_bytes);
deps/thrust/dependencies/cub/test/test_allocator.cu:    // Check that that we have 1 live block on the initial GPU
deps/thrust/dependencies/cub/test/test_allocator.cu:    // Check that that we have 1 cached block on the initial GPU
deps/thrust/dependencies/cub/test/test_allocator.cu:    // Allocate max_cached_bytes on the current gpu
deps/thrust/dependencies/cub/test/test_allocator.cu:    // Check that that we have the min_bin free bytes cached on the initial gpu (max cached was not returned because we went over)
deps/thrust/dependencies/cub/test/test_allocator.cu:    AssertEquals(allocator.cached_bytes[initial_gpu].free, allocator.min_bin_bytes);
deps/thrust/dependencies/cub/test/test_allocator.cu:    // Check that that we have 1 live block on the initial GPU
deps/thrust/dependencies/cub/test/test_allocator.cu:    // Check that that we still have 1 cached block on the initial GPU
deps/thrust/dependencies/cub/test/test_allocator.cu:    // Free all cached blocks on all GPUs
deps/thrust/dependencies/cub/test/test_allocator.cu:    // Check that that we have 0 bytes cached on the initial GPU
deps/thrust/dependencies/cub/test/test_allocator.cu:    AssertEquals(allocator.cached_bytes[initial_gpu].free, 0);
deps/thrust/dependencies/cub/test/test_allocator.cu:    // Check that that we have 0 cached blocks across all GPUs
deps/thrust/dependencies/cub/test/test_allocator.cu:    // Check that that still we have 1 live block across all GPUs
deps/thrust/dependencies/cub/test/test_allocator.cu:    // Allocate max cached bytes + 1 on the current gpu
deps/thrust/dependencies/cub/test/test_allocator.cu:    // Check that that we have 4096 free bytes cached on the initial gpu
deps/thrust/dependencies/cub/test/test_allocator.cu:    AssertEquals(allocator.cached_bytes[initial_gpu].free, rounded_bytes);
deps/thrust/dependencies/cub/test/test_allocator.cu:    // Check that that we have 1 cached blocks across all GPUs
deps/thrust/dependencies/cub/test/test_allocator.cu:    // Check that that still we have 0 live block across all GPUs
deps/thrust/dependencies/cub/test/test_allocator.cu:    // BUG: find out why these tests fail when one GPU is CDP compliant and the other is not
deps/thrust/dependencies/cub/test/test_allocator.cu:    if (num_gpus > 1)
deps/thrust/dependencies/cub/test/test_allocator.cu:        printf("\nRunning multi-gpu tests...\n"); fflush(stdout);
deps/thrust/dependencies/cub/test/test_allocator.cu:        // Allocate 768 bytes on the next gpu
deps/thrust/dependencies/cub/test/test_allocator.cu:        int next_gpu = (initial_gpu + 1) % num_gpus;
deps/thrust/dependencies/cub/test/test_allocator.cu:        CubDebugExit(allocator.DeviceAllocate(next_gpu, (void **) &d_768B_2, 768));
deps/thrust/dependencies/cub/test/test_allocator.cu:        // DeviceFree d_768B on the next gpu
deps/thrust/dependencies/cub/test/test_allocator.cu:        CubDebugExit(allocator.DeviceFree(next_gpu, d_768B_2));
deps/thrust/dependencies/cub/test/test_allocator.cu:        // Re-allocate 768 bytes on the next gpu
deps/thrust/dependencies/cub/test/test_allocator.cu:        CubDebugExit(allocator.DeviceAllocate(next_gpu, (void **) &d_768B_2, 768));
deps/thrust/dependencies/cub/test/test_allocator.cu:        // Re-free d_768B on the next gpu
deps/thrust/dependencies/cub/test/test_allocator.cu:        CubDebugExit(allocator.DeviceFree(next_gpu, d_768B_2));
deps/thrust/dependencies/cub/test/test_allocator.cu:        // Check that that we have 4096 free bytes cached on the initial gpu
deps/thrust/dependencies/cub/test/test_allocator.cu:        AssertEquals(allocator.cached_bytes[initial_gpu].free, rounded_bytes);
deps/thrust/dependencies/cub/test/test_allocator.cu:        // Check that that we have 4096 free bytes cached on the second gpu
deps/thrust/dependencies/cub/test/test_allocator.cu:        AssertEquals(allocator.cached_bytes[next_gpu].free, rounded_bytes);
deps/thrust/dependencies/cub/test/test_allocator.cu:        // Check that that we have 2 cached blocks across all GPUs
deps/thrust/dependencies/cub/test/test_allocator.cu:        // Check that that still we have 0 live block across all GPUs
deps/thrust/dependencies/cub/test/test_allocator.cu:    // CUDA
deps/thrust/dependencies/cub/test/test_allocator.cu:        CubDebugExit(cudaMalloc((void **) &d_1024MB, timing_bytes));
deps/thrust/dependencies/cub/test/test_allocator.cu:        CubDebugExit(cudaFree(d_1024MB));
deps/thrust/dependencies/cub/test/test_allocator.cu:    float cuda_malloc_elapsed_millis = cpu_timer.ElapsedMillis();
deps/thrust/dependencies/cub/test/test_allocator.cu:    printf("\t CUB CachingDeviceAllocator allocation CPU speedup: %.2f (avg cudaMalloc %.4f ms vs. avg DeviceAllocate %.4f ms)\n",
deps/thrust/dependencies/cub/test/test_allocator.cu:        cuda_malloc_elapsed_millis / cub_calloc_elapsed_millis,
deps/thrust/dependencies/cub/test/test_allocator.cu:        cuda_malloc_elapsed_millis / timing_iterations,
deps/thrust/dependencies/cub/test/test_allocator.cu:    // GPU performance comparisons.  Allocate and free a 1MB block 2000 times
deps/thrust/dependencies/cub/test/test_allocator.cu:    GpuTimer gpu_timer;
deps/thrust/dependencies/cub/test/test_allocator.cu:    printf("\nGPU Performance (%d timing iterations, %d bytes):\n", timing_iterations, timing_bytes);
deps/thrust/dependencies/cub/test/test_allocator.cu:    gpu_timer.Start();
deps/thrust/dependencies/cub/test/test_allocator.cu:    gpu_timer.Stop();
deps/thrust/dependencies/cub/test/test_allocator.cu:    float cuda_empty_elapsed_millis = gpu_timer.ElapsedMillis();
deps/thrust/dependencies/cub/test/test_allocator.cu:    // CUDA
deps/thrust/dependencies/cub/test/test_allocator.cu:    gpu_timer.Start();
deps/thrust/dependencies/cub/test/test_allocator.cu:        CubDebugExit(cudaMalloc((void **) &d_1024MB, timing_bytes));
deps/thrust/dependencies/cub/test/test_allocator.cu:        CubDebugExit(cudaFree(d_1024MB));
deps/thrust/dependencies/cub/test/test_allocator.cu:    gpu_timer.Stop();
deps/thrust/dependencies/cub/test/test_allocator.cu:    cuda_malloc_elapsed_millis = gpu_timer.ElapsedMillis() - cuda_empty_elapsed_millis;
deps/thrust/dependencies/cub/test/test_allocator.cu:    gpu_timer.Start();
deps/thrust/dependencies/cub/test/test_allocator.cu:    gpu_timer.Stop();
deps/thrust/dependencies/cub/test/test_allocator.cu:    cub_calloc_elapsed_millis = gpu_timer.ElapsedMillis() - cuda_empty_elapsed_millis;
deps/thrust/dependencies/cub/test/test_allocator.cu:    printf("\t CUB CachingDeviceAllocator allocation GPU speedup: %.2f (avg cudaMalloc %.4f ms vs. avg DeviceAllocate %.4f ms)\n",
deps/thrust/dependencies/cub/test/test_allocator.cu:        cuda_malloc_elapsed_millis / cub_calloc_elapsed_millis,
deps/thrust/dependencies/cub/test/test_allocator.cu:        cuda_malloc_elapsed_millis / timing_iterations,
deps/thrust/dependencies/cub/test/test_device_select_unique.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/test/test_device_select_unique.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/test/test_device_select_unique.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/test/test_device_select_unique.cu:// Ensure printing of CUDA runtime errors to console
deps/thrust/dependencies/cub/test/test_device_select_unique.cu:    CDP,        // GPU-based (dynamic parallelism) dispatch to CUB method
deps/thrust/dependencies/cub/test/test_device_select_unique.cu:cudaError_t Dispatch(
deps/thrust/dependencies/cub/test/test_device_select_unique.cu:    cudaError_t                 */*d_cdp_error*/,
deps/thrust/dependencies/cub/test/test_device_select_unique.cu:    cudaStream_t                stream,
deps/thrust/dependencies/cub/test/test_device_select_unique.cu:    cudaError_t error = cudaSuccess;
deps/thrust/dependencies/cub/test/test_device_select_unique.cu:// CUDA Nested Parallelism Test Kernel
deps/thrust/dependencies/cub/test/test_device_select_unique.cu:    cudaError_t                 *d_cdp_error,
deps/thrust/dependencies/cub/test/test_device_select_unique.cu:    *d_cdp_error = cudaErrorNotSupported;
deps/thrust/dependencies/cub/test/test_device_select_unique.cu:cudaError_t Dispatch(
deps/thrust/dependencies/cub/test/test_device_select_unique.cu:    cudaError_t                 *d_cdp_error,
deps/thrust/dependencies/cub/test/test_device_select_unique.cu:    cudaStream_t                stream,
deps/thrust/dependencies/cub/test/test_device_select_unique.cu:    CubDebugExit(cudaMemcpy(&temp_storage_bytes, d_temp_storage_bytes, sizeof(size_t) * 1, cudaMemcpyDeviceToHost));
deps/thrust/dependencies/cub/test/test_device_select_unique.cu:    cudaError_t retval;
deps/thrust/dependencies/cub/test/test_device_select_unique.cu:    CubDebugExit(cudaMemcpy(&retval, d_cdp_error, sizeof(cudaError_t) * 1, cudaMemcpyDeviceToHost));
deps/thrust/dependencies/cub/test/test_device_select_unique.cu:    cudaError_t     *d_cdp_error = NULL;
deps/thrust/dependencies/cub/test/test_device_select_unique.cu:    CubDebugExit(g_allocator.DeviceAllocate((void**)&d_cdp_error,           sizeof(cudaError_t) * 1));
deps/thrust/dependencies/cub/test/test_device_select_unique.cu:    CubDebugExit(cudaMemset(d_out, 0, sizeof(T) * num_items));
deps/thrust/dependencies/cub/test/test_device_select_unique.cu:    CubDebugExit(cudaMemset(d_num_selected_out, 0, sizeof(int)));
deps/thrust/dependencies/cub/test/test_device_select_unique.cu:    GpuTimer gpu_timer;
deps/thrust/dependencies/cub/test/test_device_select_unique.cu:    gpu_timer.Start();
deps/thrust/dependencies/cub/test/test_device_select_unique.cu:    gpu_timer.Stop();
deps/thrust/dependencies/cub/test/test_device_select_unique.cu:    float elapsed_millis = gpu_timer.ElapsedMillis();
deps/thrust/dependencies/cub/test/test_device_select_unique.cu:    CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(T) * num_items, cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/test/test_block_scan.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/test/test_block_scan.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/test/test_block_scan.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/test/test_block_scan.cu:// Ensure printing of CUDA runtime errors to console
deps/thrust/dependencies/cub/test/test_block_scan.cu:    CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(T) * TILE_SIZE, cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/test/test_block_scan.cu:    CubDebugExit(cudaMemset(d_out, 0, sizeof(T) * (TILE_SIZE + 1)));
deps/thrust/dependencies/cub/test/test_block_scan.cu:    CubDebugExit(cudaMemset(d_aggregate, 0, sizeof(T) * BLOCK_THREADS));
deps/thrust/dependencies/cub/test/test_block_scan.cu:    CubDebugExit(cudaPeekAtLastError());
deps/thrust/dependencies/cub/test/test_block_scan.cu:    CubDebugExit(cudaDeviceSynchronize());
deps/thrust/dependencies/cub/test/half.h: * Copyright (c) 2011-2019, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/test/half.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/test/half.h: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/test/half.h: * Utilities for interacting with the opaque CUDA __half type
deps/thrust/dependencies/cub/test/half.h:#include <cuda_fp16.h>
deps/thrust/dependencies/cub/test/test_warp_exchange.cu: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/test/test_warp_exchange.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/test/test_warp_exchange.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/test/test_warp_exchange.cu:  cudaDeviceSynchronize();
deps/thrust/dependencies/cub/test/test_warp_exchange.cu:  cudaDeviceSynchronize();
deps/thrust/dependencies/cub/test/test_block_load_store.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/test/test_block_load_store.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/test/test_block_load_store.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/test/test_block_load_store.cu:// Ensure printing of CUDA runtime errors to console
deps/thrust/dependencies/cub/test/test_block_load_store.cu:    CubDebugExit(cudaPeekAtLastError());
deps/thrust/dependencies/cub/test/test_block_load_store.cu:    CubDebugExit(cudaDeviceSynchronize());
deps/thrust/dependencies/cub/test/test_block_load_store.cu:    CubDebugExit(cudaMemset(d_out_unguarded, 0, sizeof(T) * unguarded_elements));
deps/thrust/dependencies/cub/test/test_block_load_store.cu:    CubDebugExit(cudaMemset(d_out_guarded, 0, sizeof(T) * guarded_elements));
deps/thrust/dependencies/cub/test/test_block_load_store.cu:    CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(T) * unguarded_elements, cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/test/test_block_load_store.cu:    CubDebugExit(cudaMemset(d_out_unguarded, 0, sizeof(T) * unguarded_elements));
deps/thrust/dependencies/cub/test/test_block_load_store.cu:    CubDebugExit(cudaMemset(d_out_guarded, 0, sizeof(T) * guarded_elements));
deps/thrust/dependencies/cub/test/test_block_load_store.cu:    CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(T) * unguarded_elements, cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/test/cmake/test_install/CMakeLists.txt:project(CubTestInstall CXX CUDA)
deps/thrust/dependencies/cub/test/cmake/CMakeLists.txt:      -D "CMAKE_CUDA_COMPILER=${CMAKE_CUDA_COMPILER}"
deps/thrust/dependencies/cub/test/test_iterator.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/test/test_iterator.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/test/test_iterator.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/test/test_iterator.cu:// Ensure printing of CUDA runtime errors to console
deps/thrust/dependencies/cub/test/test_iterator.cu:    CDP,        // GPU-based (dynamic parallelism) dispatch to CUB method
deps/thrust/dependencies/cub/test/test_iterator.cu:    CubDebugExit(cudaPeekAtLastError());
deps/thrust/dependencies/cub/test/test_iterator.cu:    CubDebugExit(cudaDeviceSynchronize());
deps/thrust/dependencies/cub/test/test_iterator.cu:    CubDebugExit(cudaMemcpy(d_data, h_data, sizeof(T) * TEST_VALUES, cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/test/test_iterator.cu:    CubDebugExit(cudaMemcpy(d_data, h_data, sizeof(T) * TEST_VALUES, cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/test/test_iterator.cu:    CubDebugExit(cudaMemcpy(d_data, h_data, sizeof(T) * TEST_VALUES, cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/test/test_iterator.cu:    CubDebugExit(cudaMemcpy(d_dummy, h_data + DUMMY_OFFSET, sizeof(T) * DUMMY_TEST_VALUES, cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/test/test_iterator.cu:#if CUDART_VERSION >= 5050
deps/thrust/dependencies/cub/test/test_iterator.cu:    CubDebugExit(cudaMemcpy(d_data, h_data, sizeof(T) * TEST_VALUES, cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/test/test_iterator.cu:    CubDebugExit(cudaMemcpy(d_dummy, h_data + DUMMY_OFFSET, sizeof(T) * DUMMY_TEST_VALUES, cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/test/test_iterator.cu:    CubDebugExit(cudaMemcpy(d_data, h_data, sizeof(T) * TEST_VALUES, cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/test/test_iterator.cu:#endif  // CUDART_VERSION
deps/thrust/dependencies/cub/test/test_iterator.cu:    // Test tex-obj iterators if CUDA dynamic parallelism enabled
deps/thrust/dependencies/cub/test/test_iterator.cu:#if CUDART_VERSION >= 5050
deps/thrust/dependencies/cub/test/test_iterator.cu:    // Test tex-ref iterators for CUDA 5.5
deps/thrust/dependencies/cub/test/test_iterator.cu:#endif  // CUDART_VERSION
deps/thrust/dependencies/cub/test/test_device_three_way_partition.cu: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/test/test_device_three_way_partition.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/test/test_device_three_way_partition.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/test/test_device_three_way_partition.cu:// Ensure printing of CUDA runtime errors to console
deps/thrust/dependencies/cub/test/test_block_radix_sort.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/test/test_block_radix_sort.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/test/test_block_radix_sort.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/test/test_block_radix_sort.cu:// Ensure printing of CUDA runtime errors to console
deps/thrust/dependencies/cub/test/test_block_radix_sort.cu:    cudaSharedMemConfig SMEM_CONFIG,
deps/thrust/dependencies/cub/test/test_block_radix_sort.cu:    cudaSharedMemConfig     SMEM_CONFIG,
deps/thrust/dependencies/cub/test/test_block_radix_sort.cu:    CubDebugExit(cudaMemcpy(d_keys, h_keys, sizeof(Key) * TILE_SIZE, cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/test/test_block_radix_sort.cu:    CubDebugExit(cudaMemcpy(d_values, h_values, sizeof(Value) * TILE_SIZE, cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/test/test_block_radix_sort.cu:    cudaDeviceSetSharedMemConfig(SMEM_CONFIG);
deps/thrust/dependencies/cub/test/test_block_radix_sort.cu:    CubDebugExit(cudaPeekAtLastError());
deps/thrust/dependencies/cub/test/test_block_radix_sort.cu:    CubDebugExit(cudaDeviceSynchronize());
deps/thrust/dependencies/cub/test/test_block_radix_sort.cu:    cudaSharedMemConfig     SMEM_CONFIG,
deps/thrust/dependencies/cub/test/test_block_radix_sort.cu:    cudaSharedMemConfig     SMEM_CONFIG,
deps/thrust/dependencies/cub/test/test_block_radix_sort.cu:    cudaSharedMemConfig     SMEM_CONFIG,
deps/thrust/dependencies/cub/test/test_block_radix_sort.cu:    Test<BLOCK_THREADS, ITEMS_PER_THREAD, RADIX_BITS, MEMOIZE_OUTER_SCAN, INNER_SCAN_ALGORITHM, cudaSharedMemBankSizeFourByte, Key, NullType>();    // Keys-only (4-byte smem bank config)
deps/thrust/dependencies/cub/test/test_block_radix_sort.cu:    Test<BLOCK_THREADS, ITEMS_PER_THREAD, RADIX_BITS, MEMOIZE_OUTER_SCAN, INNER_SCAN_ALGORITHM, cudaSharedMemBankSizeEightByte, Key, NullType>();   // Keys-only (8-byte smem bank config)
deps/thrust/dependencies/cub/test/test_block_radix_sort.cu:    Test<BLOCK_THREADS, ITEMS_PER_THREAD, RADIX_BITS, MEMOIZE_OUTER_SCAN, INNER_SCAN_ALGORITHM, cudaSharedMemBankSizeFourByte, Key, char>();        // With small-values
deps/thrust/dependencies/cub/test/test_block_radix_sort.cu:    Test<BLOCK_THREADS, ITEMS_PER_THREAD, RADIX_BITS, MEMOIZE_OUTER_SCAN, INNER_SCAN_ALGORITHM, cudaSharedMemBankSizeFourByte, Key, Key>();         // With same-values
deps/thrust/dependencies/cub/test/test_block_radix_sort.cu:    Test<BLOCK_THREADS, ITEMS_PER_THREAD, RADIX_BITS, MEMOIZE_OUTER_SCAN, INNER_SCAN_ALGORITHM, cudaSharedMemBankSizeFourByte, Key, TestFoo>();     // With large values
deps/thrust/dependencies/cub/test/test_device_scan_by_key.cu: * Copyright (c) 2021 NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/test/test_device_scan_by_key.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/test/test_device_scan_by_key.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/test/test_device_scan_by_key.cu:// Ensure printing of CUDA runtime errors to console
deps/thrust/dependencies/cub/test/test_device_scan_by_key.cu:    CDP,        // GPU-based (dynamic parallelism) dispatch to CUB method
deps/thrust/dependencies/cub/test/test_device_scan_by_key.cu:cudaError_t Dispatch(
deps/thrust/dependencies/cub/test/test_device_scan_by_key.cu:    cudaError_t           */*d_cdp_error*/,
deps/thrust/dependencies/cub/test/test_device_scan_by_key.cu:    cudaStream_t          stream,
deps/thrust/dependencies/cub/test/test_device_scan_by_key.cu:    cudaError_t error = cudaSuccess;
deps/thrust/dependencies/cub/test/test_device_scan_by_key.cu:cudaError_t Dispatch(
deps/thrust/dependencies/cub/test/test_device_scan_by_key.cu:    cudaError_t           */*d_cdp_error*/,
deps/thrust/dependencies/cub/test/test_device_scan_by_key.cu:    cudaStream_t          stream,
deps/thrust/dependencies/cub/test/test_device_scan_by_key.cu:    cudaError_t error = cudaSuccess;
deps/thrust/dependencies/cub/test/test_device_scan_by_key.cu:cudaError_t Dispatch(
deps/thrust/dependencies/cub/test/test_device_scan_by_key.cu:    cudaError_t           */*d_cdp_error*/,
deps/thrust/dependencies/cub/test/test_device_scan_by_key.cu:    cudaStream_t          stream,
deps/thrust/dependencies/cub/test/test_device_scan_by_key.cu:    cudaError_t error = cudaSuccess;
deps/thrust/dependencies/cub/test/test_device_scan_by_key.cu:cudaError_t Dispatch(
deps/thrust/dependencies/cub/test/test_device_scan_by_key.cu:    cudaError_t           */*d_cdp_error*/,
deps/thrust/dependencies/cub/test/test_device_scan_by_key.cu:    cudaStream_t          stream,
deps/thrust/dependencies/cub/test/test_device_scan_by_key.cu:    cudaError_t error = cudaSuccess;
deps/thrust/dependencies/cub/test/test_device_scan_by_key.cu:// CUDA Nested Parallelism Test Kernel
deps/thrust/dependencies/cub/test/test_device_scan_by_key.cu:    cudaError_t           *d_cdp_error,
deps/thrust/dependencies/cub/test/test_device_scan_by_key.cu:    *d_cdp_error = cudaErrorNotSupported;
deps/thrust/dependencies/cub/test/test_device_scan_by_key.cu:cudaError_t Dispatch(
deps/thrust/dependencies/cub/test/test_device_scan_by_key.cu:    cudaError_t           *d_cdp_error,
deps/thrust/dependencies/cub/test/test_device_scan_by_key.cu:    cudaStream_t          stream,
deps/thrust/dependencies/cub/test/test_device_scan_by_key.cu:    CubDebugExit(cudaMemcpy(&temp_storage_bytes, d_temp_storage_bytes, sizeof(size_t) * 1, cudaMemcpyDeviceToHost));
deps/thrust/dependencies/cub/test/test_device_scan_by_key.cu:    cudaError_t retval;
deps/thrust/dependencies/cub/test/test_device_scan_by_key.cu:    CubDebugExit(cudaMemcpy(&retval, d_cdp_error, sizeof(cudaError_t) * 1, cudaMemcpyDeviceToHost));
deps/thrust/dependencies/cub/test/test_device_scan_by_key.cu:    cudaError_t     *d_cdp_error = NULL;
deps/thrust/dependencies/cub/test/test_device_scan_by_key.cu:    CubDebugExit(g_allocator.DeviceAllocate((void**)&d_cdp_error,   sizeof(cudaError_t) * 1));
deps/thrust/dependencies/cub/test/test_device_scan_by_key.cu:    CubDebugExit(cudaMemset(d_values_out, 0, sizeof(OutputT) * num_items));
deps/thrust/dependencies/cub/test/test_device_scan_by_key.cu:    GpuTimer gpu_timer;
deps/thrust/dependencies/cub/test/test_device_scan_by_key.cu:    gpu_timer.Start();
deps/thrust/dependencies/cub/test/test_device_scan_by_key.cu:    gpu_timer.Stop();
deps/thrust/dependencies/cub/test/test_device_scan_by_key.cu:    float elapsed_millis = gpu_timer.ElapsedMillis();
deps/thrust/dependencies/cub/test/test_device_scan_by_key.cu:    CubDebugExit(cudaMemcpy(d_keys_in, h_keys_in, sizeof(KeyT) * num_items, cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/test/test_device_scan_by_key.cu:    CubDebugExit(cudaMemcpy(d_values_in, h_values_in, sizeof(InputT) * num_items, cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/test/test_block_merge_sort.cu: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/test/test_block_merge_sort.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/test/test_block_merge_sort.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/test/test_block_merge_sort.cu:// Ensure printing of CUDA runtime errors to console
deps/thrust/dependencies/cub/test/test_block_merge_sort.cu:  CubDebugExit(cudaPeekAtLastError());
deps/thrust/dependencies/cub/test/test_block_merge_sort.cu:  CubDebugExit(cudaDeviceSynchronize());
deps/thrust/dependencies/cub/test/test_block_merge_sort.cu:  CubDebugExit(cudaPeekAtLastError());
deps/thrust/dependencies/cub/test/test_block_merge_sort.cu:  CubDebugExit(cudaDeviceSynchronize());
deps/thrust/dependencies/cub/test/test_block_adjacent_difference.cu: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/test/test_block_adjacent_difference.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/test/test_block_adjacent_difference.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/test/test_block_adjacent_difference.cu:// Ensure printing of CUDA runtime errors to console
deps/thrust/dependencies/cub/test/test_block_adjacent_difference.cu:  CubDebugExit(cudaPeekAtLastError());
deps/thrust/dependencies/cub/test/test_block_adjacent_difference.cu:  CubDebugExit(cudaDeviceSynchronize());
deps/thrust/dependencies/cub/test/test_block_adjacent_difference.cu:  CubDebugExit(cudaPeekAtLastError());
deps/thrust/dependencies/cub/test/test_block_adjacent_difference.cu:  CubDebugExit(cudaDeviceSynchronize());
deps/thrust/dependencies/cub/test/test_block_adjacent_difference.cu:  CubDebugExit(cudaPeekAtLastError());
deps/thrust/dependencies/cub/test/test_block_adjacent_difference.cu:  CubDebugExit(cudaDeviceSynchronize());
deps/thrust/dependencies/cub/test/test_block_adjacent_difference.cu:  CubDebugExit(cudaPeekAtLastError());
deps/thrust/dependencies/cub/test/test_block_adjacent_difference.cu:  CubDebugExit(cudaDeviceSynchronize());
deps/thrust/dependencies/cub/test/test_block_adjacent_difference.cu:  CubDebugExit(cudaPeekAtLastError());
deps/thrust/dependencies/cub/test/test_block_adjacent_difference.cu:  CubDebugExit(cudaDeviceSynchronize());
deps/thrust/dependencies/cub/test/test_block_adjacent_difference.cu:  CubDebugExit(cudaPeekAtLastError());
deps/thrust/dependencies/cub/test/test_block_adjacent_difference.cu:  CubDebugExit(cudaDeviceSynchronize());
deps/thrust/dependencies/cub/test/bfloat16.h: * Copyright (c) 2021, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/test/bfloat16.h: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/test/bfloat16.h: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/test/bfloat16.h: * Utilities for interacting with the opaque CUDA __nv_bfloat16 type
deps/thrust/dependencies/cub/test/bfloat16.h:#include <cuda_bf16.h>
deps/thrust/dependencies/cub/CHANGELOG.md:each segment and utilizes the GPU more efficiently.
deps/thrust/dependencies/cub/CHANGELOG.md:device-scope algorithms when invoked via CUDA Dynamic Parallelism (CDP).**
deps/thrust/dependencies/cub/CHANGELOG.md:- NVIDIA/cub#305: The template parameters of `cub::DispatchScan` have changed to
deps/thrust/dependencies/cub/CHANGELOG.md:- NVIDIA/cub#377: Remove broken `operator->()` from
deps/thrust/dependencies/cub/CHANGELOG.md:- NVIDIA/cub#305: Add overloads to `cub::DeviceScan` algorithms that allow the
deps/thrust/dependencies/cub/CHANGELOG.md:- NVIDIA/cub#354: Add `cub::BlockRunLengthDecode` algorithm. Thanks to Elias
deps/thrust/dependencies/cub/CHANGELOG.md:- NVIDIA/cub#357: Add `cub::DeviceSegmentedSort`, an optimized version
deps/thrust/dependencies/cub/CHANGELOG.md:- NVIDIA/cub#376: Add "by key" overloads to `cub::DeviceScan`. Thanks to Xiang
deps/thrust/dependencies/cub/CHANGELOG.md:- NVIDIA/cub#349: Doxygen and unused variable fixes.
deps/thrust/dependencies/cub/CHANGELOG.md:- NVIDIA/cub#363: Maintenance updates for the new `cub::DeviceMergeSort`
deps/thrust/dependencies/cub/CHANGELOG.md:- NVIDIA/cub#382: Fix several `-Wconversion` warnings. Thanks to Matt Stack
deps/thrust/dependencies/cub/CHANGELOG.md:- NVIDIA/cub#388: Fix debug assertion on MSVC when using
deps/thrust/dependencies/cub/CHANGELOG.md:- NVIDIA/cub#395: Support building with `__CUDA_NO_HALF_CONVERSIONS__`. Thanks
deps/thrust/dependencies/cub/CHANGELOG.md:# CUB 1.14.0 (NVIDIA HPC SDK 21.9)
deps/thrust/dependencies/cub/CHANGELOG.md:CUB 1.14.0 is a major release accompanying the NVIDIA HPC SDK 21.9.
deps/thrust/dependencies/cub/CHANGELOG.md:- NVIDIA/cub#350: When the `CUB_NS_[PRE|POST]FIX` macros are set,
deps/thrust/dependencies/cub/CHANGELOG.md:- NVIDIA/cub#322: Ported the merge sort algorithm from Thrust:
deps/thrust/dependencies/cub/CHANGELOG.md:- NVIDIA/cub#326: Simplify the namespace wrapper macros, and detect when
deps/thrust/dependencies/cub/CHANGELOG.md:- NVIDIA/cub#160, NVIDIA/cub#163, NVIDIA/cub#352: Fixed several bugs in
deps/thrust/dependencies/cub/CHANGELOG.md:- NVIDIA/cub#328: Fixed error handling bug and incorrect debugging output in
deps/thrust/dependencies/cub/CHANGELOG.md:- NVIDIA/cub#335: Fixed a compile error affecting clang and NVRTC. Thanks to
deps/thrust/dependencies/cub/CHANGELOG.md:- NVIDIA/cub#351: Fixed some errors in the `cub::DeviceHistogram` documentation.
deps/thrust/dependencies/cub/CHANGELOG.md:- NVIDIA/cub#348: Add an example that demonstrates how to use dynamic shared
deps/thrust/dependencies/cub/CHANGELOG.md:# CUB 1.13.1 (CUDA Toolkit 11.5)
deps/thrust/dependencies/cub/CHANGELOG.md:CUB 1.13.1 is a minor release accompanying the CUDA Toolkit 11.5.
deps/thrust/dependencies/cub/CHANGELOG.md:NVIDIA/thrust#1401.
deps/thrust/dependencies/cub/CHANGELOG.md:- NVIDIA/cub#326: Add `THRUST_CUB_WRAPPED_NAMESPACE` hooks.
deps/thrust/dependencies/cub/CHANGELOG.md:# CUB 1.13.0 (NVIDIA HPC SDK 21.7)
deps/thrust/dependencies/cub/CHANGELOG.md:CUB 1.13.0 is the major release accompanying the NVIDIA HPC SDK 21.7 release.
deps/thrust/dependencies/cub/CHANGELOG.md:- NVIDIA/cub#320: Deprecated `cub::TexRefInputIterator<T, UNIQUE_ID>`. Use
deps/thrust/dependencies/cub/CHANGELOG.md:- NVIDIA/cub#274: Add `BLOCK_LOAD_STRIPED` and `BLOCK_STORE_STRIPED`
deps/thrust/dependencies/cub/CHANGELOG.md:- NVIDIA/cub#291: `cub::DeviceSegmentedRadixSort` and
deps/thrust/dependencies/cub/CHANGELOG.md:- NVIDIA/cub#306: Add `bfloat16` support to `cub::DeviceRadixSort`. Thanks to
deps/thrust/dependencies/cub/CHANGELOG.md:- NVIDIA/cub#320: Introduce a new `CUB_IGNORE_DEPRECATED_API` macro that
deps/thrust/dependencies/cub/CHANGELOG.md:- NVIDIA/cub#277: Fixed sanitizer warnings in `RadixSortScanBinsKernels`. Thanks
deps/thrust/dependencies/cub/CHANGELOG.md:- NVIDIA/cub#287: `cub::DeviceHistogram` now correctly handles cases
deps/thrust/dependencies/cub/CHANGELOG.md:- NVIDIA/cub#311: Fixed several bugs and added tests for the `cub::BlockShuffle`
deps/thrust/dependencies/cub/CHANGELOG.md:- NVIDIA/cub#312: Eliminate unnecessary kernel instantiations when
deps/thrust/dependencies/cub/CHANGELOG.md:- NVIDIA/cub#319: Fixed out-of-bounds memory access on debugging builds
deps/thrust/dependencies/cub/CHANGELOG.md:- NVIDIA/cub#320: Fixed harmless missing return statement warning in
deps/thrust/dependencies/cub/CHANGELOG.md:    - NVIDIA/cub#275: Fixed comments describing the `cub::If` and `cub::Equals`
deps/thrust/dependencies/cub/CHANGELOG.md:    - NVIDIA/cub#290: Documented that `cub::DeviceSegmentedReduce` will produce
deps/thrust/dependencies/cub/CHANGELOG.md:    - NVIDIA/cub#298: `CONTRIBUTING.md` now refers to Thrust's build
deps/thrust/dependencies/cub/CHANGELOG.md:    - NVIDIA/cub#301: Expand `cub::DeviceScan` documentation to include in-place
deps/thrust/dependencies/cub/CHANGELOG.md:    - NVIDIA/cub#307: Expand `cub::DeviceRadixSort` and `cub::BlockRadixSort`
deps/thrust/dependencies/cub/CHANGELOG.md:    - NVIDIA/cub#316: Move `WARP_TIME_SLICING` documentation to the correct
deps/thrust/dependencies/cub/CHANGELOG.md:    - NVIDIA/cub#321: Update URLs from deprecated github.com to preferred
deps/thrust/dependencies/cub/CHANGELOG.md:# CUB 1.12.1 (CUDA Toolkit 11.4)
deps/thrust/dependencies/cub/CHANGELOG.md:# CUB 1.12.0 (NVIDIA HPC SDK 21.3)
deps/thrust/dependencies/cub/CHANGELOG.md:CUB 1.12.0 is a bugfix release accompanying the NVIDIA HPC SDK 21.3 and
deps/thrust/dependencies/cub/CHANGELOG.md:the CUDA Toolkit 11.4.
deps/thrust/dependencies/cub/CHANGELOG.md:- NVIDIA/cub#256: Deprecate Clang < 7 and MSVC < 2019.
deps/thrust/dependencies/cub/CHANGELOG.md:- NVIDIA/cub#218: Radix sort now treats -0.0 and +0.0 as equivalent for floating
deps/thrust/dependencies/cub/CHANGELOG.md:- NVIDIA/cub#247: Suppress newly triggered warnings in Clang. Thanks to Andrew
deps/thrust/dependencies/cub/CHANGELOG.md:- NVIDIA/cub#249: Enable stricter warning flags. This fixes a number of
deps/thrust/dependencies/cub/CHANGELOG.md:  - NVIDIA/cub#221: Overflow in `temp_storage_bytes` when `num_items` close to
deps/thrust/dependencies/cub/CHANGELOG.md:  - NVIDIA/cub#228: CUB uses non-standard C++ extensions that break strict
deps/thrust/dependencies/cub/CHANGELOG.md:  - NVIDIA/cub#257: Warning when compiling `GridEvenShare` with unsigned
deps/thrust/dependencies/cub/CHANGELOG.md:- NVIDIA/cub#258: Use correct `OffsetT` in `DispatchRadixSort::InitPassConfig`.
deps/thrust/dependencies/cub/CHANGELOG.md:- NVIDIA/cub#259: Remove some problematic `__forceinline__` annotations.
deps/thrust/dependencies/cub/CHANGELOG.md:- NVIDIA/cub#123: Fix incorrect issue number in changelog. Thanks to Peet
deps/thrust/dependencies/cub/CHANGELOG.md:# CUB 1.11.0 (CUDA Toolkit 11.3)
deps/thrust/dependencies/cub/CHANGELOG.md:CUB 1.11.0 is a major release accompanying the CUDA Toolkit 11.3 release,
deps/thrust/dependencies/cub/CHANGELOG.md:- NVIDIA/cub#201: The intermediate accumulator type used when `DeviceScan` is
deps/thrust/dependencies/cub/CHANGELOG.md:- NVIDIA/cub#204: Faster `DeviceRadixSort`, up to 2x performance increase for
deps/thrust/dependencies/cub/CHANGELOG.md:  1.5-2x on Clang CUDA. Thanks to Justin Lebar for this contribution.
deps/thrust/dependencies/cub/CHANGELOG.md:- NVIDIA/cub#200: Allow CUB to be added to CMake projects via `add_subdirectory`.
deps/thrust/dependencies/cub/CHANGELOG.md:- NVIDIA/cub#214: Optionally add install rules when included with
deps/thrust/dependencies/cub/CHANGELOG.md:- NVIDIA/cub#215: Fix integer truncation in `AgentReduceByKey`, `AgentScan`,
deps/thrust/dependencies/cub/CHANGELOG.md:- NVIDIA/cub#225: Fix compile-time regression when defining `CUB_NS_PREFIX`
deps/thrust/dependencies/cub/CHANGELOG.md:- NVIDIA/cub#210: Fix some edge cases in `DeviceScan`:
deps/thrust/dependencies/cub/CHANGELOG.md:- NVIDIA/cub#217: Fix and add test for cmake package install rules. Thanks to
deps/thrust/dependencies/cub/CHANGELOG.md:- NVIDIA/cub#170, NVIDIA/cub#233: Update CUDA version checks to behave on Clang
deps/thrust/dependencies/cub/CHANGELOG.md:  CUDA and `nvc++`. Thanks to Artem Belevich, Andrew Corrigan, and David Olsen
deps/thrust/dependencies/cub/CHANGELOG.md:- NVIDIA/cub#220, NVIDIA/cub#216: Various fixes for Clang CUDA. Thanks to Andrew
deps/thrust/dependencies/cub/CHANGELOG.md:- NVIDIA/cub#231: Fix signedness mismatch warnings in unit tests.
deps/thrust/dependencies/cub/CHANGELOG.md:- NVIDIA/cub#231: Suppress GPU deprecation warnings.
deps/thrust/dependencies/cub/CHANGELOG.md:- NVIDIA/cub#214: Use semantic versioning rules for our CMake package's
deps/thrust/dependencies/cub/CHANGELOG.md:- NVIDIA/cub#214: Use `FindPackageHandleStandardArgs` to print standard status
deps/thrust/dependencies/cub/CHANGELOG.md:- NVIDIA/cub#207: Fix `CubDebug` usage
deps/thrust/dependencies/cub/CHANGELOG.md:- NVIDIA/cub#213: Remove tuning policies for unsupported hardware (<SM35).
deps/thrust/dependencies/cub/CHANGELOG.md:  - Github's `thrust/cub` repository is now `NVIDIA/cub`
deps/thrust/dependencies/cub/CHANGELOG.md:# CUB 1.10.0 (NVIDIA HPC SDK 20.9, CUDA Toolkit 11.2)
deps/thrust/dependencies/cub/CHANGELOG.md:CUB 1.10.0 is the major release accompanying the NVIDIA HPC SDK 20.9 release
deps/thrust/dependencies/cub/CHANGELOG.md:  and the CUDA Toolkit 11.2 release.
deps/thrust/dependencies/cub/CHANGELOG.md:https://github.com/NVIDIA/cub/blob/main/CODE_OF_CONDUCT.md
deps/thrust/dependencies/cub/CHANGELOG.md:- NVIDIA/thrust#1244: Check for macro collisions with system headers during
deps/thrust/dependencies/cub/CHANGELOG.md:- NVIDIA/thrust#1153: Switch to placement new instead of assignment to
deps/thrust/dependencies/cub/CHANGELOG.md:- NVIDIA/cub#38: Fix `cub::DeviceHistogram` for `size_t` `OffsetT`s.
deps/thrust/dependencies/cub/CHANGELOG.md:- NVIDIA/cub#35: Fix GCC-5 maybe-uninitialized warning.
deps/thrust/dependencies/cub/CHANGELOG.md:- NVIDIA/cub#36: Qualify namespace for `va_printf` in `_CubLog`.
deps/thrust/dependencies/cub/CHANGELOG.md:# CUB 1.9.10-1 (NVIDIA HPC SDK 20.7, CUDA Toolkit 11.1)
deps/thrust/dependencies/cub/CHANGELOG.md:CUB 1.9.10-1 is the minor release accompanying the NVIDIA HPC SDK 20.7 release
deps/thrust/dependencies/cub/CHANGELOG.md:  and the CUDA Toolkit 11.1 release.
deps/thrust/dependencies/cub/CHANGELOG.md:- NVIDIA/thrust#1217: Move static local in cub::DeviceCount to a separate
deps/thrust/dependencies/cub/CHANGELOG.md:# CUB 1.9.10 (NVIDIA HPC SDK 20.5)
deps/thrust/dependencies/cub/CHANGELOG.md:Thrust 1.9.10 is the release accompanying the NVIDIA HPC SDK 20.5 release.
deps/thrust/dependencies/cub/CHANGELOG.md:    (ex: `cmake -DCUB_DIR=/usr/local/cuda/include/cub/cmake/ .`) and then you
deps/thrust/dependencies/cub/CHANGELOG.md:# CUB 1.9.9 (CUDA 11.0)
deps/thrust/dependencies/cub/CHANGELOG.md:CUB 1.9.9 is the release accompanying the CUDA Toolkit 11.0 release.
deps/thrust/dependencies/cub/CHANGELOG.md:  GPU-accelerated C++17 Parallel Algorithms.
deps/thrust/dependencies/cub/CHANGELOG.md:    `cudaGetDeviceCount`.
deps/thrust/dependencies/cub/CHANGELOG.md:- Lazily initialize the per-device CUDAattribute caches, because CUDA context
deps/thrust/dependencies/cub/CHANGELOG.md:    creation is expensive and adds up with large CUDA binaries on machines with
deps/thrust/dependencies/cub/CHANGELOG.md:    many GPUs.
deps/thrust/dependencies/cub/CHANGELOG.md:  Thanks to the NVIDIA PyTorch team for bringing this to our attention.
deps/thrust/dependencies/cub/CHANGELOG.md:# CUB 1.9.8-1 (NVIDIA HPC SDK 20.3)
deps/thrust/dependencies/cub/CHANGELOG.md:CUB 1.9.8-1 is a variant of 1.9.8 accompanying the NVIDIA HPC SDK 20.3 release.
deps/thrust/dependencies/cub/CHANGELOG.md:  GPU-accelerated C++17 Parallel Algorithms.
deps/thrust/dependencies/cub/CHANGELOG.md:# CUB 1.9.8 (CUDA 11.0 Early Access)
deps/thrust/dependencies/cub/CHANGELOG.md:  in the CUDA Toolkit.
deps/thrust/dependencies/cub/CHANGELOG.md:When compiling CUB in C++11 mode, CUB now caches calls to CUDA attribute query
deps/thrust/dependencies/cub/CHANGELOG.md:- (C++11 or later) Cache calls to `cudaFuncGetAttributes` and
deps/thrust/dependencies/cub/CHANGELOG.md:    `cudaDeviceGetAttribute` within `cub::PtxVersion` and `cub::SmVersion`.
deps/thrust/dependencies/cub/CHANGELOG.md:    These CUDA APIs acquire locks to CUDA driver/runtime mutex and perform
deps/thrust/dependencies/cub/CHANGELOG.md:- #115: `cub::WarpReduce` segmented reduction is broken in CUDA 9 for logical
deps/thrust/dependencies/cub/CHANGELOG.md:CUB 1.7.1 delivers improved radix sort performance on SM7x (Volta) GPUs and a
deps/thrust/dependencies/cub/CHANGELOG.md:- #104: `uint64_t` `cub::WarpReduce` broken for CUB 1.7.0 on CUDA 8 and older.
deps/thrust/dependencies/cub/CHANGELOG.md:- #103: Can't mix Thrust from CUDA 9.0 and CUB.
deps/thrust/dependencies/cub/CHANGELOG.md:- #98: cuda-memcheck: --tool initcheck failed with lineOfSight.
deps/thrust/dependencies/cub/CHANGELOG.md:CUB 1.7.0 brings support for CUDA 9.0 and SM7x (Volta) GPUs.
deps/thrust/dependencies/cub/CHANGELOG.md:  However, SM1x devices are now deprecated in CUDA, and the interfaces of these
deps/thrust/dependencies/cub/CHANGELOG.md:    SM7x and newer GPUs which have independent thread scheduling.
deps/thrust/dependencies/cub/CHANGELOG.md:  (Pascal) GPUs.
deps/thrust/dependencies/cub/CHANGELOG.md:- Restore fence work-around for scan (reduce-by-key, etc.) hangs in CUDA 8.5.
deps/thrust/dependencies/cub/CHANGELOG.md:  and enhances radix sort performance for SM6x (Pascal) GPUs.
deps/thrust/dependencies/cub/CHANGELOG.md:- Radix sort tuning policies updated for SM6x (Pascal) GPUs - 6.2B 4 byte
deps/thrust/dependencies/cub/CHANGELOG.md:  GPUs.
deps/thrust/dependencies/cub/CHANGELOG.md:- Radix sort tuning policies updated for SM6x (Pascal) GPUs.
deps/thrust/dependencies/cub/CHANGELOG.md:- Issue #47: `cub::CachingDeviceAllocator` needs to clean up CUDA global error
deps/thrust/dependencies/cub/CHANGELOG.md:- Fix for generic-type reduce-by-key `cub::WarpScan` for SM3x and newer GPUs.
deps/thrust/dependencies/cub/CHANGELOG.md:    SM52 (Mawell) GPUs.
deps/thrust/dependencies/cub/CHANGELOG.md:- Fix CUDA 7.5 issues on SM52 GPUs with SHFL-based warp-scan and
deps/thrust/dependencies/cub/CHANGELOG.md:- Fix minor CUDA 7.0 performance regressions in `cub::DeviceScan` and
deps/thrust/dependencies/cub/CHANGELOG.md:    when invoking CUB device-wide rountines using CUDA dynamic parallelism.
deps/thrust/dependencies/cub/CHANGELOG.md:  GPUs.
deps/thrust/dependencies/cub/CHANGELOG.md:- Support and performance tuning for SM5x (Maxwell) GPUs.
deps/thrust/dependencies/cub/CHANGELOG.md:- Workaround for a benign WAW race warning reported by cuda-memcheck
deps/thrust/dependencies/cub/CHANGELOG.md:    SM3x (Kepler) GPUs.
deps/thrust/dependencies/cub/CHANGELOG.md:    compiled for SM1x is run on newer GPUs of higher compute-capability: the
deps/thrust/dependencies/cub/CHANGELOG.md:  `cub::DeviceReduce::RunLengthEncode` and support for CUDA 6.0.
deps/thrust/dependencies/cub/CHANGELOG.md:  - Explain that iterator (in)compatibilities with CUDA 5.0 (and older) and
deps/thrust/dependencies/cub/CHANGELOG.md:- Added workaround to make `cub::TexRefInputIteratorT` work with CUDA 6.0.
deps/thrust/dependencies/cub/CHANGELOG.md:Additionally, scan and sort performance for older GPUs has been improved and
deps/thrust/dependencies/cub/CHANGELOG.md:    GPUs (SM1x to SM3x).
deps/thrust/dependencies/cub/CHANGELOG.md:- CUDA Dynamic Parallelism (CDP, e.g. device-callable) versions of device-wide
deps/thrust/dependencies/cub/CHANGELOG.md:- Updates to accommodate CUDA 5.5 dynamic parallelism.
deps/thrust/dependencies/cub/CHANGELOG.md:  warp-level, and thread-level primitives for CUDA kernel programming.
deps/thrust/dependencies/cub/cub/util_macro.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/util_macro.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/util_macro.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/block/block_raking_layout.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/block/block_raking_layout.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/block/block_raking_layout.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/block/block_raking_layout.cuh: * This type facilitates a shared memory usage pattern where a block of CUDA
deps/thrust/dependencies/cub/cub/block/block_store.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/block/block_store.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/block/block_store.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/block/block_store.cuh: * Operations for writing linear segments of data from the CUDA thread block
deps/thrust/dependencies/cub/cub/block/block_store.cuh: * which is the default starting offset returned by \p cudaMalloc()
deps/thrust/dependencies/cub/cub/block/block_store.cuh: *   - The data type \p T is not a built-in primitive or CUDA vector type (e.g., \p short, \p int2, \p double, \p float2, etc.)
deps/thrust/dependencies/cub/cub/block/block_store.cuh:        // Maximum CUDA vector size is 4 elements
deps/thrust/dependencies/cub/cub/block/block_store.cuh: * \brief cub::BlockStoreAlgorithm enumerates alternative algorithms for cub::BlockStore to write a blocked arrangement of items across a CUDA thread block to a linear segment of memory.
deps/thrust/dependencies/cub/cub/block/block_store.cuh:     * to memory using CUDA's built-in vectorized stores as a coalescing optimization.
deps/thrust/dependencies/cub/cub/block/block_store.cuh:     *   - The data type \p T is not a built-in primitive or CUDA vector type (e.g., \p short, \p int2, \p double, \p float2, etc.)
deps/thrust/dependencies/cub/cub/block/block_store.cuh: * \brief The BlockStore class provides [<em>collective</em>](index.html#sec0) data movement methods for writing a [<em>blocked arrangement</em>](index.html#sec5sec3) of items partitioned across a CUDA thread block to a linear segment of memory.  ![](block_store_logo.png)
deps/thrust/dependencies/cub/cub/block/block_store.cuh: *      of data is written directly to memory using CUDA's built-in vectorized stores as a
deps/thrust/dependencies/cub/cub/block/specializations/block_scan_raking.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/block/specializations/block_scan_raking.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/block/specializations/block_scan_raking.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/block/specializations/block_scan_raking.cuh: * cub::BlockScanRaking provides variants of raking-based parallel prefix scan across a CUDA thread block.
deps/thrust/dependencies/cub/cub/block/specializations/block_scan_raking.cuh: * \brief BlockScanRaking provides variants of raking-based parallel prefix scan across a CUDA thread block.
deps/thrust/dependencies/cub/cub/block/specializations/block_histogram_sort.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/block/specializations/block_histogram_sort.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/block/specializations/block_histogram_sort.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/block/specializations/block_histogram_sort.cuh: * The cub::BlockHistogramSort class provides sorting-based methods for constructing block-wide histograms from data samples partitioned across a CUDA thread block.
deps/thrust/dependencies/cub/cub/block/specializations/block_histogram_sort.cuh: * \brief The BlockHistogramSort class provides sorting-based methods for constructing block-wide histograms from data samples partitioned across a CUDA thread block.
deps/thrust/dependencies/cub/cub/block/specializations/block_histogram_sort.cuh:            cudaSharedMemBankSizeFourByte,
deps/thrust/dependencies/cub/cub/block/specializations/block_reduce_raking.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/block/specializations/block_reduce_raking.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/block/specializations/block_reduce_raking.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/block/specializations/block_reduce_raking.cuh: * cub::BlockReduceRaking provides raking-based methods of parallel reduction across a CUDA thread block.  Supports non-commutative reduction operators.
deps/thrust/dependencies/cub/cub/block/specializations/block_reduce_raking.cuh: * \brief BlockReduceRaking provides raking-based methods of parallel reduction across a CUDA thread block.  Supports non-commutative reduction operators.
deps/thrust/dependencies/cub/cub/block/specializations/block_reduce_warp_reductions.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/block/specializations/block_reduce_warp_reductions.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/block/specializations/block_reduce_warp_reductions.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/block/specializations/block_reduce_warp_reductions.cuh: * cub::BlockReduceWarpReductions provides variants of warp-reduction-based parallel reduction across a CUDA thread block.  Supports non-commutative reduction operators.
deps/thrust/dependencies/cub/cub/block/specializations/block_reduce_warp_reductions.cuh: * \brief BlockReduceWarpReductions provides variants of warp-reduction-based parallel reduction across a CUDA thread block.  Supports non-commutative reduction operators.
deps/thrust/dependencies/cub/cub/block/specializations/block_histogram_atomic.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/block/specializations/block_histogram_atomic.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/block/specializations/block_histogram_atomic.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/block/specializations/block_histogram_atomic.cuh: * The cub::BlockHistogramAtomic class provides atomic-based methods for constructing block-wide histograms from data samples partitioned across a CUDA thread block.
deps/thrust/dependencies/cub/cub/block/specializations/block_histogram_atomic.cuh: * \brief The BlockHistogramAtomic class provides atomic-based methods for constructing block-wide histograms from data samples partitioned across a CUDA thread block.
deps/thrust/dependencies/cub/cub/block/specializations/block_scan_warp_scans2.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/block/specializations/block_scan_warp_scans2.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/block/specializations/block_scan_warp_scans2.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/block/specializations/block_scan_warp_scans2.cuh: * cub::BlockScanWarpscans provides warpscan-based variants of parallel prefix scan across a CUDA thread block.
deps/thrust/dependencies/cub/cub/block/specializations/block_scan_warp_scans2.cuh: * \brief BlockScanWarpScans provides warpscan-based variants of parallel prefix scan across a CUDA thread block.
deps/thrust/dependencies/cub/cub/block/specializations/block_scan_warp_scans.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/block/specializations/block_scan_warp_scans.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/block/specializations/block_scan_warp_scans.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/block/specializations/block_scan_warp_scans.cuh: * cub::BlockScanWarpscans provides warpscan-based variants of parallel prefix scan across a CUDA thread block.
deps/thrust/dependencies/cub/cub/block/specializations/block_scan_warp_scans.cuh: * \brief BlockScanWarpScans provides warpscan-based variants of parallel prefix scan across a CUDA thread block.
deps/thrust/dependencies/cub/cub/block/specializations/block_scan_warp_scans3.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/block/specializations/block_scan_warp_scans3.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/block/specializations/block_scan_warp_scans3.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/block/specializations/block_scan_warp_scans3.cuh: * cub::BlockScanWarpscans provides warpscan-based variants of parallel prefix scan across a CUDA thread block.
deps/thrust/dependencies/cub/cub/block/specializations/block_scan_warp_scans3.cuh: * \brief BlockScanWarpScans provides warpscan-based variants of parallel prefix scan across a CUDA thread block.
deps/thrust/dependencies/cub/cub/block/specializations/block_reduce_raking_commutative_only.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/block/specializations/block_reduce_raking_commutative_only.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/block/specializations/block_reduce_raking_commutative_only.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/block/specializations/block_reduce_raking_commutative_only.cuh: * cub::BlockReduceRakingCommutativeOnly provides raking-based methods of parallel reduction across a CUDA thread block.  Does not support non-commutative reduction operators.
deps/thrust/dependencies/cub/cub/block/specializations/block_reduce_raking_commutative_only.cuh: * \brief BlockReduceRakingCommutativeOnly provides raking-based methods of parallel reduction across a CUDA thread block.  Does not support non-commutative reduction operators.  Does not support block sizes that are not a multiple of the warp size.
deps/thrust/dependencies/cub/cub/block/block_radix_sort.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/block/block_radix_sort.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/block/block_radix_sort.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/block/block_radix_sort.cuh: * The cub::BlockRadixSort class provides [<em>collective</em>](index.html#sec0) methods for radix sorting of items partitioned across a CUDA thread block.
deps/thrust/dependencies/cub/cub/block/block_radix_sort.cuh: * \brief The BlockRadixSort class provides [<em>collective</em>](index.html#sec0) methods for sorting items partitioned across a CUDA thread block using a radix sorting method.  ![](sorting_logo.png)
deps/thrust/dependencies/cub/cub/block/block_radix_sort.cuh: * \tparam SMEM_CONFIG          <b>[optional]</b> Shared memory bank mode (default: \p cudaSharedMemBankSizeFourByte)
deps/thrust/dependencies/cub/cub/block/block_radix_sort.cuh: * (<tt>unsigned char</tt>, \p int, \p double, etc.) as well as CUDA's \p __half
deps/thrust/dependencies/cub/cub/block/block_radix_sort.cuh:    cudaSharedMemConfig     SMEM_CONFIG             = cudaSharedMemBankSizeFourByte,
deps/thrust/dependencies/cub/cub/block/block_adjacent_difference.cuh: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/block/block_adjacent_difference.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/block/block_adjacent_difference.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/block/block_adjacent_difference.cuh: * of adjacent elements partitioned across a CUDA thread block.
deps/thrust/dependencies/cub/cub/block/block_adjacent_difference.cuh: *        differences of adjacent elements partitioned across a CUDA thread
deps/thrust/dependencies/cub/cub/block/block_adjacent_difference.cuh: *   the elements partitioned across a CUDA thread block. Because the binary
deps/thrust/dependencies/cub/cub/block/block_adjacent_difference.cuh:     *        partitioned across a CUDA thread block.
deps/thrust/dependencies/cub/cub/block/block_adjacent_difference.cuh:     *        partitioned across a CUDA thread block.
deps/thrust/dependencies/cub/cub/block/block_adjacent_difference.cuh:     * @brief Subtracts the left element of each adjacent pair of elements partitioned across a CUDA thread block.
deps/thrust/dependencies/cub/cub/block/block_adjacent_difference.cuh:     *        partitioned across a CUDA thread block.
deps/thrust/dependencies/cub/cub/block/block_adjacent_difference.cuh:     *        partitioned across a CUDA thread block.
deps/thrust/dependencies/cub/cub/block/block_adjacent_difference.cuh:     *        elements partitioned across a CUDA thread block.
deps/thrust/dependencies/cub/cub/block/block_load.cuh: * Copyright (c) 2011-2016, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/block/block_load.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/block/block_load.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/block/block_load.cuh: * Operations for reading linear tiles of data into the CUDA thread block.
deps/thrust/dependencies/cub/cub/block/block_load.cuh: *   - The data type \p T is not a built-in primitive or CUDA vector type (e.g., \p short, \p int2, \p double, \p float2, etc.)
deps/thrust/dependencies/cub/cub/block/block_load.cuh: * \brief cub::BlockLoadAlgorithm enumerates alternative algorithms for cub::BlockLoad to read a linear segment of data from memory into a blocked arrangement across a CUDA thread block.
deps/thrust/dependencies/cub/cub/block/block_load.cuh:     * from memory using CUDA's built-in vectorized loads as a coalescing optimization.
deps/thrust/dependencies/cub/cub/block/block_load.cuh:     *   - The data type \p T is not a built-in primitive or CUDA vector type
deps/thrust/dependencies/cub/cub/block/block_load.cuh: * \brief The BlockLoad class provides [<em>collective</em>](index.html#sec0) data movement methods for loading a linear segment of items from memory into a [<em>blocked arrangement</em>](index.html#sec5sec3) across a CUDA thread block.  ![](block_load_logo.png)
deps/thrust/dependencies/cub/cub/block/block_load.cuh: *      of data is read directly from memory using CUDA's built-in vectorized loads as a
deps/thrust/dependencies/cub/cub/block/block_discontinuity.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/block/block_discontinuity.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/block/block_discontinuity.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/block/block_discontinuity.cuh: * The cub::BlockDiscontinuity class provides [<em>collective</em>](index.html#sec0) methods for flagging discontinuities within an ordered set of items partitioned across a CUDA thread block.
deps/thrust/dependencies/cub/cub/block/block_discontinuity.cuh: * \brief The BlockDiscontinuity class provides [<em>collective</em>](index.html#sec0) methods for flagging discontinuities within an ordered set of items partitioned across a CUDA thread block. ![](discont_logo.png)
deps/thrust/dependencies/cub/cub/block/block_radix_rank.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/block/block_radix_rank.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/block/block_radix_rank.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/block/block_radix_rank.cuh: * cub::BlockRadixRank provides operations for ranking unsigned integer types within a CUDA thread block
deps/thrust/dependencies/cub/cub/block/block_radix_rank.cuh: * \brief BlockRadixRank provides operations for ranking unsigned integer types within a CUDA thread block.
deps/thrust/dependencies/cub/cub/block/block_radix_rank.cuh: * \tparam SMEM_CONFIG          <b>[optional]</b> Shared memory bank mode (default: \p cudaSharedMemBankSizeFourByte)
deps/thrust/dependencies/cub/cub/block/block_radix_rank.cuh:    cudaSharedMemConfig     SMEM_CONFIG             = cudaSharedMemBankSizeFourByte,
deps/thrust/dependencies/cub/cub/block/block_radix_rank.cuh:      cub::detail::conditional_t<SMEM_CONFIG == cudaSharedMemBankSizeEightByte,
deps/thrust/dependencies/cub/cub/block/block_scan.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/block/block_scan.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/block/block_scan.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/block/block_scan.cuh: * The cub::BlockScan class provides [<em>collective</em>](index.html#sec0) methods for computing a parallel prefix sum/scan of items partitioned across a CUDA thread block.
deps/thrust/dependencies/cub/cub/block/block_scan.cuh: * \brief BlockScanAlgorithm enumerates alternative algorithms for cub::BlockScan to compute a parallel prefix scan across a CUDA thread block.
deps/thrust/dependencies/cub/cub/block/block_scan.cuh:     *   GPU is under-occupied, it can often provide higher overall throughput
deps/thrust/dependencies/cub/cub/block/block_scan.cuh:     *   across the GPU when suitably occupied.
deps/thrust/dependencies/cub/cub/block/block_scan.cuh:     *   GPU because due to a heavy reliance on inefficient warpscans, it can
deps/thrust/dependencies/cub/cub/block/block_scan.cuh:     *   often provide lower turnaround latencies when the GPU is under-occupied.
deps/thrust/dependencies/cub/cub/block/block_scan.cuh: * \brief The BlockScan class provides [<em>collective</em>](index.html#sec0) methods for computing a parallel prefix sum/scan of items partitioned across a CUDA thread block. ![](block_scan_logo.png)
deps/thrust/dependencies/cub/cub/block/block_run_length_decode.cuh: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/block/block_run_length_decode.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/block/block_run_length_decode.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/block/block_merge_sort.cuh: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/block/block_merge_sort.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/block/block_merge_sort.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/block/block_merge_sort.cuh:   * @brief Sorts items partitioned across a CUDA thread block using
deps/thrust/dependencies/cub/cub/block/block_merge_sort.cuh:   * @brief Sorts items partitioned across a CUDA thread block using
deps/thrust/dependencies/cub/cub/block/block_merge_sort.cuh:   * @brief Sorts items partitioned across a CUDA thread block using a merge sorting method.
deps/thrust/dependencies/cub/cub/block/block_merge_sort.cuh:   * @brief Sorts items partitioned across a CUDA thread block using
deps/thrust/dependencies/cub/cub/block/block_merge_sort.cuh:   * @brief Sorts items partitioned across a CUDA thread block using
deps/thrust/dependencies/cub/cub/block/block_merge_sort.cuh:   * @brief Sorts items partitioned across a CUDA thread block using
deps/thrust/dependencies/cub/cub/block/block_merge_sort.cuh:   * @brief Sorts items partitioned across a CUDA thread block using
deps/thrust/dependencies/cub/cub/block/block_merge_sort.cuh:   * @brief Sorts items partitioned across a CUDA thread block using
deps/thrust/dependencies/cub/cub/block/block_merge_sort.cuh: *        partitioned across a CUDA thread block using a merge sorting method.
deps/thrust/dependencies/cub/cub/block/block_shuffle.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/block/block_shuffle.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/block/block_shuffle.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/block/block_shuffle.cuh: * The cub::BlockShuffle class provides [<em>collective</em>](index.html#sec0) methods for shuffling data partitioned across a CUDA thread block.
deps/thrust/dependencies/cub/cub/block/block_shuffle.cuh: * \brief The BlockShuffle class provides [<em>collective</em>](index.html#sec0) methods for shuffling data partitioned across a CUDA thread block.
deps/thrust/dependencies/cub/cub/block/radix_rank_sort_operations.cuh: * Copyright (c) 2011-2020, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/block/radix_rank_sort_operations.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/block/radix_rank_sort_operations.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/block/block_exchange.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/block/block_exchange.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/block/block_exchange.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/block/block_exchange.cuh: * The cub::BlockExchange class provides [<em>collective</em>](index.html#sec0) methods for rearranging data partitioned across a CUDA thread block.
deps/thrust/dependencies/cub/cub/block/block_exchange.cuh: * \brief The BlockExchange class provides [<em>collective</em>](index.html#sec0) methods for rearranging data partitioned across a CUDA thread block. ![](transpose_logo.png)
deps/thrust/dependencies/cub/cub/block/block_histogram.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/block/block_histogram.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/block/block_histogram.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/block/block_histogram.cuh: * The cub::BlockHistogram class provides [<em>collective</em>](index.html#sec0) methods for constructing block-wide histograms from data samples partitioned across a CUDA thread block.
deps/thrust/dependencies/cub/cub/block/block_histogram.cuh: * \brief The BlockHistogram class provides [<em>collective</em>](index.html#sec0) methods for constructing block-wide histograms from data samples partitioned across a CUDA thread block. ![](histogram_logo.png)
deps/thrust/dependencies/cub/cub/block/block_reduce.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/block/block_reduce.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/block/block_reduce.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/block/block_reduce.cuh: * The cub::BlockReduce class provides [<em>collective</em>](index.html#sec0) methods for computing a parallel reduction of items partitioned across a CUDA thread block.
deps/thrust/dependencies/cub/cub/block/block_reduce.cuh: * reduction across a CUDA thread block.
deps/thrust/dependencies/cub/cub/block/block_reduce.cuh:     *   throughput across the GPU when suitably occupied.  However, turn-around latency may be
deps/thrust/dependencies/cub/cub/block/block_reduce.cuh:     *   when the GPU is under-occupied.
deps/thrust/dependencies/cub/cub/block/block_reduce.cuh:     *   throughput across the GPU when suitably occupied.  However, turn-around latency may be
deps/thrust/dependencies/cub/cub/block/block_reduce.cuh:     *   when the GPU is under-occupied.
deps/thrust/dependencies/cub/cub/block/block_reduce.cuh:     *   throughput across the GPU.  However turn-around latency may be lower and
deps/thrust/dependencies/cub/cub/block/block_reduce.cuh:     *   thus useful when the GPU is under-occupied.
deps/thrust/dependencies/cub/cub/block/block_reduce.cuh: * \brief The BlockReduce class provides [<em>collective</em>](index.html#sec0) methods for computing a parallel reduction of items partitioned across a CUDA thread block. ![](reduce_logo.png)
deps/thrust/dependencies/cub/cub/detail/device_double_buffer.cuh: *  Copyright 2021 NVIDIA Corporation
deps/thrust/dependencies/cub/cub/detail/temporary_storage.cuh:*  Copyright 2021 NVIDIA Corporation
deps/thrust/dependencies/cub/cub/detail/temporary_storage.cuh:      // `cudaMalloc` will return `nullptr`. This fact makes it impossible to
deps/thrust/dependencies/cub/cub/detail/temporary_storage.cuh:  __host__ __device__ cudaError_t map_to_buffer(void *d_temp_storage,
deps/thrust/dependencies/cub/cub/detail/temporary_storage.cuh:      return cudaErrorAlreadyMapped;
deps/thrust/dependencies/cub/cub/detail/temporary_storage.cuh:    cudaError_t error = cudaSuccess;
deps/thrust/dependencies/cub/cub/detail/type_traits.cuh: * Copyright (c) 2021, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/detail/type_traits.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/detail/type_traits.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/detail/device_synchronize.cuh: *  Copyright 2021 NVIDIA Corporation
deps/thrust/dependencies/cub/cub/detail/device_synchronize.cuh:#include <cuda_runtime_api.h>
deps/thrust/dependencies/cub/cub/detail/device_synchronize.cuh: * Call `cudaDeviceSynchronize()` using the proper API for the current CUB and
deps/thrust/dependencies/cub/cub/detail/device_synchronize.cuh: * CUDA configuration.
deps/thrust/dependencies/cub/cub/detail/device_synchronize.cuh:CUB_RUNTIME_FUNCTION inline cudaError_t device_synchronize()
deps/thrust/dependencies/cub/cub/detail/device_synchronize.cuh:  cudaError_t result = cudaErrorUnknown;
deps/thrust/dependencies/cub/cub/detail/device_synchronize.cuh:    result = cudaDeviceSynchronize();
deps/thrust/dependencies/cub/cub/detail/device_synchronize.cuh:    // Device code with the CUDA runtime.
deps/thrust/dependencies/cub/cub/detail/device_synchronize.cuh:#if defined(__CUDACC__) &&                                                     \
deps/thrust/dependencies/cub/cub/detail/device_synchronize.cuh:  ((__CUDACC_VER_MAJOR__ > 11) ||                                              \
deps/thrust/dependencies/cub/cub/detail/device_synchronize.cuh:   ((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 6)))
deps/thrust/dependencies/cub/cub/detail/device_synchronize.cuh:    // CUDA >= 11.6
deps/thrust/dependencies/cub/cub/detail/device_synchronize.cuh:    result = __cudaDeviceSynchronizeDeprecationAvoidance();
deps/thrust/dependencies/cub/cub/detail/device_synchronize.cuh:#else // CUDA < 11.6
deps/thrust/dependencies/cub/cub/detail/device_synchronize.cuh:    result = cudaDeviceSynchronize();
deps/thrust/dependencies/cub/cub/detail/device_synchronize.cuh:#else // Device code without the CUDA runtime.
deps/thrust/dependencies/cub/cub/detail/device_synchronize.cuh:    // Device side CUDA API calls are not supported in this configuration.
deps/thrust/dependencies/cub/cub/detail/device_synchronize.cuh:    result = cudaErrorInvalidConfiguration;
deps/thrust/dependencies/cub/cub/detail/exec_check_disable.cuh:*  Copyright 2021 NVIDIA Corporation
deps/thrust/dependencies/cub/cub/detail/exec_check_disable.cuh:#if defined(__CUDACC__) && \
deps/thrust/dependencies/cub/cub/detail/exec_check_disable.cuh:    !defined(_NVHPC_CUDA) && \
deps/thrust/dependencies/cub/cub/detail/exec_check_disable.cuh:    !(defined(__CUDA__) && defined(__clang__))
deps/thrust/dependencies/cub/cub/util_allocator.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/util_allocator.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/util_allocator.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/util_allocator.cuh:        cudaStream_t    associated_stream;  // Associated associated_stream
deps/thrust/dependencies/cub/cub/util_allocator.cuh:        cudaEvent_t     ready_event;        // Signal when associated stream has run to the point at which this block was freed
deps/thrust/dependencies/cub/cub/util_allocator.cuh:    typedef std::map<int, TotalBytes> GpuCachedBytes;
deps/thrust/dependencies/cub/cub/util_allocator.cuh:    const bool      skip_cleanup;       /// Whether or not to skip a call to FreeAllCached() when destructor is called.  (The CUDA runtime may have already shut down for statically declared allocators)
deps/thrust/dependencies/cub/cub/util_allocator.cuh:    GpuCachedBytes  cached_bytes;       /// Map of device ordinal to aggregate cached bytes on that device
deps/thrust/dependencies/cub/cub/util_allocator.cuh:    cudaError_t SetMaxCachedBytes(size_t max_cached_bytes_)
deps/thrust/dependencies/cub/cub/util_allocator.cuh:        return cudaSuccess;
deps/thrust/dependencies/cub/cub/util_allocator.cuh:    cudaError_t DeviceAllocate(
deps/thrust/dependencies/cub/cub/util_allocator.cuh:        cudaStream_t    active_stream = 0)  ///< [in] The stream to be associated with this allocation
deps/thrust/dependencies/cub/cub/util_allocator.cuh:        cudaError_t error               = cudaSuccess;
deps/thrust/dependencies/cub/cub/util_allocator.cuh:            if (CubDebug(error = cudaGetDevice(&entrypoint_device))) return error;
deps/thrust/dependencies/cub/cub/util_allocator.cuh:                    const cudaError_t event_status = cudaEventQuery(block_itr->ready_event);
deps/thrust/dependencies/cub/cub/util_allocator.cuh:                    if(event_status != cudaErrorNotReady)
deps/thrust/dependencies/cub/cub/util_allocator.cuh:                if (CubDebug(error = cudaGetDevice(&entrypoint_device))) return error;
deps/thrust/dependencies/cub/cub/util_allocator.cuh:                if (CubDebug(error = cudaSetDevice(device))) return error;
deps/thrust/dependencies/cub/cub/util_allocator.cuh:            if (CubDebug(error = cudaMalloc(&search_key.d_ptr, search_key.bytes)) == cudaErrorMemoryAllocation)
deps/thrust/dependencies/cub/cub/util_allocator.cuh:                error = cudaSuccess;    // Reset the error we will return
deps/thrust/dependencies/cub/cub/util_allocator.cuh:                cudaGetLastError();     // Reset CUDART's error
deps/thrust/dependencies/cub/cub/util_allocator.cuh:                    // No need to worry about synchronization with the device: cudaFree is
deps/thrust/dependencies/cub/cub/util_allocator.cuh:                    if (CubDebug(error = cudaFree(block_itr->d_ptr))) break;
deps/thrust/dependencies/cub/cub/util_allocator.cuh:                    if (CubDebug(error = cudaEventDestroy(block_itr->ready_event))) break;
deps/thrust/dependencies/cub/cub/util_allocator.cuh:                if (CubDebug(error = cudaMalloc(&search_key.d_ptr, search_key.bytes))) return error;
deps/thrust/dependencies/cub/cub/util_allocator.cuh:            if (CubDebug(error = cudaEventCreateWithFlags(&search_key.ready_event, cudaEventDisableTiming)))
deps/thrust/dependencies/cub/cub/util_allocator.cuh:                if (CubDebug(error = cudaSetDevice(entrypoint_device))) return error;
deps/thrust/dependencies/cub/cub/util_allocator.cuh:    cudaError_t DeviceAllocate(
deps/thrust/dependencies/cub/cub/util_allocator.cuh:        cudaStream_t    active_stream = 0)  ///< [in] The stream to be associated with this allocation
deps/thrust/dependencies/cub/cub/util_allocator.cuh:    cudaError_t DeviceFree(
deps/thrust/dependencies/cub/cub/util_allocator.cuh:        cudaError_t error               = cudaSuccess;
deps/thrust/dependencies/cub/cub/util_allocator.cuh:            if (CubDebug(error = cudaGetDevice(&entrypoint_device)))
deps/thrust/dependencies/cub/cub/util_allocator.cuh:            if (CubDebug(error = cudaGetDevice(&entrypoint_device))) return error;
deps/thrust/dependencies/cub/cub/util_allocator.cuh:            if (CubDebug(error = cudaSetDevice(device))) return error;
deps/thrust/dependencies/cub/cub/util_allocator.cuh:            if (CubDebug(error = cudaEventRecord(search_key.ready_event, search_key.associated_stream))) return error;
deps/thrust/dependencies/cub/cub/util_allocator.cuh:            if (CubDebug(error = cudaFree(d_ptr))) return error;
deps/thrust/dependencies/cub/cub/util_allocator.cuh:            if (CubDebug(error = cudaEventDestroy(search_key.ready_event))) return error;
deps/thrust/dependencies/cub/cub/util_allocator.cuh:            if (CubDebug(error = cudaSetDevice(entrypoint_device))) return error;
deps/thrust/dependencies/cub/cub/util_allocator.cuh:    cudaError_t DeviceFree(
deps/thrust/dependencies/cub/cub/util_allocator.cuh:    cudaError_t FreeAllCached()
deps/thrust/dependencies/cub/cub/util_allocator.cuh:        cudaError_t error         = cudaSuccess;
deps/thrust/dependencies/cub/cub/util_allocator.cuh:                if (CubDebug(error = cudaGetDevice(&entrypoint_device))) break;
deps/thrust/dependencies/cub/cub/util_allocator.cuh:                if (CubDebug(error = cudaSetDevice(begin->device))) break;
deps/thrust/dependencies/cub/cub/util_allocator.cuh:            if (CubDebug(error = cudaFree(begin->d_ptr))) break;
deps/thrust/dependencies/cub/cub/util_allocator.cuh:            if (CubDebug(error = cudaEventDestroy(begin->ready_event))) break;
deps/thrust/dependencies/cub/cub/util_allocator.cuh:            if (CubDebug(error = cudaSetDevice(entrypoint_device))) return error;
deps/thrust/dependencies/cub/cub/util_math.cuh: * Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/util_math.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/util_math.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/host/mutex.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/host/mutex.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/host/mutex.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/util_compiler.cuh: * Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/util_compiler.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/util_compiler.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/util_compiler.cuh:#if defined(__CUDACC__) || defined(_NVHPC_CUDA)
deps/thrust/dependencies/cub/cub/util_compiler.cuh:// CUDA-capable clang should behave similar to NVCC.
deps/thrust/dependencies/cub/cub/util_compiler.cuh:#  if defined(__CUDA__)
deps/thrust/dependencies/cub/cub/iterator/discard_output_iterator.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/iterator/discard_output_iterator.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/iterator/discard_output_iterator.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/iterator/constant_input_iterator.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/iterator/constant_input_iterator.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/iterator/constant_input_iterator.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/iterator/arg_index_input_iterator.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/iterator/arg_index_input_iterator.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/iterator/arg_index_input_iterator.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/iterator/tex_obj_input_iterator.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/iterator/tex_obj_input_iterator.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/iterator/tex_obj_input_iterator.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/iterator/tex_obj_input_iterator.cuh:    cudaTextureObject_t tex_obj;
deps/thrust/dependencies/cub/cub/iterator/tex_obj_input_iterator.cuh:    cudaError_t BindTexture(
deps/thrust/dependencies/cub/cub/iterator/tex_obj_input_iterator.cuh:        QualifiedT      *ptr,               ///< Native pointer to wrap that is aligned to cudaDeviceProp::textureAlignment
deps/thrust/dependencies/cub/cub/iterator/tex_obj_input_iterator.cuh:        cudaChannelFormatDesc   channel_desc = cudaCreateChannelDesc<TextureWord>();
deps/thrust/dependencies/cub/cub/iterator/tex_obj_input_iterator.cuh:        cudaResourceDesc        res_desc;
deps/thrust/dependencies/cub/cub/iterator/tex_obj_input_iterator.cuh:        cudaTextureDesc         tex_desc;
deps/thrust/dependencies/cub/cub/iterator/tex_obj_input_iterator.cuh:        memset(&res_desc, 0, sizeof(cudaResourceDesc));
deps/thrust/dependencies/cub/cub/iterator/tex_obj_input_iterator.cuh:        memset(&tex_desc, 0, sizeof(cudaTextureDesc));
deps/thrust/dependencies/cub/cub/iterator/tex_obj_input_iterator.cuh:        res_desc.resType                = cudaResourceTypeLinear;
deps/thrust/dependencies/cub/cub/iterator/tex_obj_input_iterator.cuh:        tex_desc.readMode               = cudaReadModeElementType;
deps/thrust/dependencies/cub/cub/iterator/tex_obj_input_iterator.cuh:        return CubDebug(cudaCreateTextureObject(&tex_obj, &res_desc, &tex_desc, NULL));
deps/thrust/dependencies/cub/cub/iterator/tex_obj_input_iterator.cuh:    cudaError_t UnbindTexture()
deps/thrust/dependencies/cub/cub/iterator/tex_obj_input_iterator.cuh:        return CubDebug(cudaDestroyTextureObject(tex_obj));
deps/thrust/dependencies/cub/cub/iterator/transform_input_iterator.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/iterator/transform_input_iterator.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/iterator/transform_input_iterator.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/iterator/cache_modified_output_iterator.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/iterator/cache_modified_output_iterator.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/iterator/cache_modified_output_iterator.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/iterator/cache_modified_input_iterator.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/iterator/cache_modified_input_iterator.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/iterator/cache_modified_input_iterator.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/iterator/counting_input_iterator.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/iterator/counting_input_iterator.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/iterator/counting_input_iterator.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/iterator/tex_ref_input_iterator.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/iterator/tex_ref_input_iterator.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/iterator/tex_ref_input_iterator.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/iterator/tex_ref_input_iterator.cuh:#if (CUDART_VERSION >= 5050) || defined(DOXYGEN_ACTIVE)  // This iterator is compatible with CUDA 5.5 and newer
deps/thrust/dependencies/cub/cub/iterator/tex_ref_input_iterator.cuh:// This class uses the deprecated cudaBindTexture / cudaUnbindTexture APIs.
deps/thrust/dependencies/cub/cub/iterator/tex_ref_input_iterator.cuh:// See issue NVIDIA/cub#191.
deps/thrust/dependencies/cub/cub/iterator/tex_ref_input_iterator.cuh:        static cudaError_t BindTexture(void *d_in, size_t &bytes, size_t &offset)
deps/thrust/dependencies/cub/cub/iterator/tex_ref_input_iterator.cuh:                cudaChannelFormatDesc tex_desc = cudaCreateChannelDesc<TextureWord>();
deps/thrust/dependencies/cub/cub/iterator/tex_ref_input_iterator.cuh:                return (CubDebug(cudaBindTexture(&offset, ref, d_in, bytes)));
deps/thrust/dependencies/cub/cub/iterator/tex_ref_input_iterator.cuh:            return cudaSuccess;
deps/thrust/dependencies/cub/cub/iterator/tex_ref_input_iterator.cuh:        static cudaError_t UnbindTexture()
deps/thrust/dependencies/cub/cub/iterator/tex_ref_input_iterator.cuh:            return CubDebug(cudaUnbindTexture(ref));
deps/thrust/dependencies/cub/cub/iterator/tex_ref_input_iterator.cuh: * \deprecated [Since 1.13.0] The CUDA texture management APIs used by
deps/thrust/dependencies/cub/cub/iterator/tex_ref_input_iterator.cuh:// This class uses the deprecated cudaBindTexture / cudaUnbindTexture APIs.
deps/thrust/dependencies/cub/cub/iterator/tex_ref_input_iterator.cuh:// See issue NVIDIA/cub#191.
deps/thrust/dependencies/cub/cub/iterator/tex_ref_input_iterator.cuh:    cudaError_t BindTexture(
deps/thrust/dependencies/cub/cub/iterator/tex_ref_input_iterator.cuh:        QualifiedT      *ptr,                   ///< Native pointer to wrap that is aligned to cudaDeviceProp::textureAlignment
deps/thrust/dependencies/cub/cub/iterator/tex_ref_input_iterator.cuh:        cudaError_t retval = TexId::BindTexture(this->ptr + tex_offset, bytes, offset);
deps/thrust/dependencies/cub/cub/iterator/tex_ref_input_iterator.cuh:    cudaError_t UnbindTexture()
deps/thrust/dependencies/cub/cub/iterator/tex_ref_input_iterator.cuh:#endif // CUDART_VERSION
deps/thrust/dependencies/cub/cub/device/device_merge_sort.cuh: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/device/device_merge_sort.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/device/device_merge_sort.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/device/device_merge_sort.cuh: * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_merge_sort.cuh:   * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_merge_sort.cuh:   *   **[optional]** CUDA stream to launch kernels within. Default is
deps/thrust/dependencies/cub/cub/device/device_merge_sort.cuh:  CUB_RUNTIME_FUNCTION static cudaError_t
deps/thrust/dependencies/cub/cub/device/device_merge_sort.cuh:            cudaStream_t stream    = 0,
deps/thrust/dependencies/cub/cub/device/device_merge_sort.cuh:   * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_merge_sort.cuh:   *   **[optional]** CUDA stream to launch kernels within. Default is
deps/thrust/dependencies/cub/cub/device/device_merge_sort.cuh:  CUB_RUNTIME_FUNCTION static cudaError_t
deps/thrust/dependencies/cub/cub/device/device_merge_sort.cuh:                cudaStream_t stream    = 0,
deps/thrust/dependencies/cub/cub/device/device_merge_sort.cuh:   * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_merge_sort.cuh:   *   **[optional]** CUDA stream to launch kernels within. Default is
deps/thrust/dependencies/cub/cub/device/device_merge_sort.cuh:  CUB_RUNTIME_FUNCTION static cudaError_t
deps/thrust/dependencies/cub/cub/device/device_merge_sort.cuh:           cudaStream_t stream    = 0,
deps/thrust/dependencies/cub/cub/device/device_merge_sort.cuh:   * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_merge_sort.cuh:   *   **[optional]** CUDA stream to launch kernels within. Default is
deps/thrust/dependencies/cub/cub/device/device_merge_sort.cuh:  CUB_RUNTIME_FUNCTION static cudaError_t
deps/thrust/dependencies/cub/cub/device/device_merge_sort.cuh:               cudaStream_t stream    = 0,
deps/thrust/dependencies/cub/cub/device/device_merge_sort.cuh:   * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_merge_sort.cuh:   *   **[optional]** CUDA stream to launch kernels within. Default is
deps/thrust/dependencies/cub/cub/device/device_merge_sort.cuh:  CUB_RUNTIME_FUNCTION static cudaError_t
deps/thrust/dependencies/cub/cub/device/device_merge_sort.cuh:                  cudaStream_t stream    = 0,
deps/thrust/dependencies/cub/cub/device/device_merge_sort.cuh:   * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_merge_sort.cuh:   *   **[optional]** CUDA stream to launch kernels within. Default is
deps/thrust/dependencies/cub/cub/device/device_merge_sort.cuh:  CUB_RUNTIME_FUNCTION static cudaError_t
deps/thrust/dependencies/cub/cub/device/device_merge_sort.cuh:                 cudaStream_t stream    = 0,
deps/thrust/dependencies/cub/cub/device/device_select.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/device/device_select.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/device/device_select.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/device/device_select.cuh: * performance across different CUDA architectures for \p int32 items,
deps/thrust/dependencies/cub/cub/device/device_select.cuh: * performance across different CUDA architectures for \p int32 items
deps/thrust/dependencies/cub/cub/device/device_select.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_select.cuh:    static cudaError_t Flagged(
deps/thrust/dependencies/cub/cub/device/device_select.cuh:        cudaStream_t                stream             = 0,         ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/device_select.cuh:     * CUDA architectures for \p int32 and \p int64 items, respectively.  Items are
deps/thrust/dependencies/cub/cub/device/device_select.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_select.cuh:    static cudaError_t If(
deps/thrust/dependencies/cub/cub/device/device_select.cuh:        cudaStream_t                stream             = 0,         ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/device_select.cuh:     * CUDA architectures for \p int32 and \p int64 items, respectively.  Segments have
deps/thrust/dependencies/cub/cub/device/device_select.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_select.cuh:    static cudaError_t Unique(
deps/thrust/dependencies/cub/cub/device/device_select.cuh:        cudaStream_t                stream             = 0,         ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/device_scan.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/device/device_scan.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/device/device_scan.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/device/device_scan.cuh: * [1] [Duane Merrill and Michael Garland.  "Single-pass Parallel Prefix Scan with Decoupled Look-back", <em>NVIDIA Technical Report NVR-2016-002</em>, 2016.](https://research.nvidia.com/publication/single-pass-parallel-prefix-scan-decoupled-look-back)
deps/thrust/dependencies/cub/cub/device/device_scan.cuh: * performance across different CUDA architectures for \p int32 keys.
deps/thrust/dependencies/cub/cub/device/device_scan.cuh:     *   (e.g., addition of floating point types) on the same GPU device.
deps/thrust/dependencies/cub/cub/device/device_scan.cuh:     * CUDA architectures for \p int32 and \p int64 items, respectively.
deps/thrust/dependencies/cub/cub/device/device_scan.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_scan.cuh:    static cudaError_t ExclusiveSum(
deps/thrust/dependencies/cub/cub/device/device_scan.cuh:        cudaStream_t    stream              = 0,            ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/device_scan.cuh:     *   (e.g., addition of floating point types) on the same GPU device.
deps/thrust/dependencies/cub/cub/device/device_scan.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_scan.cuh:    static cudaError_t ExclusiveScan(
deps/thrust/dependencies/cub/cub/device/device_scan.cuh:        cudaStream_t    stream              = 0,            ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/device_scan.cuh:    static cudaError_t ExclusiveScan(
deps/thrust/dependencies/cub/cub/device/device_scan.cuh:        cudaStream_t                            stream              = 0,            ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/device_scan.cuh:     *   (e.g., addition of floating point types) on the same GPU device.
deps/thrust/dependencies/cub/cub/device/device_scan.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_scan.cuh:    static cudaError_t InclusiveSum(
deps/thrust/dependencies/cub/cub/device/device_scan.cuh:        cudaStream_t        stream             = 0,         ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/device_scan.cuh:     *   (e.g., addition of floating point types) on the same GPU device.
deps/thrust/dependencies/cub/cub/device/device_scan.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_scan.cuh:    static cudaError_t InclusiveScan(
deps/thrust/dependencies/cub/cub/device/device_scan.cuh:        cudaStream_t    stream             = 0,             ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/device_scan.cuh:     *   (e.g., addition of floating point types) on the same GPU device.
deps/thrust/dependencies/cub/cub/device/device_scan.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_scan.cuh:    static cudaError_t ExclusiveSumByKey(
deps/thrust/dependencies/cub/cub/device/device_scan.cuh:        cudaStream_t          stream=0,                     ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/device_scan.cuh:     *   (e.g., addition of floating point types) on the same GPU device.
deps/thrust/dependencies/cub/cub/device/device_scan.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_scan.cuh:    static cudaError_t ExclusiveScanByKey(
deps/thrust/dependencies/cub/cub/device/device_scan.cuh:        cudaStream_t          stream=0,                     ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/device_scan.cuh:     *   (e.g., addition of floating point types) on the same GPU device.
deps/thrust/dependencies/cub/cub/device/device_scan.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_scan.cuh:    static cudaError_t InclusiveSumByKey(
deps/thrust/dependencies/cub/cub/device/device_scan.cuh:        cudaStream_t          stream=0,                     ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/device_scan.cuh:     *   (e.g., addition of floating point types) on the same GPU device.
deps/thrust/dependencies/cub/cub/device/device_scan.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_scan.cuh:    static cudaError_t InclusiveScanByKey(
deps/thrust/dependencies/cub/cub/device/device_scan.cuh:        cudaStream_t          stream=0,                     ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_radix_sort.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_radix_sort.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_radix_sort.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_radix_sort.cuh:#include <thrust/system/cuda/detail/core/triple_chevron_launch.h>
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_radix_sort.cuh:    cudaStream_t            stream;                 ///< [in] CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_radix_sort.cuh:        cudaStream_t            stream,
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_radix_sort.cuh:    cudaError_t InvokeSingleTile(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_radix_sort.cuh:        return CubDebug(cudaErrorNotSupported );
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_radix_sort.cuh:        cudaError error = cudaSuccess;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_radix_sort.cuh:            THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_radix_sort.cuh:            if (CubDebug(error = cudaPeekAtLastError())) break;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_radix_sort.cuh:    cudaError_t InvokePass(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_radix_sort.cuh:        cudaError error = cudaSuccess;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_radix_sort.cuh:            THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_radix_sort.cuh:            if (CubDebug(error = cudaPeekAtLastError())) break;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_radix_sort.cuh:            THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_radix_sort.cuh:            if (CubDebug(error = cudaPeekAtLastError())) break;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_radix_sort.cuh:            THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_radix_sort.cuh:            if (CubDebug(error = cudaPeekAtLastError())) break;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_radix_sort.cuh:        cudaError_t InitPassConfig(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_radix_sort.cuh:            cudaError error = cudaSuccess;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_radix_sort.cuh:    cudaError_t InvokeOnesweep()
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_radix_sort.cuh:        cudaError_t error = cudaSuccess;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_radix_sort.cuh:            if (CubDebug(error = cudaMemsetAsync(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_radix_sort.cuh:            if (CubDebug(error = cudaMemsetAsync
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_radix_sort.cuh:            if (CubDebug(error = cudaGetDevice(&device))) break;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_radix_sort.cuh:            if (CubDebug(error = cudaDeviceGetAttribute(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_radix_sort.cuh:                   &num_sms, cudaDevAttrMultiProcessorCount, device))) break;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_radix_sort.cuh:            if (CubDebug(error = cudaOccupancyMaxActiveBlocksPerMultiprocessor(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_radix_sort.cuh:            if (CubDebug(error = cudaPeekAtLastError())) break;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_radix_sort.cuh:            if (CubDebug(error = cudaPeekAtLastError())) break;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_radix_sort.cuh:                    if (CubDebug(error = cudaMemsetAsync(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_radix_sort.cuh:                    if (CubDebug(error = cudaPeekAtLastError())) break;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_radix_sort.cuh:    cudaError_t InvokePasses(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_radix_sort.cuh:        return CubDebug(cudaErrorNotSupported );
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_radix_sort.cuh:        cudaError error = cudaSuccess;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_radix_sort.cuh:            if (CubDebug(error = cudaGetDevice(&device_ordinal))) break;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_radix_sort.cuh:            if (CubDebug(error = cudaDeviceGetAttribute (&sm_count, cudaDevAttrMultiProcessorCount, device_ordinal))) break;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_radix_sort.cuh:                return cudaSuccess;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_radix_sort.cuh:    cudaError_t InvokeManyTiles(Int2Type<false>)
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_radix_sort.cuh:    cudaError_t InvokeManyTiles(Int2Type<true>)
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_radix_sort.cuh:    cudaError_t Invoke()
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_radix_sort.cuh:          return cudaSuccess;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_radix_sort.cuh:    static cudaError_t Dispatch(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_radix_sort.cuh:        cudaStream_t            stream,                 ///< [in] CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_radix_sort.cuh:        cudaError_t error;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_radix_sort.cuh:    cudaStream_t            stream;                 ///< [in] CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_radix_sort.cuh:        cudaStream_t            stream,
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_radix_sort.cuh:    cudaError_t InvokePass(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_radix_sort.cuh:        cudaError error = cudaSuccess;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_radix_sort.cuh:            THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_radix_sort.cuh:            if (CubDebug(error = cudaPeekAtLastError())) break;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_radix_sort.cuh:        cudaError_t InitPassConfig(SegmentedKernelT segmented_kernel)
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_radix_sort.cuh:    cudaError_t InvokePasses(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_radix_sort.cuh:        return CubDebug(cudaErrorNotSupported );
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_radix_sort.cuh:        cudaError error = cudaSuccess;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_radix_sort.cuh:                return cudaSuccess;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_radix_sort.cuh:    cudaError_t Invoke()
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_radix_sort.cuh:          return cudaSuccess;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_radix_sort.cuh:    static cudaError_t Dispatch(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_radix_sort.cuh:        cudaStream_t            stream,                 ///< [in] CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_radix_sort.cuh:        cudaError_t error;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_scan.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_scan.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_scan.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_scan.cuh:#include <thrust/system/cuda/detail/core/triple_chevron_launch.h>
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_scan.cuh:    cudaStream_t    stream;                 ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_scan.cuh:        cudaStream_t    stream,                 ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_scan.cuh:    cudaError_t Invoke(InitKernel init_kernel, ScanKernel scan_kernel)
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_scan.cuh:        return CubDebug(cudaErrorNotSupported);
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_scan.cuh:        cudaError error = cudaSuccess;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_scan.cuh:            if (CubDebug(error = cudaGetDevice(&device_ordinal))) break;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_scan.cuh:            THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_scan.cuh:            if (CubDebug(error = cudaPeekAtLastError())) break;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_scan.cuh:            if (CubDebug(error = cudaDeviceGetAttribute(&max_dim_x, cudaDevAttrMaxGridDimX, device_ordinal))) break;;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_scan.cuh:                THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_scan.cuh:                if (CubDebug(error = cudaPeekAtLastError())) break;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_scan.cuh:    cudaError_t Invoke()
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_scan.cuh:    static cudaError_t Dispatch(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_scan.cuh:        cudaStream_t    stream,                 ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_scan.cuh:        cudaError_t error;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_merge_sort.cuh: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_merge_sort.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_merge_sort.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_merge_sort.cuh:#include <thrust/system/cuda/detail/core/triple_chevron_launch.h>
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_merge_sort.cuh:                        THRUST_NS_QUALIFIER::cuda_cub::core::make_load_iterator(ActivePolicyT(), keys_in),
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_merge_sort.cuh:                        THRUST_NS_QUALIFIER::cuda_cub::core::make_load_iterator(ActivePolicyT(), items_in),
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_merge_sort.cuh:    THRUST_NS_QUALIFIER::cuda_cub::core::make_load_iterator(ActivePolicyT(), keys_ping),
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_merge_sort.cuh:    THRUST_NS_QUALIFIER::cuda_cub::core::make_load_iterator(ActivePolicyT(), items_ping),
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_merge_sort.cuh:    THRUST_NS_QUALIFIER::cuda_cub::core::make_load_iterator(ActivePolicyT(), keys_pong),
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_merge_sort.cuh:    THRUST_NS_QUALIFIER::cuda_cub::core::make_load_iterator(ActivePolicyT(), items_pong),
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_merge_sort.cuh:#if defined(_NVHPC_CUDA)
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_merge_sort.cuh:  cudaStream_t stream;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_merge_sort.cuh:                    cudaStream_t stream,
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_merge_sort.cuh:    THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_merge_sort.cuh:  cudaStream_t stream;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_merge_sort.cuh:                                                     cudaStream_t stream,
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_merge_sort.cuh:    THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_merge_sort.cuh:  /// CUDA stream to launch kernels within. Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_merge_sort.cuh:  cudaStream_t stream;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_merge_sort.cuh:                    cudaStream_t stream,
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_merge_sort.cuh:  CUB_RUNTIME_FUNCTION __forceinline__ cudaError_t Invoke()
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_merge_sort.cuh:    cudaError error = cudaSuccess;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_merge_sort.cuh:      if (CubDebug(error = cudaGetDevice(&device_ordinal)))
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_merge_sort.cuh:              error = cudaDeviceGetAttribute(&max_shmem,
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_merge_sort.cuh:                                             cudaDevAttrMaxSharedMemoryPerBlock,
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_merge_sort.cuh:      if (CubDebug(error = cudaPeekAtLastError()))
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_merge_sort.cuh:        THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_merge_sort.cuh:        if (CubDebug(error = cudaPeekAtLastError()))
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_merge_sort.cuh:        if (CubDebug(error = cudaPeekAtLastError()))
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_merge_sort.cuh:  CUB_RUNTIME_FUNCTION __forceinline__ static cudaError_t
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_merge_sort.cuh:           cudaStream_t stream,
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_merge_sort.cuh:    cudaError error = cudaSuccess;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_rle.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_rle.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_rle.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_rle.cuh:#include <thrust/system/cuda/detail/core/triple_chevron_launch.h>
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_rle.cuh:    static cudaError_t Dispatch(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_rle.cuh:        cudaStream_t                stream,                         ///< [in] CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_rle.cuh:        return CubDebug(cudaErrorNotSupported);
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_rle.cuh:        cudaError error = cudaSuccess;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_rle.cuh:            if (CubDebug(error = cudaGetDevice(&device_ordinal))) break;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_rle.cuh:            THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_rle.cuh:            if (CubDebug(error = cudaPeekAtLastError())) break;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_rle.cuh:            if (CubDebug(error = cudaDeviceGetAttribute(&max_dim_x, cudaDevAttrMaxGridDimX, device_ordinal))) break;;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_rle.cuh:            THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_rle.cuh:            if (CubDebug(error = cudaPeekAtLastError())) break;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_rle.cuh:    static cudaError_t Dispatch(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_rle.cuh:        cudaStream_t                stream,                         ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_rle.cuh:        cudaError error = cudaSuccess;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_histogram.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_histogram.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_histogram.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_histogram.cuh:#include <thrust/system/cuda/detail/core/triple_chevron_launch.h>
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_histogram.cuh:    static cudaError_t InitConfigs(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_histogram.cuh:        cudaError_t result = cudaErrorNotSupported;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_histogram.cuh:        cudaError_t Init()
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_histogram.cuh:            return cudaSuccess;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_histogram.cuh:    static cudaError_t PrivatizedDispatch(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_histogram.cuh:        cudaStream_t                        stream,                                         ///< [in] CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_histogram.cuh:        return CubDebug(cudaErrorNotSupported);
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_histogram.cuh:        cudaError error = cudaSuccess;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_histogram.cuh:            if (CubDebug(error = cudaGetDevice(&device_ordinal))) break;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_histogram.cuh:            if (CubDebug(error = cudaDeviceGetAttribute (&sm_count, cudaDevAttrMultiProcessorCount, device_ordinal))) break;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_histogram.cuh:            THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_histogram.cuh:            THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_histogram.cuh:            if (CubDebug(error = cudaPeekAtLastError())) break;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_histogram.cuh:    static cudaError_t DispatchRange(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_histogram.cuh:        cudaStream_t        stream,                                     ///< [in] CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_histogram.cuh:        cudaError error = cudaSuccess;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_histogram.cuh:    static cudaError_t DispatchRange(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_histogram.cuh:        cudaStream_t        stream,                                     ///< [in] CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_histogram.cuh:        cudaError error = cudaSuccess;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_histogram.cuh:    static cudaError_t DispatchEven(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_histogram.cuh:        cudaStream_t        stream,                                     ///< [in] CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_histogram.cuh:        cudaError error = cudaSuccess;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_histogram.cuh:    static cudaError_t DispatchEven(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_histogram.cuh:        cudaStream_t        stream,                                     ///< [in] CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_histogram.cuh:        cudaError error = cudaSuccess;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_adjacent_difference.cuh: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_adjacent_difference.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_adjacent_difference.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_adjacent_difference.cuh:#include <thrust/system/cuda/detail/core/triple_chevron_launch.h>
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_adjacent_difference.cuh:  cudaStream_t stream;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_adjacent_difference.cuh:                             cudaStream_t stream,
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_adjacent_difference.cuh:  CUB_RUNTIME_FUNCTION __forceinline__ cudaError_t Invoke()
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_adjacent_difference.cuh:    cudaError error = cudaSuccess;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_adjacent_difference.cuh:        THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(init_grid_size,
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_adjacent_difference.cuh:        if (CubDebug(error = cudaPeekAtLastError()))
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_adjacent_difference.cuh:      THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_adjacent_difference.cuh:      if (CubDebug(error = cudaPeekAtLastError()))
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_adjacent_difference.cuh:  static cudaError_t Dispatch(void *d_temp_storage,
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_adjacent_difference.cuh:                              cudaStream_t stream,
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_adjacent_difference.cuh:    cudaError error = cudaSuccess;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_segmented_sort.cuh: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_segmented_sort.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_segmented_sort.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_segmented_sort.cuh:#include <thrust/system/cuda/detail/core/triple_chevron_launch.h>
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_segmented_sort.cuh:  /// CUDA stream to launch kernels within.
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_segmented_sort.cuh:  cudaStream_t stream;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_segmented_sort.cuh:                        cudaStream_t stream,
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_segmented_sort.cuh:  CUB_RUNTIME_FUNCTION __forceinline__ cudaError_t Invoke()
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_segmented_sort.cuh:    cudaError error = cudaSuccess;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_segmented_sort.cuh:  CUB_RUNTIME_FUNCTION __forceinline__ static cudaError_t
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_segmented_sort.cuh:           cudaStream_t stream,
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_segmented_sort.cuh:    cudaError error = cudaSuccess;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_segmented_sort.cuh:  CUB_RUNTIME_FUNCTION __forceinline__ cudaError_t
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_segmented_sort.cuh:    cudaError_t error = cudaSuccess;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_segmented_sort.cuh:      if (CubDebug(error = cudaMemcpyAsync(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_segmented_sort.cuh:                     cudaMemcpyDeviceToHost,
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_segmented_sort.cuh:      THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_segmented_sort.cuh:      if (CubDebug(error = cudaPeekAtLastError()))
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_segmented_sort.cuh:      THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_segmented_sort.cuh:      if (CubDebug(error = cudaPeekAtLastError()))
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_segmented_sort.cuh:  CUB_RUNTIME_FUNCTION __forceinline__ cudaError_t SortWithoutPartitioning(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_segmented_sort.cuh:    cudaError_t error = cudaSuccess;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_segmented_sort.cuh:    THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(blocks_in_grid,
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_segmented_sort.cuh:    if (CubDebug(error = cudaPeekAtLastError()))
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_scan_by_key.cuh: * Copyright (c) 2021, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_scan_by_key.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_scan_by_key.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_scan_by_key.cuh:#include <thrust/system/cuda/detail/core/triple_chevron_launch.h>
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_scan_by_key.cuh:    cudaStream_t          stream;                 ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_scan_by_key.cuh:        cudaStream_t          stream,                 ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_scan_by_key.cuh:    cudaError_t Invoke(InitKernel init_kernel, ScanKernel scan_kernel)
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_scan_by_key.cuh:        return CubDebug(cudaErrorNotSupported);
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_scan_by_key.cuh:        cudaError error = cudaSuccess;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_scan_by_key.cuh:            if (CubDebug(error = cudaGetDevice(&device_ordinal))) break;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_scan_by_key.cuh:            THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_scan_by_key.cuh:            if (CubDebug(error = cudaPeekAtLastError())) break;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_scan_by_key.cuh:            if (CubDebug(error = cudaDeviceGetAttribute(&max_dim_x, cudaDevAttrMaxGridDimX, device_ordinal))) break;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_scan_by_key.cuh:                THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_scan_by_key.cuh:                if (CubDebug(error = cudaPeekAtLastError())) break;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_scan_by_key.cuh:    cudaError_t Invoke()
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_scan_by_key.cuh:    static cudaError_t Dispatch(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_scan_by_key.cuh:        cudaStream_t          stream,                 ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_scan_by_key.cuh:        cudaError_t error;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_reduce_by_key.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_reduce_by_key.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_reduce_by_key.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_reduce_by_key.cuh:#include <thrust/system/cuda/detail/core/triple_chevron_launch.h>
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_reduce_by_key.cuh:    static cudaError_t Dispatch(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_reduce_by_key.cuh:        cudaStream_t                stream,                     ///< [in] CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_reduce_by_key.cuh:        return CubDebug(cudaErrorNotSupported);
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_reduce_by_key.cuh:        cudaError error = cudaSuccess;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_reduce_by_key.cuh:            if (CubDebug(error = cudaGetDevice(&device_ordinal))) break;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_reduce_by_key.cuh:            THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_reduce_by_key.cuh:            if (CubDebug(error = cudaPeekAtLastError())) break;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_reduce_by_key.cuh:            if (CubDebug(error = cudaDeviceGetAttribute(&max_dim_x, cudaDevAttrMaxGridDimX, device_ordinal))) break;;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_reduce_by_key.cuh:                THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_reduce_by_key.cuh:                if (CubDebug(error = cudaPeekAtLastError())) break;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_reduce_by_key.cuh:    static cudaError_t Dispatch(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_reduce_by_key.cuh:        cudaStream_t                stream,                         ///< [in] CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_reduce_by_key.cuh:        cudaError error = cudaSuccess;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_select_if.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_select_if.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_select_if.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_select_if.cuh:#include <thrust/system/cuda/detail/core/triple_chevron_launch.h>
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_select_if.cuh:    static cudaError_t Dispatch(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_select_if.cuh:        cudaStream_t                stream,                         ///< [in] CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_select_if.cuh:        return CubDebug(cudaErrorNotSupported);
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_select_if.cuh:        cudaError error = cudaSuccess;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_select_if.cuh:            if (CubDebug(error = cudaGetDevice(&device_ordinal))) break;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_select_if.cuh:            THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_select_if.cuh:            if (CubDebug(error = cudaPeekAtLastError())) break;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_select_if.cuh:            if (CubDebug(error = cudaDeviceGetAttribute(&max_dim_x, cudaDevAttrMaxGridDimX, device_ordinal))) break;;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_select_if.cuh:            THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_select_if.cuh:            if (CubDebug(error = cudaPeekAtLastError())) break;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_select_if.cuh:    static cudaError_t Dispatch(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_select_if.cuh:        cudaStream_t                stream,                         ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_select_if.cuh:        cudaError error = cudaSuccess;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_reduce.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_reduce.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_reduce.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_reduce.cuh:#include <thrust/system/cuda/detail/core/triple_chevron_launch.h>
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_reduce.cuh:    cudaStream_t        stream;                         ///< [in] CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_reduce.cuh:        cudaStream_t            stream,
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_reduce.cuh:    cudaError_t InvokeSingleTile(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_reduce.cuh:        return CubDebug(cudaErrorNotSupported );
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_reduce.cuh:        cudaError error = cudaSuccess;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_reduce.cuh:            THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_reduce.cuh:            if (CubDebug(error = cudaPeekAtLastError())) break;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_reduce.cuh:    cudaError_t InvokePasses(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_reduce.cuh:        return CubDebug(cudaErrorNotSupported );
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_reduce.cuh:        cudaError error = cudaSuccess;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_reduce.cuh:            if (CubDebug(error = cudaGetDevice(&device_ordinal))) break;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_reduce.cuh:            if (CubDebug(error = cudaDeviceGetAttribute (&sm_count, cudaDevAttrMultiProcessorCount, device_ordinal))) break;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_reduce.cuh:                return cudaSuccess;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_reduce.cuh:            THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_reduce.cuh:            if (CubDebug(error = cudaPeekAtLastError())) break;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_reduce.cuh:            THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_reduce.cuh:            if (CubDebug(error = cudaPeekAtLastError())) break;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_reduce.cuh:    cudaError_t Invoke()
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_reduce.cuh:    static cudaError_t Dispatch(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_reduce.cuh:        cudaStream_t    stream,                             ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_reduce.cuh:        cudaError error = cudaSuccess;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_reduce.cuh:    cudaStream_t         stream;                 ///< [in] CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_reduce.cuh:        cudaStream_t            stream,
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_reduce.cuh:    cudaError_t InvokePasses(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_reduce.cuh:        return CubDebug(cudaErrorNotSupported );
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_reduce.cuh:        cudaError error = cudaSuccess;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_reduce.cuh:                return cudaSuccess;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_reduce.cuh:            THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_reduce.cuh:            if (CubDebug(error = cudaPeekAtLastError())) break;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_reduce.cuh:    cudaError_t Invoke()
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_reduce.cuh:    static cudaError_t Dispatch(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_reduce.cuh:        cudaStream_t         stream,                             ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_reduce.cuh:            return cudaSuccess;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_reduce.cuh:        cudaError error = cudaSuccess;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_spmv_orig.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_spmv_orig.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_spmv_orig.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_spmv_orig.cuh:#include <thrust/system/cuda/detail/core/triple_chevron_launch.h>
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_spmv_orig.cuh:    static cudaError_t Dispatch(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_spmv_orig.cuh:        cudaStream_t            stream,                             ///< [in] CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_spmv_orig.cuh:        return CubDebug(cudaErrorNotSupported );
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_spmv_orig.cuh:        cudaError error = cudaSuccess;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_spmv_orig.cuh:              return cudaErrorInvalidValue;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_spmv_orig.cuh:                THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_spmv_orig.cuh:                if (CubDebug(error = cudaPeekAtLastError())) break;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_spmv_orig.cuh:            if (CubDebug(error = cudaGetDevice(&device_ordinal))) break;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_spmv_orig.cuh:            if (CubDebug(error = cudaDeviceGetAttribute (&sm_count, cudaDevAttrMultiProcessorCount, device_ordinal))) break;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_spmv_orig.cuh:            if (CubDebug(error = cudaDeviceGetAttribute(&max_dim_x, cudaDevAttrMaxGridDimX, device_ordinal))) break;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_spmv_orig.cuh:                THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_spmv_orig.cuh:                if (CubDebug(error = cudaPeekAtLastError())) break;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_spmv_orig.cuh:            THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_spmv_orig.cuh:            if (CubDebug(error = cudaPeekAtLastError())) break;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_spmv_orig.cuh:                THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_spmv_orig.cuh:                if (CubDebug(error = cudaPeekAtLastError())) break;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_spmv_orig.cuh:    static cudaError_t Dispatch(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_spmv_orig.cuh:        cudaStream_t            stream                  = 0,        ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_spmv_orig.cuh:        cudaError error = cudaSuccess;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_three_way_partition.cuh: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_three_way_partition.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_three_way_partition.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_three_way_partition.cuh:#include <thrust/system/cuda/detail/core/triple_chevron_launch.h>
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_three_way_partition.cuh:  CUB_RUNTIME_FUNCTION __forceinline__ static cudaError_t
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_three_way_partition.cuh:           cudaStream_t stream,
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_three_way_partition.cuh:    cudaError error = cudaSuccess;
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_three_way_partition.cuh:      if (CubDebug(error = cudaGetDevice(&device_ordinal)))
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_three_way_partition.cuh:      THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_three_way_partition.cuh:      if (CubDebug(error = cudaPeekAtLastError()))
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_three_way_partition.cuh:      if (CubDebug(error = cudaDeviceGetAttribute(&max_dim_x,
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_three_way_partition.cuh:                                                  cudaDevAttrMaxGridDimX,
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_three_way_partition.cuh:      THRUST_NS_QUALIFIER::cuda_cub::launcher::triple_chevron(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_three_way_partition.cuh:      if (CubDebug(error = cudaPeekAtLastError()))
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_three_way_partition.cuh:  static cudaError_t Dispatch(
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_three_way_partition.cuh:    cudaStream_t                stream,
deps/thrust/dependencies/cub/cub/device/dispatch/dispatch_three_way_partition.cuh:    cudaError error = cudaSuccess;
deps/thrust/dependencies/cub/cub/device/device_spmv.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/device/device_spmv.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/device/device_spmv.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/device/device_spmv.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_spmv.cuh:    static cudaError_t CsrMV(
deps/thrust/dependencies/cub/cub/device/device_spmv.cuh:        cudaStream_t        stream                  = 0,        ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/device_segmented_radix_sort.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/device/device_segmented_radix_sort.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/device/device_segmented_radix_sort.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/device/device_segmented_radix_sort.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_segmented_radix_sort.cuh:    static cudaError_t SortPairs(
deps/thrust/dependencies/cub/cub/device/device_segmented_radix_sort.cuh:        cudaStream_t        stream              = 0,                ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/device_segmented_radix_sort.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_segmented_radix_sort.cuh:    static cudaError_t SortPairs(
deps/thrust/dependencies/cub/cub/device/device_segmented_radix_sort.cuh:        cudaStream_t            stream              = 0,                ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/device_segmented_radix_sort.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_segmented_radix_sort.cuh:    static cudaError_t SortPairsDescending(
deps/thrust/dependencies/cub/cub/device/device_segmented_radix_sort.cuh:        cudaStream_t        stream              = 0,                ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/device_segmented_radix_sort.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_segmented_radix_sort.cuh:    static cudaError_t SortPairsDescending(
deps/thrust/dependencies/cub/cub/device/device_segmented_radix_sort.cuh:        cudaStream_t            stream              = 0,                ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/device_segmented_radix_sort.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_segmented_radix_sort.cuh:    static cudaError_t SortKeys(
deps/thrust/dependencies/cub/cub/device/device_segmented_radix_sort.cuh:        cudaStream_t        stream              = 0,                ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/device_segmented_radix_sort.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_segmented_radix_sort.cuh:    static cudaError_t SortKeys(
deps/thrust/dependencies/cub/cub/device/device_segmented_radix_sort.cuh:        cudaStream_t        stream              = 0,                ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/device_segmented_radix_sort.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_segmented_radix_sort.cuh:    static cudaError_t SortKeysDescending(
deps/thrust/dependencies/cub/cub/device/device_segmented_radix_sort.cuh:        cudaStream_t        stream              = 0,                ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/device_segmented_radix_sort.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_segmented_radix_sort.cuh:    static cudaError_t SortKeysDescending(
deps/thrust/dependencies/cub/cub/device/device_segmented_radix_sort.cuh:        cudaStream_t        stream              = 0,                ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/device_run_length_encode.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/device/device_run_length_encode.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/device/device_run_length_encode.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/device/device_run_length_encode.cuh: * different CUDA architectures for \p int32 items.
deps/thrust/dependencies/cub/cub/device/device_run_length_encode.cuh:     * CUDA architectures for \p int32 and \p int64 items, respectively.  Segments have
deps/thrust/dependencies/cub/cub/device/device_run_length_encode.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_run_length_encode.cuh:    static cudaError_t Encode(
deps/thrust/dependencies/cub/cub/device/device_run_length_encode.cuh:        cudaStream_t                stream             = 0,         ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/device_run_length_encode.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_run_length_encode.cuh:    static cudaError_t NonTrivialRuns(
deps/thrust/dependencies/cub/cub/device/device_run_length_encode.cuh:        cudaStream_t            stream             = 0,         ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh: * (`unsigned char`, `int`, `double`, etc.) as well as CUDA's `__half` and
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh: * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:   * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:   *   <b>[optional]</b> CUDA stream to launch kernels within. Default is
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:  CUB_RUNTIME_FUNCTION static cudaError_t
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:           cudaStream_t stream    = 0,
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:   * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:   *   <b>[optional]</b> CUDA stream to launch kernels within. Default is
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:  CUB_RUNTIME_FUNCTION static cudaError_t
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:                     cudaStream_t stream    = 0,
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:   * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:   *   <b>[optional]</b> CUDA stream to launch kernels within. Default is
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:  CUB_RUNTIME_FUNCTION static cudaError_t
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:           cudaStream_t stream    = 0,
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:   * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:   *   <b>[optional]</b> CUDA stream to launch kernels within. Default is
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:  CUB_RUNTIME_FUNCTION static cudaError_t
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:                     cudaStream_t stream    = 0,
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:   * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:   *   <b>[optional]</b> CUDA stream to launch kernels within. Default is
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:  CUB_RUNTIME_FUNCTION static cudaError_t
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:                 cudaStream_t stream    = 0,
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:   * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:   *   <b>[optional]</b> CUDA stream to launch kernels within. Default is
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:  CUB_RUNTIME_FUNCTION static cudaError_t
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:                           cudaStream_t stream    = 0,
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:   * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:   *   <b>[optional]</b> CUDA stream to launch kernels within. Default is
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:  CUB_RUNTIME_FUNCTION static cudaError_t
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:                 cudaStream_t stream    = 0,
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:   * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:   *   <b>[optional]</b> CUDA stream to launch kernels within. Default is
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:  CUB_RUNTIME_FUNCTION static cudaError_t
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:                           cudaStream_t stream    = 0,
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:   * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:   *   <b>[optional]</b> CUDA stream to launch kernels within. Default is
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:  CUB_RUNTIME_FUNCTION static cudaError_t
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:           cudaStream_t stream = 0,
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:   * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:   *   <b>[optional]</b> CUDA stream to launch kernels within. Default is
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:  CUB_RUNTIME_FUNCTION static cudaError_t
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:                      cudaStream_t stream    = 0,
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:   * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:   *   <b>[optional]</b> CUDA stream to launch kernels within. Default is
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:  CUB_RUNTIME_FUNCTION static cudaError_t
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:            cudaStream_t stream    = 0,
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:   * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:   *   <b>[optional]</b> CUDA stream to launch kernels within. Default is
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:  CUB_RUNTIME_FUNCTION static cudaError_t
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:                      cudaStream_t stream    = 0,
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:   * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:   *   <b>[optional]</b> CUDA stream to launch kernels within. Default is
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:  CUB_RUNTIME_FUNCTION static cudaError_t
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:                  cudaStream_t stream    = 0,
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:   * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:   *   <b>[optional]</b> CUDA stream to launch kernels within. Default is
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:  CUB_RUNTIME_FUNCTION static cudaError_t
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:                            cudaStream_t stream    = 0,
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:   * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:   *   <b>[optional]</b> CUDA stream to launch kernels within. Default is
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:  CUB_RUNTIME_FUNCTION static cudaError_t
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:                  cudaStream_t stream    = 0,
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:   * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:   *   <b>[optional]</b> CUDA stream to launch kernels within. Default is
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:  CUB_RUNTIME_FUNCTION static cudaError_t
deps/thrust/dependencies/cub/cub/device/device_segmented_sort.cuh:                            cudaStream_t stream    = 0,
deps/thrust/dependencies/cub/cub/device/device_histogram.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/device/device_histogram.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/device/device_histogram.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/device/device_histogram.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_histogram.cuh:    static cudaError_t HistogramEven(
deps/thrust/dependencies/cub/cub/device/device_histogram.cuh:        cudaStream_t        stream                  = 0,                ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/device_histogram.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_histogram.cuh:    static cudaError_t HistogramEven(
deps/thrust/dependencies/cub/cub/device/device_histogram.cuh:        cudaStream_t        stream                  = 0,                ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/device_histogram.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_histogram.cuh:    static cudaError_t MultiHistogramEven(
deps/thrust/dependencies/cub/cub/device/device_histogram.cuh:        cudaStream_t        stream                  = 0,                ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/device_histogram.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_histogram.cuh:    static cudaError_t MultiHistogramEven(
deps/thrust/dependencies/cub/cub/device/device_histogram.cuh:        cudaStream_t        stream                  = 0,                ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/device_histogram.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_histogram.cuh:    static cudaError_t HistogramRange(
deps/thrust/dependencies/cub/cub/device/device_histogram.cuh:        cudaStream_t        stream              = 0,                ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/device_histogram.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_histogram.cuh:    static cudaError_t HistogramRange(
deps/thrust/dependencies/cub/cub/device/device_histogram.cuh:        cudaStream_t        stream              = 0,                ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/device_histogram.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_histogram.cuh:    static cudaError_t MultiHistogramRange(
deps/thrust/dependencies/cub/cub/device/device_histogram.cuh:        cudaStream_t        stream              = 0,                ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/device_histogram.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_histogram.cuh:    static cudaError_t MultiHistogramRange(
deps/thrust/dependencies/cub/cub/device/device_histogram.cuh:        cudaStream_t        stream              = 0,                ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/device_radix_sort.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/device/device_radix_sort.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/device/device_radix_sort.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/device/device_radix_sort.cuh: * (`unsigned char`, `int`, `double`, etc.) as well as CUDA's `__half`
deps/thrust/dependencies/cub/cub/device/device_radix_sort.cuh: * performance across different CUDA architectures for uniform-random \p uint32 keys.
deps/thrust/dependencies/cub/cub/device/device_radix_sort.cuh:     * CUDA architectures for uniform-random <tt>uint32,uint32</tt> and
deps/thrust/dependencies/cub/cub/device/device_radix_sort.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_radix_sort.cuh:    static cudaError_t SortPairs(
deps/thrust/dependencies/cub/cub/device/device_radix_sort.cuh:        cudaStream_t        stream              = 0,                ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/device_radix_sort.cuh:     * CUDA architectures for uniform-random <tt>uint32,uint32</tt> and
deps/thrust/dependencies/cub/cub/device/device_radix_sort.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_radix_sort.cuh:    static cudaError_t SortPairs(
deps/thrust/dependencies/cub/cub/device/device_radix_sort.cuh:        cudaStream_t            stream              = 0,                ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/device_radix_sort.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_radix_sort.cuh:    static cudaError_t SortPairsDescending(
deps/thrust/dependencies/cub/cub/device/device_radix_sort.cuh:        cudaStream_t        stream              = 0,                ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/device_radix_sort.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_radix_sort.cuh:    static cudaError_t SortPairsDescending(
deps/thrust/dependencies/cub/cub/device/device_radix_sort.cuh:        cudaStream_t            stream              = 0,                ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/device_radix_sort.cuh:     * CUDA architectures for uniform-random \p uint32 and \p uint64 keys, respectively.
deps/thrust/dependencies/cub/cub/device/device_radix_sort.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_radix_sort.cuh:    static cudaError_t SortKeys(
deps/thrust/dependencies/cub/cub/device/device_radix_sort.cuh:        cudaStream_t        stream              = 0,                ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/device_radix_sort.cuh:     * CUDA architectures for uniform-random \p uint32 and \p uint64 keys, respectively.
deps/thrust/dependencies/cub/cub/device/device_radix_sort.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_radix_sort.cuh:    static cudaError_t SortKeys(
deps/thrust/dependencies/cub/cub/device/device_radix_sort.cuh:        cudaStream_t        stream              = 0,                ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/device_radix_sort.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_radix_sort.cuh:    static cudaError_t SortKeysDescending(
deps/thrust/dependencies/cub/cub/device/device_radix_sort.cuh:        cudaStream_t        stream              = 0,                ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/device_radix_sort.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_radix_sort.cuh:    static cudaError_t SortKeysDescending(
deps/thrust/dependencies/cub/cub/device/device_radix_sort.cuh:        cudaStream_t        stream              = 0,                ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/device_partition.cuh: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/device/device_partition.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/device/device_partition.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/device/device_partition.cuh: * performance across different CUDA architectures for @p int32 items,
deps/thrust/dependencies/cub/cub/device/device_partition.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_partition.cuh:     *   **[optional]** CUDA stream to launch kernels within.
deps/thrust/dependencies/cub/cub/device/device_partition.cuh:    CUB_RUNTIME_FUNCTION __forceinline__ static cudaError_t
deps/thrust/dependencies/cub/cub/device/device_partition.cuh:            cudaStream_t stream    = 0,
deps/thrust/dependencies/cub/cub/device/device_partition.cuh:     * different CUDA architectures for @p int32 and @p int64 items,
deps/thrust/dependencies/cub/cub/device/device_partition.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_partition.cuh:     *   **[optional]** CUDA stream to launch kernels within.
deps/thrust/dependencies/cub/cub/device/device_partition.cuh:    CUB_RUNTIME_FUNCTION __forceinline__ static cudaError_t
deps/thrust/dependencies/cub/cub/device/device_partition.cuh:       cudaStream_t stream    = 0,
deps/thrust/dependencies/cub/cub/device/device_partition.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_partition.cuh:    CUB_RUNTIME_FUNCTION __forceinline__ static cudaError_t
deps/thrust/dependencies/cub/cub/device/device_partition.cuh:       cudaStream_t stream    = 0,
deps/thrust/dependencies/cub/cub/device/device_segmented_reduce.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/device/device_segmented_reduce.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/device/device_segmented_reduce.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/device/device_segmented_reduce.cuh:     *   (e.g., addition of floating point types) on the same GPU device.
deps/thrust/dependencies/cub/cub/device/device_segmented_reduce.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_segmented_reduce.cuh:    static cudaError_t Reduce(
deps/thrust/dependencies/cub/cub/device/device_segmented_reduce.cuh:        cudaStream_t         stream              = 0,            ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/device_segmented_reduce.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_segmented_reduce.cuh:    static cudaError_t Sum(
deps/thrust/dependencies/cub/cub/device/device_segmented_reduce.cuh:        cudaStream_t          stream              = 0,            ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/device_segmented_reduce.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_segmented_reduce.cuh:    static cudaError_t Min(
deps/thrust/dependencies/cub/cub/device/device_segmented_reduce.cuh:        cudaStream_t          stream              = 0,            ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/device_segmented_reduce.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_segmented_reduce.cuh:    static cudaError_t ArgMin(
deps/thrust/dependencies/cub/cub/device/device_segmented_reduce.cuh:        cudaStream_t         stream              = 0,            ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/device_segmented_reduce.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_segmented_reduce.cuh:    static cudaError_t Max(
deps/thrust/dependencies/cub/cub/device/device_segmented_reduce.cuh:        cudaStream_t         stream              = 0,            ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/device_segmented_reduce.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_segmented_reduce.cuh:    static cudaError_t ArgMax(
deps/thrust/dependencies/cub/cub/device/device_segmented_reduce.cuh:        cudaStream_t         stream              = 0,            ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/device_reduce.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/device/device_reduce.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/device/device_reduce.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/device/device_reduce.cuh: * performance across different CUDA architectures for \p int32 keys.
deps/thrust/dependencies/cub/cub/device/device_reduce.cuh: * performance across different CUDA architectures for \p fp32
deps/thrust/dependencies/cub/cub/device/device_reduce.cuh:     *   (e.g., addition of floating point types) on the same GPU device.
deps/thrust/dependencies/cub/cub/device/device_reduce.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_reduce.cuh:    static cudaError_t Reduce(
deps/thrust/dependencies/cub/cub/device/device_reduce.cuh:        cudaStream_t                stream              = 0,            ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/device_reduce.cuh:     *   (e.g., addition of floating point types) on the same GPU device.
deps/thrust/dependencies/cub/cub/device/device_reduce.cuh:     * CUDA architectures for \p int32 and \p int64 items, respectively.
deps/thrust/dependencies/cub/cub/device/device_reduce.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_reduce.cuh:    static cudaError_t Sum(
deps/thrust/dependencies/cub/cub/device/device_reduce.cuh:        cudaStream_t                stream              = 0,            ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/device_reduce.cuh:     *   (e.g., addition of floating point types) on the same GPU device.
deps/thrust/dependencies/cub/cub/device/device_reduce.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_reduce.cuh:    static cudaError_t Min(
deps/thrust/dependencies/cub/cub/device/device_reduce.cuh:        cudaStream_t                stream              = 0,            ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/device_reduce.cuh:     *   (e.g., addition of floating point types) on the same GPU device.
deps/thrust/dependencies/cub/cub/device/device_reduce.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_reduce.cuh:    static cudaError_t ArgMin(
deps/thrust/dependencies/cub/cub/device/device_reduce.cuh:        cudaStream_t                stream              = 0,            ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/device_reduce.cuh:     *   (e.g., addition of floating point types) on the same GPU device.
deps/thrust/dependencies/cub/cub/device/device_reduce.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_reduce.cuh:    static cudaError_t Max(
deps/thrust/dependencies/cub/cub/device/device_reduce.cuh:        cudaStream_t                stream              = 0,            ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/device_reduce.cuh:     *   (e.g., addition of floating point types) on the same GPU device.
deps/thrust/dependencies/cub/cub/device/device_reduce.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_reduce.cuh:    static cudaError_t ArgMax(
deps/thrust/dependencies/cub/cub/device/device_reduce.cuh:        cudaStream_t                stream              = 0,            ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/device_reduce.cuh:     *   (e.g., addition of floating point types) on the same GPU device.
deps/thrust/dependencies/cub/cub/device/device_reduce.cuh:     * different CUDA architectures for \p fp32 and \p fp64 values, respectively.  Segments
deps/thrust/dependencies/cub/cub/device/device_reduce.cuh:     * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_reduce.cuh:    static cudaError_t ReduceByKey(
deps/thrust/dependencies/cub/cub/device/device_reduce.cuh:        cudaStream_t                stream             = 0,         ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
deps/thrust/dependencies/cub/cub/device/device_adjacent_difference.cuh: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/device/device_adjacent_difference.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/device/device_adjacent_difference.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/device/device_adjacent_difference.cuh: * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_adjacent_difference.cuh:  static CUB_RUNTIME_FUNCTION cudaError_t
deps/thrust/dependencies/cub/cub/device/device_adjacent_difference.cuh:                     cudaStream_t stream,
deps/thrust/dependencies/cub/cub/device/device_adjacent_difference.cuh:   * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_adjacent_difference.cuh:   *   <b>[optional]</b> CUDA stream to launch kernels within. Default is
deps/thrust/dependencies/cub/cub/device/device_adjacent_difference.cuh:  static CUB_RUNTIME_FUNCTION cudaError_t
deps/thrust/dependencies/cub/cub/device/device_adjacent_difference.cuh:                   cudaStream_t stream         = 0,
deps/thrust/dependencies/cub/cub/device/device_adjacent_difference.cuh:   * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_adjacent_difference.cuh:   *   <b>[optional]</b> CUDA stream to launch kernels within. Default is
deps/thrust/dependencies/cub/cub/device/device_adjacent_difference.cuh:  static CUB_RUNTIME_FUNCTION cudaError_t
deps/thrust/dependencies/cub/cub/device/device_adjacent_difference.cuh:               cudaStream_t stream         = 0,
deps/thrust/dependencies/cub/cub/device/device_adjacent_difference.cuh:   * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_adjacent_difference.cuh:   *   <b>[optional]</b> CUDA stream to launch kernels within. Default is
deps/thrust/dependencies/cub/cub/device/device_adjacent_difference.cuh:  static CUB_RUNTIME_FUNCTION cudaError_t
deps/thrust/dependencies/cub/cub/device/device_adjacent_difference.cuh:                    cudaStream_t stream         = 0,
deps/thrust/dependencies/cub/cub/device/device_adjacent_difference.cuh:   * cudaMalloc(&d_temp_storage, temp_storage_bytes);
deps/thrust/dependencies/cub/cub/device/device_adjacent_difference.cuh:   *   <b>[optional]</b> CUDA stream to launch kernels within. Default is
deps/thrust/dependencies/cub/cub/device/device_adjacent_difference.cuh:  static CUB_RUNTIME_FUNCTION cudaError_t
deps/thrust/dependencies/cub/cub/device/device_adjacent_difference.cuh:                cudaStream_t stream         = 0,
deps/thrust/dependencies/cub/cub/util_device.cuh: * Copyright (c) 2011-2020, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/util_device.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/util_device.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/util_device.cuh: * Properties of a given CUDA device and the corresponding PTX bundle
deps/thrust/dependencies/cub/cub/util_device.cuh:cudaError_t AliasTemporaries(
deps/thrust/dependencies/cub/cub/util_device.cuh:        return cudaSuccess;
deps/thrust/dependencies/cub/cub/util_device.cuh:        return CubDebug(cudaErrorInvalidValue);
deps/thrust/dependencies/cub/cub/util_device.cuh:    return cudaSuccess;
deps/thrust/dependencies/cub/cub/util_device.cuh:#if defined(CUB_RUNTIME_ENABLED) // Host code or device code with the CUDA runtime.
deps/thrust/dependencies/cub/cub/util_device.cuh:    if (CubDebug(cudaGetDevice(&device))) return -1;
deps/thrust/dependencies/cub/cub/util_device.cuh:#else // Device code without the CUDA runtime.
deps/thrust/dependencies/cub/cub/util_device.cuh:            CubDebug(cudaSetDevice(new_device));
deps/thrust/dependencies/cub/cub/util_device.cuh:            CubDebug(cudaSetDevice(old_device));
deps/thrust/dependencies/cub/cub/util_device.cuh: * \brief Returns the number of CUDA devices available or -1 if an error
deps/thrust/dependencies/cub/cub/util_device.cuh:#if defined(CUB_RUNTIME_ENABLED) // Host code or device code with the CUDA runtime.
deps/thrust/dependencies/cub/cub/util_device.cuh:    if (CubDebug(cudaGetDeviceCount(&count)))
deps/thrust/dependencies/cub/cub/util_device.cuh:        // CUDA makes no guarantees about the state of the output parameter if
deps/thrust/dependencies/cub/cub/util_device.cuh:        // `cudaGetDeviceCount` fails; in practice, they don't, but out of
deps/thrust/dependencies/cub/cub/util_device.cuh:#else // Device code without the CUDA runtime.
deps/thrust/dependencies/cub/cub/util_device.cuh: * \brief Returns the number of CUDA devices available.
deps/thrust/dependencies/cub/cub/util_device.cuh: * \brief Per-device cache for a CUDA attribute value; the attribute is queried
deps/thrust/dependencies/cub/cub/util_device.cuh:        cudaError_t error;
deps/thrust/dependencies/cub/cub/util_device.cuh:            return DevicePayload{0, cudaErrorInvalidDevice};
deps/thrust/dependencies/cub/cub/util_device.cuh:                    // Clear the global CUDA error state which may have been
deps/thrust/dependencies/cub/cub/util_device.cuh:                    cudaGetLastError();
deps/thrust/dependencies/cub/cub/util_device.cuh:CUB_RUNTIME_FUNCTION inline cudaError_t PtxVersionUncached(int& ptx_version)
deps/thrust/dependencies/cub/cub/util_device.cuh:    cudaError_t result = cudaSuccess;
deps/thrust/dependencies/cub/cub/util_device.cuh:            cudaFuncAttributes empty_kernel_attrs;
deps/thrust/dependencies/cub/cub/util_device.cuh:            result = cudaFuncGetAttributes(&empty_kernel_attrs,
deps/thrust/dependencies/cub/cub/util_device.cuh:__host__ inline cudaError_t PtxVersionUncached(int& ptx_version, int device)
deps/thrust/dependencies/cub/cub/util_device.cuh:__host__ inline cudaError_t PtxVersion(int& ptx_version, int device)
deps/thrust/dependencies/cub/cub/util_device.cuh:CUB_RUNTIME_FUNCTION inline cudaError_t PtxVersion(int& ptx_version)
deps/thrust/dependencies/cub/cub/util_device.cuh:    cudaError_t result = cudaErrorUnknown;
deps/thrust/dependencies/cub/cub/util_device.cuh:CUB_RUNTIME_FUNCTION inline cudaError_t SmVersionUncached(int& sm_version, int device = CurrentDevice())
deps/thrust/dependencies/cub/cub/util_device.cuh:#if defined(CUB_RUNTIME_ENABLED) // Host code or device code with the CUDA runtime.
deps/thrust/dependencies/cub/cub/util_device.cuh:    cudaError_t error = cudaSuccess;
deps/thrust/dependencies/cub/cub/util_device.cuh:        if (CubDebug(error = cudaDeviceGetAttribute(&major, cudaDevAttrComputeCapabilityMajor, device))) break;
deps/thrust/dependencies/cub/cub/util_device.cuh:        if (CubDebug(error = cudaDeviceGetAttribute(&minor, cudaDevAttrComputeCapabilityMinor, device))) break;
deps/thrust/dependencies/cub/cub/util_device.cuh:#else // Device code without the CUDA runtime.
deps/thrust/dependencies/cub/cub/util_device.cuh:    // CUDA API calls are not supported from this device.
deps/thrust/dependencies/cub/cub/util_device.cuh:    return CubDebug(cudaErrorInvalidConfiguration);
deps/thrust/dependencies/cub/cub/util_device.cuh:CUB_RUNTIME_FUNCTION inline cudaError_t SmVersion(int& sm_version, int device = CurrentDevice())
deps/thrust/dependencies/cub/cub/util_device.cuh:    cudaError_t result = cudaErrorUnknown;
deps/thrust/dependencies/cub/cub/util_device.cuh:CUB_RUNTIME_FUNCTION inline cudaError_t SyncStream(cudaStream_t stream)
deps/thrust/dependencies/cub/cub/util_device.cuh:    cudaError_t result = cudaErrorUnknown;
deps/thrust/dependencies/cub/cub/util_device.cuh:            result = CubDebug(cudaStreamSynchronize(stream));
deps/thrust/dependencies/cub/cub/util_device.cuh:            #if defined(CUB_RUNTIME_ENABLED) // Device code with the CUDA runtime.
deps/thrust/dependencies/cub/cub/util_device.cuh:            #else // Device code without the CUDA runtime.
deps/thrust/dependencies/cub/cub/util_device.cuh:                // CUDA API calls are not supported from this device.
deps/thrust/dependencies/cub/cub/util_device.cuh:                result = CubDebug(cudaErrorInvalidConfiguration);
deps/thrust/dependencies/cub/cub/util_device.cuh:cudaError_t MaxSmOccupancy(
deps/thrust/dependencies/cub/cub/util_device.cuh:    // CUDA API calls not supported from this device
deps/thrust/dependencies/cub/cub/util_device.cuh:    return CubDebug(cudaErrorInvalidConfiguration);
deps/thrust/dependencies/cub/cub/util_device.cuh:    return CubDebug(cudaOccupancyMaxActiveBlocksPerMultiprocessor(
deps/thrust/dependencies/cub/cub/util_device.cuh:    cudaError_t Init(KernelPtrT kernel_ptr)
deps/thrust/dependencies/cub/cub/util_device.cuh:        cudaError_t retval   = MaxSmOccupancy(sm_occupancy, kernel_ptr, block_threads);
deps/thrust/dependencies/cub/cub/util_device.cuh:  static cudaError_t Invoke(int ptx_version, FunctorT& op)
deps/thrust/dependencies/cub/cub/util_device.cuh:    static cudaError_t Invoke(int /*ptx_version*/, FunctorT& op) {
deps/thrust/dependencies/cub/cub/util_debug.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/util_debug.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/util_debug.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/util_debug.cuh: * \brief If \p CUB_STDERR is defined and \p error is not \p cudaSuccess, the corresponding error message is printed to \p stderr (or \p stdout in device code) along with the supplied source context.
deps/thrust/dependencies/cub/cub/util_debug.cuh: * \return The CUDA error.
deps/thrust/dependencies/cub/cub/util_debug.cuh:__host__ __device__ __forceinline__ cudaError_t Debug(
deps/thrust/dependencies/cub/cub/util_debug.cuh:    cudaError_t     error,
deps/thrust/dependencies/cub/cub/util_debug.cuh:    // Clear the global CUDA error state which may have been set by the last
deps/thrust/dependencies/cub/cub/util_debug.cuh:    cudaGetLastError();
deps/thrust/dependencies/cub/cub/util_debug.cuh:                fprintf(stderr, "CUDA error %d [%s, %d]: %s\n", error, filename, line, cudaGetErrorString(error));
deps/thrust/dependencies/cub/cub/util_debug.cuh:                printf("CUDA error %d [block (%d,%d,%d) thread (%d,%d,%d), %s, %d]\n", error, blockIdx.z, blockIdx.y, blockIdx.x, threadIdx.z, threadIdx.y, threadIdx.x, filename, line);
deps/thrust/dependencies/cub/cub/util_debug.cuh:    #define CubDebug(e) CUB_NS_QUALIFIER::Debug((cudaError_t) (e), __FILE__, __LINE__)
deps/thrust/dependencies/cub/cub/util_debug.cuh:    #define CubDebugExit(e) if (CUB_NS_QUALIFIER::Debug((cudaError_t) (e), __FILE__, __LINE__)) { exit(1); }
deps/thrust/dependencies/cub/cub/util_debug.cuh:    #if defined(_NVHPC_CUDA)
deps/thrust/dependencies/cub/cub/util_debug.cuh:    #elif !(defined(__clang__) && defined(__CUDA__))
deps/thrust/dependencies/cub/cub/util_debug.cuh:        #ifdef __CUDA_ARCH__
deps/thrust/dependencies/cub/cub/util_debug.cuh:        #ifndef __CUDA_ARCH__
deps/thrust/dependencies/cub/cub/config.cuh: * Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/config.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/config.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/agent/agent_reduce_by_key.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/agent/agent_reduce_by_key.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/agent/agent_reduce_by_key.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/agent/agent_reduce_by_key.cuh: * cub::AgentReduceByKey implements a stateful abstraction of CUDA thread blocks for participating in device-wide reduce-value-by-key.
deps/thrust/dependencies/cub/cub/agent/agent_reduce_by_key.cuh: * \brief AgentReduceByKey implements a stateful abstraction of CUDA thread blocks for participating in device-wide reduce-value-by-key
deps/thrust/dependencies/cub/cub/agent/agent_merge_sort.cuh: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/agent/agent_merge_sort.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/agent/agent_merge_sort.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/agent/agent_merge_sort.cuh:#include <thrust/system/cuda/detail/core/util.h>
deps/thrust/dependencies/cub/cub/agent/agent_merge_sort.cuh:  using KeysLoadIt  = typename THRUST_NS_QUALIFIER::cuda_cub::core::LoadIterator<Policy, KeyInputIteratorT>::type;
deps/thrust/dependencies/cub/cub/agent/agent_merge_sort.cuh:  using ItemsLoadIt = typename THRUST_NS_QUALIFIER::cuda_cub::core::LoadIterator<Policy, ValueInputIteratorT>::type;
deps/thrust/dependencies/cub/cub/agent/agent_merge_sort.cuh:  using KeysLoadPingIt  = typename THRUST_NS_QUALIFIER::cuda_cub::core::LoadIterator<Policy, KeyIteratorT>::type;
deps/thrust/dependencies/cub/cub/agent/agent_merge_sort.cuh:  using ItemsLoadPingIt = typename THRUST_NS_QUALIFIER::cuda_cub::core::LoadIterator<Policy, ValueIteratorT>::type;
deps/thrust/dependencies/cub/cub/agent/agent_merge_sort.cuh:  using KeysLoadPongIt  = typename THRUST_NS_QUALIFIER::cuda_cub::core::LoadIterator<Policy, KeyT *>::type;
deps/thrust/dependencies/cub/cub/agent/agent_merge_sort.cuh:  using ItemsLoadPongIt = typename THRUST_NS_QUALIFIER::cuda_cub::core::LoadIterator<Policy, ValueT *>::type;
deps/thrust/dependencies/cub/cub/agent/agent_radix_sort_upsweep.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/agent/agent_radix_sort_upsweep.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/agent/agent_radix_sort_upsweep.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/agent/agent_radix_sort_upsweep.cuh: * AgentRadixSortUpsweep implements a stateful abstraction of CUDA thread blocks for participating in device-wide radix sort upsweep .
deps/thrust/dependencies/cub/cub/agent/agent_radix_sort_upsweep.cuh: * \brief AgentRadixSortUpsweep implements a stateful abstraction of CUDA thread blocks for participating in device-wide radix sort upsweep .
deps/thrust/dependencies/cub/cub/agent/agent_adjacent_difference.cuh: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/agent/agent_adjacent_difference.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/agent/agent_adjacent_difference.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/agent/agent_adjacent_difference.cuh:#include <thrust/system/cuda/detail/core/util.h>
deps/thrust/dependencies/cub/cub/agent/agent_adjacent_difference.cuh:  using LoadIt = typename THRUST_NS_QUALIFIER::cuda_cub::core::LoadIterator<Policy, InputIteratorT>::type;
deps/thrust/dependencies/cub/cub/agent/agent_adjacent_difference.cuh:          THRUST_NS_QUALIFIER::cuda_cub::core::make_load_iterator(Policy(),
deps/thrust/dependencies/cub/cub/agent/agent_scan_by_key.cuh: * Copyright (c) 2021, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/agent/agent_scan_by_key.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/agent/agent_scan_by_key.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/agent/agent_scan_by_key.cuh: * AgentScanByKey implements a stateful abstraction of CUDA thread blocks for participating in device-wide prefix scan by key.
deps/thrust/dependencies/cub/cub/agent/agent_scan_by_key.cuh: * \brief AgentScanByKey implements a stateful abstraction of CUDA thread blocks for participating in device-wide prefix scan by key.
deps/thrust/dependencies/cub/cub/agent/agent_scan_by_key.cuh:        Int2Type<false> /* is_incclusive */)
deps/thrust/dependencies/cub/cub/agent/agent_histogram.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/agent/agent_histogram.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/agent/agent_histogram.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/agent/agent_histogram.cuh: * cub::AgentHistogram implements a stateful abstraction of CUDA thread blocks for participating in device-wide histogram .
deps/thrust/dependencies/cub/cub/agent/agent_histogram.cuh: * \brief AgentHistogram implements a stateful abstraction of CUDA thread blocks for participating in device-wide histogram .
deps/thrust/dependencies/cub/cub/agent/agent_reduce.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/agent/agent_reduce.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/agent/agent_reduce.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/agent/agent_reduce.cuh: * cub::AgentReduce implements a stateful abstraction of CUDA thread blocks for participating in device-wide reduction .
deps/thrust/dependencies/cub/cub/agent/agent_reduce.cuh: * \brief AgentReduce implements a stateful abstraction of CUDA thread blocks for participating in device-wide reduction .
deps/thrust/dependencies/cub/cub/agent/agent_scan.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/agent/agent_scan.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/agent/agent_scan.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/agent/agent_scan.cuh: * cub::AgentScan implements a stateful abstraction of CUDA thread blocks for participating in device-wide prefix scan .
deps/thrust/dependencies/cub/cub/agent/agent_scan.cuh: * \brief AgentScan implements a stateful abstraction of CUDA thread blocks for participating in device-wide prefix scan .
deps/thrust/dependencies/cub/cub/agent/agent_sub_warp_merge_sort.cuh: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/agent/agent_sub_warp_merge_sort.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/agent/agent_sub_warp_merge_sort.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/agent/agent_sub_warp_merge_sort.cuh:#include <thrust/system/cuda/detail/core/util.h>
deps/thrust/dependencies/cub/cub/agent/agent_sub_warp_merge_sort.cuh:#if defined(__CUDA_FP16_TYPES_EXIST__) && (CUB_PTX_ARCH < 530)
deps/thrust/dependencies/cub/cub/agent/agent_sub_warp_merge_sort.cuh:#if defined(__CUDA_FP16_TYPES_EXIST__) && (CUB_PTX_ARCH < 530)
deps/thrust/dependencies/cub/cub/agent/agent_sub_warp_merge_sort.cuh:  using KeysLoadItT = typename THRUST_NS_QUALIFIER::cuda_cub::core::
deps/thrust/dependencies/cub/cub/agent/agent_sub_warp_merge_sort.cuh:  using ItemsLoadItT = typename THRUST_NS_QUALIFIER::cuda_cub::core::
deps/thrust/dependencies/cub/cub/agent/agent_radix_sort_downsweep.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/agent/agent_radix_sort_downsweep.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/agent/agent_radix_sort_downsweep.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/agent/agent_radix_sort_downsweep.cuh: * AgentRadixSortDownsweep implements a stateful abstraction of CUDA thread
deps/thrust/dependencies/cub/cub/agent/agent_radix_sort_downsweep.cuh: * \brief AgentRadixSortDownsweep implements a stateful abstraction of CUDA thread blocks for participating in device-wide radix sort downsweep .
deps/thrust/dependencies/cub/cub/agent/agent_segment_fixup.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/agent/agent_segment_fixup.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/agent/agent_segment_fixup.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/agent/agent_segment_fixup.cuh: * cub::AgentSegmentFixup implements a stateful abstraction of CUDA thread blocks for participating in device-wide reduce-value-by-key.
deps/thrust/dependencies/cub/cub/agent/agent_segment_fixup.cuh: * \brief AgentSegmentFixup implements a stateful abstraction of CUDA thread blocks for participating in device-wide reduce-value-by-key
deps/thrust/dependencies/cub/cub/agent/agent_segmented_radix_sort.cuh: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/agent/agent_segmented_radix_sort.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/agent/agent_segmented_radix_sort.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/agent/agent_segmented_radix_sort.cuh: * https://github.com/NVIDIA/cub/issues/383 is addressed.
deps/thrust/dependencies/cub/cub/agent/agent_rle.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/agent/agent_rle.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/agent/agent_rle.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/agent/agent_rle.cuh: * cub::AgentRle implements a stateful abstraction of CUDA thread blocks for participating in device-wide run-length-encode.
deps/thrust/dependencies/cub/cub/agent/agent_rle.cuh: * \brief AgentRle implements a stateful abstraction of CUDA thread blocks for participating in device-wide run-length-encode 
deps/thrust/dependencies/cub/cub/agent/agent_select_if.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/agent/agent_select_if.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/agent/agent_select_if.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/agent/agent_select_if.cuh: * cub::AgentSelectIf implements a stateful abstraction of CUDA thread blocks for participating in device-wide select.
deps/thrust/dependencies/cub/cub/agent/agent_select_if.cuh: * \brief AgentSelectIf implements a stateful abstraction of CUDA thread blocks for participating in device-wide selection
deps/thrust/dependencies/cub/cub/agent/agent_three_way_partition.cuh: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/agent/agent_three_way_partition.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/agent/agent_three_way_partition.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/agent/agent_radix_sort_histogram.cuh: * Copyright (c) 2011-2020, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/agent/agent_radix_sort_histogram.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/agent/agent_radix_sort_histogram.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/agent/agent_radix_sort_histogram.cuh: * agent_radix_sort_histogram.cuh implements a stateful abstraction of CUDA
deps/thrust/dependencies/cub/cub/agent/agent_radix_sort_histogram.cuh:                // Using cuda::atomic<> results in lower performance on GP100,
deps/thrust/dependencies/cub/cub/agent/agent_radix_sort_histogram.cuh:                    // Using cuda::atomic<> here would also require using it in
deps/thrust/dependencies/cub/cub/agent/agent_radix_sort_histogram.cuh:                    // cuda::atomic_ref<> becomes available.
deps/thrust/dependencies/cub/cub/agent/single_pass_scan_operators.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/agent/single_pass_scan_operators.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/agent/single_pass_scan_operators.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/agent/single_pass_scan_operators.cuh:    cudaError_t Init(
deps/thrust/dependencies/cub/cub/agent/single_pass_scan_operators.cuh:        return cudaSuccess;
deps/thrust/dependencies/cub/cub/agent/single_pass_scan_operators.cuh:    static cudaError_t AllocationSize(
deps/thrust/dependencies/cub/cub/agent/single_pass_scan_operators.cuh:        return cudaSuccess;
deps/thrust/dependencies/cub/cub/agent/single_pass_scan_operators.cuh:    cudaError_t Init(
deps/thrust/dependencies/cub/cub/agent/single_pass_scan_operators.cuh:        cudaError_t error = cudaSuccess;
deps/thrust/dependencies/cub/cub/agent/single_pass_scan_operators.cuh:    static cudaError_t AllocationSize(
deps/thrust/dependencies/cub/cub/agent/single_pass_scan_operators.cuh:    cudaError_t Init(
deps/thrust/dependencies/cub/cub/agent/single_pass_scan_operators.cuh:        return cudaSuccess;
deps/thrust/dependencies/cub/cub/agent/single_pass_scan_operators.cuh:    static cudaError_t AllocationSize(
deps/thrust/dependencies/cub/cub/agent/single_pass_scan_operators.cuh:        return cudaSuccess;
deps/thrust/dependencies/cub/cub/agent/agent_radix_sort_onesweep.cuh: * Copyright (c) 2011-2020, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/agent/agent_radix_sort_onesweep.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/agent/agent_radix_sort_onesweep.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/agent/agent_radix_sort_onesweep.cuh: * agent_radix_sort_onesweep.cuh implements a stateful abstraction of CUDA
deps/thrust/dependencies/cub/cub/agent/agent_spmv_orig.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/agent/agent_spmv_orig.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/agent/agent_spmv_orig.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/agent/agent_spmv_orig.cuh: * cub::AgentSpmv implements a stateful abstraction of CUDA thread blocks for participating in device-wide SpMV.
deps/thrust/dependencies/cub/cub/agent/agent_spmv_orig.cuh: * \brief AgentSpmv implements a stateful abstraction of CUDA thread blocks for participating in device-wide SpMV.
deps/thrust/dependencies/cub/cub/util_arch.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/util_arch.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/util_arch.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/util_arch.cuh:#if ((__CUDACC_VER_MAJOR__ >= 9) || defined(_NVHPC_CUDA) ||            \
deps/thrust/dependencies/cub/cub/util_arch.cuh:     CUDA_VERSION >= 9000) &&                                                  \
deps/thrust/dependencies/cub/cub/util_arch.cuh:    #if defined(_NVHPC_CUDA)
deps/thrust/dependencies/cub/cub/util_arch.cuh:        // __NVCOMPILER_CUDA_ARCH__ is the target PTX version, and is defined
deps/thrust/dependencies/cub/cub/util_arch.cuh:        #define CUB_PTX_ARCH __NVCOMPILER_CUDA_ARCH__
deps/thrust/dependencies/cub/cub/util_arch.cuh:    #elif !defined(__CUDA_ARCH__)
deps/thrust/dependencies/cub/cub/util_arch.cuh:        #define CUB_PTX_ARCH __CUDA_ARCH__
deps/thrust/dependencies/cub/cub/util_arch.cuh:    #if defined(_NVHPC_CUDA)
deps/thrust/dependencies/cub/cub/util_arch.cuh:/// Whether or not the source targeted by the active compiler pass is allowed to  invoke device kernels or methods from the CUDA runtime API.
deps/thrust/dependencies/cub/cub/util_arch.cuh:    #if !defined(__CUDA_ARCH__) || (__CUDA_ARCH__>= 350 && defined(__CUDACC_RDC__))
deps/thrust/dependencies/cub/cub/util_cpp_dialect.cuh: * Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/util_cpp_dialect.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/util_cpp_dialect.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/thread/thread_operators.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/thread/thread_operators.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/thread/thread_operators.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/thread/thread_search.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/thread/thread_search.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/thread/thread_search.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/thread/thread_sort.cuh: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/thread/thread_sort.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/thread/thread_sort.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/thread/thread_load.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/thread/thread_load.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/thread/thread_load.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/thread/thread_store.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/thread/thread_store.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/thread/thread_store.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/thread/thread_scan.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/thread/thread_scan.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/thread/thread_scan.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/thread/thread_reduce.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/thread/thread_reduce.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/thread/thread_reduce.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/util_ptx.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/util_ptx.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/util_ptx.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/util_type.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/util_type.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/util_type.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/util_type.cuh:#if (__CUDACC_VER_MAJOR__ >= 9 || CUDA_VERSION >= 9000) && !_NVHPC_CUDA
deps/thrust/dependencies/cub/cub/util_type.cuh:    #include <cuda_fp16.h>
deps/thrust/dependencies/cub/cub/util_type.cuh:#if (__CUDACC_VER_MAJOR__ >= 11 || CUDA_VERSION >= 11000) && !_NVHPC_CUDA
deps/thrust/dependencies/cub/cub/util_type.cuh:    #include <cuda_bf16.h>
deps/thrust/dependencies/cub/cub/util_type.cuh:        /// The "true CUDA" alignment of T in bytes
deps/thrust/dependencies/cub/cub/util_type.cuh: * \brief Exposes a member typedef \p Type that names the corresponding CUDA vector type if one exists.  Otherwise \p Type refers to the CubVector structure itself, which will wrap the corresponding \p x, \p y, etc. vector fields.
deps/thrust/dependencies/cub/cub/util_type.cuh:    /// The maximum number of elements in CUDA vector types
deps/thrust/dependencies/cub/cub/util_type.cuh:// Expand CUDA vector types for built-in primitives
deps/thrust/dependencies/cub/cub/util_type.cuh:#if (__CUDACC_VER_MAJOR__ >= 9 || CUDA_VERSION >= 9000) && !_NVHPC_CUDA
deps/thrust/dependencies/cub/cub/util_type.cuh:#if (__CUDACC_VER_MAJOR__ >= 11 || CUDA_VERSION >= 11000) && !_NVHPC_CUDA
deps/thrust/dependencies/cub/cub/util_type.cuh:#if (__CUDACC_VER_MAJOR__ >= 9 || CUDA_VERSION >= 9000) && !_NVHPC_CUDA
deps/thrust/dependencies/cub/cub/util_type.cuh:#if (__CUDACC_VER_MAJOR__ >= 11 || CUDA_VERSION >= 11000) && !_NVHPC_CUDA
deps/thrust/dependencies/cub/cub/util_deprecated.cuh: * Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/util_deprecated.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/util_deprecated.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/version.cuh: * Copyright (c) 2011-2020, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/version.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/version.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/cmake/cub-config.cmake:  # 3) nvcc will automatically check the CUDA Toolkit include path *before* the
deps/thrust/dependencies/cub/cub/util_namespace.cuh: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/util_namespace.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/util_namespace.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/cub.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/cub.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/cub.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/warp/warp_load.cuh: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/warp/warp_load.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/warp/warp_load.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/warp/warp_load.cuh: * Operations for reading linear tiles of data into the CUDA warp.
deps/thrust/dependencies/cub/cub/warp/warp_load.cuh: *        a CUDA warp.
deps/thrust/dependencies/cub/cub/warp/warp_load.cuh:   * from memory using CUDA's built-in vectorized loads as a coalescing optimization.
deps/thrust/dependencies/cub/cub/warp/warp_load.cuh:   *   - The data type @p T is not a built-in primitive or CUDA vector type
deps/thrust/dependencies/cub/cub/warp/warp_load.cuh: *        across a CUDA thread block.
deps/thrust/dependencies/cub/cub/warp/warp_load.cuh: *   targeted CUDA compute-capability (e.g., 32 threads for SM86). Must be a
deps/thrust/dependencies/cub/cub/warp/warp_load.cuh: *      of data is read directly from memory using CUDA's built-in vectorized
deps/thrust/dependencies/cub/cub/warp/warp_merge_sort.cuh: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/warp/warp_merge_sort.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/warp/warp_merge_sort.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/warp/warp_merge_sort.cuh: *        across a CUDA warp using a merge sorting method.
deps/thrust/dependencies/cub/cub/warp/warp_merge_sort.cuh: *   targeted CUDA compute-capability (e.g., 32 threads for SM86). Must be a
deps/thrust/dependencies/cub/cub/warp/warp_scan.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/warp/warp_scan.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/warp/warp_scan.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/warp/warp_scan.cuh: * The cub::WarpScan class provides [<em>collective</em>](index.html#sec0) methods for computing a parallel prefix scan of items partitioned across a CUDA thread warp.
deps/thrust/dependencies/cub/cub/warp/warp_scan.cuh: * \brief The WarpScan class provides [<em>collective</em>](index.html#sec0) methods for computing a parallel prefix scan of items partitioned across a CUDA thread warp.  ![](warp_scan_logo.png)
deps/thrust/dependencies/cub/cub/warp/warp_scan.cuh: * \tparam LOGICAL_WARP_THREADS     <b>[optional]</b> The number of threads per "logical" warp (may be less than the number of hardware warp threads).  Default is the warp size associated with the CUDA Compute Capability targeted by the compiler (e.g., 32 threads for SM20).
deps/thrust/dependencies/cub/cub/warp/specializations/warp_reduce_shfl.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/warp/specializations/warp_reduce_shfl.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/warp/specializations/warp_reduce_shfl.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/warp/specializations/warp_reduce_shfl.cuh: * cub::WarpReduceShfl provides SHFL-based variants of parallel reduction of items partitioned across a CUDA thread warp.
deps/thrust/dependencies/cub/cub/warp/specializations/warp_reduce_shfl.cuh: * \brief WarpReduceShfl provides SHFL-based variants of parallel reduction of items partitioned across a CUDA thread warp.
deps/thrust/dependencies/cub/cub/warp/specializations/warp_scan_shfl.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/warp/specializations/warp_scan_shfl.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/warp/specializations/warp_scan_shfl.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/warp/specializations/warp_scan_shfl.cuh: * cub::WarpScanShfl provides SHFL-based variants of parallel prefix scan of items partitioned across a CUDA thread warp.
deps/thrust/dependencies/cub/cub/warp/specializations/warp_scan_shfl.cuh: * \brief WarpScanShfl provides SHFL-based variants of parallel prefix scan of items partitioned across a CUDA thread warp.
deps/thrust/dependencies/cub/cub/warp/specializations/warp_scan_smem.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/warp/specializations/warp_scan_smem.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/warp/specializations/warp_scan_smem.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/warp/specializations/warp_scan_smem.cuh: * cub::WarpScanSmem provides smem-based variants of parallel prefix scan of items partitioned across a CUDA thread warp.
deps/thrust/dependencies/cub/cub/warp/specializations/warp_scan_smem.cuh: * \brief WarpScanSmem provides smem-based variants of parallel prefix scan of items partitioned across a CUDA thread warp.
deps/thrust/dependencies/cub/cub/warp/specializations/warp_reduce_smem.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/warp/specializations/warp_reduce_smem.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/warp/specializations/warp_reduce_smem.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/warp/specializations/warp_reduce_smem.cuh: * cub::WarpReduceSmem provides smem-based variants of parallel reduction of items partitioned across a CUDA thread warp.
deps/thrust/dependencies/cub/cub/warp/specializations/warp_reduce_smem.cuh: * \brief WarpReduceSmem provides smem-based variants of parallel reduction of items partitioned across a CUDA thread warp.
deps/thrust/dependencies/cub/cub/warp/warp_exchange.cuh: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/warp/warp_exchange.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/warp/warp_exchange.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/warp/warp_exchange.cuh: * methods for rearranging data partitioned across a CUDA warp.
deps/thrust/dependencies/cub/cub/warp/warp_exchange.cuh: *        methods for rearranging data partitioned across a CUDA warp.
deps/thrust/dependencies/cub/cub/warp/warp_exchange.cuh: *   targeted CUDA compute-capability (e.g., 32 threads for SM86). Must be a
deps/thrust/dependencies/cub/cub/warp/warp_reduce.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/warp/warp_reduce.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/warp/warp_reduce.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/warp/warp_reduce.cuh: * The cub::WarpReduce class provides [<em>collective</em>](index.html#sec0) methods for computing a parallel reduction of items partitioned across a CUDA thread warp.
deps/thrust/dependencies/cub/cub/warp/warp_reduce.cuh: * \brief The WarpReduce class provides [<em>collective</em>](index.html#sec0) methods for computing a parallel reduction of items partitioned across a CUDA thread warp. ![](warp_reduce_logo.png)
deps/thrust/dependencies/cub/cub/warp/warp_reduce.cuh: * \tparam LOGICAL_WARP_THREADS     <b>[optional]</b> The number of threads per "logical" warp (may be less than the number of hardware warp threads).  Default is the warp size of the targeted CUDA compute-capability (e.g., 32 threads for SM20).
deps/thrust/dependencies/cub/cub/warp/warp_store.cuh: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/warp/warp_store.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/warp/warp_store.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/warp/warp_store.cuh: * Operations for writing linear segments of data from the CUDA warp
deps/thrust/dependencies/cub/cub/warp/warp_store.cuh: *        cub::WarpStore to write a blocked arrangement of items across a CUDA
deps/thrust/dependencies/cub/cub/warp/warp_store.cuh:   * directly to memory using CUDA's built-in vectorized stores as a coalescing
deps/thrust/dependencies/cub/cub/warp/warp_store.cuh:   *   - The data type @p T is not a built-in primitive or CUDA vector type
deps/thrust/dependencies/cub/cub/warp/warp_store.cuh: *        of items partitioned across a CUDA warp to a linear segment of memory.
deps/thrust/dependencies/cub/cub/warp/warp_store.cuh: *   targeted CUDA compute-capability (e.g., 32 threads for SM86). Must be a
deps/thrust/dependencies/cub/cub/warp/warp_store.cuh: *      of data is written directly to memory using CUDA's built-in vectorized
deps/thrust/dependencies/cub/cub/grid/grid_even_share.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/grid/grid_even_share.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/grid/grid_even_share.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/grid/grid_even_share.cuh: * cub::GridEvenShare is a descriptor utility for distributing input among CUDA thread blocks in an "even-share" fashion.  Each thread block gets roughly the same number of fixed-size work units (grains).
deps/thrust/dependencies/cub/cub/grid/grid_even_share.cuh: * CUDA thread blocks in an "even-share" fashion.  Each thread block gets roughly
deps/thrust/dependencies/cub/cub/grid/grid_barrier.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/grid/grid_barrier.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/grid/grid_barrier.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/grid/grid_barrier.cuh: * cub::GridBarrier implements a software global barrier among thread blocks within a CUDA grid
deps/thrust/dependencies/cub/cub/grid/grid_barrier.cuh: * \brief GridBarrier implements a software global barrier among thread blocks within a CUDA grid
deps/thrust/dependencies/cub/cub/grid/grid_barrier.cuh:    cudaError_t HostReset()
deps/thrust/dependencies/cub/cub/grid/grid_barrier.cuh:        cudaError_t retval = cudaSuccess;
deps/thrust/dependencies/cub/cub/grid/grid_barrier.cuh:            CubDebug(retval = cudaFree(d_sync));
deps/thrust/dependencies/cub/cub/grid/grid_barrier.cuh:    cudaError_t Setup(int sweep_grid_size)
deps/thrust/dependencies/cub/cub/grid/grid_barrier.cuh:        cudaError_t retval = cudaSuccess;
deps/thrust/dependencies/cub/cub/grid/grid_barrier.cuh:                    if (CubDebug(retval = cudaFree(d_sync))) break;
deps/thrust/dependencies/cub/cub/grid/grid_barrier.cuh:                if (CubDebug(retval = cudaMalloc((void**) &d_sync, sync_bytes))) break;
deps/thrust/dependencies/cub/cub/grid/grid_barrier.cuh:                if (CubDebug(retval = cudaMemset(d_sync, 0, new_sync_bytes))) break;
deps/thrust/dependencies/cub/cub/grid/grid_mapping.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/grid/grid_mapping.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/grid/grid_mapping.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/grid/grid_mapping.cuh: * cub::GridMappingStrategy enumerates alternative strategies for mapping constant-sized tiles of device-wide data onto a grid of CUDA thread blocks.
deps/thrust/dependencies/cub/cub/grid/grid_mapping.cuh: * \brief cub::GridMappingStrategy enumerates alternative strategies for mapping constant-sized tiles of device-wide data onto a grid of CUDA thread blocks.
deps/thrust/dependencies/cub/cub/grid/grid_queue.cuh: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/cub/grid/grid_queue.cuh: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/cub/grid/grid_queue.cuh: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/cub/grid/grid_queue.cuh:    __host__ __device__ __forceinline__ cudaError_t FillAndResetDrain(
deps/thrust/dependencies/cub/cub/grid/grid_queue.cuh:        cudaStream_t stream = 0)
deps/thrust/dependencies/cub/cub/grid/grid_queue.cuh:        cudaError_t result = cudaErrorUnknown;
deps/thrust/dependencies/cub/cub/grid/grid_queue.cuh:                result = cudaSuccess;
deps/thrust/dependencies/cub/cub/grid/grid_queue.cuh:                result = CubDebug(cudaMemcpyAsync(d_counters, counters, sizeof(OffsetT) * 2, cudaMemcpyHostToDevice, stream));
deps/thrust/dependencies/cub/cub/grid/grid_queue.cuh:    __host__ __device__ __forceinline__ cudaError_t ResetDrain(cudaStream_t stream = 0)
deps/thrust/dependencies/cub/cub/grid/grid_queue.cuh:        cudaError_t result = cudaErrorUnknown;
deps/thrust/dependencies/cub/cub/grid/grid_queue.cuh:                result = cudaSuccess;
deps/thrust/dependencies/cub/cub/grid/grid_queue.cuh:                result = CubDebug(cudaMemsetAsync(d_counters + DRAIN, 0, sizeof(OffsetT), stream));
deps/thrust/dependencies/cub/cub/grid/grid_queue.cuh:    __host__ __device__ __forceinline__ cudaError_t ResetFill(cudaStream_t stream = 0)
deps/thrust/dependencies/cub/cub/grid/grid_queue.cuh:        cudaError_t result = cudaErrorUnknown;
deps/thrust/dependencies/cub/cub/grid/grid_queue.cuh:                result = cudaSuccess;
deps/thrust/dependencies/cub/cub/grid/grid_queue.cuh:                result = CubDebug(cudaMemsetAsync(d_counters + FILL, 0, sizeof(OffsetT), stream));
deps/thrust/dependencies/cub/cub/grid/grid_queue.cuh:    __host__ __device__ __forceinline__ cudaError_t FillSize(
deps/thrust/dependencies/cub/cub/grid/grid_queue.cuh:        cudaStream_t stream = 0)
deps/thrust/dependencies/cub/cub/grid/grid_queue.cuh:        cudaError_t result = cudaErrorUnknown;
deps/thrust/dependencies/cub/cub/grid/grid_queue.cuh:                result = cudaSuccess;
deps/thrust/dependencies/cub/cub/grid/grid_queue.cuh:                result = CubDebug(cudaMemcpyAsync(&fill_size, d_counters + FILL, sizeof(OffsetT), cudaMemcpyDeviceToHost, stream));
deps/thrust/dependencies/cub/cub/grid/grid_queue.cuh:    /// Drain \p num_items from the queue.  Returns offset from which to read items.  To be called from CUDA kernel.
deps/thrust/dependencies/cub/cub/grid/grid_queue.cuh:    /// Fill \p num_items into the queue.  Returns offset from which to write items.    To be called from CUDA kernel.
deps/thrust/dependencies/cub/CODE_OF_CONDUCT.md:This document defines the Code of Conduct followed and enforced for NVIDIA C++
deps/thrust/dependencies/cub/CODE_OF_CONDUCT.md:  reported by contacting [cpp-conduct@nvidia.com](mailto:cpp-conduct@nvidia.com).
deps/thrust/dependencies/cub/CODE_OF_CONDUCT.md:This Code of Conduct was taken from the [NVIDIA RAPIDS] project, which was
deps/thrust/dependencies/cub/CODE_OF_CONDUCT.md:Please email [cpp-conduct@nvidia.com] for any Code of Conduct related matters.
deps/thrust/dependencies/cub/CODE_OF_CONDUCT.md:[cpp-conduct@nvidia.com]: mailto:cpp-conduct@nvidia.com
deps/thrust/dependencies/cub/CODE_OF_CONDUCT.md:[NVIDIA RAPIDS]: https://docs.rapids.ai/resources/conduct/
deps/thrust/dependencies/cub/README.md:of the CUDA programming model:
deps/thrust/dependencies/cub/README.md:  - Compatible with CUDA dynamic parallelism
deps/thrust/dependencies/cub/README.md:![Orientation of collective primitives within the CUDA software stack](http://nvlabs.github.io/cub/cub_overview.png)
deps/thrust/dependencies/cub/README.md:CUB is included in the NVIDIA HPC SDK and the CUDA Toolkit.
deps/thrust/dependencies/cub/README.md:// Block-sorting CUDA kernel
deps/thrust/dependencies/cub/README.md:CUB is distributed with the NVIDIA HPC SDK and the CUDA Toolkit in addition
deps/thrust/dependencies/cub/README.md:| 1.14.0                    | NVIDIA HPC SDK 21.9                     |
deps/thrust/dependencies/cub/README.md:| 1.13.1                    | CUDA Toolkit 11.5                       |
deps/thrust/dependencies/cub/README.md:| 1.13.0                    | NVIDIA HPC SDK 21.7                     |
deps/thrust/dependencies/cub/README.md:| 1.12.1                    | CUDA Toolkit 11.4                       |
deps/thrust/dependencies/cub/README.md:| 1.12.0                    | NVIDIA HPC SDK 21.3                     |
deps/thrust/dependencies/cub/README.md:| 1.11.0                    | CUDA Toolkit 11.3                       |
deps/thrust/dependencies/cub/README.md:| 1.10.0                    | NVIDIA HPC SDK 20.9 & CUDA Toolkit 11.2 |
deps/thrust/dependencies/cub/README.md:| 1.9.10-1                  | NVIDIA HPC SDK 20.7 & CUDA Toolkit 11.1 |
deps/thrust/dependencies/cub/README.md:| 1.9.10                    | NVIDIA HPC SDK 20.5                     |
deps/thrust/dependencies/cub/README.md:| 1.9.9                     | CUDA Toolkit 11.0                       |
deps/thrust/dependencies/cub/README.md:| 1.9.8-1                   | NVIDIA HPC SDK 20.3                     |
deps/thrust/dependencies/cub/README.md:| 1.9.8                     | CUDA Toolkit 11.0 Early Access          |
deps/thrust/dependencies/cub/README.md:| 1.9.8                     | CUDA 11.0 Early Access                  |
deps/thrust/dependencies/cub/README.md:git clone --recursive https://github.com/NVIDIA/thrust.git
deps/thrust/dependencies/cub/README.md:Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/README.md:   *  Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/README.md:DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/CMakeLists.txt:# 3.18.3 for C++17 + CUDA.
deps/thrust/dependencies/cub/CMakeLists.txt:# Remove this when we use the new CUDA_ARCHITECTURES properties.
deps/thrust/dependencies/cub/CMakeLists.txt:# need the install rules. See GH issue NVIDIA/thrust#1211.
deps/thrust/dependencies/cub/CMakeLists.txt:include(cmake/CubCudaConfig.cmake)
deps/thrust/dependencies/cub/common.mk:# * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/common.mk:# *	 * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/common.mk:# * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/common.mk:    NVCCFLAGS += -rdc=true -lcudadevrt
deps/thrust/dependencies/cub/common.mk:# [abi=<0|1>] CUDA ABI option (enabled by default)
deps/thrust/dependencies/cub/common.mk:NVCCFLAGS += $(SM_DEF) -Xptxas -v -Xcudafe -\#
deps/thrust/dependencies/cub/common.mk:	CUDART_CYG = "$(shell dirname $(NVCC))/../lib/Win32/cudart.lib"
deps/thrust/dependencies/cub/common.mk:	CUDART_CYG = "$(shell dirname $(NVCC))/../lib/x64/cudart.lib"
deps/thrust/dependencies/cub/common.mk:	CUDART = "$(shell cygpath -w $(CUDART_CYG))"
deps/thrust/dependencies/cub/common.mk:    CUDART = "$(shell dirname $(NVCC))/../lib/libcudart_static.a"
deps/thrust/dependencies/cub/common.mk:    CUDART = "$(shell dirname $(NVCC))/../lib64/libcudart_static.a"
deps/thrust/dependencies/cub/cmake/CubCudaConfig.cmake:enable_language(CUDA)
deps/thrust/dependencies/cub/cmake/CubCudaConfig.cmake:# Thrust sets up the architecture flags in CMAKE_CUDA_FLAGS already. Just
deps/thrust/dependencies/cub/cmake/CubCudaConfig.cmake:# reuse them if possible. After we transition to CMake 3.18 CUDA_ARCHITECTURE
deps/thrust/dependencies/cub/cmake/CubCudaConfig.cmake:  set(CMAKE_CUDA_FLAGS "${THRUST_CUDA_FLAGS_BASE} ${THRUST_CUDA_FLAGS_NO_RDC}")
deps/thrust/dependencies/cub/cmake/CubCudaConfig.cmake:  if ("NVCXX" STREQUAL "${CMAKE_CUDA_COMPILER_ID}")
deps/thrust/dependencies/cub/cmake/CubCudaConfig.cmake:      if ("NVCXX" STREQUAL "${CMAKE_CUDA_COMPILER_ID}")
deps/thrust/dependencies/cub/cmake/CubCudaConfig.cmake:        set(arch_flag "-gpu=cc${arch}")
deps/thrust/dependencies/cub/cmake/CubCudaConfig.cmake:  if (NOT "NVCXX" STREQUAL "${CMAKE_CUDA_COMPILER_ID}")
deps/thrust/dependencies/cub/cmake/CubCudaConfig.cmake:  # TODO Once CMake 3.18 is required, use the CUDA_ARCHITECTURE target props
deps/thrust/dependencies/cub/cmake/CubCudaConfig.cmake:  string(APPEND CMAKE_CUDA_FLAGS "${arch_flags}")
deps/thrust/dependencies/cub/cmake/CubCudaConfig.cmake:if ("NVCXX" STREQUAL "${CMAKE_CUDA_COMPILER_ID}")
deps/thrust/dependencies/cub/cmake/CubBuildTargetList.cmake:        CUDA_STANDARD ${dialect}
deps/thrust/dependencies/cub/cmake/CubBuildTargetList.cmake:    # CMake still emits errors about empty CUDA_ARCHITECTURES when CMP0104
deps/thrust/dependencies/cub/cmake/CubBuildTargetList.cmake:          CUDA_ARCHITECTURES OFF
deps/thrust/dependencies/cub/cmake/CubBuildTargetList.cmake:    if ("NVCXX" STREQUAL "${CMAKE_CUDA_COMPILER_ID}")
deps/thrust/dependencies/cub/cmake/CubBuildTargetList.cmake:        CUDA_RESOLVE_DEVICE_SYMBOLS OFF
deps/thrust/dependencies/cub/cmake/CubBuildTargetList.cmake:    thrust_create_target(cub.thrust HOST CPP DEVICE CUDA)
deps/thrust/dependencies/cub/cmake/CubCompilerHacks.cmake:# `CMAKE_CUDA_COMPILER_ID=NVCXX` and `CMAKE_CUDA_COMPILER_FORCED=ON`.
deps/thrust/dependencies/cub/cmake/CubCompilerHacks.cmake:if ("NVCXX" STREQUAL "${CMAKE_CUDA_COMPILER_ID}")
deps/thrust/dependencies/cub/cmake/CubCompilerHacks.cmake:    message(FATAL_ERROR "You are using NVC++ as your CUDA C++ compiler, but have"
deps/thrust/dependencies/cub/cmake/CubCompilerHacks.cmake:  # We don't set CMAKE_CUDA_HOST_COMPILER for NVC++; if we do, CMake tries to
deps/thrust/dependencies/cub/cmake/CubCompilerHacks.cmake:  # pass `-ccbin ${CMAKE_CUDA_HOST_COMPILER}` to NVC++, which it doesn't
deps/thrust/dependencies/cub/cmake/CubCompilerHacks.cmake:  if (NOT "${CMAKE_CUDA_HOST_COMPILER}" STREQUAL "")
deps/thrust/dependencies/cub/cmake/CubCompilerHacks.cmake:    unset(CMAKE_CUDA_HOST_COMPILER CACHE)
deps/thrust/dependencies/cub/cmake/CubCompilerHacks.cmake:    message(FATAL_ERROR "You are using NVC++ as your CUDA C++ compiler, but have"
deps/thrust/dependencies/cub/cmake/CubCompilerHacks.cmake:      " please unset the CMAKE_CUDA_HOST_COMPILER variable."
deps/thrust/dependencies/cub/cmake/CubCompilerHacks.cmake:  set(CMAKE_CXX_COMPILER "${CMAKE_CUDA_COMPILER}")
deps/thrust/dependencies/cub/cmake/CubCompilerHacks.cmake:  set(CMAKE_CUDA_FLAGS "${CMAKE_CUDA_FLAGS} -stdpar")
deps/thrust/dependencies/cub/cmake/CubCompilerHacks.cmake:  set(CMAKE_CUDA_HOST_LINK_LAUNCHER "${CMAKE_CUDA_COMPILER}")
deps/thrust/dependencies/cub/cmake/CubCompilerHacks.cmake:  set(CMAKE_CUDA_LINK_EXECUTABLE
deps/thrust/dependencies/cub/cmake/CubCompilerHacks.cmake:    "<CMAKE_CUDA_HOST_LINK_LAUNCHER> <FLAGS> <LINK_FLAGS> <OBJECTS> -o <TARGET> <LINK_LIBRARIES>")
deps/thrust/dependencies/cub/cmake/CubCompilerHacks.cmake:# We don't set CMAKE_CUDA_HOST_COMPILER for NVC++; if we do, CMake tries to
deps/thrust/dependencies/cub/cmake/CubCompilerHacks.cmake:# pass `-ccbin ${CMAKE_CUDA_HOST_COMPILER}` to NVC++, which it doesn't
deps/thrust/dependencies/cub/cmake/CubCompilerHacks.cmake:if ((NOT "NVCXX" STREQUAL "${CMAKE_CUDA_COMPILER_ID}"))
deps/thrust/dependencies/cub/cmake/CubCompilerHacks.cmake:  if (NOT ("${CMAKE_CUDA_HOST_COMPILER}" STREQUAL "" OR
deps/thrust/dependencies/cub/cmake/CubCompilerHacks.cmake:    "${CMAKE_CUDA_HOST_COMPILER}" STREQUAL "${CMAKE_CXX_COMPILER}"))
deps/thrust/dependencies/cub/cmake/CubCompilerHacks.cmake:    set(tmp "${CMAKE_CUDA_HOST_COMPILER}")
deps/thrust/dependencies/cub/cmake/CubCompilerHacks.cmake:    unset(CMAKE_CUDA_HOST_COMPILER CACHE)
deps/thrust/dependencies/cub/cmake/CubCompilerHacks.cmake:      "CUDA host compiler. Refusing to overwrite specified "
deps/thrust/dependencies/cub/cmake/CubCompilerHacks.cmake:      "CMAKE_CUDA_HOST_COMPILER -- please reconfigure without setting this "
deps/thrust/dependencies/cub/cmake/CubCompilerHacks.cmake:      "CMAKE_CUDA_HOST_COMPILER=${tmp}"
deps/thrust/dependencies/cub/cmake/CubCompilerHacks.cmake:  set(CMAKE_CUDA_HOST_COMPILER "${CMAKE_CXX_COMPILER}")
deps/thrust/dependencies/cub/cmake/CubCompilerHacks.cmake:# `CMAKE_CUDA_COMPILER_ID=NVCXX` and `CMAKE_CUDA_COMPILER_FORCED=ON`.
deps/thrust/dependencies/cub/cmake/CubCompilerHacks.cmake:if ("NVCXX" STREQUAL "${CMAKE_CUDA_COMPILER_ID}")
deps/thrust/dependencies/cub/cmake/CubCompilerHacks.cmake:  set(CMAKE_CUDA_STANDARD_DEFAULT 03)
deps/thrust/dependencies/cub/cmake/CubCompilerHacks.cmake:  set(CMAKE_CUDA03_STANDARD_COMPILE_OPTION "-std=c++03")
deps/thrust/dependencies/cub/cmake/CubCompilerHacks.cmake:  set(CMAKE_CUDA03_EXTENSION_COMPILE_OPTION "-std=c++03")
deps/thrust/dependencies/cub/cmake/CubCompilerHacks.cmake:  set(CMAKE_CUDA03_STANDARD__HAS_FULL_SUPPORT TRUE)
deps/thrust/dependencies/cub/cmake/CubCompilerHacks.cmake:  set_property(GLOBAL PROPERTY CMAKE_CUDA03_KNOWN_FEATURES)
deps/thrust/dependencies/cub/cmake/CubCompilerHacks.cmake:  set(CMAKE_CUDA11_STANDARD_COMPILE_OPTION "-std=c++11")
deps/thrust/dependencies/cub/cmake/CubCompilerHacks.cmake:  set(CMAKE_CUDA11_EXTENSION_COMPILE_OPTION "-std=c++11")
deps/thrust/dependencies/cub/cmake/CubCompilerHacks.cmake:  set(CMAKE_CUDA11_STANDARD__HAS_FULL_SUPPORT TRUE)
deps/thrust/dependencies/cub/cmake/CubCompilerHacks.cmake:  set_property(GLOBAL PROPERTY CMAKE_CUDA11_KNOWN_FEATURES)
deps/thrust/dependencies/cub/cmake/CubCompilerHacks.cmake:  set(CMAKE_CUDA14_STANDARD_COMPILE_OPTION "-std=c++14")
deps/thrust/dependencies/cub/cmake/CubCompilerHacks.cmake:  set(CMAKE_CUDA14_EXTENSION_COMPILE_OPTION "-std=c++14")
deps/thrust/dependencies/cub/cmake/CubCompilerHacks.cmake:  set(CMAKE_CUDA14_STANDARD__HAS_FULL_SUPPORT TRUE)
deps/thrust/dependencies/cub/cmake/CubCompilerHacks.cmake:  set_property(GLOBAL PROPERTY CMAKE_CUDA14_KNOWN_FEATURES)
deps/thrust/dependencies/cub/cmake/CubCompilerHacks.cmake:  set(CMAKE_CUDA17_STANDARD_COMPILE_OPTION "-std=c++17")
deps/thrust/dependencies/cub/cmake/CubCompilerHacks.cmake:  set(CMAKE_CUDA17_EXTENSION_COMPILE_OPTION "-std=c++17")
deps/thrust/dependencies/cub/cmake/CubCompilerHacks.cmake:  set(CMAKE_CUDA17_STANDARD__HAS_FULL_SUPPORT TRUE)
deps/thrust/dependencies/cub/cmake/CubCompilerHacks.cmake:  set_property(GLOBAL PROPERTY CMAKE_CUDA17_KNOWN_FEATURES)
deps/thrust/dependencies/cub/cmake/CubCompilerHacks.cmake:  cmake_record_cuda_compile_features()
deps/thrust/dependencies/cub/cmake/CubCompilerHacks.cmake:  set(CMAKE_CUDA_COMPILE_FEATURES
deps/thrust/dependencies/cub/cmake/CubCompilerHacks.cmake:    ${CMAKE_CUDA03_COMPILE_FEATURES}
deps/thrust/dependencies/cub/cmake/CubCompilerHacks.cmake:    ${CMAKE_CUDA11_COMPILE_FEATURES}
deps/thrust/dependencies/cub/cmake/CubCompilerHacks.cmake:    ${CMAKE_CUDA14_COMPILE_FEATURES}
deps/thrust/dependencies/cub/cmake/CubCompilerHacks.cmake:    ${CMAKE_CUDA17_COMPILE_FEATURES}
deps/thrust/dependencies/cub/cmake/CubCompilerHacks.cmake:    ${CMAKE_CUDA20_COMPILE_FEATURES}
deps/thrust/dependencies/cub/cmake/CubUtilities.cmake:# Enable RDC for a CUDA target. Encapsulates compiler hacks:
deps/thrust/dependencies/cub/cmake/CubUtilities.cmake:function(cub_enable_rdc_for_cuda_target target_name)
deps/thrust/dependencies/cub/cmake/CubUtilities.cmake:  if ("NVCXX" STREQUAL "${CMAKE_CUDA_COMPILER_ID}")
deps/thrust/dependencies/cub/cmake/CubUtilities.cmake:      COMPILE_FLAGS "-gpu=rdc"
deps/thrust/dependencies/cub/cmake/CubUtilities.cmake:      CUDA_SEPARABLE_COMPILATION ON
deps/thrust/dependencies/cub/cmake/CubBuildCompilerTargets.cmake:    # The CUDA `host_runtime.h` header emits this for
deps/thrust/dependencies/cub/cmake/CubBuildCompilerTargets.cmake:    # `__cudaUnregisterBinaryUtil`.
deps/thrust/dependencies/cub/cmake/CubBuildCompilerTargets.cmake:    # This complains about functions in CUDA system headers when used with nvcc.
deps/thrust/dependencies/cub/cmake/CubBuildCompilerTargets.cmake:    # CUB uses deprecated texture functions (cudaBindTexture, etc). These
deps/thrust/dependencies/cub/cmake/CubBuildCompilerTargets.cmake:    # This can be removed once NVIDIA/cub#191 is fixed.
deps/thrust/dependencies/cub/cmake/CubBuildCompilerTargets.cmake:  if ("NVCXX" STREQUAL "${CMAKE_CUDA_COMPILER_ID}")
deps/thrust/dependencies/cub/cmake/CubBuildCompilerTargets.cmake:    # * NVCC accepts CUDA C++ in .cu files but not .cpp files.
deps/thrust/dependencies/cub/cmake/CubBuildCompilerTargets.cmake:    # * NVC++ accepts CUDA C++ in .cpp files but not .cu files.
deps/thrust/dependencies/cub/cmake/CubBuildCompilerTargets.cmake:      $<$<COMPILE_LANG_AND_ID:CUDA,NVCXX>:${cxx_option}>
deps/thrust/dependencies/cub/cmake/CubBuildCompilerTargets.cmake:      # if (using CUDA and CUDA_COMPILER is NVCC) add -Xcompiler=opt:
deps/thrust/dependencies/cub/cmake/CubBuildCompilerTargets.cmake:      $<$<COMPILE_LANG_AND_ID:CUDA,NVIDIA>:-Xcompiler=${cxx_option}>
deps/thrust/dependencies/cub/cmake/CubBuildCompilerTargets.cmake:  # Add these for both CUDA and CXX targets:
deps/thrust/dependencies/cub/cmake/CubBuildCompilerTargets.cmake:    # If using CUDA w/ NVCC...
deps/thrust/dependencies/cub/cmake/CubBuildCompilerTargets.cmake:    $<$<COMPILE_LANG_AND_ID:CUDA,NVIDIA>:-Xcudafe=--display_error_number>
deps/thrust/dependencies/cub/cmake/CubBuildCompilerTargets.cmake:    $<$<COMPILE_LANG_AND_ID:CUDA,NVIDIA>:-Xcudafe=--promote_warnings>
deps/thrust/dependencies/cub/cmake/CubBuildCompilerTargets.cmake:    # Don't complain about deprecated GPU targets.
deps/thrust/dependencies/cub/cmake/CubBuildCompilerTargets.cmake:    $<$<COMPILE_LANG_AND_ID:CUDA,NVIDIA>:-Wno-deprecated-gpu-targets>
deps/thrust/dependencies/cub/cmake/CubBuildCompilerTargets.cmake:    # TexRefInputIterator uses deprecated CUDART APIs, see NVIDIA/cub#191.
deps/thrust/dependencies/cub/cmake/CubBuildCompilerTargets.cmake:    $<$<AND:$<COMPILE_LANG_AND_ID:CUDA,NVIDIA>,$<VERSION_LESS:$<CUDA_COMPILER_VERSION>,11.5>>:-Wno-deprecated-declarations>
deps/thrust/dependencies/cub/examples/block/example_block_radix_sort.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/examples/block/example_block_radix_sort.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/examples/block/example_block_radix_sort.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/examples/block/example_block_radix_sort.cu: *   nvcc -arch=sm_XX example_block_radix_sort.cu -I../.. -lcudart -O3
deps/thrust/dependencies/cub/examples/block/example_block_radix_sort.cu:// Ensure printing of CUDA runtime errors to console (define before including cub.h)
deps/thrust/dependencies/cub/examples/block/example_block_radix_sort.cu:    CubDebugExit(cudaMalloc((void**)&d_in,          sizeof(Key) * TILE_SIZE * g_grid_size));
deps/thrust/dependencies/cub/examples/block/example_block_radix_sort.cu:    CubDebugExit(cudaMalloc((void**)&d_out,         sizeof(Key) * TILE_SIZE * g_grid_size));
deps/thrust/dependencies/cub/examples/block/example_block_radix_sort.cu:    CubDebugExit(cudaMalloc((void**)&d_elapsed,     sizeof(clock_t) * g_grid_size));
deps/thrust/dependencies/cub/examples/block/example_block_radix_sort.cu:    CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(Key) * TILE_SIZE * g_grid_size, cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/examples/block/example_block_radix_sort.cu:    CubDebugExit(cudaPeekAtLastError());
deps/thrust/dependencies/cub/examples/block/example_block_radix_sort.cu:    CubDebugExit(cudaDeviceSynchronize());
deps/thrust/dependencies/cub/examples/block/example_block_radix_sort.cu:    GpuTimer            timer;
deps/thrust/dependencies/cub/examples/block/example_block_radix_sort.cu:        CubDebugExit(cudaMemcpy(h_elapsed, d_elapsed, sizeof(clock_t) * g_grid_size, cudaMemcpyDeviceToHost));
deps/thrust/dependencies/cub/examples/block/example_block_radix_sort.cu:    CubDebugExit(cudaDeviceSynchronize());
deps/thrust/dependencies/cub/examples/block/example_block_radix_sort.cu:    if (d_in) CubDebugExit(cudaFree(d_in));
deps/thrust/dependencies/cub/examples/block/example_block_radix_sort.cu:    if (d_out) CubDebugExit(cudaFree(d_out));
deps/thrust/dependencies/cub/examples/block/example_block_radix_sort.cu:    if (d_elapsed) CubDebugExit(cudaFree(d_elapsed));
deps/thrust/dependencies/cub/examples/block/example_block_reduce_dyn_smem.cu: * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/examples/block/example_block_reduce_dyn_smem.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/examples/block/example_block_reduce_dyn_smem.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/examples/block/example_block_reduce_dyn_smem.cu: *   nvcc -arch=sm_XX example_block_reduce_dyn_smem.cu -I../.. -lcudart -O3 -std=c++14
deps/thrust/dependencies/cub/examples/block/example_block_reduce_dyn_smem.cu:// Ensure printing of CUDA runtime errors to console (define before including cub.h)
deps/thrust/dependencies/cub/examples/block/example_block_reduce_dyn_smem.cu:    cudaMalloc((void**)&d_in,          sizeof(int) * BLOCK_THREADS);
deps/thrust/dependencies/cub/examples/block/example_block_reduce_dyn_smem.cu:    cudaMalloc((void**)&d_out,         sizeof(int) * BLOCK_THREADS);
deps/thrust/dependencies/cub/examples/block/example_block_reduce_dyn_smem.cu:    cudaMemcpy(d_in, h_in, sizeof(int) * BLOCK_THREADS, cudaMemcpyHostToDevice);
deps/thrust/dependencies/cub/examples/block/example_block_reduce_dyn_smem.cu:    cudaStream_t stream = NULL;
deps/thrust/dependencies/cub/examples/block/example_block_reduce_dyn_smem.cu:    CubDebugExit(cudaPeekAtLastError());
deps/thrust/dependencies/cub/examples/block/example_block_reduce_dyn_smem.cu:    CubDebugExit(cudaDeviceSynchronize());
deps/thrust/dependencies/cub/examples/block/example_block_reduce_dyn_smem.cu:    if (d_in) cudaFree(d_in);
deps/thrust/dependencies/cub/examples/block/example_block_reduce_dyn_smem.cu:    if (d_out) cudaFree(d_out);
deps/thrust/dependencies/cub/examples/block/.gitignore:/cuda55.sdf
deps/thrust/dependencies/cub/examples/block/.gitignore:/cuda55.suo
deps/thrust/dependencies/cub/examples/block/.gitignore:/cuda60.sdf
deps/thrust/dependencies/cub/examples/block/.gitignore:/cuda60.suo
deps/thrust/dependencies/cub/examples/block/example_block_reduce.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/examples/block/example_block_reduce.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/examples/block/example_block_reduce.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/examples/block/example_block_reduce.cu: *   nvcc -arch=sm_XX example_block_reduce.cu -I../.. -lcudart -O3
deps/thrust/dependencies/cub/examples/block/example_block_reduce.cu:// Ensure printing of CUDA runtime errors to console (define before including cub.h)
deps/thrust/dependencies/cub/examples/block/example_block_reduce.cu:    int *h_gpu          = new int[TILE_SIZE + 1];
deps/thrust/dependencies/cub/examples/block/example_block_reduce.cu:    cudaMalloc((void**)&d_in,          sizeof(int) * TILE_SIZE);
deps/thrust/dependencies/cub/examples/block/example_block_reduce.cu:    cudaMalloc((void**)&d_out,         sizeof(int) * 1);
deps/thrust/dependencies/cub/examples/block/example_block_reduce.cu:    cudaMalloc((void**)&d_elapsed,     sizeof(clock_t));
deps/thrust/dependencies/cub/examples/block/example_block_reduce.cu:    cudaMemcpy(d_in, h_in, sizeof(int) * TILE_SIZE, cudaMemcpyHostToDevice);
deps/thrust/dependencies/cub/examples/block/example_block_reduce.cu:    GpuTimer    timer;
deps/thrust/dependencies/cub/examples/block/example_block_reduce.cu:        cudaMemcpy(d_in, h_in, sizeof(int) * TILE_SIZE, cudaMemcpyHostToDevice);
deps/thrust/dependencies/cub/examples/block/example_block_reduce.cu:        CubDebugExit(cudaMemcpy(&clocks, d_elapsed, sizeof(clock_t), cudaMemcpyDeviceToHost));
deps/thrust/dependencies/cub/examples/block/example_block_reduce.cu:    CubDebugExit(cudaPeekAtLastError());
deps/thrust/dependencies/cub/examples/block/example_block_reduce.cu:    CubDebugExit(cudaDeviceSynchronize());
deps/thrust/dependencies/cub/examples/block/example_block_reduce.cu:    if (h_gpu) delete[] h_gpu;
deps/thrust/dependencies/cub/examples/block/example_block_reduce.cu:    if (d_in) cudaFree(d_in);
deps/thrust/dependencies/cub/examples/block/example_block_reduce.cu:    if (d_out) cudaFree(d_out);
deps/thrust/dependencies/cub/examples/block/example_block_reduce.cu:    if (d_elapsed) cudaFree(d_elapsed);
deps/thrust/dependencies/cub/examples/block/example_block_scan.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/examples/block/example_block_scan.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/examples/block/example_block_scan.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/examples/block/example_block_scan.cu: *   nvcc -arch=sm_XX example_block_scan.cu -I../.. -lcudart -O3
deps/thrust/dependencies/cub/examples/block/example_block_scan.cu:// Ensure printing of CUDA runtime errors to console (define before including cub.h)
deps/thrust/dependencies/cub/examples/block/example_block_scan.cu:    int *h_gpu          = new int[TILE_SIZE + 1];
deps/thrust/dependencies/cub/examples/block/example_block_scan.cu:    cudaMalloc((void**)&d_in,          sizeof(int) * TILE_SIZE);
deps/thrust/dependencies/cub/examples/block/example_block_scan.cu:    cudaMalloc((void**)&d_out,         sizeof(int) * (TILE_SIZE + 1));
deps/thrust/dependencies/cub/examples/block/example_block_scan.cu:    cudaMalloc((void**)&d_elapsed,     sizeof(clock_t));
deps/thrust/dependencies/cub/examples/block/example_block_scan.cu:    cudaMemcpy(d_in, h_in, sizeof(int) * TILE_SIZE, cudaMemcpyHostToDevice);
deps/thrust/dependencies/cub/examples/block/example_block_scan.cu:    GpuTimer    timer;
deps/thrust/dependencies/cub/examples/block/example_block_scan.cu:        cudaMemcpy(d_in, h_in, sizeof(int) * TILE_SIZE, cudaMemcpyHostToDevice);
deps/thrust/dependencies/cub/examples/block/example_block_scan.cu:        CubDebugExit(cudaMemcpy(&clocks, d_elapsed, sizeof(clock_t), cudaMemcpyDeviceToHost));
deps/thrust/dependencies/cub/examples/block/example_block_scan.cu:    CubDebugExit(cudaPeekAtLastError());
deps/thrust/dependencies/cub/examples/block/example_block_scan.cu:    CubDebugExit(cudaDeviceSynchronize());
deps/thrust/dependencies/cub/examples/block/example_block_scan.cu:    if (h_gpu) delete[] h_gpu;
deps/thrust/dependencies/cub/examples/block/example_block_scan.cu:    if (d_in) cudaFree(d_in);
deps/thrust/dependencies/cub/examples/block/example_block_scan.cu:    if (d_out) cudaFree(d_out);
deps/thrust/dependencies/cub/examples/block/example_block_scan.cu:    if (d_elapsed) cudaFree(d_elapsed);
deps/thrust/dependencies/cub/examples/device/example_device_select_unique.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/examples/device/example_device_select_unique.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/examples/device/example_device_select_unique.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/examples/device/example_device_select_unique.cu: *   nvcc -arch=sm_XX example_device_select_unique.cu -I../.. -lcudart -O3
deps/thrust/dependencies/cub/examples/device/example_device_select_unique.cu:// Ensure printing of CUDA runtime errors to console
deps/thrust/dependencies/cub/examples/device/example_device_select_unique.cu:    CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(int) * num_items, cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/examples/device/example_device_select_if.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/examples/device/example_device_select_if.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/examples/device/example_device_select_if.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/examples/device/example_device_select_if.cu: *   nvcc -arch=sm_XX example_device_select_if.cu -I../.. -lcudart -O3
deps/thrust/dependencies/cub/examples/device/example_device_select_if.cu:// Ensure printing of CUDA runtime errors to console
deps/thrust/dependencies/cub/examples/device/example_device_select_if.cu:    CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(int) * num_items, cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/examples/device/example_device_partition_if.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/examples/device/example_device_partition_if.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/examples/device/example_device_partition_if.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/examples/device/example_device_partition_if.cu: *   nvcc -arch=sm_XX example_device_select_if.cu -I../.. -lcudart -O3
deps/thrust/dependencies/cub/examples/device/example_device_partition_if.cu:// Ensure printing of CUDA runtime errors to console
deps/thrust/dependencies/cub/examples/device/example_device_partition_if.cu:    CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(int) * num_items, cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/examples/device/example_device_reduce.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/examples/device/example_device_reduce.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/examples/device/example_device_reduce.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/examples/device/example_device_reduce.cu: *   nvcc -arch=sm_XX example_device_reduce.cu -I../.. -lcudart -O3
deps/thrust/dependencies/cub/examples/device/example_device_reduce.cu:// Ensure printing of CUDA runtime errors to console
deps/thrust/dependencies/cub/examples/device/example_device_reduce.cu:    CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(int) * num_items, cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/examples/device/example_device_radix_sort.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/examples/device/example_device_radix_sort.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/examples/device/example_device_radix_sort.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/examples/device/example_device_radix_sort.cu: *   nvcc -arch=sm_XX example_device_radix_sort.cu -I../.. -lcudart -O3
deps/thrust/dependencies/cub/examples/device/example_device_radix_sort.cu:// Ensure printing of CUDA runtime errors to console
deps/thrust/dependencies/cub/examples/device/example_device_radix_sort.cu:    CubDebugExit(cudaMemcpy(d_keys.d_buffers[d_keys.selector], h_keys, sizeof(float) * num_items, cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/examples/device/example_device_radix_sort.cu:    CubDebugExit(cudaMemcpy(d_values.d_buffers[d_values.selector], h_values, sizeof(int) * num_items, cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/examples/device/example_device_scan.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/examples/device/example_device_scan.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/examples/device/example_device_scan.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/examples/device/example_device_scan.cu: *   nvcc -arch=sm_XX example_device_scan.cu -I../.. -lcudart -O3
deps/thrust/dependencies/cub/examples/device/example_device_scan.cu:// Ensure printing of CUDA runtime errors to console
deps/thrust/dependencies/cub/examples/device/example_device_scan.cu:    CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(int) * num_items, cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/examples/device/example_device_partition_flagged.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/examples/device/example_device_partition_flagged.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/examples/device/example_device_partition_flagged.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/examples/device/example_device_partition_flagged.cu: *   nvcc -arch=sm_XX example_device_partition_flagged.cu -I../.. -lcudart -O3
deps/thrust/dependencies/cub/examples/device/example_device_partition_flagged.cu:// Ensure printing of CUDA runtime errors to console
deps/thrust/dependencies/cub/examples/device/example_device_partition_flagged.cu:    CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(int) * num_items, cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/examples/device/example_device_partition_flagged.cu:    CubDebugExit(cudaMemcpy(d_flags, h_flags, sizeof(unsigned char) * num_items, cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/examples/device/example_device_select_flagged.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/examples/device/example_device_select_flagged.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/examples/device/example_device_select_flagged.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/examples/device/example_device_select_flagged.cu: *   nvcc -arch=sm_XX example_device_select_flagged.cu -I../.. -lcudart -O3
deps/thrust/dependencies/cub/examples/device/example_device_select_flagged.cu:// Ensure printing of CUDA runtime errors to console
deps/thrust/dependencies/cub/examples/device/example_device_select_flagged.cu:    CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(int) * num_items, cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/examples/device/example_device_select_flagged.cu:    CubDebugExit(cudaMemcpy(d_flags, h_flags, sizeof(unsigned char) * num_items, cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/examples/device/example_device_sort_find_non_trivial_runs.cu: * Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
deps/thrust/dependencies/cub/examples/device/example_device_sort_find_non_trivial_runs.cu: *     * Neither the name of the NVIDIA CORPORATION nor the
deps/thrust/dependencies/cub/examples/device/example_device_sort_find_non_trivial_runs.cu: * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
deps/thrust/dependencies/cub/examples/device/example_device_sort_find_non_trivial_runs.cu: *   nvcc -arch=sm_XX example_device_sort_find_non_trivial_runs.cu -I../.. -lcudart -O3
deps/thrust/dependencies/cub/examples/device/example_device_sort_find_non_trivial_runs.cu:// Ensure printing of CUDA runtime errors to console
deps/thrust/dependencies/cub/examples/device/example_device_sort_find_non_trivial_runs.cu:    GpuTimer gpu_timer;
deps/thrust/dependencies/cub/examples/device/example_device_sort_find_non_trivial_runs.cu:    GpuTimer gpu_rle_timer;
deps/thrust/dependencies/cub/examples/device/example_device_sort_find_non_trivial_runs.cu:        CubDebugExit(cudaMemcpy(d_keys.d_buffers[d_keys.selector], h_keys, sizeof(float) * num_items, cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/examples/device/example_device_sort_find_non_trivial_runs.cu:        CubDebugExit(cudaMemcpy(d_values.d_buffers[d_values.selector], h_values, sizeof(int) * num_items, cudaMemcpyHostToDevice));
deps/thrust/dependencies/cub/examples/device/example_device_sort_find_non_trivial_runs.cu:        gpu_timer.Start();
deps/thrust/dependencies/cub/examples/device/example_device_sort_find_non_trivial_runs.cu:        gpu_rle_timer.Start();
deps/thrust/dependencies/cub/examples/device/example_device_sort_find_non_trivial_runs.cu:        gpu_timer.Stop();
deps/thrust/dependencies/cub/examples/device/example_device_sort_find_non_trivial_runs.cu:        gpu_rle_timer.Stop();
deps/thrust/dependencies/cub/examples/device/example_device_sort_find_non_trivial_runs.cu:            elapsed_millis += gpu_timer.ElapsedMillis();
deps/thrust/dependencies/cub/examples/device/example_device_sort_find_non_trivial_runs.cu:            elapsed_rle_millis += gpu_rle_timer.ElapsedMillis();
deps/thrust/dependencies/cub/examples/device/example_device_sort_find_non_trivial_runs.cu:        // GPU cleanup
deps/thrust/dependencies/cub/examples/device/.gitignore:/cuda55.sdf
deps/thrust/dependencies/cub/examples/device/.gitignore:/cuda55.suo
deps/thrust/dependencies/cub/examples/device/.gitignore:/cuda60.sdf
deps/thrust/dependencies/cub/examples/device/.gitignore:/cuda60.suo
deps/thrust/dependencies/cub/examples/CMakeLists.txt:#   instance, examples/vector.cu will be "vector", and examples/cuda/copy.cu
deps/thrust/dependencies/cub/examples/CMakeLists.txt:#   would be "cuda.copy".
deps/thrust/dependencies/cub/examples/CMakeLists.txt:    cub_enable_rdc_for_cuda_target(${example_target})
deps/thrust/dependencies/cub/examples/cmake/CMakeLists.txt:    -D "CMAKE_CUDA_COMPILER=${CMAKE_CUDA_COMPILER}"
deps/thrust/dependencies/cub/examples/cmake/add_subdir/CMakeLists.txt:# Silence warnings about empty CUDA_ARCHITECTURES properties on example targets:
deps/thrust/dependencies/cub/examples/cmake/add_subdir/CMakeLists.txt:project(CubAddSubDirExample CUDA)
deps/thrust/dependencies/cub/CONTRIBUTING.md:[CONTRIBUTING.md](https://github.com/NVIDIA/thrust/blob/main/CONTRIBUTING.md).
deps/thrust/dependencies/cub/CONTRIBUTING.md:  - Controls the targeted CUDA architecture(s)
deps/thrust/dependencies/cub/CONTRIBUTING.md:  - Multiple options may be selected when using NVCC as the CUDA compiler.
deps/thrust/dependencies/cub/CONTRIBUTING.md:  - If enabled, CUDA objects will target the most recent virtual architecture
deps/thrust/dependencies/cub/CONTRIBUTING.md:[here](https://github.com/NVIDIA/thrust/blob/main/CONTRIBUTING.md#development-model).
deps/thrust/cmake/ThrustMultiConfig.cmake:    # Option to enable all standards supported by the CUDA and CXX compilers:
deps/thrust/cmake/ThrustMultiConfig.cmake:    option(THRUST_MULTICONFIG_ENABLE_SYSTEM_CUDA "Generate build configurations that use CUDA." ON)
deps/thrust/cmake/ThrustMultiConfig.cmake:        THRUST_MULTICONFIG_ENABLE_SYSTEM_CUDA)
deps/thrust/cmake/ThrustMultiConfig.cmake:    # CPP/CUDA | F L M S   | Essential  | Expensive | Validates CUDA against CPP
deps/thrust/cmake/ThrustMultiConfig.cmake:    # TBB/CUDA | F L       | Important  | Expensive | Validates TBB/CUDA interop
deps/thrust/cmake/ThrustMultiConfig.cmake:    # OMP/CUDA | F L       | Important  | Expensive | Validates OMP/CUDA interop
deps/thrust/cmake/ThrustMultiConfig.cmake:      CPP_OMP CPP_TBB CPP_CUDA
deps/thrust/cmake/ThrustMultiConfig.cmake:      OMP_CUDA TBB_CUDA
deps/thrust/cmake/ThrustMultiConfig.cmake:        THRUST_DEVICE_SYSTEM STREQUAL "CUDA")
deps/thrust/cmake/ThrustBuildCompilerTargets.cmake:# thrust.promote_cudafe_warnings
deps/thrust/cmake/ThrustBuildCompilerTargets.cmake:# - Interface target that adds warning promotion for NVCC cudafe invocations.
deps/thrust/cmake/ThrustBuildCompilerTargets.cmake:# - Only exists to work around github issue #1174 on tbb.cuda configurations.
deps/thrust/cmake/ThrustBuildCompilerTargets.cmake:    # See NVIDIA/thrust#1273, NVBug 3129879.
deps/thrust/cmake/ThrustBuildCompilerTargets.cmake:    # The CUDA `host_runtime.h` header emits this for
deps/thrust/cmake/ThrustBuildCompilerTargets.cmake:    # `__cudaUnregisterBinaryUtil`.
deps/thrust/cmake/ThrustBuildCompilerTargets.cmake:    # This complains about functions in CUDA system headers when used with nvcc.
deps/thrust/cmake/ThrustBuildCompilerTargets.cmake:  if ("NVCXX" STREQUAL "${CMAKE_CUDA_COMPILER_ID}")
deps/thrust/cmake/ThrustBuildCompilerTargets.cmake:    # * NVCC accepts CUDA C++ in .cu files but not .cpp files.
deps/thrust/cmake/ThrustBuildCompilerTargets.cmake:    # * NVC++ accepts CUDA C++ in .cpp files but not .cu files.
deps/thrust/cmake/ThrustBuildCompilerTargets.cmake:      $<$<AND:$<COMPILE_LANGUAGE:CUDA>,$<CUDA_COMPILER_ID:NVCXX>>:${cxx_option}>
deps/thrust/cmake/ThrustBuildCompilerTargets.cmake:      # if (using CUDA and CUDA_COMPILER is NVCC) add -Xcompiler=opt:
deps/thrust/cmake/ThrustBuildCompilerTargets.cmake:      $<$<AND:$<COMPILE_LANGUAGE:CUDA>,$<CUDA_COMPILER_ID:NVIDIA>>:-Xcompiler=${cxx_option}>
deps/thrust/cmake/ThrustBuildCompilerTargets.cmake:    # Add these for both CUDA and CXX targets:
deps/thrust/cmake/ThrustBuildCompilerTargets.cmake:  # Display warning numbers from nvcc cudafe errors:
deps/thrust/cmake/ThrustBuildCompilerTargets.cmake:    # If using CUDA w/ NVCC...
deps/thrust/cmake/ThrustBuildCompilerTargets.cmake:    $<$<AND:$<COMPILE_LANGUAGE:CUDA>,$<CUDA_COMPILER_ID:NVIDIA>>:-Xcudafe=--display_error_number>
deps/thrust/cmake/ThrustBuildCompilerTargets.cmake:  # Tell NVCC to be quiet about deprecated GPU targets:
deps/thrust/cmake/ThrustBuildCompilerTargets.cmake:    # If using CUDA w/ NVCC...
deps/thrust/cmake/ThrustBuildCompilerTargets.cmake:    $<$<AND:$<COMPILE_LANGUAGE:CUDA>,$<CUDA_COMPILER_ID:NVIDIA>>:-Wno-deprecated-gpu-targets>
deps/thrust/cmake/ThrustBuildCompilerTargets.cmake:  add_library(thrust.promote_cudafe_warnings INTERFACE)
deps/thrust/cmake/ThrustBuildCompilerTargets.cmake:  target_compile_options(thrust.promote_cudafe_warnings INTERFACE
deps/thrust/cmake/ThrustBuildCompilerTargets.cmake:    $<$<AND:$<COMPILE_LANGUAGE:CUDA>,$<CUDA_COMPILER_ID:NVIDIA>>:-Xcudafe=--promote_warnings>
deps/thrust/cmake/ThrustBuildCompilerTargets.cmake:      $<$<AND:$<COMPILE_LANGUAGE:CUDA>,$<CUDA_COMPILER_ID:NVIDIA>>:-Xcompiler=/wd4702>
deps/thrust/cmake/ThrustBuildCompilerTargets.cmake:      $<$<AND:$<COMPILE_LANGUAGE:CUDA>,$<CUDA_COMPILER_ID:NVIDIA>>:-Xcompiler=/wd4127>
deps/thrust/cmake/ThrustBuildCompilerTargets.cmake:      $<$<AND:$<COMPILE_LANGUAGE:CUDA>,$<CUDA_COMPILER_ID:NVIDIA>>:-Xcompiler=/wd4127>
deps/thrust/cmake/ThrustBuildTargetList.cmake:#     - DEVICE: The device system. Valid values: CUDA, CPP, OMP, TBB.
deps/thrust/cmake/ThrustBuildTargetList.cmake:  BRIEF_DOCS "A target's device system: CUDA, CPP, TBB, or OMP."
deps/thrust/cmake/ThrustBuildTargetList.cmake:  FULL_DOCS "A target's device system: CUDA, CPP, TBB, or OMP."
deps/thrust/cmake/ThrustBuildTargetList.cmake:  BRIEF_DOCS "A prefix describing the config, eg. 'thrust.cpp.cuda.cpp14'."
deps/thrust/cmake/ThrustBuildTargetList.cmake:  FULL_DOCS "A prefix describing the config, eg. 'thrust.cpp.cuda.cpp14'."
deps/thrust/cmake/ThrustBuildTargetList.cmake:  if (CUDA IN_LIST langs)
deps/thrust/cmake/ThrustBuildTargetList.cmake:    list(APPEND standard_features cuda_std_${dialect})
deps/thrust/cmake/ThrustBuildTargetList.cmake:        CUDA_STANDARD ${dialect}
deps/thrust/cmake/ThrustBuildTargetList.cmake:        CUDA_STANDARD_REQUIRED ON
deps/thrust/cmake/ThrustBuildTargetList.cmake:    # CMake still emits errors about empty CUDA_ARCHITECTURES when CMP0104
deps/thrust/cmake/ThrustBuildTargetList.cmake:          CUDA_ARCHITECTURES OFF
deps/thrust/cmake/ThrustBuildTargetList.cmake:    if ("CUDA" STREQUAL "${device}" AND
deps/thrust/cmake/ThrustBuildTargetList.cmake:        "NVCXX" STREQUAL "${CMAKE_CUDA_COMPILER_ID}")
deps/thrust/cmake/ThrustBuildTargetList.cmake:        CUDA_RESOLVE_DEVICE_SYMBOLS OFF
deps/thrust/cmake/ThrustBuildTargetList.cmake:  # Workaround Github issue #1174. cudafe promote TBB header warnings to
deps/thrust/cmake/ThrustBuildTargetList.cmake:  if ((NOT host STREQUAL "TBB") OR (NOT device STREQUAL "CUDA"))
deps/thrust/cmake/ThrustBuildTargetList.cmake:      thrust.promote_cudafe_warnings
deps/thrust/cmake/ThrustBuildTargetList.cmake:  # Detect supported dialects if requested -- this must happen after CUDA is
deps/thrust/cmake/ThrustBuildTargetList.cmake:    if (THRUST_CUDA_FOUND)
deps/thrust/cmake/ThrustBuildTargetList.cmake:      detect_supported_standards(THRUST CUDA ${THRUST_CPP_DIALECT_OPTIONS})
deps/thrust/cmake/ThrustBuildTargetList.cmake:    # Take the union of supported standards in CXX and CUDA:
deps/thrust/cmake/ThrustBuildTargetList.cmake:          ((NOT THRUST_CUDA_FOUND) OR THRUST_CUDA_${standard}_SUPPORTED))
deps/thrust/cmake/ThrustUtilities.cmake:# Enable RDC for a CUDA target. Encapsulates compiler hacks:
deps/thrust/cmake/ThrustUtilities.cmake:function(thrust_enable_rdc_for_cuda_target target_name)
deps/thrust/cmake/ThrustUtilities.cmake:  if ("NVCXX" STREQUAL "${CMAKE_CUDA_COMPILER_ID}")
deps/thrust/cmake/ThrustUtilities.cmake:      COMPILE_FLAGS "-gpu=rdc"
deps/thrust/cmake/ThrustUtilities.cmake:      CUDA_SEPARABLE_COMPILATION ON
deps/thrust/cmake/DetectSupportedStandards.cmake:# - lang: The language to test: C, CXX, or CUDA.
deps/thrust/cmake/DetectSupportedStandards.cmake:        (lang STREQUAL "CXX" OR lang STREQUAL "CUDA") AND
deps/thrust/cmake/ThrustHeaderTesting.cmake:  # List of headers that aren't implemented for all backends, but are implemented for CUDA.
deps/thrust/cmake/ThrustHeaderTesting.cmake:  set(partially_implemented_CUDA
deps/thrust/cmake/ThrustHeaderTesting.cmake:    ${partially_implemented_CUDA}
deps/thrust/cmake/ThrustHeaderTesting.cmake:    if ("CUDA" STREQUAL "${config_device}")
deps/thrust/cmake/ThrustFindThrust.cmake:  if (THRUST_MULTICONFIG_ENABLE_SYSTEM_CUDA)
deps/thrust/cmake/ThrustFindThrust.cmake:    list(APPEND req_systems CUDA)
deps/thrust/cmake/ThrustCompilerHacks.cmake:# `CMAKE_CUDA_COMPILER_ID=NVCXX and `CMAKE_CUDA_COMPILER_FORCED=ON`.
deps/thrust/cmake/ThrustCompilerHacks.cmake:if ("NVCXX" STREQUAL "${CMAKE_CUDA_COMPILER_ID}")
deps/thrust/cmake/ThrustCompilerHacks.cmake:    message(FATAL_ERROR "You are using NVC++ as your CUDA C++ compiler, but have"
deps/thrust/cmake/ThrustCompilerHacks.cmake:  # We don't set CMAKE_CUDA_HOST_COMPILER for NVC++; if we do, CMake tries to
deps/thrust/cmake/ThrustCompilerHacks.cmake:  # pass `-ccbin ${CMAKE_CUDA_HOST_COMPILER}` to NVC++, which it doesn't
deps/thrust/cmake/ThrustCompilerHacks.cmake:  if (NOT "${CMAKE_CUDA_HOST_COMPILER}" STREQUAL "")
deps/thrust/cmake/ThrustCompilerHacks.cmake:    unset(CMAKE_CUDA_HOST_COMPILER CACHE)
deps/thrust/cmake/ThrustCompilerHacks.cmake:    message(FATAL_ERROR "You are using NVC++ as your CUDA C++ compiler, but have"
deps/thrust/cmake/ThrustCompilerHacks.cmake:      " please unset the CMAKE_CUDA_HOST_COMPILER variable."
deps/thrust/cmake/ThrustCompilerHacks.cmake:  set(CMAKE_CXX_COMPILER "${CMAKE_CUDA_COMPILER}")
deps/thrust/cmake/ThrustCompilerHacks.cmake:  set(CMAKE_CUDA_FLAGS "${CMAKE_CUDA_FLAGS} -stdpar")
deps/thrust/cmake/ThrustCompilerHacks.cmake:  set(CMAKE_CUDA_HOST_LINK_LAUNCHER "${CMAKE_CUDA_COMPILER}")
deps/thrust/cmake/ThrustCompilerHacks.cmake:  set(CMAKE_CUDA_LINK_EXECUTABLE
deps/thrust/cmake/ThrustCompilerHacks.cmake:    "<CMAKE_CUDA_HOST_LINK_LAUNCHER> <FLAGS> <LINK_FLAGS> <OBJECTS> -o <TARGET> <LINK_LIBRARIES>")
deps/thrust/cmake/ThrustCompilerHacks.cmake:# We don't set CMAKE_CUDA_HOST_COMPILER for NVC++; if we do, CMake tries to
deps/thrust/cmake/ThrustCompilerHacks.cmake:# pass `-ccbin ${CMAKE_CUDA_HOST_COMPILER}` to NVC++, which it doesn't
deps/thrust/cmake/ThrustCompilerHacks.cmake:if ((NOT "NVCXX" STREQUAL "${CMAKE_CUDA_COMPILER_ID}"))
deps/thrust/cmake/ThrustCompilerHacks.cmake:  if (NOT ("${CMAKE_CUDA_HOST_COMPILER}" STREQUAL "" OR
deps/thrust/cmake/ThrustCompilerHacks.cmake:    "${CMAKE_CUDA_HOST_COMPILER}" STREQUAL "${CMAKE_CXX_COMPILER}"))
deps/thrust/cmake/ThrustCompilerHacks.cmake:    set(tmp "${CMAKE_CUDA_HOST_COMPILER}")
deps/thrust/cmake/ThrustCompilerHacks.cmake:    unset(CMAKE_CUDA_HOST_COMPILER CACHE)
deps/thrust/cmake/ThrustCompilerHacks.cmake:      "CUDA host compiler. Refusing to overwrite specified "
deps/thrust/cmake/ThrustCompilerHacks.cmake:      "CMAKE_CUDA_HOST_COMPILER -- please reconfigure without setting this "
deps/thrust/cmake/ThrustCompilerHacks.cmake:      "CMAKE_CUDA_HOST_COMPILER=${tmp}"
deps/thrust/cmake/ThrustCompilerHacks.cmake:  set(CMAKE_CUDA_HOST_COMPILER "${CMAKE_CXX_COMPILER}")
deps/thrust/cmake/ThrustCompilerHacks.cmake:# `CMAKE_CUDA_COMPILER_ID=NVCXX and `CMAKE_CUDA_COMPILER_FORCED=ON`.
deps/thrust/cmake/ThrustCompilerHacks.cmake:if ("NVCXX" STREQUAL "${CMAKE_CUDA_COMPILER_ID}")
deps/thrust/cmake/ThrustCompilerHacks.cmake:  set(CMAKE_CUDA_STANDARD_DEFAULT 03)
deps/thrust/cmake/ThrustCompilerHacks.cmake:  set(CMAKE_CUDA03_STANDARD_COMPILE_OPTION "-std=c++03")
deps/thrust/cmake/ThrustCompilerHacks.cmake:  set(CMAKE_CUDA03_EXTENSION_COMPILE_OPTION "-std=c++03")
deps/thrust/cmake/ThrustCompilerHacks.cmake:  set(CMAKE_CUDA03_STANDARD__HAS_FULL_SUPPORT TRUE)
deps/thrust/cmake/ThrustCompilerHacks.cmake:  set_property(GLOBAL PROPERTY CMAKE_CUDA03_KNOWN_FEATURES)
deps/thrust/cmake/ThrustCompilerHacks.cmake:  set(CMAKE_CUDA11_STANDARD_COMPILE_OPTION "-std=c++11")
deps/thrust/cmake/ThrustCompilerHacks.cmake:  set(CMAKE_CUDA11_EXTENSION_COMPILE_OPTION "-std=c++11")
deps/thrust/cmake/ThrustCompilerHacks.cmake:  set(CMAKE_CUDA11_STANDARD__HAS_FULL_SUPPORT TRUE)
deps/thrust/cmake/ThrustCompilerHacks.cmake:  set_property(GLOBAL PROPERTY CMAKE_CUDA11_KNOWN_FEATURES)
deps/thrust/cmake/ThrustCompilerHacks.cmake:  set(CMAKE_CUDA14_STANDARD_COMPILE_OPTION "-std=c++14")
deps/thrust/cmake/ThrustCompilerHacks.cmake:  set(CMAKE_CUDA14_EXTENSION_COMPILE_OPTION "-std=c++14")
deps/thrust/cmake/ThrustCompilerHacks.cmake:  set(CMAKE_CUDA14_STANDARD__HAS_FULL_SUPPORT TRUE)
deps/thrust/cmake/ThrustCompilerHacks.cmake:  set_property(GLOBAL PROPERTY CMAKE_CUDA14_KNOWN_FEATURES)
deps/thrust/cmake/ThrustCompilerHacks.cmake:  set(CMAKE_CUDA17_STANDARD_COMPILE_OPTION "-std=c++17")
deps/thrust/cmake/ThrustCompilerHacks.cmake:  set(CMAKE_CUDA17_EXTENSION_COMPILE_OPTION "-std=c++17")
deps/thrust/cmake/ThrustCompilerHacks.cmake:  set(CMAKE_CUDA17_STANDARD__HAS_FULL_SUPPORT TRUE)
deps/thrust/cmake/ThrustCompilerHacks.cmake:  set_property(GLOBAL PROPERTY CMAKE_CUDA17_KNOWN_FEATURES)
deps/thrust/cmake/ThrustCompilerHacks.cmake:  cmake_record_cuda_compile_features()
deps/thrust/cmake/ThrustCompilerHacks.cmake:  set(CMAKE_CUDA_COMPILE_FEATURES
deps/thrust/cmake/ThrustCompilerHacks.cmake:    ${CMAKE_CUDA03_COMPILE_FEATURES}
deps/thrust/cmake/ThrustCompilerHacks.cmake:    ${CMAKE_CUDA11_COMPILE_FEATURES}
deps/thrust/cmake/ThrustCompilerHacks.cmake:    ${CMAKE_CUDA14_COMPILE_FEATURES}
deps/thrust/cmake/ThrustCompilerHacks.cmake:    ${CMAKE_CUDA17_COMPILE_FEATURES}
deps/thrust/cmake/ThrustCompilerHacks.cmake:    ${CMAKE_CUDA20_COMPILE_FEATURES}
deps/thrust/cmake/detect_compute_archs.cu: *  Copyright 2019-2020 NVIDIA Corporation
deps/thrust/cmake/detect_compute_archs.cu:  if ((cudaGetDeviceCount(&devices) == cudaSuccess) && (devices > 0)) {
deps/thrust/cmake/detect_compute_archs.cu:      cudaDeviceProp prop;
deps/thrust/cmake/detect_compute_archs.cu:      if(cudaGetDeviceProperties(&prop, dev) != cudaSuccess) continue;
deps/thrust/cmake/ThrustCudaConfig.cmake:enable_language(CUDA)
deps/thrust/cmake/ThrustCudaConfig.cmake:# Split CUDA_FLAGS into 3 parts:
deps/thrust/cmake/ThrustCudaConfig.cmake:# THRUST_CUDA_FLAGS_BASE: Common CUDA flags for all targets.
deps/thrust/cmake/ThrustCudaConfig.cmake:# THRUST_CUDA_FLAGS_RDC: Additional CUDA flags for targets compiled with RDC.
deps/thrust/cmake/ThrustCudaConfig.cmake:# THRUST_CUDA_FLAGS_NO_RDC: Additional CUDA flags for targets compiled without RDC.
deps/thrust/cmake/ThrustCudaConfig.cmake:# This is necessary because CUDA SMs 5.3, 6.2, and 7.2 do not support RDC, but
deps/thrust/cmake/ThrustCudaConfig.cmake:# we want to always build some targets (e.g. testing/cuda/*) with RDC.
deps/thrust/cmake/ThrustCudaConfig.cmake:# those SMs. This requires two sets of CUDA_FLAGS.
deps/thrust/cmake/ThrustCudaConfig.cmake:# Because of how CMake handles the CMAKE_CUDA_FLAGS variables, every target
deps/thrust/cmake/ThrustCudaConfig.cmake:# generated in a given directory will use the same value for CMAKE_CUDA_FLAGS,
deps/thrust/cmake/ThrustCudaConfig.cmake:# CUDA_FLAGS: https://gitlab.kitware.com/cmake/cmake/-/issues/18265
deps/thrust/cmake/ThrustCudaConfig.cmake:set(THRUST_CUDA_FLAGS_BASE "${CMAKE_CUDA_FLAGS}")
deps/thrust/cmake/ThrustCudaConfig.cmake:set(THRUST_CUDA_FLAGS_RDC)
deps/thrust/cmake/ThrustCudaConfig.cmake:set(THRUST_CUDA_FLAGS_NO_RDC)
deps/thrust/cmake/ThrustCudaConfig.cmake:  "If ON, compute architectures for all GPUs in the current system are enabled and all other compute architectures are disabled."
deps/thrust/cmake/ThrustCudaConfig.cmake:  if ("NVCXX" STREQUAL "${CMAKE_CUDA_COMPILER_ID}")
deps/thrust/cmake/ThrustCudaConfig.cmake:      COMMAND ${CMAKE_CUDA_COMPILER}
deps/thrust/cmake/ThrustCudaConfig.cmake:if ("NVCXX" STREQUAL "${CMAKE_CUDA_COMPILER_ID}")
deps/thrust/cmake/ThrustCudaConfig.cmake:  if ("NVCXX" STREQUAL "${CMAKE_CUDA_COMPILER_ID}")
deps/thrust/cmake/ThrustCudaConfig.cmake:    set(arch_flag "-gpu=cc${arch}")
deps/thrust/cmake/ThrustCudaConfig.cmake:  string(APPEND THRUST_CUDA_FLAGS_NO_RDC " ${arch_flag}")
deps/thrust/cmake/ThrustCudaConfig.cmake:    string(APPEND THRUST_CUDA_FLAGS_RDC " ${arch_flag}")
deps/thrust/cmake/ThrustCudaConfig.cmake:if (NOT "NVCXX" STREQUAL "${CMAKE_CUDA_COMPILER_ID}")
deps/thrust/cmake/ThrustCudaConfig.cmake:    string(APPEND THRUST_CUDA_FLAGS_BASE
deps/thrust/cmake/ThrustCudaConfig.cmake:if ("NVCXX" STREQUAL "${CMAKE_CUDA_COMPILER_ID}")
deps/thrust/cmake/ThrustCudaConfig.cmake:set(CMAKE_CUDA_FLAGS "${THRUST_CUDA_FLAGS_BASE} ${THRUST_CUDA_FLAGS_NO_RDC}")
deps/thrust/examples/raw_reference_cast.cu:// the result is a type such as thrust::system::cuda::reference<int>
deps/thrust/examples/README.md:  https://github.com/NVIDIA/thrust/tree/main/examples
deps/thrust/examples/include/timer.h: *  Copyright 2008-2009 NVIDIA Corporation
deps/thrust/examples/include/timer.h:#ifdef __CUDACC__
deps/thrust/examples/include/timer.h:// use CUDA's high-resolution timers when possible
deps/thrust/examples/include/timer.h:#include <cuda_runtime_api.h>
deps/thrust/examples/include/timer.h:#include <thrust/system/cuda/error.h>
deps/thrust/examples/include/timer.h:void cuda_safe_call(cudaError_t error, const std::string& message = "")
deps/thrust/examples/include/timer.h:    throw thrust::system_error(error, thrust::cuda_category(), message);
deps/thrust/examples/include/timer.h:  cudaEvent_t start;
deps/thrust/examples/include/timer.h:  cudaEvent_t end;
deps/thrust/examples/include/timer.h:    cuda_safe_call(cudaEventCreate(&start));
deps/thrust/examples/include/timer.h:    cuda_safe_call(cudaEventCreate(&end));
deps/thrust/examples/include/timer.h:    cuda_safe_call(cudaEventDestroy(start));
deps/thrust/examples/include/timer.h:    cuda_safe_call(cudaEventDestroy(end));
deps/thrust/examples/include/timer.h:    cuda_safe_call(cudaEventRecord(start, 0));
deps/thrust/examples/include/timer.h:    cuda_safe_call(cudaEventRecord(end, 0));
deps/thrust/examples/include/timer.h:    cuda_safe_call(cudaEventSynchronize(end));
deps/thrust/examples/include/timer.h:    cuda_safe_call(cudaEventElapsedTime(&ms_elapsed, start, end));
deps/thrust/examples/CMakeLists.txt:# Update flags to reflect RDC options. See note in ThrustCudaConfig.cmake --
deps/thrust/examples/CMakeLists.txt:  set(CMAKE_CUDA_FLAGS "${THRUST_CUDA_FLAGS_BASE} ${THRUST_CUDA_FLAGS_RDC}")
deps/thrust/examples/CMakeLists.txt:  set(CMAKE_CUDA_FLAGS "${THRUST_CUDA_FLAGS_BASE} ${THRUST_CUDA_FLAGS_NO_RDC}")
deps/thrust/examples/CMakeLists.txt:#   instance, examples/vector.cu will be "vector", and examples/cuda/copy.cu
deps/thrust/examples/CMakeLists.txt:#   would be "cuda.copy".
deps/thrust/examples/CMakeLists.txt:  # Wrap the .cu file in .cpp for non-CUDA backends
deps/thrust/examples/CMakeLists.txt:  if ("CUDA" STREQUAL "${config_device}")
deps/thrust/examples/CMakeLists.txt:  if ("CUDA" STREQUAL "${config_device}" AND
deps/thrust/examples/CMakeLists.txt:    thrust_enable_rdc_for_cuda_target(${example_target})
deps/thrust/examples/CMakeLists.txt:  # (e.g. "thrust.cpp.cuda.cpp14.example.xxx" -> "thrust.example.xxx.filecheck")
deps/thrust/examples/CMakeLists.txt:add_subdirectory(cuda)
deps/thrust/examples/cmake/CMakeLists.txt:if ("NVCXX" STREQUAL "${CMAKE_CUDA_COMPILER_ID}")
deps/thrust/examples/cmake/CMakeLists.txt:    -D "CMAKE_CUDA_COMPILER_ID=${CMAKE_CUDA_COMPILER_ID}"
deps/thrust/examples/cmake/CMakeLists.txt:    -D "CMAKE_CUDA_COMPILER_FORCED=${CMAKE_CUDA_COMPILER_FORCED}"
deps/thrust/examples/cmake/CMakeLists.txt:if (THRUST_CPP_FOUND AND THRUST_CUDA_FOUND)
deps/thrust/examples/cmake/CMakeLists.txt:      -D "CMAKE_CUDA_COMPILER=${CMAKE_CUDA_COMPILER}"
deps/thrust/examples/cmake/add_subdir/dummy.cpp:#elif THRUST_DEVICE_SYSTEM == THRUST_DEVICE_SYSTEM_CUDA
deps/thrust/examples/cmake/add_subdir/dummy.cpp:            << "CUDA\n";
deps/thrust/examples/cmake/add_subdir/CMakeLists.txt:# NVIDIA/thrust/cmake/README.md for more details on thrust_create_target.
deps/thrust/examples/cmake/add_subdir/CMakeLists.txt:# Silence warnings about empty CUDA_ARCHITECTURES properties on example targets:
deps/thrust/examples/cmake/add_subdir/CMakeLists.txt:# Options are: CPP, CUDA, TBB or OMP.
deps/thrust/examples/cmake/add_subdir/CMakeLists.txt:# Options are: CPP, CUDA, TBB or OMP.
deps/thrust/examples/cmake/add_subdir/CMakeLists.txt:set(THRUST_OPTIONAL_SYSTEMS CUDA)
deps/thrust/examples/cmake/add_subdir/CMakeLists.txt:# Create and use a Thrust target configured to use CUDA acceleration if CUDA
deps/thrust/examples/cmake/add_subdir/CMakeLists.txt:if (THRUST_CUDA_FOUND)
deps/thrust/examples/cmake/add_subdir/CMakeLists.txt:  enable_language(CUDA)
deps/thrust/examples/cmake/add_subdir/CMakeLists.txt:  thrust_create_target(ThrustCUDA HOST CPP DEVICE CUDA)
deps/thrust/examples/cmake/add_subdir/CMakeLists.txt:  add_executable(ExecWithCUDA dummy.cu)
deps/thrust/examples/cmake/add_subdir/CMakeLists.txt:  target_link_libraries(ExecWithCUDA ThrustCUDA)
deps/thrust/examples/cmake/add_subdir/CMakeLists.txt:assert_boolean(THRUST_CUDA_FOUND TRUE)
deps/thrust/examples/cmake/add_subdir/CMakeLists.txt:assert_target(ThrustCUDA)
deps/thrust/examples/cmake/add_subdir/CMakeLists.txt:assert_target(ExecWithCUDA)
deps/thrust/examples/cmake/add_subdir/CMakeLists.txt:thrust_debug_target(ThrustCUDA "")
deps/thrust/examples/cmake/add_subdir/CMakeLists.txt:thrust_debug_target(ExecWithCUDA "")
deps/thrust/examples/cpp_integration/host.cpp:    // interface to CUDA code
deps/thrust/examples/cpp_integration/README:  $ g++  -O2 -c host.cpp   -I/usr/local/cuda/include/
deps/thrust/examples/cpp_integration/README:  $ g++  -O2 -c host.cpp   -I/usr/local/cuda/include/
deps/thrust/examples/cpp_integration/README:  $ g++ -o tester device.o host.o -L/usr/local/cuda/lib64 -lcudart
deps/thrust/examples/cuda/global_device_vector.cu:// is freed, as the CUDA runtime cannot be used during program termination.
deps/thrust/examples/cuda/global_device_vector.cu:// deallocation failures that occur because the CUDA runtime is shut down.
deps/thrust/examples/cuda/global_device_vector.cu:extern "C" cudaError_t cudaFreeIgnoreShutdown(void* ptr) {
deps/thrust/examples/cuda/global_device_vector.cu:  cudaError_t const err = cudaFree(ptr);
deps/thrust/examples/cuda/global_device_vector.cu:  if (cudaSuccess == err || cudaErrorCudartUnloading == err)
deps/thrust/examples/cuda/global_device_vector.cu:    return cudaSuccess;
deps/thrust/examples/cuda/global_device_vector.cu:typedef thrust::system::cuda::detail::cuda_memory_resource<
deps/thrust/examples/cuda/global_device_vector.cu:  cudaMalloc, 
deps/thrust/examples/cuda/global_device_vector.cu:  cudaFreeIgnoreShutdown,
deps/thrust/examples/cuda/global_device_vector.cu:  thrust::cuda::pointer<void>
deps/thrust/examples/cuda/custom_temporary_allocation.cu:#include <thrust/system/cuda/vector.h>
deps/thrust/examples/cuda/custom_temporary_allocation.cu:#include <thrust/system/cuda/execution_policy.h>
deps/thrust/examples/cuda/custom_temporary_allocation.cu:// instead of resorting to the more expensive thrust::cuda::malloc.
deps/thrust/examples/cuda/custom_temporary_allocation.cu:// A simple allocator for caching cudaMalloc allocations.
deps/thrust/examples/cuda/custom_temporary_allocation.cu:      // `thrust::cuda::malloc`.
deps/thrust/examples/cuda/custom_temporary_allocation.cu:        // Allocate memory and convert the resulting `thrust::cuda::pointer` to
deps/thrust/examples/cuda/custom_temporary_allocation.cu:        result = thrust::cuda::malloc<char>(num_bytes).get();
deps/thrust/examples/cuda/custom_temporary_allocation.cu:      // Transform the pointer to cuda::pointer before calling cuda::free.
deps/thrust/examples/cuda/custom_temporary_allocation.cu:      thrust::cuda::free(thrust::cuda::pointer<char>(i->second));
deps/thrust/examples/cuda/custom_temporary_allocation.cu:      // Transform the pointer to cuda::pointer before calling cuda::free.
deps/thrust/examples/cuda/custom_temporary_allocation.cu:      thrust::cuda::free(thrust::cuda::pointer<char>(i->first));
deps/thrust/examples/cuda/custom_temporary_allocation.cu:  thrust::cuda::vector<int> d_input = h_input;
deps/thrust/examples/cuda/custom_temporary_allocation.cu:  thrust::cuda::vector<int> d_result(num_elements);
deps/thrust/examples/cuda/custom_temporary_allocation.cu:    // Pass alloc through cuda::par as the first parameter to sort
deps/thrust/examples/cuda/custom_temporary_allocation.cu:    thrust::sort(thrust::cuda::par(alloc), d_result.begin(), d_result.end());
deps/thrust/examples/cuda/CMakeLists.txt:  if (NOT config_device STREQUAL "CUDA")
deps/thrust/examples/cuda/CMakeLists.txt:    string(PREPEND example_name "cuda.")
deps/thrust/examples/cuda/unwrap_pointer.cu:#include <cuda.h>
deps/thrust/examples/cuda/unwrap_pointer.cu:    // use raw_ptr in CUDA API functions
deps/thrust/examples/cuda/unwrap_pointer.cu:    cudaMemset(raw_ptr, 0, N * sizeof(int));
deps/thrust/examples/cuda/async_reduce.cu:#include <thrust/system/cuda/execution_policy.h>
deps/thrust/examples/cuda/async_reduce.cu:// is stored to a pointer to CUDA global memory. The calling thread waits for the result of the reduction to 
deps/thrust/examples/cuda/async_reduce.cu:// be ready by synchronizing with the CUDA stream on which the __global__ function is launched.
deps/thrust/examples/cuda/async_reduce.cu:  *result = thrust::reduce(thrust::cuda::par, first, last, init, binary_op);
deps/thrust/examples/cuda/async_reduce.cu:  // method 1: call thrust::reduce from an asynchronous CUDA kernel launch
deps/thrust/examples/cuda/async_reduce.cu:  // create a CUDA stream 
deps/thrust/examples/cuda/async_reduce.cu:  cudaStream_t s;
deps/thrust/examples/cuda/async_reduce.cu:  cudaStreamCreate(&s);
deps/thrust/examples/cuda/async_reduce.cu:  // launch a CUDA kernel with only 1 thread on our stream
deps/thrust/examples/cuda/async_reduce.cu:  cudaStreamSynchronize(s);
deps/thrust/examples/cuda/async_reduce.cu:  cudaStreamDestroy(s);
deps/thrust/examples/cuda/wrap_pointer.cu:#include <cuda.h>
deps/thrust/examples/cuda/wrap_pointer.cu:    cudaMalloc((void **) &raw_ptr, N * sizeof(int));
deps/thrust/examples/cuda/wrap_pointer.cu:    cudaFree(raw_ptr);
deps/thrust/CONTRIBUTING.md:be cloned recursively to setup the CUB submodule (required for `CUDA`
deps/thrust/CONTRIBUTING.md:git clone --recursive https://github.com/NVIDIA/thrust.git
deps/thrust/CONTRIBUTING.md:2. Go to [the Thrust Github page](https://github.com/NVIDIA/thrust)
deps/thrust/CONTRIBUTING.md:[the CUB Github page](https://github.com/NVIDIA/cub) and repeat this process.
deps/thrust/CONTRIBUTING.md:[here](https://github.com/NVIDIA/cub/blob/main/CONTRIBUTING.md#cmake-options).
deps/thrust/CONTRIBUTING.md:  - Internal nvidia and gitlab bugs should use `nvidia` or `gitlab` in place of
deps/thrust/CONTRIBUTING.md:the full syntax, e.g. NVIDIA/cub#4 for issue 4 in the NVIDIA/cub repo.
deps/thrust/CONTRIBUTING.md:The Thrust team will review your patch, test it on NVIDIA's internal CI, and
deps/thrust/CONTRIBUTING.md:to `main` with NVIDIA's internal perforce repository.
deps/thrust/CONTRIBUTING.md:1. [CUDA Specific CMake Options](#cuda-specific-cmake-options) Options that
deps/thrust/CONTRIBUTING.md:   control CUDA compilation. Only available when one or more configurations
deps/thrust/CONTRIBUTING.md:   targets the CUDA system.
deps/thrust/CONTRIBUTING.md:- `THRUST_DEVICE_SYSTEM={CUDA, TBB, OMP, CPP}`
deps/thrust/CONTRIBUTING.md:  - Selects the device system. Default: `CUDA`
deps/thrust/CONTRIBUTING.md:  - Possible values of `XXXX` are `{CPP, CUDA, TBB, OMP}`
deps/thrust/CONTRIBUTING.md:  - By default, only `CPP` and `CUDA` are enabled.
deps/thrust/CONTRIBUTING.md:| CPP/CUDA | `F L M S` | Essential  | Expensive | Validates CUDA against CPP   |
deps/thrust/CONTRIBUTING.md:| TBB/CUDA | `F L    ` | Important  | Expensive | Validates TBB/CUDA interop   |
deps/thrust/CONTRIBUTING.md:| OMP/CUDA | `F L    ` | Important  | Expensive | Validates OMP/CUDA interop   |
deps/thrust/CONTRIBUTING.md:## CUDA Specific CMake Options
deps/thrust/CONTRIBUTING.md:  - Controls the targeted CUDA architecture(s)
deps/thrust/CONTRIBUTING.md:  - Multiple options may be selected when using NVCC as the CUDA compiler.
deps/thrust/CONTRIBUTING.md:  - If enabled, CUDA objects will target the most recent virtual architecture
deps/thrust/CONTRIBUTING.md:   * In the NVIDIA HPC SDK.
deps/thrust/CONTRIBUTING.md:   * In the CUDA Toolkit.
deps/thrust/CONTRIBUTING.md:As Thrust is developed both on GitHub and internally at NVIDIA, there are three main places where code lives:
deps/thrust/CONTRIBUTING.md:   * The Source of Truth, the [public Thrust repository](https://github.com/NVIDIA/thrust), referred to as
deps/thrust/CONTRIBUTING.md:Thrust has its own versioning system for releases, independent of the versioning scheme of the NVIDIA
deps/thrust/CONTRIBUTING.md:HPC SDK or the CUDA Toolkit.
deps/thrust/CONTRIBUTING.md:  * `github/nvhpc-X.Y`: the tag that directly corresponds to what has been shipped in the NVIDIA HPC SDK release X.Y.
deps/thrust/CONTRIBUTING.md:  * `github/cuda-X.Y`: the tag that directly corresponds to what has been shipped in the CUDA Toolkit release X.Y.
deps/thrust/CONTRIBUTING.md:  * `github/bug/<bug-system>/<bug-description>-<bug-id>`: bug fix branch, where `bug-system` is `github` or `nvidia`.
deps/thrust/CONTRIBUTING.md:  * `perforce/private`: mirrored `github/main`, plus files necessary for internal NVIDIA testing systems.
deps/thrust/CONTRIBUTING.md:[Compiler Explorer](https://www.godbolt.org/) (CE) as libraries for the CUDA
deps/thrust/CONTRIBUTING.md:      repo: NVIDIA/thrust
deps/thrust/CONTRIBUTING.md:### cuda.amazon.properties
deps/thrust/CONTRIBUTING.md:- File: etc/config/cuda.amazon.properties
deps/thrust/CONTRIBUTING.md:libs.thrustcub.description=CUDA collective and parallel algorithms
deps/thrust/CONTRIBUTING.md:libs.thrustcub.url=http://www.github.com/NVIDIA/thrust
README.md:# space_gpu
README.md:`SPACE` is a GPU centric C++ software for compaction experiments. It consists of data generators and a flexible experiment framework. Addtionally, scripts to visualize experiments are provided. For detailed information about compiling and running `SPACE`, see 
README.md:[overview.pdf](https://github.com/yogi-tud/space_gpu/blob/main/overview.pdf).
README.md:./gpu_compressstore2 0 0.25 1024 0 3 A100 0
README.md:[contribute](https://github.com/yogi-tud/space_gpu/blob/main/CONTRIBUTE.md)
README.md:JOSS Paper about SPACE_GPU:
CONTRIBUTE.md:In Case you want to add a novel gpu compaction algorithm and compare it both to SPACE and Nvidia CUB several steps need to be done.
CONTRIBUTE.md:Measurements should be done with CUDA events.
CONTRIBUTE.md:d_input is the input data array in gpu memory.
CONTRIBUTE.md:d_mask is the bitmask used for selection in gpu memory.
CMakeLists.txt:# convenient cuda clang support was added in 3.19
CMakeLists.txt:    set(CMAKE_CUDA_STANDARD 17)
CMakeLists.txt:    set(CMAKE_CUDA_STANDARD_REQUIRED ON)
CMakeLists.txt:project(gpu_compressstore2 LANGUAGES CUDA CXX)
CMakeLists.txt:    src/cuda_time.cuh
CMakeLists.txt:    src/cuda_try.cuh
CMakeLists.txt:add_executable(gpu_compressstore2 ${SOURCES})
CMakeLists.txt:    target_compile_definitions(gpu_compressstore2 PUBLIC AVXPOWER)
CMakeLists.txt:    set(CMAKE_CUDA_FLAGS "-Xcompiler -march=native")
CMakeLists.txt:#PROPERTY CUDA_ARCHITECTURES 75 61 X)
CMakeLists.txt:set_property(TARGET gpu_compressstore2 PROPERTY CUDA_ARCHITECTURES 75 61)
CMakeLists.txt:target_include_directories(gpu_compressstore2 PRIVATE ${INCLUDES})
CMakeLists.txt:target_include_directories(gpu_compressstore2 BEFORE PRIVATE ${PREFIX_INCLUDES})
CMakeLists.txt:#add_dependencies(gpu_compressstore2 cub_fix)
run_small.py:    #cmd = './build/gpu_compressstore2 '+run
run_small.py:    cmd = './gpu_compressstore2 ' + run
build.sh:cmake --build . --target gpu_compressstore2 -- -j 12
run_exp2_only.py:    #cmd = './build/gpu_compressstore2 '+run
run_exp2_only.py:    cmd = './gpu_compressstore2 ' + run
paper.md:title: 'Accelerating Parallel Operation for Compacting Selected Elements on GPUs'
paper.md:  - GPU
paper.md:parallel capabilities of GPUs to speed up the compacting operation is of great interest. We introduce smart partitioning for GPU compaction (`SPACE` ) as a set of different optimization approaches for GPUs. A detailed guide about setting up and using the software is found in @overview. @space is a preprint of the published Euro-Par 2022 paper, in which `SPACE` is described in great detail.
paper.md:`SPACE` is GPU-centric C++ software for compaction experiments. It consists of different data generators and a flexible experiment framework.
paper.md:Eight different `SPACE` variants can be compared against the NVIDIA-supplied CUB library for GPU compaction. Data type, percentage of selected data, and data distributions are modifiable as execution parameters for the generated C++ binary. Different Python runscripts for performing sets of experiments and reproducing the experiments from our paper (@space) are provided. The output of experiments is written in csv files. For visualizing the results, Python scripts based on Matplotlib are also provided. 
paper.md:`SPACE` was designed to allow researchers to evaluate compaction algorithms against a solid baseline across a variety of input data. It can be modified by adding additional compaction algorithms. It outperforms the current state-of-the-art algorithm [@cub]. Research about compaction has been performed by @bakunas2017efficient, who classify compaction on GPU into the two categories: "prefix sum based" and "atomic based". `SPACE` is a prefix sum based approach.
src/cuda_time.cuh:#ifndef CUDA_TIME_CUH
src/cuda_time.cuh:#define CUDA_TIME_CUH
src/cuda_time.cuh:#include "cuda_try.cuh"
src/cuda_time.cuh:// macro for timing gpu operations
src/cuda_time.cuh://#define DISABLE_CUDA_TIME
src/cuda_time.cuh:#define CUDA_TIME_FORCE_ENABLED(ce_start, ce_stop, stream, time, ...)                                                                                \
src/cuda_time.cuh:        CUDA_TRY(cudaEventRecord((ce_start)));                                                                                                       \
src/cuda_time.cuh:        CUDA_TRY(cudaEventRecord((ce_stop)));                                                                                                        \
src/cuda_time.cuh:        CUDA_TRY(cudaEventSynchronize((ce_stop)));                                                                                                   \
src/cuda_time.cuh:        CUDA_TRY(cudaEventElapsedTime((time), (ce_start), (ce_stop)));                                                                               \
src/cuda_time.cuh:#ifdef DISABLE_CUDA_TIME
src/cuda_time.cuh:#define CUDA_TIME(ce_start, ce_stop, stream, time, ...)                                                                                              \
src/cuda_time.cuh:        CUDA_TRY(cudaStreamSynchronize((stream)));                                                                                                   \
src/cuda_time.cuh:#define CUDA_TIME(ce_start, ce_stop, stream, time, ...) CUDA_TIME_FORCE_ENABLED(ce_start, ce_stop, stream, time, __VA_ARGS__)
src/cub_wraps.cuh:#include "cuda_time.cuh"
src/cub_wraps.cuh:#include "cuda_try.cuh"
src/cub_wraps.cuh:float launch_cub_pss(cudaStream_t stream, cudaEvent_t ce_start, cudaEvent_t ce_stop, uint32_t* d_pss, uint32_t* d_pss_total, uint32_t chunk_count)
src/cub_wraps.cuh:    CUDA_TRY(cudaMalloc(&d_pss_tmp, chunk_count * sizeof(uint32_t)));
src/cub_wraps.cuh:    CUDA_TRY(cub::DeviceScan::ExclusiveSum(d_temp_storage, temp_storage_bytes, d_pss, d_pss_tmp, chunk_count));
src/cub_wraps.cuh:    CUDA_TRY(cudaMalloc(&d_temp_storage, temp_storage_bytes));
src/cub_wraps.cuh:        CUDA_TIME(
src/cub_wraps.cuh:            CUDA_TRY(cub::DeviceScan::ExclusiveSum(d_temp_storage, temp_storage_bytes, d_pss, d_pss_tmp, chunk_count, 0)));
src/cub_wraps.cuh:        CUDA_TRY(cub::DeviceScan::ExclusiveSum(d_temp_storage, temp_storage_bytes, d_pss, d_pss_tmp, chunk_count, stream));
src/cub_wraps.cuh:    CUDA_TRY(cudaFree(d_temp_storage));
src/cub_wraps.cuh:    CUDA_TRY(cudaMemcpyAsync(d_pss, d_pss_tmp, chunk_count * sizeof(uint32_t), cudaMemcpyDeviceToDevice, stream));
src/cub_wraps.cuh:    CUDA_TRY(cudaFree(d_pss_tmp));
src/cub_wraps.cuh:    cudaEvent_t ce_start, cudaEvent_t ce_stop, T* d_input, T* d_output, uint8_t* d_mask, uint32_t* d_selected_out, uint32_t element_count)
src/cub_wraps.cuh:    CUDA_TRY(cudaMalloc(&d_temp_storage, temp_storage_bytes));
src/cub_wraps.cuh:    CUDA_TIME(
src/cub_wraps.cuh:    CUDA_TRY(cudaFree(d_temp_storage));
src/cub_wraps.cuh:    cudaEvent_t ce_start, cudaEvent_t ce_stop, T* d_input, T* d_output, uint8_t* h_mask, uint32_t* d_selected_out, uint32_t element_count)
src/cub_wraps.cuh:    CUDA_TRY(cudaMalloc(&d_bytemask, sizeof(uint8_t) * element_count));
src/cub_wraps.cuh:    // copy bytemask to gpu
src/cub_wraps.cuh:    CUDA_TRY(cudaMemcpy(d_bytemask, h_bytemask, sizeof(uint8_t) * element_count, cudaMemcpyHostToDevice));
src/cub_wraps.cuh:    CUDA_TRY(cudaMalloc(&d_temp_storage, temp_storage_bytes));
src/cub_wraps.cuh:    CUDA_TIME(
src/cub_wraps.cuh:    // free bytemask from gpu
src/cub_wraps.cuh:    CUDA_TRY(cudaFree(d_bytemask));
src/cub_wraps.cuh:    CUDA_TRY(cudaFree(d_temp_storage));
src/main.cu:#define DISABLE_CUDA_TIME
src/main.cu:#include "cuda_time.cuh"
src/main.cu:#include "cuda_try.cuh"
src/main.cu:    CUDA_TRY(cudaSetDevice(0));
src/main.cu:    //set up GPU pointers
src/main.cu:    T * d_input = vector_to_gpu(col);
src/main.cu:    T * d_output = alloc_gpu<T>(col.size() + 1);
src/main.cu:    CUDA_TRY(cudaMalloc(&d_mask, mask_bytes));
src/main.cu:    CUDA_TRY(cudaMemcpy(d_mask, &pred[0], mask_bytes, cudaMemcpyHostToDevice));
src/main.cu:    //uint8_t* d_mask = vector_to_gpu(pred);
src/main.cu:    T* d_validation = vector_to_gpu(validation);
src/main.cu:    CUDA_TRY(cudaFree(d_mask));
src/main.cu:    CUDA_TRY(cudaFree(d_input));
src/main.cu:    CUDA_TRY(cudaFree(d_output));
src/cub_minimum_example.cu:   int  *d_out= alloc_gpu<int>(num_items);                 // e.g., [ ,  ,  ,  ,  ,  ,  ,  ]
src/cub_minimum_example.cu:   int  *d_num_selected_out=alloc_gpu<int>(1);    // e.g., [ ]
src/cub_minimum_example.cu:   int * d_in = vector_to_gpu<int>(d_in_v);
src/cub_minimum_example.cu:   cudaEvent_t start, stop;
src/cub_minimum_example.cu:   CUDA_TRY(cudaEventCreate(&start));
src/cub_minimum_example.cu:   CUDA_TRY(cudaEventCreate(&stop));
src/cub_minimum_example.cu:   CUDA_TRY(cudaMalloc(&d_flags, num_items));
src/cub_minimum_example.cu:   CUDA_TRY(cudaMemcpy(d_flags, &d_flags_v[0], num_items, cudaMemcpyHostToDevice));
src/cub_minimum_example.cu:   CUDA_TRY(cudaMalloc(&d_temp_storage, temp_storage_bytes));
src/cub_minimum_example.cu:   CUDA_TRY(cudaEventRecord(start));
src/cub_minimum_example.cu:   CUDA_TRY(cub::DeviceSelect::Flagged(d_temp_storage, temp_storage_bytes, d_in, d_flags, d_out, d_num_selected_out, num_items));
src/cub_minimum_example.cu:   CUDA_TRY(cudaEventRecord(stop));
src/cub_minimum_example.cu:   cudaEventSynchronize(stop);
src/cub_minimum_example.cu:   cudaEventElapsedTime(&runtime, start, stop);
src/cub_minimum_example.cu:   gpu_buffer_print(d_num_selected_out,0,1);
src/cub_minimum_example.cu:   gpu_buffer_print(d_in,0,50);
src/cub_minimum_example.cu:   gpu_buffer_print(d_flags,0,8);
src/cub_minimum_example.cu:   gpu_buffer_print(d_out,0,50);
src/utils.cuh:template <typename T> void gpu_buffer_print(T* d_buffer, uint32_t offset, uint32_t length)
src/utils.cuh:    CUDA_TRY(cudaMemcpy(h_buffer, d_buffer + offset, length * sizeof(T), cudaMemcpyDeviceToHost));
src/utils.cuh:template <typename T> T* vector_to_gpu(const std::vector<T>& vec)
src/utils.cuh:    CUDA_TRY(cudaMalloc(&buff, size));
src/utils.cuh:    CUDA_TRY(cudaMemcpy(buff, &vec[0], size, cudaMemcpyHostToDevice));
src/utils.cuh:template <typename T> std::vector<T> gpu_to_vector(T* buff, size_t length)
src/utils.cuh:    CUDA_TRY(cudaMemcpy(&vec[0], buff, length * sizeof(T), cudaMemcpyDeviceToHost));
src/utils.cuh:template <typename T> T gpu_to_val(T* d_val)
src/utils.cuh:    CUDA_TRY(cudaMemcpy(&val, d_val, sizeof(T), cudaMemcpyDeviceToHost));
src/utils.cuh:template <typename T> void val_to_gpu(T* d_val, typename dont_deduce_t<T>::type val)
src/utils.cuh:    CUDA_TRY(cudaMemcpy(d_val, &val, sizeof(T), cudaMemcpyHostToDevice));
src/utils.cuh:template <typename T> T* alloc_gpu(size_t length)
src/utils.cuh:    CUDA_TRY(cudaMalloc(&buff, length * sizeof(T)));
src/kernels/kernel_pattern.cuh:#include "cuda_time.cuh"
src/kernels/kernel_pattern.cuh:#include "cuda_try.cuh"
src/kernels/kernel_pattern.cuh:#define CUDA_WARP_SIZE 32
src/kernels/kernel_pattern.cuh:    uint32_t warp_offset = threadIdx.x % CUDA_WARP_SIZE;
src/kernels/kernel_pattern.cuh:    uint32_t warp_index = threadIdx.x / CUDA_WARP_SIZE;
src/kernels/kernel_pattern.cuh:    uint64_t chunk_idx = warp_index + blockIdx.x * (blockDim.x / CUDA_WARP_SIZE);
src/kernels/kernel_pattern.cuh:    uint64_t chunk_stride = gridDim.x * (blockDim.x / CUDA_WARP_SIZE);
src/kernels/kernel_pattern.cuh:            in_chunk_step += CUDA_WARP_SIZE;
src/kernels/kernel_pattern.cuh:    cudaEvent_t ce_start,
src/kernels/kernel_pattern.cuh:    cudaEvent_t ce_stop,
src/kernels/kernel_pattern.cuh:    CUDA_TRY(cudaMalloc(&d_thread_offset_initials, sizeof(uint32_t) * 32));
src/kernels/kernel_pattern.cuh:    CUDA_TRY(cudaMalloc(&d_readin_offset_increments, sizeof(uint32_t) * 32));
src/kernels/kernel_pattern.cuh:    CUDA_TRY(cudaMemcpy(d_thread_offset_initials, thread_offset_initials, sizeof(uint32_t) * 32, cudaMemcpyHostToDevice));
src/kernels/kernel_pattern.cuh:    CUDA_TRY(cudaMemcpy(d_readin_offset_increments, readin_offset_increments, sizeof(uint32_t) * 32, cudaMemcpyHostToDevice));
src/kernels/kernel_pattern.cuh:    CUDA_TIME_FORCE_ENABLED(
src/kernels/kernel_pattern.cuh:    CUDA_TRY(cudaFree(d_readin_offset_increments));
src/kernels/kernel_pattern.cuh:    CUDA_TRY(cudaFree(d_thread_offset_initials));
src/kernels/kernel_streaming_add.cuh:#include "cuda_time.cuh"
src/kernels/kernel_streaming_add.cuh:void launch_streaming_add_pss_totals(cudaStream_t stream, uint32_t* d_previous_pss_total, uint32_t* d_target_pss_total)
src/kernels/data_generator.cuh:#include "cuda_time.cuh"
src/kernels/data_generator.cuh:#include "cuda_try.cuh"
src/kernels/data_generator.cuh:#if defined(__CUDACC__)
src/kernels/kernel_3pass.cuh:#include "cuda_time.cuh"
src/kernels/kernel_3pass.cuh:#define CUDA_WARP_SIZE 32
src/kernels/kernel_3pass.cuh:    cudaEvent_t ce_start,
src/kernels/kernel_3pass.cuh:    cudaEvent_t ce_stop,
src/kernels/kernel_3pass.cuh:    CUDA_TIME(
src/kernels/kernel_3pass.cuh:    CUDA_TIME(
src/kernels/kernel_3pass.cuh:void launch_3pass_pssskip(cudaStream_t stream, uint32_t* d_pss, uint32_t* d_pss_total, uint32_t chunk_count)
src/kernels/kernel_3pass.cuh:    cudaEvent_t ce_start,
src/kernels/kernel_3pass.cuh:    cudaEvent_t ce_stop,
src/kernels/kernel_3pass.cuh:            CUDA_TIME(
src/kernels/kernel_3pass.cuh:        CUDA_TIME(
src/kernels/kernel_3pass.cuh:            CUDA_TIME(
src/kernels/kernel_3pass.cuh:        CUDA_TIME(
src/kernels/kernel_3pass.cuh:            CUDA_TIME(
src/kernels/kernel_3pass.cuh:        CUDA_TIME(
src/kernels/kernel_3pass.cuh:    cudaEvent_t ce_start,
src/kernels/kernel_3pass.cuh:    cudaEvent_t ce_stop,
src/kernels/kernel_3pass.cuh:        CUDA_TIME(
src/kernels/kernel_3pass.cuh:        CUDA_TIME(
src/kernels/kernel_3pass.cuh:    constexpr uint32_t WARPS_PER_BLOCK = BLOCK_DIM / CUDA_WARP_SIZE;
src/kernels/kernel_3pass.cuh:    while (grid_stride % (CUDA_WARP_SIZE * BLOCK_DIM) != 0 || grid_stride * gridDim.x < element_count ||
src/kernels/kernel_3pass.cuh:    uint32_t warp_offset = threadIdx.x % CUDA_WARP_SIZE;
src/kernels/kernel_3pass.cuh:    uint32_t warp_index = threadIdx.x / CUDA_WARP_SIZE;
src/kernels/kernel_3pass.cuh:        for (int i = 0; i < CUDA_WARP_SIZE; i++) {
src/kernels/kernel_3pass.cuh:            uint32_t out_idx_me = __popc(s >> (CUDA_WARP_SIZE - warp_offset));
src/kernels/kernel_3pass.cuh:            bool v = (s >> ((CUDA_WARP_SIZE - 1) - warp_offset)) & 0b1;
src/kernels/kernel_3pass.cuh:            if (warp_offset == (CUDA_WARP_SIZE - 1)) {
src/kernels/kernel_3pass.cuh:            input_index += CUDA_WARP_SIZE;
src/kernels/kernel_3pass.cuh:    constexpr uint32_t WARPS_PER_BLOCK = BLOCK_DIM / CUDA_WARP_SIZE;
src/kernels/kernel_3pass.cuh:    while (grid_stride % (CUDA_WARP_SIZE * BLOCK_DIM) != 0 || grid_stride * gridDim.x < element_count ||
src/kernels/kernel_3pass.cuh:    uint32_t warp_offset = threadIdx.x % CUDA_WARP_SIZE;
src/kernels/kernel_3pass.cuh:    uint32_t warp_index = threadIdx.x / CUDA_WARP_SIZE;
src/kernels/kernel_3pass.cuh:        for (int i = 0; i < CUDA_WARP_SIZE; i++) {
src/kernels/kernel_3pass.cuh:            uint32_t out_idx_me = __popc(s >> (CUDA_WARP_SIZE - warp_offset));
src/kernels/kernel_3pass.cuh:            bool v = (s >> ((CUDA_WARP_SIZE - 1) - warp_offset)) & 0b1;
src/kernels/kernel_3pass.cuh:            if (warp_offset == CUDA_WARP_SIZE - 1) {
src/kernels/kernel_3pass.cuh:            if (elements_aquired + out_idx_warp >= CUDA_WARP_SIZE) {
src/kernels/kernel_3pass.cuh:                if (v && out_idx_me_full < CUDA_WARP_SIZE) {
src/kernels/kernel_3pass.cuh:                if (v && out_idx_me_full >= CUDA_WARP_SIZE) {
src/kernels/kernel_3pass.cuh:                    out_indices[out_idices_idx - CUDA_WARP_SIZE] = input_index;
src/kernels/kernel_3pass.cuh:                elements_aquired -= CUDA_WARP_SIZE;
src/kernels/kernel_3pass.cuh:                warp_output_index += CUDA_WARP_SIZE;
src/kernels/kernel_3pass.cuh:            input_index += CUDA_WARP_SIZE;
src/kernels/kernel_3pass.cuh:    cudaStream_t stream,
src/kernels/kernel_3pass.cuh:    cudaEvent_t ce_start,
src/kernels/kernel_3pass.cuh:    cudaEvent_t ce_stop,
src/kernels/kernel_3pass.cuh:        CUDA_TIME(
src/kernels/kernel_3pass.cuh:        CUDA_TIME(
src/kernels/kernel_3pass.cuh:    cudaEvent_t ce_start,
src/kernels/kernel_3pass.cuh:    cudaEvent_t ce_stop,
src/kernels/kernel_3pass.cuh:            CUDA_TIME(
src/kernels/kernel_3pass.cuh:            CUDA_TIME(
src/kernels/kernel_3pass.cuh:            CUDA_TIME(
src/kernels/kernel_3pass.cuh:            CUDA_TIME(
src/cuda_try.cuh:#ifndef CUDA_TRY_CUH
src/cuda_try.cuh:#define CUDA_TRY_CUH
src/cuda_try.cuh:// macro for cuda return value checking
src/cuda_try.cuh:#define CUDA_TRY(expr)                                                                                                                               \
src/cuda_try.cuh:        cudaError_t _err_ = (expr);                                                                                                                  \
src/cuda_try.cuh:        if (_err_ != cudaSuccess) {                                                                                                                  \
src/cuda_try.cuh:            report_cuda_error(_err_, #expr, __FILE__, __LINE__, true);                                                                               \
src/cuda_try.cuh:static void report_cuda_error(cudaError_t err, const char* cmd, const char* file, int line, bool die)
src/cuda_try.cuh:    printf("CUDA Error at %s:%i%s%s\n%s\n", file, line, cmd ? ": " : "", cmd ? cmd : "", cudaGetErrorString(err));
src/benchmarks.cuh:#include "cuda_try.cuh"
src/benchmarks.cuh:    cudaEvent_t dummy_event_1;
src/benchmarks.cuh:    cudaEvent_t dummy_event_2;
src/benchmarks.cuh:    cudaEvent_t start;
src/benchmarks.cuh:    cudaEvent_t stop;
src/benchmarks.cuh:    cudaStream_t* streams;
src/benchmarks.cuh:    cudaEvent_t* stream_events;
src/benchmarks.cuh:        CUDA_TRY(cub::DeviceScan::ExclusiveSum(null, temp_storage_bytes_pss, (T*)null, (T*)null, chunk_count_max));
src/benchmarks.cuh:        CUDA_TRY(cudaMalloc(&d_cub_intermediate, cub_intermediate_size));
src/benchmarks.cuh:        CUDA_TRY(cudaMalloc(&d_pss, intermediate_size_3pass));
src/benchmarks.cuh:        CUDA_TRY(cudaMalloc(&d_pss2, intermediate_size_3pass));
src/benchmarks.cuh:        CUDA_TRY(cudaMalloc(&d_popc, intermediate_size_3pass));
src/benchmarks.cuh:        CUDA_TRY(cudaMalloc(&d_out_count, sizeof(uint32_t) * (max_stream_count + 1)));
src/benchmarks.cuh:        CUDA_TRY(cudaEventCreate(&dummy_event_1));
src/benchmarks.cuh:        CUDA_TRY(cudaEventCreate(&dummy_event_2));
src/benchmarks.cuh:        CUDA_TRY(cudaEventCreate(&start));
src/benchmarks.cuh:        CUDA_TRY(cudaEventCreate(&stop));
src/benchmarks.cuh:        streams = (cudaStream_t*)malloc(max_stream_count * sizeof(cudaStream_t));
src/benchmarks.cuh:        stream_events = (cudaEvent_t*)malloc(max_stream_count * sizeof(cudaEvent_t));
src/benchmarks.cuh:            CUDA_TRY(cudaStreamCreate(&(streams[i])));
src/benchmarks.cuh:            CUDA_TRY(cudaEventCreate(&(stream_events[i])));
src/benchmarks.cuh:                uint8_t last_mask_byte = gpu_to_val(last_mask_byte_ptr);
src/benchmarks.cuh:                val_to_gpu(last_mask_byte_ptr, last_mask_byte);
src/benchmarks.cuh:        CUDA_TRY(cudaMemset(d_out_count, 0, (max_stream_count + 1) * sizeof(*d_out_count)));
src/benchmarks.cuh:        CUDA_TRY(cudaMemset(d_output, 0xFF, element_count * sizeof(T)));
src/benchmarks.cuh:        CUDA_TRY(cudaFree(d_pss));
src/benchmarks.cuh:        CUDA_TRY(cudaFree(d_pss2));
src/benchmarks.cuh:        CUDA_TRY(cudaFree(d_popc));
src/benchmarks.cuh:        CUDA_TRY(cudaFree(d_cub_intermediate));
src/benchmarks.cuh:        CUDA_TRY(cudaFree(d_out_count));
src/benchmarks.cuh:        CUDA_TRY(cudaEventDestroy(dummy_event_1));
src/benchmarks.cuh:        CUDA_TRY(cudaEventDestroy(dummy_event_2));
src/benchmarks.cuh:        CUDA_TRY(cudaEventDestroy(start));
src/benchmarks.cuh:        CUDA_TRY(cudaEventDestroy(stop));
src/benchmarks.cuh:            CUDA_TRY(cudaStreamDestroy(streams[i]));
src/benchmarks.cuh:            CUDA_TRY(cudaEventDestroy(stream_events[i]));
src/benchmarks.cuh:    CUDA_TIME_FORCE_ENABLED(id->start, id->stop, 0, &times.total, {
src/benchmarks.cuh:    CUDA_TIME_FORCE_ENABLED(id->start, id->stop, 0, &times.total, {
src/benchmarks.cuh:    CUDA_TIME_FORCE_ENABLED(id->start, id->stop, 0, &times.total, {
src/benchmarks.cuh:        CUDA_TRY(cudaMemset(id->d_out_count, 0x00, sizeof(uint32_t) * (stream_count + 1)));
src/benchmarks.cuh:            CUDA_TRY(cudaEventRecord(id->stream_events[i], id->streams[i]));
src/benchmarks.cuh:                CUDA_TRY(cudaStreamWaitEvent(id->streams[i], id->stream_events[i - 1]));
src/benchmarks.cuh:        CUDA_TRY(cudaDeviceSynchronize());
src/benchmarks.cuh:    CUDA_TIME_FORCE_ENABLED(id->start, id->stop, 0, &times.total, {
src/benchmarks.cuh:        CUDA_TRY(
src/benchmarks.cuh:    CUDA_TIME_FORCE_ENABLED(id->start, id->stop, 0, &times.total, {
src/benchmarks.cuh:        cudaMemcpy(id->d_pss, id->d_popc, id->chunk_count(chunk_length) * sizeof(uint32_t), cudaMemcpyDeviceToDevice);
src/benchmarks.cuh:    CUDA_TIME_FORCE_ENABLED(id->start, id->stop, 0, &times.total, {
src/benchmarks.cuh:        cudaMemcpy(id->d_pss, id->d_popc, id->chunk_count(chunk_length) * sizeof(uint32_t), cudaMemcpyDeviceToDevice);
src/benchmarks.cuh:    CUDA_TIME_FORCE_ENABLED(id->start, id->stop, 0, &times.total, {
src/benchmarks.cuh:        cudaMemcpy(id->d_pss, id->d_popc, id->chunk_count(chunk_length) * sizeof(uint32_t), cudaMemcpyDeviceToDevice);
src/benchmarks.cuh:        CUDA_TRY(
src/benchmarks.cuh:    CUDA_TIME_FORCE_ENABLED(id->start, id->stop, 0, &times.total, {
src/benchmarks.cuh:    // CUDA_TIME_FORCE_ENABLED(id->start, id->stop, 0, &time, {
src/benchmarks.cuh:    CUDA_TIME_FORCE_ENABLED(id->start, id->stop, 0, &times.total, {
src/benchmarks.cuh:        cudaMemcpy(id->d_pss, id->d_popc, id->chunk_count(chunk_length) * sizeof(uint32_t), cudaMemcpyDeviceToDevice);
src/benchmarks.cuh:        CUDA_TRY(
src/benchmarks.cuh:    val_to_gpu(id->d_failure_count, 0);
src/benchmarks.cuh:    auto fc = gpu_to_val(id->d_failure_count);

```
