# https://github.com/elizabethnewman/hessQuik

```console
docs/index.rst:A lightweight package for fast, GPU-accelerated computation of gradients and Hessians of functions constructed via composition.
docs/index.rst:Knowledge of second-order derivatives is paramount in many growing fields and can provide insight into the optimization problem solved to build a good model. Hessians are notoriously challenging to compute efficiently with AD and cumbersome to derive and debug analytically. Hence, many algorithms approximate Hessian information, resulting in suboptimal performance. To address these challenges, hessQuik computes Hessians analytically and efficiently with an implementation that is accelerated on GPUs.
README.md:A lightweight package for fast, GPU-accelerated computation of gradients and Hessians of functions constructed via composition.
README.md:Knowledge of second-order derivatives is paramount in many growing fields and can provide insight into the optimization problem solved to build a good model. Hessians are notoriously challenging to compute efficiently with AD and cumbersome to derive and debug analytically.  Hence, many algorithms approximate Hessian information, resulting in suboptimal performance.  To address these challenges, `hessQuik` computes Hessians analytically and efficiently with an implementation that is accelerated on GPUs.
paper/paper.bib:	abstract = {Google Colaboratory more commonly referred to as ``Google Colab'' or just simply ``Colab'' is a research project for prototyping machine learning models on powerful hardware options such as GPUs and TPUs. It provides a serverless Jupyter notebook environment for interactive development. Google Colab is free to use like other G Suite products.},
paper/paper.md:`hessQuik` is a lightweight software library for fast computation of second-order derivatives (Hessians) of composite functions (that is, functions formed via compositions) with respect to their inputs.  The core of `hessQuik` is the efficient computation of analytical Hessians with GPU acceleration. `hessQuik` is a PyTorch [@pytorch] package that is user-friendly and easily extendable.  The repository includes a variety of popular functions and layers, including residual layers and input convex layers, from which users can build complex models through composition.  `hessQuik` layers are designed for ease of composition - users only need to select the layers and the package provides a convenient wrapper to compose the functions properly.  Each layer provides two modes for derivative computation and the mode is automatically selected to maximize computational efficiency. `hessQuik` includes easy-access, [illustrative tutorials](https://colab.research.google.com/github/elizabethnewman/hessQuik/blob/main/hessQuik/examples/hessQuikPeaksHermiteInterpolation.ipynb) on Google Colaboratory [@googleColab], [reproducible experiments](https://colab.research.google.com/github/elizabethnewman/hessQuik/blob/main/hessQuik/examples/hessQuikTimingTest.ipynb), and unit tests to verify implementations. `hessQuik` enables users to obtain valuable second-order information for their models simply and efficiently.
paper/paper.md:Knowledge of second-order derivatives is paramount in many growing fields, such as physics-informed neural networks (PINNs) [@Raissi:2019hv], mean-field games [@Ruthotto9183], generative modeling [@ruthotto2021introduction], and adversarial learning [@papernot2016limitations].  In addition, second-order derivatives can provide insight into the optimization problem solved to build a good model [@olearyroseberry2020illposedness]. Hessians are notoriously challenging to compute efficiently with AD and cumbersome to derive and debug analytically.  Hence, many algorithms approximate Hessian information, resulting in sub-optimal performance.  To address these challenges, `hessQuik` computes Hessians analytically and efficiently with an implementation that is accelerated on GPUs.
paper/paper.md:![Illustration of Hessian computation of $\nabla_{\bfu_0}^ 2\bfu_{i+1}$ in forward mode. Note that for the first term, the gray three-dimensional array $\nabla_{\bfu_i} g_{i+1}(\bfu_i)$ is treated as a stack of matrices.  Then, the same Jacobian matrix $\nabla_{\bfu_0}\bfu_i$ is broadcast to each matrix in the stack, illustrated by the repeated cyan matrices. In the second term, the green matrix $\nabla_{\bfu_i}g_{i+1}(\bfu_i)$ is applied along the third dimension of the magenta three-dimensional array, $\nabla_{\bfu_0}\bfu_i$. Both of these operations can be parallelized and accelerated GPUs. \label{fig:hessianIllustration}](img/HessianIllustration.png){ width=100% }
paper/paper.md:For reproducibility, we compare the time to compute the Hessian using Google Colaboratory (Colab) Pro and provide the [notebook](https://colab.research.google.com/github/elizabethnewman/hessQuik/blob/main/hessQuik/examples/hessQuikTimingTest.ipynb) in the repository. For CPU runtimes,  Colab Pro uses an Intel(R) Xeon(R) CPU with 2.20GHz processor base speed. For GPU runtimes, Colab Pro uses a Tesla P100 with 16 GB of memory.  We note that Colab allocates resources based on availability, and hence exact quantitative reproducibility is not guaranteed.  However, we expect users to get qualitatively similar results when running on their own Colab instance or locally. 
paper/paper.md:In \autoref{fig:scalar} and \autoref{fig:vector}, we compare the performance of three approaches to compute Hessians of a neural network.  In our experiments, we see faster Hessian computations using `hessQuik` and noticeable acceleration on the GPU, especially for networks with larger input and output dimensions.  Specifically, \autoref{fig:scalar} shows that for a model with a scalar output, the timing using the `hessQuik` implementation scales better with the number of input features than either of the AD-based methods.  Additionally, \autoref{fig:vector} demonstrates that the `hessQuik` timings remain relatively constant as the number of output features changes whereas the `PytorchAD` timings significantly increase as the number of output features increases.  Note that we only compare to `PytorchAD ` for vector-valued outputs because `PytorchHessian` was noticeably slower for the scalar case.
paper/paper.md:![Average time over $10$ trials to evaluate and compute the Hessian with respect to the input features for one output feature ($n_{\ell}=1$). Solid lines represent timings on the CPU and dashed lines are timings on the GPU. The circle markers are the timings obtained using `hessQuik`. \label{fig:scalar}](img/hessQuik_timing_scalar.png){ width=80% } 
paper/paper.md:`hessQuik` is a simple, user-friendly software library for computing second-order derivatives of composite functions with respect to their inputs.  This PyTorch package includes many popular built-in layers, tutorial repositories, reproducible experiments, and unit testing for ease of use.  The implementation scales well in time with various input and output feature dimensions and performance is accelerated on GPUs, notably faster than automatic-differentiation-based second-order derivative computations.  We hope the accessibility and efficiency of this package will encourage researchers to use and contribute to `hessQuik` in the future.
hessQuik/layers/resnet_layer.py:                # TODO: compare timings for h_dfdx on CPU and GPU
hessQuik/examples/run_timing_test.py:device = 'cuda' if torch.cuda.is_available() else 'cpu'
hessQuik/examples/ex_peaks_hermite.py:device = torch.device(f'cuda:0' if torch.cuda.is_available() else 'cpu')
hessQuik/examples/ex_timing_test_hessQuik.py:device = 'cuda' if torch.cuda.is_available() else 'cpu'
hessQuik/utils/timing.py:        torch.cuda.empty_cache()
hessQuik/utils/timing.py:def timing_test_gpu(f: Union[hessQuik.networks.NN, torch.nn.Module], x: torch.Tensor,
hessQuik/utils/timing.py:    Each trial includes a ``torch.cuda.synchonize`` call.
hessQuik/utils/timing.py:        torch.cuda.synchronize()
hessQuik/utils/timing.py:        torch.cuda.empty_cache()
hessQuik/utils/timing.py:                total_time = timing_test_gpu(f, x, num_trials=num_trials, clear_memory=clear_memory)

```
