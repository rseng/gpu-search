# https://github.com/MannLabs/alphapept

```console
alphapept/performance.py:    "cuda", # Cuda is always multithreaded
alphapept/performance.py:from numba import cuda
alphapept/performance.py:    cuda.get_current_device()
alphapept/performance.py:    __GPU_AVAILABLE = True
alphapept/performance.py:    __GPU_AVAILABLE = False
alphapept/performance.py:except cuda.CudaSupportError:
alphapept/performance.py:    __GPU_AVAILABLE = False
alphapept/performance.py:    logging.info("Cuda device is not available")
alphapept/performance.py:        ModuleNotFoundError: When trying to use an unavailable GPU.
alphapept/performance.py:    if compilation_mode.startswith("cuda") and not __GPU_AVAILABLE:
alphapept/performance.py:        raise ModuleNotFoundError('Cuda functions are not available.')
alphapept/performance.py:if __GPU_AVAILABLE:
alphapept/performance.py:    COMPILATION_MODE = "cuda"
alphapept/performance.py:    Cuda functions are by default set to use `device=True`,
alphapept/performance.py:        **decorator_kwargs: Keyword arguments that will be passed to numba.jit or cuda.jit compilation decorators.
alphapept/performance.py:        elif current_compilation_mode.startswith("cuda"):
alphapept/performance.py:                compiled_function = cuda.jit(func, **decorator_kwargs)
alphapept/performance.py:                compiled_function = cuda.jit(func, **decorator_kwargs, device=True)
alphapept/performance.py:        **decorator_kwargs: Keyword arguments that will be passed to numba.jit or cuda.jit compilation decorators.
alphapept/performance.py:        def _parallel_cuda(compiled_function, iterable, *func_args):
alphapept/performance.py:            cuda_func_dict = {"cuda": cuda, "compiled_function": compiled_function}
alphapept/performance.py:            # Cuda functions cannot handle tuple unpacking but need a fixed number of arguments.
alphapept/performance.py:                cuda_string = (
alphapept/performance.py:                    f"@cuda.jit\n"
alphapept/performance.py:                    f"def cuda_func({func_string}):\n"
alphapept/performance.py:                    f"    index = arg0 + arg2 * cuda.grid(1)\n"
alphapept/performance.py:                exec(cuda_string, cuda_func_dict)
alphapept/performance.py:                cuda_func_dict["cuda_func"].forall(len(iterable), 1)(
alphapept/performance.py:                cuda_string = (
alphapept/performance.py:                    f"@cuda.jit\n"
alphapept/performance.py:                    f"def cuda_func({func_string}):\n"
alphapept/performance.py:                    f"    index = arg0[cuda.grid(1)]\n"
alphapept/performance.py:                exec(cuda_string, cuda_func_dict)
alphapept/performance.py:                cuda_func_dict["cuda_func"].forall(len(iterable), 1)(iterable, *func_args)
alphapept/performance.py:            if selected_compilation_mode == "cuda":
alphapept/performance.py:                _parallel_cuda(_compiled_function, iterable, *func_args)
alphapept/search.py:                # The code below looks weird but is necessary to be used with cuda
alphapept/search.py:    if alphapept.performance.COMPILATION_MODE == "cuda":
alphapept/feature_finding.py:    if alphapept.performance.COMPILATION_MODE == "cuda":
alphapept/feature_finding.py:    if alphapept.performance.COMPILATION_MODE == "cuda":
alphapept/feature_finding.py:    if alphapept.performance.COMPILATION_MODE == "cuda":
alphapept/feature_finding.py:    if alphapept.performance.COMPILATION_MODE == "cuda":
alphapept/contaminants.fasta:YVICTVENCCL
alphapept/contaminants.fasta:MPYNCCLPALSCRTSCSSRPCVPPSCHGCTLPGACNIPANVGNCNWFCEGSFNGNEKETM
alphapept/contaminants.fasta:EVDSHNSAPSSTMPYNCCLPAMSCRTSCSSRPCVPPSCHGCTLPGACNIPANVGNCNWFC
docs/search.json:    "text": "Connecting Centroids to Hills\n\n\n\n\n\n\nNote\n\n\n\nFeature finding relies heavily on the performance function decorator from the performance notebook: @alphapept.performance.performance_function. Part of this is that the functions will not have return values to be GPU compatible. Please check out this notebook for further information.\n\n\n\nConnecting centroids\nFeature finding starts with connecting centroids. For this we look at subsequent scans and compare peaks that are withing a defined mass tolerance (centroid_tol).\nImagine you have three scans with the following centroids:\n\nScan 0: 10, 20, 30\nScan 1: 10.2, 40.1\nScan 2: 40, 50, 60\n\nWhen comparing consecutive scans and defining the maximum delta mass to be 0.5 find the following connections: (Scan No, Centroid No) -> (Scan No, Centroid No). As we cannot easily store tuples in the matrix, we convert tuple containing the position of the connected centroid to an integer. * (0,0) -> (1,0) -> (3): 10 & 10.2 -> delta = 0.2 * (1,1) -> (2,0) -> (6): 40.1 & 40 -> delta = 0.1\nFinally, we store this in the results matrix:\n\\(\\begin{bmatrix} 3 & -1 & -1 \\\\ -1 & 6 & -1\\\\ -1 & -1 & -1 \\end{bmatrix}\\)\nThe coressponding scores matrix will look as follows:\n\\(\\begin{bmatrix} 0.2 & -1 & -1 \\\\ -1 & 0.1 & -1\\\\ -1 & -1 & -1 \\end{bmatrix}\\)\nThis allows us to not only easily store connections between centroids but also perform a quick lookup for the delta of an existing connection. Note that it also only stores the best connection for each centroid. To extract the connected centroids, we can use np.where(results >= 0). This implementation allows getting millions of connections within seconds.\nAs we are also allowing gaps, refering to that we can have connections between Scan 0 and Scan 2, we make the aforementioned matrix multdimensional, so that e.g. a first matrix stores the conncetions for no gap, the second matrix the connections with a gap of 1.\nThe functionality for this step is implemented in connect_centroids_unidirection and the wrapper find_centroid_connections.\n\nsource\n\n\nfind_centroid_connections\n\n find_centroid_connections (rowwise_peaks:numpy.ndarray,\n                            row_borders:numpy.ndarray,\n                            centroids:numpy.ndarray, max_gap:int,\n                            centroid_tol:float)\n\nWrapper function to call connect_centroids_unidirection\nArgs: rowwise_peaks (np.ndarray): Length of centroids with respect to the row borders. row_borders (np.ndarray): Row borders of the centroids array. centroids (np.ndarray): Array containing the centroids data. max_gap (int): Maximum gap when connecting centroids. centroid_tol (float): Centroid tolerance.\n\nsource\n\n\nconnect_centroids_unidirection\n\n connect_centroids_unidirection (x:numpy.ndarray,\n                                 row_borders:numpy.ndarray,\n                                 connections:numpy.ndarray,\n                                 scores:numpy.ndarray,\n                                 centroids:numpy.ndarray, max_gap:int,\n                                 centroid_tol:float)\n\nConnect centroids.\nArgs: x (np.ndarray): Index to datapoint. Note that this using the performance_function, so one passes an ndarray. row_borders (np.ndarray): Row borders of the centroids array. connections (np.ndarray): Connections matrix to store the connections scores (np.ndarray): Score matrix to store the connections centroids (np.ndarray): 1D Array containing the masses of the centroids data. max_gap (int): Maximum gap when connecting centroids. centroid_tol (float): Centroid tolerance.\nWe wrap the centroid connections in the function connect_centroids. This function converts the connections into an usable array.\n\nsource\n\n\nconnect_centroids\n\n connect_centroids (rowwise_peaks:numpy.ndarray,\n                    row_borders:numpy.ndarray, centroids:numpy.ndarray,\n                    max_gap:int, centroid_tol:float)\n\nFunction to connect centroids.\nArgs: rowwise_peaks (np.ndarray): Indexes for centroids. row_borders (np.ndarray): Row borders (for indexing). centroids (np.ndarray): Centroid data. max_gap: Maximum gap. centroid_tol: Centroid tol for matching centroids. Returns: np.ndarray: From index. np.ndarray: To index. float: Median score. float: Std deviation of the score.\n\nsource\n\n\neliminate_overarching_vertex\n\n eliminate_overarching_vertex (x:numpy.ndarray, from_idx:numpy.ndarray,\n                               to_idx:numpy.ndarray)\n\nEliminate overacrhing vertex.\nArgs: x (np.ndarray): Input index. Note that we are using the performance function so this is a range. from_idx (np.ndarray): From index. to_idx (np.ndarray): To index.\n\nsource\n\n\nconvert_connections_to_array\n\n convert_connections_to_array (x:numpy.ndarray, from_r:numpy.ndarray,\n                               from_c:numpy.ndarray, to_r:numpy.ndarray,\n                               to_c:numpy.ndarray,\n                               row_borders:numpy.ndarray,\n                               out_from_idx:numpy.ndarray,\n                               out_to_idx:numpy.ndarray)\n\nConvert integer indices of a matrix to coordinates.\nArgs: x (np.ndarray): Input index. Note that we are using the performance function so this is a range. from_r (np.ndarray): From array with row coordinates. from_c (np.ndarray): From array with column coordinates. to_r (np.ndarray): To array with row coordinates. to_c (np.ndarray): To array with column coordinates. row_borders (np.ndarray): Row borders (for indexing). out_from_idx (np.ndarray): Reporting array: 1D index from. out_to_idx (np.ndarray): Reporting array: 1D index to.\n\n#Sample snippet to show centroid conncetions\n\nimport matplotlib.pyplot as plt\n\nrow_borders = np.array([3, 6, 9])\nrowwise_peaks = np.array([3, 3, 3])\nmax_gap = 2\n\nscore = np.full((3,3, max_gap), np.inf)\nconnections = np.full((3,3, max_gap), -1)\n\ncentroids = np.array([10, 20, 30, 10.2, 20, 10, 30, 40])\ncentroid_tol = 0.5*1e5\n\nfrom_idx, to_idx, score_median, score_std = connect_centroids(rowwise_peaks, row_borders, centroids, max_gap, centroid_tol)\n\nscan_no = np.array([0, 0, 0, 1, 1, 2, 2, 2])\n\nplt.figure(figsize=(5,5))\nfor i, _ in enumerate(row_borders):\n    ctrd = centroids[_-rowwise_peaks[i]:_]\n    plt.plot(ctrd, np.ones_like(ctrd)*i, 'o')\n    \nfor i, _ in enumerate(from_idx):\n    from_ = _\n    to_ = to_idx[i]\n    plt.plot([centroids[from_], centroids[to_]], [scan_no[from_], scan_no[to_]], 'k:')\n    \nplt.ylabel('scan')\nplt.xlabel('m/z')\nplt.ylim(len(row_borders)+0.5, -1.5)\nplt.title('Peak connections')\nplt.show()\n\n\n\n\n\n\nExtracting hills.\nTo extract hills we extract connected components from the connections.\n\nsource\n\n\nremove_duplicate_hills\n\n remove_duplicate_hills (hill_ptrs, hill_data, path_node_cnt)\n\nRemoves hills that share datapoints. Starts from the largest hills.\n\nsource\n\n\nextract_hills\n\n extract_hills (query_data:dict, max_gap:int, centroid_tol:float)\n\n[summary]\nArgs: query_data (dict): Data structure containing the query data. max_gap (int): Maximum gap when connecting centroids. centroid_tol (float): Centroid tolerance.\nReturns: hill_ptrs (np.ndarray): Array containing the bounds to the hill_data. hill_data (np.ndarray): Array containing the indices to hills. path_node_cnt (int): Number of elements in this path. score_median (float): Median score. score_std (float): Std deviation of the score.\n\nsource\n\n\nget_hills\n\n get_hills (centroids:numpy.ndarray, from_idx:numpy.ndarray,\n            to_idx:numpy.ndarray, hill_length_min:int=3)\n\nFunction to get hills from centroid connections.\nArgs: centroids (np.ndarray): 1D Array containing the masses of the centroids. from_idx (np.ndarray): From index. to_idx (np.ndarray): To index. hill_length_min (int): Minimum hill length:\nReturns: hill_ptrs (np.ndarray): Array containing the bounds to the hill_data. hill_data (np.ndarray): Array containing the indices to hills. path_node_cnt (int): Number of elements in this path.\n\nsource\n\n\nfill_path_matrix\n\n fill_path_matrix (x:numpy.ndarray, path_start:numpy.ndarray,\n                   forwards:numpy.ndarray, out_hill_data:numpy.ndarray,\n                   out_hill_ptr:numpy.ndarray)\n\nFunction to fill the path matrix.\nArgs: x (np.ndarray): Input index. Note that we are using the performance function so this is a range. path_starts (np.ndarray): Array that stores the starts of the paths. forwards (np.ndarray): Forward array. out_hill_data (np.ndarray): Array containing the indices to hills. out_hill_ptr (np.ndarray): Array containing the bounds to out_hill_data.\n\nsource\n\n\nfind_path_length\n\n find_path_length (x:numpy.ndarray, path_starts:numpy.ndarray,\n                   forward:numpy.ndarray, path_cnt:numpy.ndarray)\n\nFunction to extract the length of a path.\nArgs: x (np.ndarray): Input index. Note that we are using the performance function so this is a range. path_starts (np.ndarray): Array that stores the starts of the paths. forward (np.ndarray): Array that stores forward information. path_cnt (np.ndarray): Reporting array to count the paths.\n\nsource\n\n\nfind_path_start\n\n find_path_start (x:numpy.ndarray, forward:numpy.ndarray,\n                  backward:numpy.ndarray, path_starts:numpy.ndarray)\n\nFunction to find the start of a path.\nArgs: x (np.ndarray): Input index. Note that we are using the performance function so this is a range. forward (np.ndarray): Array to report forward connection. backward (np.ndarray): Array to report backward connection. path_starts (np.ndarray): Array to report path starts.\n\nsource\n\n\npath_finder\n\n path_finder (x:numpy.ndarray, from_idx:numpy.ndarray,\n              to_idx:numpy.ndarray, forward:numpy.ndarray,\n              backward:numpy.ndarray)\n\nExtracts path information and writes to path matrix.\nArgs: x (np.ndarray): Input index. Note that we are using the performance function so this is a range. from_idx (np.ndarray): Array containing from indices. to_idx (np.ndarray): Array containing to indices. forward (np.ndarray): Array to report forward connection. backward (np.ndarray): Array to report backward connection.\n\n\nHill Splitting\nWhen having a hill with two or more maxima, we would like to split it at the minimum position. For this, we use a recursive approach. First, the minimum of a hill is detected. A hill is split at this minimum if the smaller of the surrounding maxima is at least the factor hill_split_level larger than the minimum. For each split, the process is repeated.\n\nsource\n\n\nfast_minima\n\n fast_minima (y:numpy.ndarray)\n\nFunction to calculate the local minimas of an array.\nArgs: y (np.ndarray): Input array.\nReturns: np.ndarray: Array containing minima positions.\n\nsource\n\n\nsplit_hills\n\n split_hills (hill_ptrs:numpy.ndarray, hill_data:numpy.ndarray,\n              int_data:numpy.ndarray, hill_split_level:float, window:int)\n\nWrapper function to split hills\nArgs: hill_ptrs (np.ndarray): Array containing the bounds to the hill_data. hill_data (np.ndarray): Array containing the indices to hills. int_data (np.ndarray): Array containing the intensity to each centroid. hill_split_level (float): Split level for hills. window (int): Smoothing window.\nReturns: np.ndarray: Array containing the bounds to the hill_data with splits.\n\nsource\n\n\nsplit\n\n split (k:numpy.ndarray, hill_ptrs:numpy.ndarray, int_data:numpy.ndarray,\n        hill_data:numpy.ndarray, splits:numpy.ndarray,\n        hill_split_level:float, window:int)\n\nFunction to split hills.\nArgs: k (np.ndarray): Input index. Note that we are using the performance function so this is a range. hill_ptrs (np.ndarray): Array containing the bounds to the hill_data. int_data (np.ndarray): Array containing the intensity to each centroid. hill_data (np.ndarray): Array containing the indices to hills. splits (np.ndarray): Array containing splits. hill_split_level (float): Split level for hills. window (int): Smoothing window.\n\n\nFilter Hills\nTo filter hills, we define a minimum length hill_min_length. All peaks below the threshold hill_peak_min_length are accepted as is. For longer hills, the intensity at the start and the end are compared to the maximum intensity. If the ratio of the maximum raw intensity to the smoothed intensity and the beginning and end are larger than hill_peak_factor the hills are accepted.\n\nsource\n\n\nfilter_hills\n\n filter_hills (hill_data:numpy.ndarray, hill_ptrs:numpy.ndarray,\n               int_data:numpy.ndarray, hill_check_large:int=40,\n               window:int=1)\n\nFilters large hills.\nArgs: hill_data (np.ndarray): Array containing the indices to hills. hill_ptrs (np.ndarray): Array containing the bounds to the hill_data. int_data (np.ndarray): Array containing the intensity to each centroid. hill_check_large (int, optional): Length criterion when a hill is considered large.. Defaults to 40. window (int, optional): Smoothing window. Defaults to 1.\nReturns: np.ndarray: Filtered hill data. np.ndarray: Filtered hill points.\n\nsource\n\n\ncheck_large_hills\n\n check_large_hills (idx:numpy.ndarray, large_peaks:numpy.ndarray,\n                    hill_ptrs:numpy.ndarray, hill_data:numpy.ndarray,\n                    int_data:numpy.ndarray, to_remove:numpy.ndarray,\n                    large_peak:int=40, hill_peak_factor:float=2,\n                    window:int=1)\n\nFunction to check large hills and flag them for removal.\nArgs: idx (np.ndarray): Input index. Note that we are using the performance function so this is a range. large_peaks (np.ndarray): Array containing large peaks. hill_ptrs (np.ndarray): Array containing the bounds to the hill_data. hill_data (np.ndarray): Array containing the indices to hills. int_data (np.ndarray): Array containing the intensity to each centroid. to_remove (np.ndarray): Array with indexes to remove. large_peak (int, optional): Length criterion when a peak is large. Defaults to 40. hill_peak_factor (float, optional): Hill maximum criterion. Defaults to 2. window (int, optional): Smoothing window.. Defaults to 1.\nSince the mass estimate min the equation above is more complicated than just an average of the mj, a standard deviation based estimate of the error would not be appropriate. Therefore we calculate the error as a bootstrap2 estimate over B=150 bootstrap replications"
docs/search.json:    "text": "AlphaPept deals with high-throughput data. As this can be computationally intensive, we try to make all functions as performant as possible. To do so, we rely on two principles: * Compilation * Parallelization\nA first step of compilation can be achieved by using NumPy arrays which are already heavily c-optimized. Net we consider three kinds of compilation: * Python This allows to use no compilation * Numba This allows to use just-in-time (JIT) compilation. * Cuda This allows compilation on the GPU.\nAll of these compilation approaches can be combined with parallelization approaches. We consider the following possibilities: * No parallelization Not all functionality can be parallelized. * Multithreading This is only performant when Python’s global interpreter lock (GIL) is released or when mostly using input-/output (IO) functions. * GPU This is only available if an NVIDIA GPU is available and properly configured.\nNote that not all compilation approaches can sensibly be combined with all parallelization approaches.\nNext we import all libraries, taking into account that not every machine has a GPU (with NVidia cuda cores) available:\n\nsource\n\nis_valid_compilation_mode\n\n is_valid_compilation_mode (compilation_mode:str)\n\nCheck if the provided string is a valid compilation mode.\nArgs: compilation_mode (str): The compilation mode to verify.\nRaises: ModuleNotFoundError: When trying to use an unavailable GPU. NotImplementedError: When the compilation mode is not valid.\nBy default, we will use cuda if it is available. If not, numba-multithread will be used as default.\nTo consistently use multiple threads or processes, we can set a global MAX_WORKER_COUNT parameter.\n\nsource\n\n\nset_worker_count\n\n set_worker_count (worker_count:int=1, set_global:bool=True)\n\nParse and set the (global) number of threads.\nArgs: worker_count (int): The number of workers. If larger than available cores, it is trimmed to the available maximum. If 0, it is set to the maximum cores available. If negative, it indicates how many cores NOT to use. Default is 1 set_global (bool): If False, the number of workers is only parsed to a valid value. If True, the number of workers is saved as a global variable. Default is True.\nReturns: int: The parsed worker_count.\nCompiled functions are intended to be very fast. However, they do not have the same flexibility as pure Python functions. In general, we recommend to use staticly defined compilation functions for optimal performance. We provide the option to define a default compilation mode for decorated functions, while also allowing to define the compilation mode for each individual function.\nNOTE: Compiled functions are by default expected to be performed on a single thread. Thus, ‘cuda’ funtions are always assumed to be device functions which makes them callable from within the GPU, unless explicitly stated otherwise. Similarly, ‘numba’ functions are always assumed to bo ‘nopython’ and ‘nogil’.\nNOTE If the global compilation mode is set to Python, all decorators default to python, even if a specific compilation_mode is provided.\nIn addition, we allow to enable dynamic compilation, meaning the compilation mode of functions can be changed at runtime. Do note that this comes at the cost of some performance, as compilation needs to be done at runtime as well. Moreover, functions that are defined with dynamic compilation can not be called from within other compiled functions (with the exception of ‘python’ compilation, which means no compilation is actually performe|d).\nNOTE: Dynamic compilation must be enabled before functions are decorated to take effect at runtime, otherwise they are statically compiled with the current settings at the time they are defined! Alternatively, statically compiled functions of a an ‘imported_module’ can reloaded (and thus statically be recompiled) with the commands:\nimport importlib\nimportlib.reload(imported_module)\n\nsource\n\n\ncompile_function\n\n compile_function (_func:<built-infunctioncallable>=None,\n                   compilation_mode:str=None, **decorator_kwargs)\n\nA decorator to compile a given function.\nNumba functions are by default set to use nogil=True and nopython=True, unless explicitly defined otherwise. Cuda functions are by default set to use device=True, unless explicitly defined otherwise..\nArgs: compilation_mode (str): The compilation mode to use. Will be checked with is_valid_compilation_mode. If None, the global COMPILATION_MODE will be used as soon as the function is decorated for static compilation. If DYNAMIC_COMPILATION_ENABLED, the function will always be compiled at runtime and thus by default returns a Python function. Static recompilation can be enforced by reimporting a module containing the function with importlib.reload(imported_module). If COMPILATION_MODE is Python and not DYNAMIC_COMPILATION_ENABLED, no compilation will be used. Default is None **decorator_kwargs: Keyword arguments that will be passed to numba.jit or cuda.jit compilation decorators.\nReturns: callable: A decorated function that is compiled.\n\nsource\n\n\nset_compilation_mode\n\n set_compilation_mode (compilation_mode:str=None,\n                       enable_dynamic_compilation:bool=None)\n\nSet the global compilation mode to use.\nArgs: compilation_mode (str): The compilation mode to use. Will be checked with is_valid_compilation_mode. Default is None enable_dynamic_compilation (bool): Enable dynamic compilation. If enabled, code will generally be slower and no other functions can be called from within a compiled function anymore, as they are compiled at runtime. WARNING: Enabling this is strongly disadvised in almost all cases! Default is None.\nTesting yields the expected results:\n\nimport types\n\nset_compilation_mode(compilation_mode=\"numba-multithread\")\n\n@compile_function(compilation_mode=\"python\")\ndef test_func_python(x):\n    \"\"\"Docstring test\"\"\"\n    x[0] += 1\n    \n@compile_function(compilation_mode=\"numba\")\ndef test_func_numba(x):\n    \"\"\"Docstring test\"\"\"\n    x[0] += 1\n\nset_compilation_mode(enable_dynamic_compilation=True)\n\n@compile_function\ndef test_func_dynamic_runtime(x):\n    \"\"\"Docstring test\"\"\"\n    x[0] += 1\n\nset_compilation_mode(enable_dynamic_compilation=False, compilation_mode=\"numba-multithread\")\n\n@compile_function\ndef test_func_static_runtime_numba(x):\n    \"\"\"Docstring test\"\"\"\n    x[0] += 1\n\na = np.zeros(1, dtype=np.int64)\nassert(isinstance(test_func_python, types.FunctionType))\ntest_func_python(a)\nassert(np.all(a == np.ones(1)))\n\na = np.zeros(1)\nassert(isinstance(test_func_numba, numba.core.registry.CPUDispatcher))\ntest_func_numba(a)\nassert(np.all(a == np.ones(1)))\n\nif __GPU_AVAILABLE:\n    @compile_function(compilation_mode=\"cuda\", device=None)\n    def test_func_cuda(x):\n        \"\"\"Docstring test\"\"\"\n        x[0] += 1\n\n    # Cuda function cannot be tested from outside the GPU\n    a = np.zeros(1)\n    assert(isinstance(test_func_cuda, numba.cuda.compiler.Dispatcher))\n    test_func_cuda.forall(1,1)(a)\n    assert(np.all(a == np.ones(1)))\n\nset_compilation_mode(compilation_mode=\"python\")\na = np.zeros(1)\nassert(isinstance(test_func_static_runtime_numba, numba.core.registry.CPUDispatcher))\ntest_func_static_runtime_numba(a)\nassert(np.all(a == np.ones(1)))\n\nset_compilation_mode(compilation_mode=\"python\")\na = np.zeros(1)\nassert(isinstance(test_func_dynamic_runtime, types.FunctionType))\ntest_func_dynamic_runtime(a)\nassert(np.all(a == np.ones(1)))\n\nset_compilation_mode(compilation_mode=\"numba\")\na = np.zeros(1)\nassert(isinstance(test_func_dynamic_runtime, types.FunctionType))\ntest_func_dynamic_runtime(a)\nassert(np.all(a == np.ones(1)))\n\n# # Cuda function cannot be tested from outside the GPU\n# set_compilation_mode(compilation_mode=\"cuda\")\n# a = np.zeros(1)\n# assert(isinstance(test_func_dynamic_runtime, types.FunctionType))\n# test_func_dynamic_runtime.forall(1,1)(a)\n# assert(np.all(a == np.ones(1)))\n\nC:\\ProgramData\\Miniconda3\\envs\\alphapept\\lib\\site-packages\\numba\\cuda\\compiler.py:726: NumbaPerformanceWarning: Grid size (1) < 2 * SM count (136) will likely result in GPU under utilization due to low occupancy.\n  warn(NumbaPerformanceWarning(msg))\nC:\\ProgramData\\Miniconda3\\envs\\alphapept\\lib\\site-packages\\numba\\cuda\\cudadrv\\devicearray.py:885: NumbaPerformanceWarning: Host array used in CUDA kernel will incur copy overhead to/from device.\n  warn(NumbaPerformanceWarning(msg))\n\n\nNext, we define the ‘performance_function’ decorator to take full advantage of both compilation and parallelization for maximal performance. Note that a ‘performance_function’ can not return values. Instead, it should store results in provided buffer arrays.\n\nsource\n\n\nperformance_function\n\n performance_function (_func:<built-infunctioncallable>=None,\n                       worker_count:int=None, compilation_mode:str=None,\n                       **decorator_kwargs)\n\nA decorator to compile a given function and allow multithreading over an multiple indices.\nNOTE This should only be used on functions that are compilable. Functions that need to be decorated need to have an index argument as first argument. If an iterable is provided to the decorated function, the original (compiled) function will be applied to all elements of this iterable. The most efficient way to provide iterables are with ranges, but numpy arrays work as well. Functions can not return values, results should be stored in buffer arrays inside thge function instead.\nArgs: worker_count (int): The number of workers to use for multithreading. If None, the global MAX_WORKER_COUNT is used at runtime. Default is None. compilation_mode (str): The compilation mode to use. Will be forwarded to the compile_function decorator. **decorator_kwargs: Keyword arguments that will be passed to numba.jit or cuda.jit compilation decorators.\nReturns: callable: A decorated function that is compiled and parallelized.\nWe test this function with a simple smoothing algorithm.\n\ndef smooth_func(index, in_array, out_array, window_size):\n    min_index = max(index - window_size, 0)\n    max_index = min(index + window_size + 1, len(in_array))\n    smooth_value = 0\n    for i in range(min_index, max_index):\n        smooth_value += in_array[i]\n    out_array[index] += smooth_value / (max_index - min_index)\n\n\nset_compilation_mode(compilation_mode=\"numba-multithread\")\nset_worker_count(0)\narray_size = 10**6\nsmooth_factor = 10**4\n\n# python test\nin_array = np.arange(array_size)\nout_array = np.zeros_like(in_array)\n\nfunc = performance_function(compilation_mode=\"python\")(smooth_func)\n\n\n# numba test\nin_array = np.arange(array_size)\nout_array = np.zeros_like(in_array)\n\nfunc = performance_function(compilation_mode=\"numba\")(smooth_func)\n\n\n# numba-multithread test\nin_array = np.arange(array_size)\nout_array = np.zeros_like(in_array)\n\nfunc = performance_function(compilation_mode=\"numba-multithread\")(smooth_func)\n\n\n# cuda test\nif __GPU_AVAILABLE:\n    in_array = cupy.arange(array_size)\n    out_array = cupy.zeros_like(in_array)\n\n    func = performance_function(compilation_mode=\"cuda\")(smooth_func)\n\nCPU times: total: 2.55 s\nWall time: 2.54 s\nCPU times: total: 7.47 s\nWall time: 7.49 s\nCPU times: total: 11 s\nWall time: 887 ms\n\n\nFinally, we also provide functionality to use multiprocessing instead of multithreading.\nNOTE: There are some inherent limitation with the number of processes that Python can spawn. As such, no process Pool should use more than 50 processes.\n\nsource\n\n\nAlphaPool\n\n AlphaPool (process_count:int)\n\nCreate a multiprocessing.Pool object.\nArgs: process_count (int): The number of processes. If larger than available cores, it is trimmed to the available maximum.\nReturns: multiprocessing.Pool: A Pool object to parallelize functions with multiple processes."
docs/search.json:    "text": "CI test\nThe files test_ci.py and test_gpu_.py are part of the continous integration pipeline. As indicated in the contributing-page they can be run on a local machines.\n\n\n\n\n\n\nNote\n\n\n\nUnless you are a core developer, there should not be the need to add integration tests. Feel free to create a PR in case."
docs/search.json:    "text": "Tests\nIn order to make AlphaPept a sustainable package, it is imperative that all functions have tests. This is not only to ensure the proper execution of the function but also for the long run when wanting to keep up-to-date with package updates. For tracking package updates, we rely on dependabot. For continuous integration, we use GitHub Actions.\n\nUnit Tests\nWithin the nbdev notebooks, we try to write small tests for each function. They should be lightweight so that running (and hence testing) the notebooks should not take too long. You define your code in a cell with the #export-flag and you can write a text in the following cell. To prevent the cell from showing up in the documentation, you can add a #hide-flag. To be flexible and maybe migrate from the notebook tests to pytest we write the tests with leading test_ and the function name and execute it in the cell.\n\nExample\nCell that defines the code:\n#export\n\ndef function_a():\n    return 5\nNext cell with the test function and calling the test.\ndef test_function_a():\n    assert function_a() == 5\n\ntest_function_a()\n\n\n\n\n\n\nNote\n\n\n\nAlphaPept contains a testfolder testfiles with testfiles which are used for some functions to perform tests on files.\n\n\n\n\n\nGitHub Actions\nWe use GitHub Actions as part of the continuous integration framework.\nWithin the repository, the actions are defined as yml files in ./.github/workflows/.\nCurrently, the following actions are present:\n\ncla_assistant: Action that runs a check if a contributor has signed the CLA\ngpu_test: Helper to run GPU tests. This is not part of the automated pipeline\nmain: Main CI/CD from nbdev. This installs the library and runs all notebooks\nperformance_test: This action freezes the environment and runs multiple test cases specified in test_ci.py. Used to benchmark new versions.\nquick_test: This action runs the whole AlphaPept pipeline on small test files\nrelease: This action triggers the installer compilation and creates a release draft\n\nThe quick_test and performance_test are hooked to the MongoDB, so results are reported here.\nSome actions are executed on each push (e.g. quick_test, others need to be triggered manually (e.g. release).\n\n\nAdding an integration test\n\n\n\n\n\n\nNote\n\n\n\nUnless you are a core developer, there should not be the need to add integration tests. Feel free to create a PR in case.\n\n\nThe best way to add an integration test is by specifying a new test in test_ci.py and then adding a job to the performance_test action. test_ci.py contains a file dictionary with links to files. The idea here is as follows: On the local machine, there should be a BASE_DIR that contains all files used for running the test cases. If the files are not present, the script will download them with the URL from the file dictionary. To define a test case, one needs to initiate a TestRun-class and provide the files to be used. Running a test will always take the current default_settings.yaml and modifying the respective files. You can add subsequent analysis (e.g., such as calculating the false discovery FDR for a mixed species experiment by adding a function to the test class.\nTo test an integration test locally, simply call the test_ci.py with the respective test (e.g. thermo_irt) like so: python test_ci.py thermo_irt.\n\n\nIntegrating Actions\n\n\n\n\n\n\nNote\n\n\n\nUnless you are a core developer, there should not be the need to add GitHub Actions. Feel free to create a PR in case.\n\n\nNew actions can be added by creating a .yml file and placing it in the workflows folder. We can distinguish local actions, which run on our self-hosted runners and cloud actions that run on GitHub servers.\nThe local runners are meant to run computationally intensive tasks such as the performance_test and the quick_test, whereas the cloud actions are meant for unit tests.\n\n\n\n\n\n\nNote\n\n\n\nIf you fork the repository, you will not be able to execute tests on the local runners. They are restricted to the MannLabs repository."
docs/search.json:    "text": "Python Installation Instructions\n\nRequirements\nWe highly recommend the Anaconda or Miniconda Python distribution, which comes with a powerful package manager. See below for additional instructions for Linux and Mac as they require additional installation of Mono to use the RawFileReader.\nAlphaPept can be used as an application as a whole or as a Python Package where individual modules are called. Depending on the use case, AlphaPept will need different requirements, and you might not want to install all of them.\nCurrently, we have the default requirements.txt, additional requirements to run the GUI gui and packages used for developing develop.\nTherefore, you can install AlphaPept in multiple ways:\n\nThe default alphapept\nWith GUI-packages alphapept[gui]\nWith pacakges for development alphapept[develop] (alphapept[develop,gui]) respectively\n\nThe requirements typically contain pinned versions and will be automatically upgraded and tested with dependabot. This stable version allows having a reproducible workflow. However, in order to avoid conflicts with package versions that are too strict, the requirements are not pinned when being installed. To use the strict version use the -stable-flag, e.g. alphapept[stable].\nFor end-users that want to set up a processing environment in Python, the \"alphapept[stable,gui-stable]\" is the batteries-included-version that you want to use.\n\n\nPython\nIt is strongly recommended to install AlphaPept in its own environment. 1. Open the console and create a new conda environment: conda create --name alphapept python=3.8 2. Activate the environment: conda activate alphapept 3. Install AlphaPept via pip: pip install \"alphapept[stable,gui-stable]\". If you want to use AlphaPept as a package without the GUI dependencies and without strict version dependencies, use pip install alphapept.\nIf AlphaPept is installed correctly, you should be able to import AlphaPept as a package within the environment; see below.\n\n\nLinux\n\nInstall the build-essentials: sudo apt-get install build-essential.\nInstall AlphaPept via pip: pip install \"alphapept[stable,gui-stable]\". If you want to use AlphaPept as a package withouth the GUI dependencies and strict version dependencies use pip install alphapept.\nInstall libgomp.1 with sudo apt-get install libgomp1.\n\n\nBruker Support\n\nCopy-paste the Bruker library for feature finding to your /usr/lib folder with sudo cp alphapept/ext/bruker/FF/linux64/alphapeptlibtbb.so.2 /usr/lib/libtbb.so.2.\n\n\n\nThermo Support\n\nInstall Mono from mono-project website Mono Linux. NOTE, the installed mono version should be at least 6.10, which requires you to add the ppa to your trusted sources!\nInstall pythonnet with pip install pythonnet>=2.5.2\n\n\n\n\n\nMac\n\nInstall AlphaPept via pip: pip install \"alphapept[stable,gui-stable]\". If you want to use AlphaPept as a package withouth the GUI dependencies and strict version dependencies use pip install alphapept.\n\n\nBruker Support\n\nOnly supported for preprocessed files.\n\n\n\nThermo Support\n\nInstall brew and pkg-config: brew install pkg-config\nInstall Mono from mono-project website Mono Mac\nRegister the Mono-Path to your system: For macOS Catalina, open the configuration of zsh via the terminal:\n\n\nType in cd to navigate to the home directory.\nType nano ~/.zshrc to open the configuration of the terminal\nAdd the path to your mono installation: export PKG_CONFIG_PATH=/usr/local/lib/pkgconfig:/usr/lib/pkgconfig:/Library/Frameworks/Mono.framework/Versions/Current/lib/pkgconfig:$PKG_CONFIG_PATH. Make sure that the Path matches to your version (Here 6.12.0)\nSave everything and execute . ~/.zshrc\n\n\nInstall pythonnet with pip install pythonnet>=2.5.2\n\n\n\n\n\nDeveloper\n\nRedirect to the folder of choice and clone the repository: git clone https://github.com/MannLabs/alphapept.git\nNavigate to the alphapept folder with cd alphapept and install the package with pip install . (default users) or with pip install -e . to enable developers mode. Note that you can use the different requirements here aswell (e.g. pip install \".[gui-stable]\")\n\n\n\nGPU Support\nSome functionality of AlphaPept is GPU optimized that uses Nvidia’s CUDA. To enable this, additional packages need to be installed.\n\nMake sure to have a working CUDA toolkit installation that is compatible with CuPy. To check type nvcc --version in your terminal.\nInstall cupy. Make sure to install the cupy version matching your CUDA toolkit (e.g. pip install cupy-cuda110 for CUDA toolkit 11.0.\n\n\n\n\nAdditional Notes\n\nTo access Thermo files, we have integrated RawFileReader into AlphaPept. We rely on Mono for Linux/Mac systems.\n\n\nTo access Bruker files, we rely on the timsdata-library. Currently, only Windows is supported. For feature finding, we use the Bruker Feature Finder, which can be found in the ext folder of this repository.\n\n\nNotes for NBDEV\n\nFor developing with the notebooks, install the nbdev package (see the development requirements)\nTo facilitate navigating the notebooks, use jupyter notebook extensions. They can be called from a running jupyter instance like so: http://localhost:8888/nbextensions. The extensions collapsible headings and toc2 are very beneficial."
docs/search.json:    "text": "Notebooks\nWithin the notebooks, we try to cover most aspects of a proteomics workflow:\n\nSettings: General settings to define a workflow\nChem: Chemistry related functions, e.g., for calculating isotope distributions\nInput / Output: Everything related to importing and exporting and the file formats used\nFASTA: Generating theoretical databases from FASTA files\nFeature Finding: How to extract MS1 features for quantification\nSearch: Comparing theoretical databases to experimental spectra and getting Peptide-Spectrum-Matches (PSMs)\nScore: Scoring PSMs\nRecalibration: Recalibration of data based on identified peptides\nQuantification: Functions for quantification, e.g., LFQ\nMatching: Functions for Match-between-runs\nConstants: A collection of constants\nInterface: Code that generates the command-line-interface (CLI) and makes workflow steps callable\nPerformance: Helper functions to speed up code with CPU / GPU\nExport: Helper functions to make exports compatbile to other Software tools\nLabel: Code for support isobaric label search\nDisplay: Code related to displaying in the streamlit gui\nAdditional code: Overview of additional code not covered by the notebooks\nHow to contribute: Contribution guidelines\nAlphaPept workflow and files: Overview of the worfklow, files and column names"
docs/site_libs/bootstrap/bootstrap-icons.css:.bi-gpu-card::before { content: "\f6e2"; }
README.md:#### GPU Support
README.md:Some functionality of AlphaPept is GPU optimized that uses Nvidia’s
README.md:CUDA. To enable this, additional packages need to be installed.
README.md:1.  Make sure to have a working [CUDA
README.md:    toolkit](https://developer.nvidia.com/cuda-toolkit) installation
README.md:    version matching your CUDA toolkit (e.g. `pip install cupy-cuda110`
README.md:    for CUDA toolkit 11.0.
README.md:- Performance: Helper functions to speed up code with CPU / GPU
nbs/docs/search.json:    "text": "Tests\nIn order to make AlphaPept a sustainable package, it is imperative that all functions have tests. This is not only to ensure the proper execution of the function but also for the long run when wanting to keep up-to-date with package updates. For tracking package updates, we rely on dependabot. For continuous integration, we use GitHub Actions.\n\nUnit Tests\nWithin the nbdev notebooks, we try to write small tests for each function. They should be lightweight so that running (and hence testing) the notebooks should not take too long. You define your code in a cell with the #export-flag and you can write a text in the following cell. To prevent the cell from showing up in the documentation, you can add a #hide-flag. To be flexible and maybe migrate from the notebook tests to pytest we write the tests with leading test_ and the function name and execute it in the cell.\n\nExample\nCell that defines the code:\n#export\n\ndef function_a():\n    return 5\nNext cell with the test function and calling the test.\ndef test_function_a():\n    assert function_a() == 5\n\ntest_function_a()\n\n\n\n\n\n\nNote\n\n\n\nAlphaPept contains a testfolder testfiles with testfiles which are used for some functions to perform tests on files.\n\n\n\n\n\nGitHub Actions\nWe use GitHub Actions as part of the continuous integration framework.\nWithin the repository, the actions are defined as yml files in ./.github/workflows/.\nCurrently, the following actions are present:\n\ncla_assistant: Action that runs a check if a contributor has signed the CLA\ngpu_test: Helper to run GPU tests. This is not part of the automated pipeline\nmain: Main CI/CD from nbdev. This installs the library and runs all notebooks\nperformance_test: This action freezes the environment and runs multiple test cases specified in test_ci.py. Used to benchmark new versions.\nquick_test: This action runs the whole AlphaPept pipeline on small test files\nrelease: This action triggers the installer compilation and creates a release draft\n\nThe quick_test and performance_test are hooked to the MongoDB, so results are reported here.\nSome actions are executed on each push (e.g. quick_test, others need to be triggered manually (e.g. release).\n\n\nAdding an integration test\n\n\n\n\n\n\nNote\n\n\n\nUnless you are a core developer, there should not be the need to add integration tests. Feel free to create a PR in case.\n\n\nThe best way to add an integration test is by specifying a new test in test_ci.py and then adding a job to the performance_test action. test_ci.py contains a file dictionary with links to files. The idea here is as follows: On the local machine, there should be a BASE_DIR that contains all files used for running the test cases. If the files are not present, the script will download them with the URL from the file dictionary. To define a test case, one needs to initiate a TestRun-class and provide the files to be used. Running a test will always take the current default_settings.yaml and modifying the respective files. You can add subsequent analysis (e.g., such as calculating the false discovery FDR for a mixed species experiment by adding a function to the test class.\nTo test an integration test locally, simply call the test_ci.py with the respective test (e.g. thermo_irt) like so: python test_ci.py thermo_irt.\n\n\nIntegrating Actions\n\n\n\n\n\n\nNote\n\n\n\nUnless you are a core developer, there should not be the need to add GitHub Actions. Feel free to create a PR in case.\n\n\nNew actions can be added by creating a .yml file and placing it in the workflows folder. We can distinguish local actions, which run on our self-hosted runners and cloud actions that run on GitHub servers.\nThe local runners are meant to run computationally intensive tasks such as the performance_test and the quick_test, whereas the cloud actions are meant for unit tests.\n\n\n\n\n\n\nNote\n\n\n\nIf you fork the repository, you will not be able to execute tests on the local runners. They are restricted to the MannLabs repository."
nbs/docs/search.json:    "text": "A first step of compilation can be achieved by using NumPy arrays which are already heavily c-optimized. Net we consider three kinds of compilation: * Python This allows to use no compilation * Numba This allows to use just-in-time (JIT) compilation. * Cuda This allows compilation on the GPU.\nAll of these compilation approaches can be combined with parallelization approaches. We consider the following possibilities: * No parallelization Not all functionality can be parallelized. * Multithreading This is only performant when Python’s global interpreter lock (GIL) is released or when mostly using input-/output (IO) functions. * GPU This is only available if an NVIDIA GPU is available and properly configured.\nNote that not all compilation approaches can sensibly be combined with all parallelization approaches.\nNext we import all libraries, taking into account that not every machine has a GPU (with NVidia cuda cores) available:\n\nsource\n\nis_valid_compilation_mode\n\n is_valid_compilation_mode (compilation_mode:str)\n\nCheck if the provided string is a valid compilation mode.\nArgs: compilation_mode (str): The compilation mode to verify.\nRaises: ModuleNotFoundError: When trying to use an unavailable GPU. NotImplementedError: When the compilation mode is not valid.\nBy default, we will use cuda if it is available. If not, numba-multithread will be used as default.\nTo consistently use multiple threads or processes, we can set a global MAX_WORKER_COUNT parameter.\n\nsource\n\n\nset_worker_count\n\n set_worker_count (worker_count:int=1, set_global:bool=True)\n\nParse and set the (global) number of threads.\nArgs: worker_count (int): The number of workers. If larger than available cores, it is trimmed to the available maximum. If 0, it is set to the maximum cores available. If negative, it indicates how many cores NOT to use. Default is 1 set_global (bool): If False, the number of workers is only parsed to a valid value. If True, the number of workers is saved as a global variable. Default is True.\nReturns: int: The parsed worker_count.\nCompiled functions are intended to be very fast. However, they do not have the same flexibility as pure Python functions. In general, we recommend to use staticly defined compilation functions for optimal performance. We provide the option to define a default compilation mode for decorated functions, while also allowing to define the compilation mode for each individual function.\nNOTE: Compiled functions are by default expected to be performed on a single thread. Thus, ‘cuda’ funtions are always assumed to be device functions which makes them callable from within the GPU, unless explicitly stated otherwise. Similarly, ‘numba’ functions are always assumed to bo ‘nopython’ and ‘nogil’.\nNOTE If the global compilation mode is set to Python, all decorators default to python, even if a specific compilation_mode is provided.\nIn addition, we allow to enable dynamic compilation, meaning the compilation mode of functions can be changed at runtime. Do note that this comes at the cost of some performance, as compilation needs to be done at runtime as well. Moreover, functions that are defined with dynamic compilation can not be called from within other compiled functions (with the exception of ‘python’ compilation, which means no compilation is actually performe|d).\nNOTE: Dynamic compilation must be enabled before functions are decorated to take effect at runtime, otherwise they are statically compiled with the current settings at the time they are defined! Alternatively, statically compiled functions of a an ‘imported_module’ can reloaded (and thus statically be recompiled) with the commands:\nimport importlib\nimportlib.reload(imported_module)\n\nsource\n\n\ncompile_function\n\n compile_function (_func:<built-infunctioncallable>=None,\n                   compilation_mode:str=None, **decorator_kwargs)\n\nA decorator to compile a given function.\nNumba functions are by default set to use nogil=True and nopython=True, unless explicitly defined otherwise. Cuda functions are by default set to use device=True, unless explicitly defined otherwise..\nArgs: compilation_mode (str): The compilation mode to use. Will be checked with is_valid_compilation_mode. If None, the global COMPILATION_MODE will be used as soon as the function is decorated for static compilation. If DYNAMIC_COMPILATION_ENABLED, the function will always be compiled at runtime and thus by default returns a Python function. Static recompilation can be enforced by reimporting a module containing the function with importlib.reload(imported_module). If COMPILATION_MODE is Python and not DYNAMIC_COMPILATION_ENABLED, no compilation will be used. Default is None **decorator_kwargs: Keyword arguments that will be passed to numba.jit or cuda.jit compilation decorators.\nReturns: callable: A decorated function that is compiled.\n\nsource\n\n\nset_compilation_mode\n\n set_compilation_mode (compilation_mode:str=None,\n                       enable_dynamic_compilation:bool=None)\n\nSet the global compilation mode to use.\nArgs: compilation_mode (str): The compilation mode to use. Will be checked with is_valid_compilation_mode. Default is None enable_dynamic_compilation (bool): Enable dynamic compilation. If enabled, code will generally be slower and no other functions can be called from within a compiled function anymore, as they are compiled at runtime. WARNING: Enabling this is strongly disadvised in almost all cases! Default is None.\nTesting yields the expected results:\n\nimport types\n\nset_compilation_mode(compilation_mode=\"numba-multithread\")\n\n@compile_function(compilation_mode=\"python\")\ndef test_func_python(x):\n    \"\"\"Docstring test\"\"\"\n    x[0] += 1\n    \n@compile_function(compilation_mode=\"numba\")\ndef test_func_numba(x):\n    \"\"\"Docstring test\"\"\"\n    x[0] += 1\n\nset_compilation_mode(enable_dynamic_compilation=True)\n\n@compile_function\ndef test_func_dynamic_runtime(x):\n    \"\"\"Docstring test\"\"\"\n    x[0] += 1\n\nset_compilation_mode(enable_dynamic_compilation=False, compilation_mode=\"numba-multithread\")\n\n@compile_function\ndef test_func_static_runtime_numba(x):\n    \"\"\"Docstring test\"\"\"\n    x[0] += 1\n\na = np.zeros(1, dtype=np.int64)\nassert(isinstance(test_func_python, types.FunctionType))\ntest_func_python(a)\nassert(np.all(a == np.ones(1)))\n\na = np.zeros(1)\nassert(isinstance(test_func_numba, numba.core.registry.CPUDispatcher))\ntest_func_numba(a)\nassert(np.all(a == np.ones(1)))\n\nif __GPU_AVAILABLE:\n    @compile_function(compilation_mode=\"cuda\", device=None)\n    def test_func_cuda(x):\n        \"\"\"Docstring test\"\"\"\n        x[0] += 1\n\n    # Cuda function cannot be tested from outside the GPU\n    a = np.zeros(1)\n    assert(isinstance(test_func_cuda, numba.cuda.compiler.Dispatcher))\n    test_func_cuda.forall(1,1)(a)\n    assert(np.all(a == np.ones(1)))\n\nset_compilation_mode(compilation_mode=\"python\")\na = np.zeros(1)\nassert(isinstance(test_func_static_runtime_numba, numba.core.registry.CPUDispatcher))\ntest_func_static_runtime_numba(a)\nassert(np.all(a == np.ones(1)))\n\nset_compilation_mode(compilation_mode=\"python\")\na = np.zeros(1)\nassert(isinstance(test_func_dynamic_runtime, types.FunctionType))\ntest_func_dynamic_runtime(a)\nassert(np.all(a == np.ones(1)))\n\nset_compilation_mode(compilation_mode=\"numba\")\na = np.zeros(1)\nassert(isinstance(test_func_dynamic_runtime, types.FunctionType))\ntest_func_dynamic_runtime(a)\nassert(np.all(a == np.ones(1)))\n\n# # Cuda function cannot be tested from outside the GPU\n# set_compilation_mode(compilation_mode=\"cuda\")\n# a = np.zeros(1)\n# assert(isinstance(test_func_dynamic_runtime, types.FunctionType))\n# test_func_dynamic_runtime.forall(1,1)(a)\n# assert(np.all(a == np.ones(1)))\n\nNext, we define the ‘performance_function’ decorator to take full advantage of both compilation and parallelization for maximal performance. Note that a ‘performance_function’ can not return values. Instead, it should store results in provided buffer arrays.\n\nsource\n\n\nperformance_function\n\n performance_function (_func:<built-infunctioncallable>=None,\n                       worker_count:int=None, compilation_mode:str=None,\n                       **decorator_kwargs)\n\nA decorator to compile a given function and allow multithreading over an multiple indices.\nNOTE This should only be used on functions that are compilable. Functions that need to be decorated need to have an index argument as first argument. If an iterable is provided to the decorated function, the original (compiled) function will be applied to all elements of this iterable. The most efficient way to provide iterables are with ranges, but numpy arrays work as well. Functions can not return values, results should be stored in buffer arrays inside thge function instead.\nArgs: worker_count (int): The number of workers to use for multithreading. If None, the global MAX_WORKER_COUNT is used at runtime. Default is None. compilation_mode (str): The compilation mode to use. Will be forwarded to the compile_function decorator. **decorator_kwargs: Keyword arguments that will be passed to numba.jit or cuda.jit compilation decorators.\nReturns: callable: A decorated function that is compiled and parallelized.\nWe test this function with a simple smoothing algorithm.\n\ndef smooth_func(index, in_array, out_array, window_size):\n    min_index = max(index - window_size, 0)\n    max_index = min(index + window_size + 1, len(in_array))\n    smooth_value = 0\n    for i in range(min_index, max_index):\n        smooth_value += in_array[i]\n    out_array[index] += smooth_value / (max_index - min_index)\n\n\nset_compilation_mode(compilation_mode=\"numba-multithread\")\nset_worker_count(0)\narray_size = 10**6\nsmooth_factor = 10**4\n\n# python test\nin_array = np.arange(array_size)\nout_array = np.zeros_like(in_array)\n\nfunc = performance_function(compilation_mode=\"python\")(smooth_func)\n\n\n# numba test\nin_array = np.arange(array_size)\nout_array = np.zeros_like(in_array)\n\nfunc = performance_function(compilation_mode=\"numba\")(smooth_func)\n\n\n# numba-multithread test\nin_array = np.arange(array_size)\nout_array = np.zeros_like(in_array)\n\nfunc = performance_function(compilation_mode=\"numba-multithread\")(smooth_func)\n\n\n# cuda test\nif __GPU_AVAILABLE:\n    in_array = cupy.arange(array_size)\n    out_array = cupy.zeros_like(in_array)\n\n    func = performance_function(compilation_mode=\"cuda\")(smooth_func)\n\nCPU times: total: 2.55 s\nWall time: 2.54 s\nCPU times: total: 7.47 s\nWall time: 7.49 s\nCPU times: total: 11 s\nWall time: 887 ms\n\n\nFinally, we also provide functionality to use multiprocessing instead of multithreading.\nNOTE: There are some inherent limitation with the number of processes that Python can spawn. As such, no process Pool should use more than 50 processes.\n\nsource\n\n\nAlphaPool\n\n AlphaPool (process_count:int)\n\nCreate a multiprocessing.Pool object.\nArgs: process_count (int): The number of processes. If larger than available cores, it is trimmed to the available maximum.\nReturns: multiprocessing.Pool: A Pool object to parallelize functions with multiple processes."
nbs/docs/search.json:    "text": "CI test\nThe files test_ci.py and test_gpu_.py are part of the continous integration pipeline. As indicated in the contributing-page they can be run on a local machines.\n\n\n\n\n\n\nNote\n\n\n\nUnless you are a core developer, there should not be the need to add integration tests. Feel free to create a PR in case."
nbs/docs/search.json:    "text": "Connecting Centroids to Hills\n\n\n\n\n\n\nNote\n\n\n\nFeature finding relies heavily on the performance function decorator from the performance notebook: @alphapept.performance.performance_function. Part of this is that the functions will not have return values to be GPU compatible. Please check out this notebook for further information.\n\n\n\nConnecting centroids\nFeature finding starts with connecting centroids. For this we look at subsequent scans and compare peaks that are withing a defined mass tolerance (centroid_tol).\nImagine you have three scans with the following centroids:\n\nScan 0: 10, 20, 30\nScan 1: 10.2, 40.1\nScan 2: 40, 50, 60\n\nWhen comparing consecutive scans and defining the maximum delta mass to be 0.5 find the following connections: (Scan No, Centroid No) -> (Scan No, Centroid No). As we cannot easily store tuples in the matrix, we convert tuple containing the position of the connected centroid to an integer. * (0,0) -> (1,0) -> (3): 10 & 10.2 -> delta = 0.2 * (1,1) -> (2,0) -> (6): 40.1 & 40 -> delta = 0.1\nFinally, we store this in the results matrix:\n\\(\\begin{bmatrix} 3 & -1 & -1 \\\\ -1 & 6 & -1\\\\ -1 & -1 & -1 \\end{bmatrix}\\)\nThe coressponding scores matrix will look as follows:\n\\(\\begin{bmatrix} 0.2 & -1 & -1 \\\\ -1 & 0.1 & -1\\\\ -1 & -1 & -1 \\end{bmatrix}\\)\nThis allows us to not only easily store connections between centroids but also perform a quick lookup for the delta of an existing connection. Note that it also only stores the best connection for each centroid. To extract the connected centroids, we can use np.where(results >= 0). This implementation allows getting millions of connections within seconds.\nAs we are also allowing gaps, refering to that we can have connections between Scan 0 and Scan 2, we make the aforementioned matrix multdimensional, so that e.g. a first matrix stores the conncetions for no gap, the second matrix the connections with a gap of 1.\nThe functionality for this step is implemented in connect_centroids_unidirection and the wrapper find_centroid_connections.\n\nsource\n\n\nfind_centroid_connections\n\n find_centroid_connections (rowwise_peaks:numpy.ndarray,\n                            row_borders:numpy.ndarray,\n                            centroids:numpy.ndarray, max_gap:int,\n                            centroid_tol:float)\n\nWrapper function to call connect_centroids_unidirection\nArgs: rowwise_peaks (np.ndarray): Length of centroids with respect to the row borders. row_borders (np.ndarray): Row borders of the centroids array. centroids (np.ndarray): Array containing the centroids data. max_gap (int): Maximum gap when connecting centroids. centroid_tol (float): Centroid tolerance.\n\nsource\n\n\nconnect_centroids_unidirection\n\n connect_centroids_unidirection (x:numpy.ndarray,\n                                 row_borders:numpy.ndarray,\n                                 connections:numpy.ndarray,\n                                 scores:numpy.ndarray,\n                                 centroids:numpy.ndarray, max_gap:int,\n                                 centroid_tol:float)\n\nConnect centroids.\nArgs: x (np.ndarray): Index to datapoint. Note that this using the performance_function, so one passes an ndarray. row_borders (np.ndarray): Row borders of the centroids array. connections (np.ndarray): Connections matrix to store the connections scores (np.ndarray): Score matrix to store the connections centroids (np.ndarray): 1D Array containing the masses of the centroids data. max_gap (int): Maximum gap when connecting centroids. centroid_tol (float): Centroid tolerance.\nWe wrap the centroid connections in the function connect_centroids. This function converts the connections into an usable array.\n\nsource\n\n\nconnect_centroids\n\n connect_centroids (rowwise_peaks:numpy.ndarray,\n                    row_borders:numpy.ndarray, centroids:numpy.ndarray,\n                    max_gap:int, centroid_tol:float)\n\nFunction to connect centroids.\nArgs: rowwise_peaks (np.ndarray): Indexes for centroids. row_borders (np.ndarray): Row borders (for indexing). centroids (np.ndarray): Centroid data. max_gap: Maximum gap. centroid_tol: Centroid tol for matching centroids. Returns: np.ndarray: From index. np.ndarray: To index. float: Median score. float: Std deviation of the score.\n\nsource\n\n\neliminate_overarching_vertex\n\n eliminate_overarching_vertex (x:numpy.ndarray, from_idx:numpy.ndarray,\n                               to_idx:numpy.ndarray)\n\nEliminate overacrhing vertex.\nArgs: x (np.ndarray): Input index. Note that we are using the performance function so this is a range. from_idx (np.ndarray): From index. to_idx (np.ndarray): To index.\n\nsource\n\n\nconvert_connections_to_array\n\n convert_connections_to_array (x:numpy.ndarray, from_r:numpy.ndarray,\n                               from_c:numpy.ndarray, to_r:numpy.ndarray,\n                               to_c:numpy.ndarray,\n                               row_borders:numpy.ndarray,\n                               out_from_idx:numpy.ndarray,\n                               out_to_idx:numpy.ndarray)\n\nConvert integer indices of a matrix to coordinates.\nArgs: x (np.ndarray): Input index. Note that we are using the performance function so this is a range. from_r (np.ndarray): From array with row coordinates. from_c (np.ndarray): From array with column coordinates. to_r (np.ndarray): To array with row coordinates. to_c (np.ndarray): To array with column coordinates. row_borders (np.ndarray): Row borders (for indexing). out_from_idx (np.ndarray): Reporting array: 1D index from. out_to_idx (np.ndarray): Reporting array: 1D index to.\n\n#Sample snippet to show centroid conncetions\n\nimport matplotlib.pyplot as plt\n\nrow_borders = np.array([3, 6, 9])\nrowwise_peaks = np.array([3, 3, 3])\nmax_gap = 2\n\nscore = np.full((3,3, max_gap), np.inf)\nconnections = np.full((3,3, max_gap), -1)\n\ncentroids = np.array([10, 20, 30, 10.2, 20, 10, 30, 40])\ncentroid_tol = 0.5*1e5\n\nfrom_idx, to_idx, score_median, score_std = connect_centroids(rowwise_peaks, row_borders, centroids, max_gap, centroid_tol)\n\nscan_no = np.array([0, 0, 0, 1, 1, 2, 2, 2])\n\nplt.figure(figsize=(5,5))\nfor i, _ in enumerate(row_borders):\n    ctrd = centroids[_-rowwise_peaks[i]:_]\n    plt.plot(ctrd, np.ones_like(ctrd)*i, 'o')\n    \nfor i, _ in enumerate(from_idx):\n    from_ = _\n    to_ = to_idx[i]\n    plt.plot([centroids[from_], centroids[to_]], [scan_no[from_], scan_no[to_]], 'k:')\n    \nplt.ylabel('scan')\nplt.xlabel('m/z')\nplt.ylim(len(row_borders)+0.5, -1.5)\nplt.title('Peak connections')\nplt.show()\n\n\n\n\n\n\nExtracting hills.\nTo extract hills we extract connected components from the connections.\n\nsource\n\n\nremove_duplicate_hills\n\n remove_duplicate_hills (hill_ptrs, hill_data, path_node_cnt)\n\nRemoves hills that share datapoints. Starts from the largest hills.\n\nsource\n\n\nextract_hills\n\n extract_hills (query_data:dict, max_gap:int, centroid_tol:float)\n\n[summary]\nArgs: query_data (dict): Data structure containing the query data. max_gap (int): Maximum gap when connecting centroids. centroid_tol (float): Centroid tolerance.\nReturns: hill_ptrs (np.ndarray): Array containing the bounds to the hill_data. hill_data (np.ndarray): Array containing the indices to hills. path_node_cnt (int): Number of elements in this path. score_median (float): Median score. score_std (float): Std deviation of the score.\n\nsource\n\n\nget_hills\n\n get_hills (centroids:numpy.ndarray, from_idx:numpy.ndarray,\n            to_idx:numpy.ndarray, hill_length_min:int=3)\n\nFunction to get hills from centroid connections.\nArgs: centroids (np.ndarray): 1D Array containing the masses of the centroids. from_idx (np.ndarray): From index. to_idx (np.ndarray): To index. hill_length_min (int): Minimum hill length:\nReturns: hill_ptrs (np.ndarray): Array containing the bounds to the hill_data. hill_data (np.ndarray): Array containing the indices to hills. path_node_cnt (int): Number of elements in this path.\n\nsource\n\n\nfill_path_matrix\n\n fill_path_matrix (x:numpy.ndarray, path_start:numpy.ndarray,\n                   forwards:numpy.ndarray, out_hill_data:numpy.ndarray,\n                   out_hill_ptr:numpy.ndarray)\n\nFunction to fill the path matrix.\nArgs: x (np.ndarray): Input index. Note that we are using the performance function so this is a range. path_starts (np.ndarray): Array that stores the starts of the paths. forwards (np.ndarray): Forward array. out_hill_data (np.ndarray): Array containing the indices to hills. out_hill_ptr (np.ndarray): Array containing the bounds to out_hill_data.\n\nsource\n\n\nfind_path_length\n\n find_path_length (x:numpy.ndarray, path_starts:numpy.ndarray,\n                   forward:numpy.ndarray, path_cnt:numpy.ndarray)\n\nFunction to extract the length of a path.\nArgs: x (np.ndarray): Input index. Note that we are using the performance function so this is a range. path_starts (np.ndarray): Array that stores the starts of the paths. forward (np.ndarray): Array that stores forward information. path_cnt (np.ndarray): Reporting array to count the paths.\n\nsource\n\n\nfind_path_start\n\n find_path_start (x:numpy.ndarray, forward:numpy.ndarray,\n                  backward:numpy.ndarray, path_starts:numpy.ndarray)\n\nFunction to find the start of a path.\nArgs: x (np.ndarray): Input index. Note that we are using the performance function so this is a range. forward (np.ndarray): Array to report forward connection. backward (np.ndarray): Array to report backward connection. path_starts (np.ndarray): Array to report path starts.\n\nsource\n\n\npath_finder\n\n path_finder (x:numpy.ndarray, from_idx:numpy.ndarray,\n              to_idx:numpy.ndarray, forward:numpy.ndarray,\n              backward:numpy.ndarray)\n\nExtracts path information and writes to path matrix.\nArgs: x (np.ndarray): Input index. Note that we are using the performance function so this is a range. from_idx (np.ndarray): Array containing from indices. to_idx (np.ndarray): Array containing to indices. forward (np.ndarray): Array to report forward connection. backward (np.ndarray): Array to report backward connection.\n\n\nHill Splitting\nWhen having a hill with two or more maxima, we would like to split it at the minimum position. For this, we use a recursive approach. First, the minimum of a hill is detected. A hill is split at this minimum if the smaller of the surrounding maxima is at least the factor hill_split_level larger than the minimum. For each split, the process is repeated.\n\nsource\n\n\nfast_minima\n\n fast_minima (y:numpy.ndarray)\n\nFunction to calculate the local minimas of an array.\nArgs: y (np.ndarray): Input array.\nReturns: np.ndarray: Array containing minima positions.\n\nsource\n\n\nsplit_hills\n\n split_hills (hill_ptrs:numpy.ndarray, hill_data:numpy.ndarray,\n              int_data:numpy.ndarray, hill_split_level:float, window:int)\n\nWrapper function to split hills\nArgs: hill_ptrs (np.ndarray): Array containing the bounds to the hill_data. hill_data (np.ndarray): Array containing the indices to hills. int_data (np.ndarray): Array containing the intensity to each centroid. hill_split_level (float): Split level for hills. window (int): Smoothing window.\nReturns: np.ndarray: Array containing the bounds to the hill_data with splits.\n\nsource\n\n\nsplit\n\n split (k:numpy.ndarray, hill_ptrs:numpy.ndarray, int_data:numpy.ndarray,\n        hill_data:numpy.ndarray, splits:numpy.ndarray,\n        hill_split_level:float, window:int)\n\nFunction to split hills.\nArgs: k (np.ndarray): Input index. Note that we are using the performance function so this is a range. hill_ptrs (np.ndarray): Array containing the bounds to the hill_data. int_data (np.ndarray): Array containing the intensity to each centroid. hill_data (np.ndarray): Array containing the indices to hills. splits (np.ndarray): Array containing splits. hill_split_level (float): Split level for hills. window (int): Smoothing window.\n\n\nFilter Hills\nTo filter hills, we define a minimum length hill_min_length. All peaks below the threshold hill_peak_min_length are accepted as is. For longer hills, the intensity at the start and the end are compared to the maximum intensity. If the ratio of the maximum raw intensity to the smoothed intensity and the beginning and end are larger than hill_peak_factor the hills are accepted.\n\nsource\n\n\nfilter_hills\n\n filter_hills (hill_data:numpy.ndarray, hill_ptrs:numpy.ndarray,\n               int_data:numpy.ndarray, hill_check_large:int=40,\n               window:int=1)\n\nFilters large hills.\nArgs: hill_data (np.ndarray): Array containing the indices to hills. hill_ptrs (np.ndarray): Array containing the bounds to the hill_data. int_data (np.ndarray): Array containing the intensity to each centroid. hill_check_large (int, optional): Length criterion when a hill is considered large.. Defaults to 40. window (int, optional): Smoothing window. Defaults to 1.\nReturns: np.ndarray: Filtered hill data. np.ndarray: Filtered hill points.\n\nsource\n\n\ncheck_large_hills\n\n check_large_hills (idx:numpy.ndarray, large_peaks:numpy.ndarray,\n                    hill_ptrs:numpy.ndarray, hill_data:numpy.ndarray,\n                    int_data:numpy.ndarray, to_remove:numpy.ndarray,\n                    large_peak:int=40, hill_peak_factor:float=2,\n                    window:int=1)\n\nFunction to check large hills and flag them for removal.\nArgs: idx (np.ndarray): Input index. Note that we are using the performance function so this is a range. large_peaks (np.ndarray): Array containing large peaks. hill_ptrs (np.ndarray): Array containing the bounds to the hill_data. hill_data (np.ndarray): Array containing the indices to hills. int_data (np.ndarray): Array containing the intensity to each centroid. to_remove (np.ndarray): Array with indexes to remove. large_peak (int, optional): Length criterion when a peak is large. Defaults to 40. hill_peak_factor (float, optional): Hill maximum criterion. Defaults to 2. window (int, optional): Smoothing window.. Defaults to 1.\nSince the mass estimate min the equation above is more complicated than just an average of the mj, a standard deviation based estimate of the error would not be appropriate. Therefore we calculate the error as a bootstrap2 estimate over B=150 bootstrap replications"
nbs/docs/search.json:    "text": "Python Installation Instructions\n\nRequirements\nWe highly recommend the Anaconda or Miniconda Python distribution, which comes with a powerful package manager. See below for additional instructions for Linux and Mac as they require additional installation of Mono to use the RawFileReader.\nAlphaPept can be used as an application as a whole or as a Python Package where individual modules are called. Depending on the use case, AlphaPept will need different requirements, and you might not want to install all of them.\nCurrently, we have the default requirements.txt, additional requirements to run the GUI gui and packages used for developing develop.\nTherefore, you can install AlphaPept in multiple ways:\n\nThe default alphapept\nWith GUI-packages alphapept[gui]\nWith pacakges for development alphapept[develop] (alphapept[develop,gui]) respectively\n\nThe requirements typically contain pinned versions and will be automatically upgraded and tested with dependabot. This stable version allows having a reproducible workflow. However, in order to avoid conflicts with package versions that are too strict, the requirements are not pinned when being installed. To use the strict version use the -stable-flag, e.g. alphapept[stable].\nFor end-users that want to set up a processing environment in Python, the \"alphapept[stable,gui-stable]\" is the batteries-included-version that you want to use.\n\n\nPython\nIt is strongly recommended to install AlphaPept in its own environment. 1. Open the console and create a new conda environment: conda create --name alphapept python=3.8 2. Activate the environment: conda activate alphapept 3. Install AlphaPept via pip: pip install \"alphapept[stable,gui-stable]\". If you want to use AlphaPept as a package without the GUI dependencies and without strict version dependencies, use pip install alphapept.\nIf AlphaPept is installed correctly, you should be able to import AlphaPept as a package within the environment; see below.\n\n\nLinux\n\nInstall the build-essentials: sudo apt-get install build-essential.\nInstall AlphaPept via pip: pip install \"alphapept[stable,gui-stable]\". If you want to use AlphaPept as a package withouth the GUI dependencies and strict version dependencies use pip install alphapept.\nInstall libgomp.1 with sudo apt-get install libgomp1.\n\n\nBruker Support\n\nCopy-paste the Bruker library for feature finding to your /usr/lib folder with sudo cp alphapept/ext/bruker/FF/linux64/alphapeptlibtbb.so.2 /usr/lib/libtbb.so.2.\n\n\n\nThermo Support\n\nInstall Mono from mono-project website Mono Linux. NOTE, the installed mono version should be at least 6.10, which requires you to add the ppa to your trusted sources!\nInstall pythonnet with pip install pythonnet>=2.5.2\n\n\n\n\n\nMac\n\nInstall AlphaPept via pip: pip install \"alphapept[stable,gui-stable]\". If you want to use AlphaPept as a package withouth the GUI dependencies and strict version dependencies use pip install alphapept.\n\n\nBruker Support\n\nOnly supported for preprocessed files.\n\n\n\nThermo Support\n\nInstall brew and pkg-config: brew install pkg-config\nInstall Mono from mono-project website Mono Mac\nRegister the Mono-Path to your system: For macOS Catalina, open the configuration of zsh via the terminal:\n\n\nType in cd to navigate to the home directory.\nType nano ~/.zshrc to open the configuration of the terminal\nAdd the path to your mono installation: export PKG_CONFIG_PATH=/usr/local/lib/pkgconfig:/usr/lib/pkgconfig:/Library/Frameworks/Mono.framework/Versions/Current/lib/pkgconfig:$PKG_CONFIG_PATH. Make sure that the Path matches to your version (Here 6.12.0)\nSave everything and execute . ~/.zshrc\n\n\nInstall pythonnet with pip install pythonnet>=2.5.2\n\n\n\n\n\nDeveloper\n\nRedirect to the folder of choice and clone the repository: git clone https://github.com/MannLabs/alphapept.git\nNavigate to the alphapept folder with cd alphapept and install the package with pip install . (default users) or with pip install -e . to enable developers mode. Note that you can use the different requirements here aswell (e.g. pip install \".[gui-stable]\")\n\n\n\nGPU Support\nSome functionality of AlphaPept is GPU optimized that uses Nvidia’s CUDA. To enable this, additional packages need to be installed.\n\nMake sure to have a working CUDA toolkit installation that is compatible with CuPy. To check type nvcc --version in your terminal.\nInstall cupy. Make sure to install the cupy version matching your CUDA toolkit (e.g. pip install cupy-cuda110 for CUDA toolkit 11.0.\n\n\n\n\nAdditional Notes\n\nTo access Thermo files, we have integrated RawFileReader into AlphaPept. We rely on Mono for Linux/Mac systems.\n\n\nTo access Bruker files, we rely on the timsdata-library. Currently, only Windows is supported. For feature finding, we use the Bruker Feature Finder, which can be found in the ext folder of this repository.\n\n\nNotes for NBDEV\n\nFor developing with the notebooks, install the nbdev package (see the development requirements)\nTo facilitate navigating the notebooks, use jupyter notebook extensions. They can be called from a running jupyter instance like so: http://localhost:8888/nbextensions. The extensions collapsible headings and toc2 are very beneficial."
nbs/docs/search.json:    "text": "Notebooks\nWithin the notebooks, we try to cover most aspects of a proteomics workflow:\n\nSettings: General settings to define a workflow\nChem: Chemistry related functions, e.g., for calculating isotope distributions\nInput / Output: Everything related to importing and exporting and the file formats used\nFASTA: Generating theoretical databases from FASTA files\nFeature Finding: How to extract MS1 features for quantification\nSearch: Comparing theoretical databases to experimental spectra and getting Peptide-Spectrum-Matches (PSMs)\nScore: Scoring PSMs\nRecalibration: Recalibration of data based on identified peptides\nQuantification: Functions for quantification, e.g., LFQ\nMatching: Functions for Match-between-runs\nConstants: A collection of constants\nInterface: Code that generates the command-line-interface (CLI) and makes workflow steps callable\nPerformance: Helper functions to speed up code with CPU / GPU\nExport: Helper functions to make exports compatbile to other Software tools\nLabel: Code for support isobaric label search\nDisplay: Code related to displaying in the streamlit gui\nAdditional code: Overview of additional code not covered by the notebooks\nHow to contribute: Contribution guidelines\nAlphaPept workflow and files: Overview of the worfklow, files and column names"
nbs/docs/site_libs/bootstrap/bootstrap-icons.css:.bi-gpu-card::before { content: "\f6e2"; }

```
