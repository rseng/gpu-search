# https://github.com/glotzerlab/hoomd-blue

```console
CMake/hoomd/HOOMDCUDASetup.cmake:option(ALWAYS_USE_MANAGED_MEMORY "Use CUDA managed memory also when running on single GPU" OFF)
CMake/hoomd/HOOMDCUDASetup.cmake:# setup CUDA compile options
CMake/hoomd/HOOMDCUDASetup.cmake:        set(CMAKE_CUDA_FLAGS "${CMAKE_CUDA_FLAGS} -Xcudafe --diag_suppress=code_is_unreachable -Xcompiler=-Wall -Xcompiler=-Wconversion -Xcompiler=-Wno-attributes")
CMake/hoomd/HOOMDCUDASetup.cmake:        # setup nvcc to build for all CUDA architectures. Allow user to modify the list if desired
CMake/hoomd/HOOMDCUDASetup.cmake:        if (CMAKE_CUDA_COMPILER_VERSION VERSION_GREATER_EQUAL 11.0)
CMake/hoomd/HOOMDCUDASetup.cmake:            set(CUDA_ARCH_LIST 60 70 80 CACHE STRING "List of target sm_ architectures to compile CUDA code for. Separate with semicolons.")
CMake/hoomd/HOOMDCUDASetup.cmake:        elseif (CMAKE_CUDA_COMPILER_VERSION VERSION_GREATER_EQUAL 9.0)
CMake/hoomd/HOOMDCUDASetup.cmake:            set(CUDA_ARCH_LIST 60 70 CACHE STRING "List of target sm_ architectures to compile CUDA code for. Separate with semicolons.")
CMake/hoomd/HOOMDCUDASetup.cmake:        elseif (CMAKE_CUDA_COMPILER_VERSION VERSION_GREATER_EQUAL 8.0)
CMake/hoomd/HOOMDCUDASetup.cmake:            set(CUDA_ARCH_LIST 60 CACHE STRING "List of target sm_ architectures to compile CUDA code for. Separate with semicolons.")
CMake/hoomd/HOOMDCUDASetup.cmake:        if (CMAKE_CUDA_COMPILER_VERSION VERSION_GREATER_EQUAL 11.2)
CMake/hoomd/HOOMDCUDASetup.cmake:          set(CMAKE_CUDA_FLAGS "${CMAKE_CUDA_FLAGS} -DCUSPARSE_NEW_API")
CMake/hoomd/HOOMDCUDASetup.cmake:        # need to know the minimum supported CUDA_ARCH
CMake/hoomd/HOOMDCUDASetup.cmake:        set(_cuda_arch_list_sorted ${CUDA_ARCH_LIST})
CMake/hoomd/HOOMDCUDASetup.cmake:        list(SORT _cuda_arch_list_sorted)
CMake/hoomd/HOOMDCUDASetup.cmake:        list(GET _cuda_arch_list_sorted 0 _cuda_min_arch)
CMake/hoomd/HOOMDCUDASetup.cmake:        list(GET _cuda_arch_list_sorted -1 _cuda_max_arch)
CMake/hoomd/HOOMDCUDASetup.cmake:        if (_cuda_min_arch LESS 60)
CMake/hoomd/HOOMDCUDASetup.cmake:        # only generate ptx code for the maximum supported CUDA_ARCH (saves on file size)
CMake/hoomd/HOOMDCUDASetup.cmake:        list(REVERSE _cuda_arch_list_sorted)
CMake/hoomd/HOOMDCUDASetup.cmake:        list(GET _cuda_arch_list_sorted 0 _cuda_max_arch)
CMake/hoomd/HOOMDCUDASetup.cmake:            # CMAKE 3.18 handles CUDA ARCHITECTURES with CMAKE_CUDA_ARCHITECTURES
CMake/hoomd/HOOMDCUDASetup.cmake:            set(CMAKE_CUDA_ARCHITECTURES "")
CMake/hoomd/HOOMDCUDASetup.cmake:            foreach(_cuda_arch ${CUDA_ARCH_LIST})
CMake/hoomd/HOOMDCUDASetup.cmake:                list(APPEND CMAKE_CUDA_ARCHITECTURES "${_cuda_arch}-real")
CMake/hoomd/HOOMDCUDASetup.cmake:            list(APPEND CMAKE_CUDA_ARCHITECTURES "${_cuda_max_arch}-virtual")
CMake/hoomd/HOOMDCUDASetup.cmake:            foreach(_cuda_arch ${CUDA_ARCH_LIST})
CMake/hoomd/HOOMDCUDASetup.cmake:                set(CMAKE_CUDA_FLAGS "${CMAKE_CUDA_FLAGS} -gencode=arch=compute_${_cuda_arch},code=sm_${_cuda_arch}")
CMake/hoomd/HOOMDCUDASetup.cmake:            set(CMAKE_CUDA_FLAGS "${CMAKE_CUDA_FLAGS} -gencode=arch=compute_${_cuda_max_arch},code=compute_${_cuda_max_arch}")
CMake/hoomd/HOOMDCUDASetup.cmake:        set(_cuda_min_arch 35)
CMake/hoomd/HOOMDCUDASetup.cmake:# set CUSOLVER_AVAILABLE depending on CUDA Toolkit version
CMake/hoomd/HOOMDCUDASetup.cmake:    # CUDA 8.0 requires that libgomp be linked in - see if we can link it
CMake/hoomd/HOOMDCUDASetup.cmake:    if (NOT ${CUDA_cusolver_LIBRARY} STREQUAL "" AND _can_link_gomp)
CMake/hoomd/HOOMDHIPSetup.cmake:    if (HOOMD_GPU_PLATFORM STREQUAL "HIP")
CMake/hoomd/HOOMDHIPSetup.cmake:        # setup nvcc to build for all CUDA architectures. Allow user to modify the list if desired
CMake/hoomd/HOOMDHIPSetup.cmake:        set(CMAKE_HIP_ARCHITECTURES gfx900 gfx906 gfx908 gfx90a CACHE STRING "List of AMD GPU to compile HIP code for. Separate with semicolons.")
CMake/hoomd/HOOMDHIPSetup.cmake:    elseif (HOOMD_GPU_PLATFORM STREQUAL "CUDA")
CMake/hoomd/HOOMDHIPSetup.cmake:        # here we go if hipcc is not available, fall back on internal HIP->CUDA headers
CMake/hoomd/HOOMDHIPSetup.cmake:        ENABLE_LANGUAGE(CUDA)
CMake/hoomd/HOOMDHIPSetup.cmake:        SET(HOOMD_DEVICE_LANGUAGE CUDA)
CMake/hoomd/HOOMDHIPSetup.cmake:        # use CUDA runtime version
CMake/hoomd/HOOMDHIPSetup.cmake:        string(REGEX MATCH "([0-9]*).([0-9]*).([0-9]*).*" _hip_version_match "${CMAKE_CUDA_COMPILER_VERSION}")
CMake/hoomd/HOOMDHIPSetup.cmake:        # Use system provided CUB for CUDA 11 and newer
CMake/hoomd/HOOMDHIPSetup.cmake:        message(FATAL_ERROR "HOOMD_GPU_PLATFORM must be either CUDA or HIP")
CMake/hoomd/HOOMDHIPSetup.cmake:        # set HIP_VERSION_* on non-CUDA targets (the version is already defined on AMD targets through hipcc)
CMake/hoomd/HOOMDHIPSetup.cmake:            $<$<NOT:$<COMPILE_LANGUAGE:CUDA>>:HIP_VERSION_MAJOR=${HIP_VERSION_MAJOR}>)
CMake/hoomd/HOOMDHIPSetup.cmake:            $<$<NOT:$<COMPILE_LANGUAGE:CUDA>>:HIP_VERSION_MINOR=${HIP_VERSION_MINOR}>)
CMake/hoomd/HOOMDHIPSetup.cmake:            $<$<NOT:$<COMPILE_LANGUAGE:CUDA>>:HIP_VERSION_PATCH=${HIP_VERSION_PATCH}>)
CMake/hoomd/HOOMDHIPSetup.cmake:    find_package(CUDALibs REQUIRED)
CMake/hoomd/HOOMDMPISetup.cmake:    string(REPLACE "-pthread" "$<$<AND:$<COMPILE_LANGUAGE:CUDA>,$<STREQUAL:${HIP_PLATFORM},nvcc>>:-Xcompiler>;-pthread"
CMake/hoomd/HOOMDMPISetup.cmake:    string(REPLACE "-pthread" "$<$<AND:$<COMPILE_LANGUAGE:CUDA>,$<STREQUAL:${HIP_PLATFORM},nvcc>>:-Xcompiler>;-pthread"
CMake/hoomd/FindCUDALibs.cmake:# Find CUDA libraries and binaries used by HOOMD
CMake/hoomd/FindCUDALibs.cmake:set(REQUIRED_CUDA_LIB_VARS "")
CMake/hoomd/FindCUDALibs.cmake:    # find CUDA library path
CMake/hoomd/FindCUDALibs.cmake:    get_filename_component(CUDA_BIN_PATH ${CMAKE_CUDA_COMPILER} DIRECTORY)
CMake/hoomd/FindCUDALibs.cmake:    get_filename_component(CUDA_LIB_PATH "${CUDA_BIN_PATH}/../lib64/" ABSOLUTE)
CMake/hoomd/FindCUDALibs.cmake:    find_library(CUDA_cudart_LIBRARY cudart HINTS ${CUDA_LIB_PATH})
CMake/hoomd/FindCUDALibs.cmake:    mark_as_advanced(CUDA_cudart_LIBRARY)
CMake/hoomd/FindCUDALibs.cmake:    if(CUDA_cudart_LIBRARY AND NOT TARGET CUDA::cudart)
CMake/hoomd/FindCUDALibs.cmake:      add_library(CUDA::cudart UNKNOWN IMPORTED)
CMake/hoomd/FindCUDALibs.cmake:      set_target_properties(CUDA::cudart PROPERTIES
CMake/hoomd/FindCUDALibs.cmake:        IMPORTED_LOCATION "${CUDA_cudart_LIBRARY}"
CMake/hoomd/FindCUDALibs.cmake:        INTERFACE_INCLUDE_DIRECTORIES "${CMAKE_CUDA_TOOLKIT_INCLUDE_DIRECTORIES}"
CMake/hoomd/FindCUDALibs.cmake:    find_package_message(CUDALibsCUDART "Found cudart: ${CUDA_cudart_LIBRARY}" "[${CUDA_cudart_LIBRARY}]")
CMake/hoomd/FindCUDALibs.cmake:    list(APPEND REQUIRED_CUDA_LIB_VARS "CUDA_cudart_LIBRARY")
CMake/hoomd/FindCUDALibs.cmake:    add_library(CUDA::cudart UNKNOWN IMPORTED)
CMake/hoomd/FindCUDALibs.cmake:    find_library(CUDA_cudadevrt_LIBRARY cudadevrt HINTS ${CUDA_LIB_PATH})
CMake/hoomd/FindCUDALibs.cmake:    mark_as_advanced(CUDA_cudadevrt_LIBRARY)
CMake/hoomd/FindCUDALibs.cmake:    if(CUDA_cudadevrt_LIBRARY AND NOT TARGET CUDA::cudadevrt)
CMake/hoomd/FindCUDALibs.cmake:      add_library(CUDA::cudadevrt UNKNOWN IMPORTED)
CMake/hoomd/FindCUDALibs.cmake:      set_target_properties(CUDA::cudadevrt PROPERTIES
CMake/hoomd/FindCUDALibs.cmake:        IMPORTED_LOCATION "${CUDA_cudadevrt_LIBRARY}"
CMake/hoomd/FindCUDALibs.cmake:        INTERFACE_INCLUDE_DIRECTORIES "${CMAKE_CUDA_TOOLKIT_INCLUDE_DIRECTORIES}"
CMake/hoomd/FindCUDALibs.cmake:    add_library(CUDA::cudadevrt UNKNOWN IMPORTED)
CMake/hoomd/FindCUDALibs.cmake:    find_library(CUDA_nvrtc_LIBRARY nvrtc HINTS ${CUDA_LIB_PATH})
CMake/hoomd/FindCUDALibs.cmake:    mark_as_advanced(CUDA_nvrtc_LIBRARY)
CMake/hoomd/FindCUDALibs.cmake:    if(CUDA_nvrtc_LIBRARY AND NOT TARGET CUDA::nvrtc)
CMake/hoomd/FindCUDALibs.cmake:      add_library(CUDA::nvrtc UNKNOWN IMPORTED)
CMake/hoomd/FindCUDALibs.cmake:      set_target_properties(CUDA::nvrtc PROPERTIES
CMake/hoomd/FindCUDALibs.cmake:        IMPORTED_LOCATION "${CUDA_nvrtc_LIBRARY}"
CMake/hoomd/FindCUDALibs.cmake:        INTERFACE_INCLUDE_DIRECTORIES "${CMAKE_CUDA_TOOLKIT_INCLUDE_DIRECTORIES}"
CMake/hoomd/FindCUDALibs.cmake:    find_package_message(CUDALibsNVRTC "Found nvrtc: ${CUDA_nvrtc_LIBRARY}" "[${CUDA_nvrtc_LIBRARY}]")
CMake/hoomd/FindCUDALibs.cmake:    list(APPEND REQUIRED_CUDA_LIB_VARS CUDA_nvrtc_LIBRARY)
CMake/hoomd/FindCUDALibs.cmake:    add_library(CUDA::nvrtc UNKNOWN IMPORTED)
CMake/hoomd/FindCUDALibs.cmake:    find_library(CUDA_cuda_LIBRARY cuda HINTS ${CUDA_LIB_PATH}
CMake/hoomd/FindCUDALibs.cmake:                 /usr/local/cuda-10.1/compat
CMake/hoomd/FindCUDALibs.cmake:                 /usr/local/cuda-10.2/compat
CMake/hoomd/FindCUDALibs.cmake:                 /usr/local/cuda-11.1/compat
CMake/hoomd/FindCUDALibs.cmake:                 /usr/local/cuda-11.2/compat
CMake/hoomd/FindCUDALibs.cmake:                 /usr/local/cuda-11.3/compat
CMake/hoomd/FindCUDALibs.cmake:                 /usr/local/cuda-11.4/compat
CMake/hoomd/FindCUDALibs.cmake:                 /usr/local/cuda-11.5/compat
CMake/hoomd/FindCUDALibs.cmake:                 /usr/local/cuda-11.6/compat
CMake/hoomd/FindCUDALibs.cmake:                 /usr/local/cuda-11.7/compat
CMake/hoomd/FindCUDALibs.cmake:                 /usr/local/cuda-11.8/compat
CMake/hoomd/FindCUDALibs.cmake:                 /usr/local/cuda-11.9/compat
CMake/hoomd/FindCUDALibs.cmake:                 /usr/local/cuda-11.10/compat
CMake/hoomd/FindCUDALibs.cmake:                 /usr/local/cuda-12.0/compat
CMake/hoomd/FindCUDALibs.cmake:                 /usr/local/cuda-12.1/compat
CMake/hoomd/FindCUDALibs.cmake:                 /usr/local/cuda-12.2/compat
CMake/hoomd/FindCUDALibs.cmake:                 /usr/local/cuda-12.3/compat
CMake/hoomd/FindCUDALibs.cmake:                 /usr/local/cuda-12.4/compat
CMake/hoomd/FindCUDALibs.cmake:                 /usr/local/cuda-12.5/compat
CMake/hoomd/FindCUDALibs.cmake:                 /usr/local/cuda-12.6/compat
CMake/hoomd/FindCUDALibs.cmake:                 /usr/local/cuda-12.7/compat
CMake/hoomd/FindCUDALibs.cmake:                 /usr/local/cuda-12.8/compat
CMake/hoomd/FindCUDALibs.cmake:                 /usr/local/cuda-12.9/compat
CMake/hoomd/FindCUDALibs.cmake:                 /usr/local/cuda-12.10/compat)
CMake/hoomd/FindCUDALibs.cmake:    mark_as_advanced(CUDA_cuda_LIBRARY)
CMake/hoomd/FindCUDALibs.cmake:    if(CUDA_cuda_LIBRARY AND NOT TARGET CUDA::cuda)
CMake/hoomd/FindCUDALibs.cmake:      add_library(CUDA::cuda UNKNOWN IMPORTED)
CMake/hoomd/FindCUDALibs.cmake:      set_target_properties(CUDA::cuda PROPERTIES
CMake/hoomd/FindCUDALibs.cmake:        IMPORTED_LOCATION "${CUDA_cuda_LIBRARY}"
CMake/hoomd/FindCUDALibs.cmake:        INTERFACE_INCLUDE_DIRECTORIES "${CMAKE_CUDA_TOOLKIT_INCLUDE_DIRECTORIES}"
CMake/hoomd/FindCUDALibs.cmake:    find_package_message(CUDALibsCUDA "Found cuda: ${CUDA_cuda_LIBRARY}" "[${CUDA_cuda_LIBRARY}]")
CMake/hoomd/FindCUDALibs.cmake:    list(APPEND REQUIRED_CUDA_LIB_VARS "CUDA_cuda_LIBRARY")
CMake/hoomd/FindCUDALibs.cmake:    add_library(CUDA::cuda UNKNOWN IMPORTED)
CMake/hoomd/FindCUDALibs.cmake:    find_library(CUDA_cufft_LIBRARY cufft HINTS ${CUDA_LIB_PATH})
CMake/hoomd/FindCUDALibs.cmake:    mark_as_advanced(CUDA_cufft_LIBRARY)
CMake/hoomd/FindCUDALibs.cmake:    if(CUDA_cufft_LIBRARY AND NOT TARGET CUDA::cufft)
CMake/hoomd/FindCUDALibs.cmake:      add_library(CUDA::cufft UNKNOWN IMPORTED)
CMake/hoomd/FindCUDALibs.cmake:      set_target_properties(CUDA::cufft PROPERTIES
CMake/hoomd/FindCUDALibs.cmake:        IMPORTED_LOCATION "${CUDA_cufft_LIBRARY}"
CMake/hoomd/FindCUDALibs.cmake:        INTERFACE_INCLUDE_DIRECTORIES "${CMAKE_CUDA_TOOLKIT_INCLUDE_DIRECTORIES}"
CMake/hoomd/FindCUDALibs.cmake:    find_package_message(CUDALibsCUFFT "Found cufft: ${CUDA_cufft_LIBRARY}" "[${CUDA_cufft_LIBRARY}]")
CMake/hoomd/FindCUDALibs.cmake:    list(APPEND REQUIRED_CUDA_LIB_VARS CUDA_cufft_LIBRARY)
CMake/hoomd/FindCUDALibs.cmake:    add_library(CUDA::cufft UNKNOWN IMPORTED)
CMake/hoomd/FindCUDALibs.cmake:    find_library(CUDA_nvToolsExt_LIBRARY nvToolsExt HINTS ${CUDA_LIB_PATH})
CMake/hoomd/FindCUDALibs.cmake:    mark_as_advanced(CUDA_nvToolsExt_LIBRARY)
CMake/hoomd/FindCUDALibs.cmake:    if(CUDA_nvToolsExt_LIBRARY AND NOT TARGET CUDA::nvToolsExt)
CMake/hoomd/FindCUDALibs.cmake:      add_library(CUDA::nvToolsExt UNKNOWN IMPORTED)
CMake/hoomd/FindCUDALibs.cmake:      set_target_properties(CUDA::nvToolsExt PROPERTIES
CMake/hoomd/FindCUDALibs.cmake:        IMPORTED_LOCATION "${CUDA_nvToolsExt_LIBRARY}"
CMake/hoomd/FindCUDALibs.cmake:        INTERFACE_INCLUDE_DIRECTORIES "${CMAKE_CUDA_TOOLKIT_INCLUDE_DIRECTORIES}"
CMake/hoomd/FindCUDALibs.cmake:    find_package_message(CUDALibsNVToolsExt "Found nvToolsExt: ${CUDA_nvToolsExt_LIBRARY}" "$[{CUDA_nvToolsExt_LIBRARY}]")
CMake/hoomd/FindCUDALibs.cmake:    add_library(CUDA::nvToolsExt UNKNOWN IMPORTED)
CMake/hoomd/FindCUDALibs.cmake:    find_library(CUDA_cusolver_LIBRARY cusolver HINTS ${CUDA_LIB_PATH})
CMake/hoomd/FindCUDALibs.cmake:    mark_as_advanced(CUDA_cusolver_LIBRARY)
CMake/hoomd/FindCUDALibs.cmake:    if(CUDA_cusolver_LIBRARY AND NOT TARGET CUDA::cusolver)
CMake/hoomd/FindCUDALibs.cmake:      add_library(CUDA::cusolver UNKNOWN IMPORTED)
CMake/hoomd/FindCUDALibs.cmake:      set_target_properties(CUDA::cusolver PROPERTIES
CMake/hoomd/FindCUDALibs.cmake:        IMPORTED_LOCATION "${CUDA_cusolver_LIBRARY}"
CMake/hoomd/FindCUDALibs.cmake:        INTERFACE_INCLUDE_DIRECTORIES "${CMAKE_CUDA_TOOLKIT_INCLUDE_DIRECTORIES}"
CMake/hoomd/FindCUDALibs.cmake:    find_package_message(CUDALibsCUSolver "Found cusolver: ${CUDA_cusolver_LIBRARY}" "[${CUDA_cusolver_LIBRARY}]")
CMake/hoomd/FindCUDALibs.cmake:    list(APPEND REQUIRED_CUDA_LIB_VARS CUDA_cusolver_LIBRARY)
CMake/hoomd/FindCUDALibs.cmake:    add_library(CUDA::cusolver UNKNOWN IMPORTED)
CMake/hoomd/FindCUDALibs.cmake:    find_library(CUDA_cusparse_LIBRARY cusparse HINTS ${CUDA_LIB_PATH})
CMake/hoomd/FindCUDALibs.cmake:    mark_as_advanced(CUDA_cusparse_LIBRARY)
CMake/hoomd/FindCUDALibs.cmake:    if(CUDA_cusparse_LIBRARY AND NOT TARGET CUDA::cusparse)
CMake/hoomd/FindCUDALibs.cmake:      add_library(CUDA::cusparse UNKNOWN IMPORTED)
CMake/hoomd/FindCUDALibs.cmake:      set_target_properties(CUDA::cusparse PROPERTIES
CMake/hoomd/FindCUDALibs.cmake:        IMPORTED_LOCATION "${CUDA_cusparse_LIBRARY}"
CMake/hoomd/FindCUDALibs.cmake:        INTERFACE_INCLUDE_DIRECTORIES "${CMAKE_CUDA_TOOLKIT_INCLUDE_DIRECTORIES}"
CMake/hoomd/FindCUDALibs.cmake:    find_package_message(CUDALibsCUSparse "Found cusparse: ${CUDA_cusparse_LIBRARY}" "[${CUDA_cusparse_LIBRARY}]")
CMake/hoomd/FindCUDALibs.cmake:    list(APPEND REQUIRED_CUDA_LIB_VARS CUDA_cusparse_LIBRARY)
CMake/hoomd/FindCUDALibs.cmake:    add_library(CUDA::cusparse UNKNOWN IMPORTED)
CMake/hoomd/FindCUDALibs.cmake:    # find compute-sanitizer / cuda-memcheck
CMake/hoomd/FindCUDALibs.cmake:    find_program(CUDA_MEMCHECK_EXECUTABLE
CMake/hoomd/FindCUDALibs.cmake:      HINTS "${CUDA_BIN_PATH}" "${CUDA_BIN_PATH}../../extras/compute-sanitizer"
CMake/hoomd/FindCUDALibs.cmake:    # Fall back on cuda-memcheck when compute-sanitizer is not available (CUDA 10)
CMake/hoomd/FindCUDALibs.cmake:    if (NOT CUDA_MEMCHECK_EXECUTABLE)
CMake/hoomd/FindCUDALibs.cmake:        find_program(CUDA_MEMCHECK_EXECUTABLE
CMake/hoomd/FindCUDALibs.cmake:          NAMES cuda-memcheck
CMake/hoomd/FindCUDALibs.cmake:          HINTS "${CUDA_BIN_PATH}"
CMake/hoomd/FindCUDALibs.cmake:    find_package_message(CUDALibsMemcheck "Found compute-sanitizer: ${CUDA_MEMCHECK_EXECUTABLE}" "[${CUDA_MEMCHECK_EXECUTABLE}]")
CMake/hoomd/FindCUDALibs.cmake:    mark_as_advanced(CUDA_MEMCHECK_EXECUTABLE)
CMake/hoomd/FindCUDALibs.cmake:    find_package_handle_standard_args(CUDALibs
CMake/hoomd/FindCUDALibs.cmake:        ${REQUIRED_CUDA_LIB_VARS}
CMake/hoomd/hoomd-macros.cmake:    set_target_properties(${target_name} PROPERTIES CUDA_VISIBILITY_PRESET "hidden")
.pre-commit-config.yaml:    types_or: [python, c, c++, cuda, inc]
.pre-commit-config.yaml:    types_or: [python, c, c++, cuda, inc, rst]
.pre-commit-config.yaml:    types_or: [c, c++, cuda, inc]
hoomd/SFCPackTuner.h:#include "GPUVector.h"
hoomd/SFCPackTuner.h:    GPUArray<unsigned int> m_traversal_order; //!< Generated traversal order of bins
hoomd/BondedGroupData.cu:    \brief Implements the helper functions (GPU version) for updating the GPU bonded group tables
hoomd/BondedGroupData.cu:__global__ void gpu_count_groups_kernel(const unsigned int n_groups,
hoomd/BondedGroupData.cu:__global__ void gpu_group_scatter_kernel(unsigned int n_scratch,
hoomd/BondedGroupData.cu:void gpu_update_group_table(const unsigned int n_groups,
hoomd/BondedGroupData.cu:    hipLaunchKernelGGL(HIP_KERNEL_NAME(gpu_count_groups_kernel<group_size>),
hoomd/BondedGroupData.cu:        thrust::sort_by_key(thrust::cuda::par(alloc),
hoomd/BondedGroupData.cu:        thrust::exclusive_scan_by_key(thrust::cuda::par(alloc),
hoomd/BondedGroupData.cu:        hipLaunchKernelGGL(gpu_group_scatter_kernel<group_size>,
hoomd/BondedGroupData.cu:template void gpu_update_group_table<2>(const unsigned int n_groups,
hoomd/BondedGroupData.cu:template void gpu_update_group_table<3>(const unsigned int n_groups,
hoomd/BondedGroupData.cu:template void gpu_update_group_table<4>(const unsigned int n_groups,
hoomd/BondedGroupData.cu:template void gpu_update_group_table<6>(const unsigned int n_groups,
hoomd/BoxResizeUpdaterGPU.cu:#include "BoxResizeUpdaterGPU.cuh"
hoomd/BoxResizeUpdaterGPU.cu:__global__ void gpu_box_resize_scale_kernel(Scalar4* d_pos,
hoomd/BoxResizeUpdaterGPU.cu:gpu_box_resize_wrap_kernel(unsigned int N, Scalar4* d_pos, int3* d_image, const BoxDim new_box)
hoomd/BoxResizeUpdaterGPU.cu:hipError_t gpu_box_resize_scale(Scalar4* d_pos,
hoomd/BoxResizeUpdaterGPU.cu:    hipFuncGetAttributes(&attr, (const void*)gpu_box_resize_wrap_kernel);
hoomd/BoxResizeUpdaterGPU.cu:    hipLaunchKernelGGL((gpu_box_resize_scale_kernel),
hoomd/BoxResizeUpdaterGPU.cu:hipError_t gpu_box_resize_wrap(const unsigned int N,
hoomd/BoxResizeUpdaterGPU.cu:    hipFuncGetAttributes(&attr, (const void*)gpu_box_resize_wrap_kernel);
hoomd/BoxResizeUpdaterGPU.cu:    hipLaunchKernelGGL((gpu_box_resize_wrap_kernel),
hoomd/SFCPackTunerGPU.h:/*! \file SFCPackTunerGPU.h
hoomd/SFCPackTunerGPU.h:    \brief Declares the SFCPackTunerGPU class
hoomd/SFCPackTunerGPU.h:#include "GPUArray.h"
hoomd/SFCPackTunerGPU.h:#include "SFCPackTunerGPU.cuh"
hoomd/SFCPackTunerGPU.h:#ifndef __SFCPACK_UPDATER_GPU_H__
hoomd/SFCPackTunerGPU.h:#define __SFCPACK_UPDATER_GPU_H__
hoomd/SFCPackTunerGPU.h:/*! GPU implementation of SFCPackTuner
hoomd/SFCPackTunerGPU.h:class PYBIND11_EXPORT SFCPackTunerGPU : public SFCPackTuner
hoomd/SFCPackTunerGPU.h:    SFCPackTunerGPU(std::shared_ptr<SystemDefinition> sysdef, std::shared_ptr<Trigger> trigger);
hoomd/SFCPackTunerGPU.h:    virtual ~SFCPackTunerGPU();
hoomd/SFCPackTunerGPU.h:    GlobalArray<unsigned int> m_gpu_particle_bins; //!< Particle bins
hoomd/SFCPackTunerGPU.h:    GlobalArray<unsigned int> m_gpu_sort_order;    //!< Generated sort order of the particles
hoomd/SFCPackTunerGPU.h://! Export the SFCPackTunerGPU class to python
hoomd/SFCPackTunerGPU.h:void export_SFCPackTunerGPU(pybind11::module& m);
hoomd/SFCPackTunerGPU.h:#endif // __SFC_PACK_UPDATER_GPU_H_
hoomd/ParticleData.cuh:#include "GPUPartition.cuh"
hoomd/ParticleData.cuh:    \brief Declares GPU kernel code and data structure functions used by ParticleData
hoomd/ParticleData.cuh:unsigned int gpu_pdata_remove(const unsigned int N,
hoomd/ParticleData.cuh:                              GPUPartition& gpu_partition);
hoomd/ParticleData.cuh:void gpu_pdata_add_particles(const unsigned int old_nparticles,
hoomd/device.py:.. skip: next if(gpu_not_available)
hoomd/device.py:    device = hoomd.device.GPU()
hoomd/device.py:    to creating each `Device`, especially on the GPU.
hoomd/device.py:    Provides methods and properties common to `CPU` and `GPU`, including those
hoomd/device.py:        `Device` cannot be used directly. Instantate a `CPU` or `GPU` object.
hoomd/device.py:class GPU(Device):
hoomd/device.py:    """Select a GPU or GPU(s) to execute simulations.
hoomd/device.py:        gpu_ids (list[int]): List of GPU ids to use. Set to `None` to let the
hoomd/device.py:            driver auto-select a GPU.
hoomd/device.py:                Use ``gpu_id``.
hoomd/device.py:        gpu_id (int): GPU id to use. Set to `None` to let the driver auto-select
hoomd/device.py:            a GPU.
hoomd/device.py:        Call `GPU.get_available_devices` to get a human readable list of
hoomd/device.py:        devices. ``gpu_id = 0`` will select the first device in this list,
hoomd/device.py:        The ordering of the devices is determined by the GPU driver and runtime.
hoomd/device.py:    When ``gpu_id`` is `None`, HOOMD will ask the GPU driver to auto-select a
hoomd/device.py:    GPU. In most cases, this will select device 0. When all devices are set to a
hoomd/device.py:    compute exclusive mode, the driver will choose a free GPU.
hoomd/device.py:    In MPI execution environments, create a `GPU` device on every rank. When
hoomd/device.py:    ``gpu_id`` is left `None`, HOOMD will attempt to detect the MPI local rank
hoomd/device.py:    environment and choose an appropriate GPU with ``id = local_rank %
hoomd/device.py:    num_capable_gpus``. Set `notice_level` to 3 to see status messages from this
hoomd/device.py:    .. rubric:: Multiple GPUs
hoomd/device.py:    Specify a list of GPUs to ``gpu_ids`` to activate a single-process multi-GPU
hoomd/device.py:        that all GPUs support concurrent managed memory access and have high
hoomd/device.py:    .. skip: next if(gpu_not_available)
hoomd/device.py:        gpu = hoomd.device.GPU()
hoomd/device.py:        gpu_ids=None,
hoomd/device.py:        gpu_id=None,
hoomd/device.py:        if gpu_ids is not None:
hoomd/device.py:            warnings.warn("gpu_ids is deprecated, use gpu_id.",
hoomd/device.py:        if gpu_ids is not None and gpu_id is not None:
hoomd/device.py:            raise ValueError("Set either gpu_id or gpu_ids, not both.")
hoomd/device.py:        if gpu_id is None:
hoomd/device.py:            gpu_ids = []
hoomd/device.py:            gpu_ids = [gpu_id]
hoomd/device.py:            _hoomd.ExecutionConfiguration.executionMode.GPU, gpu_ids,
hoomd/device.py:    def gpu_error_checking(self):
hoomd/device.py:        """bool: Whether to check for GPU error conditions after every call.
hoomd/device.py:        When `False` (the default), error messages from the GPU may not be
hoomd/device.py:        noticed immediately. Set to `True` to increase the accuracy of the GPU
hoomd/device.py:        .. skip: next if(gpu_not_available)
hoomd/device.py:            gpu.gpu_error_checking = True
hoomd/device.py:        return self._cpp_exec_conf.isCUDAErrorCheckingEnabled()
hoomd/device.py:    @gpu_error_checking.setter
hoomd/device.py:    def gpu_error_checking(self, new_bool):
hoomd/device.py:        self._cpp_exec_conf.setCUDAErrorChecking(new_bool)
hoomd/device.py:        The tuple includes the major and minor versions of the CUDA compute
hoomd/device.py:        """Test if the GPU device is available.
hoomd/device.py:            bool: `True` if this build of HOOMD supports GPUs, `False` if not.
hoomd/device.py:        return hoomd.version.gpu_enabled
hoomd/device.py:        """Get the available GPU devices.
hoomd/device.py:        """Enable GPU profiling.
hoomd/device.py:        When using GPU profiling tools on HOOMD, select the option to disable
hoomd/device.py:        .. skip: next if(gpu_not_available)
hoomd/device.py:            simulation = hoomd.util.make_example_simulation(device=gpu)
hoomd/device.py:            with gpu.enable_profiling():
hoomd/device.py:        Instance of `GPU` if availabile, otherwise `CPU`.
hoomd/device.py:    if len(GPU.get_available_devices()) > 0:
hoomd/device.py:        return GPU(None, None, communicator, message_filename, notice_level)
hoomd/tune/sorter.py:        if isinstance(self._simulation.device, hoomd.device.GPU):
hoomd/tune/sorter.py:            cpp_cls = getattr(_hoomd, 'SFCPackTunerGPU')
hoomd/tune/balance.py:    assumes that all algorithms are linear in :math:`N`, all GPUs are fully
hoomd/tune/balance.py:        if isinstance(self._simulation.device, hoomd.device.GPU):
hoomd/tune/balance.py:            cpp_cls = getattr(_hoomd, 'LoadBalancerGPU')
hoomd/CommunicatorGPU.cc:/*! \file CommunicatorGPU.cc
hoomd/CommunicatorGPU.cc:    \brief Implements the CommunicatorGPU class
hoomd/CommunicatorGPU.cc:#include "CommunicatorGPU.h"
hoomd/CommunicatorGPU.cc:CommunicatorGPU::CommunicatorGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/CommunicatorGPU.cc:        // inform the user to use a cuda-aware MPI
hoomd/CommunicatorGPU.cc:            << "Using unified memory with MPI. Make sure to enable CUDA-awareness in your MPI."
hoomd/CommunicatorGPU.cc:    // create cuda event
hoomd/CommunicatorGPU.cc:CommunicatorGPU::~CommunicatorGPU()
hoomd/CommunicatorGPU.cc:    m_exec_conf->msg->notice(5) << "Destroying CommunicatorGPU";
hoomd/CommunicatorGPU.cc:void CommunicatorGPU::updateMeshDefinition()
hoomd/CommunicatorGPU.cc:void CommunicatorGPU::allocateBuffers()
hoomd/CommunicatorGPU.cc:    GlobalVector<detail::pdata_element> gpu_sendbuf(m_exec_conf);
hoomd/CommunicatorGPU.cc:    m_gpu_sendbuf.swap(gpu_sendbuf);
hoomd/CommunicatorGPU.cc:    GlobalVector<detail::pdata_element> gpu_recvbuf(m_exec_conf);
hoomd/CommunicatorGPU.cc:    m_gpu_recvbuf.swap(gpu_recvbuf);
hoomd/CommunicatorGPU.cc:void CommunicatorGPU::initializeCommunicationStages()
hoomd/CommunicatorGPU.cc:    m_exec_conf->msg->notice(4) << "CommunicatorGPU: Using " << m_num_stages
hoomd/CommunicatorGPU.cc:CommunicatorGPU::GroupCommunicatorGPU<group_data>::GroupCommunicatorGPU(
hoomd/CommunicatorGPU.cc:    CommunicatorGPU& gpu_comm,
hoomd/CommunicatorGPU.cc:    : m_gpu_comm(gpu_comm), m_exec_conf(m_gpu_comm.m_exec_conf), m_gdata(gdata),
hoomd/CommunicatorGPU.cc:CommunicatorGPU::GroupCommunicatorGPU<group_data>::GroupCommunicatorGPU(CommunicatorGPU& gpu_comm)
hoomd/CommunicatorGPU.cc:    : m_gpu_comm(gpu_comm), m_exec_conf(m_gpu_comm.m_exec_conf), m_gdata(NULL),
hoomd/CommunicatorGPU.cc:void CommunicatorGPU::GroupCommunicatorGPU<group_data>::addGroupData(
hoomd/CommunicatorGPU.cc:void CommunicatorGPU::GroupCommunicatorGPU<group_data>::migrateGroups(bool incomplete,
hoomd/CommunicatorGPU.cc:            gpu_reset_rtags(m_gdata->getNGhosts(),
hoomd/CommunicatorGPU.cc:            if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/CommunicatorGPU.cc:                CHECK_CUDA_ERROR();
hoomd/CommunicatorGPU.cc:            ArrayHandle<unsigned int> d_comm_flags(m_gpu_comm.m_pdata->getCommFlags(),
hoomd/CommunicatorGPU.cc:            ArrayHandle<unsigned int> d_rtag(m_gpu_comm.m_pdata->getRTags(),
hoomd/CommunicatorGPU.cc:                = m_gpu_comm.m_pdata->getDomainDecomposition();
hoomd/CommunicatorGPU.cc:            gpu_mark_groups<group_data::size>(m_gpu_comm.m_pdata->getN(),
hoomd/CommunicatorGPU.cc:            if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/CommunicatorGPU.cc:                CHECK_CUDA_ERROR();
hoomd/CommunicatorGPU.cc:            ArrayHandle<unsigned int> d_rtag(m_gpu_comm.m_pdata->getRTags(),
hoomd/CommunicatorGPU.cc:            ArrayHandle<unsigned int> d_comm_flags(m_gpu_comm.m_pdata->getCommFlags(),
hoomd/CommunicatorGPU.cc:            gpu_scatter_ranks_and_mark_send_groups<group_data::size>(
hoomd/CommunicatorGPU.cc:            if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/CommunicatorGPU.cc:                CHECK_CUDA_ERROR();
hoomd/CommunicatorGPU.cc:            ArrayHandle<unsigned int> h_unique_neighbors(m_gpu_comm.m_unique_neighbors,
hoomd/CommunicatorGPU.cc:                    for (unsigned int ineigh = 0; ineigh < m_gpu_comm.m_n_unique_neigh; ++ineigh)
hoomd/CommunicatorGPU.cc:            ArrayHandle<unsigned int> h_unique_neighbors(m_gpu_comm.m_unique_neighbors,
hoomd/CommunicatorGPU.cc:            ArrayHandle<unsigned int> h_begin(m_gpu_comm.m_begin,
hoomd/CommunicatorGPU.cc:            ArrayHandle<unsigned int> h_end(m_gpu_comm.m_end,
hoomd/CommunicatorGPU.cc:            for (unsigned int i = 0; i < m_gpu_comm.m_n_unique_neigh; ++i)
hoomd/CommunicatorGPU.cc:        unsigned int n_send_groups[m_gpu_comm.m_n_unique_neigh];
hoomd/CommunicatorGPU.cc:        unsigned int n_recv_groups[m_gpu_comm.m_n_unique_neigh];
hoomd/CommunicatorGPU.cc:        unsigned int offs[m_gpu_comm.m_n_unique_neigh];
hoomd/CommunicatorGPU.cc:            ArrayHandle<unsigned int> h_begin(m_gpu_comm.m_begin,
hoomd/CommunicatorGPU.cc:            ArrayHandle<unsigned int> h_end(m_gpu_comm.m_end,
hoomd/CommunicatorGPU.cc:            ArrayHandle<unsigned int> h_unique_neighbors(m_gpu_comm.m_unique_neighbors,
hoomd/CommunicatorGPU.cc:            for (unsigned int ineigh = 0; ineigh < m_gpu_comm.m_n_unique_neigh; ineigh++)
hoomd/CommunicatorGPU.cc:            std::vector<MPI_Request> req(2 * m_gpu_comm.m_n_unique_neigh);
hoomd/CommunicatorGPU.cc:            std::vector<MPI_Status> stat(2 * m_gpu_comm.m_n_unique_neigh);
hoomd/CommunicatorGPU.cc:            for (unsigned int ineigh = 0; ineigh < m_gpu_comm.m_n_unique_neigh; ineigh++)
hoomd/CommunicatorGPU.cc:                          m_gpu_comm.m_mpi_comm,
hoomd/CommunicatorGPU.cc:                          m_gpu_comm.m_mpi_comm,
hoomd/CommunicatorGPU.cc:            for (unsigned int ineigh = 0; ineigh < m_gpu_comm.m_n_unique_neigh; ineigh++)
hoomd/CommunicatorGPU.cc:            ArrayHandle<unsigned int> h_begin(m_gpu_comm.m_begin,
hoomd/CommunicatorGPU.cc:            ArrayHandle<unsigned int> h_end(m_gpu_comm.m_end,
hoomd/CommunicatorGPU.cc:            ArrayHandle<unsigned int> h_unique_neighbors(m_gpu_comm.m_unique_neighbors,
hoomd/CommunicatorGPU.cc:            for (unsigned int ineigh = 0; ineigh < m_gpu_comm.m_n_unique_neigh; ineigh++)
hoomd/CommunicatorGPU.cc:                              m_gpu_comm.m_mpi_comm,
hoomd/CommunicatorGPU.cc:                              m_gpu_comm.m_mpi_comm,
hoomd/CommunicatorGPU.cc:            gpu_update_ranks_table<group_data::size>(m_gdata->getN(),
hoomd/CommunicatorGPU.cc:            if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/CommunicatorGPU.cc:                CHECK_CUDA_ERROR();
hoomd/CommunicatorGPU.cc:            ArrayHandle<unsigned int> d_rtag(m_gpu_comm.m_pdata->getRTags(),
hoomd/CommunicatorGPU.cc:            ArrayHandle<unsigned int> d_comm_flags(m_gpu_comm.m_pdata->getCommFlags(),
hoomd/CommunicatorGPU.cc:            gpu_scatter_and_mark_groups_for_removal<group_data::size>(m_gdata->getN(),
hoomd/CommunicatorGPU.cc:            if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/CommunicatorGPU.cc:                CHECK_CUDA_ERROR();
hoomd/CommunicatorGPU.cc:            gpu_remove_groups(ngroups,
hoomd/CommunicatorGPU.cc:            if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/CommunicatorGPU.cc:                CHECK_CUDA_ERROR();
hoomd/CommunicatorGPU.cc:            ArrayHandle<unsigned int> h_unique_neighbors(m_gpu_comm.m_unique_neighbors,
hoomd/CommunicatorGPU.cc:            ArrayHandle<unsigned int> h_begin(m_gpu_comm.m_begin,
hoomd/CommunicatorGPU.cc:            ArrayHandle<unsigned int> h_end(m_gpu_comm.m_end,
hoomd/CommunicatorGPU.cc:            for (unsigned int i = 0; i < m_gpu_comm.m_n_unique_neigh; ++i)
hoomd/CommunicatorGPU.cc:            ArrayHandle<unsigned int> h_begin(m_gpu_comm.m_begin,
hoomd/CommunicatorGPU.cc:            ArrayHandle<unsigned int> h_end(m_gpu_comm.m_end,
hoomd/CommunicatorGPU.cc:            ArrayHandle<unsigned int> h_unique_neighbors(m_gpu_comm.m_unique_neighbors,
hoomd/CommunicatorGPU.cc:            for (unsigned int ineigh = 0; ineigh < m_gpu_comm.m_n_unique_neigh; ineigh++)
hoomd/CommunicatorGPU.cc:            std::vector<MPI_Request> req(2 * m_gpu_comm.m_n_unique_neigh);
hoomd/CommunicatorGPU.cc:            std::vector<MPI_Status> stat(2 * m_gpu_comm.m_n_unique_neigh);
hoomd/CommunicatorGPU.cc:            for (unsigned int ineigh = 0; ineigh < m_gpu_comm.m_n_unique_neigh; ineigh++)
hoomd/CommunicatorGPU.cc:                          m_gpu_comm.m_mpi_comm,
hoomd/CommunicatorGPU.cc:                          m_gpu_comm.m_mpi_comm,
hoomd/CommunicatorGPU.cc:            for (unsigned int ineigh = 0; ineigh < m_gpu_comm.m_n_unique_neigh; ineigh++)
hoomd/CommunicatorGPU.cc:            ArrayHandle<unsigned int> h_begin(m_gpu_comm.m_begin,
hoomd/CommunicatorGPU.cc:            ArrayHandle<unsigned int> h_end(m_gpu_comm.m_end,
hoomd/CommunicatorGPU.cc:            ArrayHandle<unsigned int> h_unique_neighbors(m_gpu_comm.m_unique_neighbors,
hoomd/CommunicatorGPU.cc:            for (unsigned int ineigh = 0; ineigh < m_gpu_comm.m_n_unique_neigh; ineigh++)
hoomd/CommunicatorGPU.cc:                              m_gpu_comm.m_mpi_comm,
hoomd/CommunicatorGPU.cc:                              m_gpu_comm.m_mpi_comm,
hoomd/CommunicatorGPU.cc:            gpu_add_groups(old_ngroups,
hoomd/CommunicatorGPU.cc:            if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/CommunicatorGPU.cc:                CHECK_CUDA_ERROR();
hoomd/CommunicatorGPU.cc:void CommunicatorGPU::GroupCommunicatorGPU<group_data>::exchangeGhostGroups(
hoomd/CommunicatorGPU.cc:            << "CommunicatorGPU: ghost " << group_data::getName() << " exchange " << std::endl;
hoomd/CommunicatorGPU.cc:        n_send_ghost_groups.resize(m_gpu_comm.m_num_stages);
hoomd/CommunicatorGPU.cc:        n_recv_ghost_groups.resize(m_gpu_comm.m_num_stages);
hoomd/CommunicatorGPU.cc:        for (unsigned int istage = 0; istage < m_gpu_comm.m_num_stages; ++istage)
hoomd/CommunicatorGPU.cc:            n_send_ghost_groups[istage].resize(m_gpu_comm.m_n_unique_neigh);
hoomd/CommunicatorGPU.cc:            n_recv_ghost_groups[istage].resize(m_gpu_comm.m_n_unique_neigh);
hoomd/CommunicatorGPU.cc:        n_send_ghost_groups_tot.resize(m_gpu_comm.m_num_stages);
hoomd/CommunicatorGPU.cc:        n_recv_ghost_groups_tot.resize(m_gpu_comm.m_num_stages);
hoomd/CommunicatorGPU.cc:        ghost_group_offs.resize(m_gpu_comm.m_num_stages);
hoomd/CommunicatorGPU.cc:        for (unsigned int istage = 0; istage < m_gpu_comm.m_num_stages; ++istage)
hoomd/CommunicatorGPU.cc:            ghost_group_offs[istage].resize(m_gpu_comm.m_n_unique_neigh);
hoomd/CommunicatorGPU.cc:        m_ghost_group_begin.resize(m_gpu_comm.m_n_unique_neigh * m_gpu_comm.m_num_stages);
hoomd/CommunicatorGPU.cc:        m_ghost_group_end.resize(m_gpu_comm.m_n_unique_neigh * m_gpu_comm.m_num_stages);
hoomd/CommunicatorGPU.cc:        idx_offs.resize(m_gpu_comm.m_num_stages);
hoomd/CommunicatorGPU.cc:            ArrayHandle<unsigned int> d_rtag(m_gpu_comm.m_pdata->getRTags(),
hoomd/CommunicatorGPU.cc:            gpu_make_ghost_group_exchange_plan<group_data::size>(d_ghost_group_plan.data,
hoomd/CommunicatorGPU.cc:                                                                 m_gpu_comm.m_pdata->getN());
hoomd/CommunicatorGPU.cc:            if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/CommunicatorGPU.cc:                CHECK_CUDA_ERROR();
hoomd/CommunicatorGPU.cc:        for (unsigned int stage = 0; stage < m_gpu_comm.m_num_stages; stage++)
hoomd/CommunicatorGPU.cc:                ArrayHandle<unsigned int> d_adj_mask(m_gpu_comm.m_adj_mask,
hoomd/CommunicatorGPU.cc:                    = gpu_exchange_ghosts_count_neighbors(m_gdata->getN() + m_gdata->getNGhosts(),
hoomd/CommunicatorGPU.cc:                                                          m_gpu_comm.m_n_unique_neigh,
hoomd/CommunicatorGPU.cc:                if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/CommunicatorGPU.cc:                    CHECK_CUDA_ERROR();
hoomd/CommunicatorGPU.cc:                ArrayHandle<unsigned int> d_adj_mask(m_gpu_comm.m_adj_mask,
hoomd/CommunicatorGPU.cc:                ArrayHandle<unsigned int> d_unique_neighbors(m_gpu_comm.m_unique_neighbors,
hoomd/CommunicatorGPU.cc:                gpu_exchange_ghosts_make_indices(
hoomd/CommunicatorGPU.cc:                    d_ghost_group_begin.data + stage * m_gpu_comm.m_n_unique_neigh,
hoomd/CommunicatorGPU.cc:                    d_ghost_group_end.data + stage * m_gpu_comm.m_n_unique_neigh,
hoomd/CommunicatorGPU.cc:                    m_gpu_comm.m_n_unique_neigh,
hoomd/CommunicatorGPU.cc:                    m_gpu_comm.m_comm_mask[stage],
hoomd/CommunicatorGPU.cc:                if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/CommunicatorGPU.cc:                    CHECK_CUDA_ERROR();
hoomd/CommunicatorGPU.cc:                gpu_exchange_ghost_groups_pack(n_send_ghost_groups_tot[stage],
hoomd/CommunicatorGPU.cc:                if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/CommunicatorGPU.cc:                    CHECK_CUDA_ERROR();
hoomd/CommunicatorGPU.cc:                ArrayHandle<unsigned int> h_unique_neighbors(m_gpu_comm.m_unique_neighbors,
hoomd/CommunicatorGPU.cc:                for (unsigned int ineigh = 0; ineigh < m_gpu_comm.m_n_unique_neigh; ineigh++)
hoomd/CommunicatorGPU.cc:                        = h_ghost_group_end.data[ineigh + stage * m_gpu_comm.m_n_unique_neigh]
hoomd/CommunicatorGPU.cc:                          - h_ghost_group_begin.data[ineigh + stage * m_gpu_comm.m_n_unique_neigh];
hoomd/CommunicatorGPU.cc:                std::vector<MPI_Request> req(2 * m_gpu_comm.m_n_unique_neigh);
hoomd/CommunicatorGPU.cc:                std::vector<MPI_Status> stat(2 * m_gpu_comm.m_n_unique_neigh);
hoomd/CommunicatorGPU.cc:                for (unsigned int ineigh = 0; ineigh < m_gpu_comm.m_n_unique_neigh; ineigh++)
hoomd/CommunicatorGPU.cc:                    if (m_gpu_comm.m_stages[ineigh] != (int)stage)
hoomd/CommunicatorGPU.cc:                              m_gpu_comm.m_mpi_comm,
hoomd/CommunicatorGPU.cc:                              m_gpu_comm.m_mpi_comm,
hoomd/CommunicatorGPU.cc:                for (unsigned int ineigh = 0; ineigh < m_gpu_comm.m_n_unique_neigh; ineigh++)
hoomd/CommunicatorGPU.cc:                ArrayHandle<unsigned int> h_unique_neighbors(m_gpu_comm.m_unique_neighbors,
hoomd/CommunicatorGPU.cc:                for (unsigned int ineigh = 0; ineigh < m_gpu_comm.m_n_unique_neigh; ineigh++)
hoomd/CommunicatorGPU.cc:                                            .data[ineigh + stage * m_gpu_comm.m_n_unique_neigh],
hoomd/CommunicatorGPU.cc:                                  m_gpu_comm.m_mpi_comm,
hoomd/CommunicatorGPU.cc:                                  m_gpu_comm.m_mpi_comm,
hoomd/CommunicatorGPU.cc:                ArrayHandle<unsigned int> d_rtag(m_gpu_comm.m_pdata->getRTags(),
hoomd/CommunicatorGPU.cc:                gpu_exchange_ghost_groups_copy_buf<group_data::size>(
hoomd/CommunicatorGPU.cc:                    m_gpu_comm.m_pdata->getN() + m_gpu_comm.m_pdata->getNGhosts(),
hoomd/CommunicatorGPU.cc:                if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/CommunicatorGPU.cc:                    CHECK_CUDA_ERROR();
hoomd/CommunicatorGPU.cc:                gpu_compute_ghost_rtags(first_idx,
hoomd/CommunicatorGPU.cc:                if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/CommunicatorGPU.cc:                    CHECK_CUDA_ERROR();
hoomd/CommunicatorGPU.cc:void CommunicatorGPU::GroupCommunicatorGPU<group_data>::markGhostParticles(
hoomd/CommunicatorGPU.cc:        ArrayHandle<unsigned int> d_rtag(m_gpu_comm.m_pdata->getRTags(),
hoomd/CommunicatorGPU.cc:        ArrayHandle<Scalar4> d_pos(m_gpu_comm.m_pdata->getPositions(),
hoomd/CommunicatorGPU.cc:            = m_gpu_comm.m_pdata->getDomainDecomposition();
hoomd/CommunicatorGPU.cc:        gpu_mark_bonded_ghosts<group_data::size>(m_gdata->getN(),
hoomd/CommunicatorGPU.cc:                                                 m_gpu_comm.m_pdata->getBox(),
hoomd/CommunicatorGPU.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/CommunicatorGPU.cc:            CHECK_CUDA_ERROR();
hoomd/CommunicatorGPU.cc:void CommunicatorGPU::migrateParticles()
hoomd/CommunicatorGPU.cc:    m_exec_conf->msg->notice(7) << "CommunicatorGPU: migrate particles" << std::endl;
hoomd/CommunicatorGPU.cc:            gpu_stage_particles(m_pdata->getN(),
hoomd/CommunicatorGPU.cc:            if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/CommunicatorGPU.cc:                CHECK_CUDA_ERROR();
hoomd/CommunicatorGPU.cc:        m_pdata->removeParticlesGPU(m_gpu_sendbuf, m_comm_flags);
hoomd/CommunicatorGPU.cc:            m_send_keys.resize(m_gpu_sendbuf.size());
hoomd/CommunicatorGPU.cc:            ArrayHandle<detail::pdata_element> d_gpu_sendbuf(m_gpu_sendbuf,
hoomd/CommunicatorGPU.cc:            size_t nsend = m_gpu_sendbuf.size();
hoomd/CommunicatorGPU.cc:            gpu_sort_migrating_particles(m_gpu_sendbuf.size(),
hoomd/CommunicatorGPU.cc:                                         d_gpu_sendbuf.data,
hoomd/CommunicatorGPU.cc:            if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/CommunicatorGPU.cc:                CHECK_CUDA_ERROR();
hoomd/CommunicatorGPU.cc:        m_gpu_recvbuf.resize(n_recv_tot);
hoomd/CommunicatorGPU.cc:            ArrayHandle<detail::pdata_element> gpu_sendbuf_handle(m_gpu_sendbuf,
hoomd/CommunicatorGPU.cc:            ArrayHandle<detail::pdata_element> gpu_recvbuf_handle(m_gpu_recvbuf,
hoomd/CommunicatorGPU.cc:                    MPI_Isend(gpu_sendbuf_handle.data + h_begin.data[ineigh],
hoomd/CommunicatorGPU.cc:                    MPI_Irecv(gpu_recvbuf_handle.data + offs[ineigh],
hoomd/CommunicatorGPU.cc:            ArrayHandle<detail::pdata_element> d_gpu_recvbuf(m_gpu_recvbuf,
hoomd/CommunicatorGPU.cc:            gpu_wrap_particles(n_recv_tot, d_gpu_recvbuf.data, shifted_box);
hoomd/CommunicatorGPU.cc:            if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/CommunicatorGPU.cc:                CHECK_CUDA_ERROR();
hoomd/CommunicatorGPU.cc:        m_pdata->addParticlesGPU(m_gpu_recvbuf);
hoomd/CommunicatorGPU.cc:void CommunicatorGPU::removeGhostParticleTags()
hoomd/CommunicatorGPU.cc:            << "CommunicatorGPU: removing " << m_ghosts_added << " ghost particles " << std::endl;
hoomd/CommunicatorGPU.cc:        gpu_reset_rtags(m_ghosts_added, d_tag.data + m_pdata->getN(), d_rtag.data);
hoomd/CommunicatorGPU.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/CommunicatorGPU.cc:            CHECK_CUDA_ERROR();
hoomd/CommunicatorGPU.cc:void CommunicatorGPU::exchangeGhosts()
hoomd/CommunicatorGPU.cc:    if (current_flags[comm_flag::reverse_net_force] && this->m_exec_conf->isCUDAEnabled())
hoomd/CommunicatorGPU.cc:            "Communication error: Reverse force communication is not enabled on the GPU.");
hoomd/CommunicatorGPU.cc:    m_exec_conf->msg->notice(7) << "CommunicatorGPU: ghost exchange" << std::endl;
hoomd/CommunicatorGPU.cc:            gpu_make_ghost_exchange_plan(d_ghost_plan.data,
hoomd/CommunicatorGPU.cc:            if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/CommunicatorGPU.cc:                CHECK_CUDA_ERROR();
hoomd/CommunicatorGPU.cc:                = gpu_exchange_ghosts_count_neighbors(m_pdata->getN() + m_pdata->getNGhosts(),
hoomd/CommunicatorGPU.cc:            if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/CommunicatorGPU.cc:                CHECK_CUDA_ERROR();
hoomd/CommunicatorGPU.cc:            gpu_exchange_ghosts_make_indices(m_pdata->getN() + m_pdata->getNGhosts(),
hoomd/CommunicatorGPU.cc:            if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/CommunicatorGPU.cc:                CHECK_CUDA_ERROR();
hoomd/CommunicatorGPU.cc:            gpu_exchange_ghosts_pack(m_n_send_ghosts_tot[stage],
hoomd/CommunicatorGPU.cc:            if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/CommunicatorGPU.cc:                CHECK_CUDA_ERROR();
hoomd/CommunicatorGPU.cc:            // only unpack in non-CUDA MPI builds
hoomd/CommunicatorGPU.cc:            gpu_exchange_ghosts_copy_buf(m_n_recv_ghosts_tot[stage],
hoomd/CommunicatorGPU.cc:            if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/CommunicatorGPU.cc:                CHECK_CUDA_ERROR();
hoomd/CommunicatorGPU.cc:            gpu_compute_ghost_rtags(first_idx,
hoomd/CommunicatorGPU.cc:            if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/CommunicatorGPU.cc:                CHECK_CUDA_ERROR();
hoomd/CommunicatorGPU.cc:void CommunicatorGPU::beginUpdateGhosts(uint64_t timestep)
hoomd/CommunicatorGPU.cc:    m_exec_conf->msg->notice(7) << "CommunicatorGPU: ghost update" << std::endl;
hoomd/CommunicatorGPU.cc:            gpu_exchange_ghosts_pack(m_n_send_ghosts_tot[stage],
hoomd/CommunicatorGPU.cc:            if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/CommunicatorGPU.cc:                CHECK_CUDA_ERROR();
hoomd/CommunicatorGPU.cc:                // only unpack in non-CUDA MPI builds
hoomd/CommunicatorGPU.cc:                gpu_exchange_ghosts_copy_buf(m_n_recv_ghosts_tot[stage],
hoomd/CommunicatorGPU.cc:                if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/CommunicatorGPU.cc:                    CHECK_CUDA_ERROR();
hoomd/CommunicatorGPU.cc:void CommunicatorGPU::finishUpdateGhosts(uint64_t timestep)
hoomd/CommunicatorGPU.cc:        // only unpack in non-CUDA-MPI builds
hoomd/CommunicatorGPU.cc:            gpu_exchange_ghosts_copy_buf(m_n_recv_ghosts_tot[stage],
hoomd/CommunicatorGPU.cc:            if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/CommunicatorGPU.cc:                CHECK_CUDA_ERROR();
hoomd/CommunicatorGPU.cc:void CommunicatorGPU::updateNetForce(uint64_t timestep)
hoomd/CommunicatorGPU.cc:    oss << "CommunicatorGPU: update net ";
hoomd/CommunicatorGPU.cc:            gpu_exchange_ghosts_pack_netforce(m_n_send_ghosts_tot[stage],
hoomd/CommunicatorGPU.cc:            if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/CommunicatorGPU.cc:                CHECK_CUDA_ERROR();
hoomd/CommunicatorGPU.cc:            gpu_exchange_ghosts_pack_netforce(m_n_send_ghosts_tot[stage],
hoomd/CommunicatorGPU.cc:            if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/CommunicatorGPU.cc:                CHECK_CUDA_ERROR();
hoomd/CommunicatorGPU.cc:            gpu_exchange_ghosts_pack_netvirial(m_n_send_ghosts_tot[stage],
hoomd/CommunicatorGPU.cc:            if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/CommunicatorGPU.cc:                CHECK_CUDA_ERROR();
hoomd/CommunicatorGPU.cc:            gpu_exchange_ghosts_copy_netforce_buf(m_n_recv_ghosts_tot[stage],
hoomd/CommunicatorGPU.cc:            if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/CommunicatorGPU.cc:                CHECK_CUDA_ERROR();
hoomd/CommunicatorGPU.cc:            gpu_exchange_ghosts_copy_netforce_buf(m_n_recv_ghosts_tot[stage],
hoomd/CommunicatorGPU.cc:            if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/CommunicatorGPU.cc:                CHECK_CUDA_ERROR();
hoomd/CommunicatorGPU.cc:            gpu_exchange_ghosts_copy_netvirial_buf(
hoomd/CommunicatorGPU.cc:            if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/CommunicatorGPU.cc:                CHECK_CUDA_ERROR();
hoomd/CommunicatorGPU.cc://! Export CommunicatorGPU class to python
hoomd/CommunicatorGPU.cc:void export_CommunicatorGPU(pybind11::module& m)
hoomd/CommunicatorGPU.cc:    pybind11::class_<CommunicatorGPU, Communicator, std::shared_ptr<CommunicatorGPU>>(
hoomd/CommunicatorGPU.cc:        "CommunicatorGPU")
hoomd/CommunicatorGPU.cc:        .def("setMaxStages", &CommunicatorGPU::setMaxStages);
hoomd/Integrator.h:    /// helper function to compute net force/virial on the GPU
hoomd/Integrator.h:    virtual void computeNetForceGPU(uint64_t timestep);
hoomd/_compile.py:def get_gpu_compilation_settings(gpu):
hoomd/_compile.py:    """Helper function to set CUDA libraries for GPU execution."""
hoomd/_compile.py:        "-I" + str(hoomd.version.cuda_include_path),
hoomd/_compile.py:    compute_major, compute_minor = gpu.compute_capability
hoomd/_compile.py:        "cuda_devrt_lib_path": hoomd.version.cuda_devrt_library,
hoomd/BoxResizeUpdaterGPU.cuh:#ifndef __BOX_RESIZE_UPDATER_GPU_CUH__
hoomd/BoxResizeUpdaterGPU.cuh:#define __BOX_RESIZE_UPDATER_GPU_CUH__
hoomd/BoxResizeUpdaterGPU.cuh:hipError_t gpu_box_resize_scale(Scalar4* d_pos,
hoomd/BoxResizeUpdaterGPU.cuh:hipError_t gpu_box_resize_wrap(const unsigned int N,
hoomd/BoxResizeUpdaterGPU.cuh:#endif // __BOX_RESIZE_UPDATER_GPU_CUH__
hoomd/mpcd/BounceBackNVEGPU.cu.inc: * \file mpcd/BounceBackNVE@_geometry@GPU.cu
hoomd/mpcd/BounceBackNVEGPU.cu.inc: * \brief Template instantation for BounceBackNVEGPU driver (and so kernel) with @_geometry@.
hoomd/mpcd/BounceBackNVEGPU.cu.inc:#include "hoomd/mpcd/BounceBackNVEGPU.cuh"
hoomd/mpcd/BounceBackNVEGPU.cu.inc:namespace gpu
hoomd/mpcd/BounceBackNVEGPU.cu.inc:template cudaError_t __attribute__((visibility("default")))
hoomd/mpcd/BounceBackNVEGPU.cu.inc:    } // end namespace gpu
hoomd/mpcd/CellThermoCompute.cc:    GPUArray<double> net_properties(mpcd::detail::thermo_index::num_quantities, m_exec_conf);
hoomd/mpcd/Communicator.cc:    GPUArray<unsigned int> neighbors(neigh_max, m_exec_conf);
hoomd/mpcd/Communicator.cc:    GPUArray<unsigned int> unique_neighbors(neigh_max, m_exec_conf);
hoomd/mpcd/Communicator.cc:    GPUArray<unsigned int> adj_mask(neigh_max, m_exec_conf);
hoomd/mpcd/BulkStreamingMethodGPU.h: * \file mpcd/BulkStreamingMethodGPU.h
hoomd/mpcd/BulkStreamingMethodGPU.h: * \brief Declaration of mpcd::BulkStreamingMethodGPU
hoomd/mpcd/BulkStreamingMethodGPU.h:#ifndef MPCD_BULK_STREAMING_METHOD_GPU_H_
hoomd/mpcd/BulkStreamingMethodGPU.h:#define MPCD_BULK_STREAMING_METHOD_GPU_H_
hoomd/mpcd/BulkStreamingMethodGPU.h:#include "BounceBackStreamingMethodGPU.h"
hoomd/mpcd/BulkStreamingMethodGPU.h:class PYBIND11_EXPORT BulkStreamingMethodGPU
hoomd/mpcd/BulkStreamingMethodGPU.h:    : public BounceBackStreamingMethodGPU<mpcd::detail::BulkGeometry, Force>
hoomd/mpcd/BulkStreamingMethodGPU.h:    BulkStreamingMethodGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/mpcd/BulkStreamingMethodGPU.h:        : mpcd::BounceBackStreamingMethodGPU<mpcd::detail::BulkGeometry, Force>(
hoomd/mpcd/BulkStreamingMethodGPU.h://! Export mpcd::BulkStreamingMethodGPU to python
hoomd/mpcd/BulkStreamingMethodGPU.h:template<class Force> void export_BulkStreamingMethodGPU(pybind11::module& m)
hoomd/mpcd/BulkStreamingMethodGPU.h:    const std::string name = "BulkStreamingMethod" + Force::getName() + "GPU";
hoomd/mpcd/BulkStreamingMethodGPU.h:    pybind11::class_<mpcd::BulkStreamingMethodGPU<Force>,
hoomd/mpcd/BulkStreamingMethodGPU.h:                     std::shared_ptr<mpcd::BulkStreamingMethodGPU<Force>>>(m, name.c_str())
hoomd/mpcd/BulkStreamingMethodGPU.h:        .def_property_readonly("force", &mpcd::BulkStreamingMethodGPU<Force>::getForce);
hoomd/mpcd/BulkStreamingMethodGPU.h:#endif // MPCD_BULK_STREAMING_METHOD_GPU_H_
hoomd/mpcd/ParticleData.cuh: * \brief Declares GPU functions used by mpcd::ParticleData
hoomd/mpcd/ParticleData.cuh:#include <cuda_runtime.h>
hoomd/mpcd/ParticleData.cuh:namespace gpu
hoomd/mpcd/ParticleData.cuh:cudaError_t mark_removed_particles(unsigned char* d_remove_flags,
hoomd/mpcd/ParticleData.cuh:cudaError_t partition_particles(void* d_tmp,
hoomd/mpcd/ParticleData.cuh:cudaError_t remove_particles(mpcd::detail::pdata_element* d_out,
hoomd/mpcd/ParticleData.cuh:    } // end namespace gpu
hoomd/mpcd/CommunicatorGPU.cc: * \file mpcd/CommunicatorGPU.cc
hoomd/mpcd/CommunicatorGPU.cc: * \brief Implements the mpcd::CommunicatorGPU class
hoomd/mpcd/CommunicatorGPU.cc:#include "CommunicatorGPU.h"
hoomd/mpcd/CommunicatorGPU.cc:#include "CommunicatorGPU.cuh"
hoomd/mpcd/CommunicatorGPU.cc:mpcd::CommunicatorGPU::CommunicatorGPU(std::shared_ptr<SystemDefinition> sysdef)
hoomd/mpcd/CommunicatorGPU.cc:    GPUArray<unsigned int> neigh_send(neigh_max, m_exec_conf);
hoomd/mpcd/CommunicatorGPU.cc:    GPUArray<unsigned int> num_send(neigh_max, m_exec_conf);
hoomd/mpcd/CommunicatorGPU.cc:mpcd::CommunicatorGPU::~CommunicatorGPU() { }
hoomd/mpcd/CommunicatorGPU.cc:void mpcd::CommunicatorGPU::initializeCommunicationStages()
hoomd/mpcd/CommunicatorGPU.cc:    m_exec_conf->msg->notice(4) << "MPCD CommunicatorGPU: Using " << m_num_stages
hoomd/mpcd/CommunicatorGPU.cc:void mpcd::CommunicatorGPU::migrateParticles(uint64_t timestep)
hoomd/mpcd/CommunicatorGPU.cc:        m_mpcd_pdata->removeParticlesGPU(m_sendbuf, comm_mask, timestep);
hoomd/mpcd/CommunicatorGPU.cc:            // sort the send buffer on the gpu
hoomd/mpcd/CommunicatorGPU.cc:                num_send_neigh = (unsigned int)mpcd::gpu::sort_comm_send_buffer(
hoomd/mpcd/CommunicatorGPU.cc:            mpcd::gpu::wrap_particles(n_recv_tot, d_recvbuf.data, wrap_box);
hoomd/mpcd/CommunicatorGPU.cc:            if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/mpcd/CommunicatorGPU.cc:                CHECK_CUDA_ERROR();
hoomd/mpcd/CommunicatorGPU.cc:        m_mpcd_pdata->addParticlesGPU(m_recvbuf, comm_mask, timestep);
hoomd/mpcd/CommunicatorGPU.cc:void mpcd::CommunicatorGPU::setCommFlags(const BoxDim& box)
hoomd/mpcd/CommunicatorGPU.cc:    mpcd::gpu::stage_particles(d_comm_flag.data,
hoomd/mpcd/CommunicatorGPU.cc:    if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/mpcd/CommunicatorGPU.cc:        CHECK_CUDA_ERROR();
hoomd/mpcd/CommunicatorGPU.cc:void export_CommunicatorGPU(pybind11::module& m)
hoomd/mpcd/CommunicatorGPU.cc:    pybind11::class_<mpcd::CommunicatorGPU,
hoomd/mpcd/CommunicatorGPU.cc:                     std::shared_ptr<mpcd::CommunicatorGPU>>(m, "CommunicatorGPU")
hoomd/mpcd/CommunicatorGPU.cc:        .def("setMaxStages", &mpcd::CommunicatorGPU::setMaxStages);
hoomd/mpcd/BounceBackNVEGPU.cc.inc: * \file mpcd/BounceBackNVE@_geometry@GPU.cc
hoomd/mpcd/BounceBackNVEGPU.cc.inc: * \brief Template instantation for BounceBackNVEGPU with @_geometry@.
hoomd/mpcd/BounceBackNVEGPU.cc.inc:#include "hoomd/mpcd/BounceBackNVEGPU.h"
hoomd/mpcd/BounceBackNVEGPU.cc.inc:#define EXPORT_FUNCTION export_BounceBackNVE@_geometry@GPU
hoomd/mpcd/BounceBackNVEGPU.cc.inc:// Explicit instantiation of GPU class
hoomd/mpcd/BounceBackNVEGPU.cc.inc:template class BounceBackNVEGPU<GEOMETRY_CLASS>;
hoomd/mpcd/BounceBackNVEGPU.cc.inc:    export_BounceBackNVEGPU<GEOMETRY_CLASS>(m);
hoomd/mpcd/ReductionOperators.h: * The ops namespace contains functors for the CPU and GPU for doing things
hoomd/mpcd/CellThermoCompute.h:    const GPUArray<double4>& getCellVelocities() const
hoomd/mpcd/CellThermoCompute.h:    const GPUArray<double3>& getCellEnergies() const
hoomd/mpcd/CellThermoCompute.h:    GPUArray<double> m_net_properties; //!< Scalar properties of the system
hoomd/mpcd/CellThermoCompute.h:    GPUVector<double4> m_cell_vel;    //!< Average velocity of a cell + cell mass
hoomd/mpcd/CellThermoCompute.h:    GPUVector<double3> m_cell_energy; //!< Kinetic energy, unscaled temperature, dof in each cell
hoomd/mpcd/PlanarPoreGeometryFillerGPU.cuh:#ifndef MPCD_SLIT_PORE_GEOMETRY_FILLER_GPU_CUH_
hoomd/mpcd/PlanarPoreGeometryFillerGPU.cuh:#define MPCD_SLIT_PORE_GEOMETRY_FILLER_GPU_CUH_
hoomd/mpcd/PlanarPoreGeometryFillerGPU.cuh: * \file mpcd/PlanarPoreGeometryFillerGPU.cuh
hoomd/mpcd/PlanarPoreGeometryFillerGPU.cuh: * \brief Declaration of CUDA kernels for mpcd::PlanarPoreGeometryFillerGPU
hoomd/mpcd/PlanarPoreGeometryFillerGPU.cuh:#include <cuda_runtime.h>
hoomd/mpcd/PlanarPoreGeometryFillerGPU.cuh:namespace gpu
hoomd/mpcd/PlanarPoreGeometryFillerGPU.cuh:cudaError_t slit_pore_draw_particles(Scalar4* d_pos,
hoomd/mpcd/PlanarPoreGeometryFillerGPU.cuh:    } // end namespace gpu
hoomd/mpcd/PlanarPoreGeometryFillerGPU.cuh:#endif // MPCD_SLIT_PORE_GEOMETRY_FILLER_GPU_CUH_
hoomd/mpcd/CellList.cc:                          const GPUArray<unsigned int>& order,
hoomd/mpcd/CellList.cc:                          const GPUArray<unsigned int>& rorder)
hoomd/mpcd/ParticleData.cu: * \brief Defines GPU functions and kernels used by mpcd::ParticleData
hoomd/mpcd/ParticleData.cu:namespace gpu
hoomd/mpcd/ParticleData.cu:    } // end namespace gpu
hoomd/mpcd/ParticleData.cu: * \sa mpcd::gpu::kernel::mark_removed_particles
hoomd/mpcd/ParticleData.cu:cudaError_t mpcd::gpu::mark_removed_particles(unsigned char* d_remove_flags,
hoomd/mpcd/ParticleData.cu:    cudaFuncAttributes attr;
hoomd/mpcd/ParticleData.cu:    cudaFuncGetAttributes(&attr, (const void*)mpcd::gpu::kernel::mark_removed_particles);
hoomd/mpcd/ParticleData.cu:    mpcd::gpu::kernel::mark_removed_particles<<<grid, run_block_size>>>(d_remove_flags,
hoomd/mpcd/ParticleData.cu:    return cudaSuccess;
hoomd/mpcd/ParticleData.cu: * \returns cudaSuccess on completion
hoomd/mpcd/ParticleData.cu:cudaError_t mpcd::gpu::partition_particles(void* d_tmp,
hoomd/mpcd/ParticleData.cu:    return cudaSuccess;
hoomd/mpcd/ParticleData.cu: * \returns cudaSuccess on completion.
hoomd/mpcd/ParticleData.cu: * \sa mpcd::gpu::kernel::remove_particles
hoomd/mpcd/ParticleData.cu:cudaError_t mpcd::gpu::remove_particles(mpcd::detail::pdata_element* d_out,
hoomd/mpcd/ParticleData.cu:    cudaFuncAttributes attr;
hoomd/mpcd/ParticleData.cu:    cudaFuncGetAttributes(&attr, (const void*)mpcd::gpu::kernel::remove_particles);
hoomd/mpcd/ParticleData.cu:    mpcd::gpu::kernel::remove_particles<<<grid, run_block_size>>>(d_out,
hoomd/mpcd/ParticleData.cu:    return cudaSuccess;
hoomd/mpcd/ParticleData.cu:namespace gpu
hoomd/mpcd/ParticleData.cu:    } // end namespace gpu
hoomd/mpcd/ParticleData.cu:void mpcd::gpu::add_particles(unsigned int old_nparticles,
hoomd/mpcd/ParticleData.cu:    cudaFuncAttributes attr;
hoomd/mpcd/ParticleData.cu:    cudaFuncGetAttributes(&attr, (const void*)mpcd::gpu::kernel::add_particles);
hoomd/mpcd/ParticleData.cu:    mpcd::gpu::kernel::add_particles<<<grid, run_block_size>>>(old_nparticles,
hoomd/mpcd/Communicator.h:#include "hoomd/GPUArray.h"
hoomd/mpcd/Communicator.h:#include "hoomd/GPUVector.h"
hoomd/mpcd/Communicator.h:    const GPUArray<unsigned int>& getUniqueNeighbors() const
hoomd/mpcd/Communicator.h:    GPUArray<unsigned int> m_neighbors;        //!< Neighbor ranks
hoomd/mpcd/Communicator.h:    GPUArray<unsigned int> m_unique_neighbors; //!< Neighbor ranks w/duplicates removed
hoomd/mpcd/Communicator.h:    GPUArray<unsigned int> m_adj_mask;         //!< Adjacency mask for every neighbor
hoomd/mpcd/Communicator.h:    GPUVector<mpcd::detail::pdata_element> m_sendbuf; //!< Buffer for particles that are sent
hoomd/mpcd/Communicator.h:    GPUVector<mpcd::detail::pdata_element> m_recvbuf; //!< Buffer for particles that are received
hoomd/mpcd/RejectionVirtualParticleFillerGPU.cu: * \file mpcd/RejectionVirtualParticleFillerGPU.cu
hoomd/mpcd/RejectionVirtualParticleFillerGPU.cu: * \brief Definition of CUDA kernels for mpcd::RejectionVirtualParticleFillerGPU
hoomd/mpcd/RejectionVirtualParticleFillerGPU.cu:#include "RejectionVirtualParticleFillerGPU.cuh"
hoomd/mpcd/RejectionVirtualParticleFillerGPU.cu:namespace gpu
hoomd/mpcd/RejectionVirtualParticleFillerGPU.cu:cudaError_t __attribute__((visibility("default")))
hoomd/mpcd/RejectionVirtualParticleFillerGPU.cu:    return cudaSuccess;
hoomd/mpcd/RejectionVirtualParticleFillerGPU.cu:cudaError_t __attribute__((visibility("default")))
hoomd/mpcd/RejectionVirtualParticleFillerGPU.cu:    cudaFuncAttributes attr;
hoomd/mpcd/RejectionVirtualParticleFillerGPU.cu:    cudaFuncGetAttributes(&attr, (const void*)mpcd::gpu::kernel::copy_virtual_particles);
hoomd/mpcd/RejectionVirtualParticleFillerGPU.cu:    mpcd::gpu::kernel::copy_virtual_particles<<<grid, run_block_size>>>(d_keep_indices,
hoomd/mpcd/RejectionVirtualParticleFillerGPU.cu:    return cudaSuccess;
hoomd/mpcd/RejectionVirtualParticleFillerGPU.cu:    } // namespace gpu
hoomd/mpcd/CommunicatorGPU.cu: * \file mpcd/CommunicatorGPU.cu
hoomd/mpcd/CommunicatorGPU.cu: * \brief Implementation of communication algorithms on the GPU
hoomd/mpcd/CommunicatorGPU.cu:#include "CommunicatorGPU.cuh"
hoomd/mpcd/CommunicatorGPU.cu:namespace gpu
hoomd/mpcd/CommunicatorGPU.cu:    } // end namespace gpu
hoomd/mpcd/CommunicatorGPU.cu:cudaError_t mpcd::gpu::stage_particles(unsigned int* d_comm_flag,
hoomd/mpcd/CommunicatorGPU.cu:    cudaFuncAttributes attr;
hoomd/mpcd/CommunicatorGPU.cu:    cudaFuncGetAttributes(&attr, (const void*)mpcd::gpu::kernel::stage_particles);
hoomd/mpcd/CommunicatorGPU.cu:    mpcd::gpu::kernel::stage_particles<<<grid, run_block_size>>>(d_comm_flag, d_pos, N, box);
hoomd/mpcd/CommunicatorGPU.cu:    return cudaSuccess;
hoomd/mpcd/CommunicatorGPU.cu: * rank (see mpcd::gpu::get_migrate_key). The send buffer is then sorted using
hoomd/mpcd/CommunicatorGPU.cu:size_t mpcd::gpu::sort_comm_send_buffer(mpcd::detail::pdata_element* d_sendbuf,
hoomd/mpcd/CommunicatorGPU.cu:                      mpcd::gpu::get_migrate_key(grid_pos, di, mask, d_cart_ranks));
hoomd/mpcd/CommunicatorGPU.cu:void mpcd::gpu::reduce_comm_flags(unsigned int* d_req_flags,
hoomd/mpcd/CommunicatorGPU.cu:namespace gpu
hoomd/mpcd/CommunicatorGPU.cu:    } // end namespace gpu
hoomd/mpcd/CommunicatorGPU.cu:void mpcd::gpu::wrap_particles(const unsigned int n_recv,
hoomd/mpcd/CommunicatorGPU.cu:    thrust::transform(in_ptr, in_ptr + n_recv, in_ptr, mpcd::gpu::wrap_particle_op(box));
hoomd/mpcd/CellCommunicator.h:#include "hoomd/GPUArray.h"
hoomd/mpcd/CellCommunicator.h:#include "hoomd/GPUVector.h"
hoomd/mpcd/CellCommunicator.h:    template<typename T, class PackOpT> void communicate(const GPUArray<T>& props, const PackOpT op)
hoomd/mpcd/CellCommunicator.h:    template<typename T, class PackOpT> void begin(const GPUArray<T>& props, const PackOpT op);
hoomd/mpcd/CellCommunicator.h:    template<typename T, class PackOpT> void finalize(const GPUArray<T>& props, const PackOpT op);
hoomd/mpcd/CellCommunicator.h:    const GPUArray<unsigned int>& getCells()
hoomd/mpcd/CellCommunicator.h:    GPUVector<unsigned char> m_send_buf; //!< Send buffer
hoomd/mpcd/CellCommunicator.h:    GPUVector<unsigned char> m_recv_buf; //!< Receive buffer
hoomd/mpcd/CellCommunicator.h:    GPUArray<unsigned int> m_send_idx;   //!< Indexes of cells in send buffer
hoomd/mpcd/CellCommunicator.h:    GPUArray<unsigned int> m_cells; //!< Unique cells to receive
hoomd/mpcd/CellCommunicator.h:    GPUArray<unsigned int>
hoomd/mpcd/CellCommunicator.h:    GPUArray<unsigned int> m_recv_begin; //!< Begin offset of every unique cell
hoomd/mpcd/CellCommunicator.h:    GPUArray<unsigned int> m_recv_end;   //!< End offset of every unique cell
hoomd/mpcd/CellCommunicator.h:    template<typename T, class PackOpT> void packBuffer(const GPUArray<T>& props, const PackOpT op);
hoomd/mpcd/CellCommunicator.h:    void unpackBuffer(const GPUArray<T>& props, const PackOpT op);
hoomd/mpcd/CellCommunicator.h:    //! Packs the property buffer on the GPU
hoomd/mpcd/CellCommunicator.h:    void packBufferGPU(const GPUArray<T>& props, const PackOpT op);
hoomd/mpcd/CellCommunicator.h:    //! Unpacks the property buffer on the GPU
hoomd/mpcd/CellCommunicator.h:    void unpackBufferGPU(const GPUArray<T>& props, const PackOpT op);
hoomd/mpcd/CellCommunicator.h:void mpcd::CellCommunicator::begin(const GPUArray<T>& props, const PackOpT op)
hoomd/mpcd/CellCommunicator.h:    if (m_exec_conf->isCUDAEnabled())
hoomd/mpcd/CellCommunicator.h:        packBufferGPU(props, op);
hoomd/mpcd/CellCommunicator.h:        // determine whether to use CPU or GPU CUDA buffers
hoomd/mpcd/CellCommunicator.h:void mpcd::CellCommunicator::finalize(const GPUArray<T>& props, const PackOpT op)
hoomd/mpcd/CellCommunicator.h:    if (m_exec_conf->isCUDAEnabled())
hoomd/mpcd/CellCommunicator.h:        unpackBufferGPU(props, op);
hoomd/mpcd/CellCommunicator.h:void mpcd::CellCommunicator::packBuffer(const GPUArray<T>& props, const PackOpT op)
hoomd/mpcd/CellCommunicator.h:void mpcd::CellCommunicator::unpackBuffer(const GPUArray<T>& props, const PackOpT op)
hoomd/mpcd/CellCommunicator.h:void mpcd::CellCommunicator::packBufferGPU(const GPUArray<T>& props, const PackOpT op)
hoomd/mpcd/CellCommunicator.h:    mpcd::gpu::pack_cell_buffer(send_buf,
hoomd/mpcd/CellCommunicator.h:    if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/mpcd/CellCommunicator.h:        CHECK_CUDA_ERROR();
hoomd/mpcd/CellCommunicator.h:void mpcd::CellCommunicator::unpackBufferGPU(const GPUArray<T>& props, const PackOpT op)
hoomd/mpcd/CellCommunicator.h:    mpcd::gpu::unpack_cell_buffer(d_props.data,
hoomd/mpcd/CellCommunicator.h:    if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/mpcd/CellCommunicator.h:        CHECK_CUDA_ERROR();
hoomd/mpcd/ATCollisionMethodGPU.cu: * \file mpcd/ATCollisionMethodGPU.cu
hoomd/mpcd/ATCollisionMethodGPU.cu: * \brief Defines GPU functions and kernels used by mpcd::ATCollisionMethodGPU
hoomd/mpcd/ATCollisionMethodGPU.cu:#include "ATCollisionMethodGPU.cuh"
hoomd/mpcd/ATCollisionMethodGPU.cu:namespace gpu
hoomd/mpcd/ATCollisionMethodGPU.cu:cudaError_t at_draw_velocity(Scalar4* d_alt_vel,
hoomd/mpcd/ATCollisionMethodGPU.cu:    cudaFuncAttributes attr;
hoomd/mpcd/ATCollisionMethodGPU.cu:    cudaFuncGetAttributes(&attr, (const void*)mpcd::gpu::kernel::at_draw_velocity);
hoomd/mpcd/ATCollisionMethodGPU.cu:    mpcd::gpu::kernel::at_draw_velocity<<<grid, run_block_size>>>(d_alt_vel,
hoomd/mpcd/ATCollisionMethodGPU.cu:    return cudaSuccess;
hoomd/mpcd/ATCollisionMethodGPU.cu:cudaError_t at_apply_velocity(Scalar4* d_vel,
hoomd/mpcd/ATCollisionMethodGPU.cu:    cudaFuncAttributes attr;
hoomd/mpcd/ATCollisionMethodGPU.cu:    cudaFuncGetAttributes(&attr, (const void*)mpcd::gpu::kernel::at_apply_velocity);
hoomd/mpcd/ATCollisionMethodGPU.cu:    mpcd::gpu::kernel::at_apply_velocity<<<grid, run_block_size>>>(d_vel,
hoomd/mpcd/ATCollisionMethodGPU.cu:    return cudaSuccess;
hoomd/mpcd/ATCollisionMethodGPU.cu:    } // end namespace gpu
hoomd/mpcd/PlanarPoreGeometryFillerGPU.h: * \file mpcd/PlanarPoreGeometryFillerGPU.h
hoomd/mpcd/PlanarPoreGeometryFillerGPU.h: * \brief Definition of virtual particle filler for mpcd::PlanarPoreGeometry on the GPU.
hoomd/mpcd/PlanarPoreGeometryFillerGPU.h:#ifndef MPCD_SLIT_PORE_GEOMETRY_FILLER_GPU_H_
hoomd/mpcd/PlanarPoreGeometryFillerGPU.h:#define MPCD_SLIT_PORE_GEOMETRY_FILLER_GPU_H_
hoomd/mpcd/PlanarPoreGeometryFillerGPU.h://! Adds virtual particles to the MPCD particle data for PlanarPoreGeometry using the GPU
hoomd/mpcd/PlanarPoreGeometryFillerGPU.h:class PYBIND11_EXPORT PlanarPoreGeometryFillerGPU : public mpcd::PlanarPoreGeometryFiller
hoomd/mpcd/PlanarPoreGeometryFillerGPU.h:    PlanarPoreGeometryFillerGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/mpcd/PlanarPoreGeometryFillerGPU.h:    //! Draw particles within the fill volume on the GPU
hoomd/mpcd/PlanarPoreGeometryFillerGPU.h:#endif // MPCD_SLIT_PORE_GEOMETRY_FILLER_GPU_H_
hoomd/mpcd/ATCollisionMethodGPU.cuh:#ifndef MPCD_AT_COLLISION_METHOD_GPU_CUH_
hoomd/mpcd/ATCollisionMethodGPU.cuh:#define MPCD_AT_COLLISION_METHOD_GPU_CUH_
hoomd/mpcd/ATCollisionMethodGPU.cuh: * \file mpcd/ATCollisionMethodGPU.cuh
hoomd/mpcd/ATCollisionMethodGPU.cuh: * \brief Declaration of CUDA kernels for mpcd::ATCollisionMethodGPU
hoomd/mpcd/ATCollisionMethodGPU.cuh:#include <cuda_runtime.h>
hoomd/mpcd/ATCollisionMethodGPU.cuh:namespace gpu
hoomd/mpcd/ATCollisionMethodGPU.cuh:cudaError_t at_draw_velocity(Scalar4* d_alt_vel,
hoomd/mpcd/ATCollisionMethodGPU.cuh:cudaError_t at_apply_velocity(Scalar4* d_vel,
hoomd/mpcd/ATCollisionMethodGPU.cuh:    } // end namespace gpu
hoomd/mpcd/ATCollisionMethodGPU.cuh:#endif // MPCD_AT_COLLISION_METHOD_GPU_CUH_
hoomd/mpcd/SRDCollisionMethodGPU.cc:#include "SRDCollisionMethodGPU.h"
hoomd/mpcd/SRDCollisionMethodGPU.cc:#include "CellThermoComputeGPU.h"
hoomd/mpcd/SRDCollisionMethodGPU.cc:#include "SRDCollisionMethodGPU.cuh"
hoomd/mpcd/SRDCollisionMethodGPU.cc:mpcd::SRDCollisionMethodGPU::SRDCollisionMethodGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/mpcd/SRDCollisionMethodGPU.cc:void mpcd::SRDCollisionMethodGPU::drawRotationVectors(uint64_t timestep)
hoomd/mpcd/SRDCollisionMethodGPU.cc:        mpcd::gpu::srd_draw_vectors(d_rotvec.data,
hoomd/mpcd/SRDCollisionMethodGPU.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/mpcd/SRDCollisionMethodGPU.cc:            CHECK_CUDA_ERROR();
hoomd/mpcd/SRDCollisionMethodGPU.cc:        mpcd::gpu::srd_draw_vectors(d_rotvec.data,
hoomd/mpcd/SRDCollisionMethodGPU.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/mpcd/SRDCollisionMethodGPU.cc:            CHECK_CUDA_ERROR();
hoomd/mpcd/SRDCollisionMethodGPU.cc:void mpcd::SRDCollisionMethodGPU::rotate(uint64_t timestep)
hoomd/mpcd/SRDCollisionMethodGPU.cc:        mpcd::gpu::srd_rotate(d_vel.data,
hoomd/mpcd/SRDCollisionMethodGPU.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/mpcd/SRDCollisionMethodGPU.cc:            CHECK_CUDA_ERROR();
hoomd/mpcd/SRDCollisionMethodGPU.cc:        mpcd::gpu::srd_rotate(d_vel.data,
hoomd/mpcd/SRDCollisionMethodGPU.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/mpcd/SRDCollisionMethodGPU.cc:            CHECK_CUDA_ERROR();
hoomd/mpcd/SRDCollisionMethodGPU.cc:void mpcd::SRDCollisionMethodGPU::setCellList(std::shared_ptr<mpcd::CellList> cl)
hoomd/mpcd/SRDCollisionMethodGPU.cc:            m_thermo = std::make_shared<mpcd::CellThermoComputeGPU>(m_sysdef, m_cl);
hoomd/mpcd/SRDCollisionMethodGPU.cc:            m_thermo = std::shared_ptr<mpcd::CellThermoComputeGPU>();
hoomd/mpcd/SRDCollisionMethodGPU.cc:void export_SRDCollisionMethodGPU(pybind11::module& m)
hoomd/mpcd/SRDCollisionMethodGPU.cc:    pybind11::class_<mpcd::SRDCollisionMethodGPU,
hoomd/mpcd/SRDCollisionMethodGPU.cc:                     std::shared_ptr<mpcd::SRDCollisionMethodGPU>>(m, "SRDCollisionMethodGPU")
hoomd/mpcd/ParallelPlateGeometryFillerGPU.cu: * \file mpcd/ParallelPlateGeometryFillerGPU.cu
hoomd/mpcd/ParallelPlateGeometryFillerGPU.cu: * \brief Defines GPU functions and kernels used by mpcd::ParallelPlateGeometryFillerGPU
hoomd/mpcd/ParallelPlateGeometryFillerGPU.cu:#include "ParallelPlateGeometryFillerGPU.cuh"
hoomd/mpcd/ParallelPlateGeometryFillerGPU.cu:namespace gpu
hoomd/mpcd/ParallelPlateGeometryFillerGPU.cu:cudaError_t slit_draw_particles(Scalar4* d_pos,
hoomd/mpcd/ParallelPlateGeometryFillerGPU.cu:        return cudaSuccess;
hoomd/mpcd/ParallelPlateGeometryFillerGPU.cu:    cudaFuncAttributes attr;
hoomd/mpcd/ParallelPlateGeometryFillerGPU.cu:    cudaFuncGetAttributes(&attr, (const void*)kernel::slit_draw_particles);
hoomd/mpcd/ParallelPlateGeometryFillerGPU.cu:    return cudaSuccess;
hoomd/mpcd/ParallelPlateGeometryFillerGPU.cu:    } // end namespace gpu
hoomd/mpcd/Sorter.h:    GPUVector<unsigned int> m_order;  //!< Maps new sorted index onto old particle indexes
hoomd/mpcd/Sorter.h:    GPUVector<unsigned int> m_rorder; //!< Maps old particle indexes onto new sorted indexes
hoomd/mpcd/CellThermoComputeGPU.h: * \file mpcd/CellThermoComputeGPU.h
hoomd/mpcd/CellThermoComputeGPU.h: * \brief Declaration of mpcd::CellThermoComputeGPU
hoomd/mpcd/CellThermoComputeGPU.h:#ifndef MPCD_CELL_THERMO_COMPUTE_GPU_H_
hoomd/mpcd/CellThermoComputeGPU.h:#define MPCD_CELL_THERMO_COMPUTE_GPU_H_
hoomd/mpcd/CellThermoComputeGPU.h:#include "CellThermoComputeGPU.cuh"
hoomd/mpcd/CellThermoComputeGPU.h://! Computes the cell (thermodynamic) properties on the GPU
hoomd/mpcd/CellThermoComputeGPU.h:class PYBIND11_EXPORT CellThermoComputeGPU : public mpcd::CellThermoCompute
hoomd/mpcd/CellThermoComputeGPU.h:    CellThermoComputeGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/mpcd/CellThermoComputeGPU.h:    virtual ~CellThermoComputeGPU();
hoomd/mpcd/CellThermoComputeGPU.h:    //! Begin the calculation of outer cell properties on the GPU
hoomd/mpcd/CellThermoComputeGPU.h:    //! Finish the calculation of outer cell properties on the GPU
hoomd/mpcd/CellThermoComputeGPU.h:    //! Calculate the inner cell properties on the GPU
hoomd/mpcd/CellThermoComputeGPU.h:    GPUVector<mpcd::detail::cell_thermo_element>
hoomd/mpcd/CellThermoComputeGPU.h:    GPUFlags<mpcd::detail::cell_thermo_element> m_reduced; //!< Flags to hold reduced sum
hoomd/mpcd/CellThermoComputeGPU.h:#endif // MPCD_CELL_THERMO_COMPUTE_GPU_H_
hoomd/mpcd/ATCollisionMethodGPU.h: * \file mpcd/ATCollisionMethodGPU.h
hoomd/mpcd/ATCollisionMethodGPU.h: * \brief Declaration of mpcd::ATCollisionMethodGPU
hoomd/mpcd/ATCollisionMethodGPU.h:#ifndef MPCD_AT_COLLISION_METHOD_GPU_H_
hoomd/mpcd/ATCollisionMethodGPU.h:#define MPCD_AT_COLLISION_METHOD_GPU_H_
hoomd/mpcd/ATCollisionMethodGPU.h:class PYBIND11_EXPORT ATCollisionMethodGPU : public mpcd::ATCollisionMethod
hoomd/mpcd/ATCollisionMethodGPU.h:    ATCollisionMethodGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/mpcd/ATCollisionMethodGPU.h:    //! Draw velocities for particles in each cell on the GPU
hoomd/mpcd/ATCollisionMethodGPU.h:    //! Apply the random velocities to particles in each cell on the GPU
hoomd/mpcd/ATCollisionMethodGPU.h:#endif // MPCD_AT_COLLISION_METHOD_GPU_H_
hoomd/mpcd/test/communicator_mpi_test.cc:#include "hoomd/mpcd/CommunicatorGPU.h"
hoomd/mpcd/test/communicator_mpi_test.cc://! Typedef for function that creates the Communnicator on the CPU or GPU
hoomd/mpcd/test/communicator_mpi_test.cc:gpu_communicator_creator(std::shared_ptr<SystemDefinition> sysdef, unsigned int nstages)
hoomd/mpcd/test/communicator_mpi_test.cc:    std::shared_ptr<mpcd::CommunicatorGPU> comm
hoomd/mpcd/test/communicator_mpi_test.cc:        = std::shared_ptr<mpcd::CommunicatorGPU>(new mpcd::CommunicatorGPU(sysdef));
hoomd/mpcd/test/communicator_mpi_test.cc:UP_SUITE_BEGIN(gpu_tests)
hoomd/mpcd/test/communicator_mpi_test.cc:UP_TEST(mpcd_communicator_migrate_test_GPU_one_stage)
hoomd/mpcd/test/communicator_mpi_test.cc:    if (!exec_conf_gpu)
hoomd/mpcd/test/communicator_mpi_test.cc:        exec_conf_gpu = std::shared_ptr<ExecutionConfiguration>(
hoomd/mpcd/test/communicator_mpi_test.cc:            new ExecutionConfiguration(ExecutionConfiguration::GPU));
hoomd/mpcd/test/communicator_mpi_test.cc:    communicator_creator communicator_creator_gpu = bind(gpu_communicator_creator, _1, _2);
hoomd/mpcd/test/communicator_mpi_test.cc:    test_communicator_migrate(communicator_creator_gpu, exec_conf_gpu, BoxDim(2.0), 1);
hoomd/mpcd/test/communicator_mpi_test.cc:    test_communicator_migrate(communicator_creator_gpu, exec_conf_gpu, BoxDim(1.0, 2.0, 3.0), 1);
hoomd/mpcd/test/communicator_mpi_test.cc:UP_TEST(mpcd_communicator_migrate_test_GPU_two_stage)
hoomd/mpcd/test/communicator_mpi_test.cc:    if (!exec_conf_gpu)
hoomd/mpcd/test/communicator_mpi_test.cc:        exec_conf_gpu = std::shared_ptr<ExecutionConfiguration>(
hoomd/mpcd/test/communicator_mpi_test.cc:            new ExecutionConfiguration(ExecutionConfiguration::GPU));
hoomd/mpcd/test/communicator_mpi_test.cc:    communicator_creator communicator_creator_gpu = bind(gpu_communicator_creator, _1, _2);
hoomd/mpcd/test/communicator_mpi_test.cc:    test_communicator_migrate(communicator_creator_gpu, exec_conf_gpu, BoxDim(2.0), 2);
hoomd/mpcd/test/communicator_mpi_test.cc:    test_communicator_migrate(communicator_creator_gpu, exec_conf_gpu, BoxDim(1.0, 2.0, 3.0), 2);
hoomd/mpcd/test/communicator_mpi_test.cc:UP_TEST(mpcd_communicator_migrate_test_GPU_three_stage)
hoomd/mpcd/test/communicator_mpi_test.cc:    if (!exec_conf_gpu)
hoomd/mpcd/test/communicator_mpi_test.cc:        exec_conf_gpu = std::shared_ptr<ExecutionConfiguration>(
hoomd/mpcd/test/communicator_mpi_test.cc:            new ExecutionConfiguration(ExecutionConfiguration::GPU));
hoomd/mpcd/test/communicator_mpi_test.cc:    communicator_creator communicator_creator_gpu = bind(gpu_communicator_creator, _1, _2);
hoomd/mpcd/test/communicator_mpi_test.cc:    test_communicator_migrate(communicator_creator_gpu, exec_conf_gpu, BoxDim(2.0), 3);
hoomd/mpcd/test/communicator_mpi_test.cc:    test_communicator_migrate(communicator_creator_gpu, exec_conf_gpu, BoxDim(1.0, 2.0, 3.0), 3);
hoomd/mpcd/test/communicator_mpi_test.cc:UP_TEST(mpcd_communicator_migrate_ortho_test_GPU_one_stage)
hoomd/mpcd/test/communicator_mpi_test.cc:    if (!exec_conf_gpu)
hoomd/mpcd/test/communicator_mpi_test.cc:        exec_conf_gpu = std::shared_ptr<ExecutionConfiguration>(
hoomd/mpcd/test/communicator_mpi_test.cc:            new ExecutionConfiguration(ExecutionConfiguration::GPU));
hoomd/mpcd/test/communicator_mpi_test.cc:    communicator_creator communicator_creator_gpu = bind(gpu_communicator_creator, _1, _2);
hoomd/mpcd/test/communicator_mpi_test.cc:    test_communicator_migrate_ortho(communicator_creator_gpu, exec_conf_gpu, 1);
hoomd/mpcd/test/communicator_mpi_test.cc:UP_TEST(mpcd_communicator_migrate_ortho_test_GPU_two_stage)
hoomd/mpcd/test/communicator_mpi_test.cc:    if (!exec_conf_gpu)
hoomd/mpcd/test/communicator_mpi_test.cc:        exec_conf_gpu = std::shared_ptr<ExecutionConfiguration>(
hoomd/mpcd/test/communicator_mpi_test.cc:            new ExecutionConfiguration(ExecutionConfiguration::GPU));
hoomd/mpcd/test/communicator_mpi_test.cc:    communicator_creator communicator_creator_gpu = bind(gpu_communicator_creator, _1, _2);
hoomd/mpcd/test/communicator_mpi_test.cc:    test_communicator_migrate_ortho(communicator_creator_gpu, exec_conf_gpu, 2);
hoomd/mpcd/test/rejection_filler_test.cc:#include "hoomd/mpcd/RejectionVirtualParticleFillerGPU.h"
hoomd/mpcd/test/rejection_filler_test.cc:UP_TEST(sphere_rejection_fill_basic_gpu)
hoomd/mpcd/test/rejection_filler_test.cc:    sphere_rejection_fill_basic_test<mpcd::RejectionVirtualParticleFillerGPU<mpcd::SphereGeometry>>(
hoomd/mpcd/test/rejection_filler_test.cc:        std::make_shared<ExecutionConfiguration>(ExecutionConfiguration::GPU));
hoomd/mpcd/test/sorter_test.cc:#include "hoomd/mpcd/SorterGPU.h"
hoomd/mpcd/test/sorter_test.cc:UP_TEST(mpcd_sorter_test_gpu)
hoomd/mpcd/test/sorter_test.cc:    sorter_test<mpcd::SorterGPU>(std::shared_ptr<ExecutionConfiguration>(
hoomd/mpcd/test/sorter_test.cc:        new ExecutionConfiguration(ExecutionConfiguration::GPU)));
hoomd/mpcd/test/sorter_test.cc:UP_TEST(mpcd_sorter_virtual_test_gpu)
hoomd/mpcd/test/sorter_test.cc:    sorter_virtual_test<mpcd::SorterGPU>(std::shared_ptr<ExecutionConfiguration>(
hoomd/mpcd/test/sorter_test.cc:        new ExecutionConfiguration(ExecutionConfiguration::GPU)));
hoomd/mpcd/test/cell_list_mpi_test.cc:#include "hoomd/mpcd/CellListGPU.h"
hoomd/mpcd/test/cell_list_mpi_test.cc://! dimension test case for MPCD CellListGPU class
hoomd/mpcd/test/cell_list_mpi_test.cc:UP_TEST(mpcd_cell_list_gpu_dimensions)
hoomd/mpcd/test/cell_list_mpi_test.cc:            new ExecutionConfiguration(ExecutionConfiguration::GPU, std::vector<int>()));
hoomd/mpcd/test/cell_list_mpi_test.cc:        celllist_dimension_test<mpcd::CellListGPU>(exec_conf, true, false, false);
hoomd/mpcd/test/cell_list_mpi_test.cc:        celllist_dimension_test<mpcd::CellListGPU>(exec_conf, false, true, false);
hoomd/mpcd/test/cell_list_mpi_test.cc:        celllist_dimension_test<mpcd::CellListGPU>(exec_conf, false, false, true);
hoomd/mpcd/test/cell_list_mpi_test.cc:            new ExecutionConfiguration(ExecutionConfiguration::GPU, std::vector<int>()));
hoomd/mpcd/test/cell_list_mpi_test.cc:        celllist_dimension_test<mpcd::CellListGPU>(exec_conf, true, true, false);
hoomd/mpcd/test/cell_list_mpi_test.cc:        celllist_dimension_test<mpcd::CellListGPU>(exec_conf, true, false, true);
hoomd/mpcd/test/cell_list_mpi_test.cc:        celllist_dimension_test<mpcd::CellListGPU>(exec_conf, false, true, true);
hoomd/mpcd/test/cell_list_mpi_test.cc:            new ExecutionConfiguration(ExecutionConfiguration::GPU, std::vector<int>()));
hoomd/mpcd/test/cell_list_mpi_test.cc:        celllist_dimension_test<mpcd::CellListGPU>(exec_conf, true, true, true);
hoomd/mpcd/test/cell_list_mpi_test.cc://! basic test case for MPCD CellListGPU class
hoomd/mpcd/test/cell_list_mpi_test.cc:UP_TEST(mpcd_cell_list_gpu_basic_test)
hoomd/mpcd/test/cell_list_mpi_test.cc:    celllist_basic_test<mpcd::CellListGPU>(std::shared_ptr<ExecutionConfiguration>(
hoomd/mpcd/test/cell_list_mpi_test.cc:        new ExecutionConfiguration(ExecutionConfiguration::GPU)));
hoomd/mpcd/test/cell_list_mpi_test.cc://! edge test case for MPCD CellListGPU class
hoomd/mpcd/test/cell_list_mpi_test.cc:UP_TEST(mpcd_cell_list_gpu_edge_test)
hoomd/mpcd/test/cell_list_mpi_test.cc:    celllist_edge_test<mpcd::CellListGPU>(std::shared_ptr<ExecutionConfiguration>(
hoomd/mpcd/test/cell_list_mpi_test.cc:        new ExecutionConfiguration(ExecutionConfiguration::GPU)));
hoomd/mpcd/test/planar_pore_geometry_filler_mpi_test.cc:#include "hoomd/mpcd/PlanarPoreGeometryFillerGPU.h"
hoomd/mpcd/test/planar_pore_geometry_filler_mpi_test.cc:UP_TEST(planar_pore_fill_mpi_gpu)
hoomd/mpcd/test/planar_pore_geometry_filler_mpi_test.cc:    planar_pore_fill_mpi_test<mpcd::PlanarPoreGeometryFillerGPU>(
hoomd/mpcd/test/planar_pore_geometry_filler_mpi_test.cc:        std::make_shared<ExecutionConfiguration>(ExecutionConfiguration::GPU));
hoomd/mpcd/test/cell_list_test.cc:#include "hoomd/mpcd/CellListGPU.h"
hoomd/mpcd/test/cell_list_test.cc://! dimension test case for MPCD CellListGPU class
hoomd/mpcd/test/cell_list_test.cc:UP_TEST(mpcd_cell_list_gpu_dimensions)
hoomd/mpcd/test/cell_list_test.cc:    celllist_dimension_test<mpcd::CellListGPU>(std::shared_ptr<ExecutionConfiguration>(
hoomd/mpcd/test/cell_list_test.cc:        new ExecutionConfiguration(ExecutionConfiguration::GPU)));
hoomd/mpcd/test/cell_list_test.cc://! small system test case for MPCD CellListGPU class
hoomd/mpcd/test/cell_list_test.cc:UP_TEST(mpcd_cell_list_gpu_small_test)
hoomd/mpcd/test/cell_list_test.cc:    celllist_small_test<mpcd::CellListGPU>(std::shared_ptr<ExecutionConfiguration>(
hoomd/mpcd/test/cell_list_test.cc:        new ExecutionConfiguration(ExecutionConfiguration::GPU)));
hoomd/mpcd/test/cell_list_test.cc://! small system test case for MPCD CellListGPU class
hoomd/mpcd/test/cell_list_test.cc:UP_TEST(mpcd_cell_list_gpu_grid_shift_test)
hoomd/mpcd/test/cell_list_test.cc:    celllist_grid_shift_test<mpcd::CellListGPU>(std::shared_ptr<ExecutionConfiguration>(
hoomd/mpcd/test/cell_list_test.cc:        new ExecutionConfiguration(ExecutionConfiguration::GPU)));
hoomd/mpcd/test/cell_list_test.cc://! embedded particle test case for MPCD CellListGPU class
hoomd/mpcd/test/cell_list_test.cc:UP_TEST(mpcd_cell_list_gpu_embed_test)
hoomd/mpcd/test/cell_list_test.cc:    celllist_embed_test<mpcd::CellListGPU>(std::shared_ptr<ExecutionConfiguration>(
hoomd/mpcd/test/cell_list_test.cc:        new ExecutionConfiguration(ExecutionConfiguration::GPU)));
hoomd/mpcd/test/CMakeLists.txt:    # check for extra cuda files
hoomd/mpcd/test/CMakeLists.txt:        set(_cuda_sources ${TEST_EXE}.cu)
hoomd/mpcd/test/CMakeLists.txt:        set(_cuda_sources "")
hoomd/mpcd/test/CMakeLists.txt:    add_executable(${TEST_EXE} EXCLUDE_FROM_ALL ${TEST_SRC} ${_cuda_sources})
hoomd/mpcd/test/streaming_method_test.cc:#include "hoomd/mpcd/BulkStreamingMethodGPU.h"
hoomd/mpcd/test/streaming_method_test.cc:    streaming_method_basic_test<mpcd::BulkStreamingMethodGPU<mpcd::NoForce>>(
hoomd/mpcd/test/streaming_method_test.cc:        std::make_shared<ExecutionConfiguration>(ExecutionConfiguration::GPU));
hoomd/mpcd/test/at_collision_method_test.cc:#include "hoomd/mpcd/ATCollisionMethodGPU.h"
hoomd/mpcd/test/at_collision_method_test.cc://! basic test case for MPCD ATCollisionMethodGPU class
hoomd/mpcd/test/at_collision_method_test.cc:UP_TEST(at_collision_method_basic_gpu)
hoomd/mpcd/test/at_collision_method_test.cc:    at_collision_method_basic_test<mpcd::ATCollisionMethodGPU>(
hoomd/mpcd/test/at_collision_method_test.cc:        std::make_shared<ExecutionConfiguration>(ExecutionConfiguration::GPU));
hoomd/mpcd/test/at_collision_method_test.cc://! test embedding of particles into the MPCD ATCollisionMethodGPU class
hoomd/mpcd/test/at_collision_method_test.cc:UP_TEST(at_collision_method_embed_gpu)
hoomd/mpcd/test/at_collision_method_test.cc:    at_collision_method_embed_test<mpcd::ATCollisionMethodGPU>(
hoomd/mpcd/test/at_collision_method_test.cc:        std::make_shared<ExecutionConfiguration>(ExecutionConfiguration::GPU));
hoomd/mpcd/test/cell_thermo_compute_test.cc:#include "hoomd/mpcd/CellThermoComputeGPU.h"
hoomd/mpcd/test/cell_thermo_compute_test.cc:UP_TEST(mpcd_cell_thermo_basic_gpu)
hoomd/mpcd/test/cell_thermo_compute_test.cc:    cell_thermo_basic_test<mpcd::CellThermoComputeGPU>(std::shared_ptr<ExecutionConfiguration>(
hoomd/mpcd/test/cell_thermo_compute_test.cc:        new ExecutionConfiguration(ExecutionConfiguration::GPU)));
hoomd/mpcd/test/cell_thermo_compute_test.cc:UP_TEST(mpcd_cell_thermo_embed_gpu)
hoomd/mpcd/test/cell_thermo_compute_test.cc:    cell_thermo_embed_test<mpcd::CellThermoComputeGPU>(std::shared_ptr<ExecutionConfiguration>(
hoomd/mpcd/test/cell_thermo_compute_test.cc:        new ExecutionConfiguration(ExecutionConfiguration::GPU)));
hoomd/mpcd/test/rejection_filler_mpi_test.cc:#include "hoomd/mpcd/RejectionVirtualParticleFillerGPU.h"
hoomd/mpcd/test/rejection_filler_mpi_test.cc:UP_TEST(sphere_rejection_fill_mpi_gpu)
hoomd/mpcd/test/rejection_filler_mpi_test.cc:    sphere_rejection_fill_mpi_test<mpcd::RejectionVirtualParticleFillerGPU<mpcd::SphereGeometry>>(
hoomd/mpcd/test/rejection_filler_mpi_test.cc:        std::make_shared<ExecutionConfiguration>(ExecutionConfiguration::GPU));
hoomd/mpcd/test/parallel_plate_geometry_filler_mpi_test.cc:#include "hoomd/mpcd/ParallelPlateGeometryFillerGPU.h"
hoomd/mpcd/test/parallel_plate_geometry_filler_mpi_test.cc:UP_TEST(parallel_plate_fill_mpi_gpu)
hoomd/mpcd/test/parallel_plate_geometry_filler_mpi_test.cc:    parallel_plate_fill_mpi_test<mpcd::ParallelPlateGeometryFillerGPU>(
hoomd/mpcd/test/parallel_plate_geometry_filler_mpi_test.cc:        std::make_shared<ExecutionConfiguration>(ExecutionConfiguration::GPU));
hoomd/mpcd/test/cell_communicator_mpi_test.cc:    GPUArray<double3> props(ci.getNumElements(), exec_conf);
hoomd/mpcd/test/cell_communicator_mpi_test.cc:    GPUArray<double3> ref_props(ci.getNumElements(), exec_conf);
hoomd/mpcd/test/cell_communicator_mpi_test.cc:    GPUArray<double4> props(ci.getNumElements(), exec_conf);
hoomd/mpcd/test/cell_communicator_mpi_test.cc:UP_TEST(mpcd_cell_communicator_gpu)
hoomd/mpcd/test/cell_communicator_mpi_test.cc:    if (!exec_conf_gpu)
hoomd/mpcd/test/cell_communicator_mpi_test.cc:        exec_conf_gpu = std::shared_ptr<ExecutionConfiguration>(
hoomd/mpcd/test/cell_communicator_mpi_test.cc:            new ExecutionConfiguration(ExecutionConfiguration::GPU));
hoomd/mpcd/test/cell_communicator_mpi_test.cc:        exec_conf_gpu->getMPIConfig()->splitPartitions(2);
hoomd/mpcd/test/cell_communicator_mpi_test.cc:        cell_communicator_reduce_test(exec_conf_gpu, true, false, false);
hoomd/mpcd/test/cell_communicator_mpi_test.cc:        cell_communicator_reduce_test(exec_conf_gpu, false, true, false);
hoomd/mpcd/test/cell_communicator_mpi_test.cc:        cell_communicator_reduce_test(exec_conf_gpu, false, false, true);
hoomd/mpcd/test/cell_communicator_mpi_test.cc:        exec_conf_gpu->getMPIConfig()->splitPartitions(4);
hoomd/mpcd/test/cell_communicator_mpi_test.cc:        cell_communicator_reduce_test(exec_conf_gpu, true, true, false);
hoomd/mpcd/test/cell_communicator_mpi_test.cc:        cell_communicator_reduce_test(exec_conf_gpu, true, false, true);
hoomd/mpcd/test/cell_communicator_mpi_test.cc:        cell_communicator_reduce_test(exec_conf_gpu, false, true, true);
hoomd/mpcd/test/cell_communicator_mpi_test.cc:        exec_conf_gpu->getMPIConfig()->splitPartitions(8);
hoomd/mpcd/test/cell_communicator_mpi_test.cc:        cell_communicator_reduce_test(exec_conf_gpu, true, true, true);
hoomd/mpcd/test/srd_collision_method_test.cc:#include "hoomd/mpcd/SRDCollisionMethodGPU.h"
hoomd/mpcd/test/srd_collision_method_test.cc://! basic test case for MPCD SRDCollisionMethodGPU class
hoomd/mpcd/test/srd_collision_method_test.cc:UP_TEST(srd_collision_method_basic_gpu)
hoomd/mpcd/test/srd_collision_method_test.cc:    srd_collision_method_basic_test<mpcd::SRDCollisionMethodGPU>(
hoomd/mpcd/test/srd_collision_method_test.cc:        std::make_shared<ExecutionConfiguration>(ExecutionConfiguration::GPU));
hoomd/mpcd/test/srd_collision_method_test.cc://! test distribution of random rotation vectors for the MPCD SRDCollisionMethodGPU class
hoomd/mpcd/test/srd_collision_method_test.cc:UP_TEST(srd_collision_method_rotvec_gpu)
hoomd/mpcd/test/srd_collision_method_test.cc:    srd_collision_method_rotvec_test<mpcd::SRDCollisionMethodGPU>(
hoomd/mpcd/test/srd_collision_method_test.cc:        std::make_shared<ExecutionConfiguration>(ExecutionConfiguration::GPU));
hoomd/mpcd/test/srd_collision_method_test.cc://! test embedding of particles into the MPCD SRDCollisionMethodGPU class
hoomd/mpcd/test/srd_collision_method_test.cc:UP_TEST(srd_collision_method_embed_gpu)
hoomd/mpcd/test/srd_collision_method_test.cc:    srd_collision_method_embed_test<mpcd::SRDCollisionMethodGPU>(
hoomd/mpcd/test/srd_collision_method_test.cc:        std::make_shared<ExecutionConfiguration>(ExecutionConfiguration::GPU));
hoomd/mpcd/test/srd_collision_method_test.cc:UP_TEST(srd_collision_method_thermostat_gpu)
hoomd/mpcd/test/srd_collision_method_test.cc:    srd_collision_method_thermostat_test<mpcd::SRDCollisionMethodGPU>(
hoomd/mpcd/test/srd_collision_method_test.cc:        std::make_shared<ExecutionConfiguration>(ExecutionConfiguration::GPU));
hoomd/mpcd/test/cell_thermo_compute_mpi_test.cc:#include "hoomd/mpcd/CellThermoComputeGPU.h"
hoomd/mpcd/test/cell_thermo_compute_mpi_test.cc:UP_TEST(mpcd_cell_thermo_basic_gpu)
hoomd/mpcd/test/cell_thermo_compute_mpi_test.cc:    cell_thermo_basic_test<mpcd::CellThermoComputeGPU>(std::shared_ptr<ExecutionConfiguration>(
hoomd/mpcd/test/cell_thermo_compute_mpi_test.cc:        new ExecutionConfiguration(ExecutionConfiguration::GPU)));
hoomd/mpcd/test/parallel_plate_geometry_filler_test.cc:#include "hoomd/mpcd/ParallelPlateGeometryFillerGPU.h"
hoomd/mpcd/test/parallel_plate_geometry_filler_test.cc:UP_TEST(parallel_plate_fill_basic_gpu)
hoomd/mpcd/test/parallel_plate_geometry_filler_test.cc:    parallel_plate_fill_basic_test<mpcd::ParallelPlateGeometryFillerGPU>(
hoomd/mpcd/test/parallel_plate_geometry_filler_test.cc:        std::make_shared<ExecutionConfiguration>(ExecutionConfiguration::GPU));
hoomd/mpcd/test/planar_pore_geometry_filler_test.cc:#include "hoomd/mpcd/PlanarPoreGeometryFillerGPU.h"
hoomd/mpcd/test/planar_pore_geometry_filler_test.cc:UP_TEST(planar_pore_fill_basic_gpu)
hoomd/mpcd/test/planar_pore_geometry_filler_test.cc:    planar_pore_fill_basic_test<mpcd::PlanarPoreGeometryFillerGPU>(
hoomd/mpcd/test/planar_pore_geometry_filler_test.cc:        std::make_shared<ExecutionConfiguration>(ExecutionConfiguration::GPU));
hoomd/mpcd/CellThermoComputeGPU.cc: * \file mpcd/CellThermoComputeGPU.cc
hoomd/mpcd/CellThermoComputeGPU.cc: * \brief Definition of mpcd::CellThermoComputeGPU
hoomd/mpcd/CellThermoComputeGPU.cc:#include "CellThermoComputeGPU.h"
hoomd/mpcd/CellThermoComputeGPU.cc:mpcd::CellThermoComputeGPU::CellThermoComputeGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/mpcd/CellThermoComputeGPU.cc:mpcd::CellThermoComputeGPU::~CellThermoComputeGPU() { }
hoomd/mpcd/CellThermoComputeGPU.cc:void mpcd::CellThermoComputeGPU::beginOuterCellProperties()
hoomd/mpcd/CellThermoComputeGPU.cc:        gpu::begin_cell_thermo(args, d_cells.data, m_vel_comm->getNCells(), block_size, tpp);
hoomd/mpcd/CellThermoComputeGPU.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/mpcd/CellThermoComputeGPU.cc:            CHECK_CUDA_ERROR();
hoomd/mpcd/CellThermoComputeGPU.cc:        gpu::begin_cell_thermo(args, d_cells.data, m_vel_comm->getNCells(), block_size, tpp);
hoomd/mpcd/CellThermoComputeGPU.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/mpcd/CellThermoComputeGPU.cc:            CHECK_CUDA_ERROR();
hoomd/mpcd/CellThermoComputeGPU.cc:void mpcd::CellThermoComputeGPU::finishOuterCellProperties()
hoomd/mpcd/CellThermoComputeGPU.cc:    gpu::end_cell_thermo(d_cell_vel.data,
hoomd/mpcd/CellThermoComputeGPU.cc:    if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/mpcd/CellThermoComputeGPU.cc:        CHECK_CUDA_ERROR();
hoomd/mpcd/CellThermoComputeGPU.cc:void mpcd::CellThermoComputeGPU::calcInnerCellProperties()
hoomd/mpcd/CellThermoComputeGPU.cc:        gpu::inner_cell_thermo(args, ci, inner_ci, lo, m_sysdef->getNDimensions(), block_size, tpp);
hoomd/mpcd/CellThermoComputeGPU.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/mpcd/CellThermoComputeGPU.cc:            CHECK_CUDA_ERROR();
hoomd/mpcd/CellThermoComputeGPU.cc:        gpu::inner_cell_thermo(args, ci, inner_ci, lo, m_sysdef->getNDimensions(), block_size, tpp);
hoomd/mpcd/CellThermoComputeGPU.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/mpcd/CellThermoComputeGPU.cc:            CHECK_CUDA_ERROR();
hoomd/mpcd/CellThermoComputeGPU.cc:void mpcd::CellThermoComputeGPU::computeNetProperties()
hoomd/mpcd/CellThermoComputeGPU.cc:        mpcd::gpu::stage_net_cell_thermo(d_tmp_thermo.data,
hoomd/mpcd/CellThermoComputeGPU.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/mpcd/CellThermoComputeGPU.cc:            CHECK_CUDA_ERROR();
hoomd/mpcd/CellThermoComputeGPU.cc:        // use cub to reduce the properties on the gpu
hoomd/mpcd/CellThermoComputeGPU.cc:        mpcd::gpu::reduce_net_cell_thermo(m_reduced.getDeviceFlags(),
hoomd/mpcd/CellThermoComputeGPU.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/mpcd/CellThermoComputeGPU.cc:            CHECK_CUDA_ERROR();
hoomd/mpcd/CellThermoComputeGPU.cc:        mpcd::gpu::reduce_net_cell_thermo(m_reduced.getDeviceFlags(),
hoomd/mpcd/CellThermoComputeGPU.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/mpcd/CellThermoComputeGPU.cc:            CHECK_CUDA_ERROR();
hoomd/mpcd/CellThermoComputeGPU.cc:void export_CellThermoComputeGPU(pybind11::module& m)
hoomd/mpcd/CellThermoComputeGPU.cc:    pybind11::class_<mpcd::CellThermoComputeGPU,
hoomd/mpcd/CellThermoComputeGPU.cc:                     std::shared_ptr<mpcd::CellThermoComputeGPU>>(m, "CellThermoComputeGPU")
hoomd/mpcd/stream.py:        if isinstance(sim.device, hoomd.device.GPU):
hoomd/mpcd/stream.py:            class_info[1] += "GPU"
hoomd/mpcd/stream.py:        if isinstance(sim.device, hoomd.device.GPU):
hoomd/mpcd/stream.py:            class_info[1] += "GPU"
hoomd/mpcd/RejectionVirtualParticleFillerGPU.h: * \file mpcd/RejectionVirtualParticleFillerGPU.cuh
hoomd/mpcd/RejectionVirtualParticleFillerGPU.h: * \brief Declaration and definition of CUDA kernels for RejectionVirtualParticleFillerGPU
hoomd/mpcd/RejectionVirtualParticleFillerGPU.h:#ifndef MPCD_REJECTION_VIRTUAL_PARTICLE_FILLER_GPU_H_
hoomd/mpcd/RejectionVirtualParticleFillerGPU.h:#define MPCD_REJECTION_VIRTUAL_PARTICLE_FILLER_GPU_H_
hoomd/mpcd/RejectionVirtualParticleFillerGPU.h:#include "RejectionVirtualParticleFillerGPU.cuh"
hoomd/mpcd/RejectionVirtualParticleFillerGPU.h://! Adds virtual particles to the MPCD particle data for various confining geometries using the GPU
hoomd/mpcd/RejectionVirtualParticleFillerGPU.h:class PYBIND11_EXPORT RejectionVirtualParticleFillerGPU
hoomd/mpcd/RejectionVirtualParticleFillerGPU.h:    RejectionVirtualParticleFillerGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/mpcd/RejectionVirtualParticleFillerGPU.h:    GPUArray<bool> m_keep_particles; // Track whether particles are in/out of bounds for geometry
hoomd/mpcd/RejectionVirtualParticleFillerGPU.h:    GPUArray<unsigned int> m_keep_indices;  // Indices for particles out of bound for geometry
hoomd/mpcd/RejectionVirtualParticleFillerGPU.h:    GPUFlags<unsigned int> m_num_keep;      // Number of particles to keep
hoomd/mpcd/RejectionVirtualParticleFillerGPU.h:void RejectionVirtualParticleFillerGPU<Geometry>::fill(unsigned int timestep)
hoomd/mpcd/RejectionVirtualParticleFillerGPU.h:        GPUArray<Scalar4> tmp_pos(num_virtual_max, this->m_exec_conf);
hoomd/mpcd/RejectionVirtualParticleFillerGPU.h:        GPUArray<Scalar4> tmp_vel(num_virtual_max, this->m_exec_conf);
hoomd/mpcd/RejectionVirtualParticleFillerGPU.h:        GPUArray<bool> keep_particles(num_virtual_max, this->m_exec_conf);
hoomd/mpcd/RejectionVirtualParticleFillerGPU.h:        GPUArray<unsigned int> keep_indices(num_virtual_max, this->m_exec_conf);
hoomd/mpcd/RejectionVirtualParticleFillerGPU.h:    mpcd::gpu::draw_virtual_particles_args_t args(d_tmp_pos.data,
hoomd/mpcd/RejectionVirtualParticleFillerGPU.h:    mpcd::gpu::draw_virtual_particles<Geometry>(args, *(this->m_geom));
hoomd/mpcd/RejectionVirtualParticleFillerGPU.h:    if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/mpcd/RejectionVirtualParticleFillerGPU.h:        CHECK_CUDA_ERROR();
hoomd/mpcd/RejectionVirtualParticleFillerGPU.h:        // on GPU, we need to compact the selected particles down with CUB
hoomd/mpcd/RejectionVirtualParticleFillerGPU.h:        mpcd::gpu::compact_virtual_particle_indices(d_tmp_storage,
hoomd/mpcd/RejectionVirtualParticleFillerGPU.h:        mpcd::gpu::compact_virtual_particle_indices(d_tmp_storage,
hoomd/mpcd/RejectionVirtualParticleFillerGPU.h:    mpcd::gpu::copy_virtual_particles(d_keep_indices.data,
hoomd/mpcd/RejectionVirtualParticleFillerGPU.h:    if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/mpcd/RejectionVirtualParticleFillerGPU.h:        CHECK_CUDA_ERROR();
hoomd/mpcd/RejectionVirtualParticleFillerGPU.h://! Export RejectionVirtualParticleFillerGPU to python
hoomd/mpcd/RejectionVirtualParticleFillerGPU.h:template<class Geometry> void export_RejectionVirtualParticleFillerGPU(pybind11::module& m)
hoomd/mpcd/RejectionVirtualParticleFillerGPU.h:    const std::string name = Geometry::getName() + "GeometryFillerGPU";
hoomd/mpcd/RejectionVirtualParticleFillerGPU.h:    py::class_<mpcd::RejectionVirtualParticleFillerGPU<Geometry>,
hoomd/mpcd/RejectionVirtualParticleFillerGPU.h:               std::shared_ptr<mpcd::RejectionVirtualParticleFillerGPU<Geometry>>>(
hoomd/mpcd/RejectionVirtualParticleFillerGPU.h:#endif // MPCD_REJECTION_VIRTUAL_PARTICLE_FILLER_GPU_H_
hoomd/mpcd/CellCommunicator.cuh: * \brief Declaration of CUDA kernels for mpcd::CellCommunicator
hoomd/mpcd/CellCommunicator.cuh:#include <cuda_runtime.h>
hoomd/mpcd/CellCommunicator.cuh:namespace gpu
hoomd/mpcd/CellCommunicator.cuh:cudaError_t __attribute__((visibility("default")))
hoomd/mpcd/CellCommunicator.cuh:cudaError_t __attribute__((visibility("default")))
hoomd/mpcd/CellCommunicator.cuh: * \returns cudaSuccess on completion
hoomd/mpcd/CellCommunicator.cuh: * \sa mpcd::gpu::kernel::pack_cell_buffer
hoomd/mpcd/CellCommunicator.cuh:cudaError_t pack_cell_buffer(typename PackOpT::element* d_send_buf,
hoomd/mpcd/CellCommunicator.cuh:    cudaFuncAttributes attr;
hoomd/mpcd/CellCommunicator.cuh:    cudaFuncGetAttributes(&attr, (const void*)mpcd::gpu::kernel::pack_cell_buffer<T, PackOpT>);
hoomd/mpcd/CellCommunicator.cuh:    mpcd::gpu::kernel::pack_cell_buffer<<<grid, run_block_size>>>(d_send_buf,
hoomd/mpcd/CellCommunicator.cuh:    return cudaSuccess;
hoomd/mpcd/CellCommunicator.cuh: * \returns cudaSuccess on completion
hoomd/mpcd/CellCommunicator.cuh: * \sa mpcd::gpu::kernel::unpack_cell_buffer
hoomd/mpcd/CellCommunicator.cuh:cudaError_t unpack_cell_buffer(T* d_props,
hoomd/mpcd/CellCommunicator.cuh:    cudaFuncAttributes attr;
hoomd/mpcd/CellCommunicator.cuh:    cudaFuncGetAttributes(&attr, (const void*)mpcd::gpu::kernel::unpack_cell_buffer<T, PackOpT>);
hoomd/mpcd/CellCommunicator.cuh:    mpcd::gpu::kernel::unpack_cell_buffer<<<grid, run_block_size>>>(d_props,
hoomd/mpcd/CellCommunicator.cuh:    return cudaSuccess;
hoomd/mpcd/CellCommunicator.cuh:    } // end namespace gpu
hoomd/mpcd/CommunicatorGPU.h: * \file mpcd/CommunicatorGPU.h
hoomd/mpcd/CommunicatorGPU.h: * \brief Defines the mpcd::CommunicatorGPU class
hoomd/mpcd/CommunicatorGPU.h:#ifndef MPCD_COMMUNICATOR_GPU_H_
hoomd/mpcd/CommunicatorGPU.h:#define MPCD_COMMUNICATOR_GPU_H_
hoomd/mpcd/CommunicatorGPU.h:#include "hoomd/GPUFlags.h"
hoomd/mpcd/CommunicatorGPU.h:#include "hoomd/GPUVector.h"
hoomd/mpcd/CommunicatorGPU.h://! MPI communication of MPCD particle data on the GPU
hoomd/mpcd/CommunicatorGPU.h: * are used in parallel simulations on the GPU. A domain decomposition communication pattern
hoomd/mpcd/CommunicatorGPU.h: * (::CommunicatorGPU::migrateParticles).
hoomd/mpcd/CommunicatorGPU.h: * There is unfortunately significant code duplication with ::CommunicatorGPU, but
hoomd/mpcd/CommunicatorGPU.h: * communication base class for the GPU.
hoomd/mpcd/CommunicatorGPU.h:class PYBIND11_EXPORT CommunicatorGPU : public mpcd::Communicator
hoomd/mpcd/CommunicatorGPU.h:    CommunicatorGPU(std::shared_ptr<SystemDefinition> sysdef);
hoomd/mpcd/CommunicatorGPU.h:    virtual ~CommunicatorGPU();
hoomd/mpcd/CommunicatorGPU.h:    //! Set the communication flags for the particle data on the GPU
hoomd/mpcd/CommunicatorGPU.h:    GPUArray<unsigned int> m_neigh_send;     //!< Neighbor rank indexes for sending
hoomd/mpcd/CommunicatorGPU.h:    GPUArray<unsigned int> m_num_send;       //!< Number of particles to send to each rank
hoomd/mpcd/CommunicatorGPU.h:    GPUVector<unsigned int> m_tmp_keys;      //!< Temporary keys for sorting particles
hoomd/mpcd/CommunicatorGPU.h:#endif // MPCD_COMMUNICATOR_GPU_H_
hoomd/mpcd/ParallelPlateGeometryFillerGPU.h: * \file mpcd/ParallelPlateGeometryFillerGPU.h
hoomd/mpcd/ParallelPlateGeometryFillerGPU.h: * \brief Definition of virtual particle filler for mpcd::ParallelPlateGeometry on the GPU.
hoomd/mpcd/ParallelPlateGeometryFillerGPU.h:#ifndef MPCD_PARALLEL_PLATE_GEOMETRY_FILLER_GPU_H_
hoomd/mpcd/ParallelPlateGeometryFillerGPU.h:#define MPCD_PARALLEL_PLATE_GEOMETRY_FILLER_GPU_H_
hoomd/mpcd/ParallelPlateGeometryFillerGPU.h://! Adds virtual particles to the MPCD particle data for ParallelPlateGeometry using the GPU
hoomd/mpcd/ParallelPlateGeometryFillerGPU.h:class PYBIND11_EXPORT ParallelPlateGeometryFillerGPU : public mpcd::ParallelPlateGeometryFiller
hoomd/mpcd/ParallelPlateGeometryFillerGPU.h:    ParallelPlateGeometryFillerGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/mpcd/ParallelPlateGeometryFillerGPU.h:    //! Draw particles within the fill volume on the GPU
hoomd/mpcd/ParallelPlateGeometryFillerGPU.h:#endif // MPCD_PARALLEL_PLATE_GEOMETRY_FILLER_GPU_H_
hoomd/mpcd/collide.py:        if isinstance(sim.device, hoomd.device.GPU):
hoomd/mpcd/collide.py:            cpp_class = _mpcd.CellListGPU
hoomd/mpcd/collide.py:        if isinstance(sim.device, hoomd.device.GPU):
hoomd/mpcd/collide.py:            cpp_class = _mpcd.ATCollisionMethodGPU
hoomd/mpcd/collide.py:        if isinstance(sim.device, hoomd.device.GPU):
hoomd/mpcd/collide.py:            cpp_class = _mpcd.SRDCollisionMethodGPU
hoomd/mpcd/BounceBackStreamingMethodGPU.cu.inc: * \file mpcd/BounceBackStreamingMethod@_geometry@@_force@GPU.cu
hoomd/mpcd/BounceBackStreamingMethodGPU.cu.inc: * \brief Template instantation for BounceBackStreamingMethodGPU driver (and so kernel) with
hoomd/mpcd/BounceBackStreamingMethodGPU.cu.inc:#include "hoomd/mpcd/BounceBackStreamingMethodGPU.cuh"
hoomd/mpcd/BounceBackStreamingMethodGPU.cu.inc:namespace gpu
hoomd/mpcd/BounceBackStreamingMethodGPU.cu.inc:template cudaError_t __attribute__((visibility("default")))
hoomd/mpcd/BounceBackStreamingMethodGPU.cu.inc:    } // end namespace gpu
hoomd/mpcd/RejectionVirtualParticleFillerGPU.cc.inc: * \file mpcd/RejectionVirtualParticleFiller@_geometry@GPU.cc
hoomd/mpcd/RejectionVirtualParticleFillerGPU.cc.inc: * \brief Template instantation for RejectionVirtualParticleFillerGPU with @_geometry@.
hoomd/mpcd/RejectionVirtualParticleFillerGPU.cc.inc:#include "hoomd/mpcd/RejectionVirtualParticleFillerGPU.h"
hoomd/mpcd/RejectionVirtualParticleFillerGPU.cc.inc:#define EXPORT_FUNCTION export_@_geometry@FillerGPU
hoomd/mpcd/RejectionVirtualParticleFillerGPU.cc.inc:// Explicit instantiation of GPU class
hoomd/mpcd/RejectionVirtualParticleFillerGPU.cc.inc:template class RejectionVirtualParticleFillerGPU<GEOMETRY_CLASS>;
hoomd/mpcd/RejectionVirtualParticleFillerGPU.cc.inc:    export_RejectionVirtualParticleFillerGPU<GEOMETRY_CLASS>(m);
hoomd/mpcd/ParticleData.cc:    GPUArray<Scalar4> pos(N_max, m_exec_conf);
hoomd/mpcd/ParticleData.cc:    GPUArray<Scalar4> vel(N_max, m_exec_conf);
hoomd/mpcd/ParticleData.cc:    GPUArray<unsigned int> tag(N_max, m_exec_conf);
hoomd/mpcd/ParticleData.cc:        GPUArray<unsigned int> comm_flags(N_max, m_exec_conf);
hoomd/mpcd/ParticleData.cc:    GPUArray<Scalar4> pos_alt(N_max, m_exec_conf);
hoomd/mpcd/ParticleData.cc:    GPUArray<Scalar4> vel_alt(N_max, m_exec_conf);
hoomd/mpcd/ParticleData.cc:    GPUArray<unsigned int> tag_alt(N_max, m_exec_conf);
hoomd/mpcd/ParticleData.cc:        GPUArray<unsigned int> comm_flags_alt(N_max, m_exec_conf);
hoomd/mpcd/ParticleData.cc:        GPUArray<unsigned int> remove_ids(N_max, m_exec_conf);
hoomd/mpcd/ParticleData.cc:        GPUFlags<unsigned int> num_remove(m_exec_conf);
hoomd/mpcd/ParticleData.cc:        GPUArray<unsigned char> remove_flags(N_max, m_exec_conf);
hoomd/mpcd/ParticleData.cc: * This has the behavior defined by GPUArray for resizing.
hoomd/mpcd/ParticleData.cc:void mpcd::ParticleData::removeParticles(GPUVector<mpcd::detail::pdata_element>& out,
hoomd/mpcd/ParticleData.cc:void mpcd::ParticleData::addParticles(const GPUVector<mpcd::detail::pdata_element>& in,
hoomd/mpcd/ParticleData.cc: * into a buffer and removes them from the particle data arrays using the GPU.
hoomd/mpcd/ParticleData.cc:void mpcd::ParticleData::removeParticlesGPU(GPUVector<mpcd::detail::pdata_element>& out,
hoomd/mpcd/ParticleData.cc:        mpcd::gpu::mark_removed_particles(d_remove_flags.data,
hoomd/mpcd/ParticleData.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/mpcd/ParticleData.cc:            CHECK_CUDA_ERROR();
hoomd/mpcd/ParticleData.cc:        mpcd::gpu::partition_particles(d_tmp,
hoomd/mpcd/ParticleData.cc:        mpcd::gpu::partition_particles(d_tmp,
hoomd/mpcd/ParticleData.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/mpcd/ParticleData.cc:            CHECK_CUDA_ERROR();
hoomd/mpcd/ParticleData.cc:        mpcd::gpu::remove_particles(d_out.data,
hoomd/mpcd/ParticleData.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/mpcd/ParticleData.cc:            CHECK_CUDA_ERROR();
hoomd/mpcd/ParticleData.cc:void mpcd::ParticleData::addParticlesGPU(const GPUVector<mpcd::detail::pdata_element>& in,
hoomd/mpcd/ParticleData.cc:        // add new particles on GPU
hoomd/mpcd/ParticleData.cc:        mpcd::gpu::add_particles(old_nparticles,
hoomd/mpcd/ParticleData.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/mpcd/ParticleData.cc:            CHECK_CUDA_ERROR();
hoomd/mpcd/ParticleData.cc:    if (m_exec_conf->isCUDAEnabled())
hoomd/mpcd/SRDCollisionMethodGPU.cu: * \file mpcd/SRDCollisionMethodGPU.cu
hoomd/mpcd/SRDCollisionMethodGPU.cu: * \brief Defines GPU functions and kernels used by mpcd::SRDCollisionMethodGPU
hoomd/mpcd/SRDCollisionMethodGPU.cu:#include "SRDCollisionMethodGPU.cuh"
hoomd/mpcd/SRDCollisionMethodGPU.cu:namespace gpu
hoomd/mpcd/SRDCollisionMethodGPU.cu:cudaError_t srd_draw_vectors(double3* d_rotvec,
hoomd/mpcd/SRDCollisionMethodGPU.cu:        cudaFuncAttributes attr;
hoomd/mpcd/SRDCollisionMethodGPU.cu:        cudaFuncGetAttributes(&attr, (const void*)mpcd::gpu::kernel::srd_draw_vectors<true>);
hoomd/mpcd/SRDCollisionMethodGPU.cu:        mpcd::gpu::kernel::srd_draw_vectors<true><<<grid, run_block_size>>>(d_rotvec,
hoomd/mpcd/SRDCollisionMethodGPU.cu:        cudaFuncAttributes attr;
hoomd/mpcd/SRDCollisionMethodGPU.cu:        cudaFuncGetAttributes(&attr, (const void*)mpcd::gpu::kernel::srd_draw_vectors<false>);
hoomd/mpcd/SRDCollisionMethodGPU.cu:        mpcd::gpu::kernel::srd_draw_vectors<false><<<grid, run_block_size>>>(d_rotvec,
hoomd/mpcd/SRDCollisionMethodGPU.cu:    return cudaSuccess;
hoomd/mpcd/SRDCollisionMethodGPU.cu:cudaError_t srd_rotate(Scalar4* d_vel,
hoomd/mpcd/SRDCollisionMethodGPU.cu:    cudaFuncAttributes attr;
hoomd/mpcd/SRDCollisionMethodGPU.cu:    cudaFuncGetAttributes(&attr, (const void*)mpcd::gpu::kernel::srd_rotate);
hoomd/mpcd/SRDCollisionMethodGPU.cu:    mpcd::gpu::kernel::srd_rotate<<<grid, run_block_size>>>(d_vel,
hoomd/mpcd/SRDCollisionMethodGPU.cu:    return cudaSuccess;
hoomd/mpcd/SRDCollisionMethodGPU.cu:    } // end namespace gpu
hoomd/mpcd/PlanarPoreGeometryFillerGPU.cc: * \file mpcd/PlanarPoreGeometryFillerGPU.cc
hoomd/mpcd/PlanarPoreGeometryFillerGPU.cc: * \brief Definition of mpcd::PlanarPoreGeometryFillerGPU
hoomd/mpcd/PlanarPoreGeometryFillerGPU.cc:#include "PlanarPoreGeometryFillerGPU.h"
hoomd/mpcd/PlanarPoreGeometryFillerGPU.cc:#include "PlanarPoreGeometryFillerGPU.cuh"
hoomd/mpcd/PlanarPoreGeometryFillerGPU.cc:mpcd::PlanarPoreGeometryFillerGPU::PlanarPoreGeometryFillerGPU(
hoomd/mpcd/PlanarPoreGeometryFillerGPU.cc:void mpcd::PlanarPoreGeometryFillerGPU::drawParticles(uint64_t timestep)
hoomd/mpcd/PlanarPoreGeometryFillerGPU.cc:    mpcd::gpu::slit_pore_draw_particles(d_pos.data,
hoomd/mpcd/PlanarPoreGeometryFillerGPU.cc:    if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/mpcd/PlanarPoreGeometryFillerGPU.cc:        CHECK_CUDA_ERROR();
hoomd/mpcd/PlanarPoreGeometryFillerGPU.cc:void export_PlanarPoreGeometryFillerGPU(pybind11::module& m)
hoomd/mpcd/PlanarPoreGeometryFillerGPU.cc:    pybind11::class_<mpcd::PlanarPoreGeometryFillerGPU,
hoomd/mpcd/PlanarPoreGeometryFillerGPU.cc:                     std::shared_ptr<mpcd::PlanarPoreGeometryFillerGPU>>(
hoomd/mpcd/PlanarPoreGeometryFillerGPU.cc:        "PlanarPoreGeometryFillerGPU")
hoomd/mpcd/CellListGPU.h: * \file mpcd/CellListGPU.h
hoomd/mpcd/CellListGPU.h: * \brief Declaration of mpcd::CellListGPU
hoomd/mpcd/CellListGPU.h:#ifndef MPCD_CELL_LIST_GPU_H_
hoomd/mpcd/CellListGPU.h:#define MPCD_CELL_LIST_GPU_H_
hoomd/mpcd/CellListGPU.h://! Computes the MPCD cell list on the GPU
hoomd/mpcd/CellListGPU.h:class PYBIND11_EXPORT CellListGPU : public mpcd::CellList
hoomd/mpcd/CellListGPU.h:    CellListGPU(std::shared_ptr<SystemDefinition> sysdef, Scalar cell_size, bool shift);
hoomd/mpcd/CellListGPU.h:    virtual ~CellListGPU();
hoomd/mpcd/CellListGPU.h:    //! Compute the cell list of particles on the GPU
hoomd/mpcd/CellListGPU.h:    //! Callback to sort cell list on the GPU when particle data is sorted
hoomd/mpcd/CellListGPU.h:                      const GPUArray<unsigned int>& order,
hoomd/mpcd/CellListGPU.h:                      const GPUArray<unsigned int>& rorder);
hoomd/mpcd/CellListGPU.h:    //! Determine if embedded particles require migration on the gpu
hoomd/mpcd/CellListGPU.h:    GPUFlags<unsigned int> m_migrate_flag; //!< Flag to signal migration is needed
hoomd/mpcd/CellListGPU.h:#endif // MPCD_CELL_LIST_GPU_H_
hoomd/mpcd/CellListGPU.cuh:#ifndef MPCD_CELL_LIST_GPU_CUH_
hoomd/mpcd/CellListGPU.cuh:#define MPCD_CELL_LIST_GPU_CUH_
hoomd/mpcd/CellListGPU.cuh: * \file mpcd/CellListGPU.cuh
hoomd/mpcd/CellListGPU.cuh: * \brief Declaration of CUDA kernels for mpcd::CellListGPU
hoomd/mpcd/CellListGPU.cuh:#include <cuda_runtime.h>
hoomd/mpcd/CellListGPU.cuh:namespace gpu
hoomd/mpcd/CellListGPU.cuh:cudaError_t compute_cell_list(unsigned int* d_cell_np,
hoomd/mpcd/CellListGPU.cuh:cudaError_t cell_check_migrate_embed(unsigned int* d_migrate_flag,
hoomd/mpcd/CellListGPU.cuh:cudaError_t cell_apply_sort(unsigned int* d_cell_list,
hoomd/mpcd/CellListGPU.cuh:    } // end namespace gpu
hoomd/mpcd/CellListGPU.cuh:#endif // MPCD_CELL_LIST_GPU_CUH_
hoomd/mpcd/ParticleData.h:#include "hoomd/GPUArray.h"
hoomd/mpcd/ParticleData.h:#include "hoomd/GPUFlags.h"
hoomd/mpcd/ParticleData.h:#include "hoomd/GPUVector.h"
hoomd/mpcd/ParticleData.h:    const GPUArray<Scalar4>& getPositions() const
hoomd/mpcd/ParticleData.h:    const GPUArray<Scalar4>& getVelocities() const
hoomd/mpcd/ParticleData.h:    const GPUArray<unsigned int>& getTags() const
hoomd/mpcd/ParticleData.h:    const GPUArray<Scalar4>& getAltPositions() const
hoomd/mpcd/ParticleData.h:    const GPUArray<Scalar4>& getAltVelocities() const
hoomd/mpcd/ParticleData.h:    const GPUArray<unsigned int>& getAltTags() const
hoomd/mpcd/ParticleData.h:        void(uint64_t timestep, const GPUArray<unsigned int>&, const GPUArray<unsigned int>&)>
hoomd/mpcd/ParticleData.h:                    const GPUArray<unsigned int>& order,
hoomd/mpcd/ParticleData.h:                    const GPUArray<unsigned int>& rorder)
hoomd/mpcd/ParticleData.h:        GPUArray<unsigned int> order, rorder;
hoomd/mpcd/ParticleData.h:    void removeParticles(GPUVector<mpcd::detail::pdata_element>& out,
hoomd/mpcd/ParticleData.h:    void addParticles(const GPUVector<mpcd::detail::pdata_element>& in,
hoomd/mpcd/ParticleData.h:    //! Pack particle data into a buffer (GPU version)
hoomd/mpcd/ParticleData.h:    void removeParticlesGPU(GPUVector<mpcd::detail::pdata_element>& out,
hoomd/mpcd/ParticleData.h:    //! Add new local particles (GPU version)
hoomd/mpcd/ParticleData.h:    void addParticlesGPU(const GPUVector<mpcd::detail::pdata_element>& in,
hoomd/mpcd/ParticleData.h:    const GPUArray<unsigned int>& getCommFlags() const
hoomd/mpcd/ParticleData.h:    const GPUArray<unsigned int>& getAltCommFlags() const
hoomd/mpcd/ParticleData.h:    std::shared_ptr<const ExecutionConfiguration> m_exec_conf; //!< GPU execution configuration
hoomd/mpcd/ParticleData.h:    GPUArray<Scalar4> m_pos;                 //!< MPCD particle positions plus type
hoomd/mpcd/ParticleData.h:    GPUArray<Scalar4> m_vel;                 //!< MPCD particle velocities plus cell list id
hoomd/mpcd/ParticleData.h:    GPUArray<unsigned int> m_tag;            //!< MPCD particle tags
hoomd/mpcd/ParticleData.h:    GPUArray<unsigned int> m_comm_flags; //!< MPCD particle communication flags
hoomd/mpcd/ParticleData.h:    GPUArray<Scalar4> m_pos_alt;      //!< Alternate position array
hoomd/mpcd/ParticleData.h:    GPUArray<Scalar4> m_vel_alt;      //!< Alternate velocity array
hoomd/mpcd/ParticleData.h:    GPUArray<unsigned int> m_tag_alt; //!< Alternate tag array
hoomd/mpcd/ParticleData.h:    GPUArray<unsigned int> m_comm_flags_alt; //!< Alternate communication flags
hoomd/mpcd/ParticleData.h:    GPUArray<unsigned int> m_remove_ids;     //!< Partitioned indexes of particles to keep
hoomd/mpcd/ParticleData.h:    GPUArray<unsigned char> m_remove_flags; //!< Temporary flag to mark keeping particle
hoomd/mpcd/ParticleData.h:    GPUFlags<unsigned int> m_num_remove;    //!< Number of particles to remove
hoomd/mpcd/SorterGPU.cuh:#ifndef MPCD_SORTER_GPU_CUH_
hoomd/mpcd/SorterGPU.cuh:#define MPCD_SORTER_GPU_CUH_
hoomd/mpcd/SorterGPU.cuh: * \file mpcd/SorterGPU.cuh
hoomd/mpcd/SorterGPU.cuh: * \brief Declaration of CUDA kernels for mpcd::SorterGPU
hoomd/mpcd/SorterGPU.cuh:#include <cuda_runtime.h>
hoomd/mpcd/SorterGPU.cuh:namespace gpu
hoomd/mpcd/SorterGPU.cuh:cudaError_t sort_apply(Scalar4* d_pos_alt,
hoomd/mpcd/SorterGPU.cuh:cudaError_t sort_set_sentinel(unsigned int* d_cell_list,
hoomd/mpcd/SorterGPU.cuh:cudaError_t sort_gen_reverse(unsigned int* d_rorder,
hoomd/mpcd/SorterGPU.cuh:    } // end namespace gpu
hoomd/mpcd/SorterGPU.cuh:#endif // MPCD_SORTER_GPU_CUH_
hoomd/mpcd/CellListGPU.cc: * \file mpcd/CellListGPU.cc
hoomd/mpcd/CellListGPU.cc: * \brief Definition of mpcd::CellListGPU
hoomd/mpcd/CellListGPU.cc:#include "CellListGPU.h"
hoomd/mpcd/CellListGPU.cc:#include "CellListGPU.cuh"
hoomd/mpcd/CellListGPU.cc:mpcd::CellListGPU::CellListGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/mpcd/CellListGPU.cc:    GPUFlags<unsigned int> migrate_flag(m_exec_conf);
hoomd/mpcd/CellListGPU.cc:mpcd::CellListGPU::~CellListGPU() { }
hoomd/mpcd/CellListGPU.cc:void mpcd::CellListGPU::buildCellList()
hoomd/mpcd/CellListGPU.cc:        mpcd::gpu::compute_cell_list(d_cell_np.data,
hoomd/mpcd/CellListGPU.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/mpcd/CellListGPU.cc:            CHECK_CUDA_ERROR();
hoomd/mpcd/CellListGPU.cc:        mpcd::gpu::compute_cell_list(d_cell_np.data,
hoomd/mpcd/CellListGPU.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/mpcd/CellListGPU.cc:            CHECK_CUDA_ERROR();
hoomd/mpcd/CellListGPU.cc:void mpcd::CellListGPU::sort(uint64_t timestep,
hoomd/mpcd/CellListGPU.cc:                             const GPUArray<unsigned int>& order,
hoomd/mpcd/CellListGPU.cc:                             const GPUArray<unsigned int>& rorder)
hoomd/mpcd/CellListGPU.cc:    mpcd::gpu::cell_apply_sort(d_cell_list.data,
hoomd/mpcd/CellListGPU.cc:    if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/mpcd/CellListGPU.cc:        CHECK_CUDA_ERROR();
hoomd/mpcd/CellListGPU.cc:bool mpcd::CellListGPU::needsEmbedMigrate(uint64_t timestep)
hoomd/mpcd/CellListGPU.cc:    // check if any particles have left this rank on the gpu
hoomd/mpcd/CellListGPU.cc:    mpcd::gpu::cell_check_migrate_embed(m_migrate_flag.getDeviceFlags(),
hoomd/mpcd/CellListGPU.cc:    if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/mpcd/CellListGPU.cc:        CHECK_CUDA_ERROR();
hoomd/mpcd/CellListGPU.cc:    // read flags from the gpu, and reduce across all ranks
hoomd/mpcd/CellListGPU.cc:void export_CellListGPU(pybind11::module& m)
hoomd/mpcd/CellListGPU.cc:    pybind11::class_<mpcd::CellListGPU, mpcd::CellList, std::shared_ptr<mpcd::CellListGPU>>(
hoomd/mpcd/CellListGPU.cc:        "CellListGPU")
hoomd/mpcd/SRDCollisionMethod.h:    const GPUVector<double3>& getRotationVectors() const
hoomd/mpcd/SRDCollisionMethod.h:    const GPUVector<double>& getScaleFactors() const
hoomd/mpcd/SRDCollisionMethod.h:    GPUVector<double3> m_rotvec;                       //!< MPCD rotation vectors
hoomd/mpcd/SRDCollisionMethod.h:    GPUVector<double> m_factors;  //!< Cell-level rescale factors
hoomd/mpcd/RejectionVirtualParticleFillerGPU.cuh: * \file mpcd/RejectionVirtualParticleFillerGPU.cuh
hoomd/mpcd/RejectionVirtualParticleFillerGPU.cuh: * \brief Declaration of CUDA kernels for mpcd::RejectionVirtualParticleFillerGPU
hoomd/mpcd/RejectionVirtualParticleFillerGPU.cuh:#ifndef MPCD_REJECTION_VIRTUAL_PARTICLE_FILLER_GPU_CUH_
hoomd/mpcd/RejectionVirtualParticleFillerGPU.cuh:#define MPCD_REJECTION_VIRTUAL_PARTICLE_FILLER_GPU_CUH_
hoomd/mpcd/RejectionVirtualParticleFillerGPU.cuh:#include <cuda_runtime.h>
hoomd/mpcd/RejectionVirtualParticleFillerGPU.cuh:namespace gpu
hoomd/mpcd/RejectionVirtualParticleFillerGPU.cuh:cudaError_t __attribute__((visibility("default")))
hoomd/mpcd/RejectionVirtualParticleFillerGPU.cuh:cudaError_t __attribute__((visibility("default")))
hoomd/mpcd/RejectionVirtualParticleFillerGPU.cuh:cudaError_t __attribute__((visibility("default")))
hoomd/mpcd/RejectionVirtualParticleFillerGPU.cuh: * \sa mpcd::gpu::kernel::draw_virtual_particles
hoomd/mpcd/RejectionVirtualParticleFillerGPU.cuh:cudaError_t draw_virtual_particles(const draw_virtual_particles_args_t& args, const Geometry& geom)
hoomd/mpcd/RejectionVirtualParticleFillerGPU.cuh:    cudaFuncAttributes attr;
hoomd/mpcd/RejectionVirtualParticleFillerGPU.cuh:    cudaFuncGetAttributes(&attr, (const void*)mpcd::gpu::kernel::draw_virtual_particles<Geometry>);
hoomd/mpcd/RejectionVirtualParticleFillerGPU.cuh:    mpcd::gpu::kernel::draw_virtual_particles<Geometry>
hoomd/mpcd/RejectionVirtualParticleFillerGPU.cuh:    return cudaSuccess;
hoomd/mpcd/RejectionVirtualParticleFillerGPU.cuh:    } // end namespace gpu
hoomd/mpcd/RejectionVirtualParticleFillerGPU.cuh:#endif // MPCD_REJECTION_VIRTUAL_PARTICLE_FILLER_GPU_CUH_
hoomd/mpcd/CMakeLists.txt:        ATCollisionMethodGPU.cc
hoomd/mpcd/CMakeLists.txt:        CellThermoComputeGPU.cc
hoomd/mpcd/CMakeLists.txt:        CellListGPU.cc
hoomd/mpcd/CMakeLists.txt:        CommunicatorGPU.cc
hoomd/mpcd/CMakeLists.txt:        ParallelPlateGeometryFillerGPU.cc
hoomd/mpcd/CMakeLists.txt:        PlanarPoreGeometryFillerGPU.cc
hoomd/mpcd/CMakeLists.txt:        SorterGPU.cc
hoomd/mpcd/CMakeLists.txt:        SRDCollisionMethodGPU.cc
hoomd/mpcd/CMakeLists.txt:        ATCollisionMethodGPU.cuh
hoomd/mpcd/CMakeLists.txt:        ATCollisionMethodGPU.h
hoomd/mpcd/CMakeLists.txt:        BounceBackNVEGPU.cuh
hoomd/mpcd/CMakeLists.txt:        BounceBackNVEGPU.h
hoomd/mpcd/CMakeLists.txt:        BulkStreamingMethodGPU.h
hoomd/mpcd/CMakeLists.txt:        CellThermoComputeGPU.cuh
hoomd/mpcd/CMakeLists.txt:        CellThermoComputeGPU.h
hoomd/mpcd/CMakeLists.txt:        CellListGPU.cuh
hoomd/mpcd/CMakeLists.txt:        CellListGPU.h
hoomd/mpcd/CMakeLists.txt:        CommunicatorGPU.cuh
hoomd/mpcd/CMakeLists.txt:        CommunicatorGPU.h
hoomd/mpcd/CMakeLists.txt:        BounceBackStreamingMethodGPU.cuh
hoomd/mpcd/CMakeLists.txt:        BounceBackStreamingMethodGPU.h
hoomd/mpcd/CMakeLists.txt:        ParallelPlateGeometryFillerGPU.cuh
hoomd/mpcd/CMakeLists.txt:        ParallelPlateGeometryFillerGPU.h
hoomd/mpcd/CMakeLists.txt:        PlanarPoreGeometryFillerGPU.cuh
hoomd/mpcd/CMakeLists.txt:        PlanarPoreGeometryFillerGPU.h
hoomd/mpcd/CMakeLists.txt:        RejectionVirtualParticleFillerGPU.cuh
hoomd/mpcd/CMakeLists.txt:        RejectionVirtualParticleFillerGPU.h
hoomd/mpcd/CMakeLists.txt:        SorterGPU.cuh
hoomd/mpcd/CMakeLists.txt:        SorterGPU.h
hoomd/mpcd/CMakeLists.txt:        SRDCollisionMethodGPU.cuh
hoomd/mpcd/CMakeLists.txt:        SRDCollisionMethodGPU.h
hoomd/mpcd/CMakeLists.txt:        ATCollisionMethodGPU.cu
hoomd/mpcd/CMakeLists.txt:        BounceBackNVEGPU.cu
hoomd/mpcd/CMakeLists.txt:        CellThermoComputeGPU.cu
hoomd/mpcd/CMakeLists.txt:        CellListGPU.cu
hoomd/mpcd/CMakeLists.txt:        CommunicatorGPU.cu
hoomd/mpcd/CMakeLists.txt:        ParallelPlateGeometryFillerGPU.cu
hoomd/mpcd/CMakeLists.txt:        PlanarPoreGeometryFillerGPU.cu
hoomd/mpcd/CMakeLists.txt:        RejectionVirtualParticleFillerGPU.cu
hoomd/mpcd/CMakeLists.txt:        SorterGPU.cu
hoomd/mpcd/CMakeLists.txt:        SRDCollisionMethodGPU.cu
hoomd/mpcd/CMakeLists.txt:            BulkStreamingMethodGPU.cc.inc
hoomd/mpcd/CMakeLists.txt:            BulkStreamingMethod${_force}GPU.cc
hoomd/mpcd/CMakeLists.txt:            BulkStreamingMethodGPU.cu.inc
hoomd/mpcd/CMakeLists.txt:            BulkStreamingMethod${_force}GPU.cu
hoomd/mpcd/CMakeLists.txt:        list(APPEND _mpcd_cc_sources BulkStreamingMethod${_force}GPU.cc)
hoomd/mpcd/CMakeLists.txt:        list(APPEND _mpcd_cu_sources BulkStreamingMethod${_force}GPU.cu)
hoomd/mpcd/CMakeLists.txt:                BounceBackStreamingMethodGPU.cc.inc
hoomd/mpcd/CMakeLists.txt:                BounceBackStreamingMethod${_geometry}${_force}GPU.cc
hoomd/mpcd/CMakeLists.txt:                BounceBackStreamingMethodGPU.cu.inc
hoomd/mpcd/CMakeLists.txt:                BounceBackStreamingMethod${_geometry}${_force}GPU.cu
hoomd/mpcd/CMakeLists.txt:            list(APPEND _mpcd_cc_sources BounceBackStreamingMethod${_geometry}${_force}GPU.cc)
hoomd/mpcd/CMakeLists.txt:            list(APPEND _mpcd_cu_sources BounceBackStreamingMethod${_geometry}${_force}GPU.cu)
hoomd/mpcd/CMakeLists.txt:            BounceBackNVEGPU.cc.inc
hoomd/mpcd/CMakeLists.txt:            BounceBackNVE${_geometry}GPU.cc
hoomd/mpcd/CMakeLists.txt:            BounceBackNVEGPU.cu.inc
hoomd/mpcd/CMakeLists.txt:            BounceBackNVE${_geometry}GPU.cu
hoomd/mpcd/CMakeLists.txt:        list(APPEND _mpcd_cc_sources BounceBackNVE${_geometry}GPU.cc)
hoomd/mpcd/CMakeLists.txt:        list(APPEND _mpcd_cu_sources BounceBackNVE${_geometry}GPU.cu)
hoomd/mpcd/CMakeLists.txt:            RejectionVirtualParticleFillerGPU.cc.inc
hoomd/mpcd/CMakeLists.txt:            RejectionVirtualParticleFiller${_geometry}GPU.cc
hoomd/mpcd/CMakeLists.txt:            RejectionVirtualParticleFillerGPUDrawKernel.cu.inc
hoomd/mpcd/CMakeLists.txt:            RejectionVirtualParticleFiller${_geometry}GPUDrawKernel.cu
hoomd/mpcd/CMakeLists.txt:        list(APPEND _mpcd_cc_sources RejectionVirtualParticleFiller${_geometry}GPU.cc)
hoomd/mpcd/CMakeLists.txt:        list(APPEND _mpcd_cu_sources RejectionVirtualParticleFiller${_geometry}GPUDrawKernel.cu)
hoomd/mpcd/CommunicatorUtilities.h: * \brief Defines utilities for the mpcd::Communicator and mpcd::CommunicatorGPU classes
hoomd/mpcd/BulkStreamingMethodGPU.cu.inc: * \file mpcd/BulkStreamingMethod@_force@GPU.cu
hoomd/mpcd/BulkStreamingMethodGPU.cu.inc: * \brief Template instantation for BulkStreamingMethodGPU driver (and so kernel) with @_force@
hoomd/mpcd/BulkStreamingMethodGPU.cu.inc:#include "hoomd/mpcd/BounceBackStreamingMethodGPU.cuh"
hoomd/mpcd/BulkStreamingMethodGPU.cu.inc:namespace gpu
hoomd/mpcd/BulkStreamingMethodGPU.cu.inc:template cudaError_t __attribute__((visibility("default")))
hoomd/mpcd/BulkStreamingMethodGPU.cu.inc:    } // end namespace gpu
hoomd/mpcd/CellThermoComputeGPU.cuh: * \file mpcd/CellThermoComputeGPU.cuh
hoomd/mpcd/CellThermoComputeGPU.cuh: * \brief Declaration of CUDA kernels for mpcd::CellThermoComputeGPU
hoomd/mpcd/CellThermoComputeGPU.cuh:#ifndef MPCD_CELL_THERMO_COMPUTE_GPU_CUH_
hoomd/mpcd/CellThermoComputeGPU.cuh:#define MPCD_CELL_THERMO_COMPUTE_GPU_CUH_
hoomd/mpcd/CellThermoComputeGPU.cuh:#include <cuda_runtime.h>
hoomd/mpcd/CellThermoComputeGPU.cuh://! Custom cell thermo element for reductions on the gpu
hoomd/mpcd/CellThermoComputeGPU.cuh://! Convenience struct for common parameters passed to GPU kernels
hoomd/mpcd/CellThermoComputeGPU.cuh:namespace gpu
hoomd/mpcd/CellThermoComputeGPU.cuh:cudaError_t begin_cell_thermo(const mpcd::detail::thermo_args_t& args,
hoomd/mpcd/CellThermoComputeGPU.cuh:cudaError_t end_cell_thermo(double4* d_cell_vel,
hoomd/mpcd/CellThermoComputeGPU.cuh:cudaError_t inner_cell_thermo(const mpcd::detail::thermo_args_t& args,
hoomd/mpcd/CellThermoComputeGPU.cuh:cudaError_t stage_net_cell_thermo(mpcd::detail::cell_thermo_element* d_tmp_thermo,
hoomd/mpcd/CellThermoComputeGPU.cuh:cudaError_t reduce_net_cell_thermo(mpcd::detail::cell_thermo_element* d_reduced,
hoomd/mpcd/CellThermoComputeGPU.cuh:    } // end namespace gpu
hoomd/mpcd/CellThermoComputeGPU.cuh:#endif // MPCD_CELL_THERMO_COMPUTE_GPU_CUH_
hoomd/mpcd/fill.py:        if isinstance(sim.device, hoomd.device.GPU):
hoomd/mpcd/fill.py:            class_info[1] += "GPU"
hoomd/mpcd/BounceBackNVEGPU.cuh: * \file BounceBackNVEGPU.cuh
hoomd/mpcd/BounceBackNVEGPU.cuh: * \brief Declaration of CUDA kernels for BounceBackNVEGPU
hoomd/mpcd/BounceBackNVEGPU.cuh:#ifndef MPCD_BOUNCE_BACK_NVE_GPU_CUH_
hoomd/mpcd/BounceBackNVEGPU.cuh:#define MPCD_BOUNCE_BACK_NVE_GPU_CUH_
hoomd/mpcd/BounceBackNVEGPU.cuh:#include <cuda_runtime.h>
hoomd/mpcd/BounceBackNVEGPU.cuh:namespace gpu
hoomd/mpcd/BounceBackNVEGPU.cuh:cudaError_t nve_bounce_step_one(const bounce_args_t& args, const Geometry& geom);
hoomd/mpcd/BounceBackNVEGPU.cuh:cudaError_t nve_bounce_step_two(Scalar4* d_vel,
hoomd/mpcd/BounceBackNVEGPU.cuh:cudaError_t nve_bounce_step_one(const bounce_args_t& args, const Geometry& geom)
hoomd/mpcd/BounceBackNVEGPU.cuh:    cudaFuncAttributes attr;
hoomd/mpcd/BounceBackNVEGPU.cuh:    cudaFuncGetAttributes(&attr, (const void*)kernel::nve_bounce_step_one<Geometry>);
hoomd/mpcd/BounceBackNVEGPU.cuh:    return cudaSuccess;
hoomd/mpcd/BounceBackNVEGPU.cuh:    } // end namespace gpu
hoomd/mpcd/BounceBackNVEGPU.cuh:#endif // MPCD_BOUNCE_BACK_NVE_GPU_CUH_
hoomd/mpcd/tune.py:        if isinstance(self._simulation.device, hoomd.device.GPU):
hoomd/mpcd/tune.py:            class_ = _mpcd.SorterGPU
hoomd/mpcd/SRDCollisionMethodGPU.cuh:#ifndef MPCD_SRD_COLLISION_METHOD_GPU_CUH_
hoomd/mpcd/SRDCollisionMethodGPU.cuh:#define MPCD_SRD_COLLISION_METHOD_GPU_CUH_
hoomd/mpcd/SRDCollisionMethodGPU.cuh: * \file mpcd/SRDCollisionMethodGPU.cuh
hoomd/mpcd/SRDCollisionMethodGPU.cuh: * \brief Declaration of CUDA kernels for mpcd::SRDCollisionMethodGPU
hoomd/mpcd/SRDCollisionMethodGPU.cuh:#include <cuda_runtime.h>
hoomd/mpcd/SRDCollisionMethodGPU.cuh:namespace gpu
hoomd/mpcd/SRDCollisionMethodGPU.cuh:cudaError_t srd_draw_vectors(double3* d_rotvec,
hoomd/mpcd/SRDCollisionMethodGPU.cuh:cudaError_t srd_rotate(Scalar4* d_vel,
hoomd/mpcd/SRDCollisionMethodGPU.cuh:    } // end namespace gpu
hoomd/mpcd/SRDCollisionMethodGPU.cuh:#endif // MPCD_SRD_COLLISION_METHOD_GPU_CUH_
hoomd/mpcd/BounceBackStreamingMethodGPU.cuh:#ifndef MPCD_CONFINED_STREAMING_METHOD_GPU_CUH_
hoomd/mpcd/BounceBackStreamingMethodGPU.cuh:#define MPCD_CONFINED_STREAMING_METHOD_GPU_CUH_
hoomd/mpcd/BounceBackStreamingMethodGPU.cuh: * \file mpcd/BounceBackStreamingMethodGPU.cuh
hoomd/mpcd/BounceBackStreamingMethodGPU.cuh: * \brief Declaration of CUDA kernels for mpcd::BounceBackStreamingMethodGPU
hoomd/mpcd/BounceBackStreamingMethodGPU.cuh:namespace gpu
hoomd/mpcd/BounceBackStreamingMethodGPU.cuh:cudaError_t
hoomd/mpcd/BounceBackStreamingMethodGPU.cuh: * \sa mpcd::gpu::kernel::confined_stream
hoomd/mpcd/BounceBackStreamingMethodGPU.cuh:cudaError_t confined_stream(const stream_args_t& args, const Geometry& geom, const Force& force)
hoomd/mpcd/BounceBackStreamingMethodGPU.cuh:    cudaFuncAttributes attr;
hoomd/mpcd/BounceBackStreamingMethodGPU.cuh:    cudaFuncGetAttributes(&attr, (const void*)mpcd::gpu::kernel::confined_stream<Geometry, Force>);
hoomd/mpcd/BounceBackStreamingMethodGPU.cuh:    mpcd::gpu::kernel::confined_stream<Geometry><<<grid, run_block_size>>>(args.d_pos,
hoomd/mpcd/BounceBackStreamingMethodGPU.cuh:    return cudaSuccess;
hoomd/mpcd/BounceBackStreamingMethodGPU.cuh:    } // end namespace gpu
hoomd/mpcd/BounceBackStreamingMethodGPU.cuh:#endif // MPCD_CONFINED_STREAMING_METHOD_GPU_CUH_
hoomd/mpcd/SorterGPU.cc: * \file mpcd/SorterGPU.cc
hoomd/mpcd/SorterGPU.cc: * \brief Defines the mpcd::SorterGPU
hoomd/mpcd/SorterGPU.cc:#include "SorterGPU.h"
hoomd/mpcd/SorterGPU.cc:#include "SorterGPU.cuh"
hoomd/mpcd/SorterGPU.cc:mpcd::SorterGPU::SorterGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/mpcd/SorterGPU.cc: * Performs stream compaction on the GPU of the computed cell list into the order
hoomd/mpcd/SorterGPU.cc:void mpcd::SorterGPU::computeOrder(uint64_t timestep)
hoomd/mpcd/SorterGPU.cc:        mpcd::gpu::sort_set_sentinel(d_cell_list.data,
hoomd/mpcd/SorterGPU.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/mpcd/SorterGPU.cc:            CHECK_CUDA_ERROR();
hoomd/mpcd/SorterGPU.cc:            = mpcd::gpu::sort_cell_compact(d_order.data,
hoomd/mpcd/SorterGPU.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/mpcd/SorterGPU.cc:            CHECK_CUDA_ERROR();
hoomd/mpcd/SorterGPU.cc:        mpcd::gpu::sort_gen_reverse(d_rorder.data,
hoomd/mpcd/SorterGPU.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/mpcd/SorterGPU.cc:            CHECK_CUDA_ERROR();
hoomd/mpcd/SorterGPU.cc:void mpcd::SorterGPU::applyOrder() const
hoomd/mpcd/SorterGPU.cc:        mpcd::gpu::sort_apply(d_pos_alt.data,
hoomd/mpcd/SorterGPU.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/mpcd/SorterGPU.cc:            CHECK_CUDA_ERROR();
hoomd/mpcd/SorterGPU.cc:            cudaMemcpyAsync(d_pos_alt.data + N,
hoomd/mpcd/SorterGPU.cc:                            cudaMemcpyDeviceToDevice);
hoomd/mpcd/SorterGPU.cc:            cudaMemcpyAsync(d_vel_alt.data + N,
hoomd/mpcd/SorterGPU.cc:                            cudaMemcpyDeviceToDevice);
hoomd/mpcd/SorterGPU.cc:            cudaMemcpyAsync(d_tag_alt.data + N,
hoomd/mpcd/SorterGPU.cc:                            cudaMemcpyDeviceToDevice);
hoomd/mpcd/SorterGPU.cc:            cudaDeviceSynchronize();
hoomd/mpcd/SorterGPU.cc:void export_SorterGPU(pybind11::module& m)
hoomd/mpcd/SorterGPU.cc:    pybind11::class_<mpcd::SorterGPU, mpcd::Sorter, std::shared_ptr<mpcd::SorterGPU>>(m,
hoomd/mpcd/SorterGPU.cc:                                                                                      "SorterGPU")
hoomd/mpcd/BounceBackNVEGPU.h: * \file BounceBackNVEGPU.h
hoomd/mpcd/BounceBackNVEGPU.h: * \brief Declares the BounceBackNVEGPU class for doing NVE integration with bounce-back
hoomd/mpcd/BounceBackNVEGPU.h: *        boundary conditions imposed by a geometry using the GPU.
hoomd/mpcd/BounceBackNVEGPU.h:#ifndef MPCD_BOUNCE_BACK_NVE_GPU_H_
hoomd/mpcd/BounceBackNVEGPU.h:#define MPCD_BOUNCE_BACK_NVE_GPU_H_
hoomd/mpcd/BounceBackNVEGPU.h:#include "BounceBackNVEGPU.cuh"
hoomd/mpcd/BounceBackNVEGPU.h://! Integrator that applies bounce-back boundary conditions in NVE using the GPU.
hoomd/mpcd/BounceBackNVEGPU.h:template<class Geometry> class PYBIND11_EXPORT BounceBackNVEGPU : public BounceBackNVE<Geometry>
hoomd/mpcd/BounceBackNVEGPU.h:    BounceBackNVEGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/mpcd/BounceBackNVEGPU.h:template<class Geometry> void BounceBackNVEGPU<Geometry>::integrateStepOne(uint64_t timestep)
hoomd/mpcd/BounceBackNVEGPU.h:    gpu::bounce_args_t args(d_pos.data,
hoomd/mpcd/BounceBackNVEGPU.h:    gpu::nve_bounce_step_one<Geometry>(args, *(this->m_geom));
hoomd/mpcd/BounceBackNVEGPU.h:    if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/mpcd/BounceBackNVEGPU.h:        CHECK_CUDA_ERROR();
hoomd/mpcd/BounceBackNVEGPU.h:template<class Geometry> void BounceBackNVEGPU<Geometry>::integrateStepTwo(uint64_t timestep)
hoomd/mpcd/BounceBackNVEGPU.h:    gpu::nve_bounce_step_two(d_vel.data,
hoomd/mpcd/BounceBackNVEGPU.h:    if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/mpcd/BounceBackNVEGPU.h:        CHECK_CUDA_ERROR();
hoomd/mpcd/BounceBackNVEGPU.h://! Exports the BounceBackNVEGPU class to python
hoomd/mpcd/BounceBackNVEGPU.h:template<class Geometry> void export_BounceBackNVEGPU(pybind11::module& m)
hoomd/mpcd/BounceBackNVEGPU.h:    const std::string name = "BounceBackNVE" + Geometry::getName() + "GPU";
hoomd/mpcd/BounceBackNVEGPU.h:    pybind11::class_<BounceBackNVEGPU<Geometry>,
hoomd/mpcd/BounceBackNVEGPU.h:                     std::shared_ptr<BounceBackNVEGPU<Geometry>>>(m, name.c_str())
hoomd/mpcd/BounceBackNVEGPU.h:#endif // MPCD_BOUNCE_BACK_NVE_GPU_H_
hoomd/mpcd/RejectionVirtualParticleFillerGPUDrawKernel.cu.inc: * \file mpcd/RejectionVirtualParticleFiller@_geometry@GPU.cu
hoomd/mpcd/RejectionVirtualParticleFillerGPUDrawKernel.cu.inc: * \brief Template instantation for RejectionVirtualParticleFillerGPU driver (and so kernel)
hoomd/mpcd/RejectionVirtualParticleFillerGPUDrawKernel.cu.inc:#include "hoomd/mpcd/RejectionVirtualParticleFillerGPU.cuh"
hoomd/mpcd/RejectionVirtualParticleFillerGPUDrawKernel.cu.inc:namespace gpu
hoomd/mpcd/RejectionVirtualParticleFillerGPUDrawKernel.cu.inc:template cudaError_t __attribute__((visibility("default")))
hoomd/mpcd/RejectionVirtualParticleFillerGPUDrawKernel.cu.inc:    } // end namespace gpu
hoomd/mpcd/BounceBackNVEGPU.cu: * \file BounceBackNVEGPU.cu
hoomd/mpcd/BounceBackNVEGPU.cu: * \brief Template specialization of CUDA kernels for BounceBackNVEGPU geometries. Each instance of
hoomd/mpcd/BounceBackNVEGPU.cu:#include "BounceBackNVEGPU.cuh"
hoomd/mpcd/BounceBackNVEGPU.cu:namespace gpu
hoomd/mpcd/BounceBackNVEGPU.cu:cudaError_t nve_bounce_step_two(Scalar4* d_vel,
hoomd/mpcd/BounceBackNVEGPU.cu:    cudaFuncAttributes attr;
hoomd/mpcd/BounceBackNVEGPU.cu:    cudaFuncGetAttributes(&attr, (const void*)kernel::nve_bounce_step_two);
hoomd/mpcd/BounceBackNVEGPU.cu:    return cudaSuccess;
hoomd/mpcd/BounceBackNVEGPU.cu:    } // end namespace gpu
hoomd/mpcd/module.cc:void export_ATCollisionMethodGPU(pybind11::module&);
hoomd/mpcd/module.cc:void export_CellListGPU(pybind11::module&);
hoomd/mpcd/module.cc:void export_CellThermoComputeGPU(pybind11::module&);
hoomd/mpcd/module.cc:void export_CommunicatorGPU(pybind11::module&);
hoomd/mpcd/module.cc:void export_CosineChannelGeometryFillerGPU(pybind11::module&);
hoomd/mpcd/module.cc:void export_CosineExpansionContractionGeometryFillerGPU(pybind11::module&);
hoomd/mpcd/module.cc:void export_ParallelPlateGeometryFillerGPU(pybind11::module&);
hoomd/mpcd/module.cc:void export_PlanarPoreGeometryFillerGPU(pybind11::module&);
hoomd/mpcd/module.cc:void export_SorterGPU(pybind11::module&);
hoomd/mpcd/module.cc:void export_SphereGeometryFillerGPU(pybind11::module&);
hoomd/mpcd/module.cc:void export_SRDCollisionMethodGPU(pybind11::module&);
hoomd/mpcd/module.cc:void export_BulkStreamingMethodBlockForceGPU(pybind11::module&);
hoomd/mpcd/module.cc:void export_BulkStreamingMethodConstantForceGPU(pybind11::module&);
hoomd/mpcd/module.cc:void export_BulkStreamingMethodNoForceGPU(pybind11::module&);
hoomd/mpcd/module.cc:void export_BulkStreamingMethodSineForceGPU(pybind11::module&);
hoomd/mpcd/module.cc:void export_BounceBackStreamingMethodCosineChannelGeometryBlockForceGPU(pybind11::module&);
hoomd/mpcd/module.cc:void export_BounceBackStreamingMethodCosineChannelGeometryConstantForceGPU(pybind11::module&);
hoomd/mpcd/module.cc:void export_BounceBackStreamingMethodCosineChannelGeometryNoForceGPU(pybind11::module&);
hoomd/mpcd/module.cc:void export_BounceBackStreamingMethodCosineChannelGeometrySineForceGPU(pybind11::module&);
hoomd/mpcd/module.cc:void export_BounceBackStreamingMethodCosineExpansionContractionGeometryBlockForceGPU(
hoomd/mpcd/module.cc:void export_BounceBackStreamingMethodCosineExpansionContractionGeometryConstantForceGPU(
hoomd/mpcd/module.cc:void export_BounceBackStreamingMethodCosineExpansionContractionGeometryNoForceGPU(
hoomd/mpcd/module.cc:void export_BounceBackStreamingMethodCosineExpansionContractionGeometrySineForceGPU(
hoomd/mpcd/module.cc:void export_BounceBackStreamingMethodParallelPlateGeometryBlockForceGPU(pybind11::module&);
hoomd/mpcd/module.cc:void export_BounceBackStreamingMethodParallelPlateGeometryConstantForceGPU(pybind11::module&);
hoomd/mpcd/module.cc:void export_BounceBackStreamingMethodParallelPlateGeometryNoForceGPU(pybind11::module&);
hoomd/mpcd/module.cc:void export_BounceBackStreamingMethodParallelPlateGeometrySineForceGPU(pybind11::module&);
hoomd/mpcd/module.cc:void export_BounceBackStreamingMethodPlanarPoreGeometryBlockForceGPU(pybind11::module&);
hoomd/mpcd/module.cc:void export_BounceBackStreamingMethodPlanarPoreGeometryConstantForceGPU(pybind11::module&);
hoomd/mpcd/module.cc:void export_BounceBackStreamingMethodPlanarPoreGeometryNoForceGPU(pybind11::module&);
hoomd/mpcd/module.cc:void export_BounceBackStreamingMethodPlanarPoreGeometrySineForceGPU(pybind11::module&);
hoomd/mpcd/module.cc:void export_BounceBackStreamingMethodSphereGeometryBlockForceGPU(pybind11::module&);
hoomd/mpcd/module.cc:void export_BounceBackStreamingMethodSphereGeometryConstantForceGPU(pybind11::module&);
hoomd/mpcd/module.cc:void export_BounceBackStreamingMethodSphereGeometryNoForceGPU(pybind11::module&);
hoomd/mpcd/module.cc:void export_BounceBackStreamingMethodSphereGeometrySineForceGPU(pybind11::module&);
hoomd/mpcd/module.cc:void export_BounceBackNVECosineChannelGeometryGPU(pybind11::module&);
hoomd/mpcd/module.cc:void export_BounceBackNVECosineExpansionContractionGeometryGPU(pybind11::module&);
hoomd/mpcd/module.cc:void export_BounceBackNVEParallelPlateGeometryGPU(pybind11::module&);
hoomd/mpcd/module.cc:void export_BounceBackNVEPlanarPoreGeometryGPU(pybind11::module&);
hoomd/mpcd/module.cc:void export_BounceBackNVESphereGeometryGPU(pybind11::module&);
hoomd/mpcd/module.cc://! GPU functions for the MPCD component
hoomd/mpcd/module.cc: * The gpu namespace contains functions to drive CUDA kernels in the GPU
hoomd/mpcd/module.cc:namespace gpu
hoomd/mpcd/module.cc://! GPU kernels for the MPCD component
hoomd/mpcd/module.cc: * in the gpu namespace. They are not part of the public interface for the MPCD component,
hoomd/mpcd/module.cc:    } // end namespace gpu
hoomd/mpcd/module.cc:    export_ATCollisionMethodGPU(m);
hoomd/mpcd/module.cc:    export_CellListGPU(m);
hoomd/mpcd/module.cc:    export_CellThermoComputeGPU(m);
hoomd/mpcd/module.cc:    export_CommunicatorGPU(m);
hoomd/mpcd/module.cc:    export_CosineChannelGeometryFillerGPU(m);
hoomd/mpcd/module.cc:    export_CosineExpansionContractionGeometryFillerGPU(m);
hoomd/mpcd/module.cc:    export_ParallelPlateGeometryFillerGPU(m);
hoomd/mpcd/module.cc:    export_PlanarPoreGeometryFillerGPU(m);
hoomd/mpcd/module.cc:    export_SorterGPU(m);
hoomd/mpcd/module.cc:    export_SphereGeometryFillerGPU(m);
hoomd/mpcd/module.cc:    export_SRDCollisionMethodGPU(m);
hoomd/mpcd/module.cc:    export_BulkStreamingMethodBlockForceGPU(m);
hoomd/mpcd/module.cc:    export_BulkStreamingMethodConstantForceGPU(m);
hoomd/mpcd/module.cc:    export_BulkStreamingMethodNoForceGPU(m);
hoomd/mpcd/module.cc:    export_BulkStreamingMethodSineForceGPU(m);
hoomd/mpcd/module.cc:    export_BounceBackStreamingMethodCosineChannelGeometryBlockForceGPU(m);
hoomd/mpcd/module.cc:    export_BounceBackStreamingMethodCosineChannelGeometryConstantForceGPU(m);
hoomd/mpcd/module.cc:    export_BounceBackStreamingMethodCosineChannelGeometryNoForceGPU(m);
hoomd/mpcd/module.cc:    export_BounceBackStreamingMethodCosineChannelGeometrySineForceGPU(m);
hoomd/mpcd/module.cc:    export_BounceBackStreamingMethodCosineExpansionContractionGeometryBlockForceGPU(m);
hoomd/mpcd/module.cc:    export_BounceBackStreamingMethodCosineExpansionContractionGeometryConstantForceGPU(m);
hoomd/mpcd/module.cc:    export_BounceBackStreamingMethodCosineExpansionContractionGeometryNoForceGPU(m);
hoomd/mpcd/module.cc:    export_BounceBackStreamingMethodCosineExpansionContractionGeometrySineForceGPU(m);
hoomd/mpcd/module.cc:    export_BounceBackStreamingMethodParallelPlateGeometryBlockForceGPU(m);
hoomd/mpcd/module.cc:    export_BounceBackStreamingMethodParallelPlateGeometryConstantForceGPU(m);
hoomd/mpcd/module.cc:    export_BounceBackStreamingMethodParallelPlateGeometryNoForceGPU(m);
hoomd/mpcd/module.cc:    export_BounceBackStreamingMethodParallelPlateGeometrySineForceGPU(m);
hoomd/mpcd/module.cc:    export_BounceBackStreamingMethodPlanarPoreGeometryBlockForceGPU(m);
hoomd/mpcd/module.cc:    export_BounceBackStreamingMethodPlanarPoreGeometryConstantForceGPU(m);
hoomd/mpcd/module.cc:    export_BounceBackStreamingMethodPlanarPoreGeometryNoForceGPU(m);
hoomd/mpcd/module.cc:    export_BounceBackStreamingMethodPlanarPoreGeometrySineForceGPU(m);
hoomd/mpcd/module.cc:    export_BounceBackStreamingMethodSphereGeometryBlockForceGPU(m);
hoomd/mpcd/module.cc:    export_BounceBackStreamingMethodSphereGeometryConstantForceGPU(m);
hoomd/mpcd/module.cc:    export_BounceBackStreamingMethodSphereGeometryNoForceGPU(m);
hoomd/mpcd/module.cc:    export_BounceBackStreamingMethodSphereGeometrySineForceGPU(m);
hoomd/mpcd/module.cc:    export_BounceBackNVECosineChannelGeometryGPU(m);
hoomd/mpcd/module.cc:    export_BounceBackNVECosineExpansionContractionGeometryGPU(m);
hoomd/mpcd/module.cc:    export_BounceBackNVEParallelPlateGeometryGPU(m);
hoomd/mpcd/module.cc:    export_BounceBackNVEPlanarPoreGeometryGPU(m);
hoomd/mpcd/module.cc:    export_BounceBackNVESphereGeometryGPU(m);
hoomd/mpcd/ParallelPlateGeometryFillerGPU.cc: * \file mpcd/ParallelPlateGeometryFillerGPU.cc
hoomd/mpcd/ParallelPlateGeometryFillerGPU.cc: * \brief Definition of mpcd::ParallelPlateGeometryFillerGPU
hoomd/mpcd/ParallelPlateGeometryFillerGPU.cc:#include "ParallelPlateGeometryFillerGPU.cuh"
hoomd/mpcd/ParallelPlateGeometryFillerGPU.cc:#include "ParallelPlateGeometryFillerGPU.h"
hoomd/mpcd/ParallelPlateGeometryFillerGPU.cc:mpcd::ParallelPlateGeometryFillerGPU::ParallelPlateGeometryFillerGPU(
hoomd/mpcd/ParallelPlateGeometryFillerGPU.cc:void mpcd::ParallelPlateGeometryFillerGPU::drawParticles(uint64_t timestep)
hoomd/mpcd/ParallelPlateGeometryFillerGPU.cc:    mpcd::gpu::slit_draw_particles(d_pos.data,
hoomd/mpcd/ParallelPlateGeometryFillerGPU.cc:    if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/mpcd/ParallelPlateGeometryFillerGPU.cc:        CHECK_CUDA_ERROR();
hoomd/mpcd/ParallelPlateGeometryFillerGPU.cc:void export_ParallelPlateGeometryFillerGPU(pybind11::module& m)
hoomd/mpcd/ParallelPlateGeometryFillerGPU.cc:    pybind11::class_<mpcd::ParallelPlateGeometryFillerGPU,
hoomd/mpcd/ParallelPlateGeometryFillerGPU.cc:                     std::shared_ptr<mpcd::ParallelPlateGeometryFillerGPU>>(
hoomd/mpcd/ParallelPlateGeometryFillerGPU.cc:        "ParallelPlateGeometryFillerGPU")
hoomd/mpcd/BounceBackStreamingMethodGPU.h: * \file mpcd/BounceBackStreamingMethodGPU.h
hoomd/mpcd/BounceBackStreamingMethodGPU.h: * \brief Declaration of mpcd::BounceBackStreamingMethodGPU
hoomd/mpcd/BounceBackStreamingMethodGPU.h:#ifndef MPCD_CONFINED_STREAMING_METHOD_GPU_H_
hoomd/mpcd/BounceBackStreamingMethodGPU.h:#define MPCD_CONFINED_STREAMING_METHOD_GPU_H_
hoomd/mpcd/BounceBackStreamingMethodGPU.h:#include "BounceBackStreamingMethodGPU.cuh"
hoomd/mpcd/BounceBackStreamingMethodGPU.h: * This method implements the GPU version of ballistic propagation of MPCD
hoomd/mpcd/BounceBackStreamingMethodGPU.h:class PYBIND11_EXPORT BounceBackStreamingMethodGPU
hoomd/mpcd/BounceBackStreamingMethodGPU.h:    BounceBackStreamingMethodGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/mpcd/BounceBackStreamingMethodGPU.h:void BounceBackStreamingMethodGPU<Geometry, Force>::stream(uint64_t timestep)
hoomd/mpcd/BounceBackStreamingMethodGPU.h:    mpcd::gpu::stream_args_t args(d_pos.data,
hoomd/mpcd/BounceBackStreamingMethodGPU.h:    mpcd::gpu::confined_stream<Geometry>(args, *(this->m_geom), force);
hoomd/mpcd/BounceBackStreamingMethodGPU.h:    if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/mpcd/BounceBackStreamingMethodGPU.h:        CHECK_CUDA_ERROR();
hoomd/mpcd/BounceBackStreamingMethodGPU.h://! Export mpcd::StreamingMethodGPU to python
hoomd/mpcd/BounceBackStreamingMethodGPU.h:template<class Geometry, class Force> void export_BounceBackStreamingMethodGPU(pybind11::module& m)
hoomd/mpcd/BounceBackStreamingMethodGPU.h:        = "BounceBackStreamingMethod" + Geometry::getName() + Force::getName() + "GPU";
hoomd/mpcd/BounceBackStreamingMethodGPU.h:    pybind11::class_<mpcd::BounceBackStreamingMethodGPU<Geometry, Force>,
hoomd/mpcd/BounceBackStreamingMethodGPU.h:                     std::shared_ptr<mpcd::BounceBackStreamingMethodGPU<Geometry, Force>>>(
hoomd/mpcd/BounceBackStreamingMethodGPU.h:#endif // MPCD_CONFINED_STREAMING_METHOD_GPU_H_
hoomd/mpcd/CellCommunicator.cc:    if (m_exec_conf->isCUDAEnabled())
hoomd/mpcd/CellCommunicator.cc:        GPUArray<unsigned int> send_idx(send_map.size(), m_exec_conf);
hoomd/mpcd/CellCommunicator.cc:            GPUArray<unsigned int> recv(recv_idx.size(), m_exec_conf);
hoomd/mpcd/CellCommunicator.cc:            GPUArray<unsigned int> cells(m_num_cells, m_exec_conf);
hoomd/mpcd/CellCommunicator.cc:            GPUArray<unsigned int> recv_begin(m_num_cells, m_exec_conf);
hoomd/mpcd/CellCommunicator.cc:            GPUArray<unsigned int> recv_end(m_num_cells, m_exec_conf);
hoomd/mpcd/CellListGPU.cu: * \file mpcd/CellListGPU.cu
hoomd/mpcd/CellListGPU.cu: * \brief Defines GPU functions and kernels used by mpcd::CellListGPU
hoomd/mpcd/CellListGPU.cu:#include "CellListGPU.cuh"
hoomd/mpcd/CellListGPU.cu:namespace gpu
hoomd/mpcd/CellListGPU.cu://! Kernel to compute the MPCD cell list on the GPU
hoomd/mpcd/CellListGPU.cu:    } // end namespace gpu
hoomd/mpcd/CellListGPU.cu: * \returns cudaSuccess on completion, or an error on failure
hoomd/mpcd/CellListGPU.cu:cudaError_t mpcd::gpu::compute_cell_list(unsigned int* d_cell_np,
hoomd/mpcd/CellListGPU.cu:    cudaError_t error
hoomd/mpcd/CellListGPU.cu:        = cudaMemset(d_cell_np, 0, sizeof(unsigned int) * cell_indexer.getNumElements());
hoomd/mpcd/CellListGPU.cu:    if (error != cudaSuccess)
hoomd/mpcd/CellListGPU.cu:    cudaFuncAttributes attr;
hoomd/mpcd/CellListGPU.cu:    cudaFuncGetAttributes(&attr, (const void*)mpcd::gpu::kernel::compute_cell_list);
hoomd/mpcd/CellListGPU.cu:    mpcd::gpu::kernel::compute_cell_list<<<grid, run_block_size>>>(d_cell_np,
hoomd/mpcd/CellListGPU.cu:    return cudaSuccess;
hoomd/mpcd/CellListGPU.cu: * \sa mpcd::gpu::kernel::cell_check_migrate_embed
hoomd/mpcd/CellListGPU.cu:cudaError_t mpcd::gpu::cell_check_migrate_embed(unsigned int* d_migrate_flag,
hoomd/mpcd/CellListGPU.cu:    cudaMemset(d_migrate_flag, 0, sizeof(unsigned int));
hoomd/mpcd/CellListGPU.cu:    cudaFuncAttributes attr;
hoomd/mpcd/CellListGPU.cu:    cudaFuncGetAttributes(&attr, (const void*)mpcd::gpu::kernel::cell_check_migrate_embed);
hoomd/mpcd/CellListGPU.cu:    mpcd::gpu::kernel::cell_check_migrate_embed<<<grid, run_block_size>>>(d_migrate_flag,
hoomd/mpcd/CellListGPU.cu:    return cudaSuccess;
hoomd/mpcd/CellListGPU.cu:cudaError_t mpcd::gpu::cell_apply_sort(unsigned int* d_cell_list,
hoomd/mpcd/CellListGPU.cu:    cudaFuncAttributes attr;
hoomd/mpcd/CellListGPU.cu:    cudaFuncGetAttributes(&attr, (const void*)mpcd::gpu::kernel::cell_apply_sort);
hoomd/mpcd/CellListGPU.cu:    mpcd::gpu::kernel::cell_apply_sort<<<grid, run_block_size>>>(d_cell_list,
hoomd/mpcd/CellListGPU.cu:    return cudaSuccess;
hoomd/mpcd/CommunicatorGPU.cuh: * \file mpcd/CommunicatorGPU.cuh
hoomd/mpcd/CommunicatorGPU.cuh: * \brief Defines the GPU functions of the communication algorithms
hoomd/mpcd/CommunicatorGPU.cuh:namespace gpu
hoomd/mpcd/CommunicatorGPU.cuh:cudaError_t stage_particles(unsigned int* d_comm_flag,
hoomd/mpcd/CommunicatorGPU.cuh://! Sort the particle send buffer on the GPU
hoomd/mpcd/CommunicatorGPU.cuh:    } // end namespace gpu
hoomd/mpcd/RejectionVirtualParticleFiller.h:    GPUArray<Scalar4> m_tmp_pos;
hoomd/mpcd/RejectionVirtualParticleFiller.h:    GPUArray<Scalar4> m_tmp_vel;
hoomd/mpcd/RejectionVirtualParticleFiller.h:    // Step 1: Create temporary GPUArrays to draw Particles locally using the worst case estimate
hoomd/mpcd/RejectionVirtualParticleFiller.h:        GPUArray<Scalar4> tmp_pos(num_virtual_max, m_exec_conf);
hoomd/mpcd/RejectionVirtualParticleFiller.h:        GPUArray<Scalar4> tmp_vel(num_virtual_max, m_exec_conf);
hoomd/mpcd/SorterGPU.cu: * \file mpcd/SorterGPU.cu
hoomd/mpcd/SorterGPU.cu: * \brief Defines GPU functions and kernels used by mpcd::SorterGPU
hoomd/mpcd/SorterGPU.cu:#include "CellListGPU.cuh"
hoomd/mpcd/SorterGPU.cu:namespace gpu
hoomd/mpcd/SorterGPU.cu: * 0xffffffff). The cell list can subsequently be compacted by mpcd::gpu::sort_cell_compact.
hoomd/mpcd/SorterGPU.cu: * Using one thread per particle, the map generated by mpcd::gpu::sort_cell_compact
hoomd/mpcd/SorterGPU.cu: * \returns cudaSuccess on completion
hoomd/mpcd/SorterGPU.cu: * \sa mpcd::gpu::kernel::sort_apply
hoomd/mpcd/SorterGPU.cu:cudaError_t sort_apply(Scalar4* d_pos_alt,
hoomd/mpcd/SorterGPU.cu:        return cudaSuccess;
hoomd/mpcd/SorterGPU.cu:    cudaFuncAttributes attr;
hoomd/mpcd/SorterGPU.cu:    cudaFuncGetAttributes(&attr, (const void*)mpcd::gpu::kernel::sort_apply);
hoomd/mpcd/SorterGPU.cu:    mpcd::gpu::kernel::sort_apply<<<grid, run_block_size>>>(d_pos_alt,
hoomd/mpcd/SorterGPU.cu:    return cudaSuccess;
hoomd/mpcd/SorterGPU.cu: * \returns cudaSuccess on completion
hoomd/mpcd/SorterGPU.cu: * \sa mpcd::gpu::kernel::sort_set_sentinel
hoomd/mpcd/SorterGPU.cu:cudaError_t sort_set_sentinel(unsigned int* d_cell_list,
hoomd/mpcd/SorterGPU.cu:    cudaFuncAttributes attr;
hoomd/mpcd/SorterGPU.cu:    cudaFuncGetAttributes(&attr, (const void*)mpcd::gpu::kernel::sort_set_sentinel);
hoomd/mpcd/SorterGPU.cu:    mpcd::gpu::kernel::sort_set_sentinel<<<grid, run_block_size>>>(d_cell_list,
hoomd/mpcd/SorterGPU.cu:    return cudaSuccess;
hoomd/mpcd/SorterGPU.cu: * fill in empty entries (mpcd::gpu::sort_set_sentinel). Then, the LessThan functor
hoomd/mpcd/SorterGPU.cu: * \returns cudaSuccess on completion
hoomd/mpcd/SorterGPU.cu: * \sa mpcd::gpu::kernel::sort_gen_reverse
hoomd/mpcd/SorterGPU.cu:cudaError_t sort_gen_reverse(unsigned int* d_rorder,
hoomd/mpcd/SorterGPU.cu:        return cudaSuccess;
hoomd/mpcd/SorterGPU.cu:    cudaFuncAttributes attr;
hoomd/mpcd/SorterGPU.cu:    cudaFuncGetAttributes(&attr, (const void*)mpcd::gpu::kernel::sort_gen_reverse);
hoomd/mpcd/SorterGPU.cu:    mpcd::gpu::kernel::sort_gen_reverse<<<grid, run_block_size>>>(d_rorder, d_order, N);
hoomd/mpcd/SorterGPU.cu:    return cudaSuccess;
hoomd/mpcd/SorterGPU.cu:    } // end namespace gpu
hoomd/mpcd/BulkStreamingMethodGPU.cc.inc: * \file mpcd/BulkStreamingMethod@_force@GPU.cc
hoomd/mpcd/BulkStreamingMethodGPU.cc.inc: * \brief Template instantation for BulkStreamingMethodGPU with @_force@
hoomd/mpcd/BulkStreamingMethodGPU.cc.inc:#include "hoomd/mpcd/BulkStreamingMethodGPU.h"
hoomd/mpcd/BulkStreamingMethodGPU.cc.inc:#define EXPORT_FUNCTION export_BulkStreamingMethod@_force@GPU
hoomd/mpcd/BulkStreamingMethodGPU.cc.inc:// Explicit instantiation of GPU class
hoomd/mpcd/BulkStreamingMethodGPU.cc.inc:template class BulkStreamingMethodGPU<FORCE_CLASS>;
hoomd/mpcd/BulkStreamingMethodGPU.cc.inc:    export_BulkStreamingMethodGPU<FORCE_CLASS>(m);
hoomd/mpcd/BounceBackStreamingMethodGPU.cc.inc: * \file mpcd/BounceBackStreamingMethod@_geometry@@_force@GPU.cc
hoomd/mpcd/BounceBackStreamingMethodGPU.cc.inc: * \brief Template instantation for BounceBackStreamingMethodGPU with @_geometry@ and @_force@
hoomd/mpcd/BounceBackStreamingMethodGPU.cc.inc:#include "hoomd/mpcd/BounceBackStreamingMethodGPU.h"
hoomd/mpcd/BounceBackStreamingMethodGPU.cc.inc:#define EXPORT_FUNCTION export_BounceBackStreamingMethod@_geometry@@_force@GPU
hoomd/mpcd/BounceBackStreamingMethodGPU.cc.inc:// Explicit instantiation of GPU class
hoomd/mpcd/BounceBackStreamingMethodGPU.cc.inc:template class BounceBackStreamingMethodGPU<GEOMETRY_CLASS, FORCE_CLASS>;
hoomd/mpcd/BounceBackStreamingMethodGPU.cc.inc:    export_BounceBackStreamingMethodGPU<GEOMETRY_CLASS, FORCE_CLASS>(m);
hoomd/mpcd/ATCollisionMethodGPU.cc: * \file mpcd/ATCollisionMethodGPU.h
hoomd/mpcd/ATCollisionMethodGPU.cc: * \brief Definition of mpcd::ATCollisionMethodGPU
hoomd/mpcd/ATCollisionMethodGPU.cc:#include "ATCollisionMethodGPU.h"
hoomd/mpcd/ATCollisionMethodGPU.cc:#include "ATCollisionMethodGPU.cuh"
hoomd/mpcd/ATCollisionMethodGPU.cc:#include "CellThermoComputeGPU.h"
hoomd/mpcd/ATCollisionMethodGPU.cc:mpcd::ATCollisionMethodGPU::ATCollisionMethodGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/mpcd/ATCollisionMethodGPU.cc:void mpcd::ATCollisionMethodGPU::drawVelocities(uint64_t timestep)
hoomd/mpcd/ATCollisionMethodGPU.cc:        mpcd::gpu::at_draw_velocity(d_alt_vel.data,
hoomd/mpcd/ATCollisionMethodGPU.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/mpcd/ATCollisionMethodGPU.cc:            CHECK_CUDA_ERROR();
hoomd/mpcd/ATCollisionMethodGPU.cc:        mpcd::gpu::at_draw_velocity(d_alt_vel.data,
hoomd/mpcd/ATCollisionMethodGPU.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/mpcd/ATCollisionMethodGPU.cc:            CHECK_CUDA_ERROR();
hoomd/mpcd/ATCollisionMethodGPU.cc:void mpcd::ATCollisionMethodGPU::applyVelocities()
hoomd/mpcd/ATCollisionMethodGPU.cc:        mpcd::gpu::at_apply_velocity(d_vel.data,
hoomd/mpcd/ATCollisionMethodGPU.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/mpcd/ATCollisionMethodGPU.cc:            CHECK_CUDA_ERROR();
hoomd/mpcd/ATCollisionMethodGPU.cc:        mpcd::gpu::at_apply_velocity(d_vel.data,
hoomd/mpcd/ATCollisionMethodGPU.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/mpcd/ATCollisionMethodGPU.cc:            CHECK_CUDA_ERROR();
hoomd/mpcd/ATCollisionMethodGPU.cc:void mpcd::ATCollisionMethodGPU::setCellList(std::shared_ptr<mpcd::CellList> cl)
hoomd/mpcd/ATCollisionMethodGPU.cc:            m_thermo = std::make_shared<mpcd::CellThermoComputeGPU>(m_sysdef, m_cl);
hoomd/mpcd/ATCollisionMethodGPU.cc:            m_rand_thermo = std::make_shared<mpcd::CellThermoComputeGPU>(m_sysdef, m_cl);
hoomd/mpcd/ATCollisionMethodGPU.cc:            m_thermo = std::shared_ptr<mpcd::CellThermoComputeGPU>();
hoomd/mpcd/ATCollisionMethodGPU.cc:            m_rand_thermo = std::shared_ptr<mpcd::CellThermoComputeGPU>();
hoomd/mpcd/ATCollisionMethodGPU.cc:void export_ATCollisionMethodGPU(pybind11::module& m)
hoomd/mpcd/ATCollisionMethodGPU.cc:    pybind11::class_<mpcd::ATCollisionMethodGPU,
hoomd/mpcd/ATCollisionMethodGPU.cc:                     std::shared_ptr<mpcd::ATCollisionMethodGPU>>(m, "ATCollisionMethodGPU")
hoomd/mpcd/CellList.h:#include "hoomd/GPUFlags.h"
hoomd/mpcd/CellList.h:    const GPUArray<unsigned int>& getCellList() const
hoomd/mpcd/CellList.h:    const GPUArray<unsigned int>& getCellSizeArray() const
hoomd/mpcd/CellList.h:    const GPUArray<unsigned int>& getEmbeddedGroupCellIds() const
hoomd/mpcd/CellList.h:    GPUVector<unsigned int> m_cell_np;        //!< Number of particles per cell
hoomd/mpcd/CellList.h:    GPUVector<unsigned int> m_cell_list;      //!< Cell list of particles
hoomd/mpcd/CellList.h:    GPUVector<unsigned int> m_embed_cell_ids; //!< Cell ids of the embedded particles
hoomd/mpcd/CellList.h:    GPUFlags<uint3> m_conditions; //!< Detect conditions that might fail building cell list
hoomd/mpcd/CellList.h:                      const GPUArray<unsigned int>& order,
hoomd/mpcd/CellList.h:                      const GPUArray<unsigned int>& rorder);
hoomd/mpcd/ParticleDataUtilities.h: * \brief Utilities for mpcd::ParticleData on the CPU and GPU
hoomd/mpcd/ParticleDataUtilities.h: * Some elements must be shared between the CPU and GPU code, and there is not
hoomd/mpcd/SorterGPU.h: * \file mpcd/SorterGPU.h
hoomd/mpcd/SorterGPU.h: * \brief Declares mpcd::SorterGPU, which sorts particles in the cell list on the GPU
hoomd/mpcd/SorterGPU.h:#ifndef MPCD_SORTER_GPU_H_
hoomd/mpcd/SorterGPU.h:#define MPCD_SORTER_GPU_H_
hoomd/mpcd/SorterGPU.h:#include "hoomd/GPUFlags.h"
hoomd/mpcd/SorterGPU.h://! Sorts MPCD particles on the GPU
hoomd/mpcd/SorterGPU.h:class PYBIND11_EXPORT SorterGPU : public mpcd::Sorter
hoomd/mpcd/SorterGPU.h:    SorterGPU(std::shared_ptr<SystemDefinition> sysdef, std::shared_ptr<Trigger> trigger);
hoomd/mpcd/SorterGPU.h:    //! Compute the sorting order at the current timestep on the GPU
hoomd/mpcd/SorterGPU.h:    //! Apply the sorting order on the GPU
hoomd/mpcd/SorterGPU.h:#endif // MPCD_SORTER_GPU_H_
hoomd/mpcd/methods.py:        if isinstance(sim.device, hoomd.device.GPU):
hoomd/mpcd/methods.py:            class_info[1] += "GPU"
hoomd/mpcd/ParallelPlateGeometryFillerGPU.cuh:#ifndef MPCD_SLIT_GEOMETRY_FILLER_GPU_CUH_
hoomd/mpcd/ParallelPlateGeometryFillerGPU.cuh:#define MPCD_SLIT_GEOMETRY_FILLER_GPU_CUH_
hoomd/mpcd/ParallelPlateGeometryFillerGPU.cuh: * \file mpcd/ParallelPlateGeometryFillerGPU.cuh
hoomd/mpcd/ParallelPlateGeometryFillerGPU.cuh: * \brief Declaration of CUDA kernels for mpcd::ParallelPlateGeometryFillerGPU
hoomd/mpcd/ParallelPlateGeometryFillerGPU.cuh:#include <cuda_runtime.h>
hoomd/mpcd/ParallelPlateGeometryFillerGPU.cuh:namespace gpu
hoomd/mpcd/ParallelPlateGeometryFillerGPU.cuh:cudaError_t slit_draw_particles(Scalar4* d_pos,
hoomd/mpcd/ParallelPlateGeometryFillerGPU.cuh:    } // end namespace gpu
hoomd/mpcd/ParallelPlateGeometryFillerGPU.cuh:#endif // MPCD_SLIT_GEOMETRY_FILLER_GPU_CUH_
hoomd/mpcd/Integrator.cc:#include "CommunicatorGPU.h"
hoomd/mpcd/Integrator.cc:        if (m_exec_conf->isCUDAEnabled())
hoomd/mpcd/Integrator.cc:            mpcd_comm = std::make_shared<mpcd::CommunicatorGPU>(sysdef);
hoomd/mpcd/SRDCollisionMethodGPU.h: * \file mpcd/SRDCollisionMethodGPU.h
hoomd/mpcd/SRDCollisionMethodGPU.h: * \brief Declaration of mpcd::SRDCollisionMethodGPU
hoomd/mpcd/SRDCollisionMethodGPU.h:#ifndef MPCD_SRD_COLLISION_METHOD_GPU_H_
hoomd/mpcd/SRDCollisionMethodGPU.h:#define MPCD_SRD_COLLISION_METHOD_GPU_H_
hoomd/mpcd/SRDCollisionMethodGPU.h:class PYBIND11_EXPORT SRDCollisionMethodGPU : public mpcd::SRDCollisionMethod
hoomd/mpcd/SRDCollisionMethodGPU.h:    SRDCollisionMethodGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/mpcd/SRDCollisionMethodGPU.h:#endif // MPCD_SRD_COLLISION_METHOD_GPU_H_
hoomd/mpcd/PlanarPoreGeometryFiller.h:    GPUArray<Scalar4> m_boxes;               //!< Boxes to use in filling
hoomd/mpcd/PlanarPoreGeometryFiller.h:    GPUArray<uint2> m_ranges;                //!< Particle tag ranges for filling
hoomd/mpcd/CellThermoComputeGPU.cu: * \file mpcd/CellThermoComputeGPU.cu
hoomd/mpcd/CellThermoComputeGPU.cu: *        for mpcd::CellThermoComputeGPU.
hoomd/mpcd/CellThermoComputeGPU.cu:#include "CellThermoComputeGPU.cuh"
hoomd/mpcd/CellThermoComputeGPU.cu:namespace gpu
hoomd/mpcd/CellThermoComputeGPU.cu: * See mpcd::gpu::kernel::begin_cell_thermo for an almost identical implementation
hoomd/mpcd/CellThermoComputeGPU.cu:            cudaFuncAttributes attr;
hoomd/mpcd/CellThermoComputeGPU.cu:            cudaFuncGetAttributes(&attr,
hoomd/mpcd/CellThermoComputeGPU.cu:                                  (const void*)mpcd::gpu::kernel::begin_cell_thermo<true, cur_tpp>);
hoomd/mpcd/CellThermoComputeGPU.cu:            mpcd::gpu::kernel::begin_cell_thermo<true, cur_tpp>
hoomd/mpcd/CellThermoComputeGPU.cu:            cudaFuncAttributes attr;
hoomd/mpcd/CellThermoComputeGPU.cu:            cudaFuncGetAttributes(
hoomd/mpcd/CellThermoComputeGPU.cu:                (const void*)mpcd::gpu::kernel::begin_cell_thermo<false, cur_tpp>);
hoomd/mpcd/CellThermoComputeGPU.cu:            mpcd::gpu::kernel::begin_cell_thermo<false, cur_tpp>
hoomd/mpcd/CellThermoComputeGPU.cu: * \returns cudaSuccess on completion
hoomd/mpcd/CellThermoComputeGPU.cu: * \sa mpcd::gpu::launch_begin_cell_thermo
hoomd/mpcd/CellThermoComputeGPU.cu: * \sa mpcd::gpu::kernel::begin_cell_thermo
hoomd/mpcd/CellThermoComputeGPU.cu:cudaError_t begin_cell_thermo(const mpcd::detail::thermo_args_t& args,
hoomd/mpcd/CellThermoComputeGPU.cu:        return cudaSuccess;
hoomd/mpcd/CellThermoComputeGPU.cu:    return cudaSuccess;
hoomd/mpcd/CellThermoComputeGPU.cu: * \returns cudaSuccess on completion
hoomd/mpcd/CellThermoComputeGPU.cu: * \sa mpcd::gpu::kernel::end_cell_thermo
hoomd/mpcd/CellThermoComputeGPU.cu:cudaError_t end_cell_thermo(double4* d_cell_vel,
hoomd/mpcd/CellThermoComputeGPU.cu:        return cudaSuccess;
hoomd/mpcd/CellThermoComputeGPU.cu:        cudaFuncAttributes attr;
hoomd/mpcd/CellThermoComputeGPU.cu:        cudaFuncGetAttributes(&attr, (const void*)mpcd::gpu::kernel::end_cell_thermo<true>);
hoomd/mpcd/CellThermoComputeGPU.cu:        mpcd::gpu::kernel::end_cell_thermo<true>
hoomd/mpcd/CellThermoComputeGPU.cu:        cudaFuncAttributes attr;
hoomd/mpcd/CellThermoComputeGPU.cu:        cudaFuncGetAttributes(&attr, (const void*)mpcd::gpu::kernel::end_cell_thermo<true>);
hoomd/mpcd/CellThermoComputeGPU.cu:        mpcd::gpu::kernel::end_cell_thermo<false>
hoomd/mpcd/CellThermoComputeGPU.cu:    return cudaSuccess;
hoomd/mpcd/CellThermoComputeGPU.cu:            cudaFuncAttributes attr;
hoomd/mpcd/CellThermoComputeGPU.cu:            cudaFuncGetAttributes(&attr,
hoomd/mpcd/CellThermoComputeGPU.cu:                                  (const void*)mpcd::gpu::kernel::inner_cell_thermo<true, cur_tpp>);
hoomd/mpcd/CellThermoComputeGPU.cu:            mpcd::gpu::kernel::inner_cell_thermo<true, cur_tpp>
hoomd/mpcd/CellThermoComputeGPU.cu:            cudaFuncAttributes attr;
hoomd/mpcd/CellThermoComputeGPU.cu:            cudaFuncGetAttributes(
hoomd/mpcd/CellThermoComputeGPU.cu:                (const void*)mpcd::gpu::kernel::inner_cell_thermo<false, cur_tpp>);
hoomd/mpcd/CellThermoComputeGPU.cu:            mpcd::gpu::kernel::inner_cell_thermo<false, cur_tpp>
hoomd/mpcd/CellThermoComputeGPU.cu: * \returns cudaSuccess on completion
hoomd/mpcd/CellThermoComputeGPU.cu: * \sa mpcd::gpu::launch_inner_cell_thermo
hoomd/mpcd/CellThermoComputeGPU.cu: * \sa mpcd::gpu::kernel::inner_cell_thermo
hoomd/mpcd/CellThermoComputeGPU.cu:cudaError_t inner_cell_thermo(const mpcd::detail::thermo_args_t& args,
hoomd/mpcd/CellThermoComputeGPU.cu:        return cudaSuccess;
hoomd/mpcd/CellThermoComputeGPU.cu:    return cudaSuccess;
hoomd/mpcd/CellThermoComputeGPU.cu: * \returns cudaSuccess on completion
hoomd/mpcd/CellThermoComputeGPU.cu: * \sa mpcd::gpu::kernel::stage_net_cell_thermo
hoomd/mpcd/CellThermoComputeGPU.cu:cudaError_t stage_net_cell_thermo(mpcd::detail::cell_thermo_element* d_tmp_thermo,
hoomd/mpcd/CellThermoComputeGPU.cu:        cudaFuncAttributes attr;
hoomd/mpcd/CellThermoComputeGPU.cu:        cudaFuncGetAttributes(&attr, (const void*)mpcd::gpu::kernel::stage_net_cell_thermo<true>);
hoomd/mpcd/CellThermoComputeGPU.cu:        mpcd::gpu::kernel::stage_net_cell_thermo<true>
hoomd/mpcd/CellThermoComputeGPU.cu:        cudaFuncAttributes attr;
hoomd/mpcd/CellThermoComputeGPU.cu:        cudaFuncGetAttributes(&attr, (const void*)mpcd::gpu::kernel::stage_net_cell_thermo<false>);
hoomd/mpcd/CellThermoComputeGPU.cu:        mpcd::gpu::kernel::stage_net_cell_thermo<false>
hoomd/mpcd/CellThermoComputeGPU.cu:    return cudaSuccess;
hoomd/mpcd/CellThermoComputeGPU.cu: * \returns cudaSuccess on completion
hoomd/mpcd/CellThermoComputeGPU.cu:cudaError_t reduce_net_cell_thermo(mpcd::detail::cell_thermo_element* d_reduced,
hoomd/mpcd/CellThermoComputeGPU.cu:    return cudaSuccess;
hoomd/mpcd/CellThermoComputeGPU.cu:template cudaError_t __attribute__((visibility("default")))
hoomd/mpcd/CellThermoComputeGPU.cu:template cudaError_t __attribute__((visibility("default")))
hoomd/mpcd/CellThermoComputeGPU.cu:template cudaError_t __attribute__((visibility("default")))
hoomd/mpcd/CellThermoComputeGPU.cu:template cudaError_t __attribute__((visibility("default")))
hoomd/mpcd/CellThermoComputeGPU.cu:    } // end namespace gpu
hoomd/mpcd/PlanarPoreGeometryFillerGPU.cu: * \file mpcd/ParallelPlateGeometryFillerGPU.cu
hoomd/mpcd/PlanarPoreGeometryFillerGPU.cu: * \brief Defines GPU functions and kernels used by mpcd::ParallelPlateGeometryFillerGPU
hoomd/mpcd/PlanarPoreGeometryFillerGPU.cu:#include "PlanarPoreGeometryFillerGPU.cuh"
hoomd/mpcd/PlanarPoreGeometryFillerGPU.cu:namespace gpu
hoomd/mpcd/PlanarPoreGeometryFillerGPU.cu:cudaError_t slit_pore_draw_particles(Scalar4* d_pos,
hoomd/mpcd/PlanarPoreGeometryFillerGPU.cu:        return cudaSuccess;
hoomd/mpcd/PlanarPoreGeometryFillerGPU.cu:    cudaFuncAttributes attr;
hoomd/mpcd/PlanarPoreGeometryFillerGPU.cu:    cudaFuncGetAttributes(&attr, (const void*)kernel::slit_pore_draw_particles);
hoomd/mpcd/PlanarPoreGeometryFillerGPU.cu:    return cudaSuccess;
hoomd/mpcd/PlanarPoreGeometryFillerGPU.cu:    } // end namespace gpu
hoomd/AABB.h:   format changes. It also changes between the CPU and GPU. Instead, use the accessor methods
hoomd/Tuner.h: * sorters and GPU autotuners are system Tuners.
hoomd/GPUArray.h:/*! \file GPUArray.h
hoomd/GPUArray.h:    \brief Defines the GPUArray class
hoomd/GPUArray.h:// 4 GB is considered a large allocation for a single GPU buffer, and user should be warned
hoomd/GPUArray.h:template<class T> class GPUArray;
hoomd/GPUArray.h:                << "Freeing " << m_N * sizeof(T) << " bytes of CUDA memory." << std::endl;
hoomd/GPUArray.h:    bool m_use_device;                                         //!< Whether to use cudaMallocManaged
hoomd/GPUArray.h:    //! Delete the CUDA array
hoomd/GPUArray.h:// unregister host memory from CUDA driver
hoomd/GPUArray.h:template<class T> class GPUArrayDispatch;
hoomd/GPUArray.h://! CRTP (Curiously recurring template pattern) interface for GPUArray/GlobalArray
hoomd/GPUArray.h:template<class T, class Derived> class GPUArrayBase
hoomd/GPUArray.h:     - For 1-D allocated GPUArrays, this is the number of elements allocated.
hoomd/GPUArray.h:     - For 2-D allocated GPUArrays, this is the \b total number of elements (\a pitch * \a height)
hoomd/GPUArray.h:    //! Test if the GPUArray is NULL
hoomd/GPUArray.h:     - For 2-D allocated GPUArrays, this is the total width of a row in memory (including the
hoomd/GPUArray.h:     - For 1-D allocated GPUArrays, this is the simply the number of elements allocated.
hoomd/GPUArray.h:     - For 2-D allocated GPUArrays, this is the height given to the constructor
hoomd/GPUArray.h:     - For 1-D allocated GPUArrays, this is the simply 1.
hoomd/GPUArray.h:    //! Resize the GPUArray
hoomd/GPUArray.h:    //! Resize a 2D GPUArray
hoomd/GPUArray.h:    GPUArrayBase() { };
hoomd/GPUArray.h://! This base class is the glue between the ArrayHandle and a generic GPUArrayBase<Derived>
hoomd/GPUArray.h://! Handle to access the data pointer handled by GPUArray
hoomd/GPUArray.h:/*! The data in GPUArray is only accessible via ArrayHandle. The pointer is accessible for the
hoomd/GPUArray.h:lifetime of the ArrayHandle. When the ArrayHandle is destroyed, the GPUArray is notified that the
hoomd/GPUArray.h:GPUArray<int> gpu_array(100);
hoomd/GPUArray.h:    ArrayHandle<int> h_handle(gpu_array, access_location::host, access_mode::readwrite);
hoomd/GPUArray.h:    /*! \tparam Derived the type of GPUArray implementation
hoomd/GPUArray.h:    inline ArrayHandle(const GPUArrayBase<T, Derived>& gpu_array,
hoomd/GPUArray.h:    //! Notifies the containing GPUArray that the handle has been released
hoomd/GPUArray.h:/*! This handle can be used to speed up access to the GPUArray data when
hoomd/GPUArray.h:    ArrayHandleAsync objects maybe instantiated for multiple GPUArrays in a row, without
hoomd/GPUArray.h:GPUArray<int> gpu_array_1(100);
hoomd/GPUArray.h:GPUArray<int> gpu_array_2(100);
hoomd/GPUArray.h:    ArrayHandle<int> h_handle_1(gpu_array_1, access_location::host, access_mode::readwrite);
hoomd/GPUArray.h:    ArrayHandle<int> h_handle_2(gpu_array_2, access_location:::host, access_mode::readwrite);
hoomd/GPUArray.h:    cudaDeviceSynchronize();
hoomd/GPUArray.h:    /*! \tparam Derived the type of GPUArray implementation
hoomd/GPUArray.h:    inline ArrayHandleAsync(const GPUArrayBase<T, Derived>& gpu_array,
hoomd/GPUArray.h:    //! Notifies the containing GPUArray that the handle has been released
hoomd/GPUArray.h://! Class for managing an array of elements on the GPU mirrored to the CPU
hoomd/GPUArray.h:GPUArray provides a template class for managing the majority of the GPU<->CPU memory usage patterns
hoomd/GPUArray.h:in HOOMD. It represents a single array of elements which is present both on the CPU and GPU. Via
hoomd/GPUArray.h:GPUArray is fairly advanced, C++ wise. It is a template class, so GPUArray's of floats, float4's,
hoomd/GPUArray.h:pass GPUArray's around in arguments or overwrite one with another via assignment (inexpensive swaps
hoomd/GPUArray.h:At a high level, GPUArray encapsulates a smart pointer \a std::unique_ptr<T> \a data and with \a
hoomd/GPUArray.h:is an example of addressing element i,j in a 2-D allocated GPUArray.
hoomd/GPUArray.h:GPUArray<int> gpu_array(100, 200, m_exec_conf);
hoomd/GPUArray.h:size_t pitch = gpu_array.getPitch();
hoomd/GPUArray.h:ArrayHandle<int> h_handle(gpu_array, access_location::host, access_mode::readwrite);
hoomd/GPUArray.h:A future modification of GPUArray will allow mirroring or splitting the data across multiple GPUs.
hoomd/GPUArray.h:template<class T> class GPUArray : public GPUArrayBase<T, GPUArray<T>>
hoomd/GPUArray.h:    //! Constructs a NULL GPUArray
hoomd/GPUArray.h:    GPUArray();
hoomd/GPUArray.h:    //! Constructs a NULL GPUArray with an execution configuration
hoomd/GPUArray.h:    GPUArray(std::shared_ptr<const ExecutionConfiguration> exec_conf);
hoomd/GPUArray.h:    //! Constructs a 1-D GPUArray
hoomd/GPUArray.h:    GPUArray(size_t num_elements, std::shared_ptr<const ExecutionConfiguration> exec_conf);
hoomd/GPUArray.h:    //! Constructs a 2-D GPUArray
hoomd/GPUArray.h:    GPUArray(size_t width, size_t height, std::shared_ptr<const ExecutionConfiguration> exec_conf);
hoomd/GPUArray.h:    virtual ~GPUArray() { }
hoomd/GPUArray.h:    //! Constructs a 1-D GPUArray
hoomd/GPUArray.h:    GPUArray(size_t num_elements,
hoomd/GPUArray.h:    //! Constructs a 2-D GPUArray
hoomd/GPUArray.h:    GPUArray(size_t width,
hoomd/GPUArray.h:    GPUArray(const GPUArray& from) noexcept;
hoomd/GPUArray.h:    GPUArray& operator=(const GPUArray& rhs) noexcept;
hoomd/GPUArray.h:    GPUArray(GPUArray&& from) noexcept;
hoomd/GPUArray.h:    GPUArray& operator=(GPUArray&& rhs) noexcept;
hoomd/GPUArray.h:    //! Swap the pointers in two GPUArrays
hoomd/GPUArray.h:    inline void swap(GPUArray& from);
hoomd/GPUArray.h:     - For 1-D allocated GPUArrays, this is the number of elements allocated.
hoomd/GPUArray.h:     - For 2-D allocated GPUArrays, this is the \b total number of elements (\a pitch * \a height)
hoomd/GPUArray.h:    //! Test if the GPUArray is NULL
hoomd/GPUArray.h:     - For 2-D allocated GPUArrays, this is the total width of a row in memory (including the
hoomd/GPUArray.h:     - For 1-D allocated GPUArrays, this is the simply the number of elements allocated.
hoomd/GPUArray.h:     - For 2-D allocated GPUArrays, this is the height given to the constructor
hoomd/GPUArray.h:     - For 1-D allocated GPUArrays, this is the simply 1.
hoomd/GPUArray.h:    //! Resize the GPUArray
hoomd/GPUArray.h:        Only data from the currently active memory location (gpu/cpu) is copied over to the resized
hoomd/GPUArray.h:    //! Resize a 2D GPUArray
hoomd/GPUArray.h:            if (m_exec_conf->isCUDAEnabled())
hoomd/GPUArray.h:    friend class GPUArrayDispatch<T>;
hoomd/GPUArray.h:    friend class GPUArrayBase<T, GPUArray<T>>;
hoomd/GPUArray.h:        m_exec_conf; //!< execution configuration for working with CUDA
hoomd/GPUArray.h:/*! \param gpu_array GPUArray host to the pointer data
hoomd/GPUArray.h:ArrayHandle<T>::ArrayHandle(const GPUArrayBase<T, Derived>& array,
hoomd/GPUArray.h:ArrayHandleAsync<T>::ArrayHandleAsync(const GPUArrayBase<T, Derived>& array,
hoomd/GPUArray.h:// ArrayHandleDispatch specialization for GPUArray
hoomd/GPUArray.h:template<class T> class GPUArrayDispatch : public ArrayHandleDispatch<T>
hoomd/GPUArray.h:    GPUArrayDispatch(T* const _data, const GPUArray<T>& _gpu_array)
hoomd/GPUArray.h:        : ArrayHandleDispatch<T>(_data), gpu_array(_gpu_array)
hoomd/GPUArray.h:    virtual ~GPUArrayDispatch()
hoomd/GPUArray.h:        assert(gpu_array.isAcquired());
hoomd/GPUArray.h:        gpu_array.release();
hoomd/GPUArray.h:    const GPUArray<T>& gpu_array;
hoomd/GPUArray.h:// GPUArray implementation
hoomd/GPUArray.h:GPUArray<T>::GPUArray()
hoomd/GPUArray.h:GPUArray<T>::GPUArray(std::shared_ptr<const ExecutionConfiguration> exec_conf)
hoomd/GPUArray.h:    \param exec_conf Shared pointer to the execution configuration for managing CUDA initialization
hoomd/GPUArray.h:GPUArray<T>::GPUArray(size_t num_elements, std::shared_ptr<const ExecutionConfiguration> exec_conf)
hoomd/GPUArray.h:    \param exec_conf Shared pointer to the execution configuration for managing CUDA initialization
hoomd/GPUArray.h:GPUArray<T>::GPUArray(size_t width,
hoomd/GPUArray.h:    \param exec_conf Shared pointer to the execution configuration for managing CUDA initialization
hoomd/GPUArray.h:GPUArray<T>::GPUArray(size_t num_elements,
hoomd/GPUArray.h:    \param exec_conf Shared pointer to the execution configuration for managing CUDA initialization
hoomd/GPUArray.h:GPUArray<T>::GPUArray(size_t width,
hoomd/GPUArray.h:GPUArray<T>::GPUArray(const GPUArray& from) noexcept
hoomd/GPUArray.h:    // copy over the data to the new GPUArray
hoomd/GPUArray.h:template<class T> GPUArray<T>& GPUArray<T>::operator=(const GPUArray& rhs) noexcept
hoomd/GPUArray.h:        // copy over the data to the new GPUArray
hoomd/GPUArray.h:GPUArray<T>::GPUArray(GPUArray&& from) noexcept
hoomd/GPUArray.h:template<class T> GPUArray<T>& GPUArray<T>::operator=(GPUArray&& rhs) noexcept
hoomd/GPUArray.h:/*! \param from GPUArray to swap \a this with
hoomd/GPUArray.h:GPUArray c(a);
hoomd/GPUArray.h:template<class T> void GPUArray<T>::swap(GPUArray& from)
hoomd/GPUArray.h:    \post All memory pointers needed for GPUArray are allocated
hoomd/GPUArray.h:template<class T> void GPUArray<T>::allocate()
hoomd/GPUArray.h:            << "GPUArray is trying to allocate a very large (>4GB) amount of memory." << std::endl;
hoomd/GPUArray.h:        throw std::runtime_error("Error allocating GPUArray.");
hoomd/GPUArray.h:            << "GPUArray: Allocating " << float(m_num_elements * sizeof(T)) / 1024.0f / 1024.0f
hoomd/GPUArray.h:    bool use_device = m_exec_conf && m_exec_conf->isCUDAEnabled();
hoomd/GPUArray.h:        CHECK_CUDA_ERROR();
hoomd/GPUArray.h:    if (m_exec_conf && m_exec_conf->isCUDAEnabled())
hoomd/GPUArray.h:        CHECK_CUDA_ERROR();
hoomd/GPUArray.h:template<class T> void GPUArray<T>::memclear(size_t first)
hoomd/GPUArray.h:    if (m_exec_conf && m_exec_conf->isCUDAEnabled())
hoomd/GPUArray.h:template<class T> void GPUArray<T>::memcpyDeviceToHost(bool async) const
hoomd/GPUArray.h:            << "GPUArray: Copying " << float(m_num_elements * sizeof(T)) / 1024.0f / 1024.0f
hoomd/GPUArray.h:    if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/GPUArray.h:        CHECK_CUDA_ERROR();
hoomd/GPUArray.h:template<class T> void GPUArray<T>::memcpyHostToDevice(bool async) const
hoomd/GPUArray.h:        // rely on CUDA's implicit synchronization
hoomd/GPUArray.h:            << "GPUArray: Copying " << float(m_num_elements * sizeof(T)) / 1024.0f / 1024.0f
hoomd/GPUArray.h:    if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/GPUArray.h:        CHECK_CUDA_ERROR();
hoomd/GPUArray.h:    acquire() is the workhorse of GPUArray. It tracks the internal state variable \a data_location
hoomd/GPUArray.h:ArrayHandleDispatch<T> GPUArray<T>::acquire(const access_location::Enum location,
hoomd/GPUArray.h:    // base case - handle acquiring a NULL GPUArray by simply returning NULL to prevent any memcpys
hoomd/GPUArray.h:        return GPUArrayDispatch<T>(nullptr, *this);
hoomd/GPUArray.h:            return GPUArrayDispatch<T>(h_data.get(), *this);
hoomd/GPUArray.h:            return GPUArrayDispatch<T>(h_data.get(), *this);
hoomd/GPUArray.h:            return GPUArrayDispatch<T>(h_data.get(), *this);
hoomd/GPUArray.h:        // check that a GPU is actually specified
hoomd/GPUArray.h:        if (!m_exec_conf->isCUDAEnabled())
hoomd/GPUArray.h:                << "Requesting device acquire, but no GPU in the Execution Configuration"
hoomd/GPUArray.h:            return GPUArrayDispatch<T>(d_data.get(), *this);
hoomd/GPUArray.h:            return GPUArrayDispatch<T>(d_data.get(), *this);
hoomd/GPUArray.h:            return GPUArrayDispatch<T>(d_data.get(), *this);
hoomd/GPUArray.h:template<class T> T* GPUArray<T>::resizeHostArray(size_t num_elements)
hoomd/GPUArray.h:    if (m_exec_conf && m_exec_conf->isCUDAEnabled())
hoomd/GPUArray.h:        CHECK_CUDA_ERROR();
hoomd/GPUArray.h:    bool use_device = m_exec_conf && m_exec_conf->isCUDAEnabled();
hoomd/GPUArray.h:T* GPUArray<T>::resize2DHostArray(size_t pitch, size_t new_pitch, size_t height, size_t new_height)
hoomd/GPUArray.h:    if (m_exec_conf && m_exec_conf->isCUDAEnabled())
hoomd/GPUArray.h:        CHECK_CUDA_ERROR();
hoomd/GPUArray.h:    bool use_device = m_exec_conf && m_exec_conf->isCUDAEnabled();
hoomd/GPUArray.h:template<class T> T* GPUArray<T>::resizeDeviceArray(size_t num_elements)
hoomd/GPUArray.h:    CHECK_CUDA_ERROR();
hoomd/GPUArray.h:    CHECK_CUDA_ERROR();
hoomd/GPUArray.h:    CHECK_CUDA_ERROR();
hoomd/GPUArray.h:                                                    m_exec_conf->isCUDAEnabled(),
hoomd/GPUArray.h:T* GPUArray<T>::resize2DDeviceArray(size_t pitch,
hoomd/GPUArray.h:    CHECK_CUDA_ERROR();
hoomd/GPUArray.h:    CHECK_CUDA_ERROR();
hoomd/GPUArray.h:        CHECK_CUDA_ERROR();
hoomd/GPUArray.h:                                                    m_exec_conf->isCUDAEnabled(),
hoomd/GPUArray.h:template<class T> void GPUArray<T>::resize(size_t num_elements)
hoomd/GPUArray.h:            << "GPUArray is trying to allocate a very large (>4GB) amount of memory." << std::endl;
hoomd/GPUArray.h:            << "GPUArray: Resizing to " << float(num_elements * sizeof(T)) / 1024.0f / 1024.0f
hoomd/GPUArray.h:    if (m_exec_conf && m_exec_conf->isCUDAEnabled())
hoomd/GPUArray.h:template<class T> void GPUArray<T>::resize(size_t width, size_t height)
hoomd/GPUArray.h:            << "GPUArray is trying to allocate a very large (>4GB) amount of memory." << std::endl;
hoomd/GPUArray.h:    if (m_exec_conf && m_exec_conf->isCUDAEnabled())
hoomd/LoadBalancerGPU.cuh:/*! \file LoadBalancerGPU.cuh
hoomd/LoadBalancerGPU.cuh:    \brief Defines the GPU functions for load balancing
hoomd/LoadBalancerGPU.cuh:void gpu_load_balance_mark_rank(unsigned int* d_ranks,
hoomd/LoadBalancerGPU.cuh:unsigned int gpu_load_balance_select_off_rank(unsigned int* d_off_rank,
hoomd/ForceCompute.h:    //! Update GPU memory hints
hoomd/ForceCompute.h:    void updateGPUAdvice();
hoomd/BondedGroupData.cc:    GPUVector<members_t> groups(m_exec_conf);
hoomd/BondedGroupData.cc:    GPUVector<typeval_t> typeval(m_exec_conf);
hoomd/BondedGroupData.cc:    GPUVector<unsigned int> group_tag(m_exec_conf);
hoomd/BondedGroupData.cc:    GPUVector<unsigned int> group_rtag(m_exec_conf);
hoomd/BondedGroupData.cc:    GPUVector<members_t> gpu_table(m_exec_conf);
hoomd/BondedGroupData.cc:    m_gpu_table.swap(gpu_table);
hoomd/BondedGroupData.cc:    GPUVector<unsigned int> gpu_pos_table(m_exec_conf);
hoomd/BondedGroupData.cc:    m_gpu_pos_table.swap(gpu_pos_table);
hoomd/BondedGroupData.cc:    GPUVector<unsigned int> n_groups(m_exec_conf);
hoomd/BondedGroupData.cc:    m_gpu_n_groups.swap(n_groups);
hoomd/BondedGroupData.cc:        GPUVector<ranks_t> group_ranks(m_exec_conf);
hoomd/BondedGroupData.cc:    GPUVector<typeval_t> typeval_alt(m_exec_conf);
hoomd/BondedGroupData.cc:    GPUVector<unsigned int> group_tag_alt(m_exec_conf);
hoomd/BondedGroupData.cc:    GPUVector<members_t> groups_alt(m_exec_conf);
hoomd/BondedGroupData.cc:        GPUVector<ranks_t> group_ranks_alt(m_exec_conf);
hoomd/BondedGroupData.cc:    GPUArray<unsigned int> condition(1, m_exec_conf);
hoomd/BondedGroupData.cc:             "in shared memory errors on the GPU.";
hoomd/BondedGroupData.cc:    // GPUVector checks if the resize is necessary
hoomd/BondedGroupData.cc:void BondedGroupData<group_size, Group, name, has_type_mapping>::rebuildGPUTable()
hoomd/BondedGroupData.cc:    if (m_exec_conf->isCUDAEnabled())
hoomd/BondedGroupData.cc:        rebuildGPUTableGPU();
hoomd/BondedGroupData.cc:        m_gpu_n_groups.resize(m_pdata->getN() + m_pdata->getNGhosts());
hoomd/BondedGroupData.cc:            ArrayHandle<unsigned int> h_n_groups(m_gpu_n_groups,
hoomd/BondedGroupData.cc:        m_gpu_table_indexer = Index2D(m_pdata->getN() + m_pdata->getNGhosts(), num_groups_max);
hoomd/BondedGroupData.cc:        m_gpu_table.resize(m_gpu_table_indexer.getNumElements());
hoomd/BondedGroupData.cc:        m_gpu_pos_table.resize(m_gpu_table_indexer.getNumElements());
hoomd/BondedGroupData.cc:            ArrayHandle<unsigned int> h_n_groups(m_gpu_n_groups,
hoomd/BondedGroupData.cc:            ArrayHandle<members_t> h_gpu_table(m_gpu_table,
hoomd/BondedGroupData.cc:            ArrayHandle<unsigned int> h_gpu_pos_table(m_gpu_pos_table,
hoomd/BondedGroupData.cc:                    h_gpu_table.data[m_gpu_table_indexer(idx1, num)] = h;
hoomd/BondedGroupData.cc:                    h_gpu_pos_table.data[m_gpu_table_indexer(idx1, num)] = gpos;
hoomd/BondedGroupData.cc:void BondedGroupData<group_size, Group, name, has_type_mapping>::rebuildGPUTableGPU()
hoomd/BondedGroupData.cc:    m_gpu_n_groups.resize(m_pdata->getN() + m_pdata->getNGhosts());
hoomd/BondedGroupData.cc:    // resize GPU table to current number of particles
hoomd/BondedGroupData.cc:    m_gpu_table_indexer
hoomd/BondedGroupData.cc:        = Index2D(m_pdata->getN() + m_pdata->getNGhosts(), m_gpu_table_indexer.getH());
hoomd/BondedGroupData.cc:    m_gpu_table.resize(m_gpu_table_indexer.getNumElements());
hoomd/BondedGroupData.cc:    m_gpu_pos_table.resize(m_gpu_table_indexer.getNumElements());
hoomd/BondedGroupData.cc:            ArrayHandle<unsigned int> d_n_groups(m_gpu_n_groups,
hoomd/BondedGroupData.cc:            ArrayHandle<members_t> d_gpu_table(m_gpu_table,
hoomd/BondedGroupData.cc:            ArrayHandle<unsigned int> d_gpu_pos_table(m_gpu_pos_table,
hoomd/BondedGroupData.cc:            // fill group table on GPU
hoomd/BondedGroupData.cc:            gpu_update_group_table<group_size, members_t>(getN() + getNGhosts(),
hoomd/BondedGroupData.cc:                                                          m_gpu_table_indexer.getH(),
hoomd/BondedGroupData.cc:                                                          d_gpu_table.data,
hoomd/BondedGroupData.cc:                                                          d_gpu_pos_table.data,
hoomd/BondedGroupData.cc:                                                          m_gpu_table_indexer.getW(),
hoomd/BondedGroupData.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/BondedGroupData.cc:            CHECK_CUDA_ERROR();
hoomd/BondedGroupData.cc:            m_gpu_table_indexer
hoomd/BondedGroupData.cc:                = Index2D(m_pdata->getN() + m_pdata->getNGhosts(), m_gpu_table_indexer.getH() + 1);
hoomd/BondedGroupData.cc:            m_gpu_table.resize(m_gpu_table_indexer.getNumElements());
hoomd/BondedGroupData.cc:            m_gpu_pos_table.resize(m_gpu_table_indexer.getNumElements());
hoomd/BoxResizeUpdaterGPU.cc:#include "BoxResizeUpdaterGPU.h"
hoomd/BoxResizeUpdaterGPU.cc:#include "BoxResizeUpdaterGPU.cuh"
hoomd/BoxResizeUpdaterGPU.cc:BoxResizeUpdaterGPU::BoxResizeUpdaterGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/BoxResizeUpdaterGPU.cc:    // only one GPU is supported
hoomd/BoxResizeUpdaterGPU.cc:    if (!m_exec_conf->isCUDAEnabled())
hoomd/BoxResizeUpdaterGPU.cc:        throw std::runtime_error("Cannot initialize BoxResizeUpdaterGPU on a CPU device.");
hoomd/BoxResizeUpdaterGPU.cc:BoxResizeUpdaterGPU::~BoxResizeUpdaterGPU()
hoomd/BoxResizeUpdaterGPU.cc:void BoxResizeUpdaterGPU::scaleAndWrapParticles(const BoxDim& cur_box, const BoxDim& new_box)
hoomd/BoxResizeUpdaterGPU.cc:    kernel::gpu_box_resize_scale(d_pos.data,
hoomd/BoxResizeUpdaterGPU.cc:    kernel::gpu_box_resize_wrap(m_pdata->getN(),
hoomd/BoxResizeUpdaterGPU.cc:void export_BoxResizeUpdaterGPU(pybind11::module& m)
hoomd/BoxResizeUpdaterGPU.cc:    pybind11::class_<BoxResizeUpdaterGPU, BoxResizeUpdater, std::shared_ptr<BoxResizeUpdaterGPU>>(
hoomd/BoxResizeUpdaterGPU.cc:        "BoxResizeUpdaterGPU")
hoomd/ParticleData.cu:    \brief ImplementsGPU kernel code and data structure functions used by ParticleData
hoomd/ParticleData.cu:__global__ void gpu_scatter_particle_data_kernel(const unsigned int nwork,
hoomd/ParticleData.cu:gpu_select_sent_particles(unsigned int N, unsigned int* d_comm_flags, unsigned int* d_tmp)
hoomd/ParticleData.cu:unsigned int gpu_pdata_remove(const unsigned int N,
hoomd/ParticleData.cu:                              GPUPartition& gpu_partition)
hoomd/ParticleData.cu:    hipLaunchKernelGGL(gpu_select_sent_particles,
hoomd/ParticleData.cu:        for (int idev = gpu_partition.getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/ParticleData.cu:            auto range = gpu_partition.getRangeAndSetGPU(idev);
hoomd/ParticleData.cu:            hipLaunchKernelGGL(gpu_scatter_particle_data_kernel,
hoomd/ParticleData.cu:__global__ void gpu_pdata_add_particles_kernel(unsigned int old_nparticles,
hoomd/ParticleData.cu:void gpu_pdata_add_particles(const unsigned int old_nparticles,
hoomd/ParticleData.cu:    hipLaunchKernelGGL(gpu_pdata_add_particles_kernel,
hoomd/Communicator.h:#include "GPUVector.h"
hoomd/CommunicatorGPU.cu:/*! \file CommunicatorGPU.cu
hoomd/CommunicatorGPU.cu:    \brief Implementation of communication algorithms on the GPU
hoomd/CommunicatorGPU.cu:#include "CommunicatorGPU.cuh"
hoomd/CommunicatorGPU.cu:__global__ void gpu_select_particle_migrate(unsigned int N,
hoomd/CommunicatorGPU.cu:struct get_migrate_key_gpu : public thrust::unary_function<const unsigned int, unsigned int>
hoomd/CommunicatorGPU.cu:    __host__ __device__ get_migrate_key_gpu(const uint3 _my_pos,
hoomd/CommunicatorGPU.cu:void gpu_stage_particles(const unsigned int N,
hoomd/CommunicatorGPU.cu:    hipLaunchKernelGGL(HIP_KERNEL_NAME(gpu_select_particle_migrate),
hoomd/CommunicatorGPU.cu:void gpu_sort_migrating_particles(const size_t nsend,
hoomd/CommunicatorGPU.cu:                      get_migrate_key_gpu(my_pos, di, mask, d_cart_ranks));
hoomd/CommunicatorGPU.cu:    thrust::sort_by_key(thrust::cuda::par(alloc),
hoomd/CommunicatorGPU.cu:__global__ void gpu_wrap_particles_kernel(const unsigned int n_recv,
hoomd/CommunicatorGPU.cu:void gpu_wrap_particles(const unsigned int n_recv, detail::pdata_element* d_in, const BoxDim& box)
hoomd/CommunicatorGPU.cu:    hipLaunchKernelGGL(gpu_wrap_particles_kernel,
hoomd/CommunicatorGPU.cu:void gpu_reset_rtags(unsigned int n_delete_ptls, unsigned int* d_delete_tags, unsigned int* d_rtag)
hoomd/CommunicatorGPU.cu:__global__ void gpu_make_ghost_exchange_plan_kernel(unsigned int N,
hoomd/CommunicatorGPU.cu:void gpu_make_ghost_exchange_plan(unsigned int* d_plan,
hoomd/CommunicatorGPU.cu:    hipLaunchKernelGGL(gpu_make_ghost_exchange_plan_kernel,
hoomd/CommunicatorGPU.cu:__global__ void gpu_make_ghost_group_exchange_plan_kernel(unsigned int N,
hoomd/CommunicatorGPU.cu:void gpu_make_ghost_group_exchange_plan(unsigned int* d_ghost_group_plan,
hoomd/CommunicatorGPU.cu:    hipLaunchKernelGGL(HIP_KERNEL_NAME(gpu_make_ghost_group_exchange_plan_kernel<group_size>),
hoomd/CommunicatorGPU.cu:__global__ void gpu_ghost_neighbor_counts(unsigned int N,
hoomd/CommunicatorGPU.cu:unsigned int gpu_exchange_ghosts_count_neighbors(unsigned int N,
hoomd/CommunicatorGPU.cu:    hipLaunchKernelGGL(gpu_ghost_neighbor_counts,
hoomd/CommunicatorGPU.cu:__global__ void gpu_expand_neighbors_kernel(const unsigned int n_out,
hoomd/CommunicatorGPU.cu:void gpu_exchange_ghosts_make_indices(unsigned int N,
hoomd/CommunicatorGPU.cu:        thrust::fill(thrust::cuda::par(alloc),
hoomd/CommunicatorGPU.cu:        thrust::scatter_if(thrust::cuda::par(alloc),
hoomd/CommunicatorGPU.cu:        thrust::inclusive_scan(thrust::cuda::par(alloc),
hoomd/CommunicatorGPU.cu:        hipLaunchKernelGGL(gpu_expand_neighbors_kernel,
hoomd/CommunicatorGPU.cu:        thrust::sort_by_key(thrust::cuda::par(alloc),
hoomd/CommunicatorGPU.cu:gpu_pack_kernel(unsigned int n_out, const uint2* d_ghost_idx_adj, const T* in, T* out)
hoomd/CommunicatorGPU.cu:__global__ void gpu_pack_wrap_kernel(unsigned int n_out,
hoomd/CommunicatorGPU.cu:void gpu_exchange_ghosts_pack(unsigned int n_out,
hoomd/CommunicatorGPU.cu:        hipLaunchKernelGGL(gpu_pack_kernel,
hoomd/CommunicatorGPU.cu:        hipLaunchKernelGGL(gpu_pack_wrap_kernel,
hoomd/CommunicatorGPU.cu:        hipLaunchKernelGGL(gpu_pack_kernel,
hoomd/CommunicatorGPU.cu:        hipLaunchKernelGGL(gpu_pack_kernel,
hoomd/CommunicatorGPU.cu:        hipLaunchKernelGGL(gpu_pack_kernel,
hoomd/CommunicatorGPU.cu:        hipLaunchKernelGGL(gpu_pack_kernel,
hoomd/CommunicatorGPU.cu:        hipLaunchKernelGGL(gpu_pack_kernel,
hoomd/CommunicatorGPU.cu:void gpu_exchange_ghosts_pack_netforce(unsigned int n_out,
hoomd/CommunicatorGPU.cu:    hipLaunchKernelGGL(gpu_pack_kernel,
hoomd/CommunicatorGPU.cu:__global__ void gpu_pack_netvirial_kernel(unsigned int n_out,
hoomd/CommunicatorGPU.cu:void gpu_exchange_ghosts_pack_netvirial(unsigned int n_out,
hoomd/CommunicatorGPU.cu:    hipLaunchKernelGGL(gpu_pack_netvirial_kernel,
hoomd/CommunicatorGPU.cu:__global__ void gpu_group_pack_kernel(unsigned int n_out,
hoomd/CommunicatorGPU.cu:void gpu_exchange_ghost_groups_pack(unsigned int n_out,
hoomd/CommunicatorGPU.cu:    hipLaunchKernelGGL(gpu_group_pack_kernel,
hoomd/CommunicatorGPU.cu:template<typename T> __global__ void gpu_unpack_kernel(unsigned int n_in, const T* in, T* out)
hoomd/CommunicatorGPU.cu:void gpu_exchange_ghosts_copy_buf(unsigned int n_recv,
hoomd/CommunicatorGPU.cu:        hipLaunchKernelGGL(HIP_KERNEL_NAME(gpu_unpack_kernel<unsigned int>),
hoomd/CommunicatorGPU.cu:        hipLaunchKernelGGL(HIP_KERNEL_NAME(gpu_unpack_kernel<Scalar4>),
hoomd/CommunicatorGPU.cu:        hipLaunchKernelGGL(HIP_KERNEL_NAME(gpu_unpack_kernel<Scalar4>),
hoomd/CommunicatorGPU.cu:        hipLaunchKernelGGL(HIP_KERNEL_NAME(gpu_unpack_kernel<Scalar>),
hoomd/CommunicatorGPU.cu:        hipLaunchKernelGGL(HIP_KERNEL_NAME(gpu_unpack_kernel<Scalar>),
hoomd/CommunicatorGPU.cu:        hipLaunchKernelGGL(HIP_KERNEL_NAME(gpu_unpack_kernel<unsigned int>),
hoomd/CommunicatorGPU.cu:        hipLaunchKernelGGL(HIP_KERNEL_NAME(gpu_unpack_kernel<int3>),
hoomd/CommunicatorGPU.cu:        hipLaunchKernelGGL(HIP_KERNEL_NAME(gpu_unpack_kernel<Scalar4>),
hoomd/CommunicatorGPU.cu:void gpu_exchange_ghosts_copy_netforce_buf(unsigned int n_recv,
hoomd/CommunicatorGPU.cu:    hipLaunchKernelGGL(HIP_KERNEL_NAME(gpu_unpack_kernel<Scalar4>),
hoomd/CommunicatorGPU.cu:__global__ void gpu_unpack_netvirial_kernel(unsigned int n_in,
hoomd/CommunicatorGPU.cu:void gpu_exchange_ghosts_copy_netvirial_buf(unsigned int n_recv,
hoomd/CommunicatorGPU.cu:    hipLaunchKernelGGL(gpu_unpack_netvirial_kernel,
hoomd/CommunicatorGPU.cu:__global__ void gpu_unpack_groups_kernel(unsigned int nrecv,
hoomd/CommunicatorGPU.cu:__global__ void gpu_mark_received_ghost_groups_kernel(unsigned int nrecv,
hoomd/CommunicatorGPU.cu:void gpu_exchange_ghost_groups_copy_buf(unsigned int nrecv,
hoomd/CommunicatorGPU.cu:    hipLaunchKernelGGL(HIP_KERNEL_NAME(gpu_mark_received_ghost_groups_kernel<size>),
hoomd/CommunicatorGPU.cu:    hipLaunchKernelGGL(gpu_unpack_groups_kernel,
hoomd/CommunicatorGPU.cu:void gpu_compute_ghost_rtags(unsigned int first_idx,
hoomd/CommunicatorGPU.cu:__global__ void gpu_mark_groups_kernel(unsigned int N,
hoomd/CommunicatorGPU.cu:void gpu_mark_groups(unsigned int N,
hoomd/CommunicatorGPU.cu:    hipLaunchKernelGGL(HIP_KERNEL_NAME(gpu_mark_groups_kernel<group_size>),
hoomd/CommunicatorGPU.cu:__global__ void gpu_scatter_ranks_and_mark_send_groups_kernel(unsigned int n_groups,
hoomd/CommunicatorGPU.cu:void gpu_scatter_ranks_and_mark_send_groups(unsigned int n_groups,
hoomd/CommunicatorGPU.cu:    hipLaunchKernelGGL(HIP_KERNEL_NAME(gpu_scatter_ranks_and_mark_send_groups_kernel<group_size>),
hoomd/CommunicatorGPU.cu:__global__ void gpu_update_ranks_table_kernel(unsigned int n_groups,
hoomd/CommunicatorGPU.cu:void gpu_update_ranks_table(unsigned int n_groups,
hoomd/CommunicatorGPU.cu:    hipLaunchKernelGGL(HIP_KERNEL_NAME(gpu_update_ranks_table_kernel<group_size>),
hoomd/CommunicatorGPU.cu:__global__ void gpu_scatter_and_mark_groups_for_removal_kernel(unsigned int n_groups,
hoomd/CommunicatorGPU.cu:void gpu_scatter_and_mark_groups_for_removal(unsigned int n_groups,
hoomd/CommunicatorGPU.cu:    hipLaunchKernelGGL(HIP_KERNEL_NAME(gpu_scatter_and_mark_groups_for_removal_kernel<group_size>),
hoomd/CommunicatorGPU.cu:__global__ void gpu_remove_groups_kernel(unsigned int n_groups,
hoomd/CommunicatorGPU.cu:void gpu_remove_groups(unsigned int n_groups,
hoomd/CommunicatorGPU.cu:    hipLaunchKernelGGL(gpu_remove_groups_kernel,
hoomd/CommunicatorGPU.cu:__global__ void gpu_count_unique_groups_kernel(unsigned int n_recv,
hoomd/CommunicatorGPU.cu:__global__ void gpu_add_groups_kernel(unsigned int n_recv,
hoomd/CommunicatorGPU.cu:void gpu_add_groups(unsigned int n_groups,
hoomd/CommunicatorGPU.cu:    hipLaunchKernelGGL(gpu_count_unique_groups_kernel,
hoomd/CommunicatorGPU.cu:    hipLaunchKernelGGL(gpu_add_groups_kernel,
hoomd/CommunicatorGPU.cu:__global__ void gpu_mark_bonded_ghosts_kernel(unsigned int n_groups,
hoomd/CommunicatorGPU.cu:void gpu_mark_bonded_ghosts(unsigned int n_groups,
hoomd/CommunicatorGPU.cu:    hipLaunchKernelGGL(HIP_KERNEL_NAME(gpu_mark_bonded_ghosts_kernel<group_size>),
hoomd/CommunicatorGPU.cu:void gpu_reset_exchange_plan(unsigned int N, unsigned int* d_plan)
hoomd/CommunicatorGPU.cu:gpu_mark_groups<2, group_storage<2>, group_storage<2>>(unsigned int N,
hoomd/CommunicatorGPU.cu:template void gpu_scatter_ranks_and_mark_send_groups<2>(unsigned int n_groups,
hoomd/CommunicatorGPU.cu:template void gpu_update_ranks_table<2>(unsigned int n_groups,
hoomd/CommunicatorGPU.cu:template void gpu_scatter_and_mark_groups_for_removal<2>(unsigned int n_groups,
hoomd/CommunicatorGPU.cu:template void gpu_remove_groups(unsigned int n_groups,
hoomd/CommunicatorGPU.cu:template void gpu_add_groups(unsigned int n_groups,
hoomd/CommunicatorGPU.cu:template void gpu_mark_bonded_ghosts<2>(unsigned int n_groups,
hoomd/CommunicatorGPU.cu:gpu_mark_groups<3, group_storage<3>, group_storage<3>>(unsigned int N,
hoomd/CommunicatorGPU.cu:template void gpu_scatter_ranks_and_mark_send_groups<3>(unsigned int n_groups,
hoomd/CommunicatorGPU.cu:template void gpu_update_ranks_table<3>(unsigned int n_groups,
hoomd/CommunicatorGPU.cu:template void gpu_scatter_and_mark_groups_for_removal<3>(unsigned int n_groups,
hoomd/CommunicatorGPU.cu:template void gpu_remove_groups(unsigned int n_groups,
hoomd/CommunicatorGPU.cu:template void gpu_add_groups(unsigned int n_groups,
hoomd/CommunicatorGPU.cu:template void gpu_mark_bonded_ghosts<3>(unsigned int n_groups,
hoomd/CommunicatorGPU.cu:gpu_mark_groups<4, group_storage<4>, group_storage<4>>(unsigned int N,
hoomd/CommunicatorGPU.cu:template void gpu_scatter_ranks_and_mark_send_groups<4>(unsigned int n_groups,
hoomd/CommunicatorGPU.cu:template void gpu_update_ranks_table<4>(unsigned int n_groups,
hoomd/CommunicatorGPU.cu:template void gpu_scatter_and_mark_groups_for_removal<4>(unsigned int n_groups,
hoomd/CommunicatorGPU.cu:template void gpu_remove_groups(unsigned int n_groups,
hoomd/CommunicatorGPU.cu:template void gpu_add_groups(unsigned int n_groups,
hoomd/CommunicatorGPU.cu:template void gpu_mark_bonded_ghosts<4>(unsigned int n_groups,
hoomd/CommunicatorGPU.cu:template void gpu_make_ghost_group_exchange_plan<2>(unsigned int* d_ghost_group_plan,
hoomd/CommunicatorGPU.cu:template void gpu_exchange_ghost_groups_pack(unsigned int n_out,
hoomd/CommunicatorGPU.cu:template void gpu_exchange_ghost_groups_copy_buf<2>(unsigned int nrecv,
hoomd/RandomNumbers.h:#ifndef __CUDACC_RTC__
hoomd/RandomNumbers.h:#ifndef __CUDACC_RTC__
hoomd/RandomNumbers.h:// Amazing! cuda thinks numeric_limits::max() is a __host__ function, so
hoomd/RandomNumbers.h:template<typename T> R123_CONSTEXPR R123_STATIC_INLINE R123_CUDA_DEVICE T maxTvalue()
hoomd/RandomNumbers.h:template<typename Ftype, typename Itype> R123_CUDA_DEVICE R123_STATIC_INLINE Ftype u01(Itype in)
hoomd/RandomNumbers.h:template<typename Ftype, typename Itype> R123_CUDA_DEVICE R123_STATIC_INLINE Ftype uneg11(Itype in)
hoomd/RandomNumbers.h:     for Brownian Dynamics and Dissipative Particle Dynamics simulations on GPU devices",
hoomd/managed_allocator.h://! Class to perform cudaMallocManaged allocations
hoomd/managed_allocator.h:        \param use_device Whether to use cudaMallocManaged
hoomd/managed_allocator.h:        \param use_device Whether this is a CUDA managed memory allocation
hoomd/managed_allocator.h:    bool m_use_device; //!< Whether to use cudaMallocManaged
hoomd/managed_allocator.h:/// @param use_device whether to use cudaMallocManaged passed to the allocator
hoomd/Integrator.cuh:    \brief Declares methods and data structures used by the Integrator class on the GPU
hoomd/Integrator.cuh:#include "GPUPartition.cuh"
hoomd/Integrator.cuh:/*! To keep the argument count down to gpu_integrator_sum_accel, up to 6 force/virial array pairs
hoomd/Integrator.cuh:struct gpu_force_list
hoomd/Integrator.cuh:    gpu_force_list()
hoomd/Integrator.cuh://! Driver for gpu_integrator_sum_net_force_kernel()
hoomd/Integrator.cuh:hipError_t gpu_integrator_sum_net_force(Scalar4* d_net_force,
hoomd/Integrator.cuh:                                        const gpu_force_list& force_list,
hoomd/Integrator.cuh:                                        const GPUPartition& gpu_partition);
hoomd/PythonLocalDataAccess.cc:        .def_property_readonly("__cuda_array_interface__",
hoomd/PythonLocalDataAccess.cc:                               &HOOMDDeviceBuffer::getCudaArrayInterface)
hoomd/ParticleGroup.h:#include "GPUPartition.cuh"
hoomd/ParticleGroup.h:    Finally, the common use case on the GPU using groups will include running one thread per
hoomd/ParticleGroup.h:   facilitates this, the list of indices in the group will be stored in a GPUArray.
hoomd/ParticleGroup.h:    /*! \returns A GPUArray for directly accessing the index list, intended for use in using groups
hoomd/ParticleGroup.h:       on the GPU \note The caller \b must \b not write to or change the array.
hoomd/ParticleGroup.h:    //! Return the load balancing GPU partition
hoomd/ParticleGroup.h:    const GPUPartition& getGPUPartition()
hoomd/ParticleGroup.h:        return m_gpu_partition;
hoomd/ParticleGroup.h:    mutable GPUPartition m_gpu_partition; //!< A handy struct to store load balancing info for this
hoomd/ParticleGroup.h:        bool update_gpu_advice = false;
hoomd/ParticleGroup.h:            update_gpu_advice = true;
hoomd/ParticleGroup.h:        if (update_gpu_advice)
hoomd/ParticleGroup.h:            updateGPUAdvice();
hoomd/ParticleGroup.h:    //! Update the GPU memory advice
hoomd/ParticleGroup.h:    void updateGPUAdvice();
hoomd/ParticleGroup.h:    void rebuildIndexListGPU();
hoomd/BoxResizeUpdaterGPU.h:#ifndef __BOX_RESIZE_UPDATER_GPU_H__
hoomd/BoxResizeUpdaterGPU.h:#define __BOX_RESIZE_UPDATER_GPU_H__
hoomd/BoxResizeUpdaterGPU.h:/// Updates the simulation box over time using the GPU
hoomd/BoxResizeUpdaterGPU.h:class PYBIND11_EXPORT BoxResizeUpdaterGPU : public BoxResizeUpdater
hoomd/BoxResizeUpdaterGPU.h:    BoxResizeUpdaterGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/BoxResizeUpdaterGPU.h:    virtual ~BoxResizeUpdaterGPU();
hoomd/BoxResizeUpdaterGPU.h:/// Export the BoxResizeUpdaterGPU to python
hoomd/BoxResizeUpdaterGPU.h:void export_BoxResizeUpdaterGPU(pybind11::module& m);
hoomd/BoxResizeUpdaterGPU.h:#endif // __BOX_RESIZE_UPDATER_GPU_H__
hoomd/state.py:from hoomd.data import LocalSnapshot, LocalSnapshotGPU
hoomd/state.py:    # for single-GPU execution
hoomd/state.py:    state on rank 0. See `State.cpu_local_snapshot`, `State.gpu_local_snapshot`,
hoomd/state.py:            Data across all MPI ranks and from GPUs is gathered on the root MPI
hoomd/state.py:            is a GPU.
hoomd/state.py:        zero-copy if running on GPU).
hoomd/state.py:    def gpu_local_snapshot(self):
hoomd/state.py:        """hoomd.data.LocalSnapshotGPU: Expose simulation data on the GPU.
hoomd/state.py:        Data in `State.gpu_local_snapshot` is GPU local, and the
hoomd/state.py:        `hoomd.data.LocalSnapshotGPU` object is only usable within a context
hoomd/state.py:        manager (i.e. ``with sim.state.gpu_local_snapshot as data:``). Attempts
hoomd/state.py:        The `hoomd.data.LocalSnapshotGPU` data access is mediated through
hoomd/state.py:        `hoomd.data.array.HOOMDGPUArray` objects. This helps us maintain memory
hoomd/state.py:        zero-copy access on the GPU (assuming data was last accessed on the
hoomd/state.py:        GPU).
hoomd/state.py:            if not (gpu_not_available or cupy_not_available):
hoomd/state.py:                gpu=hoomd.device.GPU()
hoomd/state.py:                simulation = hoomd.util.make_example_simulation(device=gpu)
hoomd/state.py:        .. skip: next if(gpu_not_available or cupy_not_available)
hoomd/state.py:            with simulation.state.gpu_local_snapshot as local_snapshot:
hoomd/state.py:            This property is only available when running on a GPU (or multiple
hoomd/state.py:            GPUs).
hoomd/state.py:        if not isinstance(self._simulation.device, hoomd.device.GPU):
hoomd/state.py:                "Cannot access gpu_snapshot with a non GPU device.")
hoomd/state.py:                "Cannot enter gpu_local_snapshot context manager inside "
hoomd/state.py:            return LocalSnapshotGPU(self)
hoomd/HOOMDVersion.cc:    o << "GPU [";
hoomd/HOOMDVersion.cc:    o << "CUDA";
hoomd/HOOMDVersion.cc:    o << "ROCm";
hoomd/HOOMDVersion.cc:bool BuildInfo::getEnableGPU()
hoomd/HOOMDVersion.cc:std::string BuildInfo::getGPUAPIVersion()
hoomd/HOOMDVersion.cc:std::string BuildInfo::getGPUPlatform()
hoomd/HOOMDVersion.cc:    return std::string("CUDA");
hoomd/HOOMDVersion.cc:    return std::string("ROCm");
hoomd/ParticleGroup.cuh:    \brief Contains GPU kernel code used by ParticleGroup
hoomd/ParticleGroup.cuh://! GPU method for rebuilding the index list of a ParticleGroup
hoomd/ParticleGroup.cuh:hipError_t gpu_rebuild_index_list(unsigned int N,
hoomd/ParticleGroup.cuh://! GPU method for compacting the group member indices
hoomd/ParticleGroup.cuh:hipError_t gpu_compact_index_list(unsigned int N,
hoomd/CachedAllocator.h:    Inspired by thrust/examples/cuda/custom_temporary_allocation.cu
hoomd/CachedAllocator.h:#define CHECK_CUDA()                                                         \
hoomd/CachedAllocator.h:            throw std::runtime_error("CUDA Error in CachedAllocator "        \
hoomd/CachedAllocator.h:            throw std::runtime_error("CUDA Error in CachedAllocator "        \
hoomd/CachedAllocator.h:        // create a new one with cudaMalloc
hoomd/CachedAllocator.h:        CHECK_CUDA();
hoomd/CachedAllocator.h:            CHECK_CUDA();
hoomd/CachedAllocator.h:#undef CHECK_CUDA
hoomd/data/local_access.py:        * `hoomd.data.LocalSnapshotGPU`
hoomd/data/local_access.py:        * `hoomd.data.LocalSnapshotGPU`
hoomd/data/local_access.py:        * `hoomd.data.LocalSnapshotGPU`
hoomd/data/local_access.py:        * `hoomd.data.LocalSnapshotGPU`
hoomd/data/local_access.py:        * `hoomd.data.LocalSnapshotGPU`
hoomd/data/local_access.py:        * `hoomd.data.LocalSnapshotGPU`
hoomd/data/local_access.py:        * `hoomd.data.LocalSnapshotGPU`
hoomd/data/local_access_gpu.py:"""Implement local access classes for the GPU."""
hoomd/data/local_access_gpu.py:from hoomd.data.array import HOOMDGPUArray
hoomd/data/local_access_gpu.py:if hoomd.version.gpu_enabled:
hoomd/data/local_access_gpu.py:    class ParticleLocalAccessGPU(ParticleLocalAccessBase):
hoomd/data/local_access_gpu.py:        """Access particle data on the GPU."""
hoomd/data/local_access_gpu.py:        _array_cls = HOOMDGPUArray
hoomd/data/local_access_gpu.py:    class BondLocalAccessGPU(BondLocalAccessBase):
hoomd/data/local_access_gpu.py:        """Access bond data on the GPU."""
hoomd/data/local_access_gpu.py:        _array_cls = HOOMDGPUArray
hoomd/data/local_access_gpu.py:    class AngleLocalAccessGPU(AngleLocalAccessBase):
hoomd/data/local_access_gpu.py:        """Access angle data on the GPU."""
hoomd/data/local_access_gpu.py:        _array_cls = HOOMDGPUArray
hoomd/data/local_access_gpu.py:    class DihedralLocalAccessGPU(DihedralLocalAccessBase):
hoomd/data/local_access_gpu.py:        """Access dihedral data on the GPU."""
hoomd/data/local_access_gpu.py:        _array_cls = HOOMDGPUArray
hoomd/data/local_access_gpu.py:    class ImproperLocalAccessGPU(ImproperLocalAccessBase):
hoomd/data/local_access_gpu.py:        """Access improper data on the GPU."""
hoomd/data/local_access_gpu.py:        _array_cls = HOOMDGPUArray
hoomd/data/local_access_gpu.py:    class ConstraintLocalAccessGPU(ConstraintLocalAccessBase):
hoomd/data/local_access_gpu.py:        """Access constraint data on the GPU."""
hoomd/data/local_access_gpu.py:        _array_cls = HOOMDGPUArray
hoomd/data/local_access_gpu.py:    class PairLocalAccessGPU(PairLocalAccessBase):
hoomd/data/local_access_gpu.py:        """Access special pair data on the GPU."""
hoomd/data/local_access_gpu.py:        _array_cls = HOOMDGPUArray
hoomd/data/local_access_gpu.py:    class LocalSnapshotGPU(_LocalSnapshot):
hoomd/data/local_access_gpu.py:        """Access system state data on the GPU."""
hoomd/data/local_access_gpu.py:            self._particles = ParticleLocalAccessGPU(state)
hoomd/data/local_access_gpu.py:            self._bonds = BondLocalAccessGPU(state)
hoomd/data/local_access_gpu.py:            self._angles = AngleLocalAccessGPU(state)
hoomd/data/local_access_gpu.py:            self._dihedrals = DihedralLocalAccessGPU(state)
hoomd/data/local_access_gpu.py:            self._impropers = ImproperLocalAccessGPU(state)
hoomd/data/local_access_gpu.py:            self._pairs = PairLocalAccessGPU(state)
hoomd/data/local_access_gpu.py:            self._constraints = ConstraintLocalAccessGPU(state)
hoomd/data/local_access_gpu.py:    from hoomd.error import _NoGPU
hoomd/data/local_access_gpu.py:    class BondLocalAccessGPU(_NoGPU):
hoomd/data/local_access_gpu.py:        """GPU data access is not available in CPU builds."""
hoomd/data/local_access_gpu.py:    class AngleLocalAccessGPU(_NoGPU):
hoomd/data/local_access_gpu.py:        """GPU data access is not available in CPU builds."""
hoomd/data/local_access_gpu.py:    class DihedralLocalAccessGPU(_NoGPU):
hoomd/data/local_access_gpu.py:        """GPU data access is not available in CPU builds."""
hoomd/data/local_access_gpu.py:    class ImproperLocalAccessGPU(_NoGPU):
hoomd/data/local_access_gpu.py:        """GPU data access is not available in CPU builds."""
hoomd/data/local_access_gpu.py:    class ConstraintLocalAccessGPU(_NoGPU):
hoomd/data/local_access_gpu.py:        """GPU data access is not available in CPU builds."""
hoomd/data/local_access_gpu.py:    class PairLocalAccessGPU(_NoGPU):
hoomd/data/local_access_gpu.py:        """GPU data access is not available in CPU builds."""
hoomd/data/local_access_gpu.py:    class ParticleLocalAccessGPU(_NoGPU):
hoomd/data/local_access_gpu.py:        """GPU data access is not available in CPU builds."""
hoomd/data/local_access_gpu.py:    class LocalSnapshotGPU(_NoGPU, _LocalSnapshot):
hoomd/data/local_access_gpu.py:        """GPU data access is not available in CPU builds."""
hoomd/data/local_access_gpu.py:_gpu_snapshot_docs = """
hoomd/data/local_access_gpu.py:Provides context manager access to HOOMD-blue GPU data buffers.
hoomd/data/local_access_gpu.py:All array-like properties return a `hoomd.data.array.HOOMDGPUArray` object which
hoomd/data/local_access_gpu.py:    `hoomd.State.gpu_local_snapshot`.
hoomd/data/local_access_gpu.py:LocalSnapshotGPU.__doc__ = _gpu_snapshot_docs
hoomd/data/CMakeLists.txt:          local_access_gpu.py
hoomd/data/array.py:if hoomd.version.gpu_enabled:
hoomd/data/array.py:    class _HOOMDGPUArrayBase:
hoomd/data/array.py:        """Base GPUArray class. Functions work with or without CuPy.
hoomd/data/array.py:                information required to access GPU data buffer. Can also accept
hoomd/data/array.py:                anything that implements the ``__cuda_array_interface__``.
hoomd/data/array.py:                ``__cuda_array_interface__`` does).
hoomd/data/array.py:        def __cuda_array_interface__(self):
hoomd/data/array.py:            return deepcopy(self._buffer.__cuda_array_interface__)
hoomd/data/array.py:        _wrap_gpu_array_list = []
hoomd/data/array.py:        class HOOMDGPUArray(_HOOMDGPUArrayBase,
hoomd/data/array.py:                            metaclass=_wrap_class_factory(_wrap_gpu_array_list)
hoomd/data/array.py:            """Zero copy access to HOOMD data on the GPU."""
hoomd/data/array.py:                protocol = self._buffer.__cuda_array_interface__
hoomd/data/array.py:                protocol = self._buffer.__cuda_array_interface__
hoomd/data/array.py:                protocol = self._buffer.__cuda_array_interface__
hoomd/data/array.py:        _wrap_gpu_array_list = [
hoomd/data/array.py:        _GPUArrayMeta = _wrap_class_factory(_wrap_gpu_array_list,
hoomd/data/array.py:        class HOOMDGPUArray(_HOOMDGPUArrayBase, metaclass=_GPUArrayMeta):
hoomd/data/array.py:            """Zero copy access to HOOMD data on the GPU."""
hoomd/data/array.py:                if isinstance(index, HOOMDGPUArray):
hoomd/data/array.py:                return HOOMDGPUArray(arr, self._callback, self.read_only)
hoomd/data/array.py:    from hoomd.error import _NoGPU
hoomd/data/array.py:    class HOOMDGPUArray(_NoGPU):
hoomd/data/array.py:        """GPU arrays are not available on the CPU."""
hoomd/data/array.py:_gpu_array_docs = """
hoomd/data/array.py:A __cuda_array_interface__ to internal HOOMD-blue data on the GPU.
hoomd/data/array.py:The HOOMDGPUArray object exposes a GPU data buffer using
hoomd/data/array.py:`__cuda_array_interface__
hoomd/data/array.py:<https://nvidia.github.io/numba-cuda/user/cuda_array_interface.html>`_.
hoomd/data/array.py:memory accesses (`hoomd.State.gpu_local_snapshot`). To avoid errors, use arrays
hoomd/data/array.py:    with sim.state.gpu_local_snapshot as data:
hoomd/data/array.py:`HOOMDGPUArray` always supports getting (but not setting) the ``shape``,
hoomd/data/array.py:``strides``, and ``ndim`` properties. `HOOMDGPUArray` never supports standard
hoomd/data/array.py:    Use, ``cupy.add``, ``cupy.multiply``, etc. for binary operations on the GPU.
hoomd/data/array.py:    Packages like Numba and PyTorch can use `HOOMDGPUArray` without CuPy
hoomd/data/array.py:    `__cuda_array_interface__
hoomd/data/array.py:    <https://nvidia.github.io/numba-cuda/user/cuda_array_interface.html>`_
hoomd/data/array.py:    should support the direct use of `HOOMDGPUArray` objects.
hoomd/data/array.py:HOOMDGPUArray.__doc__ = _gpu_array_docs
hoomd/data/local_access_cpu.py:    """Access bond data on the GPU."""
hoomd/data/local_access_cpu.py:    """Access angle data on the GPU."""
hoomd/data/local_access_cpu.py:    """Access dihedral data on the GPU."""
hoomd/data/local_access_cpu.py:    """Access improper data on the GPU."""
hoomd/data/local_access_cpu.py:    """Access constraint data on the GPU."""
hoomd/data/local_access_cpu.py:    """Access special pair data on the GPU."""
hoomd/data/__init__.py:`LocalSnapshot`, `LocalSnapshotGPU`, and related classes provide direct access
hoomd/data/__init__.py:from .array import HOOMDArray, HOOMDGPUArray
hoomd/data/__init__.py:from .local_access_gpu import LocalSnapshotGPU
hoomd/pytest/test_local_snapshot.py:"""Test that `LocalSnapshot` and `LocalSnapshotGPU` work."""
hoomd/pytest/test_local_snapshot.py:from hoomd.data.array import HOOMDGPUArray
hoomd/pytest/test_local_snapshot.py:    # We use the CUPY_IMPORTED variable to allow for local GPU testing without
hoomd/pytest/test_local_snapshot.py:    # CuPy installed when build for the GPU.
hoomd/pytest/test_local_snapshot.py:    """Allows checking of equality with both HOOMDArrays and HOOMDGPUArrays."""
hoomd/pytest/test_local_snapshot.py:        if any(isinstance(a, HOOMDGPUArray) for a in (arr1, arr2)):
hoomd/pytest/test_local_snapshot.py:    if isinstance(data, HOOMDGPUArray):
hoomd/pytest/test_local_snapshot.py:    if isinstance(data, HOOMDGPUArray) and not CUPY_IMPORTED:
hoomd/pytest/test_local_snapshot.py:        pytest.skip("Not available for HOOMDGPUArray without CuPy.")
hoomd/pytest/test_local_snapshot.py:    if isinstance(data, HOOMDGPUArray):
hoomd/pytest/test_local_snapshot.py:    if isinstance(data, HOOMDGPUArray) and not CUPY_IMPORTED:
hoomd/pytest/test_local_snapshot.py:        pytest.skip("Not available for HOOMDGPUArray without CuPy.")
hoomd/pytest/test_local_snapshot.py:    if isinstance(data, HOOMDGPUArray):
hoomd/pytest/test_local_snapshot.py:    """Base class for CPU and GPU based localsnapshot tests."""
hoomd/pytest/test_local_snapshot.py:        if isinstance(prop, HOOMDGPUArray) and not CUPY_IMPORTED:
hoomd/pytest/test_local_snapshot.py:            yield 'gpu_local_snapshot'
hoomd/pytest/test_device.py:@pytest.mark.gpu
hoomd/pytest/test_device.py:def test_gpu_profile(device):
hoomd/pytest/test_device.py:@pytest.mark.gpu
hoomd/pytest/test_device.py:def test_gpu_specific_properties(device):
hoomd/pytest/test_device.py:    assert device.gpu_error_checking
hoomd/pytest/test_device.py:    device.gpu_error_checking = False
hoomd/pytest/test_device.py:    assert not device.gpu_error_checking
hoomd/pytest/test_device.py:    # make sure we can give a list of GPU ids to the constructor
hoomd/pytest/test_device.py:    hoomd.device.GPU(gpu_ids=[0])
hoomd/pytest/test_device.py:    hoomd.device.GPU(gpu_id=0)
hoomd/pytest/test_device.py:@pytest.mark.gpu
hoomd/pytest/test_device.py:def test_other_gpu_specifics(device):
hoomd/pytest/test_device.py:    # make sure GPU is available and auto-select gives a GPU
hoomd/pytest/test_device.py:    assert hoomd.device.GPU.is_available()
hoomd/pytest/test_device.py:    assert type(hoomd.device.auto_select()) == hoomd.device.GPU
hoomd/pytest/test_device.py:@pytest.mark.gpu
hoomd/pytest/test_device.py:    if hoomd.version.gpu_enabled:
hoomd/pytest/test_device.py:        pytest.skip("Don't run CPU-build specific tests when GPU is available")
hoomd/pytest/test_device.py:    assert not hoomd.device.GPU.is_available()
hoomd/test/test_system.cc:    g_gpu_error_checking = true;
hoomd/test/test_gpu_array.cuh:/*! \file gpu_array_test.cuh
hoomd/test/test_gpu_array.cuh:    \brief Definitions of GPU kernel drivers for gpu_array_test.cc
hoomd/test/test_gpu_array.cuh:#ifndef __GPU_ARRAY_TEST_CUH__
hoomd/test/test_gpu_array.cuh:#define __GPU_ARRAY_TEST_CUH__
hoomd/test/test_gpu_array.cuh:hipError_t gpu_add_one(int* d_data, size_t num);
hoomd/test/test_gpu_array.cuh:hipError_t gpu_fill_test_pattern(int* d_data, size_t num);
hoomd/test/test_gpu_array.cc:#include "hoomd/GPUArray.h"
hoomd/test/test_gpu_array.cc:#include "hoomd/GPUVector.h"
hoomd/test/test_gpu_array.cc:#include "test_gpu_array.cuh"
hoomd/test/test_gpu_array.cc:/*! \file gpu_array_test.cc
hoomd/test/test_gpu_array.cc:    \brief Implements unit tests for GPUArray and GPUVector
hoomd/test/test_gpu_array.cc://! test case for testing the basic operation of GPUArray
hoomd/test/test_gpu_array.cc:UP_TEST(GPUArray_basic_tests)
hoomd/test/test_gpu_array.cc:    GPUArray<int> gpu_array(100, exec_conf);
hoomd/test/test_gpu_array.cc:    UP_ASSERT_EQUAL((int)gpu_array.getNumElements(), 100);
hoomd/test/test_gpu_array.cc:        ArrayHandle<int> h_handle(gpu_array, access_location::host, access_mode::readwrite);
hoomd/test/test_gpu_array.cc:        for (int i = 0; i < (int)gpu_array.getNumElements(); i++)
hoomd/test/test_gpu_array.cc:        ArrayHandle<int> h_handle(gpu_array, access_location::host, access_mode::read);
hoomd/test/test_gpu_array.cc:        for (int i = 0; i < (int)gpu_array.getNumElements(); i++)
hoomd/test/test_gpu_array.cc:    // basic check 3.5: test the construction of a 2-D GPUArray
hoomd/test/test_gpu_array.cc:    GPUArray<int> gpu_array_2d(63, 120, exec_conf);
hoomd/test/test_gpu_array.cc:    UP_ASSERT_EQUAL((int)gpu_array_2d.getPitch(), 64);
hoomd/test/test_gpu_array.cc:    UP_ASSERT_EQUAL((int)gpu_array_2d.getHeight(), 120);
hoomd/test/test_gpu_array.cc:    UP_ASSERT_EQUAL((int)gpu_array_2d.getNumElements(), 7680);
hoomd/test/test_gpu_array.cc:    GPUArray<int> array_b(gpu_array);
hoomd/test/test_gpu_array.cc:    GPUArray<int> array_c(1, exec_conf);
hoomd/test/test_gpu_array.cc:    array_c = gpu_array;
hoomd/test/test_gpu_array.cc:UP_TEST(GPUArray_transfer_tests)
hoomd/test/test_gpu_array.cc:        new ExecutionConfiguration(ExecutionConfiguration::GPU));
hoomd/test/test_gpu_array.cc:    UP_ASSERT(exec_conf->isCUDAEnabled());
hoomd/test/test_gpu_array.cc:    GPUArray<int> gpu_array(100, exec_conf);
hoomd/test/test_gpu_array.cc:        ArrayHandle<int> d_handle(gpu_array, access_location::device, access_mode::readwrite);
hoomd/test/test_gpu_array.cc:        gpu_fill_test_pattern(d_handle.data, gpu_array.getNumElements());
hoomd/test/test_gpu_array.cc:        ArrayHandle<int> h_handle(gpu_array, access_location::host, access_mode::readwrite);
hoomd/test/test_gpu_array.cc:        for (int i = 0; i < (int)gpu_array.getNumElements(); i++)
hoomd/test/test_gpu_array.cc:        ArrayHandle<int> d_handle(gpu_array, access_location::device, access_mode::overwrite);
hoomd/test/test_gpu_array.cc:        gpu_add_one(d_handle.data, gpu_array.getNumElements());
hoomd/test/test_gpu_array.cc:        ArrayHandle<int> h_handle(gpu_array, access_location::host, access_mode::readwrite);
hoomd/test/test_gpu_array.cc:        for (int i = 0; i < (int)gpu_array.getNumElements(); i++)
hoomd/test/test_gpu_array.cc:        ArrayHandle<int> d_handle(gpu_array, access_location::device, access_mode::read);
hoomd/test/test_gpu_array.cc:        gpu_add_one(d_handle.data, gpu_array.getNumElements());
hoomd/test/test_gpu_array.cc:        ArrayHandle<int> h_handle(gpu_array, access_location::host, access_mode::readwrite);
hoomd/test/test_gpu_array.cc:        for (int i = 0; i < (int)gpu_array.getNumElements(); i++)
hoomd/test/test_gpu_array.cc:        ArrayHandle<int> d_handle(gpu_array, access_location::device, access_mode::readwrite);
hoomd/test/test_gpu_array.cc:        gpu_add_one(d_handle.data, gpu_array.getNumElements());
hoomd/test/test_gpu_array.cc:        ArrayHandle<int> h_handle(gpu_array, access_location::host, access_mode::read);
hoomd/test/test_gpu_array.cc:        for (int i = 0; i < (int)gpu_array.getNumElements(); i++)
hoomd/test/test_gpu_array.cc:        ArrayHandle<int> d_handle(gpu_array, access_location::device, access_mode::readwrite);
hoomd/test/test_gpu_array.cc:        gpu_add_one(d_handle.data, gpu_array.getNumElements());
hoomd/test/test_gpu_array.cc:        ArrayHandle<int> h_handle(gpu_array, access_location::host, access_mode::readwrite);
hoomd/test/test_gpu_array.cc:        for (int i = 0; i < (int)gpu_array.getNumElements(); i++)
hoomd/test/test_gpu_array.cc://! Tests operations on NULL GPUArrays
hoomd/test/test_gpu_array.cc:UP_TEST(GPUArray_null_tests)
hoomd/test/test_gpu_array.cc:    // Construct a NULL GPUArray
hoomd/test/test_gpu_array.cc:    GPUArray<int> a;
hoomd/test/test_gpu_array.cc:    // check copy construction of a NULL GPUArray
hoomd/test/test_gpu_array.cc:    GPUArray<int> b(a);
hoomd/test/test_gpu_array.cc:    // check assignment of a NULL GPUArray
hoomd/test/test_gpu_array.cc:        new ExecutionConfiguration(ExecutionConfiguration::GPU));
hoomd/test/test_gpu_array.cc:    GPUArray<int> c(1000, exec_conf);
hoomd/test/test_gpu_array.cc:    // check swapping of a NULL GPUArray
hoomd/test/test_gpu_array.cc:    GPUArray<int> d(1000, exec_conf);
hoomd/test/test_gpu_array.cc:UP_TEST(GPUArray_resize_tests)
hoomd/test/test_gpu_array.cc:        new ExecutionConfiguration(ExecutionConfiguration::GPU));
hoomd/test/test_gpu_array.cc:    // create a 1D GPUArray
hoomd/test/test_gpu_array.cc:    GPUArray<unsigned int> a(5, exec_conf);
hoomd/test/test_gpu_array.cc:    // check that it also works for a GPUArray that is initially empty
hoomd/test/test_gpu_array.cc:    GPUArray<unsigned int> b;
hoomd/test/test_gpu_array.cc:    // allocate a 2D GPUArray
hoomd/test/test_gpu_array.cc:    GPUArray<unsigned int> c(width, height, exec_conf);
hoomd/test/test_gpu_array.cc://! Tests GPUVector
hoomd/test/test_gpu_array.cc:UP_TEST(GPUVector_basic_tests)
hoomd/test/test_gpu_array.cc:        new ExecutionConfiguration(ExecutionConfiguration::GPU));
hoomd/test/test_gpu_array.cc:    // First create an empty GPUVector
hoomd/test/test_gpu_array.cc:    GPUVector<unsigned int> vec(exec_conf);
hoomd/test/test_gpu_array.cc:    GPUVector<unsigned int> v;
hoomd/test/test_gpu_array.cc:    v = std::move(GPUVector<unsigned int>(
hoomd/test/test_gpu_array.cc:        std::make_shared<ExecutionConfiguration>(ExecutionConfiguration::GPU)));
hoomd/test/test_gpu_array.cc:    GPUVector<unsigned int> v2 = std::move(GPUVector<unsigned int>(
hoomd/test/test_gpu_array.cc:        std::make_shared<ExecutionConfiguration>(ExecutionConfiguration::GPU)));
hoomd/test/test_warp_tools.cc:#include "hoomd/GPUArray.h"
hoomd/test/test_warp_tools.cc:    auto exec_conf = std::make_shared<ExecutionConfiguration>(ExecutionConfiguration::GPU);
hoomd/test/test_warp_tools.cc:    GPUArray<int> vec(N * width, exec_conf);
hoomd/test/test_warp_tools.cc:    GPUArray<int> reduce(reduce_idx.getNumElements(), exec_conf);
hoomd/test/test_warp_tools.cc:    GPUArray<int> sum(N, exec_conf);
hoomd/test/test_warp_tools.cc:    auto exec_conf = std::make_shared<ExecutionConfiguration>(ExecutionConfiguration::GPU);
hoomd/test/test_warp_tools.cc:    GPUArray<int> vec(N * width, exec_conf);
hoomd/test/test_warp_tools.cc:    GPUArray<int> scan(scan_idx.getNumElements(), exec_conf);
hoomd/test/test_warp_tools.cc:    GPUArray<int> sum(N, exec_conf);
hoomd/test/CMakeLists.txt:    test_gpu_array
hoomd/test/CMakeLists.txt:        set(_cuda_sources ${CUR_TEST}.cu)
hoomd/test/CMakeLists.txt:        set_source_files_properties(${_cuda_sources} PROPERTIES LANGUAGE ${HOOMD_DEVICE_LANGUAGE})
hoomd/test/CMakeLists.txt:        set(_cuda_sources "")
hoomd/test/CMakeLists.txt:    add_executable(${CUR_TEST} EXCLUDE_FROM_ALL ${CUR_TEST}.cc ${_cuda_sources})
hoomd/test/CMakeLists.txt:# run tests under cuda-memcheck --tool synccheck
hoomd/test/CMakeLists.txt:                         ${CUDA_MEMCHECK_EXECUTABLE} --error-exitcode 123 --tool synccheck $<TARGET_FILE:${CUR_TEST}>)
hoomd/test/CMakeLists.txt:        add_test(NAME ${CUR_TEST}-synccheck COMMAND ${CUDA_MEMCHECK_EXECUTABLE} --error-exitcode 123 --tool synccheck $<TARGET_FILE:${CUR_TEST}>)
hoomd/test/test_warp_tools.cuh: * \brief Supporting data structures and CUDA kernels for testing warp-level primitives.
hoomd/test/test_cell_list.cc:#include "hoomd/CellListGPU.h"
hoomd/test/test_cell_list.cc://! test case for celllist_small_test on the GPU
hoomd/test/test_cell_list.cc:UP_TEST(CellListGPU_small)
hoomd/test/test_cell_list.cc:    celllist_small_test<CellListGPU>(std::shared_ptr<ExecutionConfiguration>(
hoomd/test/test_cell_list.cc:        new ExecutionConfiguration(ExecutionConfiguration::GPU)));
hoomd/test/test_cell_list.cc://! test case for celllist_large_test on the GPU
hoomd/test/test_cell_list.cc:UP_TEST(CellListGPU_large)
hoomd/test/test_cell_list.cc:    celllist_large_test<CellListGPU>(std::shared_ptr<ExecutionConfiguration>(
hoomd/test/test_cell_list.cc:        new ExecutionConfiguration(ExecutionConfiguration::GPU)));
hoomd/test/test_load_balancer.cc:#include "hoomd/LoadBalancerGPU.h"
hoomd/test/test_load_balancer.cc://! Tests basic particle redistribution on the GPU
hoomd/test/test_load_balancer.cc:UP_TEST(LoadBalancerGPU_test_basic)
hoomd/test/test_load_balancer.cc:        new ExecutionConfiguration(ExecutionConfiguration::GPU));
hoomd/test/test_load_balancer.cc:    test_load_balancer_basic<LoadBalancerGPU>(exec_conf, BoxDim(2.0));
hoomd/test/test_load_balancer.cc:    test_load_balancer_basic<LoadBalancerGPU>(exec_conf, BoxDim(1.0, .1, .2, .3));
hoomd/test/test_load_balancer.cc:    test_load_balancer_basic<LoadBalancerGPU>(exec_conf, BoxDim(1.0, -.6, .7, .5));
hoomd/test/test_load_balancer.cc://! Tests particle redistribution with multiple domains and specific directions on the GPU
hoomd/test/test_load_balancer.cc:UP_TEST(LoadBalancerGPU_test_multi)
hoomd/test/test_load_balancer.cc:        new ExecutionConfiguration(ExecutionConfiguration::GPU));
hoomd/test/test_load_balancer.cc:    test_load_balancer_multi<LoadBalancerGPU>(exec_conf, BoxDim(2.0));
hoomd/test/test_load_balancer.cc:    test_load_balancer_multi<LoadBalancerGPU>(exec_conf, BoxDim(1.0, .1, .2, .3));
hoomd/test/test_load_balancer.cc:    test_load_balancer_multi<LoadBalancerGPU>(exec_conf, BoxDim(1.0, -.6, .7, .5));
hoomd/test/test_load_balancer.cc:UP_TEST(LoadBalancerGPU_test_ghost)
hoomd/test/test_load_balancer.cc:        new ExecutionConfiguration(ExecutionConfiguration::GPU));
hoomd/test/test_load_balancer.cc:    test_load_balancer_ghost<LoadBalancerGPU>(exec_conf, BoxDim(2.0));
hoomd/test/test_load_balancer.cc:    test_load_balancer_ghost<LoadBalancerGPU>(exec_conf, BoxDim(1.0, .1, .2, .3));
hoomd/test/test_load_balancer.cc:    test_load_balancer_ghost<LoadBalancerGPU>(exec_conf, BoxDim(1.0, -.6, .7, .5));
hoomd/test/test_gpu_array.cu:#include "test_gpu_array.cuh"
hoomd/test/test_gpu_array.cu:/*! \file gpu_array_test.cu
hoomd/test/test_gpu_array.cu:    \brief GPU kernels for gpu_array_test.cc
hoomd/test/test_gpu_array.cu:__global__ void gpu_add_one_kernel(int* d_data, size_t num)
hoomd/test/test_gpu_array.cu:    gpu_add_one is just a driver for gpu_add_one_kernel()
hoomd/test/test_gpu_array.cu:hipError_t gpu_add_one(int* d_data, size_t num)
hoomd/test/test_gpu_array.cu:    hipLaunchKernelGGL((gpu_add_one_kernel), dim3(grid), dim3(threads), 0, 0, d_data, num);
hoomd/test/test_gpu_array.cu:__global__ void gpu_fill_test_pattern_kernel(int* d_data, size_t num)
hoomd/test/test_gpu_array.cu:    gpu_fill_test_pattern is just a driver for gpu_fill_test_pattern_kernel()
hoomd/test/test_gpu_array.cu:hipError_t gpu_fill_test_pattern(int* d_data, size_t num)
hoomd/test/test_gpu_array.cu:    hipLaunchKernelGGL((gpu_fill_test_pattern_kernel),
hoomd/test/test_global_array.cuh:    \brief Definitions of GPU kernel drivers for global_array_test.cc
hoomd/test/test_global_array.cuh:hipError_t gpu_add_one(int* d_data, size_t num);
hoomd/test/test_global_array.cuh:hipError_t gpu_fill_test_pattern(int* d_data, size_t num);
hoomd/test/test_cell_list_stencil.cc:#include "hoomd/CellListGPU.h"
hoomd/test/test_cell_list_stencil.cc://! test case for cell list stencil on the GPU
hoomd/test/test_cell_list_stencil.cc:UP_TEST(CellListStencil_gpu)
hoomd/test/test_cell_list_stencil.cc:    celllist_stencil_basic_test<CellListGPU>(std::shared_ptr<ExecutionConfiguration>(
hoomd/test/test_cell_list_stencil.cc:        new ExecutionConfiguration(ExecutionConfiguration::GPU)));
hoomd/test/upp11_config.h:std::shared_ptr<hoomd::ExecutionConfiguration> exec_conf_gpu;
hoomd/test/upp11_config.h:        exec_conf_gpu.reset();                                                \
hoomd/test/upp11_config.h:        exec_conf_gpu.reset();                                                \
hoomd/test/test_global_array.cc:#include "hoomd/GPUVector.h"
hoomd/test/test_global_array.cc:/*! \file gpu_array_test.cc
hoomd/test/test_global_array.cc:    \brief Implements unit tests for GlobalArray and GPUVector
hoomd/test/test_global_array.cc:    GlobalArray<int> gpu_array(100, exec_conf);
hoomd/test/test_global_array.cc:    UP_ASSERT_EQUAL((int)gpu_array.getNumElements(), 100);
hoomd/test/test_global_array.cc:        ArrayHandle<int> h_handle(gpu_array, access_location::host, access_mode::readwrite);
hoomd/test/test_global_array.cc:        for (int i = 0; i < (int)gpu_array.getNumElements(); i++)
hoomd/test/test_global_array.cc:        ArrayHandle<int> h_handle(gpu_array, access_location::host, access_mode::read);
hoomd/test/test_global_array.cc:        for (int i = 0; i < (int)gpu_array.getNumElements(); i++)
hoomd/test/test_global_array.cc:    GlobalArray<int> gpu_array_2d(63, 120, exec_conf);
hoomd/test/test_global_array.cc:    UP_ASSERT_EQUAL((int)gpu_array_2d.getPitch(), 64);
hoomd/test/test_global_array.cc:    UP_ASSERT_EQUAL((int)gpu_array_2d.getHeight(), 120);
hoomd/test/test_global_array.cc:    UP_ASSERT_EQUAL((int)gpu_array_2d.getNumElements(), 7680);
hoomd/test/test_global_array.cc:    GlobalArray<int> array_b(gpu_array);
hoomd/test/test_global_array.cc:    array_c = gpu_array;
hoomd/test/test_global_array.cc:        new ExecutionConfiguration(ExecutionConfiguration::GPU));
hoomd/test/test_global_array.cc:    UP_ASSERT(exec_conf->isCUDAEnabled());
hoomd/test/test_global_array.cc:    GlobalArray<int> gpu_array(100, exec_conf);
hoomd/test/test_global_array.cc:        ArrayHandle<int> d_handle(gpu_array, access_location::device, access_mode::readwrite);
hoomd/test/test_global_array.cc:        gpu_fill_test_pattern(d_handle.data, gpu_array.getNumElements());
hoomd/test/test_global_array.cc:        ArrayHandle<int> h_handle(gpu_array, access_location::host, access_mode::readwrite);
hoomd/test/test_global_array.cc:        for (int i = 0; i < (int)gpu_array.getNumElements(); i++)
hoomd/test/test_global_array.cc:        ArrayHandle<int> d_handle(gpu_array, access_location::device, access_mode::readwrite);
hoomd/test/test_global_array.cc:        gpu_add_one(d_handle.data, gpu_array.getNumElements());
hoomd/test/test_global_array.cc:        ArrayHandle<int> h_handle(gpu_array, access_location::host, access_mode::read);
hoomd/test/test_global_array.cc:        for (int i = 0; i < (int)gpu_array.getNumElements(); i++)
hoomd/test/test_global_array.cc:        ArrayHandle<int> d_handle(gpu_array, access_location::device, access_mode::readwrite);
hoomd/test/test_global_array.cc:        gpu_add_one(d_handle.data, gpu_array.getNumElements());
hoomd/test/test_global_array.cc:        ArrayHandle<int> h_handle(gpu_array, access_location::host, access_mode::readwrite);
hoomd/test/test_global_array.cc:        for (int i = 0; i < (int)gpu_array.getNumElements(); i++)
hoomd/test/test_global_array.cc:        new ExecutionConfiguration(ExecutionConfiguration::GPU));
hoomd/test/test_global_array.cc:        new ExecutionConfiguration(ExecutionConfiguration::GPU));
hoomd/test/test_global_array.cc://! Tests GPUVector
hoomd/test/test_global_array.cc:UP_TEST(GPUVector_basic_tests)
hoomd/test/test_global_array.cc:        new ExecutionConfiguration(ExecutionConfiguration::GPU));
hoomd/test/test_global_array.cc:    // First create an empty GPUVector
hoomd/test/test_global_array.cc:    GPUVector<unsigned int> vec(exec_conf);
hoomd/test/test_warp_tools.cu: * \brief CUDA kernels for testing warp-level primitives.
hoomd/test/test_global_array.cu:/*! \file gpu_array_test.cu
hoomd/test/test_global_array.cu:    \brief GPU kernels for gpu_array_test.cc
hoomd/test/test_global_array.cu:__global__ void gpu_add_one_kernel(int* d_data, size_t num)
hoomd/test/test_global_array.cu:    gpu_add_one is just a driver for gpu_add_one_kernel()
hoomd/test/test_global_array.cu:hipError_t gpu_add_one(int* d_data, size_t num)
hoomd/test/test_global_array.cu:    hipLaunchKernelGGL((gpu_add_one_kernel), dim3(grid), dim3(threads), 0, 0, d_data, num);
hoomd/test/test_global_array.cu:__global__ void gpu_fill_test_pattern_kernel(int* d_data, size_t num)
hoomd/test/test_global_array.cu:    gpu_fill_test_pattern is just a driver for gpu_fill_test_pattern_kernel()
hoomd/test/test_global_array.cu:hipError_t gpu_fill_test_pattern(int* d_data, size_t num)
hoomd/test/test_global_array.cu:    hipLaunchKernelGGL((gpu_fill_test_pattern_kernel),
hoomd/ForceCompute.cc:    \post \c force and \c virial GPUarrays are initialized
hoomd/ForceCompute.cc:    if (m_exec_conf->isCUDAEnabled() && m_exec_conf->allConcurrentManagedAccess())
hoomd/ForceCompute.cc:        auto gpu_map = m_exec_conf->getGPUIds();
hoomd/ForceCompute.cc:        // set up GPU memory mappings
hoomd/ForceCompute.cc:        for (unsigned int idev = 0; idev < m_exec_conf->getNumActiveGPUs(); ++idev)
hoomd/ForceCompute.cc:            cudaMemAdvise(m_force.get(),
hoomd/ForceCompute.cc:                          cudaMemAdviseSetAccessedBy,
hoomd/ForceCompute.cc:                          gpu_map[idev]);
hoomd/ForceCompute.cc:            cudaMemAdvise(m_virial.get(),
hoomd/ForceCompute.cc:                          cudaMemAdviseSetAccessedBy,
hoomd/ForceCompute.cc:                          gpu_map[idev]);
hoomd/ForceCompute.cc:            cudaMemAdvise(m_torque.get(),
hoomd/ForceCompute.cc:                          cudaMemAdviseSetAccessedBy,
hoomd/ForceCompute.cc:                          gpu_map[idev]);
hoomd/ForceCompute.cc:        CHECK_CUDA_ERROR();
hoomd/ForceCompute.cc:    // initialize GPU memory hints
hoomd/ForceCompute.cc:    updateGPUAdvice();
hoomd/ForceCompute.cc:    updateGPUAdvice();
hoomd/ForceCompute.cc:void ForceCompute::updateGPUAdvice()
hoomd/ForceCompute.cc:    if (m_exec_conf->isCUDAEnabled() && m_exec_conf->allConcurrentManagedAccess())
hoomd/ForceCompute.cc:        auto gpu_map = m_exec_conf->getGPUIds();
hoomd/ForceCompute.cc:        // split preferred location of particle data across GPUs
hoomd/ForceCompute.cc:        const GPUPartition& gpu_partition = m_pdata->getGPUPartition();
hoomd/ForceCompute.cc:        for (unsigned int idev = 0; idev < m_exec_conf->getNumActiveGPUs(); ++idev)
hoomd/ForceCompute.cc:            auto range = gpu_partition.getRange(idev);
hoomd/ForceCompute.cc:            cudaMemAdvise(m_force.get() + range.first,
hoomd/ForceCompute.cc:                          cudaMemAdviseSetPreferredLocation,
hoomd/ForceCompute.cc:                          gpu_map[idev]);
hoomd/ForceCompute.cc:                cudaMemAdvise(m_virial.get() + i * m_virial.getPitch() + range.first,
hoomd/ForceCompute.cc:                              cudaMemAdviseSetPreferredLocation,
hoomd/ForceCompute.cc:                              gpu_map[idev]);
hoomd/ForceCompute.cc:            cudaMemAdvise(m_torque.get() + range.first,
hoomd/ForceCompute.cc:                          cudaMemAdviseSetPreferredLocation,
hoomd/ForceCompute.cc:                          gpu_map[idev]);
hoomd/ForceCompute.cc:            cudaMemPrefetchAsync(m_force.get() + range.first,
hoomd/ForceCompute.cc:                                 gpu_map[idev]);
hoomd/ForceCompute.cc:                cudaMemPrefetchAsync(m_virial.get() + i * m_virial.getPitch() + range.first,
hoomd/ForceCompute.cc:                                     gpu_map[idev]);
hoomd/ForceCompute.cc:            cudaMemPrefetchAsync(m_torque.get() + range.first,
hoomd/ForceCompute.cc:                                 gpu_map[idev]);
hoomd/ForceCompute.cc:        CHECK_CUDA_ERROR();
hoomd/ForceCompute.cc:        // set up GPU memory mappings
hoomd/ForceCompute.cc:        for (unsigned int idev = 0; idev < m_exec_conf->getNumActiveGPUs(); ++idev)
hoomd/ForceCompute.cc:            cudaMemAdvise(m_force.get(),
hoomd/ForceCompute.cc:                          cudaMemAdviseSetAccessedBy,
hoomd/ForceCompute.cc:                          gpu_map[idev]);
hoomd/ForceCompute.cc:            cudaMemAdvise(m_virial.get(),
hoomd/ForceCompute.cc:                          cudaMemAdviseSetAccessedBy,
hoomd/ForceCompute.cc:                          gpu_map[idev]);
hoomd/ForceCompute.cc:            cudaMemAdvise(m_torque.get(),
hoomd/ForceCompute.cc:                          cudaMemAdviseSetAccessedBy,
hoomd/ForceCompute.cc:                          gpu_map[idev]);
hoomd/ForceCompute.cc:        CHECK_CUDA_ERROR();
hoomd/PythonLocalDataAccess.h: *  or GPU(Device) buffer.  HOOMDBuffer classes need to implement a templated
hoomd/PythonLocalDataAccess.h:/// Represents the data required to implement the __cuda_array_interface__.
hoomd/PythonLocalDataAccess.h:/** Creates the Python dictionary to represent a GPU array through the
hoomd/PythonLocalDataAccess.h: *  __cuda_array_interface__. Currently supports version 2 of the protocol.
hoomd/PythonLocalDataAccess.h:    /// Convert object to a __cuda_array_interface__ v2 compliant Python dict.
hoomd/PythonLocalDataAccess.h:    pybind11::dict getCudaArrayInterface()
hoomd/PythonLocalDataAccess.h:/** @brief Base class for accessing Global or GPU arrays/vectors in Python.
hoomd/PythonLocalDataAccess.h: *  Global/GPUArray into an object of type Output.
hoomd/PythonLocalDataAccess.h:    /** @brief Convert Global/GPUArray or vector into an Ouput object for Python.
hoomd/PythonLocalDataAccess.h:/** @brief Base class for accessing per-* Global or GPU arrays/vectors in Python.
hoomd/PythonLocalDataAccess.h: *  which provide a way to automatically convert an Global/GPUArray into an
hoomd/PythonLocalDataAccess.h:    /** @brief Convert Global/GPUArray or vector into an Ouput object for Python.
hoomd/PythonLocalDataAccess.h:     *  rank or GPU, the number of positions a ranks knows about (including
hoomd/PythonLocalDataAccess.h:    /** @brief Convert Global/GPUArray or vector into an Ouput object for Python.
hoomd/PythonLocalDataAccess.h:     *  size. An example is the reverse tag index. On each MPI rank or GPU,
hoomd/VectorMath.h:// Just return x on GPU or when using JIT as exceptions are disabled on GPU and JIT code.
hoomd/VectorMath.h:            // compiler warnings on the GPU and it must be something that can be returned by
hoomd/VectorMath.h:// Just return x on GPU or when using JIT as exceptions are disabled on GPU and JIT code.
hoomd/VectorMath.h:            // compiler warnings on the GPU and returning x matches the non-const version of the
hoomd/CellListStencil.h:    const GPUArray<Scalar4>& getStencils() const
hoomd/CellListStencil.h:    const GPUArray<unsigned int>& getStencilSizes() const
hoomd/CellListStencil.h:    GPUArray<Scalar4> m_stencil;        //!< Stencil of shifts and closest distance to bin
hoomd/CellListStencil.h:    GPUArray<unsigned int> m_n_stencil; //!< Number of bins in a stencil
hoomd/version_config.py.in:cuda_include_path = "${CMAKE_CUDA_TOOLKIT_INCLUDE_DIRECTORIES}"
hoomd/version_config.py.in:cuda_devrt_library = "${CUDA_cudadevrt_LIBRARY}"
hoomd/CommunicatorGPU.h:/*! \file CommunicatorGPU.h
hoomd/CommunicatorGPU.h:    \brief Defines the CommunicatorGPU class
hoomd/CommunicatorGPU.h:#ifndef __COMMUNICATOR_GPU_H__
hoomd/CommunicatorGPU.h:#define __COMMUNICATOR_GPU_H__
hoomd/CommunicatorGPU.h:#include "CommunicatorGPU.cuh"
hoomd/CommunicatorGPU.h:#include "GPUFlags.h"
hoomd/CommunicatorGPU.h:#include "GPUVector.h"
hoomd/CommunicatorGPU.h://! Class that handles MPI communication (GPU version)
hoomd/CommunicatorGPU.h:/*! CommunicatorGPU is the GPU implementation of the base communication class.
hoomd/CommunicatorGPU.h:class PYBIND11_EXPORT CommunicatorGPU : public Communicator
hoomd/CommunicatorGPU.h:    CommunicatorGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/CommunicatorGPU.h:    virtual ~CommunicatorGPU();
hoomd/CommunicatorGPU.h:    template<class group_data> class GroupCommunicatorGPU
hoomd/CommunicatorGPU.h:        GroupCommunicatorGPU(CommunicatorGPU& gpu_comm);
hoomd/CommunicatorGPU.h:        GroupCommunicatorGPU(CommunicatorGPU& gpu_comm, std::shared_ptr<group_data> gdata);
hoomd/CommunicatorGPU.h:        CommunicatorGPU& m_gpu_comm;                               //!< The outer class
hoomd/CommunicatorGPU.h:    GlobalVector<detail::pdata_element> m_gpu_sendbuf; //!< Send buffer for particle data
hoomd/CommunicatorGPU.h:    GlobalVector<detail::pdata_element> m_gpu_recvbuf; //!< Receive buffer for particle data
hoomd/CommunicatorGPU.h:    GroupCommunicatorGPU<BondData> m_bond_comm; //!< Communication helper for bonds
hoomd/CommunicatorGPU.h:    friend class GroupCommunicatorGPU<BondData>;
hoomd/CommunicatorGPU.h:    GroupCommunicatorGPU<AngleData> m_angle_comm; //!< Communication helper for angles
hoomd/CommunicatorGPU.h:    friend class GroupCommunicatorGPU<AngleData>;
hoomd/CommunicatorGPU.h:    GroupCommunicatorGPU<DihedralData> m_dihedral_comm; //!< Communication helper for dihedrals
hoomd/CommunicatorGPU.h:    friend class GroupCommunicatorGPU<DihedralData>;
hoomd/CommunicatorGPU.h:    GroupCommunicatorGPU<ImproperData> m_improper_comm; //!< Communication helper for impropers
hoomd/CommunicatorGPU.h:    friend class GroupCommunicatorGPU<ImproperData>;
hoomd/CommunicatorGPU.h:    GroupCommunicatorGPU<ConstraintData>
hoomd/CommunicatorGPU.h:    friend class GroupCommunicatorGPU<ConstraintData>;
hoomd/CommunicatorGPU.h:    GroupCommunicatorGPU<PairData> m_pair_comm; //!< Communication helper for pairs
hoomd/CommunicatorGPU.h:    friend class GroupCommunicatorGPU<PairData>;
hoomd/CommunicatorGPU.h:    GroupCommunicatorGPU<MeshBondData> m_meshbond_comm; //!< Communication helper for mesh bonds
hoomd/CommunicatorGPU.h:    friend class GroupCommunicatorGPU<MeshBondData>;
hoomd/CommunicatorGPU.h:    GroupCommunicatorGPU<TriangleData>
hoomd/CommunicatorGPU.h:    friend class GroupCommunicatorGPU<TriangleData>;
hoomd/CommunicatorGPU.h:    hipEvent_t m_event; //!< CUDA event for synchronization
hoomd/CommunicatorGPU.h://! Export CommunicatorGPU class to python
hoomd/CommunicatorGPU.h:void export_CommunicatorGPU(pybind11::module& m);
hoomd/CommunicatorGPU.h:#endif // __COMMUNICATOR_GPU_H
hoomd/metal/EAMForceCompute.cc:    GPUArray<Scalar4> t_F(nrho * m_ntypes, m_exec_conf);
hoomd/metal/EAMForceCompute.cc:    GPUArray<Scalar4> t_rho(nr * m_ntypes * m_ntypes, m_exec_conf);
hoomd/metal/EAMForceCompute.cc:    GPUArray<Scalar4> t_rphi((int)(0.5 * nr * (m_ntypes + 1) * m_ntypes), m_exec_conf);
hoomd/metal/EAMForceCompute.cc:    GPUArray<Scalar4> t_dF(nrho * m_ntypes, m_exec_conf);
hoomd/metal/EAMForceCompute.cc:    GPUArray<Scalar4> t_drho(nr * m_ntypes * m_ntypes, m_exec_conf);
hoomd/metal/EAMForceCompute.cc:    GPUArray<Scalar4> t_drphi((int)(0.5 * nr * (m_ntypes + 1) * m_ntypes), m_exec_conf);
hoomd/metal/EAMForceGPU.cu:#include "EAMForceGPU.cuh"
hoomd/metal/EAMForceGPU.cu:/*! \file EAMForceGPU.cu
hoomd/metal/EAMForceGPU.cu: \brief Defines GPU kernel code for calculating the EAM forces. Used by EAMForceComputeGPU.
hoomd/metal/EAMForceGPU.cu://! Kernel for computing EAM forces on the GPU
hoomd/metal/EAMForceGPU.cu:__global__ void gpu_kernel_1(Scalar4* d_force,
hoomd/metal/EAMForceGPU.cu://! Second stage kernel for computing EAM forces on the GPU
hoomd/metal/EAMForceGPU.cu:__global__ void gpu_kernel_2(Scalar4* d_force,
hoomd/metal/EAMForceGPU.cu://! compute forces on GPU
hoomd/metal/EAMForceGPU.cu:hipError_t gpu_compute_eam_tex_inter_forces(Scalar4* d_force,
hoomd/metal/EAMForceGPU.cu:    hipFuncGetAttributes(&attr1, reinterpret_cast<const void*>(gpu_kernel_1));
hoomd/metal/EAMForceGPU.cu:    hipFuncGetAttributes(&attr2, reinterpret_cast<const void*>(gpu_kernel_2));
hoomd/metal/EAMForceGPU.cu:    hipLaunchKernelGGL(gpu_kernel_1,
hoomd/metal/EAMForceGPU.cu:    hipLaunchKernelGGL(gpu_kernel_2,
hoomd/metal/EAMForceComputeGPU.cc:/*! \file EAMForceComputeGPU.cc
hoomd/metal/EAMForceComputeGPU.cc: \brief Defines the EAMForceComputeGPU class
hoomd/metal/EAMForceComputeGPU.cc:#include "EAMForceComputeGPU.h"
hoomd/metal/EAMForceComputeGPU.cc:EAMForceComputeGPU::EAMForceComputeGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/metal/EAMForceComputeGPU.cc:    // can't run on the GPU if there aren't any GPUs in the execution configuration
hoomd/metal/EAMForceComputeGPU.cc:    if (!m_exec_conf->isCUDAEnabled())
hoomd/metal/EAMForceComputeGPU.cc:            << "Creating a EAMForceComputeGPU with no GPU in the execution configuration" << endl;
hoomd/metal/EAMForceComputeGPU.cc:        throw std::runtime_error("Error initializing EAMForceComputeGPU");
hoomd/metal/EAMForceComputeGPU.cc:    // allocate the coefficients data on the GPU
hoomd/metal/EAMForceComputeGPU.cc:EAMForceComputeGPU::~EAMForceComputeGPU() { }
hoomd/metal/EAMForceComputeGPU.cc:void EAMForceComputeGPU::computeForces(uint64_t timestep)
hoomd/metal/EAMForceComputeGPU.cc:    // The GPU implementation CANNOT handle a half neighborlist, error out now
hoomd/metal/EAMForceComputeGPU.cc:        m_exec_conf->msg->error() << "EAMForceComputeGPU cannot handle a half neighborlist" << endl;
hoomd/metal/EAMForceComputeGPU.cc:        throw runtime_error("Error computing forces in EAMForceComputeGPU");
hoomd/metal/EAMForceComputeGPU.cc:    GPUArray<Scalar> t_dFdP(m_pdata->getN(), m_exec_conf);
hoomd/metal/EAMForceComputeGPU.cc:    // Compute energy and forces in GPU
hoomd/metal/EAMForceComputeGPU.cc:    kernel::gpu_compute_eam_tex_inter_forces(d_force.data,
hoomd/metal/EAMForceComputeGPU.cc:    if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/metal/EAMForceComputeGPU.cc:        CHECK_CUDA_ERROR();
hoomd/metal/EAMForceComputeGPU.cc:void export_EAMForceComputeGPU(pybind11::module& m)
hoomd/metal/EAMForceComputeGPU.cc:    pybind11::class_<EAMForceComputeGPU, EAMForceCompute, std::shared_ptr<EAMForceComputeGPU>>(
hoomd/metal/EAMForceComputeGPU.cc:        "EAMForceComputeGPU")
hoomd/metal/pair.py:        if not hoomd.context.current.device.cpp_exec_conf.isCUDAEnabled():
hoomd/metal/pair.py:            self.cpp_force = _metal.EAMForceComputeGPU(
hoomd/metal/pair.py:        if hoomd.context.current.device.cpp_exec_conf.isCUDAEnabled():
hoomd/metal/EAMForceGPU.cuh:/*! \file EAMForceGPU.cuh
hoomd/metal/EAMForceGPU.cuh: \brief Declares GPU kernel code for calculating the eam forces. Used by EAMForceComputeGPU.
hoomd/metal/EAMForceGPU.cuh:#ifndef __EAMTexInterForceGPU_CUH__
hoomd/metal/EAMForceGPU.cuh:#define __EAMTexInterForceGPU_CUH__
hoomd/metal/EAMForceGPU.cuh://! Collection of parameters for EAM force GPU kernels
hoomd/metal/EAMForceGPU.cuh://! Kernel driver that computes EAM forces on the GPU for EAMForceComputeGPU
hoomd/metal/EAMForceGPU.cuh:hipError_t gpu_compute_eam_tex_inter_forces(Scalar4* d_force,
hoomd/metal/EAMForceComputeGPU.h:#include "EAMForceGPU.cuh"
hoomd/metal/EAMForceComputeGPU.h:/*! \file EAMForceComputeGPU.h
hoomd/metal/EAMForceComputeGPU.h: \brief Declares the class EAMForceComputeGPU
hoomd/metal/EAMForceComputeGPU.h:#ifndef __EAMForceComputeGPU_H__
hoomd/metal/EAMForceComputeGPU.h:#define __EAMForceComputeGPU_H__
hoomd/metal/EAMForceComputeGPU.h://! Computes EAM forces on each particle using the GPU
hoomd/metal/EAMForceComputeGPU.h:/*! Calculates the same forces as EAMForceCompute, but on the GPU by using texture
hoomd/metal/EAMForceComputeGPU.h: * memory(CUDAArray).
hoomd/metal/EAMForceComputeGPU.h:class EAMForceComputeGPU : public EAMForceCompute
hoomd/metal/EAMForceComputeGPU.h:    EAMForceComputeGPU(std::shared_ptr<SystemDefinition> sysdef, char* filename, int type_of_file);
hoomd/metal/EAMForceComputeGPU.h:    virtual ~EAMForceComputeGPU();
hoomd/metal/EAMForceComputeGPU.h://! Exports the EAMForceComputeGPU class to python
hoomd/metal/EAMForceComputeGPU.h:void export_EAMForceComputeGPU(pybind11::module& m);
hoomd/metal/CMakeLists.txt:set(_${PACKAGE_NAME}_headers EAMForceComputeGPU.h
hoomd/metal/CMakeLists.txt:     EAMForceComputeGPU.cc
hoomd/metal/CMakeLists.txt:     EAMForceGPU.cu
hoomd/metal/CMakeLists.txt:set(_cuda_sources ${_${PACKAGE_NAME}_cu_sources})
hoomd/metal/CMakeLists.txt:hoomd_add_module(_${PACKAGE_NAME} SHARED ${_${PACKAGE_NAME}_sources} ${_cuda_sources} ${_${PACKAGE_NAME}_headers} NO_EXTRAS)
hoomd/metal/module-metal.cc:// include GPU classes
hoomd/metal/module-metal.cc:#include "EAMForceComputeGPU.h"
hoomd/metal/module-metal.cc:    export_EAMForceComputeGPU(m);
hoomd/metal/EAMForceCompute.h: The potential data and the coefficients are stored in six GPUArray<Scalar> arrays: the embedded
hoomd/metal/EAMForceCompute.h:    GPUArray<Scalar4> m_F;     //!< embedded function and its coefficients
hoomd/metal/EAMForceCompute.h:    GPUArray<Scalar4> m_rho;   //!< electron density and its coefficients
hoomd/metal/EAMForceCompute.h:    GPUArray<Scalar4> m_rphi;  //!< pair wise function and its coefficients
hoomd/metal/EAMForceCompute.h:    GPUArray<Scalar4> m_dF;    //!< derivative embedded function and its coefficients
hoomd/metal/EAMForceCompute.h:    GPUArray<Scalar4> m_drho;  //!< derivative electron density and its coefficients
hoomd/metal/EAMForceCompute.h:    GPUArray<Scalar4> m_drphi; //!< derivative pair wise function and its coefficients
hoomd/metal/EAMForceCompute.h:    GPUArray<Scalar> m_dFdP;   //!< derivative F / derivative P
hoomd/ArrayView.h://     This struct primarily exists for the MD walls for GPU support. Other uses are advised against
hoomd/SFCPackTunerGPU.cu:/*! \file SFCPackTunerGPU.cu
hoomd/SFCPackTunerGPU.cu:    \brief Defines GPU kernel code for generating the space-filling curve sorted order on the GPU.
hoomd/SFCPackTunerGPU.cu:   Used by SFCPackTunerGPU.
hoomd/SFCPackTunerGPU.cu:#include "SFCPackTunerGPU.cuh"
hoomd/SFCPackTunerGPU.cu:__global__ void gpu_sfc_bin_particles_kernel(unsigned int N,
hoomd/SFCPackTunerGPU.cu:void gpu_generate_sorted_order(unsigned int N,
hoomd/SFCPackTunerGPU.cu:        hipLaunchKernelGGL(HIP_KERNEL_NAME(gpu_sfc_bin_particles_kernel<true>),
hoomd/SFCPackTunerGPU.cu:        hipLaunchKernelGGL(HIP_KERNEL_NAME(gpu_sfc_bin_particles_kernel<false>),
hoomd/SFCPackTunerGPU.cu:        thrust::sort_by_key(thrust::cuda::par(alloc),
hoomd/SFCPackTunerGPU.cu:__global__ void gpu_apply_sorted_order_kernel(unsigned int N,
hoomd/SFCPackTunerGPU.cu:void gpu_apply_sorted_order(unsigned int N,
hoomd/SFCPackTunerGPU.cu:    hipLaunchKernelGGL(gpu_apply_sorted_order_kernel,
hoomd/GPUVector.h:/*! \file GPUVector.h
hoomd/GPUVector.h:    \brief Defines the GPUVector and GlobalVector classes
hoomd/GPUVector.h:#include "GPUArray.h"
hoomd/GPUVector.h:template<class T> class GPUVector;
hoomd/GPUVector.h://! Class for managing a vector of elements on the GPU mirrored to the CPU
hoomd/GPUVector.h:/*! The GPUVectorBase class is a simple container for a variable number of elements. Its interface
hoomd/GPUVector.h:    It uses a GPUArray as the underlying storage class, thus the data in a GPUVectorBase can also be
hoomd/GPUVector.h:    In the current implementation, a GPUVectorBase can only grow (but not shrink) in size until it
hoomd/GPUVector.h:template<class T, class Array> class GPUVectorBase : public Array
hoomd/GPUVector.h:    virtual ~GPUVectorBase() = default;
hoomd/GPUVector.h:    GPUVectorBase(const GPUVectorBase& from);
hoomd/GPUVector.h:    GPUVectorBase(GPUVectorBase&& other);
hoomd/GPUVector.h:    GPUVectorBase& operator=(const GPUVectorBase& rhs);
hoomd/GPUVector.h:    GPUVectorBase& operator=(GPUVectorBase&& other);
hoomd/GPUVector.h:    //! swap this GPUVectorBase with another
hoomd/GPUVector.h:    inline void swap(GPUVectorBase& from);
hoomd/GPUVector.h:    //! Resize the GPUVectorBase
hoomd/GPUVector.h:    //! Resize the GPUVectorBase
hoomd/GPUVector.h:        data_proxy(const GPUVectorBase<T, Array>& _vec, const size_t _n) : vec(_vec), n(_n) { }
hoomd/GPUVector.h:        const GPUVectorBase<T, Array>& vec; //!< The vector that is accessed
hoomd/GPUVector.h:    GPUVectorBase();
hoomd/GPUVector.h:    //! Constructs an empty GPUVectorBase
hoomd/GPUVector.h:    GPUVectorBase(std::shared_ptr<const ExecutionConfiguration> exec_conf);
hoomd/GPUVector.h:    //! Constructs a GPUVectorBase
hoomd/GPUVector.h:    GPUVectorBase(size_t size, std::shared_ptr<const ExecutionConfiguration> exec_conf);
hoomd/GPUVector.h:    //! Constructs a GPUVectorBase of given size, initialized with a constant value
hoomd/GPUVector.h:    GPUVectorBase(unsigned int size,
hoomd/GPUVector.h:    //! Constructs an empty GPUVectorBase
hoomd/GPUVector.h:    GPUVectorBase(std::shared_ptr<const ExecutionConfiguration> exec_conf, bool mapped);
hoomd/GPUVector.h:    //! Constructs a GPUVectorBase
hoomd/GPUVector.h:    GPUVectorBase(size_t size,
hoomd/GPUVector.h:    //! Helper function to reallocate the GPUArray (using amortized array resizing)
hoomd/GPUVector.h:    //! Acquire the underlying GPU array on the host
hoomd/GPUVector.h:// GPUVectorBase implementation
hoomd/GPUVector.h:/*! \warning When using this constructor, a properly initialized GPUVectorBase with an exec_conf
hoomd/GPUVector.h:   needs to be swapped in later, after construction of the GPUVectorBase.
hoomd/GPUVector.h:template<class T, class Array> GPUVectorBase<T, Array>::GPUVectorBase() : Array(), m_size(0) { }
hoomd/GPUVector.h:GPUVectorBase<T, Array>::GPUVectorBase(std::shared_ptr<const ExecutionConfiguration> exec_conf)
hoomd/GPUVector.h:GPUVectorBase<T, Array>::GPUVectorBase(size_t size,
hoomd/GPUVector.h:GPUVectorBase<T, Array>::GPUVectorBase(unsigned int size,
hoomd/GPUVector.h:GPUVectorBase<T, Array>::GPUVectorBase(const GPUVectorBase& from) : Array(from), m_size(from.m_size)
hoomd/GPUVector.h:GPUVectorBase<T, Array>::GPUVectorBase(GPUVectorBase&& other)
hoomd/GPUVector.h:GPUVectorBase<T, Array>& GPUVectorBase<T, Array>::operator=(const GPUVectorBase& rhs)
hoomd/GPUVector.h:GPUVectorBase<T, Array>& GPUVectorBase<T, Array>::operator=(GPUVectorBase&& other)
hoomd/GPUVector.h:/*! \param from GPUVectorBase to swap \a this with
hoomd/GPUVector.h:template<class T, class Array> void GPUVectorBase<T, Array>::swap(GPUVectorBase<T, Array>& from)
hoomd/GPUVector.h: * avoid excessive copying of data. The GPUArray is only reallocated if necessary,
hoomd/GPUVector.h:template<class T, class Array> void GPUVectorBase<T, Array>::reallocate(size_t size)
hoomd/GPUVector.h:        // actually resize the underlying GPUArray
hoomd/GPUVector.h: \post The GPUVectorBase will be re-allocated if necessary to hold the new elements.
hoomd/GPUVector.h:template<class T, class Array> void GPUVectorBase<T, Array>::resize(size_t new_size)
hoomd/GPUVector.h: \post The GPUVectorBase will be re-allocated if necessary to hold the new elements.
hoomd/GPUVector.h:template<class T, class Array> void GPUVectorBase<T, Array>::resize(size_t new_size, const T& value)
hoomd/GPUVector.h:template<class T, class Array> void GPUVectorBase<T, Array>::push_back(const T& val)
hoomd/GPUVector.h:template<class T, class Array> void GPUVectorBase<T, Array>::pop_back()
hoomd/GPUVector.h:template<class T, class Array> void GPUVectorBase<T, Array>::erase(size_t i)
hoomd/GPUVector.h:template<class T, class Array> void GPUVectorBase<T, Array>::clear()
hoomd/GPUVector.h:/*! \param mode Access mode for the GPUArray
hoomd/GPUVector.h:ArrayHandleDispatch<T> GPUVectorBase<T, Array>::acquireHost(const access_mode::Enum mode) const
hoomd/GPUVector.h:    return GPUArrayBase<T, Array>::acquire(access_location::host, access_mode::readwrite, false);
hoomd/GPUVector.h:template<class T> class GPUArray;
hoomd/GPUVector.h:// Specialization for zero copy memory (GPUArray)
hoomd/GPUVector.h:template<class T> class GPUVector : public GPUVectorBase<T, GPUArray<T>>
hoomd/GPUVector.h:    GPUVector() { }
hoomd/GPUVector.h:    //! Constructs an empty GPUVector
hoomd/GPUVector.h:    GPUVector(std::shared_ptr<const ExecutionConfiguration> exec_conf)
hoomd/GPUVector.h:        : GPUVectorBase<T, GPUArray<T>>(exec_conf)
hoomd/GPUVector.h:    //! Constructs a GPUVector
hoomd/GPUVector.h:    GPUVector(size_t size, std::shared_ptr<const ExecutionConfiguration> exec_conf)
hoomd/GPUVector.h:        : GPUVectorBase<T, GPUArray<T>>(size, exec_conf)
hoomd/GPUVector.h:    //! Constructs a GPUVector
hoomd/GPUVector.h:    GPUVector(unsigned int size,
hoomd/GPUVector.h:        : GPUVectorBase<T, GPUArray<T>>(size, value, exec_conf)
hoomd/GPUVector.h:template<class T> class GlobalVector : public GPUVectorBase<T, GlobalArray<T>>
hoomd/GPUVector.h:        : GPUVectorBase<T, GlobalArray<T>>(exec_conf)
hoomd/GPUVector.h:    //! Constructs a GPUVector
hoomd/GPUVector.h:        : GPUVectorBase<T, GlobalArray<T>>(size, exec_conf)
hoomd/GPUVector.h:        : GPUVectorBase<T, GlobalArray<T>>(size, value, exec_conf)
hoomd/md/NeighborList.h:#include "hoomd/GPUFlags.h"
hoomd/md/NeighborList.h:#include "hoomd/GPUVector.h"
hoomd/md/NeighborList.h:    The CUDA profiler expects the exact same sequence of kernels on every run. Due to the
hoomd/md/NeighborList.h:    For easy support of derived GPU classes to implement overflow detection the overflow condition
hoomd/md/NeighborList.h:    GPUPartition m_last_gpu_partition; //!< The partition at the time of the last memory hints
hoomd/md/IntegratorTwoStep.cc:    if (m_exec_conf->isCUDAEnabled())
hoomd/md/IntegratorTwoStep.cc:        computeNetForceGPU(timestep + 1);
hoomd/md/IntegratorTwoStep.cc:    if (m_exec_conf->isCUDAEnabled())
hoomd/md/IntegratorTwoStep.cc:        computeNetForceGPU(timestep);
hoomd/md/IntegratorTwoStep.cc:/// helper function to compute net force/virial on the GPU
hoomd/md/IntegratorTwoStep.cc:void IntegratorTwoStep::computeNetForceGPU(uint64_t timestep)
hoomd/md/IntegratorTwoStep.cc:    Integrator::computeNetForceGPU(timestep);
hoomd/md/PotentialPairDPDThermoGPUKernel.cu.inc:#include "hoomd/md/PotentialPairDPDThermoGPU.cuh"
hoomd/md/PotentialPairDPDThermoGPUKernel.cu.inc:gpu_compute_dpd_forces<EVALUATOR_CLASS>(const dpd_pair_args_t& args,
hoomd/md/CommunicatorGridGPU.cc:#include "CommunicatorGridGPU.h"
hoomd/md/CommunicatorGridGPU.cc:#include "CommunicatorGridGPU.cuh"
hoomd/md/CommunicatorGridGPU.cc:CommunicatorGridGPU<T>::CommunicatorGridGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/md/CommunicatorGridGPU.cc:    this->m_exec_conf->msg->notice(5) << "Constructing CommunicatorGridGPU" << std::endl;
hoomd/md/CommunicatorGridGPU.cc:    initGridCommGPU();
hoomd/md/CommunicatorGridGPU.cc:template<typename T> void CommunicatorGridGPU<T>::initGridCommGPU()
hoomd/md/CommunicatorGridGPU.cc:template<typename T> void CommunicatorGridGPU<T>::communicate(const GlobalArray<T>& grid)
hoomd/md/CommunicatorGridGPU.cc:        kernel::gpu_gridcomm_scatter_send_cells<T>((unsigned int)this->m_send_buf.getNumElements(),
hoomd/md/CommunicatorGridGPU.cc:        if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/CommunicatorGridGPU.cc:            CHECK_CUDA_ERROR();
hoomd/md/CommunicatorGridGPU.cc:        kernel::gpu_gridcomm_scatter_add_recv_cells<T>(m_n_unique_recv_cells,
hoomd/md/CommunicatorGridGPU.cc:        if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/CommunicatorGridGPU.cc:            CHECK_CUDA_ERROR();
hoomd/md/CommunicatorGridGPU.cc:template class PYBIND11_EXPORT CommunicatorGridGPU<Scalar>;
hoomd/md/CommunicatorGridGPU.cc:template class PYBIND11_EXPORT CommunicatorGridGPU<unsigned int>;
hoomd/md/CommunicatorGridGPU.cc:template class PYBIND11_EXPORT CommunicatorGridGPU<hipfftComplex>;
hoomd/md/PotentialPair.h:   values are stored in GlobalArray for easy access on the GPU by a derived class. The type of the
hoomd/md/PotentialPair.h:        hoomd::detail::managed_allocator<param_type>(m_exec_conf->isCUDAEnabled()));
hoomd/md/PotentialPair.h:    if (m_pdata->getExecConf()->isCUDAEnabled())
hoomd/md/PotentialPair.h:        cudaMemAdvise(m_params.data(),
hoomd/md/PotentialPair.h:                      cudaMemAdviseSetReadMostly,
hoomd/md/PotentialPair.h:        auto& gpu_map = m_exec_conf->getGPUIds();
hoomd/md/PotentialPair.h:        for (unsigned int idev = 0; idev < m_exec_conf->getNumActiveGPUs(); ++idev)
hoomd/md/PotentialPair.h:            cudaMemPrefetchAsync(m_params.data(),
hoomd/md/PotentialPair.h:                                 gpu_map[idev]);
hoomd/md/PotentialPair.h:            cudaMemAdvise(m_rcutsq.get(),
hoomd/md/PotentialPair.h:                          cudaMemAdviseSetReadMostly,
hoomd/md/PotentialPair.h:            cudaMemAdvise(m_ronsq.get(),
hoomd/md/PotentialPair.h:                          cudaMemAdviseSetReadMostly,
hoomd/md/PotentialPair.h:            for (unsigned int idev = 0; idev < m_exec_conf->getNumActiveGPUs(); ++idev)
hoomd/md/PotentialPair.h:                // prefetch data on all GPUs
hoomd/md/PotentialPair.h:                cudaMemPrefetchAsync(m_rcutsq.get(),
hoomd/md/PotentialPair.h:                                     gpu_map[idev]);
hoomd/md/PotentialPair.h:                cudaMemPrefetchAsync(m_ronsq.get(),
hoomd/md/PotentialPair.h:                                     gpu_map[idev]);
hoomd/md/PotentialPair.h:    setParams(typ1, typ2, param_type(params, m_exec_conf->isCUDAEnabled()));
hoomd/md/NeighborListGPU.cu:/*! \file NeighborListGPU.cu
hoomd/md/NeighborListGPU.cu:    \brief Defines GPU kernel code for neighbor list processing on the GPU
hoomd/md/NeighborListGPU.cu:#include "NeighborListGPU.cuh"
hoomd/md/NeighborListGPU.cu:    \param nwork Number of particles this GPU processes
hoomd/md/NeighborListGPU.cu:    gpu_nlist_needs_update_check_new_kernel() executes one thread per particle. Every particle's
hoomd/md/NeighborListGPU.cu:__global__ void gpu_nlist_needs_update_check_new_kernel(unsigned int* d_result,
hoomd/md/NeighborListGPU.cu:#if (__CUDA_ARCH__ >= 600)
hoomd/md/NeighborListGPU.cu:hipError_t gpu_nlist_needs_update_check_new(unsigned int* d_result,
hoomd/md/NeighborListGPU.cu:                                            const GPUPartition& gpu_partition)
hoomd/md/NeighborListGPU.cu:    // iterate over active GPUs in reverse order
hoomd/md/NeighborListGPU.cu:    for (int idev = gpu_partition.getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/md/NeighborListGPU.cu:        auto range = gpu_partition.getRangeAndSetGPU(idev);
hoomd/md/NeighborListGPU.cu:        hipLaunchKernelGGL((gpu_nlist_needs_update_check_new_kernel),
hoomd/md/NeighborListGPU.cu:    gpu_nlist_filter_kernel() processes the neighbor list \a d_nlist and removes any entries that
hoomd/md/NeighborListGPU.cu:    \note The driver gpu_nlist_filter properly makes as many calls as are necessary, it only needs
hoomd/md/NeighborListGPU.cu:__global__ void gpu_nlist_filter_kernel(unsigned int* d_n_neigh,
hoomd/md/NeighborListGPU.cu:hipError_t gpu_nlist_filter(unsigned int* d_n_neigh,
hoomd/md/NeighborListGPU.cu:    hipFuncGetAttributes(&attr, (const void*)gpu_nlist_filter_kernel);
hoomd/md/NeighborListGPU.cu:        hipLaunchKernelGGL((gpu_nlist_filter_kernel),
hoomd/md/NeighborListGPU.cu://! GPU kernel to update the exclusions list
hoomd/md/NeighborListGPU.cu:__global__ void gpu_update_exclusion_list_kernel(const unsigned int* tags,
hoomd/md/NeighborListGPU.cu://! GPU function to update the exclusion list on the device
hoomd/md/NeighborListGPU.cu:hipError_t gpu_update_exclusion_list(const unsigned int* d_tag,
hoomd/md/NeighborListGPU.cu:    hipLaunchKernelGGL((gpu_update_exclusion_list_kernel),
hoomd/md/NeighborListGPU.cu://! GPU kernel to do a preliminary sizing on particles
hoomd/md/NeighborListGPU.cu: * d_Nmax. A prefix sum is then performed in gpu_nlist_build_head_list() to accumulate starting
hoomd/md/NeighborListGPU.cu:__global__ void gpu_nlist_init_head_list_kernel(size_t* d_head_list,
hoomd/md/NeighborListGPU.cu: * neighbor list. Because gpu_nlist_init_head_list_kernel() already set the number of neighbors for
hoomd/md/NeighborListGPU.cu:__global__ void gpu_nlist_get_nlist_size_kernel(size_t* d_req_size_nlist,
hoomd/md/NeighborListGPU.cu: * \param block_size Number of threads per block for gpu_nlist_init_head_list_kernel()
hoomd/md/NeighborListGPU.cu:hipError_t gpu_nlist_build_head_list(size_t* d_head_list,
hoomd/md/NeighborListGPU.cu:    hipFuncGetAttributes(&attr, (const void*)gpu_nlist_init_head_list_kernel);
hoomd/md/NeighborListGPU.cu:    hipLaunchKernelGGL((gpu_nlist_init_head_list_kernel),
hoomd/md/NeighborListGPU.cu:    hipLaunchKernelGGL((gpu_nlist_get_nlist_size_kernel),
hoomd/md/TableDihedralForceCompute.h:#include "hoomd/GPUArray.h"
hoomd/md/TableDihedralForceCompute.h:   GPU, these will be stored in a Scalar4 where x is rmin, y is rmax, and z is dr.
hoomd/md/TableDihedralForceCompute.h:    GPUArray<Scalar2> m_tables;                    //!< Stored V and F tables
hoomd/md/ConstantForceComputeGPU.cu:#include "ConstantForceComputeGPU.cuh"
hoomd/md/ConstantForceComputeGPU.cu:/*! \file ConstantForceComputeGPU.cu
hoomd/md/ConstantForceComputeGPU.cu:    \brief Declares GPU kernel code for calculating constant forces forces on the GPU. Used by
hoomd/md/ConstantForceComputeGPU.cu:   ConstantForceComputeGPU.
hoomd/md/ConstantForceComputeGPU.cu://! Kernel for setting constant force vectors on the GPU
hoomd/md/ConstantForceComputeGPU.cu:__global__ void gpu_compute_constant_force_set_forces_kernel(const unsigned int group_size,
hoomd/md/ConstantForceComputeGPU.cu:hipError_t gpu_compute_constant_force_set_forces(const unsigned int group_size,
hoomd/md/ConstantForceComputeGPU.cu:    hipLaunchKernelGGL((gpu_compute_constant_force_set_forces_kernel),
hoomd/md/BendingRigidityMeshForceCompute.h:    GPUArray<Scalar> m_params;                   //!< Parameters
hoomd/md/VolumeConservationMeshForceComputeGPU.cu:#include "VolumeConservationMeshForceComputeGPU.cuh"
hoomd/md/VolumeConservationMeshForceComputeGPU.cu:/*! \file MeshVolumeConservationGPU.cu
hoomd/md/VolumeConservationMeshForceComputeGPU.cu:    \brief Defines GPU kernel code for calculating the volume_constraint forces. Used by
hoomd/md/VolumeConservationMeshForceComputeGPU.cu:   MeshVolumeConservationComputeGPU.
hoomd/md/VolumeConservationMeshForceComputeGPU.cu://! Kernel for calculating volume_constraint sigmas on the GPU
hoomd/md/VolumeConservationMeshForceComputeGPU.cu:    \param box Box dimensions (in GPU format) to use for periodic boundary conditions
hoomd/md/VolumeConservationMeshForceComputeGPU.cu:    \param tlist List of mesh triangle indices stored on the GPU
hoomd/md/VolumeConservationMeshForceComputeGPU.cu:    \param tpos_list Position of current index in list of mesh triangles stored on the GPU
hoomd/md/VolumeConservationMeshForceComputeGPU.cu:    \param n_triangles_list List of mesh triangles stored on the GPU
hoomd/md/VolumeConservationMeshForceComputeGPU.cu:__global__ void gpu_compute_volume_constraint_volume_kernel(Scalar* d_partial_sum_volume,
hoomd/md/VolumeConservationMeshForceComputeGPU.cu:__global__ void gpu_volume_reduce_partial_sum_kernel(Scalar* d_sum,
hoomd/md/VolumeConservationMeshForceComputeGPU.cu:    \param box Box dimensions (in GPU format) to use for periodic boundary conditions
hoomd/md/VolumeConservationMeshForceComputeGPU.cu:    \param tlist List of mesh triangle indices stored on the GPU
hoomd/md/VolumeConservationMeshForceComputeGPU.cu:    \param tpos_list Position of current index in list of mesh triangles stored on the GPU
hoomd/md/VolumeConservationMeshForceComputeGPU.cu:    \param n_triangles_list List of mesh triangles stored on the GPU
hoomd/md/VolumeConservationMeshForceComputeGPU.cu:hipError_t gpu_compute_volume_constraint_volume(Scalar* d_sum_volume,
hoomd/md/VolumeConservationMeshForceComputeGPU.cu:        hipLaunchKernelGGL((gpu_compute_volume_constraint_volume_kernel),
hoomd/md/VolumeConservationMeshForceComputeGPU.cu:    hipLaunchKernelGGL((gpu_volume_reduce_partial_sum_kernel),
hoomd/md/VolumeConservationMeshForceComputeGPU.cu://! Kernel for calculating volume_constraint sigmas on the GPU
hoomd/md/VolumeConservationMeshForceComputeGPU.cu:    \param box Box dimensions (in GPU format) to use for periodic boundary conditions
hoomd/md/VolumeConservationMeshForceComputeGPU.cu:    \param tlist List of mesh triangle indices stored on the GPU
hoomd/md/VolumeConservationMeshForceComputeGPU.cu:    \param tpos_list Position of current index in list of mesh triangles stored on the GPU
hoomd/md/VolumeConservationMeshForceComputeGPU.cu:__global__ void gpu_compute_volume_constraint_force_kernel(Scalar4* d_force,
hoomd/md/VolumeConservationMeshForceComputeGPU.cu:    \param box Box dimensions (in GPU format) to use for periodic boundary conditions
hoomd/md/VolumeConservationMeshForceComputeGPU.cu:    \param tlist List of mesh triangle indices stored on the GPU
hoomd/md/VolumeConservationMeshForceComputeGPU.cu:    \param tpos_list Position of current index in list of mesh triangles stored on the GPU
hoomd/md/VolumeConservationMeshForceComputeGPU.cu:hipError_t gpu_compute_volume_constraint_force(Scalar4* d_force,
hoomd/md/VolumeConservationMeshForceComputeGPU.cu:    hipFuncGetAttributes(&attr, (const void*)gpu_compute_volume_constraint_force_kernel);
hoomd/md/VolumeConservationMeshForceComputeGPU.cu:    hipLaunchKernelGGL((gpu_compute_volume_constraint_force_kernel),
hoomd/md/HarmonicImproperForceComputeGPU.cc:/*! \file HarmonicImproperForceComputeGPU.cc
hoomd/md/HarmonicImproperForceComputeGPU.cc:    \brief Defines HarmonicImproperForceComputeGPU
hoomd/md/HarmonicImproperForceComputeGPU.cc:#include "HarmonicImproperForceComputeGPU.h"
hoomd/md/HarmonicImproperForceComputeGPU.cc:HarmonicImproperForceComputeGPU::HarmonicImproperForceComputeGPU(
hoomd/md/HarmonicImproperForceComputeGPU.cc:    // can't run on the GPU if there aren't any GPUs in the execution configuration
hoomd/md/HarmonicImproperForceComputeGPU.cc:    if (!m_exec_conf->isCUDAEnabled())
hoomd/md/HarmonicImproperForceComputeGPU.cc:            << "Creating a ImproperForceComputeGPU with no GPU in the execution configuration"
hoomd/md/HarmonicImproperForceComputeGPU.cc:        throw std::runtime_error("Error initializing ImproperForceComputeGPU");
hoomd/md/HarmonicImproperForceComputeGPU.cc:    GPUArray<Scalar2> params(m_improper_data->getNTypes(), m_exec_conf);
hoomd/md/HarmonicImproperForceComputeGPU.cc:HarmonicImproperForceComputeGPU::~HarmonicImproperForceComputeGPU() { }
hoomd/md/HarmonicImproperForceComputeGPU.cc:    parameters on the GPU.
hoomd/md/HarmonicImproperForceComputeGPU.cc:void HarmonicImproperForceComputeGPU::setParams(unsigned int type, Scalar K, Scalar chi)
hoomd/md/HarmonicImproperForceComputeGPU.cc:/*! Internal method for computing the forces on the GPU.
hoomd/md/HarmonicImproperForceComputeGPU.cc:    \post The force data on the GPU is written with the calculated forces
hoomd/md/HarmonicImproperForceComputeGPU.cc:    Calls gpu_compute_harmonic_improper_forces to do the dirty work.
hoomd/md/HarmonicImproperForceComputeGPU.cc:void HarmonicImproperForceComputeGPU::computeForces(uint64_t timestep)
hoomd/md/HarmonicImproperForceComputeGPU.cc:    ArrayHandle<ImproperData::members_t> d_gpu_dihedral_list(m_improper_data->getGPUTable(),
hoomd/md/HarmonicImproperForceComputeGPU.cc:    ArrayHandle<unsigned int> d_dihedrals_ABCD(m_improper_data->getGPUPosTable(),
hoomd/md/HarmonicImproperForceComputeGPU.cc:    // run the kernel in parallel on all GPUs
hoomd/md/HarmonicImproperForceComputeGPU.cc:    kernel::gpu_compute_harmonic_improper_forces(d_force.data,
hoomd/md/HarmonicImproperForceComputeGPU.cc:                                                 d_gpu_dihedral_list.data,
hoomd/md/HarmonicImproperForceComputeGPU.cc:                                                 m_improper_data->getGPUTableIndexer().getW(),
hoomd/md/HarmonicImproperForceComputeGPU.cc:    if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/HarmonicImproperForceComputeGPU.cc:        CHECK_CUDA_ERROR();
hoomd/md/HarmonicImproperForceComputeGPU.cc:void export_HarmonicImproperForceComputeGPU(pybind11::module& m)
hoomd/md/HarmonicImproperForceComputeGPU.cc:    pybind11::class_<HarmonicImproperForceComputeGPU,
hoomd/md/HarmonicImproperForceComputeGPU.cc:                     std::shared_ptr<HarmonicImproperForceComputeGPU>>(
hoomd/md/HarmonicImproperForceComputeGPU.cc:        "HarmonicImproperForceComputeGPU")
hoomd/md/TableAngleForceCompute.h:#include "hoomd/GPUArray.h"
hoomd/md/TableAngleForceCompute.h:   maximum r, and spacing between r values in the table respectively. For simple access on the GPU,
hoomd/md/TableAngleForceCompute.h:    GPUArray<Scalar2> m_tables;              //!< Stored V and T tables
hoomd/md/NeighborListGPUBinned.cuh:#ifndef __NEIGHBORLOSTGPUBINNED_CUH__
hoomd/md/NeighborListGPUBinned.cuh:#define __NEIGHBORLOSTGPUBINNED_CUH__
hoomd/md/NeighborListGPUBinned.cuh:#include "hoomd/GPUPartition.cuh"
hoomd/md/NeighborListGPUBinned.cuh:/*! \file NeighborListGPUBinned.cuh
hoomd/md/NeighborListGPUBinned.cuh:    \brief Declares GPU kernel code for neighbor list generation on the GPU
hoomd/md/NeighborListGPUBinned.cuh://! Kernel driver for gpu_compute_nlist_kernel()
hoomd/md/NeighborListGPUBinned.cuh:hipError_t gpu_compute_nlist_binned(unsigned int* d_nlist,
hoomd/md/NeighborListGPUBinned.cuh:                                    const GPUPartition& gpu_partition,
hoomd/md/TwoStepLangevinGPU.cuh:/*! \file TwoStepLangevinGPU.cuh
hoomd/md/TwoStepLangevinGPU.cuh:    \brief Declares GPU kernel code for Langevin dynamics on the GPU. Used by TwoStepLangevinGPU.
hoomd/md/TwoStepLangevinGPU.cuh:#ifndef __TWO_STEP_LANGEVIN_GPU_CUH__
hoomd/md/TwoStepLangevinGPU.cuh:#define __TWO_STEP_LANGEVIN_GPU_CUH__
hoomd/md/TwoStepLangevinGPU.cuh://! Temporary holder struct to limit the number of arguments passed to gpu_langevin_step_two()
hoomd/md/TwoStepLangevinGPU.cuh://! Kernel driver for the second part of the Langevin update called by TwoStepLangevinGPU
hoomd/md/TwoStepLangevinGPU.cuh:hipError_t gpu_langevin_step_two(const Scalar4* d_pos,
hoomd/md/TwoStepLangevinGPU.cuh://! TwoStepLangevinGPU
hoomd/md/TwoStepLangevinGPU.cuh:hipError_t gpu_langevin_angular_step_two(const Scalar4* d_pos,
hoomd/md/TwoStepLangevinGPU.cuh:#endif //__TWO_STEP_LANGEVIN_GPU_CUH__
hoomd/md/BondTablePotentialGPU.cu:#include "BondTablePotentialGPU.cuh"
hoomd/md/BondTablePotentialGPU.cu:/*! \file BondTablePotentialGPU.cu
hoomd/md/BondTablePotentialGPU.cu:    \brief Defines GPU kernel code for calculating the table bond forces. Used by
hoomd/md/BondTablePotentialGPU.cu:   BondTablePotentialGPU.
hoomd/md/BondTablePotentialGPU.cu:    \param blist List of bonds stored on the GPU
hoomd/md/BondTablePotentialGPU.cu:    \param n_bonds_list List of numbers of bonds stored on the GPU
hoomd/md/BondTablePotentialGPU.cu:__global__ void gpu_compute_bondtable_forces_kernel(Scalar4* d_force,
hoomd/md/BondTablePotentialGPU.cu:    \param blist List of bonds stored on the GPU
hoomd/md/BondTablePotentialGPU.cu:    \param n_bonds_list List of numbers of bonds stored on the GPU
hoomd/md/BondTablePotentialGPU.cu:    \note This is just a kernel driver. See gpu_compute_bondtable_forces_kernel for full
hoomd/md/BondTablePotentialGPU.cu:hipError_t gpu_compute_bondtable_forces(Scalar4* d_force,
hoomd/md/BondTablePotentialGPU.cu:    hipFuncGetAttributes(&attr, (const void*)gpu_compute_bondtable_forces_kernel);
hoomd/md/BondTablePotentialGPU.cu:    hipLaunchKernelGGL((gpu_compute_bondtable_forces_kernel),
hoomd/md/BondTablePotentialGPU.cc:#include "BondTablePotentialGPU.h"
hoomd/md/BondTablePotentialGPU.cc:/*! \file BondTablePotentialGPU.cc
hoomd/md/BondTablePotentialGPU.cc:    \brief Defines the BondTablePotentialGPU class
hoomd/md/BondTablePotentialGPU.cc:BondTablePotentialGPU::BondTablePotentialGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/md/BondTablePotentialGPU.cc:    m_exec_conf->msg->notice(5) << "Constructing BondTablePotentialGPU" << endl;
hoomd/md/BondTablePotentialGPU.cc:    // can't run on the GPU if there aren't any GPUs in the execution configuration
hoomd/md/BondTablePotentialGPU.cc:    if (!m_exec_conf->isCUDAEnabled())
hoomd/md/BondTablePotentialGPU.cc:            << "Creating a BondTableForceComputeGPU with no GPU in the execution configuration"
hoomd/md/BondTablePotentialGPU.cc:        throw std::runtime_error("Error initializing BondTableForceComputeGPU");
hoomd/md/BondTablePotentialGPU.cc:    // allocate flags storage on the GPU
hoomd/md/BondTablePotentialGPU.cc:    GPUArray<unsigned int> flags(1, this->m_exec_conf);
hoomd/md/BondTablePotentialGPU.cc:BondTablePotentialGPU::~BondTablePotentialGPU()
hoomd/md/BondTablePotentialGPU.cc:    m_exec_conf->msg->notice(5) << "Destroying BondTablePotentialGPU" << endl;
hoomd/md/BondTablePotentialGPU.cc:Calls gpu_compute_bondtable_forces to do the leg work
hoomd/md/BondTablePotentialGPU.cc:void BondTablePotentialGPU::computeForces(uint64_t timestep)
hoomd/md/BondTablePotentialGPU.cc:        ArrayHandle<BondData::members_t> d_gpu_bondlist(this->m_bond_data->getGPUTable(),
hoomd/md/BondTablePotentialGPU.cc:        ArrayHandle<unsigned int> d_gpu_n_bonds(this->m_bond_data->getNGroupsArray(),
hoomd/md/BondTablePotentialGPU.cc:        // run the kernel on all GPUs in parallel
hoomd/md/BondTablePotentialGPU.cc:        kernel::gpu_compute_bondtable_forces(d_force.data,
hoomd/md/BondTablePotentialGPU.cc:                                             d_gpu_bondlist.data,
hoomd/md/BondTablePotentialGPU.cc:                                             m_bond_data->getGPUTableIndexer().getW(),
hoomd/md/BondTablePotentialGPU.cc:                                             d_gpu_n_bonds.data,
hoomd/md/BondTablePotentialGPU.cc:    if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/BondTablePotentialGPU.cc:        CHECK_CUDA_ERROR();
hoomd/md/BondTablePotentialGPU.cc:void export_BondTablePotentialGPU(pybind11::module& m)
hoomd/md/BondTablePotentialGPU.cc:    pybind11::class_<BondTablePotentialGPU,
hoomd/md/BondTablePotentialGPU.cc:                     std::shared_ptr<BondTablePotentialGPU>>(m, "BondTablePotentialGPU")
hoomd/md/EvaluatorSquareDensity.h:        //! Set CUDA memory hints
hoomd/md/PPPMForceComputeGPU.cu:#include "PPPMForceComputeGPU.cuh"
hoomd/md/PPPMForceComputeGPU.cu:#define GPU_PPPM_MAX_ORDER 7
hoomd/md/PPPMForceComputeGPU.cu://! GPU implementation of sinc(x)==sin(x)/x
hoomd/md/PPPMForceComputeGPU.cu:__device__ Scalar gpu_sinc(Scalar x)
hoomd/md/PPPMForceComputeGPU.cu:__global__ void gpu_assign_particles_kernel(const uint3 mesh_dim,
hoomd/md/PPPMForceComputeGPU.cu:__global__ void gpu_reduce_meshes(const unsigned int mesh_elements,
hoomd/md/PPPMForceComputeGPU.cu:                                  unsigned int ngpu)
hoomd/md/PPPMForceComputeGPU.cu:    for (unsigned int igpu = 0; igpu < ngpu; ++igpu)
hoomd/md/PPPMForceComputeGPU.cu:        hipfftComplex m = d_mesh_scratch[idx + igpu * mesh_elements];
hoomd/md/PPPMForceComputeGPU.cu:void gpu_assign_particles(const uint3 mesh_dim,
hoomd/md/PPPMForceComputeGPU.cu:                          const GPUPartition& gpu_partition)
hoomd/md/PPPMForceComputeGPU.cu:    hipFuncGetAttributes(&attr, (const void*)gpu_assign_particles_kernel);
hoomd/md/PPPMForceComputeGPU.cu:    // iterate over active GPUs in reverse, to end up on first GPU when returning from this function
hoomd/md/PPPMForceComputeGPU.cu:    unsigned int ngpu = gpu_partition.getNumActiveGPUs();
hoomd/md/PPPMForceComputeGPU.cu:    for (int idev = ngpu - 1; idev >= 0; --idev)
hoomd/md/PPPMForceComputeGPU.cu:        auto range = gpu_partition.getRangeAndSetGPU(idev);
hoomd/md/PPPMForceComputeGPU.cu:        if (ngpu > 1)
hoomd/md/PPPMForceComputeGPU.cu:        hipLaunchKernelGGL((gpu_assign_particles_kernel),
hoomd/md/PPPMForceComputeGPU.cu:                           ngpu > 1 ? d_mesh_scratch + idev * mesh_elements : d_mesh,
hoomd/md/PPPMForceComputeGPU.cu://! Reduce temporary arrays for every GPU
hoomd/md/PPPMForceComputeGPU.cu:void gpu_reduce_meshes(const unsigned int mesh_elements,
hoomd/md/PPPMForceComputeGPU.cu:                       const unsigned int ngpu,
hoomd/md/PPPMForceComputeGPU.cu:    // reduce meshes on GPU 0
hoomd/md/PPPMForceComputeGPU.cu:    hipLaunchKernelGGL((gpu_reduce_meshes),
hoomd/md/PPPMForceComputeGPU.cu:                       ngpu);
hoomd/md/PPPMForceComputeGPU.cu:__global__ void gpu_compute_mesh_virial_kernel(const unsigned int n_wave_vectors,
hoomd/md/PPPMForceComputeGPU.cu:void gpu_compute_mesh_virial(const unsigned int n_wave_vectors,
hoomd/md/PPPMForceComputeGPU.cu:    hipLaunchKernelGGL((gpu_compute_mesh_virial_kernel),
hoomd/md/PPPMForceComputeGPU.cu:__global__ void gpu_update_meshes_kernel(const unsigned int n_wave_vectors,
hoomd/md/PPPMForceComputeGPU.cu:void gpu_update_meshes(const unsigned int n_wave_vectors,
hoomd/md/PPPMForceComputeGPU.cu:    hipFuncGetAttributes(&attr, (const void*)gpu_update_meshes_kernel);
hoomd/md/PPPMForceComputeGPU.cu:    hipLaunchKernelGGL((gpu_update_meshes_kernel),
hoomd/md/PPPMForceComputeGPU.cu:__global__ void gpu_compute_forces_kernel(const unsigned int work_size,
hoomd/md/PPPMForceComputeGPU.cu:void gpu_compute_forces(const unsigned int N,
hoomd/md/PPPMForceComputeGPU.cu:                        const GPUPartition& gpu_partition,
hoomd/md/PPPMForceComputeGPU.cu:                        const GPUPartition& all_gpu_partition,
hoomd/md/PPPMForceComputeGPU.cu:    hipFuncGetAttributes(&attr, (const void*)gpu_compute_forces_kernel);
hoomd/md/PPPMForceComputeGPU.cu:    // iterate over active GPUs in reverse, to end up on first GPU when returning from this function
hoomd/md/PPPMForceComputeGPU.cu:    for (int idev = all_gpu_partition.getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/md/PPPMForceComputeGPU.cu:        auto range = all_gpu_partition.getRangeAndSetGPU(idev);
hoomd/md/PPPMForceComputeGPU.cu:    // iterate over active GPUs in reverse, to end up on first GPU when returning from this function
hoomd/md/PPPMForceComputeGPU.cu:    for (int idev = gpu_partition.getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/md/PPPMForceComputeGPU.cu:        auto range = gpu_partition.getRangeAndSetGPU(idev);
hoomd/md/PPPMForceComputeGPU.cu:            (gpu_compute_forces_kernel),
hoomd/md/PPPMForceComputeGPU.cu:void gpu_compute_pe(unsigned int n_wave_vectors,
hoomd/md/PPPMForceComputeGPU.cu:void gpu_compute_virial(unsigned int n_wave_vectors,
hoomd/md/PPPMForceComputeGPU.cu:__global__ void gpu_compute_influence_function_kernel(const uint3 mesh_dim,
hoomd/md/PPPMForceComputeGPU.cu:            Scalar wxs = gpu_sinc(argx);
hoomd/md/PPPMForceComputeGPU.cu:                Scalar wys = gpu_sinc(argy);
hoomd/md/PPPMForceComputeGPU.cu:                    Scalar wzs = gpu_sinc(argz);
hoomd/md/PPPMForceComputeGPU.cu:void gpu_compute_influence_function(const uint3 mesh_dim,
hoomd/md/PPPMForceComputeGPU.cu:        hipFuncGetAttributes(&attr, (const void*)gpu_compute_influence_function_kernel<true>);
hoomd/md/PPPMForceComputeGPU.cu:        hipLaunchKernelGGL((gpu_compute_influence_function_kernel<true>),
hoomd/md/PPPMForceComputeGPU.cu:        hipFuncGetAttributes(&attr, (const void*)gpu_compute_influence_function_kernel<false>);
hoomd/md/PPPMForceComputeGPU.cu:        hipLaunchKernelGGL((gpu_compute_influence_function_kernel<false>),
hoomd/md/PPPMForceComputeGPU.cu:__global__ void gpu_fix_exclusions_kernel(Scalar4* d_force,
hoomd/md/PPPMForceComputeGPU.cu:hipError_t gpu_fix_exclusions(Scalar4* d_force,
hoomd/md/PPPMForceComputeGPU.cu:    hipLaunchKernelGGL((gpu_fix_exclusions_kernel),
hoomd/md/AnisoPotentialPairGBGPUKernel.cu:#include "AnisoPotentialPairGPU.cuh"
hoomd/md/AnisoPotentialPairGBGPUKernel.cu:gpu_compute_pair_aniso_forces<EvaluatorPairGB>(const a_pair_args_t& pair_args,
hoomd/md/AnisoPotentialPairALJ3GPU.cc:#include "AnisoPotentialPairGPU.h"
hoomd/md/AnisoPotentialPairALJ3GPU.cc:void export_AnisoPotentialPairALJ3DGPU(pybind11::module& m)
hoomd/md/AnisoPotentialPairALJ3GPU.cc:    export_AnisoPotentialPairGPU<EvaluatorPairALJ<3>>(m, "AnisoPotentialPairALJ3DGPU");
hoomd/md/EvaluatorPairFourier.h:        //! set CUDA memory hint
hoomd/md/TableDihedralForceComputeGPU.h:#include "TableDihedralForceGPU.cuh"
hoomd/md/TableDihedralForceComputeGPU.h:/*! \file TableDihedralForceComputeGPU.h
hoomd/md/TableDihedralForceComputeGPU.h:    \brief Declares the TableDihedralForceComputeGPU class
hoomd/md/TableDihedralForceComputeGPU.h:#ifndef __TABLEDIHEDRALFORCECOMPUTEGPU_H__
hoomd/md/TableDihedralForceComputeGPU.h:#define __TABLEDIHEDRALFORCECOMPUTEGPU_H__
hoomd/md/TableDihedralForceComputeGPU.h://! Compute table based bond potentials on the GPU
hoomd/md/TableDihedralForceComputeGPU.h:/*! Calculates exactly the same thing as TableDihedralForceCompute, but on the GPU
hoomd/md/TableDihedralForceComputeGPU.h:    The GPU kernel for calculating this can be found in TableDihedralForceComputeGPU.cu/
hoomd/md/TableDihedralForceComputeGPU.h:class PYBIND11_EXPORT TableDihedralForceComputeGPU : public TableDihedralForceCompute
hoomd/md/TableDihedralForceComputeGPU.h:    TableDihedralForceComputeGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/md/TableDihedralForceComputeGPU.h:    virtual ~TableDihedralForceComputeGPU() { }
hoomd/md/TableDihedralForceComputeGPU.h:    GPUArray<unsigned int> m_flags;        //!< Flags set during the kernel execution
hoomd/md/ZeroMomentumUpdater.cc:        } // end GPUArray scope
hoomd/md/TwoStepConstantVolume.h:    /// Pack the limit values for use in the GPU kernel.
hoomd/md/EvaluatorRevCross.h:        //! Set CUDA memory hints
hoomd/md/ConstantForceComputeGPU.cc:#include "ConstantForceComputeGPU.h"
hoomd/md/ConstantForceComputeGPU.cc:#include "ConstantForceComputeGPU.cuh"
hoomd/md/ConstantForceComputeGPU.cc:/*! \file ConstantForceComputeGPU.cc
hoomd/md/ConstantForceComputeGPU.cc:    \brief Contains code for the ConstantForceComputeGPU class
hoomd/md/ConstantForceComputeGPU.cc:ConstantForceComputeGPU::ConstantForceComputeGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/md/ConstantForceComputeGPU.cc:    if (!m_exec_conf->isCUDAEnabled())
hoomd/md/ConstantForceComputeGPU.cc:            << "Creating a ConstantForceComputeGPU with no GPU in the execution configuration"
hoomd/md/ConstantForceComputeGPU.cc:        throw std::runtime_error("Error initializing ConstantForceComputeGPU");
hoomd/md/ConstantForceComputeGPU.cc:void ConstantForceComputeGPU::setForces()
hoomd/md/ConstantForceComputeGPU.cc:    // compute the forces on the GPU
hoomd/md/ConstantForceComputeGPU.cc:    kernel::gpu_compute_constant_force_set_forces(group_size,
hoomd/md/ConstantForceComputeGPU.cc:    if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/ConstantForceComputeGPU.cc:        CHECK_CUDA_ERROR();
hoomd/md/ConstantForceComputeGPU.cc:void export_ConstantForceComputeGPU(pybind11::module& m)
hoomd/md/ConstantForceComputeGPU.cc:    pybind11::class_<ConstantForceComputeGPU,
hoomd/md/ConstantForceComputeGPU.cc:                     std::shared_ptr<ConstantForceComputeGPU>>(m, "ConstantForceComputeGPU")
hoomd/md/export_ActiveForceConstraintComputeGPU.cc.inc:#include "hoomd/md/ActiveForceConstraintComputeGPU.h"
hoomd/md/export_ActiveForceConstraintComputeGPU.cc.inc:#define EXPORT_FUNCTION export_ActiveForceConstraintCompute@_manifold@GPU
hoomd/md/export_ActiveForceConstraintComputeGPU.cc.inc:    export_ActiveForceConstraintComputeGPU<MANIFOLD_CLASS>(
hoomd/md/export_ActiveForceConstraintComputeGPU.cc.inc:        "ActiveForceConstraintCompute@_manifold@GPU");
hoomd/md/PotentialPairGPU.h:#ifndef __POTENTIAL_PAIR_GPU_H__
hoomd/md/PotentialPairGPU.h:#define __POTENTIAL_PAIR_GPU_H__
hoomd/md/PotentialPairGPU.h:#include "PotentialPairGPU.cuh"
hoomd/md/PotentialPairGPU.h:/*! \file PotentialPairGPU.h
hoomd/md/PotentialPairGPU.h:    \brief Defines the template class for standard pair potentials on the GPU
hoomd/md/PotentialPairGPU.h://! Template class for computing pair potentials on the GPU
hoomd/md/PotentialPairGPU.h:    Due to technical limitations, the instantiation of PotentialPairGPU cannot create a CUDA kernel
hoomd/md/PotentialPairGPU.h:   function to call gpu_compute_pair_forces() instantiated with the same evaluator. (See
hoomd/md/PotentialPairGPU.h:   PotentialPairLJGPU.cu and PotentialPairLJGPU.cuh for an example).
hoomd/md/PotentialPairGPU.h:    \sa export_PotentialPairGPU()
hoomd/md/PotentialPairGPU.h:template<class evaluator> class PotentialPairGPU : public PotentialPair<evaluator>
hoomd/md/PotentialPairGPU.h:    PotentialPairGPU(std::shared_ptr<SystemDefinition> sysdef, std::shared_ptr<NeighborList> nlist);
hoomd/md/PotentialPairGPU.h:    virtual ~PotentialPairGPU() { }
hoomd/md/PotentialPairGPU.h:PotentialPairGPU<evaluator>::PotentialPairGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/md/PotentialPairGPU.h:    // can't run on the GPU if there aren't any GPUs in the execution configuration
hoomd/md/PotentialPairGPU.h:    if (!this->m_exec_conf->isCUDAEnabled())
hoomd/md/PotentialPairGPU.h:            << "Creating a PotentialPairGPU with no GPU in the execution configuration"
hoomd/md/PotentialPairGPU.h:        throw std::runtime_error("Error initializing PotentialPairGPU");
hoomd/md/PotentialPairGPU.h:template<class evaluator> void PotentialPairGPU<evaluator>::computeForces(uint64_t timestep)
hoomd/md/PotentialPairGPU.h:    // The GPU implementation CANNOT handle a half neighborlist, error out now
hoomd/md/PotentialPairGPU.h:            << "PotentialPairGPU cannot handle a half neighborlist" << std::endl;
hoomd/md/PotentialPairGPU.h:        throw std::runtime_error("Error computing forces in PotentialPairGPU");
hoomd/md/PotentialPairGPU.h:    this->m_exec_conf->beginMultiGPU();
hoomd/md/PotentialPairGPU.h:    kernel::gpu_compute_pair_forces<evaluator>(
hoomd/md/PotentialPairGPU.h:                            this->m_pdata->getGPUPartition(),
hoomd/md/PotentialPairGPU.h:    if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/PotentialPairGPU.h:        CHECK_CUDA_ERROR();
hoomd/md/PotentialPairGPU.h:    this->m_exec_conf->endMultiGPU();
hoomd/md/PotentialPairGPU.h:template<class T> void export_PotentialPairGPU(pybind11::module& m, const std::string& name)
hoomd/md/PotentialPairGPU.h:    pybind11::class_<PotentialPairGPU<T>, PotentialPair<T>, std::shared_ptr<PotentialPairGPU<T>>>(
hoomd/md/PotentialPairGPU.h:#endif // __POTENTIAL_PAIR_GPU_H__
hoomd/md/PotentialTersoffGPUKernel.cu.inc:#include "hoomd/md/PotentialTersoffGPU.cuh"
hoomd/md/PotentialTersoffGPUKernel.cu.inc:gpu_compute_triplet_forces<EVALUATOR_CLASS>(const tersoff_args_t& pair_args,
hoomd/md/TwoStepRATTLENVEGPU.h:#ifndef __TWO_STEP_RATTLE_NVE_GPU_H__
hoomd/md/TwoStepRATTLENVEGPU.h:#define __TWO_STEP_RATTLE_NVE_GPU_H__
hoomd/md/TwoStepRATTLENVEGPU.h:/*! \file TwoStepRATTLENVEGPU.h
hoomd/md/TwoStepRATTLENVEGPU.h:\brief Declares the TwoStepRATTLENVEGPU class
hoomd/md/TwoStepRATTLENVEGPU.h:#include "hoomd/md/TwoStepRATTLENVEGPU.cuh"
hoomd/md/TwoStepRATTLENVEGPU.h://! Integrates part of the system forward in two steps in the NVE ensemble on the GPU
hoomd/md/TwoStepRATTLENVEGPU.h:on the GPU \ingroup updaters
hoomd/md/TwoStepRATTLENVEGPU.h:class PYBIND11_EXPORT TwoStepRATTLENVEGPU : public TwoStepRATTLENVE<Manifold>
hoomd/md/TwoStepRATTLENVEGPU.h:    TwoStepRATTLENVEGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/md/TwoStepRATTLENVEGPU.h:    virtual ~TwoStepRATTLENVEGPU() { };
hoomd/md/TwoStepRATTLENVEGPU.h:/*! \file TwoStepRATTLENVEGPU.h
hoomd/md/TwoStepRATTLENVEGPU.h:    \brief Contains code for the TwoStepRATTLENVEGPU class
hoomd/md/TwoStepRATTLENVEGPU.h:TwoStepRATTLENVEGPU<Manifold>::TwoStepRATTLENVEGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/md/TwoStepRATTLENVEGPU.h:    if (!this->m_exec_conf->isCUDAEnabled())
hoomd/md/TwoStepRATTLENVEGPU.h:            << "Creating a TwoStepRATTLENVEGPU when CUDA is disabled" << std::endl;
hoomd/md/TwoStepRATTLENVEGPU.h:        throw std::runtime_error("Error initializing TwoStepRATTLENVEGPU");
hoomd/md/TwoStepRATTLENVEGPU.h:template<class Manifold> void TwoStepRATTLENVEGPU<Manifold>::integrateStepOne(uint64_t timestep)
hoomd/md/TwoStepRATTLENVEGPU.h:    // perform the update on the GPU
hoomd/md/TwoStepRATTLENVEGPU.h:    this->m_exec_conf->beginMultiGPU();
hoomd/md/TwoStepRATTLENVEGPU.h:    kernel::gpu_rattle_nve_step_one(d_pos.data,
hoomd/md/TwoStepRATTLENVEGPU.h:                                    this->m_group->getGPUPartition(),
hoomd/md/TwoStepRATTLENVEGPU.h:    if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/TwoStepRATTLENVEGPU.h:        CHECK_CUDA_ERROR();
hoomd/md/TwoStepRATTLENVEGPU.h:    this->m_exec_conf->endMultiGPU();
hoomd/md/TwoStepRATTLENVEGPU.h:        this->m_exec_conf->beginMultiGPU();
hoomd/md/TwoStepRATTLENVEGPU.h:        kernel::gpu_rattle_nve_angular_step_one(d_orientation.data,
hoomd/md/TwoStepRATTLENVEGPU.h:                                                this->m_group->getGPUPartition(),
hoomd/md/TwoStepRATTLENVEGPU.h:        if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/TwoStepRATTLENVEGPU.h:            CHECK_CUDA_ERROR();
hoomd/md/TwoStepRATTLENVEGPU.h:        this->m_exec_conf->endMultiGPU();
hoomd/md/TwoStepRATTLENVEGPU.h:    \post particle velocities are moved forward to timestep+1 on the GPU
hoomd/md/TwoStepRATTLENVEGPU.h:template<class Manifold> void TwoStepRATTLENVEGPU<Manifold>::integrateStepTwo(uint64_t timestep)
hoomd/md/TwoStepRATTLENVEGPU.h:    // perform the update on the GPU
hoomd/md/TwoStepRATTLENVEGPU.h:    this->m_exec_conf->beginMultiGPU();
hoomd/md/TwoStepRATTLENVEGPU.h:    kernel::gpu_rattle_nve_step_two<Manifold>(d_pos.data,
hoomd/md/TwoStepRATTLENVEGPU.h:                                              this->m_group->getGPUPartition(),
hoomd/md/TwoStepRATTLENVEGPU.h:    if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/TwoStepRATTLENVEGPU.h:        CHECK_CUDA_ERROR();
hoomd/md/TwoStepRATTLENVEGPU.h:    this->m_exec_conf->endMultiGPU();
hoomd/md/TwoStepRATTLENVEGPU.h:        this->m_exec_conf->beginMultiGPU();
hoomd/md/TwoStepRATTLENVEGPU.h:        kernel::gpu_rattle_nve_angular_step_two(d_orientation.data,
hoomd/md/TwoStepRATTLENVEGPU.h:                                                this->m_group->getGPUPartition(),
hoomd/md/TwoStepRATTLENVEGPU.h:        if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/TwoStepRATTLENVEGPU.h:            CHECK_CUDA_ERROR();
hoomd/md/TwoStepRATTLENVEGPU.h:        this->m_exec_conf->endMultiGPU();
hoomd/md/TwoStepRATTLENVEGPU.h:template<class Manifold> void TwoStepRATTLENVEGPU<Manifold>::includeRATTLEForce(uint64_t timestep)
hoomd/md/TwoStepRATTLENVEGPU.h:    // perform the update on the GPU
hoomd/md/TwoStepRATTLENVEGPU.h:    this->m_exec_conf->beginMultiGPU();
hoomd/md/TwoStepRATTLENVEGPU.h:    kernel::gpu_include_rattle_force_nve<Manifold>(d_pos.data,
hoomd/md/TwoStepRATTLENVEGPU.h:                                                   this->m_group->getGPUPartition(),
hoomd/md/TwoStepRATTLENVEGPU.h:    if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/TwoStepRATTLENVEGPU.h:        CHECK_CUDA_ERROR();
hoomd/md/TwoStepRATTLENVEGPU.h:    this->m_exec_conf->endMultiGPU();
hoomd/md/TwoStepRATTLENVEGPU.h:void export_TwoStepRATTLENVEGPU(pybind11::module& m, const std::string& name)
hoomd/md/TwoStepRATTLENVEGPU.h:    pybind11::class_<TwoStepRATTLENVEGPU<Manifold>,
hoomd/md/TwoStepRATTLENVEGPU.h:                     std::shared_ptr<TwoStepRATTLENVEGPU<Manifold>>>(m, name.c_str())
hoomd/md/TwoStepRATTLENVEGPU.h:#endif // #ifndef __TWO_STEP_RATTLE_NVE_GPU_H__
hoomd/md/PeriodicImproperForceComputeGPU.h:#include "PeriodicImproperForceGPU.cuh"
hoomd/md/PeriodicImproperForceComputeGPU.h://! Implements the periodic improper force calculation on the GPU
hoomd/md/PeriodicImproperForceComputeGPU.h:/*! PeriodicImproperForceComputeGPU implements the same calculations as
hoomd/md/PeriodicImproperForceComputeGPU.h:   PeriodicImproperForceCompute, but executing on the GPU.
hoomd/md/PeriodicImproperForceComputeGPU.h:class PYBIND11_EXPORT PeriodicImproperForceComputeGPU : public PeriodicImproperForceCompute
hoomd/md/PeriodicImproperForceComputeGPU.h:    PeriodicImproperForceComputeGPU(std::shared_ptr<SystemDefinition> system);
hoomd/md/PeriodicImproperForceComputeGPU.h:    ~PeriodicImproperForceComputeGPU();
hoomd/md/ComputeThermoGPU.cu:#include "ComputeThermoGPU.cuh"
hoomd/md/ComputeThermoGPU.cu:/*! \file ComputeThermoGPU.cu
hoomd/md/ComputeThermoGPU.cu:    \brief Defines GPU kernel code for computing thermodynamic properties on the GPU. Used by
hoomd/md/ComputeThermoGPU.cu:   ComputeThermoGPU.
hoomd/md/ComputeThermoGPU.cu://! Perform partial sums of the thermo properties on the GPU
hoomd/md/ComputeThermoGPU.cu:    \param work_size Number of particles in the group this GPU processes
hoomd/md/ComputeThermoGPU.cu:    \param offset Offset of this GPU in list of group members
hoomd/md/ComputeThermoGPU.cu:    \param block_offset Offset of this GPU in the array of partial sums
hoomd/md/ComputeThermoGPU.cu:__global__ void gpu_compute_thermo_partial_sums(Scalar4* d_scratch,
hoomd/md/ComputeThermoGPU.cu://! Perform partial sums of the pressure tensor on the GPU
hoomd/md/ComputeThermoGPU.cu:    \param offset Offset of this GPU in the list of group members
hoomd/md/ComputeThermoGPU.cu:    \param block_offset Offset of this GPU in the array of partial sums
hoomd/md/ComputeThermoGPU.cu:    \param num_blocks Total number of partial sums by all GPUs
hoomd/md/ComputeThermoGPU.cu:__global__ void gpu_compute_pressure_tensor_partial_sums(Scalar* d_scratch,
hoomd/md/ComputeThermoGPU.cu://! Perform partial sums of the rotational KE on the GPU
hoomd/md/ComputeThermoGPU.cu:    \param work_size Number of particles in the group processed by this GPU
hoomd/md/ComputeThermoGPU.cu:    \param offset Offset of this GPU in the list of group members
hoomd/md/ComputeThermoGPU.cu:    \param block_offset Output offset of this GPU
hoomd/md/ComputeThermoGPU.cu:__global__ void gpu_compute_rotational_ke_partial_sums(Scalar* d_scratch,
hoomd/md/ComputeThermoGPU.cu:__global__ void gpu_compute_thermo_final_sums(Scalar* d_properties,
hoomd/md/ComputeThermoGPU.cu:        // fill out the GPUArray
hoomd/md/ComputeThermoGPU.cu:__global__ void gpu_compute_pressure_tensor_final_sums(Scalar* d_properties,
hoomd/md/ComputeThermoGPU.cu:        // fill out the GPUArray
hoomd/md/ComputeThermoGPU.cu://! Compute partial sums of thermodynamic properties of a group on the GPU,
hoomd/md/ComputeThermoGPU.cu:    \param d_vel particle velocities and masses on the GPU
hoomd/md/ComputeThermoGPU.cu:    \param gpu_partition Load balancing info for multi-GPU reduction
hoomd/md/ComputeThermoGPU.cu:    This function drives gpu_compute_thermo_partial_sums and gpu_compute_thermo_final_sums, see them
hoomd/md/ComputeThermoGPU.cu:hipError_t gpu_compute_thermo_partial(Scalar* d_properties,
hoomd/md/ComputeThermoGPU.cu:                                      const GPUPartition& gpu_partition)
hoomd/md/ComputeThermoGPU.cu:    // iterate over active GPUs in reverse, to end up on first GPU when returning from this function
hoomd/md/ComputeThermoGPU.cu:    for (int idev = gpu_partition.getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/md/ComputeThermoGPU.cu:        auto range = gpu_partition.getRangeAndSetGPU(idev);
hoomd/md/ComputeThermoGPU.cu:        hipLaunchKernelGGL(gpu_compute_thermo_partial_sums,
hoomd/md/ComputeThermoGPU.cu:            hipLaunchKernelGGL(gpu_compute_pressure_tensor_partial_sums,
hoomd/md/ComputeThermoGPU.cu:            hipLaunchKernelGGL(gpu_compute_rotational_ke_partial_sums,
hoomd/md/ComputeThermoGPU.cu://! Compute thermodynamic properties of a group on the GPU
hoomd/md/ComputeThermoGPU.cu:    \param d_vel particle velocities and masses on the GPU
hoomd/md/ComputeThermoGPU.cu:    This function drives gpu_compute_thermo_partial_sums and gpu_compute_thermo_final_sums, see them
hoomd/md/ComputeThermoGPU.cu:hipError_t gpu_compute_thermo_final(Scalar* d_properties,
hoomd/md/ComputeThermoGPU.cu:    hipLaunchKernelGGL(gpu_compute_thermo_final_sums,
hoomd/md/ComputeThermoGPU.cu:        hipLaunchKernelGGL(gpu_compute_pressure_tensor_final_sums,
hoomd/md/AnisoPotentialPairALJ3GPUKernel.cu:#include "AnisoPotentialPairGPU.cuh"
hoomd/md/AnisoPotentialPairALJ3GPUKernel.cu:gpu_compute_pair_aniso_forces<EvaluatorPairALJ<3>>(
hoomd/md/NeighborListGPUTree.h:#include "NeighborListGPU.h"
hoomd/md/NeighborListGPUTree.h:#include "NeighborListGPUTree.cuh"
hoomd/md/NeighborListGPUTree.h:/*! \file NeighborListGPUTree.h
hoomd/md/NeighborListGPUTree.h:    \brief Declares the NeighborListGPUTree class
hoomd/md/NeighborListGPUTree.h:#ifndef __NEIGHBORLISTGPUTREE_H__
hoomd/md/NeighborListGPUTree.h:#define __NEIGHBORLISTGPUTREE_H__
hoomd/md/NeighborListGPUTree.h://! Efficient neighbor list build on the GPU using BVH trees
hoomd/md/NeighborListGPUTree.h: * GPU methods mostly make use of the neighbor library to do the traversal.
hoomd/md/NeighborListGPUTree.h: * done using one CUDA stream per type to try to improve concurrency.
hoomd/md/NeighborListGPUTree.h:class PYBIND11_EXPORT NeighborListGPUTree : public NeighborListGPU
hoomd/md/NeighborListGPUTree.h:    NeighborListGPUTree(std::shared_ptr<SystemDefinition> sysdef, Scalar r_buff);
hoomd/md/NeighborListGPUTree.h:    virtual ~NeighborListGPUTree();
hoomd/md/NeighborListGPUTree.h:    GPUArray<unsigned int> m_types;          //!< Particle types (for sorting)
hoomd/md/NeighborListGPUTree.h:    GPUArray<unsigned int> m_sorted_types;   //!< Sorted particle types
hoomd/md/NeighborListGPUTree.h:    GPUArray<unsigned int> m_indexes;        //!< Particle indexes (for sorting)
hoomd/md/NeighborListGPUTree.h:    GPUArray<unsigned int> m_sorted_indexes; //!< Sorted particle indexes
hoomd/md/NeighborListGPUTree.h:    GPUArray<unsigned int> m_type_first; //!< First index of each particle type in sorted list
hoomd/md/NeighborListGPUTree.h:    GPUArray<unsigned int> m_type_last;  //!< Last index of each particle type in sorted list
hoomd/md/NeighborListGPUTree.h:    GPUFlags<unsigned int> m_lbvh_errors; //!< Error flags during particle marking (e.g., off rank)
hoomd/md/NeighborListGPUTree.h:    std::vector<hipStream_t> m_streams; //!< Array of CUDA streams per-type
hoomd/md/NeighborListGPUTree.h:    GPUArray<unsigned int> m_traverse_order; //!< Order to traverse primitives
hoomd/md/NeighborListGPUTree.h:#endif //__NEIGHBORLISTGPUTREE_H__
hoomd/md/export_PotentialExternalGPU.cc.inc:#include "hoomd/md/PotentialExternalGPU.h"
hoomd/md/export_PotentialExternalGPU.cc.inc:#define EXPORT_FUNCTION export_PotentialExternal@_evaluator@GPU
hoomd/md/export_PotentialExternalGPU.cc.inc:    export_PotentialExternalGPU<EVALUATOR_CLASS>(m, "PotentialExternal@_evaluator@GPU");
hoomd/md/ComputeThermoGPU.cuh:#ifndef _COMPUTE_THERMO_GPU_CUH_
hoomd/md/ComputeThermoGPU.cuh:#define _COMPUTE_THERMO_GPU_CUH_
hoomd/md/ComputeThermoGPU.cuh:#include "hoomd/GPUPartition.cuh"
hoomd/md/ComputeThermoGPU.cuh:/*! \file ComputeThermoGPU.cuh
hoomd/md/ComputeThermoGPU.cuh:    \brief Kernel driver function declarations for ComputeThermoGPU
hoomd/md/ComputeThermoGPU.cuh://! Holder for arguments to gpu_compute_thermo
hoomd/md/ComputeThermoGPU.cuh:    unsigned int block_size;           //!< Block size to execute on the GPU
hoomd/md/ComputeThermoGPU.cuh:hipError_t gpu_compute_thermo_partial(Scalar* d_properties,
hoomd/md/ComputeThermoGPU.cuh:                                      const GPUPartition& gpu_partition);
hoomd/md/ComputeThermoGPU.cuh:hipError_t gpu_compute_thermo_final(Scalar* d_properties,
hoomd/md/ComputeThermo.cc:    if (m_exec_conf->isCUDAEnabled() && m_exec_conf->allConcurrentManagedAccess())
hoomd/md/ComputeThermo.cc:        cudaMemAdvise(m_properties.get(),
hoomd/md/ComputeThermo.cc:                      cudaMemAdviseSetPreferredLocation,
hoomd/md/ComputeThermo.cc:                      cudaCpuDeviceId);
hoomd/md/ComputeThermo.cc:        CHECK_CUDA_ERROR();
hoomd/md/FIREEnergyMinimizerGPU.cu:#include "FIREEnergyMinimizerGPU.cuh"
hoomd/md/FIREEnergyMinimizerGPU.cu:/*! \file FIREEnergyMinimizerGPU.cu
hoomd/md/FIREEnergyMinimizerGPU.cu:    \brief Defines GPU kernel code for one performing one FIRE energy
hoomd/md/FIREEnergyMinimizerGPU.cu:    minimization iteration on the GPU. Used by FIREEnergyMinimizerGPU.
hoomd/md/FIREEnergyMinimizerGPU.cu://! The kernel function to zeros velocities, called by gpu_fire_zero_v()
hoomd/md/FIREEnergyMinimizerGPU.cu:gpu_fire_zero_v_kernel(Scalar4* d_vel, unsigned int* d_group_members, unsigned int group_size)
hoomd/md/FIREEnergyMinimizerGPU.cu://! The kernel function to zero angular momenta, called by gpu_fire_zero_angmom()
hoomd/md/FIREEnergyMinimizerGPU.cu:__global__ void gpu_fire_zero_angmom_kernel(Scalar4* d_angmom,
hoomd/md/FIREEnergyMinimizerGPU.cu:This function is just the driver for gpu_fire_zero_v_kernel(), see that function
hoomd/md/FIREEnergyMinimizerGPU.cu:hipError_t gpu_fire_zero_v(Scalar4* d_vel, unsigned int* d_group_members, unsigned int group_size)
hoomd/md/FIREEnergyMinimizerGPU.cu:    hipLaunchKernelGGL((gpu_fire_zero_v_kernel),
hoomd/md/FIREEnergyMinimizerGPU.cu:gpu_fire_zero_angmom(Scalar4* d_angmom, unsigned int* d_group_members, unsigned int group_size)
hoomd/md/FIREEnergyMinimizerGPU.cu:    hipLaunchKernelGGL((gpu_fire_zero_angmom_kernel),
hoomd/md/FIREEnergyMinimizerGPU.cu:__global__ void gpu_fire_reduce_pe_partial_kernel(unsigned int* d_group_members,
hoomd/md/FIREEnergyMinimizerGPU.cu:gpu_fire_reduce_partial_sum_kernel(Scalar* d_sum, Scalar* d_partial_sum, unsigned int num_blocks)
hoomd/md/FIREEnergyMinimizerGPU.cu:    This is a driver for gpu_fire_reduce_pe_partial_kernel() and
hoomd/md/FIREEnergyMinimizerGPU.cu:    gpu_fire_reduce_partial_sum_kernel(), see them for details
hoomd/md/FIREEnergyMinimizerGPU.cu:hipError_t gpu_fire_compute_sum_pe(unsigned int* d_group_members,
hoomd/md/FIREEnergyMinimizerGPU.cu:    hipLaunchKernelGGL((gpu_fire_reduce_pe_partial_kernel),
hoomd/md/FIREEnergyMinimizerGPU.cu:    hipLaunchKernelGGL((gpu_fire_reduce_partial_sum_kernel),
hoomd/md/FIREEnergyMinimizerGPU.cu:__global__ void gpu_fire_reduce_P_partial_kernel(const Scalar4* d_vel,
hoomd/md/FIREEnergyMinimizerGPU.cu:__global__ void gpu_fire_reduce_Pr_partial_kernel(const Scalar4* d_angmom,
hoomd/md/FIREEnergyMinimizerGPU.cu:__global__ void gpu_fire_reduce_wnorm_partial_kernel(const Scalar4* d_angmom,
hoomd/md/FIREEnergyMinimizerGPU.cu:__global__ void gpu_fire_reduce_vsq_partial_kernel(const Scalar4* d_vel,
hoomd/md/FIREEnergyMinimizerGPU.cu:__global__ void gpu_fire_reduce_asq_partial_kernel(const Scalar3* d_accel,
hoomd/md/FIREEnergyMinimizerGPU.cu:__global__ void gpu_fire_reduce_tsq_partial_kernel(const Scalar4* d_net_torque,
hoomd/md/FIREEnergyMinimizerGPU.cu:    This is a driver for gpu_fire_reduce_{X}_partial_kernel() (where X = P, vsq, asq)
hoomd/md/FIREEnergyMinimizerGPU.cu:    and gpu_fire_reduce_partial_sum_kernel(), see them for details
hoomd/md/FIREEnergyMinimizerGPU.cu:hipError_t gpu_fire_compute_sum_all(const unsigned int N,
hoomd/md/FIREEnergyMinimizerGPU.cu:    hipLaunchKernelGGL((gpu_fire_reduce_P_partial_kernel),
hoomd/md/FIREEnergyMinimizerGPU.cu:    hipLaunchKernelGGL((gpu_fire_reduce_partial_sum_kernel),
hoomd/md/FIREEnergyMinimizerGPU.cu:    hipLaunchKernelGGL((gpu_fire_reduce_vsq_partial_kernel),
hoomd/md/FIREEnergyMinimizerGPU.cu:    hipLaunchKernelGGL((gpu_fire_reduce_partial_sum_kernel),
hoomd/md/FIREEnergyMinimizerGPU.cu:    hipLaunchKernelGGL((gpu_fire_reduce_asq_partial_kernel),
hoomd/md/FIREEnergyMinimizerGPU.cu:    hipLaunchKernelGGL((gpu_fire_reduce_partial_sum_kernel),
hoomd/md/FIREEnergyMinimizerGPU.cu:hipError_t gpu_fire_compute_sum_all_angular(const unsigned int N,
hoomd/md/FIREEnergyMinimizerGPU.cu:    hipLaunchKernelGGL((gpu_fire_reduce_Pr_partial_kernel),
hoomd/md/FIREEnergyMinimizerGPU.cu:    hipLaunchKernelGGL((gpu_fire_reduce_partial_sum_kernel),
hoomd/md/FIREEnergyMinimizerGPU.cu:    hipLaunchKernelGGL((gpu_fire_reduce_wnorm_partial_kernel),
hoomd/md/FIREEnergyMinimizerGPU.cu:    hipLaunchKernelGGL((gpu_fire_reduce_partial_sum_kernel),
hoomd/md/FIREEnergyMinimizerGPU.cu:    hipLaunchKernelGGL((gpu_fire_reduce_tsq_partial_kernel),
hoomd/md/FIREEnergyMinimizerGPU.cu:    hipLaunchKernelGGL((gpu_fire_reduce_partial_sum_kernel),
hoomd/md/FIREEnergyMinimizerGPU.cu:__global__ void gpu_fire_update_v_kernel(Scalar4* d_vel,
hoomd/md/FIREEnergyMinimizerGPU.cu:    This function is a driver for gpu_fire_update_v_kernel(), see it for details.
hoomd/md/FIREEnergyMinimizerGPU.cu:hipError_t gpu_fire_update_v(Scalar4* d_vel,
hoomd/md/FIREEnergyMinimizerGPU.cu:    hipLaunchKernelGGL((gpu_fire_update_v_kernel),
hoomd/md/FIREEnergyMinimizerGPU.cu:__global__ void gpu_fire_update_angmom_kernel(const Scalar4* d_net_torque,
hoomd/md/FIREEnergyMinimizerGPU.cu:hipError_t gpu_fire_update_angmom(const Scalar4* d_net_torque,
hoomd/md/FIREEnergyMinimizerGPU.cu:    hipLaunchKernelGGL((gpu_fire_update_angmom_kernel),
hoomd/md/AreaConservationMeshForceCompute.cc:    GPUArray<area_conservation_param_t> params(n_types, m_exec_conf);
hoomd/md/AreaConservationMeshForceCompute.cc:    GPUArray<Scalar> area(n_types, m_exec_conf);
hoomd/md/PeriodicImproperForceGPU.cuh://! Kernel driver that computes periodic improper forces for PeriodicImproperForceComputeGPU
hoomd/md/PeriodicImproperForceGPU.cuh:hipError_t gpu_compute_periodic_improper_forces(Scalar4* d_force,
hoomd/md/PotentialPairGPU.cuh:#include "hoomd/GPUPartition.cuh"
hoomd/md/PotentialPairGPU.cuh:/*! \file PotentialPairGPU.cuh
hoomd/md/PotentialPairGPU.cuh:    \brief Defines templated GPU kernel code for calculating the pair forces.
hoomd/md/PotentialPairGPU.cuh:#ifndef __POTENTIAL_PAIR_GPU_CUH__
hoomd/md/PotentialPairGPU.cuh:#define __POTENTIAL_PAIR_GPU_CUH__
hoomd/md/PotentialPairGPU.cuh:const int gpu_pair_force_max_tpp = 32;
hoomd/md/PotentialPairGPU.cuh:const int gpu_pair_force_max_tpp = 64;
hoomd/md/PotentialPairGPU.cuh://! Wraps arguments to gpu_cgpf
hoomd/md/PotentialPairGPU.cuh:                const GPUPartition& _gpu_partition,
hoomd/md/PotentialPairGPU.cuh:          threads_per_particle(_threads_per_particle), gpu_partition(_gpu_partition),
hoomd/md/PotentialPairGPU.cuh:    const BoxDim box;          //!< Simulation box in GPU format
hoomd/md/PotentialPairGPU.cuh:    const GPUPartition& gpu_partition; //!< The load balancing partition of particles between GPUs
hoomd/md/PotentialPairGPU.cuh:    const hipDeviceProp_t& devprop;    //!< CUDA device properties
hoomd/md/PotentialPairGPU.cuh:gpu_compute_pair_forces_shared_kernel(Scalar4* d_force,
hoomd/md/PotentialPairGPU.cuh:    max_threads -= max_threads % gpu_pair_force_max_tpp;
hoomd/md/PotentialPairGPU.cuh:     * \param range Range of particle indices this GPU operates on
hoomd/md/PotentialPairGPU.cuh:                = get_max_block_size(gpu_compute_pair_forces_shared_kernel<evaluator,
hoomd/md/PotentialPairGPU.cuh:                reinterpret_cast<const void*>(&gpu_compute_pair_forces_shared_kernel<evaluator,
hoomd/md/PotentialPairGPU.cuh:                hipLaunchKernelGGL((gpu_compute_pair_forces_shared_kernel<evaluator,
hoomd/md/PotentialPairGPU.cuh:                hipLaunchKernelGGL((gpu_compute_pair_forces_shared_kernel<evaluator,
hoomd/md/PotentialPairGPU.cuh://! Kernel driver that computes lj forces on the GPU for LJForceComputeGPU
hoomd/md/PotentialPairGPU.cuh:    This is just a driver function for gpu_compute_pair_forces_shared_kernel(), see it for details.
hoomd/md/PotentialPairGPU.cuh:gpu_compute_pair_forces(const pair_args_t& pair_args,
hoomd/md/PotentialPairGPU.cuh:    // iterate over active GPUs in reverse, to end up on first GPU when returning from this function
hoomd/md/PotentialPairGPU.cuh:    for (int idev = pair_args.gpu_partition.getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/md/PotentialPairGPU.cuh:        auto range = pair_args.gpu_partition.getRangeAndSetGPU(idev);
hoomd/md/PotentialPairGPU.cuh:                PairForceComputeKernel<evaluator, 0, 1, gpu_pair_force_max_tpp>::launch(pair_args,
hoomd/md/PotentialPairGPU.cuh:                PairForceComputeKernel<evaluator, 1, 1, gpu_pair_force_max_tpp>::launch(pair_args,
hoomd/md/PotentialPairGPU.cuh:                PairForceComputeKernel<evaluator, 2, 1, gpu_pair_force_max_tpp>::launch(pair_args,
hoomd/md/PotentialPairGPU.cuh:                PairForceComputeKernel<evaluator, 0, 0, gpu_pair_force_max_tpp>::launch(pair_args,
hoomd/md/PotentialPairGPU.cuh:                PairForceComputeKernel<evaluator, 1, 0, gpu_pair_force_max_tpp>::launch(pair_args,
hoomd/md/PotentialPairGPU.cuh:                PairForceComputeKernel<evaluator, 2, 0, gpu_pair_force_max_tpp>::launch(pair_args,
hoomd/md/PotentialPairGPU.cuh:gpu_compute_pair_forces(const pair_args_t& pair_args,
hoomd/md/PotentialPairGPU.cuh:#endif // __POTENTIAL_PAIR_GPU_CUH__
hoomd/md/PotentialExternalGPUKernel.cu.inc:#include "hoomd/md/PotentialExternalGPU.cuh"
hoomd/md/PotentialExternalGPUKernel.cu.inc:gpu_compute_potential_external_forces<EVALUATOR_CLASS>(
hoomd/md/tune/nlist_buffer.py:        When using with a `hoomd.device.GPU` device, kernel launch parameter
hoomd/md/HarmonicAngleForceComputeGPU.cc:/*! \file HarmonicAngleForceComputeGPU.cc
hoomd/md/HarmonicAngleForceComputeGPU.cc:    \brief Defines HarmonicAngleForceComputeGPU
hoomd/md/HarmonicAngleForceComputeGPU.cc:#include "HarmonicAngleForceComputeGPU.h"
hoomd/md/HarmonicAngleForceComputeGPU.cc:HarmonicAngleForceComputeGPU::HarmonicAngleForceComputeGPU(std::shared_ptr<SystemDefinition> sysdef)
hoomd/md/HarmonicAngleForceComputeGPU.cc:    // can't run on the GPU if there aren't any GPUs in the execution configuration
hoomd/md/HarmonicAngleForceComputeGPU.cc:    if (!m_exec_conf->isCUDAEnabled())
hoomd/md/HarmonicAngleForceComputeGPU.cc:            << "Creating a AngleForceComputeGPU with no GPU in the execution configuration" << endl;
hoomd/md/HarmonicAngleForceComputeGPU.cc:        throw std::runtime_error("Error initializing AngleForceComputeGPU");
hoomd/md/HarmonicAngleForceComputeGPU.cc:    GPUArray<Scalar2> params(m_angle_data->getNTypes(), m_exec_conf);
hoomd/md/HarmonicAngleForceComputeGPU.cc:HarmonicAngleForceComputeGPU::~HarmonicAngleForceComputeGPU() { }
hoomd/md/HarmonicAngleForceComputeGPU.cc:    parameters on the GPU.
hoomd/md/HarmonicAngleForceComputeGPU.cc:void HarmonicAngleForceComputeGPU::setParams(unsigned int type, Scalar K, Scalar t_0)
hoomd/md/HarmonicAngleForceComputeGPU.cc:/*! Internal method for computing the forces on the GPU.
hoomd/md/HarmonicAngleForceComputeGPU.cc:    \post The force data on the GPU is written with the calculated forces
hoomd/md/HarmonicAngleForceComputeGPU.cc:    Calls gpu_compute_harmonic_angle_forces to do the dirty work.
hoomd/md/HarmonicAngleForceComputeGPU.cc:void HarmonicAngleForceComputeGPU::computeForces(uint64_t timestep)
hoomd/md/HarmonicAngleForceComputeGPU.cc:    ArrayHandle<AngleData::members_t> d_gpu_anglelist(m_angle_data->getGPUTable(),
hoomd/md/HarmonicAngleForceComputeGPU.cc:    ArrayHandle<unsigned int> d_gpu_angle_pos_list(m_angle_data->getGPUPosTable(),
hoomd/md/HarmonicAngleForceComputeGPU.cc:    ArrayHandle<unsigned int> d_gpu_n_angles(m_angle_data->getNGroupsArray(),
hoomd/md/HarmonicAngleForceComputeGPU.cc:    // run the kernel on the GPU
hoomd/md/HarmonicAngleForceComputeGPU.cc:    kernel::gpu_compute_harmonic_angle_forces(d_force.data,
hoomd/md/HarmonicAngleForceComputeGPU.cc:                                              d_gpu_anglelist.data,
hoomd/md/HarmonicAngleForceComputeGPU.cc:                                              d_gpu_angle_pos_list.data,
hoomd/md/HarmonicAngleForceComputeGPU.cc:                                              m_angle_data->getGPUTableIndexer().getW(),
hoomd/md/HarmonicAngleForceComputeGPU.cc:                                              d_gpu_n_angles.data,
hoomd/md/HarmonicAngleForceComputeGPU.cc:    if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/HarmonicAngleForceComputeGPU.cc:        CHECK_CUDA_ERROR();
hoomd/md/HarmonicAngleForceComputeGPU.cc:void export_HarmonicAngleForceComputeGPU(pybind11::module& m)
hoomd/md/HarmonicAngleForceComputeGPU.cc:    pybind11::class_<HarmonicAngleForceComputeGPU,
hoomd/md/HarmonicAngleForceComputeGPU.cc:                     std::shared_ptr<HarmonicAngleForceComputeGPU>>(m,
hoomd/md/HarmonicAngleForceComputeGPU.cc:                                                                    "HarmonicAngleForceComputeGPU")
hoomd/md/NeighborListGPU.cc:/*! \file NeighborListGPU.cc
hoomd/md/NeighborListGPU.cc:    \brief Implementation of the NeighborListGPU class
hoomd/md/NeighborListGPU.cc:#include "NeighborListGPU.h"
hoomd/md/NeighborListGPU.cc:#include "NeighborListGPU.cuh"
hoomd/md/NeighborListGPU.cc:void NeighborListGPU::buildNlist(uint64_t timestep)
hoomd/md/NeighborListGPU.cc:bool NeighborListGPU::distanceCheck(uint64_t timestep)
hoomd/md/NeighborListGPU.cc:        m_exec_conf->beginMultiGPU();
hoomd/md/NeighborListGPU.cc:        kernel::gpu_nlist_needs_update_check_new(d_flags.data,
hoomd/md/NeighborListGPU.cc:                                                 m_pdata->getGPUPartition());
hoomd/md/NeighborListGPU.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/NeighborListGPU.cc:            CHECK_CUDA_ERROR();
hoomd/md/NeighborListGPU.cc:        m_exec_conf->endMultiGPU();
hoomd/md/NeighborListGPU.cc:/*! Calls gpu_nlist_filter() to filter the neighbor list on the GPU
hoomd/md/NeighborListGPU.cc:void NeighborListGPU::filterNlist()
hoomd/md/NeighborListGPU.cc:    kernel::gpu_nlist_filter(d_n_neigh.data,
hoomd/md/NeighborListGPU.cc:    if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/NeighborListGPU.cc:        CHECK_CUDA_ERROR();
hoomd/md/NeighborListGPU.cc://! Update the exclusion list on the GPU
hoomd/md/NeighborListGPU.cc:void NeighborListGPU::updateExListIdx()
hoomd/md/NeighborListGPU.cc:    kernel::gpu_update_exclusion_list(d_tag.data,
hoomd/md/NeighborListGPU.cc:    if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/NeighborListGPU.cc:        CHECK_CUDA_ERROR();
hoomd/md/NeighborListGPU.cc://! Build the head list for neighbor list indexing on the GPU
hoomd/md/NeighborListGPU.cc:void NeighborListGPU::buildHeadList()
hoomd/md/NeighborListGPU.cc:        kernel::gpu_nlist_build_head_list(d_head_list.data,
hoomd/md/NeighborListGPU.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/NeighborListGPU.cc:            CHECK_CUDA_ERROR();
hoomd/md/NeighborListGPU.cc:void export_NeighborListGPU(pybind11::module& m)
hoomd/md/NeighborListGPU.cc:    pybind11::class_<NeighborListGPU, NeighborList, std::shared_ptr<NeighborListGPU>>(
hoomd/md/NeighborListGPU.cc:        "NeighborListGPU")
hoomd/md/BendingRigidityMeshForceComputeGPU.h:#include "BendingRigidityMeshForceComputeGPU.cuh"
hoomd/md/BendingRigidityMeshForceComputeGPU.h:/*! \file BendingRigidityMeshForceComputeGPU.h
hoomd/md/BendingRigidityMeshForceComputeGPU.h:    \brief Declares a class for computing bending rigidity energy forces on the GPU
hoomd/md/BendingRigidityMeshForceComputeGPU.h:#ifndef __BENDINGRIGIDITYMESHFORCECOMPUTE_GPU_H__
hoomd/md/BendingRigidityMeshForceComputeGPU.h:#define __BENDINGRIGIDITYMESHFORCECOMPUTE_GPU_H__
hoomd/md/BendingRigidityMeshForceComputeGPU.h://! Computes bending rigidity forces on the mesh on the GPU
hoomd/md/BendingRigidityMeshForceComputeGPU.h:class PYBIND11_EXPORT BendingRigidityMeshForceComputeGPU : public BendingRigidityMeshForceCompute
hoomd/md/BendingRigidityMeshForceComputeGPU.h:    BendingRigidityMeshForceComputeGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/md/BendingRigidityMeshForceComputeGPU.h:    ~BendingRigidityMeshForceComputeGPU();
hoomd/md/BendingRigidityMeshForceComputeGPU.h://! Exports the BendingRigidityMeshForceComputeGPU class to python
hoomd/md/BendingRigidityMeshForceComputeGPU.h:void export_BendingRigidityMeshForceComputeGPU(pybind11::module& m);
hoomd/md/EvaluatorPairReactionField.h:        // set CUDA memory hints
hoomd/md/NeighborListGPUBinned.cu:#include "NeighborListGPUBinned.cuh"
hoomd/md/NeighborListGPUBinned.cu:/*! \file NeighborListGPUBinned.cu
hoomd/md/NeighborListGPUBinned.cu:    \brief Defines GPU kernel code for O(N) neighbor list generation on the GPU
hoomd/md/NeighborListGPUBinned.cu://! Kernel call for generating neighbor list on the GPU (Kepler optimized version)
hoomd/md/NeighborListGPUBinned.cu:__global__ void gpu_compute_nlist_binned_kernel(unsigned int* d_nlist,
hoomd/md/NeighborListGPUBinned.cu:                                                const unsigned int ngpu)
hoomd/md/NeighborListGPUBinned.cu:    unsigned int igpu = 0;
hoomd/md/NeighborListGPUBinned.cu:                if (++igpu < ngpu)
hoomd/md/NeighborListGPUBinned.cu:                neigh_size = __ldg(d_cell_size + neigh_cell + igpu * ci.getNumElements());
hoomd/md/NeighborListGPUBinned.cu:                j = __ldg(d_cell_idx + cli(cur_offset, neigh_cell) + igpu * cli.getNumElements());
hoomd/md/NeighborListGPUBinned.cu:                     const unsigned int ngpu,
hoomd/md/NeighborListGPUBinned.cu:                    = get_max_block_size(gpu_compute_nlist_binned_kernel<0, 0, 0, cur_tpp>);
hoomd/md/NeighborListGPUBinned.cu:                hipLaunchKernelGGL((gpu_compute_nlist_binned_kernel<0, 0, 0, cur_tpp>),
hoomd/md/NeighborListGPUBinned.cu:                                   ngpu);
hoomd/md/NeighborListGPUBinned.cu:                    = get_max_block_size(gpu_compute_nlist_binned_kernel<1, 0, 0, cur_tpp>);
hoomd/md/NeighborListGPUBinned.cu:                hipLaunchKernelGGL((gpu_compute_nlist_binned_kernel<1, 0, 0, cur_tpp>),
hoomd/md/NeighborListGPUBinned.cu:                                   ngpu);
hoomd/md/NeighborListGPUBinned.cu:                    = get_max_block_size(gpu_compute_nlist_binned_kernel<0, 1, 0, cur_tpp>);
hoomd/md/NeighborListGPUBinned.cu:                hipLaunchKernelGGL((gpu_compute_nlist_binned_kernel<0, 1, 0, cur_tpp>),
hoomd/md/NeighborListGPUBinned.cu:                                   ngpu);
hoomd/md/NeighborListGPUBinned.cu:                    = get_max_block_size(gpu_compute_nlist_binned_kernel<1, 1, 0, cur_tpp>);
hoomd/md/NeighborListGPUBinned.cu:                hipLaunchKernelGGL((gpu_compute_nlist_binned_kernel<1, 1, 0, cur_tpp>),
hoomd/md/NeighborListGPUBinned.cu:                                   ngpu);
hoomd/md/NeighborListGPUBinned.cu:                    = get_max_block_size(gpu_compute_nlist_binned_kernel<0, 0, 1, cur_tpp>);
hoomd/md/NeighborListGPUBinned.cu:                hipLaunchKernelGGL((gpu_compute_nlist_binned_kernel<0, 0, 1, cur_tpp>),
hoomd/md/NeighborListGPUBinned.cu:                                   ngpu);
hoomd/md/NeighborListGPUBinned.cu:                    = get_max_block_size(gpu_compute_nlist_binned_kernel<1, 0, 1, cur_tpp>);
hoomd/md/NeighborListGPUBinned.cu:                hipLaunchKernelGGL((gpu_compute_nlist_binned_kernel<1, 0, 1, cur_tpp>),
hoomd/md/NeighborListGPUBinned.cu:                                   ngpu);
hoomd/md/NeighborListGPUBinned.cu:                    = get_max_block_size(gpu_compute_nlist_binned_kernel<0, 1, 1, cur_tpp>);
hoomd/md/NeighborListGPUBinned.cu:                hipLaunchKernelGGL((gpu_compute_nlist_binned_kernel<0, 1, 1, cur_tpp>),
hoomd/md/NeighborListGPUBinned.cu:                                   ngpu);
hoomd/md/NeighborListGPUBinned.cu:                    = get_max_block_size(gpu_compute_nlist_binned_kernel<1, 1, 1, cur_tpp>);
hoomd/md/NeighborListGPUBinned.cu:                hipLaunchKernelGGL((gpu_compute_nlist_binned_kernel<1, 1, 1, cur_tpp>),
hoomd/md/NeighborListGPUBinned.cu:                                   ngpu);
hoomd/md/NeighborListGPUBinned.cu:                              ngpu,
hoomd/md/NeighborListGPUBinned.cu:                                                   const unsigned int ngpu,
hoomd/md/NeighborListGPUBinned.cu:hipError_t gpu_compute_nlist_binned(unsigned int* d_nlist,
hoomd/md/NeighborListGPUBinned.cu:                                    const GPUPartition& gpu_partition,
hoomd/md/NeighborListGPUBinned.cu:    unsigned int ngpu = gpu_partition.getNumActiveGPUs();
hoomd/md/NeighborListGPUBinned.cu:    // iterate over active GPUs in reverse, to end up on first GPU when returning from this function
hoomd/md/NeighborListGPUBinned.cu:    for (int idev = gpu_partition.getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/md/NeighborListGPUBinned.cu:        auto range = gpu_partition.getRangeAndSetGPU(idev);
hoomd/md/NeighborListGPUBinned.cu:                                           ngpu,
hoomd/md/AreaConservationMeshForceCompute.h:    GPUArray<area_conservation_param_t> m_params; //!< Parameters
hoomd/md/AreaConservationMeshForceCompute.h:    GPUArray<Scalar> m_area;                      //!< memory space for area
hoomd/md/ActiveForceComputeGPU.cuh:/*! \file ActiveForceComputeGPU.cuh
hoomd/md/ActiveForceComputeGPU.cuh:    \brief Declares GPU kernel code for calculating active forces forces on the GPU. Used by
hoomd/md/ActiveForceComputeGPU.cuh:   ActiveForceComputeGPU.
hoomd/md/ActiveForceComputeGPU.cuh:#ifndef __ACTIVE_FORCE_COMPUTE_GPU_CUH__
hoomd/md/ActiveForceComputeGPU.cuh:#define __ACTIVE_FORCE_COMPUTE_GPU_CUH__
hoomd/md/ActiveForceComputeGPU.cuh:hipError_t gpu_compute_active_force_set_forces(const unsigned int group_size,
hoomd/md/ActiveForceComputeGPU.cuh:hipError_t gpu_compute_active_force_rotational_diffusion(const unsigned int group_size,
hoomd/md/nlist.py:    `gpu_local_nlist_arrays`.
hoomd/md/nlist.py:    def gpu_local_nlist_arrays(self):
hoomd/md/nlist.py:        """hoomd.md.data.NeighborListLocalAccessGPU: Expose nlist arrays on \
hoomd/md/nlist.py:        the GPU.
hoomd/md/nlist.py:        Provides direct access to the neighbor list arrays on the gpu. All data
hoomd/md/nlist.py:        The `hoomd.md.data.NeighborListLocalAccessGPU` object exposes the
hoomd/md/nlist.py:            with nlist.gpu_local_nlist_arrays as data:
hoomd/md/nlist.py:                with sim.state.gpu_local_snapshot as snap_data:
hoomd/md/nlist.py:            GPU local nlist data is not available if the chosen device for the
hoomd/md/nlist.py:        if not isinstance(self._simulation.device, hoomd.device.GPU):
hoomd/md/nlist.py:                "Cannot access gpu_local_nlist_arrays without a GPU device")
hoomd/md/nlist.py:            raise hoomd.error.DataAccessError("gpu_local_nlist_arrays")
hoomd/md/nlist.py:                "Cannot enter gpu_local_nlist_arrays context manager inside "
hoomd/md/nlist.py:        return hoomd.md.data.NeighborListLocalAccessGPU(self,
hoomd/md/nlist.py:        `Cell` may consume a significant amount of memory, especially on GPU
hoomd/md/nlist.py:            nlist_cls = _md.NeighborListGPUBinned
hoomd/md/nlist.py:            nlist_cls = _md.NeighborListGPUStencil
hoomd/md/nlist.py:            nlist_cls = _md.NeighborListGPUTree
hoomd/md/NeighborListGPUStencil.cuh:#ifndef __NEIGHBORLOSTGPUSTENCIL_CUH__
hoomd/md/NeighborListGPUStencil.cuh:#define __NEIGHBORLOSTGPUSTENCIL_CUH__
hoomd/md/NeighborListGPUStencil.cuh:/*! \file NeighborListGPUStencil.cuh
hoomd/md/NeighborListGPUStencil.cuh:    \brief Declares GPU kernel code for neighbor list generation on the GPU
hoomd/md/NeighborListGPUStencil.cuh://! Kernel driver for gpu_compute_nlist_multi_binned_kernel()
hoomd/md/NeighborListGPUStencil.cuh:hipError_t gpu_compute_nlist_stencil(unsigned int* d_nlist,
hoomd/md/NeighborListGPUStencil.cuh:hipError_t gpu_compute_nlist_stencil_fill_types(unsigned int* d_pids,
hoomd/md/NeighborListGPUStencil.cuh:void gpu_compute_nlist_stencil_sort_types(unsigned int* d_pids,
hoomd/md/NeighborListGPUStencil.cuh:#endif // __NEIGHBORLOSTGPUSTENCIL_CUH__
hoomd/md/OPLSDihedralForceGPU.cuh:/*! \file OPLSDihedralForceGPU.cuh
hoomd/md/OPLSDihedralForceGPU.cuh:    \brief Declares GPU kernel code for calculating the OPLS dihedral forces. Used by
hoomd/md/OPLSDihedralForceGPU.cuh:   OPLSDihedralForceComputeGPU.
hoomd/md/OPLSDihedralForceGPU.cuh:#ifndef __OPLSDIHEDRALFORCEGPU_CUH__
hoomd/md/OPLSDihedralForceGPU.cuh:#define __OPLSDIHEDRALFORCEGPU_CUH__
hoomd/md/OPLSDihedralForceGPU.cuh://! Kernel driver that computes OPLS dihedral forces for OPLSDihedralForceComputeGPU
hoomd/md/OPLSDihedralForceGPU.cuh:hipError_t gpu_compute_opls_dihedral_forces(Scalar4* d_force,
hoomd/md/TwoStepLangevinGPU.cc:#include "TwoStepLangevinGPU.h"
hoomd/md/TwoStepLangevinGPU.cc:#include "TwoStepLangevinGPU.cuh"
hoomd/md/TwoStepLangevinGPU.cc:#include "TwoStepNVEGPU.cuh"
hoomd/md/TwoStepLangevinGPU.cc:TwoStepLangevinGPU::TwoStepLangevinGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/md/TwoStepLangevinGPU.cc:    if (!m_exec_conf->isCUDAEnabled())
hoomd/md/TwoStepLangevinGPU.cc:        throw std::runtime_error("Cannot create TwoStepLangevinGPU on a CPU device.");
hoomd/md/TwoStepLangevinGPU.cc:    GPUArray<Scalar> sum(1, m_exec_conf);
hoomd/md/TwoStepLangevinGPU.cc:    GPUArray<Scalar> partial_sum1(m_num_blocks, m_exec_conf);
hoomd/md/TwoStepLangevinGPU.cc:    This method is copied directly from TwoStepNVEGPU::integrateStepOne() and reimplemented here to
hoomd/md/TwoStepLangevinGPU.cc:void TwoStepLangevinGPU::integrateStepOne(uint64_t timestep)
hoomd/md/TwoStepLangevinGPU.cc:    m_exec_conf->beginMultiGPU();
hoomd/md/TwoStepLangevinGPU.cc:    // perform the update on the GPU
hoomd/md/TwoStepLangevinGPU.cc:    kernel::gpu_nve_step_one(d_pos.data,
hoomd/md/TwoStepLangevinGPU.cc:                             m_group->getGPUPartition(),
hoomd/md/TwoStepLangevinGPU.cc:    if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/TwoStepLangevinGPU.cc:        CHECK_CUDA_ERROR();
hoomd/md/TwoStepLangevinGPU.cc:    m_exec_conf->endMultiGPU();
hoomd/md/TwoStepLangevinGPU.cc:        m_exec_conf->beginMultiGPU();
hoomd/md/TwoStepLangevinGPU.cc:        kernel::gpu_nve_angular_step_one(d_orientation.data,
hoomd/md/TwoStepLangevinGPU.cc:                                         m_group->getGPUPartition(),
hoomd/md/TwoStepLangevinGPU.cc:        m_exec_conf->endMultiGPU();
hoomd/md/TwoStepLangevinGPU.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/TwoStepLangevinGPU.cc:            CHECK_CUDA_ERROR();
hoomd/md/TwoStepLangevinGPU.cc:    \post particle velocities are moved forward to timestep+1 on the GPU
hoomd/md/TwoStepLangevinGPU.cc:void TwoStepLangevinGPU::integrateStepTwo(uint64_t timestep)
hoomd/md/TwoStepLangevinGPU.cc:        // perform the update on the GPU
hoomd/md/TwoStepLangevinGPU.cc:        kernel::gpu_langevin_step_two(d_pos.data,
hoomd/md/TwoStepLangevinGPU.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/TwoStepLangevinGPU.cc:            CHECK_CUDA_ERROR();
hoomd/md/TwoStepLangevinGPU.cc:            gpu_langevin_angular_step_two(d_pos.data,
hoomd/md/TwoStepLangevinGPU.cc:            if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/TwoStepLangevinGPU.cc:                CHECK_CUDA_ERROR();
hoomd/md/TwoStepLangevinGPU.cc:void export_TwoStepLangevinGPU(pybind11::module& m)
hoomd/md/TwoStepLangevinGPU.cc:    pybind11::class_<TwoStepLangevinGPU, TwoStepLangevin, std::shared_ptr<TwoStepLangevinGPU>>(
hoomd/md/TwoStepLangevinGPU.cc:        "TwoStepLangevinGPU")
hoomd/md/BondTablePotentialGPU.cuh:/*! \file BondTablePotentialGPU.cuh
hoomd/md/BondTablePotentialGPU.cuh:    \brief Declares GPU kernel code for calculating the table bond forces. Used by
hoomd/md/BondTablePotentialGPU.cuh:   BONDTablePotentialGPU.
hoomd/md/BondTablePotentialGPU.cuh:#ifndef __BONDTABLEPOTENTIALGPU_CUH__
hoomd/md/BondTablePotentialGPU.cuh:#define __BONDTABLEPOTENTIALGPU_CUH__
hoomd/md/BondTablePotentialGPU.cuh://! Kernel driver that computes table forces on the GPU for TablePotentialGPU
hoomd/md/BondTablePotentialGPU.cuh:hipError_t gpu_compute_bondtable_forces(Scalar4* d_force,
hoomd/md/export_PotentialMeshBondGPU.cc.inc:#include "hoomd/md/PotentialBondGPU.h"
hoomd/md/export_PotentialMeshBondGPU.cc.inc:#define EXPORT_FUNCTION export_PotentialMeshBond@_bond@GPU
hoomd/md/export_PotentialMeshBondGPU.cc.inc:    export_PotentialMeshBondGPU<EVALUATOR_CLASS>(m, "PotentialMeshBond@_bond@GPU");
hoomd/md/TwoStepConstantPressureGPU.cu:#include "TwoStepConstantPressureGPU.cuh"
hoomd/md/TwoStepConstantPressureGPU.cu:/*! \file TwoStepNPTMTKGPU.cu
hoomd/md/TwoStepConstantPressureGPU.cu:    \brief Defines GPU kernel code for NPT integration on the GPU using the Martyna-Tobias-Klein
hoomd/md/TwoStepConstantPressureGPU.cu:   update equations. Used by TwoStepNPTMTKGPU.
hoomd/md/TwoStepConstantPressureGPU.cu:__global__ void gpu_npt_mtk_step_one_kernel(Scalar4* d_pos,
hoomd/md/TwoStepConstantPressureGPU.cu:    This is just a kernel driver for gpu_npt_mtk_step_one_kernel(). See it for more details.
hoomd/md/TwoStepConstantPressureGPU.cu:hipError_t gpu_npt_rescale_step_one(Scalar4* d_pos,
hoomd/md/TwoStepConstantPressureGPU.cu:                                    const GPUPartition& gpu_partition,
hoomd/md/TwoStepConstantPressureGPU.cu:    hipFuncGetAttributes(&attr, (const void*)gpu_npt_mtk_step_one_kernel);
hoomd/md/TwoStepConstantPressureGPU.cu:    // iterate over active GPUs in reverse, to end up on first GPU when returning from this function
hoomd/md/TwoStepConstantPressureGPU.cu:    for (int idev = gpu_partition.getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/md/TwoStepConstantPressureGPU.cu:        auto range = gpu_partition.getRangeAndSetGPU(idev);
hoomd/md/TwoStepConstantPressureGPU.cu:        hipLaunchKernelGGL((gpu_npt_mtk_step_one_kernel),
hoomd/md/TwoStepConstantPressureGPU.cu:__global__ void gpu_npt_mtk_wrap_kernel(const unsigned int nwork,
hoomd/md/TwoStepConstantPressureGPU.cu:    This is just a kernel driver for gpu_npt_mtk_wrap_kernel(). See it for more details.
hoomd/md/TwoStepConstantPressureGPU.cu:hipError_t gpu_npt_rescale_wrap(const GPUPartition& gpu_partition,
hoomd/md/TwoStepConstantPressureGPU.cu:    hipFuncGetAttributes(&attr, (const void*)gpu_npt_mtk_wrap_kernel);
hoomd/md/TwoStepConstantPressureGPU.cu:    // iterate over active GPUs in reverse, to end up on first GPU when returning from this function
hoomd/md/TwoStepConstantPressureGPU.cu:    for (int idev = gpu_partition.getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/md/TwoStepConstantPressureGPU.cu:        auto range = gpu_partition.getRangeAndSetGPU(idev);
hoomd/md/TwoStepConstantPressureGPU.cu:        hipLaunchKernelGGL((gpu_npt_mtk_wrap_kernel),
hoomd/md/TwoStepConstantPressureGPU.cu:__global__ void gpu_npt_mtk_step_two_kernel(Scalar4* d_vel,
hoomd/md/TwoStepConstantPressureGPU.cu:    This is just a kernel driver for gpu_npt_mtk_step_kernel(). See it for more details.
hoomd/md/TwoStepConstantPressureGPU.cu:hipError_t gpu_npt_rescale_step_two(Scalar4* d_vel,
hoomd/md/TwoStepConstantPressureGPU.cu:                                    const GPUPartition& gpu_partition,
hoomd/md/TwoStepConstantPressureGPU.cu:    hipFuncGetAttributes(&attr, (const void*)gpu_npt_mtk_step_two_kernel);
hoomd/md/TwoStepConstantPressureGPU.cu:    // iterate over active GPUs in reverse, to end up on first GPU when returning from this function
hoomd/md/TwoStepConstantPressureGPU.cu:    for (int idev = gpu_partition.getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/md/TwoStepConstantPressureGPU.cu:        auto range = gpu_partition.getRangeAndSetGPU(idev);
hoomd/md/TwoStepConstantPressureGPU.cu:        hipLaunchKernelGGL((gpu_npt_mtk_step_two_kernel),
hoomd/md/TwoStepConstantPressureGPU.cu:__global__ void gpu_npt_mtk_rescale_kernel(const unsigned int nwork,
hoomd/md/TwoStepConstantPressureGPU.cu:void gpu_npt_rescale_rescale(const GPUPartition& gpu_partition,
hoomd/md/TwoStepConstantPressureGPU.cu:    hipFuncGetAttributes(&attr, (const void*)gpu_npt_mtk_rescale_kernel);
hoomd/md/TwoStepConstantPressureGPU.cu:    // iterate over active GPUs in reverse, to end up on first GPU when returning from this function
hoomd/md/TwoStepConstantPressureGPU.cu:    for (int idev = gpu_partition.getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/md/TwoStepConstantPressureGPU.cu:        auto range = gpu_partition.getRangeAndSetGPU(idev);
hoomd/md/TwoStepConstantPressureGPU.cu:        hipLaunchKernelGGL((gpu_npt_mtk_rescale_kernel),
hoomd/md/TwoStepNVEGPU.cuh:/*! \file TwoStepNVEGPU.cuh
hoomd/md/TwoStepNVEGPU.cuh:    \brief Declares GPU kernel code for NVE integration on the GPU. Used by TwoStepNVEGPU.
hoomd/md/TwoStepNVEGPU.cuh:#include "hoomd/GPUPartition.cuh"
hoomd/md/TwoStepNVEGPU.cuh:#ifndef __TWO_STEP_NVE_GPU_CUH__
hoomd/md/TwoStepNVEGPU.cuh:#define __TWO_STEP_NVE_GPU_CUH__
hoomd/md/TwoStepNVEGPU.cuh://! Kernel driver for the first part of the NVE update called by TwoStepNVEGPU
hoomd/md/TwoStepNVEGPU.cuh:hipError_t gpu_nve_step_one(Scalar4* d_pos,
hoomd/md/TwoStepNVEGPU.cuh:                            const GPUPartition& gpu_partition,
hoomd/md/TwoStepNVEGPU.cuh://! Kernel driver for the second part of the NVE update called by TwoStepNVEGPU
hoomd/md/TwoStepNVEGPU.cuh:hipError_t gpu_nve_step_two(Scalar4* d_vel,
hoomd/md/TwoStepNVEGPU.cuh:                            const GPUPartition& gpu_partition,
hoomd/md/TwoStepNVEGPU.cuh:hipError_t gpu_nve_angular_step_one(Scalar4* d_orientation,
hoomd/md/TwoStepNVEGPU.cuh:                                    const GPUPartition& gpu_partition,
hoomd/md/TwoStepNVEGPU.cuh:hipError_t gpu_nve_angular_step_two(const Scalar4* d_orientation,
hoomd/md/TwoStepNVEGPU.cuh:                                    const GPUPartition& gpu_partition,
hoomd/md/TwoStepNVEGPU.cuh:#endif //__TWO_STEP_NVE_GPU_CUH__
hoomd/md/PotentialTersoffGPU.h:#ifndef __POTENTIAL_TERSOFF_GPU_H__
hoomd/md/PotentialTersoffGPU.h:#define __POTENTIAL_TERSOFF_GPU_H__
hoomd/md/PotentialTersoffGPU.h:#include "PotentialTersoffGPU.cuh"
hoomd/md/PotentialTersoffGPU.h:/*! \file PotentialTersoffGPU.h
hoomd/md/PotentialTersoffGPU.h:    \brief Defines the template class computing certain three-body forces on the GPU
hoomd/md/PotentialTersoffGPU.h://! Template class for computing three-body potentials and forces on the GPU
hoomd/md/PotentialTersoffGPU.h:    \tparam gpu_cgpf Driver function that calls gpu_compute_tersoff_forces<evaluator>()
hoomd/md/PotentialTersoffGPU.h:    \sa export_PotentialTersoffGPU()
hoomd/md/PotentialTersoffGPU.h:template<class evaluator> class PotentialTersoffGPU : public PotentialTersoff<evaluator>
hoomd/md/PotentialTersoffGPU.h:    PotentialTersoffGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/md/PotentialTersoffGPU.h:    virtual ~PotentialTersoffGPU();
hoomd/md/PotentialTersoffGPU.h:PotentialTersoffGPU<evaluator>::PotentialTersoffGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/md/PotentialTersoffGPU.h:    this->m_exec_conf->msg->notice(5) << "Constructing PotentialTersoffGPU" << std::endl;
hoomd/md/PotentialTersoffGPU.h:    // can't run on the GPU if there aren't any GPUs in the execution configuration
hoomd/md/PotentialTersoffGPU.h:    if (!this->m_exec_conf->isCUDAEnabled())
hoomd/md/PotentialTersoffGPU.h:            << "***Error! Creating a PotentialTersoffGPU with no GPU in the execution configuration"
hoomd/md/PotentialTersoffGPU.h:        throw std::runtime_error("Error initializing PotentialTersoffGPU");
hoomd/md/PotentialTersoffGPU.h:template<class evaluator> PotentialTersoffGPU<evaluator>::~PotentialTersoffGPU()
hoomd/md/PotentialTersoffGPU.h:    this->m_exec_conf->msg->notice(5) << "Destroying PotentialTersoffGPU" << std::endl;
hoomd/md/PotentialTersoffGPU.h:template<class evaluator> void PotentialTersoffGPU<evaluator>::computeForces(uint64_t timestep)
hoomd/md/PotentialTersoffGPU.h:    // The GPU implementation CANNOT handle a half neighborlist, error out now
hoomd/md/PotentialTersoffGPU.h:            << "***Error! PotentialTersoffGPU cannot handle a half neighborlist" << std::endl;
hoomd/md/PotentialTersoffGPU.h:        throw std::runtime_error("Error computing forces in PotentialTersoffGPU");
hoomd/md/PotentialTersoffGPU.h:    kernel::gpu_compute_triplet_forces<evaluator>(
hoomd/md/PotentialTersoffGPU.h:    if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/PotentialTersoffGPU.h:        CHECK_CUDA_ERROR();
hoomd/md/PotentialTersoffGPU.h:template<class T> void export_PotentialTersoffGPU(pybind11::module& m, const std::string& name)
hoomd/md/PotentialTersoffGPU.h:    pybind11::class_<PotentialTersoffGPU<T>,
hoomd/md/PotentialTersoffGPU.h:                     std::shared_ptr<PotentialTersoffGPU<T>>>(m, name.c_str())
hoomd/md/ConstantForceCompute.cc:    if (m_exec_conf->isCUDAEnabled() && m_exec_conf->allConcurrentManagedAccess())
hoomd/md/ConstantForceCompute.cc:        cudaMemAdvise(m_constant_force.get(),
hoomd/md/ConstantForceCompute.cc:                      cudaMemAdviseSetReadMostly,
hoomd/md/ConstantForceCompute.cc:        cudaMemAdvise(m_constant_torque.get(),
hoomd/md/ConstantForceCompute.cc:                      cudaMemAdviseSetReadMostly,
hoomd/md/ConstantForceCompute.cc:    if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/ConstantForceCompute.cc:        CHECK_CUDA_ERROR();
hoomd/md/TableDihedralForceGPU.cu:#include "TableDihedralForceGPU.cuh"
hoomd/md/TableDihedralForceGPU.cu:/*! \file TableDihedralForceGPU.cu
hoomd/md/TableDihedralForceGPU.cu:    \brief Defines GPU kernel code for calculating the table dihedral forces. Used by
hoomd/md/TableDihedralForceGPU.cu:   TableDihedralForceComputeGPU.
hoomd/md/TableDihedralForceGPU.cu:    \param dlist List of dihedrals stored on the GPU
hoomd/md/TableDihedralForceGPU.cu:    \param n_dihedrals_list List of numbers of dihedrals stored on the GPU
hoomd/md/TableDihedralForceGPU.cu:__global__ void gpu_compute_table_dihedral_forces_kernel(Scalar4* d_force,
hoomd/md/TableDihedralForceGPU.cu:    \param dlist List of dihedrals stored on the GPU
hoomd/md/TableDihedralForceGPU.cu:    \param n_dihedrals_list List of numbers of dihedrals stored on the GPU
hoomd/md/TableDihedralForceGPU.cu:    \note This is just a kernel driver. See gpu_compute_table_dihedral_forces_kernel for full
hoomd/md/TableDihedralForceGPU.cu:hipError_t gpu_compute_table_dihedral_forces(Scalar4* d_force,
hoomd/md/TableDihedralForceGPU.cu:    hipFuncGetAttributes(&attr, (const void*)gpu_compute_table_dihedral_forces_kernel);
hoomd/md/TableDihedralForceGPU.cu:    hipLaunchKernelGGL((gpu_compute_table_dihedral_forces_kernel),
hoomd/md/TwoStepRATTLELangevinGPU.cuh:/*! \file TwoStepRATTLELangevinGPU.cuh
hoomd/md/TwoStepRATTLELangevinGPU.cuh:    \brief Declares GPU kernel code for RATTLELangevin dynamics on the GPU. Used by
hoomd/md/TwoStepRATTLELangevinGPU.cuh:   TwoStepRATTLELangevinGPU.
hoomd/md/TwoStepRATTLELangevinGPU.cuh:#ifndef __TWO_STEP_RATTLE_LANGEVIN_GPU_CUH__
hoomd/md/TwoStepRATTLELangevinGPU.cuh:#define __TWO_STEP_RATTLE_LANGEVIN_GPU_CUH__
hoomd/md/TwoStepRATTLELangevinGPU.cuh://! gpu_rattle_langevin_step_two()
hoomd/md/TwoStepRATTLELangevinGPU.cuh:gpu_rattle_langevin_angular_step_two(const Scalar4* d_pos,
hoomd/md/TwoStepRATTLELangevinGPU.cuh:__global__ void gpu_rattle_bdtally_reduce_partial_sum_kernel(Scalar* d_sum,
hoomd/md/TwoStepRATTLELangevinGPU.cuh:hipError_t gpu_rattle_langevin_step_two(const Scalar4* d_pos,
hoomd/md/TwoStepRATTLELangevinGPU.cuh:/*! \file TwoStepRATTLELangevinGPU.cu
hoomd/md/TwoStepRATTLELangevinGPU.cuh:    \brief Defines GPU kernel code for RATTLELangevin integration on the GPU. Used by
hoomd/md/TwoStepRATTLELangevinGPU.cuh:   TwoStepRATTLELangevinGPU.
hoomd/md/TwoStepRATTLELangevinGPU.cuh:    This kernel is implemented in a very similar manner to gpu_nve_step_two_kernel(), see it for
hoomd/md/TwoStepRATTLELangevinGPU.cuh:__global__ void gpu_rattle_langevin_step_two_kernel(const Scalar4* d_pos,
hoomd/md/TwoStepRATTLELangevinGPU.cuh:    \param rattle_langevin_args Collected arguments for gpu_rattle_langevin_step_two_kernel() and
hoomd/md/TwoStepRATTLELangevinGPU.cuh:   gpu_rattle_langevin_angular_step_two() \param deltaT Amount of real time to step forward in one
hoomd/md/TwoStepRATTLELangevinGPU.cuh:    This is just a driver for gpu_rattle_langevin_step_two_kernel(), see it for details.
hoomd/md/TwoStepRATTLELangevinGPU.cuh:hipError_t gpu_rattle_langevin_step_two(const Scalar4* d_pos,
hoomd/md/TwoStepRATTLELangevinGPU.cuh:    hipLaunchKernelGGL((gpu_rattle_langevin_step_two_kernel<Manifold>),
hoomd/md/TwoStepRATTLELangevinGPU.cuh:        hipLaunchKernelGGL((gpu_rattle_bdtally_reduce_partial_sum_kernel),
hoomd/md/TwoStepRATTLELangevinGPU.cuh:#endif //__TWO_STEP_RATTLE_LANGEVIN_GPU_CUH__
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cu:#include "TriangleAreaConservationMeshForceComputeGPU.cuh"
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cu:/*! \file TriangleAreaConservationMeshForceComputeGPU.cu
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cu:    \brief Defines GPU kernel code for calculating the triangle area conservation forces. Used by
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cu:   TriangleAreaConservationMeshForceComputeComputeGPU.
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cu://! Kernel for calculating area conservation force on the GPU
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cu:    \param box Box dimensions (in GPU format) to use for periodic boundary conditions
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cu:    \param tlist List of mesh triangle indices stored on the GPU
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cu:    \param tpos_list Position of current index in list of mesh triangles stored on the GPU
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cu:    \param n_triangles_list List of numbers of mesh triangles stored on the GPU
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cu:gpu_compute_TriangleAreaConservation_force_kernel(Scalar4* d_force,
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cu:    \param box Box dimensions (in GPU format) to use for periodic boundary conditions
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cu:    \param tlist List of mesh triangles stored on the GPU
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cu:    \param tpos_list Position of current index in list of mesh triangles stored on the GPU
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cu:    \param n_triangles_list List of numbers of mesh triangles stored on the GPU
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cu:hipError_t gpu_compute_TriangleAreaConservation_force(Scalar4* d_force,
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cu:    hipFuncGetAttributes(&attr, (const void*)gpu_compute_TriangleAreaConservation_force_kernel);
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cu:    hipLaunchKernelGGL((gpu_compute_TriangleAreaConservation_force_kernel),
hoomd/md/NeighborListGPU.h:#include "NeighborListGPU.cuh"
hoomd/md/NeighborListGPU.h:/*! \file NeighborListGPU.h
hoomd/md/NeighborListGPU.h:    \brief Declares the NeighborListGPU class
hoomd/md/NeighborListGPU.h:#ifndef __NEIGHBORLISTGPU_H__
hoomd/md/NeighborListGPU.h:#define __NEIGHBORLISTGPU_H__
hoomd/md/NeighborListGPU.h://! Neighbor list build on the GPU
hoomd/md/NeighborListGPU.h:    on the GPU for use by other GPU nlist classes derived from NeighborListGPU.
hoomd/md/NeighborListGPU.h:    GPU kernel methods are defined in NeighborListGPU.cuh and defined in NeighborListGPU.cu.
hoomd/md/NeighborListGPU.h:class PYBIND11_EXPORT NeighborListGPU : public NeighborList
hoomd/md/NeighborListGPU.h:    NeighborListGPU(std::shared_ptr<SystemDefinition> sysdef, Scalar r_buff)
hoomd/md/NeighborListGPU.h:        m_exec_conf->msg->notice(5) << "Constructing NeighborlistGPU" << std::endl;
hoomd/md/NeighborListGPU.h:            cudaMemAdvise(m_flags.get(),
hoomd/md/NeighborListGPU.h:                          cudaMemAdviseSetPreferredLocation,
hoomd/md/NeighborListGPU.h:                          cudaCpuDeviceId);
hoomd/md/NeighborListGPU.h:            CHECK_CUDA_ERROR();
hoomd/md/NeighborListGPU.h:            cudaMemAdvise(m_req_size_nlist.get(),
hoomd/md/NeighborListGPU.h:                          cudaMemAdviseSetPreferredLocation,
hoomd/md/NeighborListGPU.h:                          cudaCpuDeviceId);
hoomd/md/NeighborListGPU.h:            CHECK_CUDA_ERROR();
hoomd/md/NeighborListGPU.h:    virtual ~NeighborListGPU() { }
hoomd/md/NeighborListGPU.h:    //! Update the exclusion list on the GPU
hoomd/md/NeighborListGPU.h:    GlobalArray<unsigned int> m_flags; //!< Storage for device flags on the GPU
hoomd/md/NeighborListGPU.h:    //! Perform the nlist distance check on the GPU
hoomd/md/NeighborListGPU.h:    //! GPU nlists set their last updated pos in the compute kernel, this call only resets the last
hoomd/md/NeighborListGPU.h:    //! Build the head list for neighbor list indexing on the GPU
hoomd/md/export_PotentialSpecialPairGPU.cc.inc:#include "hoomd/md/PotentialSpecialPairGPU.h"
hoomd/md/export_PotentialSpecialPairGPU.cc.inc:#define EXPORT_FUNCTION export_PotentialSpecialPair@_pair@GPU
hoomd/md/export_PotentialSpecialPairGPU.cc.inc:    export_PotentialSpecialPairGPU<EVALUATOR_CLASS>(m, "PotentialSpecialPair@_pair@GPU");
hoomd/md/PotentialBondGPU.cuh:/*! \file PotentialBondGPU.cuh
hoomd/md/PotentialBondGPU.cuh:    \brief Defines templated GPU kernel code for calculating the bond forces.
hoomd/md/PotentialBondGPU.cuh:#ifndef __POTENTIAL_BOND_GPU_CUH__
hoomd/md/PotentialBondGPU.cuh:#define __POTENTIAL_BOND_GPU_CUH__
hoomd/md/PotentialBondGPU.cuh:                const group_storage<group_size>* _d_gpu_bondlist,
hoomd/md/PotentialBondGPU.cuh:                const Index2D& _gpu_table_indexer,
hoomd/md/PotentialBondGPU.cuh:                const unsigned int* _d_gpu_bond_pos,
hoomd/md/PotentialBondGPU.cuh:                const unsigned int* _d_gpu_n_bonds,
hoomd/md/PotentialBondGPU.cuh:          d_pos(_d_pos), d_charge(_d_charge), box(_box), d_gpu_bondlist(_d_gpu_bondlist),
hoomd/md/PotentialBondGPU.cuh:          gpu_table_indexer(_gpu_table_indexer), d_gpu_bond_pos(_d_gpu_bond_pos),
hoomd/md/PotentialBondGPU.cuh:          d_gpu_n_bonds(_d_gpu_n_bonds), n_bond_types(_n_bond_types), block_size(_block_size),
hoomd/md/PotentialBondGPU.cuh:    const BoxDim box;          //!< Simulation box in GPU format
hoomd/md/PotentialBondGPU.cuh:    const group_storage<group_size>* d_gpu_bondlist; //!< List of bonds stored on the GPU
hoomd/md/PotentialBondGPU.cuh:    const Index2D& gpu_table_indexer;                //!< Indexer of 2D bond list
hoomd/md/PotentialBondGPU.cuh:        d_gpu_bond_pos; //!< List of pos id of bonds stored on the GPU (needed for mesh bond)
hoomd/md/PotentialBondGPU.cuh:    const unsigned int* d_gpu_n_bonds; //!< List of number of bonds stored on the GPU
hoomd/md/PotentialBondGPU.cuh:    const hipDeviceProp_t& devprop;    //!< CUDA device properties
hoomd/md/PotentialBondGPU.cuh:    \param d_pos particle positions on the GPU
hoomd/md/PotentialBondGPU.cuh:    \param blist List of bonds stored on the GPU
hoomd/md/PotentialBondGPU.cuh:    \param bpos_list List of positions in bonds stored on the GPU
hoomd/md/PotentialBondGPU.cuh:    \param n_bonds_list List of numbers of bonds stored on the GPU
hoomd/md/PotentialBondGPU.cuh:__global__ void gpu_compute_bond_forces_kernel(Scalar4* d_force,
hoomd/md/PotentialBondGPU.cuh://! Kernel driver that computes lj forces on the GPU for LJForceComputeGPU
hoomd/md/PotentialBondGPU.cuh:    This is just a driver function for gpu_compute_bond_forces_kernel(), see it for details.
hoomd/md/PotentialBondGPU.cuh:gpu_compute_bond_forces(const kernel::bond_args_t<group_size>& bond_args,
hoomd/md/PotentialBondGPU.cuh:                             &gpu_compute_bond_forces_kernel<evaluator, group_size, true>));
hoomd/md/PotentialBondGPU.cuh:        hipLaunchKernelGGL((gpu_compute_bond_forces_kernel<evaluator, group_size, true>),
hoomd/md/PotentialBondGPU.cuh:                           bond_args.d_gpu_bondlist,
hoomd/md/PotentialBondGPU.cuh:                           bond_args.gpu_table_indexer,
hoomd/md/PotentialBondGPU.cuh:                           bond_args.d_gpu_bond_pos,
hoomd/md/PotentialBondGPU.cuh:                           bond_args.d_gpu_n_bonds,
hoomd/md/PotentialBondGPU.cuh:        hipLaunchKernelGGL((gpu_compute_bond_forces_kernel<evaluator, group_size, false>),
hoomd/md/PotentialBondGPU.cuh:                           bond_args.d_gpu_bondlist,
hoomd/md/PotentialBondGPU.cuh:                           bond_args.gpu_table_indexer,
hoomd/md/PotentialBondGPU.cuh:                           bond_args.d_gpu_bond_pos,
hoomd/md/PotentialBondGPU.cuh:                           bond_args.d_gpu_n_bonds,
hoomd/md/PotentialBondGPU.cuh:gpu_compute_bond_forces(const kernel::bond_args_t<group_size>& bond_args,
hoomd/md/PotentialBondGPU.cuh:#endif // __POTENTIAL_BOND_GPU_CUH__
hoomd/md/BondTablePotential.cc:    GPUArray<Scalar2> tables(m_table_width, m_bond_data->getNTypes(), m_exec_conf);
hoomd/md/BondTablePotential.cc:    GPUArray<Scalar4> params(m_bond_data->getNTypes(), m_exec_conf);
hoomd/md/AreaConservationMeshForceComputeGPU.cuh:/*! \file MeshAreaConservationGPU.cuh
hoomd/md/AreaConservationMeshForceComputeGPU.cuh:    \brief Declares GPU kernel code for calculating the area cnstraint forces. Used by
hoomd/md/AreaConservationMeshForceComputeGPU.cuh:   MeshAreaConservationGPU.
hoomd/md/AreaConservationMeshForceComputeGPU.cuh://! Kernel driver that computes the area for MeshAreaConservationGPU
hoomd/md/AreaConservationMeshForceComputeGPU.cuh:hipError_t gpu_compute_area_constraint_area(Scalar* d_sum_area,
hoomd/md/AreaConservationMeshForceComputeGPU.cuh://! Kernel driver that computes the forces for MeshAreaConservationGPU
hoomd/md/AreaConservationMeshForceComputeGPU.cuh:hipError_t gpu_compute_area_constraint_force(Scalar4* d_force,
hoomd/md/ForceDistanceConstraintGPU.cuh:#ifndef __FORCE_DISTANCE_CONSTRAINT_GPU_CUH__
hoomd/md/ForceDistanceConstraintGPU.cuh:#define __FORCE_DISTANCE_CONSTRAINT_GPU_CUH__
hoomd/md/ForceDistanceConstraintGPU.cuh:hipError_t gpu_fill_matrix_vector(unsigned int n_constraint,
hoomd/md/ForceDistanceConstraintGPU.cuh:                                  const group_storage<2>* d_gpu_clist,
hoomd/md/ForceDistanceConstraintGPU.cuh:                                  const Index2D& gpu_clist_indexer,
hoomd/md/ForceDistanceConstraintGPU.cuh:                                  const unsigned int* d_gpu_n_constraints,
hoomd/md/ForceDistanceConstraintGPU.cuh:                                  const unsigned int* d_gpu_cpos,
hoomd/md/ForceDistanceConstraintGPU.cuh:hipError_t gpu_count_nnz(unsigned int n_constraint,
hoomd/md/ForceDistanceConstraintGPU.cuh:hipError_t gpu_dense2sparse(unsigned int n_constraint,
hoomd/md/ForceDistanceConstraintGPU.cuh:hipError_t gpu_compute_constraint_forces(const Scalar4* d_pos,
hoomd/md/ForceDistanceConstraintGPU.cuh:                                         const group_storage<2>* d_gpu_clist,
hoomd/md/ForceDistanceConstraintGPU.cuh:                                         const Index2D& gpu_clist_indexer,
hoomd/md/ForceDistanceConstraintGPU.cuh:                                         const unsigned int* d_gpu_n_constraints,
hoomd/md/ForceDistanceConstraintGPU.cuh:                                         const unsigned int* d_gpu_cpos,
hoomd/md/PotentialPairDPDThermoGPU.cuh:/*! \file PotentialPairDPDThermoGPU.cuh
hoomd/md/PotentialPairDPDThermoGPU.cuh:    \brief Declares driver functions for computing all types of pair forces on the GPU
hoomd/md/PotentialPairDPDThermoGPU.cuh:const int gpu_dpd_pair_force_max_tpp = 32;
hoomd/md/PotentialPairDPDThermoGPU.cuh:const int gpu_dpd_pair_force_max_tpp = 64;
hoomd/md/PotentialPairDPDThermoGPU.cuh://! args struct for passing additional options to gpu_compute_dpd_forces
hoomd/md/PotentialPairDPDThermoGPU.cuh:    const BoxDim box;          //!< Simulation box in GPU format
hoomd/md/PotentialPairDPDThermoGPU.cuh:    \param d_pos particle positions on the GPU
hoomd/md/PotentialPairDPDThermoGPU.cuh:    \param d_vel particle velocities on the GPU
hoomd/md/PotentialPairDPDThermoGPU.cuh:    \param d_tag particle tags on the GPU
hoomd/md/PotentialPairDPDThermoGPU.cuh:__global__ void gpu_compute_dpd_forces_kernel(Scalar4* d_force,
hoomd/md/PotentialPairDPDThermoGPU.cuh:    max_threads -= max_threads % gpu_dpd_pair_force_max_tpp;
hoomd/md/PotentialPairDPDThermoGPU.cuh:            max_block_size = dpd_get_max_block_size(gpu_compute_dpd_forces_kernel<evaluator,
hoomd/md/PotentialPairDPDThermoGPU.cuh:            hipLaunchKernelGGL((gpu_compute_dpd_forces_kernel<evaluator,
hoomd/md/PotentialPairDPDThermoGPU.cuh://! Kernel driver that computes pair DPD thermo forces on the GPU
hoomd/md/PotentialPairDPDThermoGPU.cuh:    This is just a driver function for gpu_compute_dpd_forces_kernel(), see it for details.
hoomd/md/PotentialPairDPDThermoGPU.cuh:gpu_compute_dpd_forces(const dpd_pair_args_t& args, const typename evaluator::param_type* d_params)
hoomd/md/PotentialPairDPDThermoGPU.cuh:            DPDForceComputeKernel<evaluator, 0, 1, 0, gpu_dpd_pair_force_max_tpp>::launch(args,
hoomd/md/PotentialPairDPDThermoGPU.cuh:            DPDForceComputeKernel<evaluator, 1, 1, 0, gpu_dpd_pair_force_max_tpp>::launch(args,
hoomd/md/PotentialPairDPDThermoGPU.cuh:            DPDForceComputeKernel<evaluator, 0, 0, 0, gpu_dpd_pair_force_max_tpp>::launch(args,
hoomd/md/PotentialPairDPDThermoGPU.cuh:            DPDForceComputeKernel<evaluator, 1, 0, 0, gpu_dpd_pair_force_max_tpp>::launch(args,
hoomd/md/PotentialPairDPDThermoGPU.cuh:gpu_compute_dpd_forces(const dpd_pair_args_t& args, const typename evaluator::param_type* d_params);
hoomd/md/compute.py:            thermo_cls = _md.ComputeThermoGPU
hoomd/md/compute.py:            thermoHMA_cls = _md.ComputeThermoHMAGPU
hoomd/md/PeriodicImproperForceComputeGPU.cc:#include "PeriodicImproperForceComputeGPU.h"
hoomd/md/PeriodicImproperForceComputeGPU.cc:PeriodicImproperForceComputeGPU::PeriodicImproperForceComputeGPU(
hoomd/md/PeriodicImproperForceComputeGPU.cc:    // can't run on the GPU if there aren't any GPUs in the execution configuration
hoomd/md/PeriodicImproperForceComputeGPU.cc:    if (!m_exec_conf->isCUDAEnabled())
hoomd/md/PeriodicImproperForceComputeGPU.cc:        throw std::runtime_error("ImproperForceComputeGPU requires a GPU device.");
hoomd/md/PeriodicImproperForceComputeGPU.cc:PeriodicImproperForceComputeGPU::~PeriodicImproperForceComputeGPU() { }
hoomd/md/PeriodicImproperForceComputeGPU.cc:/*! Internal method for computing the forces on the GPU.
hoomd/md/PeriodicImproperForceComputeGPU.cc:    \post The force data on the GPU is written with the calculated forces
hoomd/md/PeriodicImproperForceComputeGPU.cc:void PeriodicImproperForceComputeGPU::computeForces(uint64_t timestep)
hoomd/md/PeriodicImproperForceComputeGPU.cc:    ArrayHandle<ImproperData::members_t> d_gpu_improper_list(m_improper_data->getGPUTable(),
hoomd/md/PeriodicImproperForceComputeGPU.cc:    ArrayHandle<unsigned int> d_impropers_ABCD(m_improper_data->getGPUPosTable(),
hoomd/md/PeriodicImproperForceComputeGPU.cc:    // run the kernel in parallel on all GPUs
hoomd/md/PeriodicImproperForceComputeGPU.cc:    kernel::gpu_compute_periodic_improper_forces(d_force.data,
hoomd/md/PeriodicImproperForceComputeGPU.cc:                                                 d_gpu_improper_list.data,
hoomd/md/PeriodicImproperForceComputeGPU.cc:                                                 m_improper_data->getGPUTableIndexer().getW(),
hoomd/md/PeriodicImproperForceComputeGPU.cc:    if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/PeriodicImproperForceComputeGPU.cc:        CHECK_CUDA_ERROR();
hoomd/md/PeriodicImproperForceComputeGPU.cc:void export_PeriodicImproperForceComputeGPU(pybind11::module& m)
hoomd/md/PeriodicImproperForceComputeGPU.cc:    pybind11::class_<PeriodicImproperForceComputeGPU,
hoomd/md/PeriodicImproperForceComputeGPU.cc:                     std::shared_ptr<PeriodicImproperForceComputeGPU>>(
hoomd/md/PeriodicImproperForceComputeGPU.cc:        "PeriodicImproperForceComputeGPU")
hoomd/md/TableDihedralForceComputeGPU.cc:#include "TableDihedralForceComputeGPU.h"
hoomd/md/TableDihedralForceComputeGPU.cc:/*! \file TableDihedralForceComputeGPU.cc
hoomd/md/TableDihedralForceComputeGPU.cc:    \brief Defines the TableDihedralForceComputeGPU class
hoomd/md/TableDihedralForceComputeGPU.cc:TableDihedralForceComputeGPU::TableDihedralForceComputeGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/md/TableDihedralForceComputeGPU.cc:    // can't run on the GPU if there aren't any GPUs in the execution configuration
hoomd/md/TableDihedralForceComputeGPU.cc:    if (!m_exec_conf->isCUDAEnabled())
hoomd/md/TableDihedralForceComputeGPU.cc:            << "Creating a BondTableForceComputeGPU with no GPU in the execution configuration"
hoomd/md/TableDihedralForceComputeGPU.cc:        throw std::runtime_error("Error initializing BondTableForceComputeGPU");
hoomd/md/TableDihedralForceComputeGPU.cc:    // allocate flags storage on the GPU
hoomd/md/TableDihedralForceComputeGPU.cc:    GPUArray<unsigned int> flags(1, this->m_exec_conf);
hoomd/md/TableDihedralForceComputeGPU.cc:Calls gpu_compute_bondtable_forces to do the leg work
hoomd/md/TableDihedralForceComputeGPU.cc:void TableDihedralForceComputeGPU::computeForces(uint64_t timestep)
hoomd/md/TableDihedralForceComputeGPU.cc:        ArrayHandle<group_storage<4>> d_gpu_dihedrallist(m_dihedral_data->getGPUTable(),
hoomd/md/TableDihedralForceComputeGPU.cc:        ArrayHandle<unsigned int> d_gpu_n_dihedrals(m_dihedral_data->getNGroupsArray(),
hoomd/md/TableDihedralForceComputeGPU.cc:        ArrayHandle<unsigned int> d_dihedrals_ABCD(m_dihedral_data->getGPUPosTable(),
hoomd/md/TableDihedralForceComputeGPU.cc:        // run the kernel on all GPUs in parallel
hoomd/md/TableDihedralForceComputeGPU.cc:        kernel::gpu_compute_table_dihedral_forces(d_force.data,
hoomd/md/TableDihedralForceComputeGPU.cc:                                                  d_gpu_dihedrallist.data,
hoomd/md/TableDihedralForceComputeGPU.cc:                                                  m_dihedral_data->getGPUTableIndexer().getW(),
hoomd/md/TableDihedralForceComputeGPU.cc:                                                  d_gpu_n_dihedrals.data,
hoomd/md/TableDihedralForceComputeGPU.cc:    if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/TableDihedralForceComputeGPU.cc:        CHECK_CUDA_ERROR();
hoomd/md/TableDihedralForceComputeGPU.cc:void export_TableDihedralForceComputeGPU(pybind11::module& m)
hoomd/md/TableDihedralForceComputeGPU.cc:    pybind11::class_<TableDihedralForceComputeGPU,
hoomd/md/TableDihedralForceComputeGPU.cc:                     std::shared_ptr<TableDihedralForceComputeGPU>>(m,
hoomd/md/TableDihedralForceComputeGPU.cc:                                                                    "TableDihedralForceComputeGPU")
hoomd/md/NeighborListBinned.cc:                // GPU)
hoomd/md/OPLSDihedralForceCompute.h:    GPUArray<Scalar4> m_params;
hoomd/md/NeighborList.cc:    if (m_exec_conf->isCUDAEnabled() && m_exec_conf->allConcurrentManagedAccess())
hoomd/md/NeighborList.cc:        cudaMemAdvise(m_r_cut.get(),
hoomd/md/NeighborList.cc:                      cudaMemAdviseSetReadMostly,
hoomd/md/NeighborList.cc:        CHECK_CUDA_ERROR();
hoomd/md/NeighborList.cc:    if (m_exec_conf->isCUDAEnabled() && m_exec_conf->allConcurrentManagedAccess())
hoomd/md/NeighborList.cc:        cudaMemAdvise(m_rcut_max.get(),
hoomd/md/NeighborList.cc:                      cudaMemAdviseSetReadMostly,
hoomd/md/NeighborList.cc:        CHECK_CUDA_ERROR();
hoomd/md/NeighborList.cc:    if (m_exec_conf->isCUDAEnabled() && m_exec_conf->allConcurrentManagedAccess())
hoomd/md/NeighborList.cc:        cudaMemAdvise(m_rcut_base.get(),
hoomd/md/NeighborList.cc:                      cudaMemAdviseSetReadMostly,
hoomd/md/NeighborList.cc:        CHECK_CUDA_ERROR();
hoomd/md/NeighborList.cc:    if (m_exec_conf->isCUDAEnabled() && m_exec_conf->allConcurrentManagedAccess())
hoomd/md/NeighborList.cc:        cudaMemAdvise(m_r_listsq.get(),
hoomd/md/NeighborList.cc:                      cudaMemAdviseSetReadMostly,
hoomd/md/NeighborList.cc:        CHECK_CUDA_ERROR();
hoomd/md/NeighborList.cc:    if (m_exec_conf->isCUDAEnabled() && m_exec_conf->allConcurrentManagedAccess())
hoomd/md/NeighborList.cc:        cudaMemAdvise(m_Nmax.get(),
hoomd/md/NeighborList.cc:                      cudaMemAdviseSetReadMostly,
hoomd/md/NeighborList.cc:        CHECK_CUDA_ERROR();
hoomd/md/NeighborList.cc:    if (m_exec_conf->isCUDAEnabled() && m_exec_conf->allConcurrentManagedAccess())
hoomd/md/NeighborList.cc:        cudaMemAdvise(m_conditions.get(),
hoomd/md/NeighborList.cc:                      cudaMemAdviseSetPreferredLocation,
hoomd/md/NeighborList.cc:                      cudaCpuDeviceId);
hoomd/md/NeighborList.cc:        CHECK_CUDA_ERROR();
hoomd/md/NeighborList.cc:    if (m_exec_conf->isCUDAEnabled())
hoomd/md/NeighborList.cc:        m_last_gpu_partition = GPUPartition(m_exec_conf->getGPUIds());
hoomd/md/NeighborList.cc://! Update GPU memory locality
hoomd/md/NeighborList.cc:    if (m_exec_conf->isCUDAEnabled() && m_exec_conf->allConcurrentManagedAccess())
hoomd/md/NeighborList.cc:        auto gpu_map = m_exec_conf->getGPUIds();
hoomd/md/NeighborList.cc:        const GPUPartition& gpu_partition = m_pdata->getGPUPartition();
hoomd/md/NeighborList.cc:        m_last_gpu_partition = gpu_partition;
hoomd/md/NeighborList.cc:            // split preferred location of neighbor list across GPUs
hoomd/md/NeighborList.cc:            for (unsigned int idev = 0; idev < m_exec_conf->getNumActiveGPUs(); ++idev)
hoomd/md/NeighborList.cc:                auto range = gpu_partition.getRange(idev);
hoomd/md/NeighborList.cc:                    cudaMemAdvise(m_nlist.get() + h_head_list.data[range.first],
hoomd/md/NeighborList.cc:                                  cudaMemAdviseSetPreferredLocation,
hoomd/md/NeighborList.cc:                                  gpu_map[idev]);
hoomd/md/NeighborList.cc:        CHECK_CUDA_ERROR();
hoomd/md/NeighborList.cc:        for (unsigned int idev = 0; idev < m_exec_conf->getNumActiveGPUs(); ++idev)
hoomd/md/NeighborList.cc:            auto range = gpu_partition.getRange(idev);
hoomd/md/NeighborList.cc:            cudaMemAdvise(m_head_list.get() + range.first,
hoomd/md/NeighborList.cc:                          cudaMemAdviseSetPreferredLocation,
hoomd/md/NeighborList.cc:                          gpu_map[idev]);
hoomd/md/NeighborList.cc:            cudaMemAdvise(m_n_neigh.get() + range.first,
hoomd/md/NeighborList.cc:                          cudaMemAdviseSetPreferredLocation,
hoomd/md/NeighborList.cc:                          gpu_map[idev]);
hoomd/md/NeighborList.cc:            cudaMemAdvise(m_last_pos.get() + range.first,
hoomd/md/NeighborList.cc:                          cudaMemAdviseSetPreferredLocation,
hoomd/md/NeighborList.cc:                          gpu_map[idev]);
hoomd/md/NeighborList.cc:            cudaMemPrefetchAsync(m_head_list.get() + range.first,
hoomd/md/NeighborList.cc:                                 gpu_map[idev]);
hoomd/md/NeighborList.cc:            cudaMemPrefetchAsync(m_n_neigh.get() + range.first,
hoomd/md/NeighborList.cc:                                 gpu_map[idev]);
hoomd/md/NeighborList.cc:            cudaMemPrefetchAsync(m_last_pos.get() + range.first,
hoomd/md/NeighborList.cc:                                 gpu_map[idev]);
hoomd/md/NeighborList.cc:        CHECK_CUDA_ERROR();
hoomd/md/NeighborList.cc:void export_LocalNeighborListDataGPU(pybind11::module& m)
hoomd/md/TwoStepRATTLENVEGPU.cu:#include "TwoStepRATTLENVEGPU.cuh"
hoomd/md/TwoStepRATTLENVEGPU.cu:__global__ void gpu_rattle_nve_step_one_kernel(Scalar4* d_pos,
hoomd/md/TwoStepRATTLENVEGPU.cu:   particles in the group See gpu_rattle_nve_step_one_kernel() for full documentation, this function
hoomd/md/TwoStepRATTLENVEGPU.cu:hipError_t gpu_rattle_nve_step_one(Scalar4* d_pos,
hoomd/md/TwoStepRATTLENVEGPU.cu:                                   const GPUPartition& gpu_partition,
hoomd/md/TwoStepRATTLENVEGPU.cu:    hipFuncGetAttributes(&attr, (const void*)gpu_rattle_nve_step_one_kernel);
hoomd/md/TwoStepRATTLENVEGPU.cu:    // iterate over active GPUs in reverse, to end up on first GPU when returning from this function
hoomd/md/TwoStepRATTLENVEGPU.cu:    for (int idev = gpu_partition.getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/md/TwoStepRATTLENVEGPU.cu:        auto range = gpu_partition.getRangeAndSetGPU(idev);
hoomd/md/TwoStepRATTLENVEGPU.cu:        hipLaunchKernelGGL((gpu_rattle_nve_step_one_kernel),
hoomd/md/TwoStepRATTLENVEGPU.cu:__global__ void gpu_rattle_nve_angular_step_one_kernel(Scalar4* d_orientation,
hoomd/md/TwoStepRATTLENVEGPU.cu:hipError_t gpu_rattle_nve_angular_step_one(Scalar4* d_orientation,
hoomd/md/TwoStepRATTLENVEGPU.cu:                                           const GPUPartition& gpu_partition,
hoomd/md/TwoStepRATTLENVEGPU.cu:    hipFuncGetAttributes(&attr, (const void*)gpu_rattle_nve_angular_step_one_kernel);
hoomd/md/TwoStepRATTLENVEGPU.cu:    // iterate over active GPUs in reverse, to end up on first GPU when returning from this function
hoomd/md/TwoStepRATTLENVEGPU.cu:    for (int idev = gpu_partition.getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/md/TwoStepRATTLENVEGPU.cu:        auto range = gpu_partition.getRangeAndSetGPU(idev);
hoomd/md/TwoStepRATTLENVEGPU.cu:        hipLaunchKernelGGL((gpu_rattle_nve_angular_step_one_kernel),
hoomd/md/TwoStepRATTLENVEGPU.cu:__global__ void gpu_rattle_nve_angular_step_two_kernel(const Scalar4* d_orientation,
hoomd/md/TwoStepRATTLENVEGPU.cu:hipError_t gpu_rattle_nve_angular_step_two(const Scalar4* d_orientation,
hoomd/md/TwoStepRATTLENVEGPU.cu:                                           const GPUPartition& gpu_partition,
hoomd/md/TwoStepRATTLENVEGPU.cu:    hipFuncGetAttributes(&attr, (const void*)gpu_rattle_nve_angular_step_two_kernel);
hoomd/md/TwoStepRATTLENVEGPU.cu:    // iterate over active GPUs in reverse, to end up on first GPU when returning from this function
hoomd/md/TwoStepRATTLENVEGPU.cu:    for (int idev = gpu_partition.getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/md/TwoStepRATTLENVEGPU.cu:        auto range = gpu_partition.getRangeAndSetGPU(idev);
hoomd/md/TwoStepRATTLENVEGPU.cu:        hipLaunchKernelGGL((gpu_rattle_nve_angular_step_two_kernel),
hoomd/md/EvaluatorPairDPDThermoDPD.h:        // CUDA memory hints
hoomd/md/EvaluatorPairGB.h:        //! Set CUDA memory hints
hoomd/md/EvaluatorPairGB.h:        //! Attach managed memory to CUDA stream
hoomd/md/CommunicatorGridGPU.cu:#include "CommunicatorGridGPU.cuh"
hoomd/md/CommunicatorGridGPU.cu:__global__ void gpu_gridcomm_scatter_send_cells_kernel(unsigned int n_send_cells,
hoomd/md/CommunicatorGridGPU.cu:__global__ void gpu_gridcomm_scatter_add_recv_cells_kernel(unsigned int n_unique_recv_cells,
hoomd/md/CommunicatorGridGPU.cu:void gpu_gridcomm_scatter_send_cells(unsigned int n_send_cells,
hoomd/md/CommunicatorGridGPU.cu:    hipLaunchKernelGGL((gpu_gridcomm_scatter_send_cells_kernel<T>),
hoomd/md/CommunicatorGridGPU.cu:void gpu_gridcomm_scatter_add_recv_cells(unsigned int n_unique_recv_cells,
hoomd/md/CommunicatorGridGPU.cu:        hipLaunchKernelGGL((gpu_gridcomm_scatter_add_recv_cells_kernel<T, true>),
hoomd/md/CommunicatorGridGPU.cu:        hipLaunchKernelGGL((gpu_gridcomm_scatter_add_recv_cells_kernel<T, false>),
hoomd/md/CommunicatorGridGPU.cu:template void gpu_gridcomm_scatter_send_cells<hipfftComplex>(unsigned int n_send_cells,
hoomd/md/CommunicatorGridGPU.cu:gpu_gridcomm_scatter_add_recv_cells<hipfftComplex>(unsigned int n_unique_recv_cells,
hoomd/md/CommunicatorGridGPU.cu:template void gpu_gridcomm_scatter_send_cells<Scalar>(unsigned int n_send_cells,
hoomd/md/CommunicatorGridGPU.cu:template void gpu_gridcomm_scatter_add_recv_cells<Scalar>(unsigned int n_unique_recv_cells,
hoomd/md/CommunicatorGridGPU.cu:template void gpu_gridcomm_scatter_send_cells<unsigned int>(unsigned int n_send_cells,
hoomd/md/CommunicatorGridGPU.cu:gpu_gridcomm_scatter_add_recv_cells<unsigned int>(unsigned int n_unique_recv_cells,
hoomd/md/external/field.py:            cls = getattr(self._ext_module, self._cpp_class_name + "GPU")
hoomd/md/external/wall.py:            cls = getattr(self._ext_module, self._cpp_class_name + "GPU")
hoomd/md/TwoStepRATTLEBDGPU.cuh:/*! \file TwoStepRATTLEBDGPU.cuh
hoomd/md/TwoStepRATTLEBDGPU.cuh:    \brief Declares GPU kernel code for Brownian dynamics on the GPU. Used by TwoStepRATTLEBDGPU.
hoomd/md/TwoStepRATTLEBDGPU.cuh:#include "hoomd/GPUPartition.cuh"
hoomd/md/TwoStepRATTLEBDGPU.cuh:#ifndef __TWO_STEP_RATTLE_BD_GPU_CUH__
hoomd/md/TwoStepRATTLEBDGPU.cuh:#define __TWO_STEP_RATTLE_BD_GPU_CUH__
hoomd/md/TwoStepRATTLEBDGPU.cuh://! Temporary holder struct to limit the number of arguments passed to gpu_rattle_bd_step_one()
hoomd/md/TwoStepRATTLEBDGPU.cuh:hipError_t gpu_rattle_brownian_step_one(Scalar4* d_pos,
hoomd/md/TwoStepRATTLEBDGPU.cuh:                                        const GPUPartition& gpu_partition);
hoomd/md/TwoStepRATTLEBDGPU.cuh:hipError_t gpu_include_rattle_force_bd(const Scalar4* d_pos,
hoomd/md/TwoStepRATTLEBDGPU.cuh:                                       const GPUPartition& gpu_partition);
hoomd/md/TwoStepRATTLEBDGPU.cuh:__global__ void gpu_rattle_brownian_step_one_kernel(Scalar4* d_pos,
hoomd/md/TwoStepRATTLEBDGPU.cuh:hipError_t gpu_rattle_brownian_step_one(Scalar4* d_pos,
hoomd/md/TwoStepRATTLEBDGPU.cuh:                                        const GPUPartition& gpu_partition)
hoomd/md/TwoStepRATTLEBDGPU.cuh:    // iterate over active GPUs in reverse, to end up on first GPU when returning from this function
hoomd/md/TwoStepRATTLEBDGPU.cuh:    for (int idev = gpu_partition.getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/md/TwoStepRATTLEBDGPU.cuh:        auto range = gpu_partition.getRangeAndSetGPU(idev);
hoomd/md/TwoStepRATTLEBDGPU.cuh:        hipLaunchKernelGGL((gpu_rattle_brownian_step_one_kernel<Manifold>),
hoomd/md/TwoStepRATTLEBDGPU.cuh:__global__ void gpu_include_rattle_force_bd_kernel(const Scalar4* d_pos,
hoomd/md/TwoStepRATTLEBDGPU.cuh:hipError_t gpu_include_rattle_force_bd(const Scalar4* d_pos,
hoomd/md/TwoStepRATTLEBDGPU.cuh:                                       const GPUPartition& gpu_partition)
hoomd/md/TwoStepRATTLEBDGPU.cuh:    // iterate over active GPUs in reverse, to end up on first GPU when returning from this function
hoomd/md/TwoStepRATTLEBDGPU.cuh:    for (int idev = gpu_partition.getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/md/TwoStepRATTLEBDGPU.cuh:        auto range = gpu_partition.getRangeAndSetGPU(idev);
hoomd/md/TwoStepRATTLEBDGPU.cuh:        hipLaunchKernelGGL((gpu_include_rattle_force_bd_kernel<Manifold>),
hoomd/md/TwoStepRATTLEBDGPU.cuh:#endif //__TWO_STEP_RATTLE_BD_GPU_CUH__
hoomd/md/HelfrichMeshForceComputeGPU.cc:#include "HelfrichMeshForceComputeGPU.h"
hoomd/md/HelfrichMeshForceComputeGPU.cc:/*! \file HelfrichMeshForceComputeGPU.cc
hoomd/md/HelfrichMeshForceComputeGPU.cc:    \brief Contains code for the HelfrichMeshForceComputeGPU class
hoomd/md/HelfrichMeshForceComputeGPU.cc:HelfrichMeshForceComputeGPU::HelfrichMeshForceComputeGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/md/HelfrichMeshForceComputeGPU.cc:    if (!m_exec_conf->isCUDAEnabled())
hoomd/md/HelfrichMeshForceComputeGPU.cc:            << "Creating a HelfrichMeshForceComputeGPU with no GPU in the execution configuration"
hoomd/md/HelfrichMeshForceComputeGPU.cc:        throw std::runtime_error("Error initializing HelfrichMeshForceComputeGPU");
hoomd/md/HelfrichMeshForceComputeGPU.cc:    GPUArray<Scalar> params(this->m_mesh_data->getMeshTriangleData()->getNTypes(),
hoomd/md/HelfrichMeshForceComputeGPU.cc:void HelfrichMeshForceComputeGPU::computeForces(uint64_t timestep)
hoomd/md/HelfrichMeshForceComputeGPU.cc:    const GPUArray<typename MeshBond::members_t>& gpu_meshbond_list
hoomd/md/HelfrichMeshForceComputeGPU.cc:        = this->m_mesh_data->getMeshBondData()->getGPUTable();
hoomd/md/HelfrichMeshForceComputeGPU.cc:    const Index2D& gpu_table_indexer = this->m_mesh_data->getMeshBondData()->getGPUTableIndexer();
hoomd/md/HelfrichMeshForceComputeGPU.cc:    ArrayHandle<typename MeshBond::members_t> d_gpu_meshbondlist(gpu_meshbond_list,
hoomd/md/HelfrichMeshForceComputeGPU.cc:    ArrayHandle<unsigned int> d_gpu_meshbond_pos_list(
hoomd/md/HelfrichMeshForceComputeGPU.cc:        this->m_mesh_data->getMeshBondData()->getGPUPosTable(),
hoomd/md/HelfrichMeshForceComputeGPU.cc:    ArrayHandle<unsigned int> d_gpu_n_meshbond(
hoomd/md/HelfrichMeshForceComputeGPU.cc:    kernel::gpu_compute_helfrich_force(d_force.data,
hoomd/md/HelfrichMeshForceComputeGPU.cc:                                       d_gpu_meshbondlist.data,
hoomd/md/HelfrichMeshForceComputeGPU.cc:                                       gpu_table_indexer,
hoomd/md/HelfrichMeshForceComputeGPU.cc:                                       d_gpu_meshbond_pos_list.data,
hoomd/md/HelfrichMeshForceComputeGPU.cc:                                       d_gpu_n_meshbond.data,
hoomd/md/HelfrichMeshForceComputeGPU.cc:    if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/HelfrichMeshForceComputeGPU.cc:        CHECK_CUDA_ERROR();
hoomd/md/HelfrichMeshForceComputeGPU.cc:void HelfrichMeshForceComputeGPU::computeSigma()
hoomd/md/HelfrichMeshForceComputeGPU.cc:    const GPUArray<typename MeshBond::members_t>& gpu_meshbond_list
hoomd/md/HelfrichMeshForceComputeGPU.cc:        = this->m_mesh_data->getMeshBondData()->getGPUTable();
hoomd/md/HelfrichMeshForceComputeGPU.cc:    const Index2D& gpu_table_indexer = this->m_mesh_data->getMeshBondData()->getGPUTableIndexer();
hoomd/md/HelfrichMeshForceComputeGPU.cc:    ArrayHandle<typename MeshBond::members_t> d_gpu_meshbondlist(gpu_meshbond_list,
hoomd/md/HelfrichMeshForceComputeGPU.cc:    ArrayHandle<unsigned int> d_gpu_meshbond_pos_list(
hoomd/md/HelfrichMeshForceComputeGPU.cc:        this->m_mesh_data->getMeshBondData()->getGPUPosTable(),
hoomd/md/HelfrichMeshForceComputeGPU.cc:    ArrayHandle<unsigned int> d_gpu_n_meshbond(
hoomd/md/HelfrichMeshForceComputeGPU.cc:    kernel::gpu_compute_helfrich_sigma(d_sigma.data,
hoomd/md/HelfrichMeshForceComputeGPU.cc:                                       d_gpu_meshbondlist.data,
hoomd/md/HelfrichMeshForceComputeGPU.cc:                                       gpu_table_indexer,
hoomd/md/HelfrichMeshForceComputeGPU.cc:                                       d_gpu_meshbond_pos_list.data,
hoomd/md/HelfrichMeshForceComputeGPU.cc:                                       d_gpu_n_meshbond.data,
hoomd/md/HelfrichMeshForceComputeGPU.cc:    if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/HelfrichMeshForceComputeGPU.cc:        CHECK_CUDA_ERROR();
hoomd/md/HelfrichMeshForceComputeGPU.cc:void export_HelfrichMeshForceComputeGPU(pybind11::module& m)
hoomd/md/HelfrichMeshForceComputeGPU.cc:    pybind11::class_<HelfrichMeshForceComputeGPU,
hoomd/md/HelfrichMeshForceComputeGPU.cc:                     std::shared_ptr<HelfrichMeshForceComputeGPU>>(m, "HelfrichMeshForceComputeGPU")
hoomd/md/ActiveForceCompute.cc:    if (m_exec_conf->isCUDAEnabled() && m_exec_conf->allConcurrentManagedAccess())
hoomd/md/ActiveForceCompute.cc:        cudaMemAdvise(m_f_activeVec.get(),
hoomd/md/ActiveForceCompute.cc:                      cudaMemAdviseSetReadMostly,
hoomd/md/ActiveForceCompute.cc:        cudaMemAdvise(m_t_activeVec.get(),
hoomd/md/ActiveForceCompute.cc:                      cudaMemAdviseSetReadMostly,
hoomd/md/ActiveForceCompute.cc:    if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/ActiveForceCompute.cc:        CHECK_CUDA_ERROR();
hoomd/md/NeighborListGPUStencil.h:#include "NeighborListGPU.h"
hoomd/md/NeighborListGPUStencil.h:#include "hoomd/CellListGPU.h"
hoomd/md/NeighborListGPUStencil.h:/*! \file NeighborListGPUStencil.h
hoomd/md/NeighborListGPUStencil.h:    \brief Declares the NeighborListGPUStencil class
hoomd/md/NeighborListGPUStencil.h:#ifndef __NEIGHBORLISTGPUSTENCIL_H__
hoomd/md/NeighborListGPUStencil.h:#define __NEIGHBORLISTGPUSTENCIL_H__
hoomd/md/NeighborListGPUStencil.h://! Neighbor list build on the GPU with multiple bin stencils
hoomd/md/NeighborListGPUStencil.h:/*! Implements the O(N) neighbor list build on the GPU using a cell list with multiple bin stencils.
hoomd/md/NeighborListGPUStencil.h:    GPU kernel methods are defined in NeighborListGPUStencil.cuh and defined in
hoomd/md/NeighborListGPUStencil.h:   NeighborListGPUStencil.cu.
hoomd/md/NeighborListGPUStencil.h:class PYBIND11_EXPORT NeighborListGPUStencil : public NeighborListGPU
hoomd/md/NeighborListGPUStencil.h:    NeighborListGPUStencil(std::shared_ptr<SystemDefinition> sysdef, Scalar r_buff);
hoomd/md/NeighborListGPUStencil.h:    virtual ~NeighborListGPUStencil();
hoomd/md/NeighborListGPUStencil.h:        NeighborListGPU::notifyRCutMatrixChange();
hoomd/md/NeighborListGPUStencil.h:    GPUArray<unsigned int> m_pid_map; //!< Particle indexes sorted by type
hoomd/md/NeighborListGPUStencil.h:#endif // __NEIGHBORLISTGPUSTENCIL_H__
hoomd/md/AreaConservationMeshForceComputeGPU.cc:#include "AreaConservationMeshForceComputeGPU.h"
hoomd/md/AreaConservationMeshForceComputeGPU.cc:/*! \file AreaConservationMeshForceComputeGPU.cc
hoomd/md/AreaConservationMeshForceComputeGPU.cc:    \brief Contains code for the AreaConservationMeshForceComputeGPU class
hoomd/md/AreaConservationMeshForceComputeGPU.cc:AreaConservationMeshForceComputeGPU::AreaConservationMeshForceComputeGPU(
hoomd/md/AreaConservationMeshForceComputeGPU.cc:    if (!m_exec_conf->isCUDAEnabled())
hoomd/md/AreaConservationMeshForceComputeGPU.cc:        m_exec_conf->msg->error() << "Creating a AreaConservationMeshForceComputeGPU with no GPU "
hoomd/md/AreaConservationMeshForceComputeGPU.cc:        throw std::runtime_error("Error initializing AreaConservationMeshForceComputeGPU");
hoomd/md/AreaConservationMeshForceComputeGPU.cc:    GPUArray<Scalar> sum(NTypes, m_exec_conf);
hoomd/md/AreaConservationMeshForceComputeGPU.cc:    GPUArray<Scalar> partial_sum(m_num_blocks, m_exec_conf);
hoomd/md/AreaConservationMeshForceComputeGPU.cc:void AreaConservationMeshForceComputeGPU::computeForces(uint64_t timestep)
hoomd/md/AreaConservationMeshForceComputeGPU.cc:    const GPUArray<typename Angle::members_t>& gpu_meshtriangle_list
hoomd/md/AreaConservationMeshForceComputeGPU.cc:        = this->m_mesh_data->getMeshTriangleData()->getGPUTable();
hoomd/md/AreaConservationMeshForceComputeGPU.cc:    const Index2D& gpu_table_indexer
hoomd/md/AreaConservationMeshForceComputeGPU.cc:        = this->m_mesh_data->getMeshTriangleData()->getGPUTableIndexer();
hoomd/md/AreaConservationMeshForceComputeGPU.cc:    ArrayHandle<typename Angle::members_t> d_gpu_meshtrianglelist(gpu_meshtriangle_list,
hoomd/md/AreaConservationMeshForceComputeGPU.cc:    ArrayHandle<unsigned int> d_gpu_meshtriangle_pos_list(
hoomd/md/AreaConservationMeshForceComputeGPU.cc:        m_mesh_data->getMeshTriangleData()->getGPUPosTable(),
hoomd/md/AreaConservationMeshForceComputeGPU.cc:    ArrayHandle<unsigned int> d_gpu_n_meshtriangle(
hoomd/md/AreaConservationMeshForceComputeGPU.cc:    kernel::gpu_compute_area_constraint_force(d_force.data,
hoomd/md/AreaConservationMeshForceComputeGPU.cc:                                              d_gpu_meshtrianglelist.data,
hoomd/md/AreaConservationMeshForceComputeGPU.cc:                                              d_gpu_meshtriangle_pos_list.data,
hoomd/md/AreaConservationMeshForceComputeGPU.cc:                                              gpu_table_indexer,
hoomd/md/AreaConservationMeshForceComputeGPU.cc:                                              d_gpu_n_meshtriangle.data,
hoomd/md/AreaConservationMeshForceComputeGPU.cc:    if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/AreaConservationMeshForceComputeGPU.cc:        CHECK_CUDA_ERROR();
hoomd/md/AreaConservationMeshForceComputeGPU.cc:void AreaConservationMeshForceComputeGPU::precomputeParameter()
hoomd/md/AreaConservationMeshForceComputeGPU.cc:    const GPUArray<typename Angle::members_t>& gpu_meshtriangle_list
hoomd/md/AreaConservationMeshForceComputeGPU.cc:        = this->m_mesh_data->getMeshTriangleData()->getGPUTable();
hoomd/md/AreaConservationMeshForceComputeGPU.cc:    const Index2D& gpu_table_indexer
hoomd/md/AreaConservationMeshForceComputeGPU.cc:        = this->m_mesh_data->getMeshTriangleData()->getGPUTableIndexer();
hoomd/md/AreaConservationMeshForceComputeGPU.cc:    ArrayHandle<typename Angle::members_t> d_gpu_meshtrianglelist(gpu_meshtriangle_list,
hoomd/md/AreaConservationMeshForceComputeGPU.cc:    ArrayHandle<unsigned int> d_gpu_meshtriangle_pos_list(
hoomd/md/AreaConservationMeshForceComputeGPU.cc:        m_mesh_data->getMeshTriangleData()->getGPUPosTable(),
hoomd/md/AreaConservationMeshForceComputeGPU.cc:    ArrayHandle<unsigned int> d_gpu_n_meshtriangle(
hoomd/md/AreaConservationMeshForceComputeGPU.cc:    kernel::gpu_compute_area_constraint_area(d_sumArea.data,
hoomd/md/AreaConservationMeshForceComputeGPU.cc:                                             d_gpu_meshtrianglelist.data,
hoomd/md/AreaConservationMeshForceComputeGPU.cc:                                             d_gpu_meshtriangle_pos_list.data,
hoomd/md/AreaConservationMeshForceComputeGPU.cc:                                             gpu_table_indexer,
hoomd/md/AreaConservationMeshForceComputeGPU.cc:                                             d_gpu_n_meshtriangle.data,
hoomd/md/AreaConservationMeshForceComputeGPU.cc:    if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/AreaConservationMeshForceComputeGPU.cc:        CHECK_CUDA_ERROR();
hoomd/md/AreaConservationMeshForceComputeGPU.cc:void export_AreaConservationMeshForceComputeGPU(pybind11::module& m)
hoomd/md/AreaConservationMeshForceComputeGPU.cc:    pybind11::class_<AreaConservationMeshForceComputeGPU,
hoomd/md/AreaConservationMeshForceComputeGPU.cc:                     std::shared_ptr<AreaConservationMeshForceComputeGPU>>(
hoomd/md/AreaConservationMeshForceComputeGPU.cc:        "AreaConservationMeshForceComputeGPU")
hoomd/md/ConstantForceComputeGPU.h:/*! \file ConstantForceComputeGPU.h
hoomd/md/ConstantForceComputeGPU.h:    \brief Declares a class for computing constant forces on the GPU
hoomd/md/ConstantForceComputeGPU.h:#ifndef __CONSTANTFORCECOMPUTE_GPU_H__
hoomd/md/ConstantForceComputeGPU.h:#define __CONSTANTFORCECOMPUTE_GPU_H__
hoomd/md/ConstantForceComputeGPU.h://! Adds a constant force to a number of particles on the GPU
hoomd/md/ConstantForceComputeGPU.h:class PYBIND11_EXPORT ConstantForceComputeGPU : public ConstantForceCompute
hoomd/md/ConstantForceComputeGPU.h:    ConstantForceComputeGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/md/ComputeThermoHMAGPU.cc:/*! \file ComputeThermoHMAGPU.cc
hoomd/md/ComputeThermoHMAGPU.cc:    \brief Contains code for the ComputeThermoHMAGPU class
hoomd/md/ComputeThermoHMAGPU.cc:#include "ComputeThermoHMAGPU.h"
hoomd/md/ComputeThermoHMAGPU.cc:#include "ComputeThermoHMAGPU.cuh"
hoomd/md/ComputeThermoHMAGPU.cc:#include "hoomd/GPUPartition.cuh"
hoomd/md/ComputeThermoHMAGPU.cc:ComputeThermoHMAGPU::ComputeThermoHMAGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/md/ComputeThermoHMAGPU.cc:    if (!m_exec_conf->isCUDAEnabled())
hoomd/md/ComputeThermoHMAGPU.cc:            << "Creating a ComputeThermoHMAGPU with no GPU in the execution configuration" << endl;
hoomd/md/ComputeThermoHMAGPU.cc:        throw std::runtime_error("Error initializing ComputeThermoHMAGPU");
hoomd/md/ComputeThermoHMAGPU.cc:        auto gpu_map = m_exec_conf->getGPUIds();
hoomd/md/ComputeThermoHMAGPU.cc:        // set up GPU memory mappings
hoomd/md/ComputeThermoHMAGPU.cc:        for (unsigned int idev = 0; idev < m_exec_conf->getNumActiveGPUs(); ++idev)
hoomd/md/ComputeThermoHMAGPU.cc:            cudaMemAdvise(m_lattice_site.get(),
hoomd/md/ComputeThermoHMAGPU.cc:                          cudaMemAdviseSetAccessedBy,
hoomd/md/ComputeThermoHMAGPU.cc:                          gpu_map[idev]);
hoomd/md/ComputeThermoHMAGPU.cc:        CHECK_CUDA_ERROR();
hoomd/md/ComputeThermoHMAGPU.cc:ComputeThermoHMAGPU::~ComputeThermoHMAGPU()
hoomd/md/ComputeThermoHMAGPU.cc:/*! Computes all thermodynamic properties of the system in one fell swoop, on the GPU.
hoomd/md/ComputeThermoHMAGPU.cc:void ComputeThermoHMAGPU::computeProperties()
hoomd/md/ComputeThermoHMAGPU.cc:    // number of blocks in reduction (round up for every GPU)
hoomd/md/ComputeThermoHMAGPU.cc:        = m_group->getNumMembers() / m_block_size + m_exec_conf->getNumActiveGPUs();
hoomd/md/ComputeThermoHMAGPU.cc:            auto& gpu_map = m_exec_conf->getGPUIds();
hoomd/md/ComputeThermoHMAGPU.cc:            // map scratch array into memory of all GPUs
hoomd/md/ComputeThermoHMAGPU.cc:            for (unsigned int idev = 0; idev < m_exec_conf->getNumActiveGPUs(); ++idev)
hoomd/md/ComputeThermoHMAGPU.cc:                cudaMemAdvise(m_scratch.get(),
hoomd/md/ComputeThermoHMAGPU.cc:                              cudaMemAdviseSetAccessedBy,
hoomd/md/ComputeThermoHMAGPU.cc:                              gpu_map[idev]);
hoomd/md/ComputeThermoHMAGPU.cc:            CHECK_CUDA_ERROR();
hoomd/md/ComputeThermoHMAGPU.cc:        m_exec_conf->beginMultiGPU();
hoomd/md/ComputeThermoHMAGPU.cc:        // perform the computation on the GPU(s)
hoomd/md/ComputeThermoHMAGPU.cc:        gpu_compute_thermo_hma_partial(d_pos.data,
hoomd/md/ComputeThermoHMAGPU.cc:                                       m_group->getGPUPartition());
hoomd/md/ComputeThermoHMAGPU.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/ComputeThermoHMAGPU.cc:            CHECK_CUDA_ERROR();
hoomd/md/ComputeThermoHMAGPU.cc:        // converge GPUs
hoomd/md/ComputeThermoHMAGPU.cc:        m_exec_conf->endMultiGPU();
hoomd/md/ComputeThermoHMAGPU.cc:        // perform the computation on GPU 0
hoomd/md/ComputeThermoHMAGPU.cc:        gpu_compute_thermo_hma_final(d_properties.data,
hoomd/md/ComputeThermoHMAGPU.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/ComputeThermoHMAGPU.cc:            CHECK_CUDA_ERROR();
hoomd/md/ComputeThermoHMAGPU.cc:void export_ComputeThermoHMAGPU(pybind11::module& m)
hoomd/md/ComputeThermoHMAGPU.cc:    pybind11::class_<ComputeThermoHMAGPU, ComputeThermoHMA, std::shared_ptr<ComputeThermoHMAGPU>>(
hoomd/md/ComputeThermoHMAGPU.cc:        "ComputeThermoHMAGPU")
hoomd/md/BendingRigidityMeshForceComputeGPU.cuh:/*! \file BendingRigidityMeshForceComputeGPU.cuh
hoomd/md/BendingRigidityMeshForceComputeGPU.cuh:    \brief Declares GPU kernel code for calculating the bending rigidity forces. Used by
hoomd/md/BendingRigidityMeshForceComputeGPU.cuh:   BendingRigidityMeshForceComputeGPU.
hoomd/md/BendingRigidityMeshForceComputeGPU.cuh://! Kernel driver that computes the forces for BendingRigidityMeshForceComputeGPU
hoomd/md/BendingRigidityMeshForceComputeGPU.cuh:hipError_t gpu_compute_bending_rigidity_force(Scalar4* d_force,
hoomd/md/CosineSqAngleForceComputeGPU.h:#include "CosineSqAngleForceGPU.cuh"
hoomd/md/CosineSqAngleForceComputeGPU.h:/*! \file CosineSqAngleForceComputeGPU.h
hoomd/md/CosineSqAngleForceComputeGPU.h:    \brief Declares the CosineSqAngleForceGPU class
hoomd/md/CosineSqAngleForceComputeGPU.h:#ifndef __COSINESQANGLEFORCECOMPUTEGPU_H__
hoomd/md/CosineSqAngleForceComputeGPU.h:#define __COSINESQANGLEFORCECOMPUTEGPU_H__
hoomd/md/CosineSqAngleForceComputeGPU.h://! Implements the cosine squared angle force calculation on the GPU
hoomd/md/CosineSqAngleForceComputeGPU.h:/*! CosineSqAngleForceComputeGPU implements the same calculations as CosineSqAngleForceCompute,
hoomd/md/CosineSqAngleForceComputeGPU.h:    but executing on the GPU.
hoomd/md/CosineSqAngleForceComputeGPU.h:    \a m_gpu_params. They are stored as Scalar2's with the \a x component being K and the
hoomd/md/CosineSqAngleForceComputeGPU.h:    The GPU kernel can be found in angleforce_kernel.cu.
hoomd/md/CosineSqAngleForceComputeGPU.h:class PYBIND11_EXPORT CosineSqAngleForceComputeGPU : public CosineSqAngleForceCompute
hoomd/md/CosineSqAngleForceComputeGPU.h:    CosineSqAngleForceComputeGPU(std::shared_ptr<SystemDefinition> sysdef);
hoomd/md/CosineSqAngleForceComputeGPU.h:    ~CosineSqAngleForceComputeGPU();
hoomd/md/CosineSqAngleForceComputeGPU.h:    GPUArray<Scalar2> m_params;            //!< Parameters stored on the GPU
hoomd/md/HelfrichMeshForceComputeGPU.cuh:/*! \file HelfrichMeshForceComputeGPU.cuh
hoomd/md/HelfrichMeshForceComputeGPU.cuh:    \brief Declares GPU kernel code for calculating the helfrich forces. Used by
hoomd/md/HelfrichMeshForceComputeGPU.cuh:   HelfrichMeshForceComputeGPU.
hoomd/md/HelfrichMeshForceComputeGPU.cuh://! Kernel driver that computes the sigmas for HelfrichMeshForceComputeGPU
hoomd/md/HelfrichMeshForceComputeGPU.cuh:hipError_t gpu_compute_helfrich_sigma(Scalar* d_sigma,
hoomd/md/HelfrichMeshForceComputeGPU.cuh://! Kernel driver that computes the forces for HelfrichMeshForceComputeGPU
hoomd/md/HelfrichMeshForceComputeGPU.cuh:hipError_t gpu_compute_helfrich_force(Scalar4* d_force,
hoomd/md/HarmonicDihedralForceComputeGPU.h:#include "HarmonicDihedralForceGPU.cuh"
hoomd/md/HarmonicDihedralForceComputeGPU.h:/*! \file HarmonicDihedralForceComputeGPU.h
hoomd/md/HarmonicDihedralForceComputeGPU.h:    \brief Declares the HarmonicDihedralForceGPU class
hoomd/md/HarmonicDihedralForceComputeGPU.h:#ifndef __HARMONICDIHEDRALFORCECOMPUTEGPU_H__
hoomd/md/HarmonicDihedralForceComputeGPU.h:#define __HARMONICDIHEDRALFORCECOMPUTEGPU_H__
hoomd/md/HarmonicDihedralForceComputeGPU.h://! Implements the harmonic dihedral force calculation on the GPU
hoomd/md/HarmonicDihedralForceComputeGPU.h:/*! HarmonicDihedralForceComputeGPU implements the same calculations as
hoomd/md/HarmonicDihedralForceComputeGPU.h:   HarmonicDihedralForceCompute, but executing on the GPU.
hoomd/md/HarmonicDihedralForceComputeGPU.h:    \a m_gpu_params. They are stored as Scalar2's with the \a x component being K and the
hoomd/md/HarmonicDihedralForceComputeGPU.h:    The GPU kernel can be found in dihedralforce_kernel.cu.
hoomd/md/HarmonicDihedralForceComputeGPU.h:class PYBIND11_EXPORT HarmonicDihedralForceComputeGPU : public HarmonicDihedralForceCompute
hoomd/md/HarmonicDihedralForceComputeGPU.h:    HarmonicDihedralForceComputeGPU(std::shared_ptr<SystemDefinition> system);
hoomd/md/HarmonicDihedralForceComputeGPU.h:    ~HarmonicDihedralForceComputeGPU();
hoomd/md/HarmonicDihedralForceComputeGPU.h:    GPUArray<Scalar4> m_params;            //!< Parameters stored on the GPU (k,sign,m)
hoomd/md/export_PotentialPairGPU.cc.inc:#include "hoomd/md/PotentialPairGPU.h"
hoomd/md/export_PotentialPairGPU.cc.inc:#define EXPORT_FUNCTION export_PotentialPair@_evaluator@GPU
hoomd/md/export_PotentialPairGPU.cc.inc:    export_PotentialPairGPU<EVALUATOR_CLASS>(m, "PotentialPair@_evaluator@GPU");
hoomd/md/ForceDistanceConstraintGPU.cu:#include "ForceDistanceConstraintGPU.cuh"
hoomd/md/ForceDistanceConstraintGPU.cu:/*! \file ForceDistanceConstraintGPU.cu
hoomd/md/ForceDistanceConstraintGPU.cu:    \brief Defines GPU kernel code for pairwise distance constraints on the GPU
hoomd/md/ForceDistanceConstraintGPU.cu:__global__ void gpu_fill_matrix_vector_kernel(unsigned int n_constraint,
hoomd/md/ForceDistanceConstraintGPU.cu:                                              const group_storage<2>* d_gpu_clist,
hoomd/md/ForceDistanceConstraintGPU.cu:                                              const Index2D gpu_clist_indexer,
hoomd/md/ForceDistanceConstraintGPU.cu:                                              const unsigned int* d_gpu_n_constraints,
hoomd/md/ForceDistanceConstraintGPU.cu:                                              const unsigned int* d_gpu_cpos,
hoomd/md/ForceDistanceConstraintGPU.cu:    unsigned int n_constraint_ptl = d_gpu_n_constraints[idx];
hoomd/md/ForceDistanceConstraintGPU.cu:        group_storage<2> cur_constraint_i = d_gpu_clist[gpu_clist_indexer(idx, cidx_i)];
hoomd/md/ForceDistanceConstraintGPU.cu:        unsigned int cpos = d_gpu_cpos[gpu_clist_indexer(idx, cidx_i)];
hoomd/md/ForceDistanceConstraintGPU.cu:        unsigned int n_constraint_i = d_gpu_n_constraints[cur_constraint_idx_i];
hoomd/md/ForceDistanceConstraintGPU.cu:                = d_gpu_clist[gpu_clist_indexer(cur_constraint_idx_i, cidx_j)];
hoomd/md/ForceDistanceConstraintGPU.cu:            if (d_gpu_cpos[gpu_clist_indexer(cur_constraint_idx_i, cidx_j)] == 0)
hoomd/md/ForceDistanceConstraintGPU.cu:hipError_t gpu_fill_matrix_vector(unsigned int n_constraint,
hoomd/md/ForceDistanceConstraintGPU.cu:                                  const group_storage<2>* d_gpu_clist,
hoomd/md/ForceDistanceConstraintGPU.cu:                                  const Index2D& gpu_clist_indexer,
hoomd/md/ForceDistanceConstraintGPU.cu:                                  const unsigned int* d_gpu_n_constraints,
hoomd/md/ForceDistanceConstraintGPU.cu:                                  const unsigned int* d_gpu_cpos,
hoomd/md/ForceDistanceConstraintGPU.cu:    hipFuncGetAttributes(&attr, (const void*)gpu_fill_matrix_vector_kernel);
hoomd/md/ForceDistanceConstraintGPU.cu:    // run GPU kernel
hoomd/md/ForceDistanceConstraintGPU.cu:    hipLaunchKernelGGL((gpu_fill_matrix_vector_kernel),
hoomd/md/ForceDistanceConstraintGPU.cu:                       d_gpu_clist,
hoomd/md/ForceDistanceConstraintGPU.cu:                       gpu_clist_indexer,
hoomd/md/ForceDistanceConstraintGPU.cu:                       d_gpu_n_constraints,
hoomd/md/ForceDistanceConstraintGPU.cu:                       d_gpu_cpos,
hoomd/md/ForceDistanceConstraintGPU.cu:__global__ void gpu_fill_constraint_forces_kernel(unsigned int nptl_local,
hoomd/md/ForceDistanceConstraintGPU.cu:                                                  const group_storage<2>* d_gpu_clist,
hoomd/md/ForceDistanceConstraintGPU.cu:                                                  const Index2D gpu_clist_indexer,
hoomd/md/ForceDistanceConstraintGPU.cu:                                                  const unsigned int* d_gpu_n_constraints,
hoomd/md/ForceDistanceConstraintGPU.cu:                                                  const unsigned int* d_gpu_cpos,
hoomd/md/ForceDistanceConstraintGPU.cu:    unsigned int n_constraint_ptl = d_gpu_n_constraints[idx];
hoomd/md/ForceDistanceConstraintGPU.cu:        group_storage<2> cur_constraint = d_gpu_clist[gpu_clist_indexer(idx, cidx)];
hoomd/md/ForceDistanceConstraintGPU.cu:        unsigned int cpos = d_gpu_cpos[gpu_clist_indexer(idx, cidx)];
hoomd/md/ForceDistanceConstraintGPU.cu:hipError_t gpu_count_nnz(unsigned int n_constraint,
hoomd/md/ForceDistanceConstraintGPU.cu:hipError_t gpu_dense2sparse(unsigned int n_constraint,
hoomd/md/ForceDistanceConstraintGPU.cu:hipError_t gpu_compute_constraint_forces(const Scalar4* d_pos,
hoomd/md/ForceDistanceConstraintGPU.cu:                                         const group_storage<2>* d_gpu_clist,
hoomd/md/ForceDistanceConstraintGPU.cu:                                         const Index2D& gpu_clist_indexer,
hoomd/md/ForceDistanceConstraintGPU.cu:                                         const unsigned int* d_gpu_n_constraints,
hoomd/md/ForceDistanceConstraintGPU.cu:                                         const unsigned int* d_gpu_cpos,
hoomd/md/ForceDistanceConstraintGPU.cu:    hipFuncGetAttributes(&attr, (const void*)gpu_fill_constraint_forces_kernel);
hoomd/md/ForceDistanceConstraintGPU.cu:    hipLaunchKernelGGL((gpu_fill_constraint_forces_kernel),
hoomd/md/ForceDistanceConstraintGPU.cu:                       d_gpu_clist,
hoomd/md/ForceDistanceConstraintGPU.cu:                       gpu_clist_indexer,
hoomd/md/ForceDistanceConstraintGPU.cu:                       d_gpu_n_constraints,
hoomd/md/ForceDistanceConstraintGPU.cu:                       d_gpu_cpos,
hoomd/md/VolumeConservationMeshForceComputeGPU.cc:#include "VolumeConservationMeshForceComputeGPU.h"
hoomd/md/VolumeConservationMeshForceComputeGPU.cc:/*! \file VolumeConservationMeshForceComputeGPU.cc
hoomd/md/VolumeConservationMeshForceComputeGPU.cc:    \brief Contains code for the VolumeConservationMeshForceComputeGPU class
hoomd/md/VolumeConservationMeshForceComputeGPU.cc:VolumeConservationMeshForceComputeGPU::VolumeConservationMeshForceComputeGPU(
hoomd/md/VolumeConservationMeshForceComputeGPU.cc:    if (!m_exec_conf->isCUDAEnabled())
hoomd/md/VolumeConservationMeshForceComputeGPU.cc:        m_exec_conf->msg->error() << "Creating a VolumeConservationMeshForceComputeGPU with no GPU "
hoomd/md/VolumeConservationMeshForceComputeGPU.cc:        throw std::runtime_error("Error initializing VolumeConservationMeshForceComputeGPU");
hoomd/md/VolumeConservationMeshForceComputeGPU.cc:    GPUArray<Scalar> sum(NTypes, m_exec_conf);
hoomd/md/VolumeConservationMeshForceComputeGPU.cc:    GPUArray<Scalar> partial_sum(m_num_blocks, m_exec_conf);
hoomd/md/VolumeConservationMeshForceComputeGPU.cc:void VolumeConservationMeshForceComputeGPU::computeForces(uint64_t timestep)
hoomd/md/VolumeConservationMeshForceComputeGPU.cc:    const GPUArray<typename Angle::members_t>& gpu_meshtriangle_list
hoomd/md/VolumeConservationMeshForceComputeGPU.cc:        = this->m_mesh_data->getMeshTriangleData()->getGPUTable();
hoomd/md/VolumeConservationMeshForceComputeGPU.cc:    const Index2D& gpu_table_indexer
hoomd/md/VolumeConservationMeshForceComputeGPU.cc:        = this->m_mesh_data->getMeshTriangleData()->getGPUTableIndexer();
hoomd/md/VolumeConservationMeshForceComputeGPU.cc:    ArrayHandle<typename Angle::members_t> d_gpu_meshtrianglelist(gpu_meshtriangle_list,
hoomd/md/VolumeConservationMeshForceComputeGPU.cc:    ArrayHandle<unsigned int> d_gpu_meshtriangle_pos_list(
hoomd/md/VolumeConservationMeshForceComputeGPU.cc:        m_mesh_data->getMeshTriangleData()->getGPUPosTable(),
hoomd/md/VolumeConservationMeshForceComputeGPU.cc:    ArrayHandle<unsigned int> d_gpu_n_meshtriangle(
hoomd/md/VolumeConservationMeshForceComputeGPU.cc:    kernel::gpu_compute_volume_constraint_force(d_force.data,
hoomd/md/VolumeConservationMeshForceComputeGPU.cc:                                                d_gpu_meshtrianglelist.data,
hoomd/md/VolumeConservationMeshForceComputeGPU.cc:                                                d_gpu_meshtriangle_pos_list.data,
hoomd/md/VolumeConservationMeshForceComputeGPU.cc:                                                gpu_table_indexer,
hoomd/md/VolumeConservationMeshForceComputeGPU.cc:                                                d_gpu_n_meshtriangle.data,
hoomd/md/VolumeConservationMeshForceComputeGPU.cc:    if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/VolumeConservationMeshForceComputeGPU.cc:        CHECK_CUDA_ERROR();
hoomd/md/VolumeConservationMeshForceComputeGPU.cc:void VolumeConservationMeshForceComputeGPU::computeVolume()
hoomd/md/VolumeConservationMeshForceComputeGPU.cc:    const GPUArray<typename Angle::members_t>& gpu_meshtriangle_list
hoomd/md/VolumeConservationMeshForceComputeGPU.cc:        = this->m_mesh_data->getMeshTriangleData()->getGPUTable();
hoomd/md/VolumeConservationMeshForceComputeGPU.cc:    const Index2D& gpu_table_indexer
hoomd/md/VolumeConservationMeshForceComputeGPU.cc:        = this->m_mesh_data->getMeshTriangleData()->getGPUTableIndexer();
hoomd/md/VolumeConservationMeshForceComputeGPU.cc:    ArrayHandle<typename Angle::members_t> d_gpu_meshtrianglelist(gpu_meshtriangle_list,
hoomd/md/VolumeConservationMeshForceComputeGPU.cc:    ArrayHandle<unsigned int> d_gpu_meshtriangle_pos_list(
hoomd/md/VolumeConservationMeshForceComputeGPU.cc:        m_mesh_data->getMeshTriangleData()->getGPUPosTable(),
hoomd/md/VolumeConservationMeshForceComputeGPU.cc:    ArrayHandle<unsigned int> d_gpu_n_meshtriangle(
hoomd/md/VolumeConservationMeshForceComputeGPU.cc:    kernel::gpu_compute_volume_constraint_volume(d_sumVol.data,
hoomd/md/VolumeConservationMeshForceComputeGPU.cc:                                                 d_gpu_meshtrianglelist.data,
hoomd/md/VolumeConservationMeshForceComputeGPU.cc:                                                 d_gpu_meshtriangle_pos_list.data,
hoomd/md/VolumeConservationMeshForceComputeGPU.cc:                                                 gpu_table_indexer,
hoomd/md/VolumeConservationMeshForceComputeGPU.cc:                                                 d_gpu_n_meshtriangle.data,
hoomd/md/VolumeConservationMeshForceComputeGPU.cc:    if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/VolumeConservationMeshForceComputeGPU.cc:        CHECK_CUDA_ERROR();
hoomd/md/VolumeConservationMeshForceComputeGPU.cc:void export_VolumeConservationMeshForceComputeGPU(pybind11::module& m)
hoomd/md/VolumeConservationMeshForceComputeGPU.cc:    pybind11::class_<VolumeConservationMeshForceComputeGPU,
hoomd/md/VolumeConservationMeshForceComputeGPU.cc:                     std::shared_ptr<VolumeConservationMeshForceComputeGPU>>(
hoomd/md/VolumeConservationMeshForceComputeGPU.cc:        "VolumeConservationMeshForceComputeGPU")
hoomd/md/NeighborListGPUStencil.cc:/*! \file NeighborListGPUStencil.cc
hoomd/md/NeighborListGPUStencil.cc:    \brief Defines NeighborListGPUStencil
hoomd/md/NeighborListGPUStencil.cc:#include "NeighborListGPUStencil.h"
hoomd/md/NeighborListGPUStencil.cc:#include "NeighborListGPUStencil.cuh"
hoomd/md/NeighborListGPUStencil.cc:NeighborListGPUStencil::NeighborListGPUStencil(std::shared_ptr<SystemDefinition> sysdef,
hoomd/md/NeighborListGPUStencil.cc:    : NeighborListGPU(sysdef, r_buff), m_cl(std::make_shared<CellListGPU>(sysdef)),
hoomd/md/NeighborListGPUStencil.cc:    m_exec_conf->msg->notice(5) << "Constructing NeighborListGPUStencil" << std::endl;
hoomd/md/NeighborListGPUStencil.cc:    CHECK_CUDA_ERROR();
hoomd/md/NeighborListGPUStencil.cc:        .connect<NeighborListGPUStencil, &NeighborListGPUStencil::slotMaxNumChanged>(this);
hoomd/md/NeighborListGPUStencil.cc:        .connect<NeighborListGPUStencil, &NeighborListGPUStencil::slotParticleSort>(this);
hoomd/md/NeighborListGPUStencil.cc:    GPUArray<unsigned int> pid_map(m_pdata->getMaxN(), m_exec_conf);
hoomd/md/NeighborListGPUStencil.cc:NeighborListGPUStencil::~NeighborListGPUStencil()
hoomd/md/NeighborListGPUStencil.cc:    m_exec_conf->msg->notice(5) << "Destroying NeighborListGPUStencil" << std::endl;
hoomd/md/NeighborListGPUStencil.cc:        .disconnect<NeighborListGPUStencil, &NeighborListGPUStencil::slotMaxNumChanged>(this);
hoomd/md/NeighborListGPUStencil.cc:        .disconnect<NeighborListGPUStencil, &NeighborListGPUStencil::slotParticleSort>(this);
hoomd/md/NeighborListGPUStencil.cc:void NeighborListGPUStencil::updateRStencil()
hoomd/md/NeighborListGPUStencil.cc:void NeighborListGPUStencil::sortTypes()
hoomd/md/NeighborListGPUStencil.cc:    kernel::gpu_compute_nlist_stencil_fill_types(d_pids.data,
hoomd/md/NeighborListGPUStencil.cc:        kernel::gpu_compute_nlist_stencil_sort_types(d_pids.data,
hoomd/md/NeighborListGPUStencil.cc:        kernel::gpu_compute_nlist_stencil_sort_types(d_pids.data,
hoomd/md/NeighborListGPUStencil.cc:void NeighborListGPUStencil::buildNlist(uint64_t timestep)
hoomd/md/NeighborListGPUStencil.cc:        throw std::runtime_error("GPU neighbor lists require a full storage mode.");
hoomd/md/NeighborListGPUStencil.cc:    kernel::gpu_compute_nlist_stencil(d_nlist.data,
hoomd/md/NeighborListGPUStencil.cc:    if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/NeighborListGPUStencil.cc:        CHECK_CUDA_ERROR();
hoomd/md/NeighborListGPUStencil.cc:void export_NeighborListGPUStencil(pybind11::module& m)
hoomd/md/NeighborListGPUStencil.cc:    pybind11::class_<NeighborListGPUStencil,
hoomd/md/NeighborListGPUStencil.cc:                     NeighborListGPU,
hoomd/md/NeighborListGPUStencil.cc:                     std::shared_ptr<NeighborListGPUStencil>>(m, "NeighborListGPUStencil")
hoomd/md/NeighborListGPUStencil.cc:        .def("setCellWidth", &NeighborListGPUStencil::setCellWidth);
hoomd/md/EvaluatorPairEwald.h:        //! Set CUDA memory hints
hoomd/md/data/local_access_gpu.py:"""Implement local access classes for the GPU."""
hoomd/md/data/local_access_gpu.py:from hoomd.data.array import HOOMDGPUArray
hoomd/md/data/local_access_gpu.py:if hoomd.version.gpu_enabled:
hoomd/md/data/local_access_gpu.py:    class ForceLocalAccessGPU(_ForceLocalAccessBase):
hoomd/md/data/local_access_gpu.py:        """Access force array data on the GPU."""
hoomd/md/data/local_access_gpu.py:        _array_cls = HOOMDGPUArray
hoomd/md/data/local_access_gpu.py:    class NeighborListLocalAccessGPU(_NeighborListLocalAccessBase):
hoomd/md/data/local_access_gpu.py:        """Access neighbor list array data on the GPU."""
hoomd/md/data/local_access_gpu.py:        _array_cls = HOOMDGPUArray
hoomd/md/data/local_access_gpu.py:    from hoomd.error import _NoGPU
hoomd/md/data/local_access_gpu.py:    class ForceLocalAccessGPU(_NoGPU):
hoomd/md/data/local_access_gpu.py:        """GPU data access is not available in CPU builds."""
hoomd/md/data/local_access_gpu.py:    class NeighborListLocalAccessGPU(_NoGPU):
hoomd/md/data/local_access_gpu.py:        """GPU data access is not available in CPU builds."""
hoomd/md/data/local_access_gpu.py:_gpu_force_access_docs = """
hoomd/md/data/local_access_gpu.py:Access HOOMD-Blue force data buffers on the GPU.
hoomd/md/data/local_access_gpu.py:ForceLocalAccessGPU.__doc__ = _gpu_force_access_docs
hoomd/md/data/local_access_gpu.py:_gpu_nlist_access_docs = """
hoomd/md/data/local_access_gpu.py:Access HOOMD-Blue neighbor list data buffers on the GPU.
hoomd/md/data/local_access_gpu.py:NeighborListLocalAccessGPU.__doc__ = _gpu_nlist_access_docs
hoomd/md/data/CMakeLists.txt:          local_access_gpu.py
hoomd/md/data/__init__.py:`ForceLocalAccess`, `ForceLocalAccessGPU`, and related classes provide direct
hoomd/md/data/__init__.py:from .local_access_gpu import ForceLocalAccessGPU, NeighborListLocalAccessGPU
hoomd/md/MuellerPlatheFlowGPU.h:/*! \file MuellerPlatheFlowGPU.h
hoomd/md/MuellerPlatheFlowGPU.h:           GPU accelerated version.
hoomd/md/MuellerPlatheFlowGPU.h:#ifndef __MUELLER_PLATHE_FLOW_GPU_H__
hoomd/md/MuellerPlatheFlowGPU.h:#define __MUELLER_PLATHE_FLOW_GPU_H__
hoomd/md/MuellerPlatheFlowGPU.h://! By exchanging velocities based on their spatial position a flow is created. GPU accelerated
hoomd/md/MuellerPlatheFlowGPU.h:class MuellerPlatheFlowGPU : public MuellerPlatheFlow
hoomd/md/MuellerPlatheFlowGPU.h:    MuellerPlatheFlowGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/md/MuellerPlatheFlowGPU.h:    virtual ~MuellerPlatheFlowGPU(void);
hoomd/md/MuellerPlatheFlowGPU.h:#endif //__MUELLER_PLATHE_FLOW_GPU_H__
hoomd/md/AnisoPotentialPairGPU.cuh:#include "hoomd/GPUPartition.cuh"
hoomd/md/AnisoPotentialPairGPU.cuh:/*! \file AnisoPotentialPairGPU.cuh
hoomd/md/AnisoPotentialPairGPU.cuh:    \brief Defines templated GPU kernel code for calculating the anisotropic ptl pair forces and
hoomd/md/AnisoPotentialPairGPU.cuh:#ifndef __ANISO_POTENTIAL_PAIR_GPU_CUH__
hoomd/md/AnisoPotentialPairGPU.cuh:#define __ANISO_POTENTIAL_PAIR_GPU_CUH__
hoomd/md/AnisoPotentialPairGPU.cuh:const int gpu_aniso_pair_force_max_tpp = 32;
hoomd/md/AnisoPotentialPairGPU.cuh:const int gpu_aniso_pair_force_max_tpp = 64;
hoomd/md/AnisoPotentialPairGPU.cuh://! Wraps arguments to gpu_cgpf
hoomd/md/AnisoPotentialPairGPU.cuh:                  const GPUPartition& _gpu_partition,
hoomd/md/AnisoPotentialPairGPU.cuh:          threads_per_particle(_threads_per_particle), gpu_partition(_gpu_partition),
hoomd/md/AnisoPotentialPairGPU.cuh:    const BoxDim box;             //!< Simulation box in GPU format
hoomd/md/AnisoPotentialPairGPU.cuh:    const GPUPartition& gpu_partition; //!< The load balancing partition of particles between GPUs
hoomd/md/AnisoPotentialPairGPU.cuh:    const hipDeviceProp_t& devprop;    //!< CUDA device properties
hoomd/md/AnisoPotentialPairGPU.cuh:    bool update_shape_param; //!< If true, update size of shape param and synchronize GPU execution
hoomd/md/AnisoPotentialPairGPU.cuh:    \param d_orientation Quaternion data on the GPU to calculate forces on
hoomd/md/AnisoPotentialPairGPU.cuh:    \param d_tag Tag data on the GPU to calculate forces on
hoomd/md/AnisoPotentialPairGPU.cuh:gpu_compute_pair_aniso_forces_kernel(Scalar4* d_force,
hoomd/md/AnisoPotentialPairGPU.cuh:     * \param range Range of particle indices this GPU operates on
hoomd/md/AnisoPotentialPairGPU.cuh:                reinterpret_cast<const void*>(&gpu_compute_pair_aniso_forces_kernel<evaluator,
hoomd/md/AnisoPotentialPairGPU.cuh:            max_block_size = max_threads - max_threads % gpu_aniso_pair_force_max_tpp;
hoomd/md/AnisoPotentialPairGPU.cuh:                (gpu_compute_pair_aniso_forces_kernel<evaluator, shift_mode, compute_virial, tpp>),
hoomd/md/AnisoPotentialPairGPU.cuh://! Kernel driver that computes lj forces on the GPU for AnisoPotentialPairGPU
hoomd/md/AnisoPotentialPairGPU.cuh:    This is just a driver function for gpu_compute_pair_aniso_forces_kernel(), see it for details.
hoomd/md/AnisoPotentialPairGPU.cuh:hipError_t gpu_compute_pair_aniso_forces(const a_pair_args_t& pair_args,
hoomd/md/AnisoPotentialPairGPU.cuh:    // iterate over active GPUs in reverse, to end up on first GPU when returning from this function
hoomd/md/AnisoPotentialPairGPU.cuh:    for (int idev = pair_args.gpu_partition.getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/md/AnisoPotentialPairGPU.cuh:        auto range = pair_args.gpu_partition.getRangeAndSetGPU(idev);
hoomd/md/AnisoPotentialPairGPU.cuh:                AnisoPairForceComputeKernel<evaluator, 0, 1, gpu_aniso_pair_force_max_tpp>::launch(
hoomd/md/AnisoPotentialPairGPU.cuh:                                            gpu_aniso_pair_force_max_tpp>::launch(pair_args,
hoomd/md/AnisoPotentialPairGPU.cuh:                AnisoPairForceComputeKernel<evaluator, 0, 0, gpu_aniso_pair_force_max_tpp>::launch(
hoomd/md/AnisoPotentialPairGPU.cuh:                                            gpu_aniso_pair_force_max_tpp>::launch(pair_args,
hoomd/md/AnisoPotentialPairGPU.cuh:hipError_t gpu_compute_pair_aniso_forces(const a_pair_args_t& pair_args,
hoomd/md/AnisoPotentialPairGPU.cuh:#endif // __ANISO_POTENTIAL_PAIR_GPU_CUH__
hoomd/md/pytest/test_custom_force.py:# cupy works implicitly to set values in GPU force arrays
hoomd/md/pytest/test_custom_force.py:    # Necessary to test failure of using GPU buffers in CPU simulation.
hoomd/md/pytest/test_custom_force.py:    if isinstance(device, hoomd.device.GPU):
hoomd/md/pytest/test_custom_force.py:        names.append('gpu_local_force_arrays')
hoomd/md/pytest/test_custom_force.py:def _gpu_device_and_no_cupy(sim):
hoomd/md/pytest/test_custom_force.py:    gpu_device = isinstance(sim.device, hoomd.device.GPU)
hoomd/md/pytest/test_custom_force.py:    return gpu_device and not CUPY_IMPORTED
hoomd/md/pytest/test_custom_force.py:def _skip_if_gpu_device_and_no_cupy(sim):
hoomd/md/pytest/test_custom_force.py:    if _gpu_device_and_no_cupy(sim):
hoomd/md/pytest/test_custom_force.py:        pytest.skip("Cannot run this test on GPU without cupy")
hoomd/md/pytest/test_custom_force.py:        if 'gpu' in self._local_force_name:
hoomd/md/pytest/test_custom_force.py:        _skip_if_gpu_device_and_no_cupy(sim)
hoomd/md/pytest/test_custom_force.py:        if arr.__class__.__name__ == 'HOOMDGPUArray':
hoomd/md/pytest/test_custom_force.py:        if 'gpu' in self._local_force_name:
hoomd/md/pytest/test_custom_force.py:        _skip_if_gpu_device_and_no_cupy(sim2)
hoomd/md/pytest/test_custom_force.py:            if not _gpu_device_and_no_cupy(sim):
hoomd/md/pytest/test_custom_force.py:def test_failure_with_cpu_device_and_gpu_buffer():
hoomd/md/pytest/test_custom_force.py:    """Assert we cannot access gpu buffers with a cpu_device."""
hoomd/md/pytest/test_custom_force.py:    custom_force = MyForce('gpu_local_force_arrays')
hoomd/md/pytest/test_custom_force.py:        _skip_if_gpu_device_and_no_cupy(sim)
hoomd/md/pytest/test_custom_force.py:    _skip_if_gpu_device_and_no_cupy(sim)
hoomd/md/pytest/test_kernel_parameters.py:@pytest.mark.gpu
hoomd/md/pytest/test_nlist.py:def test_gpu_local_nlist_arrays(simulation_factory, lattice_snapshot_factory,
hoomd/md/pytest/test_nlist.py:            with nlist.gpu_local_nlist_arrays as data:
hoomd/md/pytest/test_nlist.py:    with nlist.gpu_local_nlist_arrays as data:
hoomd/md/pytest/test_nlist.py:        with sim.state.gpu_local_snapshot as snap_data:
hoomd/md/pytest/test_potential.py:def _skip_if_triplet_gpu_mpi(sim, pair_potential):
hoomd/md/pytest/test_potential.py:    if (isinstance(sim.device, hoomd.device.GPU)
hoomd/md/pytest/test_potential.py:        pytest.skip("Cannot run triplet potentials with GPU+MPI enabled")
hoomd/md/pytest/test_potential.py:    _skip_if_triplet_gpu_mpi(sim, valid_params.pair_potential)
hoomd/md/pytest/test_potential.py:    _skip_if_triplet_gpu_mpi(sim, valid_params.pair_potential)
hoomd/md/pytest/test_potential.py:    if (pot_name == 'Tersoff' and isinstance(device, hoomd.device.GPU)
hoomd/md/pytest/test_potential.py:            and hoomd.version.gpu_platform == 'ROCm'):
hoomd/md/pytest/test_potential.py:        pytest.skip("Tersoff causes seg faults on ROCm (#1606).")
hoomd/md/pytest/test_potential.py:    _skip_if_triplet_gpu_mpi(sim, valid_params.pair_potential)
hoomd/md/pytest/test_potential.py:    _skip_if_triplet_gpu_mpi(sim, valid_params.pair_potential)
hoomd/md/test/test_harmonic_bond_force.cc:#include "hoomd/md/PotentialBondGPU.h"
hoomd/md/test/test_harmonic_bond_force.cc:           PotentialBondHarmonicGPU
hoomd/md/test/test_harmonic_bond_force.cc:typedef class PotentialBondGPU<EvaluatorBondHarmonic, BondData> PotentialBondHarmonicGPU;
hoomd/md/test/test_harmonic_bond_force.cc:std::shared_ptr<PotentialBondHarmonic> gpu_bf_creator(std::shared_ptr<SystemDefinition> sysdef)
hoomd/md/test/test_harmonic_bond_force.cc:    return std::shared_ptr<PotentialBondHarmonic>(new PotentialBondHarmonicGPU(sysdef));
hoomd/md/test/test_harmonic_bond_force.cc://! test case for bond forces on the GPU
hoomd/md/test/test_harmonic_bond_force.cc:UP_TEST(PotentialBondHarmonicGPU_basic)
hoomd/md/test/test_harmonic_bond_force.cc:    bondforce_creator bf_creator = bind(gpu_bf_creator, _1);
hoomd/md/test/test_harmonic_bond_force.cc:                               new ExecutionConfiguration(ExecutionConfiguration::GPU)));
hoomd/md/test/test_harmonic_bond_force.cc://! test case for comparing bond GPU and CPU BondForceComputes
hoomd/md/test/test_harmonic_bond_force.cc:UP_TEST(PotentialBondHarmonicGPU_compare)
hoomd/md/test/test_harmonic_bond_force.cc:    bondforce_creator bf_creator_gpu = bind(gpu_bf_creator, _1);
hoomd/md/test/test_harmonic_bond_force.cc:                                bf_creator_gpu,
hoomd/md/test/test_harmonic_bond_force.cc:                                    new ExecutionConfiguration(ExecutionConfiguration::GPU)));
hoomd/md/test/test_communicator_grid.cc:#include "hoomd/md/CommunicatorGridGPU.h"
hoomd/md/test/test_communicator_grid.cc://! Basic ghost grid exchange test on GPU
hoomd/md/test/test_communicator_grid.cc:UP_TEST(CommunicateGrid_test_basic_GPU)
hoomd/md/test/test_communicator_grid.cc:    test_communicate_grid_basic<CommunicatorGridGPU<unsigned int>>(
hoomd/md/test/test_communicator_grid.cc:            new ExecutionConfiguration(ExecutionConfiguration::GPU)));
hoomd/md/test/test_communicator_grid.cc://! Ghost grid exchange positions test on GPU
hoomd/md/test/test_communicator_grid.cc:UP_TEST(CommunicateGrid_test_positions_GPU)
hoomd/md/test/test_communicator_grid.cc:    test_communicate_grid_positions<CommunicatorGridGPU<unsigned int>>(
hoomd/md/test/test_communicator_grid.cc:            new ExecutionConfiguration(ExecutionConfiguration::GPU)));
hoomd/md/test/test_fire_energy_minimizer.cc:#include "hoomd/md/FIREEnergyMinimizerGPU.h"
hoomd/md/test/test_fire_energy_minimizer.cc:#include "hoomd/md/TwoStepConstantVolumeGPU.h"
hoomd/md/test/test_fire_energy_minimizer.cc:#include <hoomd/md/ComputeThermoGPU.h>
hoomd/md/test/test_fire_energy_minimizer.cc:#include "hoomd/md/TwoStepConstantVolumeGPU.h"
hoomd/md/test/test_fire_energy_minimizer.cc://! TwoStepNVEGPU factory for the unit tests
hoomd/md/test/test_fire_energy_minimizer.cc:std::shared_ptr<TwoStepConstantVolume> gpu_nve_creator(std::shared_ptr<SystemDefinition> sysdef,
hoomd/md/test/test_fire_energy_minimizer.cc:    auto thermo = std::make_shared<ComputeThermoGPU>(sysdef, group);
hoomd/md/test/test_fire_energy_minimizer.cc:        new TwoStepConstantVolumeGPU(sysdef, group, tstat));
hoomd/md/test/test_fire_energy_minimizer.cc://! FIREEnergyMinimizerGPU creator
hoomd/md/test/test_fire_energy_minimizer.cc:std::shared_ptr<FIREEnergyMinimizer> gpu_fire_creator(std::shared_ptr<SystemDefinition> sysdef,
hoomd/md/test/test_fire_energy_minimizer.cc:    return std::shared_ptr<FIREEnergyMinimizer>(new FIREEnergyMinimizerGPU(sysdef, dt));
hoomd/md/test/test_fire_energy_minimizer.cc:UP_TEST(FIREEnergyMinimizerGPU_twoparticle_test)
hoomd/md/test/test_fire_energy_minimizer.cc:    fire_twoparticle_test(gpu_fire_creator,
hoomd/md/test/test_fire_energy_minimizer.cc:                          gpu_nve_creator,
hoomd/md/test/test_fire_energy_minimizer.cc:                              new ExecutionConfiguration(ExecutionConfiguration::GPU)));
hoomd/md/test/test_fire_energy_minimizer.cc://! Compares the output of FIREEnergyMinimizerGPU to the conjugate gradient method from LAMMPS
hoomd/md/test/test_fire_energy_minimizer.cc:UP_TEST(FIREEnergyMinimizerGPU_smallsystem_test)
hoomd/md/test/test_fire_energy_minimizer.cc:    fire_smallsystem_test(gpu_fire_creator,
hoomd/md/test/test_fire_energy_minimizer.cc:                          gpu_nve_creator,
hoomd/md/test/test_fire_energy_minimizer.cc:                              new ExecutionConfiguration(ExecutionConfiguration::GPU)));
hoomd/md/test/test_pppm_force.cc:#include "hoomd/md/PPPMForceComputeGPU.h"
hoomd/md/test/test_pppm_force.cc:    \brief Implements unit tests for PPPMForceCompute and PPPMForceComputeGPU and descendants
hoomd/md/test/test_pppm_force.cc://! PPPMForceComputeGPU creator for unit tests
hoomd/md/test/test_pppm_force.cc:std::shared_ptr<PPPMForceCompute> gpu_pppm_creator(std::shared_ptr<SystemDefinition> sysdef,
hoomd/md/test/test_pppm_force.cc:    return std::shared_ptr<PPPMForceComputeGPU>(new PPPMForceComputeGPU(sysdef, nlist, group));
hoomd/md/test/test_pppm_force.cc://! test case for bond forces on the GPU
hoomd/md/test/test_pppm_force.cc:UP_TEST(PPPMForceComputeGPU_basic)
hoomd/md/test/test_pppm_force.cc:    pppmforce_creator pppm_creator = bind(gpu_pppm_creator, _1, _2, _3);
hoomd/md/test/test_pppm_force.cc:                                 new ExecutionConfiguration(ExecutionConfiguration::GPU)));
hoomd/md/test/test_pppm_force.cc:UP_TEST(PPPMForceComputeGPU_triclinic)
hoomd/md/test/test_pppm_force.cc:    pppmforce_creator pppm_creator = bind(gpu_pppm_creator, _1, _2, _3);
hoomd/md/test/test_pppm_force.cc:            new ExecutionConfiguration(ExecutionConfiguration::GPU)));
hoomd/md/test/test_bondtable_bond_force.cc:#include "hoomd/md/BondTablePotentialGPU.h"
hoomd/md/test/test_bondtable_bond_force.cc:           BondTablePotentialGPU
hoomd/md/test/test_bondtable_bond_force.cc:std::shared_ptr<BondTablePotential> gpu_bf_creator(std::shared_ptr<SystemDefinition> sysdef,
hoomd/md/test/test_bondtable_bond_force.cc:    return std::shared_ptr<BondTablePotential>(new BondTablePotentialGPU(sysdef, width));
hoomd/md/test/test_bondtable_bond_force.cc://! test case for bond forces on the GPU
hoomd/md/test/test_bondtable_bond_force.cc:UP_TEST(BondTablePotentialGPU_basic)
hoomd/md/test/test_bondtable_bond_force.cc:    bondforce_creator bf_creator = bind(gpu_bf_creator, _1, _2);
hoomd/md/test/test_bondtable_bond_force.cc:                               new ExecutionConfiguration(ExecutionConfiguration::GPU)));
hoomd/md/test/test_bondtable_bond_force.cc://! test case for bond force type on the GPU
hoomd/md/test/test_bondtable_bond_force.cc:UP_TEST(BondTablePotentialGPU_type)
hoomd/md/test/test_bondtable_bond_force.cc:    bondforce_creator bf_creator = bind(gpu_bf_creator, _1, _2);
hoomd/md/test/test_bondtable_bond_force.cc:                             new ExecutionConfiguration(ExecutionConfiguration::GPU)));
hoomd/md/test/test_neighborlist.cc:#include "hoomd/md/NeighborListGPU.h"
hoomd/md/test/test_neighborlist.cc:#include "hoomd/md/NeighborListGPUBinned.h"
hoomd/md/test/test_neighborlist.cc:#include "hoomd/md/NeighborListGPUStencil.h"
hoomd/md/test/test_neighborlist.cc:#include "hoomd/md/NeighborListGPUTree.h"
hoomd/md/test/test_neighborlist.cc:    // bumping over this do this with size 18 so that NeighborListGPU is forced to use kernel call
hoomd/md/test/test_neighborlist.cc:// BINNED GPU
hoomd/md/test/test_neighborlist.cc://! basic test case for GPUBinned class
hoomd/md/test/test_neighborlist.cc:UP_TEST(NeighborListGPUBinned_basic)
hoomd/md/test/test_neighborlist.cc:    neighborlist_basic_tests<NeighborListGPUBinned>(std::shared_ptr<ExecutionConfiguration>(
hoomd/md/test/test_neighborlist.cc:        new ExecutionConfiguration(ExecutionConfiguration::GPU)));
hoomd/md/test/test_neighborlist.cc://! exclusion test case for GPUBinned class
hoomd/md/test/test_neighborlist.cc:UP_TEST(NeighborListGPUBinned_exclusion)
hoomd/md/test/test_neighborlist.cc:    neighborlist_exclusion_tests<NeighborListGPUBinned>(std::shared_ptr<ExecutionConfiguration>(
hoomd/md/test/test_neighborlist.cc:        new ExecutionConfiguration(ExecutionConfiguration::GPU)));
hoomd/md/test/test_neighborlist.cc://! large exclusion test case for GPUBinned class
hoomd/md/test/test_neighborlist.cc:UP_TEST(NeighborListGPUBinned_large_ex)
hoomd/md/test/test_neighborlist.cc:    neighborlist_large_ex_tests<NeighborListGPUBinned>(std::shared_ptr<ExecutionConfiguration>(
hoomd/md/test/test_neighborlist.cc:        new ExecutionConfiguration(ExecutionConfiguration::GPU)));
hoomd/md/test/test_neighborlist.cc://! body filter test case for GPUBinned class
hoomd/md/test/test_neighborlist.cc:UP_TEST(NeighborListGPUBinned_body_filter)
hoomd/md/test/test_neighborlist.cc:    neighborlist_body_filter_tests<NeighborListGPUBinned>(std::shared_ptr<ExecutionConfiguration>(
hoomd/md/test/test_neighborlist.cc:        new ExecutionConfiguration(ExecutionConfiguration::GPU)));
hoomd/md/test/test_neighborlist.cc://! particle asymmetry test case for GPUBinned class
hoomd/md/test/test_neighborlist.cc:UP_TEST(NeighborListGPUBinned_particle_asymm)
hoomd/md/test/test_neighborlist.cc:    neighborlist_particle_asymm_tests<NeighborListGPUBinned>(
hoomd/md/test/test_neighborlist.cc:            new ExecutionConfiguration(ExecutionConfiguration::GPU)));
hoomd/md/test/test_neighborlist.cc://! cutoff exclusion test case for GPUBinned class
hoomd/md/test/test_neighborlist.cc:UP_TEST(NeighborListGPUBinned_cutoff_exclude)
hoomd/md/test/test_neighborlist.cc:    neighborlist_cutoff_exclude_tests<NeighborListGPUBinned>(
hoomd/md/test/test_neighborlist.cc:            new ExecutionConfiguration(ExecutionConfiguration::GPU)));
hoomd/md/test/test_neighborlist.cc://! type test case for GPUBinned class
hoomd/md/test/test_neighborlist.cc:UP_TEST(NeighborListGPUBinned_type)
hoomd/md/test/test_neighborlist.cc:    neighborlist_type_tests<NeighborListGPUBinned>(std::shared_ptr<ExecutionConfiguration>(
hoomd/md/test/test_neighborlist.cc:        new ExecutionConfiguration(ExecutionConfiguration::GPU)));
hoomd/md/test/test_neighborlist.cc://! 2d tests for GPUBinned class
hoomd/md/test/test_neighborlist.cc:UP_TEST(NeighborListGPUBinned_2d)
hoomd/md/test/test_neighborlist.cc:    neighborlist_2d_tests<NeighborListGPUBinned>(std::shared_ptr<ExecutionConfiguration>(
hoomd/md/test/test_neighborlist.cc:        new ExecutionConfiguration(ExecutionConfiguration::GPU)));
hoomd/md/test/test_neighborlist.cc://! comparison test case for GPUBinned class
hoomd/md/test/test_neighborlist.cc:UP_TEST(NeighborListGPUBinned_comparison)
hoomd/md/test/test_neighborlist.cc:    neighborlist_comparison_test<NeighborListBinned, NeighborListGPUBinned>(
hoomd/md/test/test_neighborlist.cc:            new ExecutionConfiguration(ExecutionConfiguration::GPU)));
hoomd/md/test/test_neighborlist.cc:// STENCIL GPU
hoomd/md/test/test_neighborlist.cc://! basic test case for GPUStencil class
hoomd/md/test/test_neighborlist.cc:UP_TEST(NeighborListGPUStencil_basic)
hoomd/md/test/test_neighborlist.cc:    neighborlist_basic_tests<NeighborListGPUStencil>(std::shared_ptr<ExecutionConfiguration>(
hoomd/md/test/test_neighborlist.cc:        new ExecutionConfiguration(ExecutionConfiguration::GPU)));
hoomd/md/test/test_neighborlist.cc://! exclusion test case for GPUStencil class
hoomd/md/test/test_neighborlist.cc:UP_TEST(NeighborListGPUStencil_exclusion)
hoomd/md/test/test_neighborlist.cc:    neighborlist_exclusion_tests<NeighborListGPUStencil>(std::shared_ptr<ExecutionConfiguration>(
hoomd/md/test/test_neighborlist.cc:        new ExecutionConfiguration(ExecutionConfiguration::GPU)));
hoomd/md/test/test_neighborlist.cc://! large exclusion test case for GPUStencil class
hoomd/md/test/test_neighborlist.cc:UP_TEST(NeighborListGPUStencil_large_ex)
hoomd/md/test/test_neighborlist.cc:    neighborlist_large_ex_tests<NeighborListGPUStencil>(std::shared_ptr<ExecutionConfiguration>(
hoomd/md/test/test_neighborlist.cc:        new ExecutionConfiguration(ExecutionConfiguration::GPU)));
hoomd/md/test/test_neighborlist.cc://! body filter test case for GPUStencil class
hoomd/md/test/test_neighborlist.cc:UP_TEST(NeighborListGPUStencil_body_filter)
hoomd/md/test/test_neighborlist.cc:    neighborlist_body_filter_tests<NeighborListGPUStencil>(std::shared_ptr<ExecutionConfiguration>(
hoomd/md/test/test_neighborlist.cc:        new ExecutionConfiguration(ExecutionConfiguration::GPU)));
hoomd/md/test/test_neighborlist.cc://! particle asymmetry test case for GPUStencil class
hoomd/md/test/test_neighborlist.cc:UP_TEST(NeighborListGPUStencil_particle_asymm)
hoomd/md/test/test_neighborlist.cc:    neighborlist_particle_asymm_tests<NeighborListGPUStencil>(
hoomd/md/test/test_neighborlist.cc:            new ExecutionConfiguration(ExecutionConfiguration::GPU)));
hoomd/md/test/test_neighborlist.cc://! cutoff exclusion test case for GPUStencil class
hoomd/md/test/test_neighborlist.cc:UP_TEST(NeighborListGPUStencil_cutoff_exclude)
hoomd/md/test/test_neighborlist.cc:    neighborlist_cutoff_exclude_tests<NeighborListGPUStencil>(
hoomd/md/test/test_neighborlist.cc:            new ExecutionConfiguration(ExecutionConfiguration::GPU)));
hoomd/md/test/test_neighborlist.cc://! type test case for GPUStencil class
hoomd/md/test/test_neighborlist.cc:UP_TEST(NeighborListGPUStencil_type)
hoomd/md/test/test_neighborlist.cc:    neighborlist_type_tests<NeighborListGPUStencil>(std::shared_ptr<ExecutionConfiguration>(
hoomd/md/test/test_neighborlist.cc:        new ExecutionConfiguration(ExecutionConfiguration::GPU)));
hoomd/md/test/test_neighborlist.cc://! 2d tests for GPUStencil class
hoomd/md/test/test_neighborlist.cc:UP_TEST(NeighborListGPUStencil_2d)
hoomd/md/test/test_neighborlist.cc:    neighborlist_2d_tests<NeighborListGPUStencil>(std::shared_ptr<ExecutionConfiguration>(
hoomd/md/test/test_neighborlist.cc:        new ExecutionConfiguration(ExecutionConfiguration::GPU)));
hoomd/md/test/test_neighborlist.cc://! comparison test case for GPUStencil class against Stencil on cpu
hoomd/md/test/test_neighborlist.cc:UP_TEST(NeighborListGPUStencil_cpu_comparison)
hoomd/md/test/test_neighborlist.cc:    neighborlist_comparison_test<NeighborListStencil, NeighborListGPUStencil>(
hoomd/md/test/test_neighborlist.cc:            new ExecutionConfiguration(ExecutionConfiguration::GPU)));
hoomd/md/test/test_neighborlist.cc://! comparison test case for GPUStencil class against GPUBinned
hoomd/md/test/test_neighborlist.cc:UP_TEST(NeighborListGPUStencil_binned_comparison)
hoomd/md/test/test_neighborlist.cc:    neighborlist_comparison_test<NeighborListGPUBinned, NeighborListGPUStencil>(
hoomd/md/test/test_neighborlist.cc:            new ExecutionConfiguration(ExecutionConfiguration::GPU)));
hoomd/md/test/test_neighborlist.cc:// TREE GPU
hoomd/md/test/test_neighborlist.cc://! basic test case for GPUTree class
hoomd/md/test/test_neighborlist.cc:UP_TEST(NeighborListGPUTree_basic)
hoomd/md/test/test_neighborlist.cc:        new ExecutionConfiguration(ExecutionConfiguration::GPU));
hoomd/md/test/test_neighborlist.cc:    neighborlist_basic_tests<NeighborListGPUTree>(exec_conf);
hoomd/md/test/test_neighborlist.cc://! exclusion test case for GPUTree class
hoomd/md/test/test_neighborlist.cc:UP_TEST(NeighborListGPUTree_exclusion)
hoomd/md/test/test_neighborlist.cc:        new ExecutionConfiguration(ExecutionConfiguration::GPU));
hoomd/md/test/test_neighborlist.cc:    neighborlist_exclusion_tests<NeighborListGPUTree>(exec_conf);
hoomd/md/test/test_neighborlist.cc://! large exclusion test case for GPUTree class
hoomd/md/test/test_neighborlist.cc:UP_TEST(NeighborListGPUTree_large_ex)
hoomd/md/test/test_neighborlist.cc:        new ExecutionConfiguration(ExecutionConfiguration::GPU));
hoomd/md/test/test_neighborlist.cc:    neighborlist_large_ex_tests<NeighborListGPUTree>(exec_conf);
hoomd/md/test/test_neighborlist.cc://! body filter test case for GPUTree class
hoomd/md/test/test_neighborlist.cc:UP_TEST(NeighborListGPUTree_body_filter)
hoomd/md/test/test_neighborlist.cc:        new ExecutionConfiguration(ExecutionConfiguration::GPU));
hoomd/md/test/test_neighborlist.cc:    neighborlist_body_filter_tests<NeighborListGPUTree>(exec_conf);
hoomd/md/test/test_neighborlist.cc://! particle asymmetry test case for GPUTree class
hoomd/md/test/test_neighborlist.cc:UP_TEST(NeighborListGPUTree_particle_asymm)
hoomd/md/test/test_neighborlist.cc:        new ExecutionConfiguration(ExecutionConfiguration::GPU));
hoomd/md/test/test_neighborlist.cc:    neighborlist_particle_asymm_tests<NeighborListGPUTree>(exec_conf);
hoomd/md/test/test_neighborlist.cc://! cutoff exclusion test case for GPUTree class
hoomd/md/test/test_neighborlist.cc:UP_TEST(NeighborListGPUTree_cutoff_exclude)
hoomd/md/test/test_neighborlist.cc:        new ExecutionConfiguration(ExecutionConfiguration::GPU));
hoomd/md/test/test_neighborlist.cc:    neighborlist_cutoff_exclude_tests<NeighborListGPUTree>(exec_conf);
hoomd/md/test/test_neighborlist.cc:UP_TEST(NeighborListGPUTree_type)
hoomd/md/test/test_neighborlist.cc:        new ExecutionConfiguration(ExecutionConfiguration::GPU));
hoomd/md/test/test_neighborlist.cc:    neighborlist_type_tests<NeighborListGPUTree>(exec_conf);
hoomd/md/test/test_neighborlist.cc:UP_TEST(NeighborListGPUTree_2d)
hoomd/md/test/test_neighborlist.cc:        new ExecutionConfiguration(ExecutionConfiguration::GPU));
hoomd/md/test/test_neighborlist.cc:    neighborlist_2d_tests<NeighborListGPUTree>(exec_conf);
hoomd/md/test/test_neighborlist.cc://! comparison test case for GPUTree class with itself
hoomd/md/test/test_neighborlist.cc:UP_TEST(NeighborListGPUTree_cpu_comparison)
hoomd/md/test/test_neighborlist.cc:        new ExecutionConfiguration(ExecutionConfiguration::GPU));
hoomd/md/test/test_neighborlist.cc:    neighborlist_comparison_test<NeighborListTree, NeighborListGPUTree>(exec_conf);
hoomd/md/test/test_neighborlist.cc://! comparison test case for GPUTree class with GPUBinned
hoomd/md/test/test_neighborlist.cc:UP_TEST(NeighborListGPUTree_binned_comparison)
hoomd/md/test/test_neighborlist.cc:        new ExecutionConfiguration(ExecutionConfiguration::GPU));
hoomd/md/test/test_neighborlist.cc:    neighborlist_comparison_test<NeighborListGPUBinned, NeighborListGPUTree>(exec_conf);
hoomd/md/test/test_MolecularForceCompute.cc:    \brief Implements unit tests for MolecularForceCompute on CPU and GPU
hoomd/md/test/test_MolecularForceCompute.cc://! Test if the CPU and the GPU implementation give consistent results
hoomd/md/test/test_MolecularForceCompute.cc:                     std::shared_ptr<ExecutionConfiguration> exec_conf_gpu)
hoomd/md/test/test_MolecularForceCompute.cc:    std::shared_ptr<SystemDefinition> sysdef_gpu(
hoomd/md/test/test_MolecularForceCompute.cc:        new SystemDefinition(nptl, BoxDim(1000.0), 1, 0, 0, 0, 0, exec_conf_gpu));
hoomd/md/test/test_MolecularForceCompute.cc:    std::shared_ptr<ParticleData> pdata_gpu = sysdef_gpu->getParticleData();
hoomd/md/test/test_MolecularForceCompute.cc:    MyMolecularForceCompute mfc_gpu(sysdef_gpu, molecule_tags, 0);
hoomd/md/test/test_MolecularForceCompute.cc:        mfc_gpu.setNMolecules((unsigned int)unique_tags.size());
hoomd/md/test/test_MolecularForceCompute.cc:        mfc_gpu.setMoleculeTags(molecule_tags);
hoomd/md/test/test_MolecularForceCompute.cc:        pdata_gpu->notifyParticleSort();
hoomd/md/test/test_MolecularForceCompute.cc:            ArrayHandle<unsigned int> h_molecule_length_gpu(mfc_gpu.getMoleculeLengths(),
hoomd/md/test/test_MolecularForceCompute.cc:            ArrayHandle<unsigned int> h_molecule_list_gpu(mfc_gpu.getMoleculeList(),
hoomd/md/test/test_MolecularForceCompute.cc:            Index2D molecule_indexer_gpu = mfc_gpu.getMoleculeIndexer();
hoomd/md/test/test_MolecularForceCompute.cc:            UP_ASSERT_EQUAL(molecule_indexer_gpu.getW(), molecule_indexer_cpu.getW());
hoomd/md/test/test_MolecularForceCompute.cc:            UP_ASSERT_EQUAL(molecule_indexer_gpu.getH(), molecule_indexer_cpu.getH());
hoomd/md/test/test_MolecularForceCompute.cc:                UP_ASSERT_EQUAL(h_molecule_length_cpu.data[j], h_molecule_length_gpu.data[j]);
hoomd/md/test/test_MolecularForceCompute.cc:                                    h_molecule_list_gpu.data[molecule_indexer_gpu(k, j)]);
hoomd/md/test/test_MolecularForceCompute.cc://! test case for particle test on GPU
hoomd/md/test/test_MolecularForceCompute.cc:UP_TEST(MolecularForceCompute_basic_GPU)
hoomd/md/test/test_MolecularForceCompute.cc:        new ExecutionConfiguration(ExecutionConfiguration::GPU)));
hoomd/md/test/test_MolecularForceCompute.cc://! test case for comparing GPU output to base class output
hoomd/md/test/test_MolecularForceCompute.cc:                        new ExecutionConfiguration(ExecutionConfiguration::GPU)));
hoomd/md/test/test_harmonic_dihedral_force.cc:#include "hoomd/md/HarmonicDihedralForceComputeGPU.h"
hoomd/md/test/test_harmonic_dihedral_force.cc:gpu_tf_creator(std::shared_ptr<SystemDefinition> sysdef)
hoomd/md/test/test_harmonic_dihedral_force.cc:        new HarmonicDihedralForceComputeGPU(sysdef));
hoomd/md/test/test_harmonic_dihedral_force.cc://! test case for dihedral forces on the GPU
hoomd/md/test/test_harmonic_dihedral_force.cc:UP_TEST(HarmonicDihedralForceComputeGPU_basic)
hoomd/md/test/test_harmonic_dihedral_force.cc:    printf(" IN UP_TEST: GPU \n");
hoomd/md/test/test_harmonic_dihedral_force.cc:    dihedralforce_creator tf_creator = bind(gpu_tf_creator, _1);
hoomd/md/test/test_harmonic_dihedral_force.cc:                                   new ExecutionConfiguration(ExecutionConfiguration::GPU)));
hoomd/md/test/test_harmonic_dihedral_force.cc:                                   new ExecutionConfiguration(ExecutionConfiguration::GPU)));
hoomd/md/test/test_harmonic_dihedral_force.cc://! test case for comparing bond GPU and CPU BondForceComputes
hoomd/md/test/test_harmonic_dihedral_force.cc:UP_TEST(HarmonicDihedralForceComputeGPU_compare)
hoomd/md/test/test_harmonic_dihedral_force.cc:    dihedralforce_creator tf_creator_gpu = bind(gpu_tf_creator, _1);
hoomd/md/test/test_harmonic_dihedral_force.cc:                                    tf_creator_gpu,
hoomd/md/test/test_harmonic_dihedral_force.cc:                                        new ExecutionConfiguration(ExecutionConfiguration::GPU)));
hoomd/md/test/test_harmonic_dihedral_force.cc://! test case for comparing calculation on the CPU to multi-gpu ones
hoomd/md/test/test_harmonic_dihedral_force.cc:UP_TEST(HarmonicDihedralForce_MultiGPU_compare)
hoomd/md/test/test_harmonic_dihedral_force.cc:        new ExecutionConfiguration(ExecutionConfiguration::GPU));
hoomd/md/test/test_harmonic_dihedral_force.cc:    dihedralforce_creator tf_creator_gpu = bind(gpu_tf_creator, _1);
hoomd/md/test/test_harmonic_dihedral_force.cc:    dihedral_force_comparison_tests(tf_creator, tf_creator_gpu, exec_conf);
hoomd/md/test/test_cosinesq_angle_force.cc:#include "hoomd/md/CosineSqAngleForceComputeGPU.h"
hoomd/md/test/test_cosinesq_angle_force.cc:std::shared_ptr<CosineSqAngleForceCompute> gpu_af_creator(std::shared_ptr<SystemDefinition> sysdef)
hoomd/md/test/test_cosinesq_angle_force.cc:    return std::shared_ptr<CosineSqAngleForceCompute>(new CosineSqAngleForceComputeGPU(sysdef));
hoomd/md/test/test_cosinesq_angle_force.cc://! test case for angle forces on the GPU
hoomd/md/test/test_cosinesq_angle_force.cc:UP_TEST(CosineSqAngleForceComputeGPU_basic)
hoomd/md/test/test_cosinesq_angle_force.cc:    printf(" IN UP_TEST: GPU \n");
hoomd/md/test/test_cosinesq_angle_force.cc:    cout << " IN UP_TEST: GPU \n";
hoomd/md/test/test_cosinesq_angle_force.cc:    angleforce_creator af_creator = bind(gpu_af_creator, _1);
hoomd/md/test/test_cosinesq_angle_force.cc:        new ExecutionConfiguration(ExecutionConfiguration::GPU));
hoomd/md/test/test_cosinesq_angle_force.cc:    exec_conf->setCUDAErrorChecking(true);
hoomd/md/test/test_cosinesq_angle_force.cc://! test case for comparing bond GPU and CPU BondForceComputes
hoomd/md/test/test_cosinesq_angle_force.cc:UP_TEST(CosineSqAngleForceComputeGPU_compare)
hoomd/md/test/test_cosinesq_angle_force.cc:    angleforce_creator af_creator_gpu = bind(gpu_af_creator, _1);
hoomd/md/test/test_cosinesq_angle_force.cc:                                 af_creator_gpu,
hoomd/md/test/test_cosinesq_angle_force.cc:                                     new ExecutionConfiguration(ExecutionConfiguration::GPU)));
hoomd/md/test/test_table_angle_force.cc:#include "hoomd/md/TableAngleForceComputeGPU.h"
hoomd/md/test/test_table_angle_force.cc:std::shared_ptr<TableAngleForceCompute> gpu_tf_creator(std::shared_ptr<SystemDefinition> sysdef,
hoomd/md/test/test_table_angle_force.cc:    return std::shared_ptr<TableAngleForceCompute>(new TableAngleForceComputeGPU(sysdef, width));
hoomd/md/test/test_table_angle_force.cc://! test case for angle forces on the GPU
hoomd/md/test/test_table_angle_force.cc:UP_TEST(TableAngleForceComputeGPU_basic)
hoomd/md/test/test_table_angle_force.cc:    printf(" IN UP_TEST: GPU \n");
hoomd/md/test/test_table_angle_force.cc:    angleforce_creator tf_creator = bind(gpu_tf_creator, _1, _2);
hoomd/md/test/test_table_angle_force.cc:                                new ExecutionConfiguration(ExecutionConfiguration::GPU)));
hoomd/md/test/test_table_angle_force.cc://! test case for comparing bond GPU and CPU BondForceComputes
hoomd/md/test/test_table_angle_force.cc:UP_TEST( TableAngleForceComputeGPU_compare )
hoomd/md/test/test_table_angle_force.cc:    angleforce_creator tf_creator_gpu = bind(gpu_tf_creator, _1);
hoomd/md/test/test_table_angle_force.cc:    angle_force_comparison_tests(tf_creator, tf_creator_gpu, std::shared_ptr<ExecutionConfiguration>(new ExecutionConfiguration(ExecutionConfiguration::GPU)));
hoomd/md/test/CMakeLists.txt:        set(_cuda_sources ${CUR_TEST}.cu)
hoomd/md/test/CMakeLists.txt:        set(_cuda_sources "")
hoomd/md/test/CMakeLists.txt:    add_executable(${CUR_TEST} EXCLUDE_FROM_ALL ${CUR_TEST}.cc ${_cuda_sources})
hoomd/md/test/test_table_dihedral_force.cc:#include "hoomd/md/TableDihedralForceComputeGPU.h"
hoomd/md/test/test_table_dihedral_force.cc:std::shared_ptr<TableDihedralForceCompute> gpu_tf_creator(std::shared_ptr<SystemDefinition> sysdef,
hoomd/md/test/test_table_dihedral_force.cc:        new TableDihedralForceComputeGPU(sysdef, width));
hoomd/md/test/test_table_dihedral_force.cc://! test case for dihedral forces on the GPU
hoomd/md/test/test_table_dihedral_force.cc:UP_TEST(TableDihedralForceComputeGPU_basic)
hoomd/md/test/test_table_dihedral_force.cc:    printf(" IN UP_TEST: GPU \n");
hoomd/md/test/test_table_dihedral_force.cc:    dihedralforce_creator tf_creator = bind(gpu_tf_creator, _1, _2);
hoomd/md/test/test_table_dihedral_force.cc:                                   new ExecutionConfiguration(ExecutionConfiguration::GPU)));
hoomd/md/test/test_table_dihedral_force.cc://! test case for comparing bond GPU and CPU BondForceComputes
hoomd/md/test/test_table_dihedral_force.cc:UP_TEST( TableDihedralForceComputeGPU_compare )
hoomd/md/test/test_table_dihedral_force.cc:    dihedralforce_creator tf_creator_gpu = bind(gpu_tf_creator, _1);
hoomd/md/test/test_table_dihedral_force.cc:    dihedral_force_comparison_tests(tf_creator, tf_creator_gpu, std::shared_ptr<ExecutionConfiguration>(new ExecutionConfiguration(ExecutionConfiguration::GPU)));
hoomd/md/test/test_external_periodic.cc:#include "hoomd/md/PotentialExternalGPU.h"
hoomd/md/test/test_external_periodic.cc:typedef PotentialExternalGPU<EvaluatorExternalPeriodic> PotentialExternalPeriodicGPU;
hoomd/md/test/test_external_periodic.cc:    \brief Implements unit tests for PotentialPairLJ and PotentialPairLJGPU and descendants
hoomd/md/test/test_external_periodic.cc://! LJForceComputeGPU creator for unit tests
hoomd/md/test/test_external_periodic.cc:gpu_periodic_creator(std::shared_ptr<SystemDefinition> sysdef)
hoomd/md/test/test_external_periodic.cc:    std::shared_ptr<PotentialExternalPeriodicGPU> periodic(
hoomd/md/test/test_external_periodic.cc:        new PotentialExternalPeriodicGPU(sysdef));
hoomd/md/test/test_external_periodic.cc://! test case for particle test on GPU
hoomd/md/test/test_external_periodic.cc:UP_TEST(PotentialExternalLamellaGPU_particle)
hoomd/md/test/test_external_periodic.cc:    periodicforce_creator periodic_creator_gpu = bind(gpu_periodic_creator, _1);
hoomd/md/test/test_external_periodic.cc:    periodic_force_particle_test(periodic_creator_gpu,
hoomd/md/test/test_external_periodic.cc:                                     new ExecutionConfiguration(ExecutionConfiguration::GPU)));
hoomd/md/test/test_external_periodic.cc://! test case for comparing GPU output to base class output
hoomd/md/test/test_external_periodic.cc:UP_TEST( LJForceGPU_compare )
hoomd/md/test/test_external_periodic.cc:    ljforce_creator lj_creator_gpu = bind(gpu_lj_creator, _1, _2);
hoomd/md/test/test_external_periodic.cc:    lj_force_comparison_test(lj_creator_base, lj_creator_gpu,
hoomd/md/test/test_external_periodic.cc:std::shared_ptr<ExecutionConfiguration>(new ExecutionConfiguration(ExecutionConfiguration::GPU)));
hoomd/md/test/test_harmonic_angle_force.cc:#include "hoomd/md/HarmonicAngleForceComputeGPU.h"
hoomd/md/test/test_harmonic_angle_force.cc:std::shared_ptr<HarmonicAngleForceCompute> gpu_af_creator(std::shared_ptr<SystemDefinition> sysdef)
hoomd/md/test/test_harmonic_angle_force.cc:    return std::shared_ptr<HarmonicAngleForceCompute>(new HarmonicAngleForceComputeGPU(sysdef));
hoomd/md/test/test_harmonic_angle_force.cc://! test case for angle forces on the GPU
hoomd/md/test/test_harmonic_angle_force.cc:UP_TEST(HarmonicAngleForceComputeGPU_basic)
hoomd/md/test/test_harmonic_angle_force.cc:    printf(" IN UP_TEST: GPU \n");
hoomd/md/test/test_harmonic_angle_force.cc:    cout << " IN UP_TEST: GPU \n";
hoomd/md/test/test_harmonic_angle_force.cc:    angleforce_creator af_creator = bind(gpu_af_creator, _1);
hoomd/md/test/test_harmonic_angle_force.cc:        new ExecutionConfiguration(ExecutionConfiguration::GPU));
hoomd/md/test/test_harmonic_angle_force.cc:    exec_conf->setCUDAErrorChecking(true);
hoomd/md/test/test_harmonic_angle_force.cc://! test case for comparing bond GPU and CPU BondForceComputes
hoomd/md/test/test_harmonic_angle_force.cc:UP_TEST(HarmonicAngleForceComputeGPU_compare)
hoomd/md/test/test_harmonic_angle_force.cc:    angleforce_creator af_creator_gpu = bind(gpu_af_creator, _1);
hoomd/md/test/test_harmonic_angle_force.cc:                                 af_creator_gpu,
hoomd/md/test/test_harmonic_angle_force.cc:                                     new ExecutionConfiguration(ExecutionConfiguration::GPU)));
hoomd/md/test/test_opls_dihedral_force.cc:#include "hoomd/md/OPLSDihedralForceComputeGPU.h"
hoomd/md/test/test_opls_dihedral_force.cc:std::shared_ptr<OPLSDihedralForceCompute> gpu_tf_creator(std::shared_ptr<SystemDefinition> sysdef)
hoomd/md/test/test_opls_dihedral_force.cc:    return std::shared_ptr<OPLSDihedralForceCompute>(new OPLSDihedralForceComputeGPU(sysdef));
hoomd/md/test/test_opls_dihedral_force.cc://! test case for dihedral forces on the GPU
hoomd/md/test/test_opls_dihedral_force.cc:UP_TEST(OPLSDihedralForceComputeGPU_basic)
hoomd/md/test/test_opls_dihedral_force.cc:    printf(" IN UP_TEST: GPU \n");
hoomd/md/test/test_opls_dihedral_force.cc:    dihedralforce_creator tf_creator = bind(gpu_tf_creator, _1);
hoomd/md/test/test_opls_dihedral_force.cc:                                   new ExecutionConfiguration(ExecutionConfiguration::GPU)));
hoomd/md/test/test_opls_dihedral_force.cc://! test case for comparing bond GPU and CPU BondForceComputes
hoomd/md/test/test_opls_dihedral_force.cc:UP_TEST(OPLSDihedralForceComputeGPU_compare)
hoomd/md/test/test_opls_dihedral_force.cc:    dihedralforce_creator tf_creator_gpu = bind(gpu_tf_creator, _1);
hoomd/md/test/test_opls_dihedral_force.cc:                                    tf_creator_gpu,
hoomd/md/test/test_opls_dihedral_force.cc:                                        new ExecutionConfiguration(ExecutionConfiguration::GPU)));
hoomd/md/test/test_opls_dihedral_force.cc://! test case for comparing calculation on the CPU to multi-gpu ones
hoomd/md/test/test_opls_dihedral_force.cc:UP_TEST(OPLSDihedralForce_MultiGPU_compare)
hoomd/md/test/test_opls_dihedral_force.cc:        new ExecutionConfiguration(ExecutionConfiguration::GPU));
hoomd/md/test/test_opls_dihedral_force.cc:    dihedralforce_creator tf_creator_gpu = bind(gpu_tf_creator, _1);
hoomd/md/test/test_opls_dihedral_force.cc:    dihedral_force_comparison_tests(tf_creator, tf_creator_gpu, exec_conf);
hoomd/md/test/test_harmonic_improper_force.cc:#include "hoomd/md/HarmonicImproperForceComputeGPU.h"
hoomd/md/test/test_harmonic_improper_force.cc:gpu_tf_creator(std::shared_ptr<SystemDefinition> sysdef)
hoomd/md/test/test_harmonic_improper_force.cc:        new HarmonicImproperForceComputeGPU(sysdef));
hoomd/md/test/test_harmonic_improper_force.cc://! test case for improper forces on the GPU
hoomd/md/test/test_harmonic_improper_force.cc:UP_TEST(HarmonicImproperForceComputeGPU_basic)
hoomd/md/test/test_harmonic_improper_force.cc:    printf(" IN UP_TEST: GPU \n");
hoomd/md/test/test_harmonic_improper_force.cc:    improperforce_creator tf_creator = bind(gpu_tf_creator, _1);
hoomd/md/test/test_harmonic_improper_force.cc:                                   new ExecutionConfiguration(ExecutionConfiguration::GPU)));
hoomd/md/test/test_harmonic_improper_force.cc://! test case for comparing bond GPU and CPU BondForceComputes
hoomd/md/test/test_harmonic_improper_force.cc:UP_TEST(HarmonicImproperForceComputeGPU_compare)
hoomd/md/test/test_harmonic_improper_force.cc:    improperforce_creator tf_creator_gpu = bind(gpu_tf_creator, _1);
hoomd/md/test/test_harmonic_improper_force.cc:                                    tf_creator_gpu,
hoomd/md/test/test_harmonic_improper_force.cc:                                        new ExecutionConfiguration(ExecutionConfiguration::GPU)));
hoomd/md/test/test_communication.cc:#include "hoomd/CommunicatorGPU.h"
hoomd/md/test/test_communication.cc://! Typedef for function that creates the Communicator on the CPU or GPU
hoomd/md/test/test_communication.cc:gpu_communicator_creator(std::shared_ptr<SystemDefinition> sysdef,
hoomd/md/test/test_communication.cc:    // communicate tags, necessary for gpu bond table
hoomd/md/test/test_communication.cc:        ArrayHandle<BondData::members_t> h_gpu_bondlist(bdata->getGPUTable(),
hoomd/md/test/test_communication.cc:        size_t pitch = bdata->getGPUTableIndexer().getW();
hoomd/md/test/test_communication.cc:        sorted_tags[0] = h_tag.data[h_gpu_bondlist.data[0].idx[0]];
hoomd/md/test/test_communication.cc:        sorted_tags[1] = h_tag.data[h_gpu_bondlist.data[pitch].idx[0]];
hoomd/md/test/test_communication.cc:        sorted_tags[2] = h_tag.data[h_gpu_bondlist.data[2 * pitch].idx[0]];
hoomd/md/test/test_communication.cc:gpu_communicator_creator(std::shared_ptr<SystemDefinition> sysdef,
hoomd/md/test/test_communication.cc:    return std::shared_ptr<hoomd::Communicator>(new hoomd::CommunicatorGPU(sysdef, decomposition));
hoomd/md/test/test_communication.cc:UP_SUITE_BEGIN(gpu_tests);
hoomd/md/test/test_communication.cc://! Tests particle distribution on GPU
hoomd/md/test/test_communication.cc:UP_TEST(DomainDecomposition_test_GPU)
hoomd/md/test/test_communication.cc:    if (!exec_conf_gpu)
hoomd/md/test/test_communication.cc:        exec_conf_gpu = std::shared_ptr<ExecutionConfiguration>(
hoomd/md/test/test_communication.cc:            new ExecutionConfiguration(ExecutionConfiguration::GPU));
hoomd/md/test/test_communication.cc:        new DomainDecomposition(exec_conf_gpu, box.getL()));
hoomd/md/test/test_communication.cc:    test_domain_decomposition(exec_conf_gpu, box, decomposition);
hoomd/md/test/test_communication.cc://! Tests balanced particle distribution on GPU
hoomd/md/test/test_communication.cc:UP_TEST(BalancedDomainDecomposition_test_GPU)
hoomd/md/test/test_communication.cc:    if (!exec_conf_gpu)
hoomd/md/test/test_communication.cc:        exec_conf_gpu = std::shared_ptr<ExecutionConfiguration>(
hoomd/md/test/test_communication.cc:            new ExecutionConfiguration(ExecutionConfiguration::GPU));
hoomd/md/test/test_communication.cc:UP_TEST(communicator_migrate_test_GPU)
hoomd/md/test/test_communication.cc:    if (!exec_conf_gpu)
hoomd/md/test/test_communication.cc:        exec_conf_gpu = std::shared_ptr<ExecutionConfiguration>(
hoomd/md/test/test_communication.cc:            new ExecutionConfiguration(ExecutionConfiguration::GPU));
hoomd/md/test/test_communication.cc:    communicator_creator communicator_creator_gpu = bind(gpu_communicator_creator, _1, _2);
hoomd/md/test/test_communication.cc:    test_communicator_migrate(communicator_creator_gpu, exec_conf_gpu, BoxDim(2.0));
hoomd/md/test/test_communication.cc:    test_communicator_migrate(communicator_creator_gpu, exec_conf_gpu, BoxDim(1.0, 2.0, 3.0));
hoomd/md/test/test_communication.cc:    test_communicator_migrate(communicator_creator_gpu, exec_conf_gpu, BoxDim(1.0, 0.5, 0.6, 0.8));
hoomd/md/test/test_communication.cc:    test_communicator_migrate(communicator_creator_gpu, exec_conf_gpu, BoxDim(1.0, -0.5, 0.7, 0.3));
hoomd/md/test/test_communication.cc:UP_TEST(communicator_balanced_migrate_test_GPU)
hoomd/md/test/test_communication.cc:    if (!exec_conf_gpu)
hoomd/md/test/test_communication.cc:        exec_conf_gpu = std::shared_ptr<ExecutionConfiguration>(
hoomd/md/test/test_communication.cc:            new ExecutionConfiguration(ExecutionConfiguration::GPU));
hoomd/md/test/test_communication.cc:    communicator_creator communicator_creator_gpu = bind(gpu_communicator_creator, _1, _2);
hoomd/md/test/test_communication.cc:    test_communicator_balanced_migrate(communicator_creator_gpu, exec_conf_gpu, BoxDim(2.0));
hoomd/md/test/test_communication.cc:    test_communicator_balanced_migrate(communicator_creator_gpu,
hoomd/md/test/test_communication.cc:                                       exec_conf_gpu,
hoomd/md/test/test_communication.cc:    test_communicator_balanced_migrate(communicator_creator_gpu,
hoomd/md/test/test_communication.cc:                                       exec_conf_gpu,
hoomd/md/test/test_communication.cc:    test_communicator_balanced_migrate(communicator_creator_gpu,
hoomd/md/test/test_communication.cc:                                       exec_conf_gpu,
hoomd/md/test/test_communication.cc:UP_TEST(communicator_ghosts_test_GPU)
hoomd/md/test/test_communication.cc:    if (!exec_conf_gpu)
hoomd/md/test/test_communication.cc:        exec_conf_gpu = std::shared_ptr<ExecutionConfiguration>(
hoomd/md/test/test_communication.cc:            new ExecutionConfiguration(ExecutionConfiguration::GPU));
hoomd/md/test/test_communication.cc:    communicator_creator communicator_creator_gpu = bind(gpu_communicator_creator, _1, _2);
hoomd/md/test/test_communication.cc:        test_communicator_ghosts(communicator_creator_gpu,
hoomd/md/test/test_communication.cc:                                 exec_conf_gpu,
hoomd/md/test/test_communication.cc:                                     new DomainDecomposition(exec_conf_gpu, box.getL())),
hoomd/md/test/test_communication.cc:        test_communicator_ghosts(communicator_creator_gpu,
hoomd/md/test/test_communication.cc:                                 exec_conf_gpu,
hoomd/md/test/test_communication.cc:                                     new DomainDecomposition(exec_conf_gpu, box.getL())),
hoomd/md/test/test_communication.cc:        test_communicator_ghosts(communicator_creator_gpu,
hoomd/md/test/test_communication.cc:                                 exec_conf_gpu,
hoomd/md/test/test_communication.cc:                                     new DomainDecomposition(exec_conf_gpu, box.getL())),
hoomd/md/test/test_communication.cc:            communicator_creator_gpu,
hoomd/md/test/test_communication.cc:            exec_conf_gpu,
hoomd/md/test/test_communication.cc:                new DomainDecomposition(exec_conf_gpu, box.getL(), fx, fy, fz)),
hoomd/md/test/test_communication.cc:            communicator_creator_gpu,
hoomd/md/test/test_communication.cc:            exec_conf_gpu,
hoomd/md/test/test_communication.cc:                new DomainDecomposition(exec_conf_gpu, box.getL(), fx, fy, fz)),
hoomd/md/test/test_communication.cc:            communicator_creator_gpu,
hoomd/md/test/test_communication.cc:            exec_conf_gpu,
hoomd/md/test/test_communication.cc:                new DomainDecomposition(exec_conf_gpu, box.getL(), fx, fy, fz)),
hoomd/md/test/test_communication.cc:UP_TEST(communicator_bonded_ghosts_test_GPU)
hoomd/md/test/test_communication.cc:    if (!exec_conf_gpu)
hoomd/md/test/test_communication.cc:        exec_conf_gpu = std::shared_ptr<ExecutionConfiguration>(
hoomd/md/test/test_communication.cc:            new ExecutionConfiguration(ExecutionConfiguration::GPU));
hoomd/md/test/test_communication.cc:    communicator_creator communicator_creator_gpu = bind(gpu_communicator_creator, _1, _2);
hoomd/md/test/test_communication.cc:            new DomainDecomposition(exec_conf_gpu, box.getL()));
hoomd/md/test/test_communication.cc:        test_communicator_bonded_ghosts(communicator_creator_gpu,
hoomd/md/test/test_communication.cc:                                        exec_conf_gpu,
hoomd/md/test/test_communication.cc:            new DomainDecomposition(exec_conf_gpu, box.getL(), fx, fy, fz));
hoomd/md/test/test_communication.cc:        test_communicator_bonded_ghosts(communicator_creator_gpu,
hoomd/md/test/test_communication.cc:                                        exec_conf_gpu,
hoomd/md/test/test_communication.cc:UP_TEST(communicator_bond_exchange_test_GPU)
hoomd/md/test/test_communication.cc:    if (!exec_conf_gpu)
hoomd/md/test/test_communication.cc:        exec_conf_gpu = std::shared_ptr<ExecutionConfiguration>(
hoomd/md/test/test_communication.cc:            new ExecutionConfiguration(ExecutionConfiguration::GPU));
hoomd/md/test/test_communication.cc:    communicator_creator communicator_creator_gpu = bind(gpu_communicator_creator, _1, _2);
hoomd/md/test/test_communication.cc:            new DomainDecomposition(exec_conf_gpu, box.getL()));
hoomd/md/test/test_communication.cc:        test_communicator_bond_exchange(communicator_creator_gpu,
hoomd/md/test/test_communication.cc:                                        exec_conf_gpu,
hoomd/md/test/test_communication.cc:            new DomainDecomposition(exec_conf_gpu, box.getL(), fx, fy, fz));
hoomd/md/test/test_communication.cc:        test_communicator_bond_exchange(communicator_creator_gpu,
hoomd/md/test/test_communication.cc:                                        exec_conf_gpu,
hoomd/md/test/test_communication.cc:UP_TEST(communicator_ghost_fields_test_GPU)
hoomd/md/test/test_communication.cc:    if (!exec_conf_gpu)
hoomd/md/test/test_communication.cc:        exec_conf_gpu = std::shared_ptr<ExecutionConfiguration>(
hoomd/md/test/test_communication.cc:            new ExecutionConfiguration(ExecutionConfiguration::GPU));
hoomd/md/test/test_communication.cc:    communicator_creator communicator_creator_gpu = bind(gpu_communicator_creator, _1, _2);
hoomd/md/test/test_communication.cc:    test_communicator_ghost_fields(communicator_creator_gpu, exec_conf_gpu);
hoomd/md/test/test_communication.cc:UP_TEST(communicator_ghost_layer_width_test_GPU)
hoomd/md/test/test_communication.cc:    if (!exec_conf_gpu)
hoomd/md/test/test_communication.cc:        exec_conf_gpu = std::shared_ptr<ExecutionConfiguration>(
hoomd/md/test/test_communication.cc:            new ExecutionConfiguration(ExecutionConfiguration::GPU));
hoomd/md/test/test_communication.cc:    communicator_creator communicator_creator_gpu = bind(gpu_communicator_creator, _1, _2);
hoomd/md/test/test_communication.cc:    test_communicator_ghost_layer_width(communicator_creator_gpu, exec_conf_gpu);
hoomd/md/test/test_communication.cc:UP_TEST(communicator_ghost_layer_per_type_test_GPU)
hoomd/md/test/test_communication.cc:    if (!exec_conf_gpu)
hoomd/md/test/test_communication.cc:        exec_conf_gpu = std::shared_ptr<ExecutionConfiguration>(
hoomd/md/test/test_communication.cc:            new ExecutionConfiguration(ExecutionConfiguration::GPU));
hoomd/md/test/test_communication.cc:    test_communicator_ghosts_per_type(communicator_creator_base, exec_conf_gpu, BoxDim(2.0));
hoomd/md/test/test_communication.cc:    if (!exec_conf_gpu)
hoomd/md/test/test_communication.cc:        exec_conf_gpu = std::shared_ptr<ExecutionConfiguration>(
hoomd/md/test/test_communication.cc:            new ExecutionConfiguration(ExecutionConfiguration::GPU));
hoomd/md/test/test_communication.cc:    communicator_creator communicator_creator_gpu = bind(gpu_communicator_creator, _1, _2);
hoomd/md/test/test_communication.cc:    std::shared_ptr<ExecutionConfiguration> exec_conf_2 = exec_conf_gpu;
hoomd/md/test/test_communication.cc:        // uniform case: compare cpu and gpu
hoomd/md/test/test_communication.cc:                                  communicator_creator_gpu,
hoomd/md/test/test_communication.cc:        // balanced case: compare cpu and gpu
hoomd/md/test/test_communication.cc:                                  communicator_creator_gpu,
hoomd/md/CosineSqAngleForceGPU.cu:#include "CosineSqAngleForceGPU.cuh"
hoomd/md/CosineSqAngleForceGPU.cu:/*! \file CosineSqAngleForceGPU.cu
hoomd/md/CosineSqAngleForceGPU.cu:    \brief Defines GPU kernel code for calculating the cosine squared angle forces. Used by
hoomd/md/CosineSqAngleForceGPU.cu:    CosineSqAngleForceComputeGPU.
hoomd/md/CosineSqAngleForceGPU.cu://! Kernel for calculating cosine squared angle forces on the GPU
hoomd/md/CosineSqAngleForceGPU.cu:    \param n_angles_list List of numbers of angles stored on the GPU
hoomd/md/CosineSqAngleForceGPU.cu:__global__ void gpu_compute_cosinesq_angle_forces_kernel(Scalar4* d_force,
hoomd/md/CosineSqAngleForceGPU.cu:    \param box Box dimensions (in GPU format) to use for periodic boundary conditions
hoomd/md/CosineSqAngleForceGPU.cu:    \param atable List of angles stored on the GPU
hoomd/md/CosineSqAngleForceGPU.cu:    \param n_angles_list List of numbers of angles stored on the GPU
hoomd/md/CosineSqAngleForceGPU.cu:hipError_t gpu_compute_cosinesq_angle_forces(Scalar4* d_force,
hoomd/md/CosineSqAngleForceGPU.cu:    hipFuncGetAttributes(&attr, (const void*)gpu_compute_cosinesq_angle_forces_kernel);
hoomd/md/CosineSqAngleForceGPU.cu:    hipLaunchKernelGGL((gpu_compute_cosinesq_angle_forces_kernel),
hoomd/md/ActiveForceComputeGPU.cu:#include "ActiveForceComputeGPU.cuh"
hoomd/md/ActiveForceComputeGPU.cu:/*! \file ActiveForceComputeGPU.cu
hoomd/md/ActiveForceComputeGPU.cu:    \brief Declares GPU kernel code for calculating active forces forces on the GPU. Used by
hoomd/md/ActiveForceComputeGPU.cu:   ActiveForceComputeGPU.
hoomd/md/ActiveForceComputeGPU.cu://! Kernel for setting active force vectors on the GPU
hoomd/md/ActiveForceComputeGPU.cu:__global__ void gpu_compute_active_force_set_forces_kernel(const unsigned int group_size,
hoomd/md/ActiveForceComputeGPU.cu://! Kernel for applying rotational diffusion to active force vectors on the GPU
hoomd/md/ActiveForceComputeGPU.cu:__global__ void gpu_compute_active_force_rotational_diffusion_kernel(const unsigned int group_size,
hoomd/md/ActiveForceComputeGPU.cu:hipError_t gpu_compute_active_force_set_forces(const unsigned int group_size,
hoomd/md/ActiveForceComputeGPU.cu:    hipLaunchKernelGGL((gpu_compute_active_force_set_forces_kernel),
hoomd/md/ActiveForceComputeGPU.cu:hipError_t gpu_compute_active_force_rotational_diffusion(const unsigned int group_size,
hoomd/md/ActiveForceComputeGPU.cu:    hipLaunchKernelGGL((gpu_compute_active_force_rotational_diffusion_kernel),
hoomd/md/PotentialExternalWallGPUKernel.cu.inc:#include "hoomd/md/PotentialExternalGPU.cuh"
hoomd/md/PotentialExternalWallGPUKernel.cu.inc:gpu_compute_potential_external_forces<EVALUATOR_CLASS>(
hoomd/md/NeighborListGPUTree.cc:/*! \file NeighborListGPUTree.cc
hoomd/md/NeighborListGPUTree.cc:    \brief Defines NeighborListGPUTree
hoomd/md/NeighborListGPUTree.cc:#include "NeighborListGPUTree.h"
hoomd/md/NeighborListGPUTree.cc:#include "NeighborListGPUTree.cuh"
hoomd/md/NeighborListGPUTree.cc:NeighborListGPUTree::NeighborListGPUTree(std::shared_ptr<SystemDefinition> sysdef, Scalar r_buff)
hoomd/md/NeighborListGPUTree.cc:    : NeighborListGPU(sysdef, r_buff), m_type_bits(1), m_lbvh_errors(m_exec_conf), m_n_images(0),
hoomd/md/NeighborListGPUTree.cc:    m_exec_conf->msg->notice(5) << "Constructing NeighborListGPUTree" << std::endl;
hoomd/md/NeighborListGPUTree.cc:        .connect<NeighborListGPUTree, &NeighborListGPUTree::slotBoxChanged>(this);
hoomd/md/NeighborListGPUTree.cc:        .connect<NeighborListGPUTree, &NeighborListGPUTree::slotMaxNumChanged>(this);
hoomd/md/NeighborListGPUTree.cc: * Any existing CUDA streams are destroyed with the object.
hoomd/md/NeighborListGPUTree.cc:NeighborListGPUTree::~NeighborListGPUTree()
hoomd/md/NeighborListGPUTree.cc:    m_exec_conf->msg->notice(5) << "Destroying NeighborListGPUTree" << std::endl;
hoomd/md/NeighborListGPUTree.cc:        .disconnect<NeighborListGPUTree, &NeighborListGPUTree::slotBoxChanged>(this);
hoomd/md/NeighborListGPUTree.cc:        .disconnect<NeighborListGPUTree, &NeighborListGPUTree::slotMaxNumChanged>(this);
hoomd/md/NeighborListGPUTree.cc:void NeighborListGPUTree::buildNlist(uint64_t timestep)
hoomd/md/NeighborListGPUTree.cc:        GPUArray<unsigned int> types(m_pdata->getMaxN(), m_exec_conf);
hoomd/md/NeighborListGPUTree.cc:        GPUArray<unsigned int> sorted_types(m_pdata->getMaxN(), m_exec_conf);
hoomd/md/NeighborListGPUTree.cc:        GPUArray<unsigned int> indexes(m_pdata->getMaxN(), m_exec_conf);
hoomd/md/NeighborListGPUTree.cc:        GPUArray<unsigned int> sorted_indexes(m_pdata->getMaxN(), m_exec_conf);
hoomd/md/NeighborListGPUTree.cc:        GPUArray<unsigned int> traverse_order(m_pdata->getMaxN(), m_exec_conf);
hoomd/md/NeighborListGPUTree.cc:            GPUArray<unsigned int> type_first(m_pdata->getNTypes(), m_exec_conf);
hoomd/md/NeighborListGPUTree.cc:            GPUArray<unsigned int> type_last(m_pdata->getNTypes(), m_exec_conf);
hoomd/md/NeighborListGPUTree.cc: * The builds and the traverser setup are done in CUDA streams. I believe that the build has
hoomd/md/NeighborListGPUTree.cc: * I also note that the use of autotuners in neighbor should break concurrency, since these CUDA
hoomd/md/NeighborListGPUTree.cc: * makes more use of CUDA streams anywhere.
hoomd/md/NeighborListGPUTree.cc:void NeighborListGPUTree::buildTree()
hoomd/md/NeighborListGPUTree.cc:            kernel::gpu_nlist_mark_types(d_types.data,
hoomd/md/NeighborListGPUTree.cc:            if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/NeighborListGPUTree.cc:                CHECK_CUDA_ERROR();
hoomd/md/NeighborListGPUTree.cc:            kernel::gpu_nlist_sort_types(d_tmp,
hoomd/md/NeighborListGPUTree.cc:            swap = kernel::gpu_nlist_sort_types(d_tmp,
hoomd/md/NeighborListGPUTree.cc:        kernel::gpu_nlist_count_types(d_type_first.data,
hoomd/md/NeighborListGPUTree.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/NeighborListGPUTree.cc:            CHECK_CUDA_ERROR();
hoomd/md/NeighborListGPUTree.cc:                kernel::gpu_nlist_copy_primitives(d_traverse_order.data + first,
hoomd/md/NeighborListGPUTree.cc:                if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/NeighborListGPUTree.cc:                    CHECK_CUDA_ERROR();
hoomd/md/NeighborListGPUTree.cc: * Traversal is performed for each particle type against all LBVHs. This is done using one CUDA
hoomd/md/NeighborListGPUTree.cc: * these CUDA timing events are placed in the default stream. This might be reconsidered in future
hoomd/md/NeighborListGPUTree.cc: * if HOOMD makes more use of CUDA streams anywhere.
hoomd/md/NeighborListGPUTree.cc:void NeighborListGPUTree::traverseTree()
hoomd/md/NeighborListGPUTree.cc:void NeighborListGPUTree::updateImageVectors()
hoomd/md/NeighborListGPUTree.cc:void export_NeighborListGPUTree(pybind11::module& m)
hoomd/md/NeighborListGPUTree.cc:    pybind11::class_<NeighborListGPUTree, NeighborListGPU, std::shared_ptr<NeighborListGPUTree>>(
hoomd/md/NeighborListGPUTree.cc:        "NeighborListGPUTree")
hoomd/md/PotentialBond.h:#include "hoomd/GPUArray.h"
hoomd/md/PotentialBond.h:    GPUArray<param_type> m_params;      //!< Bond parameters per type
hoomd/md/PotentialBond.h:    GPUArray<param_type> params(m_bond_data->getNTypes(), m_exec_conf);
hoomd/md/PotentialBond.h:    GPUArray<param_type> params(m_bond_data->getNTypes(), m_exec_conf);
hoomd/md/BondTablePotentialGPU.h:#include "BondTablePotentialGPU.cuh"
hoomd/md/BondTablePotentialGPU.h:/*! \file BondTablePotentialGPU.h
hoomd/md/BondTablePotentialGPU.h:    \brief Declares the BondTablePotentialGPU class
hoomd/md/BondTablePotentialGPU.h:#ifndef __BONDTABLEPOTENTIALGPU_H__
hoomd/md/BondTablePotentialGPU.h:#define __BONDTABLEPOTENTIALGPU_H__
hoomd/md/BondTablePotentialGPU.h://! Compute table based bond potentials on the GPU
hoomd/md/BondTablePotentialGPU.h:/*! Calculates exactly the same thing as BondTablePotential, but on the GPU
hoomd/md/BondTablePotentialGPU.h:    The GPU kernel for calculating this can be found in BondTablePotentialGPU.cu/
hoomd/md/BondTablePotentialGPU.h:class PYBIND11_EXPORT BondTablePotentialGPU : public BondTablePotential
hoomd/md/BondTablePotentialGPU.h:    BondTablePotentialGPU(std::shared_ptr<SystemDefinition> sysdef, unsigned int table_width);
hoomd/md/BondTablePotentialGPU.h:    virtual ~BondTablePotentialGPU();
hoomd/md/BondTablePotentialGPU.h:    GPUArray<unsigned int> m_flags;        //!< Flags set during the kernel execution
hoomd/md/MuellerPlatheFlowGPU.cu:#include "MuellerPlatheFlowGPU.cuh"
hoomd/md/MuellerPlatheFlowGPU.cu:#include "MuellerPlatheFlowGPU.h"
hoomd/md/MuellerPlatheFlowGPU.cu:hipError_t gpu_search_min_max_velocity(const unsigned int group_size,
hoomd/md/MuellerPlatheFlowGPU.cu:void __global__ gpu_update_min_max_velocity_kernel(const unsigned int* const d_rtag,
hoomd/md/MuellerPlatheFlowGPU.cu:hipError_t gpu_update_min_max_velocity(const unsigned int* const d_rtag,
hoomd/md/MuellerPlatheFlowGPU.cu:    hipLaunchKernelGGL((gpu_update_min_max_velocity_kernel),
hoomd/md/TwoStepLangevinBase.cc:    if (m_exec_conf->isCUDAEnabled() && m_exec_conf->allConcurrentManagedAccess())
hoomd/md/TwoStepLangevinBase.cc:        cudaMemAdvise(m_gamma.get(),
hoomd/md/TwoStepLangevinBase.cc:                      cudaMemAdviseSetReadMostly,
hoomd/md/TwoStepLangevinBase.cc:        cudaMemAdvise(m_gamma_r.get(),
hoomd/md/TwoStepLangevinBase.cc:                      cudaMemAdviseSetReadMostly,
hoomd/md/ComputeThermoHMA.cc:    GPUArray<Scalar> properties(thermoHMA_index::num_quantities, m_exec_conf);
hoomd/md/TableAngleForceComputeGPU.cc:#include "TableAngleForceComputeGPU.h"
hoomd/md/TableAngleForceComputeGPU.cc:/*! \file TableAngleForceComputeGPU.cc
hoomd/md/TableAngleForceComputeGPU.cc:    \brief Defines the TableAngleForceComputeGPU class
hoomd/md/TableAngleForceComputeGPU.cc:TableAngleForceComputeGPU::TableAngleForceComputeGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/md/TableAngleForceComputeGPU.cc:    // can't run on the GPU if there aren't any GPUs in the execution configuration
hoomd/md/TableAngleForceComputeGPU.cc:    if (!m_exec_conf->isCUDAEnabled())
hoomd/md/TableAngleForceComputeGPU.cc:            << "Creating a BondTableForceComputeGPU with no GPU in the execution configuration"
hoomd/md/TableAngleForceComputeGPU.cc:        throw std::runtime_error("Error initializing BondTableForceComputeGPU");
hoomd/md/TableAngleForceComputeGPU.cc:    // allocate flags storage on the GPU
hoomd/md/TableAngleForceComputeGPU.cc:    GPUArray<unsigned int> flags(1, this->m_exec_conf);
hoomd/md/TableAngleForceComputeGPU.cc:Calls gpu_compute_bondtable_forces to do the leg work
hoomd/md/TableAngleForceComputeGPU.cc:void TableAngleForceComputeGPU::computeForces(uint64_t timestep)
hoomd/md/TableAngleForceComputeGPU.cc:        ArrayHandle<group_storage<3>> d_gpu_anglelist(m_angle_data->getGPUTable(),
hoomd/md/TableAngleForceComputeGPU.cc:        ArrayHandle<unsigned int> d_gpu_angle_pos_list(m_angle_data->getGPUPosTable(),
hoomd/md/TableAngleForceComputeGPU.cc:        ArrayHandle<unsigned int> d_gpu_n_angles(m_angle_data->getNGroupsArray(),
hoomd/md/TableAngleForceComputeGPU.cc:        // run the kernel on all GPUs in parallel
hoomd/md/TableAngleForceComputeGPU.cc:        kernel::gpu_compute_table_angle_forces(d_force.data,
hoomd/md/TableAngleForceComputeGPU.cc:                                               d_gpu_anglelist.data,
hoomd/md/TableAngleForceComputeGPU.cc:                                               d_gpu_angle_pos_list.data,
hoomd/md/TableAngleForceComputeGPU.cc:                                               m_angle_data->getGPUTableIndexer().getW(),
hoomd/md/TableAngleForceComputeGPU.cc:                                               d_gpu_n_angles.data,
hoomd/md/TableAngleForceComputeGPU.cc:    if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/TableAngleForceComputeGPU.cc:        CHECK_CUDA_ERROR();
hoomd/md/TableAngleForceComputeGPU.cc:void export_TableAngleForceComputeGPU(pybind11::module& m)
hoomd/md/TableAngleForceComputeGPU.cc:    pybind11::class_<TableAngleForceComputeGPU,
hoomd/md/TableAngleForceComputeGPU.cc:                     std::shared_ptr<TableAngleForceComputeGPU>>(m, "TableAngleForceComputeGPU")
hoomd/md/TableAngleForceComputeGPU.h:#include "TableAngleForceGPU.cuh"
hoomd/md/TableAngleForceComputeGPU.h:/*! \file TableAngleForceComputeGPU.h
hoomd/md/TableAngleForceComputeGPU.h:    \brief Declares the TableAngleForceComputeGPU class
hoomd/md/TableAngleForceComputeGPU.h:#ifndef __TABLEANGLEFORCECOMPUTEGPU_H__
hoomd/md/TableAngleForceComputeGPU.h:#define __TABLEANGLEFORCECOMPUTEGPU_H__
hoomd/md/TableAngleForceComputeGPU.h://! Compute table based bond potentials on the GPU
hoomd/md/TableAngleForceComputeGPU.h:/*! Calculates exactly the same thing as TableAngleForceCompute, but on the GPU
hoomd/md/TableAngleForceComputeGPU.h:    The GPU kernel for calculating this can be found in TableAngleForceComputeGPU.cu/
hoomd/md/TableAngleForceComputeGPU.h:class PYBIND11_EXPORT TableAngleForceComputeGPU : public TableAngleForceCompute
hoomd/md/TableAngleForceComputeGPU.h:    TableAngleForceComputeGPU(std::shared_ptr<SystemDefinition> sysdef, unsigned int table_width);
hoomd/md/TableAngleForceComputeGPU.h:    virtual ~TableAngleForceComputeGPU() { }
hoomd/md/TableAngleForceComputeGPU.h:    GPUArray<unsigned int> m_flags;        //!< Flags set during the kernel execution
hoomd/md/TwoStepConstantVolumeGPU.h:#ifndef HOOMD_TWOSTEPCONSTANTVOLUMEGPU_H
hoomd/md/TwoStepConstantVolumeGPU.h:#define HOOMD_TWOSTEPCONSTANTVOLUMEGPU_H
hoomd/md/TwoStepConstantVolumeGPU.h:/// Implement TwoStepConstantVolume on the GPU.
hoomd/md/TwoStepConstantVolumeGPU.h:class PYBIND11_EXPORT TwoStepConstantVolumeGPU : public TwoStepConstantVolume
hoomd/md/TwoStepConstantVolumeGPU.h:    TwoStepConstantVolumeGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/md/TwoStepConstantVolumeGPU.h:    virtual ~TwoStepConstantVolumeGPU() { }
hoomd/md/TwoStepConstantVolumeGPU.h:#endif // HOOMD_TWOSTEPCONSTANTVOLUMEGPU_H
hoomd/md/EvaluatorTersoff.h:        //! Set CUDA memory hints
hoomd/md/PPPMForceComputeGPU.cuh:#include "hoomd/GPUPartition.cuh"
hoomd/md/PPPMForceComputeGPU.cuh:void gpu_assign_particles(const uint3 mesh_dim,
hoomd/md/PPPMForceComputeGPU.cuh:                          const GPUPartition& gpu_partition);
hoomd/md/PPPMForceComputeGPU.cuh:void gpu_reduce_meshes(const unsigned int mesh_elements,
hoomd/md/PPPMForceComputeGPU.cuh:                       const unsigned int ngpu,
hoomd/md/PPPMForceComputeGPU.cuh:void gpu_compute_mesh_virial(const unsigned int n_wave_vectors,
hoomd/md/PPPMForceComputeGPU.cuh:void gpu_update_meshes(const unsigned int n_wave_vectors,
hoomd/md/PPPMForceComputeGPU.cuh:void gpu_compute_forces(const unsigned int N,
hoomd/md/PPPMForceComputeGPU.cuh:                        const GPUPartition& gpu_partition,
hoomd/md/PPPMForceComputeGPU.cuh:                        const GPUPartition& all_gpu_partition,
hoomd/md/PPPMForceComputeGPU.cuh:void gpu_compute_pe(unsigned int n_wave_vectors,
hoomd/md/PPPMForceComputeGPU.cuh:void gpu_compute_virial(unsigned int n_wave_vectors,
hoomd/md/PPPMForceComputeGPU.cuh:void gpu_compute_influence_function(const uint3 mesh_dim,
hoomd/md/PPPMForceComputeGPU.cuh:hipError_t gpu_fix_exclusions(Scalar4* d_force,
hoomd/md/PPPMForceComputeGPU.cuh:void gpu_initialize_coeff(Scalar* CPU_rho_coeff, int order, const GPUPartition& gpu_partition);
hoomd/md/AnisoPotentialPairDipoleGPU.cc:#include "AnisoPotentialPairGPU.h"
hoomd/md/AnisoPotentialPairDipoleGPU.cc:void export_AnisoPotentialPairDipoleGPU(pybind11::module& m)
hoomd/md/AnisoPotentialPairDipoleGPU.cc:    export_AnisoPotentialPairGPU<EvaluatorPairDipole>(m, "AnisoPotentialPairDipoleGPU");
hoomd/md/EvaluatorPairTWF.h:        //! Set CUDA memory hints
hoomd/md/EvaluatorPairTWF.h:                // Since std::numeric_limit<>::max cannot be used for GPU
hoomd/md/TwoStepLangevinGPU.cu:#include "TwoStepLangevinGPU.cuh"
hoomd/md/TwoStepLangevinGPU.cu:/*! \file TwoStepLangevinGPU.cu
hoomd/md/TwoStepLangevinGPU.cu:    \brief Defines GPU kernel code for Langevin integration on the GPU. Used by TwoStepLangevinGPU.
hoomd/md/TwoStepLangevinGPU.cu:    This kernel is implemented in a very similar manner to gpu_nve_step_two_kernel(), see it for
hoomd/md/TwoStepLangevinGPU.cu:__global__ void gpu_langevin_step_two_kernel(const Scalar4* d_pos,
hoomd/md/TwoStepLangevinGPU.cu:gpu_bdtally_reduce_partial_sum_kernel(Scalar* d_sum, Scalar* d_partial_sum, unsigned int num_blocks)
hoomd/md/TwoStepLangevinGPU.cu:__global__ void gpu_langevin_angular_step_two_kernel(const Scalar4* d_pos,
hoomd/md/TwoStepLangevinGPU.cu:    \param langevin_args Collected arguments for gpu_langevin_step_two_kernel() and
hoomd/md/TwoStepLangevinGPU.cu:   gpu_langevin_angular_step_two() \param deltaT timestep \param D dimensionality of the system
hoomd/md/TwoStepLangevinGPU.cu:    This is just a driver for gpu_langevin_angular_step_two_kernel(), see it for details.
hoomd/md/TwoStepLangevinGPU.cu:hipError_t gpu_langevin_angular_step_two(const Scalar4* d_pos,
hoomd/md/TwoStepLangevinGPU.cu:    hipLaunchKernelGGL(gpu_langevin_angular_step_two_kernel,
hoomd/md/TwoStepLangevinGPU.cu:    \param langevin_args Collected arguments for gpu_langevin_step_two_kernel() and
hoomd/md/TwoStepLangevinGPU.cu:   gpu_langevin_angular_step_two() \param deltaT Amount of real time to step forward in one time
hoomd/md/TwoStepLangevinGPU.cu:    This is just a driver for gpu_langevin_step_two_kernel(), see it for details.
hoomd/md/TwoStepLangevinGPU.cu:hipError_t gpu_langevin_step_two(const Scalar4* d_pos,
hoomd/md/TwoStepLangevinGPU.cu:    hipLaunchKernelGGL((gpu_langevin_step_two_kernel),
hoomd/md/TwoStepLangevinGPU.cu:        hipLaunchKernelGGL((gpu_bdtally_reduce_partial_sum_kernel),
hoomd/md/export_PotentialExternalWallGPU.cc.inc:#include "hoomd/md/PotentialExternalGPU.h"
hoomd/md/export_PotentialExternalWallGPU.cc.inc:#define EXPORT_FUNCTION export_PotentialExternalWall@_evaluator@GPU
hoomd/md/export_PotentialExternalWallGPU.cc.inc:    export_PotentialExternalGPU<EVALUATOR_CLASS>(m, "WallsPotential@_evaluator@GPU");
hoomd/md/VolumeConservationMeshForceComputeGPU.cuh:/*! \file MeshVolumeConservationGPU.cuh
hoomd/md/VolumeConservationMeshForceComputeGPU.cuh:    \brief Declares GPU kernel code for calculating the volume constraint forces. Used by
hoomd/md/VolumeConservationMeshForceComputeGPU.cuh:   MeshVolumeConservationGPU.
hoomd/md/VolumeConservationMeshForceComputeGPU.cuh://! Kernel driver that computes the volume for MeshVolumeConservationGPU
hoomd/md/VolumeConservationMeshForceComputeGPU.cuh:hipError_t gpu_compute_volume_constraint_volume(Scalar* d_sum_volume,
hoomd/md/VolumeConservationMeshForceComputeGPU.cuh://! Kernel driver that computes the forces for MeshVolumeConservationGPU
hoomd/md/VolumeConservationMeshForceComputeGPU.cuh:hipError_t gpu_compute_volume_constraint_force(Scalar4* d_force,
hoomd/md/HarmonicDihedralForceGPU.cuh:/*! \file HarmonicDihedralForceGPU.cuh
hoomd/md/HarmonicDihedralForceGPU.cuh:    \brief Declares GPU kernel code for calculating the harmonic dihedral forces. Used by
hoomd/md/HarmonicDihedralForceGPU.cuh:   HarmonicDihedralForceComputeGPU.
hoomd/md/HarmonicDihedralForceGPU.cuh:#ifndef __HARMONICDIHEDRALFORCEGPU_CUH__
hoomd/md/HarmonicDihedralForceGPU.cuh:#define __HARMONICDIHEDRALFORCEGPU_CUH__
hoomd/md/HarmonicDihedralForceGPU.cuh://! Kernel driver that computes harmonic dihedral forces for HarmonicDihedralForceComputeGPU
hoomd/md/HarmonicDihedralForceGPU.cuh:hipError_t gpu_compute_harmonic_dihedral_forces(Scalar4* d_force,
hoomd/md/MuellerPlatheFlowGPU.cuh:/*! \file MuellerPlatheFlowGPU.cuh
hoomd/md/MuellerPlatheFlowGPU.cuh:    \brief Declares GPU kernel code for calculating MinMax velocities and updates for the flow.
hoomd/md/MuellerPlatheFlowGPU.cuh:#ifndef __MUELLER_PLATHE_FLOW_GPU_CUH__
hoomd/md/MuellerPlatheFlowGPU.cuh:#define __MUELLER_PLATHE_FLOW_GPU_CUH__
hoomd/md/MuellerPlatheFlowGPU.cuh:hipError_t gpu_search_min_max_velocity(const unsigned int group_size,
hoomd/md/MuellerPlatheFlowGPU.cuh:hipError_t gpu_update_min_max_velocity(const unsigned int* const d_rtag,
hoomd/md/MuellerPlatheFlowGPU.cuh:#endif //__MUELLER_PLATHE_FLOW_GPU_CUH__
hoomd/md/EvaluatorPairExpandedMie.h:        //! set CUDA memory hints
hoomd/md/PotentialBondGPUKernel.cu.inc:#include "hoomd/md/PotentialBondGPU.cuh"
hoomd/md/PotentialBondGPUKernel.cu.inc:gpu_compute_bond_forces<EVALUATOR_CLASS, 2>(const kernel::bond_args_t<2>& bond_args,
hoomd/md/special_pair.py:            cpp_cls = getattr(self._ext_module, self._cpp_class_name + "GPU")
hoomd/md/module-md.cc:void export_ActiveForceConstraintComputeCylinderGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_ActiveForceConstraintComputeDiamondGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_ActiveForceConstraintComputeEllipsoidGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_ActiveForceConstraintComputeGyroidGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_ActiveForceConstraintComputePlaneGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_ActiveForceConstraintComputePrimitiveGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_ActiveForceConstraintComputeSphereGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_ActiveForceComputeGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_ComputeThermoGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_ComputeThermoHMAGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_ConstantForceComputeGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_HarmonicAngleForceComputeGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_CosineSqAngleForceComputeGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_TableAngleForceComputeGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_HarmonicDihedralForceComputeGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_OPLSDihedralForceComputeGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_TableDihedralForceComputeGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_HarmonicImproperForceComputeGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_BondTablePotentialGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_NeighborListGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_NeighborListGPUBinned(pybind11::module& m);
hoomd/md/module-md.cc:void export_NeighborListGPUStencil(pybind11::module& m);
hoomd/md/module-md.cc:void export_NeighborListGPUTree(pybind11::module& m);
hoomd/md/module-md.cc:void export_ForceDistanceConstraintGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_ForceCompositeGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_PeriodicImproperForceComputeGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_PPPMForceComputeGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_LocalNeighborListDataGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_PotentialPairBuckinghamGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_PotentialPairLJGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_PotentialPairLJ1208GPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_PotentialPairLJ0804GPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_PotentialPairGaussGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_PotentialPairExpandedLJGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_PotentialPairExpandedGaussianGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_PotentialPairExpandedMieGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_PotentialPairYukawaGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_PotentialPairEwaldGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_PotentialPairMorseGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_PotentialPairMoliereGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_PotentialPairZBLGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_PotentialPairMieGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_PotentialPairReactionFieldGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_PotentialPairDLVOGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_PotentialPairFourierGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_PotentialPairOPPGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_PotentialPairTWFGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_PotentialPairLJGaussGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_PotentialPairForceShiftedLJGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_PotentialPairTableGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_PotentialPairConservativeDPDGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_AnisoPotentialPairALJ2DGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_AnisoPotentialPairALJ3DGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_AnisoPotentialPairDipoleGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_AnisoPotentialPairGBGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_AnisoPotentialPairPatchyExpandedGaussianGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_AnisoPotentialPairPatchyExpandedLJGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_AnisoPotentialPairPatchyExpandedMieGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_AnisoPotentialPairPatchyGaussGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_AnisoPotentialPairPatchyLJGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_AnisoPotentialPairPatchyMieGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_AnisoPotentialPairPatchyYukawaGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_AnisoPotentialPairPatchyTableGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_PotentialBondHarmonicGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_PotentialBondFENEGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_PotentialBondTetherGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_PotentialMeshBondHarmonicGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_PotentialMeshBondFENEGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_PotentialMeshBondTetherGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_BendingRigidityMeshForceComputeGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_HelfrichMeshForceComputeGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_VolumeConservationMeshForceComputeGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_AreaConservationMeshForceComputeGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_TriangleAreaConservationMeshForceComputeGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_PotentialSpecialPairLJGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_PotentialSpecialPairCoulombGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_PotentialTersoffGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_PotentialSquareDensityGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_PotentialRevCrossGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_PotentialExternalPeriodicGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_PotentialExternalElectricFieldGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_PotentialExternalMagneticFieldGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_PotentialExternalWallLJGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_PotentialExternalWallYukawaGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_PotentialExternalWallForceShiftedLJGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_PotentialExternalWallMieGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_PotentialExternalWallGaussGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_PotentialExternalWallMorseGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_PotentialPairDPDThermoDPDGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_PotentialPairDPDThermoLJGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_TwoStepConstantVolumeGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_TwoStepLangevinGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_TwoStepBDGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_TwoStepConstantPressureGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_FIREEnergyMinimizerGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_MuellerPlatheFlowGPU(pybind11::module& m);
hoomd/md/module-md.cc:void export_TwoStepRATTLEBDGPUCylinder(pybind11::module& m);
hoomd/md/module-md.cc:void export_TwoStepRATTLEBDGPUDiamond(pybind11::module& m);
hoomd/md/module-md.cc:void export_TwoStepRATTLEBDGPUEllipsoid(pybind11::module& m);
hoomd/md/module-md.cc:void export_TwoStepRATTLEBDGPUGyroid(pybind11::module& m);
hoomd/md/module-md.cc:void export_TwoStepRATTLEBDGPUPlane(pybind11::module& m);
hoomd/md/module-md.cc:void export_TwoStepRATTLEBDGPUPrimitive(pybind11::module& m);
hoomd/md/module-md.cc:void export_TwoStepRATTLEBDGPUSphere(pybind11::module& m);
hoomd/md/module-md.cc:void export_TwoStepRATTLELangevinGPUCylinder(pybind11::module& m);
hoomd/md/module-md.cc:void export_TwoStepRATTLELangevinGPUDiamond(pybind11::module& m);
hoomd/md/module-md.cc:void export_TwoStepRATTLELangevinGPUEllipsoid(pybind11::module& m);
hoomd/md/module-md.cc:void export_TwoStepRATTLELangevinGPUGyroid(pybind11::module& m);
hoomd/md/module-md.cc:void export_TwoStepRATTLELangevinGPUPlane(pybind11::module& m);
hoomd/md/module-md.cc:void export_TwoStepRATTLELangevinGPUPrimitive(pybind11::module& m);
hoomd/md/module-md.cc:void export_TwoStepRATTLELangevinGPUSphere(pybind11::module& m);
hoomd/md/module-md.cc:void export_TwoStepRATTLENVEGPUCylinder(pybind11::module& m);
hoomd/md/module-md.cc:void export_TwoStepRATTLENVEGPUDiamond(pybind11::module& m);
hoomd/md/module-md.cc:void export_TwoStepRATTLENVEGPUEllipsoid(pybind11::module& m);
hoomd/md/module-md.cc:void export_TwoStepRATTLENVEGPUGyroid(pybind11::module& m);
hoomd/md/module-md.cc:void export_TwoStepRATTLENVEGPUPlane(pybind11::module& m);
hoomd/md/module-md.cc:void export_TwoStepRATTLENVEGPUPrimitive(pybind11::module& m);
hoomd/md/module-md.cc:void export_TwoStepRATTLENVEGPUSphere(pybind11::module& m);
hoomd/md/module-md.cc:    export_NeighborListGPU(m);
hoomd/md/module-md.cc:    export_NeighborListGPUBinned(m);
hoomd/md/module-md.cc:    export_NeighborListGPUStencil(m);
hoomd/md/module-md.cc:    export_NeighborListGPUTree(m);
hoomd/md/module-md.cc:    export_ForceCompositeGPU(m);
hoomd/md/module-md.cc:    export_LocalNeighborListDataGPU(m);
hoomd/md/module-md.cc:    export_PotentialPairBuckinghamGPU(m);
hoomd/md/module-md.cc:    export_PotentialPairLJGPU(m);
hoomd/md/module-md.cc:    export_PotentialPairLJ1208GPU(m);
hoomd/md/module-md.cc:    export_PotentialPairLJ0804GPU(m);
hoomd/md/module-md.cc:    export_PotentialPairGaussGPU(m);
hoomd/md/module-md.cc:    export_PotentialPairExpandedLJGPU(m);
hoomd/md/module-md.cc:    export_PotentialPairExpandedGaussianGPU(m);
hoomd/md/module-md.cc:    export_PotentialPairExpandedMieGPU(m);
hoomd/md/module-md.cc:    export_PotentialPairYukawaGPU(m);
hoomd/md/module-md.cc:    export_PotentialPairEwaldGPU(m);
hoomd/md/module-md.cc:    export_PotentialPairMorseGPU(m);
hoomd/md/module-md.cc:    export_PotentialPairMoliereGPU(m);
hoomd/md/module-md.cc:    export_PotentialPairZBLGPU(m);
hoomd/md/module-md.cc:    export_PotentialPairMieGPU(m);
hoomd/md/module-md.cc:    export_PotentialPairReactionFieldGPU(m);
hoomd/md/module-md.cc:    export_PotentialPairDLVOGPU(m);
hoomd/md/module-md.cc:    export_PotentialPairFourierGPU(m);
hoomd/md/module-md.cc:    export_PotentialPairOPPGPU(m);
hoomd/md/module-md.cc:    export_PotentialPairTWFGPU(m);
hoomd/md/module-md.cc:    export_PotentialPairLJGaussGPU(m);
hoomd/md/module-md.cc:    export_PotentialPairForceShiftedLJGPU(m);
hoomd/md/module-md.cc:    export_PotentialPairTableGPU(m);
hoomd/md/module-md.cc:    export_PotentialPairConservativeDPDGPU(m);
hoomd/md/module-md.cc:    export_PotentialTersoffGPU(m);
hoomd/md/module-md.cc:    export_PotentialSquareDensityGPU(m);
hoomd/md/module-md.cc:    export_PotentialRevCrossGPU(m);
hoomd/md/module-md.cc:    export_PotentialPairDPDThermoDPDGPU(m);
hoomd/md/module-md.cc:    export_PotentialPairDPDThermoLJGPU(m);
hoomd/md/module-md.cc:    export_AnisoPotentialPairALJ2DGPU(m);
hoomd/md/module-md.cc:    export_AnisoPotentialPairALJ3DGPU(m);
hoomd/md/module-md.cc:    export_AnisoPotentialPairDipoleGPU(m);
hoomd/md/module-md.cc:    export_AnisoPotentialPairGBGPU(m);
hoomd/md/module-md.cc:    export_AnisoPotentialPairPatchyExpandedGaussianGPU(m);
hoomd/md/module-md.cc:    export_AnisoPotentialPairPatchyExpandedLJGPU(m);
hoomd/md/module-md.cc:    export_AnisoPotentialPairPatchyExpandedMieGPU(m);
hoomd/md/module-md.cc:    export_AnisoPotentialPairPatchyGaussGPU(m);
hoomd/md/module-md.cc:    export_AnisoPotentialPairPatchyLJGPU(m);
hoomd/md/module-md.cc:    export_AnisoPotentialPairPatchyMieGPU(m);
hoomd/md/module-md.cc:    export_AnisoPotentialPairPatchyYukawaGPU(m);
hoomd/md/module-md.cc:    export_AnisoPotentialPairPatchyTableGPU(m);
hoomd/md/module-md.cc:    export_PotentialBondHarmonicGPU(m);
hoomd/md/module-md.cc:    export_PotentialBondFENEGPU(m);
hoomd/md/module-md.cc:    export_PotentialBondTetherGPU(m);
hoomd/md/module-md.cc:    export_PotentialMeshBondHarmonicGPU(m);
hoomd/md/module-md.cc:    export_PotentialMeshBondFENEGPU(m);
hoomd/md/module-md.cc:    export_PotentialMeshBondTetherGPU(m);
hoomd/md/module-md.cc:    export_BendingRigidityMeshForceComputeGPU(m);
hoomd/md/module-md.cc:    export_HelfrichMeshForceComputeGPU(m);
hoomd/md/module-md.cc:    export_VolumeConservationMeshForceComputeGPU(m);
hoomd/md/module-md.cc:    export_AreaConservationMeshForceComputeGPU(m);
hoomd/md/module-md.cc:    export_TriangleAreaConservationMeshForceComputeGPU(m);
hoomd/md/module-md.cc:    export_PotentialSpecialPairLJGPU(m);
hoomd/md/module-md.cc:    export_PotentialSpecialPairCoulombGPU(m);
hoomd/md/module-md.cc:    export_BondTablePotentialGPU(m);
hoomd/md/module-md.cc:    export_HarmonicAngleForceComputeGPU(m);
hoomd/md/module-md.cc:    export_CosineSqAngleForceComputeGPU(m);
hoomd/md/module-md.cc:    export_TableAngleForceComputeGPU(m);
hoomd/md/module-md.cc:    export_HarmonicDihedralForceComputeGPU(m);
hoomd/md/module-md.cc:    export_OPLSDihedralForceComputeGPU(m);
hoomd/md/module-md.cc:    export_TableDihedralForceComputeGPU(m);
hoomd/md/module-md.cc:    export_HarmonicImproperForceComputeGPU(m);
hoomd/md/module-md.cc:    export_ForceDistanceConstraintGPU(m);
hoomd/md/module-md.cc:    export_ComputeThermoGPU(m);
hoomd/md/module-md.cc:    export_ComputeThermoHMAGPU(m);
hoomd/md/module-md.cc:    export_PeriodicImproperForceComputeGPU(m);
hoomd/md/module-md.cc:    export_PPPMForceComputeGPU(m);
hoomd/md/module-md.cc:    export_ActiveForceComputeGPU(m);
hoomd/md/module-md.cc:    export_ActiveForceConstraintComputeCylinderGPU(m);
hoomd/md/module-md.cc:    export_ActiveForceConstraintComputeDiamondGPU(m);
hoomd/md/module-md.cc:    export_ActiveForceConstraintComputeEllipsoidGPU(m);
hoomd/md/module-md.cc:    export_ActiveForceConstraintComputeGyroidGPU(m);
hoomd/md/module-md.cc:    export_ActiveForceConstraintComputePlaneGPU(m);
hoomd/md/module-md.cc:    export_ActiveForceConstraintComputePrimitiveGPU(m);
hoomd/md/module-md.cc:    export_ActiveForceConstraintComputeSphereGPU(m);
hoomd/md/module-md.cc:    export_ConstantForceComputeGPU(m);
hoomd/md/module-md.cc:    export_PotentialExternalPeriodicGPU(m);
hoomd/md/module-md.cc:    export_PotentialExternalElectricFieldGPU(m);
hoomd/md/module-md.cc:    export_PotentialExternalMagneticFieldGPU(m);
hoomd/md/module-md.cc:    export_PotentialExternalWallLJGPU(m);
hoomd/md/module-md.cc:    export_PotentialExternalWallYukawaGPU(m);
hoomd/md/module-md.cc:    export_PotentialExternalWallForceShiftedLJGPU(m);
hoomd/md/module-md.cc:    export_PotentialExternalWallMieGPU(m);
hoomd/md/module-md.cc:    export_PotentialExternalWallGaussGPU(m);
hoomd/md/module-md.cc:    export_PotentialExternalWallMorseGPU(m);
hoomd/md/module-md.cc:    export_TwoStepConstantVolumeGPU(m);
hoomd/md/module-md.cc:    export_TwoStepLangevinGPU(m);
hoomd/md/module-md.cc:    export_TwoStepBDGPU(m);
hoomd/md/module-md.cc:    export_TwoStepConstantPressureGPU(m);
hoomd/md/module-md.cc:    export_FIREEnergyMinimizerGPU(m);
hoomd/md/module-md.cc:    export_MuellerPlatheFlowGPU(m);
hoomd/md/module-md.cc:    export_TwoStepRATTLEBDGPUCylinder(m);
hoomd/md/module-md.cc:    export_TwoStepRATTLEBDGPUDiamond(m);
hoomd/md/module-md.cc:    export_TwoStepRATTLEBDGPUEllipsoid(m);
hoomd/md/module-md.cc:    export_TwoStepRATTLEBDGPUGyroid(m);
hoomd/md/module-md.cc:    export_TwoStepRATTLEBDGPUPlane(m);
hoomd/md/module-md.cc:    export_TwoStepRATTLEBDGPUPrimitive(m);
hoomd/md/module-md.cc:    export_TwoStepRATTLEBDGPUSphere(m);
hoomd/md/module-md.cc:    export_TwoStepRATTLELangevinGPUCylinder(m);
hoomd/md/module-md.cc:    export_TwoStepRATTLELangevinGPUDiamond(m);
hoomd/md/module-md.cc:    export_TwoStepRATTLELangevinGPUEllipsoid(m);
hoomd/md/module-md.cc:    export_TwoStepRATTLELangevinGPUGyroid(m);
hoomd/md/module-md.cc:    export_TwoStepRATTLELangevinGPUPlane(m);
hoomd/md/module-md.cc:    export_TwoStepRATTLELangevinGPUPrimitive(m);
hoomd/md/module-md.cc:    export_TwoStepRATTLELangevinGPUSphere(m);
hoomd/md/module-md.cc:    export_TwoStepRATTLENVEGPUCylinder(m);
hoomd/md/module-md.cc:    export_TwoStepRATTLENVEGPUDiamond(m);
hoomd/md/module-md.cc:    export_TwoStepRATTLENVEGPUEllipsoid(m);
hoomd/md/module-md.cc:    export_TwoStepRATTLENVEGPUGyroid(m);
hoomd/md/module-md.cc:    export_TwoStepRATTLENVEGPUPlane(m);
hoomd/md/module-md.cc:    export_TwoStepRATTLENVEGPUPrimitive(m);
hoomd/md/module-md.cc:    export_TwoStepRATTLENVEGPUSphere(m);
hoomd/md/ForceCompositeGPU.h:/*! \file ForceCompositeGPU.h
hoomd/md/ForceCompositeGPU.h:    \brief Implementation of a rigid body force compute, GPU version
hoomd/md/ForceCompositeGPU.h:#ifndef __ForceCompositeGPU_H__
hoomd/md/ForceCompositeGPU.h:#define __ForceCompositeGPU_H__
hoomd/md/ForceCompositeGPU.h:class PYBIND11_EXPORT ForceCompositeGPU : public ForceComposite
hoomd/md/ForceCompositeGPU.h:    ForceCompositeGPU(std::shared_ptr<SystemDefinition> sysdef);
hoomd/md/ForceCompositeGPU.h:    virtual ~ForceCompositeGPU();
hoomd/md/ForceCompositeGPU.h:            // identify center particles for use in GPU kernel
hoomd/md/ForceCompositeGPU.h:    GPUPartition m_gpu_partition; //!< Partition of the rigid bodies
hoomd/md/ActiveForceComputeGPU.cc:#include "ActiveForceComputeGPU.h"
hoomd/md/ActiveForceComputeGPU.cc:#include "ActiveForceComputeGPU.cuh"
hoomd/md/ActiveForceComputeGPU.cc:/*! \file ActiveForceComputeGPU.cc
hoomd/md/ActiveForceComputeGPU.cc:    \brief Contains code for the ActiveForceComputeGPU class
hoomd/md/ActiveForceComputeGPU.cc:ActiveForceComputeGPU::ActiveForceComputeGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/md/ActiveForceComputeGPU.cc:    if (!m_exec_conf->isCUDAEnabled())
hoomd/md/ActiveForceComputeGPU.cc:            << "Creating a ActiveForceComputeGPU with no GPU in the execution configuration"
hoomd/md/ActiveForceComputeGPU.cc:        throw std::runtime_error("Error initializing ActiveForceComputeGPU");
hoomd/md/ActiveForceComputeGPU.cc:void ActiveForceComputeGPU::setForces()
hoomd/md/ActiveForceComputeGPU.cc:    // compute the forces on the GPU
hoomd/md/ActiveForceComputeGPU.cc:    kernel::gpu_compute_active_force_set_forces(group_size,
hoomd/md/ActiveForceComputeGPU.cc:    if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/ActiveForceComputeGPU.cc:        CHECK_CUDA_ERROR();
hoomd/md/ActiveForceComputeGPU.cc:void ActiveForceComputeGPU::rotationalDiffusion(Scalar rotational_diffusion, uint64_t timestep)
hoomd/md/ActiveForceComputeGPU.cc:    // perform the update on the GPU
hoomd/md/ActiveForceComputeGPU.cc:    kernel::gpu_compute_active_force_rotational_diffusion(group_size,
hoomd/md/ActiveForceComputeGPU.cc:    if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/ActiveForceComputeGPU.cc:        CHECK_CUDA_ERROR();
hoomd/md/ActiveForceComputeGPU.cc:void export_ActiveForceComputeGPU(pybind11::module& m)
hoomd/md/ActiveForceComputeGPU.cc:    pybind11::class_<ActiveForceComputeGPU,
hoomd/md/ActiveForceComputeGPU.cc:                     std::shared_ptr<ActiveForceComputeGPU>>(m, "ActiveForceComputeGPU")
hoomd/md/TableAngleForceCompute.cc:    GPUArray<Scalar2> tables(m_table_width, m_angle_data->getNTypes(), m_exec_conf);
hoomd/md/OPLSDihedralForceGPU.cu:#include "OPLSDihedralForceGPU.cuh"
hoomd/md/OPLSDihedralForceGPU.cu:/*! \file OPLSDihedralForceGPU.cu
hoomd/md/OPLSDihedralForceGPU.cu:    \brief Defines GPU kernel code for calculating OPLS dihedral forces. Used by
hoomd/md/OPLSDihedralForceGPU.cu:   OPLSDihedralForceComputeGPU.
hoomd/md/OPLSDihedralForceGPU.cu://! Kernel for calculating OPLS dihedral forces on the GPU
hoomd/md/OPLSDihedralForceGPU.cu:__global__ void gpu_compute_opls_dihedral_forces_kernel(Scalar4* d_force,
hoomd/md/OPLSDihedralForceGPU.cu:    \param d_pos particle positions on the GPU
hoomd/md/OPLSDihedralForceGPU.cu:    \param box Box dimensions (in GPU format) to use for periodic boundary conditions
hoomd/md/OPLSDihedralForceGPU.cu:hipError_t gpu_compute_opls_dihedral_forces(Scalar4* d_force,
hoomd/md/OPLSDihedralForceGPU.cu:    hipFuncGetAttributes(&attr, (const void*)gpu_compute_opls_dihedral_forces_kernel);
hoomd/md/OPLSDihedralForceGPU.cu:    hipLaunchKernelGGL((gpu_compute_opls_dihedral_forces_kernel),
hoomd/md/CosineSqAngleForceGPU.cuh:/*! \file CosineSqAngleForceGPU.cuh
hoomd/md/CosineSqAngleForceGPU.cuh:    \brief Declares GPU kernel code for calculating the cosine squared angle forces. Used
hoomd/md/CosineSqAngleForceGPU.cuh:    by CosineSqAngleForceComputeGPU.
hoomd/md/CosineSqAngleForceGPU.cuh:#ifndef __COSINESQANGLEFORCEGPU_CUH__
hoomd/md/CosineSqAngleForceGPU.cuh:#define __COSINESQANGLEFORCEGPU_CUH__
hoomd/md/CosineSqAngleForceGPU.cuh://! Kernel driver that computes cosine squared angle forces for CosineSqAngleForceComputeGPU
hoomd/md/CosineSqAngleForceGPU.cuh:hipError_t gpu_compute_cosinesq_angle_forces(Scalar4* d_force,
hoomd/md/ComputeThermoHMATypes.h:    \brief Data structures common to both CPU and GPU implementations of ComputeThermoHMA
hoomd/md/ComputeThermoHMATypes.h://! Enum for indexing the GPUArray of computed values
hoomd/md/ComputeThermoHMATypes.h:        potential_energyHMA = 0, //!< Index for the potential energy in the GPUArray
hoomd/md/ComputeThermoHMATypes.h:        pressureHMA,             //!< Index for the potential energy in the GPUArray
hoomd/md/IntegratorTwoStep.h:    /// helper function to compute net force/virial on the GPU
hoomd/md/IntegratorTwoStep.h:    virtual void computeNetForceGPU(uint64_t timestep);
hoomd/md/PotentialTersoffGPU.cuh:/*! \file PotentialTersoffGPU.cuh
hoomd/md/PotentialTersoffGPU.cuh:    \brief Defines templated GPU kernel code for calculating certain three-body forces
hoomd/md/PotentialTersoffGPU.cuh:#ifndef __POTENTIAL_TERSOFF_GPU_CUH__
hoomd/md/PotentialTersoffGPU.cuh:#define __POTENTIAL_TERSOFF_GPU_CUH__
hoomd/md/PotentialTersoffGPU.cuh:const int gpu_tersoff_max_tpp = 32;
hoomd/md/PotentialTersoffGPU.cuh:const int gpu_tersoff_max_tpp = 64;
hoomd/md/PotentialTersoffGPU.cuh://! Wraps arguments to gpu_cgpf
hoomd/md/PotentialTersoffGPU.cuh:    const BoxDim box;           //!< Simulation box in GPU format
hoomd/md/PotentialTersoffGPU.cuh:    const hipDeviceProp_t& devprop; //!< CUDA device properties
hoomd/md/PotentialTersoffGPU.cuh:#if (__CUDA_ARCH__ < 600)
hoomd/md/PotentialTersoffGPU.cuh:/*! This function is only used when hoomd is compiled for double precision on the GPU.
hoomd/md/PotentialTersoffGPU.cuh:#else // CUDA_ARCH > 600)
hoomd/md/PotentialTersoffGPU.cuh:__global__ void gpu_compute_triplet_forces_kernel(Scalar4* d_force,
hoomd/md/PotentialTersoffGPU.cuh:            get_max_block_size(gpu_compute_triplet_forces_kernel<evaluator, compute_virial, tpp>,
hoomd/md/PotentialTersoffGPU.cuh:            hipLaunchKernelGGL((gpu_compute_triplet_forces_kernel<evaluator, compute_virial, tpp>),
hoomd/md/PotentialTersoffGPU.cuh:    This is just a driver function for gpu_compute_triplet_forces_kernel(), see it for details.
hoomd/md/PotentialTersoffGPU.cuh:gpu_compute_triplet_forces(const tersoff_args_t& pair_args,
hoomd/md/PotentialTersoffGPU.cuh:        TersoffComputeKernel<evaluator, 0, gpu_tersoff_max_tpp>::launch(pair_args, d_params);
hoomd/md/PotentialTersoffGPU.cuh:        TersoffComputeKernel<evaluator, 1, gpu_tersoff_max_tpp>::launch(pair_args, d_params);
hoomd/md/PotentialTersoffGPU.cuh:gpu_compute_triplet_forces(const tersoff_args_t& pair_args,
hoomd/md/PotentialTersoffGPU.cuh:#endif // __POTENTIAL_TERSOFF_GPU_CUH__
hoomd/md/TableAngleForceGPU.cuh:/*! \file TableAngleForceGPU.cuh
hoomd/md/TableAngleForceGPU.cuh:    \brief Declares GPU kernel code for calculating the table bond forces. Used by
hoomd/md/TableAngleForceGPU.cuh:   TableAngleForceGPU.
hoomd/md/TableAngleForceGPU.cuh:#ifndef __TABLEANGLEFORCECOMPUTEGPU_CUH__
hoomd/md/TableAngleForceGPU.cuh:#define __TABLEANGLEFORCECOMPUTEGPU_CUH__
hoomd/md/TableAngleForceGPU.cuh://! Kernel driver that computes table forces on the GPU for TableAngleForceGPU
hoomd/md/TableAngleForceGPU.cuh:hipError_t gpu_compute_table_angle_forces(Scalar4* d_force,
hoomd/md/NeighborListGPU.cuh:#ifndef __NEIGHBORLISTGPU_CUH__
hoomd/md/NeighborListGPU.cuh:#define __NEIGHBORLISTGPU_CUH__
hoomd/md/NeighborListGPU.cuh:/*! \file NeighborListGPU.cuh
hoomd/md/NeighborListGPU.cuh:    \brief Declares GPU kernel code for cell list generation on the GPU
hoomd/md/NeighborListGPU.cuh:#include "hoomd/GPUPartition.cuh"
hoomd/md/NeighborListGPU.cuh://! Kernel driver for gpu_nlist_needs_update_check_new_kernel()
hoomd/md/NeighborListGPU.cuh:hipError_t gpu_nlist_needs_update_check_new(unsigned int* d_result,
hoomd/md/NeighborListGPU.cuh:                                            const GPUPartition& gpu_partition);
hoomd/md/NeighborListGPU.cuh://! Kernel driver for gpu_nlist_filter_kernel()
hoomd/md/NeighborListGPU.cuh:hipError_t gpu_nlist_filter(unsigned int* d_n_neigh,
hoomd/md/NeighborListGPU.cuh://! Kernel driver to build head list on gpu
hoomd/md/NeighborListGPU.cuh:hipError_t gpu_nlist_build_head_list(size_t* d_head_list,
hoomd/md/NeighborListGPU.cuh://! GPU function to update the exclusion list on the device
hoomd/md/NeighborListGPU.cuh:hipError_t gpu_update_exclusion_list(const unsigned int* d_tag,
hoomd/md/PotentialExternal.h:#include "hoomd/GPUArray.h"
hoomd/md/PotentialExternal.h:    GPUArray<param_type> m_params;       //!< Array of per-type parameters
hoomd/md/PotentialExternal.h:          m_exec_conf->isCUDAEnabled()))
hoomd/md/PotentialExternal.h:    GPUArray<param_type> params(m_pdata->getNTypes(), m_exec_conf);
hoomd/md/EvaluatorPairTable.h:        //! Attach managed memory to CUDA stream
hoomd/md/ForceCompositeGPU.cc:#include "ForceCompositeGPU.h"
hoomd/md/ForceCompositeGPU.cc:#include "ForceCompositeGPU.cuh"
hoomd/md/ForceCompositeGPU.cc:/*! \file ForceCompositeGPU.cc
hoomd/md/ForceCompositeGPU.cc:    \brief Contains code for the ForceCompositeGPU class
hoomd/md/ForceCompositeGPU.cc:ForceCompositeGPU::ForceCompositeGPU(std::shared_ptr<SystemDefinition> sysdef)
hoomd/md/ForceCompositeGPU.cc:        cudaMemAdvise(m_body_len.get(),
hoomd/md/ForceCompositeGPU.cc:                      cudaMemAdviseSetReadMostly,
hoomd/md/ForceCompositeGPU.cc:        cudaMemAdvise(m_body_orientation.get(),
hoomd/md/ForceCompositeGPU.cc:                      cudaMemAdviseSetReadMostly,
hoomd/md/ForceCompositeGPU.cc:        cudaMemAdvise(m_body_pos.get(),
hoomd/md/ForceCompositeGPU.cc:                      cudaMemAdviseSetReadMostly,
hoomd/md/ForceCompositeGPU.cc:        cudaMemAdvise(m_body_types.get(),
hoomd/md/ForceCompositeGPU.cc:                      cudaMemAdviseSetReadMostly,
hoomd/md/ForceCompositeGPU.cc:        CHECK_CUDA_ERROR();
hoomd/md/ForceCompositeGPU.cc:ForceCompositeGPU::~ForceCompositeGPU() { }
hoomd/md/ForceCompositeGPU.cc:void ForceCompositeGPU::computeForces(uint64_t timestep)
hoomd/md/ForceCompositeGPU.cc:    // access local molecule data (need to move this on top because of GPUArray scoping issues)
hoomd/md/ForceCompositeGPU.cc:        m_exec_conf->beginMultiGPU();
hoomd/md/ForceCompositeGPU.cc:        for (int idev = m_exec_conf->getNumActiveGPUs() - 1; idev >= 0; idev--)
hoomd/md/ForceCompositeGPU.cc:                = m_pdata->getGPUPartition().getRangeAndSetGPU(idev);
hoomd/md/ForceCompositeGPU.cc:            if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/ForceCompositeGPU.cc:                CHECK_CUDA_ERROR();
hoomd/md/ForceCompositeGPU.cc:        m_exec_conf->endMultiGPU();
hoomd/md/ForceCompositeGPU.cc:        m_exec_conf->beginMultiGPU();
hoomd/md/ForceCompositeGPU.cc:        // launch GPU kernel
hoomd/md/ForceCompositeGPU.cc:        kernel::gpu_rigid_force(d_force.data,
hoomd/md/ForceCompositeGPU.cc:                                m_gpu_partition);
hoomd/md/ForceCompositeGPU.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/ForceCompositeGPU.cc:            CHECK_CUDA_ERROR();
hoomd/md/ForceCompositeGPU.cc:        m_exec_conf->endMultiGPU();
hoomd/md/ForceCompositeGPU.cc:        m_exec_conf->beginMultiGPU();
hoomd/md/ForceCompositeGPU.cc:        for (int idev = m_exec_conf->getNumActiveGPUs() - 1; idev >= 0; idev--)
hoomd/md/ForceCompositeGPU.cc:                = m_pdata->getGPUPartition().getRangeAndSetGPU(idev);
hoomd/md/ForceCompositeGPU.cc:            if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/ForceCompositeGPU.cc:                CHECK_CUDA_ERROR();
hoomd/md/ForceCompositeGPU.cc:        m_exec_conf->endMultiGPU();
hoomd/md/ForceCompositeGPU.cc:        m_exec_conf->beginMultiGPU();
hoomd/md/ForceCompositeGPU.cc:        // launch GPU kernel
hoomd/md/ForceCompositeGPU.cc:        kernel::gpu_rigid_virial(d_virial.data,
hoomd/md/ForceCompositeGPU.cc:                                 m_gpu_partition);
hoomd/md/ForceCompositeGPU.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/ForceCompositeGPU.cc:            CHECK_CUDA_ERROR();
hoomd/md/ForceCompositeGPU.cc:        m_exec_conf->endMultiGPU();
hoomd/md/ForceCompositeGPU.cc:void ForceCompositeGPU::updateCompositeParticles(uint64_t timestep)
hoomd/md/ForceCompositeGPU.cc:        m_exec_conf->beginMultiGPU();
hoomd/md/ForceCompositeGPU.cc:        kernel::gpu_update_composite(m_pdata->getN(),
hoomd/md/ForceCompositeGPU.cc:                                     m_pdata->getGPUPartition());
hoomd/md/ForceCompositeGPU.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/ForceCompositeGPU.cc:            CHECK_CUDA_ERROR();
hoomd/md/ForceCompositeGPU.cc:        m_exec_conf->endMultiGPU();
hoomd/md/ForceCompositeGPU.cc:void ForceCompositeGPU::findRigidCenters()
hoomd/md/ForceCompositeGPU.cc:        cudaMemAdvise(m_lookup_center.get(),
hoomd/md/ForceCompositeGPU.cc:                      cudaMemAdviseSetReadMostly,
hoomd/md/ForceCompositeGPU.cc:        CHECK_CUDA_ERROR();
hoomd/md/ForceCompositeGPU.cc:    kernel::gpu_find_rigid_centers(d_body.data,
hoomd/md/ForceCompositeGPU.cc:    // distribute rigid body centers over GPUs
hoomd/md/ForceCompositeGPU.cc:    m_gpu_partition = GPUPartition(m_exec_conf->getGPUIds());
hoomd/md/ForceCompositeGPU.cc:    m_gpu_partition.setN(n_rigid);
hoomd/md/ForceCompositeGPU.cc:void export_ForceCompositeGPU(pybind11::module& m)
hoomd/md/ForceCompositeGPU.cc:    pybind11::class_<ForceCompositeGPU, ForceComposite, std::shared_ptr<ForceCompositeGPU>>(
hoomd/md/ForceCompositeGPU.cc:        "ForceCompositeGPU")
hoomd/md/HelfrichMeshForceComputeGPU.h:#include "HelfrichMeshForceComputeGPU.cuh"
hoomd/md/HelfrichMeshForceComputeGPU.h:/*! \file HelfrichMeshForceComputeGPU.h
hoomd/md/HelfrichMeshForceComputeGPU.h:    \brief Declares a class for computing helfrich energy forces on the GPU
hoomd/md/HelfrichMeshForceComputeGPU.h:#ifndef __HELFRICHMESHFORCECOMPUTE_GPU_H__
hoomd/md/HelfrichMeshForceComputeGPU.h:#define __HELFRICHMESHFORCECOMPUTE_GPU_H__
hoomd/md/HelfrichMeshForceComputeGPU.h://! Computes helfrich energy forces on the mesh on the GPU
hoomd/md/HelfrichMeshForceComputeGPU.h:class PYBIND11_EXPORT HelfrichMeshForceComputeGPU : public HelfrichMeshForceCompute
hoomd/md/HelfrichMeshForceComputeGPU.h:    HelfrichMeshForceComputeGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/md/HelfrichMeshForceComputeGPU.h://! Exports the HelfrichMeshForceComputeGPU class to python
hoomd/md/HelfrichMeshForceComputeGPU.h:void export_HelfrichMeshForceComputeGPU(pybind11::module& m);
hoomd/md/AnisoPotentialPairPatchyGPUKernel.cu.inc:#include "hoomd/md/AnisoPotentialPairGPU.cuh"
hoomd/md/AnisoPotentialPairPatchyGPUKernel.cu.inc:gpu_compute_pair_aniso_forces<PairModulator<EVALUATOR_CLASS, PatchEnvelope>>(
hoomd/md/ComputeThermoHMAGPU.cuh:#ifndef _COMPUTE_THERMO_GPU_CUH_
hoomd/md/ComputeThermoHMAGPU.cuh:#define _COMPUTE_THERMO_GPU_CUH_
hoomd/md/ComputeThermoHMAGPU.cuh:#include "hoomd/GPUPartition.cuh"
hoomd/md/ComputeThermoHMAGPU.cuh:/*! \file ComputeThermoHMAGPU.cuh
hoomd/md/ComputeThermoHMAGPU.cuh:    \brief Kernel driver function declarations for ComputeThermoHMAGPU
hoomd/md/ComputeThermoHMAGPU.cuh://! Holder for arguments to gpu_compute_thermo
hoomd/md/ComputeThermoHMAGPU.cuh:    unsigned int block_size; //!< Block size to execute on the GPU
hoomd/md/ComputeThermoHMAGPU.cuh:hipError_t gpu_compute_thermo_hma_partial(Scalar4* d_pos,
hoomd/md/ComputeThermoHMAGPU.cuh:                                          const GPUPartition& gpu_partition);
hoomd/md/ComputeThermoHMAGPU.cuh:hipError_t gpu_compute_thermo_hma_final(Scalar* d_properties,
hoomd/md/CosineSqAngleForceComputeGPU.cc:/*! \file CosineSqAngleForceComputeGPU.cc
hoomd/md/CosineSqAngleForceComputeGPU.cc:    \brief Defines CosineSqAngleForceComputeGPU
hoomd/md/CosineSqAngleForceComputeGPU.cc:#include "CosineSqAngleForceComputeGPU.h"
hoomd/md/CosineSqAngleForceComputeGPU.cc:CosineSqAngleForceComputeGPU::CosineSqAngleForceComputeGPU(std::shared_ptr<SystemDefinition> sysdef)
hoomd/md/CosineSqAngleForceComputeGPU.cc:    // can't run on the GPU if there aren't any GPUs in the execution configuration
hoomd/md/CosineSqAngleForceComputeGPU.cc:    if (!m_exec_conf->isCUDAEnabled())
hoomd/md/CosineSqAngleForceComputeGPU.cc:            << "Creating a AngleForceComputeGPU with no GPU in the execution configuration" << endl;
hoomd/md/CosineSqAngleForceComputeGPU.cc:        throw std::runtime_error("Error initializing AngleForceComputeGPU");
hoomd/md/CosineSqAngleForceComputeGPU.cc:    GPUArray<Scalar2> params(m_angle_data->getNTypes(), m_exec_conf);
hoomd/md/CosineSqAngleForceComputeGPU.cc:CosineSqAngleForceComputeGPU::~CosineSqAngleForceComputeGPU() { }
hoomd/md/CosineSqAngleForceComputeGPU.cc:    parameters on the GPU.
hoomd/md/CosineSqAngleForceComputeGPU.cc:void CosineSqAngleForceComputeGPU::setParams(unsigned int type, Scalar K, Scalar t_0)
hoomd/md/CosineSqAngleForceComputeGPU.cc:/*! Internal method for computing the forces on the GPU.
hoomd/md/CosineSqAngleForceComputeGPU.cc:    \post The force data on the GPU is written with the calculated forces
hoomd/md/CosineSqAngleForceComputeGPU.cc:    Calls gpu_compute_cosinesq_angle_forces to do the dirty work.
hoomd/md/CosineSqAngleForceComputeGPU.cc:void CosineSqAngleForceComputeGPU::computeForces(uint64_t timestep)
hoomd/md/CosineSqAngleForceComputeGPU.cc:    ArrayHandle<AngleData::members_t> d_gpu_anglelist(m_angle_data->getGPUTable(),
hoomd/md/CosineSqAngleForceComputeGPU.cc:    ArrayHandle<unsigned int> d_gpu_angle_pos_list(m_angle_data->getGPUPosTable(),
hoomd/md/CosineSqAngleForceComputeGPU.cc:    ArrayHandle<unsigned int> d_gpu_n_angles(m_angle_data->getNGroupsArray(),
hoomd/md/CosineSqAngleForceComputeGPU.cc:    // run the kernel on the GPU
hoomd/md/CosineSqAngleForceComputeGPU.cc:    kernel::gpu_compute_cosinesq_angle_forces(d_force.data,
hoomd/md/CosineSqAngleForceComputeGPU.cc:                                              d_gpu_anglelist.data,
hoomd/md/CosineSqAngleForceComputeGPU.cc:                                              d_gpu_angle_pos_list.data,
hoomd/md/CosineSqAngleForceComputeGPU.cc:                                              m_angle_data->getGPUTableIndexer().getW(),
hoomd/md/CosineSqAngleForceComputeGPU.cc:                                              d_gpu_n_angles.data,
hoomd/md/CosineSqAngleForceComputeGPU.cc:    if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/CosineSqAngleForceComputeGPU.cc:        CHECK_CUDA_ERROR();
hoomd/md/CosineSqAngleForceComputeGPU.cc:void export_CosineSqAngleForceComputeGPU(pybind11::module& m)
hoomd/md/CosineSqAngleForceComputeGPU.cc:    pybind11::class_<CosineSqAngleForceComputeGPU,
hoomd/md/CosineSqAngleForceComputeGPU.cc:                     std::shared_ptr<CosineSqAngleForceComputeGPU>>(m,
hoomd/md/CosineSqAngleForceComputeGPU.cc:                                                                    "CosineSqAngleForceComputeGPU")
hoomd/md/PotentialTersoff.h:#include "hoomd/GPUArray.h"
hoomd/md/PotentialTersoff.h:   values are stored in GPUArray for easy access on the GPU by a derived class. The type of the
hoomd/md/PotentialTersoff.h:    GPUArray<Scalar> m_rcutsq;             //!< Cutoff radius squared per type pair
hoomd/md/PotentialTersoff.h:    GPUArray<param_type> m_params;         //!< Pair parameters per type pair
hoomd/md/PotentialTersoff.h:    GPUArray<Scalar> rcutsq(m_typpair_idx.getNumElements(), m_exec_conf);
hoomd/md/PotentialTersoff.h:    GPUArray<param_type> params(m_typpair_idx.getNumElements(), m_exec_conf);
hoomd/md/mesh/potential.py:            cpp_cls = getattr(_md, self._cpp_class_name + "GPU")
hoomd/md/mesh/potential.py:            cpp_cls = getattr(_md, self._cpp_class_name + "GPU")
hoomd/md/dihedral.py:            cpp_class = getattr(self._ext_module, self._cpp_class_name + "GPU")
hoomd/md/dihedral.py:            cpp_cls = _md.TableDihedralForceComputeGPU
hoomd/md/NeighborListGPUTree.cuh:#ifndef __NEIGHBORLISTGPUTREE_CUH__
hoomd/md/NeighborListGPUTree.cuh:#define __NEIGHBORLISTGPUTREE_CUH__
hoomd/md/NeighborListGPUTree.cuh:/*! \file NeighborListGPUTree.cuh
hoomd/md/NeighborListGPUTree.cuh:    \brief Declares GPU kernel code for neighbor list tree traversal on the GPU
hoomd/md/NeighborListGPUTree.cuh:hipError_t gpu_nlist_mark_types(unsigned int* d_types,
hoomd/md/NeighborListGPUTree.cuh:uchar2 gpu_nlist_sort_types(void* d_tmp,
hoomd/md/NeighborListGPUTree.cuh:hipError_t gpu_nlist_count_types(unsigned int* d_first,
hoomd/md/NeighborListGPUTree.cuh:hipError_t gpu_nlist_copy_primitives(unsigned int* d_traverse_order,
hoomd/md/NeighborListGPUTree.cuh: * in CUDA code.
hoomd/md/NeighborListGPUTree.cuh:    // Storing a bare pointer here because CUDA 11.5 fails to compile
hoomd/md/NeighborListGPUTree.cuh: * in CUDA code.
hoomd/md/NeighborListGPUTree.cuh:    // Storing a bare pointer here because CUDA 11.5 fails to compile
hoomd/md/NeighborListGPUTree.cuh:#endif //__NEIGHBORLISTGPUTREE_CUH__
hoomd/md/TwoStepRATTLELangevinGPU.h:#include "TwoStepRATTLELangevinGPU.cuh"
hoomd/md/TwoStepRATTLELangevinGPU.h:#include "TwoStepRATTLENVEGPU.cuh"
hoomd/md/TwoStepRATTLELangevinGPU.h://! Implements Langevin dynamics on the GPU
hoomd/md/TwoStepRATTLELangevinGPU.h:/*! GPU accelerated version of TwoStepLangevin
hoomd/md/TwoStepRATTLELangevinGPU.h:class PYBIND11_EXPORT TwoStepRATTLELangevinGPU : public TwoStepRATTLELangevin<Manifold>
hoomd/md/TwoStepRATTLELangevinGPU.h:    TwoStepRATTLELangevinGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/md/TwoStepRATTLELangevinGPU.h:    virtual ~TwoStepRATTLELangevinGPU() { };
hoomd/md/TwoStepRATTLELangevinGPU.h:    GPUArray<Scalar> m_partial_sum1; //!< memory space for partial sum over bd energy transfers
hoomd/md/TwoStepRATTLELangevinGPU.h:    GPUArray<Scalar> m_sum;          //!< memory space for sum over bd energy transfers
hoomd/md/TwoStepRATTLELangevinGPU.h:    This method is copied directly from TwoStepNVEGPU::integrateStepOne() and reimplemented here to
hoomd/md/TwoStepRATTLELangevinGPU.h:TwoStepRATTLELangevinGPU<Manifold>::TwoStepRATTLELangevinGPU(
hoomd/md/TwoStepRATTLELangevinGPU.h:    if (!this->m_exec_conf->isCUDAEnabled())
hoomd/md/TwoStepRATTLELangevinGPU.h:            << "Creating a TwoStepRATTLELangevinGPU while CUDA is disabled" << std::endl;
hoomd/md/TwoStepRATTLELangevinGPU.h:        throw std::runtime_error("Error initializing TwoStepRATTLELangevinGPU");
hoomd/md/TwoStepRATTLELangevinGPU.h:    GPUArray<Scalar> sum(1, this->m_exec_conf);
hoomd/md/TwoStepRATTLELangevinGPU.h:    GPUArray<Scalar> partial_sum1(m_num_blocks, this->m_exec_conf);
hoomd/md/TwoStepRATTLELangevinGPU.h:void TwoStepRATTLELangevinGPU<Manifold>::integrateStepOne(uint64_t timestep)
hoomd/md/TwoStepRATTLELangevinGPU.h:    this->m_exec_conf->beginMultiGPU();
hoomd/md/TwoStepRATTLELangevinGPU.h:    // perform the update on the GPU
hoomd/md/TwoStepRATTLELangevinGPU.h:    kernel::gpu_rattle_nve_step_one(d_pos.data,
hoomd/md/TwoStepRATTLELangevinGPU.h:                                    this->m_group->getGPUPartition(),
hoomd/md/TwoStepRATTLELangevinGPU.h:    if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/TwoStepRATTLELangevinGPU.h:        CHECK_CUDA_ERROR();
hoomd/md/TwoStepRATTLELangevinGPU.h:    this->m_exec_conf->endMultiGPU();
hoomd/md/TwoStepRATTLELangevinGPU.h:        this->m_exec_conf->beginMultiGPU();
hoomd/md/TwoStepRATTLELangevinGPU.h:        kernel::gpu_rattle_nve_angular_step_one(d_orientation.data,
hoomd/md/TwoStepRATTLELangevinGPU.h:                                                this->m_group->getGPUPartition(),
hoomd/md/TwoStepRATTLELangevinGPU.h:        this->m_exec_conf->endMultiGPU();
hoomd/md/TwoStepRATTLELangevinGPU.h:        if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/TwoStepRATTLELangevinGPU.h:            CHECK_CUDA_ERROR();
hoomd/md/TwoStepRATTLELangevinGPU.h:    \post particle velocities are moved forward to timestep+1 on the GPU
hoomd/md/TwoStepRATTLELangevinGPU.h:void TwoStepRATTLELangevinGPU<Manifold>::integrateStepTwo(uint64_t timestep)
hoomd/md/TwoStepRATTLELangevinGPU.h:        // perform the update on the GPU
hoomd/md/TwoStepRATTLELangevinGPU.h:        kernel::gpu_rattle_langevin_step_two<Manifold>(d_pos.data,
hoomd/md/TwoStepRATTLELangevinGPU.h:        if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/TwoStepRATTLELangevinGPU.h:            CHECK_CUDA_ERROR();
hoomd/md/TwoStepRATTLELangevinGPU.h:            kernel::gpu_rattle_langevin_angular_step_two(d_pos.data,
hoomd/md/TwoStepRATTLELangevinGPU.h:            if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/TwoStepRATTLELangevinGPU.h:                CHECK_CUDA_ERROR();
hoomd/md/TwoStepRATTLELangevinGPU.h:void TwoStepRATTLELangevinGPU<Manifold>::includeRATTLEForce(uint64_t timestep)
hoomd/md/TwoStepRATTLELangevinGPU.h:    // perform the update on the GPU
hoomd/md/TwoStepRATTLELangevinGPU.h:    this->m_exec_conf->beginMultiGPU();
hoomd/md/TwoStepRATTLELangevinGPU.h:    kernel::gpu_include_rattle_force_nve<Manifold>(d_pos.data,
hoomd/md/TwoStepRATTLELangevinGPU.h:                                                   this->m_group->getGPUPartition(),
hoomd/md/TwoStepRATTLELangevinGPU.h:    if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/TwoStepRATTLELangevinGPU.h:        CHECK_CUDA_ERROR();
hoomd/md/TwoStepRATTLELangevinGPU.h:    this->m_exec_conf->endMultiGPU();
hoomd/md/TwoStepRATTLELangevinGPU.h:void export_TwoStepRATTLELangevinGPU(pybind11::module& m, const std::string& name)
hoomd/md/TwoStepRATTLELangevinGPU.h:    pybind11::class_<TwoStepRATTLELangevinGPU<Manifold>,
hoomd/md/TwoStepRATTLELangevinGPU.h:                     std::shared_ptr<TwoStepRATTLELangevinGPU<Manifold>>>(m, name.c_str())
hoomd/md/VolumeConservationMeshForceCompute.cc:    GPUArray<volume_conservation_param_t> params(n_types, m_exec_conf);
hoomd/md/VolumeConservationMeshForceCompute.cc:    GPUArray<Scalar> volume(n_types, m_exec_conf);
hoomd/md/TwoStepConstantVolumeGPU.cu:#include "TwoStepConstantVolumeGPU.cuh"
hoomd/md/TwoStepConstantVolumeGPU.cu:/*! \file TwoStepNVTGPU.cu
hoomd/md/TwoStepConstantVolumeGPU.cu:    \brief Defines GPU kernel code for NVT integration on the GPU. Used by TwoStepNVTGPU.
hoomd/md/TwoStepConstantVolumeGPU.cu:    \param work_size Number of members in the group for this GPU
hoomd/md/TwoStepConstantVolumeGPU.cu:    \param offset The offset of this GPU into the list of particles
hoomd/md/TwoStepConstantVolumeGPU.cu:    See gpu_nve_step_one_kernel() for some performance notes on how to handle the group data reads
hoomd/md/TwoStepConstantVolumeGPU.cu:__global__ void gpu_nvt_rescale_step_one_kernel(Scalar4* d_pos,
hoomd/md/TwoStepConstantVolumeGPU.cu:hipError_t gpu_nvt_rescale_step_one(Scalar4* d_pos,
hoomd/md/TwoStepConstantVolumeGPU.cu:                                    const GPUPartition& gpu_partition,
hoomd/md/TwoStepConstantVolumeGPU.cu:    hipFuncGetAttributes(&attr, (const void*)gpu_nvt_rescale_step_one_kernel);
hoomd/md/TwoStepConstantVolumeGPU.cu:    // iterate over active GPUs in reverse, to end up on first GPU when returning from this function
hoomd/md/TwoStepConstantVolumeGPU.cu:    for (int idev = gpu_partition.getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/md/TwoStepConstantVolumeGPU.cu:        auto range = gpu_partition.getRangeAndSetGPU(idev);
hoomd/md/TwoStepConstantVolumeGPU.cu:        hipLaunchKernelGGL((gpu_nvt_rescale_step_one_kernel),
hoomd/md/TwoStepConstantVolumeGPU.cu:    \param work_size Number of members in the group for this GPU
hoomd/md/TwoStepConstantVolumeGPU.cu:    \param offset The offset of this GPU into the list of particles
hoomd/md/TwoStepConstantVolumeGPU.cu:__global__ void gpu_nvt_rescale_step_two_kernel(Scalar4* d_vel,
hoomd/md/TwoStepConstantVolumeGPU.cu:hipError_t gpu_nvt_rescale_step_two(Scalar4* d_vel,
hoomd/md/TwoStepConstantVolumeGPU.cu:                                    const GPUPartition& gpu_partition)
hoomd/md/TwoStepConstantVolumeGPU.cu:    hipFuncGetAttributes(&attr, (const void*)gpu_nvt_rescale_step_two_kernel);
hoomd/md/TwoStepConstantVolumeGPU.cu:    // iterate over active GPUs in reverse, to end up on first GPU when returning from this function
hoomd/md/TwoStepConstantVolumeGPU.cu:    for (int idev = gpu_partition.getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/md/TwoStepConstantVolumeGPU.cu:        auto range = gpu_partition.getRangeAndSetGPU(idev);
hoomd/md/TwoStepConstantVolumeGPU.cu:        hipLaunchKernelGGL((gpu_nvt_rescale_step_two_kernel),
hoomd/md/CommunicatorGridGPU.h:#ifndef __COMMUNICATOR_GRID_GPU_H__
hoomd/md/CommunicatorGridGPU.h:#define __COMMUNICATOR_GRID_GPU_H__
hoomd/md/CommunicatorGridGPU.h:/*! Class to communicate the boundary layer of a regular grid (GPU version)
hoomd/md/CommunicatorGridGPU.h:template<typename T> class CommunicatorGridGPU : public CommunicatorGrid<T>
hoomd/md/CommunicatorGridGPU.h:    CommunicatorGridGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/md/CommunicatorGridGPU.h:    virtual void initGridCommGPU();
hoomd/md/CommunicatorGridGPU.h:#endif // __COMMUNICATOR_GRID_GPU_H__
hoomd/md/ForceDistanceConstraintGPU.cc:#include "ForceDistanceConstraintGPU.h"
hoomd/md/ForceDistanceConstraintGPU.cc:#include "ForceDistanceConstraintGPU.cuh"
hoomd/md/ForceDistanceConstraintGPU.cc:/*! \file ForceDistanceConstraintGPU.cc
hoomd/md/ForceDistanceConstraintGPU.cc:    \brief Contains code for the ForceDistanceConstraintGPU class
hoomd/md/ForceDistanceConstraintGPU.cc:ForceDistanceConstraintGPU::ForceDistanceConstraintGPU(std::shared_ptr<SystemDefinition> sysdef)
hoomd/md/ForceDistanceConstraintGPU.cc:    GPUVector<int> csr_rowptr(m_exec_conf);
hoomd/md/ForceDistanceConstraintGPU.cc:    GPUVector<int> csr_colind(m_exec_conf);
hoomd/md/ForceDistanceConstraintGPU.cc:    GPUVector<double> sparse_val(m_exec_conf);
hoomd/md/ForceDistanceConstraintGPU.cc:    GPUVector<int> sparse_idxlookup(m_exec_conf);
hoomd/md/ForceDistanceConstraintGPU.cc:ForceDistanceConstraintGPU::~ForceDistanceConstraintGPU()
hoomd/md/ForceDistanceConstraintGPU.cc:void ForceDistanceConstraintGPU::fillMatrixVector(uint64_t timestep)
hoomd/md/ForceDistanceConstraintGPU.cc:        // access GPU constraint table on device
hoomd/md/ForceDistanceConstraintGPU.cc:        const GPUArray<ConstraintData::members_t>& gpu_constraint_list
hoomd/md/ForceDistanceConstraintGPU.cc:            = this->m_cdata->getGPUTable();
hoomd/md/ForceDistanceConstraintGPU.cc:        const Index2D& gpu_table_indexer = this->m_cdata->getGPUTableIndexer();
hoomd/md/ForceDistanceConstraintGPU.cc:        ArrayHandle<ConstraintData::members_t> d_gpu_clist(gpu_constraint_list,
hoomd/md/ForceDistanceConstraintGPU.cc:        ArrayHandle<unsigned int> d_gpu_n_constraints(this->m_cdata->getNGroupsArray(),
hoomd/md/ForceDistanceConstraintGPU.cc:        ArrayHandle<unsigned int> d_gpu_cpos(m_cdata->getGPUPosTable(),
hoomd/md/ForceDistanceConstraintGPU.cc:        // launch GPU kernel
hoomd/md/ForceDistanceConstraintGPU.cc:        kernel::gpu_fill_matrix_vector(n_constraint,
hoomd/md/ForceDistanceConstraintGPU.cc:                                       d_gpu_clist.data,
hoomd/md/ForceDistanceConstraintGPU.cc:                                       gpu_table_indexer,
hoomd/md/ForceDistanceConstraintGPU.cc:                                       d_gpu_n_constraints.data,
hoomd/md/ForceDistanceConstraintGPU.cc:                                       d_gpu_cpos.data,
hoomd/md/ForceDistanceConstraintGPU.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/ForceDistanceConstraintGPU.cc:            CHECK_CUDA_ERROR();
hoomd/md/ForceDistanceConstraintGPU.cc:void ForceDistanceConstraintGPU::solveConstraints(uint64_t timestep)
hoomd/md/ForceDistanceConstraintGPU.cc:                                CUDA_R_64F,
hoomd/md/ForceDistanceConstraintGPU.cc:                              CUDA_R_64F);
hoomd/md/ForceDistanceConstraintGPU.cc:            cudaMalloc(&d_buffer, bufferSize);
hoomd/md/ForceDistanceConstraintGPU.cc:            cudaFree(d_buffer);
hoomd/md/ForceDistanceConstraintGPU.cc:            kernel::gpu_count_nnz(n_constraint,
hoomd/md/ForceDistanceConstraintGPU.cc:            if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/ForceDistanceConstraintGPU.cc:                CHECK_CUDA_ERROR();
hoomd/md/ForceDistanceConstraintGPU.cc:            kernel::gpu_dense2sparse(n_constraint,
hoomd/md/ForceDistanceConstraintGPU.cc:            if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/ForceDistanceConstraintGPU.cc:                CHECK_CUDA_ERROR();
hoomd/md/ForceDistanceConstraintGPU.cc:            << "ForceDistanceConstraintGPU: sparsity pattern changed. Solving on CPU" << std::endl;
hoomd/md/ForceDistanceConstraintGPU.cc:void ForceDistanceConstraintGPU::computeConstraintForces(uint64_t timestep)
hoomd/md/ForceDistanceConstraintGPU.cc:    // access GPU constraint table on device
hoomd/md/ForceDistanceConstraintGPU.cc:    const GPUArray<ConstraintData::members_t>& gpu_constraint_list = this->m_cdata->getGPUTable();
hoomd/md/ForceDistanceConstraintGPU.cc:    const Index2D& gpu_table_indexer = this->m_cdata->getGPUTableIndexer();
hoomd/md/ForceDistanceConstraintGPU.cc:    ArrayHandle<ConstraintData::members_t> d_gpu_clist(gpu_constraint_list,
hoomd/md/ForceDistanceConstraintGPU.cc:    ArrayHandle<unsigned int> d_gpu_n_constraints(this->m_cdata->getNGroupsArray(),
hoomd/md/ForceDistanceConstraintGPU.cc:    ArrayHandle<unsigned int> d_gpu_cpos(m_cdata->getGPUPosTable(),
hoomd/md/ForceDistanceConstraintGPU.cc:    kernel::gpu_compute_constraint_forces(d_pos.data,
hoomd/md/ForceDistanceConstraintGPU.cc:                                          d_gpu_clist.data,
hoomd/md/ForceDistanceConstraintGPU.cc:                                          gpu_table_indexer,
hoomd/md/ForceDistanceConstraintGPU.cc:                                          d_gpu_n_constraints.data,
hoomd/md/ForceDistanceConstraintGPU.cc:                                          d_gpu_cpos.data,
hoomd/md/ForceDistanceConstraintGPU.cc:    if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/ForceDistanceConstraintGPU.cc:        CHECK_CUDA_ERROR();
hoomd/md/ForceDistanceConstraintGPU.cc:void export_ForceDistanceConstraintGPU(pybind11::module& m)
hoomd/md/ForceDistanceConstraintGPU.cc:    pybind11::class_<ForceDistanceConstraintGPU,
hoomd/md/ForceDistanceConstraintGPU.cc:                     std::shared_ptr<ForceDistanceConstraintGPU>>(m, "ForceDistanceConstraintGPU")
hoomd/md/CMakeLists.txt:set(_md_headers ActiveForceComputeGPU.h
hoomd/md/CMakeLists.txt:                ActiveForceConstraintComputeGPU.h
hoomd/md/CMakeLists.txt:                ActiveForceConstraintComputeGPU.cuh
hoomd/md/CMakeLists.txt:                AnisoPotentialPairGPU.cuh
hoomd/md/CMakeLists.txt:                AnisoPotentialPairGPU.h
hoomd/md/CMakeLists.txt:                AreaConservationMeshForceComputeGPU.h
hoomd/md/CMakeLists.txt:                AreaConservationMeshForceComputeGPU.cuh
hoomd/md/CMakeLists.txt:                BendingRigidityMeshForceComputeGPU.h
hoomd/md/CMakeLists.txt:                BondTablePotentialGPU.h
hoomd/md/CMakeLists.txt:                CommunicatorGridGPU.h
hoomd/md/CMakeLists.txt:                ComputeThermoGPU.cuh
hoomd/md/CMakeLists.txt:                ComputeThermoGPU.h
hoomd/md/CMakeLists.txt:                ComputeThermoHMAGPU.cuh
hoomd/md/CMakeLists.txt:                ComputeThermoHMAGPU.h
hoomd/md/CMakeLists.txt:                ConstantForceComputeGPU.h
hoomd/md/CMakeLists.txt:                CosineSqAngleForceComputeGPU.h
hoomd/md/CMakeLists.txt:                FIREEnergyMinimizerGPU.h
hoomd/md/CMakeLists.txt:                ForceCompositeGPU.h
hoomd/md/CMakeLists.txt:                ForceDistanceConstraintGPU.h
hoomd/md/CMakeLists.txt:                HarmonicAngleForceComputeGPU.h
hoomd/md/CMakeLists.txt:                HarmonicDihedralForceComputeGPU.h
hoomd/md/CMakeLists.txt:                HarmonicImproperForceComputeGPU.h
hoomd/md/CMakeLists.txt:            		HelfrichMeshForceComputeGPU.h
hoomd/md/CMakeLists.txt:                MuellerPlatheFlowGPU.h
hoomd/md/CMakeLists.txt:                NeighborListGPUBinned.h
hoomd/md/CMakeLists.txt:                NeighborListGPU.h
hoomd/md/CMakeLists.txt:                NeighborListGPUStencil.h
hoomd/md/CMakeLists.txt:                NeighborListGPUTree.h
hoomd/md/CMakeLists.txt:                OPLSDihedralForceComputeGPU.h
hoomd/md/CMakeLists.txt:                PotentialBondGPU.h
hoomd/md/CMakeLists.txt:                PotentialBondGPU.cuh
hoomd/md/CMakeLists.txt:                PotentialExternalGPU.h
hoomd/md/CMakeLists.txt:                PotentialExternalGPU.cuh
hoomd/md/CMakeLists.txt:                PotentialPairDPDThermoGPU.h
hoomd/md/CMakeLists.txt:                PotentialPairDPDThermoGPU.cuh
hoomd/md/CMakeLists.txt:                PotentialPairGPU.h
hoomd/md/CMakeLists.txt:                PotentialPairGPU.cuh
hoomd/md/CMakeLists.txt:                PotentialSpecialPairGPU.h
hoomd/md/CMakeLists.txt:                PotentialTersoffGPU.h
hoomd/md/CMakeLists.txt:                PeriodicImproperForceComputeGPU.h
hoomd/md/CMakeLists.txt:                PPPMForceComputeGPU.h
hoomd/md/CMakeLists.txt:                TableAngleForceComputeGPU.h
hoomd/md/CMakeLists.txt:                TableDihedralForceComputeGPU.h
hoomd/md/CMakeLists.txt:                TriangleAreaConservationMeshForceComputeGPU.h
hoomd/md/CMakeLists.txt:                TriangleAreaConservationMeshForceComputeGPU.cuh
hoomd/md/CMakeLists.txt:                TwoStepBDGPU.h
hoomd/md/CMakeLists.txt:                TwoStepRATTLEBDGPU.h
hoomd/md/CMakeLists.txt:                TwoStepRATTLEBDGPU.cuh
hoomd/md/CMakeLists.txt:                TwoStepLangevinGPU.h
hoomd/md/CMakeLists.txt:                TwoStepRATTLELangevinGPU.h
hoomd/md/CMakeLists.txt:                TwoStepRATTLELangevinGPU.cuh
hoomd/md/CMakeLists.txt:                TwoStepRATTLENVEGPU.h
hoomd/md/CMakeLists.txt:                TwoStepRATTLENVEGPU.cuh
hoomd/md/CMakeLists.txt:                TwoStepConstantVolumeGPU.h
hoomd/md/CMakeLists.txt:            		VolumeConservationMeshForceComputeGPU.h
hoomd/md/CMakeLists.txt:list(APPEND _md_sources ActiveForceComputeGPU.cc
hoomd/md/CMakeLists.txt:                           AreaConservationMeshForceComputeGPU.cc
hoomd/md/CMakeLists.txt:                           AnisoPotentialPairALJ2GPU.cc
hoomd/md/CMakeLists.txt:                           AnisoPotentialPairALJ3GPU.cc
hoomd/md/CMakeLists.txt:                           AnisoPotentialPairDipoleGPU.cc
hoomd/md/CMakeLists.txt:                           AnisoPotentialPairGBGPU.cc
hoomd/md/CMakeLists.txt:                           BendingRigidityMeshForceComputeGPU.cc
hoomd/md/CMakeLists.txt:                           BondTablePotentialGPU.cc
hoomd/md/CMakeLists.txt:                           CommunicatorGridGPU.cc
hoomd/md/CMakeLists.txt:                           ComputeThermoGPU.cc
hoomd/md/CMakeLists.txt:                           ComputeThermoHMAGPU.cc
hoomd/md/CMakeLists.txt:                           ConstantForceComputeGPU.cc
hoomd/md/CMakeLists.txt:                           FIREEnergyMinimizerGPU.cc
hoomd/md/CMakeLists.txt:                           ForceCompositeGPU.cc
hoomd/md/CMakeLists.txt:                           ForceDistanceConstraintGPU.cc
hoomd/md/CMakeLists.txt:                           HarmonicAngleForceComputeGPU.cc
hoomd/md/CMakeLists.txt:                           HarmonicDihedralForceComputeGPU.cc
hoomd/md/CMakeLists.txt:                           HarmonicImproperForceComputeGPU.cc
hoomd/md/CMakeLists.txt:                  			   HelfrichMeshForceComputeGPU.cc
hoomd/md/CMakeLists.txt:                           NeighborListGPU.cc
hoomd/md/CMakeLists.txt:                           NeighborListGPUBinned.cc
hoomd/md/CMakeLists.txt:                           NeighborListGPUStencil.cc
hoomd/md/CMakeLists.txt:                           NeighborListGPUTree.cc
hoomd/md/CMakeLists.txt:                           OPLSDihedralForceComputeGPU.cc
hoomd/md/CMakeLists.txt:                           PeriodicImproperForceComputeGPU.cc
hoomd/md/CMakeLists.txt:                           PPPMForceComputeGPU.cc
hoomd/md/CMakeLists.txt:                           TableAngleForceComputeGPU.cc
hoomd/md/CMakeLists.txt:                           TableDihedralForceComputeGPU.cc
hoomd/md/CMakeLists.txt:                           TriangleAreaConservationMeshForceComputeGPU.cc
hoomd/md/CMakeLists.txt:                           TwoStepBDGPU.cc
hoomd/md/CMakeLists.txt:                           TwoStepLangevinGPU.cc
hoomd/md/CMakeLists.txt:                  			   VolumeConservationMeshForceComputeGPU.cc
hoomd/md/CMakeLists.txt:                           TwoStepConstantVolumeGPU.cc
hoomd/md/CMakeLists.txt:                           TwoStepConstantPressureGPU.cc
hoomd/md/CMakeLists.txt:                           MuellerPlatheFlowGPU.cc
hoomd/md/CMakeLists.txt:                           CosineSqAngleForceComputeGPU.cc
hoomd/md/CMakeLists.txt:set(_md_cu_sources ActiveForceComputeGPU.cu
hoomd/md/CMakeLists.txt:                      AreaConservationMeshForceComputeGPU.cu
hoomd/md/CMakeLists.txt:                      AnisoPotentialPairALJ2GPUKernel.cu
hoomd/md/CMakeLists.txt:                      AnisoPotentialPairALJ3GPUKernel.cu
hoomd/md/CMakeLists.txt:                      AnisoPotentialPairDipoleGPUKernel.cu
hoomd/md/CMakeLists.txt:                      AnisoPotentialPairGBGPUKernel.cu
hoomd/md/CMakeLists.txt:            		      BendingRigidityMeshForceComputeGPU.cu
hoomd/md/CMakeLists.txt:                      ComputeThermoGPU.cu
hoomd/md/CMakeLists.txt:                      ComputeThermoHMAGPU.cu
hoomd/md/CMakeLists.txt:                      ConstantForceComputeGPU.cu
hoomd/md/CMakeLists.txt:                      BondTablePotentialGPU.cu
hoomd/md/CMakeLists.txt:                      CommunicatorGridGPU.cu
hoomd/md/CMakeLists.txt:                      FIREEnergyMinimizerGPU.cu
hoomd/md/CMakeLists.txt:                      ForceCompositeGPU.cu
hoomd/md/CMakeLists.txt:                      ForceDistanceConstraintGPU.cu
hoomd/md/CMakeLists.txt:                      HarmonicAngleForceGPU.cu
hoomd/md/CMakeLists.txt:                      HarmonicDihedralForceGPU.cu
hoomd/md/CMakeLists.txt:                      HarmonicImproperForceGPU.cu
hoomd/md/CMakeLists.txt:            		      HelfrichMeshForceComputeGPU.cu
hoomd/md/CMakeLists.txt:                      NeighborListGPUBinned.cu
hoomd/md/CMakeLists.txt:                      NeighborListGPU.cu
hoomd/md/CMakeLists.txt:                      NeighborListGPUStencil.cu
hoomd/md/CMakeLists.txt:                      NeighborListGPUTree.cu
hoomd/md/CMakeLists.txt:                      OPLSDihedralForceGPU.cu
hoomd/md/CMakeLists.txt:                      PeriodicImproperForceGPU.cu
hoomd/md/CMakeLists.txt:                      PPPMForceComputeGPU.cu
hoomd/md/CMakeLists.txt:                      TableAngleForceGPU.cu
hoomd/md/CMakeLists.txt:                      TableDihedralForceGPU.cu
hoomd/md/CMakeLists.txt:                      TriangleAreaConservationMeshForceComputeGPU.cu
hoomd/md/CMakeLists.txt:                      TwoStepBDGPU.cu
hoomd/md/CMakeLists.txt:                      TwoStepLangevinGPU.cu
hoomd/md/CMakeLists.txt:                      TwoStepRATTLELangevinGPU.cu
hoomd/md/CMakeLists.txt:                      TwoStepConstantPressureGPU.cu
hoomd/md/CMakeLists.txt:                      TwoStepConstantVolumeGPU.cu
hoomd/md/CMakeLists.txt:                      TwoStepNVEGPU.cu
hoomd/md/CMakeLists.txt:                      TwoStepRATTLENVEGPU.cu
hoomd/md/CMakeLists.txt:            		      VolumeConservationMeshForceComputeGPU.cu
hoomd/md/CMakeLists.txt:                      MuellerPlatheFlowGPU.cu
hoomd/md/CMakeLists.txt:                      CosineSqAngleForceGPU.cu
hoomd/md/CMakeLists.txt:set(_cuda_sources ${_md_cu_sources} ${DFFT_CU_SOURCES})
hoomd/md/CMakeLists.txt:        configure_file(export_ActiveForceConstraintComputeGPU.cc.inc
hoomd/md/CMakeLists.txt:                       export_ActiveForceConstraintCompute${_manifold}GPU.cc
hoomd/md/CMakeLists.txt:        configure_file(ActiveForceConstraintComputeGPU.cu.inc
hoomd/md/CMakeLists.txt:                       ActiveForceConstraintCompute${_manifold}GPU.cu
hoomd/md/CMakeLists.txt:        configure_file(TwoStepRATTLEGPU.cu.inc
hoomd/md/CMakeLists.txt:                       TwoStepRATTLE${_manifold}GPU.cu
hoomd/md/CMakeLists.txt:        set(_md_sources ${_md_sources} export_ActiveForceConstraintCompute${_manifold}GPU.cc)
hoomd/md/CMakeLists.txt:        set(_cuda_sources ${_cuda_sources}
hoomd/md/CMakeLists.txt:            ActiveForceConstraintCompute${_manifold}GPU.cu
hoomd/md/CMakeLists.txt:            TwoStepRATTLE${_manifold}GPU.cu
hoomd/md/CMakeLists.txt:        set_source_files_properties(${_cuda_sources} PROPERTIES LANGUAGE ${HOOMD_DEVICE_LANGUAGE})
hoomd/md/CMakeLists.txt:            configure_file(export_TwoStepRATTLEGPU.cc.inc
hoomd/md/CMakeLists.txt:                        export_TwoStepRATTLE${_method}${_manifold}GPU.cc
hoomd/md/CMakeLists.txt:            set(_md_sources ${_md_sources} export_TwoStepRATTLE${_method}${_manifold}GPU.cc)
hoomd/md/CMakeLists.txt:            set_source_files_properties(${_cuda_sources} PROPERTIES LANGUAGE ${HOOMD_DEVICE_LANGUAGE})
hoomd/md/CMakeLists.txt:        configure_file(export_PotentialBondGPU.cc.inc
hoomd/md/CMakeLists.txt:                       export_PotentialBond${_bond}GPU.cc
hoomd/md/CMakeLists.txt:        configure_file(PotentialBondGPUKernel.cu.inc
hoomd/md/CMakeLists.txt:                       PotentialBond${_bond}GPUKernel.cu
hoomd/md/CMakeLists.txt:        set(_md_sources ${_md_sources} export_PotentialBond${_bond}GPU.cc)
hoomd/md/CMakeLists.txt:        set(_cuda_sources ${_cuda_sources}
hoomd/md/CMakeLists.txt:            PotentialBond${_bond}GPUKernel.cu
hoomd/md/CMakeLists.txt:        set_source_files_properties(${_cuda_sources} PROPERTIES LANGUAGE ${HOOMD_DEVICE_LANGUAGE})
hoomd/md/CMakeLists.txt:        configure_file(export_PotentialMeshBondGPU.cc.inc
hoomd/md/CMakeLists.txt:                       export_PotentialMeshBond${_bond}GPU.cc
hoomd/md/CMakeLists.txt:        configure_file(PotentialMeshBondGPUKernel.cu.inc
hoomd/md/CMakeLists.txt:                       PotentialMeshBond${_bond}GPUKernel.cu
hoomd/md/CMakeLists.txt:        set(_md_sources ${_md_sources} export_PotentialMeshBond${_bond}GPU.cc)
hoomd/md/CMakeLists.txt:        set(_cuda_sources ${_cuda_sources}
hoomd/md/CMakeLists.txt:            PotentialMeshBond${_bond}GPUKernel.cu
hoomd/md/CMakeLists.txt:        set_source_files_properties(${_cuda_sources} PROPERTIES LANGUAGE ${HOOMD_DEVICE_LANGUAGE})
hoomd/md/CMakeLists.txt:        configure_file(export_PotentialSpecialPairGPU.cc.inc
hoomd/md/CMakeLists.txt:                       export_PotentialSpecialPair${_pair}GPU.cc
hoomd/md/CMakeLists.txt:        configure_file(PotentialSpecialPairGPUKernel.cu.inc
hoomd/md/CMakeLists.txt:                       PotentialSpecialPair${_pair}GPUKernel.cu
hoomd/md/CMakeLists.txt:        set(_md_sources ${_md_sources} export_PotentialSpecialPair${_pair}GPU.cc)
hoomd/md/CMakeLists.txt:        set(_cuda_sources ${_cuda_sources}
hoomd/md/CMakeLists.txt:            PotentialSpecialPair${_pair}GPUKernel.cu
hoomd/md/CMakeLists.txt:        set_source_files_properties(${_cuda_sources} PROPERTIES LANGUAGE ${HOOMD_DEVICE_LANGUAGE})
hoomd/md/CMakeLists.txt:        configure_file(export_PotentialTersoffGPU.cc.inc
hoomd/md/CMakeLists.txt:                       export_PotentialTersoff${_evaluator}GPU.cc
hoomd/md/CMakeLists.txt:        configure_file(PotentialTersoffGPUKernel.cu.inc
hoomd/md/CMakeLists.txt:                       PotentialTersoff${_evaluator}GPUKernel.cu
hoomd/md/CMakeLists.txt:        set(_md_sources ${_md_sources} export_PotentialTersoff${_evaluator}GPU.cc)
hoomd/md/CMakeLists.txt:        set(_cuda_sources ${_cuda_sources}
hoomd/md/CMakeLists.txt:            PotentialTersoff${_evaluator}GPUKernel.cu
hoomd/md/CMakeLists.txt:        set_source_files_properties(${_cuda_sources} PROPERTIES LANGUAGE ${HOOMD_DEVICE_LANGUAGE})
hoomd/md/CMakeLists.txt:        configure_file(export_PotentialExternalGPU.cc.inc
hoomd/md/CMakeLists.txt:                       export_PotentialExternal${_evaluator}GPU.cc
hoomd/md/CMakeLists.txt:        configure_file(PotentialExternalGPUKernel.cu.inc
hoomd/md/CMakeLists.txt:                       PotentialExternal${_evaluator}GPUKernel.cu
hoomd/md/CMakeLists.txt:        set(_md_sources ${_md_sources} export_PotentialExternal${_evaluator}GPU.cc)
hoomd/md/CMakeLists.txt:        set(_cuda_sources ${_cuda_sources}
hoomd/md/CMakeLists.txt:            PotentialExternal${_evaluator}GPUKernel.cu
hoomd/md/CMakeLists.txt:        set_source_files_properties(${_cuda_sources} PROPERTIES LANGUAGE ${HOOMD_DEVICE_LANGUAGE})
hoomd/md/CMakeLists.txt:        configure_file(export_PotentialExternalWallGPU.cc.inc
hoomd/md/CMakeLists.txt:                       export_PotentialExternalWall${_evaluator}GPU.cc
hoomd/md/CMakeLists.txt:        configure_file(PotentialExternalWallGPUKernel.cu.inc
hoomd/md/CMakeLists.txt:                       PotentialExternalWall${_evaluator}GPUKernel.cu
hoomd/md/CMakeLists.txt:        set(_md_sources ${_md_sources} export_PotentialExternalWall${_evaluator}GPU.cc)
hoomd/md/CMakeLists.txt:        set(_cuda_sources ${_cuda_sources}
hoomd/md/CMakeLists.txt:            PotentialExternalWall${_evaluator}GPUKernel.cu
hoomd/md/CMakeLists.txt:        set_source_files_properties(${_cuda_sources} PROPERTIES LANGUAGE ${HOOMD_DEVICE_LANGUAGE})
hoomd/md/CMakeLists.txt:        configure_file(export_PotentialPairDPDThermoGPU.cc.inc
hoomd/md/CMakeLists.txt:                       export_PotentialPairDPDThermo${_evaluator}GPU.cc
hoomd/md/CMakeLists.txt:        configure_file(PotentialPairDPDThermoGPUKernel.cu.inc
hoomd/md/CMakeLists.txt:                       PotentialPairDPDThermo${_evaluator}GPUKernel.cu
hoomd/md/CMakeLists.txt:        set(_md_sources ${_md_sources} export_PotentialPairDPDThermo${_evaluator}GPU.cc)
hoomd/md/CMakeLists.txt:        set(_cuda_sources ${_cuda_sources}
hoomd/md/CMakeLists.txt:            PotentialPairDPDThermo${_evaluator}GPUKernel.cu
hoomd/md/CMakeLists.txt:        set_source_files_properties(${_cuda_sources} PROPERTIES LANGUAGE ${HOOMD_DEVICE_LANGUAGE})
hoomd/md/CMakeLists.txt:        configure_file(export_PotentialPairGPU.cc.inc
hoomd/md/CMakeLists.txt:                       export_PotentialPair${_evaluator}GPU.cc
hoomd/md/CMakeLists.txt:        configure_file(PotentialPairGPUKernel.cu.inc
hoomd/md/CMakeLists.txt:                       PotentialPair${_evaluator}GPUKernel.cu
hoomd/md/CMakeLists.txt:        set(_md_sources ${_md_sources} export_PotentialPair${_evaluator}GPU.cc)
hoomd/md/CMakeLists.txt:        set(_cuda_sources ${_cuda_sources}
hoomd/md/CMakeLists.txt:            PotentialPair${_evaluator}GPUKernel.cu
hoomd/md/CMakeLists.txt:        set_source_files_properties(${_cuda_sources} PROPERTIES LANGUAGE ${HOOMD_DEVICE_LANGUAGE})
hoomd/md/CMakeLists.txt:        configure_file(export_AnisoPotentialPairPatchyGPU.cc.inc
hoomd/md/CMakeLists.txt:                       export_AnisoPotentialPairPatchy${_evaluator}GPU.cc
hoomd/md/CMakeLists.txt:        configure_file(AnisoPotentialPairPatchyGPUKernel.cu.inc
hoomd/md/CMakeLists.txt:                       AnisoPotentialPairPatchy${_evaluator}GPUKernel.cu
hoomd/md/CMakeLists.txt:        set(_md_sources ${_md_sources} export_AnisoPotentialPairPatchy${_evaluator}GPU.cc)
hoomd/md/CMakeLists.txt:        set(_cuda_sources ${_cuda_sources}
hoomd/md/CMakeLists.txt:            AnisoPotentialPairPatchy${_evaluator}GPUKernel.cu)
hoomd/md/CMakeLists.txt:        set_source_files_properties(${_cuda_sources} PROPERTIES LANGUAGE ${HOOMD_DEVICE_LANGUAGE})
hoomd/md/CMakeLists.txt:hoomd_add_module(_md SHARED ${_md_sources} ${_cuda_sources} ${DFFT_SOURCES} ${_md_headers} NO_EXTRAS)
hoomd/md/CMakeLists.txt:    # CUDA 8.0 requires that we link in gomp
hoomd/md/CMakeLists.txt:    target_link_libraries(_md PUBLIC _hoomd CUDA::cusolver CUDA::cusparse gomp)
hoomd/md/TwoStepConstantVolumeGPU.cc:#include "TwoStepConstantVolumeGPU.h"
hoomd/md/TwoStepConstantVolumeGPU.cc:#include "TwoStepConstantVolumeGPU.cuh"
hoomd/md/TwoStepConstantVolumeGPU.cc:#include "TwoStepNVEGPU.cuh"
hoomd/md/TwoStepConstantVolumeGPU.cc:TwoStepConstantVolumeGPU::TwoStepConstantVolumeGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/md/TwoStepConstantVolumeGPU.cc:    if (!m_exec_conf->isCUDAEnabled())
hoomd/md/TwoStepConstantVolumeGPU.cc:        throw std::runtime_error("Cannot create TwoStepNVTMTKGPU on a CPU device.");
hoomd/md/TwoStepConstantVolumeGPU.cc:void TwoStepConstantVolumeGPU::integrateStepOne(uint64_t timestep)
hoomd/md/TwoStepConstantVolumeGPU.cc:        m_exec_conf->beginMultiGPU();
hoomd/md/TwoStepConstantVolumeGPU.cc:        // perform the update on the GPU
hoomd/md/TwoStepConstantVolumeGPU.cc:        kernel::gpu_nvt_rescale_step_one(d_pos.data,
hoomd/md/TwoStepConstantVolumeGPU.cc:                                         m_group->getGPUPartition(),
hoomd/md/TwoStepConstantVolumeGPU.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/TwoStepConstantVolumeGPU.cc:            CHECK_CUDA_ERROR();
hoomd/md/TwoStepConstantVolumeGPU.cc:        m_exec_conf->endMultiGPU();
hoomd/md/TwoStepConstantVolumeGPU.cc:        m_exec_conf->beginMultiGPU();
hoomd/md/TwoStepConstantVolumeGPU.cc:        kernel::gpu_nve_angular_step_one(d_orientation.data,
hoomd/md/TwoStepConstantVolumeGPU.cc:                                         m_group->getGPUPartition(),
hoomd/md/TwoStepConstantVolumeGPU.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/TwoStepConstantVolumeGPU.cc:            CHECK_CUDA_ERROR();
hoomd/md/TwoStepConstantVolumeGPU.cc:        m_exec_conf->endMultiGPU();
hoomd/md/TwoStepConstantVolumeGPU.cc:void TwoStepConstantVolumeGPU::integrateStepTwo(uint64_t timestep)
hoomd/md/TwoStepConstantVolumeGPU.cc:        m_exec_conf->beginMultiGPU();
hoomd/md/TwoStepConstantVolumeGPU.cc:        // perform the update on the GPU
hoomd/md/TwoStepConstantVolumeGPU.cc:        kernel::gpu_nvt_rescale_step_two(d_vel.data,
hoomd/md/TwoStepConstantVolumeGPU.cc:                                         m_group->getGPUPartition());
hoomd/md/TwoStepConstantVolumeGPU.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/TwoStepConstantVolumeGPU.cc:            CHECK_CUDA_ERROR();
hoomd/md/TwoStepConstantVolumeGPU.cc:        m_exec_conf->endMultiGPU();
hoomd/md/TwoStepConstantVolumeGPU.cc:        m_exec_conf->beginMultiGPU();
hoomd/md/TwoStepConstantVolumeGPU.cc:        kernel::gpu_nve_angular_step_two(d_orientation.data,
hoomd/md/TwoStepConstantVolumeGPU.cc:                                         m_group->getGPUPartition(),
hoomd/md/TwoStepConstantVolumeGPU.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/TwoStepConstantVolumeGPU.cc:            CHECK_CUDA_ERROR();
hoomd/md/TwoStepConstantVolumeGPU.cc:        m_exec_conf->endMultiGPU();
hoomd/md/TwoStepConstantVolumeGPU.cc:void export_TwoStepConstantVolumeGPU(pybind11::module& m)
hoomd/md/TwoStepConstantVolumeGPU.cc:    pybind11::class_<TwoStepConstantVolumeGPU,
hoomd/md/TwoStepConstantVolumeGPU.cc:                     std::shared_ptr<TwoStepConstantVolumeGPU>>(m, "TwoStepConstantVolumeGPU")
hoomd/md/improper.py:            cpp_class = getattr(self._ext_module, self._cpp_class_name + "GPU")
hoomd/md/TwoStepRATTLENVEGPU.cuh:/*! \file TwoStepRATTLENVEGPU.cuh
hoomd/md/TwoStepRATTLENVEGPU.cuh:    \brief Declares GPU kernel code for RATTLENVE integration on the GPU. Used by
hoomd/md/TwoStepRATTLENVEGPU.cuh:   TwoStepRATTLENVEGPU.
hoomd/md/TwoStepRATTLENVEGPU.cuh:#include "TwoStepRATTLENVEGPU.cuh"
hoomd/md/TwoStepRATTLENVEGPU.cuh:#include "hoomd/GPUPartition.cuh"
hoomd/md/TwoStepRATTLENVEGPU.cuh:#ifndef __TWO_STEP_RATTLE_NVE_GPU_CUH__
hoomd/md/TwoStepRATTLENVEGPU.cuh:#define __TWO_STEP_RATTLE_NVE_GPU_CUH__
hoomd/md/TwoStepRATTLENVEGPU.cuh:hipError_t gpu_rattle_nve_step_one(Scalar4* d_pos,
hoomd/md/TwoStepRATTLENVEGPU.cuh:                                   const GPUPartition& gpu_partition,
hoomd/md/TwoStepRATTLENVEGPU.cuh:hipError_t gpu_rattle_nve_angular_step_one(Scalar4* d_orientation,
hoomd/md/TwoStepRATTLENVEGPU.cuh:                                           const GPUPartition& gpu_partition,
hoomd/md/TwoStepRATTLENVEGPU.cuh:hipError_t gpu_rattle_nve_step_two(Scalar4* d_pos,
hoomd/md/TwoStepRATTLENVEGPU.cuh:                                   const GPUPartition& gpu_partition,
hoomd/md/TwoStepRATTLENVEGPU.cuh:hipError_t gpu_rattle_nve_angular_step_two(const Scalar4* d_orientation,
hoomd/md/TwoStepRATTLENVEGPU.cuh:                                           const GPUPartition& gpu_partition,
hoomd/md/TwoStepRATTLENVEGPU.cuh:hipError_t gpu_include_rattle_force_nve(const Scalar4* d_pos,
hoomd/md/TwoStepRATTLENVEGPU.cuh:                                        const GPUPartition& gpu_partition,
hoomd/md/TwoStepRATTLENVEGPU.cuh:/*! \file TwoStepNVEGPU.cu
hoomd/md/TwoStepRATTLENVEGPU.cuh:    \brief Defines GPU kernel code for NVE integration on the GPU. Used by TwoStepNVEGPU.
hoomd/md/TwoStepRATTLENVEGPU.cuh:   gpu_rattle_nve_step_one_kernel(), see it for design details.
hoomd/md/TwoStepRATTLENVEGPU.cuh:__global__ void gpu_rattle_nve_step_two_kernel(Scalar4* d_pos,
hoomd/md/TwoStepRATTLENVEGPU.cuh:   particles in the group This is just a driver for gpu_rattle_nve_step_two_kernel(), see it for
hoomd/md/TwoStepRATTLENVEGPU.cuh:hipError_t gpu_rattle_nve_step_two(Scalar4* d_pos,
hoomd/md/TwoStepRATTLENVEGPU.cuh:                                   const GPUPartition& gpu_partition,
hoomd/md/TwoStepRATTLENVEGPU.cuh:    hipFuncGetAttributes(&attr, (const void*)gpu_rattle_nve_step_two_kernel<Manifold>);
hoomd/md/TwoStepRATTLENVEGPU.cuh:    // iterate over active GPUs in reverse, to end up on first GPU when returning from this function
hoomd/md/TwoStepRATTLENVEGPU.cuh:    for (int idev = gpu_partition.getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/md/TwoStepRATTLENVEGPU.cuh:        auto range = gpu_partition.getRangeAndSetGPU(idev);
hoomd/md/TwoStepRATTLENVEGPU.cuh:        hipLaunchKernelGGL((gpu_rattle_nve_step_two_kernel<Manifold>),
hoomd/md/TwoStepRATTLENVEGPU.cuh:__global__ void gpu_include_rattle_force_nve_kernel(const Scalar4* d_pos,
hoomd/md/TwoStepRATTLENVEGPU.cuh:hipError_t gpu_include_rattle_force_nve(const Scalar4* d_pos,
hoomd/md/TwoStepRATTLENVEGPU.cuh:                                        const GPUPartition& gpu_partition,
hoomd/md/TwoStepRATTLENVEGPU.cuh:    hipFuncGetAttributes(&attr, (const void*)gpu_include_rattle_force_nve_kernel<Manifold>);
hoomd/md/TwoStepRATTLENVEGPU.cuh:    // iterate over active GPUs in reverse, to end up on first GPU when returning from this function
hoomd/md/TwoStepRATTLENVEGPU.cuh:    for (int idev = gpu_partition.getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/md/TwoStepRATTLENVEGPU.cuh:        auto range = gpu_partition.getRangeAndSetGPU(idev);
hoomd/md/TwoStepRATTLENVEGPU.cuh:        hipLaunchKernelGGL((gpu_include_rattle_force_nve_kernel<Manifold>),
hoomd/md/TwoStepRATTLENVEGPU.cuh:#endif //__TWO_STEP_RATTLE_NVE_GPU_CUH__
hoomd/md/AnisoPotentialPairGBGPU.cc:#include "AnisoPotentialPairGPU.h"
hoomd/md/AnisoPotentialPairGBGPU.cc:void export_AnisoPotentialPairGBGPU(pybind11::module& m)
hoomd/md/AnisoPotentialPairGBGPU.cc:    export_AnisoPotentialPairGPU<EvaluatorPairGB>(m, "AnisoPotentialPairGBGPU");
hoomd/md/update.py:        `ZeroMomentum` executes on the CPU even when using a GPU device.
hoomd/md/update.py:            self._cpp_obj = _md.MuellerPlatheFlowGPU(
hoomd/md/TwoStepBDGPU.h://! Implements Brownian dynamics on the GPU
hoomd/md/TwoStepBDGPU.h:/*! GPU accelerated version of TwoStepBD
hoomd/md/TwoStepBDGPU.h:class PYBIND11_EXPORT TwoStepBDGPU : public TwoStepBD
hoomd/md/TwoStepBDGPU.h:    TwoStepBDGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/md/TwoStepBDGPU.h:    virtual ~TwoStepBDGPU() { };
hoomd/md/EvaluatorPairForceShiftedLJ.h:    //! Set CUDA memory hints
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cc:#include "TriangleAreaConservationMeshForceComputeGPU.h"
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cc:/*! \file TriangleAreaConservationMeshForceComputeGPU.cc
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cc:    \brief Contains code for the TriangleAreaConservationhMeshForceComputeGPU class
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cc:TriangleAreaConservationMeshForceComputeGPU::TriangleAreaConservationMeshForceComputeGPU(
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cc:    if (!m_exec_conf->isCUDAEnabled())
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cc:            << "Creating a TriangleAreaConservationMeshForceComputeGPU with no GPU "
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cc:        throw std::runtime_error("Error initializing TriangleAreaConservationMeshForceComputeGPU");
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cc:    GPUArray<Scalar> sum(this->m_mesh_data->getMeshTriangleData()->getNTypes(), m_exec_conf);
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cc:    GPUArray<Scalar> partial_sum(m_num_blocks, m_exec_conf);
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cc:void TriangleAreaConservationMeshForceComputeGPU::computeForces(uint64_t timestep)
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cc:    const GPUArray<typename Angle::members_t>& gpu_meshtriangle_list
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cc:        = this->m_mesh_data->getMeshTriangleData()->getGPUTable();
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cc:    const Index2D& gpu_table_indexer
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cc:        = this->m_mesh_data->getMeshTriangleData()->getGPUTableIndexer();
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cc:    ArrayHandle<typename Angle::members_t> d_gpu_meshtrianglelist(gpu_meshtriangle_list,
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cc:    ArrayHandle<unsigned int> d_gpu_meshtriangle_pos_list(
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cc:        m_mesh_data->getMeshTriangleData()->getGPUPosTable(),
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cc:    ArrayHandle<unsigned int> d_gpu_n_meshtriangle(
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cc:    kernel::gpu_compute_TriangleAreaConservation_force(
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cc:        d_gpu_meshtrianglelist.data,
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cc:        d_gpu_meshtriangle_pos_list.data,
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cc:        gpu_table_indexer,
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cc:        d_gpu_n_meshtriangle.data,
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cc:    if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cc:        CHECK_CUDA_ERROR();
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cc:void TriangleAreaConservationMeshForceComputeGPU::computeArea()
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cc:    const GPUArray<typename Angle::members_t>& gpu_meshtriangle_list
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cc:        = this->m_mesh_data->getMeshTriangleData()->getGPUTable();
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cc:    const Index2D& gpu_table_indexer
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cc:        = this->m_mesh_data->getMeshTriangleData()->getGPUTableIndexer();
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cc:    ArrayHandle<typename Angle::members_t> d_gpu_meshtrianglelist(gpu_meshtriangle_list,
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cc:    ArrayHandle<unsigned int> d_gpu_n_meshtriangle(
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cc:    ArrayHandle<unsigned int> d_gpu_meshtriangle_pos_list(
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cc:        m_mesh_data->getMeshTriangleData()->getGPUPosTable(),
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cc:    kernel::gpu_compute_area_constraint_area(d_sumA.data,
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cc:                                             d_gpu_meshtrianglelist.data,
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cc:                                             d_gpu_meshtriangle_pos_list.data,
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cc:                                             gpu_table_indexer,
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cc:                                             d_gpu_n_meshtriangle.data,
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cc:    if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cc:        CHECK_CUDA_ERROR();
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cc:void export_TriangleAreaConservationMeshForceComputeGPU(pybind11::module& m)
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cc:    pybind11::class_<TriangleAreaConservationMeshForceComputeGPU,
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cc:                     std::shared_ptr<TriangleAreaConservationMeshForceComputeGPU>>(
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cc:        "TriangleAreaConservationMeshForceComputeGPU")
hoomd/md/ComputeThermoHMAGPU.cu:#include "ComputeThermoHMAGPU.cuh"
hoomd/md/ComputeThermoHMAGPU.cu:/*! \file ComputeThermoGPU.cu
hoomd/md/ComputeThermoHMAGPU.cu:    \brief Defines GPU kernel code for computing thermodynamic properties on the GPU. Used by
hoomd/md/ComputeThermoHMAGPU.cu:   ComputeThermoGPU.
hoomd/md/ComputeThermoHMAGPU.cu://! Perform partial sums of the thermo properties on the GPU
hoomd/md/ComputeThermoHMAGPU.cu:    \param work_size Number of particles in the group this GPU processes
hoomd/md/ComputeThermoHMAGPU.cu:    \param offset Offset of this GPU in list of group members
hoomd/md/ComputeThermoHMAGPU.cu:    \param block_offset Offset of this GPU in the array of partial sums
hoomd/md/ComputeThermoHMAGPU.cu:__global__ void gpu_compute_thermo_hma_partial_sums(Scalar3* d_scratch,
hoomd/md/ComputeThermoHMAGPU.cu:__global__ void gpu_compute_thermo_hma_final_sums(Scalar* d_properties,
hoomd/md/ComputeThermoHMAGPU.cu:        // fill out the GPUArray
hoomd/md/ComputeThermoHMAGPU.cu://! Compute partial sums of thermodynamic properties of a group on the GPU,
hoomd/md/ComputeThermoHMAGPU.cu:    \param gpu_partition Load balancing info for multi-GPU reduction
hoomd/md/ComputeThermoHMAGPU.cu:    This function drives gpu_compute_thermo_partial_sums and gpu_compute_thermo_final_sums, see them
hoomd/md/ComputeThermoHMAGPU.cu:hipError_t gpu_compute_thermo_hma_partial(Scalar4* d_pos,
hoomd/md/ComputeThermoHMAGPU.cu:                                          const GPUPartition& gpu_partition)
hoomd/md/ComputeThermoHMAGPU.cu:    // iterate over active GPUs in reverse, to end up on first GPU when returning from this function
hoomd/md/ComputeThermoHMAGPU.cu:    for (int idev = gpu_partition.getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/md/ComputeThermoHMAGPU.cu:        auto range = gpu_partition.getRangeAndSetGPU(idev);
hoomd/md/ComputeThermoHMAGPU.cu:        gpu_compute_thermo_hma_partial_sums<<<grid, threads, shared_bytes>>>(args.d_scratch,
hoomd/md/ComputeThermoHMAGPU.cu://! Compute thermodynamic properties of a group on the GPU
hoomd/md/ComputeThermoHMAGPU.cu:    This function drives gpu_compute_thermo_partial_sums and gpu_compute_thermo_final_sums, see them
hoomd/md/ComputeThermoHMAGPU.cu:hipError_t gpu_compute_thermo_hma_final(Scalar* d_properties,
hoomd/md/ComputeThermoHMAGPU.cu:    gpu_compute_thermo_hma_final_sums<<<grid, threads, shared_bytes>>>(d_properties,
hoomd/md/OPLSDihedralForceCompute.cc:    GPUArray<Scalar4> params(m_dihedral_data->getNTypes(), m_exec_conf);
hoomd/md/AnisoPotentialPair.h:   stored in GlobalArray for easy access on the GPU by a derived class. The type of the parameters
hoomd/md/AnisoPotentialPair.h:        hoomd::detail::managed_allocator<param_type>(m_exec_conf->isCUDAEnabled()));
hoomd/md/AnisoPotentialPair.h:        hoomd::detail::managed_allocator<shape_type>(m_exec_conf->isCUDAEnabled()));
hoomd/md/AnisoPotentialPair.h:    if (m_exec_conf->isCUDAEnabled())
hoomd/md/AnisoPotentialPair.h:        cudaMemAdvise(m_params.data(),
hoomd/md/AnisoPotentialPair.h:                      cudaMemAdviseSetReadMostly,
hoomd/md/AnisoPotentialPair.h:        cudaMemAdvise(m_shape_params.data(),
hoomd/md/AnisoPotentialPair.h:                      cudaMemAdviseSetReadMostly,
hoomd/md/AnisoPotentialPair.h:        auto& gpu_map = m_exec_conf->getGPUIds();
hoomd/md/AnisoPotentialPair.h:        for (unsigned int idev = 0; idev < m_exec_conf->getNumActiveGPUs(); ++idev)
hoomd/md/AnisoPotentialPair.h:            // prefetch data on all GPUs
hoomd/md/AnisoPotentialPair.h:            cudaMemPrefetchAsync(m_params.data(),
hoomd/md/AnisoPotentialPair.h:                                 gpu_map[idev]);
hoomd/md/AnisoPotentialPair.h:            cudaMemPrefetchAsync(m_shape_params.data(),
hoomd/md/AnisoPotentialPair.h:                                 gpu_map[idev]);
hoomd/md/AnisoPotentialPair.h:            cudaMemAdvise(m_rcutsq.get(),
hoomd/md/AnisoPotentialPair.h:                          cudaMemAdviseSetReadMostly,
hoomd/md/AnisoPotentialPair.h:            for (unsigned int idev = 0; idev < m_exec_conf->getNumActiveGPUs(); ++idev)
hoomd/md/AnisoPotentialPair.h:                // prefetch data on all GPUs
hoomd/md/AnisoPotentialPair.h:                cudaMemPrefetchAsync(m_rcutsq.get(),
hoomd/md/AnisoPotentialPair.h:                                     gpu_map[idev]);
hoomd/md/AnisoPotentialPair.h:    setParams(typ1, typ2, param_type(params, m_exec_conf->isCUDAEnabled()));
hoomd/md/AnisoPotentialPair.h:    setShape(typ_, shape_type(shape_param, m_exec_conf->isCUDAEnabled()));
hoomd/md/AreaConservationMeshForceComputeGPU.cu:#include "AreaConservationMeshForceComputeGPU.cuh"
hoomd/md/AreaConservationMeshForceComputeGPU.cu:/*! \file MeshAreaConservationGPU.cu
hoomd/md/AreaConservationMeshForceComputeGPU.cu:    \brief Defines GPU kernel code for calculating the area_constraint forces. Used by
hoomd/md/AreaConservationMeshForceComputeGPU.cu:   MeshAreaConservationComputeGPU.
hoomd/md/AreaConservationMeshForceComputeGPU.cu://! Kernel for calculating area_constraint sigmas on the GPU
hoomd/md/AreaConservationMeshForceComputeGPU.cu:    \param box Box dimensions (in GPU format) to use for periodic boundary conditions
hoomd/md/AreaConservationMeshForceComputeGPU.cu:    \param tlist List of mesh triangle indices stored on the GPU
hoomd/md/AreaConservationMeshForceComputeGPU.cu:    \param tpos_list Position of current index in list of mesh triangles stored on the GPU
hoomd/md/AreaConservationMeshForceComputeGPU.cu:    \param n_triangles_list List of mesh triangles stored on the GPU
hoomd/md/AreaConservationMeshForceComputeGPU.cu:__global__ void gpu_compute_area_constraint_area_kernel(Scalar* d_partial_sum_area,
hoomd/md/AreaConservationMeshForceComputeGPU.cu:__global__ void gpu_area_reduce_partial_sum_kernel(Scalar* d_sum,
hoomd/md/AreaConservationMeshForceComputeGPU.cu:    \param box Box dimensions (in GPU format) to use for periodic boundary conditions
hoomd/md/AreaConservationMeshForceComputeGPU.cu:    \param tlist List of mesh triangle indices stored on the GPU
hoomd/md/AreaConservationMeshForceComputeGPU.cu:    \param tpos_list Position of current index in list of mesh triangles stored on the GPU
hoomd/md/AreaConservationMeshForceComputeGPU.cu:    \param n_triangles_list List of mesh triangles stored on the GPU
hoomd/md/AreaConservationMeshForceComputeGPU.cu:hipError_t gpu_compute_area_constraint_area(Scalar* d_sum_area,
hoomd/md/AreaConservationMeshForceComputeGPU.cu:        hipLaunchKernelGGL((gpu_compute_area_constraint_area_kernel),
hoomd/md/AreaConservationMeshForceComputeGPU.cu:    hipLaunchKernelGGL((gpu_area_reduce_partial_sum_kernel),
hoomd/md/AreaConservationMeshForceComputeGPU.cu://! Kernel for calculating area_constraint sigmas on the GPU
hoomd/md/AreaConservationMeshForceComputeGPU.cu:    \param box Box dimensions (in GPU format) to use for periodic boundary conditions
hoomd/md/AreaConservationMeshForceComputeGPU.cu:    \param tlist List of mesh triangle indices stored on the GPU
hoomd/md/AreaConservationMeshForceComputeGPU.cu:    \param tpos_list Position of current index in list of mesh triangles stored on the GPU
hoomd/md/AreaConservationMeshForceComputeGPU.cu:__global__ void gpu_compute_area_constraint_force_kernel(Scalar4* d_force,
hoomd/md/AreaConservationMeshForceComputeGPU.cu:    \param box Box dimensions (in GPU format) to use for periodic boundary conditions
hoomd/md/AreaConservationMeshForceComputeGPU.cu:    \param tlist List of mesh triangle indices stored on the GPU
hoomd/md/AreaConservationMeshForceComputeGPU.cu:    \param tpos_list Position of current index in list of mesh triangles stored on the GPU
hoomd/md/AreaConservationMeshForceComputeGPU.cu:hipError_t gpu_compute_area_constraint_force(Scalar4* d_force,
hoomd/md/AreaConservationMeshForceComputeGPU.cu:    hipFuncGetAttributes(&attr, (const void*)gpu_compute_area_constraint_force_kernel);
hoomd/md/AreaConservationMeshForceComputeGPU.cu:    hipLaunchKernelGGL((gpu_compute_area_constraint_force_kernel),
hoomd/md/BendingRigidityMeshForceComputeGPU.cc:#include "BendingRigidityMeshForceComputeGPU.h"
hoomd/md/BendingRigidityMeshForceComputeGPU.cc:/*! \file BendingRigidityMeshForceComputeGPU.cc
hoomd/md/BendingRigidityMeshForceComputeGPU.cc:    \brief Contains code for the BendingRigidityMeshForceComputeGPU class
hoomd/md/BendingRigidityMeshForceComputeGPU.cc:BendingRigidityMeshForceComputeGPU::BendingRigidityMeshForceComputeGPU(
hoomd/md/BendingRigidityMeshForceComputeGPU.cc:    if (!m_exec_conf->isCUDAEnabled())
hoomd/md/BendingRigidityMeshForceComputeGPU.cc:        m_exec_conf->msg->error() << "Creating a BendingRigidityMeshForceComputeGPU with no GPU in "
hoomd/md/BendingRigidityMeshForceComputeGPU.cc:        throw std::runtime_error("Error initializing BendingRigidityMeshForceComputeGPU");
hoomd/md/BendingRigidityMeshForceComputeGPU.cc:    GPUArray<Scalar> params(this->m_mesh_data->getMeshTriangleData()->getNTypes(),
hoomd/md/BendingRigidityMeshForceComputeGPU.cc:BendingRigidityMeshForceComputeGPU::~BendingRigidityMeshForceComputeGPU() { }
hoomd/md/BendingRigidityMeshForceComputeGPU.cc:void BendingRigidityMeshForceComputeGPU::computeForces(uint64_t timestep)
hoomd/md/BendingRigidityMeshForceComputeGPU.cc:    const GPUArray<typename MeshBond::members_t>& gpu_meshbond_list
hoomd/md/BendingRigidityMeshForceComputeGPU.cc:        = this->m_mesh_data->getMeshBondData()->getGPUTable();
hoomd/md/BendingRigidityMeshForceComputeGPU.cc:    const Index2D& gpu_table_indexer = this->m_mesh_data->getMeshBondData()->getGPUTableIndexer();
hoomd/md/BendingRigidityMeshForceComputeGPU.cc:    ArrayHandle<typename MeshBond::members_t> d_gpu_meshbondlist(gpu_meshbond_list,
hoomd/md/BendingRigidityMeshForceComputeGPU.cc:    ArrayHandle<unsigned int> d_gpu_meshbond_pos_list(
hoomd/md/BendingRigidityMeshForceComputeGPU.cc:        this->m_mesh_data->getMeshBondData()->getGPUPosTable(),
hoomd/md/BendingRigidityMeshForceComputeGPU.cc:    ArrayHandle<unsigned int> d_gpu_n_meshbond(
hoomd/md/BendingRigidityMeshForceComputeGPU.cc:    // run the kernel on the GPU
hoomd/md/BendingRigidityMeshForceComputeGPU.cc:    kernel::gpu_compute_bending_rigidity_force(d_force.data,
hoomd/md/BendingRigidityMeshForceComputeGPU.cc:                                               d_gpu_meshbondlist.data,
hoomd/md/BendingRigidityMeshForceComputeGPU.cc:                                               gpu_table_indexer,
hoomd/md/BendingRigidityMeshForceComputeGPU.cc:                                               d_gpu_meshbond_pos_list.data,
hoomd/md/BendingRigidityMeshForceComputeGPU.cc:                                               d_gpu_n_meshbond.data,
hoomd/md/BendingRigidityMeshForceComputeGPU.cc:    if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/BendingRigidityMeshForceComputeGPU.cc:        CHECK_CUDA_ERROR();
hoomd/md/BendingRigidityMeshForceComputeGPU.cc:void export_BendingRigidityMeshForceComputeGPU(pybind11::module& m)
hoomd/md/BendingRigidityMeshForceComputeGPU.cc:    pybind11::class_<BendingRigidityMeshForceComputeGPU,
hoomd/md/BendingRigidityMeshForceComputeGPU.cc:                     std::shared_ptr<BendingRigidityMeshForceComputeGPU>>(
hoomd/md/BendingRigidityMeshForceComputeGPU.cc:        "BendingRigidityMeshForceComputeGPU")
hoomd/md/OPLSDihedralForceComputeGPU.h:#include "OPLSDihedralForceGPU.cuh"
hoomd/md/OPLSDihedralForceComputeGPU.h:/*! \file OPLSDihedralForceComputeGPU.h
hoomd/md/OPLSDihedralForceComputeGPU.h:    \brief Declares the OPLSDihedralForceComputeGPU class
hoomd/md/OPLSDihedralForceComputeGPU.h:#ifndef __OPLSDIHEDRALFORCECOMPUTEGPU_H__
hoomd/md/OPLSDihedralForceComputeGPU.h:#define __OPLSDIHEDRALFORCECOMPUTEGPU_H__
hoomd/md/OPLSDihedralForceComputeGPU.h://! Computes OPLS-style dihedral potentials on the GPU
hoomd/md/OPLSDihedralForceComputeGPU.h:/*! Calculates the OPLS type dihedral force on the GPU
hoomd/md/OPLSDihedralForceComputeGPU.h:    The GPU kernel for calculating this can be found in OPLSDihedralForceComputeGPU.cu
hoomd/md/OPLSDihedralForceComputeGPU.h:class PYBIND11_EXPORT OPLSDihedralForceComputeGPU : public OPLSDihedralForceCompute
hoomd/md/OPLSDihedralForceComputeGPU.h:    OPLSDihedralForceComputeGPU(std::shared_ptr<SystemDefinition> sysdef);
hoomd/md/OPLSDihedralForceComputeGPU.h:    virtual ~OPLSDihedralForceComputeGPU() { }
hoomd/md/PotentialSpecialPair.h:#include "hoomd/GPUArray.h"
hoomd/md/PotentialSpecialPair.h:    GPUArray<param_type> m_params;         //!< SpecialPair parameters per type
hoomd/md/PotentialSpecialPair.h:    GPUArray<param_type> params(m_pair_data->getNTypes(), m_exec_conf);
hoomd/md/HelfrichMeshForceComputeGPU.cu:#include "HelfrichMeshForceComputeGPU.cuh"
hoomd/md/HelfrichMeshForceComputeGPU.cu:/*! \file HelfrichMeshForceComputeGPU.cu
hoomd/md/HelfrichMeshForceComputeGPU.cu:    \brief Defines GPU kernel code for calculating the helfrich forces. Used by
hoomd/md/HelfrichMeshForceComputeGPU.cu:   HelfrichMeshForceComputeComputeGPU.
hoomd/md/HelfrichMeshForceComputeGPU.cu://! Kernel for calculating helfrich sigmas on the GPU
hoomd/md/HelfrichMeshForceComputeGPU.cu:    \param box Box dimensions (in GPU format) to use for periodic boundary conditions
hoomd/md/HelfrichMeshForceComputeGPU.cu:    \param blist List of mesh bonds stored on the GPU
hoomd/md/HelfrichMeshForceComputeGPU.cu:    \param bpos_list Position of current index in list of mesh bonds stored on the GPU
hoomd/md/HelfrichMeshForceComputeGPU.cu:    \param n_bonds_list List of numbers of mesh bonds stored on the GPU
hoomd/md/HelfrichMeshForceComputeGPU.cu:__global__ void gpu_compute_helfrich_sigma_kernel(Scalar* d_sigma,
hoomd/md/HelfrichMeshForceComputeGPU.cu:    \param box Box dimensions (in GPU format) to use for periodic boundary conditions
hoomd/md/HelfrichMeshForceComputeGPU.cu:    \param blist List of mesh bonds stored on the GPU
hoomd/md/HelfrichMeshForceComputeGPU.cu:    \param n_bonds_list List of numbers of mesh bonds stored on the GPU
hoomd/md/HelfrichMeshForceComputeGPU.cu:hipError_t gpu_compute_helfrich_sigma(Scalar* d_sigma,
hoomd/md/HelfrichMeshForceComputeGPU.cu:    hipFuncGetAttributes(&attr, (const void*)gpu_compute_helfrich_sigma_kernel);
hoomd/md/HelfrichMeshForceComputeGPU.cu:    hipLaunchKernelGGL((gpu_compute_helfrich_sigma_kernel),
hoomd/md/HelfrichMeshForceComputeGPU.cu://! Kernel for calculating helfrich sigmas on the GPU
hoomd/md/HelfrichMeshForceComputeGPU.cu:    \param box Box dimensions (in GPU format) to use for periodic boundary conditions
hoomd/md/HelfrichMeshForceComputeGPU.cu:    \param blist List of mesh bonds stored on the GPU
hoomd/md/HelfrichMeshForceComputeGPU.cu:    \param bpos_list Position of current index in list of mesh bonds stored on the GPU
hoomd/md/HelfrichMeshForceComputeGPU.cu:    \param n_bonds_list List of numbers of mesh bonds stored on the GPU
hoomd/md/HelfrichMeshForceComputeGPU.cu:__global__ void gpu_compute_helfrich_force_kernel(Scalar4* d_force,
hoomd/md/HelfrichMeshForceComputeGPU.cu:    \param box Box dimensions (in GPU format) to use for periodic boundary conditions
hoomd/md/HelfrichMeshForceComputeGPU.cu:    \param blist List of mesh bonds stored on the GPU
hoomd/md/HelfrichMeshForceComputeGPU.cu:    \param bpos_list Position of current index in list of mesh bonds stored on the GPU
hoomd/md/HelfrichMeshForceComputeGPU.cu:    \param n_bonds_list List of numbers of mesh bonds stored on the GPU
hoomd/md/HelfrichMeshForceComputeGPU.cu:hipError_t gpu_compute_helfrich_force(Scalar4* d_force,
hoomd/md/HelfrichMeshForceComputeGPU.cu:    hipFuncGetAttributes(&attr, (const void*)gpu_compute_helfrich_force_kernel);
hoomd/md/HelfrichMeshForceComputeGPU.cu:    hipLaunchKernelGGL((gpu_compute_helfrich_force_kernel),
hoomd/md/BondTablePotential.h:#include "hoomd/GPUArray.h"
hoomd/md/BondTablePotential.h:   maximum r, and spacing between r values in the table respectively. For simple access on the GPU,
hoomd/md/BondTablePotential.h:    GPUArray<Scalar2> m_tables;            //!< Stored V and F tables
hoomd/md/BondTablePotential.h:    GPUArray<Scalar4> m_params;            //!< Parameters stored for each table
hoomd/md/TwoStepRATTLEBDGPU.h:#include "TwoStepRATTLEBDGPU.cuh"
hoomd/md/TwoStepRATTLEBDGPU.h://! Implements Brownian dynamics on the GPU
hoomd/md/TwoStepRATTLEBDGPU.h:/*! GPU accelerated version of TwoStepBD
hoomd/md/TwoStepRATTLEBDGPU.h:template<class Manifold> class PYBIND11_EXPORT TwoStepRATTLEBDGPU : public TwoStepRATTLEBD<Manifold>
hoomd/md/TwoStepRATTLEBDGPU.h:    TwoStepRATTLEBDGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/md/TwoStepRATTLEBDGPU.h:    virtual ~TwoStepRATTLEBDGPU() { };
hoomd/md/TwoStepRATTLEBDGPU.h:TwoStepRATTLEBDGPU<Manifold>::TwoStepRATTLEBDGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/md/TwoStepRATTLEBDGPU.h:    if (!this->m_exec_conf->isCUDAEnabled())
hoomd/md/TwoStepRATTLEBDGPU.h:            << "Creating a TwoStepRATTLEBDGPU while CUDA is disabled" << std::endl;
hoomd/md/TwoStepRATTLEBDGPU.h:        throw std::runtime_error("Error initializing TwoStepRATTLEBDGPU");
hoomd/md/TwoStepRATTLEBDGPU.h:template<class Manifold> void TwoStepRATTLEBDGPU<Manifold>::integrateStepOne(uint64_t timestep)
hoomd/md/TwoStepRATTLEBDGPU.h:        auto& gpu_map = this->m_exec_conf->getGPUIds();
hoomd/md/TwoStepRATTLEBDGPU.h:        for (unsigned int idev = 0; idev < this->m_exec_conf->getNumActiveGPUs(); ++idev)
hoomd/md/TwoStepRATTLEBDGPU.h:            cudaMemPrefetchAsync(this->m_gamma.get(),
hoomd/md/TwoStepRATTLEBDGPU.h:                                 gpu_map[idev]);
hoomd/md/TwoStepRATTLEBDGPU.h:            cudaMemPrefetchAsync(this->m_gamma_r.get(),
hoomd/md/TwoStepRATTLEBDGPU.h:                                 gpu_map[idev]);
hoomd/md/TwoStepRATTLEBDGPU.h:        if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/TwoStepRATTLEBDGPU.h:            CHECK_CUDA_ERROR();
hoomd/md/TwoStepRATTLEBDGPU.h:    this->m_exec_conf->beginMultiGPU();
hoomd/md/TwoStepRATTLEBDGPU.h:    // perform the update on the GPU
hoomd/md/TwoStepRATTLEBDGPU.h:    kernel::gpu_rattle_brownian_step_one(d_pos.data,
hoomd/md/TwoStepRATTLEBDGPU.h:                                         this->m_group->getGPUPartition());
hoomd/md/TwoStepRATTLEBDGPU.h:    if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/TwoStepRATTLEBDGPU.h:        CHECK_CUDA_ERROR();
hoomd/md/TwoStepRATTLEBDGPU.h:    this->m_exec_conf->endMultiGPU();
hoomd/md/TwoStepRATTLEBDGPU.h:template<class Manifold> void TwoStepRATTLEBDGPU<Manifold>::includeRATTLEForce(uint64_t timestep)
hoomd/md/TwoStepRATTLEBDGPU.h:        auto& gpu_map = this->m_exec_conf->getGPUIds();
hoomd/md/TwoStepRATTLEBDGPU.h:        for (unsigned int idev = 0; idev < this->m_exec_conf->getNumActiveGPUs(); ++idev)
hoomd/md/TwoStepRATTLEBDGPU.h:            cudaMemPrefetchAsync(this->m_gamma.get(),
hoomd/md/TwoStepRATTLEBDGPU.h:                                 gpu_map[idev]);
hoomd/md/TwoStepRATTLEBDGPU.h:        if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/TwoStepRATTLEBDGPU.h:            CHECK_CUDA_ERROR();
hoomd/md/TwoStepRATTLEBDGPU.h:    this->m_exec_conf->beginMultiGPU();
hoomd/md/TwoStepRATTLEBDGPU.h:    // perform the update on the GPU
hoomd/md/TwoStepRATTLEBDGPU.h:    kernel::gpu_include_rattle_force_bd<Manifold>(d_pos.data,
hoomd/md/TwoStepRATTLEBDGPU.h:                                                  this->m_group->getGPUPartition());
hoomd/md/TwoStepRATTLEBDGPU.h:    if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/TwoStepRATTLEBDGPU.h:        CHECK_CUDA_ERROR();
hoomd/md/TwoStepRATTLEBDGPU.h:    this->m_exec_conf->endMultiGPU();
hoomd/md/TwoStepRATTLEBDGPU.h://! Exports the TwoStepRATTLEBDGPU class to python
hoomd/md/TwoStepRATTLEBDGPU.h:void export_TwoStepRATTLEBDGPU(pybind11::module& m, const std::string& name)
hoomd/md/TwoStepRATTLEBDGPU.h:    pybind11::class_<TwoStepRATTLEBDGPU<Manifold>,
hoomd/md/TwoStepRATTLEBDGPU.h:                     std::shared_ptr<TwoStepRATTLEBDGPU<Manifold>>>(m, name.c_str())
hoomd/md/NeighborListGPUBinned.h:#include "NeighborListGPU.h"
hoomd/md/NeighborListGPUBinned.h:#include "hoomd/CellListGPU.h"
hoomd/md/NeighborListGPUBinned.h:/*! \file NeighborListGPUBinned.h
hoomd/md/NeighborListGPUBinned.h:    \brief Declares the NeighborListGPUBinned class
hoomd/md/NeighborListGPUBinned.h:#ifndef __NEIGHBORLISTGPUBINNED_H__
hoomd/md/NeighborListGPUBinned.h:#define __NEIGHBORLISTGPUBINNED_H__
hoomd/md/NeighborListGPUBinned.h://! Neighbor list build on the GPU
hoomd/md/NeighborListGPUBinned.h:/*! Implements the O(N) neighbor list build on the GPU using a cell list.
hoomd/md/NeighborListGPUBinned.h:    GPU kernel methods are defined in NeighborListGPUBinned.cuh and defined in
hoomd/md/NeighborListGPUBinned.h:   NeighborListGPUBinned.cu.
hoomd/md/NeighborListGPUBinned.h:class PYBIND11_EXPORT NeighborListGPUBinned : public NeighborListGPU
hoomd/md/NeighborListGPUBinned.h:    NeighborListGPUBinned(std::shared_ptr<SystemDefinition> sysdef, Scalar r_buff);
hoomd/md/NeighborListGPUBinned.h:    virtual ~NeighborListGPUBinned();
hoomd/md/NeighborListGPUBinned.h:        NeighborListGPU::notifyRCutMatrixChange();
hoomd/md/NeighborListGPUBinned.h:        NeighborListGPU::startAutotuning();
hoomd/md/NeighborListGPUBinned.h:        bool result = NeighborListGPU::isAutotuningComplete();
hoomd/md/EvaluatorPairLJ.h:    A pair potential evaluator class is also used on the GPU. So all of its members must be declared
hoomd/md/EvaluatorPairLJ.h:        //! Set CUDA memory hints
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.h:#include "AreaConservationMeshForceComputeGPU.cuh"
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.h:#include "TriangleAreaConservationMeshForceComputeGPU.cuh"
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.h:/*! \file TriangleAreaConservationMeshForceComputeGPU.h
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.h:    \brief Declares a class for computing the triangle area conservation energy forces on the GPU
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.h:#ifndef __TRIANGLEAREACONSERVATIONMESHFORCECOMPUTE_GPU_H__
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.h:#define __TRIANGLEAREACONSERVATIONMESHFORCECOMPUTE_GPU_H__
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.h://! Computes triangle area conservation energy forces on the mesh on the GPU
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.h:class PYBIND11_EXPORT TriangleAreaConservationMeshForceComputeGPU
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.h:    TriangleAreaConservationMeshForceComputeGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.h:    GPUArray<unsigned int> m_flags;        //!< Flags set during the kernel execution
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.h:    GPUArray<Scalar> m_partial_sum; //!< memory space for partial sum over area
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.h:    GPUArray<Scalar> m_sum;         //!< memory space for sum over area
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.h://! Exports the TriangleAreaConservationMeshForceComputeGPU class to python
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.h:void export_TriangleAreaConservationMeshForceComputeGPU(pybind11::module& m);
hoomd/md/MolecularForceCompute.cu:#define CHECK_CUDA()                                                                       \
hoomd/md/MolecularForceCompute.cu:            throw std::runtime_error("CUDA error in MolecularForceCompute "                \
hoomd/md/MolecularForceCompute.cu:            throw std::runtime_error("CUDA error " + std::string(hipGetErrorString(err))); \
hoomd/md/MolecularForceCompute.cu:    \brief Contains GPU kernel code used by MolecularForceCompute
hoomd/md/MolecularForceCompute.cu:hipError_t gpu_sort_by_molecule(unsigned int nptl,
hoomd/md/MolecularForceCompute.cu:                                bool check_cuda)
hoomd/md/MolecularForceCompute.cu:    thrust::copy(thrust::cuda::par(alloc),
hoomd/md/MolecularForceCompute.cu:    if (check_cuda)
hoomd/md/MolecularForceCompute.cu:        CHECK_CUDA();
hoomd/md/MolecularForceCompute.cu:    if (check_cuda)
hoomd/md/MolecularForceCompute.cu:        CHECK_CUDA();
hoomd/md/MolecularForceCompute.cu:    if (check_cuda)
hoomd/md/MolecularForceCompute.cu:        CHECK_CUDA();
hoomd/md/MolecularForceCompute.cu:    if (check_cuda)
hoomd/md/MolecularForceCompute.cu:        CHECK_CUDA();
hoomd/md/MolecularForceCompute.cu:    if (check_cuda)
hoomd/md/MolecularForceCompute.cu:        CHECK_CUDA();
hoomd/md/MolecularForceCompute.cu:    if (check_cuda)
hoomd/md/MolecularForceCompute.cu:        CHECK_CUDA();
hoomd/md/MolecularForceCompute.cu:    if (check_cuda)
hoomd/md/MolecularForceCompute.cu:        CHECK_CUDA();
hoomd/md/MolecularForceCompute.cu:    if (check_cuda)
hoomd/md/MolecularForceCompute.cu:        CHECK_CUDA();
hoomd/md/MolecularForceCompute.cu:    thrust::copy(thrust::cuda::par(alloc),
hoomd/md/MolecularForceCompute.cu:    if (check_cuda)
hoomd/md/MolecularForceCompute.cu:        CHECK_CUDA();
hoomd/md/MolecularForceCompute.cu:    if (check_cuda)
hoomd/md/MolecularForceCompute.cu:        CHECK_CUDA();
hoomd/md/MolecularForceCompute.cu:__global__ void gpu_fill_molecule_table_kernel(unsigned int nptl,
hoomd/md/MolecularForceCompute.cu:hipError_t gpu_fill_molecule_table(unsigned int nptl,
hoomd/md/MolecularForceCompute.cu:    thrust::exclusive_scan_by_key(thrust::cuda::par(alloc),
hoomd/md/MolecularForceCompute.cu:    hipLaunchKernelGGL((gpu_fill_molecule_table_kernel),
hoomd/md/NeighborListGPUStencil.cu:#include "NeighborListGPUStencil.cuh"
hoomd/md/NeighborListGPUStencil.cu:/*! \file NeighborListGPUStencil.cu
hoomd/md/NeighborListGPUStencil.cu:    \brief Defines GPU kernel code for O(N) neighbor list generation on the GPU with multiple bin
hoomd/md/NeighborListGPUStencil.cu://! Kernel call for generating neighbor list on the GPU using multiple stencils (Kepler optimized
hoomd/md/NeighborListGPUStencil.cu:__global__ void gpu_compute_nlist_stencil_kernel(unsigned int* d_nlist,
hoomd/md/NeighborListGPUStencil.cu:                = get_max_block_size_stencil(gpu_compute_nlist_stencil_kernel<0, cur_tpp>);
hoomd/md/NeighborListGPUStencil.cu:            hipLaunchKernelGGL((gpu_compute_nlist_stencil_kernel<0, cur_tpp>),
hoomd/md/NeighborListGPUStencil.cu:                = get_max_block_size_stencil(gpu_compute_nlist_stencil_kernel<1, cur_tpp>);
hoomd/md/NeighborListGPUStencil.cu:            hipLaunchKernelGGL((gpu_compute_nlist_stencil_kernel<1, cur_tpp>),
hoomd/md/NeighborListGPUStencil.cu:hipError_t gpu_compute_nlist_stencil(unsigned int* d_nlist,
hoomd/md/NeighborListGPUStencil.cu: * later sorted in gpu_compute_nlist_stencil_sort_types().
hoomd/md/NeighborListGPUStencil.cu:__global__ void gpu_compute_nlist_stencil_fill_types_kernel(unsigned int* d_pids,
hoomd/md/NeighborListGPUStencil.cu:hipError_t gpu_compute_nlist_stencil_fill_types(unsigned int* d_pids,
hoomd/md/NeighborListGPUStencil.cu:    hipLaunchKernelGGL((gpu_compute_nlist_stencil_fill_types_kernel),
hoomd/md/NeighborListGPUStencil.cu:void gpu_compute_nlist_stencil_sort_types(unsigned int* d_pids,
hoomd/md/export_AnisoPotentialPairPatchyGPU.cc.inc:#include "hoomd/md/AnisoPotentialPairGPU.h"
hoomd/md/export_AnisoPotentialPairPatchyGPU.cc.inc:#define EXPORT_FUNCTION export_AnisoPotentialPairPatchy@_evaluator@GPU
hoomd/md/export_AnisoPotentialPairPatchyGPU.cc.inc:    export_AnisoPotentialPairGPU<PairModulator<EVALUATOR_CLASS, PatchEnvelope>>(
hoomd/md/export_AnisoPotentialPairPatchyGPU.cc.inc:        "AnisoPotentialPairPatchy@_evaluator@GPU");
hoomd/md/MuellerPlatheFlowGPU.cc:#include "MuellerPlatheFlowGPU.h"
hoomd/md/MuellerPlatheFlowGPU.cc://! \file MuellerPlatheFlowGPU.cc Implementation of GPU version of MuellerPlatheFlow.
hoomd/md/MuellerPlatheFlowGPU.cc:#include "MuellerPlatheFlowGPU.cuh"
hoomd/md/MuellerPlatheFlowGPU.cc:MuellerPlatheFlowGPU::MuellerPlatheFlowGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/md/MuellerPlatheFlowGPU.cc:    m_exec_conf->msg->notice(5) << "Constructing MuellerPlatheFlowGPU " << endl;
hoomd/md/MuellerPlatheFlowGPU.cc:    if (!m_exec_conf->isCUDAEnabled())
hoomd/md/MuellerPlatheFlowGPU.cc:            << "Creating a MuellerPlatheGPU with no GPU in the execution configuration" << endl;
hoomd/md/MuellerPlatheFlowGPU.cc:        throw std::runtime_error("Error initializing MuellerPlatheFlowGPU");
hoomd/md/MuellerPlatheFlowGPU.cc:MuellerPlatheFlowGPU::~MuellerPlatheFlowGPU(void)
hoomd/md/MuellerPlatheFlowGPU.cc:    m_exec_conf->msg->notice(5) << "Destroying MuellerPlatheFlowGPU " << endl;
hoomd/md/MuellerPlatheFlowGPU.cc:void MuellerPlatheFlowGPU::searchMinMaxVelocity(void)
hoomd/md/MuellerPlatheFlowGPU.cc:    kernel::gpu_search_min_max_velocity(group_size,
hoomd/md/MuellerPlatheFlowGPU.cc:    if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/MuellerPlatheFlowGPU.cc:        CHECK_CUDA_ERROR();
hoomd/md/MuellerPlatheFlowGPU.cc:void MuellerPlatheFlowGPU::updateMinMaxVelocity(void)
hoomd/md/MuellerPlatheFlowGPU.cc:    kernel::gpu_update_min_max_velocity(d_rtag.data,
hoomd/md/MuellerPlatheFlowGPU.cc:    if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/MuellerPlatheFlowGPU.cc:        CHECK_CUDA_ERROR();
hoomd/md/MuellerPlatheFlowGPU.cc:void export_MuellerPlatheFlowGPU(pybind11::module& m)
hoomd/md/MuellerPlatheFlowGPU.cc:    pybind11::class_<MuellerPlatheFlowGPU,
hoomd/md/MuellerPlatheFlowGPU.cc:                     std::shared_ptr<MuellerPlatheFlowGPU>>(m, "MuellerPlatheFlowGPU")
hoomd/md/EvaluatorPairALJ.h:        //! Attach managed memory to CUDA stream
hoomd/md/EvaluatorPairALJ.h:           arrays for migration to the GPU as needed.
hoomd/md/EvaluatorPairALJ.h:        //! Attach managed memory to CUDA stream
hoomd/md/PairModulator.h:        //! Attach managed memory to CUDA stream
hoomd/md/TwoStepBDGPU.cu:#include "TwoStepBDGPU.cuh"
hoomd/md/TwoStepBDGPU.cu:/*! \file TwoSteBDGPU.cu
hoomd/md/TwoStepBDGPU.cu:    \brief Defines GPU kernel code for Brownian integration on the GPU. Used by TwoStepBDGPU.
hoomd/md/TwoStepBDGPU.cu:    \param nwork Number of group members to process on this GPU
hoomd/md/TwoStepBDGPU.cu:    \param offset Offset of this GPU into group indices
hoomd/md/TwoStepBDGPU.cu:    This kernel is implemented in a very similar manner to gpu_nve_step_one_kernel(), see it for
hoomd/md/TwoStepBDGPU.cu:__global__ void gpu_brownian_step_one_kernel(Scalar4* d_pos,
hoomd/md/TwoStepBDGPU.cu:   online documentation) \param langevin_args Collected arguments for gpu_brownian_step_one_kernel()
hoomd/md/TwoStepBDGPU.cu:    This is just a driver for gpu_brownian_step_one_kernel(), see it for details.
hoomd/md/TwoStepBDGPU.cu:hipError_t gpu_brownian_step_one(Scalar4* d_pos,
hoomd/md/TwoStepBDGPU.cu:                                 const GPUPartition& gpu_partition)
hoomd/md/TwoStepBDGPU.cu:    // iterate over active GPUs in reverse, to end up on first GPU when returning from this function
hoomd/md/TwoStepBDGPU.cu:    for (int idev = gpu_partition.getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/md/TwoStepBDGPU.cu:        auto range = gpu_partition.getRangeAndSetGPU(idev);
hoomd/md/TwoStepBDGPU.cu:        hipLaunchKernelGGL((gpu_brownian_step_one_kernel),
hoomd/md/TableDihedralForceGPU.cuh:/*! \file TableDihedralForceGPU.cuh
hoomd/md/TableDihedralForceGPU.cuh:    \brief Declares GPU kernel code for calculating the table bond forces. Used by
hoomd/md/TableDihedralForceGPU.cuh:   TableDihedralForceGPU.
hoomd/md/TableDihedralForceGPU.cuh:#ifndef __TABLEDIHEDRALFORCECOMPUTEGPU_CUH__
hoomd/md/TableDihedralForceGPU.cuh:#define __TABLEDIHEDRALFORCECOMPUTEGPU_CUH__
hoomd/md/TableDihedralForceGPU.cuh://! Kernel driver that computes table forces on the GPU for TableDihedralForceGPU
hoomd/md/TableDihedralForceGPU.cuh:hipError_t gpu_compute_table_dihedral_forces(Scalar4* d_force,
hoomd/md/many_body.py:        between MPI domains on the GPU. Since reverse force communication is
hoomd/md/many_body.py:        this potential on the GPU with MPI will result in an error.
hoomd/md/many_body.py:            cls = getattr(_md, self._cpp_class_name + "GPU")
hoomd/md/TwoStepConstantPressureGPU.cuh:#ifndef __TWOSTEP_NPT_MTK_GPU_CUH__
hoomd/md/TwoStepConstantPressureGPU.cuh:#define __TWOSTEP_NPT_MTK_GPU_CUH__
hoomd/md/TwoStepConstantPressureGPU.cuh:#include "hoomd/GPUPartition.cuh"
hoomd/md/TwoStepConstantPressureGPU.cuh:/*! \file TwoStepNPTMTKGPU.cuh
hoomd/md/TwoStepConstantPressureGPU.cuh:    \brief Declares GPU kernel code for NPT integration on the GPU using the Martyna-Tobias-Klein
hoomd/md/TwoStepConstantPressureGPU.cuh:   (MTK) equations. Used by TwoStepNPTMTKGPU.
hoomd/md/TwoStepConstantPressureGPU.cuh:hipError_t gpu_npt_rescale_step_one(Scalar4* d_pos,
hoomd/md/TwoStepConstantPressureGPU.cuh:                                    const GPUPartition& gpu_partition,
hoomd/md/TwoStepConstantPressureGPU.cuh:hipError_t gpu_npt_rescale_wrap(const GPUPartition& gpu_partition,
hoomd/md/TwoStepConstantPressureGPU.cuh://! Kernel driver for the the second step of the computation called by NPTUpdaterGPU
hoomd/md/TwoStepConstantPressureGPU.cuh:hipError_t gpu_npt_rescale_step_two(Scalar4* d_vel,
hoomd/md/TwoStepConstantPressureGPU.cuh:                                    const GPUPartition& gpu_partition,
hoomd/md/TwoStepConstantPressureGPU.cuh:void gpu_npt_rescale_rescale(const GPUPartition& gpu_partition,
hoomd/md/TwoStepConstantPressureGPU.cc:#include "TwoStepConstantPressureGPU.cuh"
hoomd/md/TwoStepConstantPressureGPU.cc:#include "TwoStepConstantPressureGPU.h"
hoomd/md/TwoStepConstantPressureGPU.cc:#include "TwoStepNVEGPU.cuh"
hoomd/md/TwoStepConstantPressureGPU.cc:/// Implement TwoStepConstantPressure on the GPU.
hoomd/md/TwoStepConstantPressureGPU.cc:TwoStepConstantPressureGPU::TwoStepConstantPressureGPU(
hoomd/md/TwoStepConstantPressureGPU.cc:void TwoStepConstantPressureGPU::integrateStepOne(uint64_t timestep)
hoomd/md/TwoStepConstantPressureGPU.cc:        // perform the update on the GPU
hoomd/md/TwoStepConstantPressureGPU.cc:        m_exec_conf->beginMultiGPU();
hoomd/md/TwoStepConstantPressureGPU.cc:        // perform the particle update on the GPU
hoomd/md/TwoStepConstantPressureGPU.cc:        kernel::gpu_npt_rescale_rescale(m_pdata->getGPUPartition(),
hoomd/md/TwoStepConstantPressureGPU.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/TwoStepConstantPressureGPU.cc:            CHECK_CUDA_ERROR();
hoomd/md/TwoStepConstantPressureGPU.cc:        m_exec_conf->endMultiGPU();
hoomd/md/TwoStepConstantPressureGPU.cc:        // perform the particle update on the GPU
hoomd/md/TwoStepConstantPressureGPU.cc:        m_exec_conf->beginMultiGPU();
hoomd/md/TwoStepConstantPressureGPU.cc:        kernel::gpu_npt_rescale_step_one(d_pos.data,
hoomd/md/TwoStepConstantPressureGPU.cc:                                         m_group->getGPUPartition(),
hoomd/md/TwoStepConstantPressureGPU.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/TwoStepConstantPressureGPU.cc:            CHECK_CUDA_ERROR();
hoomd/md/TwoStepConstantPressureGPU.cc:        m_exec_conf->endMultiGPU();
hoomd/md/TwoStepConstantPressureGPU.cc:        } // end of GPUArray scope
hoomd/md/TwoStepConstantPressureGPU.cc:        m_exec_conf->beginMultiGPU();
hoomd/md/TwoStepConstantPressureGPU.cc:        kernel::gpu_npt_rescale_wrap(m_pdata->getGPUPartition(),
hoomd/md/TwoStepConstantPressureGPU.cc:        m_exec_conf->endMultiGPU();
hoomd/md/TwoStepConstantPressureGPU.cc:        m_exec_conf->beginMultiGPU();
hoomd/md/TwoStepConstantPressureGPU.cc:        kernel::gpu_nve_angular_step_one(d_orientation.data,
hoomd/md/TwoStepConstantPressureGPU.cc:                                         m_group->getGPUPartition(),
hoomd/md/TwoStepConstantPressureGPU.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/TwoStepConstantPressureGPU.cc:            CHECK_CUDA_ERROR();
hoomd/md/TwoStepConstantPressureGPU.cc:        m_exec_conf->endMultiGPU();
hoomd/md/TwoStepConstantPressureGPU.cc:void TwoStepConstantPressureGPU::integrateStepTwo(uint64_t timestep)
hoomd/md/TwoStepConstantPressureGPU.cc:        m_exec_conf->beginMultiGPU();
hoomd/md/TwoStepConstantPressureGPU.cc:        kernel::gpu_npt_rescale_step_two(d_vel.data,
hoomd/md/TwoStepConstantPressureGPU.cc:                                         m_group->getGPUPartition(),
hoomd/md/TwoStepConstantPressureGPU.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/TwoStepConstantPressureGPU.cc:            CHECK_CUDA_ERROR();
hoomd/md/TwoStepConstantPressureGPU.cc:        m_exec_conf->endMultiGPU();
hoomd/md/TwoStepConstantPressureGPU.cc:        } // end GPUArray scope
hoomd/md/TwoStepConstantPressureGPU.cc:        m_exec_conf->beginMultiGPU();
hoomd/md/TwoStepConstantPressureGPU.cc:        kernel::gpu_nve_angular_step_two(d_orientation.data,
hoomd/md/TwoStepConstantPressureGPU.cc:                                         m_group->getGPUPartition(),
hoomd/md/TwoStepConstantPressureGPU.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/TwoStepConstantPressureGPU.cc:            CHECK_CUDA_ERROR();
hoomd/md/TwoStepConstantPressureGPU.cc:        m_exec_conf->endMultiGPU();
hoomd/md/TwoStepConstantPressureGPU.cc:void export_TwoStepConstantPressureGPU(pybind11::module& m)
hoomd/md/TwoStepConstantPressureGPU.cc:    pybind11::class_<TwoStepConstantPressureGPU,
hoomd/md/TwoStepConstantPressureGPU.cc:                     std::shared_ptr<TwoStepConstantPressureGPU>>(m, "TwoStepConstantPressureGPU")
hoomd/md/force.py:    def gpu_local_force_arrays(self):
hoomd/md/force.py:        """hoomd.md.data.ForceLocalAccessGPU: Expose force arrays on the GPU.
hoomd/md/force.py:        virial data of the particles in the system on the gpu through a context
hoomd/md/force.py:        The `hoomd.md.data.ForceLocalAccessGPU` object returned by this property
hoomd/md/force.py:            with self.gpu_local_force_arrays as arrays:
hoomd/md/force.py:            GPU local force data is not available if the chosen device for the
hoomd/md/force.py:        if not isinstance(self._simulation.device, hoomd.device.GPU):
hoomd/md/force.py:                "Cannot access gpu_local_force_arrays without a GPU device")
hoomd/md/force.py:                "Cannot enter gpu_local_force_arrays context manager inside "
hoomd/md/force.py:            raise hoomd.error.DataAccessError("gpu_local_force_arrays")
hoomd/md/force.py:        return hoomd.md.data.ForceLocalAccessGPU(self, self._simulation.state)
hoomd/md/force.py:    `gpu_local_force_arrays` property. Choose the property that corresponds to
hoomd/md/force.py:    ``_state.cpu_local_snapshot`` or the ``_state.gpu_local_snapshot`` property.
hoomd/md/force.py:            my_class = _md.ActiveForceComputeGPU
hoomd/md/force.py:        if isinstance(sim.device, hoomd.device.GPU):
hoomd/md/force.py:            base_class_str += "GPU"
hoomd/md/force.py:            my_class = _md.ConstantForceComputeGPU
hoomd/md/EvaluatorPairMie.h:        // set CUDA memory hints
hoomd/md/PotentialSpecialPairGPUKernel.cu.inc:#include "hoomd/md/PotentialBondGPU.cuh"
hoomd/md/PotentialSpecialPairGPUKernel.cu.inc:gpu_compute_bond_forces<EVALUATOR_CLASS, 2>(const kernel::bond_args_t<2>& bond_args,
hoomd/md/PotentialMeshBondGPUKernel.cu.inc:#include "hoomd/md/PotentialBondGPU.cuh"
hoomd/md/PotentialMeshBondGPUKernel.cu.inc:gpu_compute_bond_forces<EVALUATOR_CLASS, 4>(const kernel::bond_args_t<4>& bond_args,
hoomd/md/HarmonicDihedralForceComputeGPU.cc:/*! \file HarmonicDihedralForceComputeGPU.cc
hoomd/md/HarmonicDihedralForceComputeGPU.cc:    \brief Defines HarmonicDihedralForceComputeGPU
hoomd/md/HarmonicDihedralForceComputeGPU.cc:#include "HarmonicDihedralForceComputeGPU.h"
hoomd/md/HarmonicDihedralForceComputeGPU.cc:HarmonicDihedralForceComputeGPU::HarmonicDihedralForceComputeGPU(
hoomd/md/HarmonicDihedralForceComputeGPU.cc:    // can't run on the GPU if there aren't any GPUs in the execution configuration
hoomd/md/HarmonicDihedralForceComputeGPU.cc:    if (!m_exec_conf->isCUDAEnabled())
hoomd/md/HarmonicDihedralForceComputeGPU.cc:            << "Creating a DihedralForceComputeGPU with no GPU in the execution configuration"
hoomd/md/HarmonicDihedralForceComputeGPU.cc:        throw std::runtime_error("Error initializing DihedralForceComputeGPU");
hoomd/md/HarmonicDihedralForceComputeGPU.cc:    GPUArray<Scalar4> params(m_dihedral_data->getNTypes(), m_exec_conf);
hoomd/md/HarmonicDihedralForceComputeGPU.cc:HarmonicDihedralForceComputeGPU::~HarmonicDihedralForceComputeGPU() { }
hoomd/md/HarmonicDihedralForceComputeGPU.cc:    parameters on the GPU.
hoomd/md/HarmonicDihedralForceComputeGPU.cc:void HarmonicDihedralForceComputeGPU::setParams(unsigned int type,
hoomd/md/HarmonicDihedralForceComputeGPU.cc:/*! Internal method for computing the forces on the GPU.
hoomd/md/HarmonicDihedralForceComputeGPU.cc:    \post The force data on the GPU is written with the calculated forces
hoomd/md/HarmonicDihedralForceComputeGPU.cc:    Calls gpu_compute_harmonic_dihedral_forces to do the dirty work.
hoomd/md/HarmonicDihedralForceComputeGPU.cc:void HarmonicDihedralForceComputeGPU::computeForces(uint64_t timestep)
hoomd/md/HarmonicDihedralForceComputeGPU.cc:    ArrayHandle<DihedralData::members_t> d_gpu_dihedral_list(m_dihedral_data->getGPUTable(),
hoomd/md/HarmonicDihedralForceComputeGPU.cc:    ArrayHandle<unsigned int> d_dihedrals_ABCD(m_dihedral_data->getGPUPosTable(),
hoomd/md/HarmonicDihedralForceComputeGPU.cc:    // run the kernel in parallel on all GPUs
hoomd/md/HarmonicDihedralForceComputeGPU.cc:    kernel::gpu_compute_harmonic_dihedral_forces(d_force.data,
hoomd/md/HarmonicDihedralForceComputeGPU.cc:                                                 d_gpu_dihedral_list.data,
hoomd/md/HarmonicDihedralForceComputeGPU.cc:                                                 m_dihedral_data->getGPUTableIndexer().getW(),
hoomd/md/HarmonicDihedralForceComputeGPU.cc:    if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/HarmonicDihedralForceComputeGPU.cc:        CHECK_CUDA_ERROR();
hoomd/md/HarmonicDihedralForceComputeGPU.cc:void export_HarmonicDihedralForceComputeGPU(pybind11::module& m)
hoomd/md/HarmonicDihedralForceComputeGPU.cc:    pybind11::class_<HarmonicDihedralForceComputeGPU,
hoomd/md/HarmonicDihedralForceComputeGPU.cc:                     std::shared_ptr<HarmonicDihedralForceComputeGPU>>(
hoomd/md/HarmonicDihedralForceComputeGPU.cc:        "HarmonicDihedralForceComputeGPU")
hoomd/md/EvaluatorPairDLVO.h:        //! Set CUDA memory hints
hoomd/md/EvaluatorWalls.cc:#error This file cannot be compiled on the GPU.
hoomd/md/ConstantForceComputeGPU.cuh:/*! \file ConstantForceComputeGPU.cuh
hoomd/md/ConstantForceComputeGPU.cuh:    \brief Declares GPU kernel code for calculating constant forces forces on the GPU. Used by
hoomd/md/ConstantForceComputeGPU.cuh:   ConstantForceComputeGPU.
hoomd/md/ConstantForceComputeGPU.cuh:#ifndef __CONSTANT_FORCE_COMPUTE_GPU_CUH__
hoomd/md/ConstantForceComputeGPU.cuh:#define __CONSTANT_FORCE_COMPUTE_GPU_CUH__
hoomd/md/ConstantForceComputeGPU.cuh:hipError_t gpu_compute_constant_force_set_forces(const unsigned int group_size,
hoomd/md/PotentialPairDPDThermoGPU.h:#ifndef __POTENTIAL_PAIR_DPDTHERMO_GPU_H__
hoomd/md/PotentialPairDPDThermoGPU.h:#define __POTENTIAL_PAIR_DPDTHERMO_GPU_H__
hoomd/md/PotentialPairDPDThermoGPU.h:#include "PotentialPairDPDThermoGPU.cuh"
hoomd/md/PotentialPairDPDThermoGPU.h:/*! \file PotentialPairDPDThermoGPU.h
hoomd/md/PotentialPairDPDThermoGPU.h:    \brief Defines the template class for standard pair potentials on the GPU
hoomd/md/PotentialPairDPDThermoGPU.h://! Template class for computing pair potentials on the GPU
hoomd/md/PotentialPairDPDThermoGPU.h:    Due to technical limitations, the instantiation of PotentialPairDPDThermoGPU cannot create a
hoomd/md/PotentialPairDPDThermoGPU.h:   CUDA kernel automatically with the \a evaluator. Instead, a .cu file must be written that
hoomd/md/PotentialPairDPDThermoGPU.h:   provides a driver function to call gpu_compute_dpd_forces() instantiated with the same evaluator.
hoomd/md/PotentialPairDPDThermoGPU.h:   (See PotentialPairDPDThermoGPU.cuh for an example). That function is then passed into this class
hoomd/md/PotentialPairDPDThermoGPU.h:   as another template parameter \a gpu_cpdf
hoomd/md/PotentialPairDPDThermoGPU.h:    \tparam gpu_cpdf Driver function that calls gpu_compute_dpd_forces<evaluator>()
hoomd/md/PotentialPairDPDThermoGPU.h:    \sa export_PotentialPairDPDThermoGPU()
hoomd/md/PotentialPairDPDThermoGPU.h:template<class evaluator> class PotentialPairDPDThermoGPU : public PotentialPairDPDThermo<evaluator>
hoomd/md/PotentialPairDPDThermoGPU.h:    PotentialPairDPDThermoGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/md/PotentialPairDPDThermoGPU.h:    virtual ~PotentialPairDPDThermoGPU() { };
hoomd/md/PotentialPairDPDThermoGPU.h:PotentialPairDPDThermoGPU<evaluator>::PotentialPairDPDThermoGPU(
hoomd/md/PotentialPairDPDThermoGPU.h:    // can't run on the GPU if there aren't any GPUs in the execution configuration
hoomd/md/PotentialPairDPDThermoGPU.h:    if (!this->m_exec_conf->isCUDAEnabled())
hoomd/md/PotentialPairDPDThermoGPU.h:            << "Creating a PotentialPairDPDThermoGPU with no GPU in the execution configuration"
hoomd/md/PotentialPairDPDThermoGPU.h:        throw std::runtime_error("Error initializing PotentialPairDPDThermoGPU");
hoomd/md/PotentialPairDPDThermoGPU.h:void PotentialPairDPDThermoGPU<evaluator>::computeForces(uint64_t timestep)
hoomd/md/PotentialPairDPDThermoGPU.h:    // The GPU implementation CANNOT handle a half neighborlist, error out now
hoomd/md/PotentialPairDPDThermoGPU.h:            << "PotentialPairDPDThermoGPU cannot handle a half neighborlist" << std::endl
hoomd/md/PotentialPairDPDThermoGPU.h:        throw std::runtime_error("Error computing forces in PotentialPairDPDThermoGPU");
hoomd/md/PotentialPairDPDThermoGPU.h:    kernel::gpu_compute_dpd_forces<evaluator>(
hoomd/md/PotentialPairDPDThermoGPU.h:    if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/PotentialPairDPDThermoGPU.h:        CHECK_CUDA_ERROR();
hoomd/md/PotentialPairDPDThermoGPU.h:void export_PotentialPairDPDThermoGPU(pybind11::module& m, const std::string& name)
hoomd/md/PotentialPairDPDThermoGPU.h:    pybind11::class_<PotentialPairDPDThermoGPU<T>,
hoomd/md/PotentialPairDPDThermoGPU.h:                     std::shared_ptr<PotentialPairDPDThermoGPU<T>>>(m, name.c_str())
hoomd/md/PotentialPairDPDThermoGPU.h:#endif // __POTENTIAL_PAIR_DPDTHERMO_GPU_H__
hoomd/md/export_TwoStepRATTLEGPU.cc.inc:#include "hoomd/md/TwoStepRATTLE@_method@GPU.h"
hoomd/md/export_TwoStepRATTLEGPU.cc.inc:#define PYBIND_EXPORT export_TwoStepRATTLE@_method@GPU
hoomd/md/export_TwoStepRATTLEGPU.cc.inc:#define EXPORT_FUNCTION export_TwoStepRATTLE@_method@GPU@_manifold@
hoomd/md/export_TwoStepRATTLEGPU.cc.inc:    PYBIND_EXPORT<MANIFOLD_CLASS>(m, "TwoStepRATTLE@_method@@_manifold@GPU");
hoomd/md/MolecularForceCompute.cuh:    \brief Contains GPU kernel code used by MolecularForceCompute
hoomd/md/MolecularForceCompute.cuh:gpu_sort_by_molecule(unsigned int nptl,
hoomd/md/MolecularForceCompute.cuh:                     bool check_cuda);
hoomd/md/MolecularForceCompute.cuh:gpu_fill_molecule_table(unsigned int nptl,
hoomd/md/ForceCompositeGPU.cu:#include "ForceCompositeGPU.cuh"
hoomd/md/ForceCompositeGPU.cu:    \brief Defines GPU kernel code for the composite particle integration on the GPU.
hoomd/md/ForceCompositeGPU.cu:   n_bodies_per_block bodies are handled within each block of execution on the GPU. The reason for
hoomd/md/ForceCompositeGPU.cu:   this is to decrease over-parallelism and use the GPU cores more effectively when bodies are
hoomd/md/ForceCompositeGPU.cu:__global__ void gpu_rigid_force_sliding_kernel(Scalar4* d_force,
hoomd/md/ForceCompositeGPU.cu:__global__ void gpu_rigid_virial_sliding_kernel(Scalar* d_virial,
hoomd/md/ForceCompositeGPU.cu:hipError_t gpu_rigid_force(Scalar4* d_force,
hoomd/md/ForceCompositeGPU.cu:                           const GPUPartition& gpu_partition)
hoomd/md/ForceCompositeGPU.cu:    for (int idev = gpu_partition.getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/md/ForceCompositeGPU.cu:        auto range = gpu_partition.getRangeAndSetGPU(idev);
hoomd/md/ForceCompositeGPU.cu:        hipFuncGetAttributes(&attr, (const void*)gpu_rigid_force_sliding_kernel);
hoomd/md/ForceCompositeGPU.cu:        hipLaunchKernelGGL((gpu_rigid_force_sliding_kernel),
hoomd/md/ForceCompositeGPU.cu:hipError_t gpu_rigid_virial(Scalar* d_virial,
hoomd/md/ForceCompositeGPU.cu:                            const GPUPartition& gpu_partition)
hoomd/md/ForceCompositeGPU.cu:    for (int idev = gpu_partition.getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/md/ForceCompositeGPU.cu:        auto range = gpu_partition.getRangeAndSetGPU(idev);
hoomd/md/ForceCompositeGPU.cu:        hipFuncGetAttributes(&attr, (const void*)gpu_rigid_virial_sliding_kernel);
hoomd/md/ForceCompositeGPU.cu:        hipLaunchKernelGGL((gpu_rigid_virial_sliding_kernel),
hoomd/md/ForceCompositeGPU.cu:__global__ void gpu_update_composite_kernel(unsigned int N,
hoomd/md/ForceCompositeGPU.cu:void gpu_update_composite(unsigned int N,
hoomd/md/ForceCompositeGPU.cu:                          const GPUPartition& gpu_partition)
hoomd/md/ForceCompositeGPU.cu:    hipFuncGetAttributes(&attr, (const void*)gpu_update_composite_kernel);
hoomd/md/ForceCompositeGPU.cu:    // iterate over active GPUs in reverse, to end up on first GPU when returning from this function
hoomd/md/ForceCompositeGPU.cu:    for (int idev = gpu_partition.getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/md/ForceCompositeGPU.cu:        auto range = gpu_partition.getRangeAndSetGPU(idev);
hoomd/md/ForceCompositeGPU.cu:        if (idev == (int)gpu_partition.getNumActiveGPUs() - 1)
hoomd/md/ForceCompositeGPU.cu:        hipLaunchKernelGGL((gpu_update_composite_kernel),
hoomd/md/ForceCompositeGPU.cu:hipError_t gpu_find_rigid_centers(const unsigned int* d_body,
hoomd/md/EvaluatorPairDPDThermoLJ.h:        //! Set CUDA memory hints
hoomd/md/IntegrationMethodTwoStep.h:   net force and acceleration summed in Integrator::computeAccelerations. While the GPU ones only
hoomd/md/IntegrationMethodTwoStep.h:   Integrator::computeNetForceGPU() which is called at the proper time in IntegratorTwoStep() (and
hoomd/md/BendingRigidityMeshForceComputeGPU.cu:#include "BendingRigidityMeshForceComputeGPU.cuh"
hoomd/md/BendingRigidityMeshForceComputeGPU.cu:/*! \file BendingRigidityMeshForceComputeGPU.cu
hoomd/md/BendingRigidityMeshForceComputeGPU.cu:    \brief Defines GPU kernel code for calculating the bending rigidity forces. Used by
hoomd/md/BendingRigidityMeshForceComputeGPU.cu:   BendingRigidityMeshForceComputeComputeGPU.
hoomd/md/BendingRigidityMeshForceComputeGPU.cu://! Kernel for calculating helfrich sigmas on the GPU
hoomd/md/BendingRigidityMeshForceComputeGPU.cu:    \param box Box dimensions (in GPU format) to use for periodic boundary conditions
hoomd/md/BendingRigidityMeshForceComputeGPU.cu:    \param blist List of mesh bonds stored on the GPU
hoomd/md/BendingRigidityMeshForceComputeGPU.cu:    \param bpos_list Position of current index in list of mesh bonds stored on the GPU
hoomd/md/BendingRigidityMeshForceComputeGPU.cu:    \param n_bonds_list List of numbers of mesh bonds stored on the GPU
hoomd/md/BendingRigidityMeshForceComputeGPU.cu:__global__ void gpu_compute_bending_rigidity_force_kernel(Scalar4* d_force,
hoomd/md/BendingRigidityMeshForceComputeGPU.cu:    \param box Box dimensions (in GPU format) to use for periodic boundary conditions
hoomd/md/BendingRigidityMeshForceComputeGPU.cu:    \param blist List of mesh bonds stored on the GPU
hoomd/md/BendingRigidityMeshForceComputeGPU.cu:    \param bpos_list Position of current index in list of mesh bonds stored on the GPU
hoomd/md/BendingRigidityMeshForceComputeGPU.cu:    \param n_bonds_list List of numbers of mesh bonds stored on the GPU
hoomd/md/BendingRigidityMeshForceComputeGPU.cu:hipError_t gpu_compute_bending_rigidity_force(Scalar4* d_force,
hoomd/md/BendingRigidityMeshForceComputeGPU.cu:    hipFuncGetAttributes(&attr, (const void*)gpu_compute_bending_rigidity_force_kernel);
hoomd/md/BendingRigidityMeshForceComputeGPU.cu:    hipLaunchKernelGGL((gpu_compute_bending_rigidity_force_kernel),
hoomd/md/HarmonicImproperForceGPU.cu:#include "HarmonicImproperForceGPU.cuh"
hoomd/md/HarmonicImproperForceGPU.cu:/*! \file HarmonicImproperForceGPU.cu
hoomd/md/HarmonicImproperForceGPU.cu:    \brief Defines GPU kernel code for calculating the harmonic improper forces. Used by
hoomd/md/HarmonicImproperForceGPU.cu:   HarmonicImproperForceComputeGPU.
hoomd/md/HarmonicImproperForceGPU.cu://! Kernel for calculating harmonic improper forces on the GPU
hoomd/md/HarmonicImproperForceGPU.cu:__global__ void gpu_compute_harmonic_improper_forces_kernel(Scalar4* d_force,
hoomd/md/HarmonicImproperForceGPU.cu:    \param box Box dimensions (in GPU format) to use for periodic boundary conditions
hoomd/md/HarmonicImproperForceGPU.cu:hipError_t gpu_compute_harmonic_improper_forces(Scalar4* d_force,
hoomd/md/HarmonicImproperForceGPU.cu:    hipFuncGetAttributes(&attr, (const void*)gpu_compute_harmonic_improper_forces_kernel);
hoomd/md/HarmonicImproperForceGPU.cu:    hipLaunchKernelGGL((gpu_compute_harmonic_improper_forces_kernel),
hoomd/md/HarmonicAngleForceGPU.cuh:/*! \file HarmonicAngleForceGPU.cuh
hoomd/md/HarmonicAngleForceGPU.cuh:    \brief Declares GPU kernel code for calculating the harmonic angle forces. Used by
hoomd/md/HarmonicAngleForceGPU.cuh:   HarmonicAngleForceComputeGPU.
hoomd/md/HarmonicAngleForceGPU.cuh:#ifndef __HARMONICANGLEFORCEGPU_CUH__
hoomd/md/HarmonicAngleForceGPU.cuh:#define __HARMONICANGLEFORCEGPU_CUH__
hoomd/md/HarmonicAngleForceGPU.cuh://! Kernel driver that computes harmonic angle forces for HarmonicAngleForceComputeGPU
hoomd/md/HarmonicAngleForceGPU.cuh:hipError_t gpu_compute_harmonic_angle_forces(Scalar4* d_force,
hoomd/md/TwoStepConstantPressure.cc:        } // end of GPUArray scope
hoomd/md/TwoStepConstantPressure.cc:        } // end GPUArray scope
hoomd/md/TriangleAreaConservationMeshForceCompute.cc:    GPUArray<triangle_area_conservation_param_t> params(n_types, m_exec_conf);
hoomd/md/TriangleAreaConservationMeshForceCompute.cc:    GPUArray<Scalar> area(n_types, m_exec_conf);
hoomd/md/TwoStepRATTLEGPU.cu.inc:#include "hoomd/md/TwoStepRATTLEBDGPU.cuh"
hoomd/md/TwoStepRATTLEGPU.cu.inc:#include "hoomd/md/TwoStepRATTLELangevinGPU.cuh"
hoomd/md/TwoStepRATTLEGPU.cu.inc:#include "hoomd/md/TwoStepRATTLENVEGPU.cuh"
hoomd/md/TwoStepRATTLEGPU.cu.inc:gpu_rattle_brownian_step_one<MANIFOLD_CLASS>(Scalar4* d_pos,
hoomd/md/TwoStepRATTLEGPU.cu.inc:                                             const GPUPartition& gpu_partition);
hoomd/md/TwoStepRATTLEGPU.cu.inc:gpu_include_rattle_force_bd<MANIFOLD_CLASS>(const Scalar4* d_pos,
hoomd/md/TwoStepRATTLEGPU.cu.inc:                                            const GPUPartition& gpu_partition);
hoomd/md/TwoStepRATTLEGPU.cu.inc:template hipError_t gpu_rattle_langevin_step_two<MANIFOLD_CLASS>(
hoomd/md/TwoStepRATTLEGPU.cu.inc:template hipError_t gpu_rattle_nve_step_two<MANIFOLD_CLASS>(Scalar4* d_pos,
hoomd/md/TwoStepRATTLEGPU.cu.inc:                                                            const GPUPartition& gpu_partition,
hoomd/md/TwoStepRATTLEGPU.cu.inc:template hipError_t gpu_include_rattle_force_nve<MANIFOLD_CLASS>(const Scalar4* d_pos,
hoomd/md/TwoStepRATTLEGPU.cu.inc:                                                                 const GPUPartition& gpu_partition,
hoomd/md/BendingRigidityMeshForceCompute.cc:    GPUArray<Scalar> params(m_mesh_data->getMeshBondData()->getNTypes(), m_exec_conf);
hoomd/md/PotentialExternalGPU.cuh:/*! \file PotentialExternalGPU.cuh
hoomd/md/PotentialExternalGPU.cuh:    \brief Defines templated GPU kernel code for calculating the external forces.
hoomd/md/PotentialExternalGPU.cuh:#ifndef __POTENTIAL_EXTERNAL_GPU_CUH__
hoomd/md/PotentialExternalGPU.cuh:#define __POTENTIAL_EXTERNAL_GPU_CUH__
hoomd/md/PotentialExternalGPU.cuh://! Wraps arguments to gpu_compute_potential_external_forces
hoomd/md/PotentialExternalGPU.cuh:    const BoxDim box;               //!< Simulation box in GPU format
hoomd/md/PotentialExternalGPU.cuh:hipError_t __attribute__((visibility("default"))) gpu_compute_potential_external_forces(
hoomd/md/PotentialExternalGPU.cuh:__global__ void gpu_compute_external_forces_kernel(Scalar4* d_force,
hoomd/md/PotentialExternalGPU.cuh:hipError_t gpu_compute_potential_external_forces(
hoomd/md/PotentialExternalGPU.cuh:        reinterpret_cast<const void*>(&gpu_compute_external_forces_kernel<evaluator>));
hoomd/md/PotentialExternalGPU.cuh:    hipLaunchKernelGGL((gpu_compute_external_forces_kernel<evaluator>),
hoomd/md/PotentialExternalGPU.cuh:#endif // __POTENTIAL_PAIR_GPU_CUH__
hoomd/md/export_PotentialTersoffGPU.cc.inc:#include "hoomd/md/PotentialTersoffGPU.h"
hoomd/md/export_PotentialTersoffGPU.cc.inc:#define EXPORT_FUNCTION export_Potential@_evaluator@GPU
hoomd/md/export_PotentialTersoffGPU.cc.inc:    export_PotentialTersoffGPU<EVALUATOR_CLASS>(m, "Potential@_evaluator@GPU");
hoomd/md/export_PotentialPairDPDThermoGPU.cc.inc:#include "hoomd/md/PotentialPairDPDThermoGPU.h"
hoomd/md/export_PotentialPairDPDThermoGPU.cc.inc:#define EXPORT_FUNCTION export_PotentialPairDPDThermo@_evaluator@GPU
hoomd/md/export_PotentialPairDPDThermoGPU.cc.inc:    export_PotentialPairDPDThermoGPU<EVALUATOR_CLASS>(m, "PotentialPairDPDThermo@_evaluator@GPU");
hoomd/md/ActiveForceConstraintComputeGPU.cu.inc:#include "hoomd/md/ActiveForceConstraintComputeGPU.cuh"
hoomd/md/ActiveForceConstraintComputeGPU.cu.inc:gpu_compute_active_force_set_constraints<MANIFOLD_CLASS>(const unsigned int group_size,
hoomd/md/ActiveForceConstraintComputeGPU.cu.inc:template hipError_t gpu_compute_active_force_constraint_rotational_diffusion<MANIFOLD_CLASS>(
hoomd/md/methods/rattle.py:                + self.manifold_constraint.__class__.__name__ + 'GPU')
hoomd/md/methods/rattle.py:                + self.manifold_constraint.__class__.__name__ + 'GPU')
hoomd/md/methods/rattle.py:                + self.manifold_constraint.__class__.__name__ + 'GPU')
hoomd/md/methods/rattle.py:                + self.manifold_constraint.__class__.__name__ + 'GPU')
hoomd/md/methods/methods.py:            cls = _md.TwoStepConstantVolumeGPU
hoomd/md/methods/methods.py:            thermo_cls = _md.ComputeThermoGPU
hoomd/md/methods/methods.py:            cpp_cls = _md.TwoStepConstantPressureGPU
hoomd/md/methods/methods.py:            thermo_cls = _md.ComputeThermoGPU
hoomd/md/methods/methods.py:            cls = _md.TwoStepLangevinGPU
hoomd/md/methods/methods.py:            self._cpp_obj = _md.TwoStepBDGPU(sim.state._cpp_sys_def,
hoomd/md/methods/methods.py:            self._cpp_obj = _md.TwoStepBDGPU(sim.state._cpp_sys_def,
hoomd/md/TwoStepConstantVolumeGPU.cuh:#include "hoomd/GPUPartition.cuh"
hoomd/md/TwoStepConstantVolumeGPU.cuh:#ifndef HOOMD_TWOSTEPNVTBASEGPU_CUH
hoomd/md/TwoStepConstantVolumeGPU.cuh:#define HOOMD_TWOSTEPNVTBASEGPU_CUH
hoomd/md/TwoStepConstantVolumeGPU.cuh://! Kernel driver for the first part of the NVT update called by TwoStepNVTGPU
hoomd/md/TwoStepConstantVolumeGPU.cuh:hipError_t gpu_nvt_rescale_step_one(Scalar4* d_pos,
hoomd/md/TwoStepConstantVolumeGPU.cuh:                                    const GPUPartition& gpu_partition,
hoomd/md/TwoStepConstantVolumeGPU.cuh://! Kernel driver for the second part of the NVT update called by NVTUpdaterGPU
hoomd/md/TwoStepConstantVolumeGPU.cuh:hipError_t gpu_nvt_rescale_step_two(Scalar4* d_vel,
hoomd/md/TwoStepConstantVolumeGPU.cuh:                                    const GPUPartition& gpu_partition);
hoomd/md/TwoStepConstantVolumeGPU.cuh:#endif // HOOMD_TWOSTEPNVTBASEGPU_CUH
hoomd/md/PotentialSpecialPairGPU.h:#ifndef __POTENTIAL_SPECIAL_PAIR_GPU_H__
hoomd/md/PotentialSpecialPairGPU.h:#define __POTENTIAL_SPECIAL_PAIR_GPU_H__
hoomd/md/PotentialSpecialPairGPU.h://! Use GPU functions for bonds
hoomd/md/PotentialSpecialPairGPU.h:#include "PotentialBondGPU.cuh"
hoomd/md/PotentialSpecialPairGPU.h:/*! \file PotentialSpecialPairGPU.h
hoomd/md/PotentialSpecialPairGPU.h:    \brief Defines the template class for special pair potentials on the GPU
hoomd/md/PotentialSpecialPairGPU.h://! Template class for computing special pair potentials on the GPU
hoomd/md/PotentialSpecialPairGPU.h:    \sa export_PotentialSpecialPairGPU()
hoomd/md/PotentialSpecialPairGPU.h:template<class evaluator> class PotentialSpecialPairGPU : public PotentialSpecialPair<evaluator>
hoomd/md/PotentialSpecialPairGPU.h:    PotentialSpecialPairGPU(std::shared_ptr<SystemDefinition> sysdef);
hoomd/md/PotentialSpecialPairGPU.h:    virtual ~PotentialSpecialPairGPU() { }
hoomd/md/PotentialSpecialPairGPU.h:    GPUArray<unsigned int> m_flags;        //!< Flags set during the kernel execution
hoomd/md/PotentialSpecialPairGPU.h:PotentialSpecialPairGPU<evaluator>::PotentialSpecialPairGPU(
hoomd/md/PotentialSpecialPairGPU.h:    // can't run on the GPU if there aren't any GPUs in the execution configuration
hoomd/md/PotentialSpecialPairGPU.h:    if (!this->m_exec_conf->isCUDAEnabled())
hoomd/md/PotentialSpecialPairGPU.h:            << "Creating a PotentialSpecialPairGPU with no GPU in the execution configuration"
hoomd/md/PotentialSpecialPairGPU.h:        throw std::runtime_error("Error initializing PotentialSpecialPairGPU");
hoomd/md/PotentialSpecialPairGPU.h:    GPUArray<typename evaluator::param_type> params(this->m_pair_data->getNTypes(),
hoomd/md/PotentialSpecialPairGPU.h:    // allocate flags storage on the GPU
hoomd/md/PotentialSpecialPairGPU.h:    GPUArray<unsigned int> flags(1, this->m_exec_conf);
hoomd/md/PotentialSpecialPairGPU.h:template<class evaluator> void PotentialSpecialPairGPU<evaluator>::computeForces(uint64_t timestep)
hoomd/md/PotentialSpecialPairGPU.h:        const GPUArray<typename PairData::members_t>& gpu_bond_list
hoomd/md/PotentialSpecialPairGPU.h:            = this->m_pair_data->getGPUTable();
hoomd/md/PotentialSpecialPairGPU.h:        const Index2D& gpu_table_indexer = this->m_pair_data->getGPUTableIndexer();
hoomd/md/PotentialSpecialPairGPU.h:        ArrayHandle<typename PairData::members_t> d_gpu_bondlist(gpu_bond_list,
hoomd/md/PotentialSpecialPairGPU.h:        ArrayHandle<unsigned int> d_gpu_bond_pos_list(this->m_pair_data->getGPUPosTable(),
hoomd/md/PotentialSpecialPairGPU.h:        ArrayHandle<unsigned int> d_gpu_n_bonds(this->m_pair_data->getNGroupsArray(),
hoomd/md/PotentialSpecialPairGPU.h:        kernel::gpu_compute_bond_forces<evaluator, 2>(
hoomd/md/PotentialSpecialPairGPU.h:                                   d_gpu_bondlist.data,
hoomd/md/PotentialSpecialPairGPU.h:                                   gpu_table_indexer,
hoomd/md/PotentialSpecialPairGPU.h:                                   d_gpu_bond_pos_list.data,
hoomd/md/PotentialSpecialPairGPU.h:                                   d_gpu_n_bonds.data,
hoomd/md/PotentialSpecialPairGPU.h:    if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/PotentialSpecialPairGPU.h:        CHECK_CUDA_ERROR();
hoomd/md/PotentialSpecialPairGPU.h:template<class T> void export_PotentialSpecialPairGPU(pybind11::module& m, const std::string& name)
hoomd/md/PotentialSpecialPairGPU.h:    pybind11::class_<PotentialSpecialPairGPU<T>,
hoomd/md/PotentialSpecialPairGPU.h:                     std::shared_ptr<PotentialSpecialPairGPU<T>>>(m, name.c_str())
hoomd/md/PotentialSpecialPairGPU.h:#endif // __POTENTIAL_SPECIAL_PAIR_GPU_H__
hoomd/md/TwoStepLangevinGPU.h://! Implements Langevin dynamics on the GPU
hoomd/md/TwoStepLangevinGPU.h:/*! GPU accelerated version of TwoStepLangevin
hoomd/md/TwoStepLangevinGPU.h:class PYBIND11_EXPORT TwoStepLangevinGPU : public TwoStepLangevin
hoomd/md/TwoStepLangevinGPU.h:    TwoStepLangevinGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/md/TwoStepLangevinGPU.h:    virtual ~TwoStepLangevinGPU() { };
hoomd/md/TwoStepLangevinGPU.h:    GPUArray<Scalar> m_partial_sum1; //!< memory space for partial sum over bd energy transfers
hoomd/md/TwoStepLangevinGPU.h:    GPUArray<Scalar> m_sum;          //!< memory space for sum over bd energy transfers
hoomd/md/EvaluatorPairOPP.h:        //! Set CUDA memory hints
hoomd/md/NeighborListTree.h:    // accessed on the GPU, they were optimized for the CPU with SIMD support
hoomd/md/NeighborListTree.h:    GPUVector<hoomd::detail::AABB> m_aabbs;            //!< Flat array of AABBs of all types
hoomd/md/AnisoPotentialPairDipoleGPUKernel.cu:#include "AnisoPotentialPairGPU.cuh"
hoomd/md/AnisoPotentialPairDipoleGPUKernel.cu:gpu_compute_pair_aniso_forces<EvaluatorPairDipole>(
hoomd/md/HarmonicImproperForceGPU.cuh:/*! \file HarmonicImproperForceGPU.cuh
hoomd/md/HarmonicImproperForceGPU.cuh:    \brief Declares GPU kernel code for calculating the harmonic improper forces. Used by
hoomd/md/HarmonicImproperForceGPU.cuh:   HarmonicImproperForceComputeGPU.
hoomd/md/HarmonicImproperForceGPU.cuh:#ifndef __HARMONICIMPROPERFORCEGPU_CUH__
hoomd/md/HarmonicImproperForceGPU.cuh:#define __HARMONICIMPROPERFORCEGPU_CUH__
hoomd/md/HarmonicImproperForceGPU.cuh://! Kernel driver that computes harmonic IMPROPER forces for HarmonicImproperForceComputeGPU
hoomd/md/HarmonicImproperForceGPU.cuh:hipError_t gpu_compute_harmonic_improper_forces(Scalar4* d_force,
hoomd/md/NeighborListGPUBinned.cc:/*! \file NeighborListGPUBinned.cc
hoomd/md/NeighborListGPUBinned.cc:    \brief Defines NeighborListGPUBinned
hoomd/md/NeighborListGPUBinned.cc:#include "NeighborListGPUBinned.h"
hoomd/md/NeighborListGPUBinned.cc:#include "NeighborListGPUBinned.cuh"
hoomd/md/NeighborListGPUBinned.cc:NeighborListGPUBinned::NeighborListGPUBinned(std::shared_ptr<SystemDefinition> sysdef,
hoomd/md/NeighborListGPUBinned.cc:    : NeighborListGPU(sysdef, r_buff), m_cl(std::make_shared<CellListGPU>(sysdef))
hoomd/md/NeighborListGPUBinned.cc:    // with multiple GPUs, use indirect access via particle data arrays
hoomd/md/NeighborListGPUBinned.cc:    // with multiple GPUs, request a cell list per device
hoomd/md/NeighborListGPUBinned.cc:    CHECK_CUDA_ERROR();
hoomd/md/NeighborListGPUBinned.cc:NeighborListGPUBinned::~NeighborListGPUBinned() { }
hoomd/md/NeighborListGPUBinned.cc:void NeighborListGPUBinned::buildNlist(uint64_t timestep)
hoomd/md/NeighborListGPUBinned.cc:        throw std::runtime_error("GPU neighbor lists require a full storage mode.");
hoomd/md/NeighborListGPUBinned.cc:    auto& gpu_map = m_exec_conf->getGPUIds();
hoomd/md/NeighborListGPUBinned.cc:        for (unsigned int idev = 0; idev < m_exec_conf->getNumActiveGPUs(); ++idev)
hoomd/md/NeighborListGPUBinned.cc:            cudaMemPrefetchAsync(d_cell_adj.data,
hoomd/md/NeighborListGPUBinned.cc:                                 gpu_map[idev]);
hoomd/md/NeighborListGPUBinned.cc:    if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/NeighborListGPUBinned.cc:        CHECK_CUDA_ERROR();
hoomd/md/NeighborListGPUBinned.cc:    m_exec_conf->beginMultiGPU();
hoomd/md/NeighborListGPUBinned.cc:    kernel::gpu_compute_nlist_binned(
hoomd/md/NeighborListGPUBinned.cc:        m_pdata->getGPUPartition(),
hoomd/md/NeighborListGPUBinned.cc:    if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/NeighborListGPUBinned.cc:        CHECK_CUDA_ERROR();
hoomd/md/NeighborListGPUBinned.cc:    m_exec_conf->endMultiGPU();
hoomd/md/NeighborListGPUBinned.cc:void export_NeighborListGPUBinned(pybind11::module& m)
hoomd/md/NeighborListGPUBinned.cc:    pybind11::class_<NeighborListGPUBinned,
hoomd/md/NeighborListGPUBinned.cc:                     NeighborListGPU,
hoomd/md/NeighborListGPUBinned.cc:                     std::shared_ptr<NeighborListGPUBinned>>(m, "NeighborListGPUBinned")
hoomd/md/NeighborListGPUBinned.cc:                      &NeighborListGPUBinned::getDeterministic,
hoomd/md/NeighborListGPUBinned.cc:                      &NeighborListGPUBinned::setDeterministic)
hoomd/md/NeighborListGPUBinned.cc:             &NeighborListGPUBinned::getDim,
hoomd/md/NeighborListGPUBinned.cc:        .def("getNmax", &NeighborListGPUBinned::getNmax);
hoomd/md/MolecularForceCompute.h:    //! construct a list of local molecules on the GPU
hoomd/md/MolecularForceCompute.h:    virtual void initMoleculesGPU();
hoomd/md/MolecularForceCompute.h:    GPUPartition m_gpu_partition; //!< Partition of the molecules on GPUs
hoomd/md/HarmonicDihedralForceGPU.cu:#include "HarmonicDihedralForceGPU.cuh"
hoomd/md/HarmonicDihedralForceGPU.cu:/*! \file HarmonicDihedralForceGPU.cu
hoomd/md/HarmonicDihedralForceGPU.cu:    \brief Defines GPU kernel code for calculating the harmonic dihedral forces. Used by
hoomd/md/HarmonicDihedralForceGPU.cu:   HarmonicDihedralForceComputeGPU.
hoomd/md/HarmonicDihedralForceGPU.cu://! Kernel for calculating harmonic dihedral forces on the GPU
hoomd/md/HarmonicDihedralForceGPU.cu:__global__ void gpu_compute_harmonic_dihedral_forces_kernel(Scalar4* d_force,
hoomd/md/HarmonicDihedralForceGPU.cu:    \param d_pos particle positions on the GPU
hoomd/md/HarmonicDihedralForceGPU.cu:    \param box Box dimensions (in GPU format) to use for periodic boundary conditions
hoomd/md/HarmonicDihedralForceGPU.cu:hipError_t gpu_compute_harmonic_dihedral_forces(Scalar4* d_force,
hoomd/md/HarmonicDihedralForceGPU.cu:    hipFuncGetAttributes(&attr, (const void*)gpu_compute_harmonic_dihedral_forces_kernel);
hoomd/md/HarmonicDihedralForceGPU.cu:    hipLaunchKernelGGL((gpu_compute_harmonic_dihedral_forces_kernel),
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cuh:/*! \file TriangleAreaConservationMeshForceComputeGPU.cuh
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cuh:    \brief Declares GPU kernel code for calculating the area conservation forces. Used by
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cuh:   TriangleAreaConservationMeshForceComputeGPU.
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cuh://! Kernel driver that computes the forces for TriangleAreaConservationMeshForceComputeGPU
hoomd/md/TriangleAreaConservationMeshForceComputeGPU.cuh:hipError_t gpu_compute_TriangleAreaConservation_force(Scalar4* d_force,
hoomd/md/TwoStepBDGPU.cuh:/*! \file TwoStepBDGPU.cuh
hoomd/md/TwoStepBDGPU.cuh:    \brief Declares GPU kernel code for Brownian dynamics on the GPU. Used by TwoStepBDGPU.
hoomd/md/TwoStepBDGPU.cuh:#include "TwoStepLangevinGPU.cuh"
hoomd/md/TwoStepBDGPU.cuh:#include "hoomd/GPUPartition.cuh"
hoomd/md/TwoStepBDGPU.cuh:#ifndef __TWO_STEP_BD_GPU_CUH__
hoomd/md/TwoStepBDGPU.cuh:#define __TWO_STEP_BD_GPU_CUH__
hoomd/md/TwoStepBDGPU.cuh://! Kernel driver for the first part of the Brownian update called by TwoStepBDGPU
hoomd/md/TwoStepBDGPU.cuh:hipError_t gpu_brownian_step_one(Scalar4* d_pos,
hoomd/md/TwoStepBDGPU.cuh:                                 const GPUPartition& gpu_partition);
hoomd/md/TwoStepBDGPU.cuh:#endif //__TWO_STEP_BD_GPU_CUH__
hoomd/md/ActiveForceConstraintComputeGPU.h:#include "ActiveForceComputeGPU.cuh"
hoomd/md/ActiveForceConstraintComputeGPU.h:#include "ActiveForceConstraintComputeGPU.cuh"
hoomd/md/ActiveForceConstraintComputeGPU.h:/*! \file ActiveForceConstraintComputeGPU.h
hoomd/md/ActiveForceConstraintComputeGPU.h:    \brief Declares a class for computing active forces on the GPU
hoomd/md/ActiveForceConstraintComputeGPU.h:#ifndef __ACTIVEFORCECONSTRAINTCOMPUTE_GPU_H__
hoomd/md/ActiveForceConstraintComputeGPU.h:#define __ACTIVEFORCECONSTRAINTCOMPUTE_GPU_H__
hoomd/md/ActiveForceConstraintComputeGPU.h://! Adds an active force to a number of particles with confinement on the GPU
hoomd/md/ActiveForceConstraintComputeGPU.h:class PYBIND11_EXPORT ActiveForceConstraintComputeGPU
hoomd/md/ActiveForceConstraintComputeGPU.h:    ActiveForceConstraintComputeGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/md/ActiveForceConstraintComputeGPU.h:/*! \file ActiveForceConstraintComputeGPU.cc
hoomd/md/ActiveForceConstraintComputeGPU.h:    \brief Contains code for the ActiveForceConstraintComputeGPU class
hoomd/md/ActiveForceConstraintComputeGPU.h:ActiveForceConstraintComputeGPU<Manifold>::ActiveForceConstraintComputeGPU(
hoomd/md/ActiveForceConstraintComputeGPU.h:    if (!this->m_exec_conf->isCUDAEnabled())
hoomd/md/ActiveForceConstraintComputeGPU.h:        throw std::runtime_error("ActiveForceConstraintComputeGPU requires a GPU device.");
hoomd/md/ActiveForceConstraintComputeGPU.h:template<class Manifold> void ActiveForceConstraintComputeGPU<Manifold>::setForces()
hoomd/md/ActiveForceConstraintComputeGPU.h:    // compute the forces on the GPU
hoomd/md/ActiveForceConstraintComputeGPU.h:    kernel::gpu_compute_active_force_set_forces(group_size,
hoomd/md/ActiveForceConstraintComputeGPU.h:    if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/ActiveForceConstraintComputeGPU.h:        CHECK_CUDA_ERROR();
hoomd/md/ActiveForceConstraintComputeGPU.h:void ActiveForceConstraintComputeGPU<Manifold>::rotationalDiffusion(Scalar rotational_diffusion,
hoomd/md/ActiveForceConstraintComputeGPU.h:    // perform the update on the GPU
hoomd/md/ActiveForceConstraintComputeGPU.h:    kernel::gpu_compute_active_force_constraint_rotational_diffusion<Manifold>(
hoomd/md/ActiveForceConstraintComputeGPU.h:    if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/ActiveForceConstraintComputeGPU.h:        CHECK_CUDA_ERROR();
hoomd/md/ActiveForceConstraintComputeGPU.h:template<class Manifold> void ActiveForceConstraintComputeGPU<Manifold>::setConstraint()
hoomd/md/ActiveForceConstraintComputeGPU.h:    // perform the update on the GPU
hoomd/md/ActiveForceConstraintComputeGPU.h:    kernel::gpu_compute_active_force_set_constraints<Manifold>(
hoomd/md/ActiveForceConstraintComputeGPU.h:    if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/ActiveForceConstraintComputeGPU.h:        CHECK_CUDA_ERROR();
hoomd/md/ActiveForceConstraintComputeGPU.h:void export_ActiveForceConstraintComputeGPU(pybind11::module& m, const std::string& name)
hoomd/md/ActiveForceConstraintComputeGPU.h:    pybind11::class_<ActiveForceConstraintComputeGPU<Manifold>,
hoomd/md/ActiveForceConstraintComputeGPU.h:                     std::shared_ptr<ActiveForceConstraintComputeGPU<Manifold>>>(m, name.c_str())
hoomd/md/VolumeConservationMeshForceComputeGPU.h:#include "VolumeConservationMeshForceComputeGPU.cuh"
hoomd/md/VolumeConservationMeshForceComputeGPU.h:/*! \file VolumeConservationMeshForceComputeGPU.h
hoomd/md/VolumeConservationMeshForceComputeGPU.h:    \brief Declares a class for computing volume constraint forces on the GPU
hoomd/md/VolumeConservationMeshForceComputeGPU.h:#ifndef __VOLUMECONSERVATIONMESHFORCECOMPUTE_GPU_H__
hoomd/md/VolumeConservationMeshForceComputeGPU.h:#define __VOLUMECONSERVATIONMESHFORCECOMPUTE_GPU_H__
hoomd/md/VolumeConservationMeshForceComputeGPU.h://! Computes volume conservation energy forces on the mesh on the GPU
hoomd/md/VolumeConservationMeshForceComputeGPU.h:class PYBIND11_EXPORT VolumeConservationMeshForceComputeGPU
hoomd/md/VolumeConservationMeshForceComputeGPU.h:    VolumeConservationMeshForceComputeGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/md/VolumeConservationMeshForceComputeGPU.h:    GPUArray<Scalar> m_partial_sum; //!< memory space for partial sum over volume
hoomd/md/VolumeConservationMeshForceComputeGPU.h:    GPUArray<Scalar> m_sum;         //!< memory space for sum over volume
hoomd/md/VolumeConservationMeshForceComputeGPU.h://! Exports the VolumeConservationMeshForceComputeGPU class to python
hoomd/md/VolumeConservationMeshForceComputeGPU.h:void export_VolumeConservationMeshForceComputeGPU(pybind11::module& m);
hoomd/md/EvaluatorPairGauss.h:        // set CUDA memory hints
hoomd/md/TwoStepRATTLELangevinGPU.cu:#include "TwoStepRATTLELangevinGPU.cuh"
hoomd/md/TwoStepRATTLELangevinGPU.cu:__global__ void gpu_rattle_langevin_angular_step_two_kernel(const Scalar4* d_pos,
hoomd/md/TwoStepRATTLELangevinGPU.cu:    \param rattle_langevin_args Collected arguments for gpu_rattle_langevin_step_two_kernel() and
hoomd/md/TwoStepRATTLELangevinGPU.cu:   gpu_rattle_langevin_angular_step_two() \param deltaT timestep \param D dimensionality of the
hoomd/md/TwoStepRATTLELangevinGPU.cu:    This is just a driver for gpu_rattle_langevin_angular_step_two_kernel(), see it for details.
hoomd/md/TwoStepRATTLELangevinGPU.cu:gpu_rattle_langevin_angular_step_two(const Scalar4* d_pos,
hoomd/md/TwoStepRATTLELangevinGPU.cu:    hipLaunchKernelGGL(gpu_rattle_langevin_angular_step_two_kernel,
hoomd/md/TwoStepRATTLELangevinGPU.cu:__global__ void gpu_rattle_bdtally_reduce_partial_sum_kernel(Scalar* d_sum,
hoomd/md/FIREEnergyMinimizerGPU.cc:#include "FIREEnergyMinimizerGPU.h"
hoomd/md/FIREEnergyMinimizerGPU.cc:#include "FIREEnergyMinimizerGPU.cuh"
hoomd/md/FIREEnergyMinimizerGPU.cc:/*! \file FIREEnergyMinimizerGPU.h
hoomd/md/FIREEnergyMinimizerGPU.cc:    \brief Contains code for the FIREEnergyMinimizerGPU class
hoomd/md/FIREEnergyMinimizerGPU.cc:FIREEnergyMinimizerGPU::FIREEnergyMinimizerGPU(std::shared_ptr<SystemDefinition> sysdef, Scalar dt)
hoomd/md/FIREEnergyMinimizerGPU.cc:    // only one GPU is supported
hoomd/md/FIREEnergyMinimizerGPU.cc:    if (!m_exec_conf->isCUDAEnabled())
hoomd/md/FIREEnergyMinimizerGPU.cc:        throw std::runtime_error("FIREEnergyMinimizerGPU requires a GPU device.");
hoomd/md/FIREEnergyMinimizerGPU.cc:    GPUArray<Scalar> sum(1, m_exec_conf);
hoomd/md/FIREEnergyMinimizerGPU.cc:    GPUArray<Scalar> sum3(3, m_exec_conf);
hoomd/md/FIREEnergyMinimizerGPU.cc:    m_partial_sum1 = GPUVector<Scalar>(m_exec_conf);
hoomd/md/FIREEnergyMinimizerGPU.cc:    m_partial_sum2 = GPUVector<Scalar>(m_exec_conf);
hoomd/md/FIREEnergyMinimizerGPU.cc:    m_partial_sum3 = GPUVector<Scalar>(m_exec_conf);
hoomd/md/FIREEnergyMinimizerGPU.cc:void FIREEnergyMinimizerGPU::resizePartialSumArrays()
hoomd/md/FIREEnergyMinimizerGPU.cc:void FIREEnergyMinimizerGPU::update(uint64_t timestep)
hoomd/md/FIREEnergyMinimizerGPU.cc:    // compute the total energy on the GPU
hoomd/md/FIREEnergyMinimizerGPU.cc:            kernel::gpu_fire_compute_sum_pe(d_index_array.data,
hoomd/md/FIREEnergyMinimizerGPU.cc:            if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/FIREEnergyMinimizerGPU.cc:                CHECK_CUDA_ERROR();
hoomd/md/FIREEnergyMinimizerGPU.cc:            kernel::gpu_fire_compute_sum_all(m_pdata->getN(),
hoomd/md/FIREEnergyMinimizerGPU.cc:            if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/FIREEnergyMinimizerGPU.cc:                CHECK_CUDA_ERROR();
hoomd/md/FIREEnergyMinimizerGPU.cc:                kernel::gpu_fire_compute_sum_all_angular(m_pdata->getN(),
hoomd/md/FIREEnergyMinimizerGPU.cc:                if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/FIREEnergyMinimizerGPU.cc:                    CHECK_CUDA_ERROR();
hoomd/md/FIREEnergyMinimizerGPU.cc:        kernel::gpu_fire_update_v(d_vel.data,
hoomd/md/FIREEnergyMinimizerGPU.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/FIREEnergyMinimizerGPU.cc:            CHECK_CUDA_ERROR();
hoomd/md/FIREEnergyMinimizerGPU.cc:            kernel::gpu_fire_update_angmom(d_net_torque.data,
hoomd/md/FIREEnergyMinimizerGPU.cc:            if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/FIREEnergyMinimizerGPU.cc:                CHECK_CUDA_ERROR();
hoomd/md/FIREEnergyMinimizerGPU.cc:            kernel::gpu_fire_zero_v(d_vel.data, d_index_array.data, group_size);
hoomd/md/FIREEnergyMinimizerGPU.cc:            if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/FIREEnergyMinimizerGPU.cc:                CHECK_CUDA_ERROR();
hoomd/md/FIREEnergyMinimizerGPU.cc:                kernel::gpu_fire_zero_angmom(d_angmom.data, d_index_array.data, group_size);
hoomd/md/FIREEnergyMinimizerGPU.cc:                if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/FIREEnergyMinimizerGPU.cc:                    CHECK_CUDA_ERROR();
hoomd/md/FIREEnergyMinimizerGPU.cc:void export_FIREEnergyMinimizerGPU(pybind11::module& m)
hoomd/md/FIREEnergyMinimizerGPU.cc:    pybind11::class_<FIREEnergyMinimizerGPU,
hoomd/md/FIREEnergyMinimizerGPU.cc:                     std::shared_ptr<FIREEnergyMinimizerGPU>>(m, "FIREEnergyMinimizerGPU")
hoomd/md/ForceDistanceConstraintGPU.h:#include "hoomd/GPUFlags.h"
hoomd/md/ForceDistanceConstraintGPU.h:// CUDA 7.0
hoomd/md/ForceDistanceConstraintGPU.h:// CUDA 7.5
hoomd/md/ForceDistanceConstraintGPU.h:#ifndef __ForceDistanceConstraintGPU_H__
hoomd/md/ForceDistanceConstraintGPU.h:#define __ForceDistanceConstraintGPU_H__
hoomd/md/ForceDistanceConstraintGPU.h:#include "hoomd/GPUVector.h"
hoomd/md/ForceDistanceConstraintGPU.h:/*! Implements a pairwise distance constraint on the GPU
hoomd/md/ForceDistanceConstraintGPU.h:class ForceDistanceConstraintGPU : public ForceDistanceConstraint
hoomd/md/ForceDistanceConstraintGPU.h:    ForceDistanceConstraintGPU(std::shared_ptr<SystemDefinition> sysdef);
hoomd/md/ForceDistanceConstraintGPU.h:    virtual ~ForceDistanceConstraintGPU();
hoomd/md/ForceDistanceConstraintGPU.h:    GPUVector<double> m_csr_val_L; //!< Values of sparse matrix L
hoomd/md/ForceDistanceConstraintGPU.h:    GPUVector<int> m_csr_rowptr_L; //!< Row offset of sparse matrix L
hoomd/md/ForceDistanceConstraintGPU.h:    GPUVector<int> m_csr_colind_L; //!< Column index of sparse matrix L
hoomd/md/ForceDistanceConstraintGPU.h:    GPUVector<double> m_csr_val_U; //!< Values of sparse matrix U
hoomd/md/ForceDistanceConstraintGPU.h:    GPUVector<int> m_csr_rowptr_U; //!< Row offset of sparse matrix U
hoomd/md/ForceDistanceConstraintGPU.h:    GPUVector<int> m_csr_colind_U; //!< Column index of sparse matrix U
hoomd/md/ForceDistanceConstraintGPU.h:    GPUVector<int> m_P;    //!< reordered permutation P
hoomd/md/ForceDistanceConstraintGPU.h:    GPUVector<int> m_Q;    //!< reordered permutation Q
hoomd/md/ForceDistanceConstraintGPU.h:    GPUVector<double> m_T; //!< cusolverRf working space
hoomd/md/ForceDistanceConstraintGPU.h:    GPUVector<int> m_nnz;        //!< Vector of number of non-zero elements per row
hoomd/md/ForceDistanceConstraintGPU.h:    GPUVector<int> m_csr_rowptr; //!< Row offset for CSR
hoomd/md/ForceDistanceConstraintGPU.h:    GPUVector<int> m_csr_colind; //!< Column index for CSR
hoomd/md/ForceDistanceConstraintGPU.h:    GPUVector<double> m_sparse_val; //!< Sparse matrix value list
hoomd/md/NeighborListGPUTree.cu:#include "NeighborListGPUTree.cuh"
hoomd/md/NeighborListGPUTree.cu:__global__ void gpu_nlist_mark_types_kernel(unsigned int* d_types,
hoomd/md/NeighborListGPUTree.cu: * \param block_size Number of CUDA threads per block.
hoomd/md/NeighborListGPUTree.cu: * \sa gpu_nlist_mark_types_kernel
hoomd/md/NeighborListGPUTree.cu:hipError_t gpu_nlist_mark_types(unsigned int* d_types,
hoomd/md/NeighborListGPUTree.cu:    hipFuncGetAttributes(&attr, reinterpret_cast<const void*>(gpu_nlist_mark_types_kernel));
hoomd/md/NeighborListGPUTree.cu:    hipLaunchKernelGGL(gpu_nlist_mark_types_kernel,
hoomd/md/NeighborListGPUTree.cu:uchar2 gpu_nlist_sort_types(void* d_tmp,
hoomd/md/NeighborListGPUTree.cu:        // mark that the gpu arrays should be flipped if the final result is not in the sorted array
hoomd/md/NeighborListGPUTree.cu:__global__ void gpu_nlist_count_types_kernel(unsigned int* d_first,
hoomd/md/NeighborListGPUTree.cu: * \param block_size Number of CUDA threads per block.
hoomd/md/NeighborListGPUTree.cu: * \sa gpu_nlist_count_types_kernel
hoomd/md/NeighborListGPUTree.cu:hipError_t gpu_nlist_count_types(unsigned int* d_first,
hoomd/md/NeighborListGPUTree.cu:    hipFuncGetAttributes(&attr, reinterpret_cast<const void*>(gpu_nlist_count_types_kernel));
hoomd/md/NeighborListGPUTree.cu:    hipLaunchKernelGGL(gpu_nlist_count_types_kernel,
hoomd/md/NeighborListGPUTree.cu:__global__ void gpu_nlist_copy_primitives_kernel(unsigned int* d_traverse_order,
hoomd/md/NeighborListGPUTree.cu: * \param block_size Number of CUDA threads per block.
hoomd/md/NeighborListGPUTree.cu: * \sa gpu_nlist_copy_primitives_kernel
hoomd/md/NeighborListGPUTree.cu:hipError_t gpu_nlist_copy_primitives(unsigned int* d_traverse_order,
hoomd/md/NeighborListGPUTree.cu:    hipFuncGetAttributes(&attr, reinterpret_cast<const void*>(gpu_nlist_copy_primitives_kernel));
hoomd/md/NeighborListGPUTree.cu:    hipLaunchKernelGGL(gpu_nlist_copy_primitives_kernel,
hoomd/md/NeighborListGPUTree.cu: * \param stream CUDA stream for execution
hoomd/md/NeighborListGPUTree.cu: * \param stream CUDA stream for execution
hoomd/md/NeighborListGPUTree.cu: * \param block_size CUDA block size for execution
hoomd/md/NeighborListGPUTree.cu: * \param stream CUDA stream for execution
hoomd/md/NeighborListGPUTree.cu: * \param stream CUDA stream for execution
hoomd/md/NeighborListGPUTree.cu: * \param block_size CUDA block size for execution
hoomd/md/TwoStepConstantPressureGPU.h:#ifndef HOOMD_TWOSTEPCONSTANTPRESSUREGPU_H
hoomd/md/TwoStepConstantPressureGPU.h:#define HOOMD_TWOSTEPCONSTANTPRESSUREGPU_H
hoomd/md/TwoStepConstantPressureGPU.h:class TwoStepConstantPressureGPU : public TwoStepConstantPressure
hoomd/md/TwoStepConstantPressureGPU.h:    TwoStepConstantPressureGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/md/TwoStepConstantPressureGPU.h:#endif // HOOMD_TWOSTEPCONSTANTPRESSUREGPU_H
hoomd/md/EvaluatorPairLJ1208.h:        //! set CUDA memory hints
hoomd/md/TriangleAreaConservationMeshForceCompute.h:    GPUArray<triangle_area_conservation_param_t> m_params; //!< Parameters
hoomd/md/TriangleAreaConservationMeshForceCompute.h:    GPUArray<Scalar> m_area;                               //!< memory space for area
hoomd/md/ForceDistanceConstraint.h:#include "hoomd/GPUFlags.h"
hoomd/md/ForceDistanceConstraint.h:#include "hoomd/GPUVector.h"
hoomd/md/ForceDistanceConstraint.h:    GPUVector<double> m_cmatrix;  //!< The matrix for the constraint force equation (column-major)
hoomd/md/ForceDistanceConstraint.h:    GPUVector<double> m_cvec;     //!< The vector on the RHS of the constraint equation
hoomd/md/ForceDistanceConstraint.h:    GPUVector<double> m_lagrange; //!< The solution for the lagrange multipliers
hoomd/md/ForceDistanceConstraint.h:    GPUFlags<unsigned int> m_constraint_violated; //!< The id of the violated constraint + 1
hoomd/md/ForceDistanceConstraint.h:    GPUFlags<unsigned int> m_condition; //!< ==1 if sparsity pattern has changed
hoomd/md/ForceDistanceConstraint.h:    GPUVector<int>
hoomd/md/AnisoPotentialPairALJ2GPU.cc:#include "AnisoPotentialPairGPU.h"
hoomd/md/AnisoPotentialPairALJ2GPU.cc:void export_AnisoPotentialPairALJ2DGPU(pybind11::module& m)
hoomd/md/AnisoPotentialPairALJ2GPU.cc:    export_AnisoPotentialPairGPU<EvaluatorPairALJ<2>>(m, "AnisoPotentialPairALJ2DGPU");
hoomd/md/ComputeThermo.h:    All computed values are stored in a GlobalArray so that they can be accessed on the GPU without
hoomd/md/ComputeThermo.h:    //! Get the gpu array of properties
hoomd/md/ComputeThermoGPU.cc:/*! \file ComputeThermoGPU.cc
hoomd/md/ComputeThermoGPU.cc:    \brief Contains code for the ComputeThermoGPU class
hoomd/md/ComputeThermoGPU.cc:#include "ComputeThermoGPU.h"
hoomd/md/ComputeThermoGPU.cc:#include "ComputeThermoGPU.cuh"
hoomd/md/ComputeThermoGPU.cc:#include "hoomd/GPUPartition.cuh"
hoomd/md/ComputeThermoGPU.cc:ComputeThermoGPU::ComputeThermoGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/md/ComputeThermoGPU.cc:    if (!m_exec_conf->isCUDAEnabled())
hoomd/md/ComputeThermoGPU.cc:            << "Creating a ComputeThermoGPU with no GPU in the execution configuration" << endl;
hoomd/md/ComputeThermoGPU.cc:        throw std::runtime_error("Error initializing ComputeThermoGPU");
hoomd/md/ComputeThermoGPU.cc:ComputeThermoGPU::~ComputeThermoGPU()
hoomd/md/ComputeThermoGPU.cc:/*! Computes all thermodynamic properties of the system in one fell swoop, on the GPU.
hoomd/md/ComputeThermoGPU.cc:void ComputeThermoGPU::computeProperties()
hoomd/md/ComputeThermoGPU.cc:    // number of blocks in reduction (round up for every GPU)
hoomd/md/ComputeThermoGPU.cc:        = m_group->getNumMembers() / m_block_size + m_exec_conf->getNumActiveGPUs();
hoomd/md/ComputeThermoGPU.cc:            auto& gpu_map = m_exec_conf->getGPUIds();
hoomd/md/ComputeThermoGPU.cc:            // map scratch array into memory of all GPUs
hoomd/md/ComputeThermoGPU.cc:            for (unsigned int idev = 0; idev < m_exec_conf->getNumActiveGPUs(); ++idev)
hoomd/md/ComputeThermoGPU.cc:                cudaMemAdvise(m_scratch.get(),
hoomd/md/ComputeThermoGPU.cc:                              cudaMemAdviseSetAccessedBy,
hoomd/md/ComputeThermoGPU.cc:                              gpu_map[idev]);
hoomd/md/ComputeThermoGPU.cc:                cudaMemAdvise(m_scratch_pressure_tensor.get(),
hoomd/md/ComputeThermoGPU.cc:                              cudaMemAdviseSetAccessedBy,
hoomd/md/ComputeThermoGPU.cc:                              gpu_map[idev]);
hoomd/md/ComputeThermoGPU.cc:                cudaMemAdvise(m_scratch_rot.get(),
hoomd/md/ComputeThermoGPU.cc:                              cudaMemAdviseSetAccessedBy,
hoomd/md/ComputeThermoGPU.cc:                              gpu_map[idev]);
hoomd/md/ComputeThermoGPU.cc:            CHECK_CUDA_ERROR();
hoomd/md/ComputeThermoGPU.cc:        m_exec_conf->beginMultiGPU();
hoomd/md/ComputeThermoGPU.cc:        // perform the computation on the GPU(s)
hoomd/md/ComputeThermoGPU.cc:        gpu_compute_thermo_partial(d_properties.data,
hoomd/md/ComputeThermoGPU.cc:                                   m_group->getGPUPartition());
hoomd/md/ComputeThermoGPU.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/ComputeThermoGPU.cc:            CHECK_CUDA_ERROR();
hoomd/md/ComputeThermoGPU.cc:        // converge GPUs
hoomd/md/ComputeThermoGPU.cc:        m_exec_conf->endMultiGPU();
hoomd/md/ComputeThermoGPU.cc:        // perform the computation on GPU 0
hoomd/md/ComputeThermoGPU.cc:        gpu_compute_thermo_final(d_properties.data,
hoomd/md/ComputeThermoGPU.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/ComputeThermoGPU.cc:            CHECK_CUDA_ERROR();
hoomd/md/ComputeThermoGPU.cc:void export_ComputeThermoGPU(pybind11::module& m)
hoomd/md/ComputeThermoGPU.cc:    pybind11::class_<ComputeThermoGPU, ComputeThermo, std::shared_ptr<ComputeThermoGPU>>(
hoomd/md/ComputeThermoGPU.cc:        "ComputeThermoGPU")
hoomd/md/constrain.py:            cpp_cls = getattr(self._ext_module, self._cpp_class_name + "GPU")
hoomd/md/OPLSDihedralForceComputeGPU.cc:/*! \file OPLSDihedralForceComputeGPU.cc
hoomd/md/OPLSDihedralForceComputeGPU.cc:    \brief Defines OPLSDihedralForceComputeGPU
hoomd/md/OPLSDihedralForceComputeGPU.cc:#include "OPLSDihedralForceComputeGPU.h"
hoomd/md/OPLSDihedralForceComputeGPU.cc:OPLSDihedralForceComputeGPU::OPLSDihedralForceComputeGPU(std::shared_ptr<SystemDefinition> sysdef)
hoomd/md/OPLSDihedralForceComputeGPU.cc:    // can't run on the GPU if there aren't any GPUs in the execution configuration
hoomd/md/OPLSDihedralForceComputeGPU.cc:    if (!m_exec_conf->isCUDAEnabled())
hoomd/md/OPLSDihedralForceComputeGPU.cc:            << "Creating an OPLSDihedralForceComputeGPU with no GPU in execution configuration"
hoomd/md/OPLSDihedralForceComputeGPU.cc:        throw std::runtime_error("Error initializing OPLSDihedralForceComputeGPU");
hoomd/md/OPLSDihedralForceComputeGPU.cc:/*! Internal method for computing the forces on the GPU.
hoomd/md/OPLSDihedralForceComputeGPU.cc:    \post The force data on the GPU is written with the calculated forces
hoomd/md/OPLSDihedralForceComputeGPU.cc:    Calls gpu_compute_opls_dihedral_forces to do the dirty work.
hoomd/md/OPLSDihedralForceComputeGPU.cc:void OPLSDihedralForceComputeGPU::computeForces(uint64_t timestep)
hoomd/md/OPLSDihedralForceComputeGPU.cc:    ArrayHandle<DihedralData::members_t> d_gpu_dihedral_list(m_dihedral_data->getGPUTable(),
hoomd/md/OPLSDihedralForceComputeGPU.cc:    ArrayHandle<unsigned int> d_dihedrals_ABCD(m_dihedral_data->getGPUPosTable(),
hoomd/md/OPLSDihedralForceComputeGPU.cc:    // run the kernel in parallel on all GPUs
hoomd/md/OPLSDihedralForceComputeGPU.cc:    kernel::gpu_compute_opls_dihedral_forces(d_force.data,
hoomd/md/OPLSDihedralForceComputeGPU.cc:                                             d_gpu_dihedral_list.data,
hoomd/md/OPLSDihedralForceComputeGPU.cc:                                             m_dihedral_data->getGPUTableIndexer().getW(),
hoomd/md/OPLSDihedralForceComputeGPU.cc:    if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/OPLSDihedralForceComputeGPU.cc:        CHECK_CUDA_ERROR();
hoomd/md/OPLSDihedralForceComputeGPU.cc:void export_OPLSDihedralForceComputeGPU(pybind11::module& m)
hoomd/md/OPLSDihedralForceComputeGPU.cc:    pybind11::class_<OPLSDihedralForceComputeGPU,
hoomd/md/OPLSDihedralForceComputeGPU.cc:                     std::shared_ptr<OPLSDihedralForceComputeGPU>>(m, "OPLSDihedralForceComputeGPU")
hoomd/md/minimize/fire.py:            cls = getattr(_md, self._cpp_class_name + "GPU")
hoomd/md/long_range/pppm.py:            cls = hoomd.md._md.PPPMForceComputeGPU
hoomd/md/bond.py:            cpp_cls = getattr(self._ext_module, self._cpp_class_name + "GPU")
hoomd/md/bond.py:            cpp_cls = self._ext_module.BondTablePotentialGPU
hoomd/md/EvaluatorSpecialPairCoulomb.h:    //! Set CUDA memory hints
hoomd/md/PeriodicImproperForceCompute.cc:    GPUArray<periodic_improper_params> params(m_improper_data->getNTypes(), m_exec_conf);
hoomd/md/PPPMForceComputeGPU.h:#ifndef __PPPM_FORCE_COMPUTE_GPU_H__
hoomd/md/PPPMForceComputeGPU.h:#define __PPPM_FORCE_COMPUTE_GPU_H__
hoomd/md/PPPMForceComputeGPU.h:#include "CommunicatorGridGPU.h"
hoomd/md/PPPMForceComputeGPU.h:#include "hoomd/extern/dfftlib/src/dfft_cuda.h"
hoomd/md/PPPMForceComputeGPU.h:class PYBIND11_EXPORT PPPMForceComputeGPU : public PPPMForceCompute
hoomd/md/PPPMForceComputeGPU.h:    PPPMForceComputeGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/md/PPPMForceComputeGPU.h:    virtual ~PPPMForceComputeGPU();
hoomd/md/PPPMForceComputeGPU.h:    std::shared_ptr<Autotuner<1>> m_tuner_reduce_mesh; //!< Autotuner to reduce meshes for multi GPU
hoomd/md/PPPMForceComputeGPU.h:    bool m_cuda_dfft_initialized; //!< True if dfft has been initialized
hoomd/md/PPPMForceComputeGPU.h:    typedef CommunicatorGridGPU<hipfftComplex> CommunicatorGridGPUComplex;
hoomd/md/PPPMForceComputeGPU.h:    std::shared_ptr<CommunicatorGridGPUComplex> m_gpu_grid_comm_forward; //!< Communicate mesh
hoomd/md/PPPMForceComputeGPU.h:    std::shared_ptr<CommunicatorGridGPUComplex>
hoomd/md/PPPMForceComputeGPU.h:        m_gpu_grid_comm_reverse; //!< Communicate fourier mesh
hoomd/md/PPPMForceComputeGPU.h:    GlobalArray<hipfftComplex> m_mesh_scratch; //!< The particle density mesh per GPU, staging array
hoomd/md/PPPMForceComputeGPU.h:    GPUFlags<Scalar> m_sum;                   //!< Sum over fourier mesh values
hoomd/md/PPPMForceComputeGPU.h:#endif // __PPPM_FORCE_COMPUTE_GPU_H__
hoomd/md/TableDihedralForceCompute.cc:    GPUArray<Scalar2> tables(m_table_width, m_dihedral_data->getNTypes(), m_exec_conf);
hoomd/md/alchemy/pair.py:        `hoomd.md.alchemy.pair.LJGauss` does not support execution on GPUs.
hoomd/md/alchemy/pair.py:        `hoomd.md.alchemy.pair._NLJGauss` does not support execution on GPUs.
hoomd/md/alchemy/methods.py:        `hoomd.md.alchemy.methods.NVT` does not support execution on GPUs.
hoomd/md/HelfrichMeshForceCompute.h:    GPUArray<Scalar> m_params;                   //!< Parameters
hoomd/md/EvaluatorPairZBL.h:    // set CUDA memory hints
hoomd/md/PeriodicImproperForceCompute.h:    GPUArray<periodic_improper_params> m_params;
hoomd/md/ActiveForceConstraintCompute.h:    if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/ActiveForceConstraintCompute.h:        CHECK_CUDA_ERROR();
hoomd/md/ComputeThermoHMAGPU.h:/*! \file ComputeThermoHMAGPU.h
hoomd/md/ComputeThermoHMAGPU.h:    \brief Declares a class for computing HMA thermodynamic quantities on the GPU
hoomd/md/ComputeThermoHMAGPU.h:#ifndef __COMPUTE_THERMO_HMA_GPU_H__
hoomd/md/ComputeThermoHMAGPU.h:#define __COMPUTE_THERMO_HMA_GPU_H__
hoomd/md/ComputeThermoHMAGPU.h://! Computes HMA thermodynamic properties of a group of particles on the GPU
hoomd/md/ComputeThermoHMAGPU.h:/*! ComputeThermoHMAGPU is a GPU accelerated implementation of ComputeThermoHMA
hoomd/md/ComputeThermoHMAGPU.h:class PYBIND11_EXPORT ComputeThermoHMAGPU : public ComputeThermoHMA
hoomd/md/ComputeThermoHMAGPU.h:    ComputeThermoHMAGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/md/ComputeThermoHMAGPU.h:    virtual ~ComputeThermoHMAGPU();
hoomd/md/ComputeThermoHMAGPU.h:    hipEvent_t m_event;              //!< CUDA event for synchronization
hoomd/md/HarmonicAngleForceGPU.cu:#include "HarmonicAngleForceGPU.cuh"
hoomd/md/HarmonicAngleForceGPU.cu:/*! \file HarmonicAngleForceGPU.cu
hoomd/md/HarmonicAngleForceGPU.cu:    \brief Defines GPU kernel code for calculating the harmonic angle forces. Used by
hoomd/md/HarmonicAngleForceGPU.cu:   HarmonicAngleForceComputeGPU.
hoomd/md/HarmonicAngleForceGPU.cu://! Kernel for calculating harmonic angle forces on the GPU
hoomd/md/HarmonicAngleForceGPU.cu:    \param n_angles_list List of numbers of angles stored on the GPU
hoomd/md/HarmonicAngleForceGPU.cu:__global__ void gpu_compute_harmonic_angle_forces_kernel(Scalar4* d_force,
hoomd/md/HarmonicAngleForceGPU.cu:    \param box Box dimensions (in GPU format) to use for periodic boundary conditions
hoomd/md/HarmonicAngleForceGPU.cu:    \param atable List of angles stored on the GPU
hoomd/md/HarmonicAngleForceGPU.cu:    \param n_angles_list List of numbers of angles stored on the GPU
hoomd/md/HarmonicAngleForceGPU.cu:hipError_t gpu_compute_harmonic_angle_forces(Scalar4* d_force,
hoomd/md/HarmonicAngleForceGPU.cu:    hipFuncGetAttributes(&attr, (const void*)gpu_compute_harmonic_angle_forces_kernel);
hoomd/md/HarmonicAngleForceGPU.cu:    hipLaunchKernelGGL((gpu_compute_harmonic_angle_forces_kernel),
hoomd/md/EvaluatorPairYukawa.h:        // set CUDA memory hints
hoomd/md/PPPMForceComputeGPU.cc:#include "PPPMForceComputeGPU.h"
hoomd/md/PPPMForceComputeGPU.cc:#include "PPPMForceComputeGPU.cuh"
hoomd/md/PPPMForceComputeGPU.cc:PPPMForceComputeGPU::PPPMForceComputeGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/md/PPPMForceComputeGPU.cc:    m_cuda_dfft_initialized = false;
hoomd/md/PPPMForceComputeGPU.cc:PPPMForceComputeGPU::~PPPMForceComputeGPU()
hoomd/md/PPPMForceComputeGPU.cc:    else if (m_cuda_dfft_initialized)
hoomd/md/PPPMForceComputeGPU.cc:void PPPMForceComputeGPU::initializeFFT()
hoomd/md/PPPMForceComputeGPU.cc:    else if (m_cuda_dfft_initialized)
hoomd/md/PPPMForceComputeGPU.cc:        m_gpu_grid_comm_forward
hoomd/md/PPPMForceComputeGPU.cc:            = std::shared_ptr<CommunicatorGridGPUComplex>(new CommunicatorGridGPUComplex(
hoomd/md/PPPMForceComputeGPU.cc:        m_gpu_grid_comm_reverse
hoomd/md/PPPMForceComputeGPU.cc:            = std::shared_ptr<CommunicatorGridGPUComplex>(new CommunicatorGridGPUComplex(
hoomd/md/PPPMForceComputeGPU.cc:        dfft_cuda_create_plan(&m_dfft_plan_forward,
hoomd/md/PPPMForceComputeGPU.cc:        dfft_cuda_create_plan(&m_dfft_plan_inverse,
hoomd/md/PPPMForceComputeGPU.cc:        m_cuda_dfft_initialized = true;
hoomd/md/PPPMForceComputeGPU.cc:    unsigned int ngpu = m_exec_conf->getNumActiveGPUs();
hoomd/md/PPPMForceComputeGPU.cc:    if (ngpu > 1)
hoomd/md/PPPMForceComputeGPU.cc:        GlobalArray<hipfftComplex> mesh_scratch(mesh_elements * ngpu, m_exec_conf);
hoomd/md/PPPMForceComputeGPU.cc:        auto gpu_map = m_exec_conf->getGPUIds();
hoomd/md/PPPMForceComputeGPU.cc:        for (unsigned int idev = 0; idev < m_exec_conf->getNumActiveGPUs(); ++idev)
hoomd/md/PPPMForceComputeGPU.cc:            cudaMemAdvise(m_mesh_scratch.get() + idev * mesh_elements,
hoomd/md/PPPMForceComputeGPU.cc:                          cudaMemAdviseSetPreferredLocation,
hoomd/md/PPPMForceComputeGPU.cc:                          gpu_map[idev]);
hoomd/md/PPPMForceComputeGPU.cc:            cudaMemPrefetchAsync(m_mesh_scratch.get() + idev * mesh_elements,
hoomd/md/PPPMForceComputeGPU.cc:                                 gpu_map[idev]);
hoomd/md/PPPMForceComputeGPU.cc:            CHECK_CUDA_ERROR();
hoomd/md/PPPMForceComputeGPU.cc:        // accessed by GPU 0
hoomd/md/PPPMForceComputeGPU.cc:        cudaMemAdvise(m_mesh_scratch.get(),
hoomd/md/PPPMForceComputeGPU.cc:                      mesh_elements * ngpu,
hoomd/md/PPPMForceComputeGPU.cc:                      cudaMemAdviseSetAccessedBy,
hoomd/md/PPPMForceComputeGPU.cc:                      gpu_map[0]);
hoomd/md/PPPMForceComputeGPU.cc:        CHECK_CUDA_ERROR();
hoomd/md/PPPMForceComputeGPU.cc:        auto gpu_map = m_exec_conf->getGPUIds();
hoomd/md/PPPMForceComputeGPU.cc:        for (unsigned int idev = 0; idev < m_exec_conf->getNumActiveGPUs(); ++idev)
hoomd/md/PPPMForceComputeGPU.cc:            cudaMemAdvise(m_inv_fourier_mesh_x.get(),
hoomd/md/PPPMForceComputeGPU.cc:                          cudaMemAdviseSetAccessedBy,
hoomd/md/PPPMForceComputeGPU.cc:                          gpu_map[idev]);
hoomd/md/PPPMForceComputeGPU.cc:            cudaMemAdvise(m_inv_fourier_mesh_y.get(),
hoomd/md/PPPMForceComputeGPU.cc:                          cudaMemAdviseSetAccessedBy,
hoomd/md/PPPMForceComputeGPU.cc:                          gpu_map[idev]);
hoomd/md/PPPMForceComputeGPU.cc:            cudaMemAdvise(m_inv_fourier_mesh_z.get(),
hoomd/md/PPPMForceComputeGPU.cc:                          cudaMemAdviseSetAccessedBy,
hoomd/md/PPPMForceComputeGPU.cc:                          gpu_map[idev]);
hoomd/md/PPPMForceComputeGPU.cc:            CHECK_CUDA_ERROR();
hoomd/md/PPPMForceComputeGPU.cc:void PPPMForceComputeGPU::assignParticles()
hoomd/md/PPPMForceComputeGPU.cc:    this->m_exec_conf->beginMultiGPU();
hoomd/md/PPPMForceComputeGPU.cc:    kernel::gpu_assign_particles(m_mesh_points,
hoomd/md/PPPMForceComputeGPU.cc:                                 m_group->getGPUPartition());
hoomd/md/PPPMForceComputeGPU.cc:    if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/PPPMForceComputeGPU.cc:        CHECK_CUDA_ERROR();
hoomd/md/PPPMForceComputeGPU.cc:    this->m_exec_conf->endMultiGPU();
hoomd/md/PPPMForceComputeGPU.cc:    if (m_exec_conf->getNumActiveGPUs() > 1)
hoomd/md/PPPMForceComputeGPU.cc:        kernel::gpu_reduce_meshes((unsigned int)m_mesh.getNumElements(),
hoomd/md/PPPMForceComputeGPU.cc:                                  m_exec_conf->getNumActiveGPUs(),
hoomd/md/PPPMForceComputeGPU.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/PPPMForceComputeGPU.cc:            CHECK_CUDA_ERROR();
hoomd/md/PPPMForceComputeGPU.cc:void PPPMForceComputeGPU::updateMeshes()
hoomd/md/PPPMForceComputeGPU.cc:        m_gpu_grid_comm_forward->communicate(m_mesh);
hoomd/md/PPPMForceComputeGPU.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/PPPMForceComputeGPU.cc:            dfft_cuda_check_errors(&m_dfft_plan_forward, 1);
hoomd/md/PPPMForceComputeGPU.cc:            dfft_cuda_check_errors(&m_dfft_plan_forward, 0);
hoomd/md/PPPMForceComputeGPU.cc:        dfft_cuda_execute(d_mesh.data + m_ghost_offset,
hoomd/md/PPPMForceComputeGPU.cc:        kernel::gpu_update_meshes(m_n_inner_cells,
hoomd/md/PPPMForceComputeGPU.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/PPPMForceComputeGPU.cc:            CHECK_CUDA_ERROR();
hoomd/md/PPPMForceComputeGPU.cc:        m_exec_conf->beginMultiGPU();
hoomd/md/PPPMForceComputeGPU.cc:        m_exec_conf->endMultiGPU();
hoomd/md/PPPMForceComputeGPU.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/PPPMForceComputeGPU.cc:            dfft_cuda_check_errors(&m_dfft_plan_inverse, 1);
hoomd/md/PPPMForceComputeGPU.cc:            dfft_cuda_check_errors(&m_dfft_plan_inverse, 0);
hoomd/md/PPPMForceComputeGPU.cc:        dfft_cuda_execute(d_inv_fourier_mesh_x.data + m_ghost_offset,
hoomd/md/PPPMForceComputeGPU.cc:        dfft_cuda_execute(d_inv_fourier_mesh_y.data + m_ghost_offset,
hoomd/md/PPPMForceComputeGPU.cc:        dfft_cuda_execute(d_inv_fourier_mesh_z.data + m_ghost_offset,
hoomd/md/PPPMForceComputeGPU.cc:        m_gpu_grid_comm_reverse->communicate(m_inv_fourier_mesh_x);
hoomd/md/PPPMForceComputeGPU.cc:        m_gpu_grid_comm_reverse->communicate(m_inv_fourier_mesh_y);
hoomd/md/PPPMForceComputeGPU.cc:        m_gpu_grid_comm_reverse->communicate(m_inv_fourier_mesh_z);
hoomd/md/PPPMForceComputeGPU.cc:void PPPMForceComputeGPU::interpolateForces()
hoomd/md/PPPMForceComputeGPU.cc:    m_exec_conf->beginMultiGPU();
hoomd/md/PPPMForceComputeGPU.cc:    kernel::gpu_compute_forces(m_pdata->getN(),
hoomd/md/PPPMForceComputeGPU.cc:                               m_group->getGPUPartition(),
hoomd/md/PPPMForceComputeGPU.cc:                               m_pdata->getGPUPartition(),
hoomd/md/PPPMForceComputeGPU.cc:    if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/PPPMForceComputeGPU.cc:        CHECK_CUDA_ERROR();
hoomd/md/PPPMForceComputeGPU.cc:    m_exec_conf->endMultiGPU();
hoomd/md/PPPMForceComputeGPU.cc:void PPPMForceComputeGPU::computeVirial()
hoomd/md/PPPMForceComputeGPU.cc:    kernel::gpu_compute_mesh_virial(m_n_inner_cells,
hoomd/md/PPPMForceComputeGPU.cc:    if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/PPPMForceComputeGPU.cc:        CHECK_CUDA_ERROR();
hoomd/md/PPPMForceComputeGPU.cc:        kernel::gpu_compute_virial(m_n_inner_cells,
hoomd/md/PPPMForceComputeGPU.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/PPPMForceComputeGPU.cc:            CHECK_CUDA_ERROR();
hoomd/md/PPPMForceComputeGPU.cc:Scalar PPPMForceComputeGPU::computePE()
hoomd/md/PPPMForceComputeGPU.cc:    kernel::gpu_compute_pe(m_n_inner_cells,
hoomd/md/PPPMForceComputeGPU.cc:    if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/PPPMForceComputeGPU.cc:        CHECK_CUDA_ERROR();
hoomd/md/PPPMForceComputeGPU.cc:void PPPMForceComputeGPU::computeInfluenceFunction()
hoomd/md/PPPMForceComputeGPU.cc:    kernel::gpu_compute_influence_function(m_mesh_points,
hoomd/md/PPPMForceComputeGPU.cc:    if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/PPPMForceComputeGPU.cc:        CHECK_CUDA_ERROR();
hoomd/md/PPPMForceComputeGPU.cc:void PPPMForceComputeGPU::fixExclusions()
hoomd/md/PPPMForceComputeGPU.cc:    kernel::gpu_fix_exclusions(d_force.data,
hoomd/md/PPPMForceComputeGPU.cc:    if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/PPPMForceComputeGPU.cc:        CHECK_CUDA_ERROR();
hoomd/md/PPPMForceComputeGPU.cc:void export_PPPMForceComputeGPU(pybind11::module& m)
hoomd/md/PPPMForceComputeGPU.cc:    pybind11::class_<PPPMForceComputeGPU, PPPMForceCompute, std::shared_ptr<PPPMForceComputeGPU>>(
hoomd/md/PPPMForceComputeGPU.cc:        "PPPMForceComputeGPU")
hoomd/md/ForceCompositeGPU.cuh:#include "hoomd/GPUPartition.cuh"
hoomd/md/ForceCompositeGPU.cuh:    \brief Defines GPU driver functions for the composite particle integration on the GPU.
hoomd/md/ForceCompositeGPU.cuh:hipError_t gpu_rigid_force(Scalar4* d_force,
hoomd/md/ForceCompositeGPU.cuh:                           const GPUPartition& gpu_partition);
hoomd/md/ForceCompositeGPU.cuh:hipError_t gpu_rigid_virial(Scalar* d_virial,
hoomd/md/ForceCompositeGPU.cuh:                            const GPUPartition& gpu_partition);
hoomd/md/ForceCompositeGPU.cuh:void gpu_update_composite(unsigned int N,
hoomd/md/ForceCompositeGPU.cuh:                          const GPUPartition& gpu_partition);
hoomd/md/ForceCompositeGPU.cuh:hipError_t gpu_find_rigid_centers(const unsigned int* d_body,
hoomd/md/FIREEnergyMinimizerGPU.cuh:#ifndef __FIRE_ENERGY_MINIMIZER_GPU_CUH__
hoomd/md/FIREEnergyMinimizerGPU.cuh:#define __FIRE_ENERGY_MINIMIZER_GPU_CUH__
hoomd/md/FIREEnergyMinimizerGPU.cuh:/*! \file FIREEnergyMinimizerGPU.cuh
hoomd/md/FIREEnergyMinimizerGPU.cuh:    \brief Defines the interface to GPU kernel drivers used by FIREEnergyMinimizerGPU.
hoomd/md/FIREEnergyMinimizerGPU.cuh://! Kernel driver for zeroing velocities called by FIREEnergyMinimizerGPU
hoomd/md/FIREEnergyMinimizerGPU.cuh:hipError_t gpu_fire_zero_v(Scalar4* d_vel, unsigned int* d_group_members, unsigned int group_size);
hoomd/md/FIREEnergyMinimizerGPU.cuh:gpu_fire_zero_angmom(Scalar4* d_angmom, unsigned int* d_group_members, unsigned int group_size);
hoomd/md/FIREEnergyMinimizerGPU.cuh://! Kernel driver for summing the potential energy called by FIREEnergyMinimizerGPU
hoomd/md/FIREEnergyMinimizerGPU.cuh:hipError_t gpu_fire_compute_sum_pe(unsigned int* d_group_members,
hoomd/md/FIREEnergyMinimizerGPU.cuh://! Kernel driver for summing over P, vsq, and asq called by FIREEnergyMinimizerGPU
hoomd/md/FIREEnergyMinimizerGPU.cuh:hipError_t gpu_fire_compute_sum_all(const unsigned int N,
hoomd/md/FIREEnergyMinimizerGPU.cuh:hipError_t gpu_fire_compute_sum_all_angular(const unsigned int N,
hoomd/md/FIREEnergyMinimizerGPU.cuh://! Kernel driver for updating the velocities called by FIREEnergyMinimizerGPU
hoomd/md/FIREEnergyMinimizerGPU.cuh:hipError_t gpu_fire_update_v(Scalar4* d_vel,
hoomd/md/FIREEnergyMinimizerGPU.cuh:hipError_t gpu_fire_update_angmom(const Scalar4* d_net_torque,
hoomd/md/FIREEnergyMinimizerGPU.cuh:#endif //__FIRE_ENERGY_MINIMIZER_GPU_CUH__
hoomd/md/EvaluatorPairExpandedGaussian.h:        // set CUDA memory hints
hoomd/md/ComputeThermoGPU.h:/*! \file ComputeThermoGPU.h
hoomd/md/ComputeThermoGPU.h:    \brief Declares a class for computing thermodynamic quantities on the GPU
hoomd/md/ComputeThermoGPU.h:#ifndef __COMPUTE_THERMO_GPU_H__
hoomd/md/ComputeThermoGPU.h:#define __COMPUTE_THERMO_GPU_H__
hoomd/md/ComputeThermoGPU.h://! Computes thermodynamic properties of a group of particles on the GPU
hoomd/md/ComputeThermoGPU.h:/*! ComputeThermoGPU is a GPU accelerated implementation of ComputeThermo
hoomd/md/ComputeThermoGPU.h:class PYBIND11_EXPORT ComputeThermoGPU : public ComputeThermo
hoomd/md/ComputeThermoGPU.h:    ComputeThermoGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/md/ComputeThermoGPU.h:    virtual ~ComputeThermoGPU();
hoomd/md/ComputeThermoGPU.h:    hipEvent_t m_event;        //!< CUDA event for synchronization
hoomd/md/angle.py:            cpp_cls = getattr(self._ext_module, self._cpp_class_name + "GPU")
hoomd/md/angle.py:            cpp_cls = _md.TableAngleForceComputeGPU
hoomd/md/ActiveForceComputeGPU.h:/*! \file ActiveForceComputeGPU.h
hoomd/md/ActiveForceComputeGPU.h:    \brief Declares a class for computing active forces on the GPU
hoomd/md/ActiveForceComputeGPU.h:#ifndef __ACTIVEFORCECOMPUTE_GPU_H__
hoomd/md/ActiveForceComputeGPU.h:#define __ACTIVEFORCECOMPUTE_GPU_H__
hoomd/md/ActiveForceComputeGPU.h://! Adds an active force to a number of particles on the GPU
hoomd/md/ActiveForceComputeGPU.h:class PYBIND11_EXPORT ActiveForceComputeGPU : public ActiveForceCompute
hoomd/md/ActiveForceComputeGPU.h:    ActiveForceComputeGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/md/TwoStepNVEGPU.cu:#include "TwoStepNVEGPU.cuh"
hoomd/md/TwoStepNVEGPU.cu:/*! \file TwoStepNVEGPU.cu
hoomd/md/TwoStepNVEGPU.cu:    \brief Defines GPU kernel code for NVE integration on the GPU. Used by TwoStepNVEGPU.
hoomd/md/TwoStepNVEGPU.cu:__global__ void gpu_nve_step_one_kernel(Scalar4* d_pos,
hoomd/md/TwoStepNVEGPU.cu:    See gpu_nve_step_one_kernel() for full documentation, this function is just a driver.
hoomd/md/TwoStepNVEGPU.cu:hipError_t gpu_nve_step_one(Scalar4* d_pos,
hoomd/md/TwoStepNVEGPU.cu:                            const GPUPartition& gpu_partition,
hoomd/md/TwoStepNVEGPU.cu:    hipFuncGetAttributes(&attr, (const void*)gpu_nve_step_one_kernel);
hoomd/md/TwoStepNVEGPU.cu:    // iterate over active GPUs in reverse, to end up on first GPU when returning from this function
hoomd/md/TwoStepNVEGPU.cu:    for (int idev = gpu_partition.getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/md/TwoStepNVEGPU.cu:        auto range = gpu_partition.getRangeAndSetGPU(idev);
hoomd/md/TwoStepNVEGPU.cu:        hipLaunchKernelGGL((gpu_nve_step_one_kernel),
hoomd/md/TwoStepNVEGPU.cu:__global__ void gpu_nve_angular_step_one_kernel(Scalar4* d_orientation,
hoomd/md/TwoStepNVEGPU.cu:hipError_t gpu_nve_angular_step_one(Scalar4* d_orientation,
hoomd/md/TwoStepNVEGPU.cu:                                    const GPUPartition& gpu_partition,
hoomd/md/TwoStepNVEGPU.cu:    hipFuncGetAttributes(&attr, (const void*)gpu_nve_angular_step_one_kernel);
hoomd/md/TwoStepNVEGPU.cu:    // iterate over active GPUs in reverse, to end up on first GPU when returning from this function
hoomd/md/TwoStepNVEGPU.cu:    for (int idev = gpu_partition.getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/md/TwoStepNVEGPU.cu:        auto range = gpu_partition.getRangeAndSetGPU(idev);
hoomd/md/TwoStepNVEGPU.cu:        hipLaunchKernelGGL((gpu_nve_angular_step_one_kernel),
hoomd/md/TwoStepNVEGPU.cu:    This kernel is implemented in a very similar manner to gpu_nve_step_one_kernel(), see it for
hoomd/md/TwoStepNVEGPU.cu:__global__ void gpu_nve_step_two_kernel(Scalar4* d_vel,
hoomd/md/TwoStepNVEGPU.cu:    This is just a driver for gpu_nve_step_two_kernel(), see it for details.
hoomd/md/TwoStepNVEGPU.cu:hipError_t gpu_nve_step_two(Scalar4* d_vel,
hoomd/md/TwoStepNVEGPU.cu:                            const GPUPartition& gpu_partition,
hoomd/md/TwoStepNVEGPU.cu:    hipFuncGetAttributes(&attr, (const void*)gpu_nve_step_two_kernel);
hoomd/md/TwoStepNVEGPU.cu:    // iterate over active GPUs in reverse, to end up on first GPU when returning from this function
hoomd/md/TwoStepNVEGPU.cu:    for (int idev = gpu_partition.getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/md/TwoStepNVEGPU.cu:        auto range = gpu_partition.getRangeAndSetGPU(idev);
hoomd/md/TwoStepNVEGPU.cu:        hipLaunchKernelGGL((gpu_nve_step_two_kernel),
hoomd/md/TwoStepNVEGPU.cu:__global__ void gpu_nve_angular_step_two_kernel(const Scalar4* d_orientation,
hoomd/md/TwoStepNVEGPU.cu:hipError_t gpu_nve_angular_step_two(const Scalar4* d_orientation,
hoomd/md/TwoStepNVEGPU.cu:                                    const GPUPartition& gpu_partition,
hoomd/md/TwoStepNVEGPU.cu:    hipFuncGetAttributes(&attr, (const void*)gpu_nve_angular_step_two_kernel);
hoomd/md/TwoStepNVEGPU.cu:    // iterate over active GPUs in reverse, to end up on first GPU when returning from this function
hoomd/md/TwoStepNVEGPU.cu:    for (int idev = gpu_partition.getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/md/TwoStepNVEGPU.cu:        auto range = gpu_partition.getRangeAndSetGPU(idev);
hoomd/md/TwoStepNVEGPU.cu:        hipLaunchKernelGGL((gpu_nve_angular_step_two_kernel),
hoomd/md/EvaluatorPairDipole.h:        //! Set CUDA memory hints
hoomd/md/EvaluatorPairDipole.h:        //! Attach managed memory to CUDA stream
hoomd/md/AnisoPotentialPairALJ2GPUKernel.cu:#include "AnisoPotentialPairGPU.cuh"
hoomd/md/AnisoPotentialPairALJ2GPUKernel.cu:gpu_compute_pair_aniso_forces<EvaluatorPairALJ<2>>(
hoomd/md/EvaluatorPairExpandedLJ.h:        //! Set CUDA memory hints
hoomd/md/EvaluatorSpecialPairLJ.h:    //! Set CUDA memory hints
hoomd/md/PotentialBondGPU.h:#ifndef __POTENTIAL_BOND_GPU_H__
hoomd/md/PotentialBondGPU.h:#define __POTENTIAL_BOND_GPU_H__
hoomd/md/PotentialBondGPU.h:#include "PotentialBondGPU.cuh"
hoomd/md/PotentialBondGPU.h:/*! \file PotentialBondGPU.h
hoomd/md/PotentialBondGPU.h:    \brief Defines the template class for standard bond potentials on the GPU
hoomd/md/PotentialBondGPU.h://! Template class for computing bond potentials on the GPU
hoomd/md/PotentialBondGPU.h:    \sa export_PotentialBondGPU()
hoomd/md/PotentialBondGPU.h:class PotentialBondGPU : public PotentialBond<evaluator, Bonds>
hoomd/md/PotentialBondGPU.h:    PotentialBondGPU(std::shared_ptr<SystemDefinition> sysdef);
hoomd/md/PotentialBondGPU.h:    PotentialBondGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/md/PotentialBondGPU.h:    virtual ~PotentialBondGPU() { }
hoomd/md/PotentialBondGPU.h:    GPUArray<unsigned int> m_flags;        //!< Flags set during the kernel execution
hoomd/md/PotentialBondGPU.h:PotentialBondGPU<evaluator, Bonds>::PotentialBondGPU(std::shared_ptr<SystemDefinition> sysdef)
hoomd/md/PotentialBondGPU.h:    // can't run on the GPU if there aren't any GPUs in the execution configuration
hoomd/md/PotentialBondGPU.h:    if (!this->m_exec_conf->isCUDAEnabled())
hoomd/md/PotentialBondGPU.h:            << "Creating a PotentialBondGPU with no GPU in the execution configuration"
hoomd/md/PotentialBondGPU.h:        throw std::runtime_error("Error initializing PotentialBondGPU");
hoomd/md/PotentialBondGPU.h:    GPUArray<typename evaluator::param_type> params(this->m_bond_data->getNTypes(),
hoomd/md/PotentialBondGPU.h:    // allocate flags storage on the GPU
hoomd/md/PotentialBondGPU.h:    GPUArray<unsigned int> flags(1, this->m_exec_conf);
hoomd/md/PotentialBondGPU.h:PotentialBondGPU<evaluator, Bonds>::PotentialBondGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/md/PotentialBondGPU.h:    // can't run on the GPU if there aren't any GPUs in the execution configuration
hoomd/md/PotentialBondGPU.h:    if (!this->m_exec_conf->isCUDAEnabled())
hoomd/md/PotentialBondGPU.h:            << "Creating a PotentialMeshBondGPU with no GPU in the execution configuration"
hoomd/md/PotentialBondGPU.h:        throw std::runtime_error("Error initializing PotentialMeshBondGPU");
hoomd/md/PotentialBondGPU.h:    GPUArray<typename evaluator::param_type> params(this->m_bond_data->getNTypes(),
hoomd/md/PotentialBondGPU.h:    // allocate flags storage on the GPU
hoomd/md/PotentialBondGPU.h:    GPUArray<unsigned int> flags(1, this->m_exec_conf);
hoomd/md/PotentialBondGPU.h:void PotentialBondGPU<evaluator, Bonds>::computeForces(uint64_t timestep)
hoomd/md/PotentialBondGPU.h:        const GPUArray<typename Bonds::members_t>& gpu_bond_list = this->m_bond_data->getGPUTable();
hoomd/md/PotentialBondGPU.h:        const Index2D& gpu_table_indexer = this->m_bond_data->getGPUTableIndexer();
hoomd/md/PotentialBondGPU.h:        ArrayHandle<typename Bonds::members_t> d_gpu_bondlist(gpu_bond_list,
hoomd/md/PotentialBondGPU.h:        ArrayHandle<unsigned int> d_gpu_bond_pos_list(this->m_bond_data->getGPUPosTable(),
hoomd/md/PotentialBondGPU.h:        ArrayHandle<unsigned int> d_gpu_n_bonds(this->m_bond_data->getNGroupsArray(),
hoomd/md/PotentialBondGPU.h:        kernel::gpu_compute_bond_forces<evaluator, Bonds::size>(
hoomd/md/PotentialBondGPU.h:                                             d_gpu_bondlist.data,
hoomd/md/PotentialBondGPU.h:                                             gpu_table_indexer,
hoomd/md/PotentialBondGPU.h:                                             d_gpu_bond_pos_list.data,
hoomd/md/PotentialBondGPU.h:                                             d_gpu_n_bonds.data,
hoomd/md/PotentialBondGPU.h:    if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/PotentialBondGPU.h:        CHECK_CUDA_ERROR();
hoomd/md/PotentialBondGPU.h:template<class T> void export_PotentialBondGPU(pybind11::module& m, const std::string& name)
hoomd/md/PotentialBondGPU.h:    pybind11::class_<PotentialBondGPU<T, BondData>,
hoomd/md/PotentialBondGPU.h:                     std::shared_ptr<PotentialBondGPU<T, BondData>>>(m, name.c_str())
hoomd/md/PotentialBondGPU.h:template<class T> void export_PotentialMeshBondGPU(pybind11::module& m, const std::string& name)
hoomd/md/PotentialBondGPU.h:    pybind11::class_<PotentialBondGPU<T, MeshBondData>,
hoomd/md/PotentialBondGPU.h:                     std::shared_ptr<PotentialBondGPU<T, MeshBondData>>>(m, name.c_str())
hoomd/md/PotentialBondGPU.h:#endif // __POTENTIAL_PAIR_GPU_H__
hoomd/md/EvaluatorPairLJGauss.h:        //! Set CUDA memory hints
hoomd/md/EvaluatorPairMoliere.h:        // set CUDA memory hints
hoomd/md/EvaluatorPairBuckingham.h:        //! set CUDA memory hint
hoomd/md/EvaluatorPairLJ0804.h:        //! Set CUDA memory hints
hoomd/md/PotentialPairGPUKernel.cu.inc:#include "hoomd/md/PotentialPairGPU.cuh"
hoomd/md/PotentialPairGPUKernel.cu.inc:gpu_compute_pair_forces<EVALUATOR_CLASS>(const pair_args_t& pair_args,
hoomd/md/TwoStepBDGPU.cc:#include "TwoStepBDGPU.h"
hoomd/md/TwoStepBDGPU.cc:#include "TwoStepBDGPU.cuh"
hoomd/md/TwoStepBDGPU.cc:TwoStepBDGPU::TwoStepBDGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/md/TwoStepBDGPU.cc:    if (!m_exec_conf->isCUDAEnabled())
hoomd/md/TwoStepBDGPU.cc:        throw std::runtime_error("Cannot create TwoStepBDGPU on a CPU device.");
hoomd/md/TwoStepBDGPU.cc:void TwoStepBDGPU::integrateStepOne(uint64_t timestep)
hoomd/md/TwoStepBDGPU.cc:        auto& gpu_map = m_exec_conf->getGPUIds();
hoomd/md/TwoStepBDGPU.cc:        for (unsigned int idev = 0; idev < m_exec_conf->getNumActiveGPUs(); ++idev)
hoomd/md/TwoStepBDGPU.cc:            cudaMemPrefetchAsync(m_gamma.get(),
hoomd/md/TwoStepBDGPU.cc:                                 gpu_map[idev]);
hoomd/md/TwoStepBDGPU.cc:            cudaMemPrefetchAsync(m_gamma_r.get(),
hoomd/md/TwoStepBDGPU.cc:                                 gpu_map[idev]);
hoomd/md/TwoStepBDGPU.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/TwoStepBDGPU.cc:            CHECK_CUDA_ERROR();
hoomd/md/TwoStepBDGPU.cc:    m_exec_conf->beginMultiGPU();
hoomd/md/TwoStepBDGPU.cc:    // perform the update on the GPU
hoomd/md/TwoStepBDGPU.cc:    gpu_brownian_step_one(d_pos.data,
hoomd/md/TwoStepBDGPU.cc:                          m_group->getGPUPartition());
hoomd/md/TwoStepBDGPU.cc:    if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/TwoStepBDGPU.cc:        CHECK_CUDA_ERROR();
hoomd/md/TwoStepBDGPU.cc:    m_exec_conf->endMultiGPU();
hoomd/md/TwoStepBDGPU.cc:    \post particle velocities are moved forward to timestep+1 on the GPU
hoomd/md/TwoStepBDGPU.cc:void TwoStepBDGPU::integrateStepTwo(uint64_t timestep)
hoomd/md/TwoStepBDGPU.cc:void export_TwoStepBDGPU(pybind11::module& m)
hoomd/md/TwoStepBDGPU.cc:    pybind11::class_<TwoStepBDGPU, TwoStepBD, std::shared_ptr<TwoStepBDGPU>>(m, "TwoStepBDGPU")
hoomd/md/FIREEnergyMinimizerGPU.h:#ifndef __FIRE_ENERGY_MINIMIZER_GPU_H__
hoomd/md/FIREEnergyMinimizerGPU.h:#define __FIRE_ENERGY_MINIMIZER_GPU_H__
hoomd/md/FIREEnergyMinimizerGPU.h:class PYBIND11_EXPORT FIREEnergyMinimizerGPU : public FIREEnergyMinimizer
hoomd/md/FIREEnergyMinimizerGPU.h:    FIREEnergyMinimizerGPU(std::shared_ptr<SystemDefinition>, Scalar);
hoomd/md/FIREEnergyMinimizerGPU.h:    virtual ~FIREEnergyMinimizerGPU() { }
hoomd/md/FIREEnergyMinimizerGPU.h:    GPUVector<Scalar> m_partial_sum1; //!< memory space for partial sum over P and E
hoomd/md/FIREEnergyMinimizerGPU.h:    GPUVector<Scalar> m_partial_sum2; //!< memory space for partial sum over vsq
hoomd/md/FIREEnergyMinimizerGPU.h:    GPUVector<Scalar> m_partial_sum3; //!< memory space for partial sum over asq
hoomd/md/FIREEnergyMinimizerGPU.h:    GPUArray<Scalar> m_sum;           //!< memory space for sum over E
hoomd/md/FIREEnergyMinimizerGPU.h:    GPUArray<Scalar> m_sum3;          //!< memory space for the sum over P, vsq, asq
hoomd/md/FIREEnergyMinimizerGPU.h:#endif // #ifndef __FIRE_ENERGY_MINIMIZER_GPU_H__
hoomd/md/PeriodicImproperForceGPU.cu:#include "PeriodicImproperForceGPU.cuh"
hoomd/md/PeriodicImproperForceGPU.cu://! Kernel for calculating periodic improper forces on the GPU
hoomd/md/PeriodicImproperForceGPU.cu:gpu_compute_periodic_improper_forces_kernel(Scalar4* d_force,
hoomd/md/PeriodicImproperForceGPU.cu:    \param d_pos particle positions on the GPU
hoomd/md/PeriodicImproperForceGPU.cu:    \param box Box dimensions (in GPU format) to use for periodic boundary conditions
hoomd/md/PeriodicImproperForceGPU.cu:hipError_t gpu_compute_periodic_improper_forces(Scalar4* d_force,
hoomd/md/PeriodicImproperForceGPU.cu:    hipFuncGetAttributes(&attr, (const void*)gpu_compute_periodic_improper_forces_kernel);
hoomd/md/PeriodicImproperForceGPU.cu:    hipLaunchKernelGGL((gpu_compute_periodic_improper_forces_kernel),
hoomd/md/AreaConservationMeshForceComputeGPU.h:#include "AreaConservationMeshForceComputeGPU.cuh"
hoomd/md/AreaConservationMeshForceComputeGPU.h:/*! \file AreaConservationMeshForceComputeGPU.h
hoomd/md/AreaConservationMeshForceComputeGPU.h:    \brief Declares a class for computing area constraint forces on the GPU
hoomd/md/AreaConservationMeshForceComputeGPU.h:#ifndef __AREACONSERVATIONMESHFORCECOMPUTE_GPU_H__
hoomd/md/AreaConservationMeshForceComputeGPU.h:#define __AREACONSERVATIONMESHFORCECOMPUTE_GPU_H__
hoomd/md/AreaConservationMeshForceComputeGPU.h://! Computes area energy forces on the mesh on the GPU
hoomd/md/AreaConservationMeshForceComputeGPU.h:class PYBIND11_EXPORT AreaConservationMeshForceComputeGPU : public AreaConservationMeshForceCompute
hoomd/md/AreaConservationMeshForceComputeGPU.h:    AreaConservationMeshForceComputeGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/md/AreaConservationMeshForceComputeGPU.h:    GPUArray<Scalar> m_partial_sum; //!< memory space for partial sum over area
hoomd/md/AreaConservationMeshForceComputeGPU.h:    GPUArray<Scalar> m_sum;         //!< memory space for sum over area
hoomd/md/AreaConservationMeshForceComputeGPU.h://! Exports the AreaConservationMeshForceComputeGPU class to python
hoomd/md/AreaConservationMeshForceComputeGPU.h:void export_AreaConservationMeshForceComputeGPU(pybind11::module& m);
hoomd/md/HarmonicAngleForceComputeGPU.h:#include "HarmonicAngleForceGPU.cuh"
hoomd/md/HarmonicAngleForceComputeGPU.h:/*! \file HarmonicAngleForceComputeGPU.h
hoomd/md/HarmonicAngleForceComputeGPU.h:    \brief Declares the HarmonicAngleForceGPU class
hoomd/md/HarmonicAngleForceComputeGPU.h:#ifndef __HARMONICANGLEFORCECOMPUTEGPU_H__
hoomd/md/HarmonicAngleForceComputeGPU.h:#define __HARMONICANGLEFORCECOMPUTEGPU_H__
hoomd/md/HarmonicAngleForceComputeGPU.h://! Implements the harmonic angle force calculation on the GPU
hoomd/md/HarmonicAngleForceComputeGPU.h:/*! HarmonicAngleForceComputeGPU implements the same calculations as HarmonicAngleForceCompute,
hoomd/md/HarmonicAngleForceComputeGPU.h:    but executing on the GPU.
hoomd/md/HarmonicAngleForceComputeGPU.h:    \a m_gpu_params. They are stored as Scalar2's with the \a x component being K and the
hoomd/md/HarmonicAngleForceComputeGPU.h:    The GPU kernel can be found in angleforce_kernel.cu.
hoomd/md/HarmonicAngleForceComputeGPU.h:class PYBIND11_EXPORT HarmonicAngleForceComputeGPU : public HarmonicAngleForceCompute
hoomd/md/HarmonicAngleForceComputeGPU.h:    HarmonicAngleForceComputeGPU(std::shared_ptr<SystemDefinition> sysdef);
hoomd/md/HarmonicAngleForceComputeGPU.h:    ~HarmonicAngleForceComputeGPU();
hoomd/md/HarmonicAngleForceComputeGPU.h:    GPUArray<Scalar2> m_params;            //!< Parameters stored on the GPU
hoomd/md/CommunicatorGridGPU.cuh:void gpu_gridcomm_scatter_send_cells(unsigned int n_send_cells,
hoomd/md/CommunicatorGridGPU.cuh:void gpu_gridcomm_scatter_add_recv_cells(unsigned int n_unique_cell_recvs,
hoomd/md/export_PotentialBondGPU.cc.inc:#include "hoomd/md/PotentialBondGPU.h"
hoomd/md/export_PotentialBondGPU.cc.inc:#define EXPORT_FUNCTION export_PotentialBond@_bond@GPU
hoomd/md/export_PotentialBondGPU.cc.inc:    export_PotentialBondGPU<EVALUATOR_CLASS>(m, "PotentialBond@_bond@GPU");
hoomd/md/ComputeThermoHMA.h:#include "hoomd/GPUArray.h"
hoomd/md/ComputeThermoHMA.h:    All computed values are stored in a GPUArray so that they can be accessed on the GPU without
hoomd/md/ComputeThermoHMA.h:    Computed quantities available in the GPUArray:
hoomd/md/ComputeThermoHMA.h:    //! Get the gpu array of properties
hoomd/md/ComputeThermoHMA.h:    const GPUArray<Scalar>& getProperties()
hoomd/md/ComputeThermoHMA.h:    GPUArray<Scalar> m_properties;          //!< Stores the computed properties
hoomd/md/pair/pair.py:            cls = getattr(self._ext_module, self._cpp_class_name + "GPU")
hoomd/md/ComputeThermoTypes.h:    \brief Data structures common to both CPU and GPU implementations of ComputeThermo
hoomd/md/ComputeThermoTypes.h://! Enum for indexing the GPUArray of computed values
hoomd/md/ComputeThermoTypes.h:        translational_kinetic_energy = 0, //!< Index for the kinetic energy in the GPUArray
hoomd/md/ComputeThermoTypes.h:        potential_energy,                 //!< Index for the potential energy in the GPUArray
hoomd/md/ComputeThermoTypes.h:        pressure_xx,   //!< Index for the xx component of the pressure tensor in the GPUArray
hoomd/md/ComputeThermoTypes.h:        pressure_xy,   //!< Index for the xy component of the pressure tensor in the GPUArray
hoomd/md/ComputeThermoTypes.h:        pressure_xz,   //!< Index for the xz component of the pressure tensor in the GPUArray
hoomd/md/ComputeThermoTypes.h:        pressure_yy,   //!< Index for the yy component of the pressure tensor in the GPUArray
hoomd/md/ComputeThermoTypes.h:        pressure_yz,   //!< Index for the yz component of the pressure tensor in the GPUArray
hoomd/md/ComputeThermoTypes.h:        pressure_zz,   //!< Index for the zz component of the pressure tensor in the GPUArray
hoomd/md/VolumeConservationMeshForceCompute.h:    GPUArray<volume_conservation_param_t> m_params; //!< Parameters
hoomd/md/VolumeConservationMeshForceCompute.h:    GPUArray<Scalar> m_volume; //!< memory space for volume
hoomd/md/GJK_SV.h:        // TODO: Use constexpr to ensure this code path is not compiled once we can bump GPU builds
hoomd/md/GJK_SV.h: *  algorithm (even without the backup procedure) when run on modern GPUs. A
hoomd/md/GJK_SV.h:        // in one of the unused slots. We skip this check on the GPU because
hoomd/md/EvaluatorPairMorse.h:        // CUDA memory hints
hoomd/md/ForceComposite.h:#include "hoomd/GPUPartition.cuh"
hoomd/md/PotentialExternalGPU.h:#include "PotentialExternalGPU.cuh"
hoomd/md/PotentialExternalGPU.h:/*! \file PotentialExternalGPU.h
hoomd/md/PotentialExternalGPU.h:    \brief Declares a class for computing an external potential field on the GPU
hoomd/md/PotentialExternalGPU.h:#ifndef __POTENTIAL_EXTERNAL_GPU_H__
hoomd/md/PotentialExternalGPU.h:#define __POTENTIAL_EXTERNAL_GPU_H__
hoomd/md/PotentialExternalGPU.h:template<class evaluator> class PotentialExternalGPU : public PotentialExternal<evaluator>
hoomd/md/PotentialExternalGPU.h:    PotentialExternalGPU(std::shared_ptr<SystemDefinition> sysdef);
hoomd/md/PotentialExternalGPU.h:PotentialExternalGPU<evaluator>::PotentialExternalGPU(std::shared_ptr<SystemDefinition> sysdef)
hoomd/md/PotentialExternalGPU.h:template<class evaluator> void PotentialExternalGPU<evaluator>::computeForces(uint64_t timestep)
hoomd/md/PotentialExternalGPU.h:    kernel::gpu_compute_potential_external_forces<evaluator>(
hoomd/md/PotentialExternalGPU.h:    if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/PotentialExternalGPU.h:        CHECK_CUDA_ERROR();
hoomd/md/PotentialExternalGPU.h:template<class T> void export_PotentialExternalGPU(pybind11::module& m, const std::string& name)
hoomd/md/PotentialExternalGPU.h:    pybind11::class_<PotentialExternalGPU<T>,
hoomd/md/PotentialExternalGPU.h:                     std::shared_ptr<PotentialExternalGPU<T>>>(m, name.c_str())
hoomd/md/TableAngleForceGPU.cu:#include "TableAngleForceGPU.cuh"
hoomd/md/TableAngleForceGPU.cu:/*! \file TableAngleForceGPU.cu
hoomd/md/TableAngleForceGPU.cu:    \brief Defines GPU kernel code for calculating the table angle forces. Used by
hoomd/md/TableAngleForceGPU.cu:   TableAngleForceComputeGPU.
hoomd/md/TableAngleForceGPU.cu:    \param alist List of angles stored on the GPU
hoomd/md/TableAngleForceGPU.cu:    \param apos_list List of particle position in angle stored on the GPU
hoomd/md/TableAngleForceGPU.cu:    \param n_angles_list List of numbers of angles stored on the GPU
hoomd/md/TableAngleForceGPU.cu:__global__ void gpu_compute_table_angle_forces_kernel(Scalar4* d_force,
hoomd/md/TableAngleForceGPU.cu:    \param alist List of angles stored on the GPU
hoomd/md/TableAngleForceGPU.cu:    \param n_angles_list List of numbers of angles stored on the GPU
hoomd/md/TableAngleForceGPU.cu:    \note This is just a kernel driver. See gpu_compute_table_angle_forces_kernel for full
hoomd/md/TableAngleForceGPU.cu:hipError_t gpu_compute_table_angle_forces(Scalar4* d_force,
hoomd/md/TableAngleForceGPU.cu:    hipFuncGetAttributes(&attr, (const void*)gpu_compute_table_angle_forces_kernel);
hoomd/md/TableAngleForceGPU.cu:    hipLaunchKernelGGL((gpu_compute_table_angle_forces_kernel),
hoomd/md/MolecularForceCompute.cc:    if (m_exec_conf->isCUDAEnabled())
hoomd/md/MolecularForceCompute.cc:void MolecularForceCompute::initMoleculesGPU()
hoomd/md/MolecularForceCompute.cc:        kernel::gpu_sort_by_molecule(nptl_local,
hoomd/md/MolecularForceCompute.cc:                                     m_exec_conf->isCUDAErrorCheckingEnabled());
hoomd/md/MolecularForceCompute.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/MolecularForceCompute.cc:            CHECK_CUDA_ERROR();
hoomd/md/MolecularForceCompute.cc:        kernel::gpu_fill_molecule_table(nptl_local,
hoomd/md/MolecularForceCompute.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/MolecularForceCompute.cc:            CHECK_CUDA_ERROR();
hoomd/md/MolecularForceCompute.cc:    // distribute molecules evenly over GPUs
hoomd/md/MolecularForceCompute.cc:    // NOTE: going forward we could slave the GPU partition of the molecules
hoomd/md/MolecularForceCompute.cc:    m_gpu_partition = GPUPartition(m_exec_conf->getGPUIds());
hoomd/md/MolecularForceCompute.cc:    m_gpu_partition.setN(n_local_molecules);
hoomd/md/MolecularForceCompute.cc:        auto gpu_map = m_exec_conf->getGPUIds();
hoomd/md/MolecularForceCompute.cc:        for (unsigned int idev = 0; idev < m_exec_conf->getNumActiveGPUs(); ++idev)
hoomd/md/MolecularForceCompute.cc:            std::pair<unsigned int, unsigned int> range = m_gpu_partition.getRange(idev);
hoomd/md/MolecularForceCompute.cc:            cudaMemAdvise(m_molecule_length.get() + range.first,
hoomd/md/MolecularForceCompute.cc:                          cudaMemAdviseSetPreferredLocation,
hoomd/md/MolecularForceCompute.cc:                          gpu_map[idev]);
hoomd/md/MolecularForceCompute.cc:            cudaMemPrefetchAsync(m_molecule_length.get() + range.first,
hoomd/md/MolecularForceCompute.cc:                                 gpu_map[idev]);
hoomd/md/MolecularForceCompute.cc:            cudaMemAdvise(m_molecule_list.get() + m_molecule_indexer(0, range.first),
hoomd/md/MolecularForceCompute.cc:                          cudaMemAdviseSetPreferredLocation,
hoomd/md/MolecularForceCompute.cc:                          gpu_map[idev]);
hoomd/md/MolecularForceCompute.cc:            cudaMemPrefetchAsync(m_molecule_list.get() + m_molecule_indexer(0, range.first),
hoomd/md/MolecularForceCompute.cc:                                 gpu_map[idev]);
hoomd/md/MolecularForceCompute.cc:        for (unsigned int idev = 0; idev < m_exec_conf->getNumActiveGPUs(); ++idev)
hoomd/md/MolecularForceCompute.cc:            auto range = m_pdata->getGPUPartition().getRange(idev);
hoomd/md/MolecularForceCompute.cc:            cudaMemAdvise(m_molecule_idx.get() + range.first,
hoomd/md/MolecularForceCompute.cc:                          cudaMemAdviseSetPreferredLocation,
hoomd/md/MolecularForceCompute.cc:                          gpu_map[idev]);
hoomd/md/MolecularForceCompute.cc:            cudaMemPrefetchAsync(m_molecule_idx.get() + range.first,
hoomd/md/MolecularForceCompute.cc:                                 gpu_map[idev]);
hoomd/md/MolecularForceCompute.cc:        CHECK_CUDA_ERROR();
hoomd/md/MolecularForceCompute.cc:    if (m_exec_conf->isCUDAEnabled())
hoomd/md/MolecularForceCompute.cc:        initMoleculesGPU();
hoomd/md/ActiveForceConstraintComputeGPU.cuh:/*! \file ActiveForceComputeGPU.cuh
hoomd/md/ActiveForceConstraintComputeGPU.cuh:    \brief Declares GPU kernel code for calculating active forces forces on the GPU. Used by
hoomd/md/ActiveForceConstraintComputeGPU.cuh:   ActiveForceComputeGPU.
hoomd/md/ActiveForceConstraintComputeGPU.cuh:#ifndef __ACTIVE_FORCE_CONSTRAINT_COMPUTE_GPU_CUH__
hoomd/md/ActiveForceConstraintComputeGPU.cuh:#define __ACTIVE_FORCE_CONSTRAINT_COMPUTE_GPU_CUH__
hoomd/md/ActiveForceConstraintComputeGPU.cuh:hipError_t gpu_compute_active_force_set_constraints(const unsigned int group_size,
hoomd/md/ActiveForceConstraintComputeGPU.cuh:hipError_t gpu_compute_active_force_constraint_rotational_diffusion(const unsigned int group_size,
hoomd/md/ActiveForceConstraintComputeGPU.cuh://  manifold surface constraint on the GPU
hoomd/md/ActiveForceConstraintComputeGPU.cuh:__global__ void gpu_compute_active_force_set_constraints_kernel(const unsigned int group_size,
hoomd/md/ActiveForceConstraintComputeGPU.cuh://! Kernel for applying rotational diffusion to active force vectors on the GPU
hoomd/md/ActiveForceConstraintComputeGPU.cuh:gpu_compute_active_force_constraint_rotational_diffusion_kernel(const unsigned int group_size,
hoomd/md/ActiveForceConstraintComputeGPU.cuh:hipError_t gpu_compute_active_force_set_constraints(const unsigned int group_size,
hoomd/md/ActiveForceConstraintComputeGPU.cuh:    hipLaunchKernelGGL((gpu_compute_active_force_set_constraints_kernel<Manifold>),
hoomd/md/ActiveForceConstraintComputeGPU.cuh:hipError_t gpu_compute_active_force_constraint_rotational_diffusion(const unsigned int group_size,
hoomd/md/ActiveForceConstraintComputeGPU.cuh:    hipLaunchKernelGGL((gpu_compute_active_force_constraint_rotational_diffusion_kernel<Manifold>),
hoomd/md/AnisoPotentialPairGPU.h:#ifndef __ANISO_POTENTIAL_PAIR_GPU_H__
hoomd/md/AnisoPotentialPairGPU.h:#define __ANISO_POTENTIAL_PAIR_GPU_H__
hoomd/md/AnisoPotentialPairGPU.h:#include "AnisoPotentialPairGPU.cuh"
hoomd/md/AnisoPotentialPairGPU.h:/*! \file AnisoPotentialPairGPU.h
hoomd/md/AnisoPotentialPairGPU.h:    \brief Defines the template class for standard pair potentials on the GPU
hoomd/md/AnisoPotentialPairGPU.h://! Template class for computing anisotropic pair potentials on the GPU
hoomd/md/AnisoPotentialPairGPU.h:    \sa export_AnisoPotentialPairGPU()
hoomd/md/AnisoPotentialPairGPU.h:template<class evaluator> class AnisoPotentialPairGPU : public AnisoPotentialPair<evaluator>
hoomd/md/AnisoPotentialPairGPU.h:    AnisoPotentialPairGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/md/AnisoPotentialPairGPU.h:    virtual ~AnisoPotentialPairGPU() { };
hoomd/md/AnisoPotentialPairGPU.h:AnisoPotentialPairGPU<evaluator>::AnisoPotentialPairGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/md/AnisoPotentialPairGPU.h:    // can't run on the GPU if there aren't any GPUs in the execution configuration
hoomd/md/AnisoPotentialPairGPU.h:    if (!this->m_exec_conf->isCUDAEnabled())
hoomd/md/AnisoPotentialPairGPU.h:            << ": Creating a AnisoPotentialPairGPU with no GPU in the execution configuration"
hoomd/md/AnisoPotentialPairGPU.h:        throw std::runtime_error("Error initializing AnisoPotentialPairGPU");
hoomd/md/AnisoPotentialPairGPU.h:template<class evaluator> void AnisoPotentialPairGPU<evaluator>::computeForces(uint64_t timestep)
hoomd/md/AnisoPotentialPairGPU.h:    // The GPU implementation CANNOT handle a half neighborlist, error out now
hoomd/md/AnisoPotentialPairGPU.h:            << ": AnisoPotentialPairGPU cannot handle a half neighborlist" << std::endl
hoomd/md/AnisoPotentialPairGPU.h:        throw std::runtime_error("Error computing forces in AnisoPotentialPairGPU");
hoomd/md/AnisoPotentialPairGPU.h:    this->m_exec_conf->beginMultiGPU();
hoomd/md/AnisoPotentialPairGPU.h:    kernel::gpu_compute_pair_aniso_forces<evaluator>(
hoomd/md/AnisoPotentialPairGPU.h:                              this->m_pdata->getGPUPartition(),
hoomd/md/AnisoPotentialPairGPU.h:    if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/md/AnisoPotentialPairGPU.h:        CHECK_CUDA_ERROR();
hoomd/md/AnisoPotentialPairGPU.h:    this->m_exec_conf->endMultiGPU();
hoomd/md/AnisoPotentialPairGPU.h:void AnisoPotentialPairGPU<evaluator>::setParams(unsigned int typ1,
hoomd/md/AnisoPotentialPairGPU.h:void AnisoPotentialPairGPU<evaluator>::setShape(unsigned int typ,
hoomd/md/AnisoPotentialPairGPU.h:    \tparam T Class type to export. \b Must be an instantiated AnisoPotentialPairGPU class template.
hoomd/md/AnisoPotentialPairGPU.h:template<class T> void export_AnisoPotentialPairGPU(pybind11::module& m, const std::string& name)
hoomd/md/AnisoPotentialPairGPU.h:    pybind11::class_<AnisoPotentialPairGPU<T>,
hoomd/md/AnisoPotentialPairGPU.h:                     std::shared_ptr<AnisoPotentialPairGPU<T>>>(m, name.c_str())
hoomd/md/AnisoPotentialPairGPU.h:#endif // __ANISO_POTENTIAL_PAIR_GPU_H__
hoomd/md/HelfrichMeshForceCompute.cc:    GPUArray<Scalar> params(m_mesh_data->getMeshBondData()->getNTypes(), m_exec_conf);
hoomd/md/HelfrichMeshForceCompute.cc:    if (m_exec_conf->isCUDAEnabled() && m_exec_conf->allConcurrentManagedAccess())
hoomd/md/HelfrichMeshForceCompute.cc:        cudaMemAdvise(m_sigma_dash.get(),
hoomd/md/HelfrichMeshForceCompute.cc:                      cudaMemAdviseSetReadMostly,
hoomd/md/HelfrichMeshForceCompute.cc:        cudaMemAdvise(m_sigma.get(),
hoomd/md/HelfrichMeshForceCompute.cc:                      cudaMemAdviseSetReadMostly,
hoomd/md/HarmonicImproperForceComputeGPU.h:#include "HarmonicImproperForceGPU.cuh"
hoomd/md/HarmonicImproperForceComputeGPU.h:/*! \file HarmonicImproperForceComputeGPU.h
hoomd/md/HarmonicImproperForceComputeGPU.h:    \brief Declares the HarmonicImproperForceGPU class
hoomd/md/HarmonicImproperForceComputeGPU.h:#ifndef __HARMONICIMPROPERFORCECOMPUTEGPU_H__
hoomd/md/HarmonicImproperForceComputeGPU.h:#define __HARMONICIMPROPERFORCECOMPUTEGPU_H__
hoomd/md/HarmonicImproperForceComputeGPU.h://! Implements the harmonic improper force calculation on the GPU
hoomd/md/HarmonicImproperForceComputeGPU.h:/*! HarmonicImproperForceComputeGPU implements the same calculations as
hoomd/md/HarmonicImproperForceComputeGPU.h:   HarmonicImproperForceCompute, but executing on the GPU.
hoomd/md/HarmonicImproperForceComputeGPU.h:    \a m_gpu_params. They are stored as Scalar2's with the \a x component being K and the
hoomd/md/HarmonicImproperForceComputeGPU.h:    The GPU kernel can be found in improperforce_kernel.cu.
hoomd/md/HarmonicImproperForceComputeGPU.h:class PYBIND11_EXPORT HarmonicImproperForceComputeGPU : public HarmonicImproperForceCompute
hoomd/md/HarmonicImproperForceComputeGPU.h:    HarmonicImproperForceComputeGPU(std::shared_ptr<SystemDefinition> sysdef);
hoomd/md/HarmonicImproperForceComputeGPU.h:    ~HarmonicImproperForceComputeGPU();
hoomd/md/HarmonicImproperForceComputeGPU.h:    GPUArray<Scalar2> m_params;            //!< Parameters stored on the GPU (k,chi)
hoomd/simulation.py:                    cpp_communicator = _hoomd.CommunicatorGPU(
hoomd/ParticleData.cc:#include "GPUPartition.cuh"
hoomd/ParticleData.cc:    \param exec_conf ExecutionConfiguration to use when executing code on the GPU
hoomd/ParticleData.cc:    if (m_exec_conf->isCUDAEnabled())
hoomd/ParticleData.cc:        m_gpu_partition = GPUPartition(m_exec_conf->getGPUIds());
hoomd/ParticleData.cc:    if (m_exec_conf->isCUDAEnabled())
hoomd/ParticleData.cc:        m_gpu_partition = GPUPartition(m_exec_conf->getGPUIds());
hoomd/ParticleData.cc:    if (m_exec_conf->isCUDAEnabled())
hoomd/ParticleData.cc:        // need to update GPUPartition if particle number changes
hoomd/ParticleData.cc:        m_gpu_partition.setN(getN());
hoomd/ParticleData.cc:        // update our CUDA hints
hoomd/ParticleData.cc:        setGPUAdvice();
hoomd/ParticleData.cc:    \pre No memory is allocated and the per-particle GPUArrays are uninitialized
hoomd/ParticleData.cc:    \post All per-particle GPUArrays are allocated
hoomd/ParticleData.cc:    if (m_exec_conf->isCUDAEnabled() && m_exec_conf->allConcurrentManagedAccess())
hoomd/ParticleData.cc:        auto gpu_map = m_exec_conf->getGPUIds();
hoomd/ParticleData.cc:        // set up GPU memory mappings
hoomd/ParticleData.cc:        for (unsigned int idev = 0; idev < m_exec_conf->getNumActiveGPUs(); ++idev)
hoomd/ParticleData.cc:            cudaMemAdvise(m_pos.get(),
hoomd/ParticleData.cc:                          cudaMemAdviseSetAccessedBy,
hoomd/ParticleData.cc:                          gpu_map[idev]);
hoomd/ParticleData.cc:            cudaMemAdvise(m_vel.get(),
hoomd/ParticleData.cc:                          cudaMemAdviseSetAccessedBy,
hoomd/ParticleData.cc:                          gpu_map[idev]);
hoomd/ParticleData.cc:            cudaMemAdvise(m_accel.get(),
hoomd/ParticleData.cc:                          cudaMemAdviseSetAccessedBy,
hoomd/ParticleData.cc:                          gpu_map[idev]);
hoomd/ParticleData.cc:            cudaMemAdvise(m_charge.get(),
hoomd/ParticleData.cc:                          cudaMemAdviseSetAccessedBy,
hoomd/ParticleData.cc:                          gpu_map[idev]);
hoomd/ParticleData.cc:            cudaMemAdvise(m_diameter.get(),
hoomd/ParticleData.cc:                          cudaMemAdviseSetAccessedBy,
hoomd/ParticleData.cc:                          gpu_map[idev]);
hoomd/ParticleData.cc:            cudaMemAdvise(m_image.get(),
hoomd/ParticleData.cc:                          cudaMemAdviseSetAccessedBy,
hoomd/ParticleData.cc:                          gpu_map[idev]);
hoomd/ParticleData.cc:            cudaMemAdvise(m_tag.get(),
hoomd/ParticleData.cc:                          cudaMemAdviseSetAccessedBy,
hoomd/ParticleData.cc:                          gpu_map[idev]);
hoomd/ParticleData.cc:            cudaMemAdvise(m_body.get(),
hoomd/ParticleData.cc:                          cudaMemAdviseSetAccessedBy,
hoomd/ParticleData.cc:                          gpu_map[idev]);
hoomd/ParticleData.cc:            cudaMemAdvise(m_orientation.get(),
hoomd/ParticleData.cc:                          cudaMemAdviseSetAccessedBy,
hoomd/ParticleData.cc:                          gpu_map[idev]);
hoomd/ParticleData.cc:        CHECK_CUDA_ERROR();
hoomd/ParticleData.cc:    \pre No memory is allocated and the alternate per-particle GPUArrays are uninitialized
hoomd/ParticleData.cc:    \post All alternate per-particle GPUArrays are allocated
hoomd/ParticleData.cc:    if (m_exec_conf->isCUDAEnabled() && m_exec_conf->allConcurrentManagedAccess())
hoomd/ParticleData.cc:        auto gpu_map = m_exec_conf->getGPUIds();
hoomd/ParticleData.cc:        // set up GPU memory mappings
hoomd/ParticleData.cc:        for (unsigned int idev = 0; idev < m_exec_conf->getNumActiveGPUs(); ++idev)
hoomd/ParticleData.cc:            cudaMemAdvise(m_pos_alt.get(),
hoomd/ParticleData.cc:                          cudaMemAdviseSetAccessedBy,
hoomd/ParticleData.cc:                          gpu_map[idev]);
hoomd/ParticleData.cc:            cudaMemAdvise(m_vel_alt.get(),
hoomd/ParticleData.cc:                          cudaMemAdviseSetAccessedBy,
hoomd/ParticleData.cc:                          gpu_map[idev]);
hoomd/ParticleData.cc:            cudaMemAdvise(m_accel_alt.get(),
hoomd/ParticleData.cc:                          cudaMemAdviseSetAccessedBy,
hoomd/ParticleData.cc:                          gpu_map[idev]);
hoomd/ParticleData.cc:            cudaMemAdvise(m_charge_alt.get(),
hoomd/ParticleData.cc:                          cudaMemAdviseSetAccessedBy,
hoomd/ParticleData.cc:                          gpu_map[idev]);
hoomd/ParticleData.cc:            cudaMemAdvise(m_diameter_alt.get(),
hoomd/ParticleData.cc:                          cudaMemAdviseSetAccessedBy,
hoomd/ParticleData.cc:                          gpu_map[idev]);
hoomd/ParticleData.cc:            cudaMemAdvise(m_image_alt.get(),
hoomd/ParticleData.cc:                          cudaMemAdviseSetAccessedBy,
hoomd/ParticleData.cc:                          gpu_map[idev]);
hoomd/ParticleData.cc:            cudaMemAdvise(m_tag_alt.get(),
hoomd/ParticleData.cc:                          cudaMemAdviseSetAccessedBy,
hoomd/ParticleData.cc:                          gpu_map[idev]);
hoomd/ParticleData.cc:            cudaMemAdvise(m_body_alt.get(),
hoomd/ParticleData.cc:                          cudaMemAdviseSetAccessedBy,
hoomd/ParticleData.cc:                          gpu_map[idev]);
hoomd/ParticleData.cc:            cudaMemAdvise(m_orientation_alt.get(),
hoomd/ParticleData.cc:                          cudaMemAdviseSetAccessedBy,
hoomd/ParticleData.cc:                          gpu_map[idev]);
hoomd/ParticleData.cc:        CHECK_CUDA_ERROR();
hoomd/ParticleData.cc:    if (m_exec_conf->isCUDAEnabled() && m_exec_conf->allConcurrentManagedAccess())
hoomd/ParticleData.cc:        auto gpu_map = m_exec_conf->getGPUIds();
hoomd/ParticleData.cc:        // set up GPU memory mappings
hoomd/ParticleData.cc:        for (unsigned int idev = 0; idev < m_exec_conf->getNumActiveGPUs(); ++idev)
hoomd/ParticleData.cc:            cudaMemAdvise(m_rtag.get(),
hoomd/ParticleData.cc:                          cudaMemAdviseSetAccessedBy,
hoomd/ParticleData.cc:                          gpu_map[idev]);
hoomd/ParticleData.cc:        CHECK_CUDA_ERROR();
hoomd/ParticleData.cc:    if (m_exec_conf->isCUDAEnabled())
hoomd/ParticleData.cc:        m_gpu_partition.setN(new_nparticles);
hoomd/ParticleData.cc:    if (m_exec_conf->isCUDAEnabled() && m_exec_conf->allConcurrentManagedAccess())
hoomd/ParticleData.cc:        auto gpu_map = m_exec_conf->getGPUIds();
hoomd/ParticleData.cc:        // set up GPU memory mappings
hoomd/ParticleData.cc:        for (unsigned int idev = 0; idev < m_exec_conf->getNumActiveGPUs(); ++idev)
hoomd/ParticleData.cc:            cudaMemAdvise(m_pos.get(),
hoomd/ParticleData.cc:                          cudaMemAdviseSetAccessedBy,
hoomd/ParticleData.cc:                          gpu_map[idev]);
hoomd/ParticleData.cc:            cudaMemAdvise(m_vel.get(),
hoomd/ParticleData.cc:                          cudaMemAdviseSetAccessedBy,
hoomd/ParticleData.cc:                          gpu_map[idev]);
hoomd/ParticleData.cc:            cudaMemAdvise(m_accel.get(),
hoomd/ParticleData.cc:                          cudaMemAdviseSetAccessedBy,
hoomd/ParticleData.cc:                          gpu_map[idev]);
hoomd/ParticleData.cc:            cudaMemAdvise(m_charge.get(),
hoomd/ParticleData.cc:                          cudaMemAdviseSetAccessedBy,
hoomd/ParticleData.cc:                          gpu_map[idev]);
hoomd/ParticleData.cc:            cudaMemAdvise(m_diameter.get(),
hoomd/ParticleData.cc:                          cudaMemAdviseSetAccessedBy,
hoomd/ParticleData.cc:                          gpu_map[idev]);
hoomd/ParticleData.cc:            cudaMemAdvise(m_image.get(),
hoomd/ParticleData.cc:                          cudaMemAdviseSetAccessedBy,
hoomd/ParticleData.cc:                          gpu_map[idev]);
hoomd/ParticleData.cc:            cudaMemAdvise(m_tag.get(),
hoomd/ParticleData.cc:                          cudaMemAdviseSetAccessedBy,
hoomd/ParticleData.cc:                          gpu_map[idev]);
hoomd/ParticleData.cc:            cudaMemAdvise(m_body.get(),
hoomd/ParticleData.cc:                          cudaMemAdviseSetAccessedBy,
hoomd/ParticleData.cc:                          gpu_map[idev]);
hoomd/ParticleData.cc:            cudaMemAdvise(m_orientation.get(),
hoomd/ParticleData.cc:                          cudaMemAdviseSetAccessedBy,
hoomd/ParticleData.cc:                          gpu_map[idev]);
hoomd/ParticleData.cc:        CHECK_CUDA_ERROR();
hoomd/ParticleData.cc:        if (m_exec_conf->isCUDAEnabled() && m_exec_conf->allConcurrentManagedAccess())
hoomd/ParticleData.cc:            auto gpu_map = m_exec_conf->getGPUIds();
hoomd/ParticleData.cc:            // set up GPU memory mappings
hoomd/ParticleData.cc:            for (unsigned int idev = 0; idev < m_exec_conf->getNumActiveGPUs(); ++idev)
hoomd/ParticleData.cc:                cudaMemAdvise(m_pos_alt.get(),
hoomd/ParticleData.cc:                              cudaMemAdviseSetAccessedBy,
hoomd/ParticleData.cc:                              gpu_map[idev]);
hoomd/ParticleData.cc:                cudaMemAdvise(m_vel_alt.get(),
hoomd/ParticleData.cc:                              cudaMemAdviseSetAccessedBy,
hoomd/ParticleData.cc:                              gpu_map[idev]);
hoomd/ParticleData.cc:                cudaMemAdvise(m_accel_alt.get(),
hoomd/ParticleData.cc:                              cudaMemAdviseSetAccessedBy,
hoomd/ParticleData.cc:                              gpu_map[idev]);
hoomd/ParticleData.cc:                cudaMemAdvise(m_charge_alt.get(),
hoomd/ParticleData.cc:                              cudaMemAdviseSetAccessedBy,
hoomd/ParticleData.cc:                              gpu_map[idev]);
hoomd/ParticleData.cc:                cudaMemAdvise(m_diameter_alt.get(),
hoomd/ParticleData.cc:                              cudaMemAdviseSetAccessedBy,
hoomd/ParticleData.cc:                              gpu_map[idev]);
hoomd/ParticleData.cc:                cudaMemAdvise(m_image_alt.get(),
hoomd/ParticleData.cc:                              cudaMemAdviseSetAccessedBy,
hoomd/ParticleData.cc:                              gpu_map[idev]);
hoomd/ParticleData.cc:                cudaMemAdvise(m_tag_alt.get(),
hoomd/ParticleData.cc:                              cudaMemAdviseSetAccessedBy,
hoomd/ParticleData.cc:                              gpu_map[idev]);
hoomd/ParticleData.cc:                cudaMemAdvise(m_body_alt.get(),
hoomd/ParticleData.cc:                              cudaMemAdviseSetAccessedBy,
hoomd/ParticleData.cc:                              gpu_map[idev]);
hoomd/ParticleData.cc:                cudaMemAdvise(m_orientation_alt.get(),
hoomd/ParticleData.cc:                              cudaMemAdviseSetAccessedBy,
hoomd/ParticleData.cc:                              gpu_map[idev]);
hoomd/ParticleData.cc:            CHECK_CUDA_ERROR();
hoomd/ParticleData.cc:                                       "in shared memory errors on the GPU."
hoomd/ParticleData.cc://! Pack particle data into a buffer (GPU version)
hoomd/ParticleData.cc:void ParticleData::removeParticlesGPU(GlobalVector<detail::pdata_element>& out,
hoomd/ParticleData.cc:            m_exec_conf->beginMultiGPU();
hoomd/ParticleData.cc:            n_out = kernel::gpu_pdata_remove(getN(),
hoomd/ParticleData.cc:                                             m_gpu_partition);
hoomd/ParticleData.cc:            if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/ParticleData.cc:                CHECK_CUDA_ERROR();
hoomd/ParticleData.cc:            m_exec_conf->endMultiGPU();
hoomd/ParticleData.cc://! Add new particle data (GPU version)
hoomd/ParticleData.cc:void ParticleData::addParticlesGPU(const GlobalVector<detail::pdata_element>& in)
hoomd/ParticleData.cc:        // add new particles on GPU
hoomd/ParticleData.cc:        kernel::gpu_pdata_add_particles(old_nparticles,
hoomd/ParticleData.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/ParticleData.cc:            CHECK_CUDA_ERROR();
hoomd/ParticleData.cc:void ParticleData::setGPUAdvice()
hoomd/ParticleData.cc:    if (m_exec_conf->isCUDAEnabled())
hoomd/ParticleData.cc:        // only call CUDA API when necessary
hoomd/ParticleData.cc:        auto gpu_map = m_exec_conf->getGPUIds();
hoomd/ParticleData.cc:        // split preferred location of particle data across GPUs
hoomd/ParticleData.cc:        for (unsigned int idev = 0; idev < m_exec_conf->getNumActiveGPUs(); ++idev)
hoomd/ParticleData.cc:            auto range = m_gpu_partition.getRange(idev);
hoomd/ParticleData.cc:            cudaMemAdvise(m_pos.get() + range.first,
hoomd/ParticleData.cc:                          cudaMemAdviseSetPreferredLocation,
hoomd/ParticleData.cc:                          gpu_map[idev]);
hoomd/ParticleData.cc:            cudaMemAdvise(m_vel.get() + range.first,
hoomd/ParticleData.cc:                          cudaMemAdviseSetPreferredLocation,
hoomd/ParticleData.cc:                          gpu_map[idev]);
hoomd/ParticleData.cc:            cudaMemAdvise(m_accel.get() + range.first,
hoomd/ParticleData.cc:                          cudaMemAdviseSetPreferredLocation,
hoomd/ParticleData.cc:                          gpu_map[idev]);
hoomd/ParticleData.cc:            cudaMemAdvise(m_charge.get() + range.first,
hoomd/ParticleData.cc:                          cudaMemAdviseSetPreferredLocation,
hoomd/ParticleData.cc:                          gpu_map[idev]);
hoomd/ParticleData.cc:            cudaMemAdvise(m_diameter.get() + range.first,
hoomd/ParticleData.cc:                          cudaMemAdviseSetPreferredLocation,
hoomd/ParticleData.cc:                          gpu_map[idev]);
hoomd/ParticleData.cc:            cudaMemAdvise(m_image.get() + range.first,
hoomd/ParticleData.cc:                          cudaMemAdviseSetPreferredLocation,
hoomd/ParticleData.cc:                          gpu_map[idev]);
hoomd/ParticleData.cc:            cudaMemAdvise(m_tag.get() + range.first,
hoomd/ParticleData.cc:                          cudaMemAdviseSetPreferredLocation,
hoomd/ParticleData.cc:                          gpu_map[idev]);
hoomd/ParticleData.cc:            cudaMemAdvise(m_body.get() + range.first,
hoomd/ParticleData.cc:                          cudaMemAdviseSetPreferredLocation,
hoomd/ParticleData.cc:                          gpu_map[idev]);
hoomd/ParticleData.cc:            cudaMemAdvise(m_orientation.get() + range.first,
hoomd/ParticleData.cc:                          cudaMemAdviseSetPreferredLocation,
hoomd/ParticleData.cc:                          gpu_map[idev]);
hoomd/ParticleData.cc:            cudaMemAdvise(m_angmom.get() + range.first,
hoomd/ParticleData.cc:                          cudaMemAdviseSetPreferredLocation,
hoomd/ParticleData.cc:                          gpu_map[idev]);
hoomd/ParticleData.cc:            cudaMemAdvise(m_inertia.get() + range.first,
hoomd/ParticleData.cc:                          cudaMemAdviseSetPreferredLocation,
hoomd/ParticleData.cc:                          gpu_map[idev]);
hoomd/ParticleData.cc:            cudaMemAdvise(m_net_force.get() + range.first,
hoomd/ParticleData.cc:                          cudaMemAdviseSetPreferredLocation,
hoomd/ParticleData.cc:                          gpu_map[idev]);
hoomd/ParticleData.cc:                cudaMemAdvise(m_net_virial.get() + i * m_net_virial.getPitch() + range.first,
hoomd/ParticleData.cc:                              cudaMemAdviseSetPreferredLocation,
hoomd/ParticleData.cc:                              gpu_map[idev]);
hoomd/ParticleData.cc:            cudaMemAdvise(m_net_torque.get() + range.first,
hoomd/ParticleData.cc:                          cudaMemAdviseSetPreferredLocation,
hoomd/ParticleData.cc:                          gpu_map[idev]);
hoomd/ParticleData.cc:            cudaMemPrefetchAsync(m_pos.get() + range.first, sizeof(Scalar4) * nelem, gpu_map[idev]);
hoomd/ParticleData.cc:            cudaMemPrefetchAsync(m_vel.get() + range.first, sizeof(Scalar4) * nelem, gpu_map[idev]);
hoomd/ParticleData.cc:            cudaMemPrefetchAsync(m_accel.get() + range.first,
hoomd/ParticleData.cc:                                 gpu_map[idev]);
hoomd/ParticleData.cc:            cudaMemPrefetchAsync(m_charge.get() + range.first,
hoomd/ParticleData.cc:                                 gpu_map[idev]);
hoomd/ParticleData.cc:            cudaMemPrefetchAsync(m_diameter.get() + range.first,
hoomd/ParticleData.cc:                                 gpu_map[idev]);
hoomd/ParticleData.cc:            cudaMemPrefetchAsync(m_image.get() + range.first, sizeof(int3) * nelem, gpu_map[idev]);
hoomd/ParticleData.cc:            cudaMemPrefetchAsync(m_tag.get() + range.first,
hoomd/ParticleData.cc:                                 gpu_map[idev]);
hoomd/ParticleData.cc:            cudaMemPrefetchAsync(m_body.get() + range.first,
hoomd/ParticleData.cc:                                 gpu_map[idev]);
hoomd/ParticleData.cc:            cudaMemPrefetchAsync(m_orientation.get() + range.first,
hoomd/ParticleData.cc:                                 gpu_map[idev]);
hoomd/ParticleData.cc:            cudaMemPrefetchAsync(m_angmom.get() + range.first,
hoomd/ParticleData.cc:                                 gpu_map[idev]);
hoomd/ParticleData.cc:            cudaMemPrefetchAsync(m_inertia.get() + range.first,
hoomd/ParticleData.cc:                                 gpu_map[idev]);
hoomd/ParticleData.cc:            cudaMemPrefetchAsync(m_net_force.get() + range.first,
hoomd/ParticleData.cc:                                 gpu_map[idev]);
hoomd/ParticleData.cc:                cudaMemPrefetchAsync(m_net_virial.get() + i * m_net_virial.getPitch() + range.first,
hoomd/ParticleData.cc:                                     gpu_map[idev]);
hoomd/ParticleData.cc:            cudaMemPrefetchAsync(m_net_torque.get() + range.first,
hoomd/ParticleData.cc:                                 gpu_map[idev]);
hoomd/ParticleData.cc:        CHECK_CUDA_ERROR();
hoomd/ParticleData.cc:            for (unsigned int idev = 0; idev < m_exec_conf->getNumActiveGPUs(); ++idev)
hoomd/ParticleData.cc:                auto range = m_gpu_partition.getRange(idev);
hoomd/ParticleData.cc:                cudaMemAdvise(m_pos_alt.get() + range.first,
hoomd/ParticleData.cc:                              cudaMemAdviseSetPreferredLocation,
hoomd/ParticleData.cc:                              gpu_map[idev]);
hoomd/ParticleData.cc:                cudaMemAdvise(m_vel_alt.get() + range.first,
hoomd/ParticleData.cc:                              cudaMemAdviseSetPreferredLocation,
hoomd/ParticleData.cc:                              gpu_map[idev]);
hoomd/ParticleData.cc:                cudaMemAdvise(m_accel_alt.get() + range.first,
hoomd/ParticleData.cc:                              cudaMemAdviseSetPreferredLocation,
hoomd/ParticleData.cc:                              gpu_map[idev]);
hoomd/ParticleData.cc:                cudaMemAdvise(m_charge_alt.get() + range.first,
hoomd/ParticleData.cc:                              cudaMemAdviseSetPreferredLocation,
hoomd/ParticleData.cc:                              gpu_map[idev]);
hoomd/ParticleData.cc:                cudaMemAdvise(m_diameter_alt.get() + range.first,
hoomd/ParticleData.cc:                              cudaMemAdviseSetPreferredLocation,
hoomd/ParticleData.cc:                              gpu_map[idev]);
hoomd/ParticleData.cc:                cudaMemAdvise(m_image_alt.get() + range.first,
hoomd/ParticleData.cc:                              cudaMemAdviseSetPreferredLocation,
hoomd/ParticleData.cc:                              gpu_map[idev]);
hoomd/ParticleData.cc:                cudaMemAdvise(m_tag_alt.get() + range.first,
hoomd/ParticleData.cc:                              cudaMemAdviseSetPreferredLocation,
hoomd/ParticleData.cc:                              gpu_map[idev]);
hoomd/ParticleData.cc:                cudaMemAdvise(m_body_alt.get() + range.first,
hoomd/ParticleData.cc:                              cudaMemAdviseSetPreferredLocation,
hoomd/ParticleData.cc:                              gpu_map[idev]);
hoomd/ParticleData.cc:                cudaMemAdvise(m_orientation_alt.get() + range.first,
hoomd/ParticleData.cc:                              cudaMemAdviseSetPreferredLocation,
hoomd/ParticleData.cc:                              gpu_map[idev]);
hoomd/ParticleData.cc:                cudaMemAdvise(m_angmom_alt.get() + range.first,
hoomd/ParticleData.cc:                              cudaMemAdviseSetPreferredLocation,
hoomd/ParticleData.cc:                              gpu_map[idev]);
hoomd/ParticleData.cc:                cudaMemAdvise(m_inertia_alt.get() + range.first,
hoomd/ParticleData.cc:                              cudaMemAdviseSetPreferredLocation,
hoomd/ParticleData.cc:                              gpu_map[idev]);
hoomd/ParticleData.cc:                cudaMemAdvise(m_net_force_alt.get() + range.first,
hoomd/ParticleData.cc:                              cudaMemAdviseSetPreferredLocation,
hoomd/ParticleData.cc:                              gpu_map[idev]);
hoomd/ParticleData.cc:                    cudaMemAdvise(m_net_virial_alt.get() + i * m_net_virial_alt.getPitch()
hoomd/ParticleData.cc:                                  cudaMemAdviseSetPreferredLocation,
hoomd/ParticleData.cc:                                  gpu_map[idev]);
hoomd/ParticleData.cc:                cudaMemAdvise(m_net_torque_alt.get() + range.first,
hoomd/ParticleData.cc:                              cudaMemAdviseSetPreferredLocation,
hoomd/ParticleData.cc:                              gpu_map[idev]);
hoomd/ParticleData.cc:                cudaMemPrefetchAsync(m_pos_alt.get() + range.first,
hoomd/ParticleData.cc:                                     gpu_map[idev]);
hoomd/ParticleData.cc:                cudaMemPrefetchAsync(m_vel_alt.get() + range.first,
hoomd/ParticleData.cc:                                     gpu_map[idev]);
hoomd/ParticleData.cc:                cudaMemPrefetchAsync(m_accel_alt.get() + range.first,
hoomd/ParticleData.cc:                                     gpu_map[idev]);
hoomd/ParticleData.cc:                cudaMemPrefetchAsync(m_charge_alt.get() + range.first,
hoomd/ParticleData.cc:                                     gpu_map[idev]);
hoomd/ParticleData.cc:                cudaMemPrefetchAsync(m_diameter_alt.get() + range.first,
hoomd/ParticleData.cc:                                     gpu_map[idev]);
hoomd/ParticleData.cc:                cudaMemPrefetchAsync(m_image_alt.get() + range.first,
hoomd/ParticleData.cc:                                     gpu_map[idev]);
hoomd/ParticleData.cc:                cudaMemPrefetchAsync(m_tag_alt.get() + range.first,
hoomd/ParticleData.cc:                                     gpu_map[idev]);
hoomd/ParticleData.cc:                cudaMemPrefetchAsync(m_body_alt.get() + range.first,
hoomd/ParticleData.cc:                                     gpu_map[idev]);
hoomd/ParticleData.cc:                cudaMemPrefetchAsync(m_orientation_alt.get() + range.first,
hoomd/ParticleData.cc:                                     gpu_map[idev]);
hoomd/ParticleData.cc:                cudaMemPrefetchAsync(m_angmom_alt.get() + range.first,
hoomd/ParticleData.cc:                                     gpu_map[idev]);
hoomd/ParticleData.cc:                cudaMemPrefetchAsync(m_inertia_alt.get() + range.first,
hoomd/ParticleData.cc:                                     gpu_map[idev]);
hoomd/ParticleData.cc:                cudaMemPrefetchAsync(m_net_force_alt.get() + range.first,
hoomd/ParticleData.cc:                                     gpu_map[idev]);
hoomd/ParticleData.cc:                    cudaMemPrefetchAsync(m_net_virial_alt.get() + i * m_net_virial_alt.getPitch()
hoomd/ParticleData.cc:                                         gpu_map[idev]);
hoomd/ParticleData.cc:                cudaMemPrefetchAsync(m_net_torque_alt.get() + range.first,
hoomd/ParticleData.cc:                                     gpu_map[idev]);
hoomd/ParticleData.cc:            CHECK_CUDA_ERROR();
hoomd/GPUFlags.h:/*! \file GPUFlags.h
hoomd/GPUFlags.h:    \brief Defines the GPUFlags class
hoomd/GPUFlags.h:#ifndef __GPUFLAGS_H__
hoomd/GPUFlags.h:#define __GPUFLAGS_H__
hoomd/GPUFlags.h://! Class for managing a small set of flags set on the GPU
hoomd/GPUFlags.h:GPUFlags is an efficient container for a small set of flags. It is optimized for use when a given
hoomd/GPUFlags.h:set of flags is set via a kernel call on the GPU and then read on the host. The host may also reset
hoomd/GPUFlags.h:Like GPUArray, GPUFlags keeps around an ExecutionConfiguration to keep the GPU alive, and the flags
hoomd/GPUFlags.h:reading flags that were set on the GPU, the flags are stored in host mapped memory (on devices that
hoomd/GPUFlags.h:support it). GPUFlags inserts synchronization points 1) when resetFlags() is called to reset the
hoomd/GPUFlags.h:template<class T> class PYBIND11_EXPORT GPUFlags
hoomd/GPUFlags.h:    //! Constructs a NULL GPUFlags
hoomd/GPUFlags.h:    GPUFlags();
hoomd/GPUFlags.h:    //! Constructs a GPUFlags attached to a GPU
hoomd/GPUFlags.h:    GPUFlags(std::shared_ptr<const ExecutionConfiguration> exec_conf);
hoomd/GPUFlags.h:    ~GPUFlags();
hoomd/GPUFlags.h:    GPUFlags(const GPUFlags& from);
hoomd/GPUFlags.h:    GPUFlags& operator=(const GPUFlags& rhs);
hoomd/GPUFlags.h:    //! Swap the pointers in two GPUFlags
hoomd/GPUFlags.h:    inline void swap(GPUFlags& from);
hoomd/GPUFlags.h:    //! Test if the GPUFlags is NULL
hoomd/GPUFlags.h:        m_exec_conf; //!< execution configuration for working with CUDA
hoomd/GPUFlags.h:// GPUFlags implementation
hoomd/GPUFlags.h:GPUFlags<T>::GPUFlags()
hoomd/GPUFlags.h:/*! \param exec_conf Shared pointer to the execution configuration for managing CUDA initialization
hoomd/GPUFlags.h:GPUFlags<T>::GPUFlags(std::shared_ptr<const ExecutionConfiguration> exec_conf)
hoomd/GPUFlags.h:    if (m_exec_conf->isCUDAEnabled() && m_exec_conf->dev_prop.canMapHostMemory)
hoomd/GPUFlags.h:template<class T> GPUFlags<T>::~GPUFlags()
hoomd/GPUFlags.h:GPUFlags<T>::GPUFlags(const GPUFlags& from)
hoomd/GPUFlags.h:    // copy over the data to the new GPUFlags
hoomd/GPUFlags.h:template<class T> GPUFlags<T>& GPUFlags<T>::operator=(const GPUFlags& rhs)
hoomd/GPUFlags.h:        // copy over the data to the new GPUFlags
hoomd/GPUFlags.h:/*! \param from GPUFlags to swap \a this with
hoomd/GPUFlags.h:GPUFlags c(a);
hoomd/GPUFlags.h:template<class T> void GPUFlags<T>::swap(GPUFlags& from)
hoomd/GPUFlags.h:    \post All memory pointers needed for GPUFlags are allocated
hoomd/GPUFlags.h:template<class T> void GPUFlags<T>::allocate()
hoomd/GPUFlags.h:    if (m_exec_conf && m_exec_conf->isCUDAEnabled())
hoomd/GPUFlags.h:            CHECK_CUDA_ERROR();
hoomd/GPUFlags.h:            CHECK_CUDA_ERROR();
hoomd/GPUFlags.h:            CHECK_CUDA_ERROR();
hoomd/GPUFlags.h:            CHECK_CUDA_ERROR();
hoomd/GPUFlags.h:template<class T> void GPUFlags<T>::deallocate()
hoomd/GPUFlags.h:    if (m_exec_conf && m_exec_conf->isCUDAEnabled())
hoomd/GPUFlags.h:template<class T> void GPUFlags<T>::memclear()
hoomd/GPUFlags.h:    if (m_exec_conf && m_exec_conf->isCUDAEnabled() && m_mapped)
hoomd/GPUFlags.h:    if (m_exec_conf && m_exec_conf->isCUDAEnabled() && !m_mapped)
hoomd/GPUFlags.h:    \note readFlags implicitly synchronizes with the GPU execution stream. If there are any previous
hoomd/GPUFlags.h:template<class T> const T GPUFlags<T>::readFlags()
hoomd/GPUFlags.h:        if (m_exec_conf->isCUDAEnabled())
hoomd/GPUFlags.h:    \note resetFlags synchronizes with the GPU execution stream. It waits for all prior kernel
hoomd/GPUFlags.h:template<class T> void GPUFlags<T>::resetFlags(const T flags)
hoomd/GPUFlags.h:        if (m_exec_conf->isCUDAEnabled())
hoomd/TextureTools.h:/*! This function should be called whenever a CUDA kernel wants to retrieve a
hoomd/CellListGPU.h:/*! \file CellListGPU.h
hoomd/CellListGPU.h:    \brief Declares the CellListGPU class
hoomd/CellListGPU.h:#ifndef __CELLLISTGPU_H__
hoomd/CellListGPU.h:#define __CELLLISTGPU_H__
hoomd/CellListGPU.h://! Computes a cell list from the particles in the system on the GPU
hoomd/CellListGPU.h:/*! Calls GPU functions in CellListGPU.cuh and CellListGPU.cu
hoomd/CellListGPU.h:class PYBIND11_EXPORT CellListGPU : public CellList
hoomd/CellListGPU.h:    CellListGPU(std::shared_ptr<SystemDefinition> sysdef);
hoomd/CellListGPU.h:    virtual ~CellListGPU() { };
hoomd/CellListGPU.h:    //! Request a multi-GPU cell list
hoomd/CellListGPU.h:        m_cell_size_scratch; //!< Number of members in each cell, one list per GPU
hoomd/CellListGPU.h:    GlobalArray<unsigned int> m_cell_adj_scratch; //!< Cell adjacency list, one list per GPU
hoomd/CellListGPU.h:    GlobalArray<Scalar4> m_xyzf_scratch;    //!< Cell list with position and flags, one list per GPU
hoomd/CellListGPU.h:    GlobalArray<uint2> m_type_body_scratch; //!< Cell list with type,body, one list per GPU
hoomd/CellListGPU.h:    GlobalArray<Scalar4> m_orientation_scratch; //!< Cell list with orientation, one list per GPU
hoomd/CellListGPU.h:    GlobalArray<unsigned int> m_idx_scratch;    //!< Cell list with index, one list per GPU
hoomd/CellListGPU.h:    bool m_per_device; //!< True if we maintain a per-GPU cell list
hoomd/CellListGPU.h:    // Initialize GPU-specific data storage
hoomd/CellListGPU.h://! Exports CellListGPU to python
hoomd/CellListGPU.h:void export_CellListGPU(pybind11::module& m);
hoomd/version.py:    cuda_include_path (str): CUDA toolkit include directory.
hoomd/version.py:    cuda_devrt_library (str): CUDA devrt library.
hoomd/version.py:    gpu_api_version (str): The GPU API version this build was compiled against.
hoomd/version.py:    gpu_enabled (bool): ``True`` when this build supports GPUs.
hoomd/version.py:    gpu_platform (str): Name of the GPU platform this build was compiled
hoomd/version.py:    cuda_include_path,
hoomd/version.py:    cuda_devrt_library,
hoomd/version.py:gpu_enabled = _hoomd.BuildInfo.getEnableGPU()
hoomd/version.py:gpu_api_version = _hoomd.BuildInfo.getGPUAPIVersion()
hoomd/version.py:gpu_platform = _hoomd.BuildInfo.getGPUPlatform()
hoomd/CellListGPU.cuh:#ifndef __CELLLISTGPU_CUH__
hoomd/CellListGPU.cuh:#define __CELLLISTGPU_CUH__
hoomd/CellListGPU.cuh:#include "GPUPartition.cuh"
hoomd/CellListGPU.cuh:/*! \file CellListGPU.cuh
hoomd/CellListGPU.cuh:    \brief Declares GPU kernel code for cell list generation on the GPU
hoomd/CellListGPU.cuh://! Kernel driver for gpu_compute_cell_list_kernel()
hoomd/CellListGPU.cuh:void gpu_compute_cell_list(unsigned int* d_cell_size,
hoomd/CellListGPU.cuh:                           const GPUPartition& gpu_partition);
hoomd/CellListGPU.cuh://! Driver function to combine the cell lists from different GPUs into one
hoomd/CellListGPU.cuh:hipError_t gpu_combine_cell_lists(const unsigned int* d_cell_size_scratch,
hoomd/CellListGPU.cuh:                                  unsigned int ngpu,
hoomd/CellListGPU.cuh:                                  const GPUPartition& gpu_partition);
hoomd/CellListGPU.cuh:hipError_t gpu_sort_cell_list(unsigned int* d_cell_size,
hoomd/operation.py:    one or more GPU kernels. Each GPU kernel is executed with a set of
hoomd/operation.py:        The dictionary maps GPU kernel names to tuples of integers that control
hoomd/operation.py:        how the kernel executes on the GPU. These values will change during the
hoomd/ParticleData.h:#include "GPUVector.h"
hoomd/ParticleData.h:#include "GPUPartition.cuh"
hoomd/ParticleData.h:    and tag information. This data must be available both via the CPU and GPU memories.
hoomd/ParticleData.h:    All copying of data back and forth from the GPU is accomplished transparently by GlobalArray.
hoomd/ParticleData.h:    //! Pack particle data into a buffer (GPU version)
hoomd/ParticleData.h:    void removeParticlesGPU(GlobalVector<detail::pdata_element>& out,
hoomd/ParticleData.h:    //! Remove particles from local domain and add new particle data (GPU version)
hoomd/ParticleData.h:    void addParticlesGPU(const GlobalVector<detail::pdata_element>& in);
hoomd/ParticleData.h:    //! Return the load balancing GPU partition
hoomd/ParticleData.h:    const GPUPartition& getGPUPartition() const
hoomd/ParticleData.h:        return m_gpu_partition;
hoomd/ParticleData.h:    GlobalArray<Scalar> m_net_virial;  //!< Net virial calculated for each particle (2D GPU array of
hoomd/ParticleData.h:    GPUPartition m_gpu_partition; //!< The partition of the local number of particles across GPUs
hoomd/ParticleData.h:    //! Update the CUDA memory hints
hoomd/ParticleData.h:    void setGPUAdvice();
hoomd/ParticleData.h:        // Need pitch not particle numbers since GPUArrays can be padded for
hoomd/LoadBalancerGPU.cc:/*! \file LoadBalancerGPU.cc
hoomd/LoadBalancerGPU.cc:    \brief Defines the LoadBalancerGPU class
hoomd/LoadBalancerGPU.cc:#include "LoadBalancerGPU.h"
hoomd/LoadBalancerGPU.cc:#include "LoadBalancerGPU.cuh"
hoomd/LoadBalancerGPU.cc:LoadBalancerGPU::LoadBalancerGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/LoadBalancerGPU.cc:        .connect<LoadBalancerGPU, &LoadBalancerGPU::slotMaxNumChanged>(this);
hoomd/LoadBalancerGPU.cc:    GPUArray<unsigned int> off_ranks(m_pdata->getMaxN(), m_exec_conf);
hoomd/LoadBalancerGPU.cc:LoadBalancerGPU::~LoadBalancerGPU()
hoomd/LoadBalancerGPU.cc:        .disconnect<LoadBalancerGPU, &LoadBalancerGPU::slotMaxNumChanged>(this);
hoomd/LoadBalancerGPU.cc:void LoadBalancerGPU::countParticlesOffRank(std::map<unsigned int, unsigned int>& cnts)
hoomd/LoadBalancerGPU.cc:        kernel::gpu_load_balance_mark_rank(d_comm_flag.data,
hoomd/LoadBalancerGPU.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/LoadBalancerGPU.cc:            CHECK_CUDA_ERROR();
hoomd/LoadBalancerGPU.cc:            = kernel::gpu_load_balance_select_off_rank(d_off_ranks.data,
hoomd/LoadBalancerGPU.cc:void export_LoadBalancerGPU(pybind11::module& m)
hoomd/LoadBalancerGPU.cc:    pybind11::class_<LoadBalancerGPU, LoadBalancer, std::shared_ptr<LoadBalancerGPU>>(
hoomd/LoadBalancerGPU.cc:        "LoadBalancerGPU")
hoomd/SFCPackTunerGPU.cc:/*! \file SFCPackTunerGPU.cc
hoomd/SFCPackTunerGPU.cc:    \brief Defines the SFCPackTunerGPU class
hoomd/SFCPackTunerGPU.cc:#include "SFCPackTunerGPU.h"
hoomd/SFCPackTunerGPU.cc:#include "SFCPackTunerGPU.cuh"
hoomd/SFCPackTunerGPU.cc:SFCPackTunerGPU::SFCPackTunerGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/SFCPackTunerGPU.cc:    m_exec_conf->msg->notice(5) << "Constructing SFCPackTunerGPU" << endl;
hoomd/SFCPackTunerGPU.cc:    GlobalArray<unsigned int> gpu_sort_order(m_pdata->getMaxN(), m_exec_conf);
hoomd/SFCPackTunerGPU.cc:    m_gpu_sort_order.swap(gpu_sort_order);
hoomd/SFCPackTunerGPU.cc:    TAG_ALLOCATION(m_gpu_sort_order);
hoomd/SFCPackTunerGPU.cc:    GlobalArray<unsigned int> gpu_particle_bins(m_pdata->getMaxN(), m_exec_conf);
hoomd/SFCPackTunerGPU.cc:    m_gpu_particle_bins.swap(gpu_particle_bins);
hoomd/SFCPackTunerGPU.cc:    TAG_ALLOCATION(m_gpu_particle_bins);
hoomd/SFCPackTunerGPU.cc:void SFCPackTunerGPU::reallocate()
hoomd/SFCPackTunerGPU.cc:    m_gpu_sort_order.resize(m_pdata->getMaxN());
hoomd/SFCPackTunerGPU.cc:    m_gpu_particle_bins.resize(m_pdata->getMaxN());
hoomd/SFCPackTunerGPU.cc:SFCPackTunerGPU::~SFCPackTunerGPU()
hoomd/SFCPackTunerGPU.cc:    m_exec_conf->msg->notice(5) << "Destroying SFCPackTunerGPU" << endl;
hoomd/SFCPackTunerGPU.cc:void SFCPackTunerGPU::getSortedOrder2D()
hoomd/SFCPackTunerGPU.cc:    // on the GPU, getSortedOrder3D handles both cases
hoomd/SFCPackTunerGPU.cc:void SFCPackTunerGPU::getSortedOrder3D()
hoomd/SFCPackTunerGPU.cc:    assert(m_gpu_sort_order.getNumElements() >= m_pdata->getN());
hoomd/SFCPackTunerGPU.cc:        GPUArray<unsigned int> traversal_order(m_grid * m_grid * m_grid, m_exec_conf);
hoomd/SFCPackTunerGPU.cc:    assert(m_gpu_particle_bins.getNumElements() >= m_pdata->getN());
hoomd/SFCPackTunerGPU.cc:    ArrayHandle<unsigned int> d_gpu_particle_bins(m_gpu_particle_bins,
hoomd/SFCPackTunerGPU.cc:    ArrayHandle<unsigned int> d_gpu_sort_order(m_gpu_sort_order,
hoomd/SFCPackTunerGPU.cc:    kernel::gpu_generate_sorted_order(m_pdata->getN(),
hoomd/SFCPackTunerGPU.cc:                                      d_gpu_particle_bins.data,
hoomd/SFCPackTunerGPU.cc:                                      d_gpu_sort_order.data,
hoomd/SFCPackTunerGPU.cc:    if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/SFCPackTunerGPU.cc:        CHECK_CUDA_ERROR();
hoomd/SFCPackTunerGPU.cc:void SFCPackTunerGPU::applySortOrder()
hoomd/SFCPackTunerGPU.cc:    assert(m_gpu_sort_order.getNumElements() >= m_pdata->getN());
hoomd/SFCPackTunerGPU.cc:        ArrayHandle<unsigned int> d_gpu_sort_order(m_gpu_sort_order,
hoomd/SFCPackTunerGPU.cc:        kernel::gpu_apply_sorted_order(m_pdata->getN(),
hoomd/SFCPackTunerGPU.cc:                                       d_gpu_sort_order.data,
hoomd/SFCPackTunerGPU.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/SFCPackTunerGPU.cc:            CHECK_CUDA_ERROR();
hoomd/SFCPackTunerGPU.cc:void export_SFCPackTunerGPU(pybind11::module& m)
hoomd/SFCPackTunerGPU.cc:    pybind11::class_<SFCPackTunerGPU, SFCPackTuner, std::shared_ptr<SFCPackTunerGPU>>(
hoomd/SFCPackTunerGPU.cc:        "SFCPackTunerGPU")
hoomd/CellListGPU.cc:/*! \file CellListGPU.cc
hoomd/CellListGPU.cc:    \brief Defines CellListGPU
hoomd/CellListGPU.cc:#include "CellListGPU.h"
hoomd/CellListGPU.cc:#include "CellListGPU.cuh"
hoomd/CellListGPU.cc:CellListGPU::CellListGPU(std::shared_ptr<SystemDefinition> sysdef)
hoomd/CellListGPU.cc:    if (!m_exec_conf->isCUDAEnabled())
hoomd/CellListGPU.cc:            << "Creating a CellListGPU with no GPU in the execution configuration" << endl;
hoomd/CellListGPU.cc:        throw std::runtime_error("Error initializing CellListGPU");
hoomd/CellListGPU.cc:void CellListGPU::computeCellList()
hoomd/CellListGPU.cc:    unsigned int ngpu = m_exec_conf->getNumActiveGPUs();
hoomd/CellListGPU.cc:        // access the per-GPU cell list arrays (only needed with ngpu>1)
hoomd/CellListGPU.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/CellListGPU.cc:            CHECK_CUDA_ERROR();
hoomd/CellListGPU.cc:        if (ngpu > 1 || m_per_device)
hoomd/CellListGPU.cc:            if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/CellListGPU.cc:                CHECK_CUDA_ERROR();
hoomd/CellListGPU.cc:        m_exec_conf->beginMultiGPU();
hoomd/CellListGPU.cc:        // compute cell list, and write to temporary arrays with multi-GPU
hoomd/CellListGPU.cc:        gpu_compute_cell_list(
hoomd/CellListGPU.cc:            (ngpu == 1 && !m_per_device) ? d_cell_size.data : d_cell_size_scratch.data,
hoomd/CellListGPU.cc:            (ngpu == 1 && !m_per_device) ? d_xyzf.data : d_xyzf_scratch.data,
hoomd/CellListGPU.cc:            (ngpu == 1 && !m_per_device) ? d_type_body.data : d_type_body_scratch.data,
hoomd/CellListGPU.cc:            (ngpu == 1 && !m_per_device) ? d_cell_orientation.data
hoomd/CellListGPU.cc:            (ngpu == 1 && !m_per_device) ? d_cell_idx.data : d_cell_idx_scratch.data,
hoomd/CellListGPU.cc:            m_pdata->getGPUPartition());
hoomd/CellListGPU.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/CellListGPU.cc:            CHECK_CUDA_ERROR();
hoomd/CellListGPU.cc:        m_exec_conf->endMultiGPU();
hoomd/CellListGPU.cc:        // access the per-GPU cell list arrays (only needed with ngpu>1)
hoomd/CellListGPU.cc:        for (unsigned int i = 0; i < m_exec_conf->getNumActiveGPUs(); ++i)
hoomd/CellListGPU.cc:            gpu_sort_cell_list(
hoomd/CellListGPU.cc:                (ngpu == 1 && !m_per_device)
hoomd/CellListGPU.cc:                ((ngpu == 1 && !m_per_device) || !d_xyzf.data)
hoomd/CellListGPU.cc:                ((ngpu == 1 && !m_per_device) || !d_type_body.data)
hoomd/CellListGPU.cc:                ((ngpu == 1 && !m_per_device) || !d_cell_orientation.data)
hoomd/CellListGPU.cc:                ((ngpu == 1 && !m_per_device) || !d_cell_idx.data)
hoomd/CellListGPU.cc:            if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/CellListGPU.cc:                CHECK_CUDA_ERROR();
hoomd/CellListGPU.cc:    if (ngpu > 1 && !m_per_device)
hoomd/CellListGPU.cc:void CellListGPU::combineCellLists()
hoomd/CellListGPU.cc:    // access the per-GPU cell list arrays (only needed with per-device cell list)
hoomd/CellListGPU.cc:    // have to wait for all GPUs to sync up, to have cell sizes available
hoomd/CellListGPU.cc:    m_exec_conf->beginMultiGPU();
hoomd/CellListGPU.cc:    gpu_combine_cell_lists(d_cell_size_scratch.data,
hoomd/CellListGPU.cc:                           m_exec_conf->getNumActiveGPUs(),
hoomd/CellListGPU.cc:                           m_pdata->getGPUPartition());
hoomd/CellListGPU.cc:    m_exec_conf->endMultiGPU();
hoomd/CellListGPU.cc:void CellListGPU::initializeMemory()
hoomd/CellListGPU.cc:    // only need to keep separate cell lists with more than one GPU
hoomd/CellListGPU.cc:    unsigned int ngpu = m_exec_conf->getNumActiveGPUs();
hoomd/CellListGPU.cc:    if (ngpu == 1 && !m_per_device)
hoomd/CellListGPU.cc:    m_exec_conf->msg->notice(10) << "CellListGPU initialize multiGPU memory" << endl;
hoomd/CellListGPU.cc:        cudaMemAdvise(m_cell_adj.get(),
hoomd/CellListGPU.cc:                      cudaMemAdviseSetReadMostly,
hoomd/CellListGPU.cc:        CHECK_CUDA_ERROR();
hoomd/CellListGPU.cc:    GlobalArray<unsigned int> cell_size_scratch(m_cell_indexer.getNumElements() * ngpu,
hoomd/CellListGPU.cc:        GlobalArray<Scalar4> xyzf_scratch(m_cell_list_indexer.getNumElements() * ngpu, m_exec_conf);
hoomd/CellListGPU.cc:        GlobalArray<uint2> type_body_scratch(m_cell_list_indexer.getNumElements() * ngpu,
hoomd/CellListGPU.cc:        GlobalArray<Scalar4> orientation_scratch(m_cell_list_indexer.getNumElements() * ngpu,
hoomd/CellListGPU.cc:        GlobalArray<unsigned int> idx_scratch(m_cell_list_indexer.getNumElements() * ngpu,
hoomd/CellListGPU.cc:        // map cell list arrays into memory of all active GPUs
hoomd/CellListGPU.cc:        auto& gpu_map = m_exec_conf->getGPUIds();
hoomd/CellListGPU.cc:        for (unsigned int idev = 0; idev < m_exec_conf->getNumActiveGPUs(); ++idev)
hoomd/CellListGPU.cc:            cudaMemAdvise(m_cell_size.get(),
hoomd/CellListGPU.cc:                          cudaMemAdviseSetAccessedBy,
hoomd/CellListGPU.cc:                          gpu_map[idev]);
hoomd/CellListGPU.cc:        for (unsigned int idev = 0; idev < m_exec_conf->getNumActiveGPUs(); ++idev)
hoomd/CellListGPU.cc:            cudaMemAdvise(m_cell_size_scratch.get() + idev * m_cell_indexer.getNumElements(),
hoomd/CellListGPU.cc:                          cudaMemAdviseSetPreferredLocation,
hoomd/CellListGPU.cc:                          gpu_map[idev]);
hoomd/CellListGPU.cc:                cudaMemAdvise(m_idx_scratch.get() + idev * m_cell_list_indexer.getNumElements(),
hoomd/CellListGPU.cc:                              cudaMemAdviseSetPreferredLocation,
hoomd/CellListGPU.cc:                              gpu_map[idev]);
hoomd/CellListGPU.cc:                cudaMemAdvise(m_xyzf_scratch.get() + idev * m_cell_list_indexer.getNumElements(),
hoomd/CellListGPU.cc:                              cudaMemAdviseSetPreferredLocation,
hoomd/CellListGPU.cc:                              gpu_map[idev]);
hoomd/CellListGPU.cc:                cudaMemAdvise(m_type_body_scratch.get()
hoomd/CellListGPU.cc:                              cudaMemAdviseSetPreferredLocation,
hoomd/CellListGPU.cc:                              gpu_map[idev]);
hoomd/CellListGPU.cc:                cudaMemAdvise(m_orientation_scratch.get()
hoomd/CellListGPU.cc:                              cudaMemAdviseSetPreferredLocation,
hoomd/CellListGPU.cc:                              gpu_map[idev]);
hoomd/CellListGPU.cc:            cudaMemPrefetchAsync(m_cell_size_scratch.get() + idev * m_cell_indexer.getNumElements(),
hoomd/CellListGPU.cc:                                 gpu_map[idev]);
hoomd/CellListGPU.cc:                cudaMemPrefetchAsync(m_idx_scratch.get()
hoomd/CellListGPU.cc:                                     gpu_map[idev]);
hoomd/CellListGPU.cc:                cudaMemPrefetchAsync(m_xyzf_scratch.get()
hoomd/CellListGPU.cc:                                     gpu_map[idev]);
hoomd/CellListGPU.cc:                cudaMemPrefetchAsync(m_type_body_scratch.get()
hoomd/CellListGPU.cc:                                     gpu_map[idev]);
hoomd/CellListGPU.cc:                cudaMemPrefetchAsync(m_orientation_scratch.get()
hoomd/CellListGPU.cc:                                     gpu_map[idev]);
hoomd/CellListGPU.cc:        CHECK_CUDA_ERROR();
hoomd/CellListGPU.cc:void export_CellListGPU(pybind11::module& m)
hoomd/CellListGPU.cc:    pybind11::class_<CellListGPU, CellList, std::shared_ptr<CellListGPU>>(m, "CellListGPU")
hoomd/ExecutionConfiguration.cc:#include <cuda_runtime.h>
hoomd/ExecutionConfiguration.cc:bool ExecutionConfiguration::s_gpu_scan_complete = false;
hoomd/ExecutionConfiguration.cc:std::vector<std::string> ExecutionConfiguration::s_gpu_scan_messages;
hoomd/ExecutionConfiguration.cc:std::vector<int> ExecutionConfiguration::s_capable_gpu_ids;
hoomd/ExecutionConfiguration.cc:std::vector<std::string> ExecutionConfiguration::s_capable_gpu_descriptions;
hoomd/ExecutionConfiguration.cc:/*! \param mode Execution mode to set (cpu or gpu)
hoomd/ExecutionConfiguration.cc:    \param gpu_id List of GPU IDs on which to run, or empty for automatic selection
hoomd/ExecutionConfiguration.cc:    Explicitly force the use of either CPU or GPU execution. If GPU execution is selected, then a
hoomd/ExecutionConfiguration.cc:   default GPU choice is made by not calling hipSetDevice.
hoomd/ExecutionConfiguration.cc:                                               std::vector<int> gpu_id,
hoomd/ExecutionConfiguration.cc:    for (auto it = gpu_id.begin(); it != gpu_id.end(); ++it)
hoomd/ExecutionConfiguration.cc:    // scan the available GPUs
hoomd/ExecutionConfiguration.cc:    scanGPUs();
hoomd/ExecutionConfiguration.cc:    unsigned int dev_count = (unsigned int)s_capable_gpu_ids.size();
hoomd/ExecutionConfiguration.cc:        // if there are available GPUs, initialize them. Otherwise, default to running on the CPU
hoomd/ExecutionConfiguration.cc:            exec_mode = GPU;
hoomd/ExecutionConfiguration.cc:    m_concurrent = exec_mode == GPU;
hoomd/ExecutionConfiguration.cc:    m_in_multigpu_block = false;
hoomd/ExecutionConfiguration.cc:    // now, exec_mode should be either CPU or GPU - proceed with initialization
hoomd/ExecutionConfiguration.cc:    // initialize the GPU if that mode was requested
hoomd/ExecutionConfiguration.cc:    if (exec_mode == GPU)
hoomd/ExecutionConfiguration.cc:        if (!gpu_id.size() && using_mpi)
hoomd/ExecutionConfiguration.cc:            // if we found a local rank, use that to select the GPU
hoomd/ExecutionConfiguration.cc:            gpu_id.push_back((local_rank % dev_count));
hoomd/ExecutionConfiguration.cc:            s << "Selected GPU " << gpu_id[0] << " by MPI rank (" << dev_count << " available)."
hoomd/ExecutionConfiguration.cc:        if (!gpu_id.size())
hoomd/ExecutionConfiguration.cc:            // auto-detect a single GPU
hoomd/ExecutionConfiguration.cc:            msg->collectiveNoticeStr(4, "Asking the driver to choose a GPU.\n");
hoomd/ExecutionConfiguration.cc:            initializeGPU(-1);
hoomd/ExecutionConfiguration.cc:            // initialize all requested GPUs
hoomd/ExecutionConfiguration.cc:            for (auto it = gpu_id.begin(); it != gpu_id.end(); ++it)
hoomd/ExecutionConfiguration.cc:                initializeGPU(*it);
hoomd/ExecutionConfiguration.cc:    if (exec_mode == GPU)
hoomd/ExecutionConfiguration.cc:        throw runtime_error("This build of HOOMD does not include GPU support.");
hoomd/ExecutionConfiguration.cc:    if (exec_mode == GPU)
hoomd/ExecutionConfiguration.cc:        if (!m_concurrent && gpu_id.size() > 1)
hoomd/ExecutionConfiguration.cc:            throw runtime_error("Multi-GPU execution requested, but not all GPUs support "
hoomd/ExecutionConfiguration.cc:        // disable managed memory when running on single GPU
hoomd/ExecutionConfiguration.cc:        if (m_gpu_id.size() == 1)
hoomd/ExecutionConfiguration.cc:            for (unsigned int idev = 0; idev < gpu_id.size(); ++idev)
hoomd/ExecutionConfiguration.cc:                    // the autotuner may pick up different block sizes for different GPUs
hoomd/ExecutionConfiguration.cc:                    msg->warning() << "Multi-GPU execution requested, but GPUs have differing "
hoomd/ExecutionConfiguration.cc:        hipSetDevice(m_gpu_id[0]);
hoomd/ExecutionConfiguration.cc:    m_events.resize(m_gpu_id.size());
hoomd/ExecutionConfiguration.cc:    for (int idev = (unsigned int)(m_gpu_id.size() - 1); idev >= 0; --idev)
hoomd/ExecutionConfiguration.cc:        hipSetDevice(m_gpu_id[idev]);
hoomd/ExecutionConfiguration.cc:    for (int idev = (unsigned int)(m_gpu_id.size() - 1); idev >= 0; --idev)
hoomd/ExecutionConfiguration.cc:    if (exec_mode == GPU)
hoomd/ExecutionConfiguration.cc:        cudaError_t cuda_error = cudaPeekAtLastError();
hoomd/ExecutionConfiguration.cc:        s << "CUDA Error: " << string(cudaGetErrorString(cuda_error));
hoomd/ExecutionConfiguration.cc:/*! \param gpu_id Index for the GPU to initialize, set to -1 for automatic selection
hoomd/ExecutionConfiguration.cc:    initializeGPU will loop through the specified list of GPUs, validate that each one is available
hoomd/ExecutionConfiguration.cc:   for CUDA use and then setup CUDA to use the given GPU. After initializeGPU completes, hip calls
hoomd/ExecutionConfiguration.cc:void ExecutionConfiguration::initializeGPU(int gpu_id)
hoomd/ExecutionConfiguration.cc:    int capable_count = (int)s_capable_gpu_ids.size();
hoomd/ExecutionConfiguration.cc:        s << "No supported GPUs are present on this system." << std::endl;
hoomd/ExecutionConfiguration.cc:        for (const auto& msg : s_gpu_scan_messages)
hoomd/ExecutionConfiguration.cc:    if (gpu_id < -1)
hoomd/ExecutionConfiguration.cc:        s << "Invalid device ID " << gpu_id << " (Use -1 to autoselect a GPU).";
hoomd/ExecutionConfiguration.cc:    if (gpu_id >= (int)s_capable_gpu_ids.size())
hoomd/ExecutionConfiguration.cc:        s << "Invalid device ID " << gpu_id << " - select a valid device from:" << std::endl;
hoomd/ExecutionConfiguration.cc:        for (const auto& desc : s_capable_gpu_descriptions)
hoomd/ExecutionConfiguration.cc:        for (const auto& msg : s_gpu_scan_messages)
hoomd/ExecutionConfiguration.cc:    if (gpu_id != -1)
hoomd/ExecutionConfiguration.cc:        cudaSetValidDevices(&s_capable_gpu_ids[gpu_id], 1);
hoomd/ExecutionConfiguration.cc:        hipSetDevice(s_capable_gpu_ids[gpu_id]);
hoomd/ExecutionConfiguration.cc:            // initialize the default CUDA context from one of the capable GPUs
hoomd/ExecutionConfiguration.cc:        cudaSetValidDevices(&s_capable_gpu_ids[0], (int)s_capable_gpu_ids.size());
hoomd/ExecutionConfiguration.cc:    int hip_gpu_id;
hoomd/ExecutionConfiguration.cc:    hipGetDevice(&hip_gpu_id);
hoomd/ExecutionConfiguration.cc:    // add to list of active GPUs
hoomd/ExecutionConfiguration.cc:    m_gpu_id.push_back(hip_gpu_id);
hoomd/ExecutionConfiguration.cc:std::string ExecutionConfiguration::describeGPU(int id, hipDeviceProp_t prop)
hoomd/ExecutionConfiguration.cc:void ExecutionConfiguration::scanGPUs()
hoomd/ExecutionConfiguration.cc:    if (s_gpu_scan_complete)
hoomd/ExecutionConfiguration.cc:    s_gpu_scan_complete = true;
hoomd/ExecutionConfiguration.cc:    // determine the number of GPUs that CUDA thinks there is
hoomd/ExecutionConfiguration.cc:        std::string message = "Failed to get GPU device count: ";
hoomd/ExecutionConfiguration.cc:        cudaError_t cuda_error = cudaPeekAtLastError();
hoomd/ExecutionConfiguration.cc:        message += string(cudaGetErrorString(cuda_error));
hoomd/ExecutionConfiguration.cc:        s_gpu_scan_messages.push_back(message);
hoomd/ExecutionConfiguration.cc:        s_gpu_scan_messages.push_back("The GPU runtime reports there are 0 devices.");
hoomd/ExecutionConfiguration.cc:    // loop through each GPU and check it's properties
hoomd/ExecutionConfiguration.cc:            cudaError_t cuda_error = cudaPeekAtLastError();
hoomd/ExecutionConfiguration.cc:            message += string(cudaGetErrorString(cuda_error));
hoomd/ExecutionConfiguration.cc:            s_gpu_scan_messages.push_back(message);
hoomd/ExecutionConfiguration.cc:        // exclude a GPU if it's compute version is not high enough
hoomd/ExecutionConfiguration.cc:        if (compoundComputeVer < CUDA_ARCH)
hoomd/ExecutionConfiguration.cc:            s_gpu_scan_messages.push_back(s.str());
hoomd/ExecutionConfiguration.cc:        // exclude a gpu if it is compute-prohibited
hoomd/ExecutionConfiguration.cc:            s_gpu_scan_messages.push_back(s.str());
hoomd/ExecutionConfiguration.cc:        // exclude a GPU when it doesn't support mapped memory
hoomd/ExecutionConfiguration.cc:        cudaError_t cuda_error
hoomd/ExecutionConfiguration.cc:            = cudaDeviceGetAttribute(&supports_managed_memory, cudaDevAttrManagedMemory, dev);
hoomd/ExecutionConfiguration.cc:        if (cuda_error != cudaSuccess)
hoomd/ExecutionConfiguration.cc:            s_gpu_scan_messages.push_back("Failed to get device attribute: "
hoomd/ExecutionConfiguration.cc:                                          + string(cudaGetErrorString(cuda_error)));
hoomd/ExecutionConfiguration.cc:            s_gpu_scan_messages.push_back(s.str());
hoomd/ExecutionConfiguration.cc:        s_capable_gpu_descriptions.push_back(describeGPU((int)s_capable_gpu_ids.size(), prop));
hoomd/ExecutionConfiguration.cc:        s_capable_gpu_ids.push_back(dev);
hoomd/ExecutionConfiguration.cc:/*! Print out GPU stats if running on the GPU, otherwise determine and print out the CPU stats
hoomd/ExecutionConfiguration.cc:    if (exec_mode == GPU)
hoomd/ExecutionConfiguration.cc:        m_dev_prop.resize(m_gpu_id.size());
hoomd/ExecutionConfiguration.cc:        for (int idev = (unsigned int)(m_gpu_id.size() - 1); idev >= 0; idev--)
hoomd/ExecutionConfiguration.cc:            hipSetDevice(m_gpu_id[idev]);
hoomd/ExecutionConfiguration.cc:            hipGetDeviceProperties(&m_dev_prop[idev], m_gpu_id[idev]);
hoomd/ExecutionConfiguration.cc:            // CUDA API
hoomd/ExecutionConfiguration.cc:            cudaDeviceProp cuda_prop;
hoomd/ExecutionConfiguration.cc:            cudaError_t error = cudaGetDeviceProperties(&cuda_prop, m_gpu_id[idev]);
hoomd/ExecutionConfiguration.cc:            if (error != cudaSuccess)
hoomd/ExecutionConfiguration.cc:                                    + string(cudaGetErrorString(error)));
hoomd/ExecutionConfiguration.cc:            if (cuda_prop.concurrentManagedAccess)
hoomd/ExecutionConfiguration.cc:            m_active_device_descriptions.push_back(describeGPU(m_gpu_id[idev], m_dev_prop[idev]));
hoomd/ExecutionConfiguration.cc:void ExecutionConfiguration::multiGPUBarrier() const
hoomd/ExecutionConfiguration.cc:    if (getNumActiveGPUs() > 1)
hoomd/ExecutionConfiguration.cc:        // record the synchronization point on every GPU after the last kernel has finished, count
hoomd/ExecutionConfiguration.cc:        for (int idev = (unsigned int)(m_gpu_id.size() - 1); idev >= 0; --idev)
hoomd/ExecutionConfiguration.cc:            hipSetDevice(m_gpu_id[idev]);
hoomd/ExecutionConfiguration.cc:        // wait for all those events on all GPUs
hoomd/ExecutionConfiguration.cc:        for (int idev_i = (unsigned int)(m_gpu_id.size() - 1); idev_i >= 0; --idev_i)
hoomd/ExecutionConfiguration.cc:            hipSetDevice(m_gpu_id[idev_i]);
hoomd/ExecutionConfiguration.cc:            for (int idev_j = 0; idev_j < (int)m_gpu_id.size(); ++idev_j)
hoomd/ExecutionConfiguration.cc:void ExecutionConfiguration::beginMultiGPU() const
hoomd/ExecutionConfiguration.cc:    m_in_multigpu_block = true;
hoomd/ExecutionConfiguration.cc:    if (getNumActiveGPUs() > 1)
hoomd/ExecutionConfiguration.cc:        // record a syncrhonization point on GPU 0
hoomd/ExecutionConfiguration.cc:        // wait for that event on all GPUs (except GPU 0, for which we rely on implicit
hoomd/ExecutionConfiguration.cc:        for (int idev = (unsigned int)(m_gpu_id.size() - 1); idev >= 1; --idev)
hoomd/ExecutionConfiguration.cc:            hipSetDevice(m_gpu_id[idev]);
hoomd/ExecutionConfiguration.cc:        // set GPU 0
hoomd/ExecutionConfiguration.cc:        hipSetDevice(m_gpu_id[0]);
hoomd/ExecutionConfiguration.cc:        if (isCUDAErrorCheckingEnabled())
hoomd/ExecutionConfiguration.cc:void ExecutionConfiguration::endMultiGPU() const
hoomd/ExecutionConfiguration.cc:    m_in_multigpu_block = false;
hoomd/ExecutionConfiguration.cc:    if (getNumActiveGPUs() > 1)
hoomd/ExecutionConfiguration.cc:        // record the synchronization point on every GPU, except GPU 0
hoomd/ExecutionConfiguration.cc:        for (int idev = (unsigned int)(m_gpu_id.size() - 1); idev >= 1; --idev)
hoomd/ExecutionConfiguration.cc:            hipSetDevice(m_gpu_id[idev]);
hoomd/ExecutionConfiguration.cc:        // wait for these events on GPU 0
hoomd/ExecutionConfiguration.cc:        hipSetDevice(m_gpu_id[0]);
hoomd/ExecutionConfiguration.cc:        for (int idev = (unsigned int)(m_gpu_id.size() - 1); idev >= 1; --idev)
hoomd/ExecutionConfiguration.cc:        if (isCUDAErrorCheckingEnabled())
hoomd/ExecutionConfiguration.cc:    msg->notice(3) << "Using global rank to select GPUs" << std::endl;
hoomd/ExecutionConfiguration.cc:        .def("isCUDAEnabled", &ExecutionConfiguration::isCUDAEnabled)
hoomd/ExecutionConfiguration.cc:        .def("setCUDAErrorChecking", &ExecutionConfiguration::setCUDAErrorChecking)
hoomd/ExecutionConfiguration.cc:        .def("isCUDAErrorCheckingEnabled", &ExecutionConfiguration::isCUDAErrorCheckingEnabled)
hoomd/ExecutionConfiguration.cc:        .def("getNumActiveGPUs", &ExecutionConfiguration::getNumActiveGPUs)
hoomd/ExecutionConfiguration.cc:        .value("GPU", ExecutionConfiguration::executionMode::GPU)
hoomd/MeshGroupData.h:#include "GPUVector.h"
hoomd/CMakeLists.txt:                   CommunicatorGPU.cc
hoomd/CMakeLists.txt:    BoxResizeUpdaterGPU.cuh
hoomd/CMakeLists.txt:    BoxResizeUpdaterGPU.h
hoomd/CMakeLists.txt:    CellListGPU.cuh
hoomd/CMakeLists.txt:    CellListGPU.h
hoomd/CMakeLists.txt:    CommunicatorGPU.cuh
hoomd/CMakeLists.txt:    CommunicatorGPU.h
hoomd/CMakeLists.txt:    GPUArray.h
hoomd/CMakeLists.txt:    GPUFlags.h
hoomd/CMakeLists.txt:    GPUPartition.cuh
hoomd/CMakeLists.txt:    GPUVector.h
hoomd/CMakeLists.txt:    LoadBalancerGPU.cuh
hoomd/CMakeLists.txt:    LoadBalancerGPU.h
hoomd/CMakeLists.txt:    SFCPackTunerGPU.cuh
hoomd/CMakeLists.txt:    SFCPackTunerGPU.h
hoomd/CMakeLists.txt:list(APPEND _hoomd_sources BoxResizeUpdaterGPU.cc
hoomd/CMakeLists.txt:                           CellListGPU.cc
hoomd/CMakeLists.txt:                           CommunicatorGPU.cc
hoomd/CMakeLists.txt:                           LoadBalancerGPU.cc
hoomd/CMakeLists.txt:                           SFCPackTunerGPU.cc
hoomd/CMakeLists.txt:                      BoxResizeUpdaterGPU.cu
hoomd/CMakeLists.txt:                      CellListGPU.cu
hoomd/CMakeLists.txt:                      CommunicatorGPU.cu
hoomd/CMakeLists.txt:                      LoadBalancerGPU.cu
hoomd/CMakeLists.txt:                      SFCPackTunerGPU.cu)
hoomd/CMakeLists.txt:set(_cuda_sources ${_hoomd_cu_sources})
hoomd/CMakeLists.txt:hoomd_add_module(_hoomd SHARED module.cc ${_hoomd_sources} ${_cuda_sources} ${_hoomd_headers} NO_EXTRAS)
hoomd/CMakeLists.txt:    target_compile_options(_hoomd PUBLIC $<$<AND:$<COMPILE_LANGUAGE:CUDA>,$<STREQUAL:${HIP_PLATFORM},nvcc>>:-Xcompiler=>;-fsized-deallocation)
hoomd/CMakeLists.txt:# Libraries and compile definitions for CUDA enabled builds
hoomd/CMakeLists.txt:        target_link_libraries(_hoomd PUBLIC CUDA::cudart CUDA::cufft)
hoomd/CMakeLists.txt:    target_compile_definitions(_hoomd PUBLIC ENABLE_HIP CUDA_ARCH=${_cuda_min_arch})
hoomd/CMakeLists.txt:        target_link_libraries(_hoomd PUBLIC CUDA::nvToolsExt)
hoomd/Index1D.h:   used in CUDA. The decision is to go with x,y for consistency.
hoomd/ExecutionConfiguration.h:    ExecutionConfiguration is a data structure needed to support the hybrid CPU/GPU code. It
hoomd/ExecutionConfiguration.h:   initializes the CUDA GPU (if requested), stores information about the GPU on which this
hoomd/ExecutionConfiguration.h:    different compute classes are free to fall back on CPU implementations if no GPU is available.
hoomd/ExecutionConfiguration.h:   However, <b>ABSOLUTELY NO</b> CUDA calls should be made if exec_mode is set to CPU - making a
hoomd/ExecutionConfiguration.h:   CUDA call will initialize a GPU context and will error out on machines that do not have GPUs.
hoomd/ExecutionConfiguration.h:   isCUDAEnabled() is a convenience function to interpret the exec_mode and test if CUDA calls can
hoomd/ExecutionConfiguration.h:        GPU,  //!< Execute on the GPU
hoomd/ExecutionConfiguration.h:        AUTO, //!< Auto select between GPU and CPU
hoomd/ExecutionConfiguration.h:                           std::vector<int> gpu_id = std::vector<int>(),
hoomd/ExecutionConfiguration.h:    //! Returns true if CUDA is enabled
hoomd/ExecutionConfiguration.h:    bool isCUDAEnabled() const
hoomd/ExecutionConfiguration.h:        return (exec_mode == GPU);
hoomd/ExecutionConfiguration.h:    //! Returns true if CUDA error checking is enabled
hoomd/ExecutionConfiguration.h:    bool isCUDAErrorCheckingEnabled() const
hoomd/ExecutionConfiguration.h:    void setCUDAErrorChecking(bool hip_error_checking)
hoomd/ExecutionConfiguration.h:    //! Get the number of active GPUs
hoomd/ExecutionConfiguration.h:    unsigned int getNumActiveGPUs() const
hoomd/ExecutionConfiguration.h:        return (unsigned int)m_gpu_id.size();
hoomd/ExecutionConfiguration.h:    //! Get the IDs of the active GPUs
hoomd/ExecutionConfiguration.h:    const std::vector<unsigned int>& getGPUIds() const
hoomd/ExecutionConfiguration.h:        return m_gpu_id;
hoomd/ExecutionConfiguration.h:        for (int idev = (unsigned int)(m_gpu_id.size() - 1); idev >= 0; idev--)
hoomd/ExecutionConfiguration.h:            hipSetDevice(m_gpu_id[idev]);
hoomd/ExecutionConfiguration.h:        for (int idev = (unsigned int)(m_gpu_id.size() - 1); idev >= 0; idev--)
hoomd/ExecutionConfiguration.h:            hipSetDevice(m_gpu_id[idev]);
hoomd/ExecutionConfiguration.h:    //! Sync up all active GPUs
hoomd/ExecutionConfiguration.h:    void multiGPUBarrier() const;
hoomd/ExecutionConfiguration.h:    //! Begin a multi-GPU section
hoomd/ExecutionConfiguration.h:    void beginMultiGPU() const;
hoomd/ExecutionConfiguration.h:    //! End a multi-GPU section
hoomd/ExecutionConfiguration.h:    void endMultiGPU() const;
hoomd/ExecutionConfiguration.h:    hipDeviceProp_t dev_prop; //!< Cached device properties of the first GPU
hoomd/ExecutionConfiguration.h:    /// Compute capability of the GPU formatted as a tuple (major, minor)
hoomd/ExecutionConfiguration.h:    std::pair<unsigned int, unsigned int> getComputeCapability(unsigned int igpu = 0) const;
hoomd/ExecutionConfiguration.h:    //! Returns true if we are in a multi-GPU block
hoomd/ExecutionConfiguration.h:    bool inMultiGPUBlock() const
hoomd/ExecutionConfiguration.h:        return m_in_multigpu_block;
hoomd/ExecutionConfiguration.h:        scanGPUs();
hoomd/ExecutionConfiguration.h:        return s_capable_gpu_descriptions;
hoomd/ExecutionConfiguration.h:        scanGPUs();
hoomd/ExecutionConfiguration.h:        return s_gpu_scan_messages;
hoomd/ExecutionConfiguration.h:    //! Guess local rank of this processor, used for GPU initialization
hoomd/ExecutionConfiguration.h:    //! Initialize the GPU with the given id (where gpu_id is an index into s_capable_gpu_ids)
hoomd/ExecutionConfiguration.h:    void initializeGPU(int gpu_id);
hoomd/ExecutionConfiguration.h:    /// Provide a string that describes a GPU device
hoomd/ExecutionConfiguration.h:    static std::string describeGPU(int id, hipDeviceProp_t prop);
hoomd/ExecutionConfiguration.h:    /** Scans through all GPUs reported by CUDA and marks if they are available
hoomd/ExecutionConfiguration.h:        Determine which GPUs are available for use by HOOMD.
hoomd/ExecutionConfiguration.h:        @post Populate s_gpu_scan_complete, s_gpu_scan_messages, s_gpu_list, and
hoomd/ExecutionConfiguration.h:        s_capable_gpu_descriptions.
hoomd/ExecutionConfiguration.h:    static void scanGPUs();
hoomd/ExecutionConfiguration.h:    std::vector<hipEvent_t> m_events; //!< A list of events to synchronize between GPUs
hoomd/ExecutionConfiguration.h:    /// IDs of active GPUs
hoomd/ExecutionConfiguration.h:    std::vector<unsigned int> m_gpu_id;
hoomd/ExecutionConfiguration.h:    /// Device configuration of active GPUs
hoomd/ExecutionConfiguration.h:    /// True when GPU error checking is enabled
hoomd/ExecutionConfiguration.h:    static bool s_gpu_scan_complete;
hoomd/ExecutionConfiguration.h:    static std::vector<std::string> s_gpu_scan_messages;
hoomd/ExecutionConfiguration.h:    static std::vector<int> s_capable_gpu_ids;
hoomd/ExecutionConfiguration.h:    /// Description of the GPU devices
hoomd/ExecutionConfiguration.h:    static std::vector<std::string> s_capable_gpu_descriptions;
hoomd/ExecutionConfiguration.h:    bool m_concurrent; //!< True if all GPUs have concurrentManagedAccess flag
hoomd/ExecutionConfiguration.h:    mutable bool m_in_multigpu_block; //!< Tracks whether we are in a multi-GPU block
hoomd/ExecutionConfiguration.h:    //! Setup and print out stats on the chosen CPUs/GPUs
hoomd/ExecutionConfiguration.h:#define CHECK_CUDA_ERROR()                                                            \
hoomd/ExecutionConfiguration.h:        auto gpu_map = this->m_exec_conf->getGPUIds();                                \
hoomd/ExecutionConfiguration.h:        for (int idev = this->m_exec_conf->getNumActiveGPUs() - 1; idev >= 0; --idev) \
hoomd/ExecutionConfiguration.h:            hipSetDevice(gpu_map[idev]);                                              \
hoomd/ExecutionConfiguration.h:#define CHECK_CUDA_ERROR()
hoomd/ManagedArray.h://! A device-side, fixed-size array memory-managed through cudaMallocManaged
hoomd/ManagedArray.h:       the host. If the GPU isn't synced up, this can lead to errors, so proper multi-GPU
hoomd/ManagedArray.h:       the host. If the GPU isn't synced up, this can lead to errors, so proper multi-GPU
hoomd/ManagedArray.h:       available on the host. If the GPU isn't synced up, this can lead to errors, so proper
hoomd/ManagedArray.h:       multi-GPU synchronization needs to be ensured
hoomd/ManagedArray.h:       available on the host. If the GPU isn't synced up, this can lead to errors, so proper
hoomd/ManagedArray.h:       multi-GPU synchronization needs to be ensured
hoomd/ManagedArray.h:    //! Attach managed memory to CUDA stream
hoomd/ManagedArray.h:#if defined(__HIP_PLATFORM_NVCC__) && (CUDART_VERSION >= 8000)
hoomd/ManagedArray.h:            cudaMemAdvise(ptr, sizeof(T) * N, cudaMemAdviseSetReadMostly, 0);
hoomd/ManagedArray.h:        // only in GPU code
hoomd/ManagedArray.h:    unsigned int managed;    //!< True if we are CUDA managed
hoomd/BondedGroupData.h:#include "GPUVector.h"
hoomd/BondedGroupData.h:    const GPUVector<members_t>& getMembersArray() const
hoomd/BondedGroupData.h:    const GPUVector<typeval_t>& getTypeValArray() const
hoomd/BondedGroupData.h:    const GPUVector<unsigned int>& getTags() const
hoomd/BondedGroupData.h:    const GPUVector<unsigned int>& getRTags() const
hoomd/BondedGroupData.h:    const GPUVector<ranks_t>& getRanksArray() const
hoomd/BondedGroupData.h:    GPUVector<members_t>& getMembersArray()
hoomd/BondedGroupData.h:    GPUVector<typeval_t>& getTypeValArray()
hoomd/BondedGroupData.h:    GPUVector<unsigned int>& getTags()
hoomd/BondedGroupData.h:    GPUVector<unsigned int>& getRTags()
hoomd/BondedGroupData.h:    GPUVector<ranks_t>& getRanksArray()
hoomd/BondedGroupData.h:     * to enable resizing the underlying GPUVectors before swapping.
hoomd/BondedGroupData.h:    GPUVector<members_t>& getAltMembersArray()
hoomd/BondedGroupData.h:    GPUVector<typeval_t>& getAltTypeValArray()
hoomd/BondedGroupData.h:    GPUVector<unsigned int>& getAltTags()
hoomd/BondedGroupData.h:    GPUVector<ranks_t>& getAltRanksArray()
hoomd/BondedGroupData.h:     * GPU group table
hoomd/BondedGroupData.h:    //! Return GPU bonded groups list
hoomd/BondedGroupData.h:    const GPUVector<members_t>& getGPUTable()
hoomd/BondedGroupData.h:            rebuildGPUTable();
hoomd/BondedGroupData.h:        return m_gpu_table;
hoomd/BondedGroupData.h:    //! Return GPU list of particle in group position
hoomd/BondedGroupData.h:    const GPUArray<unsigned>& getGPUPosTable()
hoomd/BondedGroupData.h:            rebuildGPUTable();
hoomd/BondedGroupData.h:        return m_gpu_pos_table;
hoomd/BondedGroupData.h:    const Index2D& getGPUTableIndexer()
hoomd/BondedGroupData.h:            rebuildGPUTable();
hoomd/BondedGroupData.h:        return m_gpu_table_indexer;
hoomd/BondedGroupData.h:    const GPUArray<unsigned int>& getNGroupsArray() const
hoomd/BondedGroupData.h:        return m_gpu_n_groups;
hoomd/BondedGroupData.h:        // set flag to trigger rebuild of GPU table
hoomd/BondedGroupData.h:    //! Indicate that GPU table needs to be rebuilt
hoomd/BondedGroupData.h:        m_exec_conf;                       //!< Execution configuration for CUDA context
hoomd/BondedGroupData.h:    GPUVector<members_t> m_groups;        //!< List of groups
hoomd/BondedGroupData.h:    GPUVector<typeval_t> m_group_typeval; //!< List of group types/constraint values
hoomd/BondedGroupData.h:    GPUVector<unsigned int> m_group_tag;  //!< List of group tags
hoomd/BondedGroupData.h:    GPUVector<unsigned int> m_group_rtag; //!< Global reverse-lookup table for group tags
hoomd/BondedGroupData.h:    GPUVector<members_t>
hoomd/BondedGroupData.h:        m_gpu_table; //!< Storage for groups by particle index for access on the GPU
hoomd/BondedGroupData.h:    GPUVector<unsigned int> m_gpu_pos_table; //!< Position of particle idx in group table
hoomd/BondedGroupData.h:    Index2D m_gpu_table_indexer;             //!< Indexer for GPU table
hoomd/BondedGroupData.h:    GPUVector<unsigned int> m_gpu_n_groups;  //!< Number of entries in lookup table per particle
hoomd/BondedGroupData.h:    GPUVector<ranks_t> m_group_ranks; //!< 2D list of group member ranks
hoomd/BondedGroupData.h:    GPUVector<members_t> m_groups_alt;        //!< List of groups (swap-in)
hoomd/BondedGroupData.h:    GPUVector<typeval_t> m_group_typeval_alt; //!< List of group types/constraint values (swap-in)
hoomd/BondedGroupData.h:    GPUVector<unsigned int> m_group_tag_alt;  //!< List of group tags (swap-in)
hoomd/BondedGroupData.h:    GPUVector<ranks_t> m_group_ranks_alt; //!< 2D list of group member ranks (swap-in)
hoomd/BondedGroupData.h:    GPUVector<unsigned int>
hoomd/BondedGroupData.h:    GPUArray<unsigned int> m_condition; //!< Condition variable for rebuilding GPU table on the GPU
hoomd/BondedGroupData.h:    unsigned int m_next_flag;           //!< Next flag value for GPU table rebuild
hoomd/BondedGroupData.h:    virtual void rebuildGPUTable();
hoomd/BondedGroupData.h:    //! Helper function to rebuild lookup by index table on the GPU
hoomd/BondedGroupData.h:    virtual void rebuildGPUTableGPU();
hoomd/BondedGroupData.h:        return this->template getLocalBuffer<unsigned int, unsigned int, GPUVector>(
hoomd/BondedGroupData.h:        return this->template getGlobalBuffer<unsigned int, GPUVector>(m_rtags_handle,
hoomd/BondedGroupData.h:            GPUVector>(m_typeval_handle, &GroupData::getTypeValArray, flag, true);
hoomd/BondedGroupData.h:            ->template getLocalBuffer<typename GroupData::members_t, unsigned int, GPUVector>(
hoomd/Autotuner.h://! Autotuner for low level GPU kernel parameters
hoomd/Autotuner.h:/*! Autotuner is autotunes GPU kernel parameters (such as block size) for performance. It runs an
hoomd/Autotuner.h:    question with cudaEvent timers. A number of sweeps are combined with a median to determine the
hoomd/Autotuner.h:    drive the state machine to choose parameters and insert the cuda timing events (when needed).
hoomd/Autotuner.h:    Autotuner is not useful in non-GPU builds. Timing is performed with CUDA events and requires
hoomd/Autotuner.h:        // if we are scanning, record a cuda event - otherwise do nothing
hoomd/Autotuner.h:            if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/Autotuner.h:                CHECK_CUDA_ERROR();
hoomd/Autotuner.h:    hipEvent_t m_start; //!< CUDA event for recording start times
hoomd/Autotuner.h:    hipEvent_t m_stop;  //!< CUDA event for recording end times
hoomd/Autotuner.h:// create CUDA events
hoomd/Autotuner.h:    CHECK_CUDA_ERROR();
hoomd/Autotuner.h:        if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/Autotuner.h:            CHECK_CUDA_ERROR();
hoomd/LoadBalancerGPU.cu:/*! \file LoadBalancerGPU.cu
hoomd/LoadBalancerGPU.cu:    \brief Implementation the GPU functions for load balancing
hoomd/LoadBalancerGPU.cu:#include "LoadBalancerGPU.cuh"
hoomd/LoadBalancerGPU.cu:__global__ void gpu_load_balance_mark_rank_kernel(unsigned int* d_ranks,
hoomd/LoadBalancerGPU.cu: * This simply a kernel driver, see gpu_load_balance_mark_rank_kernel for details.
hoomd/LoadBalancerGPU.cu:void gpu_load_balance_mark_rank(unsigned int* d_ranks,
hoomd/LoadBalancerGPU.cu:    hipFuncGetAttributes(&attr, (const void*)gpu_load_balance_mark_rank_kernel);
hoomd/LoadBalancerGPU.cu:    hipLaunchKernelGGL(gpu_load_balance_mark_rank_kernel,
hoomd/LoadBalancerGPU.cu: * issues with that in mpcd/SorterGPU.cu. As a precaution, I am also replacing this method here.
hoomd/LoadBalancerGPU.cu:unsigned int gpu_load_balance_select_off_rank(unsigned int* d_off_rank,
hoomd/GPUPartition.cuh://! A thin data structure to hold the split of particles across GPUs
hoomd/GPUPartition.cuh:   cause problems when passed across shared library boundaries, such as to a GPU driver function.
hoomd/GPUPartition.cuh:class __attribute__((visibility("default"))) GPUPartition
hoomd/GPUPartition.cuh:    GPUPartition() : m_n_gpu(0), m_gpu_map(nullptr), m_gpu_range(nullptr) { }
hoomd/GPUPartition.cuh:    /*! \param gpu_id Mapping of contiguous device IDs onto CUDA devices
hoomd/GPUPartition.cuh:    GPUPartition(const std::vector<unsigned int>& gpu_id)
hoomd/GPUPartition.cuh:        m_gpu_range = nullptr;
hoomd/GPUPartition.cuh:        m_gpu_map = nullptr;
hoomd/GPUPartition.cuh:        m_n_gpu = (unsigned int)gpu_id.size();
hoomd/GPUPartition.cuh:        if (m_n_gpu != 0)
hoomd/GPUPartition.cuh:            m_gpu_map = new unsigned int[gpu_id.size()];
hoomd/GPUPartition.cuh:            for (unsigned int i = 0; i < m_n_gpu; ++i)
hoomd/GPUPartition.cuh:                m_gpu_map[i] = gpu_id[i];
hoomd/GPUPartition.cuh:            m_gpu_range = new std::pair<unsigned int, unsigned int>[gpu_id.size()];
hoomd/GPUPartition.cuh:            for (unsigned int i = 0; i < gpu_id.size(); ++i)
hoomd/GPUPartition.cuh:                m_gpu_range[i] = std::make_pair(0, 0);
hoomd/GPUPartition.cuh:    virtual ~GPUPartition()
hoomd/GPUPartition.cuh:        if (m_n_gpu)
hoomd/GPUPartition.cuh:            delete[] m_gpu_map;
hoomd/GPUPartition.cuh:            delete[] m_gpu_range;
hoomd/GPUPartition.cuh:    GPUPartition(const GPUPartition& other)
hoomd/GPUPartition.cuh:        m_gpu_range = nullptr;
hoomd/GPUPartition.cuh:        m_gpu_map = nullptr;
hoomd/GPUPartition.cuh:        m_n_gpu = other.m_n_gpu;
hoomd/GPUPartition.cuh:        if (m_n_gpu != 0)
hoomd/GPUPartition.cuh:            m_gpu_range = new std::pair<unsigned int, unsigned int>[m_n_gpu];
hoomd/GPUPartition.cuh:            m_gpu_map = new unsigned int[m_n_gpu];
hoomd/GPUPartition.cuh:        for (unsigned int i = 0; i < m_n_gpu; ++i)
hoomd/GPUPartition.cuh:            m_gpu_range[i] = other.m_gpu_range[i];
hoomd/GPUPartition.cuh:            m_gpu_map[i] = other.m_gpu_map[i];
hoomd/GPUPartition.cuh:    GPUPartition& operator=(const GPUPartition& rhs)
hoomd/GPUPartition.cuh:            m_n_gpu = rhs.m_n_gpu;
hoomd/GPUPartition.cuh:            if (m_n_gpu != 0)
hoomd/GPUPartition.cuh:                m_gpu_range = new std::pair<unsigned int, unsigned int>[m_n_gpu];
hoomd/GPUPartition.cuh:                m_gpu_map = new unsigned int[m_n_gpu];
hoomd/GPUPartition.cuh:                m_gpu_range = nullptr;
hoomd/GPUPartition.cuh:                m_gpu_map = nullptr;
hoomd/GPUPartition.cuh:            for (unsigned int i = 0; i < m_n_gpu; ++i)
hoomd/GPUPartition.cuh:                m_gpu_range[i] = rhs.m_gpu_range[i];
hoomd/GPUPartition.cuh:                m_gpu_map[i] = rhs.m_gpu_map[i];
hoomd/GPUPartition.cuh:    //! Update the number of particles and distribute particles equally between GPUs
hoomd/GPUPartition.cuh:        unsigned int n_per_gpu = N / m_n_gpu;
hoomd/GPUPartition.cuh:        for (unsigned int i = 0; i < m_n_gpu; ++i)
hoomd/GPUPartition.cuh:            m_gpu_range[i] = std::make_pair(offset, offset + n_per_gpu);
hoomd/GPUPartition.cuh:            offset += n_per_gpu;
hoomd/GPUPartition.cuh:        // fill last GPU with remaining particles
hoomd/GPUPartition.cuh:        m_gpu_range[m_n_gpu - 1].second = ini_offset + N;
hoomd/GPUPartition.cuh:    //! Get the number of active GPUs
hoomd/GPUPartition.cuh:    inline unsigned int getNumActiveGPUs() const
hoomd/GPUPartition.cuh:        return m_n_gpu;
hoomd/GPUPartition.cuh:    //! Get the index range for a given GPU
hoomd/GPUPartition.cuh:    /*! \param igpu The logical ID of the GPU
hoomd/GPUPartition.cuh:    inline std::pair<unsigned int, unsigned int> getRange(unsigned int igpu) const
hoomd/GPUPartition.cuh:        if (igpu > m_n_gpu)
hoomd/GPUPartition.cuh:            throw std::runtime_error("GPU " + std::to_string(igpu)
hoomd/GPUPartition.cuh:        return m_gpu_range[igpu];
hoomd/GPUPartition.cuh:    //! Get the index range for a given GPU
hoomd/GPUPartition.cuh:    /*! \param igpu The logical ID of the GPU
hoomd/GPUPartition.cuh:    inline std::pair<unsigned int, unsigned int> getRangeAndSetGPU(unsigned int igpu) const
hoomd/GPUPartition.cuh:        if (igpu > m_n_gpu)
hoomd/GPUPartition.cuh:            throw std::runtime_error("GPU " + std::to_string(igpu)
hoomd/GPUPartition.cuh:        unsigned int gpu_id = m_gpu_map[igpu];
hoomd/GPUPartition.cuh:        // set the active GPU
hoomd/GPUPartition.cuh:        hipSetDevice(gpu_id);
hoomd/GPUPartition.cuh:        return getRange(igpu);
hoomd/GPUPartition.cuh:    unsigned int m_n_gpu;
hoomd/GPUPartition.cuh:    unsigned int* m_gpu_map;
hoomd/GPUPartition.cuh:    std::pair<unsigned int, unsigned int>* m_gpu_range;
hoomd/CellListStencil.cc:    GPUArray<unsigned int> n_stencil(m_pdata->getNTypes(), m_exec_conf);
hoomd/CellListStencil.cc:    GPUArray<Scalar4> stencil(m_pdata->getNTypes(), m_exec_conf);
hoomd/CellListStencil.cc:        GPUArray<Scalar4> stencil(max_n_stencil * m_pdata->getNTypes(), m_exec_conf);
hoomd/HOOMDVersion.h.inc:    /// Determine if ENABLE_GPU is set
hoomd/HOOMDVersion.h.inc:    static bool getEnableGPU();
hoomd/HOOMDVersion.h.inc:    /// Format the cuda API version as a string
hoomd/HOOMDVersion.h.inc:    static std::string getGPUAPIVersion();
hoomd/HOOMDVersion.h.inc:    /// Get the GPU platform
hoomd/HOOMDVersion.h.inc:    static std::string getGPUPlatform();
hoomd/module.cc:// include GPU classes
hoomd/module.cc:#include "BoxResizeUpdaterGPU.h"
hoomd/module.cc:#include "CellListGPU.h"
hoomd/module.cc:#include "LoadBalancerGPU.h"
hoomd/module.cc:#include "SFCPackTunerGPU.h"
hoomd/module.cc:#include "CommunicatorGPU.h"
hoomd/module.cc:char env_enable_mpi_cuda[] = "MV2_USE_CUDA=1";
hoomd/module.cc:        .def_static("getEnableGPU", BuildInfo::getEnableGPU)
hoomd/module.cc:        .def_static("getGPUAPIVersion", BuildInfo::getGPUAPIVersion)
hoomd/module.cc:        .def_static("getGPUPlatform", BuildInfo::getGPUPlatform)
hoomd/module.cc:    export_CellListGPU(m);
hoomd/module.cc:    export_BoxResizeUpdaterGPU(m);
hoomd/module.cc:    export_SFCPackTunerGPU(m);
hoomd/module.cc:    export_LoadBalancerGPU(m);
hoomd/module.cc:    export_CommunicatorGPU(m);
hoomd/HOOMDMath.cc:    // The use of shared_ptr's for exporting CUDA vector types is a workaround
hoomd/CellListGPU.cu:#include "CellListGPU.cuh"
hoomd/CellListGPU.cu:/*! \file CellListGPU.cu
hoomd/CellListGPU.cu:    \brief Defines GPU kernel code for cell list generation on the GPU
hoomd/CellListGPU.cu://! Kernel that computes the cell list on the GPU
hoomd/CellListGPU.cu:__global__ void gpu_compute_cell_list_kernel(unsigned int* d_cell_size,
hoomd/CellListGPU.cu:#if (__CUDA_ARCH__ >= 600)
hoomd/CellListGPU.cu:#if (__CUDA_ARCH__ >= 600)
hoomd/CellListGPU.cu:void gpu_compute_cell_list(unsigned int* d_cell_size,
hoomd/CellListGPU.cu:                           const GPUPartition& gpu_partition)
hoomd/CellListGPU.cu:    hipFuncGetAttributes(&attr, reinterpret_cast<const void*>(&gpu_compute_cell_list_kernel));
hoomd/CellListGPU.cu:    // iterate over active GPUs in reverse, to end up on first GPU when returning from this function
hoomd/CellListGPU.cu:    for (int idev = gpu_partition.getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/CellListGPU.cu:        auto range = gpu_partition.getRangeAndSetGPU(idev);
hoomd/CellListGPU.cu:        if (idev == (int)gpu_partition.getNumActiveGPUs() - 1)
hoomd/CellListGPU.cu:        hipLaunchKernelGGL(HIP_KERNEL_NAME(gpu_compute_cell_list_kernel),
hoomd/CellListGPU.cu:__global__ void gpu_fill_indices_kernel(unsigned int cl_size,
hoomd/CellListGPU.cu://! Kernel to combine ngpu cell lists into one, in parallel
hoomd/CellListGPU.cu:__global__ void gpu_combine_cell_lists_kernel(const unsigned int* d_cell_size_scratch,
hoomd/CellListGPU.cu:                                              unsigned int igpu,
hoomd/CellListGPU.cu:                                              unsigned int ngpu,
hoomd/CellListGPU.cu:    // reduce cell sizes for 0..igpu
hoomd/CellListGPU.cu:    for (unsigned int i = 0; i < ngpu; ++i)
hoomd/CellListGPU.cu:        if (i == igpu)
hoomd/CellListGPU.cu:        if (i < igpu)
hoomd/CellListGPU.cu:    // write out cell size total on GPU 0
hoomd/CellListGPU.cu:    if (igpu == 0 && local_idx == 0)
hoomd/CellListGPU.cu:#if (__CUDA_ARCH__ >= 600)
hoomd/CellListGPU.cu:        d_idx[write_pos] = d_idx_scratch[idx + igpu * cli.getNumElements()];
hoomd/CellListGPU.cu:        d_xyzf[write_pos] = d_xyzf_scratch[idx + igpu * cli.getNumElements()];
hoomd/CellListGPU.cu:        d_type_body[write_pos] = d_type_body_scratch[idx + igpu * cli.getNumElements()];
hoomd/CellListGPU.cu:            = d_cell_orientation_scratch[idx + igpu * cli.getNumElements()];
hoomd/CellListGPU.cu:/*! Driver function to sort the cell lists from different GPUs into one
hoomd/CellListGPU.cu:   \param d_cell_size_scratch List of cell sizes (per GPU)
hoomd/CellListGPU.cu:   \param d_cell_idx_scratch List particle index (per GPU)
hoomd/CellListGPU.cu:   \param block_size GPU block size
hoomd/CellListGPU.cu:   \param gpu_partition multi-GPU partition
hoomd/CellListGPU.cu:hipError_t gpu_combine_cell_lists(const unsigned int* d_cell_size_scratch,
hoomd/CellListGPU.cu:                                  unsigned int ngpu,
hoomd/CellListGPU.cu:                                  const GPUPartition& gpu_partition)
hoomd/CellListGPU.cu:    for (int idev = gpu_partition.getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/CellListGPU.cu:        gpu_partition.getRangeAndSetGPU(idev);
hoomd/CellListGPU.cu:        hipLaunchKernelGGL(HIP_KERNEL_NAME(gpu_combine_cell_lists_kernel),
hoomd/CellListGPU.cu:                           ngpu,
hoomd/CellListGPU.cu:__global__ void gpu_apply_sorted_cell_list_order(unsigned int cl_size,
hoomd/CellListGPU.cu:/*! Driver function to sort the cell list on the GPU
hoomd/CellListGPU.cu:hipError_t gpu_sort_cell_list(unsigned int* d_cell_size,
hoomd/CellListGPU.cu:    hipLaunchKernelGGL(HIP_KERNEL_NAME(gpu_fill_indices_kernel),
hoomd/CellListGPU.cu:    hipLaunchKernelGGL(HIP_KERNEL_NAME(gpu_apply_sorted_cell_list_order),
hoomd/CommunicatorGPU.cuh:/*! \file CommunicatorGPU.cuh
hoomd/CommunicatorGPU.cuh:    \brief Defines the GPU functions of the communication algorithms
hoomd/CommunicatorGPU.cuh:enum gpu_send_flags
hoomd/CommunicatorGPU.cuh:void gpu_stage_particles(const unsigned int n,
hoomd/CommunicatorGPU.cuh:void gpu_sort_migrating_particles(const size_t nsend,
hoomd/CommunicatorGPU.cuh:void gpu_wrap_particles(const unsigned int n_recv, detail::pdata_element* d_in, const BoxDim& box);
hoomd/CommunicatorGPU.cuh:void gpu_reset_rtags(unsigned int n_delete_ptls, unsigned int* d_delete_tags, unsigned int* d_rtag);
hoomd/CommunicatorGPU.cuh:void gpu_make_ghost_exchange_plan(unsigned int* d_plan,
hoomd/CommunicatorGPU.cuh:unsigned int gpu_exchange_ghosts_count_neighbors(unsigned int N,
hoomd/CommunicatorGPU.cuh:void gpu_exchange_ghosts_make_indices(unsigned int N,
hoomd/CommunicatorGPU.cuh:void gpu_exchange_ghosts_pack(unsigned int n_out,
hoomd/CommunicatorGPU.cuh:void gpu_exchange_ghosts_copy_buf(unsigned int n_recv,
hoomd/CommunicatorGPU.cuh:void gpu_compute_ghost_rtags(unsigned int first_idx,
hoomd/CommunicatorGPU.cuh:void gpu_reset_exchange_plan(unsigned int N, unsigned int* d_plan);
hoomd/CommunicatorGPU.cuh:void gpu_mark_groups(unsigned int N,
hoomd/CommunicatorGPU.cuh:void gpu_scatter_ranks_and_mark_send_groups(unsigned int n_groups,
hoomd/CommunicatorGPU.cuh:void gpu_update_ranks_table(unsigned int n_groups,
hoomd/CommunicatorGPU.cuh:void gpu_scatter_and_mark_groups_for_removal(unsigned int n_groups,
hoomd/CommunicatorGPU.cuh:void gpu_remove_groups(unsigned int n_groups,
hoomd/CommunicatorGPU.cuh:void gpu_add_groups(unsigned int n_groups,
hoomd/CommunicatorGPU.cuh:void gpu_mark_bonded_ghosts(unsigned int n_groups,
hoomd/CommunicatorGPU.cuh:void gpu_make_ghost_group_exchange_plan(unsigned int* d_ghost_group_plan,
hoomd/CommunicatorGPU.cuh:void gpu_exchange_ghost_groups_pack(unsigned int n_out,
hoomd/CommunicatorGPU.cuh:void gpu_exchange_ghost_groups_copy_buf(unsigned int nrecv,
hoomd/CommunicatorGPU.cuh:void gpu_exchange_ghosts_pack_netforce(unsigned int n_out,
hoomd/CommunicatorGPU.cuh:void gpu_exchange_ghosts_copy_netforce_buf(unsigned int n_recv,
hoomd/CommunicatorGPU.cuh:void gpu_exchange_ghosts_pack_netvirial(unsigned int n_out,
hoomd/CommunicatorGPU.cuh:void gpu_exchange_ghosts_copy_netvirial_buf(unsigned int n_recv,
hoomd/util.py:from hoomd.error import GPUNotAvailableError  # noqa: F401
hoomd/LoadBalancerGPU.h:/*! \file LoadBalancerGPU.h
hoomd/LoadBalancerGPU.h:   the GPU
hoomd/LoadBalancerGPU.h:#include "GPUFlags.h"
hoomd/LoadBalancerGPU.h://! GPU implementation of dynamic load balancing
hoomd/LoadBalancerGPU.h:class PYBIND11_EXPORT LoadBalancerGPU : public LoadBalancer
hoomd/LoadBalancerGPU.h:    LoadBalancerGPU(std::shared_ptr<SystemDefinition> sysdef, std::shared_ptr<Trigger> trigger);
hoomd/LoadBalancerGPU.h:    virtual ~LoadBalancerGPU();
hoomd/LoadBalancerGPU.h:        GPUArray<unsigned int> off_ranks(m_pdata->getMaxN(), m_exec_conf);
hoomd/LoadBalancerGPU.h:    //! on the GPU
hoomd/LoadBalancerGPU.h:    GPUArray<unsigned int> m_off_ranks;
hoomd/LoadBalancerGPU.h://! Export the LoadBalancerGPU to python
hoomd/LoadBalancerGPU.h:void export_LoadBalancerGPU(pybind11::module& m);
hoomd/SFCPackTuner.cc:        GPUArray<unsigned int> traversal_order(m_grid * m_grid * m_grid, m_exec_conf);
hoomd/error.py:class GPUNotAvailableError(NotImplementedError):
hoomd/error.py:    """Error for when a GPU specific feature was requested without a GPU."""
hoomd/error.py:class _NoGPU:
hoomd/error.py:    """Used in nonGPU builds of hoomd to raise errors for attempted use."""
hoomd/error.py:        raise GPUNotAvailableError(
hoomd/error.py:            "This build of HOOMD-blue does not support GPUs.")
hoomd/WarpTools.cuh:#ifndef __CUDACC_RTC__
hoomd/WarpTools.cuh:#ifndef __CUDACC_RTC__
hoomd/BondedGroupData.cuh:    \brief Defines the helper functions (GPU version) for updating the GPU bonded group tables
hoomd/BondedGroupData.cuh:    //! Storage for group members (GPU declaration)
hoomd/BondedGroupData.cuh:    //! A union to allow storing a Scalar constraint value or a type integer (GPU declaration)
hoomd/BondedGroupData.cuh://! Packed group entry for communication (GPU declaration)
hoomd/BondedGroupData.cuh:void gpu_update_group_table(const unsigned int n_groups,
hoomd/HOOMDMath.h:// for builds on systems where CUDA is not available, include copies of the CUDA header
hoomd/HOOMDMath.h:#include "hoomd/extern/cudacpu_vector_functions.h"
hoomd/HOOMDMath.h:#include "hoomd/extern/cudacpu_vector_types.h"
hoomd/HOOMDMath.h:/*! Routines in the fast namespace map to fast math routines on the CPU and GPU. Where possible,
hoomd/HOOMDMath.h:   these use the less accurate intrinsics on the GPU (i.e. __sinf). The routines are provide
hoomd/HOOMDMath.h:   and GPU. The routines are provide overloads for both single and double so that macro tricks
hoomd/ParticleGroup.cc:    if (m_pdata->getExecConf()->isCUDAEnabled())
hoomd/ParticleGroup.cc:        m_gpu_partition = GPUPartition(m_exec_conf->getGPUIds());
hoomd/ParticleGroup.cc:    // update GPU memory hints
hoomd/ParticleGroup.cc:    updateGPUAdvice();
hoomd/ParticleGroup.cc:    if (m_pdata->getExecConf()->isCUDAEnabled())
hoomd/ParticleGroup.cc:        m_gpu_partition = GPUPartition(m_exec_conf->getGPUIds());
hoomd/ParticleGroup.cc:    // update GPU memory hints
hoomd/ParticleGroup.cc:    updateGPUAdvice();
hoomd/ParticleGroup.cc:    if (m_pdata->getExecConf()->isCUDAEnabled())
hoomd/ParticleGroup.cc:        rebuildIndexListGPU();
hoomd/ParticleGroup.cc:    if (m_pdata->getExecConf()->isCUDAEnabled())
hoomd/ParticleGroup.cc:        // Update GPU load balancing info
hoomd/ParticleGroup.cc:        m_gpu_partition.setN(m_num_local_members);
hoomd/ParticleGroup.cc:void ParticleGroup::updateGPUAdvice()
hoomd/ParticleGroup.cc:    if (m_exec_conf->isCUDAEnabled() && m_exec_conf->allConcurrentManagedAccess())
hoomd/ParticleGroup.cc:        // split preferred location of group indices across GPUs
hoomd/ParticleGroup.cc:        auto gpu_map = m_exec_conf->getGPUIds();
hoomd/ParticleGroup.cc:        for (unsigned int idev = 0; idev < m_exec_conf->getNumActiveGPUs(); ++idev)
hoomd/ParticleGroup.cc:            auto range = m_gpu_partition.getRange(idev);
hoomd/ParticleGroup.cc:            cudaMemAdvise(m_member_idx.get() + range.first,
hoomd/ParticleGroup.cc:                          cudaMemAdviseSetPreferredLocation,
hoomd/ParticleGroup.cc:                          gpu_map[idev]);
hoomd/ParticleGroup.cc:            cudaMemAdvise(m_is_member.get() + range.first,
hoomd/ParticleGroup.cc:                          cudaMemAdviseSetPreferredLocation,
hoomd/ParticleGroup.cc:                          gpu_map[idev]);
hoomd/ParticleGroup.cc:            cudaMemPrefetchAsync(m_member_idx.get() + range.first,
hoomd/ParticleGroup.cc:                                 gpu_map[idev]);
hoomd/ParticleGroup.cc:            cudaMemPrefetchAsync(m_is_member.get() + range.first,
hoomd/ParticleGroup.cc:                                 gpu_map[idev]);
hoomd/ParticleGroup.cc:        CHECK_CUDA_ERROR();
hoomd/ParticleGroup.cc://! rebuild index list on the GPU
hoomd/ParticleGroup.cc:void ParticleGroup::rebuildIndexListGPU()
hoomd/ParticleGroup.cc:        kernel::gpu_rebuild_index_list(m_pdata->getN(),
hoomd/ParticleGroup.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/ParticleGroup.cc:            CHECK_CUDA_ERROR();
hoomd/ParticleGroup.cc:        kernel::gpu_compact_index_list(m_pdata->getN(),
hoomd/ParticleGroup.cc:        if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/ParticleGroup.cc:            CHECK_CUDA_ERROR();
hoomd/CellList.h:   between GPUs.
hoomd/CellList.h:    For easy support of derived GPU classes to implement overflow detection and error handling, all
hoomd/CellList.h:    //! Request a multi-GPU cell list
hoomd/CellList.h:        // base class doesn't support GPU
hoomd/Integrator.cu:    \brief Defines methods and data structures used by the Integrator class on the GPU
hoomd/Integrator.cu://! Kernel for summing forces on the GPU
hoomd/Integrator.cu:    \param nwork Number of particles this GPU processes
hoomd/Integrator.cu:   current \a d_net_force and \a d_net_virial and adds to that \param offset of this GPU in ptls
hoomd/Integrator.cu:__global__ void gpu_integrator_sum_net_force_kernel(Scalar4* d_net_force,
hoomd/Integrator.cu:                                                    const gpu_force_list force_list,
hoomd/Integrator.cu:hipError_t gpu_integrator_sum_net_force(Scalar4* d_net_force,
hoomd/Integrator.cu:                                        const gpu_force_list& force_list,
hoomd/Integrator.cu:                                        const GPUPartition& gpu_partition)
hoomd/Integrator.cu:    // iterate over active GPUs in reverse, to end up on first GPU when returning from this function
hoomd/Integrator.cu:    for (int idev = gpu_partition.getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/Integrator.cu:        auto range = gpu_partition.getRangeAndSetGPU(idev);
hoomd/Integrator.cu:            hipLaunchKernelGGL(HIP_KERNEL_NAME(gpu_integrator_sum_net_force_kernel<1>),
hoomd/Integrator.cu:            hipLaunchKernelGGL(HIP_KERNEL_NAME(gpu_integrator_sum_net_force_kernel<0>),
hoomd/update/box_resize.py:            self._cpp_obj = _hoomd.BoxResizeUpdaterGPU(
hoomd/update/box_resize.py:            updater = _hoomd.BoxResizeUpdaterGPU(state._cpp_sys_def,
hoomd/update/remove_drift.py:        if isinstance(self._simulation.device, hoomd.device.GPU):
hoomd/update/remove_drift.py:                "Falling back on CPU. No GPU implementation available.\n")
hoomd/GlobalArray.h:    GlobalArray<> supports all functionality that GPUArray<> does, and should eventually replace
hoomd/GlobalArray.h:   GPUArray. In fact, for performance considerations in single GPU situations, GlobalArray
hoomd/GlobalArray.h:   internally falls back on GPUArray (and whenever it doesn't have an ExecutionConfiguration). This
hoomd/GlobalArray.h:    One difference to GPUArray is that GlobalArray doesn't zero its memory space, so it is important
hoomd/GlobalArray.h:    As for GPUArray, access to the data is provided through ArrayHandle<> objects, with proper
hoomd/GlobalArray.h:#include "GPUArray.h"
hoomd/GlobalArray.h:            CHECK_CUDA_ERROR();
hoomd/GlobalArray.h:    /*! \param exec_conf The execution configuration (needed for CHECK_CUDA_ERROR)
hoomd/GlobalArray.h:        CHECK_CUDA_ERROR();
hoomd/GlobalArray.h:template<class T> class GlobalArray : public GPUArrayBase<T, GlobalArray<T>>
hoomd/GlobalArray.h:                      || (force_managed && exec_conf->isCUDAEnabled()))
hoomd/GlobalArray.h:                         ? GPUArray<T>()
hoomd/GlobalArray.h:                         : GPUArray<T>(num_elements, exec_conf)),
hoomd/GlobalArray.h:                       || (force_managed && exec_conf->isCUDAEnabled()))
hoomd/GlobalArray.h:        if (this->m_exec_conf->isCUDAEnabled())
hoomd/GlobalArray.h:            if (this->m_exec_conf && this->m_exec_conf->isCUDAEnabled())
hoomd/GlobalArray.h:                // synchronize all active GPUs
hoomd/GlobalArray.h:                auto gpu_map = this->m_exec_conf->getGPUIds();
hoomd/GlobalArray.h:                for (int idev = this->m_exec_conf->getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/GlobalArray.h:                    hipSetDevice(gpu_map[idev]);
hoomd/GlobalArray.h:                if (this->m_exec_conf && this->m_exec_conf->isCUDAEnabled())
hoomd/GlobalArray.h:                    // synchronize all active GPUs
hoomd/GlobalArray.h:                    auto gpu_map = this->m_exec_conf->getGPUIds();
hoomd/GlobalArray.h:                    for (int idev = this->m_exec_conf->getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/GlobalArray.h:                        hipSetDevice(gpu_map[idev]);
hoomd/GlobalArray.h:        \param exec_conf Shared pointer to the execution configuration for managing CUDA
hoomd/GlobalArray.h:                      || (force_managed && exec_conf->isCUDAEnabled()))
hoomd/GlobalArray.h:                         ? GPUArray<T>()
hoomd/GlobalArray.h:                         : GPUArray<T>(width, height, exec_conf)),
hoomd/GlobalArray.h:                       || (force_managed && exec_conf->isCUDAEnabled()))
hoomd/GlobalArray.h:        if (this->m_exec_conf->isCUDAEnabled())
hoomd/GlobalArray.h:     - For 1-D allocated GPUArrays, this is the number of elements allocated.
hoomd/GlobalArray.h:     - For 2-D allocated GPUArrays, this is the \b total number of elements (\a pitch * \a height)
hoomd/GlobalArray.h:    //! Test if the GPUArray is NULL
hoomd/GlobalArray.h:     - For 2-D allocated GPUArrays, this is the total width of a row in memory (including the
hoomd/GlobalArray.h:     - For 1-D allocated GPUArrays, this is the simply the number of elements allocated.
hoomd/GlobalArray.h:     - For 2-D allocated GPUArrays, this is the height given to the constructor
hoomd/GlobalArray.h:     - For 1-D allocated GPUArrays, this is the simply 1.
hoomd/GlobalArray.h:        if (this->m_exec_conf && this->m_exec_conf->isCUDAEnabled())
hoomd/GlobalArray.h:            // synchronize all active GPUs
hoomd/GlobalArray.h:            auto gpu_map = this->m_exec_conf->getGPUIds();
hoomd/GlobalArray.h:            for (int idev = this->m_exec_conf->getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/GlobalArray.h:                hipSetDevice(gpu_map[idev]);
hoomd/GlobalArray.h:        if (this->m_exec_conf->isCUDAEnabled())
hoomd/GlobalArray.h:            // synchronize all active GPUs
hoomd/GlobalArray.h:            auto gpu_map = this->m_exec_conf->getGPUIds();
hoomd/GlobalArray.h:            for (int idev = this->m_exec_conf->getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/GlobalArray.h:                hipSetDevice(gpu_map[idev]);
hoomd/GlobalArray.h:    friend class GPUArrayBase<T, GlobalArray<T>>;
hoomd/GlobalArray.h:        m_exec_conf; //!< execution configuration for working with CUDA
hoomd/GlobalArray.h:    //! We hold a GPUArray<T> object for fallback onto zero-copy memory
hoomd/GlobalArray.h:    GPUArray<T> m_fallback;
hoomd/GlobalArray.h:        m_event; //! CUDA event for synchronization
hoomd/GlobalArray.h:        bool use_device = this->m_exec_conf && this->m_exec_conf->isCUDAEnabled();
hoomd/GlobalArray.h:            CHECK_CUDA_ERROR();
hoomd/GlobalArray.h:            CHECK_CUDA_ERROR();
hoomd/GlobalArray.h:            CHECK_CUDA_ERROR();
hoomd/GlobalArray.h:// ArrayHandleDispatch specialization for GPUArray
hoomd/GlobalArray.h:    if (this->m_exec_conf && this->m_exec_conf->inMultiGPUBlock())
hoomd/GlobalArray.h:        // we throw this error because we are not syncing all GPUs upon acquire
hoomd/GlobalArray.h:        throw std::runtime_error("GlobalArray should not be acquired in a multi-GPU block.");
hoomd/GlobalArray.h:    bool use_device = this->m_exec_conf && this->m_exec_conf->isCUDAEnabled();
hoomd/GlobalArray.h:        // synchronize GPU 0
hoomd/GlobalArray.h:        if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/GlobalArray.h:            CHECK_CUDA_ERROR();
hoomd/ParticleGroup.cu:    \brief Contains GPU kernel code used by ParticleGroup
hoomd/ParticleGroup.cu://! GPU kernel to translate between global and local membership lookup table
hoomd/ParticleGroup.cu:__global__ void gpu_rebuild_index_list_kernel(unsigned int N,
hoomd/ParticleGroup.cu:__global__ void gpu_scatter_member_indices(unsigned int N,
hoomd/ParticleGroup.cu://! GPU method for rebuilding the index list of a ParticleGroup
hoomd/ParticleGroup.cu:hipError_t gpu_rebuild_index_list(unsigned int N,
hoomd/ParticleGroup.cu:    hipLaunchKernelGGL(gpu_rebuild_index_list_kernel,
hoomd/ParticleGroup.cu://! GPU method for compacting the group member indices
hoomd/ParticleGroup.cu:hipError_t gpu_compact_index_list(unsigned int N,
hoomd/ParticleGroup.cu:    num_local_members = thrust::reduce(thrust::cuda::par(alloc),
hoomd/ParticleGroup.cu:    hipLaunchKernelGGL(gpu_scatter_member_indices,
hoomd/Integrator.cc:   of data traffic back and forth if the forces and/or integrator are on the GPU. Call
hoomd/Integrator.cc:   computeNetForcesGPU() to sum the forces on the GPU
hoomd/Integrator.cc:   \a m_net_virial \note The summation step is performed <b>on the GPU</b>.
hoomd/Integrator.cc:void Integrator::computeNetForceGPU(uint64_t timestep)
hoomd/Integrator.cc:    if (!m_exec_conf->isCUDAEnabled())
hoomd/Integrator.cc:        throw runtime_error("Cannot compute net force on the GPU if CUDA is disabled.");
hoomd/Integrator.cc:            if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/Integrator.cc:                CHECK_CUDA_ERROR();
hoomd/Integrator.cc:            kernel::gpu_force_list force_list;
hoomd/Integrator.cc:            m_exec_conf->beginMultiGPU();
hoomd/Integrator.cc:            gpu_integrator_sum_net_force(d_net_force.data,
hoomd/Integrator.cc:                                         m_pdata->getGPUPartition());
hoomd/Integrator.cc:            if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/Integrator.cc:                CHECK_CUDA_ERROR();
hoomd/Integrator.cc:            m_exec_conf->endMultiGPU();
hoomd/Integrator.cc:            kernel::gpu_force_list force_list;
hoomd/Integrator.cc:            m_exec_conf->beginMultiGPU();
hoomd/Integrator.cc:            gpu_integrator_sum_net_force(d_net_force.data,
hoomd/Integrator.cc:                                         m_pdata->getGPUPartition());
hoomd/Integrator.cc:            if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/Integrator.cc:                CHECK_CUDA_ERROR();
hoomd/Integrator.cc:            m_exec_conf->endMultiGPU();
hoomd/Messenger.h:        - 7 memory allocation/reallocation notices from GPUArray
hoomd/conftest.py:_n_available_gpu = len(hoomd.device.GPU.get_available_devices())
hoomd/conftest.py:_require_gpu_tests = (os.environ.get('_HOOMD_REQUIRE_GPU_TESTS_IN_GPU_BUILDS_')
hoomd/conftest.py:if hoomd.version.gpu_enabled and (_n_available_gpu > 0 or _require_gpu_tests):
hoomd/conftest.py:    if os.environ.get('_HOOMD_SKIP_CPU_TESTS_WHEN_GPUS_PRESENT_') is not None:
hoomd/conftest.py:    devices.append(hoomd.device.GPU)
hoomd/conftest.py:    namespace['gpu_not_available'] = _n_available_gpu == 0
hoomd/conftest.py:    Tests that use `device` will run once on the CPU and once on the GPU. The
hoomd/conftest.py:    # enable GPU error checking
hoomd/conftest.py:    if isinstance(d, hoomd.device.GPU):
hoomd/conftest.py:        d.gpu_error_checking = True
hoomd/conftest.py:def only_gpu(request):
hoomd/conftest.py:    """Skip CPU tests marked ``gpu``."""
hoomd/conftest.py:    if request.node.get_closest_marker('gpu'):
hoomd/conftest.py:                              hoomd.device.GPU):
hoomd/conftest.py:                pytest.skip('Test is run only on GPU(s).')
hoomd/conftest.py:            raise ValueError('only_gpu requires the *device* fixture')
hoomd/conftest.py:    """Skip GPU tests marked ``cpu``."""
hoomd/conftest.py:                            "gpu: Tests that should only run on the gpu.")
hoomd/conftest.py:    config.addinivalue_line("markers", "gpu: Tests that only run on the GPU.")
hoomd/conftest.py:        # GPU instances have parameters and start incomplete.
hoomd/SFCPackTunerGPU.cuh:#ifndef __SFC_PACK_UPDATER_GPU_CUH__
hoomd/SFCPackTunerGPU.cuh:#define __SFC_PACK_UPDATER_GPU_CUH__
hoomd/SFCPackTunerGPU.cuh:/*! \file SFCPackTunerGPU.cuh
hoomd/SFCPackTunerGPU.cuh:    \brief Defines GPU functions for generating the space-filling curve sorted order on the GPU.
hoomd/SFCPackTunerGPU.cuh:   Used by SFCPackTunerGPU.
hoomd/SFCPackTunerGPU.cuh://! Generate sorted order on GPU
hoomd/SFCPackTunerGPU.cuh:void gpu_generate_sorted_order(unsigned int N,
hoomd/SFCPackTunerGPU.cuh://! Reorder particle data (GPU driver function)
hoomd/SFCPackTunerGPU.cuh:void gpu_apply_sorted_order(unsigned int N,
hoomd/SFCPackTunerGPU.cuh:#endif // __SFC_PACK_UPDATER_GPU_CUH__
hoomd/hpmc/IntegratorHPMCMonoGPUDepletantsAuxilliaryTypes.cuh:#include "IntegratorHPMCMonoGPUDepletants.cuh"
hoomd/hpmc/IntegratorHPMCMonoGPUDepletantsAuxilliaryTypes.cuh:namespace gpu
hoomd/hpmc/IntegratorHPMCMonoGPUDepletantsAuxilliaryTypes.cuh:                           const GPUPartition& _gpu_partition_rank)
hoomd/hpmc/IntegratorHPMCMonoGPUDepletantsAuxilliaryTypes.cuh:          gpu_partition_rank(_gpu_partition_rank) { };
hoomd/hpmc/IntegratorHPMCMonoGPUDepletantsAuxilliaryTypes.cuh:    const unsigned int* nwork_local;           //!< Number of insertions this rank handles, per GPU
hoomd/hpmc/IntegratorHPMCMonoGPUDepletantsAuxilliaryTypes.cuh:    const GPUPartition& gpu_partition_rank; //!< Split of particles for this rank
hoomd/hpmc/IntegratorHPMCMonoGPUDepletantsAuxilliaryTypes.cuh:                            const GPUPartition& gpu_partition,
hoomd/hpmc/IntegratorHPMCMonoGPUDepletantsAuxilliaryTypes.cuh:                                    const GPUPartition& gpu_partition,
hoomd/hpmc/IntegratorHPMCMonoGPUDepletantsAuxilliaryTypes.cuh:                                   const GPUPartition& gpu_partition,
hoomd/hpmc/IntegratorHPMCMonoGPUDepletantsAuxilliaryTypes.cuh:    } // end namespace gpu
hoomd/hpmc/HPMCCounters.h:/*! \file IntegratorHPMCMonoGPU.cuh
hoomd/hpmc/HPMCCounters.h:    \brief Declaration of CUDA kernels drivers
hoomd/hpmc/OBB.h:   format changes. It also changes between the CPU and GPU. Instead, use the accessor methods
hoomd/hpmc/IntegratorHPMCMonoGPUDepletantsTypes.cuh:#include "hoomd/GPUPartition.cuh"
hoomd/hpmc/IntegratorHPMCMonoGPUDepletantsTypes.cuh:#include "IntegratorHPMCMonoGPUTypes.cuh"
hoomd/hpmc/IntegratorHPMCMonoGPUDepletantsTypes.cuh:namespace gpu
hoomd/hpmc/IntegratorHPMCMonoGPUDepletantsTypes.cuh:                             const GPUPartition& gpu_partition,
hoomd/hpmc/IntegratorHPMCMonoGPUDepletantsTypes.cuh:                            const GPUPartition& gpu_partition,
hoomd/hpmc/IntegratorHPMCMonoGPUDepletantsTypes.cuh:void reduce_counters(const unsigned int ngpu,
hoomd/hpmc/IntegratorHPMCMonoGPUDepletantsTypes.cuh:    } // end namespace gpu
hoomd/hpmc/ExternalFieldJIT.h:                        hoomd::detail::managed_allocator<float>(m_exec_conf->isCUDAEnabled()))
hoomd/hpmc/GPUEvalFactory.cc:#include "GPUEvalFactory.h"
hoomd/hpmc/GPUEvalFactory.cc:#include <cuda.h>
hoomd/hpmc/GPUEvalFactory.cc:void GPUEvalFactory::compileGPU(const std::string& code,
hoomd/hpmc/GPUEvalFactory.cc:                                const std::string& cuda_devrt_library_path,
hoomd/hpmc/GPUEvalFactory.cc:        "--gpu-architecture=compute_" + std::to_string(compute_arch),
hoomd/hpmc/GPUEvalFactory.cc:    // compile on each GPU, substituting common headers with fake headers
hoomd/hpmc/GPUEvalFactory.cc:    auto gpu_map = m_exec_conf->getGPUIds();
hoomd/hpmc/GPUEvalFactory.cc:    for (int idev = m_exec_conf->getNumActiveGPUs() - 1; idev >= 0; idev--)
hoomd/hpmc/GPUEvalFactory.cc:        m_exec_conf->msg->notice(3) << "Compiling nvrtc code on GPU " << idev << std::endl;
hoomd/hpmc/GPUEvalFactory.cc:        cudaSetDevice(gpu_map[idev]);
hoomd/hpmc/module_simple_polygon.cc:#include "ComputeFreeVolumeGPU.h"
hoomd/hpmc/module_simple_polygon.cc:#include "IntegratorHPMCMonoGPU.h"
hoomd/hpmc/module_simple_polygon.cc:#include "UpdaterClustersGPU.h"
hoomd/hpmc/module_simple_polygon.cc:    export_IntegratorHPMCMonoGPU<ShapeSimplePolygon>(m, "IntegratorHPMCMonoSimplePolygonGPU");
hoomd/hpmc/module_simple_polygon.cc:    export_ComputeFreeVolumeGPU<ShapeSimplePolygon>(m, "ComputeFreeVolumeSimplePolygonGPU");
hoomd/hpmc/module_simple_polygon.cc:    export_UpdaterClustersGPU<ShapeSimplePolygon>(m, "UpdaterClustersSimplePolygonGPU");
hoomd/hpmc/UpdaterClustersGPUDepletants.cuh:/*! \file UpdaterClustersGPUDepletants.cuh
hoomd/hpmc/UpdaterClustersGPUDepletants.cuh:    \brief Implements the depletant kernels for the geometric cluster algorithm the GPU
hoomd/hpmc/UpdaterClustersGPUDepletants.cuh:#include "hoomd/GPUPartition.cuh"
hoomd/hpmc/UpdaterClustersGPUDepletants.cuh:#include "hoomd/hpmc/GPUHelpers.cuh"
hoomd/hpmc/UpdaterClustersGPUDepletants.cuh:#include "IntegratorHPMCMonoGPUDepletants.cuh"
hoomd/hpmc/UpdaterClustersGPUDepletants.cuh:#include "UpdaterClustersGPU.cuh"
hoomd/hpmc/UpdaterClustersGPUDepletants.cuh:namespace gpu
hoomd/hpmc/UpdaterClustersGPUDepletants.cuh:#if (__CUDA_ARCH__ >= 600)
hoomd/hpmc/UpdaterClustersGPUDepletants.cuh:        for (int idev = args.gpu_partition.getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/hpmc/UpdaterClustersGPUDepletants.cuh:            auto range = args.gpu_partition.getRangeAndSetGPU(idev);
hoomd/hpmc/UpdaterClustersGPUDepletants.cuh:    } // end namespace gpu
hoomd/hpmc/IntegratorHPMCMonoGPUDepletants.cuh:#include "hoomd/GPUPartition.cuh"
hoomd/hpmc/IntegratorHPMCMonoGPUDepletants.cuh:#include "GPUHelpers.cuh"
hoomd/hpmc/IntegratorHPMCMonoGPUDepletants.cuh:#include "IntegratorHPMCMonoGPUDepletantsTypes.cuh"
hoomd/hpmc/IntegratorHPMCMonoGPUDepletants.cuh:namespace gpu
hoomd/hpmc/IntegratorHPMCMonoGPUDepletants.cuh:#if (__CUDA_ARCH__ >= 600)
hoomd/hpmc/IntegratorHPMCMonoGPUDepletants.cuh:#if (__CUDA_ARCH__ >= 600)
hoomd/hpmc/IntegratorHPMCMonoGPUDepletants.cuh:        for (int idev = args.gpu_partition.getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/hpmc/IntegratorHPMCMonoGPUDepletants.cuh:            auto range = args.gpu_partition.getRangeAndSetGPU(idev);
hoomd/hpmc/IntegratorHPMCMonoGPUDepletants.cuh:    } // end namespace gpu
hoomd/hpmc/integrate.py:degrades performance due to cache incoherency). In the GPU and MPI
hoomd/hpmc/integrate.py:        if (isinstance(self._simulation.device, hoomd.device.GPU)
hoomd/hpmc/integrate.py:                and (self._cpp_cls + 'GPU') in _hpmc.__dict__):
hoomd/hpmc/integrate.py:            self._cpp_cell = _hoomd.CellListGPU(sys_def)
hoomd/hpmc/integrate.py:                                    self._cpp_cls + 'GPU')(sys_def,
hoomd/hpmc/integrate.py:            if isinstance(self._simulation.device, hoomd.device.GPU):
hoomd/hpmc/integrate.py:                    "Falling back on CPU. No GPU implementation for shape.\n")
hoomd/hpmc/kernel_narrow_phase.cu.inc:#include "IntegratorHPMCMonoGPU.cuh"
hoomd/hpmc/kernel_narrow_phase.cu.inc:namespace gpu
hoomd/hpmc/kernel_narrow_phase.cu.inc:    } // namespace gpu
hoomd/hpmc/module_ellipsoid.cc:#include "ComputeFreeVolumeGPU.h"
hoomd/hpmc/module_ellipsoid.cc:#include "IntegratorHPMCMonoGPU.h"
hoomd/hpmc/module_ellipsoid.cc:#include "UpdaterClustersGPU.h"
hoomd/hpmc/module_ellipsoid.cc:    export_IntegratorHPMCMonoGPU<ShapeEllipsoid>(m, "IntegratorHPMCMonoEllipsoidGPU");
hoomd/hpmc/module_ellipsoid.cc:    export_ComputeFreeVolumeGPU<ShapeEllipsoid>(m, "ComputeFreeVolumeEllipsoidGPU");
hoomd/hpmc/module_ellipsoid.cc:    export_UpdaterClustersGPU<ShapeEllipsoid>(m, "UpdaterClustersEllipsoidGPU");
hoomd/hpmc/ShapeConvexPolygon.h:    /// Set CUDA memory hint
hoomd/hpmc/GPUEvalFactory.h:#include "EvaluatorUnionGPU.cuh"
hoomd/hpmc/GPUEvalFactory.h:#include <cuda.h>
hoomd/hpmc/GPUEvalFactory.h:#include <cuda_runtime.h>
hoomd/hpmc/GPUEvalFactory.h://! Evaluate patch energies via runtime generated code, GPU version
hoomd/hpmc/GPUEvalFactory.h:class GPUEvalFactory
hoomd/hpmc/GPUEvalFactory.h:    GPUEvalFactory(std::shared_ptr<ExecutionConfiguration> exec_conf,
hoomd/hpmc/GPUEvalFactory.h:                   const std::string& cuda_devrt_library_path,
hoomd/hpmc/GPUEvalFactory.h:        m_cache.resize(this->m_exec_conf->getNumActiveGPUs());
hoomd/hpmc/GPUEvalFactory.h:        compileGPU(code, kernel_name, options, cuda_devrt_library_path, compute_arch);
hoomd/hpmc/GPUEvalFactory.h:    ~GPUEvalFactory() { }
hoomd/hpmc/GPUEvalFactory.h:    /* \param idev the logical GPU id
hoomd/hpmc/GPUEvalFactory.h:    /* \param idev the logical GPU id
hoomd/hpmc/GPUEvalFactory.h:        if (custatus != CUDA_SUCCESS)
hoomd/hpmc/GPUEvalFactory.h:    /* \param idev the logical GPU id
hoomd/hpmc/GPUEvalFactory.h:        if (custatus != CUDA_SUCCESS)
hoomd/hpmc/GPUEvalFactory.h:/*! \param idev logical GPU id to launch on
hoomd/hpmc/GPUEvalFactory.h:                                           cudaStream_t hStream,
hoomd/hpmc/GPUEvalFactory.h:        cudaSetDevice(m_exec_conf->getGPUIds()[idev]);
hoomd/hpmc/GPUEvalFactory.h:        auto gpu_map = m_exec_conf->getGPUIds();
hoomd/hpmc/GPUEvalFactory.h:        for (int idev = m_exec_conf->getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/hpmc/GPUEvalFactory.h:            cudaSetDevice(gpu_map[idev]);
hoomd/hpmc/GPUEvalFactory.h:                    if (custatus != CUDA_SUCCESS)
hoomd/hpmc/GPUEvalFactory.h:        auto gpu_map = m_exec_conf->getGPUIds();
hoomd/hpmc/GPUEvalFactory.h:        for (int idev = m_exec_conf->getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/hpmc/GPUEvalFactory.h:            cudaSetDevice(gpu_map[idev]);
hoomd/hpmc/GPUEvalFactory.h:                    if (custatus != CUDA_SUCCESS)
hoomd/hpmc/GPUEvalFactory.h:        auto gpu_map = m_exec_conf->getGPUIds();
hoomd/hpmc/GPUEvalFactory.h:        for (int idev = m_exec_conf->getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/hpmc/GPUEvalFactory.h:            cudaSetDevice(gpu_map[idev]);
hoomd/hpmc/GPUEvalFactory.h:                    if (custatus != CUDA_SUCCESS)
hoomd/hpmc/GPUEvalFactory.h:        auto gpu_map = m_exec_conf->getGPUIds();
hoomd/hpmc/GPUEvalFactory.h:        for (int idev = m_exec_conf->getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/hpmc/GPUEvalFactory.h:            cudaSetDevice(gpu_map[idev]);
hoomd/hpmc/GPUEvalFactory.h:                    if (custatus != CUDA_SUCCESS)
hoomd/hpmc/GPUEvalFactory.h:    void compileGPU(const std::string& code,
hoomd/hpmc/GPUEvalFactory.h:                    const std::string& cuda_devrt_library_path,
hoomd/hpmc/GPUEvalFactory.h:    std::vector<jitify::JitCache> m_cache;  //!< jitify kernel cache, one per GPU
hoomd/hpmc/GPUEvalFactory.h:    std::vector<jitify::Program> m_program; //!< The kernel object, one per GPU
hoomd/hpmc/compute.py:    other particles, including self overlap. On GPU devices, `FreeVolume`
hoomd/hpmc/compute.py:                                  'ComputeFreeVolume' + integrator_name + 'GPU')
hoomd/hpmc/ComputeFreeVolumeGPU.cuh:    \brief Declaration of CUDA kernels drivers
hoomd/hpmc/ComputeFreeVolumeGPU.cuh://! Wraps arguments to gpu_hpmc_free_volume
hoomd/hpmc/ComputeFreeVolumeGPU.cuh:    const hipDeviceProp_t& devprop;       //!< CUDA device properties
hoomd/hpmc/ComputeFreeVolumeGPU.cuh:hipError_t gpu_hpmc_free_volume(const hpmc_free_volume_args_t& args,
hoomd/hpmc/ComputeFreeVolumeGPU.cuh:__global__ void gpu_hpmc_free_volume_kernel(unsigned int n_sample,
hoomd/hpmc/ComputeFreeVolumeGPU.cuh://! Kernel driver for gpu_hpmc_free_volume_kernel()
hoomd/hpmc/ComputeFreeVolumeGPU.cuh:    \returns Error codes generated by any CUDA calls, or hipSuccess when there is no error
hoomd/hpmc/ComputeFreeVolumeGPU.cuh:hipError_t gpu_hpmc_free_volume(const hpmc_free_volume_args_t& args,
hoomd/hpmc/ComputeFreeVolumeGPU.cuh:    hipFuncGetAttributes(&attr, reinterpret_cast<const void*>(gpu_hpmc_free_volume_kernel<Shape>));
hoomd/hpmc/ComputeFreeVolumeGPU.cuh:    hipLaunchKernelGGL(HIP_KERNEL_NAME(gpu_hpmc_free_volume_kernel<Shape>),
hoomd/hpmc/PatchEnergyJITUnionGPU.h:#include "EvaluatorUnionGPU.cuh"
hoomd/hpmc/PatchEnergyJITUnionGPU.h:#include "GPUEvalFactory.h"
hoomd/hpmc/PatchEnergyJITUnionGPU.h://! Evaluate patch energies via runtime generated code, GPU version
hoomd/hpmc/PatchEnergyJITUnionGPU.h:class PYBIND11_EXPORT PatchEnergyJITUnionGPU : public PatchEnergyJITUnion
hoomd/hpmc/PatchEnergyJITUnionGPU.h:    PatchEnergyJITUnionGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/hpmc/PatchEnergyJITUnionGPU.h:                           const std::string& cuda_devrt_library_path,
hoomd/hpmc/PatchEnergyJITUnionGPU.h:          m_gpu_factory(exec_conf,
hoomd/hpmc/PatchEnergyJITUnionGPU.h:                        cuda_devrt_library_path,
hoomd/hpmc/PatchEnergyJITUnionGPU.h:              hoomd::detail::managed_allocator<jit::union_params_t>(m_exec_conf->isCUDAEnabled()))
hoomd/hpmc/PatchEnergyJITUnionGPU.h:        m_gpu_factory.setAlphaPtr(m_param_array.data(), this->m_is_union);
hoomd/hpmc/PatchEnergyJITUnionGPU.h:        m_gpu_factory.setAlphaUnionPtr(m_param_array_constituent.data());
hoomd/hpmc/PatchEnergyJITUnionGPU.h:        m_gpu_factory.setUnionParamsPtr(m_d_union_params.data());
hoomd/hpmc/PatchEnergyJITUnionGPU.h:        m_gpu_factory.setRCutUnion(float(m_r_cut_constituent));
hoomd/hpmc/PatchEnergyJITUnionGPU.h:        auto& launch_bounds = m_gpu_factory.getLaunchBounds();
hoomd/hpmc/PatchEnergyJITUnionGPU.h:    virtual ~PatchEnergyJITUnionGPU() { }
hoomd/hpmc/PatchEnergyJITUnionGPU.h:        // cudaMemadviseReadMostly
hoomd/hpmc/PatchEnergyJITUnionGPU.h:        // cudaMemadviseReadMostly
hoomd/hpmc/PatchEnergyJITUnionGPU.h:        // cudaMemadviseReadMostly
hoomd/hpmc/PatchEnergyJITUnionGPU.h:        // cudaMemadviseReadMostly
hoomd/hpmc/PatchEnergyJITUnionGPU.h:        // cudaMemadviseReadMostly
hoomd/hpmc/PatchEnergyJITUnionGPU.h:    virtual void computePatchEnergyGPU(const gpu_args_t& args, hipStream_t hStream);
hoomd/hpmc/PatchEnergyJITUnionGPU.h:    GPUEvalFactory m_gpu_factory; //!< JIT implementation
hoomd/hpmc/PatchEnergyJITUnionGPU.h:        m_d_union_params; //!< Parameters for each particle type on GPU
hoomd/hpmc/PatchEnergyJITUnionGPU.h://! Exports the PatchEnergyJITUnionGPU class to python
hoomd/hpmc/PatchEnergyJITUnionGPU.h:void export_PatchEnergyJITUnionGPU(pybind11::module& m);
hoomd/hpmc/UpdaterClustersGPU.cu:#include "UpdaterClustersGPU.cuh"
hoomd/hpmc/UpdaterClustersGPU.cu:/*! \file UpdaterClustersGPU.cu
hoomd/hpmc/UpdaterClustersGPU.cu:    \brief Implements a connected components algorithm on the GPU
hoomd/hpmc/UpdaterClustersGPU.cu:namespace gpu
hoomd/hpmc/UpdaterClustersGPU.cu:                                                              const GPUPartition& gpu_partition,
hoomd/hpmc/UpdaterClustersGPU.cu:    for (int idev = gpu_partition.getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/hpmc/UpdaterClustersGPU.cu:        auto range = gpu_partition.getRangeAndSetGPU(idev);
hoomd/hpmc/UpdaterClustersGPU.cu:        thrust::exclusive_scan(thrust::cuda::par(alloc),
hoomd/hpmc/UpdaterClustersGPU.cu:        nneigh_total += thrust::reduce(thrust::cuda::par(alloc),
hoomd/hpmc/UpdaterClustersGPU.cu:                           const GPUPartition& gpu_partition,
hoomd/hpmc/UpdaterClustersGPU.cu:    for (int idev = gpu_partition.getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/hpmc/UpdaterClustersGPU.cu:        auto range = gpu_partition.getRangeAndSetGPU(idev);
hoomd/hpmc/UpdaterClustersGPU.cu:                                                          const GPUPartition& gpu_partition,
hoomd/hpmc/UpdaterClustersGPU.cu:    for (int idev = gpu_partition.getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/hpmc/UpdaterClustersGPU.cu:        auto range = gpu_partition.getRangeAndSetGPU(idev);
hoomd/hpmc/UpdaterClustersGPU.cu:    thrust::sort(thrust::cuda::par(alloc), adj, adj + n_elements, pair_less());
hoomd/hpmc/UpdaterClustersGPU.cu:    auto new_last = thrust::unique(thrust::cuda::par(alloc), adj, adj + n_elements);
hoomd/hpmc/UpdaterClustersGPU.cu:    thrust::copy(thrust::cuda::par(alloc), components, components + nverts, work);
hoomd/hpmc/UpdaterClustersGPU.cu:    thrust::sort(thrust::cuda::par(alloc), work, work + nverts);
hoomd/hpmc/UpdaterClustersGPU.cu:    auto it = thrust::reduce_by_key(thrust::cuda::par(alloc),
hoomd/hpmc/UpdaterClustersGPU.cu:    thrust::lower_bound(thrust::cuda::par(alloc),
hoomd/hpmc/UpdaterClustersGPU.cu:    } // end namespace gpu
hoomd/hpmc/UpdaterClusters.h:    \param h_postype Pointer to GPUArray containing particle positions
hoomd/hpmc/UpdaterClusters.h:    \param h_orientation Pointer to GPUArray containing particle orientations
hoomd/hpmc/UpdaterClusters.h:    \param h_overlaps Pointer to GPUArray containing interaction matrix
hoomd/hpmc/UpdaterClusters.h:    NOTE: To avoid numerous acquires and releases of GPUArrays, data pointers are passed directly into this const function.
hoomd/hpmc/external/user.py:        `CPPExternalPotential` does not support execution on GPUs.
hoomd/hpmc/external/user.py:        if (isinstance(self._simulation.device, hoomd.device.GPU)):
hoomd/hpmc/external/user.py:            msg = 'User-defined external fields are not supported on the GPU.'
hoomd/hpmc/external/external.py:        if isinstance(device, hoomd.device.GPU):
hoomd/hpmc/external/external.py:            raise RuntimeError("Not implemented on the GPU")
hoomd/hpmc/external/field.py:        `Harmonic` does not support execution on GPUs.
hoomd/hpmc/external/field.py:        if not isinstance(device, hoomd.device.GPU):
hoomd/hpmc/external/field.py:            msg += 'GPU not supported.'
hoomd/hpmc/external/wall.py:        `WallPotential` does not support execution on GPUs.
hoomd/hpmc/external/wall.py:        if isinstance(self._simulation.device, hoomd.device.GPU):
hoomd/hpmc/external/wall.py:            raise RuntimeError('HPMC walls are not supported on the GPU.')
hoomd/hpmc/IntegratorHPMCMonoGPUMoves.cuh:#include "hoomd/GPUPartition.cuh"
hoomd/hpmc/IntegratorHPMCMonoGPUMoves.cuh:#include "GPUHelpers.cuh"
hoomd/hpmc/IntegratorHPMCMonoGPUMoves.cuh:#include "IntegratorHPMCMonoGPUTypes.cuh"
hoomd/hpmc/IntegratorHPMCMonoGPUMoves.cuh:namespace gpu
hoomd/hpmc/IntegratorHPMCMonoGPUMoves.cuh:#if (__CUDA_ARCH__ >= 600)
hoomd/hpmc/IntegratorHPMCMonoGPUMoves.cuh:    for (int idev = args.gpu_partition.getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/hpmc/IntegratorHPMCMonoGPUMoves.cuh:        auto range = args.gpu_partition.getRangeAndSetGPU(idev);
hoomd/hpmc/IntegratorHPMCMonoGPUMoves.cuh:    } // end namespace gpu
hoomd/hpmc/kernel_cluster_depletants.cu.inc:#include "UpdaterClustersGPUDepletants.cuh"
hoomd/hpmc/kernel_cluster_depletants.cu.inc:namespace gpu
hoomd/hpmc/kernel_cluster_depletants.cu.inc:    } // namespace gpu
hoomd/hpmc/module_convex_polyhedron.cc:#include "ComputeFreeVolumeGPU.h"
hoomd/hpmc/module_convex_polyhedron.cc:#include "IntegratorHPMCMonoGPU.h"
hoomd/hpmc/module_convex_polyhedron.cc:#include "UpdaterClustersGPU.h"
hoomd/hpmc/module_convex_polyhedron.cc:    export_IntegratorHPMCMonoGPU<ShapeConvexPolyhedron>(m, "IntegratorHPMCMonoConvexPolyhedronGPU");
hoomd/hpmc/module_convex_polyhedron.cc:    export_ComputeFreeVolumeGPU<ShapeConvexPolyhedron>(m, "ComputeFreeVolumeConvexPolyhedronGPU");
hoomd/hpmc/module_convex_polyhedron.cc:    export_UpdaterClustersGPU<ShapeConvexPolyhedron>(m, "UpdaterClustersConvexPolyhedronGPU");
hoomd/hpmc/pytest/test_quick_compress.py:    if isinstance(device, hoomd.device.GPU):
hoomd/hpmc/pytest/test_quick_compress.py:        n = 8  # Increase simulation size even further to accomodate GPU
hoomd/hpmc/pytest/test_compute_sdf.py:        # good test results even with different seeds or GPU runs
hoomd/hpmc/pytest/test_compute_sdf.py:@pytest.mark.cpu  # SDF runs on the CPU only, no need to test on the GPU
hoomd/hpmc/pytest/test_shape.py:        pytest.skip("Sphinx does not build on the GPU by default.")
hoomd/hpmc/pytest/test_shape.py:# test_fugacity fails on the GPU for unknown reasons - not fixing as the
hoomd/hpmc/pytest/test_clusters.py:    if (isinstance(device, hoomd.device.GPU)
hoomd/hpmc/pytest/test_clusters.py:            and hoomd.version.gpu_platform == 'ROCm'):
hoomd/hpmc/pytest/test_clusters.py:        pytest.xfail("Clusters fails on ROCm (#1605)")
hoomd/hpmc/pytest/test_pair_union_user.py:    # catch the error if running on the GPU
hoomd/hpmc/pytest/test_pair_union_user.py:    if isinstance(device, hoomd.device.GPU):
hoomd/hpmc/pytest/test_pair_union_user.py:    # catch the error if running on the GPU
hoomd/hpmc/pytest/test_pair_union_user.py:    if isinstance(device, hoomd.device.GPU):
hoomd/hpmc/pytest/test_pair_union_user.py:    # catch the error if running on the GPU
hoomd/hpmc/pytest/test_pair_union_user.py:    if isinstance(device, hoomd.device.GPU):
hoomd/hpmc/pytest/test_pair_union_user.py:@pytest.mark.gpu
hoomd/hpmc/pytest/test_pair_union_user.py:def test_param_array_union_gpu(device, simulation_factory,
hoomd/hpmc/pytest/test_pair_union_user.py:    correctly. We test this on the GPU where code_isotropic is unused, so we do
hoomd/hpmc/pytest/test_pair_union_user.py:    # GPU
hoomd/hpmc/test/test_polyhedron.cc:GPUTree build_tree(TriangleMesh& data)
hoomd/hpmc/test/test_polyhedron.cc:    GPUTree gpu_tree(tree);
hoomd/hpmc/test/test_polyhedron.cc:    return gpu_tree;
hoomd/hpmc/test/CMakeLists.txt:        set(_cuda_sources ${CUR_TEST}.cu)
hoomd/hpmc/test/CMakeLists.txt:        set_source_files_properties(${_cuda_sources} PROPERTIES LANGUAGE ${HOOMD_DEVICE_LANGUAGE})
hoomd/hpmc/test/CMakeLists.txt:        set(_cuda_sources "")
hoomd/hpmc/test/CMakeLists.txt:    add_executable(${CUR_TEST} EXCLUDE_FROM_ALL ${CUR_TEST}.cc ${_cuda_sources})
hoomd/hpmc/test/test_sphere_union.cc:    data.tree = GPUTree(tree);
hoomd/hpmc/module_union_convex_polyhedron.cc:#include "ComputeFreeVolumeGPU.h"
hoomd/hpmc/module_union_convex_polyhedron.cc:#include "IntegratorHPMCMonoGPU.h"
hoomd/hpmc/module_union_convex_polyhedron.cc:#include "UpdaterClustersGPU.h"
hoomd/hpmc/module_union_convex_polyhedron.cc:    export_IntegratorHPMCMonoGPU<ShapeUnion<ShapeSpheropolyhedron>>(
hoomd/hpmc/module_union_convex_polyhedron.cc:        "IntegratorHPMCMonoConvexPolyhedronUnionGPU");
hoomd/hpmc/module_union_convex_polyhedron.cc:    export_ComputeFreeVolumeGPU<ShapeUnion<ShapeSpheropolyhedron>>(
hoomd/hpmc/module_union_convex_polyhedron.cc:        "ComputeFreeVolumeConvexPolyhedronUnionGPU");
hoomd/hpmc/module_union_convex_polyhedron.cc:    export_UpdaterClustersGPU<ShapeUnion<ShapeSpheropolyhedron>>(
hoomd/hpmc/module_union_convex_polyhedron.cc:        "UpdaterClustersConvexSpheropolyhedronUnionGPU");
hoomd/hpmc/ShapePolyhedron.h:#include "GPUTree.h"
hoomd/hpmc/ShapePolyhedron.h:  tree on the CPU. On the GPU, leave against tree traversal may be faster due to the possibility of
hoomd/hpmc/ShapePolyhedron.h:  tree traversal is non-recursive, occasionally I see stack errors (= overflows) on Pascal GPUs when
hoomd/hpmc/ShapePolyhedron.h:  cudaDeviceSetLimit(). Since GPU performance is mostly deplorable for concave polyhedra, I have not
hoomd/hpmc/ShapePolyhedron.h:        tree = GPUTree(tree_obb, managed);
hoomd/hpmc/ShapePolyhedron.h:    GPUTree tree;
hoomd/hpmc/ShapePolyhedron.h:    const detail::GPUTree& tree;
hoomd/hpmc/ShapePolyhedron.h:    const detail::GPUTree& tree_a = a.tree;
hoomd/hpmc/ShapePolyhedron.h:    const detail::GPUTree& tree_b = b.tree;
hoomd/hpmc/ShapePolyhedron.h:    // stackless traversal on GPU
hoomd/hpmc/kernel_depletants_auxilliary_phase2.cu.inc:#include "IntegratorHPMCMonoGPUDepletantsAuxilliaryPhase2.cuh"
hoomd/hpmc/kernel_depletants_auxilliary_phase2.cu.inc:namespace gpu
hoomd/hpmc/kernel_depletants_auxilliary_phase2.cu.inc:    } // namespace gpu
hoomd/hpmc/module_sphere.cc:#include "ComputeFreeVolumeGPU.h"
hoomd/hpmc/module_sphere.cc:#include "IntegratorHPMCMonoGPU.h"
hoomd/hpmc/module_sphere.cc:#include "UpdaterClustersGPU.h"
hoomd/hpmc/module_sphere.cc:    export_IntegratorHPMCMonoGPU<ShapeSphere>(m, "IntegratorHPMCMonoSphereGPU");
hoomd/hpmc/module_sphere.cc:    export_ComputeFreeVolumeGPU<ShapeSphere>(m, "ComputeFreeVolumeSphereGPU");
hoomd/hpmc/module_sphere.cc:    export_UpdaterClustersGPU<ShapeSphere>(m, "UpdaterClustersSphereGPU");
hoomd/hpmc/IntegratorHPMCMonoGPUDepletantsAuxilliaryPhase1.cuh:#include "hoomd/GPUPartition.cuh"
hoomd/hpmc/IntegratorHPMCMonoGPUDepletantsAuxilliaryPhase1.cuh:#include "GPUHelpers.cuh"
hoomd/hpmc/IntegratorHPMCMonoGPUDepletantsAuxilliaryPhase1.cuh:#include "IntegratorHPMCMonoGPUDepletants.cuh"
hoomd/hpmc/IntegratorHPMCMonoGPUDepletantsAuxilliaryPhase1.cuh:#include "IntegratorHPMCMonoGPUDepletantsAuxilliaryTypes.cuh"
hoomd/hpmc/IntegratorHPMCMonoGPUDepletantsAuxilliaryPhase1.cuh:namespace gpu
hoomd/hpmc/IntegratorHPMCMonoGPUDepletantsAuxilliaryPhase1.cuh:#if (__CUDA_ARCH__ >= 600)
hoomd/hpmc/IntegratorHPMCMonoGPUDepletantsAuxilliaryPhase1.cuh:#if (__CUDA_ARCH__ >= 600)
hoomd/hpmc/IntegratorHPMCMonoGPUDepletantsAuxilliaryPhase1.cuh:        for (int idev = auxilliary_args.gpu_partition_rank.getNumActiveGPUs() - 1; idev >= 0;
hoomd/hpmc/IntegratorHPMCMonoGPUDepletantsAuxilliaryPhase1.cuh:            auto range = auxilliary_args.gpu_partition_rank.getRangeAndSetGPU(idev);
hoomd/hpmc/IntegratorHPMCMonoGPUDepletantsAuxilliaryPhase1.cuh:    } // end namespace gpu
hoomd/hpmc/GPUHelpers.cuh://! Helper functions used by GPU kernels
hoomd/hpmc/GPUHelpers.cuh:namespace gpu
hoomd/hpmc/GPUHelpers.cuh:    } // end namespace gpu
hoomd/hpmc/IntegratorHPMCMonoGPUTypes.cuh:#include "hoomd/GPUPartition.cuh"
hoomd/hpmc/IntegratorHPMCMonoGPUTypes.cuh:namespace gpu
hoomd/hpmc/IntegratorHPMCMonoGPUTypes.cuh:                const GPUPartition& _gpu_partition,
hoomd/hpmc/IntegratorHPMCMonoGPUTypes.cuh:          gpu_partition(_gpu_partition), streams(_streams) { };
hoomd/hpmc/IntegratorHPMCMonoGPUTypes.cuh:    const unsigned int counters_pitch;    //!< Pitch of 2D array counters per GPU
hoomd/hpmc/IntegratorHPMCMonoGPUTypes.cuh:    const hipDeviceProp_t& devprop;            //!< CUDA device properties
hoomd/hpmc/IntegratorHPMCMonoGPUTypes.cuh:    const GPUPartition& gpu_partition;         //!< Multi-GPU partition
hoomd/hpmc/IntegratorHPMCMonoGPUTypes.cuh:                       const GPUPartition& _gpu_partition,
hoomd/hpmc/IntegratorHPMCMonoGPUTypes.cuh:          d_counters(_d_counters), counters_pitch(_counters_pitch), gpu_partition(_gpu_partition),
hoomd/hpmc/IntegratorHPMCMonoGPUTypes.cuh:    const GPUPartition& gpu_partition;
hoomd/hpmc/IntegratorHPMCMonoGPUTypes.cuh:                 const unsigned int ngpu,
hoomd/hpmc/IntegratorHPMCMonoGPUTypes.cuh:                            const GPUPartition& gpu_partition,
hoomd/hpmc/IntegratorHPMCMonoGPUTypes.cuh:    } // end namespace gpu
hoomd/hpmc/ShapeSphere.h:    /// Set CUDA memory hints
hoomd/hpmc/IntegratorHPMCMonoGPUDepletantsAuxilliaryPhase2.cuh:#include "hoomd/GPUPartition.cuh"
hoomd/hpmc/IntegratorHPMCMonoGPUDepletantsAuxilliaryPhase2.cuh:#include "GPUHelpers.cuh"
hoomd/hpmc/IntegratorHPMCMonoGPUDepletantsAuxilliaryPhase2.cuh:#include "IntegratorHPMCMonoGPUDepletants.cuh"
hoomd/hpmc/IntegratorHPMCMonoGPUDepletantsAuxilliaryPhase2.cuh:#include "IntegratorHPMCMonoGPUDepletantsAuxilliaryTypes.cuh"
hoomd/hpmc/IntegratorHPMCMonoGPUDepletantsAuxilliaryPhase2.cuh:namespace gpu
hoomd/hpmc/IntegratorHPMCMonoGPUDepletantsAuxilliaryPhase2.cuh:#if (__CUDA_ARCH__ >= 600)
hoomd/hpmc/IntegratorHPMCMonoGPUDepletantsAuxilliaryPhase2.cuh:#if (__CUDA_ARCH__ >= 600)
hoomd/hpmc/IntegratorHPMCMonoGPUDepletantsAuxilliaryPhase2.cuh:        for (int idev = auxilliary_args.gpu_partition_rank.getNumActiveGPUs() - 1; idev >= 0;
hoomd/hpmc/IntegratorHPMCMonoGPUDepletantsAuxilliaryPhase2.cuh:            auto range = auxilliary_args.gpu_partition_rank.getRangeAndSetGPU(idev);
hoomd/hpmc/IntegratorHPMCMonoGPUDepletantsAuxilliaryPhase2.cuh:            if (idev == (int)auxilliary_args.gpu_partition_rank.getNumActiveGPUs() - 1
hoomd/hpmc/IntegratorHPMCMonoGPUDepletantsAuxilliaryPhase2.cuh:    } // end namespace gpu
hoomd/hpmc/EvaluatorUnionGPU.cuh:#include "hoomd/hpmc/GPUTree.h"
hoomd/hpmc/EvaluatorUnionGPU.cuh:    //! Set CUDA memory hints
hoomd/hpmc/EvaluatorUnionGPU.cuh:    hpmc::detail::GPUTree tree;             //!< OBB tree for constituent particles
hoomd/hpmc/EvaluatorUnionGPU.cuh:    const hpmc::detail::GPUTree& tree_a = params[type_i].tree;
hoomd/hpmc/EvaluatorUnionGPU.cuh:    const hpmc::detail::GPUTree& tree_b = params[type_j].tree;
hoomd/hpmc/PatchEnergyJITUnion.cc:    m_tree[type] = hpmc::detail::GPUTree(tree, m_managed_memory);
hoomd/hpmc/PatchEnergyJITUnion.cc:    const hpmc::detail::GPUTree& tree_a = m_tree[type_i];
hoomd/hpmc/PatchEnergyJITUnion.cc:    const hpmc::detail::GPUTree& tree_b = m_tree[type_j];
hoomd/hpmc/IntegratorHPMCMonoGPU.cuh:#include "hoomd/GPUPartition.cuh"
hoomd/hpmc/IntegratorHPMCMonoGPU.cuh:#include "GPUHelpers.cuh"
hoomd/hpmc/IntegratorHPMCMonoGPU.cuh:#include "IntegratorHPMCMonoGPUTypes.cuh"
hoomd/hpmc/IntegratorHPMCMonoGPU.cuh:namespace gpu
hoomd/hpmc/IntegratorHPMCMonoGPU.cuh:#if (__CUDA_ARCH__ >= 600)
hoomd/hpmc/IntegratorHPMCMonoGPU.cuh:        for (int idev = args.gpu_partition.getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/hpmc/IntegratorHPMCMonoGPU.cuh:            auto range = args.gpu_partition.getRangeAndSetGPU(idev);
hoomd/hpmc/IntegratorHPMCMonoGPU.cuh:    } // namespace gpu
hoomd/hpmc/IntegratorHPMCMono.h:        std::vector<param_type, hoomd::detail::managed_allocator<param_type> > m_params;   //!< Parameters for each particle type on GPU
hoomd/hpmc/IntegratorHPMCMono.h:                                                                       hoomd::detail::managed_allocator<param_type>(m_exec_conf->isCUDAEnabled()));
hoomd/hpmc/IntegratorHPMCMono.h:    setParam(id, typename Shape::param_type(v, m_exec_conf->isCUDAEnabled()));
hoomd/hpmc/IntegratorHPMCMono.h:    \param h_postype Pointer to GPUArray containing particle positions
hoomd/hpmc/IntegratorHPMCMono.h:    \param h_orientation Pointer to GPUArray containing particle orientations
hoomd/hpmc/IntegratorHPMCMono.h:    \param h_overlaps Pointer to GPUArray containing interaction matrix
hoomd/hpmc/IntegratorHPMCMono.h:    NOTE: To avoid numerous acquires and releases of GPUArrays, data pointers are passed directly into this const function.
hoomd/hpmc/IntegratorHPMCMonoGPU.h:#include "hoomd/hpmc/IntegratorHPMCMonoGPUDepletantsAuxilliaryTypes.cuh"
hoomd/hpmc/IntegratorHPMCMonoGPU.h:#include "hoomd/hpmc/IntegratorHPMCMonoGPUDepletantsTypes.cuh"
hoomd/hpmc/IntegratorHPMCMonoGPU.h:#include "hoomd/hpmc/IntegratorHPMCMonoGPUTypes.cuh"
hoomd/hpmc/IntegratorHPMCMonoGPU.h:#include "hoomd/GPUVector.h"
hoomd/hpmc/IntegratorHPMCMonoGPU.h:#include "hoomd/GPUPartition.cuh"
hoomd/hpmc/IntegratorHPMCMonoGPU.h:/*! \file IntegratorHPMCMonoGPU.h
hoomd/hpmc/IntegratorHPMCMonoGPU.h:    \brief Defines the template class for HPMC on the GPU
hoomd/hpmc/IntegratorHPMCMonoGPU.h:    \note we use GPUArrays instead of GlobalArrays currently to allow host access to the shuffled
hoomd/hpmc/IntegratorHPMCMonoGPU.h:class UpdateOrderGPU
hoomd/hpmc/IntegratorHPMCMonoGPU.h:    UpdateOrderGPU(std::shared_ptr<const ExecutionConfiguration> exec_conf, unsigned int N = 0)
hoomd/hpmc/IntegratorHPMCMonoGPU.h://! Template class for HPMC update on the GPU
hoomd/hpmc/IntegratorHPMCMonoGPU.h:template<class Shape> class IntegratorHPMCMonoGPU : public IntegratorHPMCMono<Shape>
hoomd/hpmc/IntegratorHPMCMonoGPU.h:    IntegratorHPMCMonoGPU(std::shared_ptr<SystemDefinition> sysdef, std::shared_ptr<CellList> cl);
hoomd/hpmc/IntegratorHPMCMonoGPU.h:    virtual ~IntegratorHPMCMonoGPU();
hoomd/hpmc/IntegratorHPMCMonoGPU.h:    detail::UpdateOrderGPU m_update_order; //!< Particle update order
hoomd/hpmc/IntegratorHPMCMonoGPU.h:    //! Update GPU memory hints
hoomd/hpmc/IntegratorHPMCMonoGPU.h:    virtual void updateGPUAdvice();
hoomd/hpmc/IntegratorHPMCMonoGPU.h:IntegratorHPMCMonoGPU<Shape>::IntegratorHPMCMonoGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/hpmc/IntegratorHPMCMonoGPU.h:    // with multiple GPUs, request a cell list per device
hoomd/hpmc/IntegratorHPMCMonoGPU.h:        auto gpu_map = this->m_exec_conf->getGPUIds();
hoomd/hpmc/IntegratorHPMCMonoGPU.h:        cudaMemAdvise(m_condition.get(),
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                      cudaMemAdviseSetPreferredLocation,
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                      cudaCpuDeviceId);
hoomd/hpmc/IntegratorHPMCMonoGPU.h:        cudaMemPrefetchAsync(m_condition.get(), sizeof(unsigned int), cudaCpuDeviceId);
hoomd/hpmc/IntegratorHPMCMonoGPU.h:        for (unsigned int idev = 0; idev < this->m_exec_conf->getNumActiveGPUs(); ++idev)
hoomd/hpmc/IntegratorHPMCMonoGPU.h:            cudaMemAdvise(m_condition.get(),
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                          cudaMemAdviseSetAccessedBy,
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                          gpu_map[idev]);
hoomd/hpmc/IntegratorHPMCMonoGPU.h:        CHECK_CUDA_ERROR();
hoomd/hpmc/IntegratorHPMCMonoGPU.h:    //! One counter per GPU, separated by an entire memory page
hoomd/hpmc/IntegratorHPMCMonoGPU.h:    GlobalArray<hpmc_counters_t>(pitch, this->m_exec_conf->getNumActiveGPUs(), this->m_exec_conf)
hoomd/hpmc/IntegratorHPMCMonoGPU.h:        auto gpu_map = this->m_exec_conf->getGPUIds();
hoomd/hpmc/IntegratorHPMCMonoGPU.h:        for (unsigned int idev = 0; idev < this->m_exec_conf->getNumActiveGPUs(); ++idev)
hoomd/hpmc/IntegratorHPMCMonoGPU.h:            cudaMemAdvise(m_counters.get() + idev * m_counters.getPitch(),
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                          cudaMemAdviseSetPreferredLocation,
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                          gpu_map[idev]);
hoomd/hpmc/IntegratorHPMCMonoGPU.h:            cudaMemPrefetchAsync(m_counters.get() + idev * m_counters.getPitch(),
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                                 gpu_map[idev]);
hoomd/hpmc/IntegratorHPMCMonoGPU.h:        CHECK_CUDA_ERROR();
hoomd/hpmc/IntegratorHPMCMonoGPU.h:    // ntypes counters per GPU, separated by at least a memory page
hoomd/hpmc/IntegratorHPMCMonoGPU.h:        this->m_exec_conf->getNumActiveGPUs(),
hoomd/hpmc/IntegratorHPMCMonoGPU.h:        auto gpu_map = this->m_exec_conf->getGPUIds();
hoomd/hpmc/IntegratorHPMCMonoGPU.h:        for (unsigned int idev = 0; idev < this->m_exec_conf->getNumActiveGPUs(); ++idev)
hoomd/hpmc/IntegratorHPMCMonoGPU.h:            cudaMemAdvise(m_implicit_counters.get() + idev * m_implicit_counters.getPitch(),
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                          cudaMemAdviseSetPreferredLocation,
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                          gpu_map[idev]);
hoomd/hpmc/IntegratorHPMCMonoGPU.h:            cudaMemPrefetchAsync(m_implicit_counters.get() + idev * m_implicit_counters.getPitch(),
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                                 gpu_map[idev]);
hoomd/hpmc/IntegratorHPMCMonoGPU.h:    m_narrow_phase_streams.resize(this->m_exec_conf->getNumActiveGPUs());
hoomd/hpmc/IntegratorHPMCMonoGPU.h:    for (int idev = this->m_exec_conf->getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/hpmc/IntegratorHPMCMonoGPU.h:        hipSetDevice(this->m_exec_conf->getGPUIds()[idev]);
hoomd/hpmc/IntegratorHPMCMonoGPU.h:        m_depletant_streams[itype].resize(this->m_exec_conf->getNumActiveGPUs());
hoomd/hpmc/IntegratorHPMCMonoGPU.h:        m_depletant_streams_phase1[itype].resize(this->m_exec_conf->getNumActiveGPUs());
hoomd/hpmc/IntegratorHPMCMonoGPU.h:        m_depletant_streams_phase2[itype].resize(this->m_exec_conf->getNumActiveGPUs());
hoomd/hpmc/IntegratorHPMCMonoGPU.h:        for (int idev = this->m_exec_conf->getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/hpmc/IntegratorHPMCMonoGPU.h:            hipSetDevice(this->m_exec_conf->getGPUIds()[idev]);
hoomd/hpmc/IntegratorHPMCMonoGPU.h:        m_sync[itype].resize(this->m_exec_conf->getNumActiveGPUs());
hoomd/hpmc/IntegratorHPMCMonoGPU.h:        m_sync_phase1[itype].resize(this->m_exec_conf->getNumActiveGPUs());
hoomd/hpmc/IntegratorHPMCMonoGPU.h:        m_sync_phase2[itype].resize(this->m_exec_conf->getNumActiveGPUs());
hoomd/hpmc/IntegratorHPMCMonoGPU.h:        for (int idev = this->m_exec_conf->getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/hpmc/IntegratorHPMCMonoGPU.h:            hipSetDevice(this->m_exec_conf->getGPUIds()[idev]);
hoomd/hpmc/IntegratorHPMCMonoGPU.h:        cudaMemAdvise(this->m_overlaps.get(),
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                      cudaMemAdviseSetReadMostly,
hoomd/hpmc/IntegratorHPMCMonoGPU.h:        CHECK_CUDA_ERROR();
hoomd/hpmc/IntegratorHPMCMonoGPU.h:template<class Shape> IntegratorHPMCMonoGPU<Shape>::~IntegratorHPMCMonoGPU()
hoomd/hpmc/IntegratorHPMCMonoGPU.h:        for (int idev = this->m_exec_conf->getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/hpmc/IntegratorHPMCMonoGPU.h:            hipSetDevice(this->m_exec_conf->getGPUIds()[idev]);
hoomd/hpmc/IntegratorHPMCMonoGPU.h:        for (int idev = this->m_exec_conf->getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/hpmc/IntegratorHPMCMonoGPU.h:            hipSetDevice(this->m_exec_conf->getGPUIds()[idev]);
hoomd/hpmc/IntegratorHPMCMonoGPU.h:        for (int idev = this->m_exec_conf->getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/hpmc/IntegratorHPMCMonoGPU.h:            hipSetDevice(this->m_exec_conf->getGPUIds()[idev]);
hoomd/hpmc/IntegratorHPMCMonoGPU.h:        for (int idev = this->m_exec_conf->getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/hpmc/IntegratorHPMCMonoGPU.h:            hipSetDevice(this->m_exec_conf->getGPUIds()[idev]);
hoomd/hpmc/IntegratorHPMCMonoGPU.h:        for (int idev = this->m_exec_conf->getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/hpmc/IntegratorHPMCMonoGPU.h:            hipSetDevice(this->m_exec_conf->getGPUIds()[idev]);
hoomd/hpmc/IntegratorHPMCMonoGPU.h:        for (int idev = this->m_exec_conf->getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/hpmc/IntegratorHPMCMonoGPU.h:            hipSetDevice(this->m_exec_conf->getGPUIds()[idev]);
hoomd/hpmc/IntegratorHPMCMonoGPU.h:    for (int idev = this->m_exec_conf->getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/hpmc/IntegratorHPMCMonoGPU.h:        hipSetDevice(this->m_exec_conf->getGPUIds()[idev]);
hoomd/hpmc/IntegratorHPMCMonoGPU.h:template<class Shape> void IntegratorHPMCMonoGPU<Shape>::updateGPUAdvice()
hoomd/hpmc/IntegratorHPMCMonoGPU.h:        auto gpu_map = this->m_exec_conf->getGPUIds();
hoomd/hpmc/IntegratorHPMCMonoGPU.h:        for (unsigned int idev = 0; idev < this->m_exec_conf->getNumActiveGPUs(); ++idev)
hoomd/hpmc/IntegratorHPMCMonoGPU.h:            auto range = this->m_pdata->getGPUPartition().getRange(idev);
hoomd/hpmc/IntegratorHPMCMonoGPU.h:            cudaMemAdvise(m_trial_postype.get() + range.first,
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                          cudaMemAdviseSetPreferredLocation,
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                          gpu_map[idev]);
hoomd/hpmc/IntegratorHPMCMonoGPU.h:            cudaMemPrefetchAsync(m_trial_postype.get() + range.first,
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                                 gpu_map[idev]);
hoomd/hpmc/IntegratorHPMCMonoGPU.h:            cudaMemAdvise(m_trial_move_type.get() + range.first,
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                          cudaMemAdviseSetPreferredLocation,
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                          gpu_map[idev]);
hoomd/hpmc/IntegratorHPMCMonoGPU.h:            cudaMemPrefetchAsync(m_trial_move_type.get() + range.first,
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                                 gpu_map[idev]);
hoomd/hpmc/IntegratorHPMCMonoGPU.h:            cudaMemAdvise(m_reject.get() + range.first,
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                          cudaMemAdviseSetPreferredLocation,
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                          gpu_map[idev]);
hoomd/hpmc/IntegratorHPMCMonoGPU.h:            cudaMemPrefetchAsync(m_reject.get() + range.first,
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                                 gpu_map[idev]);
hoomd/hpmc/IntegratorHPMCMonoGPU.h:            cudaMemAdvise(m_trial_orientation.get() + range.first,
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                          cudaMemAdviseSetPreferredLocation,
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                          gpu_map[idev]);
hoomd/hpmc/IntegratorHPMCMonoGPU.h:            cudaMemPrefetchAsync(m_trial_orientation.get() + range.first,
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                                 gpu_map[idev]);
hoomd/hpmc/IntegratorHPMCMonoGPU.h:            cudaMemAdvise(m_trial_vel.get() + range.first,
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                          cudaMemAdviseSetPreferredLocation,
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                          gpu_map[idev]);
hoomd/hpmc/IntegratorHPMCMonoGPU.h:            cudaMemPrefetchAsync(m_trial_vel.get() + range.first,
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                                 gpu_map[idev]);
hoomd/hpmc/IntegratorHPMCMonoGPU.h:            cudaMemAdvise(m_reject_out.get() + range.first,
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                          cudaMemAdviseSetPreferredLocation,
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                          gpu_map[idev]);
hoomd/hpmc/IntegratorHPMCMonoGPU.h:            cudaMemPrefetchAsync(m_reject_out.get() + range.first,
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                                 gpu_map[idev]);
hoomd/hpmc/IntegratorHPMCMonoGPU.h:            cudaMemAdvise(m_reject_out_of_cell.get() + range.first,
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                          cudaMemAdviseSetPreferredLocation,
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                          gpu_map[idev]);
hoomd/hpmc/IntegratorHPMCMonoGPU.h:            cudaMemPrefetchAsync(m_reject_out_of_cell.get() + range.first,
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                                 gpu_map[idev]);
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                cudaMemAdvise(m_n_depletants.get() + itype + range.first,
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                              cudaMemAdviseSetPreferredLocation,
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                              gpu_map[idev]);
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                cudaMemPrefetchAsync(m_n_depletants.get() + itype + range.first,
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                                     gpu_map[idev]);
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                cudaMemAdvise(m_n_depletants_ntrial.get() + ntrial_offset + range.first,
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                              cudaMemAdviseSetPreferredLocation,
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                              gpu_map[idev]);
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                cudaMemPrefetchAsync(m_n_depletants_ntrial.get() + ntrial_offset + range.first,
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                                     gpu_map[idev]);
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                cudaMemAdvise(m_deltaF_int.get() + itype + range.first,
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                              cudaMemAdviseSetPreferredLocation,
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                              gpu_map[idev]);
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                cudaMemPrefetchAsync(m_deltaF_int.get() + itype + range.first,
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                                     gpu_map[idev]);
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                CHECK_CUDA_ERROR();
hoomd/hpmc/IntegratorHPMCMonoGPU.h:template<class Shape> void IntegratorHPMCMonoGPU<Shape>::update(uint64_t timestep)
hoomd/hpmc/IntegratorHPMCMonoGPU.h:            oss << "Simulation box too small for GPU accelerated HPMC execution - increase it so "
hoomd/hpmc/IntegratorHPMCMonoGPU.h:        bool update_gpu_advice = false;
hoomd/hpmc/IntegratorHPMCMonoGPU.h:            update_gpu_advice = true;
hoomd/hpmc/IntegratorHPMCMonoGPU.h:            update_gpu_advice = true;
hoomd/hpmc/IntegratorHPMCMonoGPU.h:        GPUPartition gpu_partition_rank = this->m_pdata->getGPUPartition();
hoomd/hpmc/IntegratorHPMCMonoGPU.h:            gpu_partition_rank.setN(np, offset);
hoomd/hpmc/IntegratorHPMCMonoGPU.h:            update_gpu_advice = true;
hoomd/hpmc/IntegratorHPMCMonoGPU.h:            update_gpu_advice = true;
hoomd/hpmc/IntegratorHPMCMonoGPU.h:        if (update_gpu_advice)
hoomd/hpmc/IntegratorHPMCMonoGPU.h:            updateGPUAdvice();
hoomd/hpmc/IntegratorHPMCMonoGPU.h:        unsigned int ngpu = this->m_exec_conf->getNumActiveGPUs();
hoomd/hpmc/IntegratorHPMCMonoGPU.h:        if (ngpu > 1)
hoomd/hpmc/IntegratorHPMCMonoGPU.h:            if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                CHECK_CUDA_ERROR();
hoomd/hpmc/IntegratorHPMCMonoGPU.h:            if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                CHECK_CUDA_ERROR();
hoomd/hpmc/IntegratorHPMCMonoGPU.h:        gpu::hpmc_excell(d_excell_idx.data,
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                         this->m_exec_conf->getNumActiveGPUs(),
hoomd/hpmc/IntegratorHPMCMonoGPU.h:        if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/hpmc/IntegratorHPMCMonoGPU.h:            CHECK_CUDA_ERROR();
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                // fill the parameter structure for the GPU kernels
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                gpu::hpmc_args_t args(d_postype.data,
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                                      ngpu > 1 ? d_counters_per_device.data : d_counters.data,
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                                      this->m_pdata->getGPUPartition(),
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                // propose trial moves, \sa gpu::kernel::hpmc_moves
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                gpu::hpmc_gen_moves<Shape>(args, params.data());
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                    CHECK_CUDA_ERROR();
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                this->m_exec_conf->beginMultiGPU();
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                for (int idev = this->m_exec_conf->getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                    hipSetDevice(this->m_exec_conf->getGPUIds()[idev]);
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                    auto range = this->m_pdata->getGPUPartition().getRange(idev);
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                    if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                        CHECK_CUDA_ERROR();
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                this->m_exec_conf->endMultiGPU();
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                    if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                        CHECK_CUDA_ERROR();
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                    this->m_exec_conf->beginMultiGPU();
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                            for (int idev = this->m_exec_conf->getNumActiveGPUs() - 1; idev >= 0;
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                                hipSetDevice(this->m_exec_conf->getGPUIds()[idev]);
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                                auto range = this->m_pdata->getGPUPartition().getRange(idev);
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                                if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                                    CHECK_CUDA_ERROR();
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                    this->m_exec_conf->endMultiGPU();
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                    // fill the parameter structure for the GPU kernels
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                    gpu::hpmc_args_t args(d_postype.data,
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                                          ngpu > 1 ? d_counters_per_device.data : d_counters.data,
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                                          this->m_pdata->getGPUPartition(),
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                    this->m_exec_conf->beginMultiGPU();
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                    gpu::hpmc_narrow_phase<Shape>(args, params.data());
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                    if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                        CHECK_CUDA_ERROR();
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                    // allow concurrency between depletant types in multi GPU block
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                            gpu::generate_num_depletants(this->m_sysdef->getSeed(),
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                                                         this->m_pdata->getGPUPartition(),
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                            if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                                CHECK_CUDA_ERROR();
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                            unsigned int max_n_depletants[this->m_exec_conf->getNumActiveGPUs()];
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                            gpu::get_max_num_depletants(
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                                this->m_pdata->getGPUPartition(),
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                            if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                                CHECK_CUDA_ERROR();
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                            gpu::hpmc_implicit_args_t implicit_args(
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                                ngpu > 1 ? d_implicit_counters_per_device.data
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                            gpu::hpmc_insert_depletants<Shape>(args, implicit_args, params.data());
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                            if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                                CHECK_CUDA_ERROR();
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                            gpu::generate_num_depletants_ntrial(
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                                gpu_partition_rank,
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                            if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                                CHECK_CUDA_ERROR();
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                            unsigned int max_n_depletants[this->m_exec_conf->getNumActiveGPUs()];
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                            gpu::get_max_num_depletants_ntrial(
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                                gpu_partition_rank,
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                            if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                                CHECK_CUDA_ERROR();
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                            auto gpu_map = this->m_exec_conf->getGPUIds();
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                            for (int idev = (int)gpu_map.size() - 1; idev >= 0; --idev)
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                                hipSetDevice(gpu_map[idev]);
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                                if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                                    CHECK_CUDA_ERROR();
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                            gpu::hpmc_implicit_args_t implicit_args(
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                                ngpu > 1 ? d_implicit_counters_per_device.data
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                            unsigned int nwork_rank[this->m_exec_conf->getNumActiveGPUs()];
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                            unsigned int work_offset[this->m_exec_conf->getNumActiveGPUs()];
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                                 idev < this->m_exec_conf->getNumActiveGPUs();
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                                     idev < this->m_exec_conf->getNumActiveGPUs();
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                            gpu::hpmc_auxilliary_args_t auxilliary_args(
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                                gpu_partition_rank);
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                            gpu::hpmc_depletants_auxilliary_phase1<Shape>(args,
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                            if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                                CHECK_CUDA_ERROR();
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                            gpu::hpmc_depletants_auxilliary_phase2<Shape>(args,
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                            if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                                CHECK_CUDA_ERROR();
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                            for (int idev = (int)gpu_map.size() - 1; idev >= 0; --idev)
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                                hipSetDevice(gpu_map[idev]);
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                                if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                                    CHECK_CUDA_ERROR();
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                    this->m_exec_conf->endMultiGPU();
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                    this->m_exec_conf->beginMultiGPU();
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                    gpu::hpmc_depletants_accept(this->m_sysdef->getSeed(),
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                                                this->m_pdata->getGPUPartition(),
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                    if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                        CHECK_CUDA_ERROR();
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                    this->m_exec_conf->endMultiGPU();
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                    PatchEnergy::gpu_args_t patch_args(d_postype.data,
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                                                       this->m_pdata->getGPUPartition());
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                    this->m_patch->computePatchEnergyGPU(patch_args, 0);
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                    this->m_exec_conf->beginMultiGPU();
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                    gpu::hpmc_check_convergence(d_trial_move_type.data,
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                                                this->m_pdata->getGPUPartition(),
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                    if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                        CHECK_CUDA_ERROR();
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                    this->m_exec_conf->endMultiGPU();
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                this->m_exec_conf->beginMultiGPU();
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                gpu::hpmc_update_args_t args(d_postype.data,
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                                             ngpu > 1 ? d_counters_per_device.data
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                                             this->m_pdata->getGPUPartition(),
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                gpu::hpmc_update_pdata<Shape>(args, params.data());
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                    CHECK_CUDA_ERROR();
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                this->m_exec_conf->endMultiGPU();
hoomd/hpmc/IntegratorHPMCMonoGPU.h:        if (ngpu > 1)
hoomd/hpmc/IntegratorHPMCMonoGPU.h:            gpu::reduce_counters(this->m_exec_conf->getNumActiveGPUs(),
hoomd/hpmc/IntegratorHPMCMonoGPU.h:        gpu::hpmc_shift(d_postype.data, d_image.data, this->m_pdata->getN(), box, shift, 128);
hoomd/hpmc/IntegratorHPMCMonoGPU.h:    if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/hpmc/IntegratorHPMCMonoGPU.h:        CHECK_CUDA_ERROR();
hoomd/hpmc/IntegratorHPMCMonoGPU.h:template<class Shape> void IntegratorHPMCMonoGPU<Shape>::initializeExcellMem()
hoomd/hpmc/IntegratorHPMCMonoGPU.h:        = this->m_cl->getPerDevice() ? this->m_exec_conf->getNumActiveGPUs() : 1;
hoomd/hpmc/IntegratorHPMCMonoGPU.h:    && 0 // excell is currently not multi-GPU optimized, let the CUDA driver figure this out
hoomd/hpmc/IntegratorHPMCMonoGPU.h:        auto gpu_map = this->m_exec_conf->getGPUIds();
hoomd/hpmc/IntegratorHPMCMonoGPU.h:        for (unsigned int idev = 0; idev < this->m_exec_conf->getNumActiveGPUs(); ++idev)
hoomd/hpmc/IntegratorHPMCMonoGPU.h:            cudaMemAdvise(m_excell_idx.get(),
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                          cudaMemAdviseSetAccessedBy,
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                          gpu_map[idev]);
hoomd/hpmc/IntegratorHPMCMonoGPU.h:            cudaMemAdvise(m_excell_size.get(),
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                          cudaMemAdviseSetAccessedBy,
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                          gpu_map[idev]);
hoomd/hpmc/IntegratorHPMCMonoGPU.h:            CHECK_CUDA_ERROR();
hoomd/hpmc/IntegratorHPMCMonoGPU.h:template<class Shape> void IntegratorHPMCMonoGPU<Shape>::updateCellWidth()
hoomd/hpmc/IntegratorHPMCMonoGPU.h:    cudaMemAdvise(this->m_params.data(),
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                  cudaMemAdviseSetReadMostly,
hoomd/hpmc/IntegratorHPMCMonoGPU.h:    CHECK_CUDA_ERROR();
hoomd/hpmc/IntegratorHPMCMonoGPU.h:        CHECK_CUDA_ERROR();
hoomd/hpmc/IntegratorHPMCMonoGPU.h:void export_IntegratorHPMCMonoGPU(pybind11::module& m, const std::string& name)
hoomd/hpmc/IntegratorHPMCMonoGPU.h:    pybind11::class_<IntegratorHPMCMonoGPU<Shape>,
hoomd/hpmc/IntegratorHPMCMonoGPU.h:                     std::shared_ptr<IntegratorHPMCMonoGPU<Shape>>>(m, name.c_str())
hoomd/hpmc/IntegratorHPMCMonoGPU.h:        .def("setNtrialCommunicator", &IntegratorHPMCMonoGPU<Shape>::setNtrialCommunicator)
hoomd/hpmc/IntegratorHPMCMonoGPU.h:        .def("setParticleCommunicator", &IntegratorHPMCMonoGPU<Shape>::setParticleCommunicator)
hoomd/hpmc/PatchEnergyJITUnion.h:#include "hoomd/hpmc/GPUTree.h"
hoomd/hpmc/PatchEnergyJITUnion.h:              hoomd::detail::managed_allocator<float>(m_exec_conf->isCUDAEnabled()))
hoomd/hpmc/PatchEnergyJITUnion.h:    std::vector<hpmc::detail::GPUTree> m_tree; // The tree acceleration structure per particle type
hoomd/hpmc/PatchEnergyJITUnion.h:    bool m_managed_memory; // Flag to managed memory on the GPU (only used for OBB construction)
hoomd/hpmc/ShapeEllipsoid.h:    /// Set CUDA memory hints
hoomd/hpmc/UpdaterClustersGPU.cuh:/*! \file UpdaterClustersGPU.cuh
hoomd/hpmc/UpdaterClustersGPU.cuh:    \brief Implements the overlap kernels for the geometric cluster algorithm the GPU
hoomd/hpmc/UpdaterClustersGPU.cuh:#include "hoomd/GPUPartition.cuh"
hoomd/hpmc/UpdaterClustersGPU.cuh:#include "GPUHelpers.cuh"
hoomd/hpmc/UpdaterClustersGPU.cuh:#include "IntegratorHPMCMonoGPUTypes.cuh"
hoomd/hpmc/UpdaterClustersGPU.cuh:namespace gpu
hoomd/hpmc/UpdaterClustersGPU.cuh://! Wraps arguments to GPU driver functions
hoomd/hpmc/UpdaterClustersGPU.cuh:                   const GPUPartition& _gpu_partition,
hoomd/hpmc/UpdaterClustersGPU.cuh:          devprop(_devprop), gpu_partition(_gpu_partition), streams(_streams) { };
hoomd/hpmc/UpdaterClustersGPU.cuh:    const hipDeviceProp_t& devprop;       //!< CUDA device properties
hoomd/hpmc/UpdaterClustersGPU.cuh:    const GPUPartition& gpu_partition;    //!< Multi-GPU partition
hoomd/hpmc/UpdaterClustersGPU.cuh:                       const GPUPartition& gpu_partition,
hoomd/hpmc/UpdaterClustersGPU.cuh:                                const GPUPartition& gpu_partition,
hoomd/hpmc/UpdaterClustersGPU.cuh:                   const GPUPartition& gpu_partition,
hoomd/hpmc/UpdaterClustersGPU.cuh://! Arguments to gpu::transform_particles
hoomd/hpmc/UpdaterClustersGPU.cuh:                              const GPUPartition& _gpu_partition,
hoomd/hpmc/UpdaterClustersGPU.cuh:          q(_q), line(_line), gpu_partition(_gpu_partition), box(_box), num_types(_num_types),
hoomd/hpmc/UpdaterClustersGPU.cuh:    const GPUPartition& gpu_partition;
hoomd/hpmc/UpdaterClustersGPU.cuh:#if (__CUDA_ARCH__ >= 600)
hoomd/hpmc/UpdaterClustersGPU.cuh:        for (int idev = args.gpu_partition.getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/hpmc/UpdaterClustersGPU.cuh:            auto range = args.gpu_partition.getRangeAndSetGPU(idev);
hoomd/hpmc/UpdaterClustersGPU.cuh:    for (int idev = args.gpu_partition.getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/hpmc/UpdaterClustersGPU.cuh:        auto range = args.gpu_partition.getRangeAndSetGPU(idev);
hoomd/hpmc/UpdaterClustersGPU.cuh:    } // end namespace gpu
hoomd/hpmc/module_spheropolygon.cc:#include "ComputeFreeVolumeGPU.h"
hoomd/hpmc/module_spheropolygon.cc:#include "IntegratorHPMCMonoGPU.h"
hoomd/hpmc/module_spheropolygon.cc:#include "UpdaterClustersGPU.h"
hoomd/hpmc/module_spheropolygon.cc:    export_IntegratorHPMCMonoGPU<ShapeSpheropolygon>(m, "IntegratorHPMCMonoSpheropolygonGPU");
hoomd/hpmc/module_spheropolygon.cc:    export_ComputeFreeVolumeGPU<ShapeSpheropolygon>(m, "ComputeFreeVolumeSpheropolygonGPU");
hoomd/hpmc/module_spheropolygon.cc:    export_UpdaterClustersGPU<ShapeSpheropolygon>(m, "UpdaterClustersConvexSpheropolygonGPU");
hoomd/hpmc/PatchEnergyJIT.cc:                    hoomd::detail::managed_allocator<float>(m_exec_conf->isCUDAEnabled())),
hoomd/hpmc/PairPotentialUnion.cc:        m_tree[type_id] = hpmc::detail::GPUTree(tree, false);
hoomd/hpmc/CMakeLists.txt:set(_hpmc_gpu_shapes ShapeSphere
hoomd/hpmc/CMakeLists.txt:option(ENABLE_HPMC_SPHINX_GPU "Enable sphinx on the GPU" OFF)
hoomd/hpmc/CMakeLists.txt:option(ENABLE_DEBUG_JIT "Enable printing of debug info of JIT compilation on GPU" off)
hoomd/hpmc/CMakeLists.txt:if (ENABLE_HPMC_SPHINX_GPU)
hoomd/hpmc/CMakeLists.txt:    set(_hpmc_gpu_shapes ${_hpmc_gpu_shapes} ShapeSphinx)
hoomd/hpmc/CMakeLists.txt:set(_hpmc_gpu_union_shapes ShapeSphere
hoomd/hpmc/CMakeLists.txt:    ComputeFreeVolumeGPU.cuh
hoomd/hpmc/CMakeLists.txt:    ComputeFreeVolumeGPU.h
hoomd/hpmc/CMakeLists.txt:    GPUHelpers.cuh
hoomd/hpmc/CMakeLists.txt:    GPUTree.h
hoomd/hpmc/CMakeLists.txt:    IntegratorHPMCMonoGPU.cuh
hoomd/hpmc/CMakeLists.txt:    IntegratorHPMCMonoGPUJIT.inc
hoomd/hpmc/CMakeLists.txt:    IntegratorHPMCMonoGPUMoves.cuh
hoomd/hpmc/CMakeLists.txt:    IntegratorHPMCMonoGPUTypes.cuh
hoomd/hpmc/CMakeLists.txt:    IntegratorHPMCMonoGPUDepletants.cuh
hoomd/hpmc/CMakeLists.txt:    IntegratorHPMCMonoGPUDepletantsTypes.cuh
hoomd/hpmc/CMakeLists.txt:    IntegratorHPMCMonoGPUDepletantsAuxilliaryPhase1.cuh
hoomd/hpmc/CMakeLists.txt:    IntegratorHPMCMonoGPUDepletantsAuxilliaryPhase2.cuh
hoomd/hpmc/CMakeLists.txt:    IntegratorHPMCMonoGPUDepletantsAuxilliaryTypes.cuh
hoomd/hpmc/CMakeLists.txt:    IntegratorHPMCMonoGPU.h
hoomd/hpmc/CMakeLists.txt:    UpdaterClustersGPU.cuh
hoomd/hpmc/CMakeLists.txt:    UpdaterClustersGPU.h
hoomd/hpmc/CMakeLists.txt:    UpdaterClustersGPUDepletants.cuh
hoomd/hpmc/CMakeLists.txt:set(_hpmc_cu_sources IntegratorHPMCMonoGPU.cu
hoomd/hpmc/CMakeLists.txt:                     IntegratorHPMCMonoGPUDepletants.cu
hoomd/hpmc/CMakeLists.txt:                     UpdaterClustersGPU.cu
hoomd/hpmc/CMakeLists.txt:    # expand the shape x GPU kernel matrix of template instantiations
hoomd/hpmc/CMakeLists.txt:        foreach(SHAPE ${_hpmc_gpu_shapes})
hoomd/hpmc/CMakeLists.txt:        foreach(SHAPE ${_hpmc_gpu_union_shapes})
hoomd/hpmc/CMakeLists.txt:set(_cuda_sources ${_hpmc_cu_sources})
hoomd/hpmc/CMakeLists.txt:hoomd_add_module(_hpmc SHARED ${_hpmc_sources} ${_cuda_sources} ${_hpmc_headers} NO_EXTRAS)
hoomd/hpmc/CMakeLists.txt:target_link_libraries(_hpmc PUBLIC CUDA::cusparse )
hoomd/hpmc/CMakeLists.txt:         GPUEvalFactory.cc
hoomd/hpmc/CMakeLists.txt:         PatchEnergyJITGPU.cc
hoomd/hpmc/CMakeLists.txt:         PatchEnergyJITUnionGPU.cc
hoomd/hpmc/CMakeLists.txt:                                 PatchEnergyJITGPU.h
hoomd/hpmc/CMakeLists.txt:                                 PatchEnergyJITUnionGPU.h
hoomd/hpmc/CMakeLists.txt:                                 EvaluatorUnionGPU.cuh
hoomd/hpmc/CMakeLists.txt:                                 GPUEvalFactory.h
hoomd/hpmc/CMakeLists.txt:        target_link_libraries(_${PACKAGE_NAME} PUBLIC CUDA::cuda CUDA::nvrtc)
hoomd/hpmc/nec/integrate.py:        `Sphere` does not support execution on GPUs.
hoomd/hpmc/nec/integrate.py:        `ConvexPolyhedron` does not support execution on GPUs.
hoomd/hpmc/module_faceted_ellipsoid.cc:#include "ComputeFreeVolumeGPU.h"
hoomd/hpmc/module_faceted_ellipsoid.cc:#include "IntegratorHPMCMonoGPU.h"
hoomd/hpmc/module_faceted_ellipsoid.cc:#include "UpdaterClustersGPU.h"
hoomd/hpmc/module_faceted_ellipsoid.cc:    export_IntegratorHPMCMonoGPU<ShapeFacetedEllipsoid>(m, "IntegratorHPMCMonoFacetedEllipsoidGPU");
hoomd/hpmc/module_faceted_ellipsoid.cc:    export_ComputeFreeVolumeGPU<ShapeFacetedEllipsoid>(m, "ComputeFreeVolumeFacetedEllipsoidGPU");
hoomd/hpmc/module_faceted_ellipsoid.cc:    export_UpdaterClustersGPU<ShapeFacetedEllipsoid>(m, "UpdaterClustersFacetedEllipsoidGPU");
hoomd/hpmc/update.py:        use_gpu = (isinstance(self._simulation.device, hoomd.device.GPU)
hoomd/hpmc/update.py:                   and (cpp_cls_name + 'GPU') in _hpmc.__dict__)
hoomd/hpmc/update.py:        if use_gpu:
hoomd/hpmc/update.py:            cpp_cls_name += "GPU"
hoomd/hpmc/update.py:        if use_gpu:
hoomd/hpmc/update.py:            self._cpp_cell = _hoomd.CellListGPU(sys_def)
hoomd/hpmc/UpdaterShape.h:    GPUArray<Scalar>
hoomd/hpmc/UpdaterShape.h:    GPUArray<unsigned int> m_ntypes;  // number of particle types in the simulation
hoomd/hpmc/UpdaterShape.h:        GPUArray<Scalar> determinant_inertia_tensor_old(m_determinant_inertia_tensor);
hoomd/hpmc/UpdaterShape.h:                                          m_exec_conf->isCUDAEnabled());
hoomd/hpmc/kernel_depletants_auxilliary_phase1.cu.inc:#include "IntegratorHPMCMonoGPUDepletantsAuxilliaryPhase1.cuh"
hoomd/hpmc/kernel_depletants_auxilliary_phase1.cu.inc:namespace gpu
hoomd/hpmc/kernel_depletants_auxilliary_phase1.cu.inc:    } // namespace gpu
hoomd/hpmc/kernel_free_volume.cu.inc:#include "ComputeFreeVolumeGPU.cuh"
hoomd/hpmc/kernel_free_volume.cu.inc://! HPMC kernel for ComputeFreeVolumeGPU
hoomd/hpmc/kernel_free_volume.cu.inc:gpu_hpmc_free_volume<SHAPE_CLASS(SHAPE)>(const hpmc_free_volume_args_t& args,
hoomd/hpmc/PairPotentialUnion.h:#include "GPUTree.h"
hoomd/hpmc/PairPotentialUnion.h:    std::vector<hpmc::detail::GPUTree> m_tree;
hoomd/hpmc/PairPotentialUnion.h:        const hpmc::detail::GPUTree& tree_a = m_tree[type_i];
hoomd/hpmc/PairPotentialUnion.h:        const hpmc::detail::GPUTree& tree_b = m_tree[type_j];
hoomd/hpmc/module_convex_polygon.cc:#include "ComputeFreeVolumeGPU.h"
hoomd/hpmc/module_convex_polygon.cc:#include "IntegratorHPMCMonoGPU.h"
hoomd/hpmc/module_convex_polygon.cc:#include "UpdaterClustersGPU.h"
hoomd/hpmc/module_convex_polygon.cc:    export_IntegratorHPMCMonoGPU<ShapeConvexPolygon>(m, "IntegratorHPMCMonoConvexPolygonGPU");
hoomd/hpmc/module_convex_polygon.cc:    export_ComputeFreeVolumeGPU<ShapeConvexPolygon>(m, "ComputeFreeVolumeConvexPolygonGPU");
hoomd/hpmc/module_convex_polygon.cc:    export_UpdaterClustersGPU<ShapeConvexPolygon>(m, "UpdaterClustersConvexPolygonGPU");
hoomd/hpmc/IntegratorHPMCMonoGPUJIT.inc:#include "hoomd/hpmc/EvaluatorUnionGPU.cuh"
hoomd/hpmc/IntegratorHPMCMonoGPUJIT.inc:#include "GPUHelpers.cuh"
hoomd/hpmc/IntegratorHPMCMonoGPUJIT.inc:namespace gpu
hoomd/hpmc/IntegratorHPMCMonoGPUJIT.inc:    } // end namespace gpu
hoomd/hpmc/PatchEnergyJITUnionGPU.cc:#include "PatchEnergyJITUnionGPU.h"
hoomd/hpmc/PatchEnergyJITUnionGPU.cc:#include "hoomd/hpmc/EvaluatorUnionGPU.cuh"
hoomd/hpmc/PatchEnergyJITUnionGPU.cc:void PatchEnergyJITUnionGPU::computePatchEnergyGPU(const gpu_args_t& args, hipStream_t hStream)
hoomd/hpmc/PatchEnergyJITUnionGPU.cc:    this->m_exec_conf->beginMultiGPU();
hoomd/hpmc/PatchEnergyJITUnionGPU.cc:                   m_gpu_factory.getKernelMaxThreads(0, eval_threads, block_size)); // fixme GPU 0
hoomd/hpmc/PatchEnergyJITUnionGPU.cc:        = m_gpu_factory.getKernelSharedSize(0, eval_threads, block_size); // fixme GPU 0
hoomd/hpmc/PatchEnergyJITUnionGPU.cc:    auto& gpu_partition = args.gpu_partition;
hoomd/hpmc/PatchEnergyJITUnionGPU.cc:    for (int idev = gpu_partition.getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/hpmc/PatchEnergyJITUnionGPU.cc:        auto range = gpu_partition.getRangeAndSetGPU(idev);
hoomd/hpmc/PatchEnergyJITUnionGPU.cc:        auto launcher = m_gpu_factory.configureKernel(idev,
hoomd/hpmc/PatchEnergyJITUnionGPU.cc:        if (res != CUDA_SUCCESS)
hoomd/hpmc/PatchEnergyJITUnionGPU.cc:    if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/hpmc/PatchEnergyJITUnionGPU.cc:        CHECK_CUDA_ERROR();
hoomd/hpmc/PatchEnergyJITUnionGPU.cc:    m_exec_conf->endMultiGPU();
hoomd/hpmc/PatchEnergyJITUnionGPU.cc:void export_PatchEnergyJITUnionGPU(pybind11::module& m)
hoomd/hpmc/PatchEnergyJITUnionGPU.cc:    pybind11::class_<PatchEnergyJITUnionGPU,
hoomd/hpmc/PatchEnergyJITUnionGPU.cc:                     std::shared_ptr<PatchEnergyJITUnionGPU>>(m, "PatchEnergyJITUnionGPU")
hoomd/hpmc/IntegratorHPMCMonoGPUDepletants.cu:#include "IntegratorHPMCMonoGPUDepletants.cuh"
hoomd/hpmc/IntegratorHPMCMonoGPUDepletants.cu:#include "IntegratorHPMCMonoGPUDepletantsTypes.cuh"
hoomd/hpmc/IntegratorHPMCMonoGPUDepletants.cu:#include "hoomd/GPUPartition.cuh"
hoomd/hpmc/IntegratorHPMCMonoGPUDepletants.cu:namespace gpu
hoomd/hpmc/IntegratorHPMCMonoGPUDepletants.cu:__global__ void hpmc_reduce_counters(const unsigned int ngpu,
hoomd/hpmc/IntegratorHPMCMonoGPUDepletants.cu:    for (unsigned int igpu = 0; igpu < ngpu; ++igpu)
hoomd/hpmc/IntegratorHPMCMonoGPUDepletants.cu:        *d_counters = *d_counters + d_per_device_counters[igpu * pitch];
hoomd/hpmc/IntegratorHPMCMonoGPUDepletants.cu:                  + d_per_device_implicit_counters[itype + igpu * implicit_pitch];
hoomd/hpmc/IntegratorHPMCMonoGPUDepletants.cu:                        const GPUPartition& gpu_partition,
hoomd/hpmc/IntegratorHPMCMonoGPUDepletants.cu:    for (int idev = gpu_partition.getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/hpmc/IntegratorHPMCMonoGPUDepletants.cu:        auto range = gpu_partition.getRangeAndSetGPU(idev);
hoomd/hpmc/IntegratorHPMCMonoGPUDepletants.cu:                               const GPUPartition& gpu_partition,
hoomd/hpmc/IntegratorHPMCMonoGPUDepletants.cu:    for (int idev = gpu_partition.getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/hpmc/IntegratorHPMCMonoGPUDepletants.cu:        auto range = gpu_partition.getRangeAndSetGPU(idev);
hoomd/hpmc/IntegratorHPMCMonoGPUDepletants.cu:        if (idev == (int)gpu_partition.getNumActiveGPUs() - 1 && add_ghosts)
hoomd/hpmc/IntegratorHPMCMonoGPUDepletants.cu:                       const GPUPartition& gpu_partition,
hoomd/hpmc/IntegratorHPMCMonoGPUDepletants.cu:    for (int idev = gpu_partition.getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/hpmc/IntegratorHPMCMonoGPUDepletants.cu:        auto range = gpu_partition.getRangeAndSetGPU(idev);
hoomd/hpmc/IntegratorHPMCMonoGPUDepletants.cu:        max_n_depletants[idev] = thrust::reduce(thrust::cuda::par(alloc).on(streams[idev]),
hoomd/hpmc/IntegratorHPMCMonoGPUDepletants.cu:                              const GPUPartition& gpu_partition,
hoomd/hpmc/IntegratorHPMCMonoGPUDepletants.cu:    for (int idev = gpu_partition.getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/hpmc/IntegratorHPMCMonoGPUDepletants.cu:        auto range = gpu_partition.getRangeAndSetGPU(idev);
hoomd/hpmc/IntegratorHPMCMonoGPUDepletants.cu:        if (idev == (int)gpu_partition.getNumActiveGPUs() - 1 && add_ghosts)
hoomd/hpmc/IntegratorHPMCMonoGPUDepletants.cu:        max_n_depletants[idev] = thrust::reduce(thrust::cuda::par(alloc).on(streams[idev]),
hoomd/hpmc/IntegratorHPMCMonoGPUDepletants.cu:reduce_counters(const unsigned int ngpu,
hoomd/hpmc/IntegratorHPMCMonoGPUDepletants.cu:                       ngpu,
hoomd/hpmc/IntegratorHPMCMonoGPUDepletants.cu:                       const GPUPartition& gpu_partition,
hoomd/hpmc/IntegratorHPMCMonoGPUDepletants.cu:    for (int idev = gpu_partition.getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/hpmc/IntegratorHPMCMonoGPUDepletants.cu:        auto range = gpu_partition.getRangeAndSetGPU(idev);
hoomd/hpmc/IntegratorHPMCMonoGPUDepletants.cu:    } // end namespace gpu
hoomd/hpmc/module_convex_spheropolyhedron.cc:#include "ComputeFreeVolumeGPU.h"
hoomd/hpmc/module_convex_spheropolyhedron.cc:#include "IntegratorHPMCMonoGPU.h"
hoomd/hpmc/module_convex_spheropolyhedron.cc:#include "UpdaterClustersGPU.h"
hoomd/hpmc/module_convex_spheropolyhedron.cc:    export_IntegratorHPMCMonoGPU<ShapeSpheropolyhedron>(m, "IntegratorHPMCMonoSpheropolyhedronGPU");
hoomd/hpmc/module_convex_spheropolyhedron.cc:    export_ComputeFreeVolumeGPU<ShapeSpheropolyhedron>(m, "ComputeFreeVolumeSpheropolyhedronGPU");
hoomd/hpmc/module_convex_spheropolyhedron.cc:    export_UpdaterClustersGPU<ShapeSpheropolyhedron>(m, "UpdaterClustersConvexSpheropolyhedronGPU");
hoomd/hpmc/module.cc:#include "GPUTree.h"
hoomd/hpmc/module.cc:#include "IntegratorHPMCMonoGPU.h"
hoomd/hpmc/module.cc:    HPMC GPU kernels
hoomd/hpmc/IntegratorHPMCMonoGPU.cu:#include "IntegratorHPMCMonoGPUTypes.cuh"
hoomd/hpmc/IntegratorHPMCMonoGPU.cu:#include "hoomd/GPUPartition.cuh"
hoomd/hpmc/IntegratorHPMCMonoGPU.cu:namespace gpu
hoomd/hpmc/IntegratorHPMCMonoGPU.cu:    \param ngpu Number of active devices
hoomd/hpmc/IntegratorHPMCMonoGPU.cu:    gpu_hpmc_excell_kernel executes one thread per cell. It gathers the particle indices from all
hoomd/hpmc/IntegratorHPMCMonoGPU.cu:                            const unsigned int ngpu)
hoomd/hpmc/IntegratorHPMCMonoGPU.cu:        for (unsigned int igpu = 0; igpu < ngpu; ++igpu)
hoomd/hpmc/IntegratorHPMCMonoGPU.cu:            unsigned int neigh_cell_size = d_cell_size[neigh_cell + igpu * ci.getNumElements()];
hoomd/hpmc/IntegratorHPMCMonoGPU.cu:                unsigned int new_idx = d_cell_idx[cli(k, neigh_cell) + igpu * cli.getNumElements()];
hoomd/hpmc/IntegratorHPMCMonoGPU.cu:                                                        const unsigned int ngpu,
hoomd/hpmc/IntegratorHPMCMonoGPU.cu:                       ngpu);
hoomd/hpmc/IntegratorHPMCMonoGPU.cu:    // after this kernel we return control of cuda managed memory to the host
hoomd/hpmc/IntegratorHPMCMonoGPU.cu:                       const GPUPartition& gpu_partition,
hoomd/hpmc/IntegratorHPMCMonoGPU.cu:    for (int idev = gpu_partition.getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/hpmc/IntegratorHPMCMonoGPU.cu:        auto range = gpu_partition.getRangeAndSetGPU(idev);
hoomd/hpmc/IntegratorHPMCMonoGPU.cu:    } // end namespace gpu
hoomd/hpmc/kernel_gen_moves.cu.inc:#include "IntegratorHPMCMonoGPUMoves.cuh"
hoomd/hpmc/kernel_gen_moves.cu.inc:namespace gpu
hoomd/hpmc/kernel_gen_moves.cu.inc:    } // namespace gpu
hoomd/hpmc/PatchEnergyJITGPU.h:#ifndef _PATCH_ENERGY_JIT_GPU_H_
hoomd/hpmc/PatchEnergyJITGPU.h:#define _PATCH_ENERGY_JIT_GPU_H_
hoomd/hpmc/PatchEnergyJITGPU.h:#include "GPUEvalFactory.h"
hoomd/hpmc/PatchEnergyJITGPU.h://! Evaluate patch energies via runtime generated code, GPU version
hoomd/hpmc/PatchEnergyJITGPU.h:class PYBIND11_EXPORT PatchEnergyJITGPU : public PatchEnergyJIT
hoomd/hpmc/PatchEnergyJITGPU.h:    PatchEnergyJITGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/hpmc/PatchEnergyJITGPU.h:                      const std::string& gpu_code,
hoomd/hpmc/PatchEnergyJITGPU.h:                      const std::string& cuda_devrt_library_path,
hoomd/hpmc/PatchEnergyJITGPU.h:          m_gpu_factory(exec_conf,
hoomd/hpmc/PatchEnergyJITGPU.h:                        gpu_code,
hoomd/hpmc/PatchEnergyJITGPU.h:                        cuda_devrt_library_path,
hoomd/hpmc/PatchEnergyJITGPU.h:        m_gpu_factory.setAlphaPtr(&m_param_array.front(), this->m_is_union);
hoomd/hpmc/PatchEnergyJITGPU.h:        auto& launch_bounds = m_gpu_factory.getLaunchBounds();
hoomd/hpmc/PatchEnergyJITGPU.h:    virtual void computePatchEnergyGPU(const gpu_args_t& args, hipStream_t hStream);
hoomd/hpmc/PatchEnergyJITGPU.h:    GPUEvalFactory m_gpu_factory; //!< JIT implementation
hoomd/hpmc/PatchEnergyJITGPU.h:inline void export_PatchEnergyJITGPU(pybind11::module& m)
hoomd/hpmc/PatchEnergyJITGPU.h:    pybind11::class_<PatchEnergyJITGPU, PatchEnergyJIT, std::shared_ptr<PatchEnergyJITGPU>>(
hoomd/hpmc/PatchEnergyJITGPU.h:        "PatchEnergyJITGPU")
hoomd/hpmc/PatchEnergyJITGPU.h:#endif // _PATCH_ENERGY_JIT_GPU_H_
hoomd/hpmc/module_union_sphere.cc:#include "ComputeFreeVolumeGPU.h"
hoomd/hpmc/module_union_sphere.cc:#include "IntegratorHPMCMonoGPU.h"
hoomd/hpmc/module_union_sphere.cc:#include "UpdaterClustersGPU.h"
hoomd/hpmc/module_union_sphere.cc:    export_IntegratorHPMCMonoGPU<ShapeUnion<ShapeSphere>>(m, "IntegratorHPMCMonoSphereUnionGPU");
hoomd/hpmc/module_union_sphere.cc:    export_ComputeFreeVolumeGPU<ShapeUnion<ShapeSphere>>(m, "ComputeFreeVolumeSphereUnionGPU");
hoomd/hpmc/module_union_sphere.cc:    export_UpdaterClustersGPU<ShapeUnion<ShapeSphere>>(m, "UpdaterClustersSphereUnionGPU");
hoomd/hpmc/IntegratorHPMC.h:#include "hoomd/GPUPartition.cuh"
hoomd/hpmc/IntegratorHPMC.h:                      const GPUPartition& _gpu_partition)
hoomd/hpmc/IntegratorHPMC.h:          d_reject_out_of_cell(_d_reject_out_of_cell), gpu_partition(_gpu_partition)
hoomd/hpmc/IntegratorHPMC.h:    const GPUPartition& gpu_partition; //!< split particles among GPUs
hoomd/hpmc/IntegratorHPMC.h:    typedef detail::hpmc_patch_args_t gpu_args_t;
hoomd/hpmc/IntegratorHPMC.h:    virtual void computePatchEnergyGPU(const gpu_args_t& args, hipStream_t hStream)
hoomd/hpmc/IntegratorHPMC.h:    const GPUArray<Scalar>& getDArray() const
hoomd/hpmc/IntegratorHPMC.h:    const GPUArray<Scalar>& getAArray() const
hoomd/hpmc/IntegratorHPMC.h:    GPUVector<Scalar> m_d; //!< Maximum move displacement by type
hoomd/hpmc/IntegratorHPMC.h:    GPUVector<Scalar> m_a; //!< Maximum angular displacement by type
hoomd/hpmc/UpdaterMuVT.h:    GPUVector<Scalar4> m_postype_backup; //!< Backup of postype array
hoomd/hpmc/UpdaterMuVT.h:    GPUVector<Scalar4> m_pos_backup;         //!< Backup of particle positions for volume move
hoomd/hpmc/UpdaterMuVT.h:    GPUVector<Scalar4> m_orientation_backup; //!< Backup of particle orientations for volume move
hoomd/hpmc/UpdaterMuVT.h:    GPUVector<Scalar> m_charge_backup;       //!< Backup of particle charges for volume move
hoomd/hpmc/UpdaterMuVT.h:    GPUVector<Scalar> m_diameter_backup;     //!< Backup of particle diameters for volume move
hoomd/hpmc/UpdaterMuVT.h:        GPUVector<Scalar4> postype_backup(m_exec_conf);
hoomd/hpmc/GPUTree.h:#ifndef __GPU_TREE_H__
hoomd/hpmc/GPUTree.h:#define __GPU_TREE_H__
hoomd/hpmc/GPUTree.h://! Adapter class to AABTree for query on the GPU
hoomd/hpmc/GPUTree.h:class GPUTree
hoomd/hpmc/GPUTree.h:    HOSTDEVICE GPUTree() : m_num_nodes(0), m_num_leaves(0), m_leaf_capacity(0) { }
hoomd/hpmc/GPUTree.h:     *  \param managed True if we use CUDA managed memory
hoomd/hpmc/GPUTree.h:    GPUTree(const OBBTree& tree, bool managed = false)
hoomd/hpmc/GPUTree.h:    //! Set CUDA memory hints
hoomd/hpmc/GPUTree.h:// from: A Binary Stack Tandem Traversal and an Ancestor Counter Data Structure for GPU friendly
hoomd/hpmc/GPUTree.h:DEVICE inline bool traverseBinaryStack(const GPUTree& a,
hoomd/hpmc/GPUTree.h:                                       const GPUTree& b,
hoomd/hpmc/GPUTree.h:DEVICE inline bool traverseBinaryStackIntersection(const GPUTree& a,
hoomd/hpmc/GPUTree.h:                                                   const GPUTree& b,
hoomd/hpmc/GPUTree.h:#endif // __GPU_TREE_H__
hoomd/hpmc/UpdaterQuickCompress.cc:    GPUArray<Scalar4>(MaxN, m_exec_conf).swap(m_pos_backup);
hoomd/hpmc/IntegratorHPMC.cc:    GPUVector<Scalar> d(this->m_pdata->getNTypes(), this->m_exec_conf);
hoomd/hpmc/IntegratorHPMC.cc:    GPUVector<Scalar> a(this->m_pdata->getNTypes(), this->m_exec_conf);
hoomd/hpmc/module-jit.cc:#include "PatchEnergyJITGPU.h"
hoomd/hpmc/module-jit.cc:#include "PatchEnergyJITUnionGPU.h"
hoomd/hpmc/module-jit.cc:    export_PatchEnergyJITGPU(m);
hoomd/hpmc/module-jit.cc:    export_PatchEnergyJITUnionGPU(m);
hoomd/hpmc/kernel_insert_depletants.cu.inc:#include "IntegratorHPMCMonoGPUDepletants.cuh"
hoomd/hpmc/kernel_insert_depletants.cu.inc:namespace gpu
hoomd/hpmc/kernel_insert_depletants.cu.inc:    } // namespace gpu
hoomd/hpmc/kernel_cluster_overlaps.cu.inc:#include "UpdaterClustersGPU.cuh"
hoomd/hpmc/kernel_cluster_overlaps.cu.inc:namespace gpu
hoomd/hpmc/kernel_cluster_overlaps.cu.inc:    } // namespace gpu
hoomd/hpmc/ComputeFreeVolume.h:    GPUArray<unsigned int> m_n_overlap_all; //!< Number of overlap volume particles in box
hoomd/hpmc/ComputeFreeVolume.h:    GPUArray<unsigned int> n_overlap_all(1, this->m_exec_conf);
hoomd/hpmc/UpdaterClustersGPU.h:#ifndef _UPDATER_HPMC_CLUSTERS_GPU_
hoomd/hpmc/UpdaterClustersGPU.h:#define _UPDATER_HPMC_CLUSTERS_GPU_
hoomd/hpmc/UpdaterClustersGPU.h:#include "IntegratorHPMCMonoGPUDepletants.cuh"
hoomd/hpmc/UpdaterClustersGPU.h:#include "UpdaterClustersGPU.cuh"
hoomd/hpmc/UpdaterClustersGPU.h:#include "UpdaterClustersGPUDepletants.cuh"
hoomd/hpmc/UpdaterClustersGPU.h:   Implementation of UpdaterClusters on the GPU
hoomd/hpmc/UpdaterClustersGPU.h:template<class Shape> class UpdaterClustersGPU : public UpdaterClusters<Shape>
hoomd/hpmc/UpdaterClustersGPU.h:    UpdaterClustersGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/hpmc/UpdaterClustersGPU.h:    virtual ~UpdaterClustersGPU();
hoomd/hpmc/UpdaterClustersGPU.h:    GPUPartition m_old_gpu_partition;          //!< The partition in the old configuration
hoomd/hpmc/UpdaterClustersGPU.h:    //! Update GPU memory hints
hoomd/hpmc/UpdaterClustersGPU.h:    virtual void updateGPUAdvice();
hoomd/hpmc/UpdaterClustersGPU.h:UpdaterClustersGPU<Shape>::UpdaterClustersGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/hpmc/UpdaterClustersGPU.h:    this->m_exec_conf->msg->notice(5) << "Constructing UpdaterClustersGPU" << std::endl;
hoomd/hpmc/UpdaterClustersGPU.h:    // with multiple GPUs, request a cell list per device
hoomd/hpmc/UpdaterClustersGPU.h:    m_overlaps_streams.resize(this->m_exec_conf->getNumActiveGPUs());
hoomd/hpmc/UpdaterClustersGPU.h:    for (int idev = this->m_exec_conf->getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/hpmc/UpdaterClustersGPU.h:        hipSetDevice(this->m_exec_conf->getGPUIds()[idev]);
hoomd/hpmc/UpdaterClustersGPU.h:        m_depletant_streams[itype].resize(this->m_exec_conf->getNumActiveGPUs());
hoomd/hpmc/UpdaterClustersGPU.h:        for (int idev = this->m_exec_conf->getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/hpmc/UpdaterClustersGPU.h:            hipSetDevice(this->m_exec_conf->getGPUIds()[idev]);
hoomd/hpmc/UpdaterClustersGPU.h:template<class Shape> UpdaterClustersGPU<Shape>::~UpdaterClustersGPU()
hoomd/hpmc/UpdaterClustersGPU.h:    this->m_exec_conf->msg->notice(5) << "Destroying UpdaterClustersGPU" << std::endl;
hoomd/hpmc/UpdaterClustersGPU.h:        for (int idev = this->m_exec_conf->getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/hpmc/UpdaterClustersGPU.h:            hipSetDevice(this->m_exec_conf->getGPUIds()[idev]);
hoomd/hpmc/UpdaterClustersGPU.h:    for (int idev = this->m_exec_conf->getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/hpmc/UpdaterClustersGPU.h:        hipSetDevice(this->m_exec_conf->getGPUIds()[idev]);
hoomd/hpmc/UpdaterClustersGPU.h:template<class Shape> void UpdaterClustersGPU<Shape>::update(uint64_t timestep)
hoomd/hpmc/UpdaterClustersGPU.h:    m_old_gpu_partition = this->m_pdata->getGPUPartition();
hoomd/hpmc/UpdaterClustersGPU.h:template<class Shape> void UpdaterClustersGPU<Shape>::connectedComponents()
hoomd/hpmc/UpdaterClustersGPU.h:    this->m_exec_conf->beginMultiGPU();
hoomd/hpmc/UpdaterClustersGPU.h:    gpu::get_num_neighbors(d_nneigh.data,
hoomd/hpmc/UpdaterClustersGPU.h:                           this->m_pdata->getGPUPartition(),
hoomd/hpmc/UpdaterClustersGPU.h:    if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/hpmc/UpdaterClustersGPU.h:        CHECK_CUDA_ERROR();
hoomd/hpmc/UpdaterClustersGPU.h:    this->m_exec_conf->endMultiGPU();
hoomd/hpmc/UpdaterClustersGPU.h:    this->m_exec_conf->beginMultiGPU();
hoomd/hpmc/UpdaterClustersGPU.h:    gpu::concatenate_adjacency_list(d_adjacency.data,
hoomd/hpmc/UpdaterClustersGPU.h:                                    this->m_pdata->getGPUPartition(),
hoomd/hpmc/UpdaterClustersGPU.h:    if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/hpmc/UpdaterClustersGPU.h:        CHECK_CUDA_ERROR();
hoomd/hpmc/UpdaterClustersGPU.h:    this->m_exec_conf->endMultiGPU();
hoomd/hpmc/UpdaterClustersGPU.h:        gpu::connected_components(d_adjacency_copy.data,
hoomd/hpmc/UpdaterClustersGPU.h:        if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/hpmc/UpdaterClustersGPU.h:            CHECK_CUDA_ERROR();
hoomd/hpmc/UpdaterClustersGPU.h:template<class Shape> void UpdaterClustersGPU<Shape>::initializeExcellMem()
hoomd/hpmc/UpdaterClustersGPU.h:        = this->m_cl->getPerDevice() ? this->m_exec_conf->getNumActiveGPUs() : 1;
hoomd/hpmc/UpdaterClustersGPU.h:    && 0 // excell is currently not multi-GPU optimized, let the CUDA driver figure this out
hoomd/hpmc/UpdaterClustersGPU.h:        auto gpu_map = this->m_exec_conf->getGPUIds();
hoomd/hpmc/UpdaterClustersGPU.h:        for (unsigned int idev = 0; idev < this->m_exec_conf->getNumActiveGPUs(); ++idev)
hoomd/hpmc/UpdaterClustersGPU.h:            cudaMemAdvise(m_excell_idx.get(),
hoomd/hpmc/UpdaterClustersGPU.h:                          cudaMemAdviseSetAccessedBy,
hoomd/hpmc/UpdaterClustersGPU.h:                          gpu_map[idev]);
hoomd/hpmc/UpdaterClustersGPU.h:            cudaMemAdvise(m_excell_size.get(),
hoomd/hpmc/UpdaterClustersGPU.h:                          cudaMemAdviseSetAccessedBy,
hoomd/hpmc/UpdaterClustersGPU.h:                          gpu_map[idev]);
hoomd/hpmc/UpdaterClustersGPU.h:            CHECK_CUDA_ERROR();
hoomd/hpmc/UpdaterClustersGPU.h:template<class Shape> void UpdaterClustersGPU<Shape>::backupState()
hoomd/hpmc/UpdaterClustersGPU.h:        this->m_exec_conf->beginMultiGPU();
hoomd/hpmc/UpdaterClustersGPU.h:        for (int idev = this->m_exec_conf->getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/hpmc/UpdaterClustersGPU.h:            hipSetDevice(this->m_exec_conf->getGPUIds()[idev]);
hoomd/hpmc/UpdaterClustersGPU.h:            auto range = this->m_pdata->getGPUPartition().getRange(idev);
hoomd/hpmc/UpdaterClustersGPU.h:            if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/hpmc/UpdaterClustersGPU.h:                CHECK_CUDA_ERROR();
hoomd/hpmc/UpdaterClustersGPU.h:        this->m_exec_conf->endMultiGPU();
hoomd/hpmc/UpdaterClustersGPU.h:void UpdaterClustersGPU<Shape>::transform(const quat<Scalar>& q,
hoomd/hpmc/UpdaterClustersGPU.h:    gpu::clusters_transform_args_t args(d_postype.data,
hoomd/hpmc/UpdaterClustersGPU.h:                                        this->m_pdata->getGPUPartition(),
hoomd/hpmc/UpdaterClustersGPU.h:    this->m_exec_conf->beginMultiGPU();
hoomd/hpmc/UpdaterClustersGPU.h:    gpu::transform_particles<Shape>(args, params.data());
hoomd/hpmc/UpdaterClustersGPU.h:    if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/hpmc/UpdaterClustersGPU.h:        CHECK_CUDA_ERROR();
hoomd/hpmc/UpdaterClustersGPU.h:    this->m_exec_conf->endMultiGPU();
hoomd/hpmc/UpdaterClustersGPU.h:template<class Shape> void UpdaterClustersGPU<Shape>::flip(uint64_t timestep)
hoomd/hpmc/UpdaterClustersGPU.h:    this->m_exec_conf->beginMultiGPU();
hoomd/hpmc/UpdaterClustersGPU.h:    gpu::flip_clusters(d_postype.data,
hoomd/hpmc/UpdaterClustersGPU.h:                       this->m_pdata->getGPUPartition(),
hoomd/hpmc/UpdaterClustersGPU.h:    if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/hpmc/UpdaterClustersGPU.h:        CHECK_CUDA_ERROR();
hoomd/hpmc/UpdaterClustersGPU.h:    this->m_exec_conf->endMultiGPU();
hoomd/hpmc/UpdaterClustersGPU.h:void UpdaterClustersGPU<Shape>::findInteractions(uint64_t timestep,
hoomd/hpmc/UpdaterClustersGPU.h:            << "Simulation box too small for GPU accelerated HPMC execution - increase it so "
hoomd/hpmc/UpdaterClustersGPU.h:    gpu::hpmc_excell(d_excell_idx.data,
hoomd/hpmc/UpdaterClustersGPU.h:                     this->m_exec_conf->getNumActiveGPUs(),
hoomd/hpmc/UpdaterClustersGPU.h:    if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/hpmc/UpdaterClustersGPU.h:        CHECK_CUDA_ERROR();
hoomd/hpmc/UpdaterClustersGPU.h:        updateGPUAdvice();
hoomd/hpmc/UpdaterClustersGPU.h:            // fill the parameter structure for the GPU kernel
hoomd/hpmc/UpdaterClustersGPU.h:            gpu::cluster_args_t args(d_postype_backup.data,
hoomd/hpmc/UpdaterClustersGPU.h:                                     m_old_gpu_partition,
hoomd/hpmc/UpdaterClustersGPU.h:            this->m_exec_conf->beginMultiGPU();
hoomd/hpmc/UpdaterClustersGPU.h:            for (int idev = this->m_exec_conf->getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/hpmc/UpdaterClustersGPU.h:                hipSetDevice(this->m_exec_conf->getGPUIds()[idev]);
hoomd/hpmc/UpdaterClustersGPU.h:                auto range = this->m_pdata->getGPUPartition().getRange(idev);
hoomd/hpmc/UpdaterClustersGPU.h:                if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/hpmc/UpdaterClustersGPU.h:                    CHECK_CUDA_ERROR();
hoomd/hpmc/UpdaterClustersGPU.h:            gpu::hpmc_cluster_overlaps<Shape>(args, params.data());
hoomd/hpmc/UpdaterClustersGPU.h:            if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/hpmc/UpdaterClustersGPU.h:                CHECK_CUDA_ERROR();
hoomd/hpmc/UpdaterClustersGPU.h:            // allow concurrency between depletant types in multi GPU block
hoomd/hpmc/UpdaterClustersGPU.h:                        "Negative fugacities are not supported by UpdaterClustersGPU.");
hoomd/hpmc/UpdaterClustersGPU.h:                gpu::generate_num_depletants(this->m_sysdef->getSeed(),
hoomd/hpmc/UpdaterClustersGPU.h:                                             m_old_gpu_partition,
hoomd/hpmc/UpdaterClustersGPU.h:                if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/hpmc/UpdaterClustersGPU.h:                    CHECK_CUDA_ERROR();
hoomd/hpmc/UpdaterClustersGPU.h:                unsigned int max_n_depletants[this->m_exec_conf->getNumActiveGPUs()];
hoomd/hpmc/UpdaterClustersGPU.h:                gpu::get_max_num_depletants(d_n_depletants.data + itype * this->m_pdata->getMaxN(),
hoomd/hpmc/UpdaterClustersGPU.h:                                            m_old_gpu_partition,
hoomd/hpmc/UpdaterClustersGPU.h:                if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/hpmc/UpdaterClustersGPU.h:                    CHECK_CUDA_ERROR();
hoomd/hpmc/UpdaterClustersGPU.h:                gpu::hpmc_implicit_args_t implicit_args(itype,
hoomd/hpmc/UpdaterClustersGPU.h:                gpu::hpmc_clusters_depletants<Shape>(args, implicit_args, params.data());
hoomd/hpmc/UpdaterClustersGPU.h:                if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/hpmc/UpdaterClustersGPU.h:                    CHECK_CUDA_ERROR();
hoomd/hpmc/UpdaterClustersGPU.h:                this->m_exec_conf->endMultiGPU();
hoomd/hpmc/UpdaterClustersGPU.h:template<class Shape> bool UpdaterClustersGPU<Shape>::checkReallocate()
hoomd/hpmc/UpdaterClustersGPU.h:            auto gpu_map = this->m_exec_conf->getGPUIds();
hoomd/hpmc/UpdaterClustersGPU.h:            for (unsigned int idev = 0; idev < this->m_exec_conf->getNumActiveGPUs(); ++idev)
hoomd/hpmc/UpdaterClustersGPU.h:                auto range = this->m_pdata->getGPUPartition().getRange(idev);
hoomd/hpmc/UpdaterClustersGPU.h:                cudaMemAdvise(m_adjacency.get() + range.first * m_maxn,
hoomd/hpmc/UpdaterClustersGPU.h:                              cudaMemAdviseSetPreferredLocation,
hoomd/hpmc/UpdaterClustersGPU.h:                              gpu_map[idev]);
hoomd/hpmc/UpdaterClustersGPU.h:                cudaMemPrefetchAsync(m_adjacency.get() + range.first * m_maxn,
hoomd/hpmc/UpdaterClustersGPU.h:                                     gpu_map[idev]);
hoomd/hpmc/UpdaterClustersGPU.h:                CHECK_CUDA_ERROR();
hoomd/hpmc/UpdaterClustersGPU.h:template<class Shape> void UpdaterClustersGPU<Shape>::updateGPUAdvice()
hoomd/hpmc/UpdaterClustersGPU.h:        auto gpu_map = this->m_exec_conf->getGPUIds();
hoomd/hpmc/UpdaterClustersGPU.h:        for (unsigned int idev = 0; idev < this->m_exec_conf->getNumActiveGPUs(); ++idev)
hoomd/hpmc/UpdaterClustersGPU.h:            auto range = m_old_gpu_partition.getRange(idev);
hoomd/hpmc/UpdaterClustersGPU.h:            cudaMemAdvise(this->m_postype_backup.get() + range.first,
hoomd/hpmc/UpdaterClustersGPU.h:                          cudaMemAdviseSetPreferredLocation,
hoomd/hpmc/UpdaterClustersGPU.h:                          gpu_map[idev]);
hoomd/hpmc/UpdaterClustersGPU.h:            cudaMemPrefetchAsync(this->m_postype_backup.get() + range.first,
hoomd/hpmc/UpdaterClustersGPU.h:                                 gpu_map[idev]);
hoomd/hpmc/UpdaterClustersGPU.h:            cudaMemAdvise(this->m_orientation_backup.get() + range.first,
hoomd/hpmc/UpdaterClustersGPU.h:                          cudaMemAdviseSetPreferredLocation,
hoomd/hpmc/UpdaterClustersGPU.h:                          gpu_map[idev]);
hoomd/hpmc/UpdaterClustersGPU.h:            cudaMemPrefetchAsync(this->m_orientation_backup.get() + range.first,
hoomd/hpmc/UpdaterClustersGPU.h:                                 gpu_map[idev]);
hoomd/hpmc/UpdaterClustersGPU.h:            cudaMemAdvise(this->m_nneigh.get() + range.first,
hoomd/hpmc/UpdaterClustersGPU.h:                          cudaMemAdviseSetPreferredLocation,
hoomd/hpmc/UpdaterClustersGPU.h:                          gpu_map[idev]);
hoomd/hpmc/UpdaterClustersGPU.h:            cudaMemPrefetchAsync(this->m_nneigh.get() + range.first,
hoomd/hpmc/UpdaterClustersGPU.h:                                 gpu_map[idev]);
hoomd/hpmc/UpdaterClustersGPU.h:            cudaMemAdvise(this->m_nneigh_scan.get() + range.first,
hoomd/hpmc/UpdaterClustersGPU.h:                          cudaMemAdviseSetPreferredLocation,
hoomd/hpmc/UpdaterClustersGPU.h:                          gpu_map[idev]);
hoomd/hpmc/UpdaterClustersGPU.h:            cudaMemPrefetchAsync(this->m_nneigh_scan.get() + range.first,
hoomd/hpmc/UpdaterClustersGPU.h:                                 gpu_map[idev]);
hoomd/hpmc/UpdaterClustersGPU.h:template<class Shape> void export_UpdaterClustersGPU(pybind11::module& m, const std::string& name)
hoomd/hpmc/UpdaterClustersGPU.h:    pybind11::class_<UpdaterClustersGPU<Shape>,
hoomd/hpmc/UpdaterClustersGPU.h:                     std::shared_ptr<UpdaterClustersGPU<Shape>>>(m, name.c_str())
hoomd/hpmc/UpdaterClustersGPU.h:#endif // ENABLE_CUDA
hoomd/hpmc/UpdaterClustersGPU.h:#endif // _UPDATER_HPMC_CLUSTERS_GPU_
hoomd/hpmc/ComputeFreeVolumeGPU.h:#ifndef __COMPUTE_FREE_VOLUME_GPU_H__
hoomd/hpmc/ComputeFreeVolumeGPU.h:#define __COMPUTE_FREE_VOLUME_GPU_H__
hoomd/hpmc/ComputeFreeVolumeGPU.h:#include "ComputeFreeVolumeGPU.cuh"
hoomd/hpmc/ComputeFreeVolumeGPU.h:#include "IntegratorHPMCMonoGPU.cuh"
hoomd/hpmc/ComputeFreeVolumeGPU.h:/*! \file ComputeFreeVolumeGPU.h
hoomd/hpmc/ComputeFreeVolumeGPU.h:template<class Shape> class ComputeFreeVolumeGPU : public ComputeFreeVolume<Shape>
hoomd/hpmc/ComputeFreeVolumeGPU.h:    ComputeFreeVolumeGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/hpmc/ComputeFreeVolumeGPU.h:    virtual ~ComputeFreeVolumeGPU();
hoomd/hpmc/ComputeFreeVolumeGPU.h:    GPUArray<unsigned int> m_excell_idx;  //!< Particle indices in expanded cells
hoomd/hpmc/ComputeFreeVolumeGPU.h:    GPUArray<unsigned int> m_excell_size; //!< Number of particles in each expanded cell
hoomd/hpmc/ComputeFreeVolumeGPU.h:ComputeFreeVolumeGPU<Shape>::ComputeFreeVolumeGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/hpmc/ComputeFreeVolumeGPU.h:    GPUArray<unsigned int> excell_size(0, this->m_exec_conf);
hoomd/hpmc/ComputeFreeVolumeGPU.h:    GPUArray<unsigned int> excell_idx(0, this->m_exec_conf);
hoomd/hpmc/ComputeFreeVolumeGPU.h:template<class Shape> ComputeFreeVolumeGPU<Shape>::~ComputeFreeVolumeGPU() { }
hoomd/hpmc/ComputeFreeVolumeGPU.h:template<class Shape> void ComputeFreeVolumeGPU<Shape>::computeFreeVolume(uint64_t timestep)
hoomd/hpmc/ComputeFreeVolumeGPU.h:    gpu::hpmc_excell(d_excell_idx.data,
hoomd/hpmc/ComputeFreeVolumeGPU.h:                     this->m_cl->getPerDevice() ? this->m_exec_conf->getNumActiveGPUs() : 1,
hoomd/hpmc/ComputeFreeVolumeGPU.h:    if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/hpmc/ComputeFreeVolumeGPU.h:        CHECK_CUDA_ERROR();
hoomd/hpmc/ComputeFreeVolumeGPU.h:        detail::gpu_hpmc_free_volume<Shape>(free_volume_args, params.data());
hoomd/hpmc/ComputeFreeVolumeGPU.h:        if (this->m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/hpmc/ComputeFreeVolumeGPU.h:            CHECK_CUDA_ERROR();
hoomd/hpmc/ComputeFreeVolumeGPU.h:template<class Shape> void ComputeFreeVolumeGPU<Shape>::initializeExcellMem()
hoomd/hpmc/ComputeFreeVolumeGPU.h:template<class Shape> void export_ComputeFreeVolumeGPU(pybind11::module& m, const std::string& name)
hoomd/hpmc/ComputeFreeVolumeGPU.h:    pybind11::class_<ComputeFreeVolumeGPU<Shape>,
hoomd/hpmc/ComputeFreeVolumeGPU.h:                     std::shared_ptr<ComputeFreeVolumeGPU<Shape>>>(m, name.c_str())
hoomd/hpmc/ComputeFreeVolumeGPU.h:#endif // __COMPUTE_FREE_VOLUME_GPU_H__
hoomd/hpmc/module_union_faceted_ellipsoid.cc:#include "ComputeFreeVolumeGPU.h"
hoomd/hpmc/module_union_faceted_ellipsoid.cc:#include "IntegratorHPMCMonoGPU.h"
hoomd/hpmc/module_union_faceted_ellipsoid.cc:#include "UpdaterClustersGPU.h"
hoomd/hpmc/module_union_faceted_ellipsoid.cc:    export_IntegratorHPMCMonoGPU<ShapeUnion<ShapeFacetedEllipsoid>>(
hoomd/hpmc/module_union_faceted_ellipsoid.cc:        "IntegratorHPMCMonoFacetedEllipsoidUnionGPU");
hoomd/hpmc/module_union_faceted_ellipsoid.cc:    export_ComputeFreeVolumeGPU<ShapeUnion<ShapeFacetedEllipsoid>>(
hoomd/hpmc/module_union_faceted_ellipsoid.cc:        "ComputeFreeVolumeFacetedEllipsoidUnionGPU");
hoomd/hpmc/module_union_faceted_ellipsoid.cc:    export_UpdaterClustersGPU<ShapeUnion<ShapeFacetedEllipsoid>>(
hoomd/hpmc/module_union_faceted_ellipsoid.cc:        "UpdaterClustersFacetedEllipsoidUnionGPU");
hoomd/hpmc/module_polyhedron.cc:#include "ComputeFreeVolumeGPU.h"
hoomd/hpmc/module_polyhedron.cc:#include "IntegratorHPMCMonoGPU.h"
hoomd/hpmc/module_polyhedron.cc:#include "UpdaterClustersGPU.h"
hoomd/hpmc/module_polyhedron.cc:    export_IntegratorHPMCMonoGPU<ShapePolyhedron>(m, "IntegratorHPMCMonoPolyhedronGPU");
hoomd/hpmc/module_polyhedron.cc:    export_ComputeFreeVolumeGPU<ShapePolyhedron>(m, "ComputeFreeVolumePolyhedronGPU");
hoomd/hpmc/module_polyhedron.cc:    export_UpdaterClustersGPU<ShapePolyhedron>(m, "UpdaterClustersPolyhedronGPU");
hoomd/hpmc/UpdaterBoxMC.h:    GPUArray<Scalar4> m_pos_backup; //!< hold backup copy of particle positions
hoomd/hpmc/UpdaterQuickCompress.h:    GPUArray<Scalar4> m_pos_backup;
hoomd/hpmc/UpdaterBoxMC.cc:    GPUArray<Scalar4>(MaxN, m_exec_conf).swap(m_pos_backup);
hoomd/hpmc/HPMCMiscFunctions.h:// !helper to call CPU or GPU signbit
hoomd/hpmc/kernel_update_pdata.cu.inc:#include "IntegratorHPMCMonoGPUMoves.cuh"
hoomd/hpmc/kernel_update_pdata.cu.inc:namespace gpu
hoomd/hpmc/kernel_update_pdata.cu.inc:    } // namespace gpu
hoomd/hpmc/pair/pair.py:        if isinstance(device, hoomd.device.GPU):
hoomd/hpmc/pair/pair.py:            raise RuntimeError("Not implemented on the GPU")
hoomd/hpmc/pair/user.py:    def _wrap_gpu_code(self, code):
hoomd/hpmc/pair/user.py:                        #include "hoomd/hpmc/IntegratorHPMCMonoGPUJIT.inc"
hoomd/hpmc/pair/user.py:        if isinstance(device, hoomd.device.GPU):
hoomd/hpmc/pair/user.py:            gpu_settings = _compile.get_gpu_compilation_settings(device)
hoomd/hpmc/pair/user.py:            gpu_code = self._wrap_gpu_code(self.code)
hoomd/hpmc/pair/user.py:            self._cpp_obj = _jit.PatchEnergyJITGPU(
hoomd/hpmc/pair/user.py:                gpu_code,
hoomd/hpmc/pair/user.py:                "hpmc::gpu::kernel::hpmc_narrow_phase_patch",
hoomd/hpmc/pair/user.py:                gpu_settings["includes"],
hoomd/hpmc/pair/user.py:                gpu_settings["cuda_devrt_lib_path"],
hoomd/hpmc/pair/user.py:                gpu_settings["max_arch"],
hoomd/hpmc/pair/user.py:                Must be ``''`` when executing on a GPU.
hoomd/hpmc/pair/user.py:        GPU. A `RuntimeError` is raised on attachment if the value of this
hoomd/hpmc/pair/user.py:        argument is anything besides ``''`` on the GPU.
hoomd/hpmc/pair/user.py:        if isinstance(self._simulation.device, hoomd.device.GPU):
hoomd/hpmc/pair/user.py:                msg += 'GPU is unused.'
hoomd/hpmc/pair/user.py:        if isinstance(self._simulation.device, hoomd.device.GPU):
hoomd/hpmc/pair/user.py:            gpu_settings = _compile.get_gpu_compilation_settings(device)
hoomd/hpmc/pair/user.py:            gpu_code_constituent = self._wrap_gpu_code(self.code_constituent)
hoomd/hpmc/pair/user.py:            self._cpp_obj = _jit.PatchEnergyJITUnionGPU(
hoomd/hpmc/pair/user.py:                gpu_code_constituent,
hoomd/hpmc/pair/user.py:                "hpmc::gpu::kernel::hpmc_narrow_phase_patch",
hoomd/hpmc/pair/user.py:                gpu_settings["includes"] + ["-DUNION_EVAL"],
hoomd/hpmc/pair/user.py:                gpu_settings["cuda_devrt_lib_path"],
hoomd/hpmc/pair/user.py:                gpu_settings["max_arch"],
hoomd/hpmc/ShapeConvexPolyhedron.h:            // this code path also triggers on the GPU
hoomd/hpmc/PatchEnergyJITGPU.cc:#include "PatchEnergyJITGPU.h"
hoomd/hpmc/PatchEnergyJITGPU.cc:void PatchEnergyJITGPU::computePatchEnergyGPU(const gpu_args_t& args, hipStream_t hStream)
hoomd/hpmc/PatchEnergyJITGPU.cc:    this->m_exec_conf->beginMultiGPU();
hoomd/hpmc/PatchEnergyJITGPU.cc:                   m_gpu_factory.getKernelMaxThreads(0, eval_threads, block_size)); // fixme GPU 0
hoomd/hpmc/PatchEnergyJITGPU.cc:        = m_gpu_factory.getKernelSharedSize(0, eval_threads, block_size); // fixme GPU 0
hoomd/hpmc/PatchEnergyJITGPU.cc:    auto& gpu_partition = args.gpu_partition;
hoomd/hpmc/PatchEnergyJITGPU.cc:    for (int idev = gpu_partition.getNumActiveGPUs() - 1; idev >= 0; --idev)
hoomd/hpmc/PatchEnergyJITGPU.cc:        auto range = gpu_partition.getRangeAndSetGPU(idev);
hoomd/hpmc/PatchEnergyJITGPU.cc:        auto launcher = m_gpu_factory.configureKernel(idev,
hoomd/hpmc/PatchEnergyJITGPU.cc:        if (res != CUDA_SUCCESS)
hoomd/hpmc/PatchEnergyJITGPU.cc:    if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/hpmc/PatchEnergyJITGPU.cc:        CHECK_CUDA_ERROR();
hoomd/hpmc/PatchEnergyJITGPU.cc:    m_exec_conf->endMultiGPU();
hoomd/hpmc/ShapeUnion.h:#include "GPUTree.h"
hoomd/hpmc/ShapeUnion.h:    /// Set CUDA memory hints
hoomd/hpmc/ShapeUnion.h:        // build tree and store GPU accessible version in parameter structure
hoomd/hpmc/ShapeUnion.h:        tree = GPUTree(tree_obb, managed);
hoomd/hpmc/ShapeUnion.h:    GPUTree tree;
hoomd/hpmc/ShapeUnion.h:    const detail::GPUTree& tree_a = a.members.tree;
hoomd/hpmc/ShapeUnion.h:    const detail::GPUTree& tree_b = b.members.tree;
hoomd/hpmc/ShapeUnion.h:    const detail::GPUTree& tree_a = a.members.tree;
hoomd/hpmc/ShapeUnion.h:    const detail::GPUTree& tree_b = b.members.tree;
hoomd/hpmc/ShapeUnion.h:    const detail::GPUTree& tree_a = a.members.tree;
hoomd/hpmc/ShapeUnion.h:    const detail::GPUTree& tree_b = b.members.tree;
hoomd/hpmc/ShapeUnion.h:    const detail::GPUTree& tree_a = a.members.tree;
hoomd/hpmc/ShapeUnion.h:    const detail::GPUTree& tree_b = b.members.tree;
hoomd/hpmc/ShapeUnion.h:    const detail::GPUTree& tree_a = a.members.tree;
hoomd/hpmc/ShapeUnion.h:    const detail::GPUTree& tree_b = b.members.tree;
hoomd/hpmc/ShapeUnion.h:    const detail::GPUTree& tree_a = a.members.tree;
hoomd/hpmc/ShapeUnion.h:    const detail::GPUTree& tree_b = b.members.tree;
hoomd/hpmc/ShapeUnion.h:    const detail::GPUTree& tree_a = a.members.tree;
hoomd/hpmc/ShapeUnion.h:    const detail::GPUTree& tree_b = b.members.tree;
hoomd/hpmc/module_sphinx.cc:#include "ComputeFreeVolumeGPU.h"
hoomd/hpmc/module_sphinx.cc:#include "IntegratorHPMCMonoGPU.h"
hoomd/hpmc/module_sphinx.cc:#include "UpdaterClustersGPU.h"
hoomd/hpmc/module_sphinx.cc:#ifdef ENABLE_SPHINX_GPU
hoomd/hpmc/module_sphinx.cc:    export_IntegratorHPMCMonoGPU<ShapeSphinx>(m, "IntegratorHPMCMonoSphinxGPU");
hoomd/hpmc/module_sphinx.cc:    export_ComputeFreeVolumeGPU<ShapeSphinx>(m, "ComputeFreeVolumeSphinxGPU");
hoomd/hpmc/module_sphinx.cc:    export_UpdaterClustersGPU<ShapeSphinx>(m, "UpdaterClustersSphinxGPU");
hoomd/hpmc/kernel_cluster_transform.cu.inc:#include "UpdaterClustersGPU.cuh"
hoomd/hpmc/kernel_cluster_transform.cu.inc:namespace gpu
hoomd/hpmc/kernel_cluster_transform.cu.inc:    } // namespace gpu
hoomd/extern/ECL.cuh:ECL-CC code: ECL-CC is a connected components algorithm. The CUDA
hoomd/extern/ECL.cuh:struct GPUTimer
hoomd/extern/ECL.cuh:  GPUTimer() {hipEventCreate(&beg);  hipEventCreate(&end);}
hoomd/extern/ECL.cuh:  ~GPUTimer() {hipEventDestroy(beg);  hipEventDestroy(end);}
hoomd/extern/ECL.cuh:  if ((deviceProp.major == 9999) && (deviceProp.minor == 9999)) {fprintf(stderr, "ERROR: there is no CUDA capable device\n\n");  exit(-1);}
hoomd/extern/ECL.cuh:  printf("gpu: %s with %d SMs and %d mTpSM (%.1f MHz and %.1f MHz)\n", deviceProp.name, SMs, mTSM, deviceProp.clockRate * 0.001, deviceProp.memoryClockRate * 0.001);
hoomd/extern/ECL.cuh:  GPUTimer timer;
hoomd/extern/cudacpu_vector_types.h: * Copyright 1993-2012 NVIDIA Corporation.  All rights reserved.
hoomd/extern/cudacpu_vector_types.h: * subject to NVIDIA intellectual property rights under U.S. and
hoomd/extern/cudacpu_vector_types.h: * CONFIDENTIAL to NVIDIA and is being provided under the terms and
hoomd/extern/cudacpu_vector_types.h: * conditions of a form of NVIDIA software license agreement by and
hoomd/extern/cudacpu_vector_types.h: * between NVIDIA and Licensee ("License Agreement") or electronically
hoomd/extern/cudacpu_vector_types.h: * written consent of NVIDIA is prohibited.
hoomd/extern/cudacpu_vector_types.h: * LICENSE AGREEMENT, NVIDIA MAKES NO REPRESENTATION ABOUT THE
hoomd/extern/cudacpu_vector_types.h: * NVIDIA DISCLAIMS ALL WARRANTIES WITH REGARD TO THESE LICENSED
hoomd/extern/cudacpu_vector_types.h: * LICENSE AGREEMENT, IN NO EVENT SHALL NVIDIA BE LIABLE FOR ANY
hoomd/extern/cudacpu_vector_types.h:#if !defined(__CUDA_LIBDEVICE__)
hoomd/extern/cudacpu_vector_types.h:#endif /* !__CUDA_LIBDEVICE__ */
hoomd/extern/cudacpu_vector_types.h:#include "cudacpu_host_defines.h"
hoomd/extern/cudacpu_vector_types.h:#if !defined(__CUDACC__) && !defined(__CUDABE__) && \
hoomd/extern/cudacpu_vector_types.h:#define __cuda_builtin_vector_align8(tag, members) \
hoomd/extern/cudacpu_vector_types.h:#else /* !__CUDACC__ && !__CUDABE__ && _WIN32 && !_WIN64 */
hoomd/extern/cudacpu_vector_types.h:#define __cuda_builtin_vector_align8(tag, members) \
hoomd/extern/cudacpu_vector_types.h:#endif /* !__CUDACC__ && !__CUDABE__ && _WIN32 && !_WIN64 */
hoomd/extern/cudacpu_vector_types.h:__cuda_builtin_vector_align8(short4, short x; short y; short z; short w;);
hoomd/extern/cudacpu_vector_types.h:__cuda_builtin_vector_align8(ushort4, unsigned short x; unsigned short y; unsigned short z; unsigned short w;);
hoomd/extern/cudacpu_vector_types.h:__cuda_builtin_vector_align8(int2, int x; int y;);
hoomd/extern/cudacpu_vector_types.h:__cuda_builtin_vector_align8(uint2, unsigned int x; unsigned int y;);
hoomd/extern/cudacpu_vector_types.h:__cuda_builtin_vector_align8(long2, long int x; long int y;);
hoomd/extern/cudacpu_vector_types.h:__cuda_builtin_vector_align8(ulong2, unsigned long int x; unsigned long int y;);
hoomd/extern/cudacpu_vector_types.h:#if !defined(__CUDACC__) && !defined(__CUDABE__) && defined(__arm__) && \
hoomd/extern/cudacpu_vector_types.h:    float x; float y; float __cuda_gnu_arm_ice_workaround[0];
hoomd/extern/cudacpu_vector_types.h:#pragma GCC poison __cuda_gnu_arm_ice_workaround
hoomd/extern/cudacpu_vector_types.h:#else /* !__CUDACC__ && !__CUDABE__ && __arm__ && __ARM_PCS_VFP &&
hoomd/extern/cudacpu_vector_types.h:__cuda_builtin_vector_align8(float2, float x; float y;);
hoomd/extern/cudacpu_vector_types.h:#endif /* !__CUDACC__ && !__CUDABE__ && __arm__ && __ARM_PCS_VFP &&
hoomd/extern/cudacpu_vector_types.h:#if !defined(__CUDACC__) && !defined(__CUDABE__) && \
hoomd/extern/cudacpu_vector_types.h:#endif /* !__CUDACC__ && !__CUDABE__ && _WIN32 && !_WIN64 */
hoomd/extern/cudacpu_vector_types.h:#undef  __cuda_builtin_vector_align8
hoomd/extern/cudacpu_vector_functions.h: * Copyright 1993-2012 NVIDIA Corporation.  All rights reserved.
hoomd/extern/cudacpu_vector_functions.h: * subject to NVIDIA intellectual property rights under U.S. and
hoomd/extern/cudacpu_vector_functions.h: * CONFIDENTIAL to NVIDIA and is being provided under the terms and
hoomd/extern/cudacpu_vector_functions.h: * conditions of a form of NVIDIA software license agreement by and
hoomd/extern/cudacpu_vector_functions.h: * between NVIDIA and Licensee ("License Agreement") or electronically
hoomd/extern/cudacpu_vector_functions.h: * written consent of NVIDIA is prohibited.
hoomd/extern/cudacpu_vector_functions.h: * LICENSE AGREEMENT, NVIDIA MAKES NO REPRESENTATION ABOUT THE
hoomd/extern/cudacpu_vector_functions.h: * NVIDIA DISCLAIMS ALL WARRANTIES WITH REGARD TO THESE LICENSED
hoomd/extern/cudacpu_vector_functions.h: * LICENSE AGREEMENT, IN NO EVENT SHALL NVIDIA BE LIABLE FOR ANY
hoomd/extern/cudacpu_vector_functions.h:#include "cudacpu_host_defines.h"
hoomd/extern/cudacpu_vector_functions.h:#include "cudacpu_vector_types.h"
hoomd/extern/cudacpu_host_defines.h: * Copyright 1993-2012 NVIDIA Corporation.  All rights reserved.
hoomd/extern/cudacpu_host_defines.h: * subject to NVIDIA intellectual property rights under U.S. and
hoomd/extern/cudacpu_host_defines.h: * CONFIDENTIAL to NVIDIA and is being provided under the terms and
hoomd/extern/cudacpu_host_defines.h: * conditions of a form of NVIDIA software license agreement by and
hoomd/extern/cudacpu_host_defines.h: * between NVIDIA and Licensee ("License Agreement") or electronically
hoomd/extern/cudacpu_host_defines.h: * written consent of NVIDIA is prohibited.
hoomd/extern/cudacpu_host_defines.h: * LICENSE AGREEMENT, NVIDIA MAKES NO REPRESENTATION ABOUT THE
hoomd/extern/cudacpu_host_defines.h: * NVIDIA DISCLAIMS ALL WARRANTIES WITH REGARD TO THESE LICENSED
hoomd/extern/cudacpu_host_defines.h: * LICENSE AGREEMENT, IN NO EVENT SHALL NVIDIA BE LIABLE FOR ANY
hoomd/extern/cudacpu_host_defines.h:#if defined(__GNUC__) || defined(__CUDA_LIBDEVICE__)
hoomd/extern/cudacpu_host_defines.h:#if defined(__CUDACC__) || defined(__CUDA_ARCH__)
hoomd/extern/cudacpu_host_defines.h:   Consider a non-CUDA source file (e.g. .cpp) that has the
hoomd/extern/cudacpu_host_defines.h:   by a  CUDA compiler component.
hoomd/extern/cudacpu_host_defines.h:#endif /* __CUDACC__  || __CUDA_ARCH__ */
hoomd/extern/cudacpu_host_defines.h:#define CUDARTAPI
hoomd/extern/cudacpu_host_defines.h:#define CUDARTAPI \
hoomd/extern/cudacpu_host_defines.h:#else /* __GNUC__ || __CUDA_LIBDEVICE__ */
hoomd/extern/cudacpu_host_defines.h:#error --- !!! UNKNOWN COMPILER: please provide a CUDA compatible definition for '__align__' !!! ---
hoomd/extern/cudacpu_host_defines.h:#if !defined(CUDARTAPI)
hoomd/extern/cudacpu_host_defines.h:#error --- !!! UNKNOWN COMPILER: please provide a CUDA compatible definition for 'CUDARTAPI' !!! ---
hoomd/extern/cudacpu_host_defines.h:#endif /* !CUDARTAPI */
hoomd/extern/cudacpu_host_defines.h:#if !defined(__CUDACC__) && !defined(__CUDABE__)
hoomd/extern/cudacpu_host_defines.h:#else /* !__CUDACC__ && !__CUDABE__ */
hoomd/extern/cudacpu_host_defines.h:#endif /* !__CUDACC__ && !__CUDABE__ */
hoomd/extern/cudacpu_host_defines.h:#if defined(__CUDACC__) || defined(__CUDABE__) || \
hoomd/extern/cudacpu_host_defines.h:#else /* __CUDACC__ || __CUDABE__ || __GNUC__ || _WIN64 */
hoomd/extern/cudacpu_host_defines.h:#endif /* __CUDACC__ || __CUDABE__ || __GNUC__  || _WIN64 */
hoomd/extern/cudacpu_host_defines.h:#if defined(__CUDABE__) || !defined(__CUDACC__)
hoomd/extern/cudacpu_host_defines.h:#define __cudart_builtin__
hoomd/extern/cudacpu_host_defines.h:#else /* __CUDABE__  || !__CUDACC__ */
hoomd/extern/cudacpu_host_defines.h:#define __cudart_builtin__ \
hoomd/extern/cudacpu_host_defines.h:        __location__(cudart_builtin)
hoomd/extern/cudacpu_host_defines.h:#endif /* __CUDABE__ || !__CUDACC__ */
hoomd/extern/CMakeLists.txt:    target_compile_definitions(hipper INTERFACE HIPPER_CUDA)
hoomd/extern/dfftlib/CMakeLists.txt:#find_package(CUDA)
hoomd/extern/dfftlib/CMakeLists.txt:#if(CUDA_FOUND)
hoomd/extern/dfftlib/CMakeLists.txt:#    set(CUDA_NVCC_FLAGS "${CUDA_NVCC_FLAGS};-arch=sm_20")
hoomd/extern/dfftlib/CMakeLists.txt:#    option(ENABLE_HIP "CUDA support" ON)
hoomd/extern/dfftlib/CMakeLists.txt:#    option(ENABLE_HIP "CUDA support" OFF)
hoomd/extern/dfftlib/CMakeLists.txt:# include(CUDA_MPI.cmake)
hoomd/extern/dfftlib/CMakeLists.txt:    # CUFFT is default for CUDA
hoomd/extern/dfftlib/CMakeLists.txt:    set(CUDA_SOURCES ${CMAKE_CURRENT_SOURCE_DIR}/src/cufft_single_interface.cc)
hoomd/extern/dfftlib/CMakeLists.txt:    set(CUDA_SOURCES ${CMAKE_CURRENT_SOURCE_DIR}/src/dfft_cuda.cc ${CUDA_SOURCES})
hoomd/extern/dfftlib/CMakeLists.txt:    # cuda_compile(DFFT_CUDA_O ${CMAKE_CURRENT_SOURCE_DIR}/src/dfft_cuda.cu OPTIONS ${CUDA_ADDITIONAL_OPTIONS} SHARED)
hoomd/extern/dfftlib/CMakeLists.txt:    # set(CUDA_OBJECTS ${DFFT_CUDA_O})
hoomd/extern/dfftlib/CMakeLists.txt:    # set(CUDA_LIBS ${CUDA_CUDART_LIBRARY} ${CUDA_cufft_LIBRARY})
hoomd/extern/dfftlib/CMakeLists.txt:    set(DFFT_CU_SOURCES ${CMAKE_CURRENT_SOURCE_DIR}/src/dfft_cuda.cu PARENT_SCOPE)
hoomd/extern/dfftlib/CMakeLists.txt:set(DFFT_SOURCES ${CMAKE_CURRENT_SOURCE_DIR}/src/dfft_common.cc ${HOST_SOURCES} ${CUDA_SOURCES} PARENT_SCOPE)
hoomd/extern/dfftlib/README:implementations include host (MKL) and CUDA device (CUFFT) implementations.
hoomd/extern/dfftlib/README:supports "CUDA-aware" MPI-libraries (MVAPICH2 >= 1.8, OpenMPI 1.7).
hoomd/extern/dfftlib/README:Please see their documentation for how to enable CUDA-MPI communication.
hoomd/extern/dfftlib/README:- CUDA (tested with 5.0)
hoomd/extern/dfftlib/README:If no CUDA toolkit is available, only the host backend will be built.
hoomd/extern/dfftlib/README:look at the unit tests, test/unit_test_host.c and test/unit_test_cuda.c.
hoomd/extern/dfftlib/src/dfft_common.cc:#include "dfft_cuda.h"
hoomd/extern/dfftlib/src/dfft_common.cc:        #ifdef CUDA_FFT_SUPPORTS_MULTI
hoomd/extern/dfftlib/src/dfft_common.cc:        decompose_1d = ((plan->ndim > CUDA_FFT_MAX_N) ? 1 : 0);
hoomd/extern/dfftlib/src/dfft_common.cc:            plan->cuda_plans_multi_fw =
hoomd/extern/dfftlib/src/dfft_common.cc:                (cuda_plan_t **)malloc(sizeof(cuda_plan_t *)*plan->max_depth);
hoomd/extern/dfftlib/src/dfft_common.cc:            plan->cuda_plans_multi_bw =
hoomd/extern/dfftlib/src/dfft_common.cc:                (cuda_plan_t **)malloc(sizeof(cuda_plan_t *)*plan->max_depth);
hoomd/extern/dfftlib/src/dfft_common.cc:                plan->cuda_plans_multi_fw[d] =
hoomd/extern/dfftlib/src/dfft_common.cc:                    (cuda_plan_t *)malloc(sizeof(cuda_plan_t)*plan->ndim);
hoomd/extern/dfftlib/src/dfft_common.cc:                plan->cuda_plans_multi_bw[d] =
hoomd/extern/dfftlib/src/dfft_common.cc:                    (cuda_plan_t *)malloc(sizeof(cuda_plan_t)*plan->ndim);
hoomd/extern/dfftlib/src/dfft_common.cc:                        res = dfft_cuda_create_1d_plan(&plan->cuda_plans_multi_fw[d][i],
hoomd/extern/dfftlib/src/dfft_common.cc:                        res = dfft_cuda_create_1d_plan(&plan->cuda_plans_multi_bw[d][i],
hoomd/extern/dfftlib/src/dfft_common.cc:                plan->cuda_plans_multi_fw[d] =
hoomd/extern/dfftlib/src/dfft_common.cc:                    (cuda_plan_t *)malloc(sizeof(cuda_plan_t));
hoomd/extern/dfftlib/src/dfft_common.cc:                plan->cuda_plans_multi_bw[d] =
hoomd/extern/dfftlib/src/dfft_common.cc:                    (cuda_plan_t *)malloc(sizeof(cuda_plan_t));
hoomd/extern/dfftlib/src/dfft_common.cc:                res = dfft_cuda_create_nd_plan(&plan->cuda_plans_multi_fw[d][0],
hoomd/extern/dfftlib/src/dfft_common.cc:                res = dfft_cuda_create_nd_plan(&plan->cuda_plans_multi_bw[d][0],
hoomd/extern/dfftlib/src/dfft_common.cc:            plan->cuda_plans_final_fw =
hoomd/extern/dfftlib/src/dfft_common.cc:                (cuda_plan_t *)malloc(sizeof(cuda_plan_t)*plan->ndim);
hoomd/extern/dfftlib/src/dfft_common.cc:            plan->cuda_plans_final_bw =
hoomd/extern/dfftlib/src/dfft_common.cc:                (cuda_plan_t *)malloc(sizeof(cuda_plan_t)*plan->ndim);
hoomd/extern/dfftlib/src/dfft_common.cc:            res = dfft_cuda_create_1d_plan(&plan->cuda_plans_final_fw[i],
hoomd/extern/dfftlib/src/dfft_common.cc:            res = dfft_cuda_create_1d_plan(&plan->cuda_plans_final_bw[i],
hoomd/extern/dfftlib/src/dfft_common.cc:            plan->cuda_plans_final_fw =
hoomd/extern/dfftlib/src/dfft_common.cc:                (cuda_plan_t *)malloc(sizeof(cuda_plan_t));
hoomd/extern/dfftlib/src/dfft_common.cc:            plan->cuda_plans_final_bw =
hoomd/extern/dfftlib/src/dfft_common.cc:                (cuda_plan_t *)malloc(sizeof(cuda_plan_t));
hoomd/extern/dfftlib/src/dfft_common.cc:            res = dfft_cuda_create_nd_plan(&plan->cuda_plans_final_fw[0],
hoomd/extern/dfftlib/src/dfft_common.cc:            res = dfft_cuda_create_nd_plan(&plan->cuda_plans_final_bw[0],
hoomd/extern/dfftlib/src/dfft_common.cc:        res = dfft_cuda_init_local_fft();
hoomd/extern/dfftlib/src/dfft_common.cc:        dfft_cuda_allocate_aligned_memory(&(p->d_scratch),sizeof(cuda_cpx_t)*scratch_size);
hoomd/extern/dfftlib/src/dfft_common.cc:        dfft_cuda_allocate_aligned_memory(&(p->d_scratch_2),sizeof(cuda_cpx_t)*scratch_size);
hoomd/extern/dfftlib/src/dfft_common.cc:        dfft_cuda_allocate_aligned_memory(&(p->d_scratch_3),sizeof(cuda_cpx_t)*scratch_size);
hoomd/extern/dfftlib/src/dfft_common.cc:    p->check_cuda_errors = 1;
hoomd/extern/dfftlib/src/dfft_common.cc:    p->check_cuda_errors = 0;
hoomd/extern/dfftlib/src/dfft_common.cc:        dfft_cuda_free_aligned_memory(p.d_scratch);
hoomd/extern/dfftlib/src/dfft_common.cc:        dfft_cuda_free_aligned_memory(p.d_scratch_2);
hoomd/extern/dfftlib/src/dfft_common.cc:        dfft_cuda_free_aligned_memory(p.d_scratch_3);
hoomd/extern/dfftlib/src/dfft_common.cc:        dfft_cuda_teardown_local_fft();
hoomd/extern/dfftlib/src/dfft_common.cc:                    dfft_cuda_destroy_local_plan(&p.cuda_plans_multi_fw[d][j]);
hoomd/extern/dfftlib/src/dfft_common.cc:                    dfft_cuda_destroy_local_plan(&p.cuda_plans_multi_bw[d][j]);
hoomd/extern/dfftlib/src/dfft_common.cc:                dfft_cuda_destroy_local_plan(&p.cuda_plans_final_fw[i]);
hoomd/extern/dfftlib/src/dfft_common.cc:                dfft_cuda_destroy_local_plan(&p.cuda_plans_final_bw[i]);
hoomd/extern/dfftlib/src/dfft_common.h:    cuda_cpx_t *d_scratch;   /* Scratch array */
hoomd/extern/dfftlib/src/dfft_common.h:    cuda_cpx_t *d_scratch_2;
hoomd/extern/dfftlib/src/dfft_common.h:    cuda_cpx_t *d_scratch_3;
hoomd/extern/dfftlib/src/dfft_common.h:    cuda_cpx_t *h_stage_in;  /* Staging array for MPI calls */
hoomd/extern/dfftlib/src/dfft_common.h:    cuda_cpx_t *h_stage_out; /* Staging array for MPI calls */
hoomd/extern/dfftlib/src/dfft_common.h:    int check_cuda_errors; /* == 1 if we are checking errors */
hoomd/extern/dfftlib/src/dfft_common.h:    cuda_plan_t **cuda_plans_multi_fw; /* Multidimensional plan configuration (forward)*/
hoomd/extern/dfftlib/src/dfft_common.h:    cuda_plan_t **cuda_plans_multi_bw; /* backward plans */
hoomd/extern/dfftlib/src/dfft_common.h:    cuda_plan_t *cuda_plans_final_fw; /* Level-0 plans, forward */
hoomd/extern/dfftlib/src/dfft_common.h:    cuda_plan_t *cuda_plans_final_bw; /* backward plans */
hoomd/extern/dfftlib/src/dfft_common.h:    cuda_scalar_t **d_alpha;          /* alpha variables for twiddle factors per dim */
hoomd/extern/dfftlib/src/dfft_common.h:    cuda_scalar_t **h_alpha;          /* host variable */
hoomd/extern/dfftlib/src/dfft_cuda.h:#ifndef __DFFT_CUDA_H__
hoomd/extern/dfftlib/src/dfft_cuda.h:#define __DFFT_CUDA_H__
hoomd/extern/dfftlib/src/dfft_cuda.h:EXTERN_DFFT int dfft_cuda_create_plan(dfft_plan *p,
hoomd/extern/dfftlib/src/dfft_cuda.h:EXTERN_DFFT void dfft_cuda_destroy_plan(dfft_plan plan);
hoomd/extern/dfftlib/src/dfft_cuda.h:EXTERN_DFFT void dfft_cuda_check_errors(dfft_plan *plan, int check_err);
hoomd/extern/dfftlib/src/dfft_cuda.h:EXTERN_DFFT int dfft_cuda_execute(cuda_cpx_t *id_in, cuda_cpx_t *d_out, int dir, dfft_plan *p);
hoomd/extern/dfftlib/src/CMakeLists.txt:include_directories(${MPI_INCLUDE_PATH} ${CUDA_TOOLKIT_INCLUDE})
hoomd/extern/dfftlib/src/CMakeLists.txt:    # CUFFT is default for CUDA
hoomd/extern/dfftlib/src/CMakeLists.txt:    set(CUDA_SOURCES cufft_single_interface.c)
hoomd/extern/dfftlib/src/CMakeLists.txt:    set(CUDA_SOURCES dfft_cuda.cc ${CUDA_SOURCES})
hoomd/extern/dfftlib/src/CMakeLists.txt:    cuda_compile(DFFT_CUDA_O dfft_cuda.cu SHARED)
hoomd/extern/dfftlib/src/CMakeLists.txt:    set(CUDA_OBJECTS ${DFFT_CUDA_O})
hoomd/extern/dfftlib/src/CMakeLists.txt:    set(CUDA_LIBS ${CUDA_CUDART_LIBRARY} ${CUDA_cufft_LIBRARY})
hoomd/extern/dfftlib/src/CMakeLists.txt:add_library (dfft SHARED dfft_common.cc ${HOST_SOURCES} ${CUDA_SOURCES} ${CUDA_OBJECTS})
hoomd/extern/dfftlib/src/CMakeLists.txt:target_link_libraries(dfft ${MPI_C_LIBRARIES} ${LOCAL_FFT_LIBRARIES} ${CUDA_LIBS})
hoomd/extern/dfftlib/src/dfft_cuda.cc:#include "dfft_cuda.h"
hoomd/extern/dfftlib/src/dfft_cuda.cc:#include "dfft_cuda.h"
hoomd/extern/dfftlib/src/dfft_cuda.cc:#include "dfft_cuda.cuh"
hoomd/extern/dfftlib/src/dfft_cuda.cc:#define CHECK_CUDA() \
hoomd/extern/dfftlib/src/dfft_cuda.cc:        printf("CUDA Error in file %s, line %d: %s\n", __FILE__,__LINE__,   \
hoomd/extern/dfftlib/src/dfft_cuda.cc:void dfft_cuda_redistribute_nd( dfft_plan *plan,int stage, int size_in, int *embed, int *d_embed, int dir,
hoomd/extern/dfftlib/src/dfft_cuda.cc:                  cuda_cpx_t *d_work, int **rho_c0, int **rho_c1, int **rho_plc0c1, int **rho_pc0,
hoomd/extern/dfftlib/src/dfft_cuda.cc:        plan->nsend[rank] = (unsigned int)(send_size*sizeof(cuda_cpx_t));
hoomd/extern/dfftlib/src/dfft_cuda.cc:        plan->nrecv[rank] = (unsigned int)(recv_size*sizeof(cuda_cpx_t));
hoomd/extern/dfftlib/src/dfft_cuda.cc:        plan->offset_send[rank] = (unsigned int)(soffs*sizeof(cuda_cpx_t));
hoomd/extern/dfftlib/src/dfft_cuda.cc:        plan->offset_recv[rank] = (unsigned int)(roffs*sizeof(cuda_cpx_t));
hoomd/extern/dfftlib/src/dfft_cuda.cc:        gpu_b2c_pack_nd(size_in, plan->d_c0[stage], plan->d_c1[stage], plan->ndim, d_embed,
hoomd/extern/dfftlib/src/dfft_cuda.cc:        if (plan->check_cuda_errors) CHECK_CUDA();
hoomd/extern/dfftlib/src/dfft_cuda.cc:        gpu_c2b_pack_nd(size_in, plan->d_c0[stage], plan->d_c1[stage], plan->ndim, d_embed,
hoomd/extern/dfftlib/src/dfft_cuda.cc:        if (plan->check_cuda_errors) CHECK_CUDA();
hoomd/extern/dfftlib/src/dfft_cuda.cc:    hipMemcpy(plan->h_stage_in, plan->d_scratch, sizeof(cuda_cpx_t)*size_in,hipMemcpyDefault);
hoomd/extern/dfftlib/src/dfft_cuda.cc:    if (plan->check_cuda_errors) CHECK_CUDA();
hoomd/extern/dfftlib/src/dfft_cuda.cc:    hipMemcpy(plan->d_scratch_2,plan->h_stage_out, sizeof(cuda_cpx_t)*size_in,hipMemcpyDefault);
hoomd/extern/dfftlib/src/dfft_cuda.cc:    if (plan->check_cuda_errors) CHECK_CUDA();
hoomd/extern/dfftlib/src/dfft_cuda.cc:        gpu_b2c_unpack_nd(size_in, plan->d_c0[stage], plan->d_c1[stage], plan->ndim, d_embed,
hoomd/extern/dfftlib/src/dfft_cuda.cc:        if (plan->check_cuda_errors) CHECK_CUDA();
hoomd/extern/dfftlib/src/dfft_cuda.cc:        gpu_c2b_unpack_nd(size_in, plan->d_c0[stage], plan->d_c1[stage], plan->ndim, d_embed,
hoomd/extern/dfftlib/src/dfft_cuda.cc:        if (plan->check_cuda_errors) CHECK_CUDA();
hoomd/extern/dfftlib/src/dfft_cuda.cc:void dfft_cuda_redistribute_block_to_cyclic_1d(
hoomd/extern/dfftlib/src/dfft_cuda.cc:                  cuda_cpx_t *d_work,
hoomd/extern/dfftlib/src/dfft_cuda.cc:                  cuda_cpx_t *d_scratch,
hoomd/extern/dfftlib/src/dfft_cuda.cc:                  cuda_cpx_t *h_stage_in,
hoomd/extern/dfftlib/src/dfft_cuda.cc:                  cuda_cpx_t *h_stage_out,
hoomd/extern/dfftlib/src/dfft_cuda.cc:        dfft_nsend[destproc] = (unsigned int)(size*sizeof(cuda_cpx_t));
hoomd/extern/dfftlib/src/dfft_cuda.cc:        dfft_offset_send[destproc] = (unsigned int)(offset*sizeof(cuda_cpx_t));
hoomd/extern/dfftlib/src/dfft_cuda.cc:    gpu_b2c_pack(npackets*size, ratio, size, npackets, stride, d_work, d_scratch);
hoomd/extern/dfftlib/src/dfft_cuda.cc:    if (check_err) CHECK_CUDA();
hoomd/extern/dfftlib/src/dfft_cuda.cc:        dfft_nrecv[srcproc] = (unsigned int)(size*sizeof(cuda_cpx_t));
hoomd/extern/dfftlib/src/dfft_cuda.cc:        dfft_offset_recv[srcproc] = (unsigned int)(offset*sizeof(cuda_cpx_t));
hoomd/extern/dfftlib/src/dfft_cuda.cc:    hipMemcpy(h_stage_in, d_scratch, sizeof(cuda_cpx_t)*npackets*size,hipMemcpyDefault);
hoomd/extern/dfftlib/src/dfft_cuda.cc:    if (check_err) CHECK_CUDA();
hoomd/extern/dfftlib/src/dfft_cuda.cc:    hipMemcpy(d_work,h_stage_out, sizeof(cuda_cpx_t)*size_in,hipMemcpyDefault);
hoomd/extern/dfftlib/src/dfft_cuda.cc:    if (check_err) CHECK_CUDA();
hoomd/extern/dfftlib/src/dfft_cuda.cc:void dfft_cuda_redistribute_cyclic_to_block_1d(int *dim,
hoomd/extern/dfftlib/src/dfft_cuda.cc:                     cuda_cpx_t *d_work,
hoomd/extern/dfftlib/src/dfft_cuda.cc:                     cuda_cpx_t *d_scratch,
hoomd/extern/dfftlib/src/dfft_cuda.cc:                     cuda_cpx_t *h_stage_in,
hoomd/extern/dfftlib/src/dfft_cuda.cc:                     cuda_cpx_t *h_stage_out,
hoomd/extern/dfftlib/src/dfft_cuda.cc:        dfft_offset_send[destproc] = (unsigned int)((send ? (stride*j1*sizeof(cuda_cpx_t)) : 0));
hoomd/extern/dfftlib/src/dfft_cuda.cc:            dfft_offset_recv[destproc] = (unsigned int)(stride*j0_remote*length/c0*sizeof(cuda_cpx_t));
hoomd/extern/dfftlib/src/dfft_cuda.cc:            dfft_offset_recv[destproc] = (unsigned int)(offset*sizeof(cuda_cpx_t));
hoomd/extern/dfftlib/src/dfft_cuda.cc:        dfft_nsend[destproc] = (unsigned int)(send_size*sizeof(cuda_cpx_t));
hoomd/extern/dfftlib/src/dfft_cuda.cc:        dfft_nrecv[destproc] = (unsigned int)(recv_size*sizeof(cuda_cpx_t));
hoomd/extern/dfftlib/src/dfft_cuda.cc:            dfft_offset_send[destproc] = (unsigned int)(offset*sizeof(cuda_cpx_t)*stride);
hoomd/extern/dfftlib/src/dfft_cuda.cc:            int n = (unsigned int)(dfft_nsend[destproc]/stride/sizeof(cuda_cpx_t));
hoomd/extern/dfftlib/src/dfft_cuda.cc:        gpu_b2c_pack(size_in, c0, size, c0, stride, d_work, d_scratch);
hoomd/extern/dfftlib/src/dfft_cuda.cc:        if (check_err) CHECK_CUDA();
hoomd/extern/dfftlib/src/dfft_cuda.cc:        hipMemcpy(h_stage_in, d_scratch, sizeof(cuda_cpx_t)*length*stride,hipMemcpyDefault);
hoomd/extern/dfftlib/src/dfft_cuda.cc:        if (check_err) CHECK_CUDA();
hoomd/extern/dfftlib/src/dfft_cuda.cc:        hipMemcpy(d_work,h_stage_out, sizeof(cuda_cpx_t)*npackets*size,hipMemcpyDefault);
hoomd/extern/dfftlib/src/dfft_cuda.cc:        if (check_err) CHECK_CUDA();
hoomd/extern/dfftlib/src/dfft_cuda.cc:        hipMemcpy(h_stage_in, d_work, sizeof(cuda_cpx_t)*size_in,hipMemcpyDefault);
hoomd/extern/dfftlib/src/dfft_cuda.cc:        if (check_err) CHECK_CUDA();
hoomd/extern/dfftlib/src/dfft_cuda.cc:        hipMemcpy(d_scratch,h_stage_out, sizeof(cuda_cpx_t)*npackets*size,hipMemcpyDefault);
hoomd/extern/dfftlib/src/dfft_cuda.cc:        if (check_err) CHECK_CUDA();
hoomd/extern/dfftlib/src/dfft_cuda.cc:        gpu_c2b_unpack(npackets*size, length, c0, c1, size, j0_new_local, stride, rev, d_work, d_scratch);
hoomd/extern/dfftlib/src/dfft_cuda.cc:        if (check_err) CHECK_CUDA();
hoomd/extern/dfftlib/src/dfft_cuda.cc:void cuda_mpifft1d_dif(int *dim,
hoomd/extern/dfftlib/src/dfft_cuda.cc:            cuda_cpx_t *d_in,
hoomd/extern/dfftlib/src/dfft_cuda.cc:            cuda_cpx_t *d_out,
hoomd/extern/dfftlib/src/dfft_cuda.cc:            cuda_cpx_t *h_stage_in,
hoomd/extern/dfftlib/src/dfft_cuda.cc:            cuda_cpx_t *h_stage_out,
hoomd/extern/dfftlib/src/dfft_cuda.cc:            cuda_plan_t plan_short,
hoomd/extern/dfftlib/src/dfft_cuda.cc:            cuda_plan_t plan_long,
hoomd/extern/dfftlib/src/dfft_cuda.cc:        dfft_cuda_local_fft(d_in, d_out, plan_long, inverse);
hoomd/extern/dfftlib/src/dfft_cuda.cc:        gpu_twiddle(size, length, stride, float(alpha), d_out, d_in, inverse);
hoomd/extern/dfftlib/src/dfft_cuda.cc:        if (check_err) CHECK_CUDA();
hoomd/extern/dfftlib/src/dfft_cuda.cc:        dfft_cuda_redistribute_cyclic_to_block_1d(dim,pdim,ndim,current_dim,
hoomd/extern/dfftlib/src/dfft_cuda.cc:    dfft_cuda_local_fft(d_in, d_out, plan_short,inverse);
hoomd/extern/dfftlib/src/dfft_cuda.cc:void cuda_mpifftnd_dif(int *dim,
hoomd/extern/dfftlib/src/dfft_cuda.cc:            cuda_cpx_t *d_work,
hoomd/extern/dfftlib/src/dfft_cuda.cc:            cuda_cpx_t *d_scratch,
hoomd/extern/dfftlib/src/dfft_cuda.cc:            cuda_cpx_t *h_stage_in,
hoomd/extern/dfftlib/src/dfft_cuda.cc:            cuda_cpx_t *h_stage_out,
hoomd/extern/dfftlib/src/dfft_cuda.cc:            cuda_plan_t *plans_short,
hoomd/extern/dfftlib/src/dfft_cuda.cc:            cuda_plan_t *plans_long,
hoomd/extern/dfftlib/src/dfft_cuda.cc:        cuda_mpifft1d_dif(dim, pdim,ndim,current_dim,pidx, inv,
hoomd/extern/dfftlib/src/dfft_cuda.cc:        gpu_transpose(size,l,stride, oembed[current_dim],d_scratch, d_work);
hoomd/extern/dfftlib/src/dfft_cuda.cc:        if (check_err) CHECK_CUDA();
hoomd/extern/dfftlib/src/dfft_cuda.cc:void cuda_fftnd_multi(dfft_plan *p,
hoomd/extern/dfftlib/src/dfft_cuda.cc:                      cuda_cpx_t *d_in,
hoomd/extern/dfftlib/src/dfft_cuda.cc:                      cuda_cpx_t *d_out,
hoomd/extern/dfftlib/src/dfft_cuda.cc:                      cuda_plan_t **cuda_plans_multi,
hoomd/extern/dfftlib/src/dfft_cuda.cc:                      cuda_plan_t *cuda_plans_final,
hoomd/extern/dfftlib/src/dfft_cuda.cc:        cuda_cpx_t *cur_in = d_in;
hoomd/extern/dfftlib/src/dfft_cuda.cc:        cuda_cpx_t *cur_out = p->d_scratch;
hoomd/extern/dfftlib/src/dfft_cuda.cc:                    res = dfft_cuda_local_fft(cur_in, cur_out, cuda_plans_multi[d][j], inv);
hoomd/extern/dfftlib/src/dfft_cuda.cc:                    if (p->check_cuda_errors) CHECK_CUDA();
hoomd/extern/dfftlib/src/dfft_cuda.cc:                    gpu_transpose(p->size_in,l,stride, p->inembed[j],cur_in,cur_out);
hoomd/extern/dfftlib/src/dfft_cuda.cc:                    if (p->check_cuda_errors) CHECK_CUDA();
hoomd/extern/dfftlib/src/dfft_cuda.cc:                cuda_cpx_t *tmp;
hoomd/extern/dfftlib/src/dfft_cuda.cc:            hipMemcpy(p->d_alpha[d], p->h_alpha[d], sizeof(cuda_scalar_t)*p->ndim,hipMemcpyDefault);
hoomd/extern/dfftlib/src/dfft_cuda.cc:            CHECK_CUDA();
hoomd/extern/dfftlib/src/dfft_cuda.cc:            gpu_twiddle_nd(p->size_in, p->ndim, p->d_iembed, p->d_length,
hoomd/extern/dfftlib/src/dfft_cuda.cc:            if (p->check_cuda_errors) CHECK_CUDA();
hoomd/extern/dfftlib/src/dfft_cuda.cc:            CHECK_CUDA();
hoomd/extern/dfftlib/src/dfft_cuda.cc:            CHECK_CUDA();
hoomd/extern/dfftlib/src/dfft_cuda.cc:            CHECK_CUDA();
hoomd/extern/dfftlib/src/dfft_cuda.cc:            CHECK_CUDA();
hoomd/extern/dfftlib/src/dfft_cuda.cc:            CHECK_CUDA();
hoomd/extern/dfftlib/src/dfft_cuda.cc:            dfft_cuda_redistribute_nd(p, d, p->size_in, p->inembed, p->d_iembed,
hoomd/extern/dfftlib/src/dfft_cuda.cc:                cuda_cpx_t *d_in_ptr = ((i == 0) ? d_in : p->d_scratch);
hoomd/extern/dfftlib/src/dfft_cuda.cc:                res = dfft_cuda_local_fft(d_in_ptr, p->d_scratch_2, cuda_plans_final[i] , inv);
hoomd/extern/dfftlib/src/dfft_cuda.cc:                if (p->check_cuda_errors) CHECK_CUDA();
hoomd/extern/dfftlib/src/dfft_cuda.cc:                cuda_cpx_t *d_out_ptr = ((i == p->ndim-1) ? d_out : p->d_scratch);
hoomd/extern/dfftlib/src/dfft_cuda.cc:                gpu_transpose(size,l,stride, p->oembed[i],p->d_scratch_2, d_out_ptr);
hoomd/extern/dfftlib/src/dfft_cuda.cc:                if (p->check_cuda_errors) CHECK_CUDA();
hoomd/extern/dfftlib/src/dfft_cuda.cc:            res = dfft_cuda_local_fft(d_in, d_out, cuda_plans_final[0] , inv);
hoomd/extern/dfftlib/src/dfft_cuda.cc:            if (p->check_cuda_errors) CHECK_CUDA();
hoomd/extern/dfftlib/src/dfft_cuda.cc:void dfft_cuda_redistribute(dfft_plan *plan, int size, int *embed, int *d_embed,
hoomd/extern/dfftlib/src/dfft_cuda.cc:            cuda_cpx_t *d_work, int c2b)
hoomd/extern/dfftlib/src/dfft_cuda.cc:        CHECK_CUDA();
hoomd/extern/dfftlib/src/dfft_cuda.cc:        CHECK_CUDA();
hoomd/extern/dfftlib/src/dfft_cuda.cc:        CHECK_CUDA();
hoomd/extern/dfftlib/src/dfft_cuda.cc:        CHECK_CUDA();
hoomd/extern/dfftlib/src/dfft_cuda.cc:        CHECK_CUDA();
hoomd/extern/dfftlib/src/dfft_cuda.cc:        dfft_cuda_redistribute_nd(plan, d, size, embed, d_embed, dir,
hoomd/extern/dfftlib/src/dfft_cuda.cc:int dfft_cuda_execute(cuda_cpx_t *d_in, cuda_cpx_t *d_out, int dir, dfft_plan *p)
hoomd/extern/dfftlib/src/dfft_cuda.cc:    int check_err = p->check_cuda_errors;
hoomd/extern/dfftlib/src/dfft_cuda.cc:    cuda_cpx_t *d_work = NULL;
hoomd/extern/dfftlib/src/dfft_cuda.cc:            hipMemcpy(d_work, d_in, p->size_in*sizeof(cuda_cpx_t),hipMemcpyDefault);
hoomd/extern/dfftlib/src/dfft_cuda.cc:            if (check_err) CHECK_CUDA();
hoomd/extern/dfftlib/src/dfft_cuda.cc:        dfft_cuda_redistribute(p,p->size_in, p->inembed, p->d_iembed, d_work, 0);
hoomd/extern/dfftlib/src/dfft_cuda.cc:    /*cuda_mpifftnd_dif(p.gdim, p.pdim, p.ndim, p.pidx, dir,
hoomd/extern/dfftlib/src/dfft_cuda.cc:        dir ? p.cuda_plans_short_inverse : p.cuda_plans_short_forward,
hoomd/extern/dfftlib/src/dfft_cuda.cc:        dir ? p.cuda_plans_long_inverse : p.cuda_plans_long_forward,
hoomd/extern/dfftlib/src/dfft_cuda.cc:    cuda_fftnd_multi(p, d_work, d_out,
hoomd/extern/dfftlib/src/dfft_cuda.cc:                     dir ? p->cuda_plans_multi_bw : p->cuda_plans_multi_fw,
hoomd/extern/dfftlib/src/dfft_cuda.cc:                     dir ? p->cuda_plans_final_bw : p->cuda_plans_final_fw,
hoomd/extern/dfftlib/src/dfft_cuda.cc:        dfft_cuda_redistribute(p,p->size_out, p->oembed, p->d_oembed, d_out, 1);
hoomd/extern/dfftlib/src/dfft_cuda.cc:int dfft_cuda_create_plan(dfft_plan *p,
hoomd/extern/dfftlib/src/dfft_cuda.cc:    int size = (unsigned int)(p->scratch_size*sizeof(cuda_cpx_t));
hoomd/extern/dfftlib/src/dfft_cuda.cc:    CHECK_CUDA();
hoomd/extern/dfftlib/src/dfft_cuda.cc:    CHECK_CUDA();
hoomd/extern/dfftlib/src/dfft_cuda.cc:    CHECK_CUDA();
hoomd/extern/dfftlib/src/dfft_cuda.cc:    CHECK_CUDA();
hoomd/extern/dfftlib/src/dfft_cuda.cc:    CHECK_CUDA();
hoomd/extern/dfftlib/src/dfft_cuda.cc:    CHECK_CUDA();
hoomd/extern/dfftlib/src/dfft_cuda.cc:    CHECK_CUDA();
hoomd/extern/dfftlib/src/dfft_cuda.cc:    /* initialize cuda buffers */
hoomd/extern/dfftlib/src/dfft_cuda.cc:    CHECK_CUDA();
hoomd/extern/dfftlib/src/dfft_cuda.cc:    CHECK_CUDA();
hoomd/extern/dfftlib/src/dfft_cuda.cc:    CHECK_CUDA();
hoomd/extern/dfftlib/src/dfft_cuda.cc:    CHECK_CUDA();
hoomd/extern/dfftlib/src/dfft_cuda.cc:    CHECK_CUDA();
hoomd/extern/dfftlib/src/dfft_cuda.cc:        p->h_alpha = (cuda_scalar_t **) malloc(sizeof(cuda_scalar_t *)*p->max_depth);
hoomd/extern/dfftlib/src/dfft_cuda.cc:        p->d_alpha = (cuda_scalar_t **) malloc(sizeof(cuda_scalar_t *)*p->max_depth);
hoomd/extern/dfftlib/src/dfft_cuda.cc:        CHECK_CUDA();
hoomd/extern/dfftlib/src/dfft_cuda.cc:        CHECK_CUDA();
hoomd/extern/dfftlib/src/dfft_cuda.cc:        CHECK_CUDA();
hoomd/extern/dfftlib/src/dfft_cuda.cc:        CHECK_CUDA();
hoomd/extern/dfftlib/src/dfft_cuda.cc:        CHECK_CUDA();
hoomd/extern/dfftlib/src/dfft_cuda.cc:        hipMalloc((void **)&(p->d_alpha[d]), sizeof(cuda_scalar_t)*ndim);
hoomd/extern/dfftlib/src/dfft_cuda.cc:        CHECK_CUDA();
hoomd/extern/dfftlib/src/dfft_cuda.cc:        p->h_alpha[d] = (cuda_scalar_t *) malloc(sizeof(cuda_scalar_t)*ndim);
hoomd/extern/dfftlib/src/dfft_cuda.cc:    dfft_cuda_execute(NULL, NULL, 0, p);
hoomd/extern/dfftlib/src/dfft_cuda.cc:void dfft_cuda_destroy_plan(dfft_plan plan)
hoomd/extern/dfftlib/src/dfft_cuda.cc:void dfft_cuda_check_errors(dfft_plan *plan, int check_err)
hoomd/extern/dfftlib/src/dfft_cuda.cc:    plan->check_cuda_errors = check_err;
hoomd/extern/dfftlib/src/dfft_cuda.cu:#include "dfft_cuda.cuh"
hoomd/extern/dfftlib/src/dfft_cuda.cu:__global__ void gpu_b2c_pack_kernel_nd(unsigned int local_size,
hoomd/extern/dfftlib/src/dfft_cuda.cu:                                    const cuda_cpx_t *local_data,
hoomd/extern/dfftlib/src/dfft_cuda.cu:                                    cuda_cpx_t *send_data
hoomd/extern/dfftlib/src/dfft_cuda.cu:__global__ void gpu_b2c_unpack_kernel_nd(unsigned int local_size,
hoomd/extern/dfftlib/src/dfft_cuda.cu:                                    const cuda_cpx_t *recv_data,
hoomd/extern/dfftlib/src/dfft_cuda.cu:                                    cuda_cpx_t *local_data
hoomd/extern/dfftlib/src/dfft_cuda.cu:void gpu_b2c_pack_nd(unsigned int local_size,
hoomd/extern/dfftlib/src/dfft_cuda.cu:                     const cuda_cpx_t *local_data,
hoomd/extern/dfftlib/src/dfft_cuda.cu:                     cuda_cpx_t *send_data
hoomd/extern/dfftlib/src/dfft_cuda.cu:    hipLaunchKernelGGL(gpu_b2c_pack_kernel_nd, dim3(n_blocks), dim3(block_size), shared_size, 0, local_size,
hoomd/extern/dfftlib/src/dfft_cuda.cu:void gpu_b2c_unpack_nd(unsigned int local_size,
hoomd/extern/dfftlib/src/dfft_cuda.cu:                     const cuda_cpx_t *recv_data,
hoomd/extern/dfftlib/src/dfft_cuda.cu:                     cuda_cpx_t *local_data
hoomd/extern/dfftlib/src/dfft_cuda.cu:    hipLaunchKernelGGL(gpu_b2c_unpack_kernel_nd, dim3(n_blocks), dim3(block_size), 0, 0, local_size,
hoomd/extern/dfftlib/src/dfft_cuda.cu:__global__ void gpu_c2b_pack_kernel_nd(unsigned int local_size,
hoomd/extern/dfftlib/src/dfft_cuda.cu:                                    const cuda_cpx_t *local_data,
hoomd/extern/dfftlib/src/dfft_cuda.cu:                                    cuda_cpx_t *send_data
hoomd/extern/dfftlib/src/dfft_cuda.cu:__global__ void gpu_c2b_unpack_kernel_nd(unsigned int local_size,
hoomd/extern/dfftlib/src/dfft_cuda.cu:                                    const cuda_cpx_t *recv_data,
hoomd/extern/dfftlib/src/dfft_cuda.cu:                                    cuda_cpx_t *local_data
hoomd/extern/dfftlib/src/dfft_cuda.cu:void gpu_c2b_pack_nd(unsigned int local_size,
hoomd/extern/dfftlib/src/dfft_cuda.cu:                     const cuda_cpx_t *local_data,
hoomd/extern/dfftlib/src/dfft_cuda.cu:                     cuda_cpx_t *send_data
hoomd/extern/dfftlib/src/dfft_cuda.cu:    hipLaunchKernelGGL(gpu_c2b_pack_kernel_nd, dim3(n_blocks), dim3(block_size), shared_size, 0, local_size,
hoomd/extern/dfftlib/src/dfft_cuda.cu:void gpu_c2b_unpack_nd(unsigned int local_size,
hoomd/extern/dfftlib/src/dfft_cuda.cu:                     const cuda_cpx_t *recv_data,
hoomd/extern/dfftlib/src/dfft_cuda.cu:                     cuda_cpx_t *local_data
hoomd/extern/dfftlib/src/dfft_cuda.cu:    hipLaunchKernelGGL(gpu_c2b_unpack_kernel_nd, dim3(n_blocks), dim3(block_size), 0, 0, local_size,
hoomd/extern/dfftlib/src/dfft_cuda.cu:__global__ void gpu_b2c_pack_kernel(unsigned int local_size,
hoomd/extern/dfftlib/src/dfft_cuda.cu:                                    cuda_cpx_t *local_data,
hoomd/extern/dfftlib/src/dfft_cuda.cu:                                    cuda_cpx_t *send_data
hoomd/extern/dfftlib/src/dfft_cuda.cu:void gpu_b2c_pack(unsigned int local_size,
hoomd/extern/dfftlib/src/dfft_cuda.cu:                  cuda_cpx_t *local_data,
hoomd/extern/dfftlib/src/dfft_cuda.cu:                  cuda_cpx_t *send_data)
hoomd/extern/dfftlib/src/dfft_cuda.cu:    hipLaunchKernelGGL(gpu_b2c_pack_kernel, dim3(n_blocks), dim3(block_size), 0, 0, local_size,
hoomd/extern/dfftlib/src/dfft_cuda.cu:__global__ void gpu_twiddle_kernel(unsigned int local_size,
hoomd/extern/dfftlib/src/dfft_cuda.cu:                                   cuda_cpx_t *d_in,
hoomd/extern/dfftlib/src/dfft_cuda.cu:                                   cuda_cpx_t *d_out,
hoomd/extern/dfftlib/src/dfft_cuda.cu:    cuda_cpx_t w;
hoomd/extern/dfftlib/src/dfft_cuda.cu:    CUDA_RE(w) = cosf((float)j*theta);
hoomd/extern/dfftlib/src/dfft_cuda.cu:    CUDA_IM(w) = sinf((float)j*theta);
hoomd/extern/dfftlib/src/dfft_cuda.cu:    cuda_cpx_t in = d_in[idx];
hoomd/extern/dfftlib/src/dfft_cuda.cu:    cuda_cpx_t out;
hoomd/extern/dfftlib/src/dfft_cuda.cu:    CUDA_RE(out) = CUDA_RE(in) * CUDA_RE(w) - CUDA_IM(in) * CUDA_IM(w);
hoomd/extern/dfftlib/src/dfft_cuda.cu:    CUDA_IM(out) = CUDA_RE(in) * CUDA_IM(w) + CUDA_IM(in) * CUDA_RE(w);
hoomd/extern/dfftlib/src/dfft_cuda.cu:__global__ void gpu_twiddle_kernel_nd(unsigned int local_size,
hoomd/extern/dfftlib/src/dfft_cuda.cu:                                   cuda_cpx_t *d_in,
hoomd/extern/dfftlib/src/dfft_cuda.cu:                                   cuda_cpx_t *d_out,
hoomd/extern/dfftlib/src/dfft_cuda.cu:    cuda_cpx_t w;
hoomd/extern/dfftlib/src/dfft_cuda.cu:    CUDA_RE(w) = cosf(theta);
hoomd/extern/dfftlib/src/dfft_cuda.cu:    CUDA_IM(w) = sinf(theta);
hoomd/extern/dfftlib/src/dfft_cuda.cu:    cuda_cpx_t out;
hoomd/extern/dfftlib/src/dfft_cuda.cu:    cuda_cpx_t in = d_in[idx];
hoomd/extern/dfftlib/src/dfft_cuda.cu:    CUDA_RE(out) = CUDA_RE(in) * CUDA_RE(w) - CUDA_IM(in) * CUDA_IM(w);
hoomd/extern/dfftlib/src/dfft_cuda.cu:    CUDA_IM(out) = CUDA_RE(in) * CUDA_IM(w) + CUDA_IM(in) * CUDA_RE(w);
hoomd/extern/dfftlib/src/dfft_cuda.cu:void gpu_twiddle(unsigned int local_size,
hoomd/extern/dfftlib/src/dfft_cuda.cu:                 cuda_cpx_t *d_in,
hoomd/extern/dfftlib/src/dfft_cuda.cu:                 cuda_cpx_t *d_out,
hoomd/extern/dfftlib/src/dfft_cuda.cu:    hipLaunchKernelGGL(gpu_twiddle_kernel, dim3(n_block), dim3(block_size), 0, 0, local_size,
hoomd/extern/dfftlib/src/dfft_cuda.cu:void gpu_twiddle_nd(unsigned int local_size,
hoomd/extern/dfftlib/src/dfft_cuda.cu:                 cuda_cpx_t *d_in,
hoomd/extern/dfftlib/src/dfft_cuda.cu:                 cuda_cpx_t *d_out,
hoomd/extern/dfftlib/src/dfft_cuda.cu:    hipLaunchKernelGGL(gpu_twiddle_kernel_nd, dim3(n_block), dim3(block_size), 0, 0, local_size, ndim, d_embed,
hoomd/extern/dfftlib/src/dfft_cuda.cu:__global__ void gpu_c2b_unpack_kernel(const unsigned int local_size,
hoomd/extern/dfftlib/src/dfft_cuda.cu:                                      cuda_cpx_t *d_local_data,
hoomd/extern/dfftlib/src/dfft_cuda.cu:                                      const cuda_cpx_t *d_scratch)
hoomd/extern/dfftlib/src/dfft_cuda.cu:void gpu_c2b_unpack(const unsigned int local_size,
hoomd/extern/dfftlib/src/dfft_cuda.cu:                    cuda_cpx_t *d_local_data,
hoomd/extern/dfftlib/src/dfft_cuda.cu:                    const cuda_cpx_t *d_scratch)
hoomd/extern/dfftlib/src/dfft_cuda.cu:    hipLaunchKernelGGL(gpu_c2b_unpack_kernel, dim3(n_block), dim3(block_size), 0, 0, local_size,
hoomd/extern/dfftlib/src/dfft_cuda.cu:__global__ void gpu_transpose_kernel(const unsigned int size,
hoomd/extern/dfftlib/src/dfft_cuda.cu:                                     const cuda_cpx_t *in,
hoomd/extern/dfftlib/src/dfft_cuda.cu:                                     cuda_cpx_t *out)
hoomd/extern/dfftlib/src/dfft_cuda.cu:__global__ void transpose_sdk(cuda_cpx_t *odata, const cuda_cpx_t *idata, int width, int height, int embed)
hoomd/extern/dfftlib/src/dfft_cuda.cu:    __shared__ cuda_cpx_t tile[TILE_DIM][TILE_DIM+1];
hoomd/extern/dfftlib/src/dfft_cuda.cu:void gpu_transpose(const unsigned int size,
hoomd/extern/dfftlib/src/dfft_cuda.cu:                   const cuda_cpx_t *in,
hoomd/extern/dfftlib/src/dfft_cuda.cu:                   cuda_cpx_t *out)
hoomd/extern/dfftlib/src/dfft_cuda.cu://    hipLaunchKernelGGL(gpu_transpose_kernel, dim3(n_block), dim3(block_size), 0, 0, size, length, stride, embed, in, out);
hoomd/extern/dfftlib/src/dfft_cuda.cu:        hipMemcpy(out,in,sizeof(cuda_cpx_t)*stride*length,hipMemcpyDefault);
hoomd/extern/dfftlib/src/cufft_single_interface.h:typedef float cuda_scalar_t;
hoomd/extern/dfftlib/src/cufft_single_interface.h:typedef hipfftComplex cuda_cpx_t;
hoomd/extern/dfftlib/src/cufft_single_interface.h:typedef hipfftHandle cuda_plan_t;
hoomd/extern/dfftlib/src/cufft_single_interface.h:#define CUDA_RE(X) X.x
hoomd/extern/dfftlib/src/cufft_single_interface.h:#define CUDA_IM(X) X.y
hoomd/extern/dfftlib/src/cufft_single_interface.h:#define CUDA_FFT_SUPPORTS_MULTI
hoomd/extern/dfftlib/src/cufft_single_interface.h:#define CUDA_FFT_MAX_N 3
hoomd/extern/dfftlib/src/cufft_single_interface.h:int dfft_cuda_init_local_fft();
hoomd/extern/dfftlib/src/cufft_single_interface.h:void dfft_cuda_teardown_local_fft();
hoomd/extern/dfftlib/src/cufft_single_interface.h:int dfft_cuda_create_1d_plan(
hoomd/extern/dfftlib/src/cufft_single_interface.h:    cuda_plan_t *plan,
hoomd/extern/dfftlib/src/cufft_single_interface.h:int dfft_cuda_create_nd_plan(
hoomd/extern/dfftlib/src/cufft_single_interface.h:    cuda_plan_t *plan,
hoomd/extern/dfftlib/src/cufft_single_interface.h:int dfft_cuda_allocate_aligned_memory(cuda_cpx_t **ptr, size_t size);
hoomd/extern/dfftlib/src/cufft_single_interface.h:void dfft_cuda_free_aligned_memory(cuda_cpx_t *ptr);
hoomd/extern/dfftlib/src/cufft_single_interface.h:int dfft_cuda_destroy_local_plan(cuda_plan_t *p);
hoomd/extern/dfftlib/src/cufft_single_interface.h:int dfft_cuda_local_fft( cuda_cpx_t *in, cuda_cpx_t *out, cuda_plan_t p, int dir);
hoomd/extern/dfftlib/src/dfft_cuda.cuh:EXTERN_DFFT void gpu_b2c_pack(unsigned int local_size,
hoomd/extern/dfftlib/src/dfft_cuda.cuh:                  cuda_cpx_t *local_data,
hoomd/extern/dfftlib/src/dfft_cuda.cuh:                  cuda_cpx_t *send_data);
hoomd/extern/dfftlib/src/dfft_cuda.cuh:EXTERN_DFFT void gpu_twiddle(unsigned int np,
hoomd/extern/dfftlib/src/dfft_cuda.cuh:                 cuda_cpx_t *d_in,
hoomd/extern/dfftlib/src/dfft_cuda.cuh:                 cuda_cpx_t *d_out,
hoomd/extern/dfftlib/src/dfft_cuda.cuh:EXTERN_DFFT void gpu_twiddle_nd(unsigned int local_size,
hoomd/extern/dfftlib/src/dfft_cuda.cuh:                 cuda_cpx_t *d_in,
hoomd/extern/dfftlib/src/dfft_cuda.cuh:                 cuda_cpx_t *d_out,
hoomd/extern/dfftlib/src/dfft_cuda.cuh:EXTERN_DFFT void gpu_c2b_unpack(const unsigned int local_size,
hoomd/extern/dfftlib/src/dfft_cuda.cuh:                    cuda_cpx_t *d_local_data,
hoomd/extern/dfftlib/src/dfft_cuda.cuh:                    const cuda_cpx_t *d_scratch);
hoomd/extern/dfftlib/src/dfft_cuda.cuh:EXTERN_DFFT void gpu_b2c_pack_nd(unsigned int local_size,
hoomd/extern/dfftlib/src/dfft_cuda.cuh:                     const cuda_cpx_t *local_data,
hoomd/extern/dfftlib/src/dfft_cuda.cuh:                     cuda_cpx_t *send_data
hoomd/extern/dfftlib/src/dfft_cuda.cuh:EXTERN_DFFT void gpu_b2c_unpack_nd(unsigned int local_size,
hoomd/extern/dfftlib/src/dfft_cuda.cuh:                     const cuda_cpx_t *recv_data,
hoomd/extern/dfftlib/src/dfft_cuda.cuh:                     cuda_cpx_t *local_data
hoomd/extern/dfftlib/src/dfft_cuda.cuh:EXTERN_DFFT void gpu_c2b_pack_nd(unsigned int local_size,
hoomd/extern/dfftlib/src/dfft_cuda.cuh:                     const cuda_cpx_t *local_data,
hoomd/extern/dfftlib/src/dfft_cuda.cuh:                     cuda_cpx_t *send_data
hoomd/extern/dfftlib/src/dfft_cuda.cuh:EXTERN_DFFT void gpu_c2b_unpack_nd(unsigned int local_size,
hoomd/extern/dfftlib/src/dfft_cuda.cuh:                     const cuda_cpx_t *recv_data,
hoomd/extern/dfftlib/src/dfft_cuda.cuh:                     cuda_cpx_t *local_data
hoomd/extern/dfftlib/src/dfft_cuda.cuh:EXTERN_DFFT void gpu_transpose(const unsigned int local_size,
hoomd/extern/dfftlib/src/dfft_cuda.cuh:                   const cuda_cpx_t *in,
hoomd/extern/dfftlib/src/dfft_cuda.cuh:                   cuda_cpx_t *out);
hoomd/extern/dfftlib/src/cufft_single_interface.cc:int dfft_cuda_init_local_fft()
hoomd/extern/dfftlib/src/cufft_single_interface.cc:void dfft_cuda_teardown_local_fft()
hoomd/extern/dfftlib/src/cufft_single_interface.cc:int dfft_cuda_create_1d_plan(
hoomd/extern/dfftlib/src/cufft_single_interface.cc:    cuda_plan_t *plan,
hoomd/extern/dfftlib/src/cufft_single_interface.cc:int dfft_cuda_create_nd_plan(
hoomd/extern/dfftlib/src/cufft_single_interface.cc:    cuda_plan_t *plan,
hoomd/extern/dfftlib/src/cufft_single_interface.cc:int dfft_cuda_allocate_aligned_memory(cuda_cpx_t **ptr, size_t size)
hoomd/extern/dfftlib/src/cufft_single_interface.cc:void dfft_cuda_free_aligned_memory(cuda_cpx_t *ptr)
hoomd/extern/dfftlib/src/cufft_single_interface.cc:int dfft_cuda_destroy_local_plan(cuda_plan_t *p)
hoomd/extern/dfftlib/src/cufft_single_interface.cc:int dfft_cuda_local_fft(
hoomd/extern/dfftlib/src/cufft_single_interface.cc:    cuda_cpx_t *in,
hoomd/extern/dfftlib/src/cufft_single_interface.cc:    cuda_cpx_t *out,
hoomd/extern/dfftlib/src/cufft_single_interface.cc:    cuda_plan_t p,
hoomd/extern/jitify.hpp: * Copyright (c) 2017-2020, NVIDIA CORPORATION. All rights reserved.
hoomd/extern/jitify.hpp: * * Neither the name of NVIDIA CORPORATION nor the names of its
hoomd/extern/jitify.hpp:  A C++ library for easy integration of CUDA runtime compilation into
hoomd/extern/jitify.hpp:  Linker dependencies:   dl cuda nvrtc
hoomd/extern/jitify.hpp: *  \p Use class jitify::JitCache to manage and launch JIT-compiled CUDA
hoomd/extern/jitify.hpp:#include <cuda.h>
hoomd/extern/jitify.hpp:#include <cuda_runtime_api.h>  // For dim3, cudaStream_t
hoomd/extern/jitify.hpp:#if CUDA_VERSION >= 8000
hoomd/extern/jitify.hpp:  source = "#define cudaDeviceSynchronize() cudaSuccess\n" + source;
hoomd/extern/jitify.hpp:  ////source = "cudaError_t cudaDeviceSynchronize() { return cudaSuccess; }\n" +
hoomd/extern/jitify.hpp://#if CUDA_VERSION < 8000
hoomd/extern/jitify.hpp:inline std::string demangle_cuda_symbol(const char* mangled_name) {
hoomd/extern/jitify.hpp:  // We don't have a way to demangle CUDA symbol names under MSVC.
hoomd/extern/jitify.hpp:inline std::string demangle_cuda_symbol(const char* mangled_name) {
hoomd/extern/jitify.hpp:  return demangle_cuda_symbol(typeinfo.name());
hoomd/extern/jitify.hpp://#endif // CUDA_VERSION < 8000
hoomd/extern/jitify.hpp:    //#if CUDA_VERSION < 8000
hoomd/extern/jitify.hpp:inline CUjitInputType get_cuda_jit_input_type(std::string* filename) {
hoomd/extern/jitify.hpp:class CUDAKernel {
hoomd/extern/jitify.hpp:  inline void cuda_safe_call(CUresult res) const {
hoomd/extern/jitify.hpp:    if (res != CUDA_SUCCESS) {
hoomd/extern/jitify.hpp:      cuda_safe_call(cuLinkCreate((unsigned)_opts.size(), _opts.data(),
hoomd/extern/jitify.hpp:      cuda_safe_call(cuLinkAddData(_link_state, CU_JIT_INPUT_PTX,
hoomd/extern/jitify.hpp:          jit_input_type = get_cuda_jit_input_type(&link_file);
hoomd/extern/jitify.hpp:        while (result == CUDA_ERROR_FILE_NOT_FOUND &&
hoomd/extern/jitify.hpp:        if (result == CUDA_ERROR_FILE_NOT_FOUND) {
hoomd/extern/jitify.hpp:        } else if (result != CUDA_SUCCESS) {
hoomd/extern/jitify.hpp:        cuda_safe_call(result);
hoomd/extern/jitify.hpp:      if (result == CUDA_SUCCESS) {
hoomd/extern/jitify.hpp:              << reflection::detail::demangle_cuda_symbol(_func_name.c_str())
hoomd/extern/jitify.hpp:    cuda_safe_call(result);
hoomd/extern/jitify.hpp:      cuda_safe_call(
hoomd/extern/jitify.hpp:      cuda_safe_call(cuLinkDestroy(_link_state));
hoomd/extern/jitify.hpp:  inline CUDAKernel() : _link_state(0), _module(0), _kernel(0) {}
hoomd/extern/jitify.hpp:  inline CUDAKernel(const CUDAKernel& other) = delete;
hoomd/extern/jitify.hpp:  inline CUDAKernel& operator=(const CUDAKernel& other) = delete;
hoomd/extern/jitify.hpp:  inline CUDAKernel(CUDAKernel&& other) = delete;
hoomd/extern/jitify.hpp:  inline CUDAKernel& operator=(CUDAKernel&& other) = delete;
hoomd/extern/jitify.hpp:  inline CUDAKernel(const char* func_name, const char* ptx,
hoomd/extern/jitify.hpp:  inline CUDAKernel& set(const char* func_name, const char* ptx,
hoomd/extern/jitify.hpp:  inline ~CUDAKernel() { this->destroy_module(); }
hoomd/extern/jitify.hpp:    return cuda_safe_call(cuLaunchKernel(_kernel, grid.x, grid.y, grid.z,
hoomd/extern/jitify.hpp:    cuda_safe_call(cuFuncGetAttribute(&value, attribute, _kernel));
hoomd/extern/jitify.hpp:    cuda_safe_call(cuFuncSetAttribute(_kernel, attribute, value));
hoomd/extern/jitify.hpp:      cuda_safe_call(cuModuleGetGlobal(&global_ptr, size, _module,
hoomd/extern/jitify.hpp:    // Note: Global namespace already includes CUDA math funcs
hoomd/extern/jitify.hpp:inline void detect_and_add_cuda_arch(std::vector<std::string>& options) {
hoomd/extern/jitify.hpp:    // Note that this will also match the middle of "--gpu-architecture".
hoomd/extern/jitify.hpp:  cudaError_t status;
hoomd/extern/jitify.hpp:  status = cudaGetDevice(&device);
hoomd/extern/jitify.hpp:  if (status != cudaSuccess) {
hoomd/extern/jitify.hpp:            "Failed to detect GPU architecture: cudaGetDevice failed: ") +
hoomd/extern/jitify.hpp:        cudaGetErrorString(status));
hoomd/extern/jitify.hpp:  cudaDeviceGetAttribute(&cc_major, cudaDevAttrComputeCapabilityMajor, device);
hoomd/extern/jitify.hpp:  cudaDeviceGetAttribute(&cc_minor, cudaDevAttrComputeCapabilityMinor, device);
hoomd/extern/jitify.hpp:  //         on older versions of CUDA.
hoomd/extern/jitify.hpp:    // ensure that future CUDA versions just work (even if suboptimal)
hoomd/extern/jitify.hpp:    const int cuda_major = std::min(10, CUDA_VERSION / 1000);
hoomd/extern/jitify.hpp:    switch (cuda_major) {
hoomd/extern/jitify.hpp:        throw std::runtime_error("Unexpected CUDA major version " +
hoomd/extern/jitify.hpp:                                 std::to_string(cuda_major));
hoomd/extern/jitify.hpp:  // TODO: This WAR is expected to be unnecessary as of CUDA > 10.2.
hoomd/extern/jitify.hpp:#if CUDA_VERSION < 8000
hoomd/extern/jitify.hpp:    // WAR for no nvrtcAddNameExpression before CUDA 8.0
hoomd/extern/jitify.hpp:#if CUDA_VERSION >= 8000
hoomd/extern/jitify.hpp:#if CUDA_VERSION >= 8000
hoomd/extern/jitify.hpp:inline void load_program(std::string const& cuda_source,
hoomd/extern/jitify.hpp:  if (!detail::load_source(cuda_source, *program_sources, "", *include_paths,
hoomd/extern/jitify.hpp:    throw std::runtime_error("Source not found: " + cuda_source);
hoomd/extern/jitify.hpp:  detail::detect_and_add_cuda_arch(compiler_options);
hoomd/extern/jitify.hpp:  if (res != CUDA_SUCCESS) {
hoomd/extern/jitify.hpp:  jitify::ObjectCache<key_type, detail::CUDAKernel> _kernel_cache;
hoomd/extern/jitify.hpp:    // Bootstrap the cuda context to avoid errors
hoomd/extern/jitify.hpp:    cudaFree(0);
hoomd/extern/jitify.hpp:  detail::CUDAKernel* _cuda_kernel;
hoomd/extern/jitify.hpp:  detail::CUDAKernel const& cuda_kernel() const { return *_cuda_kernel; }
hoomd/extern/jitify.hpp:  cudaStream_t _stream;
hoomd/extern/jitify.hpp:                             cudaStream_t stream = 0)
hoomd/extern/jitify.hpp:                        cudaStream_t stream = 0);
hoomd/extern/jitify.hpp:  /*! Launch the kernel and check for cuda errors.
hoomd/extern/jitify.hpp:  /*! Launch the kernel and check for cuda errors.
hoomd/extern/jitify.hpp:   * \note This allows use of CUDA APIs like
hoomd/extern/jitify.hpp:  inline operator CUfunction() const { return _impl->cuda_kernel(); }
hoomd/extern/jitify.hpp:                                   cudaStream_t stream = 0) const {
hoomd/extern/jitify.hpp:   *  \param stream The CUDA stream to launch the kernel in.
hoomd/extern/jitify.hpp:                                  cudaStream_t stream = 0) const {
hoomd/extern/jitify.hpp:   * \param stream The CUDA stream to launch the kernel in.
hoomd/extern/jitify.hpp:      CUoccupancyB2DSize smem_callback = 0, cudaStream_t stream = 0,
hoomd/extern/jitify.hpp:    CUfunction func = _impl->cuda_kernel();
hoomd/extern/jitify.hpp:    return _impl->cuda_kernel().get_func_attribute(attribute);
hoomd/extern/jitify.hpp:    _impl->cuda_kernel().set_func_attribute(attribute, value);
hoomd/extern/jitify.hpp:    return _impl->cuda_kernel().get_global_ptr(name, size);
hoomd/extern/jitify.hpp:    return _impl->cuda_kernel().get_global_data(name, data, count, stream);
hoomd/extern/jitify.hpp:    return _impl->cuda_kernel().set_global_data(name, data, count, stream);
hoomd/extern/jitify.hpp:    return _impl->cuda_kernel().function_name();
hoomd/extern/jitify.hpp:  const std::string& ptx() const { return _impl->cuda_kernel().ptx(); }
hoomd/extern/jitify.hpp:    return _impl->cuda_kernel().link_files();
hoomd/extern/jitify.hpp:    return _impl->cuda_kernel().link_paths();
hoomd/extern/jitify.hpp:/*! An object that manages a cache of JIT-compiled CUDA kernels.
hoomd/extern/jitify.hpp:                                      cudaStream_t stream)
hoomd/extern/jitify.hpp:  if (!_kernel_inst._cuda_kernel) {
hoomd/extern/jitify.hpp:  return _kernel_inst._cuda_kernel->launch(_grid, _block, _smem, _stream,
hoomd/extern/jitify.hpp:  _kernel_inst._cuda_kernel->safe_launch(_grid, _block, _smem, _stream,
hoomd/extern/jitify.hpp:    _cuda_kernel = &cache._kernel_cache.get(cache_key);
hoomd/extern/jitify.hpp:    _cuda_kernel = &cache._kernel_cache.emplace(cache_key);
hoomd/extern/jitify.hpp:  _cuda_kernel->set(mangled_instantiation.c_str(), ptx.c_str(), linker_files,
hoomd/extern/jitify.hpp:  detail::detect_and_add_cuda_arch(_options);
hoomd/extern/jitify.hpp: *  \param stream        The CUDA stream on which to execute.
hoomd/extern/jitify.hpp: *  \param block_size    The size of the CUDA thread block with which to
hoomd/extern/jitify.hpp:  /*! CUDA stream on which to execute.*/
hoomd/extern/jitify.hpp:  cudaStream_t stream;
hoomd/extern/jitify.hpp:  /*! CUDA device on which to execute.*/
hoomd/extern/jitify.hpp:  /*! CUDA block size with which to execute.*/
hoomd/extern/jitify.hpp:                  cudaStream_t stream_ = 0, int device_ = 0,
hoomd/extern/jitify.hpp://       Add compile guard for NOCUDA compilation
hoomd/extern/jitify.hpp:    return CUDA_SUCCESS;  // FIXME - replace with non-CUDA enum type?
hoomd/extern/jitify.hpp:  cudaSetDevice(policy.device);
hoomd/extern/jitify.hpp:  Program(std::string const& cuda_source,
hoomd/extern/jitify.hpp:    detail::load_program(cuda_source, headers, file_callback, &include_paths,
hoomd/extern/jitify.hpp:  std::unique_ptr<detail::CUDAKernel> _cuda_kernel;
hoomd/extern/jitify.hpp:      : _cuda_kernel(new detail::CUDAKernel(func_name.c_str(), ptx.c_str(),
hoomd/extern/jitify.hpp:    detail::detect_and_add_cuda_arch(options);
hoomd/extern/jitify.hpp:    _cuda_kernel.reset(new detail::CUDAKernel(mangled_instantiation.c_str(),
hoomd/extern/jitify.hpp:   * \note This allows use of CUDA APIs like
hoomd/extern/jitify.hpp:  operator CUfunction() const { return *_cuda_kernel; }
hoomd/extern/jitify.hpp:        _cuda_kernel->function_name(), _cuda_kernel->ptx(),
hoomd/extern/jitify.hpp:        _cuda_kernel->link_files(), _cuda_kernel->link_paths());
hoomd/extern/jitify.hpp:   *  \param stream The CUDA stream to launch the kernel in.
hoomd/extern/jitify.hpp:                           cudaStream_t stream = 0) const;
hoomd/extern/jitify.hpp:   * \param stream The CUDA stream to launch the kernel in.
hoomd/extern/jitify.hpp:      CUoccupancyB2DSize smem_callback = 0, cudaStream_t stream = 0,
hoomd/extern/jitify.hpp:    return _cuda_kernel->get_func_attribute(attribute);
hoomd/extern/jitify.hpp:    _cuda_kernel->set_func_attribute(attribute, value);
hoomd/extern/jitify.hpp:    return _cuda_kernel->get_global_ptr(name, size);
hoomd/extern/jitify.hpp:    return _cuda_kernel->get_global_data(name, data, count, stream);
hoomd/extern/jitify.hpp:    return _cuda_kernel->set_global_data(name, data, count, stream);
hoomd/extern/jitify.hpp:    return _cuda_kernel->function_name();
hoomd/extern/jitify.hpp:  const std::string& ptx() const { return _cuda_kernel->ptx(); }
hoomd/extern/jitify.hpp:    return _cuda_kernel->link_files();
hoomd/extern/jitify.hpp:    return _cuda_kernel->link_paths();
hoomd/extern/jitify.hpp:  cudaStream_t _stream;
hoomd/extern/jitify.hpp:    std::cout << "Launching " << _kernel_inst->_cuda_kernel->function_name()
hoomd/extern/jitify.hpp:                 unsigned int smem = 0, cudaStream_t stream = 0)
hoomd/extern/jitify.hpp:    return _kernel_inst->_cuda_kernel->launch(_grid, _block, _smem, _stream,
hoomd/extern/jitify.hpp:    _kernel_inst->_cuda_kernel->safe_launch(_grid, _block, _smem, _stream,
hoomd/extern/jitify.hpp:  /*! Launch the kernel and check for cuda errors.
hoomd/extern/jitify.hpp:    dim3 grid, dim3 block, unsigned int smem, cudaStream_t stream) const {
hoomd/extern/jitify.hpp:    cudaStream_t stream, unsigned int flags) const {
hoomd/extern/jitify.hpp:  CUfunction func = *_cuda_kernel;
hoomd/example_plugins/shape_plugin/ShapeMySphere.h:    /// Set CUDA memory hints
hoomd/example_plugins/shape_plugin/CMakeLists.txt:# Specify any CUDA sources
hoomd/example_plugins/shape_plugin/CMakeLists.txt:set(_cuda_sources ${_${COMPONENT_NAME}_cu_sources}
hoomd/example_plugins/shape_plugin/CMakeLists.txt:hoomd_add_module(_${COMPONENT_NAME} SHARED ${_${COMPONENT_NAME}_sources} ${_cuda_sources} NO_EXTRAS)
hoomd/example_plugins/shape_plugin/kernels.cu:#include "hoomd/hpmc/ComputeFreeVolumeGPU.cuh"
hoomd/example_plugins/shape_plugin/kernels.cu:#include "hoomd/hpmc/IntegratorHPMCMonoGPU.cuh"
hoomd/example_plugins/shape_plugin/kernels.cu:#include "hoomd/hpmc/IntegratorHPMCMonoGPUDepletants.cuh"
hoomd/example_plugins/shape_plugin/kernels.cu:#include "hoomd/hpmc/IntegratorHPMCMonoGPUDepletantsAuxilliaryPhase1.cuh"
hoomd/example_plugins/shape_plugin/kernels.cu:#include "hoomd/hpmc/IntegratorHPMCMonoGPUDepletantsAuxilliaryPhase2.cuh"
hoomd/example_plugins/shape_plugin/kernels.cu:#include "hoomd/hpmc/IntegratorHPMCMonoGPUMoves.cuh"
hoomd/example_plugins/shape_plugin/kernels.cu:#include "hoomd/hpmc/UpdaterClustersGPU.cuh"
hoomd/example_plugins/shape_plugin/kernels.cu:#include "hoomd/hpmc/UpdaterClustersGPUDepletants.cuh"
hoomd/example_plugins/shape_plugin/kernels.cu:gpu_hpmc_free_volume<ShapeMySphere>(const hpmc_free_volume_args_t& args,
hoomd/example_plugins/shape_plugin/kernels.cu:namespace gpu
hoomd/example_plugins/shape_plugin/kernels.cu:    } // namespace gpu
hoomd/example_plugins/shape_plugin/module.cc:#include "hoomd/hpmc/ComputeFreeVolumeGPU.h"
hoomd/example_plugins/shape_plugin/module.cc:#include "hoomd/hpmc/IntegratorHPMCMonoGPU.h"
hoomd/example_plugins/shape_plugin/module.cc:#include "hoomd/hpmc/UpdaterClustersGPU.h"
hoomd/example_plugins/shape_plugin/module.cc:    export_IntegratorHPMCMonoGPU<ShapeMySphere>(m, "IntegratorHPMCMonoMySphereGPU");
hoomd/example_plugins/shape_plugin/module.cc:    export_ComputeFreeVolumeGPU<ShapeMySphere>(m, "ComputeFreeVolumeMySphereGPU");
hoomd/example_plugins/shape_plugin/module.cc:    export_UpdaterClustersGPU<ShapeMySphere>(m, "UpdaterClustersMySphereGPU");
hoomd/example_plugins/pair_plugin/CMakeLists.txt:# Specify any CUDA sources
hoomd/example_plugins/pair_plugin/CMakeLists.txt:    PotentialPairExampleGPUKernel.cu
hoomd/example_plugins/pair_plugin/CMakeLists.txt:set(_cuda_sources ${_${COMPONENT_NAME}_cu_sources})
hoomd/example_plugins/pair_plugin/CMakeLists.txt:hoomd_add_module(_${COMPONENT_NAME} SHARED ${_${COMPONENT_NAME}_sources} ${_cuda_sources} NO_EXTRAS)
hoomd/example_plugins/pair_plugin/module.cc:#include "hoomd/md/PotentialPairGPU.h"
hoomd/example_plugins/pair_plugin/module.cc:    detail::export_PotentialPairGPU<EvaluatorPairExample>(m, "PotentialPairExampleGPU");
hoomd/example_plugins/pair_plugin/EvaluatorPairExample.h:        //! Set CUDA memory hints
hoomd/example_plugins/pair_plugin/PotentialPairExampleGPUKernel.cu:#include "hoomd/md/PotentialPairGPU.cuh"
hoomd/example_plugins/pair_plugin/PotentialPairExampleGPUKernel.cu:gpu_compute_pair_forces<EvaluatorPairExample>(const pair_args_t& pair_args,
hoomd/example_plugins/updater_plugin/ExampleUpdater.h:// but not if we are compiling GPU kernels
hoomd/example_plugins/updater_plugin/ExampleUpdater.h:// Third, this class offers a GPU accelerated method in order to demonstrate how to include CUDA
hoomd/example_plugins/updater_plugin/ExampleUpdater.h://! A GPU accelerated nonsense particle updater written to demonstrate how to write a plugin w/ CUDA
hoomd/example_plugins/updater_plugin/ExampleUpdater.h:/*! This updater simply sets all of the particle's velocities to 0 (on the GPU) when update() is
hoomd/example_plugins/updater_plugin/ExampleUpdater.h:class ExampleUpdaterGPU : public ExampleUpdater
hoomd/example_plugins/updater_plugin/ExampleUpdater.h:    ExampleUpdaterGPU(std::shared_ptr<SystemDefinition> sysdef, std::shared_ptr<Trigger> trigger);
hoomd/example_plugins/updater_plugin/ExampleUpdater.h://! Export the ExampleUpdaterGPU class to python
hoomd/example_plugins/updater_plugin/ExampleUpdater.h:void export_ExampleUpdaterGPU(pybind11::module& m);
hoomd/example_plugins/updater_plugin/pytest/test_example_updater.py:    # handle iterating tests over different CPU and GPU devices.
hoomd/example_plugins/updater_plugin/CMakeLists.txt:# Specify any CUDA sources
hoomd/example_plugins/updater_plugin/CMakeLists.txt:set(_cuda_sources ${_${COMPONENT_NAME}_cu_sources})
hoomd/example_plugins/updater_plugin/CMakeLists.txt:hoomd_add_module(_${COMPONENT_NAME} SHARED ${_${COMPONENT_NAME}_sources} ${_cuda_sources} NO_EXTRAS)
hoomd/example_plugins/updater_plugin/update.py:            self._cpp_obj = _updater_plugin.ExampleUpdaterGPU(
hoomd/example_plugins/updater_plugin/module.cc:    export_ExampleUpdaterGPU(m);
hoomd/example_plugins/updater_plugin/ExampleUpdater.cc:// here follows the code for ExampleUpdater on the GPU
hoomd/example_plugins/updater_plugin/ExampleUpdater.cc:ExampleUpdaterGPU::ExampleUpdaterGPU(std::shared_ptr<SystemDefinition> sysdef,
hoomd/example_plugins/updater_plugin/ExampleUpdater.cc:void ExampleUpdaterGPU::update(uint64_t timestep)
hoomd/example_plugins/updater_plugin/ExampleUpdater.cc:    // access the particle data arrays for writing on the GPU
hoomd/example_plugins/updater_plugin/ExampleUpdater.cc:    kernel::gpu_zero_velocities(d_vel.data, m_pdata->getN());
hoomd/example_plugins/updater_plugin/ExampleUpdater.cc:    // check for error codes from the GPU if error checking is enabled
hoomd/example_plugins/updater_plugin/ExampleUpdater.cc:    if (m_exec_conf->isCUDAErrorCheckingEnabled())
hoomd/example_plugins/updater_plugin/ExampleUpdater.cc:        CHECK_CUDA_ERROR();
hoomd/example_plugins/updater_plugin/ExampleUpdater.cc:/* Export the GPU updater to be visible in the python module
hoomd/example_plugins/updater_plugin/ExampleUpdater.cc:void export_ExampleUpdaterGPU(pybind11::module& m)
hoomd/example_plugins/updater_plugin/ExampleUpdater.cc:    pybind11::class_<ExampleUpdaterGPU, ExampleUpdater, std::shared_ptr<ExampleUpdaterGPU>>(
hoomd/example_plugins/updater_plugin/ExampleUpdater.cc:        "ExampleUpdaterGPU")
hoomd/example_plugins/updater_plugin/ExampleUpdater.cu:    \brief CUDA kernels for ExampleUpdater
hoomd/example_plugins/updater_plugin/ExampleUpdater.cu:// First, the kernel code for zeroing the velocities on the GPU
hoomd/example_plugins/updater_plugin/ExampleUpdater.cu://! Kernel that zeroes velocities on the GPU
hoomd/example_plugins/updater_plugin/ExampleUpdater.cu:__global__ void gpu_zero_velocities_kernel(Scalar4* d_vel, unsigned int N)
hoomd/example_plugins/updater_plugin/ExampleUpdater.cu:    This is just a driver for gpu_zero_velocities_kernel(), see it for the details
hoomd/example_plugins/updater_plugin/ExampleUpdater.cu:hipError_t gpu_zero_velocities(Scalar4* d_vel, unsigned int N)
hoomd/example_plugins/updater_plugin/ExampleUpdater.cu:    hipLaunchKernelGGL(gpu_zero_velocities_kernel, dim3(grid), dim3(threads), 0, 0, d_vel, N);
hoomd/example_plugins/updater_plugin/ExampleUpdater.cu:    // this method always succeeds. If you had a cuda* call in this driver, you could return its
hoomd/example_plugins/updater_plugin/ExampleUpdater.cuh:    \brief Declaration of CUDA kernels for ExampleUpdater
hoomd/example_plugins/updater_plugin/ExampleUpdater.cuh://! Zeros velocities on the GPU
hoomd/example_plugins/updater_plugin/ExampleUpdater.cuh:hipError_t gpu_zero_velocities(Scalar4* d_vel, unsigned int N);
README.md:**HOOMD-blue** is a Python package that runs simulations of particle systems on CPUs and GPUs. It
README.md:gpu = hoomd.device.GPU()
README.md:sim = hoomd.Simulation(device=gpu)
hoomd-config.cmake.in:set(HOOMD_GPU_PLATFORM "@HOOMD_GPU_PLATFORM@")
hoomd-config.cmake.in:# CUDA architectures
hoomd-config.cmake.in:set(CMAKE_CUDA_ARCHITECTURES "@CMAKE_CUDA_ARCHITECTURES@")
hoomd-config.cmake.in:set(CMAKE_CUDA_STANDARD "@CMAKE_CUDA_STANDARD@")
hoomd-config.cmake.in:    set(CMAKE_CUDA_FLAGS "${CMAKE_CUDA_FLAGS} @CMAKE_CUDA_FLAGS@")
CMakeLists.txt:# >= 3.8 is required for CUDA language support
CMakeLists.txt:# Since we support the last two major releases of the CUDA toolkit, we cannot
CMakeLists.txt:# use C++17 as the CUDA standard until CUDA 12.
CMakeLists.txt:set(CMAKE_CUDA_STANDARD 14)
CMakeLists.txt:    if (NOT (ENABLE_GPU AND HOOMD_GPU_PLATFORM STREQUAL "HIP"))
CMakeLists.txt:OPTION(ENABLE_GPU "True if we are compiling for a GPU target" FALSE)
CMakeLists.txt:SET(ENABLE_HIP ${ENABLE_GPU})
CMakeLists.txt:set(HOOMD_GPU_PLATFORM "CUDA" CACHE STRING "Choose the GPU backend: HIP or CUDA.")
CMakeLists.txt:if (ENABLE_GPU AND HOOMD_GPU_PLATFORM STREQUAL "HIP")
CMakeLists.txt:message("Defaulting BUILD_MPCD=off due to HIP GPU platform.")
CMakeLists.txt:# this needs to go before CUDA setup
CMakeLists.txt:# Find CUDA and set it up
CMakeLists.txt:include (HOOMDCUDASetup)
CMakeLists.txt:              CMake/hoomd/FindCUDALibs.cmake
hoomd.code-workspace:            "cuda",
hoomd.code-workspace:            "GPUs",
hoomd.code-workspace:            "HOOMDGPU",
INSTALLING.rst:You must build **HOOMD-blue** from source to enable support for the native **MPI** and **CUDA**
INSTALLING.rst:Serial CPU and single GPU builds
INSTALLING.rst:**HOOMD-blue** binaries for **serial CPU** and **single GPU** are available on conda-forge_ for the
INSTALLING.rst:By default, micromamba auto-detects whether your system has a GPU and attempts to install the
INSTALLING.rst:appropriate package. Override this and force the GPU enabled package installation with:
INSTALLING.rst:    export CONDA_OVERRIDE_CUDA="12.0"
INSTALLING.rst:    micromamba install "hoomd=4.9.1=*gpu*" "cuda-version=12.0"
INSTALLING.rst:    CUDA 11.8 compatible packages are also available. Replace "12.0" with "11.8" above when
INSTALLING.rst:    installing HOOMD-blue on systems with CUDA 11 compatible drivers.
CONTRIBUTING.rst:Optimize for the current GPU generation
CONTRIBUTING.rst:Write, test, and optimize your GPU kernels on the latest generation of GPUs.
sphinx-doc/howto/determine-the-most-efficient-device.py:# Wait until GPU kernel parameter autotuning is complete.
sphinx-doc/howto/determine-the-most-efficient-device.py:if args.device == 'GPU':
sphinx-doc/howto/choose-the-neighbor-list-buffer-distance.py:# Complete GPU kernel autotuning before making sensitive timing measurements.
sphinx-doc/howto/choose-the-neighbor-list-buffer-distance.py:if isinstance(device, hoomd.device.GPU):
sphinx-doc/howto/determine-the-most-efficient-device.rst:available hardware all impact the resulting performance. When benchmarking, make sure that all GPU
sphinx-doc/howto/determine-the-most-efficient-device.rst:On AMD EPYC 7742 (PSC Bridges-2) and NVIDIA A100 (NCSA Delta), this script reports
sphinx-doc/howto/determine-the-most-efficient-device.rst:   * - GPU
sphinx-doc/howto/determine-the-most-efficient-device.rst:Some HPC resources may assign an effective cost to GPUs. When this is not the case, use the ratio of
sphinx-doc/howto/determine-the-most-efficient-device.rst:available GPU hours to CPU core hours as a substitute. This example will assign a relative cost of
sphinx-doc/howto/determine-the-most-efficient-device.rst:1 GPU hour to 64 CPU core hours. The efficiency is:
sphinx-doc/howto/determine-the-most-efficient-device.rst:    \frac{S_\mathrm{P\ GPUs}}{S_\mathrm{1\ CPU}} \cdot \frac{1}{64 P} & \mathrm{GPU} \\
sphinx-doc/howto/determine-the-most-efficient-device.rst:With 2048 particles in this example, the CPU is always more efficient than the GPU and the CPU is
sphinx-doc/howto/determine-the-most-efficient-device.rst:faster than the GPU when :math:`P \ge 16`. Therefore, the CPU is always the optimal choice. Choose a
sphinx-doc/howto/determine-the-most-efficient-device.rst:   * - GPU
sphinx-doc/howto/determine-the-most-efficient-device.rst:At a this system size, the GPU is always both faster and more efficient than the CPU.
sphinx-doc/howto/determine-the-most-efficient-device.rst:Compare the two examples and notice that the TPS achieved by the GPU is only cut in half when the
sphinx-doc/howto/determine-the-most-efficient-device.rst:to utilize all the parallel processing units on the GPU.
sphinx-doc/license.rst:`CUDA <https://developer.nvidia.com/cuda-downloads>`_, used under the
sphinx-doc/license.rst:`NVIDIA Software License Agreement <https://docs.nvidia.com/cuda/eula/index.html>`_.
sphinx-doc/license.rst:`CUB <https://nvidia.github.io/cccl/cub/>`_, used under the following license::
sphinx-doc/license.rst:    Copyright (c) 2011-2016, NVIDIA CORPORATION.  All rights reserved.
sphinx-doc/license.rst:        * Neither the name of the NVIDIA CORPORATION nor the
sphinx-doc/license.rst:    DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
sphinx-doc/license.rst:A CUDA `neighbor <https://github.com/mphoward/neighbor>`_ search library,
sphinx-doc/license.rst:`HIP <https://github.com/ROCm-Developer-Tools/HIP>`_ is included under the
sphinx-doc/license.rst:`hipCUB <https://github.com/ROCmSoftwarePlatform/hipCUB/>`_, used under the
sphinx-doc/license.rst:    Copyright (c) 2011-2018, NVIDIA CORPORATION.  All rights reserved.
sphinx-doc/license.rst:       *  Neither the name of the NVIDIA CORPORATION nor the
sphinx-doc/license.rst:    DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
sphinx-doc/license.rst:`rocPRIM <https://github.com/ROCmSoftwarePlatform/rocPRIM>`_, used under the
sphinx-doc/license.rst:`rocThrust <https://github.com/ROCmSoftwarePlatform/rocThrust>`_, used under
sphinx-doc/license.rst:`rocFFT <https://github.com/ROCmSoftwarePlatform/rocFFT>`_, used under the
sphinx-doc/license.rst:HOOMD-blue uses headers from `jitify <https://github.com/NVIDIA/jitify>`_ under
sphinx-doc/license.rst:    Copyright (c) 2017-2020, NVIDIA CORPORATION. All rights reserved.
sphinx-doc/license.rst:    * Neither the name of NVIDIA CORPORATION nor the names of its
sphinx-doc/license.rst:    ECL-CC code: ECL-CC is a connected components algorithm. The CUDA
sphinx-doc/module-hoomd-error.rst:    GPUNotAvailableError
sphinx-doc/module-hoomd-error.rst:              GPUNotAvailableError
sphinx-doc/citing.rst:    dynamics simulations on GPUs. Computer Physics Communications 192: 97-107,
sphinx-doc/citing.rst:Intra-node scaling on multiple GPUs:
sphinx-doc/citing.rst:When including historical development of HOOMD-blue, or noting that HOOMD-blue was first implemented on GPUs, please also cite:
sphinx-doc/citing.rst:    M. Spellings, R. L. Marson, J. A. Anderson, and S. C. Glotzer. GPU
sphinx-doc/citing.rst:    parallel GPU acceleration Computer Physics Communications 230: 10-20, Sep.
sphinx-doc/citing.rst:    simulations on GPU devices Journal of Computational Physics 230(19):
sphinx-doc/citing.rst:    processes in the condensed matter on GPUs. Computer Physics Communications
sphinx-doc/citing.rst:    GPU-accelerated molecular dynamics software. Journal of Computational
sphinx-doc/citing.rst:    Klein. Micellization studied by GPU-accelerated coarse-grained molecular
sphinx-doc/module-hoomd-data.rst:    LocalSnapshotGPU
sphinx-doc/module-hoomd-data.rst:    :synopsis: Provide access in Python to data buffers on CPU or GPU.
sphinx-doc/module-hoomd-data.rst:    .. autoclass:: LocalSnapshotGPU
sphinx-doc/style.rst:C++/CUDA
sphinx-doc/index.rst:**HOOMD-blue** is a Python package that runs simulations of particle systems on CPUs and GPUs. It
sphinx-doc/index.rst:    gpu = hoomd.device.GPU()
sphinx-doc/index.rst:    sim = hoomd.Simulation(device=gpu)
sphinx-doc/features.rst:CPU and GPU devices
sphinx-doc/features.rst:HOOMD-blue can execute simulations on CPUs or GPUs. Typical simulations run more efficiently on
sphinx-doc/features.rst:GPUs for system sizes larger than a few thousand particles, although this strongly depends on the
sphinx-doc/features.rst:details of the simulation. The provided binaries support NVIDIA GPUs. Build from source to enable
sphinx-doc/features.rst:preliminary support for AMD GPUs. CPU support is always enabled. GPU support must be enabled at
sphinx-doc/features.rst:compile time with the ``ENABLE_GPU`` CMake option (see :doc:`building`). Select the device to use at
sphinx-doc/features.rst:**all** operations and methods support GPU execution. At runtime, `hoomd.version.gpu_enabled` indicates
sphinx-doc/features.rst:whether the build supports GPU devices.
sphinx-doc/features.rst:HOOMD-blue automatically tunes kernel parameters to improve performance when executing on a GPU
sphinx-doc/features.rst:more than one CPU core or GPU. Unless otherwise stated in the documentation, **all** operations and
sphinx-doc/features.rst:supports both CPUs and NVIDIA GPUs while `hpmc.external.user` only supports CPUs. Run time
sphinx-doc/testing.rst:These tests take hours to execute on many CPU cores or GPUs. Find HOOMD's validation tests in the
sphinx-doc/module-hoomd-device.rst:    GPU
sphinx-doc/module-hoomd-device.rst:        GPU,
sphinx-doc/deprecated.rst:* ``hoomd.util.GPUNotAvailableError`` (since 4.5.0).
sphinx-doc/deprecated.rst:  * use ``hoomd.error.GPUNotAvailableError``.
sphinx-doc/deprecated.rst:* Single-process multi-GPU code path (since 4.5.0)
sphinx-doc/deprecated.rst:* ``gpu_ids`` argument to ``GPU`` (since 4.5.0)
sphinx-doc/deprecated.rst:  * Use ``gpu_id``.
sphinx-doc/deprecated.rst:* ``GPU.devices`` (since 4.5.0)
sphinx-doc/module-hoomd-array.rst:    HOOMDGPUArray
sphinx-doc/module-hoomd-array.rst:    :members: HOOMDGPUArray
sphinx-doc/migrating.rst:* ``hoomd.device.GPU.memory_traceback`` parameter
sphinx-doc/migrating.rst:* HOOMD-blue v4 no longer builds on macOS with ``ENABLE_GPU=on``.
sphinx-doc/migrating.rst:* Remove ``fix_cudart_rpath(_${COMPONENT_NAME})`` from your components ``CMakeLists.txt``
sphinx-doc/migrating.rst:     - `hoomd.device.CPU` and `hoomd.device.GPU`
sphinx-doc/migrating.rst:     -  Enable GPU profiling with `hoomd.device.GPU.enable_profiling`.
sphinx-doc/migrating.rst:   * - Compute < 6.0 GPUs
sphinx-doc/migrating.rst:     - Compute >= 6.0 GPUs
sphinx-doc/migrating.rst:     - `device.CPU` / `device.GPU`
sphinx-doc/migrating.rst:* To compile with GPU support, use the option ``ENABLE_GPU=ON``.
sphinx-doc/migrating.rst:* HOOMD now uses native CUDA support in CMake. Use ``CMAKE_CUDA_COMPILER`` to
sphinx-doc/migrating.rst:  - Remove ``CUDA_COMPILE``.
sphinx-doc/migrating.rst:  - HPMC GPU kernels are now instantiated by template .cu files that are generated by CMake at
sphinx-doc/migrating.rst:  - All GPU code is now written with HIP to support NVIDIA and AMD GPUs.
sphinx-doc/module-md-data.rst:    ForceLocalAccessGPU
sphinx-doc/module-md-data.rst:    NeighborListLocalAccessGPU
sphinx-doc/module-md-data.rst:    :synopsis: Provide access in Python to force data buffers on CPU or GPU.
sphinx-doc/module-md-data.rst:    .. autoclass:: ForceLocalAccessGPU
sphinx-doc/module-md-data.rst:    .. autoclass:: NeighborListLocalAccessGPU
.mailmap:Vyas Ramasubramani <vyasr@nvidia.com> Vyas <vramasub@umich.edu>
.mailmap:Vyas Ramasubramani <vyasr@nvidia.com> vramasub <vramasub@umich.edu>
.mailmap:Vyas Ramasubramani <vyasr@nvidia.com> Vyas Ramasubramani <vyas.ramasubramani@gmail.com>
.mailmap:Vyas Ramasubramani <vyasr@nvidia.com> vyasr <vramasub@umich.edu>
.mailmap:Vyas Ramasubramani <vyasr@nvidia.com> Vyas Ramasubramani <vramasub@umich.edu>
BUILDING.rst:``ENABLE_MPI``, ``ENABLE_GPU``, ``ENABLE_TBB``, and ``ENABLE_LLVM`` each require additional
BUILDING.rst:**For GPU execution** (required when ``ENABLE_GPU=on``):
BUILDING.rst:- **NVIDIA CUDA Toolkit**
BUILDING.rst:- AMD ROCm
BUILDING.rst:    When ``ENABLE_GPU=on``, HOOMD-blue will default to CUDA. Set ``HOOMD_GPU_PLATFORM=HIP`` to
BUILDING.rst:- ``CMAKE_CUDA_COMPILER`` - Specify which ``nvcc`` or ``hipcc`` to build with.
BUILDING.rst:- ``ENABLE_GPU`` - When enabled, compiled GPU accelerated computations (default: ``off``).
BUILDING.rst:- ``HOOMD_GPU_PLATFORM`` - Choose either ``CUDA`` or ``HIP`` as a GPU backend (default: ``CUDA``).
BUILDING.rst:- ``ENABLE_MPI`` - Enable multi-processor/GPU simulations using MPI.
BUILDING.rst:  - When set to ``on``, multi-processor/multi-GPU simulations are supported.
BUILDING.rst:  - When set to ``off`` (the default), always run in single-processor/single-GPU mode.
BUILDING.rst:These options control CUDA compilation via ``nvcc``:
BUILDING.rst:- ``CUDA_ARCH_LIST`` - A semicolon-separated list of GPU architectures to compile.
ARCHITECTURE.md:* NVIDIA GPUs released in the four prior years.
ARCHITECTURE.md:* The most recent major **CUDA** toolkit version.
ARCHITECTURE.md:HOOMD-blue implements all operations on the CPU and GPU. The CPU implementation is vanilla C++, and
ARCHITECTURE.md:the GPU implementation uses HIP to support both AMD and NVIDIA GPUs. The `ExecutionConfiguration`
ARCHITECTURE.md:class selects the device (CPU, GPU, or multiple GPUs) and configures global execution options. Each
ARCHITECTURE.md:To minimize code duplication and to provide a common interface for both CPU and GPU code paths,
ARCHITECTURE.md:HOOMD defines the CPU implementation of an operation in `ClassName` and the GPU implementation in a
ARCHITECTURE.md:subclass `ClassNameGPU`. The base class defines the data structures, parameters, getter/setter
ARCHITECTURE.md:methods, initialization, and other common tasks. The GPU class overrides key methods to perform the
ARCHITECTURE.md:expensive part of the computation on the GPU. The GPU subclass may use alternate data structures for
ARCHITECTURE.md:HOOMD-blue uses MPI for domain decomposition simulations on multiple CPUs or GPUs. The
ARCHITECTURE.md:is a product of continual development since the year 2007 and has grown along with improving CUDA
ARCHITECTURE.md:  the CUDA vector types which are aligned properly to enable efficient vector load instructions on
ARCHITECTURE.md:  the GPU. Use these types to store arrays of vector data. Prefer the `2` and `4` size vectors as
ARCHITECTURE.md:* `GPUArray<T>` - Template array data type that stores two copies of the data, one on the CPU and
ARCHITECTURE.md:  one on the GPU. Use `ArrayHandle` to request a pointer to the data, which will copy the most
ARCHITECTURE.md:  access existing data structures that use `GPUArray`. New code **should not** define new `GPUArray`
ARCHITECTURE.md:* `GlobalArray<T>` - Template array data type that stores one copy of the data in CUDA's unified
ARCHITECTURE.md:  memory in single process, multi-GPU execution. When using a single GPU per process, falls back on
ARCHITECTURE.md:  `GPUArray`. Use `ArrayHandle` to access data in `GlobalArray`.
ARCHITECTURE.md:  CUDA's unifed memory. This data type is useful for parameter arrays that are exposed to Python.
ARCHITECTURE.md:`cudaMemadvise` to set the appropriate memory hints for the array. Small parameter arrays should be
ARCHITECTURE.md:set to `cudaMemAdviseSetReadMostly`. Larger arrays accessed in portions in single-process multi-GPU
ARCHITECTURE.md:execution should be set to `cudaMemAdviseSetPreferredLocation` appropriately for the different
ARCHITECTURE.md:the method (e.g. pair potential evaluation) to be implemented only twice (once on the GPU and once
ARCHITECTURE.md:2. Instantiate the GPU kernel driver with the evaluator.
ARCHITECTURE.md:## GPU kernel driver functions
ARCHITECTURE.md:Early versions of CUDA could compile only a minimal subset of C++ code. While the modern CUDA
ARCHITECTURE.md:even when the use of that code is used only in host code. To work around these cases, all GPU
ARCHITECTURE.md:It dynamically cycles through the possible parameters and records the performance of each using CUDA
ARCHITECTURE.md:executing. GPU code in HOOMD-blue should instantiate and use one `Autotuner` for each kernel.
ARCHITECTURE.md:HOOMD allows for C++ classes to expose their GPU and CPU data buffers directly
ARCHITECTURE.md:in Python using the `__cuda_array_interface__` and `__array_interface__`. This
example_plugins/shape_plugin/ShapeMySphere.h:    /// Set CUDA memory hints
example_plugins/shape_plugin/CMakeLists.txt:# Specify any CUDA sources
example_plugins/shape_plugin/CMakeLists.txt:set(_cuda_sources ${_${COMPONENT_NAME}_cu_sources}
example_plugins/shape_plugin/CMakeLists.txt:hoomd_add_module(_${COMPONENT_NAME} SHARED ${_${COMPONENT_NAME}_sources} ${_cuda_sources} NO_EXTRAS)
example_plugins/shape_plugin/kernels.cu:#include "hoomd/hpmc/ComputeFreeVolumeGPU.cuh"
example_plugins/shape_plugin/kernels.cu:#include "hoomd/hpmc/IntegratorHPMCMonoGPU.cuh"
example_plugins/shape_plugin/kernels.cu:#include "hoomd/hpmc/IntegratorHPMCMonoGPUDepletants.cuh"
example_plugins/shape_plugin/kernels.cu:#include "hoomd/hpmc/IntegratorHPMCMonoGPUDepletantsAuxilliaryPhase1.cuh"
example_plugins/shape_plugin/kernels.cu:#include "hoomd/hpmc/IntegratorHPMCMonoGPUDepletantsAuxilliaryPhase2.cuh"
example_plugins/shape_plugin/kernels.cu:#include "hoomd/hpmc/IntegratorHPMCMonoGPUMoves.cuh"
example_plugins/shape_plugin/kernels.cu:#include "hoomd/hpmc/UpdaterClustersGPU.cuh"
example_plugins/shape_plugin/kernels.cu:#include "hoomd/hpmc/UpdaterClustersGPUDepletants.cuh"
example_plugins/shape_plugin/kernels.cu:gpu_hpmc_free_volume<ShapeMySphere>(const hpmc_free_volume_args_t& args,
example_plugins/shape_plugin/kernels.cu:namespace gpu
example_plugins/shape_plugin/kernels.cu:    } // namespace gpu
example_plugins/shape_plugin/module.cc:#include "hoomd/hpmc/ComputeFreeVolumeGPU.h"
example_plugins/shape_plugin/module.cc:#include "hoomd/hpmc/IntegratorHPMCMonoGPU.h"
example_plugins/shape_plugin/module.cc:#include "hoomd/hpmc/UpdaterClustersGPU.h"
example_plugins/shape_plugin/module.cc:    export_IntegratorHPMCMonoGPU<ShapeMySphere>(m, "IntegratorHPMCMonoMySphereGPU");
example_plugins/shape_plugin/module.cc:    export_ComputeFreeVolumeGPU<ShapeMySphere>(m, "ComputeFreeVolumeMySphereGPU");
example_plugins/shape_plugin/module.cc:    export_UpdaterClustersGPU<ShapeMySphere>(m, "UpdaterClustersMySphereGPU");
example_plugins/pair_plugin/CMakeLists.txt:# Specify any CUDA sources
example_plugins/pair_plugin/CMakeLists.txt:    PotentialPairExampleGPUKernel.cu
example_plugins/pair_plugin/CMakeLists.txt:set(_cuda_sources ${_${COMPONENT_NAME}_cu_sources})
example_plugins/pair_plugin/CMakeLists.txt:hoomd_add_module(_${COMPONENT_NAME} SHARED ${_${COMPONENT_NAME}_sources} ${_cuda_sources} NO_EXTRAS)
example_plugins/pair_plugin/module.cc:#include "hoomd/md/PotentialPairGPU.h"
example_plugins/pair_plugin/module.cc:    detail::export_PotentialPairGPU<EvaluatorPairExample>(m, "PotentialPairExampleGPU");
example_plugins/pair_plugin/EvaluatorPairExample.h:        //! Set CUDA memory hints
example_plugins/pair_plugin/PotentialPairExampleGPUKernel.cu:#include "hoomd/md/PotentialPairGPU.cuh"
example_plugins/pair_plugin/PotentialPairExampleGPUKernel.cu:gpu_compute_pair_forces<EvaluatorPairExample>(const pair_args_t& pair_args,
example_plugins/updater_plugin/ExampleUpdater.h:// but not if we are compiling GPU kernels
example_plugins/updater_plugin/ExampleUpdater.h:// Third, this class offers a GPU accelerated method in order to demonstrate how to include CUDA
example_plugins/updater_plugin/ExampleUpdater.h://! A GPU accelerated nonsense particle updater written to demonstrate how to write a plugin w/ CUDA
example_plugins/updater_plugin/ExampleUpdater.h:/*! This updater simply sets all of the particle's velocities to 0 (on the GPU) when update() is
example_plugins/updater_plugin/ExampleUpdater.h:class ExampleUpdaterGPU : public ExampleUpdater
example_plugins/updater_plugin/ExampleUpdater.h:    ExampleUpdaterGPU(std::shared_ptr<SystemDefinition> sysdef, std::shared_ptr<Trigger> trigger);
example_plugins/updater_plugin/ExampleUpdater.h://! Export the ExampleUpdaterGPU class to python
example_plugins/updater_plugin/ExampleUpdater.h:void export_ExampleUpdaterGPU(pybind11::module& m);
example_plugins/updater_plugin/pytest/test_example_updater.py:    # handle iterating tests over different CPU and GPU devices.
example_plugins/updater_plugin/CMakeLists.txt:# Specify any CUDA sources
example_plugins/updater_plugin/CMakeLists.txt:set(_cuda_sources ${_${COMPONENT_NAME}_cu_sources})
example_plugins/updater_plugin/CMakeLists.txt:hoomd_add_module(_${COMPONENT_NAME} SHARED ${_${COMPONENT_NAME}_sources} ${_cuda_sources} NO_EXTRAS)
example_plugins/updater_plugin/update.py:            self._cpp_obj = _updater_plugin.ExampleUpdaterGPU(
example_plugins/updater_plugin/module.cc:    export_ExampleUpdaterGPU(m);
example_plugins/updater_plugin/ExampleUpdater.cc:// here follows the code for ExampleUpdater on the GPU
example_plugins/updater_plugin/ExampleUpdater.cc:ExampleUpdaterGPU::ExampleUpdaterGPU(std::shared_ptr<SystemDefinition> sysdef,
example_plugins/updater_plugin/ExampleUpdater.cc:void ExampleUpdaterGPU::update(uint64_t timestep)
example_plugins/updater_plugin/ExampleUpdater.cc:    // access the particle data arrays for writing on the GPU
example_plugins/updater_plugin/ExampleUpdater.cc:    kernel::gpu_zero_velocities(d_vel.data, m_pdata->getN());
example_plugins/updater_plugin/ExampleUpdater.cc:    // check for error codes from the GPU if error checking is enabled
example_plugins/updater_plugin/ExampleUpdater.cc:    if (m_exec_conf->isCUDAErrorCheckingEnabled())
example_plugins/updater_plugin/ExampleUpdater.cc:        CHECK_CUDA_ERROR();
example_plugins/updater_plugin/ExampleUpdater.cc:/* Export the GPU updater to be visible in the python module
example_plugins/updater_plugin/ExampleUpdater.cc:void export_ExampleUpdaterGPU(pybind11::module& m)
example_plugins/updater_plugin/ExampleUpdater.cc:    pybind11::class_<ExampleUpdaterGPU, ExampleUpdater, std::shared_ptr<ExampleUpdaterGPU>>(
example_plugins/updater_plugin/ExampleUpdater.cc:        "ExampleUpdaterGPU")
example_plugins/updater_plugin/ExampleUpdater.cu:    \brief CUDA kernels for ExampleUpdater
example_plugins/updater_plugin/ExampleUpdater.cu:// First, the kernel code for zeroing the velocities on the GPU
example_plugins/updater_plugin/ExampleUpdater.cu://! Kernel that zeroes velocities on the GPU
example_plugins/updater_plugin/ExampleUpdater.cu:__global__ void gpu_zero_velocities_kernel(Scalar4* d_vel, unsigned int N)
example_plugins/updater_plugin/ExampleUpdater.cu:    This is just a driver for gpu_zero_velocities_kernel(), see it for the details
example_plugins/updater_plugin/ExampleUpdater.cu:hipError_t gpu_zero_velocities(Scalar4* d_vel, unsigned int N)
example_plugins/updater_plugin/ExampleUpdater.cu:    hipLaunchKernelGGL(gpu_zero_velocities_kernel, dim3(grid), dim3(threads), 0, 0, d_vel, N);
example_plugins/updater_plugin/ExampleUpdater.cu:    // this method always succeeds. If you had a cuda* call in this driver, you could return its
example_plugins/updater_plugin/ExampleUpdater.cuh:    \brief Declaration of CUDA kernels for ExampleUpdater
example_plugins/updater_plugin/ExampleUpdater.cuh://! Zeros velocities on the GPU
example_plugins/updater_plugin/ExampleUpdater.cuh:hipError_t gpu_zero_velocities(Scalar4* d_vel, unsigned int N);
CHANGELOG.rst:* Correct compile errors with ``-DENABLE_GPU=on -DHOOMD_GPU_PLATFORM=HIP``
CHANGELOG.rst:* Prevent compile errors with ``-DENABLE_GPU=on -DHOOMD_GPU_PLATFORM=HIP``
CHANGELOG.rst:* Provide the full CUDA error message when scanning devices
CHANGELOG.rst:* ``hoomd.hpmc.update.Shape`` now functions with ``hoomd.device.GPU``
CHANGELOG.rst:* ``hoomd.error.GPUNotAvailableError``
CHANGELOG.rst:* ``hoomd.util.GPUNotAvailableError``
CHANGELOG.rst:* Single-process multi-gpu code path
CHANGELOG.rst:* Use ``mpirun`` specific local ranks to select GPUs before checking ``SLURM_LOCALID``
CHANGELOG.rst:* More consistent notice messages regarding MPI ranks used in GPU selection
CHANGELOG.rst:* Access valid GPU memory in ``hoomd.hpmc.update.Clusters``
CHANGELOG.rst:* Test suite passes on the ROCm GPU platform
CHANGELOG.rst:* CMake completes without error when ``HOOMD_GPU_PLATFORM=HIP``
CHANGELOG.rst:* Provide full CUDA error message when possible
CHANGELOG.rst:* Notice level 4 gives additional GPU initialization details
CHANGELOG.rst:* More detailed status message for found CUDA libraries
CHANGELOG.rst:* ``fix_cudart_rpath`` CMake macro
CHANGELOG.rst:* ``ENABLE_MPI_CUDA`` CMake option
CHANGELOG.rst:* ``hoomd.device.GPU.memory_traceback parameter``.
CHANGELOG.rst:* The ``message_filename`` property and argument to ``Device``, ``CPU``, and ``GPU`` to replace
CHANGELOG.rst:* The ``msg_file`` property and argument to ``Device``, ``CPU``, and ``GPU``.
CHANGELOG.rst:* GPU code path for ``hoomd.update.BoxResize``
CHANGELOG.rst:* Compile without errors using ``hipcc`` and ROCM 5.1.0
CHANGELOG.rst:* Support CUDA 11.8.
CHANGELOG.rst:* Support CUDA 12.0.0 final.
CHANGELOG.rst:* Support for CUDA 10.
CHANGELOG.rst:* ``Neighborlist.gpu_local_nlist_arrays`` provides zero-copy access to the computed neighbor list.
CHANGELOG.rst:* Support a large number of particle and bond types (subject to available GPU memory and user
CHANGELOG.rst:* Raise descriptive error messages when the shared memory request exceeds that available on the GPU.
CHANGELOG.rst:* External components build correctly when ``ENABLE_MPI=on`` or ``ENABLE_GPU=on``.
CHANGELOG.rst:* Compile with CUDA 12.0.
CHANGELOG.rst:* Compile with current versions of HIP and ROCm.
CHANGELOG.rst:* Compilation errors with CUDA >=11.8.
CHANGELOG.rst:* Add quotes to conda-forge gpu package installation example.
CHANGELOG.rst:* User-defined pair potentials work with HPMC on the GPU.
CHANGELOG.rst:* Compile error with CUDA 11.7.
CHANGELOG.rst:* Compile errors with ``ENABLE_GPU=on`` and ``clang`` as a host compiler.
CHANGELOG.rst:- Support user-defined pair potentials in HPMC on the GPU.
CHANGELOG.rst:- RATTLE integration methods execute on the GPU.
CHANGELOG.rst:- Kernel launch errors when one process uses different GPU devices.
CHANGELOG.rst:- ``GPU.compute_capability`` property.
CHANGELOG.rst:- Correctly identify GPUs by ID in ``GPU.devices``.
CHANGELOG.rst:- Don't initialize contexts on extra GPUs on MPI ranks.
CHANGELOG.rst:- Support ``hpmc.update.Clusters`` on the GPU.
CHANGELOG.rst:- Testing with CUDA 9, GCC 4.8, GCC 5.x, GCC 6.x, clang 5
CHANGELOG.rst:- Skip GPU tests when no GPU is present.
CHANGELOG.rst:- Find CUDA libraries on additional Linux distributions.
CHANGELOG.rst:- Zero-copy data access through numpy (CPU) and cupy (GPU).
CHANGELOG.rst:- ``jit.patch.user()`` and ``jit.patch.user_union()`` now support GPUs via
CHANGELOG.rst:- HOOMD requires a GPU that supports concurrent managed memory access (Pascal
CHANGELOG.rst:- Improved accuracy of DLVO potential on the GPU.
CHANGELOG.rst:- Support for NVIDIA GPUS with compute capability < 6.0.
CHANGELOG.rst:* Support CUDA 11.5. A bug in CUDA 11.4 may result in the error
CHANGELOG.rst:* Fix a compile error with CUDA 11
CHANGELOG.rst:* Support CUDA 11.
CHANGELOG.rst:* Fix calculation of cell widths in HPMC (GPU) and ``nlist.cell()`` with MPI.
CHANGELOG.rst:  GPU configurations.
CHANGELOG.rst:- Fix ``charge.pppm()`` execution on multiple GPUs.
CHANGELOG.rst:- Fix a bug in ``md.integrate.langevin()`` and ``md.integrate.bd()`` where on the GPU the value of ``gamma`` would be ignored.
CHANGELOG.rst:  - Remove support for compute 3.0 GPUs.
CHANGELOG.rst:  - Report detailed CUDA errors on initialization.
CHANGELOG.rst:- Fix illegal memory access in NeighborListGPU with ``-DALWAYS_USE_MANAGED_MEMORY=ON`` on single GPUs
CHANGELOG.rst:- Improve ``pair.table`` performance with multi-GPU execution
CHANGELOG.rst:- Improve ``charge.pppm`` performance with multi-GPU execution
CHANGELOG.rst:- Improve rigid body performance with multi-GPU execution
CHANGELOG.rst:- Fix a sporadic data corruption / bus error issue when data structures are dynamically resized in simulations that use unified memory (multi-GPU, or with -DALWAYS_USE_MANAGED_MEMORY=ON compile time option)
CHANGELOG.rst:- Improve ``integrate.nve`` and ``integrate.npt`` performance with multi-GPU execution
CHANGELOG.rst:- Improve some angular degrees of freedom integrators with multi-GPU execution
CHANGELOG.rst:- Improve rigid body pressure calculation performance with multi-GPU execution
CHANGELOG.rst:   -  Fix BondedGroupData and CommunicatorGPU compile errors in certain
CHANGELOG.rst:-  Improve startup time with multi-GPU simulations
CHANGELOG.rst:-  Correctly assign GPUs to MPI processes on Summit when launching with
CHANGELOG.rst:   more than one GPU per resource set
CHANGELOG.rst:-  Optimize multi-GPU performance with NVLINK
CHANGELOG.rst:-  Do not use mapped memory with MPI/GPU anymore
CHANGELOG.rst:-  Fix some cases where a multi-GPU simulation fails with an alignment
CHANGELOG.rst:-  Hide CMake warning regarding missing MPI<->CUDA interoperability
CHANGELOG.rst:-  May break some plug-ins which rely on ``GPUArray`` data type being
CHANGELOG.rst:   -  CUDA 10 support and testing
CHANGELOG.rst:   -  Support multi-GPU execution on dense nodes using CUDA managed
CHANGELOG.rst:      memory. Execute with ``--gpu=0,1,..,n-1`` command line option to
CHANGELOG.rst:      run on the first n GPUs (Pascal and above).
CHANGELOG.rst:      -  Combine the ``--gpu=..`` command line option with mpirun to
CHANGELOG.rst:   -  When building with ENABLE_CUDA=on, CUDA 8.0 is now a minimum
CHANGELOG.rst:   -  Support and enable compilation for sm70 with CUDA 9 and newer.
CHANGELOG.rst:      ``ntrial > 0`` does not support compute 7.0 (Volta) and newer GPUs
CHANGELOG.rst:      GPUs.
CHANGELOG.rst:   -  Improve performance with ``md.constrain.rigid`` in multi-GPU
CHANGELOG.rst:-  Pin cuda compatible version in conda package to resolve ``libcu*.so``
CHANGELOG.rst:   GPU.
CHANGELOG.rst:-  Fix ``PotentialPairDPDThermoGPU.h`` for use in external plugins
CHANGELOG.rst:   on the GPU
CHANGELOG.rst:      active CUDA device.
CHANGELOG.rst:   -  Deterministic HPMC integration on the GPU (optional):
CHANGELOG.rst:   -  Fix alignment error when running implicit depletants on GPU with
CHANGELOG.rst:-  ``ENABLE_CUDA`` and ``ENABLE_MPI`` CMake options default OFF. User
CHANGELOG.rst:-  HOOMD now builds on powerpc+CUDA platforms (tested on summitdev)
CHANGELOG.rst:-  Improve performance of GPU PPPM force calculation
CHANGELOG.rst:   particles did not work on the GPU.
CHANGELOG.rst:   on the GPU
CHANGELOG.rst:   depending on Saru or SaruGPU will need to update their includes. The
CHANGELOG.rst:   ``SaruGPU`` class has been removed. Use ``hoomd::detail::Saru``
CHANGELOG.rst:   instead for both CPU and GPU plugins.
CHANGELOG.rst:-  Drop support for compute 2.0 GPU devices
CHANGELOG.rst:-  Support cusolver with CUDA 8.0
CHANGELOG.rst:   optional argument, allowing to use GPU memory more efficiently
CHANGELOG.rst:-  Shorter compile time for HPMC GPU kernels
CHANGELOG.rst:-  Fix invalid forces in simulations with many bond types (on GPU)
CHANGELOG.rst:   ``md.constrain.rigid()`` in GPU MPI simulations
CHANGELOG.rst:-  Fix pressure computation with pair.dpd() on the GPU
CHANGELOG.rst:-  Support CUDA Toolkit 8.0
CHANGELOG.rst:-  Multi-GPU electrostatics ``charge.pppm`` - the long range
CHANGELOG.rst:   -  Walls execute on the GPU.
CHANGELOG.rst:-  Fall back to global rank to assign GPUs if local rank is not
CHANGELOG.rst:-  Fix a bug where simulations hung on sm 5.x GPUs with CUDA 7.5
CHANGELOG.rst:-  Enable CUDA enabled builds with the intel compiler
CHANGELOG.rst:-  Use CMake builtin FindCUDA on recent versions of CMake
CHANGELOG.rst:-  Auto-assign GPU-ids on non-compute exclusive systems even with
CHANGELOG.rst:   mode=gpu
CHANGELOG.rst:   Fermi generation GPUs
CHANGELOG.rst:-  Provide more useful error messages when cuda drivers are not present
CHANGELOG.rst:-  Assume device count is 0 when ``cudaGetDeviceCount()`` returns an
CHANGELOG.rst:3.  Fixed compile error with gcc4.4 and cuda5.0
CHANGELOG.rst:2. Support CUDA 6.5
CHANGELOG.rst:-  Improve performance with smaller numbers of particles per GPU
CHANGELOG.rst:-  Full double precision computations on the GPU (compile time option
CHANGELOG.rst:-  Support for G80, G200 GPUs.
CHANGELOG.rst:-  Fixed a bug where PPPM forces were incorrect on the GPU
CHANGELOG.rst:-  Support for CUDA 5.0
CHANGELOG.rst:1.  Support for Kepler GPUs (GTX 680)
CHANGELOG.rst:    *options.set_gpu(2)*
CHANGELOG.rst:3. Fixed a bug where using OpenMP and CUDA at the same time caused
CHANGELOG.rst:4. Fixed a bug where RPM packages did not work on systems where the CUDA
CHANGELOG.rst:    -  The GPU specific data structures are now generated on the GPU
CHANGELOG.rst:7.  CUDA 4.0 is the new minimum requirement
CHANGELOG.rst:1. Automated test suite now performs tests on OpenMPI + CUDA builds
CHANGELOG.rst:7. Expose name of executing gpu, n_cpu, hoomd version, git sha1, cuda
CHANGELOG.rst:19. Fix an issue where ENABLE_CUDA=off builds gave nonsense errors when
CHANGELOG.rst:    mode=gpu was requested.
CHANGELOG.rst:30. Tuned block sizes for CUDA 4.0
CHANGELOG.rst:31. Removed unsupported GPUS from CUDA_ARCH_LIST
CHANGELOG.rst:5. *CUDA 3.2 support:* HOOMD-blue is now fully tested and performance
CHANGELOG.rst:   tuned for use with CUDA 3.2.
CHANGELOG.rst:6. *CUDA 4.0 support:* HOOMD-blue compiles with CUDA 4.0 and passes
CHANGELOG.rst:    integrate.npt on the GPU.
CHANGELOG.rst:18. hoomd now builds on all cuda architectures. Modify CUDA_ARCH_LIST in
CHANGELOG.rst:4.  Fermi GPUs are now prioritized over per-Fermi GPUs in systems where
CHANGELOG.rst:5.  HOOMD now compiles against CUDA 3.1
CHANGELOG.rst:9.  CUDA 2.2 and older are no longer supported
CHANGELOG.rst:25. *New hardware support*: 0.9.0 and newer support Fermi GPUs
CHANGELOG.rst:    GPUs but that hardware is no longer officially supported
CHANGELOG.rst:    running on the GPU
CHANGELOG.rst:    GPUs
CHANGELOG.rst:22. ULF stability improvements for G200 GPUs.
CHANGELOG.rst:    which to run (all require CUDA 2.2 or newer)
CHANGELOG.rst:    -  there are now checks that an appropriate NVIDIA drivers is
CHANGELOG.rst:       revert to running on the CPU if no capable GPUs are installed
CHANGELOG.rst:    -  when no gpu is explicitly specified, the default choice is now
CHANGELOG.rst:       prioritized to choose the fastest GPU and one that is not
CHANGELOG.rst:    -  new command line option: ignore-display-gpu will prevent HOOMD
CHANGELOG.rst:       from executing on any GPU attached to a display
CHANGELOG.rst:    -  HOOMD now prints out a short description of the GPU(s) it is
CHANGELOG.rst:       will then automatically choose the first free GPU (see the
CHANGELOG.rst:2.  ULF workaround on GTX 280 now works with CUDA 2.2
CHANGELOG.rst:    in CUDA 2.2
CHANGELOG.rst:7.  Added support for CUDA 2.2
CHANGELOG.rst:12. Significantly increased performance of dual-GPU runs when build with
CHANGELOG.rst:    CUDA 2.2 or newer
CHANGELOG.rst:15. Emulation mode builds now work on systems without an NVIDIA card
CHANGELOG.rst:    (CUDA 2.2 or newer)
CHANGELOG.rst:16. HOOMD now compiles with CUDA 2.3
CHANGELOG.rst:    ENABLE_CAC_GPU_ID compiles HOOMD to read in the *$CAC_GPU_ID*
CHANGELOG.rst:    environment variable and use it to determine which GPUs to execute
CHANGELOG.rst:    on. No gpu command line required in job scripts any more.
CHANGELOG.rst:4.  Validation tests now run with gpu_error_checking
CHANGELOG.rst:    needed. This boosts performance on C1060 and newer GPUs.
CHANGELOG.rst:4. Multi-GPU simulations
CHANGELOG.rst:    the GPU (overall performance improvements are approximately 10%)
CHANGELOG.rst:22. The CUDA toolkit no longer needs to be installed to run a packaged
CHANGELOG.rst:11. Support for CUDA 2.0
CHANGELOG.rst:14. Cleaned up GPU code interface
CHANGELOG.rst:25. GPU selection from the command line
CHANGELOG.rst:27. GPU can now handle neighbor lists that overflow

```
