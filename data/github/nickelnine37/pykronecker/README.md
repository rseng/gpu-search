# https://github.com/nickelnine37/pykronecker

```console
paper.bib:  title        = "CuPy: A NumPy-Compatible Library for NVIDIA GPU Calculations",
docs/getting_started/jax.md:The main benefit of using Jax is the Just In Time (JIT) compiler, which significantly decreases the execution time for matrix-vector multiplications. In addition, Jax supports [automatic differentiation](https://pykronecker.readthedocs.io/en/latest/getting_started/autodiff/).  With Jax installed, PyKronecker can also be run on systems with a compatible Nvidia GPU/TPU. Note that Jax does not bundle CUDA or CuDNN as part of the `pip` package, and as such these must be installed separately. 
docs/getting_started/jax.md:The figure below compares some of the compute times for the vanilla version of PyKronecker, PyKronecker with Jax's JIT, and PyKronecker with Jax + a 1050TI GPU. 
docs/install.md:### Installation with GPU support   
docs/install.md:For Linux users with an Nvidia graphics card, PyKronecker is also compatible with the GPU and TPU version of Jax. However, this Jax version relies on [CUDA Toolkit](https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html) and [cuDNN](https://docs.nvidia.com/deeplearning/cudnn/developer-guide/index.html) which can be slightly more difficult to install. For this reason, it is recommended to install the GPU-compatible version of Jax prior to installing PyKronecker. 
docs/install.md:One way to install CUDA and cuDNN with `conda` is to run
docs/install.md:conda install cuda -c nvidia
docs/install.md:pip install --upgrade "jax[cuda]" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html
docs/install.md:The figure below shows the compute time for multiplying a [`KroneckerProduct`](../api/kroneckerproduct), [`KroneckerSum`](../api/kroneckersum) and [`KroneckerDiag`](../api/kroneckerdiag) onto a tensor of shape `(100, 120, 140)` using NumPy arrays only, NumPy arrays with the Jax JIT compiler, and Jax arrays on an Nvidia 1050Ti GPU.   
README.md:For Linux users with an Nvidia graphics card, PyKronecker is also compatible with the GPU and TPU version of Jax. However, since this relies on CUDA and cuDNN, it is recommended to follow the instructions [here](https://github.com/google/jax#installation) to install Jax first. 
mkdocs.yml:              - GPU support and JAX: getting_started/jax.md
paper.md:  - GPU
paper.md:Matrix operators composed of Kronecker products and related objects, such as Kronecker sums, arise in many areas of applied mathematics including signal processing, semidefinite programming, and quantum computing [@loan2000]. As such, a computational toolkit for manipulating Kronecker-based systems, in a way that is both efficient and idiomatic, has the potential to aid research in many fields.  PyKronecker aims to deliver this in the Python programming language by providing a simple API that integrates well with the widely-used NumPy library [@harris2020], and that supports automatic differentiation and accelerated computation on GPU/TPU hardware using Jax [@jax2018].  
paper.md:b) *To execute matrix-vector multiplications in a way that is maximally efficient and runs on parallel GPU/TPU hardware.*
paper.md:Significant effort has gone into optimising the execution of matrix-vector and matrix-tensor multiplications. In particular, this comprises the kronx algorithm, Just In Time (JIT) compilation, and parallel processing on GPU/TPU hardware. As a result of this, PyKronecker is able to achieve very fast execution times compared to alternative implementations (see Table 1) .  
paper.md:One potential alternative in Python is the PyLops library which provides an interface for general functionally-defined linear operators, and includes a Kronecker product implementation [@Ravasi2020]. It also supports GPU acceleration with CuPy [@okuta2017]. However, as a more general library, PyLops does not provide support for the Kronecker product of more than two matrices, implement a Kronecker sum operator, implement matrix-tensor multiplication, or provide automatic differentiation. It is also significantly slower than PyKronecker when operating on simple NumPy arrays. 
paper.md:Another alternative is the library Kronecker.jl [@Stock2020], implemented in the Julia programming language [@bezanson2017]. Kronecker.jl has many of the same aims as PyKronecker and has a a clean interface, making use of Julia's support for unicode and infix functions to create Kronecker products with a custom $\otimes$ operator. However, at this time, the library does not support GPU acceleration or automatic differentiation, although the former is in development. 
paper.md:Table 1. shows a feature comparison of these libraries, along with the kronx algorithm implemented in "vanilla" (i.e., running on the CPU without JIT compilation) NumPy. The table also shows the time to compute the multiplication of a Kronecker product against a vector in two scenarios. In the first scenario, the Kronecker product is constructed from two matrices of size $(400 \times 400)$ and $(500 \times 500)$, and in the second scenario Kronecker product is constructed from three matrices of size $(100 \times 100)$,  $(150 \times 150)$ and  $(200 \times 200)$, respectively. Experiments were performed with an Intel Core  2.80GHz i7-7700HQ CPU, and an Nvidia 1050Ti GPU.  In both cases, PyKronecker on the GPU is the fastest by a significant margin. 
paper.md:| Implementation    | Python | Auto-diff | GPU support | Compute time (400, 500) | Compute time (100, 150, 200) |
paper.md:| PyLops (GPU)      | Yes    | No        | Yes         | 54.6 ms ± 1.04 ms       | 4.06 s ± 182 ms              |
paper.md:| PyKronecker (GPU) | Yes    | Yes       | Yes         | 261 µs ± 17.3 µs        | 220 µs ± 59.5 µs             |

```
