# https://github.com/spack/spack

```console
share/spack/gitlab/cloud_pipelines/configs/linux/ci.yaml:      - llvm-amdgpu
share/spack/gitlab/cloud_pipelines/configs/linux/ci.yaml:      - cuda
share/spack/gitlab/cloud_pipelines/stacks/ml-linux-aarch64-cpu/spack.yaml:      - ~cuda
share/spack/gitlab/cloud_pipelines/stacks/ml-linux-aarch64-cpu/spack.yaml:      - ~rocm
share/spack/gitlab/cloud_pipelines/stacks/ml-linux-aarch64-cpu/spack.yaml:    - py-torch-nvidia-apex
share/spack/gitlab/cloud_pipelines/stacks/aws-pcluster-neoverse_v1/packages.yaml:    variants: ~atomics ~cuda ~cxx ~cxx_exceptions ~internal-hwloc ~java +legacylaunchers ~lustre ~memchecker +pmi +romio ~singularity +vt +wrapper-rpath fabrics=ofi schedulers=slurm
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:    # ROCm
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:        prefix: /opt/rocm-6.2.1/
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:        prefix: /opt/rocm-6.2.1/hip
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:        prefix: /opt/rocm-6.2.1/
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:        prefix: /opt/rocm-6.2.1/
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:        prefix: /opt/rocm-6.2.1/
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:        prefix: /opt/rocm-6.2.1/
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:        prefix: /opt/rocm-6.2.1/
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:        prefix: /opt/rocm-6.2.1/
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:        prefix: /opt/rocm-6.2.1/
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:        prefix: /opt/rocm-6.2.1/
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:        prefix: /opt/rocm-6.2.1/
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:    rocm-clang-ocl:
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:      - spec: rocm-clang-ocl@6.2.1
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:        prefix: /opt/rocm-6.2.1/
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:    rocm-cmake:
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:      - spec: rocm-cmake@6.2.1
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:        prefix: /opt/rocm-6.2.1/
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:    rocm-dbgapi:
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:      - spec: rocm-dbgapi@6.2.1
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:        prefix: /opt/rocm-6.2.1/
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:    rocm-debug-agent:
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:      - spec: rocm-debug-agent@6.2.1
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:        prefix: /opt/rocm-6.2.1/
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:    rocm-device-libs:
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:      - spec: rocm-device-libs@6.2.1
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:        prefix: /opt/rocm-6.2.1/
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:    rocm-gdb:
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:      - spec: rocm-gdb@6.2.1
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:        prefix: /opt/rocm-6.2.1/
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:    rocm-opencl:
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:      - spec: rocm-opencl@6.2.1
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:        prefix: /opt/rocm-6.2.1/opencl
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:    rocm-smi-lib:
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:      - spec: rocm-smi-lib@6.2.1
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:        prefix: /opt/rocm-6.2.1/
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:        prefix: /opt/rocm-6.2.1
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:            c: /opt/rocm-6.2.1/llvm/bin/clang++
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:            c++: /opt/rocm-6.2.1/llvm/bin/clang++
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:            hip: /opt/rocm-6.2.1/hip/bin/hipcc
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:        prefix: /opt/rocm-6.2.1
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:    llvm-amdgpu:
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:      - spec: llvm-amdgpu@6.2.1
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:        prefix: /opt/rocm-6.2.1/llvm
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:            c: /opt/rocm-6.2.1/llvm/bin/clang++
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:            cxx: /opt/rocm-6.2.1/llvm/bin/clang++
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:        prefix: /opt/rocm-6.2.1/
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:        prefix: /opt/rocm-6.2.1/
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:            c: /opt/rocm-6.2.1/llvm/bin/clang++
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:            cxx: /opt/rocm-6.2.1/llvm/bin/clang++
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:        prefix: /opt/rocm-6.2.1
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:        prefix: /opt/rocm-6.2.1
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:        prefix: /opt/rocm-6.2.1
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:        prefix: /opt/rocm-6.2.1
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:        prefix: /opt/rocm-6.2.1
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:        prefix: /opt/rocm-6.2.1
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:        prefix: /opt/rocm-6.2.1
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:        prefix: /opt/rocm-6.2.1
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:        prefix: /opt/rocm-6.2.1
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:    rocm-core:
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:      - spec: rocm-core@6.2.1
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:        prefix: /opt/rocm-6.2.1
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  # ROCM NOARCH
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  - hpctoolkit +rocm
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  - tau +mpi +rocm +syscall
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  # ROCM 908
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  - adios2 +kokkos +rocm amdgpu_target=gfx908
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  - amrex +rocm amdgpu_target=gfx908
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  - arborx +rocm amdgpu_target=gfx908
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  - cabana +rocm amdgpu_target=gfx908
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  - caliper +rocm amdgpu_target=gfx908
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  - chai +rocm amdgpu_target=gfx908
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  - ecp-data-vis-sdk +paraview +vtkm +rocm amdgpu_target=gfx908
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  - fftx +rocm amdgpu_target=gfx908
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  - gasnet +rocm amdgpu_target=gfx908
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  - ginkgo +rocm amdgpu_target=gfx908
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  - heffte +rocm amdgpu_target=gfx908
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  - hpx +rocm amdgpu_target=gfx908
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  - hypre +rocm amdgpu_target=gfx908
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  - kokkos +rocm amdgpu_target=gfx908
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  - legion +rocm amdgpu_target=gfx908
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  - magma ~cuda +rocm amdgpu_target=gfx908
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  - mfem +rocm amdgpu_target=gfx908
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  - raja ~openmp +rocm amdgpu_target=gfx908
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  - strumpack ~slate +rocm amdgpu_target=gfx908
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  - superlu-dist +rocm amdgpu_target=gfx908
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  - tasmanian ~openmp +rocm amdgpu_target=gfx908
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  - trilinos +amesos +amesos2 +anasazi +aztec +belos +boost +epetra +epetraext +ifpack ~ifpack2 +intrepid +intrepid2 +isorropia +kokkos +ml +minitensor +muelu +nox +piro +phalanx +rol +rythmos +sacado +stk +shards +shylu ~stokhos +stratimikos +teko +tempus +tpetra +trilinoscouplings +zoltan +zoltan2 +superlu-dist gotype=long_long +rocm amdgpu_target=gfx908
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  - umpire +rocm amdgpu_target=gfx908
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  - upcxx +rocm amdgpu_target=gfx908
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  # INCLUDED IN ECP DAV ROCM
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  # - paraview +rocm amdgpu_target=gfx908 # mesa: https://github.com/spack/spack/issues/44745
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  # - vtk-m ~openmp +rocm amdgpu_target=gfx908  # vtk-m: https://github.com/spack/spack/issues/40268
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  # - chapel +rocm amdgpu_target=gfx908         # chapel: need chapel >= 2.2 to support ROCm >5.4
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  # - cp2k +mpi +rocm amdgpu_target=gfx908      # cp2k: Error: KeyError: 'No spec with name rocm in... "-L{}".format(spec["rocm"].libs.directories[0]),
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  # - exago +mpi +python +raja +hiop +rocm amdgpu_target=gfx908 ~ipopt cxxflags="-Wno-error=non-pod-varargs" ^hiop@1.0.0 ~sparse +mpi +raja +rocm amdgpu_target=gfx908 # raja: https://github.com/spack/spack/issues/44593
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  # - lbann ~cuda +rocm amdgpu_target=gfx908    # aluminum: https://github.com/spack/spack/issues/38807
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  # - papi +rocm amdgpu_target=gfx908           # papi: https://github.com/spack/spack/issues/27898
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  # - petsc +rocm amdgpu_target=gfx908          # petsc: https://github.com/spack/spack/issues/44600
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  # - slate +rocm amdgpu_target=gfx908          # slate: hip/device_gescale_row_col.hip.cc:58:49: error: use of overloaded operator '*' is ambiguous (with operand types 'HIP_vector_type<double, 2>' and 'const HIP_vector_type<double, 2>')
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  # - slepc +rocm amdgpu_target=gfx908 ^petsc +rocm amdgpu_target=gfx908 # petsc: https://github.com/spack/spack/issues/44600
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  # - sundials +rocm amdgpu_target=gfx908       # sundials: https://github.com/spack/spack/issues/44601
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  # ROCM 90a
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  - adios2 +kokkos +rocm amdgpu_target=gfx90a
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  - amrex +rocm amdgpu_target=gfx90a
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  - arborx +rocm amdgpu_target=gfx90a
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  - cabana +rocm amdgpu_target=gfx90a
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  - caliper +rocm amdgpu_target=gfx90a
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  - chai +rocm amdgpu_target=gfx90a
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  - ecp-data-vis-sdk +paraview +vtkm +rocm amdgpu_target=gfx90a
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  - fftx +rocm amdgpu_target=gfx90a
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  - gasnet +rocm amdgpu_target=gfx90a
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  - ginkgo +rocm amdgpu_target=gfx90a
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  - heffte +rocm amdgpu_target=gfx90a
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  - hpx +rocm amdgpu_target=gfx90a
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  - hypre +rocm amdgpu_target=gfx90a
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  - kokkos +rocm amdgpu_target=gfx90a
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  - legion +rocm amdgpu_target=gfx90a
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  - magma ~cuda +rocm amdgpu_target=gfx90a
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  - mfem +rocm amdgpu_target=gfx90a
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  - raja ~openmp +rocm amdgpu_target=gfx90a
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  - strumpack ~slate +rocm amdgpu_target=gfx90a
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  - superlu-dist +rocm amdgpu_target=gfx90a
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  - tasmanian ~openmp +rocm amdgpu_target=gfx90a
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  - trilinos +amesos +amesos2 +anasazi +aztec +belos +boost +epetra +epetraext +ifpack ~ifpack2 +intrepid +intrepid2 +isorropia +kokkos +ml +minitensor +muelu +nox +piro +phalanx +rol +rythmos +sacado +stk +shards +shylu ~stokhos +stratimikos +teko +tempus +tpetra +trilinoscouplings +zoltan +zoltan2 +superlu-dist gotype=long_long +rocm amdgpu_target=gfx90a
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  - umpire +rocm amdgpu_target=gfx90a
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  - upcxx +rocm amdgpu_target=gfx90a
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  # INCLUDED IN ECP DAV ROCM
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  # - paraview +rocm amdgpu_target=gfx90a # mesa: https://github.com/spack/spack/issues/44745
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  # - vtk-m ~openmp +rocm amdgpu_target=gfx90a  # vtk-m: https://github.com/spack/spack/issues/40268
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  # - chapel +rocm amdgpu_target=gfx9a          # chapel: need chapel >= 2.2 to support ROCm >5.4
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  # - cp2k +mpi +rocm amdgpu_target=gfx90a      # cp2k: Error: KeyError: 'No spec with name rocm in... "-L{}".format(spec["rocm"].libs.directories[0]),
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  # - exago +mpi +python +raja +hiop +rocm amdgpu_target=gfx90a ~ipopt cxxflags="-Wno-error=non-pod-varargs" ^hiop@1.0.0 ~sparse +mpi +raja +rocm amdgpu_target=gfx90a # raja: https://github.com/spack/spack/issues/44593
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  # - lbann ~cuda +rocm amdgpu_target=gfx90a    # aluminum: https://github.com/spack/spack/issues/38807
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  # - papi +rocm amdgpu_target=gfx90a           # papi: https://github.com/spack/spack/issues/27898
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  # - petsc +rocm amdgpu_target=gfx90a          # petsc: https://github.com/spack/spack/issues/44600
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  # - slate +rocm amdgpu_target=gfx90a          # slate: hip/device_gescale_row_col.hip.cc:58:49: error: use of overloaded operator '*' is ambiguous (with operand types 'HIP_vector_type<double, 2>' and 'const HIP_vector_type<double, 2>')
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  # - slepc +rocm amdgpu_target=gfx90a ^petsc +rocm amdgpu_target=gfx90a # petsc: https://github.com/spack/spack/issues/44600  
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:  # - sundials +rocm amdgpu_target=gfx90a       # sundials: https://github.com/spack/spack/issues/44601
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:        image: ghcr.io/spack/spack/ubuntu22.04-runner-amd64-gcc-11.4-rocm6.2.1:2024.10.08
share/spack/gitlab/cloud_pipelines/stacks/e4s-rocm-external/spack.yaml:    build-group: E4S ROCm External
share/spack/gitlab/cloud_pipelines/stacks/aws-isc/spack.yaml:  - cuda_specs:
share/spack/gitlab/cloud_pipelines/stacks/aws-isc/spack.yaml:    #- relion +cuda cuda_arch=70
share/spack/gitlab/cloud_pipelines/stacks/aws-isc/spack.yaml:    - raja +cuda cuda_arch=70
share/spack/gitlab/cloud_pipelines/stacks/aws-isc/spack.yaml:    - mfem +cuda cuda_arch=70
share/spack/gitlab/cloud_pipelines/stacks/aws-isc/spack.yaml:    - - $cuda_specs
share/spack/gitlab/cloud_pipelines/stacks/gpu-tests/spack.yaml:      variants: +mpi amdgpu_target=gfx90a cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/gpu-tests/spack.yaml:  - kokkos +rocm amdgpu_target=gfx90a
share/spack/gitlab/cloud_pipelines/stacks/gpu-tests/spack.yaml:  - raja +cuda cuda_arch=80 ^cuda@12.0.0
share/spack/gitlab/cloud_pipelines/stacks/gpu-tests/spack.yaml:  # - kokkos +wrapper +cuda cuda_arch=80 ^cuda@12.0.0     # https://github.com/spack/spack/issues/35378
share/spack/gitlab/cloud_pipelines/stacks/gpu-tests/spack.yaml:        - kokkos +rocm amdgpu_target=gfx90a
share/spack/gitlab/cloud_pipelines/stacks/gpu-tests/spack.yaml:          tags: [ "x86_64_v3-rocm", "rocm-5.4.0", "mi210" ]
share/spack/gitlab/cloud_pipelines/stacks/gpu-tests/spack.yaml:        - kokkos +cuda cuda_arch=80 ^cuda@12.0.0
share/spack/gitlab/cloud_pipelines/stacks/gpu-tests/spack.yaml:        - raja +cuda cuda_arch=80 ^cuda@12.0.0
share/spack/gitlab/cloud_pipelines/stacks/gpu-tests/spack.yaml:          tags: [ "x86_64_v3-cuda", "nvidia-525.85.12", "cuda-12.0", "a100" ]
share/spack/gitlab/cloud_pipelines/stacks/gpu-tests/spack.yaml:    build-group: GPU Testing
share/spack/gitlab/cloud_pipelines/stacks/radiuss-aws/spack.yaml:      variants: +mpi cuda_arch=70
share/spack/gitlab/cloud_pipelines/stacks/radiuss-aws/spack.yaml:    - caliper +cuda
share/spack/gitlab/cloud_pipelines/stacks/radiuss-aws/spack.yaml:    - camp +cuda
share/spack/gitlab/cloud_pipelines/stacks/radiuss-aws/spack.yaml:    - chai +cuda +raja
share/spack/gitlab/cloud_pipelines/stacks/radiuss-aws/spack.yaml:    - mfem +cuda ^hypre+cuda
share/spack/gitlab/cloud_pipelines/stacks/radiuss-aws/spack.yaml:    - raja +cuda
share/spack/gitlab/cloud_pipelines/stacks/radiuss-aws/spack.yaml:    - umpire +cuda
share/spack/gitlab/cloud_pipelines/stacks/ml-linux-aarch64-cuda/spack.yaml:      - ~rocm
share/spack/gitlab/cloud_pipelines/stacks/ml-linux-aarch64-cuda/spack.yaml:      - +cuda
share/spack/gitlab/cloud_pipelines/stacks/ml-linux-aarch64-cuda/spack.yaml:      - cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/ml-linux-aarch64-cuda/spack.yaml:      require: ~cuda
share/spack/gitlab/cloud_pipelines/stacks/ml-linux-aarch64-cuda/spack.yaml:    - py-torch-nvidia-apex
share/spack/gitlab/cloud_pipelines/stacks/ml-linux-aarch64-cuda/spack.yaml:    # torchtext requires older pytorch, which requires older cuda, which doesn't support newer GCC
share/spack/gitlab/cloud_pipelines/stacks/ml-linux-aarch64-cuda/spack.yaml:    # xgboost requires older cuda, which doesn't support newer GCC
share/spack/gitlab/cloud_pipelines/stacks/aws-pcluster-x86_64_v4/packages.yaml:    variants: ~atomics ~cuda ~cxx ~cxx_exceptions ~internal-hwloc ~java +legacylaunchers ~lustre ~memchecker +pmi +romio ~singularity +vt +wrapper-rpath fabrics=ofi schedulers=slurm
share/spack/gitlab/cloud_pipelines/stacks/e4s-cray-sles/spack.yaml:    cuda:
share/spack/gitlab/cloud_pipelines/stacks/e4s-cray-sles/spack.yaml:  - mgard +serial +openmp +timing +unstructured ~cuda
share/spack/gitlab/cloud_pipelines/stacks/e4s-cray-sles/spack.yaml:  - slate ~cuda
share/spack/gitlab/cloud_pipelines/stacks/e4s-cray-sles/spack.yaml:  # - libpressio +bitgrooming +bzip2 ~cuda ~cusz +fpzip +hdf5 +libdistributed +lua +openmp +python +sz +sz3 +unix +zfp +json +remote +netcdf +mgard # python: Could not find platform dependent libraries <exec_prefix>
share/spack/gitlab/cloud_pipelines/stacks/e4s-cray-sles/spack.yaml:  # - ecp-data-vis-sdk ~cuda ~rocm +adios2 +ascent +cinema +darshan +faodel +hdf5 +paraview +pnetcdf +sz +unifyfs +veloc ~visit +vtkm +zfp ^hdf5@1.14 # llvm@14.0.6: ?;
share/spack/gitlab/cloud_pipelines/stacks/e4s-cray-sles/spack.yaml:  # - parsec ~cuda
share/spack/gitlab/cloud_pipelines/stacks/e4s-oneapi/spack.yaml:  - bricks ~cuda
share/spack/gitlab/cloud_pipelines/stacks/e4s-oneapi/spack.yaml:  - ecp-data-vis-sdk ~cuda ~rocm +adios2 ~ascent +cinema +darshan +faodel +hdf5 +paraview +pnetcdf +sz +unifyfs +veloc ~visit +vtkm +zfp # +ascent: fides: fides/xgc/XGCCommon.cxx:233:10: error: no member named 'iota' in namespace 'std'; +visit: visit_vtk/lightweight/vtkSkewLookupTable.C:32:10: error: cannot initialize return object of type 'unsigned char *' with an rvalue of type 'const unsigned char *'
share/spack/gitlab/cloud_pipelines/stacks/e4s-oneapi/spack.yaml:  - libpressio +bitgrooming +bzip2 ~cuda ~cusz +fpzip +hdf5 +libdistributed +lua +openmp +python +sz +sz3 +unix +zfp
share/spack/gitlab/cloud_pipelines/stacks/e4s-oneapi/spack.yaml:  - mgard +serial +openmp +timing +unstructured ~cuda
share/spack/gitlab/cloud_pipelines/stacks/e4s-oneapi/spack.yaml:  - parsec ~cuda
share/spack/gitlab/cloud_pipelines/stacks/e4s-oneapi/spack.yaml:  - slate ~cuda
share/spack/gitlab/cloud_pipelines/stacks/e4s-oneapi/spack.yaml:  # - chapel ~cuda ~rocm                                    # llvm: closures.c:(.text+0x305e): undefined reference to `_intel_fast_memset'
share/spack/gitlab/cloud_pipelines/stacks/e4s-oneapi/spack.yaml:  # GPU
share/spack/gitlab/cloud_pipelines/stacks/e4s-oneapi/spack.yaml:  - tau +mpi +opencl +level_zero ~pdt +syscall                # requires libdrm.so to be installed
share/spack/gitlab/cloud_pipelines/stacks/e4s-oneapi/spack.yaml:  - warpx ~qed +python ~python_ipo compute=sycl ^py-amrex ~ipo  # qed for https://github.com/ECP-WarpX/picsar/pull/53 prior to 24.09 release; ~ipo for oneAPI 2024.2.0 GPU builds do not support IPO/LTO says CMake, even though pybind11 strongly encourages it
share/spack/gitlab/cloud_pipelines/stacks/e4s-cray-rhel/spack.yaml:    cuda:
share/spack/gitlab/cloud_pipelines/stacks/e4s-cray-rhel/spack.yaml:  #- libpressio +bitgrooming +bzip2 ~cuda ~cusz +fpzip +hdf5 +libdistributed +lua +openmp +python +sz +sz3 +unix +zfp +json +remote +netcdf +mgard # mgard: 
share/spack/gitlab/cloud_pipelines/stacks/e4s-cray-rhel/spack.yaml:  # - mgard +serial +openmp +timing +unstructured ~cuda # mgard
share/spack/gitlab/cloud_pipelines/stacks/e4s-cray-rhel/spack.yaml:  - slate ~cuda
share/spack/gitlab/cloud_pipelines/stacks/e4s-cray-rhel/spack.yaml:  # - ecp-data-vis-sdk ~cuda ~rocm +adios2 +ascent +cinema +darshan +faodel +hdf5 +paraview +pnetcdf +sz +unifyfs +veloc ~visit +vtkm +zfp ^hdf5@1.14 # llvm@14.0.6: ?;
share/spack/gitlab/cloud_pipelines/stacks/e4s-cray-rhel/spack.yaml:  # - parsec ~cuda        # parsec: parsec/fortran/CMakeFiles/parsec_fortran.dir/parsecf.F90.o: ftn-2103 ftn: WARNING in command line. The -W extra option is not supported or invalid and will be ignored.
share/spack/gitlab/cloud_pipelines/stacks/data-vis-sdk/spack.yaml:      - - ~cuda ~rocm
share/spack/gitlab/cloud_pipelines/stacks/data-vis-sdk/spack.yaml:        # Current testing of GPU supported configurations
share/spack/gitlab/cloud_pipelines/stacks/data-vis-sdk/spack.yaml:        # - +cuda ~rocm
share/spack/gitlab/cloud_pipelines/stacks/data-vis-sdk/spack.yaml:        # - ~cuda +rocm
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - chapel ~cuda ~rocm
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - ecp-data-vis-sdk ~cuda ~rocm +adios2 +ascent +cinema +darshan +faodel +hdf5 +paraview +pnetcdf +sz +unifyfs +veloc ~visit +vtkm +zfp # +visit: ?
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - mgard +serial +openmp +timing +unstructured ~cuda
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - parsec ~cuda
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - slate ~cuda
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  # - bricks ~cuda                    # not respecting target=aarch64?
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  # - libpressio +bitgrooming +bzip2 ~cuda ~cusz +fpzip +hdf5 +libdistributed +lua +openmp +python +sz +sz3 +unix +zfp # py-numcodecs@0.7.3: gcc: error: unrecognized command-line option '-mno-sse2'
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  # CUDA NOARCH
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - flux-core +cuda
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - hpctoolkit +cuda
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - papi +cuda
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - tau +mpi +cuda +syscall
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  # - bricks +cuda                    # not respecting target=aarch64?
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  # - legion +cuda                    # legion: needs NVIDIA driver
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  # CUDA 75
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - amrex +cuda cuda_arch=75
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - arborx +cuda cuda_arch=75 ^kokkos +wrapper
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - axom +cuda cuda_arch=75
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - cabana +cuda cuda_arch=75 ^kokkos +wrapper +cuda_lambda +cuda cuda_arch=75
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - caliper +cuda cuda_arch=75
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - chai +cuda cuda_arch=75 ^umpire ~shared
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - chapel +cuda cuda_arch=75
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - cusz +cuda cuda_arch=75
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - dealii +cuda cuda_arch=75
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - ecp-data-vis-sdk +adios2 +hdf5 +vtkm +zfp ~paraview +cuda cuda_arch=75  # # +paraview: job killed oom?
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - fftx +cuda cuda_arch=75
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - flecsi +cuda cuda_arch=75
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - ginkgo +cuda cuda_arch=75
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - gromacs +cuda cuda_arch=75
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - heffte +cuda cuda_arch=75
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - hpx +cuda cuda_arch=75
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - hypre +cuda cuda_arch=75
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - kokkos +wrapper +cuda cuda_arch=75
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - kokkos-kernels +cuda cuda_arch=75 ^kokkos +wrapper +cuda cuda_arch=75
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - magma +cuda cuda_arch=75
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - mfem +cuda cuda_arch=75
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - mgard +serial +openmp +timing +unstructured +cuda cuda_arch=75
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - omega-h +cuda cuda_arch=75
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - parsec +cuda cuda_arch=75
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - petsc +cuda cuda_arch=75
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - py-torch +cuda cuda_arch=75
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - raja +cuda cuda_arch=75
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - slate +cuda cuda_arch=75
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - slepc +cuda cuda_arch=75
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - strumpack ~slate +cuda cuda_arch=75
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - sundials +cuda cuda_arch=75
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - superlu-dist +cuda cuda_arch=75
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - tasmanian +cuda cuda_arch=75
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - trilinos +cuda cuda_arch=75
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - umpire ~shared +cuda cuda_arch=75
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  # INCLUDED IN ECP DAV CUDA
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - adios2 +cuda cuda_arch=75
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  # - ascent +cuda cuda_arch=75       # ascent: https://github.com/spack/spack/issues/38045
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - vtk-m +cuda cuda_arch=75
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - zfp +cuda cuda_arch=75
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  # - cp2k +mpi +cuda cuda_arch=75    # cp2k: cp2k only supports cuda_arch ('35', '37', '60', '70', '80')
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  # - lammps +cuda cuda_arch=75       # lammps: needs NVIDIA driver
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  # - lbann +cuda cuda_arch=75        # aluminum: include/aluminum/base.hpp:53: multiple definition of `cub::CUB_200400___CUDA_ARCH_LIST___NS::Debug(cudaError,char const*, int)'; lbann: https://github.com/spack/spack/issues/38788
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  # - libpressio +bitgrooming +bzip2 +fpzip +hdf5 +libdistributed +lua +openmp +python +sz +sz3 +unix +zfp +json +remote +netcdf ~cusz +mgard +cuda cuda_arch=75 # libpressio: CMake Error at CMakeLists.txt:498 (find_library): Could not find CUFile_LIBRARY using the following names: cufile ; +cusz: https://github.com/spack/spack/issues/38787
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  # - paraview +cuda cuda_arch=75     # paraview: killed oom?
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  # - upcxx +cuda cuda_arch=75        # upcxx: needs NVIDIA driver
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  # CUDA 80
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - amrex +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - arborx +cuda cuda_arch=80 ^kokkos +wrapper
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - axom +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - cabana +cuda cuda_arch=80 ^kokkos +wrapper +cuda_lambda +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - caliper +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - chai +cuda cuda_arch=80 ^umpire ~shared
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - chapel +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - cusz +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - dealii +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - ecp-data-vis-sdk +adios2 +hdf5 +vtkm +zfp ~paraview +cuda cuda_arch=80 # +paraview: job killed oom?
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - fftx +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - flecsi +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - ginkgo +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - gromacs +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - heffte +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - hpx +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - hypre +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - kokkos +wrapper +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - kokkos-kernels +cuda cuda_arch=80 ^kokkos +wrapper +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - magma +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - mfem +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - mgard +serial +openmp +timing +unstructured +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - omega-h +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - parsec +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - petsc +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - py-torch +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - raja +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - slate +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - slepc +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - strumpack ~slate +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - sundials +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - superlu-dist +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - tasmanian +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - trilinos +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - umpire ~shared +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  # INCLUDED IN ECP DAV CUDA
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  # - ascent +cuda cuda_arch=80       # ascent: https://github.com/spack/spack/issues/38045
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - adios2 +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - vtk-m +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - zfp +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  # - cp2k +mpi +cuda cuda_arch=80    # cp2k: Error: KeyError: 'Point environment variable LIBSMM_PATH to the absolute path of the libsmm.a file'
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  # - lammps +cuda cuda_arch=80       # lammps: needs NVIDIA driver
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  # - lbann +cuda cuda_arch=80        # aluminum: include/aluminum/base.hpp:53: multiple definition of `cub::CUB_200400___CUDA_ARCH_LIST___NS::Debug(cudaError,char const*, int)'; lbann: https://github.com/spack/spack/issues/38788
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  # - libpressio +bitgrooming +bzip2 +fpzip +hdf5 +libdistributed +lua +openmp +python +sz +sz3 +unix +zfp +json +remote +netcdf ~cusz +mgard +cuda cuda_arch=80 # libpressio: CMake Error at CMakeLists.txt:498 (find_library): Could not find CUFile_LIBRARY using the following names: cufile ; +cusz: https://github.com/spack/spack/issues/38787
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  # - paraview +cuda cuda_arch=80     # paraview: killed oom?
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  # - upcxx +cuda cuda_arch=80        # upcxx: needs NVIDIA driver
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  # CUDA 90
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - amrex +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - arborx +cuda cuda_arch=90 ^kokkos +wrapper
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - axom +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - cabana +cuda cuda_arch=90  ^kokkos +wrapper +cuda_lambda +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - caliper +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - chai +cuda cuda_arch=90 ^umpire ~shared
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - chapel +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - ecp-data-vis-sdk +adios2 +hdf5 +vtkm +zfp ~paraview +cuda cuda_arch=90 # +paraview: vtkm/exec/cuda/internal/ThrustPatches.h(213): error: this declaration has no storage class or type specifier
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - fftx +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - flecsi +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - ginkgo +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - gromacs +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - heffte +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - hpx +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - hypre +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - kokkos +wrapper +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - kokkos-kernels +cuda cuda_arch=90 ^kokkos +wrapper +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - magma +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - mfem +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - mgard +serial +openmp +timing +unstructured +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - parsec +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - petsc +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - py-torch +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - raja +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - slate +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - slepc +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - strumpack ~slate +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - sundials +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - superlu-dist +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - tasmanian +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - trilinos +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - umpire ~shared +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  # INCLUDED IN ECP DAV CUDA
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - adios2 +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  # - paraview +cuda cuda_arch=90     # paraview: InstallError: Incompatible cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - vtk-m +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  - zfp +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  # - ascent +cuda cuda_arch=90       # ascent: https://github.com/spack/spack/issues/38045
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  # - cp2k +mpi +cuda cuda_arch=90    # cp2k: cp2k only supports cuda_arch ('35', '37', '60', '70', '80')
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  # - cusz +cuda cuda_arch=90         # cusz: cuda-12.5.1-pil77yk7gsseyqitybr47qmhdtszbcwa/targets/sbsa-linux/include/cub/util_device.cuh:160:63: error: 'blockIdx' was not declared in this scope
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  # - dealii +cuda cuda_arch=90       # dealii: concretize conflict + https://github.com/spack/spack/issues/39532
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  # - lammps +cuda cuda_arch=90       # lammps: needs NVIDIA driver
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  # - lbann +cuda cuda_arch=90        # aluminum: include/aluminum/base.hpp:53: multiple definition of `cub::CUB_200400___CUDA_ARCH_LIST___NS::Debug(cudaError,char const*, int)'; 
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  # - libpressio +bitgrooming +bzip2 +fpzip +hdf5 +libdistributed +lua +openmp +python +sz +sz3 +unix +zfp +json +remote +netcdf ~cusz +mgard +cuda cuda_arch=90 # libpressio: CMake Error at CMakeLists.txt:498 (find_library): Could not find CUFile_LIBRARY using the following names: cufile ; +cusz: https://github.com/spack/spack/issues/38787
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  # - omega-h +cuda cuda_arch=90      # omega-h: https://github.com/spack/spack/issues/39535
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse_v1/spack.yaml:  # - upcxx +cuda cuda_arch=90        # upcxx: needs NVIDIA driver
share/spack/gitlab/cloud_pipelines/stacks/ml-linux-x86_64-rocm/spack.yaml:      - ~cuda
share/spack/gitlab/cloud_pipelines/stacks/ml-linux-x86_64-rocm/spack.yaml:      - +rocm
share/spack/gitlab/cloud_pipelines/stacks/ml-linux-x86_64-rocm/spack.yaml:      - amdgpu_target=gfx90a
share/spack/gitlab/cloud_pipelines/stacks/ml-linux-x86_64-rocm/spack.yaml:    # Does not yet support Spack-installed ROCm
share/spack/gitlab/cloud_pipelines/stacks/ml-linux-x86_64-rocm/spack.yaml:    # Does not yet support Spack-installed ROCm
share/spack/gitlab/cloud_pipelines/stacks/ml-linux-x86_64-rocm/spack.yaml:    # - py-torch-nvidia-apex
share/spack/gitlab/cloud_pipelines/stacks/ml-linux-x86_64-rocm/spack.yaml:    # Does not yet support Spack-installed ROCm
share/spack/gitlab/cloud_pipelines/stacks/ml-linux-x86_64-cuda/spack.yaml:      - ~rocm
share/spack/gitlab/cloud_pipelines/stacks/ml-linux-x86_64-cuda/spack.yaml:      - +cuda
share/spack/gitlab/cloud_pipelines/stacks/ml-linux-x86_64-cuda/spack.yaml:      - cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/ml-linux-x86_64-cuda/spack.yaml:      require: ~cuda
share/spack/gitlab/cloud_pipelines/stacks/ml-linux-x86_64-cuda/spack.yaml:    - py-torch-nvidia-apex
share/spack/gitlab/cloud_pipelines/stacks/ml-linux-x86_64-cuda/spack.yaml:    # torchtext requires older pytorch, which requires older cuda, which doesn't support newer GCC
share/spack/gitlab/cloud_pipelines/stacks/ml-linux-x86_64-cuda/spack.yaml:    # xgboost requires older cuda, which doesn't support newer GCC
share/spack/gitlab/cloud_pipelines/stacks/ml-linux-x86_64-cpu/spack.yaml:      - ~cuda
share/spack/gitlab/cloud_pipelines/stacks/ml-linux-x86_64-cpu/spack.yaml:      - ~rocm
share/spack/gitlab/cloud_pipelines/stacks/ml-linux-x86_64-cpu/spack.yaml:    - py-torch-nvidia-apex
share/spack/gitlab/cloud_pipelines/stacks/e4s-power/spack.yaml:      variants: +mpi cuda_arch=70
share/spack/gitlab/cloud_pipelines/stacks/e4s-power/spack.yaml:    cuda:
share/spack/gitlab/cloud_pipelines/stacks/e4s-power/spack.yaml:  - chapel ~rocm ~cuda
share/spack/gitlab/cloud_pipelines/stacks/e4s-power/spack.yaml:  - mgard +serial +openmp +timing +unstructured ~cuda
share/spack/gitlab/cloud_pipelines/stacks/e4s-power/spack.yaml:  - paraview ~cuda ~rocm
share/spack/gitlab/cloud_pipelines/stacks/e4s-power/spack.yaml:  - parsec ~cuda
share/spack/gitlab/cloud_pipelines/stacks/e4s-power/spack.yaml:  - slate ~cuda
share/spack/gitlab/cloud_pipelines/stacks/e4s-power/spack.yaml:  # - ecp-data-vis-sdk ~cuda ~rocm +adios2 +ascent +cinema +darshan +faodel +hdf5 ~paraview +pnetcdf +sz +unifyfs +veloc ~visit +vtkm +zfp # +visit: libext, libxkbfile, libxrender, libxt, silo (https://github.com/spack/spack/issues/39538), cairo
share/spack/gitlab/cloud_pipelines/stacks/e4s-power/spack.yaml:  # - libpressio +bitgrooming +bzip2 ~cuda ~cusz +fpzip +hdf5 +libdistributed +lua +openmp +python +sz +sz3 +unix +zfp # py-numcodecs: gcc: error: unrecognized command line option '-mno-sse2'; did you mean '-mno-isel'? gcc: error: unrecognized command line option '-mno-avx2'
share/spack/gitlab/cloud_pipelines/stacks/e4s-power/spack.yaml:  # CUDA NOARCH
share/spack/gitlab/cloud_pipelines/stacks/e4s-power/spack.yaml:  - bricks +cuda
share/spack/gitlab/cloud_pipelines/stacks/e4s-power/spack.yaml:  - cabana +cuda ^kokkos +wrapper +cuda_lambda +cuda cuda_arch=70
share/spack/gitlab/cloud_pipelines/stacks/e4s-power/spack.yaml:  - flux-core +cuda
share/spack/gitlab/cloud_pipelines/stacks/e4s-power/spack.yaml:  - hpctoolkit +cuda
share/spack/gitlab/cloud_pipelines/stacks/e4s-power/spack.yaml:  - papi +cuda
share/spack/gitlab/cloud_pipelines/stacks/e4s-power/spack.yaml:  - tau +mpi +cuda
share/spack/gitlab/cloud_pipelines/stacks/e4s-power/spack.yaml:  # - legion +cuda                          # legion: needs NVIDIA driver
share/spack/gitlab/cloud_pipelines/stacks/e4s-power/spack.yaml:  # CUDA 70
share/spack/gitlab/cloud_pipelines/stacks/e4s-power/spack.yaml:  - amrex +cuda cuda_arch=70
share/spack/gitlab/cloud_pipelines/stacks/e4s-power/spack.yaml:  - arborx +cuda cuda_arch=70 ^kokkos +wrapper
share/spack/gitlab/cloud_pipelines/stacks/e4s-power/spack.yaml:  - caliper +cuda cuda_arch=70
share/spack/gitlab/cloud_pipelines/stacks/e4s-power/spack.yaml:  - chai +cuda cuda_arch=70 ^umpire ~shared
share/spack/gitlab/cloud_pipelines/stacks/e4s-power/spack.yaml:  - ecp-data-vis-sdk ~rocm +adios2 ~ascent +hdf5 +vtkm +zfp ~paraview +cuda cuda_arch=70
share/spack/gitlab/cloud_pipelines/stacks/e4s-power/spack.yaml:  - exago +mpi +python +raja +hiop ~rocm +cuda cuda_arch=70 ~ipopt ^hiop@1.0.0 ~sparse +mpi +raja ~rocm +cuda cuda_arch=70 #^raja@0.14.0
share/spack/gitlab/cloud_pipelines/stacks/e4s-power/spack.yaml:  - flecsi +cuda cuda_arch=70
share/spack/gitlab/cloud_pipelines/stacks/e4s-power/spack.yaml:  - ginkgo +cuda cuda_arch=70
share/spack/gitlab/cloud_pipelines/stacks/e4s-power/spack.yaml:  - gromacs +cuda cuda_arch=70
share/spack/gitlab/cloud_pipelines/stacks/e4s-power/spack.yaml:  - heffte +cuda cuda_arch=70
share/spack/gitlab/cloud_pipelines/stacks/e4s-power/spack.yaml:  - hpx +cuda cuda_arch=70
share/spack/gitlab/cloud_pipelines/stacks/e4s-power/spack.yaml:  - hypre +cuda cuda_arch=70
share/spack/gitlab/cloud_pipelines/stacks/e4s-power/spack.yaml:  - kokkos +wrapper +cuda cuda_arch=70
share/spack/gitlab/cloud_pipelines/stacks/e4s-power/spack.yaml:  - kokkos-kernels +cuda cuda_arch=70 ^kokkos +wrapper +cuda cuda_arch=70
share/spack/gitlab/cloud_pipelines/stacks/e4s-power/spack.yaml:  - magma +cuda cuda_arch=70
share/spack/gitlab/cloud_pipelines/stacks/e4s-power/spack.yaml:  - mfem +cuda cuda_arch=70
share/spack/gitlab/cloud_pipelines/stacks/e4s-power/spack.yaml:  - mgard +serial +openmp +timing +unstructured +cuda cuda_arch=70
share/spack/gitlab/cloud_pipelines/stacks/e4s-power/spack.yaml:  - omega-h +cuda cuda_arch=70
share/spack/gitlab/cloud_pipelines/stacks/e4s-power/spack.yaml:  - parsec +cuda cuda_arch=70
share/spack/gitlab/cloud_pipelines/stacks/e4s-power/spack.yaml:  - petsc +cuda cuda_arch=70
share/spack/gitlab/cloud_pipelines/stacks/e4s-power/spack.yaml:  - raja +cuda cuda_arch=70
share/spack/gitlab/cloud_pipelines/stacks/e4s-power/spack.yaml:  - slate +cuda cuda_arch=70
share/spack/gitlab/cloud_pipelines/stacks/e4s-power/spack.yaml:  - slepc +cuda cuda_arch=70
share/spack/gitlab/cloud_pipelines/stacks/e4s-power/spack.yaml:  - strumpack ~slate +cuda cuda_arch=70
share/spack/gitlab/cloud_pipelines/stacks/e4s-power/spack.yaml:  - sundials +cuda cuda_arch=70
share/spack/gitlab/cloud_pipelines/stacks/e4s-power/spack.yaml:  - superlu-dist +cuda cuda_arch=70
share/spack/gitlab/cloud_pipelines/stacks/e4s-power/spack.yaml:  - tasmanian +cuda cuda_arch=70
share/spack/gitlab/cloud_pipelines/stacks/e4s-power/spack.yaml:  - umpire ~shared +cuda cuda_arch=70
share/spack/gitlab/cloud_pipelines/stacks/e4s-power/spack.yaml:  # INCLUDED IN ECP DAV CUDA
share/spack/gitlab/cloud_pipelines/stacks/e4s-power/spack.yaml:  - adios2 +cuda cuda_arch=70
share/spack/gitlab/cloud_pipelines/stacks/e4s-power/spack.yaml:  # - ascent +cuda cuda_arch=70             # ascent: https://github.com/spack/spack/issues/38045
share/spack/gitlab/cloud_pipelines/stacks/e4s-power/spack.yaml:  - paraview +cuda cuda_arch=70
share/spack/gitlab/cloud_pipelines/stacks/e4s-power/spack.yaml:  - vtk-m +cuda cuda_arch=70
share/spack/gitlab/cloud_pipelines/stacks/e4s-power/spack.yaml:  - zfp +cuda cuda_arch=70
share/spack/gitlab/cloud_pipelines/stacks/e4s-power/spack.yaml:  # - axom +cuda cuda_arch=70               # axom: https://github.com/spack/spack/issues/29520
share/spack/gitlab/cloud_pipelines/stacks/e4s-power/spack.yaml:  # - cp2k +mpi +cuda cuda_arch=70          # dbcsr
share/spack/gitlab/cloud_pipelines/stacks/e4s-power/spack.yaml:  # - cusz +cuda cuda_arch=70               # cusz: https://github.com/spack/spack/issues/38787
share/spack/gitlab/cloud_pipelines/stacks/e4s-power/spack.yaml:  # - dealii +cuda cuda_arch=70             # fltk: https://github.com/spack/spack/issues/38791
share/spack/gitlab/cloud_pipelines/stacks/e4s-power/spack.yaml:  # - lammps +cuda cuda_arch=70             # lammps: needs NVIDIA driver
share/spack/gitlab/cloud_pipelines/stacks/e4s-power/spack.yaml:  # - lbann +cuda cuda_arch=70              # lbann: https://github.com/spack/spack/issues/38788
share/spack/gitlab/cloud_pipelines/stacks/e4s-power/spack.yaml:  # - libpressio +bitgrooming +bzip2 +fpzip +hdf5 +libdistributed +lua +openmp +python +sz +sz3 +unix +zfp +json +remote +netcdf +cusz +mgard +cuda cuda_arch=70 ^cusz +cuda cuda_arch=70 # depends_on("cuda@11.7.1:", when="+cuda")
share/spack/gitlab/cloud_pipelines/stacks/e4s-power/spack.yaml:  # - py-torch +cuda cuda_arch=70           # skipped
share/spack/gitlab/cloud_pipelines/stacks/e4s-power/spack.yaml:  # - trilinos +cuda cuda_arch=70           # trilinos: https://github.com/trilinos/Trilinos/issues/11630
share/spack/gitlab/cloud_pipelines/stacks/e4s-power/spack.yaml:  # - upcxx +cuda cuda_arch=70              # upcxx: needs NVIDIA driver
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse-v2/spack.yaml:  - ecp-data-vis-sdk ~cuda ~rocm +adios2 +ascent +cinema +darshan +faodel +hdf5 ~paraview +pnetcdf +sz +unifyfs +veloc ~visit +vtkm +zfp  # +visit: ?
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse-v2/spack.yaml:  - mgard +serial +openmp +timing +unstructured ~cuda
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse-v2/spack.yaml:  - parsec ~cuda
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse-v2/spack.yaml:  - slate ~cuda
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse-v2/spack.yaml:  # - bricks ~cuda                  # not respecting target=aarch64?
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse-v2/spack.yaml:  # - libpressio +bitgrooming +bzip2 ~cuda ~cusz +fpzip +hdf5 +libdistributed +lua +openmp +python +sz +sz3 +unix +zfp # py-numcodecs@0.7.3: gcc: error: unrecognized command-line option '-mno-sse2'
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse-v2/spack.yaml:  # CUDA NOARCH
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse-v2/spack.yaml:  - flux-core +cuda
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse-v2/spack.yaml:  - hpctoolkit +cuda
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse-v2/spack.yaml:  - papi +cuda
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse-v2/spack.yaml:  - tau +mpi +cuda +syscall
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse-v2/spack.yaml:  # - bricks +cuda                  # not respecting target=aarch64?
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse-v2/spack.yaml:  # - legion +cuda                  # legion: needs NVIDIA driver
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse-v2/spack.yaml:  # CUDA 90
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse-v2/spack.yaml:  - amrex +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse-v2/spack.yaml:  - arborx +cuda cuda_arch=90 ^kokkos +wrapper
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse-v2/spack.yaml:  - cabana +cuda cuda_arch=90  ^kokkos +wrapper +cuda_lambda +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse-v2/spack.yaml:  - caliper +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse-v2/spack.yaml:  - chai +cuda cuda_arch=90 ^umpire ~shared
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse-v2/spack.yaml:  - fftx +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse-v2/spack.yaml:  - flecsi +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse-v2/spack.yaml:  - ginkgo +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse-v2/spack.yaml:  - gromacs +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse-v2/spack.yaml:  - heffte +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse-v2/spack.yaml:  - hpx +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse-v2/spack.yaml:  - kokkos +wrapper +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse-v2/spack.yaml:  - kokkos-kernels +cuda cuda_arch=90 ^kokkos +wrapper +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse-v2/spack.yaml:  - magma +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse-v2/spack.yaml:  - mfem +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse-v2/spack.yaml:  - mgard +serial +openmp +timing +unstructured +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse-v2/spack.yaml:  - parsec +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse-v2/spack.yaml:  - petsc +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse-v2/spack.yaml:  - raja +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse-v2/spack.yaml:  - slate +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse-v2/spack.yaml:  - strumpack ~slate +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse-v2/spack.yaml:  - sundials +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse-v2/spack.yaml:  - superlu-dist +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse-v2/spack.yaml:  - trilinos +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse-v2/spack.yaml:  - umpire ~shared +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse-v2/spack.yaml:  # INCLUDED IN ECP DAV CUDA
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse-v2/spack.yaml:  - adios2 +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse-v2/spack.yaml:  # - ascent +cuda cuda_arch=90     # ascent: https://github.com/spack/spack/issues/38045
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse-v2/spack.yaml:  # - paraview +cuda cuda_arch=90   # paraview: InstallError: Incompatible cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse-v2/spack.yaml:  - vtk-m +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse-v2/spack.yaml:  - zfp +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse-v2/spack.yaml:  # - axom +cuda cuda_arch=90       # axom: https://github.com/spack/spack/issues/29520
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse-v2/spack.yaml:  # - cp2k +mpi +cuda cuda_arch=90  # cp2k: cp2k only supports cuda_arch ('35', '37', '60', '70', '80')
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse-v2/spack.yaml:  # - cusz +cuda cuda_arch=90       # cusz: https://github.com/spack/spack/issues/38787
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse-v2/spack.yaml:  # - dealii +cuda cuda_arch=90     # dealii: https://github.com/spack/spack/issues/39532
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse-v2/spack.yaml:  # - ecp-data-vis-sdk +adios2 +hdf5 +vtkm +zfp +paraview +cuda cuda_arch=90 # embree: https://github.com/spack/spack/issues/39534
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse-v2/spack.yaml:  # - hypre +cuda cuda_arch=90      # concretizer: hypre +cuda requires cuda@:11, but cuda_arch=90 requires cuda@12:
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse-v2/spack.yaml:  # - lammps +cuda cuda_arch=90     # lammps: needs NVIDIA driver
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse-v2/spack.yaml:  # - lbann +cuda cuda_arch=90      # concretizer: Cannot select a single "version" for package "lbann"
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse-v2/spack.yaml:  # - libpressio +bitgrooming +bzip2 +fpzip +hdf5 +libdistributed +lua +openmp +python +sz +sz3 +unix +zfp +json +remote +netcdf ~cusz +mgard +cuda cuda_arch=90 # libpressio: CMake Error at CMakeLists.txt:498 (find_library): Could not find CUFile_LIBRARY using the following names: cufile ; +cusz: https://github.com/spack/spack/issues/38787
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse-v2/spack.yaml:  # - omega-h +cuda cuda_arch=90    # omega-h: https://github.com/spack/spack/issues/39535
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse-v2/spack.yaml:  # - py-torch +cuda cuda_arch=90   # skipped, installed by other means
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse-v2/spack.yaml:  # - slepc +cuda cuda_arch=90      # slepc: make[1]: *** internal error: invalid --jobserver-auth string 'fifo:/tmp/GMfifo1313'.
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse-v2/spack.yaml:  # - tasmanian +cuda cuda_arch=90  # tasmanian: conflicts with cuda@12
share/spack/gitlab/cloud_pipelines/stacks/e4s-neoverse-v2/spack.yaml:  # - upcxx +cuda cuda_arch=90      # upcxx: needs NVIDIA driver
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - bricks ~cuda
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - chapel ~rocm ~cuda
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - ecp-data-vis-sdk ~cuda ~rocm +adios2 +ascent +cinema +darshan +faodel +hdf5 +paraview +pnetcdf +sz +unifyfs +veloc +visit +vtkm +zfp # adios2~cuda, ascent~cuda, darshan-runtime, darshan-util, faodel, hdf5, libcatalyst, parallel-netcdf, paraview~cuda, py-cinemasci, sz, unifyfs, veloc, visit, vtk-m, zfp
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - julia ^llvm ~clang ~gold ~polly targets=amdgpu,bpf,nvptx,webassembly
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - libpressio +bitgrooming +bzip2 ~cuda ~cusz +fpzip +hdf5 +libdistributed +lua +openmp +python +sz +sz3 +unix +zfp
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - mgard +serial +openmp +timing +unstructured ~cuda
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - parsec ~cuda
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - slate ~cuda
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  # CUDA NOARCH
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - bricks +cuda
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - flux-core +cuda
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - hpctoolkit +cuda
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - papi +cuda
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - tau +mpi +cuda +syscall
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  # - legion +cuda                    # legion: needs NVIDIA driver
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  # CUDA 80
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - amrex +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - arborx +cuda cuda_arch=80 ^kokkos +wrapper
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - axom +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - cabana +cuda cuda_arch=80 ^kokkos +wrapper +cuda_lambda +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - caliper +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - chai +cuda cuda_arch=80 ^umpire ~shared
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - chapel +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - cusz +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - ecp-data-vis-sdk ~rocm +adios2 ~ascent +hdf5 +vtkm +zfp +paraview +cuda cuda_arch=80 # +ascent fails because fides fetch error
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - exago +mpi +python +raja +hiop ~rocm +cuda cuda_arch=80 ~ipopt ^hiop@1.0.0 ~sparse +mpi +raja ~rocm +cuda cuda_arch=80 #^raja@0.14.0
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - fftx +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - flecsi +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - ginkgo +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - gromacs +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - heffte +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - hpx +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - hypre +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - kokkos +wrapper +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - kokkos-kernels +cuda cuda_arch=80 ^kokkos +wrapper +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - libpressio +bitgrooming +bzip2 +fpzip +hdf5 +libdistributed +lua +openmp +python +sz +sz3 +unix +zfp +json +remote +netcdf +cusz +mgard +cuda cuda_arch=80 ^cusz +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - magma +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - mfem +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - omega-h +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - parsec +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - petsc +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - py-torch +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - raja +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - slate +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - slepc +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - strumpack ~slate +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - sundials +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - superlu-dist +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - tasmanian +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - trilinos +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - umpire ~shared +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  # INCLUDED IN ECP DAV CUDA
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - adios2 +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  # - ascent +cuda cuda_arch=80     # ascent: https://github.com/spack/spack/issues/38045
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - paraview +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - vtk-m +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - zfp +cuda cuda_arch=80
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  # - dealii +cuda cuda_arch=80     # dealii: conflicts with '+cuda ^cuda@12:'
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  # - lammps +cuda cuda_arch=80     # lammps: needs NVIDIA driver
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  # - lbann +cuda cuda_arch=80      # lbann: layers/transform/cereal_registration/../permute/cutensor_support.hpp:95:18: error: 'cutensorInit' was not declared in this scope; did you mean 'cutensorPlan_t'?
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  # - mgard +serial +openmp +timing +unstructured +cuda cuda_arch=80 # mgard: https://github.com/spack/spack/issues/44833
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  # - upcxx +cuda cuda_arch=80      # upcxx: needs NVIDIA driver
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  # CUDA 90
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - amrex +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - arborx +cuda cuda_arch=90 ^kokkos +wrapper
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - axom +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - cabana +cuda cuda_arch=90 ^kokkos +wrapper +cuda_lambda +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - caliper +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - chai +cuda cuda_arch=90 ^umpire ~shared
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - chapel +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - fftx +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - flecsi +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - ginkgo +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - gromacs +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - heffte +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - hpx +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - hypre +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - kokkos +wrapper +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - kokkos-kernels +cuda cuda_arch=90 ^kokkos +wrapper +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - magma +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - mfem +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - mgard +serial +openmp +timing +unstructured +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - parsec +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - petsc +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - raja +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - slate +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - slepc +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - strumpack ~slate +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - sundials +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - superlu-dist +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - tasmanian +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - trilinos +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - umpire ~shared +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  # INCLUDED IN ECP DAV CUDA
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - adios2 +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  # - ascent +cuda cuda_arch=90       # ascent: https://github.com/spack/spack/issues/38045
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  # - paraview +cuda cuda_arch=90     # paraview: InstallError: Incompatible cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - vtk-m +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - zfp +cuda cuda_arch=90
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  # - cusz +cuda cuda_arch=90         # cusz: cuda-12.5.0-ndrzb7undvancjdj3fi6bhthdxdo7gr5/targets/x86_64-linux/include/cub/util_device.cuh:202:50: error: 'blockDim' was not declared in this scope
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  # - dealii +cuda cuda_arch=90       # dealii: conflicts with '+cuda ^cuda@12:'; dealii: https://github.com/spack/spack/issues/39532
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  # - ecp-data-vis-sdk ~rocm +adios2 +ascent +hdf5 +vtkm +zfp +paraview +cuda cuda_arch=90 # +ascent: # ascent: https://github.com/spack/spack/issues/38045; +paraview: VTK/ThirdParty/vtkm/vtkvtkm/vtk-m/vtkm/exec/cuda/internal/ExecutionPolicy.h(121): error: namespace "thrust" has no member "sort"
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  # - lammps +cuda cuda_arch=90       # lammps: needs NVIDIA driver
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  # - lbann +cuda cuda_arch=90        # aluminum: /usr/include/c++/11/bits/basic_string.h:1260: multiple definition of `cub::CUB_200400___CUDA_ARCH_LIST___NS::Debug(cudaError, char const*, int)'; 
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  # - libpressio +bitgrooming +bzip2 +fpzip +hdf5 +libdistributed +lua +openmp +python +sz +sz3 +unix +zfp +json +remote +netcdf +cusz +mgard +cuda cuda_arch=90 ^cusz +cuda cuda_arch=90 # cusz: cuda-12.5.0-e3rny44pq5z5x3nnoljynbsq5on5fnl3/targets/x86_64-linux/include/cub/util_device.cuh:202:50: error: 'blockDim' was not declared in this scope
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  # - omega-h +cuda cuda_arch=90      # omega-h: https://github.com/spack/spack/issues/39535
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  # - py-torch +cuda cuda_arch=90     # py-torch: FAILED: caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_bwd_hdim256_bf16_sm80.cu.o
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  # - upcxx +cuda cuda_arch=90        # upcxx: needs NVIDIA driver
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  # ROCM NOARCH
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - hpctoolkit +rocm
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - tau +mpi +rocm +syscall           # tau: has issue with `spack env depfile` build
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  # ROCM 90a
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - amrex +rocm amdgpu_target=gfx90a
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - caliper +rocm amdgpu_target=gfx90a
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - chai +rocm amdgpu_target=gfx90a
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - ecp-data-vis-sdk +paraview +vtkm +rocm amdgpu_target=gfx90a
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - gasnet +rocm amdgpu_target=gfx90a
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - ginkgo +rocm amdgpu_target=gfx90a
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - heffte +rocm amdgpu_target=gfx90a
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - hpx +rocm amdgpu_target=gfx90a
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - hypre +rocm amdgpu_target=gfx90a
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - magma ~cuda +rocm amdgpu_target=gfx90a
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - mfem +rocm amdgpu_target=gfx90a
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - raja ~openmp +rocm amdgpu_target=gfx90a
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - slate +rocm amdgpu_target=gfx90a
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - strumpack ~slate +rocm amdgpu_target=gfx90a
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - sundials +rocm amdgpu_target=gfx90a
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - superlu-dist +rocm amdgpu_target=gfx90a
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - tasmanian ~openmp +rocm amdgpu_target=gfx90a
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - trilinos +amesos +amesos2 +anasazi +aztec +belos +boost +epetra +epetraext +ifpack ~ifpack2 +intrepid +intrepid2 +isorropia +kokkos +ml +minitensor +muelu +nox +piro +phalanx +rol +rythmos +sacado +stk +shards +shylu ~stokhos +stratimikos +teko +tempus +tpetra +trilinoscouplings +zoltan +zoltan2 +superlu-dist gotype=long_long +rocm amdgpu_target=gfx90a
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - umpire +rocm amdgpu_target=gfx90a
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - upcxx +rocm amdgpu_target=gfx90a
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  # INCLUDED IN ECP DAV ROCM
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  - paraview +rocm amdgpu_target=gfx90a
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  # - vtk-m ~openmp +rocm amdgpu_target=gfx90a  # vtk-m: https://github.com/spack/spack/issues/40268
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  # - adios2 +kokkos +rocm amdgpu_target=gfx90a # +kokkos: https://github.com/spack/spack/issues/44832
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  # - arborx +rocm amdgpu_target=gfx90a         # kokkos: https://github.com/spack/spack/issues/44832
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  # - cabana +rocm amdgpu_target=gfx90a         # kokkos: https://github.com/spack/spack/issues/44832
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  # - chapel +rocm amdgpu_target=gfx90a         # chapel: need chapel >= 2.2 to support ROCm >5.4
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  # - exago +mpi +python +raja +hiop +rocm amdgpu_target=gfx90a ~ipopt cxxflags="-Wno-error=non-pod-varargs" ^hiop@1.0.0 ~sparse +mpi +raja +rocm amdgpu_target=gfx90a # hiop: CMake Error at cmake/FindHiopHipLibraries.cmake:23 (find_package)
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  # - fftx +rocm amdgpu_target=gfx90a           # fftx: https://github.com/spack/spack/issues/47034
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  # - kokkos +rocm amdgpu_target=gfx90a         # kokkos: https://github.com/spack/spack/issues/44832
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  # - lbann ~cuda +rocm amdgpu_target=gfx90a    # aluminum: https://github.com/spack/spack/issues/38807
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  # - legion +rocm amdgpu_target=gfx90a         # kokkos: https://github.com/spack/spack/issues/44832
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  # - papi +rocm amdgpu_target=gfx90a           # papi: https://github.com/spack/spack/issues/27898
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  # - petsc +rocm amdgpu_target=gfx90a          # petsc: https://github.com/spack/spack/issues/44600
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  # - slepc +rocm amdgpu_target=gfx90a ^petsc +rocm amdgpu_target=gfx90a # petsc: https://github.com/spack/spack/issues/44600
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  # - cp2k +mpi +cuda cuda_arch=80              # cp2k: spack-stage-libxsmm-1.17-r2zqxa24bhufaj5i3ili5se25cw7tioo/spack-src/./src/libxsmm_gemm.c:238: undefined reference to `sgemv_'
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  # - cp2k +mpi +cuda cuda_arch=90              # cp2k: cp2k only supports cuda_arch ('35', '37', '60', '70', '80')
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  # - cp2k +mpi +rocm amdgpu_target=gfx908      # cp2k: "-L{}".format(spec["rocm"].libs.directories[0]),
share/spack/gitlab/cloud_pipelines/stacks/e4s/spack.yaml:  # - cp2k +mpi +rocm amdgpu_target=gfx90a      # cp2k: "-L{}".format(spec["rocm"].libs.directories[0]),
share/spack/gitlab/cloud_pipelines/stacks/ml-darwin-aarch64-mps/spack.yaml:      - ~cuda
share/spack/gitlab/cloud_pipelines/stacks/ml-darwin-aarch64-mps/spack.yaml:      - ~rocm
share/spack/gitlab/cloud_pipelines/stacks/ml-darwin-aarch64-mps/spack.yaml:  - py-torch-nvidia-apex
share/spack/gitlab/cloud_pipelines/.gitlab-ci.yml:# E4S ROCm External pipeline
share/spack/gitlab/cloud_pipelines/.gitlab-ci.yml:.e4s-rocm-external:
share/spack/gitlab/cloud_pipelines/.gitlab-ci.yml:    SPACK_CI_STACK_NAME: e4s-rocm-external
share/spack/gitlab/cloud_pipelines/.gitlab-ci.yml:e4s-rocm-external-generate:
share/spack/gitlab/cloud_pipelines/.gitlab-ci.yml:  extends: [ ".e4s-rocm-external", ".generate-x86_64"]
share/spack/gitlab/cloud_pipelines/.gitlab-ci.yml:  image: ghcr.io/spack/spack/ubuntu22.04-runner-amd64-gcc-11.4-rocm6.2.1:2024.10.08
share/spack/gitlab/cloud_pipelines/.gitlab-ci.yml:e4s-rocm-external-build:
share/spack/gitlab/cloud_pipelines/.gitlab-ci.yml:  extends: [ ".e4s-rocm-external", ".build" ]
share/spack/gitlab/cloud_pipelines/.gitlab-ci.yml:        job: e4s-rocm-external-generate
share/spack/gitlab/cloud_pipelines/.gitlab-ci.yml:      job: e4s-rocm-external-generate
share/spack/gitlab/cloud_pipelines/.gitlab-ci.yml:# GPU Testing Pipeline
share/spack/gitlab/cloud_pipelines/.gitlab-ci.yml:# .gpu-tests:
share/spack/gitlab/cloud_pipelines/.gitlab-ci.yml:#     SPACK_CI_STACK_NAME: gpu-tests
share/spack/gitlab/cloud_pipelines/.gitlab-ci.yml:# gpu-tests-generate:
share/spack/gitlab/cloud_pipelines/.gitlab-ci.yml:#   extends: [ ".gpu-tests", ".generate-x86_64"]
share/spack/gitlab/cloud_pipelines/.gitlab-ci.yml:# gpu-tests-build:
share/spack/gitlab/cloud_pipelines/.gitlab-ci.yml:#   extends: [ ".gpu-tests", ".build" ]
share/spack/gitlab/cloud_pipelines/.gitlab-ci.yml:#         job: gpu-tests-generate
share/spack/gitlab/cloud_pipelines/.gitlab-ci.yml:#       job: gpu-tests-generate
share/spack/gitlab/cloud_pipelines/.gitlab-ci.yml:# Machine Learning - Linux x86_64 (CUDA)
share/spack/gitlab/cloud_pipelines/.gitlab-ci.yml:.ml-linux-x86_64-cuda:
share/spack/gitlab/cloud_pipelines/.gitlab-ci.yml:    SPACK_CI_STACK_NAME: ml-linux-x86_64-cuda
share/spack/gitlab/cloud_pipelines/.gitlab-ci.yml:ml-linux-x86_64-cuda-generate:
share/spack/gitlab/cloud_pipelines/.gitlab-ci.yml:  extends: [ ".generate-x86_64", .ml-linux-x86_64-cuda, ".tags-x86_64_v4" ]
share/spack/gitlab/cloud_pipelines/.gitlab-ci.yml:ml-linux-x86_64-cuda-build:
share/spack/gitlab/cloud_pipelines/.gitlab-ci.yml:  extends: [ ".build", ".ml-linux-x86_64-cuda"  ]
share/spack/gitlab/cloud_pipelines/.gitlab-ci.yml:        job: ml-linux-x86_64-cuda-generate
share/spack/gitlab/cloud_pipelines/.gitlab-ci.yml:      job: ml-linux-x86_64-cuda-generate
share/spack/gitlab/cloud_pipelines/.gitlab-ci.yml:# Machine Learning - Linux x86_64 (ROCm)
share/spack/gitlab/cloud_pipelines/.gitlab-ci.yml:.ml-linux-x86_64-rocm:
share/spack/gitlab/cloud_pipelines/.gitlab-ci.yml:    SPACK_CI_STACK_NAME: ml-linux-x86_64-rocm
share/spack/gitlab/cloud_pipelines/.gitlab-ci.yml:ml-linux-x86_64-rocm-generate:
share/spack/gitlab/cloud_pipelines/.gitlab-ci.yml:  extends: [ ".generate-x86_64", .ml-linux-x86_64-rocm, ".tags-x86_64_v4" ]
share/spack/gitlab/cloud_pipelines/.gitlab-ci.yml:ml-linux-x86_64-rocm-build:
share/spack/gitlab/cloud_pipelines/.gitlab-ci.yml:  extends: [ ".build", ".ml-linux-x86_64-rocm" ]
share/spack/gitlab/cloud_pipelines/.gitlab-ci.yml:        job: ml-linux-x86_64-rocm-generate
share/spack/gitlab/cloud_pipelines/.gitlab-ci.yml:      job: ml-linux-x86_64-rocm-generate
share/spack/gitlab/cloud_pipelines/.gitlab-ci.yml:# Machine Learning - Linux aarch64 (CUDA)
share/spack/gitlab/cloud_pipelines/.gitlab-ci.yml:.ml-linux-aarch64-cuda:
share/spack/gitlab/cloud_pipelines/.gitlab-ci.yml:    SPACK_CI_STACK_NAME: ml-linux-aarch64-cuda
share/spack/gitlab/cloud_pipelines/.gitlab-ci.yml:ml-linux-aarch64-cuda-generate:
share/spack/gitlab/cloud_pipelines/.gitlab-ci.yml:  extends: [ ".generate-aarch64", .ml-linux-aarch64-cuda ]
share/spack/gitlab/cloud_pipelines/.gitlab-ci.yml:ml-linux-aarch64-cuda-build:
share/spack/gitlab/cloud_pipelines/.gitlab-ci.yml:  extends: [ ".build", ".ml-linux-aarch64-cuda" ]
share/spack/gitlab/cloud_pipelines/.gitlab-ci.yml:        job: ml-linux-aarch64-cuda-generate
share/spack/gitlab/cloud_pipelines/.gitlab-ci.yml:      job: ml-linux-aarch64-cuda-generate
CHANGELOG.md:       - one_of: ["+cuda", "+rocm"]
CHANGELOG.md:* Added 3 new machine learning-centric stacks to binary cache: `x86_64_v3`, CUDA, ROCm
CHANGELOG.md:* Lots of GPU updates:
CHANGELOG.md:    * ~77 CUDA-related commits
CHANGELOG.md:* HIP/ROCm support (#19715, #20095)
CHANGELOG.md:   `bison`, `cuda`, `findutils`, `flex`, `git`, `lustre` `m4`, `mpich`,
CHANGELOG.md:* ROCm packages (`hip`, `aomp`, more) added by AMD (#19957, #19832, others)
CHANGELOG.md:Several bugfixes for CUDA, binary packaging, and `spack -V`:
CHANGELOG.md:* CUDA package's `.libs` method searches for `libcudart` instead of `libcuda` (#18000)
CHANGELOG.md:* Don't set `CUDAHOSTCXX` in environments that contain CUDA (#17826)
CHANGELOG.md:* LLVM flang only builds CUDA offload components when +cuda (#17466)
CHANGELOG.md:* cuda: fix conflict statements for x86-64 targets (#13472)
etc/spack/defaults/packages.yaml:      opencl: [pocl]
var/spack/repos/builtin/packages/alpaka/package.py:class Alpaka(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/alpaka/package.py:            "cuda",
var/spack/repos/builtin/packages/alpaka/package.py:            "cuda_only",
var/spack/repos/builtin/packages/alpaka/package.py:            "sycl_gpu",
var/spack/repos/builtin/packages/alpaka/package.py:    # make sure no other backend is enabled if using cuda_only or hip_only
var/spack/repos/builtin/packages/alpaka/package.py:        "cuda",
var/spack/repos/builtin/packages/alpaka/package.py:        "sycl_gpu",
var/spack/repos/builtin/packages/alpaka/package.py:        conflicts("backend=cuda_only,%s" % v)
var/spack/repos/builtin/packages/alpaka/package.py:    conflicts("backend=cuda_only,hip_only")
var/spack/repos/builtin/packages/alpaka/package.py:    for v in ("sycl_cpu", "sycl_gpu", "sycl_fpga"):
var/spack/repos/builtin/packages/alpaka/package.py:    # todo: add conflict between cuda 11.3 and gcc 10.3.0
var/spack/repos/builtin/packages/alpaka/package.py:        if spec.satisfies("backend=cuda"):
var/spack/repos/builtin/packages/alpaka/package.py:            args.append(self.define("ALPAKA_ACC_GPU_CUDA_ENABLE", True))
var/spack/repos/builtin/packages/alpaka/package.py:        if spec.satisfies("backend=cuda_only"):
var/spack/repos/builtin/packages/alpaka/package.py:            args.append(self.define("ALPAKA_ACC_GPU_CUDA_ENABLE", True))
var/spack/repos/builtin/packages/alpaka/package.py:            args.append(self.define("ALPAKA_ACC_GPU_CUDA_ONLY_MODE", True))
var/spack/repos/builtin/packages/alpaka/package.py:            args.append(self.define("ALPAKA_ACC_GPU_HIP_ENABLE", True))
var/spack/repos/builtin/packages/alpaka/package.py:            args.append(self.define("ALPAKA_ACC_GPU_HIP_ENABLE", True))
var/spack/repos/builtin/packages/alpaka/package.py:            args.append(self.define("ALPAKA_ACC_GPU_HIP_ONLY_MODE", True))
var/spack/repos/builtin/packages/alpaka/package.py:        if spec.satisfies("backend=sycl_gpu"):
var/spack/repos/builtin/packages/alpaka/package.py:            args.append(self.define("ALPAKA_SYCL_ONEAPI_GPU", True))
var/spack/repos/builtin/packages/alpaka/package.py:            args.append(self.define("ALPAKA_SYCL_ONEAPI_GPU_DEVICES", "spir64"))
var/spack/repos/builtin/packages/hpcx-mpi/package.py:    """The HPC-X MPI implementation from NVIDIA/Mellanox based on OpenMPI.
var/spack/repos/builtin/packages/hpcx-mpi/package.py:    homepage = "https://developer.nvidia.com/networking/hpc-x"
var/spack/repos/builtin/packages/dedisp/package.py:class Dedisp(MakefilePackage, CudaPackage):
var/spack/repos/builtin/packages/dedisp/package.py:    """GPU-based dedispersion package."""
var/spack/repos/builtin/packages/dedisp/package.py:    conflicts("~cuda", msg="You must specify +cuda")
var/spack/repos/builtin/packages/dedisp/package.py:    conflicts("cuda@11.8")
var/spack/repos/builtin/packages/dedisp/package.py:    conflicts("cuda_arch=none", msg="You must specify the CUDA architecture")
var/spack/repos/builtin/packages/dedisp/package.py:    depends_on("cuda", type="build")
var/spack/repos/builtin/packages/dedisp/package.py:        makefile.filter(r"^\s*CUDA_PATH\s*\?=.*", "CUDA_PATH ?= " + spec["cuda"].prefix)
var/spack/repos/builtin/packages/dedisp/package.py:            r"^\s*GPU_ARCH\s*\?=.*", "GPU_ARCH ?= sm_" + spec.variants["cuda_arch"].value[0]
var/spack/repos/builtin/packages/ollama/package.py:class Ollama(GoPackage, CudaPackage):
var/spack/repos/builtin/packages/ollama/package.py:        if self.spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/ollama/package.py:            cuda_prefix = self.spec["cuda"].prefix
var/spack/repos/builtin/packages/ollama/package.py:            env.set("CUDACXX", cuda_prefix.bin.nvcc)
var/spack/repos/builtin/packages/ollama/package.py:            env.set("CUDA_LIB_DIR", cuda_prefix.lib)
var/spack/repos/builtin/packages/ollama/package.py:            env.set("CMAKE_CUDA_ARCHITECTURES", spec.variants["cuda_arch"].value)
var/spack/repos/builtin/packages/rmgdft/package.py:class Rmgdft(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/rmgdft/package.py:    variant("rocm", default=False, description="Build rocm enabled variant.")
var/spack/repos/builtin/packages/rmgdft/package.py:    depends_on("cuda", when="+cuda")
var/spack/repos/builtin/packages/rmgdft/package.py:    with when("+rocm"):
var/spack/repos/builtin/packages/rmgdft/package.py:        if "+cuda" in spec:
var/spack/repos/builtin/packages/rmgdft/package.py:            targets = ["rmg-gpu"]
var/spack/repos/builtin/packages/rmgdft/package.py:            cuda_arch_list = spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/rmgdft/package.py:            cuda_arch = cuda_arch_list[0]
var/spack/repos/builtin/packages/rmgdft/package.py:            if cuda_arch != "none":
var/spack/repos/builtin/packages/rmgdft/package.py:                args.append("-DCUDA_FLAGS=-arch=sm_{0}".format(cuda_arch))
var/spack/repos/builtin/packages/rmgdft/package.py:                targets.append("rmg-on-gpu")
var/spack/repos/builtin/packages/rmgdft/package.py:        if "+cuda" in spec:
var/spack/repos/builtin/packages/rmgdft/package.py:            args.append("-DRMG_CUDA_ENABLED=1")
var/spack/repos/builtin/packages/rmgdft/package.py:            if "+cuda" in spec:
var/spack/repos/builtin/packages/rmgdft/package.py:                install("rmg-gpu", prefix.bin)
var/spack/repos/builtin/packages/rmgdft/package.py:                    install("rmg-on-gpu", prefix.bin)
var/spack/repos/builtin/packages/mshadow/package.py:    """MShadow is a lightweight CPU/GPU Matrix/Tensor C++ Template Library.
var/spack/repos/builtin/packages/mshadow/package.py:    in C++/CUDA."""
var/spack/repos/builtin/packages/gaudi/package.py:    # NOTE: pocl cannot be added as a minimal OpenCL implementation because
var/spack/repos/builtin/packages/py-fastfold/package.py:    GPU Clusters."""
var/spack/repos/builtin/packages/py-fastfold/package.py:    depends_on("cuda@11.1:", type=("build", "run"))
var/spack/repos/builtin/packages/py-fastfold/package.py:    depends_on("py-torch@1.10:+cuda", type=("build", "run"))
var/spack/repos/builtin/packages/py-fastfold/package.py:    depends_on("openmm@7.7.0:+cuda", type=("build", "run"))
var/spack/repos/builtin/packages/ascent/package.py:def propagate_cuda_arch(package, spec=None):
var/spack/repos/builtin/packages/ascent/package.py:    for cuda_arch in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/ascent/package.py:            "{0} +cuda cuda_arch={1}".format(package, cuda_arch),
var/spack/repos/builtin/packages/ascent/package.py:            when="{0} +cuda cuda_arch={1}".format(spec, cuda_arch),
var/spack/repos/builtin/packages/ascent/package.py:class Ascent(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/ascent/package.py:    # patch for allowing +shared+cuda
var/spack/repos/builtin/packages/ascent/package.py:    patch("ascent-shared-cuda-pr903.patch", when="@0.8.0")
var/spack/repos/builtin/packages/ascent/package.py:        depends_on("vtk-m~cuda", when="@0.9.0: ~cuda")
var/spack/repos/builtin/packages/ascent/package.py:        depends_on("vtk-m+cuda", when="@0.9.0: +cuda")
var/spack/repos/builtin/packages/ascent/package.py:        depends_on("vtk-h+cuda", when="@:0.8.0 +cuda")
var/spack/repos/builtin/packages/ascent/package.py:        depends_on("vtk-h~cuda", when="@:0.8.0 ~cuda")
var/spack/repos/builtin/packages/ascent/package.py:    propagate_cuda_arch("vtk-h", "@:0.8.0 +vtkh")
var/spack/repos/builtin/packages/ascent/package.py:    depends_on("dray+cuda", when="@:0.8.0 +dray+cuda")
var/spack/repos/builtin/packages/ascent/package.py:    depends_on("dray~cuda", when="@:0.8.0 +dray~cuda")
var/spack/repos/builtin/packages/ascent/package.py:    propagate_cuda_arch("dray", "@:0.8.0 +dray")
var/spack/repos/builtin/packages/ascent/package.py:        "+shared", when="@:0.7 +cuda", msg="Ascent needs to be built with ~shared for CUDA builds."
var/spack/repos/builtin/packages/ascent/package.py:        # CUDA
var/spack/repos/builtin/packages/ascent/package.py:        cfg.write("# CUDA Support\n")
var/spack/repos/builtin/packages/ascent/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/ascent/package.py:            cfg.write(cmake_cache_entry("ENABLE_CUDA", "ON"))
var/spack/repos/builtin/packages/ascent/package.py:            cfg.write(cmake_cache_entry("ENABLE_CUDA", "OFF"))
var/spack/repos/builtin/packages/ascent/package.py:            if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/ascent/package.py:                cfg.write(cmake_cache_entry("VTKm_ENABLE_CUDA", "ON"))
var/spack/repos/builtin/packages/ascent/package.py:                cfg.write(cmake_cache_entry("CMAKE_CUDA_HOST_COMPILER", env["SPACK_CXX"]))
var/spack/repos/builtin/packages/ascent/package.py:                cfg.write(cmake_cache_entry("VTKm_ENABLE_CUDA", "OFF"))
var/spack/repos/builtin/packages/ascent/ascent-shared-cuda-pr903.patch:+        cuda-11.4.0-shared:
var/spack/repos/builtin/packages/ascent/ascent-shared-cuda-pr903.patch:+          containerImage: ${{ variables.ubuntu_18_cuda_11_4_0_tag }}
var/spack/repos/builtin/packages/ascent/ascent-shared-cuda-pr903.patch:-if(ENABLE_CUDA AND BUILD_SHARED_LIBS)
var/spack/repos/builtin/packages/ascent/ascent-shared-cuda-pr903.patch:-  message(FATAL_ERROR "Static libraries are required when building with CUDA")
var/spack/repos/builtin/packages/ascent/ascent-shared-cuda-pr903.patch: if(ENABLE_CUDA)
var/spack/repos/builtin/packages/ascent/ascent-shared-cuda-pr903.patch:     set (CMAKE_CUDA_SEPARABLE_COMPILATION ON CACHE BOOL "" )
var/spack/repos/builtin/packages/ascent/ascent-shared-cuda-pr903.patch:       set_source_files_properties(runtimes/expressions/ascent_array_internals.cpp  PROPERTIES LANGUAGE CUDA)
var/spack/repos/builtin/packages/ascent/ascent-shared-cuda-pr903.patch:       set_source_files_properties(runtimes/expressions/ascent_derived_jit.cpp  PROPERTIES LANGUAGE CUDA)
var/spack/repos/builtin/packages/ascent/ascent-shared-cuda-pr903.patch:-        if(CUDA_FOUND)
var/spack/repos/builtin/packages/ascent/ascent-shared-cuda-pr903.patch:-        if(CUDA_FOUND)
var/spack/repos/builtin/packages/ascent/ascent-shared-cuda-pr903.patch:-    if(VTKM_FOUND AND CUDA_FOUND)
var/spack/repos/builtin/packages/ascent/ascent-shared-cuda-pr903.patch:+    # Static linking CUDA
var/spack/repos/builtin/packages/ascent/ascent-shared-cuda-pr903.patch:+    if(VTKM_FOUND AND CUDA_FOUND AND NOT BUILD_SHARED_LIBS)
var/spack/repos/builtin/packages/ascent/ascent-shared-cuda-pr903.patch:-    if(VTKM_FOUND AND CUDA_FOUND)
var/spack/repos/builtin/packages/ascent/ascent-shared-cuda-pr903.patch:+    # Static linking CUDA
var/spack/repos/builtin/packages/ascent/ascent-shared-cuda-pr903.patch:+    if(VTKM_FOUND AND CUDA_FOUND AND NOT BUILD_SHARED_LIBS)
var/spack/repos/builtin/packages/ascent/ascent-shared-cuda-pr903.patch:@@ -26,6 +26,11 @@ if(ENABLE_CUDA AND NOT VTKm_ENABLE_CUDA)
var/spack/repos/builtin/packages/ascent/ascent-shared-cuda-pr903.patch:    message(FATAL_ERROR "Ascent CUDA support requires VTK-m with CUDA support (ENABLE_CUDA == TRUE, however VTKm_ENABLE_CUDA == FALSE")
var/spack/repos/builtin/packages/ascent/ascent-shared-cuda-pr903.patch:+if(ENABLE_CUDA AND BUILD_SHARED_LIBS)
var/spack/repos/builtin/packages/ascent/ascent-shared-cuda-pr903.patch:+    message(FATAL_ERROR "Cannot build shared libs with CUDA when VTKm is < v1.7.0")
var/spack/repos/builtin/packages/sys-sage/package.py:        "nvidia_mig",
var/spack/repos/builtin/packages/sys-sage/package.py:        description="Build and install functionality regarding NVidia MIG(multi-instance GPU, "
var/spack/repos/builtin/packages/sys-sage/package.py:    depends_on("cuda", when="+nvidia_mig platform=linux")
var/spack/repos/builtin/packages/sys-sage/package.py:    depends_on("cuda", when="+build_data_sources platform=linux")
var/spack/repos/builtin/packages/sys-sage/package.py:        args.append(self.define_from_variant("NVIDIA_MIG", "nvidia_mig"))
var/spack/repos/builtin/packages/intel-oneapi-advisor/package.py:    OpenCL code, and Python. It helps with the following: Performant
var/spack/repos/builtin/packages/intel-oneapi-advisor/package.py:    vectorization, and memory use. Efficient GPU Offload: Identify
var/spack/repos/builtin/packages/clblast/package.py:    """CLBlast is a modern, lightweight, performant and tunable OpenCL BLAS
var/spack/repos/builtin/packages/clblast/package.py:    potential of a wide variety of OpenCL devices from different vendors,
var/spack/repos/builtin/packages/clblast/package.py:    including desktop and laptop GPUs, embedded GPUs, and other accelerators.
var/spack/repos/builtin/packages/clblast/package.py:    depends_on("opencl")
var/spack/repos/builtin/packages/clblast/package.py:    depends_on("pocl+icd", when="^[virtuals=opencl] pocl")
var/spack/repos/builtin/packages/mesa/package.py:    # needing to check which llvm targets were built (ptx or amdgpu, etc.)
var/spack/repos/builtin/packages/mesa/package.py:    # ROCm 5.3.0 is providing llvm15. Gallivm coroutine is disabled in mesa upstream version
var/spack/repos/builtin/packages/mesa/package.py:    # in order to move on with ROCm 5.3.0 and ROCm 5.4.0.
var/spack/repos/builtin/packages/mesa/package.py:            "-Dgallium-opencl=disabled",
var/spack/repos/builtin/packages/survey/package.py:    AMD, ARM, and IBM P8/9 processors and integrated NVIDIA GPUs.
var/spack/repos/builtin/packages/kaldi/package.py:    variant("cuda", default=False, description="build with CUDA")
var/spack/repos/builtin/packages/kaldi/package.py:    depends_on("cuda", when="+cuda")
var/spack/repos/builtin/packages/kaldi/package.py:    depends_on("cub", when="@2019-07-29:^cuda@:10")
var/spack/repos/builtin/packages/kaldi/package.py:    patch("0001_CMakeLists_txt.patch", when="+cuda@11:")
var/spack/repos/builtin/packages/kaldi/package.py:        configure_args.append("--cub-root=" + spec["cuda"].prefix.include)
var/spack/repos/builtin/packages/kaldi/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/kaldi/package.py:            configure_args.append("--use-cuda=yes")
var/spack/repos/builtin/packages/kaldi/package.py:            configure_args.append("--cudatk-dir=" + spec["cuda"].prefix)
var/spack/repos/builtin/packages/kaldi/package.py:        if spec.satisfies("@2019-07-29: ^cuda@:10"):
var/spack/repos/builtin/packages/kaldi/0001_CMakeLists_txt.patch:+    include_directories(${CUDA_INCLUDE_DIRS})
var/spack/repos/builtin/packages/py-transformer-engine/package.py:    A library for accelerating Transformer models on NVIDIA GPUs, including fp8 precision on Hopper
var/spack/repos/builtin/packages/py-transformer-engine/package.py:    GPUs.
var/spack/repos/builtin/packages/py-transformer-engine/package.py:    homepage = "https://github.com/NVIDIA/TransformerEngine"
var/spack/repos/builtin/packages/py-transformer-engine/package.py:    url = "https://github.com/NVIDIA/TransformerEngine/archive/refs/tags/v0.0.tar.gz"
var/spack/repos/builtin/packages/py-transformer-engine/package.py:    git = "https://github.com/NVIDIA/TransformerEngine.git"
var/spack/repos/builtin/packages/py-transformer-engine/package.py:        depends_on("py-torch+cuda+cudnn")
var/spack/repos/builtin/packages/migraphx/package.py:    homepage = "https://github.com/ROCm/AMDMIGraphX"
var/spack/repos/builtin/packages/migraphx/package.py:    git = "https://github.com/ROCm/AMDMIGraphX.git"
var/spack/repos/builtin/packages/migraphx/package.py:    url = "https://github.com/ROCm/AMDMIGraphX/archive/rocm-6.1.2.tar.gz"
var/spack/repos/builtin/packages/migraphx/package.py:    tags = ["rocm"]
var/spack/repos/builtin/packages/migraphx/package.py:        "https://github.com/ROCm/AMDMIGraphX/commit/728bea3489c97c9e1ddda0a0ae527ffd2d70cb97.patch?full_index=1",
var/spack/repos/builtin/packages/migraphx/package.py:        "https://github.com/ROCm/AMDMIGraphX/commit/624f8ef549522f64fdddad7f49a2afe1890b0b79.patch?full_index=1",
var/spack/repos/builtin/packages/migraphx/package.py:        depends_on(f"rocm-cmake@{ver}:", type="build", when=f"@{ver}")
var/spack/repos/builtin/packages/migraphx/package.py:        depends_on(f"llvm-amdgpu@{ver}", when=f"@{ver}")
var/spack/repos/builtin/packages/migraphx/package.py:        depends_on(f"rocmlir@{ver}", when=f"@{ver}")
var/spack/repos/builtin/packages/migraphx/package.py:            env.set("CC", f"{self.spec['llvm-amdgpu'].prefix}/bin/clang")
var/spack/repos/builtin/packages/migraphx/package.py:            env.set("CXX", f"{self.spec['llvm-amdgpu'].prefix}/bin/clang++")
var/spack/repos/builtin/packages/migraphx/package.py:            self.define("CMAKE_CXX_COMPILER", f"{self.spec['llvm-amdgpu'].prefix}/bin/clang++"),
var/spack/repos/builtin/packages/migraphx/package.py:                self.define("GPU_TARGETS", "gfx906;gfx908;gfx90a;gfx1030;gfx1100;gfx1101;gfx1102")
var/spack/repos/builtin/packages/migraphx/0006-add-option-to-turn-off-ck.patch:diff --git a/src/targets/gpu/CMakeLists.txt b/src/targets/gpu/CMakeLists.txt
var/spack/repos/builtin/packages/migraphx/0006-add-option-to-turn-off-ck.patch:--- a/src/targets/gpu/CMakeLists.txt
var/spack/repos/builtin/packages/migraphx/0006-add-option-to-turn-off-ck.patch:+++ b/src/targets/gpu/CMakeLists.txt
var/spack/repos/builtin/packages/migraphx/0006-add-option-to-turn-off-ck.patch:@@ -98,8 +97,7 @@ rocm_clang_tidy_check(kernel_file_check)
var/spack/repos/builtin/packages/migraphx/0006-add-option-to-turn-off-ck.patch: file(GLOB JIT_GPU_SRCS CONFIGURE_DEPENDS ${CMAKE_CURRENT_SOURCE_DIR}/jit/*.cpp)
var/spack/repos/builtin/packages/migraphx/0006-add-option-to-turn-off-ck.patch:     list(REMOVE_ITEM JIT_GPU_SRCS ${CMAKE_CURRENT_SOURCE_DIR}/jit/ck_gemm.cpp)
var/spack/repos/builtin/packages/migraphx/0006-add-option-to-turn-off-ck.patch: target_link_libraries(migraphx_gpu PUBLIC migraphx MIOpen roc::rocblas)
var/spack/repos/builtin/packages/migraphx/0006-add-option-to-turn-off-ck.patch: target_link_libraries(migraphx_gpu PRIVATE migraphx_device migraphx_kernels)
var/spack/repos/builtin/packages/migraphx/0006-add-option-to-turn-off-ck.patch:     target_link_libraries(migraphx_gpu PRIVATE composable_kernel::jit_library)
var/spack/repos/builtin/packages/migraphx/0006-add-option-to-turn-off-ck.patch:+              ${CMAKE_SOURCE_DIR}/src/targets/gpu/include/migraphx/gpu/ck.hpp)
var/spack/repos/builtin/packages/migraphx/0005-Adding-half-include-directory-path-migraphx.patch: find_package(ROCM REQUIRED)
var/spack/repos/builtin/packages/rodinia/package.py:class Rodinia(MakefilePackage, CudaPackage):
var/spack/repos/builtin/packages/rodinia/package.py:    depends_on("cuda")
var/spack/repos/builtin/packages/rodinia/package.py:    conflicts("~cuda")
var/spack/repos/builtin/packages/rodinia/package.py:    build_targets = ["CUDA"]
var/spack/repos/builtin/packages/rodinia/package.py:        # set cuda paths
var/spack/repos/builtin/packages/rodinia/package.py:            "CUDA_DIR = /usr/local/cuda",
var/spack/repos/builtin/packages/rodinia/package.py:            "CUDA_DIR = {0}".format(self.spec["cuda"].prefix),
var/spack/repos/builtin/packages/rodinia/package.py:            "SDK_DIR = /usr/local/cuda-5.5/samples/",
var/spack/repos/builtin/packages/rodinia/package.py:            "SDK_DIR = {0}/samples".format(self.spec["cuda"].prefix),
var/spack/repos/builtin/packages/rodinia/package.py:        # set cuda arch flags in various makefiles
var/spack/repos/builtin/packages/rodinia/package.py:            "compute_{0}".format(spec.variants["cuda_arch"].value[0]),
var/spack/repos/builtin/packages/rodinia/package.py:            "cuda/cfd/Makefile",
var/spack/repos/builtin/packages/rodinia/package.py:            "cuda/lavaMD/makefile",
var/spack/repos/builtin/packages/rodinia/package.py:            "cuda/particlefilter/Makefile",
var/spack/repos/builtin/packages/rodinia/package.py:            "cuda/hybridsort/Makefile",
var/spack/repos/builtin/packages/rodinia/package.py:            "cuda/dwt2d/Makefile",
var/spack/repos/builtin/packages/rodinia/package.py:            "cuda/hotspot3D/Makefile",
var/spack/repos/builtin/packages/rodinia/package.py:            "cuda/b+tree/Makefile",
var/spack/repos/builtin/packages/rodinia/package.py:                "sm_[0-9]+", "sm_{0}".format(spec.variants["cuda_arch"].value[0]), makefile
var/spack/repos/builtin/packages/rodinia/package.py:        filter_file("%.o: %.[ch]", "%.o: %.c", "cuda/kmeans/Makefile", string=True)
var/spack/repos/builtin/packages/rodinia/package.py:            "cuda/mummergpu/src/suffix-tree.cpp",
var/spack/repos/builtin/packages/rodinia/package.py:        install_tree("bin/linux/cuda", prefix.bin)
var/spack/repos/builtin/packages/ghost/package.py:class Ghost(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/ghost/package.py:    clusters and on heterogenous CPU/GPU machines. For an iterative solver
var/spack/repos/builtin/packages/ghost/package.py:            self.define_from_variant("GHOST_USE_CUDA", "cuda"),
var/spack/repos/builtin/packages/flux-sched/package.py:    variant("cuda", default=False, description="Build dependencies with support for CUDA")
var/spack/repos/builtin/packages/flux-sched/package.py:    depends_on("flux-core+cuda", when="+cuda", type=("build", "run", "link"))
var/spack/repos/builtin/packages/py-rmm/package.py:    performance in GPU-centric workflows frequently requires
var/spack/repos/builtin/packages/py-rmm/package.py:    depends_on("cuda@9:")
var/spack/repos/builtin/packages/py-nvidia-ml-py/package.py:class PyNvidiaMlPy(PythonPackage):
var/spack/repos/builtin/packages/py-nvidia-ml-py/package.py:    """Python Bindings for the NVIDIA Management Library."""
var/spack/repos/builtin/packages/py-nvidia-ml-py/package.py:    homepage = "https://www.nvidia.com/"
var/spack/repos/builtin/packages/py-nvidia-ml-py/package.py:    pypi = "nvidia-ml-py/nvidia-ml-py-11.450.51.tar.gz"
var/spack/repos/builtin/packages/py-pylikwid/package.py:    variant("cuda", default=False, description="with Nvidia GPU profiling support")
var/spack/repos/builtin/packages/py-pylikwid/package.py:    depends_on("likwid", when="~cuda")
var/spack/repos/builtin/packages/py-pylikwid/package.py:    depends_on("likwid+cuda", when="+cuda")
var/spack/repos/builtin/packages/numactl/package.py:        # Remove flags not recognized by the NVIDIA compiler
var/spack/repos/builtin/packages/singularity-eos/package.py:class SingularityEos(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/singularity-eos/package.py:    # linear algebra when not using GPUs
var/spack/repos/builtin/packages/singularity-eos/package.py:    depends_on("eigen@3.3.8", when="~kokkos-kernels~cuda")
var/spack/repos/builtin/packages/singularity-eos/package.py:            "https://raw.githubusercontent.com/lanl/singularity-eos/main/utils/cuda_compatibility.patch",
var/spack/repos/builtin/packages/singularity-eos/package.py:        when="+cuda",
var/spack/repos/builtin/packages/singularity-eos/package.py:    for _flag in ("~cuda", "+cuda", "~openmp", "+openmp"):
var/spack/repos/builtin/packages/singularity-eos/package.py:    # specfic specs when using GPU/cuda offloading
var/spack/repos/builtin/packages/singularity-eos/package.py:    depends_on("kokkos +wrapper+cuda_lambda", when="+cuda+kokkos")
var/spack/repos/builtin/packages/singularity-eos/package.py:    for _flag in list(CudaPackage.cuda_arch_values):
var/spack/repos/builtin/packages/singularity-eos/package.py:        depends_on("kokkos cuda_arch=" + _flag, when="+cuda+kokkos cuda_arch=" + _flag)
var/spack/repos/builtin/packages/singularity-eos/package.py:        depends_on("kokkos-kernels cuda_arch=" + _flag, when="+cuda+kokkos cuda_arch=" + _flag)
var/spack/repos/builtin/packages/singularity-eos/package.py:        depends_on("spiner cuda_arch=" + _flag, when="+cuda+kokkos cuda_arch=" + _flag)
var/spack/repos/builtin/packages/singularity-eos/package.py:    conflicts("cuda_arch=none", when="+cuda", msg="CUDA architecture is required")
var/spack/repos/builtin/packages/singularity-eos/package.py:    conflicts("+cuda", when="~kokkos")
var/spack/repos/builtin/packages/singularity-eos/package.py:            self.define_from_variant("SINGULARITY_USE_CUDA", "cuda"),
var/spack/repos/builtin/packages/singularity-eos/package.py:        if "+kokkos+cuda" in self.spec:
var/spack/repos/builtin/packages/aws-ofi-nccl/package.py:class AwsOfiNccl(AutotoolsPackage):
var/spack/repos/builtin/packages/aws-ofi-nccl/package.py:    """AWS OFI NCCL is a plug-in which enables EC2 developers to use
var/spack/repos/builtin/packages/aws-ofi-nccl/package.py:    libfabric as a network provider while running NVIDIA's NCCL based
var/spack/repos/builtin/packages/aws-ofi-nccl/package.py:    homepage = "https://github.com/aws/aws-ofi-nccl"
var/spack/repos/builtin/packages/aws-ofi-nccl/package.py:    url = "https://github.com/aws/aws-ofi-nccl/archive/v0.0.0.tar.gz"
var/spack/repos/builtin/packages/aws-ofi-nccl/package.py:    git = "https://github.com/aws/aws-ofi-nccl.git"
var/spack/repos/builtin/packages/aws-ofi-nccl/package.py:    depends_on("cuda")
var/spack/repos/builtin/packages/aws-ofi-nccl/package.py:    depends_on("nccl")
var/spack/repos/builtin/packages/aws-ofi-nccl/package.py:        url_fmt = "https://github.com/aws/aws-ofi-nccl/archive/v{0}-aws.tar.gz"
var/spack/repos/builtin/packages/aws-ofi-nccl/package.py:    # To enable this plug-in to work with NCCL add it to the LD_LIBRARY_PATH
var/spack/repos/builtin/packages/aws-ofi-nccl/package.py:        aws_ofi_nccl_home = self.spec.prefix
var/spack/repos/builtin/packages/aws-ofi-nccl/package.py:        env.append_path("LD_LIBRARY_PATH", aws_ofi_nccl_home.lib)
var/spack/repos/builtin/packages/aws-ofi-nccl/package.py:    # To enable this plug-in to work with NCCL add it to the LD_LIBRARY_PATH
var/spack/repos/builtin/packages/aws-ofi-nccl/package.py:        aws_ofi_nccl_home = self.spec["aws-ofi-nccl"].prefix
var/spack/repos/builtin/packages/aws-ofi-nccl/package.py:        env.append_path("LD_LIBRARY_PATH", aws_ofi_nccl_home.lib)
var/spack/repos/builtin/packages/aws-ofi-nccl/package.py:                "--with-cuda={0}".format(spec["cuda"].prefix),
var/spack/repos/builtin/packages/aws-ofi-nccl/package.py:                "--with-nccl={0}".format(spec["nccl"].prefix),
var/spack/repos/builtin/packages/kineto/package.py:    """A CPU+GPU Profiling library that provides access to timeline traces
var/spack/repos/builtin/packages/kineto/package.py:    depends_on("cuda")
var/spack/repos/builtin/packages/cbtf-argonavis/package.py:    """CBTF Argo Navis project contains the CUDA collector and supporting
var/spack/repos/builtin/packages/cbtf-argonavis/package.py:    # For CUDA
var/spack/repos/builtin/packages/cbtf-argonavis/package.py:    depends_on("cuda")
var/spack/repos/builtin/packages/cbtf-argonavis/package.py:            "-DCUDA_DIR=%s" % spec["cuda"].prefix,
var/spack/repos/builtin/packages/cbtf-argonavis/package.py:            "-DCUDA_INSTALL_PATH=%s" % spec["cuda"].prefix,
var/spack/repos/builtin/packages/cbtf-argonavis/package.py:            "-DCUDA_TOOLKIT_ROOT_DIR=%s" % spec["cuda"].prefix,
var/spack/repos/builtin/packages/cbtf-argonavis/package.py:            "-DCUPTI_DIR=%s" % spec["cuda"].prefix.extras.CUPTI,
var/spack/repos/builtin/packages/cbtf-argonavis/package.py:            "-DCUPTI_ROOT=%s" % spec["cuda"].prefix.extras.CUPTI,
var/spack/repos/builtin/packages/cbtf-argonavis/package.py:        env.prepend_path("LD_LIBRARY_PATH", self.spec["cuda"].prefix + "/extras/CUPTI/lib64")
var/spack/repos/builtin/packages/cbtf-argonavis/package.py:        env.prepend_path("LD_LIBRARY_PATH", self.spec["cuda"].prefix + "/extras/CUPTI/lib64")
var/spack/repos/builtin/packages/gcc/package.py:    variant("nvptx", default=False, description="Target nvptx offloading to NVIDIA GPUs")
var/spack/repos/builtin/packages/gcc/package.py:        depends_on("cuda")
var/spack/repos/builtin/packages/gcc/package.py:            # backport of 383400a6078d upstream to allow support of cuda@11:
var/spack/repos/builtin/packages/gcc/package.py:                    "--with-cuda-driver-include={0}".format(spec["cuda"].prefix.include),
var/spack/repos/builtin/packages/gcc/package.py:                    "--with-cuda-driver-lib={0}".format(spec["cuda"].libs.directories[0]),
var/spack/repos/builtin/packages/gcc/package.py:            "--with-cuda-driver-include={0}".format(spec["cuda"].prefix.include),
var/spack/repos/builtin/packages/gcc/package.py:            "--with-cuda-driver-lib={0}".format(spec["cuda"].libs.directories[0]),
var/spack/repos/builtin/packages/gcc/stack_t.patch: uptr internal_sigprocmask(int how, __sanitizer_sigset_t *set,
var/spack/repos/builtin/packages/gcc/stack_t-4.9.patch: uptr internal_sigprocmask(int how, __sanitizer_kernel_sigset_t *set,
var/spack/repos/builtin/packages/rsbench/package.py:    # CUDA, Sycl, OpenCL, OpenMP Offload
var/spack/repos/builtin/packages/bigdft-psolver/package.py:class BigdftPsolver(AutotoolsPackage, CudaPackage):
var/spack/repos/builtin/packages/bigdft-psolver/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/bigdft-psolver/package.py:            args.append("--enable-cuda-gpu")
var/spack/repos/builtin/packages/bigdft-psolver/package.py:            args.append(f"--with-cuda-path={spec['cuda'].prefix}")
var/spack/repos/builtin/packages/bigdft-psolver/package.py:            args.append(f"--with-cuda-libs={spec['cuda'].libs.link_flags}")
var/spack/repos/builtin/packages/q-e-sirius/package.py:            "-DQE_ENABLE_CUDA=OFF",
var/spack/repos/builtin/packages/amr-wind/package.py:class AmrWind(CMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/amr-wind/package.py:    variant("gpu-aware-mpi", default=False, description="Enable GPU aware MPI")
var/spack/repos/builtin/packages/amr-wind/package.py:    depends_on("hypre+gpu-aware-mpi", when="+hypre+gpu-aware-mpi")
var/spack/repos/builtin/packages/amr-wind/package.py:    for arch in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/amr-wind/package.py:        depends_on("hypre+cuda cuda_arch=%s" % arch, when="+cuda+hypre cuda_arch=%s" % arch)
var/spack/repos/builtin/packages/amr-wind/package.py:    for arch in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/amr-wind/package.py:            "hypre+rocm amdgpu_target=%s" % arch, when="+rocm+hypre amdgpu_target=%s" % arch
var/spack/repos/builtin/packages/amr-wind/package.py:    for arch in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/amr-wind/package.py:        depends_on("ascent+cuda cuda_arch=%s" % arch, when="+ascent+cuda cuda_arch=%s" % arch)
var/spack/repos/builtin/packages/amr-wind/package.py:    conflicts("+openmp", when="+cuda")
var/spack/repos/builtin/packages/amr-wind/package.py:    conflicts("+shared", when="+cuda")
var/spack/repos/builtin/packages/amr-wind/package.py:            "cuda",
var/spack/repos/builtin/packages/amr-wind/package.py:            "rocm",
var/spack/repos/builtin/packages/amr-wind/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/amr-wind/package.py:            args.append(define("CMAKE_CUDA_ARCHITECTURES", spec.variants["cuda_arch"].value))
var/spack/repos/builtin/packages/amr-wind/package.py:        if spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/amr-wind/package.py:            targets = spec.variants["amdgpu_target"].value
var/spack/repos/builtin/packages/amr-wind/package.py:                    "AMReX's SYCL GPU Backend requires DPC++ (dpcpp) "
var/spack/repos/builtin/packages/rocm-debug-agent/0002-add-hip-architecture.patch: hip_add_executable(rocm-debug-agent-test ${SOURCES}
var/spack/repos/builtin/packages/rocm-debug-agent/0002-add-hip-architecture.patch:   HIPCC_OPTIONS --amdgpu-target=gfx900 --amdgpu-target=gfx906
var/spack/repos/builtin/packages/rocm-debug-agent/0002-add-hip-architecture.patch:                 --amdgpu-target=gfx908 --amdgpu-target=gfx90a
var/spack/repos/builtin/packages/rocm-debug-agent/package.py:class RocmDebugAgent(CMakePackage):
var/spack/repos/builtin/packages/rocm-debug-agent/package.py:    """Radeon Open Compute (ROCm) debug agent"""
var/spack/repos/builtin/packages/rocm-debug-agent/package.py:    homepage = "https://github.com/ROCm/rocr_debug_agent"
var/spack/repos/builtin/packages/rocm-debug-agent/package.py:    git = "https://github.com/ROCm/rocr_debug_agent.git"
var/spack/repos/builtin/packages/rocm-debug-agent/package.py:    url = "https://github.com/ROCm/rocr_debug_agent/archive/rocm-6.1.2.tar.gz"
var/spack/repos/builtin/packages/rocm-debug-agent/package.py:    tags = ["rocm"]
var/spack/repos/builtin/packages/rocm-debug-agent/package.py:    libraries = ["librocm-debug-agent"]
var/spack/repos/builtin/packages/rocm-debug-agent/package.py:        depends_on(f"rocm-dbgapi@{ver}", when=f"@{ver}")
var/spack/repos/builtin/packages/rocm-debug-agent/package.py:        depends_on(f"rocm-core@{ver}", when=f"@{ver}")
var/spack/repos/builtin/packages/rocm-debug-agent/package.py:    # https://github.com/ROCm/rocr_debug_agent/pull/4
var/spack/repos/builtin/packages/rocm-debug-agent/package.py:            env.set("CC", f"{self.spec['llvm-amdgpu'].prefix}/bin/clang")
var/spack/repos/builtin/packages/rocm-debug-agent/package.py:            env.set("CXX", f"{self.spec['llvm-amdgpu'].prefix}/bin/clang++")
var/spack/repos/builtin/packages/rocm-debug-agent/0001-Drop-overly-strict-Werror-flag.patch: target_include_directories(rocm-debug-agent
var/spack/repos/builtin/packages/rocm-debug-agent/0001-Drop-overly-strict-Werror-flag.patch:-target_compile_options(rocm-debug-agent PRIVATE -Werror -Wall)
var/spack/repos/builtin/packages/rocm-debug-agent/0001-Drop-overly-strict-Werror-flag.patch:+target_compile_options(rocm-debug-agent PRIVATE -Wall)
var/spack/repos/builtin/packages/rocm-debug-agent/0001-Drop-overly-strict-Werror-flag.patch: if(DEFINED ENV{ROCM_BUILD_ID})
var/spack/repos/builtin/packages/rocm-debug-agent/0001-Drop-overly-strict-Werror-flag.patch:   # ROCM_BUILD_ID is set by the ROCm-CI build environment.
var/spack/repos/builtin/packages/gdrcopy/package.py:class Gdrcopy(MakefilePackage, CudaPackage):
var/spack/repos/builtin/packages/gdrcopy/package.py:    """A fast GPU memory copy library based on NVIDIA GPUDirect
var/spack/repos/builtin/packages/gdrcopy/package.py:    homepage = "https://github.com/NVIDIA/gdrcopy"
var/spack/repos/builtin/packages/gdrcopy/package.py:    url = "https://github.com/NVIDIA/gdrcopy/archive/v2.1.tar.gz"
var/spack/repos/builtin/packages/gdrcopy/package.py:    git = "https://github.com/NVIDIA/gdrcopy"
var/spack/repos/builtin/packages/gdrcopy/package.py:    # Don't call ldconfig: https://github.com/NVIDIA/gdrcopy/pull/229
var/spack/repos/builtin/packages/gdrcopy/package.py:    # Allow tests to build against libcuda.so stub
var/spack/repos/builtin/packages/gdrcopy/package.py:        "https://github.com/NVIDIA/gdrcopy/commit/508dd6179dcb04ba7720e2da5124b77bbdb615b0.patch?full_index=1",
var/spack/repos/builtin/packages/gdrcopy/package.py:    requires("+cuda")
var/spack/repos/builtin/packages/gdrcopy/package.py:        env.set("CUDA", self.spec["cuda"].prefix)
var/spack/repos/builtin/packages/nccl-tests/package.py:class NcclTests(MakefilePackage, CudaPackage):
var/spack/repos/builtin/packages/nccl-tests/package.py:    the correctness of NCCL operations."""
var/spack/repos/builtin/packages/nccl-tests/package.py:    homepage = "https://github.com/NVIDIA/nccl-tests"
var/spack/repos/builtin/packages/nccl-tests/package.py:    url = "https://github.com/NVIDIA/nccl-tests/archive/v2.0.0.tar.gz"
var/spack/repos/builtin/packages/nccl-tests/package.py:    variant("cuda", default=True, description="with CUDA support, must be true")
var/spack/repos/builtin/packages/nccl-tests/package.py:    conflicts("~cuda", msg="nccl-tests require cuda")
var/spack/repos/builtin/packages/nccl-tests/package.py:    depends_on("nccl")
var/spack/repos/builtin/packages/nccl-tests/package.py:    depends_on("cuda")
var/spack/repos/builtin/packages/nccl-tests/package.py:        targets.append("CUDA_HOME={0}".format(self.spec["cuda"].prefix))
var/spack/repos/builtin/packages/nccl-tests/package.py:        targets.append("NCCL_HOME={0}".format(self.spec["nccl"].prefix))
var/spack/repos/builtin/packages/nccl-tests/package.py:        cuda_arch = self.spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/nccl-tests/package.py:        cuda_gencode = " ".join(self.cuda_flags(cuda_arch))
var/spack/repos/builtin/packages/nccl-tests/package.py:        env.set("NVCC_GENCODE", cuda_gencode)
var/spack/repos/builtin/packages/quantum-espresso/package.py:    # Add Cuda Fortran support
var/spack/repos/builtin/packages/quantum-espresso/package.py:    # depends on NVHPC compiler, not directly on CUDA toolkit
var/spack/repos/builtin/packages/quantum-espresso/package.py:        variant("cuda", default=False, description="Build with CUDA Fortran")
var/spack/repos/builtin/packages/quantum-espresso/package.py:        with when("+cuda"):
var/spack/repos/builtin/packages/quantum-espresso/package.py:            # GPUs are enabled since v6.6
var/spack/repos/builtin/packages/quantum-espresso/package.py:            # cuda version >= 10.1
var/spack/repos/builtin/packages/quantum-espresso/package.py:            # conflicts("cuda@:10.0.130")
var/spack/repos/builtin/packages/quantum-espresso/package.py:                    msg="bugs with NVHPCSDK from v21.11 to v22.3, OpenMP and GPU",
var/spack/repos/builtin/packages/quantum-espresso/package.py:            conflicts("build_system=generic", msg="Only CMake supported for GPU-enabled version")
var/spack/repos/builtin/packages/quantum-espresso/package.py:    # requires linking to CUDA runtime APIs , handled by CMake
var/spack/repos/builtin/packages/quantum-espresso/package.py:    with when("+nvtx~cuda"):
var/spack/repos/builtin/packages/quantum-espresso/package.py:        depends_on("cuda")
var/spack/repos/builtin/packages/quantum-espresso/package.py:        with when("%nvhpc+cuda"):
var/spack/repos/builtin/packages/quantum-espresso/package.py:            # add mpi_gpu_aware variant, False by default
var/spack/repos/builtin/packages/quantum-espresso/package.py:            variant("mpigpu", default=False, description="Enables GPU-aware MPI operations")
var/spack/repos/builtin/packages/quantum-espresso/package.py:    # Configure updated to work with NVIDIA compilers
var/spack/repos/builtin/packages/quantum-espresso/package.py:            self.define_from_variant("QE_ENABLE_CUDA", "cuda"),
var/spack/repos/builtin/packages/quantum-espresso/package.py:            self.define_from_variant("QE_ENABLE_MPI_GPU_AWARE", "mpigpu"),
var/spack/repos/builtin/packages/quantum-espresso/package.py:        if "+cuda" in self.spec:
var/spack/repos/builtin/packages/quantum-espresso/package.py:            cmake_args.append(self.define("QE_ENABLE_OPENACC", True))
var/spack/repos/builtin/packages/py-accelerate/package.py:    """A simple way to train and use PyTorch models with multi-GPU, TPU, mixed-precision."""
var/spack/repos/builtin/packages/openmpi/accelerator-build-components-as-dso-s-by-default.patch:also need to switch rcache/gpsum and rcache/rgpusum
var/spack/repos/builtin/packages/openmpi/accelerator-build-components-as-dso-s-by-default.patch:+           enable_mca_dso="accelerator-cuda,accelerator-rocm,accelerator-ze,btl-smcuda,rcache-gpusm,rcache-rgpusm"
var/spack/repos/builtin/packages/openmpi/package.py:class Openmpi(AutotoolsPackage, CudaPackage):
var/spack/repos/builtin/packages/openmpi/package.py:    # Patches to accelerator CUDA component to link in libcuda
var/spack/repos/builtin/packages/openmpi/package.py:    patch("accelerator-cuda-fix-bug-in-makefile.patch", when="@5.0.0")
var/spack/repos/builtin/packages/openmpi/package.py:    patch("btlsmcuda-fix-problem-with-makefile.patch", when="@5.0.0")
var/spack/repos/builtin/packages/openmpi/package.py:    depends_on("hwloc +cuda", when="+cuda ~internal-hwloc")
var/spack/repos/builtin/packages/openmpi/package.py:    depends_on("cuda", type=("build", "link", "run"), when="@5: +cuda")
var/spack/repos/builtin/packages/openmpi/package.py:    # CUDA support was added in 1.7, and since the variant is part of the
var/spack/repos/builtin/packages/openmpi/package.py:    conflicts("+cuda", when="@:1.6")
var/spack/repos/builtin/packages/openmpi/package.py:            # cuda
var/spack/repos/builtin/packages/openmpi/package.py:                r'parameter "mpi_built_with_cuda_support" ' + r'\(current value: "(\S+)"', output
var/spack/repos/builtin/packages/openmpi/package.py:                variants.append("+cuda")
var/spack/repos/builtin/packages/openmpi/package.py:                variants.append("~cuda")
var/spack/repos/builtin/packages/openmpi/package.py:        # CUDA support
var/spack/repos/builtin/packages/openmpi/package.py:        # See https://www.open-mpi.org/faq/?category=buildcuda
var/spack/repos/builtin/packages/openmpi/package.py:        if "+cuda" in spec:
var/spack/repos/builtin/packages/openmpi/package.py:            # OpenMPI dynamically loads libcuda.so, requires dlopen
var/spack/repos/builtin/packages/openmpi/package.py:            config_args.append("--with-cuda={0}".format(spec["cuda"].prefix))
var/spack/repos/builtin/packages/openmpi/package.py:                    "--with-cuda-libdir={0}".format(spec["cuda"].libs.directories[0])
var/spack/repos/builtin/packages/openmpi/package.py:                    "--with-cuda-libdir={0}".format(spec["cuda"].libs.directories[0] + "/stubs")
var/spack/repos/builtin/packages/openmpi/package.py:            if spec.satisfies("%pgi^cuda@7.0:7"):
var/spack/repos/builtin/packages/openmpi/package.py:                # OpenMPI has problems with CUDA 7 and PGI
var/spack/repos/builtin/packages/openmpi/package.py:            config_args.append("--without-cuda")
var/spack/repos/builtin/packages/openmpi/btlsmcuda-fix-problem-with-makefile.patch:Subject: [PATCH] btlsmcuda: fix problem with makefile
var/spack/repos/builtin/packages/openmpi/btlsmcuda-fix-problem-with-makefile.patch:when libcuda.so is in a non-standard location.
var/spack/repos/builtin/packages/openmpi/btlsmcuda-fix-problem-with-makefile.patch:also fix rcache/gpusm and rcache/rgpsum
var/spack/repos/builtin/packages/openmpi/btlsmcuda-fix-problem-with-makefile.patch:diff --git a/opal/mca/btl/smcuda/Makefile.am b/opal/mca/btl/smcuda/Makefile.am
var/spack/repos/builtin/packages/openmpi/btlsmcuda-fix-problem-with-makefile.patch:--- a/opal/mca/btl/smcuda/Makefile.am
var/spack/repos/builtin/packages/openmpi/btlsmcuda-fix-problem-with-makefile.patch:+++ b/opal/mca/btl/smcuda/Makefile.am
var/spack/repos/builtin/packages/openmpi/btlsmcuda-fix-problem-with-makefile.patch: mca_btl_smcuda_la_SOURCES = $(libmca_btl_smcuda_la_sources)
var/spack/repos/builtin/packages/openmpi/btlsmcuda-fix-problem-with-makefile.patch:-mca_btl_smcuda_la_LDFLAGS = -module -avoid-version
var/spack/repos/builtin/packages/openmpi/btlsmcuda-fix-problem-with-makefile.patch:+mca_btl_smcuda_la_LDFLAGS = -module -avoid-version $(btl_smcuda_LDFLAGS)
var/spack/repos/builtin/packages/openmpi/btlsmcuda-fix-problem-with-makefile.patch: mca_btl_smcuda_la_LIBADD = $(top_builddir)/opal/lib@OPAL_LIB_NAME@.la \
var/spack/repos/builtin/packages/openmpi/btlsmcuda-fix-problem-with-makefile.patch:     $(btl_smcuda_LIBS)
var/spack/repos/builtin/packages/openmpi/btlsmcuda-fix-problem-with-makefile.patch:@@ -59,6 +59,6 @@ mca_btl_smcuda_la_CPPFLAGS = $(btl_smcuda_CPPFLAGS)
var/spack/repos/builtin/packages/openmpi/btlsmcuda-fix-problem-with-makefile.patch: libmca_btl_smcuda_la_SOURCES = $(libmca_btl_smcuda_la_sources)
var/spack/repos/builtin/packages/openmpi/btlsmcuda-fix-problem-with-makefile.patch:-libmca_btl_smcuda_la_LDFLAGS = -module -avoid-version
var/spack/repos/builtin/packages/openmpi/btlsmcuda-fix-problem-with-makefile.patch:+libmca_btl_smcuda_la_LDFLAGS = -module -avoid-version $(btl_smcuda_LDFLAGS)
var/spack/repos/builtin/packages/openmpi/btlsmcuda-fix-problem-with-makefile.patch: libmca_btl_smcuda_la_CPPFLAGS = $(btl_smcuda_CPPFLAGS)
var/spack/repos/builtin/packages/openmpi/btlsmcuda-fix-problem-with-makefile.patch: libmca_btl_smcuda_la_LIBADD = $(btl_smcuda_LIBS)
var/spack/repos/builtin/packages/openmpi/btlsmcuda-fix-problem-with-makefile.patch:diff --git a/opal/mca/rcache/gpusm/Makefile.am b/opal/mca/rcache/gpusm/Makefile.am
var/spack/repos/builtin/packages/openmpi/btlsmcuda-fix-problem-with-makefile.patch:--- a/opal/mca/rcache/gpusm/Makefile.am
var/spack/repos/builtin/packages/openmpi/btlsmcuda-fix-problem-with-makefile.patch:+++ b/opal/mca/rcache/gpusm/Makefile.am
var/spack/repos/builtin/packages/openmpi/btlsmcuda-fix-problem-with-makefile.patch: mca_rcache_gpusm_la_SOURCES = $(sources)
var/spack/repos/builtin/packages/openmpi/btlsmcuda-fix-problem-with-makefile.patch:-mca_rcache_gpusm_la_LDFLAGS = -module -avoid-version
var/spack/repos/builtin/packages/openmpi/btlsmcuda-fix-problem-with-makefile.patch:+mca_rcache_gpusm_la_LDFLAGS = -module -avoid-version $(rcache_gpusm_LDFLAGS)
var/spack/repos/builtin/packages/openmpi/btlsmcuda-fix-problem-with-makefile.patch: mca_rcache_gpusm_la_LIBADD = $(top_builddir)/opal/lib@OPAL_LIB_NAME@.la \
var/spack/repos/builtin/packages/openmpi/btlsmcuda-fix-problem-with-makefile.patch: 	$(rcache_gpusm_LIBS)
var/spack/repos/builtin/packages/openmpi/btlsmcuda-fix-problem-with-makefile.patch: libmca_rcache_gpusm_la_SOURCES = $(sources)
var/spack/repos/builtin/packages/openmpi/btlsmcuda-fix-problem-with-makefile.patch:-libmca_rcache_gpusm_la_LDFLAGS = -module -avoid-version
var/spack/repos/builtin/packages/openmpi/btlsmcuda-fix-problem-with-makefile.patch:+libmca_rcache_gpusm_la_LDFLAGS = -module -avoid-version $(rcache_gpusm_LDFLAGS)
var/spack/repos/builtin/packages/openmpi/btlsmcuda-fix-problem-with-makefile.patch: libmca_rcache_gpusm_la_LIBADD = $(rcache_gpusm_LIBS)
var/spack/repos/builtin/packages/openmpi/btlsmcuda-fix-problem-with-makefile.patch:diff --git a/opal/mca/rcache/rgpusm/Makefile.am b/opal/mca/rcache/rgpusm/Makefile.am
var/spack/repos/builtin/packages/openmpi/btlsmcuda-fix-problem-with-makefile.patch:--- a/opal/mca/rcache/rgpusm/Makefile.am
var/spack/repos/builtin/packages/openmpi/btlsmcuda-fix-problem-with-makefile.patch:+++ b/opal/mca/rcache/rgpusm/Makefile.am
var/spack/repos/builtin/packages/openmpi/btlsmcuda-fix-problem-with-makefile.patch: mca_rcache_rgpusm_la_SOURCES = $(sources)
var/spack/repos/builtin/packages/openmpi/btlsmcuda-fix-problem-with-makefile.patch:-mca_rcache_rgpusm_la_LDFLAGS = -module -avoid-version
var/spack/repos/builtin/packages/openmpi/btlsmcuda-fix-problem-with-makefile.patch:+mca_rcache_rgpusm_la_LDFLAGS = -module -avoid-version $(rcache_rgpusm_LDFLAGS)
var/spack/repos/builtin/packages/openmpi/btlsmcuda-fix-problem-with-makefile.patch: mca_rcache_rgpusm_la_LIBADD = $(top_builddir)/opal/lib@OPAL_LIB_NAME@.la \
var/spack/repos/builtin/packages/openmpi/btlsmcuda-fix-problem-with-makefile.patch: 	$(rcache_rgpusm_LIBS)
var/spack/repos/builtin/packages/openmpi/btlsmcuda-fix-problem-with-makefile.patch: libmca_rcache_rgpusm_la_SOURCES = $(sources)
var/spack/repos/builtin/packages/openmpi/btlsmcuda-fix-problem-with-makefile.patch:-libmca_rcache_rgpusm_la_LDFLAGS = -module -avoid-version
var/spack/repos/builtin/packages/openmpi/btlsmcuda-fix-problem-with-makefile.patch:+libmca_rcache_rgpusm_la_LDFLAGS = -module -avoid-version $(rcache_rgpusm_LDFLAGS)
var/spack/repos/builtin/packages/openmpi/btlsmcuda-fix-problem-with-makefile.patch: libmca_rcache_rgpusm_la_LIBADD = $(rcache_rgpusm_LIBS)
var/spack/repos/builtin/packages/openmpi/accelerator-cuda-fix-bug-in-makefile.patch:Subject: [PATCH] accelerator/cuda: fix bug in makefile.am
var/spack/repos/builtin/packages/openmpi/accelerator-cuda-fix-bug-in-makefile.patch:that prevents correct linkage of libcuda.so if it is in
var/spack/repos/builtin/packages/openmpi/accelerator-cuda-fix-bug-in-makefile.patch:diff --git a/opal/mca/accelerator/cuda/Makefile.am b/opal/mca/accelerator/cuda/Makefile.am
var/spack/repos/builtin/packages/openmpi/accelerator-cuda-fix-bug-in-makefile.patch:--- a/opal/mca/accelerator/cuda/Makefile.am
var/spack/repos/builtin/packages/openmpi/accelerator-cuda-fix-bug-in-makefile.patch:+++ b/opal/mca/accelerator/cuda/Makefile.am
var/spack/repos/builtin/packages/openmpi/accelerator-cuda-fix-bug-in-makefile.patch: mca_accelerator_cuda_la_SOURCES = $(sources)
var/spack/repos/builtin/packages/openmpi/accelerator-cuda-fix-bug-in-makefile.patch:-mca_accelerator_cuda_la_LDFLAGS = -module -avoid-version
var/spack/repos/builtin/packages/openmpi/accelerator-cuda-fix-bug-in-makefile.patch:+mca_accelerator_cuda_la_LDFLAGS = -module -avoid-version $(accelerator_cuda_LDFLAGS)
var/spack/repos/builtin/packages/openmpi/accelerator-cuda-fix-bug-in-makefile.patch: mca_accelerator_cuda_la_LIBADD = $(top_builddir)/opal/lib@OPAL_LIB_NAME@.la \
var/spack/repos/builtin/packages/openmpi/accelerator-cuda-fix-bug-in-makefile.patch:         $(accelerator_cuda_LIBS)
var/spack/repos/builtin/packages/openmpi/accelerator-cuda-fix-bug-in-makefile.patch: libmca_accelerator_cuda_la_SOURCES =$(sources)
var/spack/repos/builtin/packages/openmpi/accelerator-cuda-fix-bug-in-makefile.patch:-libmca_accelerator_cuda_la_LDFLAGS = -module -avoid-version
var/spack/repos/builtin/packages/openmpi/accelerator-cuda-fix-bug-in-makefile.patch:+libmca_accelerator_cuda_la_LDFLAGS = -module -avoid-version $(accelerator_cuda_LDFLAGS)
var/spack/repos/builtin/packages/openmpi/accelerator-cuda-fix-bug-in-makefile.patch: libmca_accelerator_cuda_la_LIBADD = $(accelerator_cuda_LIBS)
var/spack/repos/builtin/packages/root/package.py:    variant("cuda", when="@6.08.00:", default=False, description="Enable CUDA support")
var/spack/repos/builtin/packages/root/package.py:        "tmva-gpu",
var/spack/repos/builtin/packages/root/package.py:        description="Build TMVA with GPU support for deep learning (requries CUDA)",
var/spack/repos/builtin/packages/root/package.py:    depends_on("cuda", when="+tmva-gpu")
var/spack/repos/builtin/packages/root/package.py:    depends_on("cuda", when="+cuda")
var/spack/repos/builtin/packages/root/package.py:    depends_on("cuda", when="+cudnn")
var/spack/repos/builtin/packages/root/package.py:    conflicts("+tmva-gpu", when="~tmva", msg="root+tmva-gpu requires TMVA")
var/spack/repos/builtin/packages/root/package.py:    conflicts("+tmva-gpu", when="~cuda", msg="root+tmva-gpu requires CUDA")
var/spack/repos/builtin/packages/root/package.py:        _add_variant(v, f, "tmva-gpu", "+tmva-gpu")
var/spack/repos/builtin/packages/root/package.py:            options.append(define_from_variant("cuda"))
var/spack/repos/builtin/packages/root/package.py:            options.append(define_from_variant("tmva-gpu"))
var/spack/repos/builtin/packages/citcoms/package.py:    variant("cuda", default=False, description="use CUDA")
var/spack/repos/builtin/packages/citcoms/package.py:    depends_on("cuda", when="+cuda")
var/spack/repos/builtin/packages/citcoms/package.py:        if self.spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/citcoms/package.py:            args.append("--with-cuda")
var/spack/repos/builtin/packages/citcoms/package.py:            args.append("--without-cuda")
var/spack/repos/builtin/packages/hdf5-vfd-gds/package.py:class Hdf5VfdGds(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/hdf5-vfd-gds/package.py:    """This package enables GPU Direct Storage Virtual File Driver in HDF5."""
var/spack/repos/builtin/packages/hdf5-vfd-gds/package.py:    conflicts("~cuda")
var/spack/repos/builtin/packages/hdf5-vfd-gds/package.py:    conflicts("^cuda@:11.7.0")
var/spack/repos/builtin/packages/gaussian-src/16-C.01-replace-deprecated-pgf77-with-pgfortran.patch: GPUFLAG = $(GPUFLAG1) $(GPUFLAG2) $(GPUFLAG3)
var/spack/repos/builtin/packages/gaussian-src/16-C.01-replace-deprecated-pgf77-with-pgfortran.patch: RUNF77 = $(PGNAME) $(PGISTATIC) $(I8FLAG) $(R8FLAG) $(MMODEL) $(DEBUGF) $(SPECFLAG) $(GPUFLAG)
var/spack/repos/builtin/packages/gaussian-src/16-C.01-replace-deprecated-pgf77-with-pgfortran.patch:   set acclib = "-Mcudalib=cublas"
var/spack/repos/builtin/packages/gaussian-src/package.py:        #  if spec.satisfies('+cuda'):
var/spack/repos/builtin/packages/gaussian-src/package.py:        #      opts += [spec.variants['cuda_family'].value]
var/spack/repos/builtin/packages/nvshmem/package.py:class Nvshmem(MakefilePackage, CudaPackage):
var/spack/repos/builtin/packages/nvshmem/package.py:    provides efficient and scalable communication for NVIDIA GPU
var/spack/repos/builtin/packages/nvshmem/package.py:    the memory of multiple GPUs and can be accessed with fine-grained
var/spack/repos/builtin/packages/nvshmem/package.py:    GPU-initiated operations, CPU-initiated operations, and operations on
var/spack/repos/builtin/packages/nvshmem/package.py:    CUDA streams."""
var/spack/repos/builtin/packages/nvshmem/package.py:    homepage = "https://developer.nvidia.com/nvshmem"
var/spack/repos/builtin/packages/nvshmem/package.py:    variant("cuda", default=True, description="Build with CUDA")
var/spack/repos/builtin/packages/nvshmem/package.py:    variant("nccl", default=True, description="Build with NCCL support")
var/spack/repos/builtin/packages/nvshmem/package.py:        "gpu_initiated_support",
var/spack/repos/builtin/packages/nvshmem/package.py:        description="Build with support for GPU initiated communication",
var/spack/repos/builtin/packages/nvshmem/package.py:    conflicts("~cuda")
var/spack/repos/builtin/packages/nvshmem/package.py:        url_fmt = "https://developer.download.nvidia.com/compute/redist/nvshmem/{0}/source/nvshmem_src_{1}.txz"
var/spack/repos/builtin/packages/nvshmem/package.py:    depends_on("nccl", when="+nccl")
var/spack/repos/builtin/packages/nvshmem/package.py:        env.set("CUDA_HOME", self.spec["cuda"].prefix)
var/spack/repos/builtin/packages/nvshmem/package.py:        if "+nccl" in self.spec:
var/spack/repos/builtin/packages/nvshmem/package.py:            env.set("NVSHMEM_USE_NCCL", "1")
var/spack/repos/builtin/packages/nvshmem/package.py:            env.set("NCCL_HOME", self.spec["nccl"].prefix)
var/spack/repos/builtin/packages/nvshmem/package.py:        if "+gpu_initiated_support" in self.spec:
var/spack/repos/builtin/packages/nvshmem/package.py:            env.set("NVSHMEM_GPUINITIATED_SUPPORT", "1")
var/spack/repos/builtin/packages/scorep/package.py:    variant("cuda", default=False, description="Enable CUDA support")
var/spack/repos/builtin/packages/scorep/package.py:    variant("hip", default=False, description="Enable ROCm/HIP support", when="@8.0:")
var/spack/repos/builtin/packages/scorep/package.py:    depends_on("cuda@7:", when="@8.0:+cuda")
var/spack/repos/builtin/packages/scorep/package.py:    depends_on("cuda", when="+cuda")
var/spack/repos/builtin/packages/scorep/package.py:    depends_on("rocm-smi-lib", when="+hip")
var/spack/repos/builtin/packages/scorep/package.py:    # Score-P first has support for ROCm 6.x as of v8.4
var/spack/repos/builtin/packages/scorep/package.py:        renames = {"cce": "cray", "rocmcc": "amdclang"}
var/spack/repos/builtin/packages/scorep/package.py:        if "+cuda" in spec:
var/spack/repos/builtin/packages/scorep/package.py:            config_args.append("--with-libcudart=%s" % spec["cuda"].prefix)
var/spack/repos/builtin/packages/scorep/package.py:            cuda_driver_path = self.find_libpath("libcuda", spec["cuda"].prefix)
var/spack/repos/builtin/packages/scorep/package.py:            config_args.append("--with-libcuda-lib=%s" % cuda_driver_path)
var/spack/repos/builtin/packages/scorep/package.py:            config_args.append("--with-rocm=%s" % spec["hip"].prefix)
var/spack/repos/builtin/packages/hip-rocclr/package.py:    homepage = "https://github.com/ROCm/ROCclr"
var/spack/repos/builtin/packages/hip-rocclr/package.py:    url = "https://github.com/ROCm/ROCclr/archive/rocm-5.6.1.tar.gz"
var/spack/repos/builtin/packages/hip-rocclr/package.py:    git = "https://github.com/ROCm/ROCclr.git"
var/spack/repos/builtin/packages/hip-rocclr/package.py:    tags = ["rocm"]
var/spack/repos/builtin/packages/hip-rocclr/package.py:    # Add opencl sources thru the below
var/spack/repos/builtin/packages/hip-rocclr/package.py:            name="opencl-on-vdi",
var/spack/repos/builtin/packages/hip-rocclr/package.py:            url=f"https://github.com/ROCm/ROCm-OpenCL-Runtime/archive/rocm-{d_version}.tar.gz",
var/spack/repos/builtin/packages/hip-rocclr/package.py:            placement="opencl-on-vdi",
var/spack/repos/builtin/packages/hip-rocclr/package.py:        name="opencl-on-vdi",
var/spack/repos/builtin/packages/hip-rocclr/package.py:        git="https://github.com/ROCm/ROCm-OpenCL-Runtime.git",
var/spack/repos/builtin/packages/hip-rocclr/package.py:        placement="opencl-on-vdi",
var/spack/repos/builtin/packages/hip-rocclr/package.py:            self.define("OPENCL_DIR", join_path(self.stage.source_path, "opencl-on-vdi")),
var/spack/repos/builtin/packages/rocm-openmp-extras/0001-Avoid-duplicate-registration-on-cuda-env-6.2.patch:diff --git a/rocm-openmp-extras/llvm-project/openmp/libomptarget/plugins-nextgen/cuda/CMakeLists.txt b/rocm-openmp-extras/llvm-project/openmp/libomptarget/plugins-nextgen/cuda/CMakeLists.txt
var/spack/repos/builtin/packages/rocm-openmp-extras/0001-Avoid-duplicate-registration-on-cuda-env-6.2.patch:--- a/rocm-openmp-extras/llvm-project/openmp/libomptarget/plugins-nextgen/cuda/CMakeLists.txt
var/spack/repos/builtin/packages/rocm-openmp-extras/0001-Avoid-duplicate-registration-on-cuda-env-6.2.patch:+++ b/rocm-openmp-extras/llvm-project/openmp/libomptarget/plugins-nextgen/cuda/CMakeLists.txt
var/spack/repos/builtin/packages/rocm-openmp-extras/0001-Avoid-duplicate-registration-on-cuda-env-6.2.patch:@@ -68,8 +68,6 @@ target_include_directories(omptarget.rtl.cuda PRIVATE ${LIBOMPTARGET_INCLUDE_DIR
var/spack/repos/builtin/packages/rocm-openmp-extras/0001-Avoid-duplicate-registration-on-cuda-env-6.2.patch: option(LIBOMPTARGET_FORCE_NVIDIA_TESTS "Build NVIDIA libomptarget tests" OFF)
var/spack/repos/builtin/packages/rocm-openmp-extras/0001-Avoid-duplicate-registration-on-cuda-env-6.2.patch: if (LIBOMPTARGET_FOUND_NVIDIA_GPU OR LIBOMPTARGET_FORCE_NVIDIA_TESTS)
var/spack/repos/builtin/packages/rocm-openmp-extras/0001-Avoid-duplicate-registration-on-cuda-env-6.2.patch:   libomptarget_say("Enable tests using CUDA plugin")
var/spack/repos/builtin/packages/rocm-openmp-extras/0001-Avoid-duplicate-registration-on-cuda-env-6.2.patch:-      "${LIBOMPTARGET_SYSTEM_TARGETS} nvptx64-nvidia-cuda nvptx64-nvidia-cuda-LTO" PARENT_SCOPE)
var/spack/repos/builtin/packages/rocm-openmp-extras/0001-Avoid-duplicate-registration-on-cuda-env-6.2.patch:   list(APPEND LIBOMPTARGET_TESTED_PLUGINS "omptarget.rtl.cuda")
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:tools_url = "https://github.com/ROCm"
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:compute_url = "https://github.com/ROCm"
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:class RocmOpenmpExtras(Package):
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:    """OpenMP support for ROCm LLVM."""
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:    url = tools_url + "/aomp/archive/rocm-6.1.2.tar.gz"
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:    tags = ["rocm"]
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:        depends_on(f"rocm-core@{ver}", when=f"@{ver}")
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:        depends_on(f"llvm-amdgpu@{ver}", when=f"@{ver}")
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:            name="rocm-device-libs",
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:            url=f"{compute_url}/ROCm-Device-Libs/archive/rocm-{ver}.tar.gz",
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:            destination="rocm-openmp-extras",
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:            placement="rocm-device-libs",
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:            url=f"{tools_url}/flang/archive/rocm-{ver}.tar.gz",
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:            destination="rocm-openmp-extras",
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:            url=f"{tools_url}/aomp-extras/archive/rocm-{ver}.tar.gz",
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:            destination="rocm-openmp-extras",
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:            url=f"{compute_url}/llvm-project/archive/rocm-{ver}.tar.gz",
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:            destination="rocm-openmp-extras",
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:        depends_on(f"llvm-amdgpu@{ver}", when=f"@{ver}")
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:            url=f"{tools_url}/flang/archive/rocm-{ver}.tar.gz",
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:            destination="rocm-openmp-extras",
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:            url=f"{tools_url}/aomp-extras/archive/rocm-{ver}.tar.gz",
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:            destination="rocm-openmp-extras",
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:            url=f"{compute_url}/llvm-project/archive/rocm-{ver}.tar.gz",
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:            destination="rocm-openmp-extras",
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:        working_dir="rocm-openmp-extras/llvm-project/openmp/libomptarget",
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:    patch("0001-Avoid-duplicate-registration-on-cuda-env.patch", when="@6.1")
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:    patch("0001-Avoid-duplicate-registration-on-cuda-env-6.2.patch", when="@6.2")
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:        devlibs_prefix = self.spec["llvm-amdgpu"].prefix
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:        openmp_extras_prefix = self.spec["rocm-openmp-extras"].prefix
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:        llvm_prefix = self.spec["llvm-amdgpu"].prefix
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:        env.set("AOMP_GPU", "`{0}/bin/mygpu`".format(openmp_extras_prefix))
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:        openmp_extras_prefix = self.spec["rocm-openmp-extras"].prefix
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:        llvm_prefix = self.spec["llvm-amdgpu"].prefix
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:                self.spec["llvm-amdgpu"].prefix.bin.clang
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:        libomptarget = "{0}/rocm-openmp-extras/llvm-project/openmp/libomptarget"
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:        flang = "{0}/rocm-openmp-extras/flang/"
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:        plugin = "/plugins/amdgpu/CMakeLists.txt"
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:                "{ROCM_DIR}/amdgcn/bitcode",
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:                "-nogpulib",
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:                "-nogpulib -nogpuinc",
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:                "-x hip -nogpulib -nogpuinc",
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:                "-c -nogpulib -nogpuinc -I{LIMIT}",
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:                r"${ROCM_DIR}/hsa/include ${ROCM_DIR}/hsa/include/hsa",
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:            filter_file("{ROCM_DIR}/hsa/lib", "{HSA_LIB}", libomptarget.format(src) + plugin)
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:                r"{ROCM_DIR}/lib\)",
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:            filter_file("{ROCM_DIR}/include", "{COMGR_INCLUDE}", libomptarget.format(src) + plugin)
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:            "if (LIBOMPTARGET_DEP_CUDA_FOUND)",
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:            "if (LIBOMPTARGET_DEP_CUDA_FOUND AND NOT LIBOMPTARGET_AMDGPU_ARCH)",
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:        openmp_extras_prefix = self.spec["rocm-openmp-extras"].prefix
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:        devlibs_prefix = self.spec["llvm-amdgpu"].prefix
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:            devlibs_src = "{0}/rocm-openmp-extras/llvm-project/amd/device-libs".format(src)
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:            devlibs_src = "{0}/rocm-openmp-extras/rocm-device-libs".format(src)
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:        llvm_inc = "/rocm-openmp-extras/llvm-project/llvm/include"
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:        llvm_prefix = self.spec["llvm-amdgpu"].prefix
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:        libpgmath = "/rocm-openmp-extras/flang/runtime/libpgmath/lib/common"
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:        # libdevice symlink to rocm-openmp-extras for runtime
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:        # libdebug symlink to rocm-openmp-extras for runtime
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:            "../rocm-openmp-extras/aomp-extras",
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:            "-DROCM_DIR={0}".format(hsa_prefix),
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:        components["openmp"] = ["../rocm-openmp-extras/llvm-project/openmp"]
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:            "../rocm-openmp-extras/llvm-project/openmp",
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:            "-DLLVM_TARGETS_TO_BUILD=AMDGPU;x86",
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:        components["pgmath"] = ["../rocm-openmp-extras/flang/runtime/libpgmath"]
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:            "-DLLVM_TARGETS_TO_BUILD=AMDGPU;X86",
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:            "../../rocm-openmp-extras/flang/flang-legacy/{0}/llvm-legacy/llvm".format(
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:            "../rocm-openmp-extras/flang/flang-legacy/{0}".format(flang_legacy_version),
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:            "../rocm-openmp-extras/flang",
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:            "-DFLANG_OPENMP_GPU_AMD=ON",
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:            "-DFLANG_OPENMP_GPU_NVIDIA=ON",
var/spack/repos/builtin/packages/rocm-openmp-extras/package.py:            "../rocm-openmp-extras/flang",
var/spack/repos/builtin/packages/rocm-openmp-extras/0001-Linking-hsakmt-libdrm-and-numactl-libraries.patch: .../openmp/libomptarget/plugins-nextgen/amdgpu/CMakeLists.txt  | 3 ++-
var/spack/repos/builtin/packages/rocm-openmp-extras/0001-Linking-hsakmt-libdrm-and-numactl-libraries.patch: .../openmp/libomptarget/plugins/amdgpu/CMakeLists.txt          | 3 ++-
var/spack/repos/builtin/packages/rocm-openmp-extras/0001-Linking-hsakmt-libdrm-and-numactl-libraries.patch:diff --git a/rocm-openmp-extras/llvm-project/openmp/libomptarget/plugins-nextgen/amdgpu/CMakeLists.txt b/rocm-openmp-extras/llvm-project/openmp/libomptarget/plugins-nextgen/amdgpu/CMakeLists.txt
var/spack/repos/builtin/packages/rocm-openmp-extras/0001-Linking-hsakmt-libdrm-and-numactl-libraries.patch:--- a/rocm-openmp-extras/llvm-project/openmp/libomptarget/plugins-nextgen/amdgpu/CMakeLists.txt
var/spack/repos/builtin/packages/rocm-openmp-extras/0001-Linking-hsakmt-libdrm-and-numactl-libraries.patch:+++ b/rocm-openmp-extras/llvm-project/openmp/libomptarget/plugins-nextgen/amdgpu/CMakeLists.txt
var/spack/repos/builtin/packages/rocm-openmp-extras/0001-Linking-hsakmt-libdrm-and-numactl-libraries.patch: find_library ( HSAKMT_LIB libhsakmt.a REQURIED HINTS ${CMAKE_INSTALL_PREFIX} PATHS /opt/rocm)
var/spack/repos/builtin/packages/rocm-openmp-extras/0001-Linking-hsakmt-libdrm-and-numactl-libraries.patch: # lib_amdgpu
var/spack/repos/builtin/packages/rocm-openmp-extras/0001-Linking-hsakmt-libdrm-and-numactl-libraries.patch: pkg_check_modules(drm_amdgpu REQUIRED IMPORTED_TARGET libdrm_amdgpu)
var/spack/repos/builtin/packages/rocm-openmp-extras/0001-Linking-hsakmt-libdrm-and-numactl-libraries.patch:diff --git a/rocm-openmp-extras/llvm-project/openmp/libomptarget/plugins/amdgpu/CMakeLists.txt b/rocm-openmp-extras/llvm-project/openmp/libomptarget/plugins/amdgpu/CMakeLists.txt
var/spack/repos/builtin/packages/rocm-openmp-extras/0001-Linking-hsakmt-libdrm-and-numactl-libraries.patch:--- a/rocm-openmp-extras/llvm-project/openmp/libomptarget/plugins/amdgpu/CMakeLists.txt
var/spack/repos/builtin/packages/rocm-openmp-extras/0001-Linking-hsakmt-libdrm-and-numactl-libraries.patch:+++ b/rocm-openmp-extras/llvm-project/openmp/libomptarget/plugins/amdgpu/CMakeLists.txt
var/spack/repos/builtin/packages/rocm-openmp-extras/0001-Linking-hsakmt-libdrm-and-numactl-libraries.patch: find_library ( HSAKMT_LIB libhsakmt.a REQURIED HINTS ${CMAKE_INSTALL_PREFIX} PATHS /opt/rocm)
var/spack/repos/builtin/packages/rocm-openmp-extras/0001-Linking-hsakmt-libdrm-and-numactl-libraries.patch: # lib_drm_amdgpu
var/spack/repos/builtin/packages/rocm-openmp-extras/0001-Linking-hsakmt-libdrm-and-numactl-libraries.patch: pkg_check_modules(drm_amdgpu REQUIRED IMPORTED_TARGET libdrm_amdgpu)
var/spack/repos/builtin/packages/rocm-openmp-extras/0001-Avoid-duplicate-registration-on-cuda-env.patch:Subject: [PATCH] Avoiding registration of duplicate when built on cuda
var/spack/repos/builtin/packages/rocm-openmp-extras/0001-Avoid-duplicate-registration-on-cuda-env.patch: .../openmp/libomptarget/plugins/cuda/CMakeLists.txt             | 2 --
var/spack/repos/builtin/packages/rocm-openmp-extras/0001-Avoid-duplicate-registration-on-cuda-env.patch:diff --git a/rocm-openmp-extras/llvm-project/openmp/libomptarget/plugins/cuda/CMakeLists.txt b/rocm-openmp-extras/llvm-project/openmp/libomptarget/plugins/cuda/CMakeLists.txt
var/spack/repos/builtin/packages/rocm-openmp-extras/0001-Avoid-duplicate-registration-on-cuda-env.patch:--- a/rocm-openmp-extras/llvm-project/openmp/libomptarget/plugins/cuda/CMakeLists.txt
var/spack/repos/builtin/packages/rocm-openmp-extras/0001-Avoid-duplicate-registration-on-cuda-env.patch:+++ b/rocm-openmp-extras/llvm-project/openmp/libomptarget/plugins/cuda/CMakeLists.txt
var/spack/repos/builtin/packages/rocm-openmp-extras/0001-Avoid-duplicate-registration-on-cuda-env.patch:@@ -102,8 +102,6 @@ target_include_directories(omptarget.rtl.cuda PRIVATE
var/spack/repos/builtin/packages/rocm-openmp-extras/0001-Avoid-duplicate-registration-on-cuda-env.patch: option(LIBOMPTARGET_FORCE_NVIDIA_TESTS "Build NVIDIA libomptarget tests" OFF)
var/spack/repos/builtin/packages/rocm-openmp-extras/0001-Avoid-duplicate-registration-on-cuda-env.patch: if (LIBOMPTARGET_FOUND_NVIDIA_GPU OR LIBOMPTARGET_FORCE_NVIDIA_TESTS)
var/spack/repos/builtin/packages/rocm-openmp-extras/0001-Avoid-duplicate-registration-on-cuda-env.patch:   libomptarget_say("Enable tests using CUDA plugin")
var/spack/repos/builtin/packages/rocm-openmp-extras/0001-Avoid-duplicate-registration-on-cuda-env.patch:-  set(LIBOMPTARGET_SYSTEM_TARGETS "${LIBOMPTARGET_SYSTEM_TARGETS} nvptx64-nvidia-cuda nvptx64-nvidia-cuda-oldDriver" PARENT_SCOPE)
var/spack/repos/builtin/packages/rocm-openmp-extras/0001-Avoid-duplicate-registration-on-cuda-env.patch:-  set(LIBOMPTARGET_SYSTEM_TARGETS "${LIBOMPTARGET_SYSTEM_TARGETS} nvptx64-nvidia-cuda nvptx64-nvidia-cuda-LTO" PARENT_SCOPE)
var/spack/repos/builtin/packages/rocm-openmp-extras/0001-Avoid-duplicate-registration-on-cuda-env.patch:   list(APPEND LIBOMPTARGET_TESTED_PLUGINS "omptarget.rtl.cuda")
var/spack/repos/builtin/packages/py-torch-cluster/package.py:        if "+cuda" in self.spec["py-torch"]:
var/spack/repos/builtin/packages/py-torch-cluster/package.py:            env.set("FORCE_CUDA", 1)
var/spack/repos/builtin/packages/py-torch-cluster/package.py:            env.set("FORCE_ONLY_CUDA", 0)
var/spack/repos/builtin/packages/py-torch-cluster/package.py:            env.set("FORCE_CUDA", 0)
var/spack/repos/builtin/packages/py-torch-cluster/package.py:            env.set("FORCE_ONLY_CUDA", 0)
var/spack/repos/builtin/packages/kokkos/package.py:class Kokkos(CMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/kokkos/package.py:    conflicts("cmake@3.28", when="@:4.2.01 +cuda")
var/spack/repos/builtin/packages/kokkos/package.py:        "cuda": [False, "Whether to build CUDA backend"],
var/spack/repos/builtin/packages/kokkos/package.py:        "rocm": [False, "Whether to build HIP backend"],
var/spack/repos/builtin/packages/kokkos/package.py:    conflicts("+rocm", when="@:3.0")
var/spack/repos/builtin/packages/kokkos/package.py:        "cuda_constexpr": [False, "Activate experimental constexpr features"],
var/spack/repos/builtin/packages/kokkos/package.py:        "cuda_lambda": [False, "Activate experimental lambda features"],
var/spack/repos/builtin/packages/kokkos/package.py:        "cuda_ldg_intrinsic": [False, "Use CUDA LDG intrinsics"],
var/spack/repos/builtin/packages/kokkos/package.py:        "cuda_relocatable_device_code": [False, "Enable RDC for CUDA"],
var/spack/repos/builtin/packages/kokkos/package.py:        "cuda_uvm": [False, "Enable unified virtual memory (UVM) for CUDA"],
var/spack/repos/builtin/packages/kokkos/package.py:    spack_cuda_arch_map = {
var/spack/repos/builtin/packages/kokkos/package.py:    cuda_arches = spack_cuda_arch_map.values()
var/spack/repos/builtin/packages/kokkos/package.py:    conflicts("+cuda", when="cuda_arch=none")
var/spack/repos/builtin/packages/kokkos/package.py:    # Kokkos support only one cuda_arch at a time
var/spack/repos/builtin/packages/kokkos/package.py:        "cuda_arch",
var/spack/repos/builtin/packages/kokkos/package.py:        description="CUDA architecture",
var/spack/repos/builtin/packages/kokkos/package.py:        values=("none",) + CudaPackage.cuda_arch_values,
var/spack/repos/builtin/packages/kokkos/package.py:        when="+cuda",
var/spack/repos/builtin/packages/kokkos/package.py:    amdgpu_arch_map = {
var/spack/repos/builtin/packages/kokkos/package.py:        "Kokkos supports the following AMD GPU targets: " + ", ".join(amdgpu_arch_map.keys())
var/spack/repos/builtin/packages/kokkos/package.py:    for arch in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/kokkos/package.py:        if arch not in amdgpu_arch_map:
var/spack/repos/builtin/packages/kokkos/package.py:                "+rocm",
var/spack/repos/builtin/packages/kokkos/package.py:                when="amdgpu_target={0}".format(arch),
var/spack/repos/builtin/packages/kokkos/package.py:    intel_gpu_arches = (
var/spack/repos/builtin/packages/kokkos/package.py:        "intel_gpu_arch",
var/spack/repos/builtin/packages/kokkos/package.py:        values=("none",) + intel_gpu_arches,
var/spack/repos/builtin/packages/kokkos/package.py:        description="Intel GPU architecture",
var/spack/repos/builtin/packages/kokkos/package.py:    conflicts("+cuda", when="+rocm", msg="CUDA and ROCm are not compatible in Kokkos.")
var/spack/repos/builtin/packages/kokkos/package.py:    depends_on("rocthrust", when="@4.3: +rocm")
var/spack/repos/builtin/packages/kokkos/package.py:        variant(opt, default=dflt, description=desc, when=("+cuda" if "cuda" in opt else None))
var/spack/repos/builtin/packages/kokkos/package.py:    variant("wrapper", default=False, description="Use nvcc-wrapper for CUDA build")
var/spack/repos/builtin/packages/kokkos/package.py:        description="Use CMake language support for CUDA/HIP",
var/spack/repos/builtin/packages/kokkos/package.py:    conflicts("+wrapper", when="~cuda")
var/spack/repos/builtin/packages/kokkos/package.py:    conflicts("+cuda", when="cxxstd=17 ^cuda@:10")
var/spack/repos/builtin/packages/kokkos/package.py:    conflicts("+cuda", when="cxxstd=20 ^cuda@:11")
var/spack/repos/builtin/packages/kokkos/package.py:    # Expose a way to disable CudaMallocAsync that can cause problems
var/spack/repos/builtin/packages/kokkos/package.py:    variant("alloc_async", default=False, description="Use CudaMallocAsync", when="@4.2: +cuda")
var/spack/repos/builtin/packages/kokkos/package.py:        variant_to_cmake_option = {"rocm": "hip"}
var/spack/repos/builtin/packages/kokkos/package.py:        if spec.satisfies("~wrapper+cuda") and not (
var/spack/repos/builtin/packages/kokkos/package.py:                "Kokkos requires +wrapper when using +cuda without %clang, %cce or +cmake_lang"
var/spack/repos/builtin/packages/kokkos/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/kokkos/package.py:            if isinstance(spec.variants["cuda_arch"].value, str):
var/spack/repos/builtin/packages/kokkos/package.py:                cuda_arch = spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/kokkos/package.py:                if len(spec.variants["cuda_arch"].value) > 1:
var/spack/repos/builtin/packages/kokkos/package.py:                    msg = "Kokkos supports only one cuda_arch at a time."
var/spack/repos/builtin/packages/kokkos/package.py:                cuda_arch = spec.variants["cuda_arch"].value[0]
var/spack/repos/builtin/packages/kokkos/package.py:            if cuda_arch != "none":
var/spack/repos/builtin/packages/kokkos/package.py:                kokkos_arch_name = self.spack_cuda_arch_map[cuda_arch]
var/spack/repos/builtin/packages/kokkos/package.py:        if spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/kokkos/package.py:            for amdgpu_target in spec.variants["amdgpu_target"].value:
var/spack/repos/builtin/packages/kokkos/package.py:                if amdgpu_target != "none":
var/spack/repos/builtin/packages/kokkos/package.py:                    if amdgpu_target in self.amdgpu_arch_map:
var/spack/repos/builtin/packages/kokkos/package.py:                        spack_microarches.append(self.amdgpu_arch_map[amdgpu_target])
var/spack/repos/builtin/packages/kokkos/package.py:                        # choosing an unsupported AMD GPU target
var/spack/repos/builtin/packages/kokkos/package.py:                        raise SpackError("Unsupported target: {0}".format(amdgpu_target))
var/spack/repos/builtin/packages/kokkos/package.py:        if self.spec.variants["intel_gpu_arch"].value != "none":
var/spack/repos/builtin/packages/kokkos/package.py:            spack_microarches.append(self.spec.variants["intel_gpu_arch"].value)
var/spack/repos/builtin/packages/kokkos/package.py:        elif "+rocm" in self.spec:
var/spack/repos/builtin/packages/kokkos/package.py:                        join_path(self.spec["llvm-amdgpu"].prefix.bin, "amdclang++"),
var/spack/repos/builtin/packages/kokkos/package.py:        elif "+cuda" in self.spec and "+cmake_lang" in self.spec:
var/spack/repos/builtin/packages/kokkos/package.py:                self.define("CMAKE_CUDA_COMPILER", join_path(self.spec["cuda"].prefix.bin, "nvcc"))
var/spack/repos/builtin/packages/kokkos/package.py:            options.append(from_variant("CMAKE_CUDA_STANDARD", "cxxstd"))
var/spack/repos/builtin/packages/kokkos/package.py:            self.define_from_variant("Kokkos_ENABLE_IMPL_CUDA_MALLOC_ASYNC", "alloc_async")
var/spack/repos/builtin/packages/kokkos/package.py:        if self.spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/occa/package.py:    multi-core/many-core architectures. Devices (such as CPUs, GPUs,
var/spack/repos/builtin/packages/occa/package.py:    variant("cuda", default=True, description="Activates support for CUDA")
var/spack/repos/builtin/packages/occa/package.py:    variant("opencl", default=True, description="Activates support for OpenCL")
var/spack/repos/builtin/packages/occa/package.py:    depends_on("cuda", when="+cuda")
var/spack/repos/builtin/packages/occa/package.py:    conflicts("%gcc@6:", when="^cuda@:8")
var/spack/repos/builtin/packages/occa/package.py:    conflicts("%gcc@7:", when="^cuda@:9")
var/spack/repos/builtin/packages/occa/package.py:        if "+cuda" in spec:
var/spack/repos/builtin/packages/occa/package.py:            cuda_dir = spec["cuda"].prefix
var/spack/repos/builtin/packages/occa/package.py:            # Run-time CUDA compiler:
var/spack/repos/builtin/packages/occa/package.py:            s_env.set("OCCA_CUDA_COMPILER", join_path(cuda_dir, "bin", "nvcc"))
var/spack/repos/builtin/packages/occa/package.py:        # For the cuda, openmp, and opencl variants, set the environment
var/spack/repos/builtin/packages/occa/package.py:        # variable OCCA_{CUDA,OPENMP,OPENCL}_ENABLED only if the variant is
var/spack/repos/builtin/packages/occa/package.py:        if "+cuda" in spec:
var/spack/repos/builtin/packages/occa/package.py:            cuda_dir = spec["cuda"].prefix
var/spack/repos/builtin/packages/occa/package.py:            cuda_libs_list = ["libcuda", "libcudart", "libOpenCL"]
var/spack/repos/builtin/packages/occa/package.py:            cuda_libs = find_libraries(cuda_libs_list, cuda_dir, shared=True, recursive=True)
var/spack/repos/builtin/packages/occa/package.py:            env.set("OCCA_INCLUDE_PATH", cuda_dir.include)
var/spack/repos/builtin/packages/occa/package.py:            env.set("OCCA_LIBRARY_PATH", ":".join(cuda_libs.directories))
var/spack/repos/builtin/packages/occa/package.py:            env.set("OCCA_CUDA_ENABLED", "0")
var/spack/repos/builtin/packages/occa/package.py:        if "~opencl" in spec:
var/spack/repos/builtin/packages/occa/package.py:            env.set("OCCA_OPENCL_ENABLED", "0")
var/spack/repos/builtin/packages/hipfort/package.py:    homepage = "https://github.com/ROCm/hipfort"
var/spack/repos/builtin/packages/hipfort/package.py:    git = "https://github.com/ROCm/hipfort.git"
var/spack/repos/builtin/packages/hipfort/package.py:    url = "https://github.com/ROCm/hipfort/archive/rocm-6.1.1.tar.gz"
var/spack/repos/builtin/packages/hipfort/package.py:    tags = ["rocm"]
var/spack/repos/builtin/packages/hipfort/package.py:    depends_on("rocm-cmake@3.8.0:", type="build")
var/spack/repos/builtin/packages/hipfort/package.py:            args.append(self.define("__skip_rocmclang", "ON"))
var/spack/repos/builtin/packages/py-amrex/package.py:class PyAmrex(CMakePackage, PythonExtension, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/py-amrex/package.py:    with when("+cuda"):
var/spack/repos/builtin/packages/py-amrex/package.py:        depends_on("amrex +cuda")
var/spack/repos/builtin/packages/py-amrex/package.py:        # todo: how to forward cuda_arch?
var/spack/repos/builtin/packages/py-amrex/package.py:    with when("~cuda"):
var/spack/repos/builtin/packages/py-amrex/package.py:        depends_on("amrex ~cuda")
var/spack/repos/builtin/packages/py-amrex/package.py:    with when("+rocm"):
var/spack/repos/builtin/packages/py-amrex/package.py:        depends_on("amrex +rocm")
var/spack/repos/builtin/packages/py-amrex/package.py:        # todo: how to forward amdgpu_target?
var/spack/repos/builtin/packages/py-amrex/package.py:    with when("~rocm"):
var/spack/repos/builtin/packages/py-amrex/package.py:        depends_on("amrex ~rocm")
var/spack/repos/builtin/packages/py-amrex/package.py:    depends_on("py-cupy", type="test", when="+cuda")
var/spack/repos/builtin/packages/hipsparselt/0001-update-llvm-path-add-hipsparse-include-dir-for-spack-6.2.patch:         set( tensile_fork "ROCmSoftwarePlatform" CACHE STRING "Tensile fork to use" )
var/spack/repos/builtin/packages/hipsparselt/0001-update-llvm-path-add-hipsparse-include-dir-for-spack-6.2.patch:+        virtualenv_install("git+https://github.com/ROCm/hipBLASLt.git@modify-tensilelite-spack-6.2#subdirectory=tensilelite")
var/spack/repos/builtin/packages/hipsparselt/0001-update-llvm-path-add-hipsparse-include-dir-for-spack-6.2.patch: if(NOT BUILD_CUDA)
var/spack/repos/builtin/packages/hipsparselt/0001-update-llvm-path-add-hipsparse-include-dir-for-spack-6.2.patch:@@ -61,7 +61,7 @@ if(NOT BUILD_CUDA)
var/spack/repos/builtin/packages/hipsparselt/0001-update-llvm-path-add-hipsparse-include-dir-for-spack-6.2.patch:@@ -72,7 +72,7 @@ if(NOT BUILD_CUDA)
var/spack/repos/builtin/packages/hipsparselt/package.py:class Hipsparselt(CMakePackage, ROCmPackage):
var/spack/repos/builtin/packages/hipsparselt/package.py:    homepage = "https://github.com/ROCm/hipsparselt"
var/spack/repos/builtin/packages/hipsparselt/package.py:    url = "https://github.com/ROCm/hipSPARSELt/archive/refs/tags/rocm-6.1.2.tar.gz"
var/spack/repos/builtin/packages/hipsparselt/package.py:    git = "https://github.com/ROCm/hipsparseLt.git"
var/spack/repos/builtin/packages/hipsparselt/package.py:    amdgpu_targets = ROCmPackage.amdgpu_targets
var/spack/repos/builtin/packages/hipsparselt/package.py:        "amdgpu_target",
var/spack/repos/builtin/packages/hipsparselt/package.py:        description="AMD GPU architecture",
var/spack/repos/builtin/packages/hipsparselt/package.py:        values=spack.variant.DisjointSetsOfValues(("auto",), ("none",), amdgpu_targets)
var/spack/repos/builtin/packages/hipsparselt/package.py:        depends_on(f"rocm-openmp-extras@{ver}", when=f"@{ver}", type="test")
var/spack/repos/builtin/packages/hipsparselt/package.py:        if "auto" not in self.spec.variants["amdgpu_target"]:
var/spack/repos/builtin/packages/hipsparselt/package.py:            args.append(self.define_from_variant("AMDGPU_TARGETS", "amdgpu_target"))
var/spack/repos/builtin/packages/hipsparselt/package.py:                self.define("ROCM_OPENMP_EXTRAS_DIR", self.spec["rocm-openmp-extras"].prefix)
var/spack/repos/builtin/packages/hipsparselt/0001-update-llvm-path-add-hipsparse-include-dir-for-spack-6.1.patch:         set( tensile_fork "ROCmSoftwarePlatform" CACHE STRING "Tensile fork to use" )
var/spack/repos/builtin/packages/hipsparselt/0001-update-llvm-path-add-hipsparse-include-dir-for-spack-6.1.patch:+        virtualenv_install("git+https://github.com/ROCm/hipBLASLt.git@modify-tensilelite-spack#subdirectory=tensilelite")
var/spack/repos/builtin/packages/hipsparselt/0001-update-llvm-path-add-hipsparse-include-dir-for-spack-6.1.patch: if(NOT BUILD_CUDA)
var/spack/repos/builtin/packages/hipsparselt/0001-update-llvm-path-add-hipsparse-include-dir-for-spack-6.1.patch:@@ -61,7 +61,7 @@ if(NOT BUILD_CUDA)
var/spack/repos/builtin/packages/hipsparselt/0001-update-llvm-path-add-hipsparse-include-dir-for-spack-6.1.patch:@@ -72,7 +72,7 @@ if(NOT BUILD_CUDA)
var/spack/repos/builtin/packages/hipsparselt/0001-update-llvm-path-add-hipsparse-include-dir-for-spack.patch:         set( tensile_fork "ROCmSoftwarePlatform" CACHE STRING "Tensile fork to use" )
var/spack/repos/builtin/packages/hipsparselt/0001-update-llvm-path-add-hipsparse-include-dir-for-spack.patch:+        virtualenv_install("git+https://github.com/ROCm/hipBLASLt.git@spack-change-tensilelite#subdirectory=tensilelite")
var/spack/repos/builtin/packages/hipsparselt/0001-update-llvm-path-add-hipsparse-include-dir-for-spack.patch: if(NOT BUILD_CUDA)
var/spack/repos/builtin/packages/hipsparselt/0001-update-llvm-path-add-hipsparse-include-dir-for-spack.patch:@@ -61,7 +61,7 @@ if(NOT BUILD_CUDA)
var/spack/repos/builtin/packages/hipsparselt/0001-update-llvm-path-add-hipsparse-include-dir-for-spack.patch:@@ -72,7 +72,7 @@ if(NOT BUILD_CUDA)
var/spack/repos/builtin/packages/findutils/package.py:    # The NVIDIA compilers do not currently support some GNU builtins.
var/spack/repos/builtin/packages/hpgmg/package.py:    variant("cuda", default=False, description="Build with CUDA")
var/spack/repos/builtin/packages/hpgmg/package.py:    depends_on("cuda", when="+cuda")
var/spack/repos/builtin/packages/kripke/package.py:class Kripke(CMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/kripke/package.py:    with when("+rocm @1.2.5:"):
var/spack/repos/builtin/packages/kripke/package.py:        depends_on("raja+rocm", when="+rocm")
var/spack/repos/builtin/packages/kripke/package.py:        depends_on("chai+rocm", when="+rocm")
var/spack/repos/builtin/packages/kripke/package.py:        for arch in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/kripke/package.py:                "raja+rocm amdgpu_target={0}".format(arch), when="amdgpu_target={0}".format(arch)
var/spack/repos/builtin/packages/kripke/package.py:                "chai+rocm amdgpu_target={0}".format(arch), when="amdgpu_target={0}".format(arch)
var/spack/repos/builtin/packages/kripke/package.py:    conflicts("^blt@:0.3.6", when="+rocm")
var/spack/repos/builtin/packages/kripke/package.py:        if spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/kripke/package.py:            rocm_archs = spec.variants["amdgpu_target"].value
var/spack/repos/builtin/packages/kripke/package.py:            if "none" not in rocm_archs:
var/spack/repos/builtin/packages/kripke/package.py:                args.append("-DHIP_HIPCC_FLAGS=--amdgpu-target={0}".format(",".join(rocm_archs)))
var/spack/repos/builtin/packages/kripke/package.py:                args.append("-DCMAKE_HIP_ARCHITECTURES={0}".format(rocm_archs))
var/spack/repos/builtin/packages/kripke/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/kripke/package.py:            args.append("-DENABLE_CUDA=ON")
var/spack/repos/builtin/packages/kripke/package.py:            args.append(self.define("CMAKE_CUDA_HOST_COMPILER", self.spec["mpi"].mpicxx))
var/spack/repos/builtin/packages/kripke/package.py:            if not spec.satisfies("cuda_arch=none"):
var/spack/repos/builtin/packages/kripke/package.py:                cuda_arch = spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/kripke/package.py:                args.append("-DCUDA_ARCH={0}".format(cuda_arch[0]))
var/spack/repos/builtin/packages/kripke/package.py:                args.append("-DCMAKE_CUDA_ARCHITECTURES={0}".format(cuda_arch[0]))
var/spack/repos/builtin/packages/kripke/package.py:                    "-DCMAKE_CUDA_FLAGS=--expt-extended-lambda -I%s -I=%s"
var/spack/repos/builtin/packages/kripke/package.py:            args.append("-DENABLE_CUDA=OFF")
var/spack/repos/builtin/packages/ucc/package.py:class Ucc(AutotoolsPackage, CudaPackage):
var/spack/repos/builtin/packages/ucc/package.py:    variant("cuda", default=False, description="Enable CUDA TL")
var/spack/repos/builtin/packages/ucc/package.py:    variant("nccl", default=False, description="Enable NCCL TL", when="+cuda")
var/spack/repos/builtin/packages/ucc/package.py:        when="@1.2.0 +cuda",
var/spack/repos/builtin/packages/ucc/package.py:    depends_on("nccl", when="+nccl")
var/spack/repos/builtin/packages/ucc/package.py:    with when("+nccl"):
var/spack/repos/builtin/packages/ucc/package.py:        for arch in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/ucc/package.py:                "nccl +cuda cuda_arch={0}".format(arch), when="+cuda cuda_arch={0}".format(arch)
var/spack/repos/builtin/packages/ucc/package.py:        args.extend(self.with_or_without("cuda", activation_value="prefix"))
var/spack/repos/builtin/packages/ucc/package.py:        args.extend(self.with_or_without("nccl", activation_value="prefix"))
var/spack/repos/builtin/packages/specfem3d-globe/package.py:class Specfem3dGlobe(AutotoolsPackage, CudaPackage):
var/spack/repos/builtin/packages/specfem3d-globe/package.py:    variant("opencl", default=False, description="Build with OpenCL code generator")
var/spack/repos/builtin/packages/specfem3d-globe/package.py:    depends_on("opencl", when="+opencl")
var/spack/repos/builtin/packages/specfem3d-globe/package.py:        if "+cuda" in self.spec:
var/spack/repos/builtin/packages/specfem3d-globe/package.py:            args.append("--with-cuda")
var/spack/repos/builtin/packages/specfem3d-globe/package.py:            args.append("CUDA_LIB={0}".format(spec["cuda"].libs.directories[0]))
var/spack/repos/builtin/packages/specfem3d-globe/package.py:            args.append("CUDA_INC={0}".format(spec["cuda"].prefix.include))
var/spack/repos/builtin/packages/specfem3d-globe/package.py:        if "+opencl" in self.spec:
var/spack/repos/builtin/packages/specfem3d-globe/package.py:            args.append("--with-opencl")
var/spack/repos/builtin/packages/specfem3d-globe/package.py:            args.append("OCL_LIB={0}".format(spec["opencl"].libs.directories[0]))
var/spack/repos/builtin/packages/specfem3d-globe/package.py:            args.append("OCL_INC={0}".format(spec["opencl"].prefix.include))
var/spack/repos/builtin/packages/ufo-core/package.py:    CPUs, GPUs or clusters. This package contains the run-time system and
var/spack/repos/builtin/packages/nvcomp/package.py:class Nvcomp(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/nvcomp/package.py:    /decompression on NVIDIA GPUs
var/spack/repos/builtin/packages/nvcomp/package.py:    forked from: https://github.com/NVIDIA/nvcomp after NVIDIA made this closed source
var/spack/repos/builtin/packages/nvcomp/package.py:    homepage = "https://github.com/NVIDIA/nvcomp"
var/spack/repos/builtin/packages/nvcomp/package.py:    url = "https://github.com/NVIDIA/nvcomp/archive/refs/tags/v2.0.2.tar.gz"
var/spack/repos/builtin/packages/nvcomp/package.py:    depends_on("cuda")
var/spack/repos/builtin/packages/nvcomp/package.py:    conflicts("~cuda")
var/spack/repos/builtin/packages/nvcomp/package.py:        cuda_arch_list = self.spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/nvcomp/package.py:        args.append("CMAKE_CUDA_ARCHITECTURES={0}".format(";".join(cuda_arch_list)))
var/spack/repos/builtin/packages/hpx/package.py:class Hpx(CMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/hpx/package.py:    variant("async_cuda", default=False, description="Enable CUDA Futures.")
var/spack/repos/builtin/packages/hpx/package.py:    depends_on("cuda", when="+async_cuda")
var/spack/repos/builtin/packages/hpx/package.py:    # Only ROCm or CUDA maybe be enabled at once
var/spack/repos/builtin/packages/hpx/package.py:    conflicts("+rocm", when="+cuda")
var/spack/repos/builtin/packages/hpx/package.py:        depends_on("cuda@11:", when="+cuda")
var/spack/repos/builtin/packages/hpx/package.py:    conflicts("+rocm", when="@:1.5")
var/spack/repos/builtin/packages/hpx/package.py:    # C++17. Starting with CUDA 11.3 they compile again.
var/spack/repos/builtin/packages/hpx/package.py:    conflicts("^asio@1.17.0:", when="+cuda cxxstd=17 ^cuda@:11.2")
var/spack/repos/builtin/packages/hpx/package.py:    # Starting from ROCm 5.0.0 hipcc miscompiles asio 1.17.0 and newer
var/spack/repos/builtin/packages/hpx/package.py:    conflicts("^asio@1.17.0:", when="+rocm ^hip@5:")
var/spack/repos/builtin/packages/hpx/package.py:    conflicts("^boost@:1.77.0", when="@:1.7 +rocm")
var/spack/repos/builtin/packages/hpx/package.py:    conflicts("%gcc@9.1:9.4", when="+rocm")
var/spack/repos/builtin/packages/hpx/package.py:    conflicts("%gcc@10.1:10.3", when="+rocm")
var/spack/repos/builtin/packages/hpx/package.py:    conflicts("%gcc@11.2", when="+rocm")
var/spack/repos/builtin/packages/hpx/package.py:            self.define_from_variant("HPX_WITH_CUDA", "cuda"),
var/spack/repos/builtin/packages/hpx/package.py:            self.define_from_variant("HPX_WITH_HIP", "rocm"),
var/spack/repos/builtin/packages/hpx/package.py:            self.define_from_variant("HPX_WITH_ASYNC_CUDA", "async_cuda"),
var/spack/repos/builtin/packages/hpx/package.py:        if self.spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/hpx/package.py:                args += [self.define("__skip_rocmclang", True)]
var/spack/repos/builtin/packages/mpibind/package.py:    variant("cuda", default=False, description="Build w/support for NVIDIA GPUs.")
var/spack/repos/builtin/packages/mpibind/package.py:    variant("rocm", default=False, description="Build w/support for AMD GPUs.")
var/spack/repos/builtin/packages/mpibind/package.py:    depends_on("hwloc@2:+cuda+nvml", type="link", when="+cuda")
var/spack/repos/builtin/packages/mpibind/package.py:    depends_on("hwloc@2.4:+rocm+opencl", type="link", when="+rocm")
var/spack/repos/builtin/packages/hpx-kokkos/package.py:class HpxKokkos(CMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/hpx-kokkos/package.py:        description="Integration type for GPU futures",
var/spack/repos/builtin/packages/hpx-kokkos/package.py:    # HPXKokkos explicitly supports CUDA and ROCm. Other GPU backends can be
var/spack/repos/builtin/packages/hpx-kokkos/package.py:    depends_on("hpx +cuda", when="+cuda")
var/spack/repos/builtin/packages/hpx-kokkos/package.py:    depends_on("kokkos +cuda +cuda_lambda +cuda_constexpr", when="+cuda")
var/spack/repos/builtin/packages/hpx-kokkos/package.py:    depends_on("hpx +rocm", when="+rocm")
var/spack/repos/builtin/packages/hpx-kokkos/package.py:    depends_on("kokkos +rocm", when="+rocm")
var/spack/repos/builtin/packages/hpx-kokkos/package.py:                "HPX_KOKKOS_CUDA_FUTURE_TYPE",
var/spack/repos/builtin/packages/hpx-kokkos/package.py:        if self.spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/portage/package.py:        description="Enable on-node parallelism using NVidia Thrust library",
var/spack/repos/builtin/packages/portage/package.py:    variant("cuda", default=False, description="Enable GPU parallelism using CUDA")
var/spack/repos/builtin/packages/portage/package.py:    for _variant in ["mpi", "jali", "openmp", "thrust", "kokkos", "cuda"]:
var/spack/repos/builtin/packages/portage/package.py:    wonton_variant = ["mpi", "jali", "openmp", "thrust", "kokkos", "cuda"]
var/spack/repos/builtin/packages/portage/package.py:    # Thrust with CUDA does not work as yet
var/spack/repos/builtin/packages/portage/package.py:    conflicts("+thrust +cuda")
var/spack/repos/builtin/packages/py-dgl/package.py:class PyDgl(CMakePackage, PythonExtension, CudaPackage):
var/spack/repos/builtin/packages/py-dgl/package.py:    variant("cuda", default=True, description="Build with CUDA")
var/spack/repos/builtin/packages/py-dgl/package.py:    # Cuda
var/spack/repos/builtin/packages/py-dgl/package.py:    depends_on("cuda@:10", when="@:0.4 +cuda", type=("build", "run"))
var/spack/repos/builtin/packages/py-dgl/package.py:    depends_on("cudnn", when="+cuda", type=("build", "run"))
var/spack/repos/builtin/packages/py-dgl/package.py:        if "+cuda" in self.spec:
var/spack/repos/builtin/packages/py-dgl/package.py:            args.append("-DUSE_CUDA=ON")
var/spack/repos/builtin/packages/py-dgl/package.py:            # Prevent defaulting to old compute_ and sm_ despite defining cuda_arch
var/spack/repos/builtin/packages/py-dgl/package.py:            args.append("-DCUDA_ARCH_NAME=Manual")
var/spack/repos/builtin/packages/py-dgl/package.py:            cuda_arch_list = " ".join(list(self.spec.variants["cuda_arch"].value))
var/spack/repos/builtin/packages/py-dgl/package.py:            args.append("-DCUDA_ARCH_BIN={0}".format(cuda_arch_list))
var/spack/repos/builtin/packages/py-dgl/package.py:            args.append("_DCUDA_ARCH_PTX={0}".format(cuda_arch_list))
var/spack/repos/builtin/packages/py-dgl/package.py:            args.append("-DUSE_CUDA=OFF")
var/spack/repos/builtin/packages/openspeedshop-utils/package.py:    variant("cuda", default=False, description="build with cuda packages included.")
var/spack/repos/builtin/packages/openspeedshop-utils/package.py:    depends_on("cbtf-argonavis@develop", when="@develop+cuda", type=("build", "link", "run"))
var/spack/repos/builtin/packages/openspeedshop-utils/package.py:    depends_on("cbtf-argonavis@1.9.3:9999", when="@2.4.0:9999+cuda", type=("build", "link", "run"))
var/spack/repos/builtin/packages/camp/package.py:class Camp(CMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/camp/package.py:    type operations and tuples for C++ and cuda
var/spack/repos/builtin/packages/camp/package.py:    depends_on("cub", when="+cuda")
var/spack/repos/builtin/packages/camp/package.py:    patch("camp-rocm6.patch", when="@0.2.3 +rocm ^hip@6:")
var/spack/repos/builtin/packages/camp/package.py:    conflicts("^blt@:0.3.6", when="+rocm")
var/spack/repos/builtin/packages/camp/package.py:    conflicts("+omptarget +rocm")
var/spack/repos/builtin/packages/camp/package.py:    conflicts("+sycl +rocm")
var/spack/repos/builtin/packages/camp/package.py:        options.append(self.define_from_variant("ENABLE_CUDA", "cuda"))
var/spack/repos/builtin/packages/camp/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/camp/package.py:            options.append("-DCUDA_TOOLKIT_ROOT_DIR={0}".format(spec["cuda"].prefix))
var/spack/repos/builtin/packages/camp/package.py:            if not spec.satisfies("cuda_arch=none"):
var/spack/repos/builtin/packages/camp/package.py:                cuda_arch = spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/camp/package.py:                options.append("-DCMAKE_CUDA_ARCHITECTURES={0}".format(cuda_arch[0]))
var/spack/repos/builtin/packages/camp/package.py:                options.append("-DCUDA_ARCH=sm_{0}".format(cuda_arch[0]))
var/spack/repos/builtin/packages/camp/package.py:                flag = "-arch sm_{0}".format(cuda_arch[0])
var/spack/repos/builtin/packages/camp/package.py:                options.append("-DCMAKE_CUDA_FLAGS:STRING={0}".format(flag))
var/spack/repos/builtin/packages/camp/package.py:        options.append(self.define_from_variant("ENABLE_HIP", "rocm"))
var/spack/repos/builtin/packages/camp/package.py:        if spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/camp/package.py:            archs = self.spec.variants["amdgpu_target"].value
var/spack/repos/builtin/packages/camp/package.py:            options.append("-DGPU_TARGETS={0}".format(archs))
var/spack/repos/builtin/packages/camp/package.py:            options.append("-DAMDGPU_TARGETS={0}".format(archs))
var/spack/repos/builtin/packages/llvm/add-include-for-libelf-llvm-15.patch: openmp/libomptarget/plugins/amdgpu/CMakeLists.txt | 1 +
var/spack/repos/builtin/packages/llvm/add-include-for-libelf-llvm-15.patch:diff --git a/openmp/libomptarget/plugins/amdgpu/CMakeLists.txt b/openmp/libomptarget/plugins/amdgpu/CMakeLists.txt
var/spack/repos/builtin/packages/llvm/add-include-for-libelf-llvm-15.patch:--- a/openmp/libomptarget/plugins/amdgpu/CMakeLists.txt
var/spack/repos/builtin/packages/llvm/add-include-for-libelf-llvm-15.patch:+++ b/openmp/libomptarget/plugins/amdgpu/CMakeLists.txt
var/spack/repos/builtin/packages/llvm/llvm17-18-thread.patch:@@ -280,4 +280,5 @@ if (NOT LIBOMPTARGET_CUDA_TOOLKIT_ROOT_DIR_PRESET AND
var/spack/repos/builtin/packages/llvm/package.py:            r"-(vscode|cpp|cl|gpu|tidy|rename|scan-deps|format|refactor|offload|"
var/spack/repos/builtin/packages/llvm/package.py:class Llvm(CMakePackage, CudaPackage, LlvmDetection, CompilerPackage):
var/spack/repos/builtin/packages/llvm/package.py:            "amdgpu",
var/spack/repos/builtin/packages/llvm/package.py:        depends_on("elf", when="+cuda")
var/spack/repos/builtin/packages/llvm/package.py:    conflicts("+cuda", when="@15:")  # +cuda variant is obselete since LLVM 15
var/spack/repos/builtin/packages/llvm/package.py:    # cuda_arch value must be specified
var/spack/repos/builtin/packages/llvm/package.py:    conflicts("cuda_arch=none", when="+cuda", msg="A value for cuda_arch must be specified.")
var/spack/repos/builtin/packages/llvm/package.py:    conflicts("cuda_arch=10")
var/spack/repos/builtin/packages/llvm/package.py:    conflicts("cuda_arch=11")
var/spack/repos/builtin/packages/llvm/package.py:    conflicts("cuda_arch=12")
var/spack/repos/builtin/packages/llvm/package.py:    conflicts("cuda_arch=13")
var/spack/repos/builtin/packages/llvm/package.py:    conflicts("cuda_arch=75", when="@:13")
var/spack/repos/builtin/packages/llvm/package.py:    conflicts("cuda_arch=80", when="@:13")
var/spack/repos/builtin/packages/llvm/package.py:    conflicts("cuda_arch=86", when="@:13")
var/spack/repos/builtin/packages/llvm/package.py:    conflicts("cuda_arch=87", when="@:15")
var/spack/repos/builtin/packages/llvm/package.py:    conflicts("cuda_arch=89", when="@:15")
var/spack/repos/builtin/packages/llvm/package.py:    conflicts("cuda_arch=90", when="@:15")
var/spack/repos/builtin/packages/llvm/package.py:    conflicts("cuda_arch=90a", when="@:17")
var/spack/repos/builtin/packages/llvm/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/llvm/package.py:                    define("CUDA_TOOLKIT_ROOT_DIR", spec["cuda"].prefix),
var/spack/repos/builtin/packages/llvm/package.py:                        ",".join(spec.variants["cuda_arch"].value),
var/spack/repos/builtin/packages/llvm/package.py:                        "sm_{0}".format(spec.variants["cuda_arch"].value[-1]),
var/spack/repos/builtin/packages/llvm/package.py:            # still build libomptarget but disable cuda
var/spack/repos/builtin/packages/llvm/package.py:                    define("CUDA_TOOLKIT_ROOT_DIR", "IGNORE"),
var/spack/repos/builtin/packages/llvm/package.py:                    define("CUDA_SDK_ROOT_DIR", "IGNORE"),
var/spack/repos/builtin/packages/llvm/package.py:                    define("CUDA_NVCC_EXECUTABLE", "IGNORE"),
var/spack/repos/builtin/packages/llvm/package.py:                    define("LIBOMPTARGET_DEP_CUDA_DRIVER_LIBRARIES", "IGNORE"),
var/spack/repos/builtin/packages/llvm/package.py:            # finding libhsa and enabling the AMDGPU plugin. Since we don't support this yet,
var/spack/repos/builtin/packages/llvm/package.py:            cmake_args.append(define("LIBOMPTARGET_BUILD_AMDGPU_PLUGIN", False))
var/spack/repos/builtin/packages/llvm/package.py:        if self.spec.satisfies("+cuda openmp=project"):
var/spack/repos/builtin/packages/llvm/package.py:        "amdgpu": "AMDGPU",
var/spack/repos/builtin/packages/llvm/llvm15-thread.patch:@@ -280,4 +280,5 @@ if (NOT LIBOMPTARGET_CUDA_TOOLKIT_ROOT_DIR_PRESET AND
var/spack/repos/builtin/packages/llvm/add-include-for-libelf-llvm-12-14.patch: openmp/libomptarget/plugins/amdgpu/CMakeLists.txt | 1 +
var/spack/repos/builtin/packages/llvm/add-include-for-libelf-llvm-12-14.patch:diff --git a/openmp/libomptarget/plugins/amdgpu/CMakeLists.txt b/openmp/libomptarget/plugins/amdgpu/CMakeLists.txt
var/spack/repos/builtin/packages/llvm/add-include-for-libelf-llvm-12-14.patch:--- a/openmp/libomptarget/plugins/amdgpu/CMakeLists.txt
var/spack/repos/builtin/packages/llvm/add-include-for-libelf-llvm-12-14.patch:+++ b/openmp/libomptarget/plugins/amdgpu/CMakeLists.txt
var/spack/repos/builtin/packages/llvm/llvm12-thread.patch:@@ -249,3 +249,6 @@ if (NOT LIBOMPTARGET_CUDA_TOOLKIT_ROOT_DIR_PRESET AND
var/spack/repos/builtin/packages/omnitrace/package.py:    homepage = "https://rocm.docs.amd.com/projects/omnitrace/en/latest/index.html"
var/spack/repos/builtin/packages/omnitrace/package.py:    git = "https://github.com/ROCm/omnitrace.git"
var/spack/repos/builtin/packages/omnitrace/package.py:    url = "https://github.com/ROCm/omnitrace/archive/refs/tags/rocm-6.2.0.tar.gz"
var/spack/repos/builtin/packages/omnitrace/package.py:        "rocm-6.2.1",
var/spack/repos/builtin/packages/omnitrace/package.py:        tag="rocm-6.2.1",
var/spack/repos/builtin/packages/omnitrace/package.py:        "rocm-6.2.0",
var/spack/repos/builtin/packages/omnitrace/package.py:        tag="rocm-6.2.0",
var/spack/repos/builtin/packages/omnitrace/package.py:        "rocm",
var/spack/repos/builtin/packages/omnitrace/package.py:        description="Enable ROCm API, kernel tracing, and GPU HW counters support",
var/spack/repos/builtin/packages/omnitrace/package.py:    depends_on("hip", when="+rocm")
var/spack/repos/builtin/packages/omnitrace/package.py:    depends_on("rocm-smi-lib", when="+rocm")
var/spack/repos/builtin/packages/omnitrace/package.py:    depends_on("roctracer-dev", when="+rocm")
var/spack/repos/builtin/packages/omnitrace/package.py:    depends_on("rocprofiler-dev", when="@1.3.0: +rocm")
var/spack/repos/builtin/packages/omnitrace/package.py:    depends_on("hip@5", when="@1:1.10 +rocm")
var/spack/repos/builtin/packages/omnitrace/package.py:    depends_on("rocm-smi-lib@5", when="@1:1.10 +rocm")
var/spack/repos/builtin/packages/omnitrace/package.py:    depends_on("roctracer-dev@5", when="@1:1.10 +rocm")
var/spack/repos/builtin/packages/omnitrace/package.py:    depends_on("rocprofiler-dev@5", when="@1.3.0:1.10 +rocm")
var/spack/repos/builtin/packages/omnitrace/package.py:        depends_on(f"rocm-smi-lib@{ver}", when=f"@rocm-{ver} +rocm")
var/spack/repos/builtin/packages/omnitrace/package.py:        depends_on(f"hip@{ver}", when=f"@rocm-{ver} +rocm")
var/spack/repos/builtin/packages/omnitrace/package.py:        depends_on(f"roctracer-dev@{ver}", when=f"@rocm-{ver} +rocm")
var/spack/repos/builtin/packages/omnitrace/package.py:        depends_on(f"rocprofiler-dev@{ver}", when=f"@rocm-{ver} +rocm")
var/spack/repos/builtin/packages/omnitrace/package.py:    depends_on("dyninst@12", when="@1.8:,rocm-6.2:0 +rocm")
var/spack/repos/builtin/packages/omnitrace/package.py:    depends_on("m4", when="@1.8:,rocm-6.2:0 +rocm")
var/spack/repos/builtin/packages/omnitrace/package.py:    depends_on("texinfo", when="@1.8:,rocm-6.2:0 +rocm")
var/spack/repos/builtin/packages/omnitrace/package.py:    depends_on("libunwind", when="@1.8:,rocm-6.2:0 +rocm")
var/spack/repos/builtin/packages/omnitrace/package.py:            self.define_from_variant("OMNITRACE_USE_HIP", "rocm"),
var/spack/repos/builtin/packages/omnitrace/package.py:            self.define_from_variant("OMNITRACE_USE_RCCL", "rocm"),
var/spack/repos/builtin/packages/omnitrace/package.py:            self.define_from_variant("OMNITRACE_USE_ROCM_SMI", "rocm"),
var/spack/repos/builtin/packages/omnitrace/package.py:            self.define_from_variant("OMNITRACE_USE_ROCTRACER", "rocm"),
var/spack/repos/builtin/packages/omnitrace/package.py:            self.define_from_variant("OMNITRACE_USE_ROCPROFILER", "rocm"),
var/spack/repos/builtin/packages/omnitrace/package.py:        if spec.satisfies("@1.8:,rocm-6.2:0"):
var/spack/repos/builtin/packages/omnitrace/package.py:        if self.spec.satisfies("@1.8:,rocm-6.2:0"):
var/spack/repos/builtin/packages/oneapi-igc/package.py:    """The Intel Graphics Compiler for OpenCL is an LLVM based compiler
var/spack/repos/builtin/packages/oneapi-igc/package.py:    for OpenCL targeting Intel Gen graphics hardware architecture.
var/spack/repos/builtin/packages/miopen-opencl/package.py:class MiopenOpencl(CMakePackage):
var/spack/repos/builtin/packages/miopen-opencl/package.py:    homepage = "https://github.com/ROCm/MIOpen"
var/spack/repos/builtin/packages/miopen-opencl/package.py:    git = "https://github.com/ROCm/MIOpen.git"
var/spack/repos/builtin/packages/miopen-opencl/package.py:    url = "https://github.com/ROCm/MIOpen/archive/rocm-6.0.0.tar.gz"
var/spack/repos/builtin/packages/miopen-opencl/package.py:    tags = ["rocm"]
var/spack/repos/builtin/packages/miopen-opencl/package.py:        depends_on(f"rocm-cmake@{ver}:", type="build", when=f"@{ver}")
var/spack/repos/builtin/packages/miopen-opencl/package.py:        depends_on(f"rocm-opencl@{ver}", when=f"@{ver}")
var/spack/repos/builtin/packages/miopen-opencl/package.py:        depends_on(f"rocmlir@{ver}", when=f"@{ver}")
var/spack/repos/builtin/packages/miopen-opencl/package.py:            self.define("MIOPEN_BACKEND", "OpenCL"),
var/spack/repos/builtin/packages/miopen-opencl/package.py:                "MIOPEN_HIP_COMPILER", "{0}/bin/clang++".format(self.spec["llvm-amdgpu"].prefix)
var/spack/repos/builtin/packages/miopen-opencl/package.py:                "HIP_CXX_COMPILER", "{0}/bin/clang++".format(self.spec["llvm-amdgpu"].prefix)
var/spack/repos/builtin/packages/miopen-opencl/package.py:                "MIOPEN_AMDGCN_ASSEMBLER", "{0}/bin/clang".format(self.spec["llvm-amdgpu"].prefix)
var/spack/repos/builtin/packages/covfie/package.py:class Covfie(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/covfie/package.py:            self.define_from_variant("COVFIE_PLATFORM_CUDA", "cuda"),
var/spack/repos/builtin/packages/upcxx/fix_configure_ldflags.patch:@@ -1168,7 +1168,7 @@ if [[ $UPCXX_CUDA -eq 1 ]]; then
var/spack/repos/builtin/packages/upcxx/fix_configure_ldflags.patch:   # check that the CUDA Driver API is linkable, adding explicit link flags if needed
var/spack/repos/builtin/packages/upcxx/fix_configure_ldflags.patch:   echo -e "#include <cuda.h>\n#include <cuda_runtime_api.h>\nint main() { cuInit(0); return 0; }" >| conftest.cpp
var/spack/repos/builtin/packages/upcxx/fix_configure_ldflags.patch:   for ldextra in '' '-lcuda' '-framework CUDA' 'FAIL'; do
var/spack/repos/builtin/packages/upcxx/fix_configure_ldflags.patch:-    eval $CXX $UPCXX_CUDA_CPPFLAGS conftest.cpp -o conftest.exe $UPCXX_CUDA_LIBFLAGS $ldextra &> /dev/null
var/spack/repos/builtin/packages/upcxx/fix_configure_ldflags.patch:+    eval $CXX $UPCXX_CUDA_CPPFLAGS $LDFLAGS conftest.cpp -o conftest.exe $UPCXX_CUDA_LIBFLAGS $ldextra &> /dev/null
var/spack/repos/builtin/packages/upcxx/fix_configure_ldflags.patch:       [[ -n "$ldextra" ]] && UPCXX_CUDA_LIBFLAGS+=" $ldextra"
var/spack/repos/builtin/packages/upcxx/package.py:class Upcxx(Package, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/upcxx/package.py:    efficiently with MPI, OpenMP, CUDA, ROCm/HIP and AMTs. It leverages GASNet-EX to
var/spack/repos/builtin/packages/upcxx/package.py:        "cuda",
var/spack/repos/builtin/packages/upcxx/package.py:        description="Enables UPC++ support for the CUDA memory kind on NVIDIA GPUs.\n"
var/spack/repos/builtin/packages/upcxx/package.py:        + "NOTE: Requires CUDA Driver library be present on the build system",
var/spack/repos/builtin/packages/upcxx/package.py:        "+cuda", when="@:2019.2", msg="UPC++ version 2019.3.0 or newer required for CUDA support"
var/spack/repos/builtin/packages/upcxx/package.py:        "rocm",
var/spack/repos/builtin/packages/upcxx/package.py:        description="Enables UPC++ support for the ROCm/HIP memory kind on AMD GPUs",
var/spack/repos/builtin/packages/upcxx/package.py:        "+rocm", when="@:2022.2", msg="UPC++ version 2022.3.0 or newer required for ROCm support"
var/spack/repos/builtin/packages/upcxx/package.py:        description="Enables UPC++ support for the Level Zero memory kind on Intel GPUs",
var/spack/repos/builtin/packages/upcxx/package.py:    conflicts("^hip@:4.4.0", when="+rocm")
var/spack/repos/builtin/packages/upcxx/package.py:        if "+cuda" in spec:
var/spack/repos/builtin/packages/upcxx/package.py:            options.append("--enable-cuda")
var/spack/repos/builtin/packages/upcxx/package.py:            options.append("--with-cuda-home=" + spec["cuda"].prefix)
var/spack/repos/builtin/packages/upcxx/package.py:            options.append("--with-nvcc=" + spec["cuda"].prefix.bin.nvcc)
var/spack/repos/builtin/packages/upcxx/package.py:                "--with-ldflags=" + self.compiler.cc_rpath_arg + spec["cuda"].prefix.lib64
var/spack/repos/builtin/packages/upcxx/package.py:        if "+rocm" in spec:
var/spack/repos/builtin/packages/upcxx/package.py:        if re.search(r"-DUPCXXI_CUDA_ENABLED=1", output):
var/spack/repos/builtin/packages/upcxx/package.py:            variants += "+cuda"
var/spack/repos/builtin/packages/upcxx/package.py:            variants += "~cuda"
var/spack/repos/builtin/packages/upcxx/package.py:            variants += "+rocm"
var/spack/repos/builtin/packages/upcxx/package.py:            variants += "~rocm"
var/spack/repos/builtin/packages/r-rviennacl/package.py:    many-core architectures (GPUs, MIC) and multi-core CPUs. The library is
var/spack/repos/builtin/packages/r-rviennacl/package.py:    written in C++ and supports 'CUDA', 'OpenCL', and 'OpenMP' (including
var/spack/repos/builtin/packages/py-torch-harmonics/package.py:    homepage = "https://github.com/NVIDIA/torch-harmonics"
var/spack/repos/builtin/packages/kassiopeia/package.py:    variant("opencl", default=False, description="Include OpenCL support for field calculations")
var/spack/repos/builtin/packages/kassiopeia/package.py:    depends_on("opencl", when="+opencl")
var/spack/repos/builtin/packages/kassiopeia/package.py:            self.define_from_variant("KEMField_USE_OPENCL", "opencl"),
var/spack/repos/builtin/packages/vtk-h/vtk-h-shared-cuda.patch:-if(ENABLE_CUDA AND BUILD_SHARED_LIBS)
var/spack/repos/builtin/packages/vtk-h/vtk-h-shared-cuda.patch:-  message(FATAL_ERROR "Static libraries are required when building with CUDA")
var/spack/repos/builtin/packages/vtk-h/vtk-h-shared-cuda.patch: if(ENABLE_CUDA)
var/spack/repos/builtin/packages/vtk-h/vtk-h-shared-cuda.patch:     set (CMAKE_CUDA_SEPARABLE_COMPILATION ON CACHE BOOL "" )
var/spack/repos/builtin/packages/vtk-h/vtk-h-shared-cuda.patch:@@ -64,6 +64,11 @@ if(ENABLE_CUDA AND NOT VTKm_ENABLE_CUDA)
var/spack/repos/builtin/packages/vtk-h/vtk-h-shared-cuda.patch:    message(FATAL_ERROR "VTK-h CUDA support requires VTK-m with CUDA support (ENABLE_CUDA == TRUE, however VTKm_ENABLE_CUDA == FALSE")
var/spack/repos/builtin/packages/vtk-h/vtk-h-shared-cuda.patch:+if(ENABLE_CUDA AND BUILD_SHARED_LIBS)
var/spack/repos/builtin/packages/vtk-h/vtk-h-shared-cuda.patch:+    message(FATAL_ERROR "Cannot build shared libs with CUDA when VTKm is < v1.7.0")
var/spack/repos/builtin/packages/vtk-h/package.py:class VtkH(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/vtk-h/package.py:    depends_on("cuda", when="+cuda")
var/spack/repos/builtin/packages/vtk-h/package.py:    depends_on("vtk-m~cuda", when="~cuda")
var/spack/repos/builtin/packages/vtk-h/package.py:    depends_on("vtk-m+cuda", when="+cuda")
var/spack/repos/builtin/packages/vtk-h/package.py:    for _arch in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/vtk-h/package.py:            "vtk-m+cuda cuda_arch={0}".format(_arch),
var/spack/repos/builtin/packages/vtk-h/package.py:            when="+cuda+openmp cuda_arch={0}".format(_arch),
var/spack/repos/builtin/packages/vtk-h/package.py:    patch("vtk-h-shared-cuda.patch", when="@0.8")
var/spack/repos/builtin/packages/vtk-h/package.py:        # CUDA
var/spack/repos/builtin/packages/vtk-h/package.py:        cfg.write("# CUDA Support\n")
var/spack/repos/builtin/packages/vtk-h/package.py:        if "+cuda" in spec:
var/spack/repos/builtin/packages/vtk-h/package.py:            cfg.write(cmake_cache_entry("ENABLE_CUDA", "ON"))
var/spack/repos/builtin/packages/vtk-h/package.py:            cfg.write(cmake_cache_entry("VTKm_ENABLE_CUDA", "ON"))
var/spack/repos/builtin/packages/vtk-h/package.py:            cfg.write(cmake_cache_entry("CMAKE_CUDA_HOST_COMPILER", env["SPACK_CXX"]))
var/spack/repos/builtin/packages/vtk-h/package.py:            cfg.write(cmake_cache_entry("ENABLE_CUDA", "OFF"))
var/spack/repos/builtin/packages/vtk-h/package.py:            cfg.write(cmake_cache_entry("VTKm_ENABLE_CUDA", "OFF"))
var/spack/repos/builtin/packages/whip/package.py:class Whip(CMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/whip/package.py:    """whip is a small C++ abstraction layer for CUDA and HIP."""
var/spack/repos/builtin/packages/whip/package.py:    # Exactly one of +cuda and +rocm need to be set
var/spack/repos/builtin/packages/whip/package.py:    conflicts("~cuda ~rocm")
var/spack/repos/builtin/packages/whip/package.py:    conflicts("+cuda +rocm")
var/spack/repos/builtin/packages/whip/package.py:        if self.spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/whip/package.py:            return [self.define("WHIP_BACKEND", "CUDA")]
var/spack/repos/builtin/packages/btop/package.py:    variant("gpu", default=False, description="Enable GPU support", when="build_system=cmake")
var/spack/repos/builtin/packages/btop/package.py:    # Fix linking GPU support, by adding an explicit "target_link_libraries" to ${CMAKE_DL_LIBS}
var/spack/repos/builtin/packages/btop/package.py:    patch("link-dl.patch", when="+gpu")
var/spack/repos/builtin/packages/btop/package.py:        return [self.define_from_variant("BTOP_GPU", "gpu")]
var/spack/repos/builtin/packages/btop/link-dl.patch:     target_link_libraries(btop ROCm)
var/spack/repos/builtin/packages/libtool/package.py:        # Remove flags not recognized by the NVIDIA compiler
var/spack/repos/builtin/packages/oneapi-level-zero/package.py:    https://dgpu-docs.intel.com/technologies/level-zero.html for
var/spack/repos/builtin/packages/oneapi-level-zero/package.py:    homepage = "https://dgpu-docs.intel.com/technologies/level-zero.html"
var/spack/repos/builtin/packages/py-onnxruntime/package.py:class PyOnnxruntime(CMakePackage, PythonExtension, ROCmPackage):
var/spack/repos/builtin/packages/py-onnxruntime/package.py:    variant("cuda", default=False, description="Build with CUDA support")
var/spack/repos/builtin/packages/py-onnxruntime/package.py:    depends_on("cuda", when="+cuda")
var/spack/repos/builtin/packages/py-onnxruntime/package.py:    depends_on("cudnn", when="+cuda")
var/spack/repos/builtin/packages/py-onnxruntime/package.py:    rocm_dependencies = [
var/spack/repos/builtin/packages/py-onnxruntime/package.py:        "llvm-amdgpu",
var/spack/repos/builtin/packages/py-onnxruntime/package.py:        "rocminfo",
var/spack/repos/builtin/packages/py-onnxruntime/package.py:        "rocm-core",
var/spack/repos/builtin/packages/py-onnxruntime/package.py:        "rocm-cmake",
var/spack/repos/builtin/packages/py-onnxruntime/package.py:    with when("+rocm"):
var/spack/repos/builtin/packages/py-onnxruntime/package.py:        for pkg_dep in rocm_dependencies:
var/spack/repos/builtin/packages/py-onnxruntime/package.py:    # ORT is assuming all ROCm components are installed in a single path,
var/spack/repos/builtin/packages/py-onnxruntime/package.py:    patch("0001-Find-ROCm-Packages-Individually.patch", when="@1.17: +rocm")
var/spack/repos/builtin/packages/py-onnxruntime/package.py:        if self.spec.satisfies("@1.17 +rocm"):
var/spack/repos/builtin/packages/py-onnxruntime/package.py:                r"${onnxruntime_ROCM_HOME}/.info/version-dev",
var/spack/repos/builtin/packages/py-onnxruntime/package.py:                "{0}/.info/version".format(self.spec["rocm-core"].prefix),
var/spack/repos/builtin/packages/py-onnxruntime/package.py:        if self.spec.satisfies("@1.18: +rocm"):
var/spack/repos/builtin/packages/py-onnxruntime/package.py:                r"${onnxruntime_ROCM_HOME}/.info/version",
var/spack/repos/builtin/packages/py-onnxruntime/package.py:                "{0}/.info/version".format(self.spec["rocm-core"].prefix),
var/spack/repos/builtin/packages/py-onnxruntime/package.py:        if self.spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/py-onnxruntime/package.py:            define_from_variant("onnxruntime_USE_CUDA", "cuda"),
var/spack/repos/builtin/packages/py-onnxruntime/package.py:        if self.spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/py-onnxruntime/package.py:                    define("onnxruntime_CUDA_HOME", self.spec["cuda"].prefix),
var/spack/repos/builtin/packages/py-onnxruntime/package.py:                    define("CMAKE_CUDA_FLAGS", "-cudart shared"),
var/spack/repos/builtin/packages/py-onnxruntime/package.py:                    define("CMAKE_CUDA_RUNTIME_LIBRARY", "Shared"),
var/spack/repos/builtin/packages/py-onnxruntime/package.py:                    define("CMAKE_TRY_COMPILE_PLATFORM_VARIABLES", "CMAKE_CUDA_RUNTIME_LIBRARY"),
var/spack/repos/builtin/packages/py-onnxruntime/package.py:        if self.spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/py-onnxruntime/package.py:                    define("CMAKE_HIP_COMPILER", f"{self.spec['llvm-amdgpu'].prefix}/bin/clang++"),
var/spack/repos/builtin/packages/py-onnxruntime/package.py:                    define("onnxruntime_USE_ROCM", "ON"),
var/spack/repos/builtin/packages/py-onnxruntime/package.py:                    define("onnxruntime_ROCM_HOME", self.spec["hip"].prefix),
var/spack/repos/builtin/packages/py-onnxruntime/package.py:                    define("onnxruntime_ROCM_VERSION", self.spec["hip"].version),
var/spack/repos/builtin/packages/py-onnxruntime/libiconv-1.10.patch: if (onnxruntime_USE_ROCM)
var/spack/repos/builtin/packages/py-onnxruntime/libiconv-1.10.patch:   target_include_directories(onnx_test_runner PRIVATE ${CMAKE_CURRENT_BINARY_DIR}/amdgpu/onnxruntime ${CMAKE_CURRENT_BINARY_DIR}/amdgpu/orttraining)
var/spack/repos/builtin/packages/py-onnxruntime/libiconv.patch:   target_compile_options(onnx_test_runner PRIVATE "$<$<COMPILE_LANGUAGE:CUDA>:SHELL:--compiler-options /utf-8>"
var/spack/repos/builtin/packages/py-onnxruntime/libiconv.patch:           "$<$<NOT:$<COMPILE_LANGUAGE:CUDA>>:/utf-8>")
var/spack/repos/builtin/packages/py-onnxruntime/0001-Find-ROCm-Packages-Individually.patch:Subject: [PATCH] Find individual ROCm dependencies
var/spack/repos/builtin/packages/py-onnxruntime/0001-Find-ROCm-Packages-Individually.patch: cmake/onnxruntime_providers_rocm.cmake | 15 ++++++++++++++-
var/spack/repos/builtin/packages/py-onnxruntime/0001-Find-ROCm-Packages-Individually.patch:diff --git a/cmake/onnxruntime_providers_rocm.cmake b/cmake/onnxruntime_providers_rocm.cmake
var/spack/repos/builtin/packages/py-onnxruntime/0001-Find-ROCm-Packages-Individually.patch:--- a/cmake/onnxruntime_providers_rocm.cmake
var/spack/repos/builtin/packages/py-onnxruntime/0001-Find-ROCm-Packages-Individually.patch:+++ b/cmake/onnxruntime_providers_rocm.cmake
var/spack/repos/builtin/packages/py-onnxruntime/0001-Find-ROCm-Packages-Individually.patch:       ${onnxruntime_ROCM_HOME}/include
var/spack/repos/builtin/packages/py-onnxruntime/0001-Find-ROCm-Packages-Individually.patch:-      ${onnxruntime_ROCM_HOME}/include/roctracer)
var/spack/repos/builtin/packages/py-onnxruntime/0001-Find-ROCm-Packages-Individually.patch:+      ${onnxruntime_ROCM_HOME}/include/roctracer
var/spack/repos/builtin/packages/py-onnxruntime/0001-Find-ROCm-Packages-Individually.patch:   set_target_properties(onnxruntime_providers_rocm PROPERTIES LINKER_LANGUAGE CXX)
var/spack/repos/builtin/packages/py-onnxruntime/0001-Find-ROCm-Packages-Individually.patch:   set_target_properties(onnxruntime_providers_rocm PROPERTIES FOLDER "ONNXRuntime")
var/spack/repos/builtin/packages/rccl-tests/package.py:    homepage = "https://github.com/ROCm/rccl-tests"
var/spack/repos/builtin/packages/rccl-tests/package.py:    git = "https://github.com/ROCm/rccl-tests.git"
var/spack/repos/builtin/packages/rccl-tests/package.py:    url = "https://github.com/ROCm/rccl-tests.git"
var/spack/repos/builtin/packages/rccl-tests/package.py:    tags = ["rocm"]
var/spack/repos/builtin/packages/sirius/package.py:class Sirius(CMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/sirius/package.py:        depends_on("spfft+cuda", when="+cuda")
var/spack/repos/builtin/packages/sirius/package.py:        depends_on("spfft+rocm", when="+rocm")
var/spack/repos/builtin/packages/sirius/package.py:        depends_on("spla+cuda", when="+cuda")
var/spack/repos/builtin/packages/sirius/package.py:        depends_on("spla+rocm", when="+rocm")
var/spack/repos/builtin/packages/sirius/package.py:    depends_on("nlcglib+rocm", when="+nlcglib+rocm")
var/spack/repos/builtin/packages/sirius/package.py:    depends_on("nlcglib+cuda", when="+nlcglib+cuda")
var/spack/repos/builtin/packages/sirius/package.py:        depends_on("dla-future +cuda", when="+cuda")
var/spack/repos/builtin/packages/sirius/package.py:        depends_on("dla-future +rocm", when="+rocm")
var/spack/repos/builtin/packages/sirius/package.py:        conflicts("^pika@:0.22.1", when="+cuda")
var/spack/repos/builtin/packages/sirius/package.py:        conflicts("^pika@:0.22.1", when="+rocm")
var/spack/repos/builtin/packages/sirius/package.py:    depends_on("rocblas", when="+rocm")
var/spack/repos/builtin/packages/sirius/package.py:    depends_on("rocsolver", when="@7.5.0: +rocm")
var/spack/repos/builtin/packages/sirius/package.py:    conflicts("+rocm", when="@:7.2.0")
var/spack/repos/builtin/packages/sirius/package.py:        depends_on("umpire~cuda~rocm", when="~cuda~rocm")
var/spack/repos/builtin/packages/sirius/package.py:        depends_on("umpire+cuda~device_alloc", when="+cuda")
var/spack/repos/builtin/packages/sirius/package.py:        depends_on("umpire+rocm~device_alloc", when="+rocm")
var/spack/repos/builtin/packages/sirius/package.py:            self.define_from_variant(cm_label + "USE_CUDA", "cuda"),
var/spack/repos/builtin/packages/sirius/package.py:            self.define_from_variant(cm_label + "USE_ROCM", "rocm"),
var/spack/repos/builtin/packages/sirius/package.py:        if "+cuda" in spec:
var/spack/repos/builtin/packages/sirius/package.py:            cuda_arch = spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/sirius/package.py:            if cuda_arch[0] != "none":
var/spack/repos/builtin/packages/sirius/package.py:                    args.append(self.define("CMAKE_CUDA_ARCH", ";".join(cuda_arch)))
var/spack/repos/builtin/packages/sirius/package.py:                    args.append(self.define("CMAKE_CUDA_ARCHITECTURES", ";".join(cuda_arch)))
var/spack/repos/builtin/packages/sirius/package.py:        if "+rocm" in spec:
var/spack/repos/builtin/packages/sirius/package.py:            archs = ",".join(self.spec.variants["amdgpu_target"].value)
var/spack/repos/builtin/packages/seissol/package.py:class Seissol(CMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/seissol/package.py:    # GPU options
var/spack/repos/builtin/packages/seissol/package.py:    variant("intel_gpu", default=False, description="Compile for Intel GPUs")
var/spack/repos/builtin/packages/seissol/package.py:        "intel_gpu_arch",
var/spack/repos/builtin/packages/seissol/package.py:        description="The Intel GPU to compile for",
var/spack/repos/builtin/packages/seissol/package.py:        when="+intel_gpu",
var/spack/repos/builtin/packages/seissol/package.py:    forwarded_variants = ["cuda", "intel_gpu", "rocm"]
var/spack/repos/builtin/packages/seissol/package.py:            description="Use SYCL also for the wave propagation part (default for Intel GPUs)",
var/spack/repos/builtin/packages/seissol/package.py:        "-cuda -rocm -intel_gpu",
var/spack/repos/builtin/packages/seissol/package.py:        "+cuda",
var/spack/repos/builtin/packages/seissol/package.py:        "+rocm",
var/spack/repos/builtin/packages/seissol/package.py:        "+intel_gpu",
var/spack/repos/builtin/packages/seissol/package.py:        msg="You may either compile for one GPU backend, or for CPU.",
var/spack/repos/builtin/packages/seissol/package.py:    depends_on("hipsycl@0.9.3: +cuda", when="+cuda sycl_backend=acpp")
var/spack/repos/builtin/packages/seissol/package.py:    # TODO: this one needs to be +rocm as well--but that's not implemented yet
var/spack/repos/builtin/packages/seissol/package.py:    depends_on("hipsycl@develop", when="+rocm sycl_backend=acpp")
var/spack/repos/builtin/packages/seissol/package.py:    depends_on("hipsycl@develop", when="+intel_gpu sycl_backend=acpp")
var/spack/repos/builtin/packages/seissol/package.py:    # GPU architecture requirements
var/spack/repos/builtin/packages/seissol/package.py:        "cuda_arch=none",
var/spack/repos/builtin/packages/seissol/package.py:        when="+cuda",
var/spack/repos/builtin/packages/seissol/package.py:        msg="A value for cuda_arch must be specified. Add cuda_arch=XX",
var/spack/repos/builtin/packages/seissol/package.py:        "amdgpu_target=none",
var/spack/repos/builtin/packages/seissol/package.py:        when="+rocm",
var/spack/repos/builtin/packages/seissol/package.py:        msg="A value for amdgpu_arch must be specified. Add amdgpu_arch=XX",
var/spack/repos/builtin/packages/seissol/package.py:        "intel_gpu_arch=none",
var/spack/repos/builtin/packages/seissol/package.py:        when="+intel_gpu",
var/spack/repos/builtin/packages/seissol/package.py:        msg="A value for intel_gpu_arch must be specified. Add intel_gpu_arch=XX",
var/spack/repos/builtin/packages/seissol/package.py:    with when("+cuda"):
var/spack/repos/builtin/packages/seissol/package.py:            depends_on(f"{var} +cuda", when=f"^[virtuals=mpi] {var}")
var/spack/repos/builtin/packages/seissol/package.py:    with when("+rocm"):
var/spack/repos/builtin/packages/seissol/package.py:            depends_on(f"{var} +rocm", when=f"^[virtuals=mpi] {var}")
var/spack/repos/builtin/packages/seissol/package.py:    # with cuda 12 and llvm 14:15, we have the issue: "error: no template named 'texture"
var/spack/repos/builtin/packages/seissol/package.py:    conflicts("cuda@12", when="+cuda ^llvm@14:15")
var/spack/repos/builtin/packages/seissol/package.py:    depends_on("cuda@11:", when="+cuda")
var/spack/repos/builtin/packages/seissol/package.py:    depends_on("hip", when="+rocm")
var/spack/repos/builtin/packages/seissol/package.py:        forwarded_variants = ["cuda", "intel_gpu", "rocm"]
var/spack/repos/builtin/packages/seissol/package.py:        with_gpu = (
var/spack/repos/builtin/packages/seissol/package.py:            self.spec.satisfies("+cuda")
var/spack/repos/builtin/packages/seissol/package.py:            or self.spec.satisfies("+rocm")
var/spack/repos/builtin/packages/seissol/package.py:            or self.spec.satisfies("+intel_gpu")
var/spack/repos/builtin/packages/seissol/package.py:        if with_gpu:
var/spack/repos/builtin/packages/seissol/package.py:            # Nvidia GPUs
var/spack/repos/builtin/packages/seissol/package.py:            if self.spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/seissol/package.py:                cuda_arch = self.spec.variants["cuda_arch"].value[0]
var/spack/repos/builtin/packages/seissol/package.py:                args.append(f"-DDEVICE_ARCH=sm_{cuda_arch}")
var/spack/repos/builtin/packages/seissol/package.py:                    args.append("-DDEVICE_BACKEND=cuda")
var/spack/repos/builtin/packages/seissol/package.py:            # ROCm/AMD GPUs
var/spack/repos/builtin/packages/seissol/package.py:            if self.spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/seissol/package.py:                amdgpu_target = self.spec.variants["amdgpu_target"].value[0]
var/spack/repos/builtin/packages/seissol/package.py:                args.append(f"-DDEVICE_ARCH={amdgpu_target}")
var/spack/repos/builtin/packages/seissol/package.py:                if self.spec.satisfies("+rocm@:5.6"):
var/spack/repos/builtin/packages/seissol/package.py:            # Intel GPUs
var/spack/repos/builtin/packages/seissol/package.py:            if self.spec.satisfies("+intel_gpu"):
var/spack/repos/builtin/packages/seissol/package.py:                assert self.spec.variants["intel_gpu_arch"].value != "none"
var/spack/repos/builtin/packages/seissol/package.py:                intel_gpu_arch = self.spec.variants["intel_gpu_arch"].value
var/spack/repos/builtin/packages/seissol/package.py:                args.append(f"-DDEVICE_ARCH={intel_gpu_arch}")
var/spack/repos/builtin/packages/lapackpp/package.py:class Lapackpp(CMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/lapackpp/package.py:    depends_on("blaspp ~cuda", when="~cuda")
var/spack/repos/builtin/packages/lapackpp/package.py:    depends_on("blaspp +cuda", when="+cuda")
var/spack/repos/builtin/packages/lapackpp/package.py:    depends_on("blaspp ~rocm", when="~rocm")
var/spack/repos/builtin/packages/lapackpp/package.py:    for val in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/lapackpp/package.py:        depends_on("blaspp +rocm amdgpu_target=%s" % val, when="amdgpu_target=%s" % val)
var/spack/repos/builtin/packages/lapackpp/package.py:    depends_on("rocblas", when="+rocm")
var/spack/repos/builtin/packages/lapackpp/package.py:    depends_on("rocsolver", when="+rocm")
var/spack/repos/builtin/packages/lapackpp/package.py:    backend_msg = "LAPACK++ supports only one GPU backend at a time"
var/spack/repos/builtin/packages/lapackpp/package.py:    conflicts("+rocm", when="+cuda", msg=backend_msg)
var/spack/repos/builtin/packages/lapackpp/package.py:    conflicts("+rocm", when="+sycl", msg=backend_msg)
var/spack/repos/builtin/packages/lapackpp/package.py:    conflicts("+cuda", when="+sycl", msg=backend_msg)
var/spack/repos/builtin/packages/lapackpp/package.py:            if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/lapackpp/package.py:                backend = "cuda"
var/spack/repos/builtin/packages/lapackpp/package.py:            if spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/lapackpp/package.py:            "-Dgpu_backend=%s" % backend,
var/spack/repos/builtin/packages/py-ray/package.py:        depends_on("py-gpustat@1:", type=("build", "run"))
var/spack/repos/builtin/packages/py-ray/package.py:        depends_on("py-gpustat", type=("build", "run"))
var/spack/repos/builtin/packages/intel-gpu-tools/package.py:class IntelGpuTools(AutotoolsPackage, XorgPackage):
var/spack/repos/builtin/packages/intel-gpu-tools/package.py:    """Intel GPU Tools is a collection of tools for development and testing of
var/spack/repos/builtin/packages/intel-gpu-tools/package.py:    environments to get useful results. Therefore, Intel GPU Tools includes
var/spack/repos/builtin/packages/intel-gpu-tools/package.py:    homepage = "https://cgit.freedesktop.org/xorg/app/intel-gpu-tools/"
var/spack/repos/builtin/packages/intel-gpu-tools/package.py:    xorg_mirror_path = "app/intel-gpu-tools-1.16.tar.gz"
var/spack/repos/builtin/packages/vtk/vtkm-findmpi-downstream.patch:-    #Fixes related to MPI+CUDA that VTK-m needs are
var/spack/repos/builtin/packages/vtk/vtkm-findmpi-downstream.patch:       ${VTKm_SOURCE_DIR}/CMake/VTKmDetectCUDAVersion.cu
var/spack/repos/builtin/packages/mgcfd-op2/package.py:            if "+cuda" in self.spec and spec.variants["cuda_arch"].value[0] != "none":
var/spack/repos/builtin/packages/mgcfd-op2/package.py:                builds.append("mpi_cuda")
var/spack/repos/builtin/packages/mgcfd-op2/package.py:            if "+cuda" in self.spec and spec.variants["cuda_arch"].value[0] != "none":
var/spack/repos/builtin/packages/mgcfd-op2/package.py:                builds.append("cuda")
var/spack/repos/builtin/packages/hashcat/package.py:    optimized hashing algorithms. hashcat currently supports CPUs, GPUs,
var/spack/repos/builtin/packages/lz4/package.py:        # Remove flags not recognized by the NVIDIA compiler
var/spack/repos/builtin/packages/oclgrind/package.py:    """An OpenCL device simulator and debugger."""
var/spack/repos/builtin/packages/genesis/package.py:class Genesis(AutotoolsPackage, CudaPackage):
var/spack/repos/builtin/packages/genesis/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/genesis/package.py:            options.append("--enable-gpu")
var/spack/repos/builtin/packages/genesis/package.py:            options.append("--with-cuda=%s" % spec["cuda"].prefix)
var/spack/repos/builtin/packages/genesis/package.py:            options.append("--disable-gpu")
var/spack/repos/builtin/packages/genesis/package.py:        if self.spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/genesis/package.py:            cuda_arch = self.spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/genesis/package.py:            cuda_gencode = " ".join(self.cuda_flags(cuda_arch))
var/spack/repos/builtin/packages/genesis/package.py:            env.set("NVCCFLAGS", cuda_gencode)
var/spack/repos/builtin/packages/xsbench/package.py:class Xsbench(MakefilePackage, CudaPackage):
var/spack/repos/builtin/packages/xsbench/package.py:    variant("cuda", default=False, when="@19:", description="Build with CUDA support")
var/spack/repos/builtin/packages/xsbench/package.py:    conflicts("cuda_arch=none", when="+cuda", msg="Must select a CUDA architecture")
var/spack/repos/builtin/packages/xsbench/package.py:    conflicts("+cuda", when="+openmp", msg="OpenMP must be disabled to support CUDA")
var/spack/repos/builtin/packages/xsbench/package.py:        if "+cuda" in self.spec:
var/spack/repos/builtin/packages/xsbench/package.py:            return "cuda"
var/spack/repos/builtin/packages/xsbench/package.py:        if "+cuda" in self.spec:
var/spack/repos/builtin/packages/xsbench/package.py:            return ["SM_VERSION={0}".format(self.spec.variants["cuda_arch"].value[0])]
var/spack/repos/builtin/packages/intel-oneapi-dpl/package.py:    productivity and performance across CPUs, GPUs, and FPGAs.
var/spack/repos/builtin/packages/mlirmiopen/package.py:    """Multi-Level Intermediate Representation for rocm miopen project."""
var/spack/repos/builtin/packages/mlirmiopen/package.py:    homepage = "https://github.com/ROCm/llvm-project-mlir"
var/spack/repos/builtin/packages/mlirmiopen/package.py:    url = "https://github.com/ROCm/llvm-project-mlir/archive/refs/tags/rocm-5.4.0.tar.gz"
var/spack/repos/builtin/packages/mlirmiopen/package.py:    git = "https://github.com/ROCm/llvm-project-mlir.git"
var/spack/repos/builtin/packages/mlirmiopen/package.py:    tags = ["rocm"]
var/spack/repos/builtin/packages/mlirmiopen/package.py:                "${ROCM_PATH}/bin",
var/spack/repos/builtin/packages/mlirmiopen/package.py:                self.spec["rocminfo"].prefix.bin,
var/spack/repos/builtin/packages/mlirmiopen/package.py:        depends_on("llvm-amdgpu@" + ver, when="@" + ver)
var/spack/repos/builtin/packages/mlirmiopen/package.py:        depends_on("rocm-cmake@" + ver, type="build", when="@" + ver)
var/spack/repos/builtin/packages/mlirmiopen/package.py:        depends_on("rocminfo@" + ver, type="build", when="@" + ver)
var/spack/repos/builtin/packages/mlirmiopen/package.py:                "CMAKE_CXX_COMPILER", "{0}/bin/clang++".format(spec["llvm-amdgpu"].prefix)
var/spack/repos/builtin/packages/mlirmiopen/package.py:            self.define("CMAKE_C_COMPILER", "{0}/bin/clang".format(spec["llvm-amdgpu"].prefix)),
var/spack/repos/builtin/packages/py-charm4py/package.py:    depends_on("cuda")
var/spack/repos/builtin/packages/py-charm4py/package.py:    # ends up with a cuda dependency causing unresolved symbol errors
var/spack/repos/builtin/packages/py-charm4py/package.py:    # link libcudart when building the charm++ library.
var/spack/repos/builtin/packages/py-charm4py/package.py:    # causing the Makefile to include libcudart when building libcharm.so
var/spack/repos/builtin/packages/py-charm4py/package.py:        env.set("SPACK_CHARM4PY_EXTRALIBS", self.spec["cuda"].libs.ld_flags)
var/spack/repos/builtin/packages/rocm-clang-ocl/package.py:class RocmClangOcl(CMakePackage):
var/spack/repos/builtin/packages/rocm-clang-ocl/package.py:    """OpenCL compilation with clang compiler"""
var/spack/repos/builtin/packages/rocm-clang-ocl/package.py:    homepage = "https://github.com/ROCm/clang-ocl"
var/spack/repos/builtin/packages/rocm-clang-ocl/package.py:    git = "https://github.com/ROCm/clang-ocl.git"
var/spack/repos/builtin/packages/rocm-clang-ocl/package.py:    url = "https://github.com/ROCm/clang-ocl/archive/rocm-6.1.2.tar.gz"
var/spack/repos/builtin/packages/rocm-clang-ocl/package.py:    tags = ["rocm"]
var/spack/repos/builtin/packages/rocm-clang-ocl/package.py:        depends_on(f"rocm-cmake@{ver}:", type="build", when=f"@{ver}")
var/spack/repos/builtin/packages/rocm-clang-ocl/package.py:        depends_on(f"llvm-amdgpu@{ver}", when=f"@{ver}")
var/spack/repos/builtin/packages/rocm-clang-ocl/package.py:        depends_on(f"rocm-device-libs@{ver}", when=f"@{ver} ^llvm-amdgpu ~rocm-device-libs")
var/spack/repos/builtin/packages/rocm-clang-ocl/package.py:        depends_on(f"rocm-core@{ver}", when=f"@{ver}")
var/spack/repos/builtin/packages/rocm-clang-ocl/package.py:            cmake("-DCMAKE_PREFIX_PATH=" + self.spec["rocm-clang-ocl"].prefix, ".")
var/spack/repos/builtin/packages/parsec/package.py:class Parsec(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/parsec/package.py:    variant("cuda", default=True, description="Build with CUDA")
var/spack/repos/builtin/packages/parsec/package.py:            self.define_from_variant("PARSEC_GPU_WITH_CUDA", "cuda"),
var/spack/repos/builtin/packages/parsec/package.py:            if "+cuda" in self.spec:
var/spack/repos/builtin/packages/parsec/package.py:                args.append("-DCUDA_TOOLKIT_ROOT_DIR:STRING=" + self.spec["cuda"].prefix)
var/spack/repos/builtin/packages/cloverleaf/package.py:        values=("cuda", "mpi_only", "openacc_cray", "openmp_only", "ref", "serial"),
var/spack/repos/builtin/packages/cloverleaf/package.py:    depends_on("mpi", when="build=cuda")
var/spack/repos/builtin/packages/cloverleaf/package.py:    depends_on("mpi", when="build=openacc_cray")
var/spack/repos/builtin/packages/cloverleaf/package.py:    depends_on("cuda", when="build=cuda")
var/spack/repos/builtin/packages/cloverleaf/package.py:    conflicts("build=cuda", when="%aocc", msg="Currently AOCC supports only ref variant")
var/spack/repos/builtin/packages/cloverleaf/package.py:    conflicts("build=openacc_cray", when="%aocc", msg="Currently AOCC supports only ref variant")
var/spack/repos/builtin/packages/cloverleaf/package.py:        if self.spec.satisfies("build=cuda"):
var/spack/repos/builtin/packages/cloverleaf/package.py:            build = "CUDA"
var/spack/repos/builtin/packages/cloverleaf/package.py:        elif self.spec.satisfies("build=openacc_cray"):
var/spack/repos/builtin/packages/cloverleaf/package.py:            build = "OpenACC_CRAY"
var/spack/repos/builtin/packages/openmm/package.py:class Openmm(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/openmm/package.py:    depends_on("cuda", when="+cuda", type=("build", "link", "run"))
var/spack/repos/builtin/packages/openmm/package.py:    # `openmm@7.5.1+cuda`, which is the version currently required by
var/spack/repos/builtin/packages/openmm/package.py:        when="@7.5.1+cuda",
var/spack/repos/builtin/packages/openmm/package.py:        if "+cuda" in spec:
var/spack/repos/builtin/packages/openmm/package.py:            env.set("OPENMM_CUDA_COMPILER", self.spec["cuda"].prefix.bin.nvcc)
var/spack/repos/builtin/packages/openmm/package.py:            env.set("CUDA_HOST_COMPILER", self.compiler.cxx)
var/spack/repos/builtin/packages/openmm/package.py:        if "+cuda" in spec:
var/spack/repos/builtin/packages/openmm/package.py:            env.set("OPENMM_CUDA_COMPILER", self.spec["cuda"].prefix.bin.nvcc)
var/spack/repos/builtin/packages/openmm/package.py:            env.set("CUDA_HOST_COMPILER", self.compiler.cxx)
var/spack/repos/builtin/packages/openmm/package.py:        if "+cuda" in spec:
var/spack/repos/builtin/packages/openmm/package.py:            env.set("OPENMM_CUDA_COMPILER", self.spec["cuda"].prefix.bin.nvcc)
var/spack/repos/builtin/packages/openmm/package.py:            env.set("CUDA_HOST_COMPILER", self.compiler.cxx)
var/spack/repos/builtin/packages/openmm/package.py:        if "+cuda" in spec:
var/spack/repos/builtin/packages/openmm/package.py:            env.set("OPENMM_CUDA_COMPILER", self.spec["cuda"].prefix.bin.nvcc)
var/spack/repos/builtin/packages/openmm/package.py:            env.set("CUDA_HOST_COMPILER", self.compiler.cxx)
var/spack/repos/builtin/packages/hsa-rocr-dev/package.py:    HSA ROCm kernel agents.AMD Heterogeneous System Architecture HSA -
var/spack/repos/builtin/packages/hsa-rocr-dev/package.py:    Linux HSA Runtime for Boltzmann (ROCm) platforms."""
var/spack/repos/builtin/packages/hsa-rocr-dev/package.py:    homepage = "https://github.com/ROCm/ROCR-Runtime"
var/spack/repos/builtin/packages/hsa-rocr-dev/package.py:    git = "https://github.com/ROCm/ROCR-Runtime.git"
var/spack/repos/builtin/packages/hsa-rocr-dev/package.py:    url = "https://github.com/ROCm/ROCR-Runtime/archive/rocm-6.2.1.tar.gz"
var/spack/repos/builtin/packages/hsa-rocr-dev/package.py:    tags = ["rocm"]
var/spack/repos/builtin/packages/hsa-rocr-dev/package.py:        depends_on(f"llvm-amdgpu@{ver}", when=f"@{ver}")
var/spack/repos/builtin/packages/hsa-rocr-dev/package.py:        # allow standalone rocm-device-libs (useful for aomp)
var/spack/repos/builtin/packages/hsa-rocr-dev/package.py:        depends_on(f"rocm-device-libs@{ver}", when=f"@{ver} ^llvm-amdgpu ~rocm-device-libs")
var/spack/repos/builtin/packages/hsa-rocr-dev/package.py:        depends_on(f"rocm-core@{ver}", when=f"@{ver}")
var/spack/repos/builtin/packages/hsa-rocr-dev/package.py:        # device libs is bundled with llvm-amdgpu (default) or standalone
var/spack/repos/builtin/packages/hsa-rocr-dev/package.py:        if self.spec.satisfies("^rocm-device-libs"):
var/spack/repos/builtin/packages/hsa-rocr-dev/package.py:            bitcode_dir = spec["rocm-device-libs"].prefix.amdgcn.bitcode
var/spack/repos/builtin/packages/hsa-rocr-dev/package.py:            bitcode_dir = spec["llvm-amdgpu"].prefix.amdgcn.bitcode
var/spack/repos/builtin/packages/hsa-rocr-dev/package.py:            args.append(self.define("ROCM_PATCH_VERSION", "60000"))
var/spack/repos/builtin/packages/hsa-rocr-dev/package.py:            args.append(self.define("ROCM_PATCH_VERSION", "60100"))
var/spack/repos/builtin/packages/halide/package.py:    variant("opencl", default=False, description="Build Non-llvm based OpenCl-C backend.")
var/spack/repos/builtin/packages/halide/package.py:        "amdgpu",
var/spack/repos/builtin/packages/halide/package.py:            self.define_from_variant("TARGET_OPENCL", "opencl"),
var/spack/repos/builtin/packages/halide/package.py:        "amdgpu": "AMDGPU",
var/spack/repos/builtin/packages/superlu-dist/package.py:class SuperluDist(CMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/superlu-dist/package.py:    depends_on("hipblas", when="+rocm")
var/spack/repos/builtin/packages/superlu-dist/package.py:    depends_on("rocsolver", when="+rocm")
var/spack/repos/builtin/packages/superlu-dist/package.py:    conflicts("+rocm", when="+cuda")
var/spack/repos/builtin/packages/superlu-dist/package.py:    conflicts("+cuda", when="@:6.3")
var/spack/repos/builtin/packages/superlu-dist/package.py:    conflicts("^cuda@11.5.0:", when="@7.1.0:7.1 +cuda")
var/spack/repos/builtin/packages/superlu-dist/package.py:        if "+cuda" in spec:
var/spack/repos/builtin/packages/superlu-dist/package.py:            append_define("TPL_ENABLE_CUDALIB", True)
var/spack/repos/builtin/packages/superlu-dist/package.py:            cuda_arch = spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/superlu-dist/package.py:            if cuda_arch[0] != "none":
var/spack/repos/builtin/packages/superlu-dist/package.py:                append_define("CMAKE_CUDA_ARCHITECTURES", cuda_arch[0])
var/spack/repos/builtin/packages/superlu-dist/package.py:        if "+rocm" in spec and (spec.satisfies("@amd") or spec.satisfies("@8:")):
var/spack/repos/builtin/packages/superlu-dist/package.py:            rocm_archs = spec.variants["amdgpu_target"].value
var/spack/repos/builtin/packages/superlu-dist/package.py:            if "none" not in rocm_archs:
var/spack/repos/builtin/packages/superlu-dist/package.py:                    "HIP_HIPCC_FLAGS", "--amdgpu-target=" + ",".join(rocm_archs) + " -I/" + mpiinc
var/spack/repos/builtin/packages/superlu-dist/CMAKE_INSTALL_LIBDIR.patch:@@ -29,7 +30,8 @@ HAVE_CUDA       = @HAVE_CUDA@
var/spack/repos/builtin/packages/meson/oneapi.patch:     NvidiaHPC_CCompiler,
var/spack/repos/builtin/packages/meson/oneapi.patch:     NvidiaHPC_CPPCompiler,
var/spack/repos/builtin/packages/stdexec/package.py:    homepage = "https://github.com/NVIDIA/stdexec"
var/spack/repos/builtin/packages/stdexec/package.py:    url = "https://github.com/NVIDIA/stdexec/archive/nvhpc-0.0.tar.gz"
var/spack/repos/builtin/packages/stdexec/package.py:    git = "https://github.com/NVIDIA/stdexec.git"
var/spack/repos/builtin/packages/berkeley-db/package.py:        # proper atomic support when using the NVIDIA compilers
var/spack/repos/builtin/packages/rdc/package.py:    """ROCm Data Center Tool"""
var/spack/repos/builtin/packages/rdc/package.py:    homepage = "https://github.com/ROCm/rdc"
var/spack/repos/builtin/packages/rdc/package.py:    url = "https://github.com/ROCm/rdc/archive/rocm-6.1.2.tar.gz"
var/spack/repos/builtin/packages/rdc/package.py:    tags = ["rocm"]
var/spack/repos/builtin/packages/rdc/package.py:            return "https://github.com/ROCm/rdc/archive/rdc_so_ver-0.3.tar.gz"
var/spack/repos/builtin/packages/rdc/package.py:        url = "https://github.com/ROCm/rdc/archive/rocm-{0}.tar.gz"
var/spack/repos/builtin/packages/rdc/package.py:        depends_on(f"rocm-smi-lib@{ver}", type=("build", "link"), when=f"@{ver}")
var/spack/repos/builtin/packages/rdc/package.py:        depends_on(f"rocm-core@{ver}", when=f"@{ver}")
var/spack/repos/builtin/packages/rdc/package.py:        filter_file(r"\${ROCM_DIR}/rocm_smi", "${ROCM_SMI_DIR}", "CMakeLists.txt")
var/spack/repos/builtin/packages/rdc/package.py:                r"${ROCM_DIR}/${CMAKE_INSTALL_INCLUDEDIR}",
var/spack/repos/builtin/packages/rdc/package.py:                "{0}/include".format(self.spec["rocm-smi-lib"].prefix),
var/spack/repos/builtin/packages/rdc/package.py:                r"${ROCM_DIR}/${CMAKE_INSTALL_LIBDIR}",
var/spack/repos/builtin/packages/rdc/package.py:                "{0}/lib".format(self.spec["rocm-smi-lib"].prefix),
var/spack/repos/builtin/packages/rdc/package.py:            self.define("ROCM_SMI_DIR", self.spec["rocm-smi-lib"].prefix),
var/spack/repos/builtin/packages/qgis/package.py:                "-DUSE_OPENCL=OFF",
var/spack/repos/builtin/packages/caffe/package.py:class Caffe(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/caffe/package.py:    variant("cuda", default=False, description="Builds with support for GPUs via CUDA and cuDNN")
var/spack/repos/builtin/packages/caffe/package.py:    depends_on("cuda", when="+cuda")
var/spack/repos/builtin/packages/caffe/package.py:            "-DCPU_ONLY=%s" % ("~cuda" in spec),
var/spack/repos/builtin/packages/caffe/package.py:            "-DUSE_CUDNN=%s" % ("+cuda" in spec),
var/spack/repos/builtin/packages/caffe/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/caffe/package.py:            if spec.variants["cuda_arch"].value[0] != "none":
var/spack/repos/builtin/packages/caffe/package.py:                cuda_arch = spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/caffe/package.py:                args.append(self.define("CUDA_ARCH_NAME", "Manual"))
var/spack/repos/builtin/packages/caffe/package.py:                args.append(self.define("CUDA_ARCH_BIN", " ".join(cuda_arch)))
var/spack/repos/builtin/packages/sgpp/package.py:    # Fix compilation issue with opencl introduced in 3.2.0
var/spack/repos/builtin/packages/sgpp/package.py:    patch("ocl.patch", when="@3.2.0+opencl")
var/spack/repos/builtin/packages/sgpp/package.py:        "opencl", default=False, description="Enables support for OpenCL accelerated operations"
var/spack/repos/builtin/packages/sgpp/package.py:    # OpenCL dependency
var/spack/repos/builtin/packages/sgpp/package.py:    depends_on("opencl@1.1:", when="+opencl", type=("build", "run"))
var/spack/repos/builtin/packages/sgpp/package.py:        # OpenCL Flags
var/spack/repos/builtin/packages/sgpp/package.py:        self.args.append("USE_OCL={0}".format("1" if "+opencl" in spec else "0"))
var/spack/repos/builtin/packages/raja-perf/package.py:class RajaPerf(CachedCMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/raja-perf/package.py:    depends_on("cmake@3.23:", when="@0.12.0:2023.06.0 +rocm", type="build")
var/spack/repos/builtin/packages/raja-perf/package.py:    depends_on("rocprim", when="+rocm")
var/spack/repos/builtin/packages/raja-perf/package.py:    depends_on("caliper@2.9.0: +cuda", when="+caliper +cuda")
var/spack/repos/builtin/packages/raja-perf/package.py:    depends_on("caliper@2.9.0: +rocm", when="+caliper +rocm")
var/spack/repos/builtin/packages/raja-perf/package.py:    with when("@0.12.0: +rocm +caliper"):
var/spack/repos/builtin/packages/raja-perf/package.py:        depends_on("caliper +rocm")
var/spack/repos/builtin/packages/raja-perf/package.py:        for arch in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/raja-perf/package.py:                "caliper +rocm amdgpu_target={0}".format(arch),
var/spack/repos/builtin/packages/raja-perf/package.py:                when="amdgpu_target={0}".format(arch),
var/spack/repos/builtin/packages/raja-perf/package.py:    with when("@0.12.0: +cuda +caliper"):
var/spack/repos/builtin/packages/raja-perf/package.py:        depends_on("caliper +cuda")
var/spack/repos/builtin/packages/raja-perf/package.py:        for sm_ in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/raja-perf/package.py:            depends_on("caliper +cuda cuda_arch={0}".format(sm_), when="cuda_arch={0}".format(sm_))
var/spack/repos/builtin/packages/raja-perf/package.py:    conflicts("+cuda", when="+omptarget", msg="Cuda may not be activated when omptarget is ON")
var/spack/repos/builtin/packages/raja-perf/package.py:    conflicts("+omptarget +rocm")
var/spack/repos/builtin/packages/raja-perf/package.py:    conflicts("+sycl +rocm")
var/spack/repos/builtin/packages/raja-perf/package.py:        if spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/raja-perf/package.py:        # T benefit from the shared function "cuda_for_radiuss_projects",
var/spack/repos/builtin/packages/raja-perf/package.py:        # we do not modify CMAKE_CUDA_FLAGS: it is already appended by the
var/spack/repos/builtin/packages/raja-perf/package.py:        if "+cuda" in spec:
var/spack/repos/builtin/packages/raja-perf/package.py:            entries.append(cmake_cache_option("ENABLE_CUDA", True))
var/spack/repos/builtin/packages/raja-perf/package.py:            # Shared handling of cuda.
var/spack/repos/builtin/packages/raja-perf/package.py:            # We place everything in CMAKE_CUDA_FLAGS_(RELEASE|RELWITHDEBINFO|DEBUG)
var/spack/repos/builtin/packages/raja-perf/package.py:            # which are not set by cuda_for_radiuss_projects
var/spack/repos/builtin/packages/raja-perf/package.py:                cuda_release_flags = "-O3 -Xcompiler -O2 " + all_targets_flags
var/spack/repos/builtin/packages/raja-perf/package.py:                cuda_reldebinf_flags = "-O3 -g -Xcompiler -O2 " + all_targets_flags
var/spack/repos/builtin/packages/raja-perf/package.py:                cuda_debug_flags = "-O0 -g -Xcompiler -O2 " + all_targets_flags
var/spack/repos/builtin/packages/raja-perf/package.py:                cuda_release_flags = "-O3 -Xcompiler -Ofast " + all_targets_flags
var/spack/repos/builtin/packages/raja-perf/package.py:                cuda_reldebinf_flags = "-O3 -g -Xcompiler -Ofast " + all_targets_flags
var/spack/repos/builtin/packages/raja-perf/package.py:                cuda_debug_flags = "-O0 -g -Xcompiler -O0 " + all_targets_flags
var/spack/repos/builtin/packages/raja-perf/package.py:                cuda_release_flags = "-O3 -Xcompiler -Ofast " + all_targets_flags
var/spack/repos/builtin/packages/raja-perf/package.py:                cuda_reldebinf_flags = "-O3 -g -Xcompiler -Ofast " + all_targets_flags
var/spack/repos/builtin/packages/raja-perf/package.py:                cuda_debug_flags = "-O0 -g -Xcompiler -O0 " + all_targets_flags
var/spack/repos/builtin/packages/raja-perf/package.py:            entries.append(cmake_cache_string("CMAKE_CUDA_FLAGS_RELEASE", cuda_release_flags))
var/spack/repos/builtin/packages/raja-perf/package.py:                cmake_cache_string("CMAKE_CUDA_FLAGS_RELWITHDEBINFO", cuda_reldebinf_flags)
var/spack/repos/builtin/packages/raja-perf/package.py:            entries.append(cmake_cache_string("CMAKE_CUDA_FLAGS_DEBUG", cuda_debug_flags))
var/spack/repos/builtin/packages/raja-perf/package.py:            entries.append(cmake_cache_option("ENABLE_CUDA", False))
var/spack/repos/builtin/packages/raja-perf/package.py:        if "+rocm" in spec:
var/spack/repos/builtin/packages/raja-perf/package.py:                        "BLT_OPENMP_COMPILE_FLAGS", "-fopenmp;-fopenmp-targets=nvptx64-nvidia-cuda"
var/spack/repos/builtin/packages/raja-perf/package.py:                        "BLT_OPENMP_LINK_FLAGS", "-fopenmp;-fopenmp-targets=nvptx64-nvidia-cuda"
var/spack/repos/builtin/packages/gsl-lite/package.py:    variant("cuda_tests", default=False, description="Build and perform gsl-lite CUDA tests")
var/spack/repos/builtin/packages/gsl-lite/package.py:            self.define_from_variant("GSL_LITE_OPT_BUILD_CUDA_TESTS", "cuda_tests"),
var/spack/repos/builtin/packages/nvpl-fft/package.py:    """NVPL FFT (NVIDIA Performance Libraries FFT) is part of NVIDIA Performance Libraries
var/spack/repos/builtin/packages/nvpl-fft/package.py:    homepage = "https://docs.nvidia.com/nvpl/_static/blas/index.html"
var/spack/repos/builtin/packages/nvpl-fft/package.py:        "https://developer.download.nvidia.com/compute/nvpl/redist"
var/spack/repos/builtin/packages/nvpl-fft/package.py:        url = "https://developer.download.nvidia.com/compute/nvpl/redist/nvpl_fft/linux-sbsa/nvpl_fft-linux-sbsa-{0}-archive.tar.xz"
var/spack/repos/builtin/packages/hydrogen/package.py:#   - CUDA must be v11.0.0 or newer.
var/spack/repos/builtin/packages/hydrogen/package.py:class Hydrogen(CachedCMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/hydrogen/package.py:        "cub", default=True, when="+cuda", description="Use CUB/hipCUB for GPU memory management"
var/spack/repos/builtin/packages/hydrogen/package.py:        "cub", default=True, when="+rocm", description="Use CUB/hipCUB for GPU memory management"
var/spack/repos/builtin/packages/hydrogen/package.py:    # TODO: Add netlib-lapack. For GPU-enabled builds, typical
var/spack/repos/builtin/packages/hydrogen/package.py:    conflicts("+cuda", when="+rocm", msg="CUDA and ROCm support are mutually exclusive")
var/spack/repos/builtin/packages/hydrogen/package.py:    conflicts("+half", when="+rocm", msg="FP16 support not implemented for ROCm.")
var/spack/repos/builtin/packages/hydrogen/package.py:    depends_on("aluminum +cuda +ht", when="+al +cuda")
var/spack/repos/builtin/packages/hydrogen/package.py:    depends_on("aluminum +rocm +ht", when="+al +rocm")
var/spack/repos/builtin/packages/hydrogen/package.py:    for arch in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/hydrogen/package.py:        depends_on("aluminum +cuda cuda_arch=%s" % arch, when="+al +cuda cuda_arch=%s" % arch)
var/spack/repos/builtin/packages/hydrogen/package.py:    # variants +rocm and amdgpu_targets are not automatically passed to
var/spack/repos/builtin/packages/hydrogen/package.py:    for val in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/hydrogen/package.py:            "aluminum +rocm amdgpu_target=%s" % val, when="+al +rocm amdgpu_target=%s" % val
var/spack/repos/builtin/packages/hydrogen/package.py:    depends_on("cuda@11.0.0:", when="+cuda")
var/spack/repos/builtin/packages/hydrogen/package.py:    depends_on("hipcub +rocm", when="+rocm +cub")
var/spack/repos/builtin/packages/hydrogen/package.py:    def get_cuda_flags(self):
var/spack/repos/builtin/packages/hydrogen/package.py:        if spec.satisfies("^cuda+allow-unsupported-compilers"):
var/spack/repos/builtin/packages/hydrogen/package.py:        entries.append(cmake_cache_option("Hydrogen_ENABLE_CUDA", spec.satisfies("+cuda")))
var/spack/repos/builtin/packages/hydrogen/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/hydrogen/package.py:            entries.append(cmake_cache_string("CMAKE_CUDA_STANDARD", "17"))
var/spack/repos/builtin/packages/hydrogen/package.py:            if not spec.satisfies("cuda_arch=none"):
var/spack/repos/builtin/packages/hydrogen/package.py:                archs = spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/hydrogen/package.py:                entries.append(cmake_cache_string("CMAKE_CUDA_ARCHITECTURES", arch_str))
var/spack/repos/builtin/packages/hydrogen/package.py:            # FIXME: Should this use the "cuda_flags" function of the
var/spack/repos/builtin/packages/hydrogen/package.py:            # CudaPackage class or something? There might be other
var/spack/repos/builtin/packages/hydrogen/package.py:            cuda_flags = self.get_cuda_flags()
var/spack/repos/builtin/packages/hydrogen/package.py:            if len(cuda_flags) > 0:
var/spack/repos/builtin/packages/hydrogen/package.py:                entries.append(cmake_cache_string("CMAKE_CUDA_FLAGS", " ".join(cuda_flags)))
var/spack/repos/builtin/packages/hydrogen/package.py:        entries.append(cmake_cache_option("Hydrogen_ENABLE_ROCM", spec.satisfies("+rocm")))
var/spack/repos/builtin/packages/hydrogen/package.py:        if spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/hydrogen/package.py:            if not spec.satisfies("amdgpu_target=none"):
var/spack/repos/builtin/packages/hydrogen/package.py:                archs = self.spec.variants["amdgpu_target"].value
var/spack/repos/builtin/packages/hydrogen/package.py:                entries.append(cmake_cache_string("AMDGPU_TARGETS", arch_str))
var/spack/repos/builtin/packages/hydrogen/package.py:                entries.append(cmake_cache_string("GPU_TARGETS", arch_str))
var/spack/repos/builtin/packages/hydrogen/package.py:            cmake_cache_option("Hydrogen_ENABLE_GPU_FP16", spec.satisfies("+cuda +half"))
var/spack/repos/builtin/packages/hydrogen/package.py:        # Note that CUDA/ROCm are handled above.
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:class PyNvidiaDali(PythonPackage):
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:    """A GPU-accelerated library containing highly optimized building blocks and
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:    homepage = "https://developer.nvidia.com/dali"
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:    url = "https://developer.download.nvidia.com/compute/redist/"
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            "1.41.0-cuda120",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            url="https://developer.download.nvidia.com/compute/redist/nvidia-dali-cuda120/nvidia_dali_cuda120-1.41.0-17427117-py3-none-manylinux2014_x86_64.whl",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            "1.41.0.cuda110",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            url="https://developer.download.nvidia.com/compute/redist/nvidia-dali-cuda110/nvidia_dali_cuda110-1.41.0-17427118-py3-none-manylinux2014_x86_64.whl",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            "1.36.0-cuda120",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            url="https://developer.download.nvidia.com/compute/redist/nvidia-dali-cuda120/nvidia_dali_cuda120-1.36.0-13435171-py3-none-manylinux2014_x86_64.whl",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            "1.36.0-cuda110",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            url="https://developer.download.nvidia.com/compute/redist/nvidia-dali-cuda110/nvidia_dali_cuda110-1.36.0-13435172-py3-none-manylinux2014_x86_64.whl",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            "1.27.0-cuda120",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            url="https://developer.download.nvidia.com/compute/redist/nvidia-dali-cuda120/nvidia_dali_cuda120-1.27.0-8625314-py3-none-manylinux2014_x86_64.whl",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            "1.27.0-cuda110",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            url="https://developer.download.nvidia.com/compute/redist/nvidia-dali-cuda110/nvidia_dali_cuda110-1.27.0-8625303-py3-none-manylinux2014_x86_64.whl",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            "1.26.0-cuda120",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            url="https://developer.download.nvidia.com/compute/redist/nvidia-dali-cuda120/nvidia_dali_cuda120-1.26.0-8269288-py3-none-manylinux2014_x86_64.whl",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            "1.26.0-cuda110",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            url="https://developer.download.nvidia.com/compute/redist/nvidia-dali-cuda110/nvidia_dali_cuda110-1.26.0-8269290-py3-none-manylinux2014_x86_64.whl",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            "1.25.0-cuda120",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            url="https://developer.download.nvidia.com/compute/redist/nvidia-dali-cuda120/nvidia_dali_cuda120-1.25.0-7922358-py3-none-manylinux2014_x86_64.whl",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            "1.25.0-cuda110",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            url="https://developer.download.nvidia.com/compute/redist/nvidia-dali-cuda110/nvidia_dali_cuda110-1.25.0-7922357-py3-none-manylinux2014_x86_64.whl",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            "1.24.0-cuda120",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            url="https://developer.download.nvidia.com/compute/redist/nvidia-dali-cuda120/nvidia_dali_cuda120-1.24.0-7582307-py3-none-manylinux2014_x86_64.whl",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            "1.24.0-cuda110",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            url="https://developer.download.nvidia.com/compute/redist/nvidia-dali-cuda110/nvidia_dali_cuda110-1.24.0-7582302-py3-none-manylinux2014_x86_64.whl",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            "1.23.0-cuda120",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            url="https://developer.download.nvidia.com/compute/redist/nvidia-dali-cuda120/nvidia_dali_cuda120-1.23.0-7355174-py3-none-manylinux2014_x86_64.whl",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            "1.23.0-cuda110",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            url="https://developer.download.nvidia.com/compute/redist/nvidia-dali-cuda110/nvidia_dali_cuda110-1.23.0-7355173-py3-none-manylinux2014_x86_64.whl",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            "1.22.0-cuda120",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            url="https://developer.download.nvidia.com/compute/redist/nvidia-dali-cuda120/nvidia_dali_cuda120-1.22.0-6971317-py3-none-manylinux2014_x86_64.whl",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            "1.22.0-cuda110",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            url="https://developer.download.nvidia.com/compute/redist/nvidia-dali-cuda110/nvidia_dali_cuda110-1.22.0-6988993-py3-none-manylinux2014_x86_64.whl",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            "1.41.0-cuda120",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            url="https://developer.download.nvidia.com/compute/redist/nvidia-dali-cuda120/nvidia_dali_cuda120-1.41.0-17427117-py3-none-manylinux2014_aarch64.whl",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            "1.41.0-cuda110",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            url="https://developer.download.nvidia.com/compute/redist/nvidia-dali-cuda110/nvidia_dali_cuda110-1.41.0-17427118-py3-none-manylinux2014_aarch64.whl",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            "1.36.0-cuda120",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            url="https://developer.download.nvidia.com/compute/redist/nvidia-dali-cuda120/nvidia_dali_cuda120-1.36.0-13435171-py3-none-manylinux2014_aarch64.whl",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            "1.36.0-cuda110",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            url="https://developer.download.nvidia.com/compute/redist/nvidia-dali-cuda110/nvidia_dali_cuda110-1.36.0-13435172-py3-none-manylinux2014_aarch64.whl",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            "1.27.0-cuda120",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            url="https://developer.download.nvidia.com/compute/redist/nvidia-dali-cuda120/nvidia_dali_cuda120-1.27.0-8625314-py3-none-manylinux2014_aarch64.whl",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            "1.27.0-cuda110",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            url="https://developer.download.nvidia.com/compute/redist/nvidia-dali-cuda110/nvidia_dali_cuda110-1.27.0-8625303-py3-none-manylinux2014_aarch64.whl",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            "1.26.0-cuda120",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            url="https://developer.download.nvidia.com/compute/redist/nvidia-dali-cuda120/nvidia_dali_cuda120-1.26.0-8269288-py3-none-manylinux2014_aarch64.whl",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            "1.26.0-cuda110",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            url="https://developer.download.nvidia.com/compute/redist/nvidia-dali-cuda110/nvidia_dali_cuda110-1.26.0-8269290-py3-none-manylinux2014_aarch64.whl",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            "1.25.0-cuda120",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            url="https://developer.download.nvidia.com/compute/redist/nvidia-dali-cuda120/nvidia_dali_cuda120-1.25.0-7922358-py3-none-manylinux2014_aarch64.whl",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            "1.25.0-cuda110",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            url="https://developer.download.nvidia.com/compute/redist/nvidia-dali-cuda110/nvidia_dali_cuda110-1.25.0-7922357-py3-none-manylinux2014_aarch64.whl",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            "1.24.0-cuda120",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            url="https://developer.download.nvidia.com/compute/redist/nvidia-dali-cuda120/nvidia_dali_cuda120-1.24.0-7582307-py3-none-manylinux2014_aarch64.whl",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            "1.24.0-cuda110",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            url="https://developer.download.nvidia.com/compute/redist/nvidia-dali-cuda110/nvidia_dali_cuda110-1.24.0-7582302-py3-none-manylinux2014_aarch64.whl",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            "1.23.0-cuda120",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            url="https://developer.download.nvidia.com/compute/redist/nvidia-dali-cuda120/nvidia_dali_cuda120-1.23.0-7355174-py3-none-manylinux2014_aarch64.whl",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            "1.23.0-cuda110",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            url="https://developer.download.nvidia.com/compute/redist/nvidia-dali-cuda110/nvidia_dali_cuda110-1.23.0-7355173-py3-none-manylinux2014_aarch64.whl",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            "1.22.0-cuda120",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            url="https://developer.download.nvidia.com/compute/redist/nvidia-dali-cuda120/nvidia_dali_cuda120-1.22.0-6971317-py3-none-manylinux2014_aarch64.whl",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            "1.22.0-cuda110",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:            url="https://developer.download.nvidia.com/compute/redist/nvidia-dali-cuda110/nvidia_dali_cuda110-1.22.0-6988993-py3-none-manylinux2014_aarch64.whl",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:    cuda120_versions = (
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:        "@1.41.0-cuda120",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:        "@1.36.0-cuda120",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:        "@1.27.0-cuda120",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:        "@1.26.0-cuda120",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:        "@1.25.0-cuda120",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:        "@1.24.0-cuda120",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:        "@1.23.0-cuda120",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:        "@1.22.0-cuda120",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:    cuda110_versions = (
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:        "@1.41.0-cuda110",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:        "@1.36.0-cuda110",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:        "@1.27.0-cuda110",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:        "@1.26.0-cuda110",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:        "@1.25.0-cuda110",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:        "@1.24.0-cuda110",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:        "@1.23.0-cuda110",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:        "@1.22.0-cuda110",
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:    for v in cuda120_versions:
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:        depends_on("cuda@12", when=v, type=("build", "run"))
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:    for v in cuda110_versions:
var/spack/repos/builtin/packages/py-nvidia-dali/package.py:        depends_on("cuda@11", when=v, type=("build", "run"))
var/spack/repos/builtin/packages/nalu-wind/package.py:class NaluWind(CMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/nalu-wind/package.py:    variant("gpu-aware-mpi", default=False, description="gpu-aware-mpi")
var/spack/repos/builtin/packages/nalu-wind/package.py:    depends_on("trilinos~cuda~wrapper", when="~cuda")
var/spack/repos/builtin/packages/nalu-wind/package.py:    depends_on("kokkos-nvcc-wrapper", type="build", when="+cuda")
var/spack/repos/builtin/packages/nalu-wind/package.py:    depends_on("hypre+gpu-aware-mpi", when="+gpu-aware-mpi")
var/spack/repos/builtin/packages/nalu-wind/package.py:    for _arch in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/nalu-wind/package.py:            "trilinos~shared+cuda+cuda_rdc+wrapper cuda_arch={0}".format(_arch),
var/spack/repos/builtin/packages/nalu-wind/package.py:            when="+cuda cuda_arch={0}".format(_arch),
var/spack/repos/builtin/packages/nalu-wind/package.py:            "hypre@2.30.0: +cuda cuda_arch={0}".format(_arch),
var/spack/repos/builtin/packages/nalu-wind/package.py:            when="+hypre+cuda cuda_arch={0}".format(_arch),
var/spack/repos/builtin/packages/nalu-wind/package.py:    for _arch in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/nalu-wind/package.py:            "trilinos~shared+rocm+rocm_rdc amdgpu_target={0}".format(_arch),
var/spack/repos/builtin/packages/nalu-wind/package.py:            when="+rocm amdgpu_target={0}".format(_arch),
var/spack/repos/builtin/packages/nalu-wind/package.py:            "hypre@2.30.0: +rocm amdgpu_target={0}".format(_arch),
var/spack/repos/builtin/packages/nalu-wind/package.py:            when="+hypre+rocm amdgpu_target={0}".format(_arch),
var/spack/repos/builtin/packages/nalu-wind/package.py:        when="+cuda",
var/spack/repos/builtin/packages/nalu-wind/package.py:        msg="invalid device functions are generated with shared libs and cuda",
var/spack/repos/builtin/packages/nalu-wind/package.py:        when="+rocm",
var/spack/repos/builtin/packages/nalu-wind/package.py:        msg="invalid device functions are generated with shared libs and rocm",
var/spack/repos/builtin/packages/nalu-wind/package.py:    conflicts("+cuda", when="+rocm")
var/spack/repos/builtin/packages/nalu-wind/package.py:    conflicts("+rocm", when="+cuda")
var/spack/repos/builtin/packages/nalu-wind/package.py:    conflicts("^hypre+cuda", when="~cuda")
var/spack/repos/builtin/packages/nalu-wind/package.py:    conflicts("^hypre+rocm", when="~rocm")
var/spack/repos/builtin/packages/nalu-wind/package.py:    conflicts("^trilinos+cuda", when="~cuda")
var/spack/repos/builtin/packages/nalu-wind/package.py:    conflicts("^trilinos+rocm", when="~rocm")
var/spack/repos/builtin/packages/nalu-wind/package.py:        if spec.satisfies("+cuda") or spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/nalu-wind/package.py:            env.set("CUDA_LAUNCH_BLOCKING", "1")
var/spack/repos/builtin/packages/nalu-wind/package.py:            env.set("CUDA_MANAGED_FORCE_DEVICE_ALLOC", "1")
var/spack/repos/builtin/packages/nalu-wind/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/nalu-wind/package.py:        if spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/nalu-wind/package.py:            env.append_flags("CXXFLAGS", "-fgpu-rdc")
var/spack/repos/builtin/packages/nalu-wind/package.py:            self.define_from_variant("ENABLE_CUDA", "cuda"),
var/spack/repos/builtin/packages/nalu-wind/package.py:        if spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/nalu-wind/package.py:            args.append(self.define("ENABLE_ROCM", True))
var/spack/repos/builtin/packages/nalu-wind/package.py:            targets = spec.variants["amdgpu_target"].value
var/spack/repos/builtin/packages/nalu-wind/package.py:            args.append(self.define("GPU_TARGETS", ";".join(str(x) for x in targets)))
var/spack/repos/builtin/packages/xgboost/package.py:class Xgboost(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/xgboost/package.py:    variant("nccl", default=False, description="Build with NCCL to enable distributed GPU support")
var/spack/repos/builtin/packages/xgboost/package.py:    with when("+cuda"):
var/spack/repos/builtin/packages/xgboost/package.py:        depends_on("cuda@10:")
var/spack/repos/builtin/packages/xgboost/package.py:        depends_on("cuda@:11.4", when="@:1.5.0")
var/spack/repos/builtin/packages/xgboost/package.py:        depends_on("cuda@:11", when="@:1.6")
var/spack/repos/builtin/packages/xgboost/package.py:        depends_on("cuda@:12.3", when="@:1.7")
var/spack/repos/builtin/packages/xgboost/package.py:        depends_on("cuda@:12.4", when="@:2.1")
var/spack/repos/builtin/packages/xgboost/package.py:    depends_on("nccl", when="+nccl")
var/spack/repos/builtin/packages/xgboost/package.py:    conflicts("%gcc@:7", when="+cuda")
var/spack/repos/builtin/packages/xgboost/package.py:    conflicts("+nccl", when="~cuda", msg="NCCL requires CUDA")
var/spack/repos/builtin/packages/xgboost/package.py:    conflicts("+cuda", when="~openmp", msg="CUDA requires OpenMP")
var/spack/repos/builtin/packages/xgboost/package.py:        "cuda_arch=none",
var/spack/repos/builtin/packages/xgboost/package.py:        when="+cuda",
var/spack/repos/builtin/packages/xgboost/package.py:        msg="Must specify CUDA compute capabilities of your GPU, see "
var/spack/repos/builtin/packages/xgboost/package.py:        "https://developer.nvidia.com/cuda-gpus",
var/spack/repos/builtin/packages/xgboost/package.py:            self.define_from_variant("USE_CUDA", "cuda"),
var/spack/repos/builtin/packages/xgboost/package.py:            self.define_from_variant("USE_NCCL", "nccl"),
var/spack/repos/builtin/packages/xgboost/package.py:        if "+cuda" in self.spec:
var/spack/repos/builtin/packages/xgboost/package.py:            args.append(self.define("GPU_COMPUTE_VER", self.spec.variants["cuda_arch"].value))
var/spack/repos/builtin/packages/xgboost/package.py:        if "@1.5: ^cuda@11.4:" in self.spec:
var/spack/repos/builtin/packages/xgboost/package.py:            args.append(self.define("BUILD_WITH_CUDA_CUB", True))
var/spack/repos/builtin/packages/mumax/package.py:class Mumax(MakefilePackage, CudaPackage):
var/spack/repos/builtin/packages/mumax/package.py:    """GPU accelerated micromagnetic simulator."""
var/spack/repos/builtin/packages/mumax/package.py:    variant("cuda", default=True, description="Use CUDA; must be true")
var/spack/repos/builtin/packages/mumax/package.py:    depends_on("cuda")
var/spack/repos/builtin/packages/mumax/package.py:    conflicts("~cuda", msg="mumax requires cuda")
var/spack/repos/builtin/packages/mumax/package.py:    def cuda_arch(self):
var/spack/repos/builtin/packages/mumax/package.py:        cuda_arch = " ".join(self.spec.variants["cuda_arch"].value)
var/spack/repos/builtin/packages/mumax/package.py:        if cuda_arch == "none":
var/spack/repos/builtin/packages/mumax/package.py:            raise InstallError("Must select at least one value for cuda_arch")
var/spack/repos/builtin/packages/mumax/package.py:        return cuda_arch
var/spack/repos/builtin/packages/mumax/package.py:        filter_file(r"(^all: cudakernels) hooks$", r"\1", "Makefile")
var/spack/repos/builtin/packages/mumax/package.py:        filter_file(r"(for cc in ).*(; do)", r"\1{0}\2".format(self.cuda_arch), "cuda/make.bash")
var/spack/repos/builtin/packages/mumax/package.py:        env.set("CUDA_CC", self.cuda_arch)
var/spack/repos/builtin/packages/vacuumms/package.py:    variant("cuda", default=False, description="Build CUDA applications and utilities")
var/spack/repos/builtin/packages/vacuumms/package.py:    depends_on("cuda", type=("link", "run"), when="+cuda")
var/spack/repos/builtin/packages/vacuumms/package.py:            self.define_from_variant("BUILD_CUDA_COMPONENTS", "cuda"),
var/spack/repos/builtin/packages/vasp/package.py:class Vasp(MakefilePackage, CudaPackage):
var/spack/repos/builtin/packages/vasp/package.py:    variant("cuda", default=False, description="Enables running on Nvidia GPUs")
var/spack/repos/builtin/packages/vasp/package.py:    depends_on("nccl", when="@6.3: +cuda")
var/spack/repos/builtin/packages/vasp/package.py:    conflicts("+vaspsol", when="+cuda", msg="+vaspsol only available for CPU")
var/spack/repos/builtin/packages/vasp/package.py:    requires("%nvhpc", when="@6.3: +cuda", msg="vasp requires nvhpc to build the openacc build")
var/spack/repos/builtin/packages/vasp/package.py:        "cuda_arch=none", when="@6.3: +cuda", msg="CUDA arch required when building openacc port"
var/spack/repos/builtin/packages/vasp/package.py:                if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/vasp/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/vasp/package.py:                # openacc
var/spack/repos/builtin/packages/vasp/package.py:                cpp_options.extend(["-D_OPENACC", "-DUSENCCL"])
var/spack/repos/builtin/packages/vasp/package.py:                llibs.extend(["-cudalib=cublas,cusolver,cufft,nccl", "-cuda"])
var/spack/repos/builtin/packages/vasp/package.py:                cuda_flags = [f"cuda{str(spec['cuda'].version.dotted[0:2])}", "rdc"]
var/spack/repos/builtin/packages/vasp/package.py:                for f in spec.variants["cuda_arch"].value:
var/spack/repos/builtin/packages/vasp/package.py:                    cuda_flags.append(f"cc{f}")
var/spack/repos/builtin/packages/vasp/package.py:                fc.append(f"-gpu={','.join(cuda_flags)}")
var/spack/repos/builtin/packages/vasp/package.py:                fcl.append(f"-gpu={','.join(cuda_flags)}")
var/spack/repos/builtin/packages/vasp/package.py:                # old cuda thing
var/spack/repos/builtin/packages/vasp/package.py:                cflags.extend(["-DGPUSHMEM=300", "-DHAVE_CUBLAS"])
var/spack/repos/builtin/packages/vasp/package.py:                filter_file(r"^CUDA_ROOT[ \t]*\?=.*$", spec["cuda"].prefix, make_include)
var/spack/repos/builtin/packages/vasp/package.py:        if self.spec.satisfies("%nvhpc +cuda"):
var/spack/repos/builtin/packages/vasp/package.py:            spack_env.set("NVHPC_CUDA_HOME", self.spec["cuda"].prefix)
var/spack/repos/builtin/packages/vasp/package.py:            if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/tiled-mm/package.py:class TiledMm(CMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/tiled-mm/package.py:    """Matrix multiplication on GPUs for matrices stored on a CPU. Similar to cublasXt,
var/spack/repos/builtin/packages/tiled-mm/package.py:    but ported to both NVIDIA and AMD GPUs."""
var/spack/repos/builtin/packages/tiled-mm/package.py:    depends_on("rocblas", when="+rocm")
var/spack/repos/builtin/packages/tiled-mm/package.py:    conflicts("~cuda~rocm")
var/spack/repos/builtin/packages/tiled-mm/package.py:    conflicts("+cuda", when="+rocm")
var/spack/repos/builtin/packages/tiled-mm/package.py:        if "+rocm" in self.spec:
var/spack/repos/builtin/packages/tiled-mm/package.py:            args.extend([self.define("TILEDMM_GPU_BACKEND", "ROCM")])
var/spack/repos/builtin/packages/tiled-mm/package.py:        if "+cuda" in self.spec:
var/spack/repos/builtin/packages/tiled-mm/package.py:            args.extend([self.define("TILEDMM_GPU_BACKEND", "CUDA")])
var/spack/repos/builtin/packages/openspeedshop/package.py:    variant("cuda", default=False, description="build with cuda packages included.")
var/spack/repos/builtin/packages/openspeedshop/package.py:    depends_on("cbtf-argonavis@develop", when="@develop+cuda", type=("build", "link", "run"))
var/spack/repos/builtin/packages/openspeedshop/package.py:    depends_on("cbtf-argonavis@1.9.3:9999", when="@2.4.0:9999+cuda", type=("build", "link", "run"))
var/spack/repos/builtin/packages/openspeedshop/package.py:            if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/openspeedshop/package.py:            if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/ftk/package.py:    variant("cuda", default=False, description="Use CUDA")
var/spack/repos/builtin/packages/ftk/package.py:    depends_on("cuda", when="+cuda")
var/spack/repos/builtin/packages/ftk/package.py:        self.add_cmake_option(args, "+cuda", "FTK_USE_CUDA")
var/spack/repos/builtin/packages/arrow/package.py:class Arrow(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/arrow/package.py:        args.append(self.define_from_variant("ARROW_CUDA", "cuda"))
var/spack/repos/builtin/packages/megadock/package.py:class Megadock(MakefilePackage, CudaPackage):
var/spack/repos/builtin/packages/megadock/package.py:        # point cuda samples relative to cuda installation
var/spack/repos/builtin/packages/megadock/package.py:            "/opt/cuda/6.5/samples", "$(CUDA_INSTALL_PATH)/samples", "Makefile", string=True
var/spack/repos/builtin/packages/megadock/package.py:        # makefile has a weird var for cuda_arch, use conditionally
var/spack/repos/builtin/packages/megadock/package.py:        if "+cuda" in spec:
var/spack/repos/builtin/packages/megadock/package.py:            arch = spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/megadock/package.py:            "USE_GPU=%s" % ("1" if "+cuda" in spec else "0"),
var/spack/repos/builtin/packages/megadock/package.py:        if "+cuda" in spec:
var/spack/repos/builtin/packages/megadock/package.py:            targets.append("CUDA_INSTALL_PATH=%s" % self.spec["cuda"].prefix)
var/spack/repos/builtin/packages/megadock/package.py:        for suffix in ["", "-gpu", "-dp", "-gpu-dp"]:
var/spack/repos/builtin/packages/composable-kernel/package.py:    homepage = "https://github.com/ROCm/composable_kernel"
var/spack/repos/builtin/packages/composable-kernel/package.py:    git = "https://github.com/ROCm/composable_kernel.git"
var/spack/repos/builtin/packages/composable-kernel/package.py:    url = "https://github.com/ROCm/composable_kernel/archive/refs/tags/rocm-6.1.2.tar.gz"
var/spack/repos/builtin/packages/composable-kernel/package.py:    amdgpu_targets = ROCmPackage.amdgpu_targets
var/spack/repos/builtin/packages/composable-kernel/package.py:        "amdgpu_target",
var/spack/repos/builtin/packages/composable-kernel/package.py:        values=auto_or_any_combination_of(*amdgpu_targets),
var/spack/repos/builtin/packages/composable-kernel/package.py:        description="set gpu targets",
var/spack/repos/builtin/packages/composable-kernel/package.py:        depends_on("llvm-amdgpu@" + ver, when="@" + ver)
var/spack/repos/builtin/packages/composable-kernel/package.py:        depends_on("rocm-cmake@" + ver, when="@" + ver, type="build")
var/spack/repos/builtin/packages/composable-kernel/package.py:    # https://github.com/ROCm/composable_kernel/commit/959073842c0db839d45d565eb260fd018c996ce4
var/spack/repos/builtin/packages/composable-kernel/package.py:                "CMAKE_CXX_COMPILER", "{0}/bin/clang++".format(spec["llvm-amdgpu"].prefix)
var/spack/repos/builtin/packages/composable-kernel/package.py:            self.define("CMAKE_C_COMPILER", "{0}/bin/clang".format(spec["llvm-amdgpu"].prefix)),
var/spack/repos/builtin/packages/composable-kernel/package.py:        if "auto" not in self.spec.variants["amdgpu_target"]:
var/spack/repos/builtin/packages/composable-kernel/package.py:            args.append(self.define_from_variant("GPU_TARGETS", "amdgpu_target"))
var/spack/repos/builtin/packages/composable-kernel/0001-mark-kernels-maybe-unused.patch:diff --git a/include/ck/tensor_operation/gpu/device/impl/device_grouped_conv_bwd_weight_two_stage_xdl_cshuffle.hpp b/include/ck/tensor_operation/gpu/device/impl/device_grouped_conv_bwd_weight_two_stage_xdl_cshuffle.hpp
var/spack/repos/builtin/packages/composable-kernel/0001-mark-kernels-maybe-unused.patch:--- a/include/ck/tensor_operation/gpu/device/impl/device_grouped_conv_bwd_weight_two_stage_xdl_cshuffle.hpp
var/spack/repos/builtin/packages/composable-kernel/0001-mark-kernels-maybe-unused.patch:+++ b/include/ck/tensor_operation/gpu/device/impl/device_grouped_conv_bwd_weight_two_stage_xdl_cshuffle.hpp
var/spack/repos/builtin/packages/composable-kernel/0001-mark-kernels-maybe-unused.patch:diff --git a/include/ck/tensor_operation/gpu/device/impl/device_grouped_conv_fwd_multiple_abd_xdl_cshuffle_v3.hpp b/include/ck/tensor_operation/gpu/device/impl/device_grouped_conv_fwd_multiple_abd_xdl_cshuffle_v3.hpp
var/spack/repos/builtin/packages/composable-kernel/0001-mark-kernels-maybe-unused.patch:--- a/include/ck/tensor_operation/gpu/device/impl/device_grouped_conv_fwd_multiple_abd_xdl_cshuffle_v3.hpp
var/spack/repos/builtin/packages/composable-kernel/0001-mark-kernels-maybe-unused.patch:+++ b/include/ck/tensor_operation/gpu/device/impl/device_grouped_conv_fwd_multiple_abd_xdl_cshuffle_v3.hpp
var/spack/repos/builtin/packages/opencv/package.py:class Opencv(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/opencv/package.py:    # Patch to fix conflict between CUDA and OpenCV (reproduced with 3.3.0
var/spack/repos/builtin/packages/opencv/package.py:    patch("dnn_cuda.patch", when="@3.3.0:3.4.1+cuda+dnn")
var/spack/repos/builtin/packages/opencv/package.py:        "cudaarithm",
var/spack/repos/builtin/packages/opencv/package.py:        "cudabgsegm",
var/spack/repos/builtin/packages/opencv/package.py:        "cudacodec",
var/spack/repos/builtin/packages/opencv/package.py:        "cudafeatures2d",
var/spack/repos/builtin/packages/opencv/package.py:        "cudafilters",
var/spack/repos/builtin/packages/opencv/package.py:        "cudaimgproc",
var/spack/repos/builtin/packages/opencv/package.py:        "cudalegacy",
var/spack/repos/builtin/packages/opencv/package.py:        "cudaobjdetect",
var/spack/repos/builtin/packages/opencv/package.py:        "cudaoptflow",
var/spack/repos/builtin/packages/opencv/package.py:        "cudastereo",
var/spack/repos/builtin/packages/opencv/package.py:        "cudawarping",
var/spack/repos/builtin/packages/opencv/package.py:        conflicts("~cuda")
var/spack/repos/builtin/packages/opencv/package.py:    with when("+cuda"):
var/spack/repos/builtin/packages/opencv/package.py:    with when("+cudaarithm"):
var/spack/repos/builtin/packages/opencv/package.py:        conflicts("~cuda")
var/spack/repos/builtin/packages/opencv/package.py:    with when("+cudabgsegm"):
var/spack/repos/builtin/packages/opencv/package.py:        conflicts("~cuda")
var/spack/repos/builtin/packages/opencv/package.py:    with when("+cudacodec"):
var/spack/repos/builtin/packages/opencv/package.py:    with when("+cudafeatures2d"):
var/spack/repos/builtin/packages/opencv/package.py:        conflicts("~cuda")
var/spack/repos/builtin/packages/opencv/package.py:        conflicts("~cudafilters")
var/spack/repos/builtin/packages/opencv/package.py:        conflicts("~cudawarping")
var/spack/repos/builtin/packages/opencv/package.py:    with when("+cudafilters"):
var/spack/repos/builtin/packages/opencv/package.py:        conflicts("~cuda")
var/spack/repos/builtin/packages/opencv/package.py:        conflicts("~cudaarithm")
var/spack/repos/builtin/packages/opencv/package.py:    with when("+cudaimgproc"):
var/spack/repos/builtin/packages/opencv/package.py:        conflicts("~cuda")
var/spack/repos/builtin/packages/opencv/package.py:    with when("+cudalegacy"):
var/spack/repos/builtin/packages/opencv/package.py:        conflicts("~cuda")
var/spack/repos/builtin/packages/opencv/package.py:    with when("+cudaobjdetect"):
var/spack/repos/builtin/packages/opencv/package.py:        conflicts("~cuda")
var/spack/repos/builtin/packages/opencv/package.py:        conflicts("~cudaarithm")
var/spack/repos/builtin/packages/opencv/package.py:        conflicts("~cudawarping")
var/spack/repos/builtin/packages/opencv/package.py:    with when("+cudaoptflow"):
var/spack/repos/builtin/packages/opencv/package.py:        conflicts("~cuda")
var/spack/repos/builtin/packages/opencv/package.py:        conflicts("~cudaarithm")
var/spack/repos/builtin/packages/opencv/package.py:        conflicts("~cudaimgproc")
var/spack/repos/builtin/packages/opencv/package.py:        conflicts("~cudawarping")
var/spack/repos/builtin/packages/opencv/package.py:    with when("+cudastereo"):
var/spack/repos/builtin/packages/opencv/package.py:        conflicts("~cuda")
var/spack/repos/builtin/packages/opencv/package.py:    with when("+cudawarping"):
var/spack/repos/builtin/packages/opencv/package.py:        conflicts("~cuda")
var/spack/repos/builtin/packages/opencv/package.py:        conflicts("~cuda")
var/spack/repos/builtin/packages/opencv/package.py:        with when("+cuda"):
var/spack/repos/builtin/packages/opencv/package.py:        with when("+cuda"):
var/spack/repos/builtin/packages/opencv/package.py:        with when("+cuda"):
var/spack/repos/builtin/packages/opencv/package.py:        with when("+cuda"):
var/spack/repos/builtin/packages/opencv/package.py:        "cuda",
var/spack/repos/builtin/packages/opencv/package.py:        "opencl",
var/spack/repos/builtin/packages/opencv/package.py:        "opencl_d3d11_nv",
var/spack/repos/builtin/packages/opencv/package.py:        "opencl_svm",
var/spack/repos/builtin/packages/opencv/package.py:        "openclamdblas",
var/spack/repos/builtin/packages/opencv/package.py:        "openclamdfft",
var/spack/repos/builtin/packages/opencv/package.py:    depends_on("cuda@6.5:", when="+cuda")
var/spack/repos/builtin/packages/opencv/package.py:    depends_on("cuda@:10.2", when="@4.0:4.2+cuda")
var/spack/repos/builtin/packages/opencv/package.py:    depends_on("cuda@:9.0", when="@3.3.1:3.4+cuda")
var/spack/repos/builtin/packages/opencv/package.py:    depends_on("cuda@:8", when="@:3.3.0+cuda")
var/spack/repos/builtin/packages/opencv/package.py:    depends_on("opencl", when="+opencl")
var/spack/repos/builtin/packages/opencv/package.py:    conflicts("+cublas", when="~cuda")
var/spack/repos/builtin/packages/opencv/package.py:    conflicts("+cudnn", when="~cuda")
var/spack/repos/builtin/packages/opencv/package.py:    conflicts("+cufft", when="~cuda")
var/spack/repos/builtin/packages/opencv/package.py:    conflicts("+opencl_d3d11_nv", when="platform=darwin", msg="Windows only")
var/spack/repos/builtin/packages/opencv/package.py:    conflicts("+opencl_d3d11_nv", when="platform=linux", msg="Windows only")
var/spack/repos/builtin/packages/opencv/package.py:        if "+cuda" in spec:
var/spack/repos/builtin/packages/opencv/package.py:            if spec.variants["cuda_arch"].value[0] != "none":
var/spack/repos/builtin/packages/opencv/package.py:                cuda_arch = spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/opencv/package.py:                args.append(self.define("CUDA_ARCH_BIN", " ".join(cuda_arch)))
var/spack/repos/builtin/packages/opencv/dnn_cuda.patch: #include "opencl_kernels_dnn.hpp"
var/spack/repos/builtin/packages/opencv/dnn_cuda.patch: #include "opencl_kernels_dnn.hpp"
var/spack/repos/builtin/packages/opencv/dnn_cuda.patch: #include "opencl_kernels_dnn.hpp"
var/spack/repos/builtin/packages/opencv/dnn_cuda.patch: #ifdef HAVE_OPENCL
var/spack/repos/builtin/packages/dav-sdk/package.py:class DavSdk(BundlePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/dav-sdk/package.py:    cuda_arch_variants = ["cuda_arch={0}".format(x) for x in CudaPackage.cuda_arch_values]
var/spack/repos/builtin/packages/dav-sdk/package.py:    amdgpu_target_variants = ["amdgpu_target={0}".format(x) for x in ROCmPackage.amdgpu_targets]
var/spack/repos/builtin/packages/dav-sdk/package.py:        propagate=["cuda", "hdf5", "zfp", "fortran"] + cuda_arch_variants,
var/spack/repos/builtin/packages/dav-sdk/package.py:    # hdf5-vfd-gds needs cuda@11.7.1 or later, only enable when 11.7.1+ available.
var/spack/repos/builtin/packages/dav-sdk/package.py:    depends_on("hdf5-vfd-gds@1.0.2:", when="+cuda+hdf5 ^cuda@11.7.1: ^hdf5@1.14:")
var/spack/repos/builtin/packages/dav-sdk/package.py:    for cuda_arch in cuda_arch_variants:
var/spack/repos/builtin/packages/dav-sdk/package.py:            "hdf5-vfd-gds@1.0.2: {0}".format(cuda_arch),
var/spack/repos/builtin/packages/dav-sdk/package.py:            when="+cuda+hdf5 {0} ^cuda@11.7.1: ^hdf5@1.14:".format(cuda_arch),
var/spack/repos/builtin/packages/dav-sdk/package.py:    conflicts("~cuda", when="^hdf5-vfd-gds@1.0.2:")
var/spack/repos/builtin/packages/dav-sdk/package.py:        propagate=["adios2", "cuda"] + cuda_arch_variants,
var/spack/repos/builtin/packages/dav-sdk/package.py:    depends_on("ascent+openmp", when="~rocm+ascent")
var/spack/repos/builtin/packages/dav-sdk/package.py:    depends_on("ascent~openmp", when="+rocm+ascent")
var/spack/repos/builtin/packages/dav-sdk/package.py:    # ParaView needs @5.11: in order to use CUDA/ROCM, therefore it is the minimum
var/spack/repos/builtin/packages/dav-sdk/package.py:    # required version since GPU capability is desired for ECP
var/spack/repos/builtin/packages/dav-sdk/package.py:        propagate=["adios2", "cuda", "hdf5", "rocm"] + amdgpu_target_variants + cuda_arch_variants,
var/spack/repos/builtin/packages/dav-sdk/package.py:        propagate=["cuda", "rocm"] + cuda_arch_variants + amdgpu_target_variants,
var/spack/repos/builtin/packages/dav-sdk/package.py:    depends_on("vtk-m+openmp", when="~rocm+vtkm")
var/spack/repos/builtin/packages/dav-sdk/package.py:    depends_on("vtk-m~openmp", when="+rocm+vtkm")
var/spack/repos/builtin/packages/dav-sdk/package.py:    dav_sdk_depends_on("zfp", when="+zfp", propagate=["cuda"] + cuda_arch_variants)
var/spack/repos/builtin/packages/hiop/package.py:class Hiop(CMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/hiop/package.py:        when="+cuda @0.7.1:",
var/spack/repos/builtin/packages/hiop/package.py:    for arch in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/hiop/package.py:        cuda_dep = "+cuda cuda_arch={0}".format(arch)
var/spack/repos/builtin/packages/hiop/package.py:        depends_on("magma {0}".format(cuda_dep), when=cuda_dep)
var/spack/repos/builtin/packages/hiop/package.py:        depends_on("raja {0}".format(cuda_dep), when="+raja {0}".format(cuda_dep))
var/spack/repos/builtin/packages/hiop/package.py:        depends_on("ginkgo {0}".format(cuda_dep), when="+ginkgo {0}".format(cuda_dep))
var/spack/repos/builtin/packages/hiop/package.py:        depends_on("umpire {0}".format(cuda_dep), when="+raja {0}".format(cuda_dep))
var/spack/repos/builtin/packages/hiop/package.py:        # Camp GPU arch doesn't get propogated correctly
var/spack/repos/builtin/packages/hiop/package.py:        depends_on("camp {0}".format(cuda_dep), when="+raja {0}".format(cuda_dep))
var/spack/repos/builtin/packages/hiop/package.py:    for arch in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/hiop/package.py:        rocm_dep = "+rocm amdgpu_target={0}".format(arch)
var/spack/repos/builtin/packages/hiop/package.py:        depends_on("magma {0}".format(rocm_dep), when=rocm_dep)
var/spack/repos/builtin/packages/hiop/package.py:        depends_on("raja {0}".format(rocm_dep), when="+raja {0}".format(rocm_dep))
var/spack/repos/builtin/packages/hiop/package.py:        depends_on("ginkgo {0}".format(rocm_dep), when="+ginkgo {0}".format(rocm_dep))
var/spack/repos/builtin/packages/hiop/package.py:        depends_on("umpire {0}".format(rocm_dep), when="+raja {0}".format(rocm_dep))
var/spack/repos/builtin/packages/hiop/package.py:        # Camp GPU arch doesn't get propogated correctly
var/spack/repos/builtin/packages/hiop/package.py:        depends_on("camp {0}".format(rocm_dep), when="+raja {0}".format(rocm_dep))
var/spack/repos/builtin/packages/hiop/package.py:    # Depends on Magma when +rocm or +cuda
var/spack/repos/builtin/packages/hiop/package.py:        depends_on("magma@{0}:".format(magma_v), when="@{0}:+cuda".format(hiop_v))
var/spack/repos/builtin/packages/hiop/package.py:        depends_on("magma@{0}:".format(magma_v), when="@{0}:+rocm".format(hiop_v))
var/spack/repos/builtin/packages/hiop/package.py:    # 1.0.2 fixes bug with cuda 12 compatibility
var/spack/repos/builtin/packages/hiop/package.py:    # hiop@0.6.0 requires cusolver API in cuda@11
var/spack/repos/builtin/packages/hiop/package.py:    depends_on("cuda@11:11.9", when="@0.6.0:1.0.1+cuda")
var/spack/repos/builtin/packages/hiop/package.py:    # CUDA 11.3.1, at least according to the CUDA online documentation.
var/spack/repos/builtin/packages/hiop/package.py:    depends_on("cuda@11.3.1:", when="@0.7:+cuda")
var/spack/repos/builtin/packages/hiop/package.py:    # Before hiop@0.6.0 only cuda requirement was magma
var/spack/repos/builtin/packages/hiop/package.py:    depends_on("cuda", when="@:0.5.4+cuda")
var/spack/repos/builtin/packages/hiop/package.py:    depends_on("raja+openmp", when="+raja~cuda~rocm")
var/spack/repos/builtin/packages/hiop/package.py:    depends_on("umpire+cuda~shared", when="+raja+cuda ^raja@:0.14")
var/spack/repos/builtin/packages/hiop/package.py:        when="+cuda+raja ^raja@:0.14",
var/spack/repos/builtin/packages/hiop/package.py:        msg="umpire+cuda exports device code and requires static libs",
var/spack/repos/builtin/packages/hiop/package.py:    # We rely on RAJA / Umpire utilities when supporting CUDA backend
var/spack/repos/builtin/packages/hiop/package.py:    conflicts("~raja", when="+cuda", msg="RAJA is required for CUDA support")
var/spack/repos/builtin/packages/hiop/package.py:    conflicts("~raja", when="+rocm", msg="RAJA is required for ROCm support")
var/spack/repos/builtin/packages/hiop/package.py:    depends_on("hip", when="+rocm")
var/spack/repos/builtin/packages/hiop/package.py:    depends_on("hiprand", when="+rocm")
var/spack/repos/builtin/packages/hiop/package.py:    depends_on("hipblas", when="+rocm")
var/spack/repos/builtin/packages/hiop/package.py:    depends_on("hipsparse", when="+rocm")
var/spack/repos/builtin/packages/hiop/package.py:        use_gpu = spec.satisfies("+cuda") or spec.satisfies("+rocm")
var/spack/repos/builtin/packages/hiop/package.py:        if use_gpu:
var/spack/repos/builtin/packages/hiop/package.py:                    self.define("HIOP_USE_GPU", True),
var/spack/repos/builtin/packages/hiop/package.py:                self.define_from_variant("HIOP_USE_CUDA", "cuda"),
var/spack/repos/builtin/packages/hiop/package.py:                self.define_from_variant("HIOP_USE_HIP", "rocm"),
var/spack/repos/builtin/packages/hiop/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/hiop/package.py:            cuda_arch_list = spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/hiop/package.py:            if cuda_arch_list[0] != "none":
var/spack/repos/builtin/packages/hiop/package.py:                args.append(self.define("CMAKE_CUDA_ARCHITECTURES", cuda_arch_list))
var/spack/repos/builtin/packages/hiop/package.py:        # NOTE: if +rocm, some HIP CMake variables may not be set correctly.
var/spack/repos/builtin/packages/hiop/package.py:        #         '/opt/rocm-X.Y.Z/llvm/lib/clang/14.0.0/include/'))
var/spack/repos/builtin/packages/hiop/package.py:        if spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/hiop/package.py:            rocm_arch_list = spec.variants["amdgpu_target"].value
var/spack/repos/builtin/packages/hiop/package.py:            if rocm_arch_list[0] != "none":
var/spack/repos/builtin/packages/hiop/package.py:                args.append(self.define("GPU_TARGETS", rocm_arch_list))
var/spack/repos/builtin/packages/hiop/package.py:                args.append(self.define("AMDGPU_TARGETS", rocm_arch_list))
var/spack/repos/builtin/packages/py-cupy/package.py:class PyCupy(PythonPackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/py-cupy/package.py:    NVIDIA CUDA. CuPy provides GPU accelerated computing with
var/spack/repos/builtin/packages/py-cupy/package.py:    Python. CuPy uses CUDA-related libraries including cuBLAS,
var/spack/repos/builtin/packages/py-cupy/package.py:    cuDNN, cuRand, cuSolver, cuSPARSE, cuFFT and NCCL to make
var/spack/repos/builtin/packages/py-cupy/package.py:    full use of the GPU architecture."""
var/spack/repos/builtin/packages/py-cupy/package.py:    depends_on("cuda@:11.9", when="@:11 +cuda")
var/spack/repos/builtin/packages/py-cupy/package.py:    depends_on("cuda@:12.1", when="@12:12.1.0 +cuda")
var/spack/repos/builtin/packages/py-cupy/package.py:    depends_on("cuda@:12.4", when="@13: +cuda")
var/spack/repos/builtin/packages/py-cupy/package.py:    for a in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/py-cupy/package.py:        depends_on("nccl +cuda cuda_arch={0}".format(a), when="+cuda cuda_arch={0}".format(a))
var/spack/repos/builtin/packages/py-cupy/package.py:    depends_on("cudnn@8.8", when="@12.0.0: +cuda")
var/spack/repos/builtin/packages/py-cupy/package.py:    depends_on("cudnn@8.5", when="@11.2.0:11.6.0 +cuda")
var/spack/repos/builtin/packages/py-cupy/package.py:    depends_on("cutensor", when="@:12.1.0 +cuda")
var/spack/repos/builtin/packages/py-cupy/package.py:    depends_on("cutensor@2.0.1.2", when="@13.1: +cuda")
var/spack/repos/builtin/packages/py-cupy/package.py:    for _arch in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/py-cupy/package.py:        arch_str = "amdgpu_target={0}".format(_arch)
var/spack/repos/builtin/packages/py-cupy/package.py:        rocm_str = "+rocm {0}".format(arch_str)
var/spack/repos/builtin/packages/py-cupy/package.py:        depends_on("rocprim {0}".format(arch_str), when=rocm_str, type=("link"))
var/spack/repos/builtin/packages/py-cupy/package.py:        depends_on("rocsolver {0}".format(arch_str), when=rocm_str, type=("link"))
var/spack/repos/builtin/packages/py-cupy/package.py:        depends_on("rocthrust {0}".format(arch_str), when=rocm_str, type=("link"))
var/spack/repos/builtin/packages/py-cupy/package.py:        depends_on("rocrand {0}".format(arch_str), when=rocm_str, type=("link"))
var/spack/repos/builtin/packages/py-cupy/package.py:        depends_on("hipcub {0}".format(rocm_str), when=rocm_str, type=("link"))
var/spack/repos/builtin/packages/py-cupy/package.py:        depends_on("hipblas {0}".format(rocm_str), when=rocm_str, type=("link"))
var/spack/repos/builtin/packages/py-cupy/package.py:        depends_on("hiprand {0}".format(rocm_str), when=rocm_str, type=("link"))
var/spack/repos/builtin/packages/py-cupy/package.py:        depends_on("hipsparse {0}".format(rocm_str), when=rocm_str, type=("link"))
var/spack/repos/builtin/packages/py-cupy/package.py:        depends_on("hipfft {0}".format(rocm_str), when=rocm_str, type=("link"))
var/spack/repos/builtin/packages/py-cupy/package.py:    depends_on("rccl", when="+rocm", type=("link"))
var/spack/repos/builtin/packages/py-cupy/package.py:    depends_on("roctracer-dev", when="+rocm", type=("link"))
var/spack/repos/builtin/packages/py-cupy/package.py:    depends_on("rocprofiler-dev", when="+rocm", type=("link"))
var/spack/repos/builtin/packages/py-cupy/package.py:    conflicts("~cuda ~rocm")
var/spack/repos/builtin/packages/py-cupy/package.py:    conflicts("+cuda +rocm")
var/spack/repos/builtin/packages/py-cupy/package.py:    conflicts("+cuda cuda_arch=none")
var/spack/repos/builtin/packages/py-cupy/package.py:        if self.spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/py-cupy/package.py:            cuda_arch = self.spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/py-cupy/package.py:            arch_str = ";".join("arch=compute_{0},code=sm_{0}".format(i) for i in cuda_arch)
var/spack/repos/builtin/packages/py-cupy/package.py:        elif self.spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/py-cupy/package.py:            env.set("ROCM_HOME", self.spec["hipcub"].prefix)
var/spack/repos/builtin/packages/libpressio-nvcomp/package.py:class LibpressioNvcomp(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/libpressio-nvcomp/package.py:    depends_on("libpressio+cuda")
var/spack/repos/builtin/packages/libpressio-nvcomp/package.py:    conflicts("~cuda")
var/spack/repos/builtin/packages/libpressio-nvcomp/package.py:    conflicts("cuda_arch=none", when="+cuda")
var/spack/repos/builtin/packages/libpressio-nvcomp/package.py:        cuda_arch = self.spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/libpressio-nvcomp/package.py:        args = [("-DCMAKE_CUDA_ARCHITECTURES=%s" % cuda_arch)]
var/spack/repos/builtin/packages/spiral-software/package.py:        " (SIMT) to generate code for GPUs.",
var/spack/repos/builtin/packages/py-meldmd/package.py:class PyMeldmd(CMakePackage, PythonExtension, CudaPackage):
var/spack/repos/builtin/packages/py-meldmd/package.py:    depends_on("openmm+cuda")
var/spack/repos/builtin/packages/py-meldmd/package.py:    # C++ / CUDA
var/spack/repos/builtin/packages/py-meldmd/package.py:            "MAXFLOAT", "FLT_MAX", "plugin/platforms/cuda/src/kernels/computeMeld.cu", string=True
var/spack/repos/builtin/packages/aml/package.py:    variant("opencl", default=False, description="Support for memory operations on top of OpenCL.")
var/spack/repos/builtin/packages/aml/package.py:    variant("cuda", default=False, description="Support for memory operations on top of CUDA.")
var/spack/repos/builtin/packages/aml/package.py:        values=("none", conditional("amd", when="+hip"), conditional("nvidia", when="+cuda")),
var/spack/repos/builtin/packages/aml/package.py:    # - cuda dependency. We use the environment variable CUDA_HOME in the configure.
var/spack/repos/builtin/packages/aml/package.py:    depends_on("cuda", when="+cuda")
var/spack/repos/builtin/packages/aml/package.py:    # - ocl-icd >= 2.1 becomes a dependency when +opencl variant is used.
var/spack/repos/builtin/packages/aml/package.py:    depends_on("ocl-icd@2.1:", when="+opencl")
var/spack/repos/builtin/packages/aml/package.py:        for b in ["opencl", "hwloc", "ze", "hip", "cuda"]:
var/spack/repos/builtin/packages/aml/package.py:        if self.spec.variants["hip-platform"].value == "nvidia":
var/spack/repos/builtin/packages/aml/package.py:            config_args += ["--with-hip-platform=nvidia"]
var/spack/repos/builtin/packages/py-scs/package.py:class PyScs(PythonPackage, CudaPackage):
var/spack/repos/builtin/packages/py-scs/package.py:            "+cuda" in spec
var/spack/repos/builtin/packages/py-scs/package.py:        if "+cuda" in spec:
var/spack/repos/builtin/packages/py-scs/package.py:            args.append("--gpu")
var/spack/repos/builtin/packages/extrae/package.py:    OpenMP, CUDA, OpenCL, pthread, OmpSs"""
var/spack/repos/builtin/packages/extrae/package.py:    variant("cuda", default=False, description="Enable support for tracing CUDA")
var/spack/repos/builtin/packages/extrae/package.py:    depends_on("cuda", when="+cuda")
var/spack/repos/builtin/packages/extrae/package.py:    depends_on("cuda", when="+cupti")
var/spack/repos/builtin/packages/extrae/package.py:    conflicts("+cupti", when="~cuda", msg="CUPTI requires CUDA")
var/spack/repos/builtin/packages/extrae/package.py:            ["--with-cuda=%s" % spec["cuda"].prefix]
var/spack/repos/builtin/packages/extrae/package.py:            if spec.satisfies("+cuda")
var/spack/repos/builtin/packages/extrae/package.py:            else ["--without-cuda"]
var/spack/repos/builtin/packages/extrae/package.py:            cupti_h = find_headers("cupti", spec["cuda"].prefix, recursive=True)
var/spack/repos/builtin/packages/qrmumps/package.py:    variant("cuda", default=False, when="+starpu", description="Enable StarPU+CUDA")
var/spack/repos/builtin/packages/qrmumps/package.py:    depends_on("cuda", when="+starpu+cuda")
var/spack/repos/builtin/packages/qrmumps/package.py:    depends_on("starpu+cuda", when="+starpu+cuda")
var/spack/repos/builtin/packages/qrmumps/package.py:            self.define_from_variant("QRM_WITH_CUDA", "cuda"),
var/spack/repos/builtin/packages/rocm-opencl/package.py:class RocmOpencl(CMakePackage):
var/spack/repos/builtin/packages/rocm-opencl/package.py:    """OpenCL: Open Computing Language on ROCclr"""
var/spack/repos/builtin/packages/rocm-opencl/package.py:    homepage = "https://github.com/ROCm/ROCm-OpenCL-Runtime"
var/spack/repos/builtin/packages/rocm-opencl/package.py:    git = "https://github.com/ROCm/ROCm-OpenCL-Runtime.git"
var/spack/repos/builtin/packages/rocm-opencl/package.py:    tags = ["rocm"]
var/spack/repos/builtin/packages/rocm-opencl/package.py:    libraries = ["libOpenCL"]
var/spack/repos/builtin/packages/rocm-opencl/package.py:                "https://github.com/RadeonOpenCompute/ROCm-OpenCL-Runtime/archive/roc-3.5.0.tar.gz"
var/spack/repos/builtin/packages/rocm-opencl/package.py:                "https://github.com/RadeonOpenCompute/ROCm-OpenCL-Runtime/archive/rocm-{0}.tar.gz"
var/spack/repos/builtin/packages/rocm-opencl/package.py:            url = "https://github.com/ROCm/clr/archive/refs/tags/rocm-{0}.tar.gz"
var/spack/repos/builtin/packages/rocm-opencl/package.py:    depends_on("opencl-icd-loader@2024.05.08", when="@6.2")
var/spack/repos/builtin/packages/rocm-opencl/package.py:            url=f"https://github.com/ROCm/ROCclr/archive/rocm-{d_version}.tar.gz",
var/spack/repos/builtin/packages/rocm-opencl/package.py:        "https://github.com/ROCm/clr/commit/c4f773db0b4ccbbeed4e3d6c0f6bff299c2aa3f0.patch?full_index=1",
var/spack/repos/builtin/packages/rocm-opencl/package.py:        "https://github.com/ROCm/clr/commit/7868876db742fb4d44483892856a66d2993add03.patch?full_index=1",
var/spack/repos/builtin/packages/rocm-opencl/package.py:    # Patch to set package installation path for OpenCL.
var/spack/repos/builtin/packages/rocm-opencl/package.py:    patch("0001-fix-build-error-rocm-opencl-5.1.0.patch", when="@5.1")
var/spack/repos/builtin/packages/rocm-opencl/package.py:        depends_on(f"rocm-core@{ver}", when=f"@{ver}")
var/spack/repos/builtin/packages/rocm-opencl/package.py:            args.append(self.define("AMD_OPENCL_PATH", self.stage.source_path))
var/spack/repos/builtin/packages/rocm-opencl/package.py:            args.append(self.define("AMD_ICD_LIBRARY_DIR", self.spec["opencl-icd-loader"].prefix))
var/spack/repos/builtin/packages/rocm-opencl/package.py:            env.set("CC", f"{self.spec['llvm-amdgpu'].prefix}/bin/clang")
var/spack/repos/builtin/packages/rocm-opencl/package.py:            env.set("CXX", f"{self.spec['llvm-amdgpu'].prefix}/bin/clang++")
var/spack/repos/builtin/packages/rocm-opencl/package.py:        test_dir = "tests/ocltst" if sys.platform == "win32" else "share/opencl/ocltst"
var/spack/repos/builtin/packages/rocm-opencl/0001-fix-build-error-rocm-opencl-5.1.0.patch:@@ -62,6 +62,9 @@ if(DEFINED ENV{ROCM_LIBPATCH_VERSION})
var/spack/repos/builtin/packages/rocm-opencl/0001-fix-build-error-rocm-opencl-5.1.0.patch:+  set( CPACK_PACKAGING_INSTALL_PREFIX "${CMAKE_INSTALL_PREFIX}" CACHE PATH "Package Installation path for OpenCL")
var/spack/repos/builtin/packages/rocm-opencl/0001-fix-build-error-rocm-opencl-5.1.0.patch: #ROCM_PATH is needed to create symlink of libraries
var/spack/repos/builtin/packages/rocm-opencl/0001-fix-build-error-rocm-opencl-5.1.0.patch: if(NOT DEFINED ROCM_PATH)
var/spack/repos/builtin/packages/rocm-opencl/0001-fix-build-error-rocm-opencl-5.1.0.patch:   string(REPLACE "/opencl" "" ROCM_PATH ${CPACK_PACKAGING_INSTALL_PREFIX})
var/spack/repos/builtin/packages/hypre/ij_gptune.patch:    /* when using cuda-memcheck --leak-check full, uncomment this */
var/spack/repos/builtin/packages/hypre/hypre21800-compat.patch: #ifdef HYPRE_USING_CUDA
var/spack/repos/builtin/packages/hypre/package.py:class Hypre(AutotoolsPackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/hypre/package.py:    variant("gpu-aware-mpi", default=False, description="Use gpu-aware mpi")
var/spack/repos/builtin/packages/hypre/package.py:    depends_on("rocsparse", when="+rocm")
var/spack/repos/builtin/packages/hypre/package.py:    depends_on("rocthrust", when="+rocm")
var/spack/repos/builtin/packages/hypre/package.py:    depends_on("rocrand", when="+rocm")
var/spack/repos/builtin/packages/hypre/package.py:    depends_on("rocprim", when="+rocm")
var/spack/repos/builtin/packages/hypre/package.py:    depends_on("hipblas", when="+rocm +superlu-dist")
var/spack/repos/builtin/packages/hypre/package.py:    depends_on("umpire+rocm", when="+umpire+rocm")
var/spack/repos/builtin/packages/hypre/package.py:    depends_on("umpire+cuda", when="+umpire+cuda")
var/spack/repos/builtin/packages/hypre/package.py:    gpu_pkgs = ["magma", "umpire"]
var/spack/repos/builtin/packages/hypre/package.py:    for sm_ in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/hypre/package.py:        for pkg in gpu_pkgs:
var/spack/repos/builtin/packages/hypre/package.py:            depends_on(f"{pkg}+cuda cuda_arch={sm_}", when=f"+{pkg}+cuda cuda_arch={sm_}")
var/spack/repos/builtin/packages/hypre/package.py:    for gfx in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/hypre/package.py:        for pkg in gpu_pkgs:
var/spack/repos/builtin/packages/hypre/package.py:            depends_on(f"{pkg}+rocm amdgpu_target={gfx}", when=f"+{pkg}+rocm amdgpu_target={gfx}")
var/spack/repos/builtin/packages/hypre/package.py:    depends_on("cuda@:11", when="@:2.28.0+cuda")
var/spack/repos/builtin/packages/hypre/package.py:    conflicts("+cuda", when="+int64")
var/spack/repos/builtin/packages/hypre/package.py:    conflicts("+rocm", when="+int64")
var/spack/repos/builtin/packages/hypre/package.py:    conflicts("+rocm", when="@:2.20")
var/spack/repos/builtin/packages/hypre/package.py:    conflicts("+unified-memory", when="~cuda~rocm")
var/spack/repos/builtin/packages/hypre/package.py:    conflicts("+umpire", when="+shared+cuda")
var/spack/repos/builtin/packages/hypre/package.py:    conflicts("+cublas", when="~cuda", msg="cuBLAS requires CUDA to be enabled")
var/spack/repos/builtin/packages/hypre/package.py:    conflicts("+rocblas", when="~rocm", msg="rocBLAS requires ROCm to be enabled")
var/spack/repos/builtin/packages/hypre/package.py:            if spec.satisfies("~cuda~rocm"):
var/spack/repos/builtin/packages/hypre/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/hypre/package.py:            configure_args.extend(["--with-cuda", "--enable-curand", "--enable-cusparse"])
var/spack/repos/builtin/packages/hypre/package.py:            cuda_arch_vals = spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/hypre/package.py:            if cuda_arch_vals:
var/spack/repos/builtin/packages/hypre/package.py:                cuda_arch_sorted = list(sorted(cuda_arch_vals, reverse=True))
var/spack/repos/builtin/packages/hypre/package.py:                cuda_arch = cuda_arch_sorted[0]
var/spack/repos/builtin/packages/hypre/package.py:                configure_args.append(f"--with-gpu-arch={cuda_arch}")
var/spack/repos/builtin/packages/hypre/package.py:                configure_args.append(f"--with-cuda-home={spec['cuda'].prefix}")
var/spack/repos/builtin/packages/hypre/package.py:            configure_args.extend(["--without-cuda", "--disable-curand", "--disable-cusparse"])
var/spack/repos/builtin/packages/hypre/package.py:        if spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/hypre/package.py:            rocm_pkgs = ["rocsparse", "rocthrust", "rocprim", "rocrand"]
var/spack/repos/builtin/packages/hypre/package.py:                rocm_pkgs.append("hipblas")
var/spack/repos/builtin/packages/hypre/package.py:            rocm_inc = ""
var/spack/repos/builtin/packages/hypre/package.py:            for pkg in rocm_pkgs:
var/spack/repos/builtin/packages/hypre/package.py:                rocm_inc += spec[pkg].headers.include_flags + " "
var/spack/repos/builtin/packages/hypre/package.py:                    f"--with-extra-CUFLAGS={rocm_inc}",
var/spack/repos/builtin/packages/hypre/package.py:            rocm_arch_vals = spec.variants["amdgpu_target"].value
var/spack/repos/builtin/packages/hypre/package.py:            if rocm_arch_vals:
var/spack/repos/builtin/packages/hypre/package.py:                rocm_arch_sorted = list(sorted(rocm_arch_vals, reverse=True))
var/spack/repos/builtin/packages/hypre/package.py:                rocm_arch = rocm_arch_sorted[0]
var/spack/repos/builtin/packages/hypre/package.py:                configure_args.append(f"--with-gpu-arch={rocm_arch}")
var/spack/repos/builtin/packages/hypre/package.py:                    "Hypre's SYCL GPU Backend requires the oneAPI CXX (icpx) compiler."
var/spack/repos/builtin/packages/hypre/package.py:        if spec.satisfies("+gpu-aware-mpi"):
var/spack/repos/builtin/packages/hypre/package.py:            configure_args.append("--enable-gpu-aware-mpi")
var/spack/repos/builtin/packages/hypre/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/hypre/package.py:            env.set("CUDA_HOME", spec["cuda"].prefix)
var/spack/repos/builtin/packages/hypre/package.py:            env.set("CUDA_PATH", spec["cuda"].prefix)
var/spack/repos/builtin/packages/hypre/package.py:            # In CUDA builds hypre currently doesn't handle flags correctly
var/spack/repos/builtin/packages/hypre/package.py:        if spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/hypre/package.py:            # As of 2022/04/05, the following are set by 'llvm-amdgpu' and
var/spack/repos/builtin/packages/gpu-burn/package.py:class GpuBurn(MakefilePackage, CudaPackage):
var/spack/repos/builtin/packages/gpu-burn/package.py:    """Multi-GPU CUDA stress test."""
var/spack/repos/builtin/packages/gpu-burn/package.py:    homepage = "http://wili.cc/blog/gpu-burn.html"
var/spack/repos/builtin/packages/gpu-burn/package.py:    url = "http://wili.cc/blog/entries/gpu-burn/gpu_burn-1.0.tar.gz"
var/spack/repos/builtin/packages/gpu-burn/package.py:    git = "https://github.com/wilicc/gpu-burn"
var/spack/repos/builtin/packages/gpu-burn/package.py:    # This package uses CudaPackage to pick up the cuda_arch variant. A side
var/spack/repos/builtin/packages/gpu-burn/package.py:    # effect is that it also picks up the cuda variant, but cuda is required
var/spack/repos/builtin/packages/gpu-burn/package.py:    # for gpu-burn so is not really a variant.
var/spack/repos/builtin/packages/gpu-burn/package.py:    variant("cuda", default=True, description="Use CUDA; must be true")
var/spack/repos/builtin/packages/gpu-burn/package.py:    conflicts("~cuda", msg="gpu-burn requires cuda")
var/spack/repos/builtin/packages/gpu-burn/package.py:    conflicts("cuda_arch=none", msg="must select a CUDA architecture")
var/spack/repos/builtin/packages/gpu-burn/package.py:        # update cuda architecture if necessary
var/spack/repos/builtin/packages/gpu-burn/package.py:        if self.spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/gpu-burn/package.py:            cuda_arch = self.spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/gpu-burn/package.py:            archflag = " ".join(CudaPackage.cuda_flags(cuda_arch))
var/spack/repos/builtin/packages/gpu-burn/package.py:                fh.write("\t{0} -O3 -c gpu_burn-drv.cpp\n".format(spack_cxx))
var/spack/repos/builtin/packages/gpu-burn/package.py:                    "\t{0} -o gpu_burn gpu_burn-drv.o -O3 -lcuda "
var/spack/repos/builtin/packages/gpu-burn/package.py:                    "-lcublas -lcudart -o gpu_burn\n".format(spack_cxx)
var/spack/repos/builtin/packages/gpu-burn/package.py:                "gpu_burn-drv.cpp",
var/spack/repos/builtin/packages/gpu-burn/package.py:        install("gpu_burn", prefix.bin)
var/spack/repos/builtin/packages/icon/package.py:    nvidia_targets = {"nvidia-{0}".format(cc): cc for cc in CudaPackage.cuda_arch_values}
var/spack/repos/builtin/packages/icon/package.py:    # TODO: add AMD GPU support
var/spack/repos/builtin/packages/icon/package.py:        "gpu",
var/spack/repos/builtin/packages/icon/package.py:        values=("none",) + tuple(nvidia_targets.keys()),
var/spack/repos/builtin/packages/icon/package.py:        description="Enable GPU support for the specified architecture",
var/spack/repos/builtin/packages/icon/package.py:    for __x in nvidia_targets.keys():
var/spack/repos/builtin/packages/icon/package.py:        requires("%nvhpc@21.3:", when="gpu={0}".format(__x))
var/spack/repos/builtin/packages/icon/package.py:    variant("mpi-gpu", default=True, description="Enable usage of the GPU-aware MPI features")
var/spack/repos/builtin/packages/icon/package.py:    requires("+mpi", when="+mpi-gpu")
var/spack/repos/builtin/packages/icon/package.py:    conflicts("gpu=none", when="+mpi-gpu")
var/spack/repos/builtin/packages/icon/package.py:    for __x in nvidia_targets.keys():
var/spack/repos/builtin/packages/icon/package.py:        depends_on("cuda", when="gpu={0}".format(__x))
var/spack/repos/builtin/packages/icon/package.py:            "mpi-gpu",
var/spack/repos/builtin/packages/icon/package.py:        gpu = self.spec.variants["gpu"].value
var/spack/repos/builtin/packages/icon/package.py:        if gpu in self.nvidia_targets:
var/spack/repos/builtin/packages/icon/package.py:            args.append("--enable-gpu=openacc+cuda")
var/spack/repos/builtin/packages/icon/package.py:            flags["CUDAFLAGS"] = [
var/spack/repos/builtin/packages/icon/package.py:                "-arch=sm_{0}".format(self.nvidia_targets[gpu]),
var/spack/repos/builtin/packages/icon/package.py:            libs += self.spec["cuda"].libs
var/spack/repos/builtin/packages/icon/package.py:            args.append("--disable-gpu")
var/spack/repos/builtin/packages/icon/package.py:            if gpu in self.nvidia_targets:
var/spack/repos/builtin/packages/icon/package.py:                    ["-acc=gpu", "-gpu=cc{0}".format(self.nvidia_targets[gpu])]
var/spack/repos/builtin/packages/blaspp/package.py:class Blaspp(CMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/blaspp/package.py:    depends_on("rocblas", when="+rocm")
var/spack/repos/builtin/packages/blaspp/package.py:        "+rocm", when="@:2020.10.02", msg="ROCm support requires BLAS++ 2021.04.00 or greater"
var/spack/repos/builtin/packages/blaspp/package.py:    backend_msg = "BLAS++ supports only one GPU backend at a time"
var/spack/repos/builtin/packages/blaspp/package.py:    conflicts("+rocm", when="+cuda", msg=backend_msg)
var/spack/repos/builtin/packages/blaspp/package.py:    conflicts("+rocm", when="+sycl", msg=backend_msg)
var/spack/repos/builtin/packages/blaspp/package.py:    conflicts("+cuda", when="+sycl", msg=backend_msg)
var/spack/repos/builtin/packages/blaspp/package.py:    patch("0001-fix-blaspp-build-error-with-rocm-6.0.0.patch", when="@2023.06.00: ^hip@6.0 +rocm")
var/spack/repos/builtin/packages/blaspp/package.py:        backend_config = "-Duse_cuda=%s" % ("+cuda" in spec)
var/spack/repos/builtin/packages/blaspp/package.py:            if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/blaspp/package.py:                backend = "cuda"
var/spack/repos/builtin/packages/blaspp/package.py:            if spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/blaspp/package.py:            backend_config = "-Dgpu_backend=%s" % backend
var/spack/repos/builtin/packages/blaspp/0001-fix-blaspp-build-error-with-rocm-6.0.0.patch:Subject: [PATCH] fix build error with rocm-6.0.0 by adding extra parameters
var/spack/repos/builtin/packages/strumpack/package.py:class Strumpack(CMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/strumpack/package.py:    depends_on("cuda", when="@4.0.0: +cuda")
var/spack/repos/builtin/packages/strumpack/package.py:    depends_on("hipblas", when="+rocm")
var/spack/repos/builtin/packages/strumpack/package.py:    depends_on("hipsparse", type="link", when="@7.0.1: +rocm")
var/spack/repos/builtin/packages/strumpack/package.py:    depends_on("rocsolver", when="+rocm")
var/spack/repos/builtin/packages/strumpack/package.py:    depends_on("rocthrust", when="+rocm")
var/spack/repos/builtin/packages/strumpack/package.py:    depends_on("magma+cuda", when="+magma+cuda")
var/spack/repos/builtin/packages/strumpack/package.py:    depends_on("magma+rocm", when="+magma+rocm")
var/spack/repos/builtin/packages/strumpack/package.py:    depends_on("slate+cuda", when="+cuda+slate")
var/spack/repos/builtin/packages/strumpack/package.py:    depends_on("slate+rocm", when="+rocm+slate")
var/spack/repos/builtin/packages/strumpack/package.py:    for val in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/strumpack/package.py:            "slate amdgpu_target={0}".format(val), when="+slate amdgpu_target={0}".format(val)
var/spack/repos/builtin/packages/strumpack/package.py:    conflicts("+cuda", when="@:3.9")
var/spack/repos/builtin/packages/strumpack/package.py:    conflicts("+rocm", when="@:5.0")
var/spack/repos/builtin/packages/strumpack/package.py:    conflicts("+rocm", when="+cuda")
var/spack/repos/builtin/packages/strumpack/package.py:    conflicts("+magma", when="~rocm~cuda")
var/spack/repos/builtin/packages/strumpack/package.py:    patch("shared-rocm.patch", when="@5.1.1")
var/spack/repos/builtin/packages/strumpack/package.py:    patch("strumpack-7.0.1-mpich-hipcc.patch", when="@7.0.1 +rocm ^mpich")
var/spack/repos/builtin/packages/strumpack/package.py:            self.define_from_variant("STRUMPACK_USE_CUDA", "cuda"),
var/spack/repos/builtin/packages/strumpack/package.py:            self.define_from_variant("STRUMPACK_USE_HIP", "rocm"),
var/spack/repos/builtin/packages/strumpack/package.py:        if "+cuda" in spec:
var/spack/repos/builtin/packages/strumpack/package.py:                    "-DCUDA_TOOLKIT_ROOT_DIR={0}".format(spec["cuda"].prefix),
var/spack/repos/builtin/packages/strumpack/package.py:                    "-DCMAKE_CUDA_HOST_COMPILER={0}".format(env["SPACK_CXX"]),
var/spack/repos/builtin/packages/strumpack/package.py:            cuda_archs = spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/strumpack/package.py:            if "none" not in cuda_archs:
var/spack/repos/builtin/packages/strumpack/package.py:                args.append("-DCUDA_NVCC_FLAGS={0}".format(" ".join(self.cuda_flags(cuda_archs))))
var/spack/repos/builtin/packages/strumpack/package.py:        if "+rocm" in spec:
var/spack/repos/builtin/packages/strumpack/package.py:            rocm_archs = spec.variants["amdgpu_target"].value
var/spack/repos/builtin/packages/strumpack/package.py:            if spec.satisfies("@7.0.1: +rocm"):
var/spack/repos/builtin/packages/strumpack/package.py:            if "none" not in rocm_archs:
var/spack/repos/builtin/packages/strumpack/package.py:                hipcc_flags.append("--amdgpu-target={0}".format(",".join(rocm_archs)))
var/spack/repos/builtin/packages/strumpack/shared-rocm.patch:Subject: [PATCH 1/2] patch for building shared lib with ROCm
var/spack/repos/builtin/packages/dray/package.py:def propagate_cuda_arch(package, spec=None):
var/spack/repos/builtin/packages/dray/package.py:    for cuda_arch in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/dray/package.py:            "{0} +cuda cuda_arch={1}".format(package, cuda_arch),
var/spack/repos/builtin/packages/dray/package.py:            when="{0} +cuda cuda_arch={1}".format(spec, cuda_arch),
var/spack/repos/builtin/packages/dray/package.py:class Dray(Package, CudaPackage):
var/spack/repos/builtin/packages/dray/package.py:    depends_on("cmake@3.14:", when="+cuda", type="build")
var/spack/repos/builtin/packages/dray/package.py:    depends_on("raja~cuda", when="~cuda")
var/spack/repos/builtin/packages/dray/package.py:    depends_on("raja+cuda", when="+cuda")
var/spack/repos/builtin/packages/dray/package.py:    propagate_cuda_arch("raja")
var/spack/repos/builtin/packages/dray/package.py:    # Only use umpire cuda if not shared.
var/spack/repos/builtin/packages/dray/package.py:    depends_on("umpire+cuda", when="+cuda")
var/spack/repos/builtin/packages/dray/package.py:    depends_on("umpire~cuda", when="~cuda")
var/spack/repos/builtin/packages/dray/package.py:    depends_on("umpire+cuda~shared", when="+cuda+shared")
var/spack/repos/builtin/packages/dray/package.py:    depends_on("umpire~cuda+shared", when="~cuda+shared")
var/spack/repos/builtin/packages/dray/package.py:    propagate_cuda_arch("umpire")
var/spack/repos/builtin/packages/dray/package.py:        cfg.write("# CUDA Support\n")
var/spack/repos/builtin/packages/dray/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/dray/package.py:            cfg.write(cmake_cache_entry("ENABLE_CUDA", "ON"))
var/spack/repos/builtin/packages/dray/package.py:            if "cuda_arch" in spec.variants:
var/spack/repos/builtin/packages/dray/package.py:                cuda_value = spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/dray/package.py:                cuda_arch = cuda_value[0]
var/spack/repos/builtin/packages/dray/package.py:                cfg.write(cmake_cache_entry("CUDA_ARCH", "sm_{0}".format(cuda_arch)))
var/spack/repos/builtin/packages/dray/package.py:            cfg.write(cmake_cache_entry("ENABLE_CUDA", "OFF"))
var/spack/repos/builtin/packages/amg2023/package.py:class Amg2023(CMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/amg2023/package.py:    depends_on("hypre+cuda", when="+cuda")
var/spack/repos/builtin/packages/amg2023/package.py:    requires("+cuda", when="^hypre+cuda")
var/spack/repos/builtin/packages/amg2023/package.py:    depends_on("hypre+rocm", when="+rocm")
var/spack/repos/builtin/packages/amg2023/package.py:    requires("+rocm", when="^hypre+rocm")
var/spack/repos/builtin/packages/amg2023/package.py:        if self.spec["hypre"].satisfies("+cuda"):
var/spack/repos/builtin/packages/amg2023/package.py:            cmake_options.append("-DAMG_WITH_CUDA=ON")
var/spack/repos/builtin/packages/amg2023/package.py:        if self.spec["hypre"].satisfies("+rocm"):
var/spack/repos/builtin/packages/nccl-fastsocket/package.py:class NcclFastsocket(Package):
var/spack/repos/builtin/packages/nccl-fastsocket/package.py:    """NCCL Fast Socket GCP Net plugin for NCCL"""
var/spack/repos/builtin/packages/nccl-fastsocket/package.py:    homepage = "https://github.com/google/nccl-fastsocket"
var/spack/repos/builtin/packages/nccl-fastsocket/package.py:    git = "https://github.com/google/nccl-fastsocket.git"
var/spack/repos/builtin/packages/nccl-fastsocket/package.py:    depends_on("nccl", type=["build", "run"])
var/spack/repos/builtin/packages/nccl-fastsocket/package.py:        env.set("NCCL_INSTALL_PATH", spec["nccl"].prefix)
var/spack/repos/builtin/packages/nccl-fastsocket/package.py:        env.set("NCCL_HDR_PATH", spec["nccl"].prefix.include)
var/spack/repos/builtin/packages/nccl-fastsocket/package.py:            "libnccl-net.so",
var/spack/repos/builtin/packages/nccl-fastsocket/package.py:        # The current plugin pickup method of NCCL is to scan for libraries with certain
var/spack/repos/builtin/packages/nccl-fastsocket/package.py:        # names in the standard library search paths. Consequently, to make nccl-fastsocket
var/spack/repos/builtin/packages/nccl-fastsocket/package.py:        # discoverable to NCCL it is necessary to add it to the LD_LIBRARY_PATH.
var/spack/repos/builtin/packages/nccl-fastsocket/package.py:        # NCCL_NET_PLUGIN can be used to change part of the library-name NCCL is looking
var/spack/repos/builtin/packages/nccl-fastsocket/package.py:        env.set("NCCL_NET_PLUGIN", "")
var/spack/repos/builtin/packages/nccl/package.py:class Nccl(MakefilePackage, CudaPackage):
var/spack/repos/builtin/packages/nccl/package.py:    """Optimized primitives for collective multi-GPU communication."""
var/spack/repos/builtin/packages/nccl/package.py:    homepage = "https://github.com/NVIDIA/nccl"
var/spack/repos/builtin/packages/nccl/package.py:    url = "https://github.com/NVIDIA/nccl/archive/v2.7.3-1.tar.gz"
var/spack/repos/builtin/packages/nccl/package.py:    libraries = ["libnccl.so"]
var/spack/repos/builtin/packages/nccl/package.py:    variant("cuda", default=True, description="Build with CUDA")
var/spack/repos/builtin/packages/nccl/package.py:    # https://github.com/NVIDIA/nccl/issues/244
var/spack/repos/builtin/packages/nccl/package.py:    conflicts("~cuda", msg="NCCL requires CUDA")
var/spack/repos/builtin/packages/nccl/package.py:        "cuda_arch=none",
var/spack/repos/builtin/packages/nccl/package.py:        msg="Must specify CUDA compute capabilities of your GPU, see "
var/spack/repos/builtin/packages/nccl/package.py:        "https://developer.nvidia.com/cuda-gpus",
var/spack/repos/builtin/packages/nccl/package.py:        cuda_arch = self.spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/nccl/package.py:        cuda_gencode = " ".join(self.cuda_flags(cuda_arch))
var/spack/repos/builtin/packages/nccl/package.py:            "CUDA_HOME={0}".format(self.spec["cuda"].prefix),
var/spack/repos/builtin/packages/nccl/package.py:            "NVCC_GENCODE={0}".format(cuda_gencode),
var/spack/repos/builtin/packages/nccl/so_reuseport.patch:@@ -327,7 +327,11 @@ static ncclResult_t createListenSocket(int *fd, union socketAddress *localAddr)
var/spack/repos/builtin/packages/ndzip/package.py:class Ndzip(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/ndzip/package.py:    variant("cuda", description="build with cuda support", default=False)
var/spack/repos/builtin/packages/ndzip/package.py:    variant("openmp", description="build with cuda support", default=False)
var/spack/repos/builtin/packages/ndzip/package.py:            self.define_from_variant("NDZIP_WITH_CUDA", "cuda"),
var/spack/repos/builtin/packages/ndzip/package.py:        if "+cuda" in self.spec and self.spec.variants["cuda_arch"].value != "none":
var/spack/repos/builtin/packages/ndzip/package.py:            arch_str = ";".join(self.spec.variants["cuda_arch"].value)
var/spack/repos/builtin/packages/ndzip/package.py:            args.append(self.define("CMAKE_CUDA_ARCHITECTURES", arch_str))
var/spack/repos/builtin/packages/llvm-amdgpu/patch-llvm-5.5.0.patch: llvm-amdgpu. The flag is required here as well as standalon build.
var/spack/repos/builtin/packages/llvm-amdgpu/patch-llvm-5.5.0.patch: rocm-device-libs/cmake/OCL.cmake | 13 ++-----------
var/spack/repos/builtin/packages/llvm-amdgpu/patch-llvm-5.5.0.patch:diff --git a/rocm-device-libs/cmake/OCL.cmake b/rocm-device-libs/cmake/OCL.cmake
var/spack/repos/builtin/packages/llvm-amdgpu/patch-llvm-5.5.0.patch:--- a/rocm-device-libs/cmake/OCL.cmake
var/spack/repos/builtin/packages/llvm-amdgpu/patch-llvm-5.5.0.patch:+++ b/rocm-device-libs/cmake/OCL.cmake
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns-5.7.patch: llvm/lib/Target/AMDGPU/SIISelLowering.cpp |   2 +
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns-5.7.patch: llvm/test/CodeGen/AMDGPU/mul.ll           | 400 ++++++++++++++++++----
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns-5.7.patch:diff --git a/llvm/lib/Target/AMDGPU/SIISelLowering.cpp b/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns-5.7.patch:--- a/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns-5.7.patch:+++ b/llvm/lib/Target/AMDGPU/SIISelLowering.cpp
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns-5.7.patch:diff --git a/llvm/test/CodeGen/AMDGPU/mul.ll b/llvm/test/CodeGen/AMDGPU/mul.ll
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns-5.7.patch:--- a/llvm/test/CodeGen/AMDGPU/mul.ll
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns-5.7.patch:+++ b/llvm/test/CodeGen/AMDGPU/mul.ll
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns-5.7.patch:+define amdgpu_kernel void @s_mul_i1(ptr addrspace(1) %out, [8 x i32], i1 %a, [8 x i32], i1 %b) nounwind {
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns-5.7.patch:+define amdgpu_kernel void @v_mul_i1(ptr addrspace(1) %out, ptr addrspace(1) %in) {
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns-5.7.patch:@@ -1213,7 +1461,7 @@ define amdgpu_kernel void @v_mul_i64(ptr addrspace(1) %out, ptr addrspace(1) %ap
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns-5.7.patch:@@ -1367,30 +1615,30 @@ define amdgpu_kernel void @mul32_in_branch(ptr addrspace(1) %out, ptr addrspace(
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns-5.7.patch:@@ -1402,18 +1650,18 @@ define amdgpu_kernel void @mul32_in_branch(ptr addrspace(1) %out, ptr addrspace(
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns-5.7.patch:@@ -1421,10 +1669,10 @@ define amdgpu_kernel void @mul32_in_branch(ptr addrspace(1) %out, ptr addrspace(
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns-5.7.patch:@@ -1437,18 +1685,18 @@ define amdgpu_kernel void @mul32_in_branch(ptr addrspace(1) %out, ptr addrspace(
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns-5.7.patch:@@ -1456,10 +1704,10 @@ define amdgpu_kernel void @mul32_in_branch(ptr addrspace(1) %out, ptr addrspace(
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns-5.7.patch:@@ -1473,17 +1721,17 @@ define amdgpu_kernel void @mul32_in_branch(ptr addrspace(1) %out, ptr addrspace(
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns-5.7.patch:@@ -1491,10 +1739,10 @@ define amdgpu_kernel void @mul32_in_branch(ptr addrspace(1) %out, ptr addrspace(
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns-5.7.patch:@@ -1508,17 +1756,17 @@ define amdgpu_kernel void @mul32_in_branch(ptr addrspace(1) %out, ptr addrspace(
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns-5.7.patch:@@ -1526,10 +1774,10 @@ define amdgpu_kernel void @mul32_in_branch(ptr addrspace(1) %out, ptr addrspace(
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns-5.7.patch:@@ -1601,7 +1849,7 @@ define amdgpu_kernel void @mul64_in_branch(ptr addrspace(1) %out, ptr addrspace(
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns-5.7.patch:@@ -1612,22 +1860,22 @@ define amdgpu_kernel void @mul64_in_branch(ptr addrspace(1) %out, ptr addrspace(
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns-5.7.patch:@@ -1635,7 +1883,7 @@ define amdgpu_kernel void @mul64_in_branch(ptr addrspace(1) %out, ptr addrspace(
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns-5.7.patch:@@ -1644,22 +1892,22 @@ define amdgpu_kernel void @mul64_in_branch(ptr addrspace(1) %out, ptr addrspace(
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns-5.7.patch:@@ -1667,7 +1915,7 @@ define amdgpu_kernel void @mul64_in_branch(ptr addrspace(1) %out, ptr addrspace(
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns-5.7.patch:@@ -1676,21 +1924,21 @@ define amdgpu_kernel void @mul64_in_branch(ptr addrspace(1) %out, ptr addrspace(
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns-5.7.patch:@@ -1702,7 +1950,7 @@ define amdgpu_kernel void @mul64_in_branch(ptr addrspace(1) %out, ptr addrspace(
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns-5.7.patch:@@ -1711,22 +1959,22 @@ define amdgpu_kernel void @mul64_in_branch(ptr addrspace(1) %out, ptr addrspace(
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns-5.7.patch:@@ -1738,7 +1986,7 @@ define amdgpu_kernel void @mul64_in_branch(ptr addrspace(1) %out, ptr addrspace(
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns-5.7.patch:@@ -1747,21 +1995,21 @@ define amdgpu_kernel void @mul64_in_branch(ptr addrspace(1) %out, ptr addrspace(
var/spack/repos/builtin/packages/llvm-amdgpu/package.py:class LlvmAmdgpu(CMakePackage, CompilerPackage):
var/spack/repos/builtin/packages/llvm-amdgpu/package.py:    homepage = "https://github.com/ROCm/llvm-project"
var/spack/repos/builtin/packages/llvm-amdgpu/package.py:    git = "https://github.com/ROCm/llvm-project.git"
var/spack/repos/builtin/packages/llvm-amdgpu/package.py:    url = "https://github.com/ROCm/llvm-project/archive/rocm-6.2.0.tar.gz"
var/spack/repos/builtin/packages/llvm-amdgpu/package.py:    tags = ["rocm"]
var/spack/repos/builtin/packages/llvm-amdgpu/package.py:        "rocm-device-libs",
var/spack/repos/builtin/packages/llvm-amdgpu/package.py:            "Build ROCm device libs as external LLVM project instead of a "
var/spack/repos/builtin/packages/llvm-amdgpu/package.py:    # https://github.com/ROCm/ROCm-Device-Libs/commit/f0356159dbdc93ea9e545f9b61a7842f9c881fdf
var/spack/repos/builtin/packages/llvm-amdgpu/package.py:    patch("patch-llvm-5.5.0.patch", when="@5.5:5.7 +rocm-device-libs")
var/spack/repos/builtin/packages/llvm-amdgpu/package.py:    # This fix is targeting 6.1 rocm release.
var/spack/repos/builtin/packages/llvm-amdgpu/package.py:    # fixes the libamdhip64.so not found in some ROCm math lib tests
var/spack/repos/builtin/packages/llvm-amdgpu/package.py:        "https://github.com/ROCm/llvm-project/commit/444d1d12bbc0269fed5451fb1a9110a049679ca5.patch?full_index=1",
var/spack/repos/builtin/packages/llvm-amdgpu/package.py:        "https://github.com/ROCm/llvm-project/commit/c651b2b0d9d1393fb5191ac3acfe96e5ecc94bbc.patch?full_index=1",
var/spack/repos/builtin/packages/llvm-amdgpu/package.py:            name="rocm-device-libs",
var/spack/repos/builtin/packages/llvm-amdgpu/package.py:            placement="rocm-device-libs",
var/spack/repos/builtin/packages/llvm-amdgpu/package.py:            url=f"https://github.com/ROCm/ROCm-Device-Libs/archive/rocm-{d_version}.tar.gz",
var/spack/repos/builtin/packages/llvm-amdgpu/package.py:            when=f"@{d_version} +rocm-device-libs",
var/spack/repos/builtin/packages/llvm-amdgpu/package.py:        name="rocm-device-libs",
var/spack/repos/builtin/packages/llvm-amdgpu/package.py:        placement="rocm-device-libs",
var/spack/repos/builtin/packages/llvm-amdgpu/package.py:        git="https://github.com/ROCm/ROCm-Device-Libs.git",
var/spack/repos/builtin/packages/llvm-amdgpu/package.py:        when="@master +rocm-device-libs",
var/spack/repos/builtin/packages/llvm-amdgpu/package.py:            url=f"https://github.com/ROCm/ROCR-Runtime/archive/rocm-{d_version}.tar.gz",
var/spack/repos/builtin/packages/llvm-amdgpu/package.py:        git="https://github.com/ROCm/ROCR-Runtime.git",
var/spack/repos/builtin/packages/llvm-amdgpu/package.py:            url=f"https://github.com/ROCm/ROCm-CompilerSupport/archive/rocm-{d_version}.tar.gz",
var/spack/repos/builtin/packages/llvm-amdgpu/package.py:        git="https://github.com/ROCm/ROCm-CompilerSupport.git",
var/spack/repos/builtin/packages/llvm-amdgpu/package.py:            self.define("LLVM_AMDGPU_ALLOW_NPI_TARGETS", "ON"),
var/spack/repos/builtin/packages/llvm-amdgpu/package.py:            args.append(self.define("LLVM_TARGETS_TO_BUILD", "AMDGPU;AArch64"))
var/spack/repos/builtin/packages/llvm-amdgpu/package.py:            args.append(self.define("LLVM_TARGETS_TO_BUILD", "AMDGPU;X86"))
var/spack/repos/builtin/packages/llvm-amdgpu/package.py:        # Enable rocm-device-libs as a external project
var/spack/repos/builtin/packages/llvm-amdgpu/package.py:        if self.spec.satisfies("+rocm-device-libs"):
var/spack/repos/builtin/packages/llvm-amdgpu/package.py:                dir = os.path.join(self.stage.source_path, "rocm-device-libs")
var/spack/repos/builtin/packages/llvm-amdgpu/package.py:            args.append("-DSANITIZER_AMDGPU:Bool=ON")
var/spack/repos/builtin/packages/llvm-amdgpu/package.py:        llvm_amdgpu_home = self.spec["llvm-amdgpu"].prefix
var/spack/repos/builtin/packages/llvm-amdgpu/package.py:        env.prepend_path("LD_LIBRARY_PATH", llvm_amdgpu_home + "/lib")
var/spack/repos/builtin/packages/llvm-amdgpu/package.py:        llvm_amdgpu_home = self.spec["llvm-amdgpu"].prefix
var/spack/repos/builtin/packages/llvm-amdgpu/package.py:        env.prepend_path("LD_LIBRARY_PATH", llvm_amdgpu_home + "/lib")
var/spack/repos/builtin/packages/llvm-amdgpu/package.py:        if self.spec.satisfies("@6.1: +rocm-device-libs"):
var/spack/repos/builtin/packages/llvm-amdgpu/package.py:        for root, _, files in os.walk(self.spec["llvm-amdgpu"].prefix):
var/spack/repos/builtin/packages/llvm-amdgpu/adjust-openmp-bitcode-directory-for-llvm-link.patch: clang/lib/Driver/ToolChains/AMDGPUOpenMP.cpp | 2 +-
var/spack/repos/builtin/packages/llvm-amdgpu/adjust-openmp-bitcode-directory-for-llvm-link.patch:diff --git a/clang/lib/Driver/ToolChains/AMDGPUOpenMP.cpp b/clang/lib/Driver/ToolChains/AMDGPUOpenMP.cpp
var/spack/repos/builtin/packages/llvm-amdgpu/adjust-openmp-bitcode-directory-for-llvm-link.patch:--- a/clang/lib/Driver/ToolChains/AMDGPUOpenMP.cpp
var/spack/repos/builtin/packages/llvm-amdgpu/adjust-openmp-bitcode-directory-for-llvm-link.patch:+++ b/clang/lib/Driver/ToolChains/AMDGPUOpenMP.cpp
var/spack/repos/builtin/packages/llvm-amdgpu/detection_test.yaml:      echo "InstalledDir: /opt/rocm-5.7.1/llvm/bin"
var/spack/repos/builtin/packages/llvm-amdgpu/detection_test.yaml:      echo "Configuration file: /opt/rocm-5.7.1/llvm/bin/clang.cfg"
var/spack/repos/builtin/packages/llvm-amdgpu/detection_test.yaml:      echo "InstalledDir: /opt/rocm-5.7.1/llvm/bin"
var/spack/repos/builtin/packages/llvm-amdgpu/detection_test.yaml:  - spec: llvm-amdgpu@5.7.1
var/spack/repos/builtin/packages/llvm-amdgpu/detection_test.yaml:      echo "InstalledDir: /opt/rocm-5.7.1/llvm/bin"
var/spack/repos/builtin/packages/llvm-amdgpu/detection_test.yaml:      echo "Configuration file: /opt/rocm-5.7.1/llvm/bin/clang.cfg"
var/spack/repos/builtin/packages/llvm-amdgpu/detection_test.yaml:  - spec: llvm-amdgpu@5.7.1
var/spack/repos/builtin/packages/llvm-amdgpu/detection_test.yaml:      echo "InstalledDir: /opt/rocm-5.7.1/llvm/bin"
var/spack/repos/builtin/packages/llvm-amdgpu/detection_test.yaml:      echo "Configuration file: /opt/rocm-5.7.1/llvm/bin/clang.cfg"
var/spack/repos/builtin/packages/llvm-amdgpu/detection_test.yaml:      echo "InstalledDir: /opt/rocm-5.7.1/llvm/bin"
var/spack/repos/builtin/packages/llvm-amdgpu/detection_test.yaml:      echo "InstalledDir: /opt/rocm-6.0.2/llvm/bin"
var/spack/repos/builtin/packages/llvm-amdgpu/detection_test.yaml:      echo "Configuration file: /opt/rocm-6.0.2/llvm/bin/clang.cfg"
var/spack/repos/builtin/packages/llvm-amdgpu/detection_test.yaml:      echo "InstalledDir: /opt/rocm-6.0.2/llvm/bin"
var/spack/repos/builtin/packages/llvm-amdgpu/detection_test.yaml:  - spec: llvm-amdgpu@6.0.2
var/spack/repos/builtin/packages/llvm-amdgpu/detection_test.yaml:  - spec: llvm-amdgpu@5.7.1
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns.patch: llvm/lib/Target/AMDGPU/SIInstructions.td |   10 +
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns.patch: llvm/test/CodeGen/AMDGPU/mul.ll          | 2676 ++++++++++++++++++++--
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns.patch:diff --git a/llvm/lib/Target/AMDGPU/SIInstructions.td b/llvm/lib/Target/AMDGPU/SIInstructions.td
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns.patch:--- a/llvm/lib/Target/AMDGPU/SIInstructions.td
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns.patch:+++ b/llvm/lib/Target/AMDGPU/SIInstructions.td
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns.patch:diff --git a/llvm/test/CodeGen/AMDGPU/mul.ll b/llvm/test/CodeGen/AMDGPU/mul.ll
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns.patch:--- a/llvm/test/CodeGen/AMDGPU/mul.ll
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns.patch:+++ b/llvm/test/CodeGen/AMDGPU/mul.ll
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns.patch:-; RUN:  llc -amdgpu-scalarize-global-loads=false  -march=amdgcn -mcpu=verde -verify-machineinstrs < %s | FileCheck -allow-deprecated-dag-overlap -check-prefixes=GCN,SI,FUNC %s
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns.patch:-; RUN:  llc -amdgpu-scalarize-global-loads=false  -march=amdgcn -mcpu=tonga -mattr=-flat-for-global -verify-machineinstrs < %s | FileCheck -allow-deprecated-dag-overlap -check-prefixes=GCN,VI,FUNC %s
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns.patch:-; RUN:  llc -amdgpu-scalarize-global-loads=false  -march=amdgcn -mcpu=gfx900 -mattr=-flat-for-global -verify-machineinstrs < %s | FileCheck -allow-deprecated-dag-overlap -check-prefixes=FUNC,GFX9PLUS %s
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns.patch:-; RUN:  llc -amdgpu-scalarize-global-loads=false  -march=amdgcn -mcpu=gfx1010 -mattr=-flat-for-global -verify-machineinstrs < %s | FileCheck -allow-deprecated-dag-overlap -check-prefixes=FUNC,GFX9PLUS %s
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns.patch:-; RUN:  llc -amdgpu-scalarize-global-loads=false  -march=amdgcn -mcpu=gfx1100 -mattr=-flat-for-global -verify-machineinstrs < %s | FileCheck -allow-deprecated-dag-overlap -check-prefixes=FUNC,GFX9PLUS %s
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns.patch:-; RUN:  llc -amdgpu-scalarize-global-loads=false  -march=r600 -mcpu=redwood < %s | FileCheck -allow-deprecated-dag-overlap -check-prefixes=EG,FUNC %s
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns.patch:+; RUN:  llc -amdgpu-scalarize-global-loads=false  -march=amdgcn -mcpu=verde -verify-machineinstrs < %s | FileCheck -check-prefixes=SI %s
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns.patch:+; RUN:  llc -amdgpu-scalarize-global-loads=false  -march=amdgcn -mcpu=tonga -mattr=-flat-for-global -verify-machineinstrs < %s | FileCheck -check-prefixes=VI %s
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns.patch:+; RUN:  llc -amdgpu-scalarize-global-loads=false  -march=amdgcn -mcpu=gfx900 -mattr=-flat-for-global -verify-machineinstrs < %s | FileCheck -check-prefixes=GFX9 %s
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns.patch:+; RUN:  llc -amdgpu-scalarize-global-loads=false  -march=amdgcn -mcpu=gfx1010 -mattr=-flat-for-global -verify-machineinstrs < %s | FileCheck -check-prefixes=GFX10 %s
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns.patch:+; RUN:  llc -amdgpu-scalarize-global-loads=false  -march=amdgcn -mcpu=gfx1100 -mattr=-flat-for-global -verify-machineinstrs < %s | FileCheck -check-prefixes=GFX11 %s
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns.patch:+; RUN:  llc -amdgpu-scalarize-global-loads=false  -march=r600 -mcpu=redwood < %s | FileCheck -check-prefixes=EG %s
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns.patch: define amdgpu_kernel void @test_mul_v2i32(ptr addrspace(1) %out, ptr addrspace(1) %in) {
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns.patch:@@ -23,18 +132,142 @@ define amdgpu_kernel void @test_mul_v2i32(ptr addrspace(1) %out, ptr addrspace(1
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns.patch: define amdgpu_kernel void @v_mul_v4i32(ptr addrspace(1) %out, ptr addrspace(1) %in) {
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns.patch:@@ -43,24 +276,232 @@ define amdgpu_kernel void @v_mul_v4i32(ptr addrspace(1) %out, ptr addrspace(1) %
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns.patch: define amdgpu_kernel void @s_trunc_i64_mul_to_i32(ptr addrspace(1) %out, i64 %a, i64 %b) {
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns.patch: define amdgpu_kernel void @v_trunc_i64_mul_to_i32(ptr addrspace(1) %out, ptr addrspace(1) %aptr, ptr addrspace(1) %bptr) nounwind {
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns.patch:@@ -71,13 +512,93 @@ define amdgpu_kernel void @v_trunc_i64_mul_to_i32(ptr addrspace(1) %out, ptr add
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns.patch: define amdgpu_kernel void @mul64_sext_c(ptr addrspace(1) %out, i32 %in) {
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns.patch: define amdgpu_kernel void @v_mul64_sext_c(ptr addrspace(1) %out, ptr addrspace(1) %in) {
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns.patch:@@ -100,12 +732,122 @@ define amdgpu_kernel void @v_mul64_sext_c(ptr addrspace(1) %out, ptr addrspace(1
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns.patch: define amdgpu_kernel void @v_mul64_sext_inline_imm(ptr addrspace(1) %out, ptr addrspace(1) %in) {
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns.patch:@@ -113,22 +855,202 @@ define amdgpu_kernel void @v_mul64_sext_inline_imm(ptr addrspace(1) %out, ptr ad
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns.patch: define amdgpu_kernel void @s_mul_i32(ptr addrspace(1) %out, [8 x i32], i32 %a, [8 x i32], i32 %b) nounwind {
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns.patch: define amdgpu_kernel void @v_mul_i32(ptr addrspace(1) %out, ptr addrspace(1) %in) {
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns.patch:@@ -137,6 +1059,298 @@ define amdgpu_kernel void @v_mul_i32(ptr addrspace(1) %out, ptr addrspace(1) %in
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns.patch:+define amdgpu_kernel void @s_mul_i1(ptr addrspace(1) %out, [8 x i32], i1 %a, [8 x i32], i1 %b) nounwind {
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns.patch:+define amdgpu_kernel void @v_mul_i1(ptr addrspace(1) %out, ptr addrspace(1) %in) {
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns.patch:@@ -144,21 +1358,294 @@ define amdgpu_kernel void @v_mul_i32(ptr addrspace(1) %out, ptr addrspace(1) %in
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns.patch: define amdgpu_kernel void @s_mul_i64(ptr addrspace(1) %out, i64 %a, i64 %b) nounwind {
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns.patch: define amdgpu_kernel void @v_mul_i64(ptr addrspace(1) %out, ptr addrspace(1) %aptr, ptr addrspace(1) %bptr) {
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns.patch:@@ -166,9 +1653,220 @@ define amdgpu_kernel void @v_mul_i64(ptr addrspace(1) %out, ptr addrspace(1) %ap
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns.patch: define amdgpu_kernel void @mul32_in_branch(ptr addrspace(1) %out, ptr addrspace(1) %in, i32 %a, i32 %b, i32 %c) {
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns.patch: define amdgpu_kernel void @mul64_in_branch(ptr addrspace(1) %out, ptr addrspace(1) %in, i64 %a, i64 %b, i64 %c) {
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns.patch: define amdgpu_kernel void @s_mul_i128(ptr addrspace(1) %out, [8 x i32], i128 %a, [8 x i32], i128 %b) nounwind #0 {
var/spack/repos/builtin/packages/llvm-amdgpu/001-Add-i1-mul-patterns.patch: define amdgpu_kernel void @v_mul_i128(ptr addrspace(1) %out, ptr addrspace(1) %aptr, ptr addrspace(1) %bptr) #0 {
var/spack/repos/builtin/packages/llvm-amdgpu/0001-update-HIP_PATH-deduction-for-5.7.0.patch: clang/lib/Driver/ToolChains/AMDGPU.cpp | 20 ++++++++++++++++----
var/spack/repos/builtin/packages/llvm-amdgpu/0001-update-HIP_PATH-deduction-for-5.7.0.patch:diff --git a/clang/lib/Driver/ToolChains/AMDGPU.cpp b/clang/lib/Driver/ToolChains/AMDGPU.cpp
var/spack/repos/builtin/packages/llvm-amdgpu/0001-update-HIP_PATH-deduction-for-5.7.0.patch:--- a/clang/lib/Driver/ToolChains/AMDGPU.cpp
var/spack/repos/builtin/packages/llvm-amdgpu/0001-update-HIP_PATH-deduction-for-5.7.0.patch:+++ b/clang/lib/Driver/ToolChains/AMDGPU.cpp
var/spack/repos/builtin/packages/llvm-amdgpu/0001-update-HIP_PATH-deduction-for-5.7.0.patch:@@ -437,12 +437,13 @@ void RocmInstallationDetector::detectDeviceLibrary() {
var/spack/repos/builtin/packages/llvm-amdgpu/0001-update-HIP_PATH-deduction-for-5.7.0.patch: void RocmInstallationDetector::detectHIPRuntime() {
var/spack/repos/builtin/packages/llvm-amdgpu/0001-update-HIP_PATH-deduction-for-5.7.0.patch:@@ -464,10 +465,21 @@ void RocmInstallationDetector::detectHIPRuntime() {
var/spack/repos/builtin/packages/fltk/package.py:        # Remove flags not recognized by the NVIDIA compiler
var/spack/repos/builtin/packages/opencl-headers/package.py:class OpenclHeaders(BundlePackage):
var/spack/repos/builtin/packages/opencl-headers/package.py:    """Bundled OpenCL (Open Computing Language) header files"""
var/spack/repos/builtin/packages/opencl-headers/package.py:    homepage = "https://www.khronos.org/registry/OpenCL/"
var/spack/repos/builtin/packages/opencl-headers/package.py:    depends_on("opencl-c-headers@2020.12.18:", when="@3.0:")
var/spack/repos/builtin/packages/opencl-headers/package.py:    depends_on("opencl-c-headers@2020.03.13:", when="@2.0:2.2")
var/spack/repos/builtin/packages/opencl-headers/package.py:    depends_on("opencl-clhpp@2.0.13:", when="@3.0:")
var/spack/repos/builtin/packages/opencl-headers/package.py:    depends_on("opencl-clhpp@2.0.11:", when="@2.1:2.2")
var/spack/repos/builtin/packages/opencl-headers/package.py:    depends_on("opencl-clhpp@2.0.9:", when="@2.0")
var/spack/repos/builtin/packages/bohrium/package.py:class Bohrium(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/bohrium/package.py:    variant("cuda", default=True, description="Build with CUDA code generator")
var/spack/repos/builtin/packages/bohrium/package.py:    variant("opencl", default=True, description="Build with OpenCL code generator")
var/spack/repos/builtin/packages/bohrium/package.py:    conflicts("~openmp~opencl~cuda")
var/spack/repos/builtin/packages/bohrium/package.py:    # cuda dependencies managed by CudaPackage class
var/spack/repos/builtin/packages/bohrium/package.py:    depends_on("opencl", when="+opencl")
var/spack/repos/builtin/packages/bohrium/package.py:    depends_on("opencv+cudev", when="+opencv+cuda")
var/spack/repos/builtin/packages/bohrium/package.py:        # TODO: Use cuda_arch to specify compute capabilities to build.
var/spack/repos/builtin/packages/bohrium/package.py:        # limiting for generic builds and the ability to run CUDA builds on
var/spack/repos/builtin/packages/bohrium/package.py:        args += ["-DVE_OPENCL=" + str("+opencl" in spec), "-DVE_CUDA=" + str("+cuda" in spec)]
var/spack/repos/builtin/packages/bohrium/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/bohrium/package.py:            stacks.append("cuda")
var/spack/repos/builtin/packages/bohrium/package.py:        if spec.satisfies("+opencl"):
var/spack/repos/builtin/packages/bohrium/package.py:            stacks.append("opencl")
var/spack/repos/builtin/packages/kim-api/package.py:        # Remove flags not recognized by the NVIDIA compiler
var/spack/repos/builtin/packages/intel-oneapi-dpct/package.py:    existing CUDA code to SYCL code. The tool ports both CUDA
var/spack/repos/builtin/packages/intel-oneapi-dpct/package.py:    CUDA code automatically migrates to SYCL code.
var/spack/repos/builtin/packages/py-earth2mip/package.py:    homepage = "https://github.com/NVIDIA/earth2mip"
var/spack/repos/builtin/packages/py-earth2mip/package.py:    url = "https://github.com/NVIDIA/earth2mip/archive/refs/tags/v0.1.0.tar.gz"
var/spack/repos/builtin/packages/py-earth2mip/package.py:    git = "https://github.com/NVIDIA/earth2mip.git"
var/spack/repos/builtin/packages/py-earth2mip/package.py:        depends_on("py-nvidia-modulus@0.4.0:")
var/spack/repos/builtin/packages/cnmem/package.py:    """CNMem mempool for CUDA devices"""
var/spack/repos/builtin/packages/cnmem/package.py:    homepage = "https://github.com/NVIDIA/cnmem"
var/spack/repos/builtin/packages/cnmem/package.py:    git = "https://github.com/NVIDIA/cnmem.git"
var/spack/repos/builtin/packages/lbann/package.py:class Lbann(CachedCMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/lbann/package.py:    conflicts("~cuda", when="+nvprof")
var/spack/repos/builtin/packages/lbann/package.py:    conflicts("~cuda", when="+nvshmem")
var/spack/repos/builtin/packages/lbann/package.py:    conflicts("+cuda", when="+rocm", msg="CUDA and ROCm support are mutually exclusive")
var/spack/repos/builtin/packages/lbann/package.py:    forwarded_variants = ["cuda", "rocm", "half", "nvshmem"]
var/spack/repos/builtin/packages/lbann/package.py:        if v == "cuda" or v == "rocm":
var/spack/repos/builtin/packages/lbann/package.py:            depends_on("aluminum +{0} +nccl".format(v), when="+{0}".format(v))
var/spack/repos/builtin/packages/lbann/package.py:    with when("+cuda"):
var/spack/repos/builtin/packages/lbann/package.py:            depends_on("aws-ofi-nccl")  # Note: NOT a CudaPackage
var/spack/repos/builtin/packages/lbann/package.py:    with when("+rocm"):
var/spack/repos/builtin/packages/lbann/package.py:    for arch in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/lbann/package.py:        depends_on("hydrogen cuda_arch=%s" % arch, when="+cuda cuda_arch=%s" % arch)
var/spack/repos/builtin/packages/lbann/package.py:        depends_on("aluminum cuda_arch=%s" % arch, when="+cuda cuda_arch=%s" % arch)
var/spack/repos/builtin/packages/lbann/package.py:        depends_on("dihydrogen cuda_arch=%s" % arch, when="+cuda cuda_arch=%s" % arch)
var/spack/repos/builtin/packages/lbann/package.py:        depends_on("nccl cuda_arch=%s" % arch, when="+cuda cuda_arch=%s" % arch)
var/spack/repos/builtin/packages/lbann/package.py:        depends_on("hwloc cuda_arch=%s" % arch, when="+cuda cuda_arch=%s" % arch)
var/spack/repos/builtin/packages/lbann/package.py:    # variants +rocm and amdgpu_targets are not automatically passed to
var/spack/repos/builtin/packages/lbann/package.py:    for val in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/lbann/package.py:        depends_on("hydrogen amdgpu_target=%s" % val, when="+rocm amdgpu_target=%s" % val)
var/spack/repos/builtin/packages/lbann/package.py:        depends_on("aluminum amdgpu_target=%s" % val, when="+rocm amdgpu_target=%s" % val)
var/spack/repos/builtin/packages/lbann/package.py:        depends_on("dihydrogen amdgpu_target=%s" % val, when="+rocm amdgpu_target=%s" % val)
var/spack/repos/builtin/packages/lbann/package.py:        depends_on(f"hwloc amdgpu_target={val}", when=f"+rocm amdgpu_target={val}")
var/spack/repos/builtin/packages/lbann/package.py:    depends_on("roctracer-dev", when="+rocm +distconv")
var/spack/repos/builtin/packages/lbann/package.py:    depends_on("cudnn@8.0.2:", when="+cuda")
var/spack/repos/builtin/packages/lbann/package.py:    depends_on("cutensor", when="+cuda")
var/spack/repos/builtin/packages/lbann/package.py:    depends_on("hipcub", when="+rocm")
var/spack/repos/builtin/packages/lbann/package.py:    depends_on("hwloc +cuda +nvml ~rocm", when="+cuda")
var/spack/repos/builtin/packages/lbann/package.py:    depends_on("hwloc@2.3.0: +rocm ~cuda", when="+rocm")
var/spack/repos/builtin/packages/lbann/package.py:    depends_on("hiptt", when="+rocm")
var/spack/repos/builtin/packages/lbann/package.py:        "+imgcodecs +imgproc +jpeg +png +tiff +fast-math ~cuda",
var/spack/repos/builtin/packages/lbann/package.py:    depends_on("nccl", when="@0.94:0.98.2 +cuda")
var/spack/repos/builtin/packages/lbann/package.py:    depends_on("onednn cpu_runtime=omp gpu_runtime=none", when="+onednn")
var/spack/repos/builtin/packages/lbann/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/lbann/package.py:                            cmake_cache_string("CMAKE_CUDA_FLAGS", "-Xcompiler={0}".format(flag))
var/spack/repos/builtin/packages/lbann/package.py:            if spec.satisfies("^cuda@11.0:"):
var/spack/repos/builtin/packages/lbann/package.py:                entries.append(cmake_cache_string("CMAKE_CUDA_STANDARD", "17"))
var/spack/repos/builtin/packages/lbann/package.py:                entries.append(cmake_cache_string("CMAKE_CUDA_STANDARD", "14"))
var/spack/repos/builtin/packages/lbann/package.py:            if spec.satisfies("%cce") and spec.satisfies("^cuda+allow-unsupported-compilers"):
var/spack/repos/builtin/packages/lbann/package.py:                    cmake_cache_string("CMAKE_CUDA_FLAGS", "-allow-unsupported-compiler")
var/spack/repos/builtin/packages/lbann/package.py:            cmake_cache_option("LBANN_WITH_ROCTRACER", spec.satisfies("+rocm +distconv"))
var/spack/repos/builtin/packages/lbann/lbann_v0.104_build_cleanup.patch:-#if defined (LBANN_HAS_ROCM) && defined (LBANN_HAS_GPU_FP16)
var/spack/repos/builtin/packages/lbann/lbann_v0.104_build_cleanup.patch:+#if defined(LBANN_HAS_ROCM) && defined(LBANN_HAS_GPU_FP16)
var/spack/repos/builtin/packages/ocl-icd/package.py:    OpenCL ICD loaders."""
var/spack/repos/builtin/packages/ocl-icd/package.py:        description="Install also OpenCL headers to use this as OpenCL provider",
var/spack/repos/builtin/packages/ocl-icd/package.py:    depends_on("opencl-headers@3.0:", when="+headers")
var/spack/repos/builtin/packages/ocl-icd/package.py:    provides("opencl@:3.0", when="@2.2.13:+headers")
var/spack/repos/builtin/packages/ocl-icd/package.py:    provides("opencl@:2.2", when="@2.2.12+headers")
var/spack/repos/builtin/packages/ocl-icd/package.py:    provides("opencl@:2.1", when="@2.2.8:2.2.11+headers")
var/spack/repos/builtin/packages/ocl-icd/package.py:    provides("opencl@:2.0", when="@2.2.3:2.2.7+headers")
var/spack/repos/builtin/packages/ocl-icd/package.py:    # official khronos OpenCL C headers release, ie opencl-c-headers@2021.04.29
var/spack/repos/builtin/packages/ports-of-call/package.py:        values=("Kokkos", "Cuda", "None"),
var/spack/repos/builtin/packages/rocsolver/package.py:    subset of LAPACK functionality on the ROCm platform."""
var/spack/repos/builtin/packages/rocsolver/package.py:    homepage = "https://github.com/ROCm/rocSOLVER"
var/spack/repos/builtin/packages/rocsolver/package.py:    git = "https://github.com/ROCm/rocSOLVER.git"
var/spack/repos/builtin/packages/rocsolver/package.py:    url = "https://github.com/ROCm/rocSOLVER/archive/rocm-6.1.2.tar.gz"
var/spack/repos/builtin/packages/rocsolver/package.py:    tags = ["rocm"]
var/spack/repos/builtin/packages/rocsolver/package.py:    amdgpu_targets = ROCmPackage.amdgpu_targets
var/spack/repos/builtin/packages/rocsolver/package.py:        "amdgpu_target",
var/spack/repos/builtin/packages/rocsolver/package.py:        description="AMD GPU architecture",
var/spack/repos/builtin/packages/rocsolver/package.py:        values=auto_or_any_combination_of(*amdgpu_targets),
var/spack/repos/builtin/packages/rocsolver/package.py:    depends_on("rocm-cmake@master", type="build", when="@master:")
var/spack/repos/builtin/packages/rocsolver/package.py:    depends_on("rocm-cmake@4.5.0:", type="build", when="@4.5.0:")
var/spack/repos/builtin/packages/rocsolver/package.py:    depends_on("rocm-cmake@4.3.0:", type="build", when="@4.3.0:")
var/spack/repos/builtin/packages/rocsolver/package.py:    depends_on("rocm-cmake@3.5.0:", type="build")
var/spack/repos/builtin/packages/rocsolver/package.py:    for tgt in itertools.chain(["auto"], amdgpu_targets):
var/spack/repos/builtin/packages/rocsolver/package.py:        depends_on(f"rocblas amdgpu_target={tgt}", when=f"amdgpu_target={tgt}")
var/spack/repos/builtin/packages/rocsolver/package.py:        tgt = self.spec.variants["amdgpu_target"]
var/spack/repos/builtin/packages/rocsolver/package.py:            args.append(self.define_from_variant("AMDGPU_TARGETS", "amdgpu_target"))
var/spack/repos/builtin/packages/rocsolver/package.py:            args.append(self.define("__skip_rocmclang", "ON"))
var/spack/repos/builtin/packages/rocsolver/package.py:            env.set("CC", f"{self.spec['llvm-amdgpu'].prefix}/bin/clang")
var/spack/repos/builtin/packages/rocsolver/package.py:            env.set("CXX", f"{self.spec['llvm-amdgpu'].prefix}/bin/clang++")
var/spack/repos/builtin/packages/chapel/package.py:class Chapel(AutotoolsPackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/chapel/package.py:        "rocmcc": "clang",
var/spack/repos/builtin/packages/chapel/package.py:        "gpu_mem_strategy",
var/spack/repos/builtin/packages/chapel/package.py:        description="The memory allocation strategy for GPU data",
var/spack/repos/builtin/packages/chapel/package.py:        "CHPL_GPU",
var/spack/repos/builtin/packages/chapel/package.py:        "CHPL_GPU_ARCH",
var/spack/repos/builtin/packages/chapel/package.py:        "CHPL_GPU_MEM_STRATEGY",
var/spack/repos/builtin/packages/chapel/package.py:    # Ensure GPU support is Sticky: never allow the concretizer to choose this
var/spack/repos/builtin/packages/chapel/package.py:    variant("rocm", default=False, sticky=True, description="Enable AMD ROCm GPU support")
var/spack/repos/builtin/packages/chapel/package.py:    variant("cuda", default=False, sticky=True, description="Enable Nvidia CUDA GPU support")
var/spack/repos/builtin/packages/chapel/package.py:    conflicts("+rocm", when="+cuda", msg="Chapel must be built with either CUDA or ROCm, not both")
var/spack/repos/builtin/packages/chapel/package.py:    conflicts("+rocm", when="@:1", msg="ROCm support in spack requires Chapel 2.0.0 or later")
var/spack/repos/builtin/packages/chapel/package.py:    # Chapel restricts the allowable ROCm versions
var/spack/repos/builtin/packages/chapel/package.py:    with when("@2:2.1 +rocm"):
var/spack/repos/builtin/packages/chapel/package.py:    with when("@2.2: +rocm"):
var/spack/repos/builtin/packages/chapel/package.py:    depends_on("llvm-amdgpu@4:5.4", when="+rocm llvm=spack")
var/spack/repos/builtin/packages/chapel/package.py:    requires("llvm=bundled", when="+rocm ^hip@6.0:6.2", msg="ROCm 6 support requires llvm=bundled")
var/spack/repos/builtin/packages/chapel/package.py:        conflicts("+cuda", msg="Cuda support requires building with LLVM")
var/spack/repos/builtin/packages/chapel/package.py:        conflicts("+rocm", msg="ROCm support requires building with LLVM")
var/spack/repos/builtin/packages/chapel/package.py:    with when("llvm=spack ~rocm"):
var/spack/repos/builtin/packages/chapel/package.py:    # Based on docs https://chapel-lang.org/docs/technotes/gpu.html#requirements
var/spack/repos/builtin/packages/chapel/package.py:    depends_on("llvm@16:", when="llvm=spack +cuda ^cuda@12:")
var/spack/repos/builtin/packages/chapel/package.py:        msg="llvm=spack +cuda requires LLVM support the nvptx target",
var/spack/repos/builtin/packages/chapel/package.py:        when="llvm=spack +cuda",
var/spack/repos/builtin/packages/chapel/package.py:        depends_on("cuda@11:", when="+cuda")
var/spack/repos/builtin/packages/chapel/package.py:        "%rocmcc",
var/spack/repos/builtin/packages/chapel/package.py:            self.spec.satisfies("+rocm")
var/spack/repos/builtin/packages/chapel/package.py:            or self.spec.satisfies("+cuda")
var/spack/repos/builtin/packages/chapel/package.py:        if self.spec.satisfies("+rocm llvm=spack"):
var/spack/repos/builtin/packages/chapel/package.py:                join_path(self.spec["llvm-amdgpu"].prefix, "bin", "llvm-config"),
var/spack/repos/builtin/packages/chapel/package.py:            real_cc = join_path(self.spec["llvm-amdgpu"].prefix, "bin", "clang")
var/spack/repos/builtin/packages/chapel/package.py:            real_cxx = join_path(self.spec["llvm-amdgpu"].prefix, "bin", "clang++")
var/spack/repos/builtin/packages/chapel/package.py:            # +rocm appears to also require a matching LLVM host compiler to guarantee linkage
var/spack/repos/builtin/packages/chapel/package.py:        # We'll set to GPU later if +rocm or +cuda requested
var/spack/repos/builtin/packages/chapel/package.py:        if self.spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/chapel/package.py:            # TODO: why must we add to LD_LIBRARY_PATH to find libcudart?
var/spack/repos/builtin/packages/chapel/package.py:            env.prepend_path("LD_LIBRARY_PATH", self.spec["cuda"].prefix.lib64)
var/spack/repos/builtin/packages/chapel/package.py:            env.set("CHPL_CUDA_PATH", self.spec["cuda"].prefix)
var/spack/repos/builtin/packages/chapel/package.py:            env.set("CHPL_LOCALE_MODEL", "gpu")
var/spack/repos/builtin/packages/chapel/package.py:            env.set("CHPL_GPU", "nvidia")
var/spack/repos/builtin/packages/chapel/package.py:        if self.spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/chapel/package.py:            env.set("CHPL_LOCALE_MODEL", "gpu")
var/spack/repos/builtin/packages/chapel/package.py:            env.set("CHPL_GPU", "amd")
var/spack/repos/builtin/packages/chapel/package.py:            env.set("CHPL_GPU_ARCH", self.spec.variants["amdgpu_target"].value[0])
var/spack/repos/builtin/packages/chapel/package.py:            env.set("CHPL_ROCM_PATH", self.spec["hip"].prefix)
var/spack/repos/builtin/packages/chapel/package.py:                    if self.spec.satisfies("+cuda") or self.spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/exawind/package.py:class Exawind(CMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/exawind/package.py:    variant("amr_wind_gpu", default=False, description="Enable AMR-Wind on the GPU")
var/spack/repos/builtin/packages/exawind/package.py:    variant("nalu_wind_gpu", default=False, description="Enable Nalu-Wind on the GPU")
var/spack/repos/builtin/packages/exawind/package.py:    variant("gpu-aware-mpi", default=False, description="gpu-aware-mpi")
var/spack/repos/builtin/packages/exawind/package.py:    for arch in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/exawind/package.py:            "amr-wind+cuda cuda_arch=%s" % arch, when="+amr_wind_gpu+cuda cuda_arch=%s" % arch
var/spack/repos/builtin/packages/exawind/package.py:            "nalu-wind+cuda cuda_arch=%s" % arch, when="+nalu_wind_gpu+cuda cuda_arch=%s" % arch
var/spack/repos/builtin/packages/exawind/package.py:            "trilinos+cuda cuda_arch=%s" % arch, when="+nalu_wind_gpu+cuda cuda_arch=%s" % arch
var/spack/repos/builtin/packages/exawind/package.py:    for arch in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/exawind/package.py:            "amr-wind+rocm amdgpu_target=%s" % arch,
var/spack/repos/builtin/packages/exawind/package.py:            when="+amr_wind_gpu+rocm amdgpu_target=%s" % arch,
var/spack/repos/builtin/packages/exawind/package.py:            "nalu-wind+rocm amdgpu_target=%s" % arch,
var/spack/repos/builtin/packages/exawind/package.py:            when="+nalu_wind_gpu+rocm amdgpu_target=%s" % arch,
var/spack/repos/builtin/packages/exawind/package.py:            "trilinos+rocm amdgpu_target=%s" % arch,
var/spack/repos/builtin/packages/exawind/package.py:            when="+nalu_wind_gpu+rocm amdgpu_target=%s" % arch,
var/spack/repos/builtin/packages/exawind/package.py:    depends_on("amr-wind+sycl", when="+amr_wind_gpu+sycl")
var/spack/repos/builtin/packages/exawind/package.py:    depends_on("kokkos-nvcc-wrapper", type="build", when="+cuda")
var/spack/repos/builtin/packages/exawind/package.py:    depends_on("nalu-wind+gpu-aware-mpi", when="+gpu-aware-mpi")
var/spack/repos/builtin/packages/exawind/package.py:    depends_on("amr-wind+gpu-aware-mpi", when="+gpu-aware-mpi")
var/spack/repos/builtin/packages/exawind/package.py:    with when("~amr_wind_gpu~nalu_wind_gpu"):
var/spack/repos/builtin/packages/exawind/package.py:        conflicts("+cuda")
var/spack/repos/builtin/packages/exawind/package.py:        conflicts("+rocm")
var/spack/repos/builtin/packages/exawind/package.py:    with when("~nalu_wind_gpu"):
var/spack/repos/builtin/packages/exawind/package.py:        conflicts("^nalu-wind+cuda")
var/spack/repos/builtin/packages/exawind/package.py:        conflicts("^nalu-wind+rocm")
var/spack/repos/builtin/packages/exawind/package.py:    with when("~amr_wind_gpu"):
var/spack/repos/builtin/packages/exawind/package.py:        conflicts("^amr-wind+cuda")
var/spack/repos/builtin/packages/exawind/package.py:        conflicts("^amr-wind+rocm")
var/spack/repos/builtin/packages/exawind/package.py:    conflicts("+amr_wind_gpu", when="~cuda~rocm~sycl")
var/spack/repos/builtin/packages/exawind/package.py:    conflicts("+nalu_wind_gpu", when="~cuda~rocm")
var/spack/repos/builtin/packages/exawind/package.py:    conflicts("+nalu_wind_gpu", when="+sycl")
var/spack/repos/builtin/packages/exawind/package.py:    conflicts("^amr-wind+hypre", when="~amr_wind_gpu+nalu_wind_gpu")
var/spack/repos/builtin/packages/exawind/package.py:    conflicts("^amr-wind+hypre", when="+amr_wind_gpu~nalu_wind_gpu")
var/spack/repos/builtin/packages/exawind/package.py:    conflicts("+sycl", when="+cuda")
var/spack/repos/builtin/packages/exawind/package.py:    conflicts("+rocm", when="+cuda")
var/spack/repos/builtin/packages/exawind/package.py:    conflicts("+sycl", when="+rocm")
var/spack/repos/builtin/packages/exawind/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/exawind/package.py:            args.append(self.define("EXAWIND_ENABLE_CUDA", True))
var/spack/repos/builtin/packages/exawind/package.py:            args.append(self.define("CUDAToolkit_ROOT", self.spec["cuda"].prefix))
var/spack/repos/builtin/packages/exawind/package.py:            args.append(self.define("EXAWIND_CUDA_ARCH", self.spec.variants["cuda_arch"].value))
var/spack/repos/builtin/packages/exawind/package.py:        if spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/exawind/package.py:            targets = self.spec.variants["amdgpu_target"].value
var/spack/repos/builtin/packages/exawind/package.py:            args.append(self.define("EXAWIND_ENABLE_ROCM", True))
var/spack/repos/builtin/packages/exawind/package.py:            args.append(self.define("AMDGPU_TARGETS", ";".join(str(x) for x in targets)))
var/spack/repos/builtin/packages/exawind/package.py:            args.append(self.define("GPU_TARGETS", ";".join(str(x) for x in targets)))
var/spack/repos/builtin/packages/exawind/package.py:        if self.spec.satisfies("+rocm+amr_wind_gpu~nalu_wind_gpu"):
var/spack/repos/builtin/packages/exawind/package.py:        if self.spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/exawind/package.py:        if self.spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/rocpydecode/package.py:    full HW acceleration for video decoding on AMD GPUs."""
var/spack/repos/builtin/packages/rocpydecode/package.py:    homepage = "https://github.com/ROCm/rocPyDecode"
var/spack/repos/builtin/packages/rocpydecode/package.py:    url = "https://github.com/ROCm/rocPyDecode/archive/refs/tags/rocm-6.2.0.tar.gz"
var/spack/repos/builtin/packages/rocpydecode/package.py:            r"${ROCM_PATH}/llvm/bin/clang++",
var/spack/repos/builtin/packages/rocpydecode/package.py:            "{0}/bin/clang++".format(self.spec["llvm-amdgpu"].prefix),
var/spack/repos/builtin/packages/rocpydecode/package.py:            r"${ROCM_PATH}/share/rocdecode/utils",
var/spack/repos/builtin/packages/llvm-doe/package.py:class LlvmDoe(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/llvm-doe/package.py:        "<current arch>,NVPTX,AMDGPU,CppBackend",
var/spack/repos/builtin/packages/llvm-doe/package.py:    depends_on("libelf", when="+cuda")  # libomptarget
var/spack/repos/builtin/packages/llvm-doe/package.py:    depends_on("libffi", when="+cuda")  # libomptarget
var/spack/repos/builtin/packages/llvm-doe/package.py:    # cuda_arch value must be specified
var/spack/repos/builtin/packages/llvm-doe/package.py:    conflicts("cuda_arch=none", when="+cuda", msg="A value for cuda_arch must be specified.")
var/spack/repos/builtin/packages/llvm-doe/package.py:            if any(x in exe for x in ("vscode", "cpp", "-cl", "-gpu")):
var/spack/repos/builtin/packages/llvm-doe/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/llvm-doe/package.py:                    define("CUDA_TOOLKIT_ROOT_DIR", spec["cuda"].prefix),
var/spack/repos/builtin/packages/llvm-doe/package.py:                        ",".join(spec.variants["cuda_arch"].value),
var/spack/repos/builtin/packages/llvm-doe/package.py:                        "sm_{0}".format(spec.variants["cuda_arch"].value[-1]),
var/spack/repos/builtin/packages/llvm-doe/package.py:            # still build libomptarget but disable cuda
var/spack/repos/builtin/packages/llvm-doe/package.py:                    define("CUDA_TOOLKIT_ROOT_DIR", "IGNORE"),
var/spack/repos/builtin/packages/llvm-doe/package.py:                    define("CUDA_SDK_ROOT_DIR", "IGNORE"),
var/spack/repos/builtin/packages/llvm-doe/package.py:                    define("CUDA_NVCC_EXECUTABLE", "IGNORE"),
var/spack/repos/builtin/packages/llvm-doe/package.py:                    define("LIBOMPTARGET_DEP_CUDA_DRIVER_LIBRARIES", "IGNORE"),
var/spack/repos/builtin/packages/llvm-doe/package.py:            targets = ["NVPTX", "AMDGPU"]
var/spack/repos/builtin/packages/llvm-doe/package.py:        if self.spec.satisfies("+cuda ~omp_as_runtime"):
var/spack/repos/builtin/packages/fftw/package.py:        # Workaround NVIDIA/PGI compiler bug when avx512 is enabled
var/spack/repos/builtin/packages/fftw/package.py:        # NVIDIA compiler does not support Altivec intrinsics
var/spack/repos/builtin/packages/fftw/package.py:        # NVIDIA compiler does not support Neon intrinsics
var/spack/repos/builtin/packages/fftw/package.py:            # Workaround NVIDIA compiler bug
var/spack/repos/builtin/packages/cray-libsci/package.py:        "nvhpc": "NVIDIA",
var/spack/repos/builtin/packages/cray-libsci/package.py:        "rocmcc": "AMD",
var/spack/repos/builtin/packages/libcumlprims/package.py:    url = "https://anaconda.org/nvidia/libcumlprims/0.15.0/download/linux-64/libcumlprims-0.15.0-cuda11.0_gdbd0d39_0.tar.bz2"
var/spack/repos/builtin/packages/libcumlprims/package.py:        "0.15.0-cuda11.0_gdbd0d39_0",
var/spack/repos/builtin/packages/libcumlprims/package.py:        "0.15.0-cuda10.2_gdbd0d39_0",
var/spack/repos/builtin/packages/libcumlprims/package.py:        "0.15.0-cuda10.1_gdbd0d39_0",
var/spack/repos/builtin/packages/libcumlprims/package.py:    depends_on("cuda@11.0.0:11.0", when="@0.15.0-cuda11.0_gdbd0d39_0")
var/spack/repos/builtin/packages/libcumlprims/package.py:    depends_on("cuda@10.2.0:10.2", when="@0.15.0-cuda10.2_gdbd0d39_0")
var/spack/repos/builtin/packages/libcumlprims/package.py:    depends_on("cuda@10.1.0:10.1", when="@0.15.0-cuda10.1_gdbd0d39_0")
var/spack/repos/builtin/packages/libcumlprims/package.py:        url = "https://anaconda.org/nvidia/libcumlprims/{0}/download/linux-64/libcumlprims-{1}.tar.bz2"
var/spack/repos/builtin/packages/json-cwx/package.py:        # Remove flags not recognized by the NVIDIA compiler
var/spack/repos/builtin/packages/exaca/package.py:class Exaca(CMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/exaca/package.py:        if _backend != "cuda" and _backend != "rocm":
var/spack/repos/builtin/packages/exaca/package.py:    for arch in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/exaca/package.py:        cuda_dep = "+cuda cuda_arch={0}".format(arch)
var/spack/repos/builtin/packages/exaca/package.py:        depends_on("kokkos {0}".format(cuda_dep), when=cuda_dep)
var/spack/repos/builtin/packages/exaca/package.py:    for arch in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/exaca/package.py:        rocm_dep = "+rocm amdgpu_target={0}".format(arch)
var/spack/repos/builtin/packages/exaca/package.py:        depends_on("kokkos {0}".format(rocm_dep), when=rocm_dep)
var/spack/repos/builtin/packages/exaca/package.py:        # Use hipcc if compiling for rocm. Modifying this instead of CMAKE_CXX_COMPILER
var/spack/repos/builtin/packages/exaca/package.py:        if self.spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/py-tomopy/package.py:    # GPU accel needs PTL which is a git submodule. Thus, we can only build it on master
var/spack/repos/builtin/packages/py-tomopy/package.py:    depends_on("cuda", when="@master")
var/spack/repos/builtin/packages/clamr/package.py:    using MPI and OpenCL GPU code.
var/spack/repos/builtin/packages/scs/package.py:    variant("cuda", default=False, description="Build with Cuda support")
var/spack/repos/builtin/packages/scs/package.py:    depends_on("cuda", when="+cuda")
var/spack/repos/builtin/packages/scs/package.py:    # make sure install_gpu target installs all libs not only the gpu ones
var/spack/repos/builtin/packages/scs/package.py:    patch("make_gpu.patch")
var/spack/repos/builtin/packages/scs/package.py:        if "+cuda" in spec:
var/spack/repos/builtin/packages/scs/package.py:            make("default", "gpu")
var/spack/repos/builtin/packages/scs/package.py:        if "+cuda" in spec:
var/spack/repos/builtin/packages/scs/package.py:            make("PREFIX=" + prefix, "install_gpu")
var/spack/repos/builtin/packages/scs/make_gpu.patch:-install_gpu: $(INSTALL_INC_FILES) $(INSTALL_GPU_TARGETS)
var/spack/repos/builtin/packages/scs/make_gpu.patch:+install_gpu: $(INSTALL_INC_FILES) $(INSTALL_GPU_TARGETS) $(INSTALL_TARGETS)
var/spack/repos/builtin/packages/scs/make_gpu.patch: 	$(INSTALL) -m 644 $(INSTALL_GPU_TARGETS) $(INSTALL_LIB_DIR)
var/spack/repos/builtin/packages/cntk/kaldireader-openblas.patch:   KALDI_LIBS_LIST := kaldi-util kaldi-matrix kaldi-base kaldi-hmm kaldi-cudamatrix kaldi-nnet kaldi-lat
var/spack/repos/builtin/packages/cntk/package.py:    variant("cuda", default=False, description="Enable CUDA support.")
var/spack/repos/builtin/packages/cntk/package.py:    depends_on("cuda", when="+cuda")
var/spack/repos/builtin/packages/cntk/package.py:    depends_on("cub@1.4.1", when="+cuda")
var/spack/repos/builtin/packages/cntk/package.py:    depends_on("cudnn@5.1", when="+cuda")
var/spack/repos/builtin/packages/cntk/package.py:    depends_on("nccl", when="+cuda")
var/spack/repos/builtin/packages/cntk/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/cntk/package.py:            args.append("--cuda=yes")
var/spack/repos/builtin/packages/cntk/package.py:            args.append("--with-cuda={0}".format(spec["cuda"].prefix))
var/spack/repos/builtin/packages/cntk/package.py:            args.append("--with-nccl={0}".format(spec["nccl"].prefix))
var/spack/repos/builtin/packages/cntk/package.py:            args.append("--with-gdk-include={0}".format(spec["cuda"].prefix.include))
var/spack/repos/builtin/packages/cntk/package.py:            args.append("--with-gdk-nvml-lib={0}/stubs".format(spec["cuda"].prefix.lib64))
var/spack/repos/builtin/packages/cntk/build.patch: #     If not specified, GPU will not be enabled
var/spack/repos/builtin/packages/cntk/build.patch: #   CUB_PATH= path to NVIDIA CUB installation, so $(CUB_PATH)/cub/cub.cuh exists
var/spack/repos/builtin/packages/cntk/build.patch:-#   CUDNN_PATH= path to NVIDIA cuDNN installation so $(CUDNN_PATH)/cuda/include/cudnn.h exists
var/spack/repos/builtin/packages/cntk/build.patch:+#   CUDNN_PATH= path to NVIDIA cuDNN installation so $(CUDNN_PATH)/include/cudnn.h exists
var/spack/repos/builtin/packages/cntk/build.patch:@@ -149,8 +153,8 @@ ifdef CUDA_PATH
var/spack/repos/builtin/packages/cntk/build.patch:-    INCLUDEPATH += $(CUDNN_PATH)/cuda/include
var/spack/repos/builtin/packages/cntk/build.patch:-    LIBPATH += $(CUDNN_PATH)/cuda/lib64
var/spack/repos/builtin/packages/cntk/build.patch:   KALDI_LIBS_LIST := kaldi-util kaldi-matrix kaldi-base kaldi-hmm kaldi-cudamatrix kaldi-nnet kaldi-lat
var/spack/repos/builtin/packages/cntk/build.patch:-  $(error Build with Multiverso was requested but cannot find the code. Please check https://github.com/Microsoft/CNTK/wiki/Multiple-GPUs-and-machines#24-data-parallel-asgd to learn more.)
var/spack/repos/builtin/packages/cntk/build.patch:-cudnn_check=cuda/include/cudnn.h
var/spack/repos/builtin/packages/cntk/build.patch: default_cudas="cuda-8.0 cuda-7.5"
var/spack/repos/builtin/packages/cntk/build.patch: default_nccls="nccl"
var/spack/repos/builtin/packages/cntk/build.patch: default_gdk_includes="include/nvidia/gdk"
var/spack/repos/builtin/packages/cntk/build.patch: function find_nccl ()
var/spack/repos/builtin/packages/cntk/build.patch:     find_dir "$default_nccls" "$nccl_check"
var/spack/repos/builtin/packages/cntk/build.patch:+        echo https://github.com/Microsoft/CNTK/wiki/Multiple-GPUs-and-machines#24-data-parallel-asgd 
var/spack/repos/builtin/packages/rocprim/fix-device-merge-mismatched-param-5.3.0.patch:Subject: [PATCH 2/2] Update CHANGELOG.md for ROCm 5.3.2
var/spack/repos/builtin/packages/rocprim/fix-device-merge-mismatched-param-5.3.0.patch: Full documentation for rocPRIM is available at [https://codedocs.xyz/ROCmSoftwarePlatform/rocPRIM/](https://codedocs.xyz/ROCmSoftwarePlatform/rocPRIM/)
var/spack/repos/builtin/packages/rocprim/fix-device-merge-mismatched-param-5.3.0.patch:+## [rocPRIM-2.11.1 for ROCm 5.3.2]
var/spack/repos/builtin/packages/rocprim/fix-device-merge-mismatched-param-5.3.0.patch: ## [rocPRIM-2.11.0 for ROCm 5.3.0]
var/spack/repos/builtin/packages/rocprim/package.py:    homepage = "https://github.com/ROCm/rocPRIM"
var/spack/repos/builtin/packages/rocprim/package.py:    git = "https://github.com/ROCm/rocPRIM.git"
var/spack/repos/builtin/packages/rocprim/package.py:    url = "https://github.com/ROCm/rocPRIM/archive/rocm-6.1.0.tar.gz"
var/spack/repos/builtin/packages/rocprim/package.py:    tags = ["rocm"]
var/spack/repos/builtin/packages/rocprim/package.py:    amdgpu_targets = ROCmPackage.amdgpu_targets
var/spack/repos/builtin/packages/rocprim/package.py:        "amdgpu_target",
var/spack/repos/builtin/packages/rocprim/package.py:        description="AMD GPU architecture",
var/spack/repos/builtin/packages/rocprim/package.py:        values=auto_or_any_combination_of(*amdgpu_targets),
var/spack/repos/builtin/packages/rocprim/package.py:        depends_on(f"llvm-amdgpu@{ver}", when=f"@{ver}")
var/spack/repos/builtin/packages/rocprim/package.py:        depends_on(f"rocm-cmake@{ver}:", type="build", when=f"@{ver}")
var/spack/repos/builtin/packages/rocprim/package.py:            env.set("CC", f"{self.spec['llvm-amdgpu'].prefix}/bin/clang++")
var/spack/repos/builtin/packages/rocprim/package.py:            env.set("CXX", f"{self.spec['llvm-amdgpu'].prefix}/bin/clang++")
var/spack/repos/builtin/packages/rocprim/package.py:        if "auto" not in self.spec.variants["amdgpu_target"]:
var/spack/repos/builtin/packages/rocprim/package.py:            args.append(self.define_from_variant("AMDGPU_TARGETS", "amdgpu_target"))
var/spack/repos/builtin/packages/rocprim/package.py:            args.append(self.define("__skip_rocmclang", "ON"))
var/spack/repos/builtin/packages/py-tensorly/package.py:    methods at scale on CPU or GPU."""
var/spack/repos/builtin/packages/minigmg/package.py:    highly-optimized implementations for CPUs and GPUs.
var/spack/repos/builtin/packages/countdown/package.py:class Countdown(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/countdown/package.py:            self.define_from_variant("CNTD_ENABLE_CUDA", "cuda"),
var/spack/repos/builtin/packages/paraview/package.py:class Paraview(CMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/paraview/package.py:    variant("nvindex", default=False, description="Enable the pvNVIDIAIndeX plugin")
var/spack/repos/builtin/packages/paraview/package.py:    conflicts("~shared", when="+cuda")
var/spack/repos/builtin/packages/paraview/package.py:    conflicts("+cuda", when="@5.8:5.10")
var/spack/repos/builtin/packages/paraview/package.py:    conflicts("+cuda", when="use_vtkm=off")
var/spack/repos/builtin/packages/paraview/package.py:    conflicts("+rocm", when="+cuda")
var/spack/repos/builtin/packages/paraview/package.py:    conflicts("+rocm", when="use_vtkm=off")
var/spack/repos/builtin/packages/paraview/package.py:    conflicts("paraview@:5.10", when="+rocm")
var/spack/repos/builtin/packages/paraview/package.py:    # CUDA ARCH
var/spack/repos/builtin/packages/paraview/package.py:    supported_cuda_archs = {
var/spack/repos/builtin/packages/paraview/package.py:        conflicts(f"cuda_arch={_arch}", when="+cuda", msg="ParaView requires cuda_arch >= 20")
var/spack/repos/builtin/packages/paraview/package.py:    # Starting from cmake@3.18, CUDA architecture managament can be delegated to CMake.
var/spack/repos/builtin/packages/paraview/package.py:    for _arch in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/paraview/package.py:            conflicts("cmake@:3.17", when=f"cuda_arch={_arch}")
var/spack/repos/builtin/packages/paraview/package.py:    for _arch, _other_arch in itertools.permutations(CudaPackage.cuda_arch_values, 2):
var/spack/repos/builtin/packages/paraview/package.py:            "cuda_arch={0}".format(_arch),
var/spack/repos/builtin/packages/paraview/package.py:            when="cuda_arch={0}".format(_other_arch),
var/spack/repos/builtin/packages/paraview/package.py:    depends_on("cmake@3.21:", type="build", when="+rocm")
var/spack/repos/builtin/packages/paraview/package.py:    depends_on("hip@5.2:", when="+rocm")
var/spack/repos/builtin/packages/paraview/package.py:    for target in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/paraview/package.py:            "kokkos@:3.7.01 +rocm amdgpu_target={0}".format(target),
var/spack/repos/builtin/packages/paraview/package.py:            when="+rocm amdgpu_target={0}".format(target),
var/spack/repos/builtin/packages/paraview/package.py:    # Patches to vendored VTK-m are needed for forward compat with CUDA 12 (mr 2972 and 3259)
var/spack/repos/builtin/packages/paraview/package.py:    depends_on("cuda@:11", when="+cuda")
var/spack/repos/builtin/packages/paraview/package.py:            cmake_args.append("-DPARAVIEW_USE_CUDA:BOOL=%s" % variant_bool("+cuda"))
var/spack/repos/builtin/packages/paraview/package.py:            cmake_args.append("-DVTK_USE_CUDA:BOOL=%s" % variant_bool("+cuda"))
var/spack/repos/builtin/packages/paraview/package.py:            cmake_args.append("-DVTKm_ENABLE_CUDA:BOOL=%s" % variant_bool("+cuda"))
var/spack/repos/builtin/packages/paraview/package.py:        # VTK-m expects cuda_arch to be the arch name vs. the arch version.
var/spack/repos/builtin/packages/paraview/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/paraview/package.py:                        "CMAKE_CUDA_ARCHITECTURES", ";".join(spec.variants["cuda_arch"].value)
var/spack/repos/builtin/packages/paraview/package.py:                requested_arch = spec.variants["cuda_arch"].value[0]
var/spack/repos/builtin/packages/paraview/package.py:                    cuda_arch_value = "native"
var/spack/repos/builtin/packages/paraview/package.py:                        cuda_arch_value = supported_cuda_archs[requested_arch]
var/spack/repos/builtin/packages/paraview/package.py:                        raise InstallError("Incompatible cuda_arch=" + requested_arch)
var/spack/repos/builtin/packages/paraview/package.py:                cmake_args.append(self.define("VTKm_CUDA_Architecture", cuda_arch_value))
var/spack/repos/builtin/packages/paraview/package.py:            cmake_args.append("-DPARAVIEW_PLUGIN_ENABLE_pvNVIDIAIndeX:BOOL=ON")
var/spack/repos/builtin/packages/paraview/package.py:            cmake_args.append("-DPARAVIEW_USE_HIP:BOOL=%s" % variant_bool("+rocm"))
var/spack/repos/builtin/packages/paraview/package.py:            if "+rocm" in spec:
var/spack/repos/builtin/packages/paraview/package.py:                archs = spec.variants["amdgpu_target"].value
var/spack/repos/builtin/packages/py-flash-attn/package.py:        depends_on("py-torch+cuda")
var/spack/repos/builtin/packages/opencl-clhpp/package.py:class OpenclClhpp(CMakePackage):
var/spack/repos/builtin/packages/opencl-clhpp/package.py:    """C++ headers for OpenCL development"""
var/spack/repos/builtin/packages/opencl-clhpp/package.py:    homepage = "https://www.khronos.org/registry/OpenCL/"
var/spack/repos/builtin/packages/opencl-clhpp/package.py:    url = "https://github.com/KhronosGroup/OpenCL-CLHPP/archive/v2.0.12.tar.gz"
var/spack/repos/builtin/packages/opencl-clhpp/package.py:            ln("-s", prefix.include.CL, prefix.include.OpenCL)
var/spack/repos/builtin/packages/mpich/package.py:class Mpich(AutotoolsPackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/mpich/package.py:            depends_on("yaksa+cuda", when="+cuda")
var/spack/repos/builtin/packages/mpich/package.py:            depends_on("yaksa+rocm", when="+rocm")
var/spack/repos/builtin/packages/mpich/package.py:    conflicts("datatype-engine=dataloop", when="+cuda")
var/spack/repos/builtin/packages/mpich/package.py:    conflicts("datatype-engine=dataloop", when="+rocm")
var/spack/repos/builtin/packages/mpich/package.py:    # Todo: cuda can be a conditional variant, but it does not seem to work when
var/spack/repos/builtin/packages/mpich/package.py:    # overriding the variant from CudaPackage.
var/spack/repos/builtin/packages/mpich/package.py:    conflicts("+cuda", when="@:3.3")
var/spack/repos/builtin/packages/mpich/package.py:    conflicts("+cuda", when="device=ch3")
var/spack/repos/builtin/packages/mpich/package.py:    conflicts("+cuda", when="device=ch3:sock")
var/spack/repos/builtin/packages/mpich/package.py:    conflicts("+rocm", when="@:4.0")
var/spack/repos/builtin/packages/mpich/package.py:    conflicts("+rocm", when="device=ch3")
var/spack/repos/builtin/packages/mpich/package.py:    conflicts("+rocm", when="device=ch3:sock")
var/spack/repos/builtin/packages/mpich/package.py:    conflicts("+cuda", when="+rocm", msg="CUDA must be disabled to support ROCm")
var/spack/repos/builtin/packages/mpich/package.py:        if "+cuda" in spec:
var/spack/repos/builtin/packages/mpich/package.py:            config_args.append("--with-cuda={0}".format(spec["cuda"].prefix))
var/spack/repos/builtin/packages/mpich/package.py:            # Versions from 3.4 to 3.4.3 cannot handle --without-cuda
var/spack/repos/builtin/packages/mpich/package.py:            config_args.append("--without-cuda")
var/spack/repos/builtin/packages/mpich/package.py:        if "+rocm" in spec:
var/spack/repos/builtin/packages/ompss/package.py:    like GPUs). However, it can also be understood as new directives
var/spack/repos/builtin/packages/ompss/package.py:    extending other accelerator based APIs like CUDA or OpenCL. Our
var/spack/repos/builtin/packages/pastix/package.py:class Pastix(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/pastix/package.py:    variant("cuda", default=False, when="runtime=starpu", description="Enable CUDA")
var/spack/repos/builtin/packages/pastix/package.py:        depends_on("starpu~cuda", when="~cuda")
var/spack/repos/builtin/packages/pastix/package.py:        depends_on("starpu+cuda", when="+cuda")
var/spack/repos/builtin/packages/pastix/package.py:        depends_on("cuda", when="+cuda")
var/spack/repos/builtin/packages/pastix/package.py:            args.extend([self.define_from_variant("PASTIX_WITH_CUDA", "cuda")])
var/spack/repos/builtin/packages/py-chainforgecodegen/package.py:    into a single GPU kernel, holding intermediate results in shared memory as long as necessary.
var/spack/repos/builtin/packages/zerosum/package.py:    including GPU utilization.
var/spack/repos/builtin/packages/zerosum/package.py:    variant("cuda", default=False, description="Enable CUDA support")
var/spack/repos/builtin/packages/zerosum/package.py:    depends_on("rocm-smi-lib", when="+hip")
var/spack/repos/builtin/packages/zerosum/package.py:    depends_on("cuda", when="+cuda")
var/spack/repos/builtin/packages/zerosum/package.py:            self.define_from_variant("ZeroSum_WITH_CUDA", "cuda"),
var/spack/repos/builtin/packages/zerosum/package.py:        if "+cuda" in self.spec:
var/spack/repos/builtin/packages/zerosum/package.py:            args.append(self.define("CUDAToolkit_ROOT", self.spec["cuda"].prefix))
var/spack/repos/builtin/packages/zerosum/package.py:            args.append(self.define("ROCM_ROOT}", self.spec["hip"].prefix))
var/spack/repos/builtin/packages/py-jaxlib/package.py:rocm_dependencies = [
var/spack/repos/builtin/packages/py-jaxlib/package.py:    "rocminfo",
var/spack/repos/builtin/packages/py-jaxlib/package.py:class PyJaxlib(PythonPackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/py-jaxlib/package.py:    variant("cuda", default=True, description="Build with CUDA enabled")
var/spack/repos/builtin/packages/py-jaxlib/package.py:    variant("nccl", default=True, description="Build with NCCL enabled", when="+cuda")
var/spack/repos/builtin/packages/py-jaxlib/package.py:    with when("+cuda"):
var/spack/repos/builtin/packages/py-jaxlib/package.py:        depends_on("cuda@12.1:", when="@0.4.26:")
var/spack/repos/builtin/packages/py-jaxlib/package.py:        depends_on("cuda@11.8:", when="@0.4.11:")
var/spack/repos/builtin/packages/py-jaxlib/package.py:        depends_on("cuda@11.4:", when="@0.4.0:0.4.7")
var/spack/repos/builtin/packages/py-jaxlib/package.py:    with when("+nccl"):
var/spack/repos/builtin/packages/py-jaxlib/package.py:        depends_on("nccl@2.18:", when="@0.4.26:")
var/spack/repos/builtin/packages/py-jaxlib/package.py:        depends_on("nccl@2.16:", when="@0.4.18:")
var/spack/repos/builtin/packages/py-jaxlib/package.py:        depends_on("nccl")
var/spack/repos/builtin/packages/py-jaxlib/package.py:    with when("+rocm"):
var/spack/repos/builtin/packages/py-jaxlib/package.py:        for pkg_dep in rocm_dependencies:
var/spack/repos/builtin/packages/py-jaxlib/package.py:        "cuda_arch=none",
var/spack/repos/builtin/packages/py-jaxlib/package.py:        when="+cuda",
var/spack/repos/builtin/packages/py-jaxlib/package.py:        msg="Must specify CUDA compute capabilities of your GPU, see "
var/spack/repos/builtin/packages/py-jaxlib/package.py:        "https://developer.nvidia.com/cuda-gpus",
var/spack/repos/builtin/packages/py-jaxlib/package.py:        if "+cuda" in spec:
var/spack/repos/builtin/packages/py-jaxlib/package.py:            args.append("--enable_cuda")
var/spack/repos/builtin/packages/py-jaxlib/package.py:            args.append("--cuda_path={0}".format(self.spec["cuda"].prefix))
var/spack/repos/builtin/packages/py-jaxlib/package.py:            capabilities = CudaPackage.compute_capabilities(spec.variants["cuda_arch"].value)
var/spack/repos/builtin/packages/py-jaxlib/package.py:            args.append("--cuda_compute_capabilities={0}".format(",".join(capabilities)))
var/spack/repos/builtin/packages/py-jaxlib/package.py:        if "+rocm" in spec:
var/spack/repos/builtin/packages/py-jaxlib/package.py:            args.append("--enable_rocm")
var/spack/repos/builtin/packages/py-jaxlib/package.py:            args.append("--rocm_path={0}".format(self.spec["hip"].prefix))
var/spack/repos/builtin/packages/py-jaxlib/jaxxlatsl.patch:+ .../tsl/third_party/absl/nvidia_jetson.patch  | 35 +++++++++++++++++++
var/spack/repos/builtin/packages/py-jaxlib/jaxxlatsl.patch:+ create mode 100644 third_party/tsl/third_party/absl/nvidia_jetson.patch
var/spack/repos/builtin/packages/py-jaxlib/jaxxlatsl.patch:+diff --git a/third_party/tsl/third_party/absl/nvidia_jetson.patch b/third_party/tsl/third_party/absl/nvidia_jetson.patch
var/spack/repos/builtin/packages/py-jaxlib/jaxxlatsl.patch:++++ b/third_party/tsl/third_party/absl/nvidia_jetson.patch
var/spack/repos/builtin/packages/py-jaxlib/jaxxlatsl.patch:++Subject: [PATCH] PR #1732: Fix build on NVIDIA Jetson board. Fix #1665
var/spack/repos/builtin/packages/py-jaxlib/jaxxlatsl.patch:++Fix build on NVIDIA Jetson board. Fix #1665
var/spack/repos/builtin/packages/py-jaxlib/jaxxlatsl.patch:++ // https://llvm.org/docs/CompileCudaWithLLVM.html#detecting-clang-vs-nvcc-from-code
var/spack/repos/builtin/packages/py-jaxlib/jaxxlatsl.patch:++-#elif defined(__ARM_NEON) && !defined(__CUDA_ARCH__)
var/spack/repos/builtin/packages/py-jaxlib/jaxxlatsl.patch:+++#elif defined(__ARM_NEON) && !(defined(__NVCC__) && defined(__CUDACC__))
var/spack/repos/builtin/packages/py-jaxlib/jaxxlatsl.patch:++        patch_file = ["//third_party/absl:nvidia_jetson.patch"],
var/spack/repos/builtin/packages/templight/package.py:        targets = ["NVPTX", "AMDGPU"]
var/spack/repos/builtin/packages/cmdstan/package.py:    variant("opencl", default=False, description="enable OpenCl support")
var/spack/repos/builtin/packages/cmdstan/package.py:    depends_on("opencl", when="+opencl")
var/spack/repos/builtin/packages/cmdstan/package.py:        if spec.satisfies("+opencl"):
var/spack/repos/builtin/packages/cmdstan/package.py:            make_options.append("STAN_OPENCL=true\n")
var/spack/repos/builtin/packages/hip/0014-remove-compiler-rt-linkage-for-host.5.5.0.patch:Subject: [PATCH] update the llvm-path and rocm-info path based on install
var/spack/repos/builtin/packages/hip/0014-remove-compiler-rt-linkage-for-host.5.5.0.patch: prefix for llvm-amdgpu and rocminfo;remove compiler rt builtin linkage for
var/spack/repos/builtin/packages/hip/0014-remove-compiler-rt-linkage-for-host.5.5.0.patch:             $targetsStr = $ENV{HCC_AMDGPU_TARGET};
var/spack/repos/builtin/packages/hip/0014-remove-compiler-rt-linkage-for-host.5.5.0.patch:             # Else try using rocm_agent_enumerator
var/spack/repos/builtin/packages/hip/0014-remove-compiler-rt-linkage-for-host.5.5.0.patch:-            $ROCM_AGENT_ENUM = "${ROCM_PATH}/bin/rocm_agent_enumerator";
var/spack/repos/builtin/packages/hip/0014-remove-compiler-rt-linkage-for-host.5.5.0.patch:+	    $ROCMINFO_PATH = $ENV{'ROCMINFO_PATH'} // $ROCMINFO_PATH;
var/spack/repos/builtin/packages/hip/0014-remove-compiler-rt-linkage-for-host.5.5.0.patch:+            $ROCM_AGENT_ENUM = "${ROCMINFO_PATH}/bin/rocm_agent_enumerator";
var/spack/repos/builtin/packages/hip/0014-remove-compiler-rt-linkage-for-host.5.5.0.patch:             $targetsStr = `${ROCM_AGENT_ENUM} -t GPU`;
var/spack/repos/builtin/packages/hip/0013-remove-compiler-rt-linkage-for-host.5.3.0.patch:             $targetsStr = $ENV{HCC_AMDGPU_TARGET};
var/spack/repos/builtin/packages/hip/0013-remove-compiler-rt-linkage-for-host.5.3.0.patch:             # Else try using rocm_agent_enumerator
var/spack/repos/builtin/packages/hip/0013-remove-compiler-rt-linkage-for-host.5.3.0.patch:-            $ROCM_AGENT_ENUM = "${ROCM_PATH}/bin/rocm_agent_enumerator";
var/spack/repos/builtin/packages/hip/0013-remove-compiler-rt-linkage-for-host.5.3.0.patch:+            $ROCMINFO_PATH = $ENV{'ROCMINFO_PATH'} // $ROCM_PATH;
var/spack/repos/builtin/packages/hip/0013-remove-compiler-rt-linkage-for-host.5.3.0.patch:+            $ROCM_AGENT_ENUM = "${ROCMINFO_PATH}/bin/rocm_agent_enumerator";
var/spack/repos/builtin/packages/hip/0013-remove-compiler-rt-linkage-for-host.5.3.0.patch:             $targetsStr = `${ROCM_AGENT_ENUM} -t GPU`;
var/spack/repos/builtin/packages/hip/package.py:    create portable applications for AMD and NVIDIA GPUs from
var/spack/repos/builtin/packages/hip/package.py:    homepage = "https://github.com/ROCm/HIP"
var/spack/repos/builtin/packages/hip/package.py:    git = "https://github.com/ROCm/HIP.git"
var/spack/repos/builtin/packages/hip/package.py:    url = "https://github.com/ROCm/HIP/archive/rocm-6.1.2.tar.gz"
var/spack/repos/builtin/packages/hip/package.py:    tags = ["rocm"]
var/spack/repos/builtin/packages/hip/package.py:    variant("rocm", default=True, description="Enable ROCm support")
var/spack/repos/builtin/packages/hip/package.py:    variant("cuda", default=False, description="Build with CUDA")
var/spack/repos/builtin/packages/hip/package.py:    conflicts("+cuda +rocm", msg="CUDA and ROCm support are mutually exclusive")
var/spack/repos/builtin/packages/hip/package.py:    conflicts("~cuda ~rocm", msg="CUDA or ROCm support is required")
var/spack/repos/builtin/packages/hip/package.py:    conflicts("~rocm +asan", msg="ROCm must be enabled for asan")
var/spack/repos/builtin/packages/hip/package.py:    depends_on("cuda", when="+cuda")
var/spack/repos/builtin/packages/hip/package.py:    with when("+rocm"):
var/spack/repos/builtin/packages/hip/package.py:            depends_on(f"llvm-amdgpu@{ver} +rocm-device-libs", when=f"@{ver}")
var/spack/repos/builtin/packages/hip/package.py:            depends_on(f"rocminfo@{ver}", when=f"@{ver}")
var/spack/repos/builtin/packages/hip/package.py:            depends_on(f"rocm-core@{ver}", when=f"@{ver}")
var/spack/repos/builtin/packages/hip/package.py:        # ref https://github.com/ROCm/HIP/pull/2202
var/spack/repos/builtin/packages/hip/package.py:            url=f"https://github.com/ROCm/hipamd/archive/rocm-{d_version}.tar.gz",
var/spack/repos/builtin/packages/hip/package.py:    # Add opencl sources thru the below
var/spack/repos/builtin/packages/hip/package.py:            name="opencl",
var/spack/repos/builtin/packages/hip/package.py:            url=f"https://github.com/ROCm/ROCm-OpenCL-Runtime/archive/rocm-{d_version}.tar.gz",
var/spack/repos/builtin/packages/hip/package.py:            placement="opencl",
var/spack/repos/builtin/packages/hip/package.py:            url=f"https://github.com/ROCm/ROCclr/archive/rocm-{d_version}.tar.gz",
var/spack/repos/builtin/packages/hip/package.py:            url=f"https://github.com/ROCm/clr/archive/refs/tags/rocm-{d_version}.tar.gz",
var/spack/repos/builtin/packages/hip/package.py:            "https://github.com/ROCm/clr/commit/c4f773db0b4ccbbeed4e3d6c0f6bff299c2aa3f0.patch?full_index=1",
var/spack/repos/builtin/packages/hip/package.py:            "https://github.com/ROCm/clr/commit/7868876db742fb4d44483892856a66d2993add03.patch?full_index=1",
var/spack/repos/builtin/packages/hip/package.py:            url=f"https://github.com/ROCm/HIPCC/archive/refs/tags/rocm-{d_version}.tar.gz",
var/spack/repos/builtin/packages/hip/package.py:            url=f"https://github.com/ROCm/hipother/archive/refs/tags/rocm-{d_version}.tar.gz",
var/spack/repos/builtin/packages/hip/package.py:            when=f"@{d_version} +cuda",
var/spack/repos/builtin/packages/hip/package.py:            url=f"https://github.com/ROCm/hip-tests/archive/refs/tags/rocm-{d_version}.tar.gz",
var/spack/repos/builtin/packages/hip/package.py:    # See https://github.com/ROCm/HIP/pull/3206
var/spack/repos/builtin/packages/hip/package.py:        "https://github.com/ROCm/HIP/commit/50ee82f6bc4aad10908ce09198c9f7ebfb2a3561.patch?full_index=1",
var/spack/repos/builtin/packages/hip/package.py:            # We assume self.spec.prefix is  /opt/rocm-x.y.z for rocm-5.2.0 and newer
var/spack/repos/builtin/packages/hip/package.py:            # and /opt/rocm-x.y.z/hip for older versions
var/spack/repos/builtin/packages/hip/package.py:                    rocm_prefix = Prefix(self.spec.prefix)
var/spack/repos/builtin/packages/hip/package.py:                    rocm_prefix = Prefix(os.path.dirname(self.spec.prefix))
var/spack/repos/builtin/packages/hip/package.py:                # We assume self.spec.prefix is /opt/rocm-x.y.z/hip and rocm has a
var/spack/repos/builtin/packages/hip/package.py:                # /opt/rocm-x.y.z
var/spack/repos/builtin/packages/hip/package.py:                # /opt/rocm-x.y.z/lib tree, it is possible that the package is detected
var/spack/repos/builtin/packages/hip/package.py:                    rocm_prefix = Prefix(self.spec.prefix)
var/spack/repos/builtin/packages/hip/package.py:                    rocm_prefix = Prefix(os.path.dirname(self.spec.prefix))
var/spack/repos/builtin/packages/hip/package.py:            if not os.path.isdir(rocm_prefix):
var/spack/repos/builtin/packages/hip/package.py:                msg = "Could not determine prefix for other rocm components\n"
var/spack/repos/builtin/packages/hip/package.py:                msg += "manually edit rocm_prefix in the package file as "
var/spack/repos/builtin/packages/hip/package.py:                "rocm-path": rocm_prefix,
var/spack/repos/builtin/packages/hip/package.py:                "llvm-amdgpu": rocm_prefix.llvm,
var/spack/repos/builtin/packages/hip/package.py:                "hsa-rocr-dev": rocm_prefix.hsa,
var/spack/repos/builtin/packages/hip/package.py:                "rocminfo": rocm_prefix,
var/spack/repos/builtin/packages/hip/package.py:                "comgr": rocm_prefix,
var/spack/repos/builtin/packages/hip/package.py:                "rocm-device-libs": rocm_prefix,
var/spack/repos/builtin/packages/hip/package.py:                "hipify-clang": rocm_prefix,
var/spack/repos/builtin/packages/hip/package.py:                paths["hip-path"] = rocm_prefix
var/spack/repos/builtin/packages/hip/package.py:                paths["hsa-rocr-dev"] = rocm_prefix
var/spack/repos/builtin/packages/hip/package.py:                "rocm-path": self.spec.prefix,
var/spack/repos/builtin/packages/hip/package.py:                "llvm-amdgpu": self.spec["llvm-amdgpu"].prefix,
var/spack/repos/builtin/packages/hip/package.py:                "rocminfo": self.spec["rocminfo"].prefix,
var/spack/repos/builtin/packages/hip/package.py:                "rocm-device-libs": self.spec["llvm-amdgpu"].prefix,
var/spack/repos/builtin/packages/hip/package.py:        paths["bitcode"] = paths["rocm-device-libs"].amdgcn.bitcode
var/spack/repos/builtin/packages/hip/package.py:        if self.spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/hip/package.py:            # there is a common prefix /opt/rocm-x.y.z.
var/spack/repos/builtin/packages/hip/package.py:            env.set("ROCM_PATH", paths["rocm-path"])
var/spack/repos/builtin/packages/hip/package.py:            env.set("HIP_CLANG_PATH", paths["llvm-amdgpu"].bin)
var/spack/repos/builtin/packages/hip/package.py:            # in a patch of ours since 3.5.0 to locate rocm_agent_enumerator:
var/spack/repos/builtin/packages/hip/package.py:            # https://github.com/ROCm/HIP/pull/2138
var/spack/repos/builtin/packages/hip/package.py:            env.set("ROCMINFO_PATH", paths["rocminfo"])
var/spack/repos/builtin/packages/hip/package.py:            # https://github.com/ROCm/ROCm-CompilerSupport/blob/rocm-4.0.0/lib/comgr/src/comgr-env.cpp
var/spack/repos/builtin/packages/hip/package.py:            env.set("LLVM_PATH", paths["llvm-amdgpu"])
var/spack/repos/builtin/packages/hip/package.py:            # Finally we have to set --rocm-path=<prefix> ourselves, which is not
var/spack/repos/builtin/packages/hip/package.py:            # /opt/rocm again...). If this path is set, there is no strict checking
var/spack/repos/builtin/packages/hip/package.py:            # See also https://github.com/ROCm/HIP/issues/2223
var/spack/repos/builtin/packages/hip/package.py:                "--rocm-path={0}".format(paths["rocm-path"]),
var/spack/repos/builtin/packages/hip/package.py:        elif self.spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/hip/package.py:            env.set("CUDA_PATH", self.spec["cuda"].prefix)
var/spack/repos/builtin/packages/hip/package.py:            env.set("HIP_PLATFORM", "nvidia")
var/spack/repos/builtin/packages/hip/package.py:        # being used to compile. This is only important for external ROCm
var/spack/repos/builtin/packages/hip/package.py:        if "amdgpu_target" in dependent_spec.variants:
var/spack/repos/builtin/packages/hip/package.py:            arch = dependent_spec.variants["amdgpu_target"]
var/spack/repos/builtin/packages/hip/package.py:                env.set("HCC_AMDGPU_TARGET", ",".join(arch.value))
var/spack/repos/builtin/packages/hip/package.py:        if self.spec.satisfies("@5.2:5.4 +rocm"):
var/spack/repos/builtin/packages/hip/package.py:                '"${ROCM_PATH}/llvm"',
var/spack/repos/builtin/packages/hip/package.py:                self.spec["llvm-amdgpu"].prefix,
var/spack/repos/builtin/packages/hip/package.py:        if self.spec.satisfies("@5.6.0:5.6 +rocm"):
var/spack/repos/builtin/packages/hip/package.py:                '"${ROCM_PATH}/llvm"',
var/spack/repos/builtin/packages/hip/package.py:                self.spec["llvm-amdgpu"].prefix,
var/spack/repos/builtin/packages/hip/package.py:        if self.spec.satisfies("@5.7: +rocm"):
var/spack/repos/builtin/packages/hip/package.py:                '"${ROCM_PATH}/llvm"',
var/spack/repos/builtin/packages/hip/package.py:                self.spec["llvm-amdgpu"].prefix,
var/spack/repos/builtin/packages/hip/package.py:                '"${ROCM_PATH}/llvm"',
var/spack/repos/builtin/packages/hip/package.py:                self.spec["llvm-amdgpu"].prefix,
var/spack/repos/builtin/packages/hip/package.py:        if self.spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/hip/package.py:        if self.spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/hip/package.py:                args.append(self.define("HIP_LLVM_ROOT", self.spec["llvm-amdgpu"].prefix))
var/spack/repos/builtin/packages/hip/package.py:                    self.define("CMAKE_C_COMPILER", f"{self.spec['llvm-amdgpu'].prefix}/bin/clang")
var/spack/repos/builtin/packages/hip/package.py:                        "CMAKE_CXX_COMPILER", f"{self.spec['llvm-amdgpu'].prefix}/bin/clang++"
var/spack/repos/builtin/packages/hip/package.py:        if self.spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/hip/package.py:            args.append(self.define("HIP_PLATFORM", "nvidia"))
var/spack/repos/builtin/packages/hip/package.py:            args.append(self.define("AMD_OPENCL_PATH", self.stage.source_path + "opencl"))
var/spack/repos/builtin/packages/hip/package.py:            args.append(self.define("AMD_OPENCL_PATH", self.stage.source_path + "/clr/opencl"))
var/spack/repos/builtin/packages/hip/package.py:        amdclang_path = join_path(self.spec["llvm-amdgpu"].prefix, "bin", "amdclang++")
var/spack/repos/builtin/packages/hip/0014-hip-test-file-reorg-5.4.0.patch:@@ -28,11 +28,15 @@ if (NOT DEFINED ROCM_PATH )
var/spack/repos/builtin/packages/hip/0014-hip-test-file-reorg-5.4.0.patch:      set ( ROCM_PATH "/opt/rocm"  CACHE STRING "Default ROCM installation directory." )
var/spack/repos/builtin/packages/hip/0014-hip-test-file-reorg-5.4.0.patch: # Search for rocm in common locations
var/spack/repos/builtin/packages/hip/0014-hip-test-file-reorg-5.4.0.patch: list(APPEND CMAKE_PREFIX_PATH ${ROCM_PATH}/hip ${ROCM_PATH})
var/spack/repos/builtin/packages/hip/0014-hip-test-file-reorg-5.4.0.patch:-execute_process(COMMAND sh -c "${ROCM_PATH}/hip/bin/hipify-perl ../square.cu > ../square.cpp")
var/spack/repos/builtin/packages/hip/0014-hip-test-file-reorg-5.4.0.patch:-        set(HIP_PATH "${ROCM_PATH}/hip" CACHE PATH "Path to which HIP has been installed")
var/spack/repos/builtin/packages/hip/0014-hip-test-file-reorg-5.4.0.patch:+        set(HIP_PATH "${ROCM_PATH}" CACHE PATH "Path to which HIP has been installed")
var/spack/repos/builtin/packages/hip/0014-hip-test-file-reorg-5.4.0.patch: # Search for rocm in common locations
var/spack/repos/builtin/packages/hip/0014-hip-test-file-reorg-5.4.0.patch:-list(APPEND CMAKE_PREFIX_PATH ${HIP_PATH} ${ROCM_PATH})
var/spack/repos/builtin/packages/hip/0014-hip-test-file-reorg-5.4.0.patch:+list(APPEND CMAKE_PREFIX_PATH ${ROCM_PATH}/hip ${ROCM_PATH})
var/spack/repos/builtin/packages/hip/0014-hip-test-file-reorg-5.4.0.patch: ROCM_PATH?= $(wildcard /opt/rocm/)
var/spack/repos/builtin/packages/hip/0014-hip-test-file-reorg-5.4.0.patch:-HIP_PATH?= $(wildcard $(ROCM_PATH)/hip)
var/spack/repos/builtin/packages/hip/0014-hip-test-file-reorg-5.4.0.patch:+HIP_PATH?= $(ROCM_PATH)
var/spack/repos/builtin/packages/hip/0014-hip-test-file-reorg-5.4.0.patch: ROCM_PATH?= $(wildcard /opt/rocm/)
var/spack/repos/builtin/packages/hip/0014-hip-test-file-reorg-5.4.0.patch:-HIP_PATH?= $(wildcard $(ROCM_PATH)/hip)
var/spack/repos/builtin/packages/hip/0014-hip-test-file-reorg-5.4.0.patch:+HIP_PATH?= $(ROCM_PATH)
var/spack/repos/builtin/packages/hip/0014-remove-compiler-rt-linkage-for-host.5.6.0.patch:             $targetsStr = $ENV{HCC_AMDGPU_TARGET};
var/spack/repos/builtin/packages/hip/0014-remove-compiler-rt-linkage-for-host.5.6.0.patch:             # Else try using rocm_agent_enumerator
var/spack/repos/builtin/packages/hip/0014-remove-compiler-rt-linkage-for-host.5.6.0.patch:-            $ROCM_AGENT_ENUM = "${ROCM_PATH}/bin/rocm_agent_enumerator";
var/spack/repos/builtin/packages/hip/0014-remove-compiler-rt-linkage-for-host.5.6.0.patch:+            $ROCMINFO_PATH = $ENV{'ROCMINFO_PATH'} // $ROCMINFO_PATH;
var/spack/repos/builtin/packages/hip/0014-remove-compiler-rt-linkage-for-host.5.6.0.patch:+            $ROCM_AGENT_ENUM = "${ROCMINFO_PATH}/bin/rocm_agent_enumerator";
var/spack/repos/builtin/packages/hip/0014-remove-compiler-rt-linkage-for-host.5.6.0.patch:             $targetsStr = `${ROCM_AGENT_ENUM} -t GPU`;
var/spack/repos/builtin/packages/hip/0016-hip-sample-fix-hipMalloc-call.patch: target_include_directories(HipOptLibrary PRIVATE /opt/rocm/hsa/include)
var/spack/repos/builtin/packages/hip/0014-Remove-compiler-rt-linkage-for-host-for-5.7.0.patch:             $targetsStr = $ENV{HCC_AMDGPU_TARGET};
var/spack/repos/builtin/packages/hip/0014-Remove-compiler-rt-linkage-for-host-for-5.7.0.patch:             # Else try using rocm_agent_enumerator
var/spack/repos/builtin/packages/hip/0014-Remove-compiler-rt-linkage-for-host-for-5.7.0.patch:-            $ROCM_AGENT_ENUM = "${ROCM_PATH}/bin/rocm_agent_enumerator";
var/spack/repos/builtin/packages/hip/0014-Remove-compiler-rt-linkage-for-host-for-5.7.0.patch:+            $ROCMINFO_PATH = $ENV{'ROCMINFO_PATH'} // $ROCMINFO_PATH;
var/spack/repos/builtin/packages/hip/0014-Remove-compiler-rt-linkage-for-host-for-5.7.0.patch:+            $ROCM_AGENT_ENUM = "${ROCMINFO_PATH}/bin/rocm_agent_enumerator";
var/spack/repos/builtin/packages/hip/0014-Remove-compiler-rt-linkage-for-host-for-5.7.0.patch:             $targetsStr = `${ROCM_AGENT_ENUM} -t GPU`;
var/spack/repos/builtin/packages/fsl/package.py:class Fsl(Package, CudaPackage):
var/spack/repos/builtin/packages/fsl/package.py:    conflicts("cuda_arch=none", when="+cuda", msg="must select a CUDA architecture")
var/spack/repos/builtin/packages/fsl/package.py:    # Allow fsl to use newer versions of cuda
var/spack/repos/builtin/packages/fsl/package.py:        "https://aur.archlinux.org/cgit/aur.git/plain/005-fix_cuda_thrust_include.patch?h=fsl",
var/spack/repos/builtin/packages/fsl/package.py:        build_settings.filter(r"^CUDAVER", "#CUDAVER")
var/spack/repos/builtin/packages/fsl/package.py:        if self.spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/fsl/package.py:            cuda_arch = self.spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/fsl/package.py:            cuda_gencode = " ".join(self.cuda_flags(cuda_arch))
var/spack/repos/builtin/packages/fsl/package.py:            cuda_installation = self.spec["cuda"].prefix
var/spack/repos/builtin/packages/fsl/package.py:                r"(^CUDA_INSTALLATION)\s*=.*", r"\1 = {0}".format(cuda_installation)
var/spack/repos/builtin/packages/fsl/package.py:                r"(^LIB_CUDA)\s*=.*", r"\1 = {0}".format(join_path(cuda_installation, "lib64"))
var/spack/repos/builtin/packages/fsl/package.py:                r"(^INC_CUDA)\s*=.*", r"\1 = {0}".format(join_path(cuda_installation, "include"))
var/spack/repos/builtin/packages/fsl/package.py:                r"(^NVCC11)\s*=.*", r"\1 = {0}".format(join_path(cuda_installation, "bin", "nvcc"))
var/spack/repos/builtin/packages/fsl/package.py:                r"(^NVCC)\s*=.*", r"\1 = {0}".format(join_path(cuda_installation, "bin", "nvcc"))
var/spack/repos/builtin/packages/fsl/package.py:            build_settings.filter(r"(^GENCODE_FLAGS)\s*=.*", r"\1 = {0}".format(cuda_gencode))
var/spack/repos/builtin/packages/fsl/package.py:                build_settings.filter(r"(^EDDYBUILDPARAMETERS)\s*=.*", r'\1 = "cuda=1" "cpu=1"')
var/spack/repos/builtin/packages/fsl/package.py:                build_settings.filter(r"(^fdt_MASTERBUILD)\s*=.*", r"\1 = COMPILE_GPU=1")
var/spack/repos/builtin/packages/fsl/package.py:                build_settings.filter(r"(^ptx2_MASTERBUILD)\s*=.*", r"\1 = COMPILE_GPU=1")
var/spack/repos/builtin/packages/fsl/package.py:                    f.write("COMPILE_GPU=1\n")
var/spack/repos/builtin/packages/fsl/package.py:            build_settings.filter(r"^CUDA_INSTALLATION", "#CUDA_INSTALLATION")
var/spack/repos/builtin/packages/fsl/package.py:            build_settings.filter(r"^LIB_CUDA", "#LIB_CUDA")
var/spack/repos/builtin/packages/fsl/package.py:            build_settings.filter(r"^INC_CUDA", "#INC_CUDA")
var/spack/repos/builtin/packages/fsl/package.py:                build_settings.filter(r"(^fdt_MASTERBUILD)\s*=.*", r"\1 = COMPILE_GPU=0")
var/spack/repos/builtin/packages/fsl/package.py:                build_settings.filter(r"(^ptx2_MASTERBUILD)\s*=.*", r"\1 = COMPILE_GPU=0")
var/spack/repos/builtin/packages/fsl/eddy_Makefile.patch: 	CUDACXXFLAGS=-DCOMPILE_GPU
var/spack/repos/builtin/packages/fsl/eddy_Makefile.patch: 	CUDAOBJS=CBFKernelDefinitions.o CBFSparseDiagonalMatrix.o CBFSplineField.o LSResampler_cuda.o DiffusionGP_cuda.o PostEddyCF_cuda.o EddyGpuUtils.o EddyInternalGpuUtils.o CudaVolume.o EddyMatrixKernels.o EddyKernels.o GpuPredictorChunk.o StackResampler.o DerivativeCalculator.o
var/spack/repos/builtin/packages/fsl/eddy_Makefile.patch:-	CUDALDFLAGS= -Xlinker -rpath $(LIB_CUDA) -L$(LIB_CUDA) -lcublas -lcudart
var/spack/repos/builtin/packages/fsl/eddy_Makefile.patch:+	CUDALDFLAGS= -Xlinker -rpath -Xlinker $(LIB_CUDA) -L$(LIB_CUDA) -lcublas -lcudart
var/spack/repos/builtin/packages/rocwmma/package.py:    and accumulation (MFMA) operations leveraging specialized GPU matrix cores.
var/spack/repos/builtin/packages/rocwmma/package.py:    distributed in parallel across GPU wavefronts. The API is a header library
var/spack/repos/builtin/packages/rocwmma/package.py:    of GPU device code meaning that matrix core acceleration may be compiled directly
var/spack/repos/builtin/packages/rocwmma/package.py:    homepage = "https://github.com/ROCm/rocWMMA"
var/spack/repos/builtin/packages/rocwmma/package.py:    git = "https://github.com/ROCm/rocWMMA.git"
var/spack/repos/builtin/packages/rocwmma/package.py:    url = "https://github.com/ROCm/rocWMMA/archive/refs/tags/rocm-6.1.2.tar.gz"
var/spack/repos/builtin/packages/rocwmma/package.py:    tags = ["rocm"]
var/spack/repos/builtin/packages/rocwmma/package.py:    amdgpu_targets = ("gfx908:xnack-", "gfx90a", "gfx90a:xnack-", "gfx90a:xnack+")
var/spack/repos/builtin/packages/rocwmma/package.py:        "amdgpu_target",
var/spack/repos/builtin/packages/rocwmma/package.py:        description="AMD GPU architecture",
var/spack/repos/builtin/packages/rocwmma/package.py:        values=auto_or_any_combination_of(*amdgpu_targets),
var/spack/repos/builtin/packages/rocwmma/package.py:        depends_on("rocm-cmake@%s:" % ver, type="build", when="@" + ver)
var/spack/repos/builtin/packages/rocwmma/package.py:        depends_on("llvm-amdgpu@" + ver, type="build", when="@" + ver)
var/spack/repos/builtin/packages/rocwmma/package.py:        depends_on("rocm-openmp-extras@" + ver, type="build", when="@" + ver)
var/spack/repos/builtin/packages/rocwmma/package.py:        depends_on("rocm-smi-lib@" + ver, when="@" + ver)
var/spack/repos/builtin/packages/rocwmma/package.py:    for tgt in itertools.chain(["auto"], amdgpu_targets):
var/spack/repos/builtin/packages/rocwmma/package.py:        depends_on("rocblas amdgpu_target={0}".format(tgt), when="amdgpu_target={0}".format(tgt))
var/spack/repos/builtin/packages/rocwmma/package.py:    patch("0001-add-rocm-smi-lib-path-for-building-tests.patch", when="@5.6:")
var/spack/repos/builtin/packages/rocwmma/package.py:                    self.spec["rocm-openmp-extras"].prefix
var/spack/repos/builtin/packages/rocwmma/package.py:        tgt = self.spec.variants["amdgpu_target"]
var/spack/repos/builtin/packages/rocwmma/package.py:            args.append(self.define_from_variant("AMDGPU_TARGETS", "amdgpu_target"))
var/spack/repos/builtin/packages/rocwmma/package.py:            args.append(self.define("ROCM_SMI_DIR", self.spec["rocm-smi-lib"].prefix))
var/spack/repos/builtin/packages/rocwmma/0001-add-rocm-smi-lib-path-for-building-tests.patch:Subject: [PATCH] add rocm-smi-lib path for building tests
var/spack/repos/builtin/packages/rocwmma/0001-add-rocm-smi-lib-path-for-building-tests.patch:+  target_link_libraries(${TEST_TARGET} rocwmma gtest ${ROCM_SMI_DIR}/lib)
var/spack/repos/builtin/packages/rocwmma/0001-add-rocm-smi-lib-path-for-building-tests.patch:+			     ${ROCM_SMI_DIR}/include)
var/spack/repos/builtin/packages/py-cuml/package.py:    depends_on("cuda")
var/spack/repos/builtin/packages/py-cuml/package.py:        depends_on("libcumlprims@0.15.0-cuda{0}_gdbd0d39_0".format(v), when="^cuda@{0}".format(v))
var/spack/repos/builtin/packages/isaac/package.py:    variant("cuda", default=True, description="Generate CUDA kernels for Nvidia GPUs")
var/spack/repos/builtin/packages/isaac/package.py:    #         description='Generate kernels via Alpaka, for CPUs or GPUs')
var/spack/repos/builtin/packages/isaac/package.py:    depends_on("boost@1.65.1:", type="link", when="^cuda@9:")
var/spack/repos/builtin/packages/isaac/package.py:    depends_on("cuda@7.0:", type="link", when="+cuda")
var/spack/repos/builtin/packages/rocrand/package.py:    homepage = "https://github.com/ROCm/rocRAND"
var/spack/repos/builtin/packages/rocrand/package.py:    git = "https://github.com/ROCm/rocRAND.git"
var/spack/repos/builtin/packages/rocrand/package.py:    url = "https://github.com/ROCm/rocRAND/archive/rocm-6.0.2.tar.gz"
var/spack/repos/builtin/packages/rocrand/package.py:    tags = ["rocm"]
var/spack/repos/builtin/packages/rocrand/package.py:    amdgpu_targets = ROCmPackage.amdgpu_targets
var/spack/repos/builtin/packages/rocrand/package.py:        "amdgpu_target",
var/spack/repos/builtin/packages/rocrand/package.py:        description="AMD GPU architecture",
var/spack/repos/builtin/packages/rocrand/package.py:        values=auto_or_any_combination_of(*amdgpu_targets),
var/spack/repos/builtin/packages/rocrand/package.py:            git="https://github.com/ROCm/hipRAND.git",
var/spack/repos/builtin/packages/rocrand/package.py:        git="https://github.com/ROCm/hipRAND.git",
var/spack/repos/builtin/packages/rocrand/package.py:        git="https://github.com/ROCm/hipRAND.git",
var/spack/repos/builtin/packages/rocrand/package.py:        depends_on(f"rocm-cmake@{ver}:", type="build", when=f"@{ver}")
var/spack/repos/builtin/packages/rocrand/package.py:            env.set("CC", f"{self.spec['llvm-amdgpu'].prefix}/bin/clang")
var/spack/repos/builtin/packages/rocrand/package.py:            env.set("CXX", f"{self.spec['llvm-amdgpu'].prefix}/bin/clang++")
var/spack/repos/builtin/packages/rocrand/package.py:        if "auto" not in self.spec.variants["amdgpu_target"]:
var/spack/repos/builtin/packages/rocrand/package.py:            args.append(self.define_from_variant("AMDGPU_TARGETS", "amdgpu_target"))
var/spack/repos/builtin/packages/rocrand/package.py:            args.append(self.define("__skip_rocmclang", "ON"))
var/spack/repos/builtin/packages/rocrand/hiprand_prefer_samedir_rocrand.patch: rocm_install(
var/spack/repos/builtin/packages/lvarray/package.py:class Lvarray(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/lvarray/package.py:    depends_on("camp+cuda", when="+cuda")
var/spack/repos/builtin/packages/lvarray/package.py:    depends_on("raja+cuda", when="+cuda")
var/spack/repos/builtin/packages/lvarray/package.py:    # At the moment Umpire doesn't support shared when building with CUDA.
var/spack/repos/builtin/packages/lvarray/package.py:    depends_on("umpire+cuda~shared", when="+umpire+cuda")
var/spack/repos/builtin/packages/lvarray/package.py:    depends_on("chai+raja+cuda", when="+chai+cuda")
var/spack/repos/builtin/packages/lvarray/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/lvarray/package.py:            var = "-".join([var, "cuda"])
var/spack/repos/builtin/packages/lvarray/package.py:            if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/lvarray/package.py:                cfg.write("# Cuda\n")
var/spack/repos/builtin/packages/lvarray/package.py:                cfg.write(cmake_cache_option("ENABLE_CUDA", True))
var/spack/repos/builtin/packages/lvarray/package.py:                cfg.write(cmake_cache_entry("CMAKE_CUDA_STANDARD", 14))
var/spack/repos/builtin/packages/lvarray/package.py:                cudatoolkitdir = spec["cuda"].prefix
var/spack/repos/builtin/packages/lvarray/package.py:                cfg.write(cmake_cache_entry("CUDA_TOOLKIT_ROOT_DIR", cudatoolkitdir))
var/spack/repos/builtin/packages/lvarray/package.py:                cudacompiler = "${CUDA_TOOLKIT_ROOT_DIR}/bin/nvcc"
var/spack/repos/builtin/packages/lvarray/package.py:                cfg.write(cmake_cache_entry("CMAKE_CUDA_COMPILER", cudacompiler))
var/spack/repos/builtin/packages/lvarray/package.py:                cmake_cuda_flags = (
var/spack/repos/builtin/packages/lvarray/package.py:                            cmake_cuda_flags += " -Xcompiler " + compilerArg
var/spack/repos/builtin/packages/lvarray/package.py:                if not spec.satisfies("cuda_arch=none"):
var/spack/repos/builtin/packages/lvarray/package.py:                    cuda_arch = spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/lvarray/package.py:                    cmake_cuda_flags += " -arch sm_{0}".format(cuda_arch[0])
var/spack/repos/builtin/packages/lvarray/package.py:                cfg.write(cmake_cache_string("CMAKE_CUDA_FLAGS", cmake_cuda_flags))
var/spack/repos/builtin/packages/lvarray/package.py:                    cmake_cache_string("CMAKE_CUDA_FLAGS_RELEASE", "-O3 -Xcompiler -O3 -DNDEBUG")
var/spack/repos/builtin/packages/lvarray/package.py:                        "CMAKE_CUDA_FLAGS_RELWITHDEBINFO", "-O3 -g -lineinfo -Xcompiler -O3"
var/spack/repos/builtin/packages/lvarray/package.py:                cfg.write(cmake_cache_string("CMAKE_CUDA_FLAGS_DEBUG", "-O0 -Xcompiler -O0 -g -G"))
var/spack/repos/builtin/packages/lvarray/package.py:                cfg.write(cmake_cache_option("ENABLE_CUDA", False))
var/spack/repos/builtin/packages/scan-for-matches/package.py:        cc("-O", "-o", "scan_for_matches", "ggpunit.c", "scan_for_matches.c")
var/spack/repos/builtin/packages/rocm-bandwidth-test/package.py:class RocmBandwidthTest(CMakePackage):
var/spack/repos/builtin/packages/rocm-bandwidth-test/package.py:    """Test to measure PciE bandwidth on ROCm platforms"""
var/spack/repos/builtin/packages/rocm-bandwidth-test/package.py:    homepage = "https://github.com/ROCm/rocm_bandwidth_test"
var/spack/repos/builtin/packages/rocm-bandwidth-test/package.py:    git = "https://github.com/ROCm/rocm_bandwidth_test.git"
var/spack/repos/builtin/packages/rocm-bandwidth-test/package.py:    url = "https://github.com/ROCm/rocm_bandwidth_test/archive/rocm-6.1.2.tar.gz"
var/spack/repos/builtin/packages/rocm-bandwidth-test/package.py:    tags = ["rocm"]
var/spack/repos/builtin/packages/rocm-bandwidth-test/package.py:        depends_on(f"rocm-core@{ver}", when=f"@{ver}")
var/spack/repos/builtin/packages/cub/package.py:    and other utilities for CUDA kernel programming."""
var/spack/repos/builtin/packages/cub/package.py:    url = "https://github.com/NVIDIA/cub/archive/1.12.0.zip"
var/spack/repos/builtin/packages/cub/package.py:    git = "https://github.com/NVIDIA/cub.git"
var/spack/repos/builtin/packages/hip-tensor/package.py:class HipTensor(CMakePackage, ROCmPackage):
var/spack/repos/builtin/packages/hip-tensor/package.py:    homepage = "https://github.com/ROCm/hipTensor"
var/spack/repos/builtin/packages/hip-tensor/package.py:    git = "https://github.com/ROCm/hipTensor.git"
var/spack/repos/builtin/packages/hip-tensor/package.py:    url = "https://github.com/ROCm/hipTensor/archive/refs/tags/rocm-6.1.2.tar.gz"
var/spack/repos/builtin/packages/hip-tensor/package.py:    tags = ["rocm"]
var/spack/repos/builtin/packages/hip-tensor/package.py:        depends_on(f"rocm-cmake@{ver}", when=f"@{ver}")
var/spack/repos/builtin/packages/py-cudf/package.py:    cuDF is a GPU DataFrame library for loading, joining,
var/spack/repos/builtin/packages/py-cudf/package.py:    depends_on("py-pyarrow+cuda+orc+parquet", type=("build", "run"))
var/spack/repos/builtin/packages/py-cudf/package.py:    depends_on("cuda@10:")
var/spack/repos/builtin/packages/arrayfire/package.py:class Arrayfire(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/arrayfire/package.py:    variant("opencl", default=False, description="Enable OpenCL backend")
var/spack/repos/builtin/packages/arrayfire/package.py:    depends_on("cuda@7.5:", when="+cuda")
var/spack/repos/builtin/packages/arrayfire/package.py:    depends_on("cudnn", when="+cuda")
var/spack/repos/builtin/packages/arrayfire/package.py:    depends_on("opencl", when="+opencl")
var/spack/repos/builtin/packages/arrayfire/package.py:    depends_on("pocl+icd", when="^[virtuals=opencl] pocl")
var/spack/repos/builtin/packages/arrayfire/package.py:    # TODO add more opencl backends:
var/spack/repos/builtin/packages/arrayfire/package.py:    # currently only Cuda backend is enabled
var/spack/repos/builtin/packages/arrayfire/package.py:    # https://github.com/arrayfire/arrayfire/wiki/Build-Instructions-for-Linux#opencl-backend-dependencies
var/spack/repos/builtin/packages/arrayfire/package.py:    conflicts("cuda_arch=none", when="+cuda", msg="CUDA architecture is required")
var/spack/repos/builtin/packages/arrayfire/package.py:        if "cuda" in query_parameters and "+cuda" in self.spec:
var/spack/repos/builtin/packages/arrayfire/package.py:            libraries.append("libafcuda")
var/spack/repos/builtin/packages/arrayfire/package.py:        if "opencl" in query_parameters and "+opencl" in self.spec:
var/spack/repos/builtin/packages/arrayfire/package.py:            libraries.append("libafopencl")
var/spack/repos/builtin/packages/arrayfire/package.py:                self.define_from_variant("AF_BUILD_CUDA", "cuda"),
var/spack/repos/builtin/packages/arrayfire/package.py:                self.define_from_variant("AF_BUILD_OPENCL", "opencl"),
var/spack/repos/builtin/packages/arrayfire/package.py:        if self.spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/arrayfire/package.py:                for arch in self.spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/arrayfire/package.py:            args.append(self.define("CUDA_architecture_build_targets", arch_list))
var/spack/repos/builtin/packages/arrayfire/package.py:                args.append(self.define("USE_OPENCL_MKL", True))
var/spack/repos/builtin/packages/arrayfire/package.py:                args.append(self.define("USE_OPENCL_MKL", False))
var/spack/repos/builtin/packages/libcudf/package.py:    cuDF is a GPU DataFrame library for loading, joining,
var/spack/repos/builtin/packages/libcudf/package.py:    depends_on("cuda@10.0:")
var/spack/repos/builtin/packages/libcudf/package.py:    depends_on("arrow+cuda+orc+parquet")
var/spack/repos/builtin/packages/libcudf/package.py:        # args.append('-DGPU_ARCHES')
var/spack/repos/builtin/packages/kokkos-legacy/package.py:    variant("cuda", default=False, description="enable Cuda backend")
var/spack/repos/builtin/packages/kokkos-legacy/package.py:    # CUDA options
var/spack/repos/builtin/packages/kokkos-legacy/package.py:    variant("force_uvm", default=False, description="set force_uvm Kokkos CUDA option")
var/spack/repos/builtin/packages/kokkos-legacy/package.py:    variant("use_ldg", default=False, description="set use_ldg Kokkos CUDA option")
var/spack/repos/builtin/packages/kokkos-legacy/package.py:    variant("rdc", default=False, description="set rdc Kokkos CUDA option")
var/spack/repos/builtin/packages/kokkos-legacy/package.py:    variant("enable_lambda", default=False, description="set enable_lambda Kokkos CUDA option")
var/spack/repos/builtin/packages/kokkos-legacy/package.py:    gpu_values = (
var/spack/repos/builtin/packages/kokkos-legacy/package.py:    # GPU architecture variant
var/spack/repos/builtin/packages/kokkos-legacy/package.py:        "gpu_arch",
var/spack/repos/builtin/packages/kokkos-legacy/package.py:        values=gpu_values + ("none",),
var/spack/repos/builtin/packages/kokkos-legacy/package.py:        description="Set the GPU architecture to use",
var/spack/repos/builtin/packages/kokkos-legacy/package.py:    # Check that we haven't specified a gpu architecture
var/spack/repos/builtin/packages/kokkos-legacy/package.py:    # without specifying CUDA
var/spack/repos/builtin/packages/kokkos-legacy/package.py:    for p in gpu_values:
var/spack/repos/builtin/packages/kokkos-legacy/package.py:            f"gpu_arch={p}",
var/spack/repos/builtin/packages/kokkos-legacy/package.py:            when="~cuda",
var/spack/repos/builtin/packages/kokkos-legacy/package.py:            msg="Must specify CUDA backend to use a GPU architecture.",
var/spack/repos/builtin/packages/kokkos-legacy/package.py:    # Check that we haven't specified a Kokkos CUDA option
var/spack/repos/builtin/packages/kokkos-legacy/package.py:    # without specifying CUDA
var/spack/repos/builtin/packages/kokkos-legacy/package.py:    conflicts("+force_uvm", when="~cuda", msg="Must enable CUDA to use force_uvm.")
var/spack/repos/builtin/packages/kokkos-legacy/package.py:    conflicts("+use_ldg", when="~cuda", msg="Must enable CUDA to use use_ldg.")
var/spack/repos/builtin/packages/kokkos-legacy/package.py:    conflicts("+rdc", when="~cuda", msg="Must enable CUDA to use rdc.")
var/spack/repos/builtin/packages/kokkos-legacy/package.py:    conflicts("+enable_lambda", when="~cuda", msg="Must enable CUDA to use enable_lambda.")
var/spack/repos/builtin/packages/kokkos-legacy/package.py:    # Check that we haven't asked for a GPU architecture that
var/spack/repos/builtin/packages/kokkos-legacy/package.py:    conflicts("gpu_arch=Volta70", when="@:2.5")
var/spack/repos/builtin/packages/kokkos-legacy/package.py:    conflicts("gpu_arch=Volta72", when="@:2.5")
var/spack/repos/builtin/packages/kokkos-legacy/package.py:    # conflicts on kokkos version and cuda enabled
var/spack/repos/builtin/packages/kokkos-legacy/package.py:        "+cuda",
var/spack/repos/builtin/packages/kokkos-legacy/package.py:        msg="Kokkos build system has issue (#1296) when CUDA enabled"
var/spack/repos/builtin/packages/kokkos-legacy/package.py:    depends_on("cuda", when="+cuda")
var/spack/repos/builtin/packages/kokkos-legacy/package.py:            cuda_options_args = []
var/spack/repos/builtin/packages/kokkos-legacy/package.py:            if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/kokkos-legacy/package.py:                g_args.append(f"--with-cuda={spec['cuda'].prefix}")
var/spack/repos/builtin/packages/kokkos-legacy/package.py:            # GPU architectures
var/spack/repos/builtin/packages/kokkos-legacy/package.py:            gpu_arch = spec.variants["gpu_arch"].value
var/spack/repos/builtin/packages/kokkos-legacy/package.py:            if gpu_arch != "none":
var/spack/repos/builtin/packages/kokkos-legacy/package.py:                arch_args.append(gpu_arch)
var/spack/repos/builtin/packages/kokkos-legacy/package.py:            # CUDA options
var/spack/repos/builtin/packages/kokkos-legacy/package.py:                cuda_options_args.append("force_uvm")
var/spack/repos/builtin/packages/kokkos-legacy/package.py:                cuda_options_args.append("use_ldg")
var/spack/repos/builtin/packages/kokkos-legacy/package.py:                cuda_options_args.append("rdc")
var/spack/repos/builtin/packages/kokkos-legacy/package.py:                cuda_options_args.append("enable_lambda")
var/spack/repos/builtin/packages/kokkos-legacy/package.py:            if cuda_options_args:
var/spack/repos/builtin/packages/kokkos-legacy/package.py:                g_args.append(f"--with-cuda-options={','.join(cuda_options_args)}")
var/spack/repos/builtin/packages/py-heat/package.py:    computations using CPUs, GPUs and distributed cluster systems on top of MPI."""
var/spack/repos/builtin/packages/rccl/package.py:    of standard collective communication routines for GPUs,
var/spack/repos/builtin/packages/rccl/package.py:    homepage = "https://github.com/ROCm/rccl"
var/spack/repos/builtin/packages/rccl/package.py:    git = "https://github.com/ROCm/rccl.git"
var/spack/repos/builtin/packages/rccl/package.py:    url = "https://github.com/ROCm/rccl/archive/rocm-6.1.2.tar.gz"
var/spack/repos/builtin/packages/rccl/package.py:    tags = ["rocm"]
var/spack/repos/builtin/packages/rccl/package.py:    amdgpu_targets = ROCmPackage.amdgpu_targets
var/spack/repos/builtin/packages/rccl/package.py:        "amdgpu_target",
var/spack/repos/builtin/packages/rccl/package.py:        description="AMD GPU architecture",
var/spack/repos/builtin/packages/rccl/package.py:        values=auto_or_any_combination_of(*amdgpu_targets),
var/spack/repos/builtin/packages/rccl/package.py:    patch("0003-Fix-numactl-rocm-smi-path-issue.patch", when="@5.2.3:5.6")
var/spack/repos/builtin/packages/rccl/package.py:    patch("0004-Set-rocm-core-path-for-version-file.patch", when="@6.0:")
var/spack/repos/builtin/packages/rccl/package.py:        depends_on(f"rocm-cmake@{ver}:", type="build", when=f"@{ver}")
var/spack/repos/builtin/packages/rccl/package.py:        depends_on(f"rocm-smi-lib@{ver}", when=f"@{ver}")
var/spack/repos/builtin/packages/rccl/package.py:        depends_on(f"rocm-core@{ver}", when=f"@{ver}")
var/spack/repos/builtin/packages/rccl/package.py:        env.set("ROCMCORE_PATH", self.spec["rocm-core"].prefix)
var/spack/repos/builtin/packages/rccl/package.py:            self.define("ROCM_SMI_DIR", self.spec["rocm-smi-lib"].prefix),
var/spack/repos/builtin/packages/rccl/package.py:        if "auto" not in self.spec.variants["amdgpu_target"]:
var/spack/repos/builtin/packages/rccl/package.py:            args.append(self.define_from_variant("AMDGPU_TARGETS", "amdgpu_target"))
var/spack/repos/builtin/packages/rccl/package.py:            args.append(self.define("__skip_rocmclang", "ON"))
var/spack/repos/builtin/packages/rccl/0004-Set-rocm-core-path-for-version-file.patch: ## Check for ROCm version
var/spack/repos/builtin/packages/rccl/0004-Set-rocm-core-path-for-version-file.patch:-  COMMAND         bash "-c" "cat ${ROCM_PATH}/.info/version"
var/spack/repos/builtin/packages/rccl/0004-Set-rocm-core-path-for-version-file.patch:+  COMMAND         bash "-c" "cat $ENV{ROCMCORE_PATH}/.info/version"
var/spack/repos/builtin/packages/rccl/0004-Set-rocm-core-path-for-version-file.patch:   OUTPUT_VARIABLE rocm_version_string
var/spack/repos/builtin/packages/rccl/0004-Set-rocm-core-path-for-version-file.patch: string(REGEX MATCH "([0-9]+)\\.([0-9]+)\\.([0-9]+)" rocm_version_matches ${rocm_version_string})
var/spack/repos/builtin/packages/hsakmt-roct/package.py:    homepage = "https://github.com/ROCm/ROCT-Thunk-Interface"
var/spack/repos/builtin/packages/hsakmt-roct/package.py:    git = "https://github.com/ROCm/ROCT-Thunk-Interface.git"
var/spack/repos/builtin/packages/hsakmt-roct/package.py:    url = "https://github.com/ROCm/ROCT-Thunk-Interface/archive/rocm-6.1.0.tar.gz"
var/spack/repos/builtin/packages/hsakmt-roct/package.py:    tags = ["rocm"]
var/spack/repos/builtin/packages/hsakmt-roct/package.py:        depends_on(f"llvm-amdgpu@{ver}", type="test", when=f"@{ver}")
var/spack/repos/builtin/packages/hsakmt-roct/package.py:        depends_on(f"rocm-core@{ver}", when=f"@{ver}")
var/spack/repos/builtin/packages/hsakmt-roct/package.py:        depends_on(f"llvm-amdgpu@{ver}", type="test", when=f"@{ver}")
var/spack/repos/builtin/packages/hsakmt-roct/package.py:    # See https://github.com/ROCm/ROCT-Thunk-Interface/issues/72
var/spack/repos/builtin/packages/hsakmt-roct/package.py:                    self.spec["llvm-amdgpu"].prefix,
var/spack/repos/builtin/packages/exago/package.py:class Exago(CMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/exago/package.py:        "+python", when="+ipopt+rocm", msg="Python bindings require -fPIC with Ipopt for rocm."
var/spack/repos/builtin/packages/exago/package.py:    depends_on("cuda", when="+cuda")
var/spack/repos/builtin/packages/exago/package.py:        ("magma", "hiop+cuda"),
var/spack/repos/builtin/packages/exago/package.py:        ("magma", "hiop+rocm"),
var/spack/repos/builtin/packages/exago/package.py:    depends_on("umpire+cuda~shared", when="+raja+cuda ^raja@:0.14")
var/spack/repos/builtin/packages/exago/package.py:    for arch in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/exago/package.py:        cuda_dep = "+cuda cuda_arch={0}".format(arch)
var/spack/repos/builtin/packages/exago/package.py:        depends_on("hiop {0}".format(cuda_dep), when=cuda_dep)
var/spack/repos/builtin/packages/exago/package.py:        depends_on("raja {0}".format(cuda_dep), when="+raja {0}".format(cuda_dep))
var/spack/repos/builtin/packages/exago/package.py:        depends_on("umpire {0}".format(cuda_dep), when="+raja {0}".format(cuda_dep))
var/spack/repos/builtin/packages/exago/package.py:        depends_on("camp {0}".format(cuda_dep), when="+raja {0}".format(cuda_dep))
var/spack/repos/builtin/packages/exago/package.py:    for arch in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/exago/package.py:        rocm_dep = "+rocm amdgpu_target={0}".format(arch)
var/spack/repos/builtin/packages/exago/package.py:        depends_on("hiop {0}".format(rocm_dep), when=rocm_dep)
var/spack/repos/builtin/packages/exago/package.py:        depends_on("raja {0}".format(rocm_dep), when="+raja {0}".format(rocm_dep))
var/spack/repos/builtin/packages/exago/package.py:        depends_on("umpire {0}".format(rocm_dep), when="+raja {0}".format(rocm_dep))
var/spack/repos/builtin/packages/exago/package.py:        depends_on("camp {0}".format(rocm_dep), when="+raja {0}".format(rocm_dep))
var/spack/repos/builtin/packages/exago/package.py:            if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/exago/package.py:                self.define("EXAGO_ENABLE_GPU", "+cuda" in spec or "+rocm" in spec),
var/spack/repos/builtin/packages/exago/package.py:                self.define_from_variant("EXAGO_ENABLE_CUDA", "cuda"),
var/spack/repos/builtin/packages/exago/package.py:                self.define_from_variant("EXAGO_ENABLE_HIP", "rocm"),
var/spack/repos/builtin/packages/exago/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/exago/package.py:            cuda_arch_list = spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/exago/package.py:            if cuda_arch_list[0] != "none":
var/spack/repos/builtin/packages/exago/package.py:                args.append(self.define("CMAKE_CUDA_ARCHITECTURES", cuda_arch_list))
var/spack/repos/builtin/packages/exago/package.py:        # NOTE: if +rocm, some HIP CMake variables may not be set correctly.
var/spack/repos/builtin/packages/exago/package.py:        #         '/opt/rocm-X.Y.Z/llvm/lib/clang/14.0.0/include/'))
var/spack/repos/builtin/packages/exago/package.py:        if spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/exago/package.py:            rocm_arch_list = spec.variants["amdgpu_target"].value
var/spack/repos/builtin/packages/exago/package.py:            if rocm_arch_list[0] != "none":
var/spack/repos/builtin/packages/exago/package.py:                args.append(self.define("GPU_TARGETS", rocm_arch_list))
var/spack/repos/builtin/packages/exago/package.py:                args.append(self.define("AMDGPU_TARGETS", rocm_arch_list))
var/spack/repos/builtin/packages/exago/exago-1.3.0.patch:@@ -222,11 +222,13 @@ endif(EXAGO_ENABLE_GPU)
var/spack/repos/builtin/packages/exago/exago-1.5.0.patch:@@ -217,11 +217,13 @@ endif(EXAGO_ENABLE_GPU)
var/spack/repos/builtin/packages/exago/exago-1.6.0.patch:@@ -217,11 +217,13 @@ endif(EXAGO_ENABLE_GPU)
var/spack/repos/builtin/packages/essl/package.py:    variant("cuda", default=False, description="CUDA acceleration")
var/spack/repos/builtin/packages/essl/package.py:        "+cuda",
var/spack/repos/builtin/packages/essl/package.py:        msg="ESSL+cuda+ilp64 cannot combine CUDA acceleration 64 bit integers",
var/spack/repos/builtin/packages/essl/package.py:        "+cuda",
var/spack/repos/builtin/packages/essl/package.py:        msg="ESSL+cuda threads=none cannot combine CUDA acceleration"
var/spack/repos/builtin/packages/essl/package.py:                    if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/essl/package.py:                        essl_lib = ["libesslsmpcuda"]
var/spack/repos/builtin/packages/szx/package.py:class Szx(CMakePackage, AutotoolsPackage, CudaPackage):
var/spack/repos/builtin/packages/szx/package.py:    conflicts("+cuda", when="@:1.1.0")
var/spack/repos/builtin/packages/szx/package.py:                self.define_from_variant("SZx_BUILD_CUDA", "cuda"),
var/spack/repos/builtin/packages/flecsi/package.py:class Flecsi(CMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/flecsi/package.py:    depends_on("kokkos +cuda +cuda_constexpr +cuda_lambda", when="+kokkos +cuda")
var/spack/repos/builtin/packages/flecsi/package.py:    depends_on("kokkos +rocm", when="+kokkos +rocm")
var/spack/repos/builtin/packages/flecsi/package.py:    depends_on("legion+cuda", when="backend=legion +cuda")
var/spack/repos/builtin/packages/flecsi/package.py:    depends_on("legion+rocm", when="backend=legion +rocm")
var/spack/repos/builtin/packages/flecsi/package.py:    # Propagate cuda_arch requirement to dependencies
var/spack/repos/builtin/packages/flecsi/package.py:    for _flag in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/flecsi/package.py:        depends_on(f"kokkos cuda_arch={_flag}", when=f"+cuda+kokkos cuda_arch={_flag}")
var/spack/repos/builtin/packages/flecsi/package.py:        depends_on(f"legion cuda_arch={_flag}", when=f"backend=legion +cuda cuda_arch={_flag}")
var/spack/repos/builtin/packages/flecsi/package.py:    # Propagate amdgpu_target requirement to dependencies
var/spack/repos/builtin/packages/flecsi/package.py:    for _flag in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/flecsi/package.py:        depends_on(f"kokkos amdgpu_target={_flag}", when=f"+kokkos +rocm amdgpu_target={_flag}")
var/spack/repos/builtin/packages/flecsi/package.py:            f"legion amdgpu_target={_flag}", when=f"backend=legion +rocm amdgpu_target={_flag}"
var/spack/repos/builtin/packages/flecsi/package.py:            if self.spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/graphblast/package.py:class Graphblast(MakefilePackage, CudaPackage):
var/spack/repos/builtin/packages/graphblast/package.py:    """High-Performance Linear Algebra-based Graph Primitives on GPUs"""
var/spack/repos/builtin/packages/graphblast/package.py:    variant("cuda", default=True, description="Build with Cuda support")
var/spack/repos/builtin/packages/graphblast/package.py:    #   gcc@:5.4.0,7.5.0 , boost@1.58.0:1.60.0 , cuda@9:
var/spack/repos/builtin/packages/graphblast/package.py:        "cuda_arch=none",
var/spack/repos/builtin/packages/graphblast/package.py:        when="+cuda",
var/spack/repos/builtin/packages/graphblast/package.py:        msg='Must specify CUDA compute capabilities of your GPU. \
var/spack/repos/builtin/packages/graphblast/package.py:        cuda_arch_list = self.spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/graphblast/package.py:        for i in cuda_arch_list:
var/spack/repos/builtin/packages/matio/package.py:            # workaround anonymous version tag linker error for the NVIDIA
var/spack/repos/builtin/packages/rocsparse/package.py:    implemented on top of AMD's Radeon Open eCosystem Platform ROCm runtime
var/spack/repos/builtin/packages/rocsparse/package.py:    language and optimized for AMD's latest discrete GPUs."""
var/spack/repos/builtin/packages/rocsparse/package.py:    homepage = "https://github.com/ROCm/rocSPARSE"
var/spack/repos/builtin/packages/rocsparse/package.py:    git = "https://github.com/ROCm/rocSPARSE.git"
var/spack/repos/builtin/packages/rocsparse/package.py:    url = "https://github.com/ROCm/rocSPARSE/archive/rocm-6.1.1.tar.gz"
var/spack/repos/builtin/packages/rocsparse/package.py:    tags = ["rocm"]
var/spack/repos/builtin/packages/rocsparse/package.py:    amdgpu_targets = ROCmPackage.amdgpu_targets
var/spack/repos/builtin/packages/rocsparse/package.py:        "amdgpu_target",
var/spack/repos/builtin/packages/rocsparse/package.py:        description="AMD GPU architecture",
var/spack/repos/builtin/packages/rocsparse/package.py:        values=auto_or_any_combination_of(*amdgpu_targets),
var/spack/repos/builtin/packages/rocsparse/package.py:        depends_on(f"rocm-cmake@{ver}:", type="build", when=f"@{ver}")
var/spack/repos/builtin/packages/rocsparse/package.py:            env.set("CC", f"{self.spec['llvm-amdgpu'].prefix}/bin/clang")
var/spack/repos/builtin/packages/rocsparse/package.py:            env.set("CXX", f"{self.spec['llvm-amdgpu'].prefix}/bin/clang++")
var/spack/repos/builtin/packages/rocsparse/package.py:        if "auto" not in self.spec.variants["amdgpu_target"]:
var/spack/repos/builtin/packages/rocsparse/package.py:            args.append(self.define_from_variant("AMDGPU_TARGETS", "amdgpu_target"))
var/spack/repos/builtin/packages/rocsparse/package.py:            args.append(self.define("__skip_rocmclang", "ON"))
var/spack/repos/builtin/packages/apex/package.py:    variant("cuda", default=False, description="Enables CUDA support")
var/spack/repos/builtin/packages/apex/package.py:    variant("hip", default=False, description="Enables ROCm/HIP support")
var/spack/repos/builtin/packages/apex/package.py:    depends_on("cuda", when="+cuda")
var/spack/repos/builtin/packages/apex/package.py:    depends_on("rocm-smi-lib", when="+hip")
var/spack/repos/builtin/packages/apex/package.py:        args.append(self.define_from_variant("APEX_WITH_CUDA", "cuda"))
var/spack/repos/builtin/packages/apex/package.py:            args.append("-DROCM_ROOT={0}".format(spec["hip"].prefix))
var/spack/repos/builtin/packages/apex/package.py:            args.append("-DRSMI_ROOT={0}".format(spec["rocm-smi-lib"].prefix))
var/spack/repos/builtin/packages/hipsparse/package.py:class Hipsparse(CMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/hipsparse/package.py:    homepage = "https://github.com/ROCm/hipSPARSE"
var/spack/repos/builtin/packages/hipsparse/package.py:    git = "https://github.com/ROCm/hipSPARSE.git"
var/spack/repos/builtin/packages/hipsparse/package.py:    url = "https://github.com/ROCm/hipSPARSE/archive/rocm-6.1.1.tar.gz"
var/spack/repos/builtin/packages/hipsparse/package.py:    tags = ["rocm"]
var/spack/repos/builtin/packages/hipsparse/package.py:    # default to an 'auto' variant until amdgpu_targets can be given a better default than 'none'
var/spack/repos/builtin/packages/hipsparse/package.py:    amdgpu_targets = ROCmPackage.amdgpu_targets
var/spack/repos/builtin/packages/hipsparse/package.py:        "amdgpu_target",
var/spack/repos/builtin/packages/hipsparse/package.py:        description="AMD GPU architecture",
var/spack/repos/builtin/packages/hipsparse/package.py:        values=spack.variant.DisjointSetsOfValues(("auto",), ("none",), amdgpu_targets)
var/spack/repos/builtin/packages/hipsparse/package.py:    variant("rocm", default=True, description="Enable ROCm support")
var/spack/repos/builtin/packages/hipsparse/package.py:    conflicts("+cuda +rocm", msg="CUDA and ROCm support are mutually exclusive")
var/spack/repos/builtin/packages/hipsparse/package.py:    conflicts("~cuda ~rocm", msg="CUDA or ROCm support is required")
var/spack/repos/builtin/packages/hipsparse/package.py:    depends_on("hip +cuda", when="+cuda")
var/spack/repos/builtin/packages/hipsparse/package.py:        depends_on(f"rocm-cmake@{ver}:", type="build", when=f"@{ver}")
var/spack/repos/builtin/packages/hipsparse/package.py:        depends_on(f"rocsparse@{ver}", when=f"+rocm @{ver}")
var/spack/repos/builtin/packages/hipsparse/package.py:    for tgt in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/hipsparse/package.py:        depends_on(f"rocsparse amdgpu_target={tgt}", when=f"+rocm amdgpu_target={tgt}")
var/spack/repos/builtin/packages/hipsparse/package.py:        args.append(self.define_from_variant("BUILD_CUDA", "cuda"))
var/spack/repos/builtin/packages/hipsparse/package.py:        # FindHIP.cmake is still used for +cuda
var/spack/repos/builtin/packages/hipsparse/package.py:        if self.spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/hipsparse/0a90ddc4c33ed409a938513b9dbdca8bfad65e06.patch:Subject: [PATCH] Fix cuda compilation (#287)
var/spack/repos/builtin/packages/hipsparse/0a90ddc4c33ed409a938513b9dbdca8bfad65e06.patch:@@ -360,6 +360,7 @@ cusparseSolvePolicy_t hipPolicyToCudaPolicy(hipsparseSolvePolicy_t policy)
var/spack/repos/builtin/packages/hipsparse/0a90ddc4c33ed409a938513b9dbdca8bfad65e06.patch:+#if CUDART_VERSION < 11050
var/spack/repos/builtin/packages/hipsparse/0a90ddc4c33ed409a938513b9dbdca8bfad65e06.patch: cusparseSideMode_t hipSideToCudaSide(hipsparseSideMode_t side)
var/spack/repos/builtin/packages/hipsparse/0a90ddc4c33ed409a938513b9dbdca8bfad65e06.patch:@@ -385,6 +386,7 @@ hipsparseSideMode_t CudaSideToHIPSide(cusparseSideMode_t side)
var/spack/repos/builtin/packages/hipsparse/0a90ddc4c33ed409a938513b9dbdca8bfad65e06.patch: #if CUDART_VERSION > 10000
var/spack/repos/builtin/packages/hipsparse/0a90ddc4c33ed409a938513b9dbdca8bfad65e06.patch: cudaDataType hipDataTypeToCudaDataType(hipDataType datatype)
var/spack/repos/builtin/packages/amgx/package.py:class Amgx(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/amgx/package.py:    NVIDIA GPUs. AmgX provides up to 10x acceleration to the computationally
var/spack/repos/builtin/packages/amgx/package.py:    homepage = "https://developer.nvidia.com/amgx"
var/spack/repos/builtin/packages/amgx/package.py:    url = "https://github.com/nvidia/amgx/archive/v2.1.0.tar.gz"
var/spack/repos/builtin/packages/amgx/package.py:    variant("cuda", default=True, description="Build with CUDA")
var/spack/repos/builtin/packages/amgx/package.py:        if self.spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/amgx/package.py:            args.append("-DWITH_CUDA=ON")
var/spack/repos/builtin/packages/amgx/package.py:            cuda_arch = self.spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/amgx/package.py:            if cuda_arch != "none":
var/spack/repos/builtin/packages/amgx/package.py:                args.append("-DCUDA_ARCH={0}".format(cuda_arch[0]))
var/spack/repos/builtin/packages/amgx/package.py:            args.append("-DWITH_CUDA=OFF")
var/spack/repos/builtin/packages/spla/package.py:    functionality for specific matrix distributions with optional GPU
var/spack/repos/builtin/packages/spla/package.py:    variant("cuda", default=False, description="CUDA backend")
var/spack/repos/builtin/packages/spla/package.py:    variant("rocm", default=False, description="ROCm backend")
var/spack/repos/builtin/packages/spla/package.py:    conflicts("+cuda", when="+rocm", msg="+cuda and +rocm are mutually exclusive")
var/spack/repos/builtin/packages/spla/package.py:    depends_on("cuda", when="+cuda")
var/spack/repos/builtin/packages/spla/package.py:    depends_on("cuda@11:", when="@1.6.0: +cuda")
var/spack/repos/builtin/packages/spla/package.py:    depends_on("hip", when="+rocm")
var/spack/repos/builtin/packages/spla/package.py:    depends_on("rocblas", when="+rocm")
var/spack/repos/builtin/packages/spla/package.py:    conflicts("^rocblas@6.0.0:", when="@:1.5.5 +rocm")
var/spack/repos/builtin/packages/spla/package.py:    conflicts("^hip@6.0.0:", when="@:1.6.0 +rocm")  # v1.6.1 includes fix for hip 6.0
var/spack/repos/builtin/packages/spla/package.py:        if "+cuda" in spec:
var/spack/repos/builtin/packages/spla/package.py:            args += ["-DSPLA_GPU_BACKEND=CUDA"]
var/spack/repos/builtin/packages/spla/package.py:        elif "+rocm" in spec:
var/spack/repos/builtin/packages/spla/package.py:            args += ["-DSPLA_GPU_BACKEND=ROCM"]
var/spack/repos/builtin/packages/spla/package.py:            args += ["-DSPLA_GPU_BACKEND=OFF"]
var/spack/repos/builtin/packages/cutensor/package.py:    """NVIDIA cuTENSOR Library is a GPU-accelerated tensor linear algebra
var/spack/repos/builtin/packages/cutensor/package.py:    homepage = "https://developer.nvidia.com/cutensor"
var/spack/repos/builtin/packages/cutensor/package.py:        cuda_ver = "10.0"
var/spack/repos/builtin/packages/cutensor/package.py:            cuda_ver = "11.0"
var/spack/repos/builtin/packages/cutensor/package.py:            # Add constraints matching CUDA version to cuTensor version
var/spack/repos/builtin/packages/cutensor/package.py:            cuda_req = "cuda@{0}:".format(cuda_ver)
var/spack/repos/builtin/packages/cutensor/package.py:            depends_on(cuda_req, when=cutensor_ver_req)
var/spack/repos/builtin/packages/cutensor/package.py:        # Munge it to match Nvidia's naming scheme
var/spack/repos/builtin/packages/cutensor/package.py:        url = "https://developer.download.nvidia.com/compute/cutensor/redist/libcutensor/{0}/libcutensor-{0}-{1}-archive.tar.xz"
var/spack/repos/builtin/packages/genomeworks/package.py:class Genomeworks(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/genomeworks/package.py:    """SDK for GPU accelerated genome assembly and analysis."""
var/spack/repos/builtin/packages/genomeworks/package.py:    depends_on("cuda@11:", type=("build", "run"))
var/spack/repos/builtin/packages/genomeworks/package.py:    # Disable CUB compilation, as it is already included in CUDA 11.
var/spack/repos/builtin/packages/genomeworks/package.py:    # This patch breaks GenomeWorks with Cuda <11, cuda@11: is
var/spack/repos/builtin/packages/genomeworks/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/genomeworks/package.py:            args.append("-DWITH_CUDA=ON")
var/spack/repos/builtin/packages/genomeworks/package.py:            args.append("-Dgw_cuda_gen_all_arch=ON")
var/spack/repos/builtin/packages/genomeworks/package.py:            cuda_arch = spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/genomeworks/package.py:            if cuda_arch != "none":
var/spack/repos/builtin/packages/genomeworks/package.py:                args.append("-DCUDA_FLAGS=-arch=sm_{0}".format(cuda_arch[0]))
var/spack/repos/builtin/packages/genomeworks/package.py:            args.append("-DWITH_CUDA=OFF")
var/spack/repos/builtin/packages/rocminfo/package.py:class Rocminfo(CMakePackage):
var/spack/repos/builtin/packages/rocminfo/package.py:    """Radeon Open Compute (ROCm) Runtime rocminfo tool"""
var/spack/repos/builtin/packages/rocminfo/package.py:    homepage = "https://github.com/ROCm/rocminfo"
var/spack/repos/builtin/packages/rocminfo/package.py:    git = "https://github.com/ROCm/rocminfo.git"
var/spack/repos/builtin/packages/rocminfo/package.py:    url = "https://github.com/ROCm/rocminfo/archive/rocm-6.1.2.tar.gz"
var/spack/repos/builtin/packages/rocminfo/package.py:    tags = ["rocm"]
var/spack/repos/builtin/packages/rocminfo/package.py:        depends_on(f"rocm-core@{ver}", when=f"@{ver}")
var/spack/repos/builtin/packages/rocminfo/package.py:        return [self.define("ROCM_DIR", self.spec["hsa-rocr-dev"].prefix)]
var/spack/repos/builtin/packages/flux-core/package.py:    variant("cuda", default=False, description="Build dependencies with support for CUDA")
var/spack/repos/builtin/packages/flux-core/package.py:    depends_on("hwloc +cuda", when="+cuda")
var/spack/repos/builtin/packages/nsimd/package.py:            conditional("CUDA", "ROCM", when="@2:"),
var/spack/repos/builtin/packages/opa-psm2/package.py:class OpaPsm2(MakefilePackage, CudaPackage):
var/spack/repos/builtin/packages/opa-psm2/package.py:    depends_on("cuda@8:", when="+cuda")
var/spack/repos/builtin/packages/opa-psm2/package.py:        if "+cuda" in self.spec:
var/spack/repos/builtin/packages/opa-psm2/package.py:            env.set("PSM_CUDA", "1")
var/spack/repos/builtin/packages/mvapich/package.py:    variant("cuda", default=False, description="Enable CUDA extension")
var/spack/repos/builtin/packages/mvapich/package.py:    depends_on("cuda", when="+cuda")
var/spack/repos/builtin/packages/mvapich/package.py:        if "+cuda" in self.spec:
var/spack/repos/builtin/packages/mvapich/package.py:            args.extend(["--enable-cuda", "--with-cuda={0}".format(spec["cuda"].prefix)])
var/spack/repos/builtin/packages/mvapich/package.py:            args.append("--disable-cuda")
var/spack/repos/builtin/packages/lammps/hip_cmake.patch: cmake/Modules/Packages/GPU.cmake | 13 +++----------
var/spack/repos/builtin/packages/lammps/hip_cmake.patch:diff --git a/cmake/Modules/Packages/GPU.cmake b/cmake/Modules/Packages/GPU.cmake
var/spack/repos/builtin/packages/lammps/hip_cmake.patch:--- a/cmake/Modules/Packages/GPU.cmake
var/spack/repos/builtin/packages/lammps/hip_cmake.patch:+++ b/cmake/Modules/Packages/GPU.cmake
var/spack/repos/builtin/packages/lammps/hip_cmake.patch:@@ -412,7 +412,8 @@ elseif(GPU_API STREQUAL "HIP")
var/spack/repos/builtin/packages/lammps/hip_cmake.patch:       set_property(TARGET gpu PROPERTY CXX_STANDARD 14)
var/spack/repos/builtin/packages/lammps/hip_cmake.patch:-    target_include_directories(gpu PRIVATE ${HIP_ROOT_DIR}/../include)
var/spack/repos/builtin/packages/lammps/hip_cmake.patch:+    target_link_libraries(gpu PRIVATE hip::hipcub)
var/spack/repos/builtin/packages/lammps/hip_cmake.patch:     target_compile_definitions(gpu PRIVATE -DUSE_HIP_DEVICE_SORT)
var/spack/repos/builtin/packages/lammps/hip_cmake.patch:@@ -461,30 +462,22 @@ elseif(GPU_API STREQUAL "HIP")
var/spack/repos/builtin/packages/lammps/hip_cmake.patch:   add_executable(hip_get_devices ${LAMMPS_LIB_SOURCE_DIR}/gpu/geryon/ucl_get_devices.cpp)
var/spack/repos/builtin/packages/lammps/hip_cmake.patch:     target_compile_definitions(gpu PRIVATE -D__HIP_PLATFORM_NVCC__)
var/spack/repos/builtin/packages/lammps/hip_cmake.patch:-    target_include_directories(gpu PRIVATE ${HIP_ROOT_DIR}/../include)
var/spack/repos/builtin/packages/lammps/hip_cmake.patch:     target_include_directories(gpu PRIVATE ${CUDA_INCLUDE_DIRS})
var/spack/repos/builtin/packages/lammps/hip_cmake.patch:     target_link_libraries(gpu PRIVATE ${CUDA_LIBRARIES} ${CUDA_CUDA_LIBRARY})
var/spack/repos/builtin/packages/lammps/hip_cmake.patch:     target_include_directories(hip_get_devices PRIVATE ${CUDA_INCLUDE_DIRS})
var/spack/repos/builtin/packages/lammps/hip_cmake.patch:     target_link_libraries(hip_get_devices PRIVATE ${CUDA_LIBRARIES} ${CUDA_CUDA_LIBRARY})
var/spack/repos/builtin/packages/lammps/hip_cmake.patch:     target_compile_definitions(gpu PRIVATE -D__HIP_PLATFORM_HCC__)
var/spack/repos/builtin/packages/lammps/hip_cmake.patch:-    target_include_directories(gpu PRIVATE ${HIP_ROOT_DIR}/../include)
var/spack/repos/builtin/packages/lammps/hip_cmake.patch:     target_compile_definitions(gpu PRIVATE -D__HIP_PLATFORM_AMD__)
var/spack/repos/builtin/packages/lammps/hip_cmake.patch:-    target_include_directories(gpu PRIVATE ${HIP_ROOT_DIR}/../include)
var/spack/repos/builtin/packages/lammps/hip_cmake.patch:   target_link_libraries(lammps PRIVATE gpu)
var/spack/repos/builtin/packages/lammps/package.py:class Lammps(CMakePackage, CudaPackage, ROCmPackage, PythonExtension):
var/spack/repos/builtin/packages/lammps/package.py:    variant("opencl", default=False, description="Build with OpenCL")
var/spack/repos/builtin/packages/lammps/package.py:        "cuda_mps",
var/spack/repos/builtin/packages/lammps/package.py:        description="(CUDA only) Enable tweaks for running "
var/spack/repos/builtin/packages/lammps/package.py:        + "with Nvidia CUDA Multi-process services daemon",
var/spack/repos/builtin/packages/lammps/package.py:        "gpu_precision",
var/spack/repos/builtin/packages/lammps/package.py:        description="Select GPU precision (used by GPU package)",
var/spack/repos/builtin/packages/lammps/package.py:    depends_on("hipfft", when="+kokkos+kspace+rocm fft_kokkos=hipfft")
var/spack/repos/builtin/packages/lammps/package.py:    depends_on("opencl", when="+opencl")
var/spack/repos/builtin/packages/lammps/package.py:    depends_on("hipcub", when="~kokkos +rocm")
var/spack/repos/builtin/packages/lammps/package.py:    depends_on("llvm-amdgpu ", when="+rocm", type="build")
var/spack/repos/builtin/packages/lammps/package.py:    depends_on("rocm-openmp-extras", when="+rocm +openmp", type="build")
var/spack/repos/builtin/packages/lammps/package.py:    # propagate CUDA and ROCm architecture when +kokkos
var/spack/repos/builtin/packages/lammps/package.py:    for arch in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/lammps/package.py:        depends_on("kokkos+cuda cuda_arch=%s" % arch, when="+kokkos+cuda cuda_arch=%s" % arch)
var/spack/repos/builtin/packages/lammps/package.py:    for arch in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/lammps/package.py:            "kokkos+rocm amdgpu_target=%s" % arch, when="+kokkos+rocm amdgpu_target=%s" % arch
var/spack/repos/builtin/packages/lammps/package.py:    conflicts("+cuda", when="+opencl")
var/spack/repos/builtin/packages/lammps/package.py:    conflicts("+rocm", when="+opencl")
var/spack/repos/builtin/packages/lammps/package.py:        "~kokkos+rocm",
var/spack/repos/builtin/packages/lammps/package.py:        msg="ROCm builds of the GPU package not maintained prior to version 20220623",
var/spack/repos/builtin/packages/lammps/package.py:    conflicts("+kokkos+rocm+kspace", when="@:20210929.3")
var/spack/repos/builtin/packages/lammps/package.py:        when="@20220623.3:20230208 +kokkos +rocm +kspace",
var/spack/repos/builtin/packages/lammps/package.py:        when="@20200721 +cuda",
var/spack/repos/builtin/packages/lammps/package.py:    patch("hip_cmake.patch", when="@20220623:20221222 ~kokkos+rocm")
var/spack/repos/builtin/packages/lammps/package.py:            # LAMMPS can be build with the GPU package OR the KOKKOS package
var/spack/repos/builtin/packages/lammps/package.py:            # +cuda only implies that one of the two is used
var/spack/repos/builtin/packages/lammps/package.py:            # by default it will use the GPU package if kokkos wasn't enabled
var/spack/repos/builtin/packages/lammps/package.py:            if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/lammps/package.py:                args.append(self.define("PKG_GPU", True))
var/spack/repos/builtin/packages/lammps/package.py:                args.append(self.define("GPU_API", "cuda"))
var/spack/repos/builtin/packages/lammps/package.py:                args.append(self.define_from_variant("GPU_PREC", "gpu_precision"))
var/spack/repos/builtin/packages/lammps/package.py:                cuda_arch = spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/lammps/package.py:                if cuda_arch != "none":
var/spack/repos/builtin/packages/lammps/package.py:                    args.append(self.define("GPU_ARCH", "sm_{0}".format(cuda_arch[0])))
var/spack/repos/builtin/packages/lammps/package.py:                args.append(self.define_from_variant("CUDA_MPS_SUPPORT", "cuda_mps"))
var/spack/repos/builtin/packages/lammps/package.py:            elif spec.satisfies("+opencl"):
var/spack/repos/builtin/packages/lammps/package.py:                # LAMMPS downloads and bundles its own OpenCL ICD Loader by default
var/spack/repos/builtin/packages/lammps/package.py:                args.append(self.define("USE_STATIC_OPENCL_LOADER", False))
var/spack/repos/builtin/packages/lammps/package.py:                args.append(self.define("PKG_GPU", True))
var/spack/repos/builtin/packages/lammps/package.py:                args.append(self.define("GPU_API", "opencl"))
var/spack/repos/builtin/packages/lammps/package.py:                args.append(self.define_from_variant("GPU_PREC", "gpu_precision"))
var/spack/repos/builtin/packages/lammps/package.py:            elif spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/lammps/package.py:                args.append(self.define("PKG_GPU", True))
var/spack/repos/builtin/packages/lammps/package.py:                args.append(self.define("GPU_API", "hip"))
var/spack/repos/builtin/packages/lammps/package.py:                args.append(self.define_from_variant("GPU_PREC", "gpu_precision"))
var/spack/repos/builtin/packages/lammps/package.py:                args.append(self.define_from_variant("HIP_ARCH", "amdgpu_target"))
var/spack/repos/builtin/packages/lammps/package.py:                args.append(self.define("PKG_GPU", False))
var/spack/repos/builtin/packages/lammps/package.py:        if spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/fmt/package.py:    # Fix compilation with clang in CUDA mode: https://github.com/fmtlib/fmt/issues/3740
var/spack/repos/builtin/packages/hipcub/package.py:class Hipcub(CMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/hipcub/package.py:    homepage = "https://github.com/ROCm/hipCUB"
var/spack/repos/builtin/packages/hipcub/package.py:    git = "https://github.com/ROCm/hipCUB.git"
var/spack/repos/builtin/packages/hipcub/package.py:    url = "https://github.com/ROCm/hipCUB/archive/rocm-6.1.2.tar.gz"
var/spack/repos/builtin/packages/hipcub/package.py:    tags = ["rocm"]
var/spack/repos/builtin/packages/hipcub/package.py:    # default to an 'auto' variant until amdgpu_targets can be given a better default than 'none'
var/spack/repos/builtin/packages/hipcub/package.py:    amdgpu_targets = ROCmPackage.amdgpu_targets
var/spack/repos/builtin/packages/hipcub/package.py:        "amdgpu_target",
var/spack/repos/builtin/packages/hipcub/package.py:        description="AMD GPU architecture",
var/spack/repos/builtin/packages/hipcub/package.py:        values=spack.variant.DisjointSetsOfValues(("auto",), ("none",), amdgpu_targets)
var/spack/repos/builtin/packages/hipcub/package.py:    variant("rocm", default=True, description="Enable ROCm support")
var/spack/repos/builtin/packages/hipcub/package.py:    conflicts("+cuda +rocm", msg="CUDA and ROCm support are mutually exclusive")
var/spack/repos/builtin/packages/hipcub/package.py:    conflicts("~cuda ~rocm", msg="CUDA or ROCm support is required")
var/spack/repos/builtin/packages/hipcub/package.py:        depends_on(f"rocprim@{ver}", when=f"+rocm @{ver}")
var/spack/repos/builtin/packages/hipcub/package.py:        depends_on(f"rocm-cmake@{ver}:", type="build", when=f"@{ver}")
var/spack/repos/builtin/packages/hipcub/package.py:        depends_on(f"hip +cuda@{ver}", when=f"+cuda @{ver}")
var/spack/repos/builtin/packages/hipcub/package.py:    # fix hardcoded search in /opt/rocm and broken config mode search
var/spack/repos/builtin/packages/hipcub/package.py:    patch("find-hip-cuda-rocm-5.3.patch", when="@5.3: +cuda")
var/spack/repos/builtin/packages/hipcub/package.py:        if self.spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/hipcub/package.py:        if self.spec.satisfies("+rocm ^cmake@3.21.0:3.21.2"):
var/spack/repos/builtin/packages/hipcub/package.py:            args.append(self.define("__skip_rocmclang", "ON"))
var/spack/repos/builtin/packages/hipcub/package.py:        # FindHIP.cmake is still used for +cuda
var/spack/repos/builtin/packages/hipcub/package.py:        if self.spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/hipcub/find-hip-cuda-rocm-5.3.patch: list(APPEND CMAKE_PREFIX_PATH /opt/rocm /opt/rocm/hip)
var/spack/repos/builtin/packages/hipcub/find-hip-cuda-rocm-5.3.patch:-    list(APPEND CMAKE_MODULE_PATH /opt/rocm/hip/cmake)
var/spack/repos/builtin/packages/hipcub/find-hip-cuda-rocm-5.3.patch:-    find_package(hip QUIET CONFIG PATHS /opt/rocm)
var/spack/repos/builtin/packages/nwchem/package.py:    variant("tcecuda", default=False, description="Enable TCE CCSD(T) CUDA support")
var/spack/repos/builtin/packages/nwchem/package.py:    depends_on("cuda", when="+tcecuda")
var/spack/repos/builtin/packages/nwchem/package.py:        if spec.satisfies("+tcecuda"):
var/spack/repos/builtin/packages/nwchem/package.py:            args.extend(["TCE_CUDA=y"])
var/spack/repos/builtin/packages/nwchem/package.py:            args.extend(["CUDA_INCLUDE=-I{0}".format(self.spec["cuda"].headers.directories[0])])
var/spack/repos/builtin/packages/nwchem/package.py:            # args.extend(["CUDA_LIBS={0}".format(self.spec["cuda"].libs)])
var/spack/repos/builtin/packages/nwchem/package.py:            args.extend(["CUDA_LIBS=-L{0} -lcudart".format(self.spec["cuda"].libs.directories[0])])
var/spack/repos/builtin/packages/phist/package.py:    in particular support row-major storage of block vectors and using GPUs
var/spack/repos/builtin/packages/phist/package.py:        #    /opt/rocm/llvm/include/iso_fortran_env.mod
var/spack/repos/builtin/packages/hipify-clang/package.py:    """hipify-clang is a clang-based tool for translation CUDA
var/spack/repos/builtin/packages/hipify-clang/package.py:    homepage = "https://github.com/ROCm/HIPIFY"
var/spack/repos/builtin/packages/hipify-clang/package.py:    git = "https://github.com/ROCm/HIPIFY.git"
var/spack/repos/builtin/packages/hipify-clang/package.py:    url = "https://github.com/ROCm/HIPIFY/archive/rocm-6.1.2.tar.gz"
var/spack/repos/builtin/packages/hipify-clang/package.py:    tags = ["rocm"]
var/spack/repos/builtin/packages/hipify-clang/package.py:        depends_on(f"llvm-amdgpu@{ver}", when=f"@{ver}")
var/spack/repos/builtin/packages/hipify-clang/package.py:        depends_on(f"rocm-core@{ver}", when=f"@{ver}")
var/spack/repos/builtin/packages/eztrace/package.py:class Eztrace(CMakePackage, AutotoolsPackage, CudaPackage):
var/spack/repos/builtin/packages/eztrace/package.py:    depends_on("cuda", when="+cuda")
var/spack/repos/builtin/packages/eztrace/package.py:    # CUDA support from 2.1
var/spack/repos/builtin/packages/eztrace/package.py:    conflicts("+cuda", when="@:2.0")
var/spack/repos/builtin/packages/eztrace/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/eztrace/package.py:            args.append(self.define("EZTRACE_ENABLE_CUDA", True))
var/spack/repos/builtin/packages/libcuml/package.py:    depends_on("cuda@9.2:")
var/spack/repos/builtin/packages/libcuml/package.py:    depends_on("nccl@2.4:")
var/spack/repos/builtin/packages/libcuml/package.py:        args.append("-DNCCL_PATH={0}".format(self.spec["nccl"].prefix))
var/spack/repos/builtin/packages/libcuml/package.py:        args.append("-DSINGLEGPU=OFF")
var/spack/repos/builtin/packages/py-nvtx/package.py:    homepage = "https://github.com/NVIDIA/nvtx"
var/spack/repos/builtin/packages/acts/package.py:class Acts(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/acts/package.py:    depends_on("cuda @12:", when="+traccc")
var/spack/repos/builtin/packages/acts/package.py:            plugin_cmake_variant("CUDA", "cuda"),
var/spack/repos/builtin/packages/acts/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/acts/package.py:            cuda_arch = spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/acts/package.py:            if cuda_arch != "none":
var/spack/repos/builtin/packages/acts/package.py:                args.append(f"-DCUDA_FLAGS=-arch=sm_{cuda_arch[0]}")
var/spack/repos/builtin/packages/acts/package.py:                arch_str = ";".join(self.spec.variants["cuda_arch"].value)
var/spack/repos/builtin/packages/acts/package.py:                args.append(self.define("CMAKE_CUDA_ARCHITECTURES", arch_str))
var/spack/repos/builtin/packages/py-dace/package.py:    high-performance CPU, GPU, and FPGA programs, which can be
var/spack/repos/builtin/packages/sympack/package.py:class Sympack(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/sympack/package.py:    with optional support for CUDA devices"""
var/spack/repos/builtin/packages/sympack/package.py:    depends_on("upcxx@2022.3.0:+cuda", when="+cuda")
var/spack/repos/builtin/packages/sympack/package.py:        "cuda",
var/spack/repos/builtin/packages/sympack/package.py:        description="Enables solver to offload large operations to CUDA GPUs",
var/spack/repos/builtin/packages/sympack/package.py:    conflicts("+cuda", when="@:2", msg="symPACK version 3.0 or later required for CUDA support")
var/spack/repos/builtin/packages/sympack/package.py:    depends_on("cuda@6.0:", when="+cuda")
var/spack/repos/builtin/packages/sympack/package.py:            self.define_from_variant("ENABLE_CUDA", "cuda"),
var/spack/repos/builtin/packages/libceed/package.py:class Libceed(MakefilePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/libceed/package.py:    conflicts("+rocm", when="@:0.7")
var/spack/repos/builtin/packages/libceed/package.py:    with when("+rocm"):
var/spack/repos/builtin/packages/libceed/package.py:        depends_on("occa+cuda", when="+cuda")
var/spack/repos/builtin/packages/libceed/package.py:        depends_on("occa~cuda", when="~cuda")
var/spack/repos/builtin/packages/libceed/package.py:    patch("libceed-v0.8-hip.patch", when="@0.8+rocm")
var/spack/repos/builtin/packages/libceed/package.py:            if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/libceed/package.py:                makeopts += ["CUDA_DIR=%s" % spec["cuda"].prefix]
var/spack/repos/builtin/packages/libceed/package.py:                makeopts += ["CUDA_ARCH=sm_%s" % spec.variants["cuda_arch"].value]
var/spack/repos/builtin/packages/libceed/package.py:                # Disable CUDA auto-detection:
var/spack/repos/builtin/packages/libceed/package.py:                makeopts += ["CUDA_DIR=/disable-cuda"]
var/spack/repos/builtin/packages/libceed/package.py:            if spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/libceed/package.py:                amdgpu_target = ",".join(spec.variants["amdgpu_target"].value)
var/spack/repos/builtin/packages/libceed/package.py:                makeopts += ["HIP_ARCH=%s" % amdgpu_target]
var/spack/repos/builtin/packages/nasm/package.py:        # Remove flags not recognized by the NVIDIA compiler
var/spack/repos/builtin/packages/op2-dsl/package.py:class Op2Dsl(MakefilePackage, CudaPackage):
var/spack/repos/builtin/packages/op2-dsl/package.py:        if "+cuda" in self.spec and spec.variants["cuda_arch"].value[0] != "none":
var/spack/repos/builtin/packages/op2-dsl/package.py:            env["CUDA_GEN"] = ",".join(spec.variants["cuda_arch"].value)
var/spack/repos/builtin/packages/redis-ai/package.py:    variant("cuda", default=False, description="Use CUDA")
var/spack/repos/builtin/packages/redis-ai/package.py:    variant("rocm", default=False, description="Use ROCm")
var/spack/repos/builtin/packages/redis-ai/package.py:    conflicts("+cuda+rocm")
var/spack/repos/builtin/packages/redis-ai/package.py:    # GPU deps
var/spack/repos/builtin/packages/redis-ai/package.py:    depends_on("cuda@11.2:", type=("build", "link", "run"), when="+cuda")
var/spack/repos/builtin/packages/redis-ai/package.py:    depends_on("cudnn@8.1:", type=("build", "link", "run"), when="+cuda")
var/spack/repos/builtin/packages/redis-ai/package.py:    with when("+rocm"):
var/spack/repos/builtin/packages/redis-ai/package.py:        depends_on("rocminfo")
var/spack/repos/builtin/packages/redis-ai/package.py:        depends_on("py-torch@1.11.0:~cuda~rocm", type=("build", "link"), when="~cuda~rocm")
var/spack/repos/builtin/packages/redis-ai/package.py:        depends_on("py-torch@1.11.0:+cuda+cudnn~rocm", type=("build", "link"), when="+cuda")
var/spack/repos/builtin/packages/redis-ai/package.py:        depends_on("py-torch@1.11.0:~cuda+rocm", type=("build", "link"), when="+rocm")
var/spack/repos/builtin/packages/redis-ai/package.py:    def use_gpu(self):
var/spack/repos/builtin/packages/redis-ai/package.py:        return self.spec.satisfies("+cuda") or self.spec.satisfies("+rocm")
var/spack/repos/builtin/packages/redis-ai/package.py:            "GPU": "1" if self.use_gpu else "0",
var/spack/repos/builtin/packages/redis-ai/package.py:                # Decide if we want GPU
var/spack/repos/builtin/packages/redis-ai/package.py:                "GPU": "1" if self.use_gpu else "0",
var/spack/repos/builtin/packages/libbeagle/package.py:class Libbeagle(AutotoolsPackage, CudaPackage):
var/spack/repos/builtin/packages/libbeagle/package.py:    depends_on("opencl", when="+opencl")
var/spack/repos/builtin/packages/libbeagle/package.py:    cuda_arch_values = CudaPackage.cuda_arch_values
var/spack/repos/builtin/packages/libbeagle/package.py:    variant("opencl", default=False, description="Include OpenCL (GPU) support")
var/spack/repos/builtin/packages/libbeagle/package.py:        "cuda_arch",
var/spack/repos/builtin/packages/libbeagle/package.py:        description="CUDA architecture",
var/spack/repos/builtin/packages/libbeagle/package.py:        values=("none",) + cuda_arch_values,
var/spack/repos/builtin/packages/libbeagle/package.py:    conflicts("cuda_arch=none", when="+cuda", msg="must select a CUDA architecture")
var/spack/repos/builtin/packages/libbeagle/package.py:        # update cuda architecture if necessary
var/spack/repos/builtin/packages/libbeagle/package.py:        if self.spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/libbeagle/package.py:            cuda_arch = self.spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/libbeagle/package.py:            archflag = "-arch=compute_{0}".format(cuda_arch)
var/spack/repos/builtin/packages/libbeagle/package.py:                "-arch compute_13", "", "libhmsbeagle/GPU/kernels/Makefile.am", string=True
var/spack/repos/builtin/packages/libbeagle/package.py:            # point CUDA_LIBS to libcuda.so
var/spack/repos/builtin/packages/libbeagle/package.py:                "-L$with_cuda/lib", "-L$with_cuda/lib64/stubs", "configure.ac", string=True
var/spack/repos/builtin/packages/libbeagle/package.py:        if self.spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/libbeagle/package.py:            args.append("--with-cuda={0}".format(self.spec["cuda"].prefix))
var/spack/repos/builtin/packages/libbeagle/package.py:            args.append("--without-cuda")
var/spack/repos/builtin/packages/libbeagle/package.py:        if self.spec.satisfies("+opencl"):
var/spack/repos/builtin/packages/libbeagle/package.py:            args.append("--with-opencl={0}".format(self.spec["opencl"].prefix))
var/spack/repos/builtin/packages/libbeagle/package.py:            args.append("--without-opencl")
var/spack/repos/builtin/packages/virtualgl/package.py:    onto a server-side GPU and converts the rendered 3D images into a video
var/spack/repos/builtin/packages/gromacs/package.py:class Gromacs(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/gromacs/package.py:    available and can run on CPUs as well as GPUs. It is free, open source
var/spack/repos/builtin/packages/gromacs/package.py:        when="@2022: +cuda+mpi",
var/spack/repos/builtin/packages/gromacs/package.py:        description="Enable multi-GPU FFT support with cuFFTMp",
var/spack/repos/builtin/packages/gromacs/package.py:        description="Enable multi-GPU FFT support with HeFFTe",
var/spack/repos/builtin/packages/gromacs/package.py:    variant("opencl", default=False, description="Enable OpenCL support")
var/spack/repos/builtin/packages/gromacs/package.py:        "intel-data-center-gpu-max",
var/spack/repos/builtin/packages/gromacs/package.py:        description="Enable support for Intel Data Center GPU Max",
var/spack/repos/builtin/packages/gromacs/package.py:        when="@2024:+mpi+cuda",
var/spack/repos/builtin/packages/gromacs/package.py:        description="Enable NVSHMEM support for Nvidia GPUs",
var/spack/repos/builtin/packages/gromacs/package.py:    depends_on("cuda", when="+cuda")
var/spack/repos/builtin/packages/gromacs/package.py:        if self.spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/gromacs/package.py:            # versions of CUDA library.
var/spack/repos/builtin/packages/gromacs/package.py:            # Hardware version 3.0 is supported up to CUDA 10.2 (Gromacs 4.6-2020.3
var/spack/repos/builtin/packages/gromacs/package.py:            if self.spec.satisfies("@4.6:2020.3^cuda@11:"):
var/spack/repos/builtin/packages/gromacs/package.py:            # Hardware version 2.0 is supported up to CUDA 8 (Gromacs 4.6-2016.3
var/spack/repos/builtin/packages/gromacs/package.py:            if self.spec.satisfies("@4.6:2016.3^cuda@9:"):
var/spack/repos/builtin/packages/gromacs/package.py:            if self.spec.satisfies("@4.6:5.0^cuda@9:"):
var/spack/repos/builtin/packages/gromacs/package.py:            if self.spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/gromacs/package.py:                options.append("-DGMX_GPU:STRING=CUDA")
var/spack/repos/builtin/packages/gromacs/package.py:            elif self.spec.satisfies("+opencl"):
var/spack/repos/builtin/packages/gromacs/package.py:                options.append("-DGMX_GPU:STRING=OpenCL")
var/spack/repos/builtin/packages/gromacs/package.py:                options.append("-DGMX_GPU:STRING=SYCL")
var/spack/repos/builtin/packages/gromacs/package.py:                options.append("-DGMX_GPU:STRING=OFF")
var/spack/repos/builtin/packages/gromacs/package.py:            if self.spec.satisfies("+cuda") or self.spec.satisfies("+opencl"):
var/spack/repos/builtin/packages/gromacs/package.py:                options.append("-DGMX_GPU:BOOL=ON")
var/spack/repos/builtin/packages/gromacs/package.py:                if self.spec.satisfies("+opencl"):
var/spack/repos/builtin/packages/gromacs/package.py:                    options.append("-DGMX_USE_OPENCL=ON")
var/spack/repos/builtin/packages/gromacs/package.py:                options.append("-DGMX_GPU:BOOL=OFF")
var/spack/repos/builtin/packages/gromacs/package.py:        if self.spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/gromacs/package.py:            options.append("-DCUDA_TOOLKIT_ROOT_DIR:STRING=" + self.spec["cuda"].prefix)
var/spack/repos/builtin/packages/gromacs/package.py:            if not self.spec.satisfies("cuda_arch=none"):
var/spack/repos/builtin/packages/gromacs/package.py:                cuda_arch = self.spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/gromacs/package.py:                options.append(f"-DGMX_CUDA_TARGET_SM:STRING={';'.join(cuda_arch)}")
var/spack/repos/builtin/packages/gromacs/package.py:        if self.spec.satisfies("+intel-data-center-gpu-max"):
var/spack/repos/builtin/packages/gromacs/package.py:            options.append("-DGMX_GPU_NB_CLUSTER_SIZE=8")
var/spack/repos/builtin/packages/gromacs/package.py:            options.append("-DGMX_GPU_NB_NUM_CLUSTER_PER_CELL_X=1")
var/spack/repos/builtin/packages/gromacs/package.py:            # Workaround NVIDIA compiler bug when avx512 is enabled
var/spack/repos/builtin/packages/creduce/package.py:    """C-Reduce is a tool that takes a large C, C++, or OpenCL file that has a
var/spack/repos/builtin/packages/cloverleaf3d/package.py:    variant("opencl", default=False, description="Enable OpenCL Support")
var/spack/repos/builtin/packages/cloverleaf3d/package.py:    variant("openacc", default=False, description="Enable OpenACC Support")
var/spack/repos/builtin/packages/cloverleaf3d/package.py:        if self.spec.satisfies("+opencl"):
var/spack/repos/builtin/packages/cloverleaf3d/package.py:            build = "OpenCL"
var/spack/repos/builtin/packages/cloverleaf3d/package.py:        elif self.spec.satisfies("+openacc"):
var/spack/repos/builtin/packages/cloverleaf3d/package.py:            build = "OpenACC"
var/spack/repos/builtin/packages/axom/examples-oneapi.patch:     # When CUDA is enabled, BLT will determine the correct linker, so don't override it here
var/spack/repos/builtin/packages/axom/examples-oneapi.patch:     if (NOT ENABLE_CUDA)
var/spack/repos/builtin/packages/axom/package.py:class Axom(CachedCMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/axom/package.py:    depends_on("cmake@3.21:", type="build", when="+rocm")
var/spack/repos/builtin/packages/axom/package.py:        depends_on("caliper+cuda", when="+cuda")
var/spack/repos/builtin/packages/axom/package.py:        depends_on("caliper~cuda", when="~cuda")
var/spack/repos/builtin/packages/axom/package.py:        depends_on("caliper+rocm", when="+rocm")
var/spack/repos/builtin/packages/axom/package.py:        depends_on("caliper~rocm", when="~rocm")
var/spack/repos/builtin/packages/axom/package.py:    for val in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/axom/package.py:        ext_cuda_dep = f"+cuda cuda_arch={val}"
var/spack/repos/builtin/packages/axom/package.py:        depends_on(f"raja {ext_cuda_dep}", when=f"+raja {ext_cuda_dep}")
var/spack/repos/builtin/packages/axom/package.py:        depends_on(f"umpire {ext_cuda_dep}", when=f"+umpire {ext_cuda_dep}")
var/spack/repos/builtin/packages/axom/package.py:        depends_on(f"caliper {ext_cuda_dep}", when=f"+profiling {ext_cuda_dep}")
var/spack/repos/builtin/packages/axom/package.py:    for val in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/axom/package.py:        ext_rocm_dep = f"+rocm amdgpu_target={val}"
var/spack/repos/builtin/packages/axom/package.py:        depends_on(f"raja {ext_rocm_dep}", when=f"+raja {ext_rocm_dep}")
var/spack/repos/builtin/packages/axom/package.py:        depends_on(f"umpire {ext_rocm_dep}", when=f"+umpire {ext_rocm_dep}")
var/spack/repos/builtin/packages/axom/package.py:        depends_on(f"caliper {ext_rocm_dep}", when=f"+profiling {ext_rocm_dep}")
var/spack/repos/builtin/packages/axom/package.py:    depends_on("rocprim", when="+rocm")
var/spack/repos/builtin/packages/axom/package.py:    conflicts("+openmp", when="+rocm")
var/spack/repos/builtin/packages/axom/package.py:    conflicts("+cuda", when="+rocm")
var/spack/repos/builtin/packages/axom/package.py:    conflicts("^blt@:0.3.6", when="+rocm")
var/spack/repos/builtin/packages/axom/package.py:        if self.spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/axom/package.py:            special_case += "_cuda"
var/spack/repos/builtin/packages/axom/package.py:        if self.spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/axom/package.py:                        if spec.satisfies("^cuda"):
var/spack/repos/builtin/packages/axom/package.py:        if spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/axom/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/axom/package.py:            entries.append(cmake_cache_option("ENABLE_CUDA", True))
var/spack/repos/builtin/packages/axom/package.py:            entries.append(cmake_cache_option("CMAKE_CUDA_SEPARABLE_COMPILATION", True))
var/spack/repos/builtin/packages/axom/package.py:            # CUDA_FLAGS
var/spack/repos/builtin/packages/axom/package.py:            cudaflags = "${CMAKE_CUDA_FLAGS} -restrict --expt-extended-lambda "
var/spack/repos/builtin/packages/axom/package.py:            cudaflags += " ".join(["-Xcompiler=%s " % flag for flag in host_cxx_flags])
var/spack/repos/builtin/packages/axom/package.py:                    cudaflags += " -std=c++14"
var/spack/repos/builtin/packages/axom/package.py:                    cudaflags += " -std=c++11"
var/spack/repos/builtin/packages/axom/package.py:            entries.append(cmake_cache_string("CMAKE_CUDA_FLAGS", cudaflags, force=True))
var/spack/repos/builtin/packages/axom/package.py:        if spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/axom/package.py:            entries.append("# Axom ROCm specifics\n")
var/spack/repos/builtin/packages/axom/package.py:            rocm_root = hip_root + "/.."
var/spack/repos/builtin/packages/axom/package.py:                    rocm_root + "/llvm/lib/clang/" + clang_version + "/include"
var/spack/repos/builtin/packages/axom/package.py:            # Fixes for mpi for rocm until wrapper paths are fixed
var/spack/repos/builtin/packages/axom/package.py:                "ENABLE_GTEST_DEATH_TESTS", not spec.satisfies("+cuda target=ppc64le:")
var/spack/repos/builtin/packages/thrust/package.py:    url = "https://github.com/NVIDIA/thrust/archive/1.12.0.tar.gz"
var/spack/repos/builtin/packages/thrust/package.py:    git = "https://github.com/NVIDIA/thrust.git"
var/spack/repos/builtin/packages/tiramisu/package.py:class Tiramisu(CMakePackage, CudaPackage, PythonExtension):
var/spack/repos/builtin/packages/tiramisu/package.py:            self.define_from_variant("USE_GPU", "cuda"),
var/spack/repos/builtin/packages/py-devito/package.py:    on several computer platforms, including CPUs, GPUs, and clusters thereof.
var/spack/repos/builtin/packages/py-devito/4.8.1.patch:diff -Nur a/requirements-nvidia.txt b/requirements-nvidia.txt
var/spack/repos/builtin/packages/py-devito/4.8.1.patch:--- a/requirements-nvidia.txt   1970-01-01 01:00:00.000000000 +0100
var/spack/repos/builtin/packages/py-devito/4.8.1.patch:+++ b/requirements-nvidia.txt   2023-05-25 16:17:56.000000000 +0200
var/spack/repos/builtin/packages/py-devito/4.8.1.patch:+cupy-cuda110
var/spack/repos/builtin/packages/py-devito/4.8.1.patch:+dask-cuda
var/spack/repos/builtin/packages/py-tensorflow-probability/package.py:    deep learning on modern hardware (TPU, GPU). It's for data
var/spack/repos/builtin/packages/hiptt/package.py:class Hiptt(MakefilePackage, ROCmPackage):
var/spack/repos/builtin/packages/hiptt/package.py:    """hipTT - Fast GPU Tensor Transpose for NVIDIA and AMD GPU."""
var/spack/repos/builtin/packages/hiptt/bugfix_make.patch:@@ -93,11 +93,11 @@ CUDAROOT = $(subst /bin/,,$(dir $(shell which $(CUDAC))))
var/spack/repos/builtin/packages/hiptt/bugfix_make.patch: #CFLAGS = -I${CUDAROOT}/include -std=c++11 $(DEFS) $(OPTLEV) -fPIC -D__HIP_PLATFORM_NVCC__
var/spack/repos/builtin/packages/hiptt/bugfix_make.patch: CFLAGS = -I${CUDAROOT}/include -std=c++11 $(DEFS) $(OPTLEV) -fPIC -D__HIP_PLATFORM_HCC__ -D__HIP_ROCclr__
var/spack/repos/builtin/packages/hiptt/bugfix_make.patch: #CUDA_CFLAGS = -ccbin $(GPU_CC) -I${CUDAROOT}/include -std=c++11 $(OPTLEV) -Xptxas -dlcm=ca -lineinfo $(GENCODE_FLAGS) --resource-usage -Xcompiler -fPIC -D_FORCE_INLINES -x cu -Wno-deprecated-declarations
var/spack/repos/builtin/packages/hiptt/bugfix_make.patch:-CUDA_CFLAGS = --amdgpu-target=gfx906,gfx908 -std=c++11 $(OPTLEV) -D_FORCE_INLINES
var/spack/repos/builtin/packages/hiptt/bugfix_make.patch:+CUDA_CFLAGS = --amdgpu-target=gfx906,gfx908,gfx90a -std=c++11 $(OPTLEV) -D_FORCE_INLINES -fPIC
var/spack/repos/builtin/packages/hiptt/bugfix_make.patch: CUDA_LFLAGS = -L$(CUDAROOT)/lib
var/spack/repos/builtin/packages/slepc/package.py:class Slepc(Package, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/slepc/package.py:    depends_on("petsc+cuda", when="+cuda")
var/spack/repos/builtin/packages/slepc/package.py:    for arch in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/slepc/package.py:        rocm_dep = "+rocm amdgpu_target={0}".format(arch)
var/spack/repos/builtin/packages/slepc/package.py:        depends_on("petsc {0}".format(rocm_dep), when=rocm_dep)
var/spack/repos/builtin/packages/slepc/package.py:        if self.spec.satisfies("^kokkos+cuda+wrapper"):
var/spack/repos/builtin/packages/wonton/package.py:        description="Enable on-node parallelism using NVidia Thrust library",
var/spack/repos/builtin/packages/wonton/package.py:    variant("cuda", default=False, description="Enable GPU parallelism using CUDA")
var/spack/repos/builtin/packages/wonton/package.py:    conflicts("+thrust +cuda")  # Thrust with CUDA does not work as yet
var/spack/repos/builtin/packages/wonton/package.py:    # NVidia thrust library
var/spack/repos/builtin/packages/wonton/package.py:    # CUDA library
var/spack/repos/builtin/packages/wonton/package.py:    depends_on("cuda", when="+cuda")
var/spack/repos/builtin/packages/wonton/package.py:    depends_on("kokkos +cuda", when="+kokkos +cuda")
var/spack/repos/builtin/packages/wonton/package.py:            if "+cuda" in self.spec:
var/spack/repos/builtin/packages/wonton/package.py:                options.append("-DTHRUST_DEVICE_BACKEND:STRING=THRUST_DEVICE_SYSTEM_CUDA")
var/spack/repos/builtin/packages/wonton/package.py:            if "+cuda" in self.spec:
var/spack/repos/builtin/packages/wonton/package.py:                options.append("-DWONTON_ENABLE_Kokkos_CUDA=ON")
var/spack/repos/builtin/packages/gdal/package.py:    variant("opencl", default=False, description="Required to accelerate warping computations")
var/spack/repos/builtin/packages/gdal/package.py:    depends_on("opencl", when="+opencl")
var/spack/repos/builtin/packages/gdal/package.py:            self.define_from_variant("GDAL_USE_OPENCL", "opencl"),
var/spack/repos/builtin/packages/gdal/package.py:            self.with_or_without("opencl"),
var/spack/repos/builtin/packages/openssl/package.py:            # Last tested on nvidia@22.3 for x86_64:
var/spack/repos/builtin/packages/openssl/package.py:        # atomic support when using the NVIDIA compilers
var/spack/repos/builtin/packages/bart/package.py:class Bart(MakefilePackage, CudaPackage):
var/spack/repos/builtin/packages/bart/package.py:    # patch to fix Makefile for openblas and cuda
var/spack/repos/builtin/packages/bart/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/bart/package.py:            cuda_arch = self.spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/bart/package.py:            env["CUDA"] = "1"
var/spack/repos/builtin/packages/bart/package.py:            env["CUDA_BASE"] = spec["cuda"].prefix
var/spack/repos/builtin/packages/bart/package.py:            env["GPUARCH_FLAGS"] = " ".join(self.cuda_flags(cuda_arch))
var/spack/repos/builtin/packages/bart/Makefile-0.7.00.patch: CUDA_L := -L$(CUDA_BASE)/lib -lcufft -lcudart -lcublas -m64 -lstdc++
var/spack/repos/builtin/packages/bart/Makefile-0.7.00.patch:-CUDA_L := -L$(CUDA_BASE)/lib -lcufft -lcudart -lcublas -lstdc++ -Wl,-rpath $(CUDA_BASE)/lib
var/spack/repos/builtin/packages/bart/Makefile-0.7.00.patch:+CUDA_L := -L$(CUDA_BASE)/lib -lcufft -lcudart -lcublas -lstdc++
var/spack/repos/builtin/packages/bart/Makefile-0.7.00.patch: CUDA_H :=
var/spack/repos/builtin/packages/bart/Makefile.patch: CUDA_L := -L$(CUDA_BASE)/lib -lcufft -lcudart -lcublas -m64 -lstdc++
var/spack/repos/builtin/packages/bart/Makefile.patch:-CUDA_L := -L$(CUDA_BASE)/lib64 -lcufft -lcudart -lcublas -lstdc++ -Wl,-rpath $(CUDA_BASE)/lib64
var/spack/repos/builtin/packages/bart/Makefile.patch:+CUDA_L := -L$(CUDA_BASE)/lib64 -lcufft -lcudart -lcublas -lstdc++
var/spack/repos/builtin/packages/bart/Makefile.patch: CUDA_H :=
var/spack/repos/builtin/packages/py-minkowskiengine/package.py:class PyMinkowskiengine(PythonPackage, CudaPackage):
var/spack/repos/builtin/packages/py-minkowskiengine/package.py:    homepage = "https://nvidia.github.io/MinkowskiEngine/"
var/spack/repos/builtin/packages/py-minkowskiengine/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/py-minkowskiengine/package.py:            options.append("--force_cuda")
var/spack/repos/builtin/packages/nvbandwidth/package.py:class Nvbandwidth(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/nvbandwidth/package.py:    nvbandwidth: A tool for bandwidth measurements on NVIDIA GPUs.
var/spack/repos/builtin/packages/nvbandwidth/package.py:    git = "https://github.com/NVIDIA/nvbandwidth"
var/spack/repos/builtin/packages/nvbandwidth/package.py:        url="https://github.com/NVIDIA/nvbandwidth/archive/refs/tags/v0.4.tar.gz",
var/spack/repos/builtin/packages/nvbandwidth/package.py:        url="https://github.com/NVIDIA/nvbandwidth/archive/refs/tags/v0.3.tar.gz",
var/spack/repos/builtin/packages/nvbandwidth/package.py:        url="https://github.com/NVIDIA/nvbandwidth/archive/refs/tags/v0.2.tar.gz",
var/spack/repos/builtin/packages/nvbandwidth/package.py:        url="https://github.com/NVIDIA/nvbandwidth/archive/refs/tags/v0.1.tar.gz",
var/spack/repos/builtin/packages/grass/package.py:    variant("opencl", default=False, description="Support OpenCL functionality")
var/spack/repos/builtin/packages/grass/package.py:    depends_on("opencl", when="+opencl")
var/spack/repos/builtin/packages/grass/package.py:        if spec.satisfies("+opencl"):
var/spack/repos/builtin/packages/grass/package.py:            args.append("--with-opencl")
var/spack/repos/builtin/packages/grass/package.py:            args.append("--without-opencl")
var/spack/repos/builtin/packages/gnina/package.py:class Gnina(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/gnina/package.py:    depends_on("cuda@11")
var/spack/repos/builtin/packages/ufo-filters/package.py:    CPUs, GPUs or clusters. This package contains filter plugins."""
var/spack/repos/builtin/packages/py-open-clip-torch/package.py:class PyOpenClipTorch(PythonPackage):
var/spack/repos/builtin/packages/examinimd/package.py:    # TODO: Set up cuda variant when test machine available
var/spack/repos/builtin/packages/eckit/package.py:            self.define("ENABLE_CUDA", False),
var/spack/repos/builtin/packages/perl/package.py:    # Enable builds with the NVIDIA compiler
var/spack/repos/builtin/packages/perl/package.py:        msg="The NVIDIA compilers are incompatible with version 5.32 and later",
var/spack/repos/builtin/packages/py-cgen/package.py:    (C++/CUDA/OpenCL) to allow structured code generation from Python.
var/spack/repos/builtin/packages/nvpl-blas/package.py:    NVPL BLAS (NVIDIA Performance Libraries BLAS) is part of NVIDIA Performance Libraries
var/spack/repos/builtin/packages/nvpl-blas/package.py:    homepage = "https://docs.nvidia.com/nvpl/_static/blas/index.html"
var/spack/repos/builtin/packages/nvpl-blas/package.py:        "https://developer.download.nvidia.com/compute/nvpl/redist"
var/spack/repos/builtin/packages/nvpl-blas/package.py:        url = "https://developer.download.nvidia.com/compute/nvpl/redist/nvpl_blas/linux-sbsa/nvpl_blas-linux-sbsa-{0}-archive.tar.xz"
var/spack/repos/builtin/packages/hipsycl/package.py:class Hipsycl(CMakePackage, ROCmPackage):
var/spack/repos/builtin/packages/hipsycl/package.py:    over NVIDIA CUDA/AMD HIP"""
var/spack/repos/builtin/packages/hipsycl/package.py:    variant("cuda", default=False, description="Enable CUDA backend for SYCL kernels")
var/spack/repos/builtin/packages/hipsycl/package.py:    variant("rocm", default=False, description="Enable ROCM backend for SYCL kernels")
var/spack/repos/builtin/packages/hipsycl/package.py:    depends_on("llvm@8: +clang", when="~cuda")
var/spack/repos/builtin/packages/hipsycl/package.py:    depends_on("llvm@9: +clang", when="+cuda")
var/spack/repos/builtin/packages/hipsycl/package.py:    # LLVM PTX backend requires cuda7:10.1 (https://tinyurl.com/v82k5qq)
var/spack/repos/builtin/packages/hipsycl/package.py:    depends_on("cuda@9:10.1", when="@0.8.1: +cuda ^llvm@9")
var/spack/repos/builtin/packages/hipsycl/package.py:    depends_on("cuda@9:", when="@0.8.1: +cuda ^llvm@10:")
var/spack/repos/builtin/packages/hipsycl/package.py:    # hipSYCL@:0.8.0 requires cuda@9:10.0 due to a known bug
var/spack/repos/builtin/packages/hipsycl/package.py:    depends_on("cuda@9:10.0", when="@:0.8.0 +cuda")
var/spack/repos/builtin/packages/hipsycl/package.py:        when="+cuda",
var/spack/repos/builtin/packages/hipsycl/package.py:        msg="LLVM debug builds don't work with hipSYCL CUDA backend; for "
var/spack/repos/builtin/packages/hipsycl/package.py:        "https://github.com/illuhad/hipSYCL/blob/master/doc/install-cuda.md",
var/spack/repos/builtin/packages/hipsycl/package.py:            "-DWITH_ROCM_BACKEND:Bool={0}".format("TRUE" if spec.satisfies("+rocm") else "FALSE"),
var/spack/repos/builtin/packages/hipsycl/package.py:            "-DWITH_CUDA_BACKEND:Bool={0}".format("TRUE" if spec.satisfies("+cuda") else "FALSE"),
var/spack/repos/builtin/packages/hipsycl/package.py:            spec["llvm"].prefix, "__clang_cuda_runtime_wrapper.h"
var/spack/repos/builtin/packages/hipsycl/package.py:        # explicit CUDA toolkit
var/spack/repos/builtin/packages/hipsycl/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/hipsycl/package.py:            args.append("-DCUDA_TOOLKIT_ROOT_DIR:String={0}".format(spec["cuda"].prefix))
var/spack/repos/builtin/packages/hipsycl/package.py:        if spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/hipsycl/package.py:            args.append("-DROCM_PATH:STRING={0}".format(os.environ.get("ROCM_PATH")))
var/spack/repos/builtin/packages/hipsycl/package.py:            configfiles = {"core": "syclcc.json", "cuda": "syclcc.json"}
var/spack/repos/builtin/packages/hipsycl/package.py:            configfiles = {"core": "acpp-core.json", "cuda": "acpp-cuda.json"}
var/spack/repos/builtin/packages/hipsycl/package.py:        if self.spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/hipsycl/package.py:            # 2. Fix stdlib: we need to make sure cuda-enabled binaries find
var/spack/repos/builtin/packages/hipsycl/package.py:            if self.spec.satisfies("~rocm"):
var/spack/repos/builtin/packages/hipsycl/package.py:                def adjust_cuda_config(config):
var/spack/repos/builtin/packages/hipsycl/package.py:                    config["default-cuda-link-line"] += " " + " ".join(
var/spack/repos/builtin/packages/hipsycl/package.py:                edit_config(configfiles["cuda"], adjust_cuda_config)
var/spack/repos/builtin/packages/likwid/package.py:    variant("cuda", default=False, description="with Nvidia GPU profiling support")
var/spack/repos/builtin/packages/likwid/package.py:    variant("rocm", default=False, description="with AMD GPU profiling support")
var/spack/repos/builtin/packages/likwid/package.py:    depends_on("cuda", when="@5: +cuda")
var/spack/repos/builtin/packages/likwid/package.py:    depends_on("rocprofiler-dev", when="@5.3: +rocm")
var/spack/repos/builtin/packages/likwid/package.py:    depends_on("rocm-core", when="@5.3: +rocm")
var/spack/repos/builtin/packages/likwid/package.py:    depends_on("rocm-smi-lib", when="@5.3: +rocm")
var/spack/repos/builtin/packages/likwid/package.py:        if self.spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/likwid/package.py:                "libcupti", root=self.spec["cuda"].prefix, shared=True, recursive=True
var/spack/repos/builtin/packages/likwid/package.py:        if self.spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/likwid/package.py:                root=self.spec["rocm-core"].prefix,
var/spack/repos/builtin/packages/likwid/package.py:                "librocm_smi64.so",
var/spack/repos/builtin/packages/likwid/package.py:                root=self.spec["rocm-smi-lib"].prefix,
var/spack/repos/builtin/packages/likwid/package.py:        if self.spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/likwid/package.py:            filter_file("^NVIDIA_INTERFACE.*", "NVIDIA_INTERFACE = true", "config.mk")
var/spack/repos/builtin/packages/likwid/package.py:            cudainc = spec["cuda"].prefix.include
var/spack/repos/builtin/packages/likwid/package.py:            filter_file("^CUDAINCLUDE.*", "CUDAINCLUDE = {0}".format(cudainc), "config.mk")
var/spack/repos/builtin/packages/likwid/package.py:            cuptihead = HeaderList(find(spec["cuda"].prefix, "cupti.h", recursive=True))
var/spack/repos/builtin/packages/likwid/package.py:            filter_file("^NVIDIA_INTERFACE.*", "NVIDIA_INTERFACE = false", "config.mk")
var/spack/repos/builtin/packages/likwid/package.py:        if self.spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/likwid/package.py:            env["ROCM_HOME"] = spec["rocm-core"].prefix
var/spack/repos/builtin/packages/likwid/package.py:            filter_file("^ROCM_INTERFACE.*", "ROCM_INTERFACE = true", "config.mk")
var/spack/repos/builtin/packages/likwid/package.py:            filter_file("^ROCM_INTERFACE.*", "ROCM_INTERFACE = false", "config.mk")
var/spack/repos/builtin/packages/motioncor2/package.py:    """MotionCor2 is a multi-GPU program that corrects beam-induced sample
var/spack/repos/builtin/packages/motioncor2/package.py:    depends_on("cuda@10.2,11.1:11.8,12.1", type="run")
var/spack/repos/builtin/packages/motioncor2/package.py:        cuda_version = spec["cuda"].version.up_to(2).joined
var/spack/repos/builtin/packages/motioncor2/package.py:            "MotionCor2_{0}_Cuda{1}_*".format(spec.version, cuda_version),
var/spack/repos/builtin/packages/motioncor2/package.py:            "--set-rpath", self.spec["cuda"].prefix.lib64, join_path(self.prefix.bin, "MotionCor2")
var/spack/repos/builtin/packages/geopm-service/0001-Support-NVML-via-CUDA-installation.patch:Subject: [PATCH] Support NVML via CUDA installation
var/spack/repos/builtin/packages/geopm-service/package.py:    patch("0001-Support-NVML-via-CUDA-installation.patch", when="+nvml")
var/spack/repos/builtin/packages/geopm-service/package.py:    depends_on("cuda", when="+nvml")
var/spack/repos/builtin/packages/geopm-service/package.py:                    self.spec["cuda"].prefix, "targets", f"{self.spec.target.family}-linux"
var/spack/repos/builtin/packages/suite-sparse/package.py:    variant("cuda", default=False, description="Build with CUDA")
var/spack/repos/builtin/packages/suite-sparse/package.py:    depends_on("cuda", when="+cuda")
var/spack/repos/builtin/packages/suite-sparse/package.py:    # CUDA-11 dropped sm_30 code generation, remove hardcoded sm_30 from makefile
var/spack/repos/builtin/packages/suite-sparse/package.py:    patch("fix_cuda11.patch", when="@5.9.0:5.10.0+cuda ^cuda@11:")
var/spack/repos/builtin/packages/suite-sparse/package.py:            # CUDA=no does NOT disable cuda, it only disables internal search
var/spack/repos/builtin/packages/suite-sparse/package.py:            # for CUDA_PATH. If in addition the latter is empty, then CUDA is
var/spack/repos/builtin/packages/suite-sparse/package.py:            "CUDA=no",
var/spack/repos/builtin/packages/suite-sparse/package.py:            f"CUDA_PATH={spec['cuda'].prefix if '+cuda' in spec else ''}",
var/spack/repos/builtin/packages/suite-sparse/package.py:                    f"-DENABLE_CUDA={'ON' if '+cuda' in spec else 'OFF'}",
var/spack/repos/builtin/packages/suite-sparse/package.py:                    f"-DSUITESPARSE_USE_CUDA={'ON' if '+cuda' in spec else 'OFF'}",
var/spack/repos/builtin/packages/suite-sparse/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/suite-sparse/package.py:            targets.extend(["SuiteSparse_GPURuntime", "GPUQREngine"])
var/spack/repos/builtin/packages/suite-sparse/fix_cuda11.patch:         NVCC          = $(CUDA_PATH)/bin/nvcc
var/spack/repos/builtin/packages/rocdecode/package.py:    homepage = "https://github.com/ROCm/rocDecode"
var/spack/repos/builtin/packages/rocdecode/package.py:    git = "https://github.com/ROCm/rocDecode.git"
var/spack/repos/builtin/packages/rocdecode/package.py:    url = "https://github.com/ROCm/rocDecode/archive/refs/tags/rocm-6.2.0.tar.gz"
var/spack/repos/builtin/packages/rocdecode/package.py:    tags = ["rocm"]
var/spack/repos/builtin/packages/rocdecode/package.py:    amdgpu_targets = ROCmPackage.amdgpu_targets
var/spack/repos/builtin/packages/rocdecode/package.py:        "amdgpu_target",
var/spack/repos/builtin/packages/rocdecode/package.py:        description="AMD GPU architecture",
var/spack/repos/builtin/packages/rocdecode/package.py:        values=auto_or_any_combination_of(*amdgpu_targets),
var/spack/repos/builtin/packages/rocdecode/package.py:            r"${ROCM_PATH}/llvm/bin/clang++",
var/spack/repos/builtin/packages/rocdecode/package.py:            "{0}/bin/clang++".format(self.spec["llvm-amdgpu"].prefix),
var/spack/repos/builtin/packages/rocdecode/package.py:        if "auto" not in self.spec.variants["amdgpu_target"]:
var/spack/repos/builtin/packages/rocdecode/package.py:            args.append(self.define_from_variant("AMDGPU_TARGETS", "amdgpu_target"))
var/spack/repos/builtin/packages/petsc/package.py:class Petsc(Package, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/petsc/package.py:    with when("+rocm"):
var/spack/repos/builtin/packages/petsc/package.py:            "0001-Handle-the-hipsparse-api-changes-for-rocm-6.0.patch",
var/spack/repos/builtin/packages/petsc/package.py:    conflicts("^openmpi~cuda", when="+cuda")  # +cuda requires CUDA enabled OpenMPI
var/spack/repos/builtin/packages/petsc/package.py:    patch("disable-DEPRECATED_ENUM.diff", when="@3.14.1 +cuda")
var/spack/repos/builtin/packages/petsc/package.py:    depends_on("cuda", when="+cuda")
var/spack/repos/builtin/packages/petsc/package.py:        "^cuda@12.4:", when="@:3.20.5 +cuda", msg="Deprecation in CCCL 2.3 causes build failure."
var/spack/repos/builtin/packages/petsc/package.py:    depends_on("hip", when="+rocm")
var/spack/repos/builtin/packages/petsc/package.py:    with when("+rocm"):
var/spack/repos/builtin/packages/petsc/package.py:        depends_on("rocm-core")
var/spack/repos/builtin/packages/petsc/package.py:    for cuda_arch in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/petsc/package.py:            "kokkos+cuda+cuda_lambda cuda_arch=%s" % cuda_arch,
var/spack/repos/builtin/packages/petsc/package.py:            when="+kokkos +cuda cuda_arch=%s" % cuda_arch,
var/spack/repos/builtin/packages/petsc/package.py:            "kokkos-kernels+cuda cuda_arch=%s" % cuda_arch,
var/spack/repos/builtin/packages/petsc/package.py:            when="+kokkos +cuda cuda_arch=%s" % cuda_arch,
var/spack/repos/builtin/packages/petsc/package.py:    for rocm_arch in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/petsc/package.py:            "kokkos+rocm amdgpu_target=%s" % rocm_arch,
var/spack/repos/builtin/packages/petsc/package.py:            when="+kokkos +rocm amdgpu_target=%s" % rocm_arch,
var/spack/repos/builtin/packages/petsc/package.py:                raise InstallError("PETSc's SYCL GPU Backend requires oneAPI CXX (icpx) compiler.")
var/spack/repos/builtin/packages/petsc/package.py:            ("cuda", "cuda", False, False),
var/spack/repos/builtin/packages/petsc/package.py:        if "+cuda" in spec:
var/spack/repos/builtin/packages/petsc/package.py:            if not spec.satisfies("cuda_arch=none"):
var/spack/repos/builtin/packages/petsc/package.py:                cuda_arch = spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/petsc/package.py:                    options.append("--with-cuda-gencodearch={0}".format(cuda_arch[0]))
var/spack/repos/builtin/packages/petsc/package.py:                        "CUDAFLAGS=-gencode arch=compute_{0},code=sm_{0}".format(cuda_arch[0])
var/spack/repos/builtin/packages/petsc/package.py:        if "+rocm" in spec:
var/spack/repos/builtin/packages/petsc/package.py:            if not spec.satisfies("amdgpu_target=none"):
var/spack/repos/builtin/packages/petsc/package.py:                hip_arch = spec.variants["amdgpu_target"].value
var/spack/repos/builtin/packages/petsc/package.py:            hip_ipkgs = hip_pkgs + ["rocthrust", "rocprim", "rocm-core"]
var/spack/repos/builtin/packages/petsc/package.py:            if spec["superlu-dist"].satisfies("+rocm"):
var/spack/repos/builtin/packages/petsc/package.py:                options.append("CXXPPFLAGS=-DROCM_NO_WRAPPER_HEADER_WARNING")
var/spack/repos/builtin/packages/petsc/package.py:        if self.spec.satisfies("^kokkos+cuda+wrapper"):
var/spack/repos/builtin/packages/petsc/package.py:        if "+cuda" not in self.spec:
var/spack/repos/builtin/packages/petsc/package.py:            raise SkipTest("Package must be built with +cuda")
var/spack/repos/builtin/packages/petsc/package.py:                "-use_gpu_aware_mpi",
var/spack/repos/builtin/packages/petsc/package.py:                "-use_gpu_aware_mpi",
var/spack/repos/builtin/packages/petsc/hip-5.6.0-for-3.18.diff:   PetscCall(PetscLogGpuTimeEnd());
var/spack/repos/builtin/packages/petsc/0001-Handle-the-hipsparse-api-changes-for-rocm-6.0.patch:Subject: [PATCH] Handle the hipsparse api changes for rocm 6.0
var/spack/repos/builtin/packages/petsc/hip-5.7-plus-for-3.18.diff: /* As of ROCm 3.10 llint atomicMin/Max(llint*, llint) is not supported */
var/spack/repos/builtin/packages/m4/package.py:    # The NVIDIA compilers do not currently support some GNU builtins.
var/spack/repos/builtin/packages/r-ggpubr/package.py:class RGgpubr(RPackage):
var/spack/repos/builtin/packages/r-ggpubr/package.py:    difficulty for researchers with no advanced R programming skills. 'ggpubr'
var/spack/repos/builtin/packages/r-ggpubr/package.py:    cran = "ggpubr"
var/spack/repos/builtin/packages/cutlass/package.py:class Cutlass(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/cutlass/package.py:    """CUDA Templates for Linear Algebra Subroutines"""
var/spack/repos/builtin/packages/cutlass/package.py:    homepage = "https://github.com/NVIDIA/cutlass"
var/spack/repos/builtin/packages/cutlass/package.py:    url = "https://github.com/NVIDIA/cutlass/archive/refs/tags/v3.3.0.tar.gz"
var/spack/repos/builtin/packages/cutlass/package.py:    variant("cuda", default=True, description="Build with CUDA")
var/spack/repos/builtin/packages/cutlass/package.py:    conflicts("~cuda", msg="Cutlass requires CUDA")
var/spack/repos/builtin/packages/cutlass/package.py:        "cuda_arch=none",
var/spack/repos/builtin/packages/cutlass/package.py:        msg="Must specify CUDA compute capabilities of your GPU, see "
var/spack/repos/builtin/packages/cutlass/package.py:        "https://developer.nvidia.com/cuda-gpus",
var/spack/repos/builtin/packages/cutlass/package.py:        env.set("CUDACXX", self.spec["cuda"].prefix.bin.nvcc)
var/spack/repos/builtin/packages/cutlass/package.py:        cuda_arch = self.spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/cutlass/package.py:        return [self.define("CUTLASS_NVCC_ARCHS", ";".join(cuda_arch))]
var/spack/repos/builtin/packages/nvpl-lapack/package.py:    NVPL LAPACK (NVIDIA Performance Libraries LAPACK) is part of NVIDIA Performance Libraries
var/spack/repos/builtin/packages/nvpl-lapack/package.py:    homepage = "https://docs.nvidia.com/nvpl/_static/lapack/index.html"
var/spack/repos/builtin/packages/nvpl-lapack/package.py:        "https://developer.download.nvidia.com/compute/nvpl/redist"
var/spack/repos/builtin/packages/nvpl-lapack/package.py:        url = "https://developer.download.nvidia.com/compute/nvpl/redist/nvpl_lapack/linux-sbsa/nvpl_lapack-linux-sbsa-{0}-archive.tar.xz"
var/spack/repos/builtin/packages/changa/package.py:class Changa(AutotoolsPackage, CudaPackage):
var/spack/repos/builtin/packages/changa/package.py:    depends_on("charmpp +cuda", when="+cuda")
var/spack/repos/builtin/packages/changa/package.py:        if self.spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/changa/package.py:            args.append(f"--with-cuda={self.spec['cuda'].prefix}")
var/spack/repos/builtin/packages/pgi/package.py:        "nvidia", default=False, description="Enable installation of optional NVIDIA components"
var/spack/repos/builtin/packages/pgi/package.py:        if "+nvidia" in spec:
var/spack/repos/builtin/packages/pgi/package.py:            os.environ["PGI_INSTALL_NVIDIA"] = "true"
var/spack/repos/builtin/packages/pgi/detection_test.yaml:      echo "Copyright (c) 2015, NVIDIA CORPORATION.  All rights reserved."
var/spack/repos/builtin/packages/pgi/detection_test.yaml:        echo "Copyright (c) 2017, NVIDIA CORPORATION.  All rights reserved."
var/spack/repos/builtin/packages/pgi/detection_test.yaml:        echo "Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved."
var/spack/repos/builtin/packages/nvhpc/package.py:# Copyright (c) 2020, NVIDIA CORPORATION. All rights reserved.
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/24.9/nvhpc_2024_249_Linux_aarch64_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/24.9/nvhpc_2024_249_Linux_x86_64_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/24.7/nvhpc_2024_247_Linux_aarch64_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/24.7/nvhpc_2024_247_Linux_x86_64_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/24.5/nvhpc_2024_245_Linux_aarch64_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/24.5/nvhpc_2024_245_Linux_x86_64_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/24.3/nvhpc_2024_243_Linux_aarch64_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/24.3/nvhpc_2024_243_Linux_x86_64_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/24.1/nvhpc_2024_241_Linux_aarch64_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/24.1/nvhpc_2024_241_Linux_ppc64le_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/24.1/nvhpc_2024_241_Linux_x86_64_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/23.11/nvhpc_2023_2311_Linux_aarch64_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/23.11/nvhpc_2023_2311_Linux_ppc64le_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/23.11/nvhpc_2023_2311_Linux_x86_64_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/23.9/nvhpc_2023_239_Linux_aarch64_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/23.9/nvhpc_2023_239_Linux_ppc64le_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/23.9/nvhpc_2023_239_Linux_x86_64_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/23.7/nvhpc_2023_237_Linux_aarch64_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/23.7/nvhpc_2023_237_Linux_ppc64le_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/23.7/nvhpc_2023_237_Linux_x86_64_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/23.5/nvhpc_2023_235_Linux_aarch64_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/23.5/nvhpc_2023_235_Linux_ppc64le_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/23.5/nvhpc_2023_235_Linux_x86_64_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/23.3/nvhpc_2023_233_Linux_aarch64_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/23.3/nvhpc_2023_233_Linux_ppc64le_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/23.3/nvhpc_2023_233_Linux_x86_64_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/23.1/nvhpc_2023_231_Linux_aarch64_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/23.1/nvhpc_2023_231_Linux_ppc64le_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/23.1/nvhpc_2023_231_Linux_x86_64_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/22.11/nvhpc_2022_2211_Linux_aarch64_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/22.11/nvhpc_2022_2211_Linux_ppc64le_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/22.11/nvhpc_2022_2211_Linux_x86_64_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/22.9/nvhpc_2022_229_Linux_aarch64_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/22.9/nvhpc_2022_229_Linux_ppc64le_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/22.9/nvhpc_2022_229_Linux_x86_64_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/22.7/nvhpc_2022_227_Linux_aarch64_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/22.7/nvhpc_2022_227_Linux_ppc64le_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/22.7/nvhpc_2022_227_Linux_x86_64_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/22.5/nvhpc_2022_225_Linux_aarch64_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/22.5/nvhpc_2022_225_Linux_ppc64le_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/22.5/nvhpc_2022_225_Linux_x86_64_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/22.3/nvhpc_2022_223_Linux_aarch64_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/22.3/nvhpc_2022_223_Linux_ppc64le_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/22.3/nvhpc_2022_223_Linux_x86_64_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/22.2/nvhpc_2022_222_Linux_aarch64_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/22.2/nvhpc_2022_222_Linux_ppc64le_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/22.2/nvhpc_2022_222_Linux_x86_64_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/22.1/nvhpc_2022_221_Linux_aarch64_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/22.1/nvhpc_2022_221_Linux_ppc64le_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/22.1/nvhpc_2022_221_Linux_x86_64_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/21.11/nvhpc_2021_2111_Linux_aarch64_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/21.11/nvhpc_2021_2111_Linux_ppc64le_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/21.11/nvhpc_2021_2111_Linux_x86_64_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/21.9/nvhpc_2021_219_Linux_aarch64_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/21.9/nvhpc_2021_219_Linux_ppc64le_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/21.9/nvhpc_2021_219_Linux_x86_64_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/21.7/nvhpc_2021_217_Linux_aarch64_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/21.7/nvhpc_2021_217_Linux_ppc64le_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/21.7/nvhpc_2021_217_Linux_x86_64_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/21.5/nvhpc_2021_215_Linux_aarch64_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/21.5/nvhpc_2021_215_Linux_ppc64le_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/21.5/nvhpc_2021_215_Linux_x86_64_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/21.3/nvhpc_2021_213_Linux_aarch64_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/21.3/nvhpc_2021_213_Linux_ppc64le_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/21.3/nvhpc_2021_213_Linux_x86_64_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/21.2/nvhpc_2021_212_Linux_aarch64_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/21.2/nvhpc_2021_212_Linux_ppc64le_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/21.2/nvhpc_2021_212_Linux_x86_64_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/21.1/nvhpc_2021_211_Linux_aarch64_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/21.1/nvhpc_2021_211_Linux_ppc64le_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/21.1/nvhpc_2021_211_Linux_x86_64_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/20.11/nvhpc_2020_2011_Linux_aarch64_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/20.11/nvhpc_2020_2011_Linux_ppc64le_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/20.11/nvhpc_2020_2011_Linux_x86_64_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/20.9/nvhpc_2020_209_Linux_aarch64_cuda_11.0.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/20.9/nvhpc_2020_209_Linux_ppc64le_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/20.9/nvhpc_2020_209_Linux_x86_64_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/20.7/nvhpc_2020_207_Linux_aarch64_cuda_11.0.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/20.7/nvhpc_2020_207_Linux_ppc64le_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:            "https://developer.download.nvidia.com/hpc-sdk/20.7/nvhpc_2020_207_Linux_x86_64_cuda_multi.tar.gz",
var/spack/repos/builtin/packages/nvhpc/package.py:    """The NVIDIA HPC SDK is a comprehensive suite of compilers, libraries
var/spack/repos/builtin/packages/nvhpc/package.py:    performance and portability of HPC applications. The NVIDIA HPC
var/spack/repos/builtin/packages/nvhpc/package.py:    SDK C, C++, and Fortran compilers support GPU acceleration of HPC
var/spack/repos/builtin/packages/nvhpc/package.py:    Fortran, OpenACC directives, and CUDA. GPU-accelerated math
var/spack/repos/builtin/packages/nvhpc/package.py:    multi-GPU and scalable systems programming. Performance profiling
var/spack/repos/builtin/packages/nvhpc/package.py:    homepage = "https://developer.nvidia.com/hpc-sdk"
var/spack/repos/builtin/packages/nvhpc/package.py:        "default_cuda", default="default", description="Default CUDA version, for example 11.8"
var/spack/repos/builtin/packages/nvhpc/package.py:        # TODO: use other exes to determine default_cuda/install_type/blas/lapack/mpi variants
var/spack/repos/builtin/packages/nvhpc/package.py:        if self.spec.variants["default_cuda"].value != "default":
var/spack/repos/builtin/packages/nvhpc/package.py:            env.set("NVHPC_DEFAULT_CUDA", self.spec.variants["default_cuda"].value)
var/spack/repos/builtin/packages/nvhpc/detection_test.yaml:      echo "NVIDIA Compilers and Tools"
var/spack/repos/builtin/packages/nvhpc/detection_test.yaml:      echo "Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved."
var/spack/repos/builtin/packages/nvhpc/detection_test.yaml:      echo "NVIDIA Compilers and Tools"
var/spack/repos/builtin/packages/nvhpc/detection_test.yaml:      echo "Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved."
var/spack/repos/builtin/packages/nvhpc/detection_test.yaml:      echo "NVIDIA Compilers and Tools"
var/spack/repos/builtin/packages/nvhpc/detection_test.yaml:      echo "Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved."
var/spack/repos/builtin/packages/nvhpc/detection_test.yaml:      echo "NVIDIA Compilers and Tools"
var/spack/repos/builtin/packages/nvhpc/detection_test.yaml:      echo "Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved."
var/spack/repos/builtin/packages/nvhpc/detection_test.yaml:      echo "NVIDIA Compilers and Tools"
var/spack/repos/builtin/packages/nvhpc/detection_test.yaml:      echo "Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved."
var/spack/repos/builtin/packages/nvhpc/detection_test.yaml:      echo "NVIDIA Compilers and Tools"
var/spack/repos/builtin/packages/nvhpc/detection_test.yaml:      echo "Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved."
var/spack/repos/builtin/packages/nvhpc/detection_test.yaml:      echo "NVIDIA Compilers and Tools"
var/spack/repos/builtin/packages/nvhpc/detection_test.yaml:      echo "Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved."
var/spack/repos/builtin/packages/nvhpc/detection_test.yaml:      echo "NVIDIA Compilers and Tools"
var/spack/repos/builtin/packages/nvhpc/detection_test.yaml:      echo "Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved."
var/spack/repos/builtin/packages/nvhpc/detection_test.yaml:      echo "NVIDIA Compilers and Tools"
var/spack/repos/builtin/packages/nvhpc/detection_test.yaml:      echo "Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved."
var/spack/repos/builtin/packages/umpire/dual_blt_import_umpire_2022.10_2023.06.patch:-if (UMPIRE_ENABLE_MPI OR UMPIRE_ENABLE_HIP OR UMPIRE_ENABLE_OPENMP OR UMPIRE_ENABLE_CUDA)
var/spack/repos/builtin/packages/umpire/dual_blt_import_umpire_2022.10_2023.06.patch:+blt_list_append(TO UMPIRE_BLT_TPL_DEPS_EXPORTS ELEMENTS cuda cuda_runtime IF UMPIRE_ENABLE_CUDA)
var/spack/repos/builtin/packages/umpire/dual_blt_import_umpire_2022.10_2023.06.patch: blt_list_append( TO umpire_interface_c_fortran_depends ELEMENTS cuda_runtime IF UMPIRE_ENABLE_CUDA )
var/spack/repos/builtin/packages/umpire/package.py:class Umpire(CachedCMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/umpire/package.py:    """An application-focused API for memory management on NUMA & GPU
var/spack/repos/builtin/packages/umpire/package.py:    patch("std-filesystem-pr784.patch", when="@2022.03.1 +rocm ^blt@0.5.2:")
var/spack/repos/builtin/packages/umpire/package.py:    depends_on("cmake@3.23:", when="@2022.10.0: +rocm", type="build")
var/spack/repos/builtin/packages/umpire/package.py:    depends_on("cmake@:3.20", when="@2022.03.0:2022.03 +rocm", type="build")
var/spack/repos/builtin/packages/umpire/package.py:    depends_on("cmake@3.9:", when="+cuda", type="build")
var/spack/repos/builtin/packages/umpire/package.py:    conflicts("^blt@:0.3.6", when="+rocm")
var/spack/repos/builtin/packages/umpire/package.py:    depends_on("camp~cuda", when="~cuda")
var/spack/repos/builtin/packages/umpire/package.py:    depends_on("camp~rocm", when="~rocm")
var/spack/repos/builtin/packages/umpire/package.py:        with when("+cuda"):
var/spack/repos/builtin/packages/umpire/package.py:            depends_on("camp+cuda")
var/spack/repos/builtin/packages/umpire/package.py:            for sm_ in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/umpire/package.py:                depends_on("camp+cuda cuda_arch={0}".format(sm_), when="cuda_arch={0}".format(sm_))
var/spack/repos/builtin/packages/umpire/package.py:        with when("+rocm"):
var/spack/repos/builtin/packages/umpire/package.py:            depends_on("camp+rocm")
var/spack/repos/builtin/packages/umpire/package.py:            for arch_ in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/umpire/package.py:                    "camp+rocm amdgpu_target={0}".format(arch_),
var/spack/repos/builtin/packages/umpire/package.py:                    when="amdgpu_target={0}".format(arch_),
var/spack/repos/builtin/packages/umpire/package.py:    # device allocator must be used with more current umpire versions, rocm 5.4.0 and greater,
var/spack/repos/builtin/packages/umpire/package.py:    # and with either rocm or cuda enabled
var/spack/repos/builtin/packages/umpire/package.py:    conflicts("+device_alloc", when="~rocm~cuda")
var/spack/repos/builtin/packages/umpire/package.py:    conflicts("+deviceconst", when="~rocm~cuda")
var/spack/repos/builtin/packages/umpire/package.py:    conflicts("+cuda", when="+rocm")
var/spack/repos/builtin/packages/umpire/package.py:    conflicts("+tools", when="+rocm")
var/spack/repos/builtin/packages/umpire/package.py:        "+rocm", when="+omptarget", msg="Cant support both rocm and openmp device backends at once"
var/spack/repos/builtin/packages/umpire/package.py:    # currently only available for cuda.
var/spack/repos/builtin/packages/umpire/package.py:    conflicts("+shared", when="+cuda")
var/spack/repos/builtin/packages/umpire/package.py:        if "+rocm" in spec:
var/spack/repos/builtin/packages/umpire/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/umpire/package.py:            entries.append(cmake_cache_option("ENABLE_CUDA", True))
var/spack/repos/builtin/packages/umpire/package.py:            entries.append(cmake_cache_option("ENABLE_CUDA", False))
var/spack/repos/builtin/packages/umpire/package.py:        if spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/tasmanian/package.py:class Tasmanian(CMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/tasmanian/package.py:    variant("cuda", default=False, description="add CUDA support to Tasmanian")
var/spack/repos/builtin/packages/tasmanian/package.py:    variant("rocm", default=False, description="add ROCm support to Tasmanian")
var/spack/repos/builtin/packages/tasmanian/package.py:    depends_on("cuda@10.0:", when="+cuda", type=("build", "run"))
var/spack/repos/builtin/packages/tasmanian/package.py:    depends_on("cuda@10.0:", when="+magma", type=("build", "run"))
var/spack/repos/builtin/packages/tasmanian/package.py:    depends_on("hip@5.0:", when="+rocm", type=("build", "run"))
var/spack/repos/builtin/packages/tasmanian/package.py:    depends_on("rocblas@5.0:", when="+rocm", type=("build", "run"))
var/spack/repos/builtin/packages/tasmanian/package.py:    depends_on("rocsparse@5.0:", when="+rocm", type=("build", "run"))
var/spack/repos/builtin/packages/tasmanian/package.py:    depends_on("rocsolver@5.0:", when="+rocm", type=("build", "run"))
var/spack/repos/builtin/packages/tasmanian/package.py:    conflicts("^cuda@12", when="@:7.9 +cuda")
var/spack/repos/builtin/packages/tasmanian/package.py:    conflicts("+magma", when="~cuda~rocm")  # currently MAGMA only works with CUDA
var/spack/repos/builtin/packages/tasmanian/package.py:    conflicts("+cuda", when="+rocm")  # can pick CUDA or ROCm, not both
var/spack/repos/builtin/packages/tasmanian/package.py:        if "+rocm" in self.spec:
var/spack/repos/builtin/packages/tasmanian/package.py:            self.define_from_variant("Tasmanian_ENABLE_CUDA", "cuda"),
var/spack/repos/builtin/packages/tasmanian/package.py:            self.define_from_variant("Tasmanian_ENABLE_HIP", "rocm"),
var/spack/repos/builtin/packages/tasmanian/package.py:        if "+rocm" in self.spec:
var/spack/repos/builtin/packages/tasmanian/package.py:                f"-DAMDDeviceLibs_DIR={self.spec['llvm-amdgpu'].prefix.lib.cmake.AMDDeviceLibs}"
var/spack/repos/builtin/packages/ratel/package.py:class Ratel(MakefilePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/ratel/package.py:    # Note: '+cuda' and 'cuda_arch' variants are added by the CudaPackage
var/spack/repos/builtin/packages/ratel/package.py:    #       '+rocm' and 'amdgpu_target' variants are added by the ROCmPackage
var/spack/repos/builtin/packages/ratel/package.py:    # But we need to sync cuda/rocm with libCEED and PETSc
var/spack/repos/builtin/packages/ratel/package.py:    for sm_ in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/ratel/package.py:            "libceed+cuda cuda_arch={0}".format(sm_), when="+cuda cuda_arch={0}".format(sm_)
var/spack/repos/builtin/packages/ratel/package.py:        depends_on("petsc+cuda cuda_arch={0}".format(sm_), when="+cuda cuda_arch={0}".format(sm_))
var/spack/repos/builtin/packages/ratel/package.py:    for gfx in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/ratel/package.py:            "libceed+rocm amdgpu_target={0}".format(gfx),
var/spack/repos/builtin/packages/ratel/package.py:            when="+rocm amdgpu_target={0}".format(gfx),
var/spack/repos/builtin/packages/ratel/package.py:            "petsc+rocm amdgpu_target={0}".format(gfx), when="+rocm amdgpu_target={0}".format(gfx)
var/spack/repos/builtin/packages/ratel/package.py:    # Kokkos required for AMD GPUs
var/spack/repos/builtin/packages/ratel/package.py:    depends_on("petsc+kokkos", when="+rocm")
var/spack/repos/builtin/packages/raylib/package.py:    # If enabled, allows Mesa to utilize software-based OpenGL rendering on systems without a GPU.
var/spack/repos/builtin/packages/tar/package.py:    # The NVIDIA compilers do not currently support some GNU builtins.
var/spack/repos/builtin/packages/sensei/sensei-find-mpi-component-cxx-pr68.patch:@@ -16,7 +16,7 @@ elseif (ENABLE_CORI_GPU OR (NOT DEFINED ENABLE_CORI_GPU AND NOT ("$ENV{OPENMPI_D
var/spack/repos/builtin/packages/arborx/package.py:class Arborx(CMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/arborx/package.py:    depends_on("rocthrust", when="+rocm")
var/spack/repos/builtin/packages/arborx/package.py:    patch("0001-update-major-version-required-for-rocm-6.0.patch", when="@:1.5+rocm ^hip@6.0:")
var/spack/repos/builtin/packages/arborx/package.py:    for arch in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/arborx/package.py:        cuda_dep = f"+cuda cuda_arch={arch}"
var/spack/repos/builtin/packages/arborx/package.py:        depends_on(f"kokkos {cuda_dep}", when=f"~trilinos {cuda_dep}")
var/spack/repos/builtin/packages/arborx/package.py:        depends_on(f"trilinos {cuda_dep}", when=f"+trilinos {cuda_dep}")
var/spack/repos/builtin/packages/arborx/package.py:    for arch in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/arborx/package.py:        rocm_dep = f"+rocm amdgpu_target={arch}"
var/spack/repos/builtin/packages/arborx/package.py:        depends_on(f"kokkos {rocm_dep}", when=f"~trilinos {rocm_dep}")
var/spack/repos/builtin/packages/arborx/package.py:        depends_on(f"trilinos {rocm_dep}", when=f"+trilinos {rocm_dep}")
var/spack/repos/builtin/packages/arborx/package.py:    conflicts("+cuda", when="cuda_arch=none")
var/spack/repos/builtin/packages/arborx/package.py:    depends_on("kokkos+cuda_lambda", when="~trilinos+cuda")
var/spack/repos/builtin/packages/arborx/package.py:    # - current version of Trilinos package does not allow enabling CUDA
var/spack/repos/builtin/packages/arborx/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/arborx/package.py:        if spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/arborx/0001-update-major-version-required-for-rocm-6.0.patch:Subject: [PATCH] Changed required version of rocthrust to 3 for rocm 6.0
var/spack/repos/builtin/packages/arborx/0001-update-major-version-required-for-rocm-6.0.patch:   # Require at least rocThrust-2.10.5 (that comes with ROCm 3.9) because
var/spack/repos/builtin/packages/arborx/trilinos14.0-kokkos-major-version.patch: if(Kokkos_ENABLE_CUDA)
var/spack/repos/builtin/packages/arborx/trilinos14.0-kokkos-major-version.patch:   kokkos_check(OPTIONS CUDA_LAMBDA)
var/spack/repos/builtin/packages/mvapich2/package.py:    variant("cuda", default=False, description="Enable CUDA extension")
var/spack/repos/builtin/packages/mvapich2/package.py:    depends_on("cuda", when="+cuda")
var/spack/repos/builtin/packages/mvapich2/package.py:            if re.search("--enable-cuda", output):
var/spack/repos/builtin/packages/mvapich2/package.py:                variants += "+cuda"
var/spack/repos/builtin/packages/mvapich2/package.py:                variants += "~cuda"
var/spack/repos/builtin/packages/mvapich2/package.py:        if "+cuda" in self.spec:
var/spack/repos/builtin/packages/mvapich2/package.py:            args.extend(["--enable-cuda", "--with-cuda={0}".format(spec["cuda"].prefix)])
var/spack/repos/builtin/packages/mvapich2/package.py:            args.append("--disable-cuda")
var/spack/repos/builtin/packages/mvapich2/package.py:            args.append("--disable-opencl")
var/spack/repos/builtin/packages/rocm-tensile/package.py:class RocmTensile(CMakePackage):
var/spack/repos/builtin/packages/rocm-tensile/package.py:    homepage = "https://github.com/ROCm/Tensile/"
var/spack/repos/builtin/packages/rocm-tensile/package.py:    git = "https://github.com/ROCm/Tensile.git"
var/spack/repos/builtin/packages/rocm-tensile/package.py:    url = "https://github.com/ROCm/Tensile/archive/rocm-6.1.1.tar.gz"
var/spack/repos/builtin/packages/rocm-tensile/package.py:    tags = ["rocm"]
var/spack/repos/builtin/packages/rocm-tensile/package.py:        description="AMD GPU architecture",
var/spack/repos/builtin/packages/rocm-tensile/package.py:        depends_on(f"rocm-cmake@{ver}", type="build", when=f"@{ver}")
var/spack/repos/builtin/packages/rocm-tensile/package.py:        depends_on(f"rocminfo@{ver}", type="build", when=f"@{ver}")
var/spack/repos/builtin/packages/rocm-tensile/package.py:        depends_on(f"rocm-openmp-extras@{ver}", when=f"@{ver}")
var/spack/repos/builtin/packages/rocm-tensile/package.py:        depends_on(f"rocm-smi-lib@{ver}", type="build", when=f"@{ver}")
var/spack/repos/builtin/packages/rocm-tensile/package.py:    def get_gpulist_for_tensile_support(self):
var/spack/repos/builtin/packages/rocm-tensile/package.py:            self.define("ROCM_OPENMP_EXTRAS_DIR", self.spec["rocm-openmp-extras"].prefix),
var/spack/repos/builtin/packages/rocm-tensile/package.py:                self.define("CMAKE_HIP_ARCHITECTURES", self.get_gpulist_for_tensile_support())
var/spack/repos/builtin/packages/rocm-tensile/package.py:                self.define("Tensile_ARCHITECTURE", self.get_gpulist_for_tensile_support())
var/spack/repos/builtin/packages/rocm-tensile/package.py:            args.append(self.define("__skip_rocmclang", "ON"))
var/spack/repos/builtin/packages/rocm-tensile/0003-require-openmp-extras-when-tensile-use-openmp.patch:Subject: [PATCH] Include rocm-openmp-extras header and omp library
var/spack/repos/builtin/packages/rocm-tensile/0003-require-openmp-extras-when-tensile-use-openmp.patch:+    target_include_directories(TensileClient PUBLIC "${ROCM_OPENMP_EXTRAS_DIR}/include")
var/spack/repos/builtin/packages/rocm-tensile/0003-require-openmp-extras-when-tensile-use-openmp.patch:+    target_link_libraries(TensileClient PRIVATE "${ROCM_OPENMP_EXTRAS_DIR}/lib/libomp.so")
var/spack/repos/builtin/packages/rocm-tensile/0003-require-openmp-extras-when-tensile-use-openmp.patch:+    target_include_directories(tensile_client PUBLIC "${ROCM_OPENMP_EXTRAS_DIR}/include")
var/spack/repos/builtin/packages/rocm-tensile/0003-require-openmp-extras-when-tensile-use-openmp.patch:+    target_link_libraries(tensile_client PRIVATE "${ROCM_OPENMP_EXTRAS_DIR}/lib/libomp.so")
var/spack/repos/builtin/packages/aws-ofi-rccl/package.py:    homepage = "https://github.com/ROCm/aws-ofi-rccl"
var/spack/repos/builtin/packages/aws-ofi-rccl/package.py:    git = "https://github.com/ROCm/aws-ofi-rccl.git"
var/spack/repos/builtin/packages/aws-ofi-rccl/package.py:    url = "https://github.com/ROCm/aws-ofi-rccl.git"
var/spack/repos/builtin/packages/aws-ofi-rccl/package.py:    tags = ["rocm"]
var/spack/repos/builtin/packages/aluminum/package.py:class Aluminum(CachedCMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/aluminum/package.py:    algorithms. Blocking and non-blocking algorithms and GPU-aware
var/spack/repos/builtin/packages/aluminum/package.py:        "cuda_rma",
var/spack/repos/builtin/packages/aluminum/package.py:        when="+cuda",
var/spack/repos/builtin/packages/aluminum/package.py:        description="Builds with support for CUDA intra-node "
var/spack/repos/builtin/packages/aluminum/package.py:    variant("nccl", default=False, description="Builds with support for NCCL communication lib")
var/spack/repos/builtin/packages/aluminum/package.py:    variant("nvtx", default=False, when="+cuda", description="Enable profiling via nvprof/NVTX")
var/spack/repos/builtin/packages/aluminum/package.py:        "roctracer", default=False, when="+rocm", description="Enable profiling via rocprof/roctx"
var/spack/repos/builtin/packages/aluminum/package.py:    conflicts("+cuda", when="+rocm", msg="CUDA and ROCm support are mutually exclusive")
var/spack/repos/builtin/packages/aluminum/package.py:    with when("+cuda"):
var/spack/repos/builtin/packages/aluminum/package.py:        depends_on("cub", when="^cuda@:10")
var/spack/repos/builtin/packages/aluminum/package.py:        depends_on("hwloc +cuda +nvml")
var/spack/repos/builtin/packages/aluminum/package.py:        with when("+nccl"):
var/spack/repos/builtin/packages/aluminum/package.py:            depends_on("nccl@2.7.0-0:")
var/spack/repos/builtin/packages/aluminum/package.py:            for arch in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/aluminum/package.py:                    "nccl +cuda cuda_arch={0}".format(arch),
var/spack/repos/builtin/packages/aluminum/package.py:                    when="+cuda cuda_arch={0}".format(arch),
var/spack/repos/builtin/packages/aluminum/package.py:                depends_on("aws-ofi-nccl")  # Note: NOT a CudaPackage
var/spack/repos/builtin/packages/aluminum/package.py:    with when("+rocm"):
var/spack/repos/builtin/packages/aluminum/package.py:        for val in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/aluminum/package.py:                "hipcub +rocm amdgpu_target={0}".format(val), when="amdgpu_target={0}".format(val)
var/spack/repos/builtin/packages/aluminum/package.py:                "hwloc@2.3.0: +rocm amdgpu_target={0}".format(val),
var/spack/repos/builtin/packages/aluminum/package.py:                when="amdgpu_target={0}".format(val),
var/spack/repos/builtin/packages/aluminum/package.py:            # RCCL is *NOT* implented as a ROCmPackage
var/spack/repos/builtin/packages/aluminum/package.py:                "rccl amdgpu_target={0}".format(val), when="+nccl amdgpu_target={0}".format(val)
var/spack/repos/builtin/packages/aluminum/package.py:                "roctracer-dev +rocm amdgpu_target={0}".format(val),
var/spack/repos/builtin/packages/aluminum/package.py:                when="+roctracer amdgpu_target={0}".format(val),
var/spack/repos/builtin/packages/aluminum/package.py:            depends_on("aws-ofi-rccl", when="+nccl")
var/spack/repos/builtin/packages/aluminum/package.py:    def get_cuda_flags(self):
var/spack/repos/builtin/packages/aluminum/package.py:        if spec.satisfies("^cuda+allow-unsupported-compilers"):
var/spack/repos/builtin/packages/aluminum/package.py:        entries.append(cmake_cache_option("ALUMINUM_ENABLE_CUDA", "+cuda" in spec))
var/spack/repos/builtin/packages/aluminum/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/aluminum/package.py:            entries.append(cmake_cache_string("CMAKE_CUDA_STANDARD", "17"))
var/spack/repos/builtin/packages/aluminum/package.py:            if not spec.satisfies("cuda_arch=none"):
var/spack/repos/builtin/packages/aluminum/package.py:                archs = spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/aluminum/package.py:                entries.append(cmake_cache_string("CMAKE_CUDA_ARCHITECTURES", arch_str))
var/spack/repos/builtin/packages/aluminum/package.py:            # FIXME: Should this use the "cuda_flags" function of the
var/spack/repos/builtin/packages/aluminum/package.py:            # CudaPackage class or something? There might be other
var/spack/repos/builtin/packages/aluminum/package.py:            cuda_flags = self.get_cuda_flags()
var/spack/repos/builtin/packages/aluminum/package.py:            if len(cuda_flags) > 0:
var/spack/repos/builtin/packages/aluminum/package.py:                entries.append(cmake_cache_string("CMAKE_CUDA_FLAGS", " ".join(cuda_flags)))
var/spack/repos/builtin/packages/aluminum/package.py:        entries.append(cmake_cache_option("ALUMINUM_ENABLE_ROCM", "+rocm" in spec))
var/spack/repos/builtin/packages/aluminum/package.py:        if spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/aluminum/package.py:            if not spec.satisfies("amdgpu_target=none"):
var/spack/repos/builtin/packages/aluminum/package.py:                archs = self.spec.variants["amdgpu_target"].value
var/spack/repos/builtin/packages/aluminum/package.py:                entries.append(cmake_cache_string("AMDGPU_TARGETS", arch_str))
var/spack/repos/builtin/packages/aluminum/package.py:                entries.append(cmake_cache_string("GPU_TARGETS", arch_str))
var/spack/repos/builtin/packages/aluminum/package.py:        entries.append(cmake_cache_option("ALUMINUM_ENABLE_MPI_CUDA", "+cuda_rma" in spec))
var/spack/repos/builtin/packages/aluminum/package.py:        entries.append(cmake_cache_option("ALUMINUM_ENABLE_MPI_CUDA_RMA", "+cuda_rma" in spec))
var/spack/repos/builtin/packages/aluminum/package.py:        entries.append(cmake_cache_option("ALUMINUM_ENABLE_NCCL", "+nccl" in spec))
var/spack/repos/builtin/packages/rocm-gdb/package.py:class RocmGdb(AutotoolsPackage):
var/spack/repos/builtin/packages/rocm-gdb/package.py:    """This is ROCmgdb, the ROCm source-level debugger for Linux,
var/spack/repos/builtin/packages/rocm-gdb/package.py:    homepage = "https://github.com/ROCm/ROCgdb"
var/spack/repos/builtin/packages/rocm-gdb/package.py:    url = "https://github.com/ROCm/ROCgdb/archive/rocm-6.1.2.tar.gz"
var/spack/repos/builtin/packages/rocm-gdb/package.py:    tags = ["rocm"]
var/spack/repos/builtin/packages/rocm-gdb/package.py:        depends_on(f"rocm-dbgapi@{ver}", type="link", when=f"@{ver}")
var/spack/repos/builtin/packages/rocm-gdb/package.py:        depends_on(f"rocm-core@{ver}", when=f"@{ver}")
var/spack/repos/builtin/packages/rocm-gdb/package.py:            "--with-bugurl=https://github.com/ROCm/ROCgdb/issues",
var/spack/repos/builtin/packages/rocm-gdb/package.py:            "--with-pkgversion=-ROCm",
var/spack/repos/builtin/packages/rocm-gdb/package.py:            "--with-rocm-dbgapi={0}".format(self.spec["rocm-dbgapi"].prefix),
var/spack/repos/builtin/packages/libtiff/package.py:        # Remove flags not recognized by the NVIDIA compiler
var/spack/repos/builtin/packages/apache-tvm/package.py:class ApacheTvm(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/apache-tvm/package.py:    CPUs, GPUs, and machine learning accelerators. It aims to enable machine
var/spack/repos/builtin/packages/apache-tvm/package.py:    depends_on("cuda@8:", when="+cuda")
var/spack/repos/builtin/packages/apache-tvm/package.py:            self.define_from_variant("USE_CUDA", "cuda"),
var/spack/repos/builtin/packages/flann/package.py:    variant("cuda", default=False, description="Build the CUDA library.")
var/spack/repos/builtin/packages/flann/package.py:    depends_on("cuda", when="+cuda")
var/spack/repos/builtin/packages/flann/package.py:        cuda_lib = "ON" if "+cuda" in spec else "OFF"
var/spack/repos/builtin/packages/flann/package.py:        args.append("-DBUILD_CUDA_LIB:BOOL={0}".format(cuda_lib))
var/spack/repos/builtin/packages/flann/linux-gcc-cmakev3.11-plus.patch:@@ -29,7 +29,7 @@ if (BUILD_CUDA_LIB)
var/spack/repos/builtin/packages/tiny-tensor-compiler/package.py:    """A compiler for tensor computations on GPUs and other devices,
var/spack/repos/builtin/packages/tiny-tensor-compiler/package.py:    supporting the OpenCL, Level Zero, and SYCL runtime."""
var/spack/repos/builtin/packages/tiny-tensor-compiler/package.py:    variant("opencl", default=True, description="Build tintc_cl (OpenCL runtime)")
var/spack/repos/builtin/packages/tiny-tensor-compiler/package.py:    requires("+opencl +level-zero", when="+sycl")
var/spack/repos/builtin/packages/tiny-tensor-compiler/package.py:    depends_on("double-batched-fft-library ~sycl ~level-zero ~opencl@0.5.1:", type="link")
var/spack/repos/builtin/packages/tiny-tensor-compiler/package.py:    depends_on("opencl-c-headers@2022.01.04:", when="+opencl")
var/spack/repos/builtin/packages/tiny-tensor-compiler/package.py:    depends_on("opencl-icd-loader@2022.01.04:", when="+opencl", type="link")
var/spack/repos/builtin/packages/tiny-tensor-compiler/package.py:            self.define_from_variant("BUILD_OPENCL", "opencl"),
var/spack/repos/builtin/packages/abinit/fix_for_fujitsu.patch: dependencies = gpu
var/spack/repos/builtin/packages/cosmoflow-benchmark/package.py:class CosmoflowBenchmark(Package, CudaPackage):
var/spack/repos/builtin/packages/cosmoflow-benchmark/package.py:    depends_on("py-tensorflow+cuda", when="+cuda", type=("build", "run"))
var/spack/repos/builtin/packages/cosmoflow-benchmark/package.py:    depends_on("py-tensorflow~cuda~nccl", when="~cuda", type=("build", "run"))
var/spack/repos/builtin/packages/cosmoflow-benchmark/package.py:    depends_on("py-torch+cuda", when="+cuda", type=("build", "run"))
var/spack/repos/builtin/packages/cosmoflow-benchmark/package.py:    depends_on("py-torch~cuda~nccl", when="~cuda", type=("build", "run"))
var/spack/repos/builtin/packages/cosmoflow-benchmark/package.py:    depends_on("py-horovod tensor_ops=mpi", when="~cuda", type=("build", "run"))
var/spack/repos/builtin/packages/psrdada/package.py:class Psrdada(AutotoolsPackage, CudaPackage):
var/spack/repos/builtin/packages/psrdada/package.py:    conflicts("~cuda", msg="You must specify +cuda")
var/spack/repos/builtin/packages/psrdada/package.py:    conflicts("cuda@11.8")
var/spack/repos/builtin/packages/psrdada/package.py:    conflicts("cuda_arch=none", msg="You must specify the CUDA architecture")
var/spack/repos/builtin/packages/psrdada/package.py:    depends_on("cuda", type="build")
var/spack/repos/builtin/packages/pika/package.py:class Pika(CMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/pika/package.py:    conflicts("cxxstd=20", when="+cuda ^cmake@:3.25.1")
var/spack/repos/builtin/packages/pika/package.py:    conflicts("cxxstd=23", when="+cuda")
var/spack/repos/builtin/packages/pika/package.py:        requires("%nvhpc", when=f"cxxstd={cxxstd} ^cuda@:11")
var/spack/repos/builtin/packages/pika/package.py:    conflicts("^fmt@10:", when="@:0.15 +cuda")
var/spack/repos/builtin/packages/pika/package.py:    conflicts("^fmt@10:", when="@:0.15 +rocm")
var/spack/repos/builtin/packages/pika/package.py:    depends_on("cuda@11:", when="+cuda")
var/spack/repos/builtin/packages/pika/package.py:    depends_on("hip@5.2:", when="@0.8: +rocm")
var/spack/repos/builtin/packages/pika/package.py:    conflicts("%gcc@13:", when="+rocm")
var/spack/repos/builtin/packages/pika/package.py:    depends_on("hipblas", when="@:0.8 +rocm")
var/spack/repos/builtin/packages/pika/package.py:    depends_on("rocblas", when="+rocm")
var/spack/repos/builtin/packages/pika/package.py:    depends_on("rocsolver", when="@0.5: +rocm")
var/spack/repos/builtin/packages/pika/package.py:    depends_on("whip@0.1: +rocm", when="@0.9: +rocm")
var/spack/repos/builtin/packages/pika/package.py:    depends_on("whip@0.1: +cuda", when="@0.9: +cuda")
var/spack/repos/builtin/packages/pika/package.py:    with when("+rocm"):
var/spack/repos/builtin/packages/pika/package.py:        for val in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/pika/package.py:            depends_on(f"whip@0.1: amdgpu_target={val}", when=f"@0.9: amdgpu_target={val}")
var/spack/repos/builtin/packages/pika/package.py:            depends_on(f"rocsolver amdgpu_target={val}", when=f"@0.5: amdgpu_target={val}")
var/spack/repos/builtin/packages/pika/package.py:            depends_on(f"rocblas amdgpu_target={val}", when=f"amdgpu_target={val}")
var/spack/repos/builtin/packages/pika/package.py:    with when("+cuda"):
var/spack/repos/builtin/packages/pika/package.py:        for val in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/pika/package.py:            depends_on(f"whip@0.1: cuda_arch={val}", when=f"@0.9: cuda_arch={val}")
var/spack/repos/builtin/packages/pika/package.py:            self.define_from_variant("PIKA_WITH_CUDA", "cuda"),
var/spack/repos/builtin/packages/pika/package.py:            self.define_from_variant("PIKA_WITH_HIP", "rocm"),
var/spack/repos/builtin/packages/pika/package.py:        if spec.satisfies("@:0.7 +rocm"):
var/spack/repos/builtin/packages/pika/package.py:                args.append(self.define("__skip_rocmclang", True))
var/spack/repos/builtin/packages/pika/package.py:        if spec.satisfies("@0.8: +rocm"):
var/spack/repos/builtin/packages/pika/package.py:            rocm_archs = spec.variants["amdgpu_target"].value
var/spack/repos/builtin/packages/pika/package.py:            if "none" not in rocm_archs:
var/spack/repos/builtin/packages/pika/package.py:                rocm_archs = ";".join(rocm_archs)
var/spack/repos/builtin/packages/pika/package.py:                args.append(self.define("CMAKE_HIP_ARCHITECTURES", rocm_archs))
var/spack/repos/builtin/packages/gearshifft/package.py:    depends_on("cuda@8.0:", when="+cufft")
var/spack/repos/builtin/packages/gearshifft/package.py:    depends_on("opencl@1.2:", when="+clfft")
var/spack/repos/builtin/packages/kokkos-nvcc-wrapper/package.py:    depends_on("cuda")
var/spack/repos/builtin/packages/kokkos-nvcc-wrapper/package.py:        env.set("CUDA_ROOT", dependent_spec["cuda"].prefix)
var/spack/repos/builtin/packages/cardioid/package.py:    variant("cuda", default=False, description="Build with cuda support")
var/spack/repos/builtin/packages/cardioid/package.py:    depends_on("cuda", when="+cuda")
var/spack/repos/builtin/packages/cardioid/package.py:    depends_on("hypre+cuda", when="+mfem+cuda")
var/spack/repos/builtin/packages/cardioid/package.py:        if self.spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/cardioid/package.py:            args.append("-DENABLE_CUDA:BOOL=ON")
var/spack/repos/builtin/packages/cardioid/package.py:            args.append("-DCUDA_TOOLKIT_ROOT:PATH=" + spec["cuda"].prefix)
var/spack/repos/builtin/packages/cardioid/package.py:            args.append("-DENABLE_CUDA:BOOL=OFF")
var/spack/repos/builtin/packages/rocm-cmake/package.py:class RocmCmake(CMakePackage):
var/spack/repos/builtin/packages/rocm-cmake/package.py:    """rocm-cmake provides CMake modules for common build tasks
var/spack/repos/builtin/packages/rocm-cmake/package.py:    in the ROCm software stack"""
var/spack/repos/builtin/packages/rocm-cmake/package.py:    homepage = "https://github.com/ROCm/rocm-cmake"
var/spack/repos/builtin/packages/rocm-cmake/package.py:    git = "https://github.com/ROCm/rocm-cmake.git"
var/spack/repos/builtin/packages/rocm-cmake/package.py:    url = "https://github.com/ROCm/rocm-cmake/archive/rocm-6.1.2.tar.gz"
var/spack/repos/builtin/packages/rocm-cmake/package.py:    tags = ["rocm"]
var/spack/repos/builtin/packages/rocm-cmake/package.py:        depends_on(f"rocm-core@{ver}", when=f"@{ver}")
var/spack/repos/builtin/packages/rocm-cmake/package.py:            prefixes = ";".join([self.spec["rocm-cmake"].prefix])
var/spack/repos/builtin/packages/py-pyfr/package.py:class PyPyfr(PythonPackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/py-pyfr/package.py:    variant("cuda", default=False, description="CUDA backend support")
var/spack/repos/builtin/packages/py-pyfr/package.py:    depends_on("cuda@8.0: +allow-unsupported-compilers", when="@:1.14.0 +cuda", type=("run"))
var/spack/repos/builtin/packages/py-pyfr/package.py:    depends_on("cuda@11.4.0: +allow-unsupported-compilers", when="@1.15.0: +cuda", type=("run"))
var/spack/repos/builtin/packages/py-pyfr/package.py:        # LD_LIBRARY_PATH needed for cuda
var/spack/repos/builtin/packages/py-pyfr/package.py:        if "+cuda" in self.spec:
var/spack/repos/builtin/packages/py-pyfr/package.py:            env.prepend_path("LD_LIBRARY_PATH", self.spec["cuda"].libs.directories[0])
var/spack/repos/builtin/packages/roctracer-dev/0002-use-clang-18.patch:@@ -34,7 +34,7 @@ if(DEFINED ROCM_PATH)
var/spack/repos/builtin/packages/roctracer-dev/0002-use-clang-18.patch:              PATHS "${ROCM_PATH}"
var/spack/repos/builtin/packages/roctracer-dev/package.py:class RoctracerDev(CMakePackage, ROCmPackage):
var/spack/repos/builtin/packages/roctracer-dev/package.py:    homepage = "https://github.com/ROCm/roctracer"
var/spack/repos/builtin/packages/roctracer-dev/package.py:    git = "https://github.com/ROCm/roctracer.git"
var/spack/repos/builtin/packages/roctracer-dev/package.py:    url = "https://github.com/ROCm/roctracer/archive/rocm-6.1.2.tar.gz"
var/spack/repos/builtin/packages/roctracer-dev/package.py:    tags = ["rocm"]
var/spack/repos/builtin/packages/roctracer-dev/package.py:        depends_on(f"rocminfo@{ver}", when=f"@{ver}")
var/spack/repos/builtin/packages/roctracer-dev/package.py:        depends_on(f"rocm-core@{ver}", when=f"@{ver}")
var/spack/repos/builtin/packages/roctracer-dev/package.py:        match = re.search(r"rocm-(\d+)\.(\d+)\.(\d)/lib/lib\S*\.so\.\d+\.\d+\.\d+", lib)
var/spack/repos/builtin/packages/bigdft-chess/package.py:class BigdftChess(AutotoolsPackage, CudaPackage):
var/spack/repos/builtin/packages/bigdft-chess/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/bigdft-chess/package.py:            args.append("--enable-cuda-gpu")
var/spack/repos/builtin/packages/bigdft-chess/package.py:            args.append(f"--with-cuda-path={spec['cuda'].prefix}")
var/spack/repos/builtin/packages/bigdft-chess/package.py:            args.append(f"--with-cuda-libs={spec['cuda'].libs.link_flags}")
var/spack/repos/builtin/packages/rocal/package.py:    homepage = "https://github.com/ROCm/rocAL"
var/spack/repos/builtin/packages/rocal/package.py:    url = "https://github.com/ROCm/rocAL/archive/refs/tags/rocm-6.2.0.tar.gz"
var/spack/repos/builtin/packages/rocal/package.py:        depends_on(f"llvm-amdgpu@{ver}", when=f"@{ver}")
var/spack/repos/builtin/packages/rocal/package.py:            r"${ROCM_PATH}/llvm/bin/clang++",
var/spack/repos/builtin/packages/rocal/package.py:            "{0}/bin/clang++".format(self.spec["llvm-amdgpu"].prefix),
var/spack/repos/builtin/packages/rocthrust/package.py:    HIP/ROCm platform, which uses the rocPRIM library. The HIP ported
var/spack/repos/builtin/packages/rocthrust/package.py:    library works on HIP/ROCm platforms"""
var/spack/repos/builtin/packages/rocthrust/package.py:    homepage = "https://github.com/ROCm/rocThrust"
var/spack/repos/builtin/packages/rocthrust/package.py:    git = "https://github.com/ROCm/rocThrust.git"
var/spack/repos/builtin/packages/rocthrust/package.py:    url = "https://github.com/ROCm/rocThrust/archive/rocm-6.1.0.tar.gz"
var/spack/repos/builtin/packages/rocthrust/package.py:    tags = ["rocm"]
var/spack/repos/builtin/packages/rocthrust/package.py:    amdgpu_targets = ROCmPackage.amdgpu_targets
var/spack/repos/builtin/packages/rocthrust/package.py:    # the rocthrust library itself is header-only, but the build_type and amdgpu_target
var/spack/repos/builtin/packages/rocthrust/package.py:        "amdgpu_target",
var/spack/repos/builtin/packages/rocthrust/package.py:        description="AMD GPU architecture",
var/spack/repos/builtin/packages/rocthrust/package.py:        values=auto_or_any_combination_of(*amdgpu_targets),
var/spack/repos/builtin/packages/rocthrust/package.py:        depends_on(f"rocm-cmake@{ver}:", type="build", when=f"@{ver}")
var/spack/repos/builtin/packages/rocthrust/package.py:        if "auto" not in self.spec.variants["amdgpu_target"]:
var/spack/repos/builtin/packages/rocthrust/package.py:            args.append(self.define_from_variant("AMDGPU_TARGETS", "amdgpu_target"))
var/spack/repos/builtin/packages/rocthrust/package.py:            args.append(self.define("__skip_rocmclang", "ON"))
var/spack/repos/builtin/packages/sundials/package.py:class Sundials(CMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/sundials/package.py:    conflicts("+cuda", when="@:2.7.0")
var/spack/repos/builtin/packages/sundials/package.py:    conflicts("+rocm", when="@:5.6.0")
var/spack/repos/builtin/packages/sundials/package.py:    # rocm+examples and cstd do not work together in 6.0.0
var/spack/repos/builtin/packages/sundials/package.py:    conflicts("+rocm+examples", when="@6.0.0")
var/spack/repos/builtin/packages/sundials/package.py:    depends_on("raja+cuda", when="+raja +cuda")
var/spack/repos/builtin/packages/sundials/package.py:    depends_on("raja+rocm", when="+raja +rocm")
var/spack/repos/builtin/packages/sundials/package.py:    for cuda_arch in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/sundials/package.py:            "kokkos+cuda+cuda_lambda+cuda_constexpr cuda_arch=%s" % cuda_arch,
var/spack/repos/builtin/packages/sundials/package.py:            when="+kokkos +cuda cuda_arch=%s" % cuda_arch,
var/spack/repos/builtin/packages/sundials/package.py:            "kokkos-kernels+cuda cuda_arch=%s" % cuda_arch,
var/spack/repos/builtin/packages/sundials/package.py:            when="+kokkos-kernels +cuda cuda_arch=%s" % cuda_arch,
var/spack/repos/builtin/packages/sundials/package.py:    for rocm_arch in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/sundials/package.py:            "kokkos+rocm amdgpu_target=%s" % rocm_arch,
var/spack/repos/builtin/packages/sundials/package.py:            when="+kokkos +rocm amdgpu_target=%s" % rocm_arch,
var/spack/repos/builtin/packages/sundials/package.py:    patch("sundials-hip-platform.patch", when="@7.0.0 +rocm")
var/spack/repos/builtin/packages/sundials/package.py:    patch("nvector-pic.patch", when="@6.1.0:6.2.0 +rocm")
var/spack/repos/builtin/packages/sundials/package.py:    # Backward compatibility is stopped from ROCm 6.0
var/spack/repos/builtin/packages/sundials/package.py:    patch("Change-HIP_PLATFORM-from-HCC-to-AMD-and-NVCC-to-NVIDIA.patch", when="^hip@6.0 +rocm")
var/spack/repos/builtin/packages/sundials/package.py:                from_variant("CUDA_ENABLE", "cuda"),
var/spack/repos/builtin/packages/sundials/package.py:                from_variant("ENABLE_HIP", "rocm"),
var/spack/repos/builtin/packages/sundials/package.py:        if "+cuda" in spec:
var/spack/repos/builtin/packages/sundials/package.py:            args.append(define("CMAKE_CUDA_ARCHITECTURES", spec.variants["cuda_arch"].value))
var/spack/repos/builtin/packages/sundials/package.py:        if "+rocm" in spec:
var/spack/repos/builtin/packages/sundials/package.py:                    define("CMAKE_C_COMPILER", spec["llvm-amdgpu"].prefix.bin.clang),
var/spack/repos/builtin/packages/sundials/package.py:                    define("HIP_CLANG_INCLUDE_PATH", spec["llvm-amdgpu"].prefix.include),
var/spack/repos/builtin/packages/sundials/package.py:                    define("ROCM_PATH", spec["llvm-amdgpu"].prefix),
var/spack/repos/builtin/packages/sundials/package.py:                    define("AMDGPU_TARGETS", spec.variants["amdgpu_target"].value),
var/spack/repos/builtin/packages/sundials/package.py:            if "+cuda" in spec["ginkgo"] and "+cuda" in spec:
var/spack/repos/builtin/packages/sundials/package.py:                gko_backends.append("CUDA")
var/spack/repos/builtin/packages/sundials/package.py:            if "+rocm" in spec["ginkgo"] and "+rocm" in spec:
var/spack/repos/builtin/packages/sundials/package.py:            if "+cuda" in spec:
var/spack/repos/builtin/packages/sundials/package.py:                args.extend([define("SUNDIALS_MAGMA_BACKENDS", "CUDA")])
var/spack/repos/builtin/packages/sundials/package.py:            if "+rocm" in spec:
var/spack/repos/builtin/packages/sundials/package.py:                    define("EXAMPLES_ENABLE_CUDA", "+examples+cuda" in spec),
var/spack/repos/builtin/packages/sundials/package.py:            "arkode/CXX_serial/Makefile" "cvode/cuda/Makefile",
var/spack/repos/builtin/packages/sundials/package.py:            "nvector/cuda/Makefile",
var/spack/repos/builtin/packages/sundials/package.py:    def test_nvector_cuda(self):
var/spack/repos/builtin/packages/sundials/package.py:        """build and run CUDA N_Vector"""
var/spack/repos/builtin/packages/sundials/package.py:        if "+cuda" not in self.spec:
var/spack/repos/builtin/packages/sundials/package.py:            raise SkipTest("Package must be installed with +cuda")
var/spack/repos/builtin/packages/sundials/package.py:        self.run_example(join_path("nvector", "cuda", "test_nvector_cuda"), ["10", "0", "0"], True)
var/spack/repos/builtin/packages/sundials/package.py:    def test_cvadvdiff_cuda(self):
var/spack/repos/builtin/packages/sundials/package.py:        """build and run CUDA cvAdvDiff_kry"""
var/spack/repos/builtin/packages/sundials/package.py:        if "+cuda" not in self.spec or "+CVODE" not in self.spec:
var/spack/repos/builtin/packages/sundials/package.py:            raise SkipTest("Package must be installed with +cuda+CVODE")
var/spack/repos/builtin/packages/sundials/package.py:        self.run_example(join_path("cvode", "cuda", "cvAdvDiff_kry_cuda"), [], True)
var/spack/repos/builtin/packages/sundials/package.py:        """build and run ROCM N_Vector"""
var/spack/repos/builtin/packages/sundials/package.py:        if "+rocm" not in self.spec:
var/spack/repos/builtin/packages/sundials/package.py:            raise SkipTest("Package must be installed with +rocm")
var/spack/repos/builtin/packages/sundials/package.py:        """build and run ROCM cvAdvDiff_kry"""
var/spack/repos/builtin/packages/sundials/package.py:        if "+rocm" not in self.spec or "+CVODE" not in self.spec:
var/spack/repos/builtin/packages/sundials/package.py:            raise SkipTest("Package must be installed with +rocm+CVODE")
var/spack/repos/builtin/packages/sundials/Change-HIP_PLATFORM-from-HCC-to-AMD-and-NVCC-to-NVIDIA.patch:Subject: [PATCH] Change HIP_PLATFORM from HCC to AMD and NVCC to NVIDIA
var/spack/repos/builtin/packages/sundials/Change-HIP_PLATFORM-from-HCC-to-AMD-and-NVCC-to-NVIDIA.patch:+#elif defined(__HIP_PLATFORM_NVIDIA__)
var/spack/repos/builtin/packages/sundials/sundials-hip-platform.patch:+    set(HIP_PLATFORM "amd" CACHE STRING "HIP platform (amd, nvidia)")
var/spack/repos/builtin/packages/sundials/sundials-hip-platform.patch:+    set(HIP_PLATFORM "$ENV{HIP_PLATFORM}" CACHE STRING "HIP platform (amd, nvidia)")
var/spack/repos/builtin/packages/py-theano/package.py:class PyTheano(PythonPackage, CudaPackage):
var/spack/repos/builtin/packages/py-theano/package.py:    and GPUs."""
var/spack/repos/builtin/packages/py-theano/package.py:    depends_on("cuda", when="+cuda")
var/spack/repos/builtin/packages/py-theano/package.py:    depends_on("cudnn", when="+cuda")
var/spack/repos/builtin/packages/py-theano/package.py:    depends_on("py-pygpu", when="+cuda", type=("build", "run"))
var/spack/repos/builtin/packages/py-theano/package.py:    depends_on("libgpuarray", when="+cuda")
var/spack/repos/builtin/packages/sw4lite/package.py:class Sw4lite(MakefilePackage, CudaPackage):
var/spack/repos/builtin/packages/sw4lite/package.py:        if "+cuda" in self.spec:
var/spack/repos/builtin/packages/sw4lite/package.py:            targets.append("NVCC = {0}".format(self.spec["cuda"].prefix.bin.nvcc))
var/spack/repos/builtin/packages/sw4lite/package.py:            targets.append("gpuarch= {0}".format(self.cuda_flags(cuda_arch)))
var/spack/repos/builtin/packages/sw4lite/package.py:        if "+cuda" in spec:
var/spack/repos/builtin/packages/sw4lite/package.py:            make("-f", "Makefile.cuda", *self.build_targets)
var/spack/repos/builtin/packages/xsdk-examples/package.py:class XsdkExamples(CMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/xsdk-examples/package.py:    depends_on("xsdk+cuda", when="+cuda")
var/spack/repos/builtin/packages/xsdk-examples/package.py:    depends_on("xsdk~cuda", when="~cuda")
var/spack/repos/builtin/packages/xsdk-examples/package.py:    for sm_ in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/xsdk-examples/package.py:        depends_on("xsdk+cuda cuda_arch={0}".format(sm_), when="+cuda cuda_arch={0}".format(sm_))
var/spack/repos/builtin/packages/xsdk-examples/package.py:    depends_on("xsdk+rocm", when="+rocm")
var/spack/repos/builtin/packages/xsdk-examples/package.py:    depends_on("xsdk~rocm", when="~rocm")
var/spack/repos/builtin/packages/xsdk-examples/package.py:    for ac_ in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/xsdk-examples/package.py:            "xsdk+rocm amdgpu_target={0}".format(ac_), when="+rocm amdgpu_target={0}".format(ac_)
var/spack/repos/builtin/packages/xsdk-examples/package.py:        depends_on("sundials+magma", when="+cuda")
var/spack/repos/builtin/packages/xsdk-examples/package.py:            # when CUDA is enabled.
var/spack/repos/builtin/packages/xsdk-examples/package.py:        if "+cuda" in spec:
var/spack/repos/builtin/packages/xsdk-examples/package.py:            archs = ";".join(spec.variants["cuda_arch"].value)
var/spack/repos/builtin/packages/xsdk-examples/package.py:            args.extend(["-DENABLE_CUDA=ON", "-DCMAKE_CUDA_ARCHITECTURES=%s" % archs])
var/spack/repos/builtin/packages/xsdk-examples/package.py:        if "+rocm" in spec:
var/spack/repos/builtin/packages/xsdk-examples/package.py:            archs = ";".join(spec.variants["amdgpu_target"].value)
var/spack/repos/builtin/packages/py-pynvml/package.py:    """Provides a Python interface to GPU management and monitoring
var/spack/repos/builtin/packages/py-pynvml/package.py:    https://developer.nvidia.com/nvidia-management-library-nvml"""
var/spack/repos/builtin/packages/py-pynvml/package.py:    homepage = "https://www.nvidia.com/"
var/spack/repos/builtin/packages/rpp/0003-changes-to-rpp-unit-tests-6.1.patch:     set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -DGPU_SUPPORT=1 -DRPP_BACKEND_HIP=1 -std=gnu++17")
var/spack/repos/builtin/packages/rpp/package.py:    performance computer vision library for AMD (CPU and GPU) with HIP
var/spack/repos/builtin/packages/rpp/package.py:    and OPENCL back-ends"""
var/spack/repos/builtin/packages/rpp/package.py:    homepage = "https://github.com/GPUOpen-ProfessionalCompute-Libraries/rpp"
var/spack/repos/builtin/packages/rpp/package.py:    git = "https://github.com/GPUOpen-ProfessionalCompute-Libraries/rpp.git"
var/spack/repos/builtin/packages/rpp/package.py:    url = "https://github.com/GPUOpen-ProfessionalCompute-Libraries/rpp/archive/refs/tags/rocm-6.1.2.tar.gz"
var/spack/repos/builtin/packages/rpp/package.py:            url = "https://github.com/GPUOpen-ProfessionalCompute-Libraries/rpp/archive/refs/tags/rocm-{0}.tar.gz"
var/spack/repos/builtin/packages/rpp/package.py:            url = "https://github.com/GPUOpen-ProfessionalCompute-Libraries/rpp/archive/{0}.tar.gz"
var/spack/repos/builtin/packages/rpp/package.py:    tags = ["rocm"]
var/spack/repos/builtin/packages/rpp/package.py:    # Adding 3 variants OPENCL ,HIP , CPU with HIP as default.
var/spack/repos/builtin/packages/rpp/package.py:    variant("opencl", default=False, description="Use OPENCL as the backend")
var/spack/repos/builtin/packages/rpp/package.py:                "${ROCM_PATH}/llvm", self.spec["llvm-amdgpu"].prefix, "CMakeLists.txt", string=True
var/spack/repos/builtin/packages/rpp/package.py:                    "CMAKE_CXX_COMPILER {0}/bin/clang++".format(self.spec["llvm-amdgpu"].prefix),
var/spack/repos/builtin/packages/rpp/package.py:        if self.spec.satisfies("+opencl"):
var/spack/repos/builtin/packages/rpp/package.py:                "${ROCM_PATH}",
var/spack/repos/builtin/packages/rpp/package.py:                self.spec["rocm-opencl"].prefix,
var/spack/repos/builtin/packages/rpp/package.py:                "cmake/FindOpenCL.cmake",
var/spack/repos/builtin/packages/rpp/package.py:                "${ROCM_PATH}/include/rpp",
var/spack/repos/builtin/packages/rpp/package.py:                "${ROCM_PATH}/lib",
var/spack/repos/builtin/packages/rpp/package.py:                "${ROCM_PATH}/include/rpp",
var/spack/repos/builtin/packages/rpp/package.py:                "${ROCM_PATH}/lib",
var/spack/repos/builtin/packages/rpp/package.py:                "${ROCM_PATH}/share/rpp/test/cmake",
var/spack/repos/builtin/packages/rpp/package.py:                "${ROCM_PATH}/share/rpp/test/cmake",
var/spack/repos/builtin/packages/rpp/package.py:    depends_on("rocm-openmp-extras")
var/spack/repos/builtin/packages/rpp/package.py:    conflicts("+opencl+hip")
var/spack/repos/builtin/packages/rpp/package.py:        depends_on("rocm-opencl@5:")
var/spack/repos/builtin/packages/rpp/package.py:            env.set("CC", f"{self.spec['llvm-amdgpu'].prefix}/bin/clang")
var/spack/repos/builtin/packages/rpp/package.py:            env.set("CXX", f"{self.spec['llvm-amdgpu'].prefix}/bin/clang")
var/spack/repos/builtin/packages/rpp/package.py:        args.append(self.define("ROCM_OPENMP_EXTRAS_DIR", spec["rocm-openmp-extras"].prefix))
var/spack/repos/builtin/packages/rpp/package.py:        if self.spec.satisfies("+opencl"):
var/spack/repos/builtin/packages/rpp/package.py:            args.append(self.define("BACKEND", "OPENCL"))
var/spack/repos/builtin/packages/rpp/package.py:                    "COMPILER_FOR_HIP", "{0}/bin/clang++".format(spec["llvm-amdgpu"].prefix)
var/spack/repos/builtin/packages/rpp/0001-include-half-openmp-through-spack-package.patch:+ set(LINK_LIBRARY_LIST ${LINK_LIBRARY_LIST} ${ROCM_OPENMP_EXTRAS_DIR}/lib/libomp.so)
var/spack/repos/builtin/packages/rpp/0001-include-half-openmp-through-spack-package.patch:         ${ROCM_PATH}/include
var/spack/repos/builtin/packages/rpp/0001-include-half-openmp-through-spack-package.patch:+        ${ROCM_OPENMP_EXTRAS_DIR}/include
var/spack/repos/builtin/packages/rpp/0001-include-half-openmp-through-spack-package.patch:     set(ROCM_INC ${ROCM_PATH}/include/)
var/spack/repos/builtin/packages/rpp/0001-include-half-openmp-through-spack-package.patch:-    set(INCLUDE_LIST ${ROCM_INC} ${HIP_LOCAL_INCLUDE_DIRS} ${INCLUDE_LIST})
var/spack/repos/builtin/packages/rpp/0001-include-half-openmp-through-spack-package.patch:+    set(INCLUDE_LIST ${ROCM_INC} ${HIP_LOCAL_INCLUDE_DIRS} ${INCLUDE_LIST} ${HALF_INCLUDE_DIR} ${ROCM_OPENMP_EXTRAS_DIR}/include)
var/spack/repos/builtin/packages/rpp/0001-include-half-openmp-through-spack-package.patch:     # Add OpenCL kernels
var/spack/repos/builtin/packages/rpp/0001-include-half-openmp-through-spack-package.patch:     # Add OpenCL specific includes
var/spack/repos/builtin/packages/rpp/0001-include-half-openmp-through-spack-package.patch:     set(ROCM_INC ${ROCM_PATH}/include/)
var/spack/repos/builtin/packages/rpp/0001-include-half-openmp-through-spack-package.patch:-    set(INCLUDE_LIST ${ROCM_INC} ${OCL_LOCAL_INCLUDE_LIST} ${INCLUDE_LIST})
var/spack/repos/builtin/packages/rpp/0001-include-half-openmp-through-spack-package.patch:+    set(INCLUDE_LIST ${ROCM_INC} ${OCL_LOCAL_INCLUDE_LIST} ${INCLUDE_LIST} ${HALF_INCLUDE_DIR} ${ROCM_OPENMP_EXTRAS_DIR}/include)
var/spack/repos/builtin/packages/rpp/0001-include-half-openmp-through-spack-package.patch:         ${ROCM_INC}
var/spack/repos/builtin/packages/rpp/0001-include-half-openmp-through-spack-package.patch:+        ${ROCM_OPENMP_EXTRAS_DIR}/include
var/spack/repos/builtin/packages/rpp/0003-changes-to-rpp-unit-tests.patch:     set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -DGPU_SUPPORT=1 -DRPP_BACKEND_HIP=1 -std=gnu++14")
var/spack/repos/builtin/packages/rpp/0002-declare-handle-in-header.patch:     void rpp_destroy_object_gpu();
var/spack/repos/builtin/packages/rpp/0003-include-half-through-spack-package.patch:         ${ROCM_PATH}/include
var/spack/repos/builtin/packages/rpp/0003-include-half-through-spack-package.patch:     set(ROCM_INC ${ROCM_PATH}/include/)
var/spack/repos/builtin/packages/rpp/0003-include-half-through-spack-package.patch:-    set(INCLUDE_LIST ${ROCM_INC} ${HIP_LOCAL_INCLUDE_DIRS} ${INCLUDE_LIST})
var/spack/repos/builtin/packages/rpp/0003-include-half-through-spack-package.patch:+    set(INCLUDE_LIST ${ROCM_INC} ${HIP_LOCAL_INCLUDE_DIRS} ${INCLUDE_LIST} ${HALF_INCLUDE_DIR})
var/spack/repos/builtin/packages/rpp/0003-include-half-through-spack-package.patch:     # Add OpenCL kernels
var/spack/repos/builtin/packages/rpp/0003-include-half-through-spack-package.patch:     # Add OpenCL specific includes
var/spack/repos/builtin/packages/rpp/0003-include-half-through-spack-package.patch:     set(ROCM_INC ${ROCM_PATH}/include/)
var/spack/repos/builtin/packages/rpp/0003-include-half-through-spack-package.patch:-    set(INCLUDE_LIST ${ROCM_INC} ${OCL_LOCAL_INCLUDE_LIST} ${INCLUDE_LIST})
var/spack/repos/builtin/packages/rpp/0003-include-half-through-spack-package.patch:+    set(INCLUDE_LIST ${ROCM_INC} ${OCL_LOCAL_INCLUDE_LIST} ${INCLUDE_LIST} ${HALF_INCLUDE_DIR})
var/spack/repos/builtin/packages/rpp/0003-include-half-through-spack-package.patch:         ${ROCM_INC}
var/spack/repos/builtin/packages/charmpp/package.py:    # support NVIDIA compilers
var/spack/repos/builtin/packages/charmpp/package.py:    variant("cuda", default=False, description="Enable CUDA toolkit")
var/spack/repos/builtin/packages/charmpp/package.py:    depends_on("cuda", when="+cuda")
var/spack/repos/builtin/packages/charmpp/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/charmpp/package.py:            options.append("cuda")
var/spack/repos/builtin/packages/rocm-smi-lib/package.py:class RocmSmiLib(CMakePackage):
var/spack/repos/builtin/packages/rocm-smi-lib/package.py:    for applications to monitor and control GPU applications."""
var/spack/repos/builtin/packages/rocm-smi-lib/package.py:    homepage = "https://github.com/ROCm/rocm_smi_lib"
var/spack/repos/builtin/packages/rocm-smi-lib/package.py:    git = "https://github.com/ROCm/rocm_smi_lib.git"
var/spack/repos/builtin/packages/rocm-smi-lib/package.py:    url = "https://github.com/ROCm/rocm_smi_lib/archive/rocm-6.1.2.tar.gz"
var/spack/repos/builtin/packages/rocm-smi-lib/package.py:    tags = ["rocm"]
var/spack/repos/builtin/packages/rocm-smi-lib/package.py:    libraries = ["librocm_smi64"]
var/spack/repos/builtin/packages/rocm-smi-lib/package.py:        depends_on(f"rocm-core@{ver}", when=f"@{ver}")
var/spack/repos/builtin/packages/rocm-smi-lib/package.py:        "https://github.com/ROCm/rocm_smi_lib/commit/11f12b86517d0e9868f4d16d74d4e8504c3ba7da.patch?full_index=1",
var/spack/repos/builtin/packages/rocm-smi-lib/package.py:            install_tree(self.prefix.rocm_smi, self.prefix)
var/spack/repos/builtin/packages/rocm-smi-lib/package.py:            shutil.rmtree(self.prefix.rocm_smi)
var/spack/repos/builtin/packages/rocm-smi-lib/package.py:        exe = which(join_path(self.build_directory, "tests", "rocm_smi_test", "rsmitst"))
var/spack/repos/builtin/packages/rocm-smi-lib/disable_pdf_generation_with_doxygen_and_latex.patch:diff --git a/rocm_smi/CMakeLists.txt b/rocm_smi/CMakeLists.txt
var/spack/repos/builtin/packages/rocm-smi-lib/disable_pdf_generation_with_doxygen_and_latex.patch:--- a/rocm_smi/CMakeLists.txt
var/spack/repos/builtin/packages/rocm-smi-lib/disable_pdf_generation_with_doxygen_and_latex.patch:+++ b/rocm_smi/CMakeLists.txt
var/spack/repos/builtin/packages/rocm-smi-lib/disable_pdf_generation_with_doxygen_and_latex.patch:   set (RSMI_MANUAL_NAME "ROCm_SMI_Manual")
var/spack/repos/builtin/packages/papi/package.py:class Papi(AutotoolsPackage, ROCmPackage):
var/spack/repos/builtin/packages/papi/package.py:    variant("cuda", default=False, description="Enable CUDA support")
var/spack/repos/builtin/packages/papi/package.py:    variant("rocm_smi", default=False, description="Enable ROCm SMI support")
var/spack/repos/builtin/packages/papi/package.py:    depends_on("cuda", when="+cuda")
var/spack/repos/builtin/packages/papi/package.py:    depends_on("cuda", when="+nvml")
var/spack/repos/builtin/packages/papi/package.py:    depends_on("hsa-rocr-dev", when="+rocm")
var/spack/repos/builtin/packages/papi/package.py:    depends_on("rocprofiler-dev", when="+rocm")
var/spack/repos/builtin/packages/papi/package.py:    depends_on("llvm-amdgpu", when="+rocm")
var/spack/repos/builtin/packages/papi/package.py:    depends_on("rocm-openmp-extras", when="+rocm")
var/spack/repos/builtin/packages/papi/package.py:    depends_on("rocm-smi-lib", when="+rocm_smi")
var/spack/repos/builtin/packages/papi/package.py:    conflicts("^cuda", when="@:5", msg="CUDA support for versions < 6.0.0 not implemented")
var/spack/repos/builtin/packages/papi/package.py:    conflicts("^cuda@12.4:", when="@:7.1")
var/spack/repos/builtin/packages/papi/package.py:        if "+cuda" in spec:
var/spack/repos/builtin/packages/papi/package.py:            env.set("PAPI_CUDA_ROOT", spec["cuda"].prefix)
var/spack/repos/builtin/packages/papi/package.py:        if "+rocm" in spec:
var/spack/repos/builtin/packages/papi/package.py:            env.set("PAPI_ROCM_ROOT", spec["hsa-rocr-dev"].prefix)
var/spack/repos/builtin/packages/papi/package.py:            env.append_flags("LDFLAGS", "-L%s/lib" % spec["llvm-amdgpu"].prefix)
var/spack/repos/builtin/packages/papi/package.py:        if "+rocm_smi" in spec:
var/spack/repos/builtin/packages/papi/package.py:            env.append_flags("CFLAGS", "-I%s/rocm_smi" % spec["rocm-smi-lib"].prefix.include)
var/spack/repos/builtin/packages/papi/package.py:                "cuda",
var/spack/repos/builtin/packages/papi/package.py:                "rocm",
var/spack/repos/builtin/packages/papi/package.py:                "rocm_smi",
var/spack/repos/builtin/packages/papi/spack-hip-path.patch:diff -Naur papi-7.0.1-orig/src/components/rocm/tests/Makefile papi-7.0.1/src/components/rocm/tests/Makefile
var/spack/repos/builtin/packages/papi/spack-hip-path.patch:--- papi-7.0.1-orig/src/components/rocm/tests/Makefile	2023-06-19 14:27:44.220943888 -0400
var/spack/repos/builtin/packages/papi/spack-hip-path.patch:+++ papi-7.0.1/src/components/rocm/tests/Makefile	2023-06-20 19:09:56.564352488 -0400
var/spack/repos/builtin/packages/papi/spack-hip-path.patch: NAME = rocm
var/spack/repos/builtin/packages/papi/spack-hip-path.patch: PAPI_ROCM_ROOT ?= /opt/rocm
var/spack/repos/builtin/packages/papi/spack-hip-path.patch:-CC       = $(PAPI_ROCM_ROOT)/hip/bin/hipcc
var/spack/repos/builtin/packages/papi/spack-hip-path.patch:-CXX      = $(PAPI_ROCM_ROOT)/hip/bin/hipcc
var/spack/repos/builtin/packages/papi/spack-hip-path.patch:+HIP_PATH ?= $(PAPI_ROCM_ROOT)/hip 
var/spack/repos/builtin/packages/papi/spack-hip-path.patch: CPPFLAGS+= -I$(PAPI_ROCM_ROOT)/include          \
var/spack/repos/builtin/packages/papi/spack-hip-path.patch:            -I$(PAPI_ROCM_ROOT)/include/hip      \
var/spack/repos/builtin/packages/papi/spack-hip-path.patch:            -I$(PAPI_ROCM_ROOT)/include/hsa      \
var/spack/repos/builtin/packages/papi/spack-hip-path.patch:diff -Naur papi-7.0.1-orig/src/components/rocm_smi/tests/Makefile papi-7.0.1/src/components/rocm_smi/tests/Makefile
var/spack/repos/builtin/packages/papi/spack-hip-path.patch:--- papi-7.0.1-orig/src/components/rocm_smi/tests/Makefile	2023-06-19 14:27:44.220943888 -0400
var/spack/repos/builtin/packages/papi/spack-hip-path.patch:+++ papi-7.0.1/src/components/rocm_smi/tests/Makefile	2023-06-20 19:10:49.383840816 -0400
var/spack/repos/builtin/packages/papi/spack-hip-path.patch: NAME=rocm_smi
var/spack/repos/builtin/packages/papi/spack-hip-path.patch: PAPI_ROCM_ROOT ?= /opt/rocm
var/spack/repos/builtin/packages/papi/spack-hip-path.patch:-HIP_PATH= ${PAPI_ROCM_ROOT}/hip
var/spack/repos/builtin/packages/papi/spack-hip-path.patch:+HIP_PATH ?= $(PAPI_ROCM_ROOT)/hip
var/spack/repos/builtin/packages/papi/spack-hip-path.patch: INCLUDE += -I$(PAPI_ROCM_ROOT)/include
var/spack/repos/builtin/packages/papi/spack-hip-path.patch: INCLUDE += -I$(PAPI_ROCM_ROOT)/include/rocm_smi
var/spack/repos/builtin/packages/py-alphafold/package.py:class PyAlphafold(PythonPackage, CudaPackage):
var/spack/repos/builtin/packages/py-alphafold/package.py:        "openmm@7.5.1+cuda",
var/spack/repos/builtin/packages/py-fairscale/package.py:    variant("extra", default=False, description="support for cuda.list.gpu, scaler, weight")
var/spack/repos/builtin/packages/py-fairscale/package.py:    # added extra to support cuda.list.gpu, scaler, and weight (not in pip install)
var/spack/repos/builtin/packages/detray/package.py:    use on GPU platforms."""
var/spack/repos/builtin/packages/detray/package.py:            self.define_from_variant("CMAKE_CUDA_STANDARD", "cxxstd"),
var/spack/repos/builtin/packages/zfp/package.py:class Zfp(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/zfp/package.py:    supports serial and parallel (OpenMP and CUDA) compression of whole
var/spack/repos/builtin/packages/zfp/package.py:    depends_on("cuda@7:", type=("build", "test", "run"), when="+cuda")
var/spack/repos/builtin/packages/zfp/package.py:    variant("cuda", default=False, when="@0.5.4:", description="Enable CUDA execution")
var/spack/repos/builtin/packages/zfp/package.py:            self.define_from_variant("ZFP_WITH_CUDA", "cuda"),
var/spack/repos/builtin/packages/zfp/package.py:        if "+cuda" in spec:
var/spack/repos/builtin/packages/zfp/package.py:            args.append("-DCUDA_BIN_DIR={0}".format(spec["cuda"].prefix.bin))
var/spack/repos/builtin/packages/zfp/package.py:            if not spec.satisfies("cuda_arch=none"):
var/spack/repos/builtin/packages/zfp/package.py:                cuda_arch = spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/zfp/package.py:                args.append("-DCMAKE_CUDA_FLAGS=-arch sm_{0}".format(cuda_arch[0]))
var/spack/repos/builtin/packages/librmm/package.py:    performance in GPU-centric workflows frequently requires
var/spack/repos/builtin/packages/librmm/package.py:    depends_on("cuda@9.0:")
var/spack/repos/builtin/packages/py-gpustat/package.py:class PyGpustat(PythonPackage):
var/spack/repos/builtin/packages/py-gpustat/package.py:    """An utility to monitor NVIDIA GPU status and usage."""
var/spack/repos/builtin/packages/py-gpustat/package.py:    homepage = "https://github.com/wookayin/gpustat"
var/spack/repos/builtin/packages/py-gpustat/package.py:    pypi = "gpustat/gpustat-0.6.0.tar.gz"
var/spack/repos/builtin/packages/py-gpustat/package.py:    depends_on("py-nvidia-ml-py3@7.352.0:", type=("build", "run"))
var/spack/repos/builtin/packages/mgard/package.py:class Mgard(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/mgard/package.py:    depends_on("nvcomp@2.2.0:", when="@2022-11-18:+cuda")
var/spack/repos/builtin/packages/mgard/package.py:    depends_on("nvcomp@2.0.2", when="@:2021-11-12+cuda")
var/spack/repos/builtin/packages/mgard/package.py:    conflicts("cuda_arch=none", when="+cuda")
var/spack/repos/builtin/packages/mgard/package.py:        "~cuda", when="@2021-11-12", msg="without cuda MGARD@2021-11-12 has undefined symbols"
var/spack/repos/builtin/packages/mgard/package.py:    conflicts("protobuf@3.22:", when="+cuda target=aarch64:", msg="nvcc fails on ARM SIMD headers")
var/spack/repos/builtin/packages/mgard/package.py:    conflicts("abseil-cpp@20240116.1", when="+cuda", msg="triggers nvcc parser bug")
var/spack/repos/builtin/packages/mgard/package.py:                "@2020-10-01 %rocmcc@4:",
var/spack/repos/builtin/packages/mgard/package.py:        args.append(self.define_from_variant("MGARD_ENABLE_CUDA", "cuda"))
var/spack/repos/builtin/packages/mgard/package.py:        if "+cuda" in spec:
var/spack/repos/builtin/packages/mgard/package.py:            cuda_arch_list = spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/mgard/package.py:            arch_str = ";".join(cuda_arch_list)
var/spack/repos/builtin/packages/mgard/package.py:            if cuda_arch_list[0] != "none":
var/spack/repos/builtin/packages/mgard/package.py:                args.append(self.define("CMAKE_CUDA_ARCHITECTURES", arch_str))
var/spack/repos/builtin/packages/mgard/package.py:            if "+cuda" in self.spec:
var/spack/repos/builtin/packages/mgard/package.py:                if "75" in cuda_arch:
var/spack/repos/builtin/packages/mgard/package.py:                    args.append("-DMGARD_ENABLE_CUDA_OPTIMIZE_TURING=ON")
var/spack/repos/builtin/packages/mgard/package.py:                if "70" in cuda_arch:
var/spack/repos/builtin/packages/mgard/package.py:                    args.append("-DMGARD_ENABLE_CUDA_OPTIMIZE_VOLTA=ON")
var/spack/repos/builtin/packages/mlc-llm/package.py:class MlcLlm(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/mlc-llm/package.py:    depends_on("cuda@11.8:", when="+cuda")
var/spack/repos/builtin/packages/mlc-llm/package.py:        description="Use FlashInfer? (need CUDA w/ compute capability 80;86;89;90)",
var/spack/repos/builtin/packages/mlc-llm/package.py:        when="+cuda",
var/spack/repos/builtin/packages/mlc-llm/package.py:    conflicts("cuda_arch=none", when="+flash-infer")
var/spack/repos/builtin/packages/mlc-llm/package.py:    unsupported_flash_infer_cuda_archs = filter(
var/spack/repos/builtin/packages/mlc-llm/package.py:        lambda arch: arch not in ["80", "86", "89", "90"], CudaPackage.cuda_arch_values
var/spack/repos/builtin/packages/mlc-llm/package.py:    for arch in unsupported_flash_infer_cuda_archs:
var/spack/repos/builtin/packages/mlc-llm/package.py:            f"cuda_arch={arch}",
var/spack/repos/builtin/packages/mlc-llm/package.py:            msg=f"CUDA architecture {arch} is not supported when +flash-infer",
var/spack/repos/builtin/packages/mlc-llm/package.py:        if self.spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/mlc-llm/package.py:            cmake_config_str += "set(USE_CUDA ON)\n"
var/spack/repos/builtin/packages/mlc-llm/package.py:            cmake_config_str += "set(USE_CUDA OFF)\n"
var/spack/repos/builtin/packages/mlc-llm/package.py:            cuda_archs = ";".join(self.spec.variants["cuda_arch"].value)
var/spack/repos/builtin/packages/mlc-llm/package.py:            cmake_config_str += f"set(FLASHINFER_CUDA_ARCHITECTURES {cuda_archs})\n"
var/spack/repos/builtin/packages/mlc-llm/package.py:            cmake_config_str += f"set(CMAKE_CUDA_ARCHITECTURES {cuda_archs})\n"
var/spack/repos/builtin/packages/py-qiskit-aer/package.py:class PyQiskitAer(PythonPackage, CudaPackage):
var/spack/repos/builtin/packages/py-qiskit-aer/package.py:    depends_on("cuda@10.1:", when="+cuda")
var/spack/repos/builtin/packages/py-qiskit-aer/package.py:        env.set("CUDAHOSTCXX", spack_cxx)
var/spack/repos/builtin/packages/py-qiskit-aer/package.py:        if "+cuda" in self.spec:
var/spack/repos/builtin/packages/py-qiskit-aer/package.py:            args.append("-DAER_THRUST_BACKEND=CUDA")
var/spack/repos/builtin/packages/py-qiskit-aer/package.py:            cuda_archs = spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/py-qiskit-aer/package.py:            if "none" not in cuda_archs:
var/spack/repos/builtin/packages/py-qiskit-aer/package.py:                args.append("-DCUDA_NVCC_FLAGS={0}".format(" ".join(self.cuda_flags(cuda_archs))))
var/spack/repos/builtin/packages/hpctoolkit/correcting-hsa-include-path.patch:@@ -4861,7 +4861,7 @@ case "$ROCM_HSA" in
var/spack/repos/builtin/packages/hpctoolkit/correcting-hsa-include-path.patch:     elif test -f "$ROCM_HSA/include/hsa/hsa.h" ; then
var/spack/repos/builtin/packages/hpctoolkit/correcting-hsa-include-path.patch:       AC_MSG_NOTICE([found $ROCM_HSA/include/hsa/hsa.h])
var/spack/repos/builtin/packages/hpctoolkit/correcting-hsa-include-path.patch:-      ROCM_HSA_IFLAGS="-I$ROCM_HSA/include/hsa"
var/spack/repos/builtin/packages/hpctoolkit/correcting-hsa-include-path.patch:+      ROCM_HSA_IFLAGS="-I$ROCM_HSA/include/hsa -I$ROCM_HSA/include"
var/spack/repos/builtin/packages/hpctoolkit/correcting-hsa-include-path.patch:       ROCM_HSA_INC_MESG="$ROCM_HSA"
var/spack/repos/builtin/packages/hpctoolkit/gcc10-enum.patch:diff --git a/src/tool/hpcrun/gpu/gpu-metrics.h b/src/tool/hpcrun/gpu/gpu-metrics.h
var/spack/repos/builtin/packages/hpctoolkit/gcc10-enum.patch:--- a/src/tool/hpcrun/gpu/gpu-metrics.h
var/spack/repos/builtin/packages/hpctoolkit/gcc10-enum.patch:+++ b/src/tool/hpcrun/gpu/gpu-metrics.h
var/spack/repos/builtin/packages/hpctoolkit/gcc10-enum.patch:   GPU_INST_STALL_ANY                   = 0
var/spack/repos/builtin/packages/hpctoolkit/gcc10-enum.patch: } gpu_inst_stall_all_t;
var/spack/repos/builtin/packages/hpctoolkit/package.py:    # Accelerator variants: cuda, rocm, etc.
var/spack/repos/builtin/packages/hpctoolkit/package.py:    variant("cuda", default=False, description="Support CUDA on NVIDIA GPUs.", when="@2020.03:")
var/spack/repos/builtin/packages/hpctoolkit/package.py:        description="Support Level Zero on Intel GPUs.",
var/spack/repos/builtin/packages/hpctoolkit/package.py:        description="Support instrumenting Intel GPU kernels with Intel GT-Pin",
var/spack/repos/builtin/packages/hpctoolkit/package.py:    variant("opencl", default=False, description="Support offloading with OpenCL.")
var/spack/repos/builtin/packages/hpctoolkit/package.py:    variant("rocm", default=False, description="Support ROCM on AMD GPUs.", when="@2022.04:")
var/spack/repos/builtin/packages/hpctoolkit/package.py:    depends_on("cuda", when="+cuda")
var/spack/repos/builtin/packages/hpctoolkit/package.py:    depends_on("opencl-c-headers", when="+opencl")
var/spack/repos/builtin/packages/hpctoolkit/package.py:    depends_on("hip@4.5:", type=("build", "run"), when="+rocm")
var/spack/repos/builtin/packages/hpctoolkit/package.py:    depends_on("hsa-rocr-dev@4.5:", type=("build", "run"), when="+rocm")
var/spack/repos/builtin/packages/hpctoolkit/package.py:    depends_on("roctracer-dev@4.5:", type=("build", "run"), when="+rocm")
var/spack/repos/builtin/packages/hpctoolkit/package.py:    depends_on("rocprofiler-dev@4.5:", type=("build", "run"), when="+rocm")
var/spack/repos/builtin/packages/hpctoolkit/package.py:        msg="avoid elfutils 0.191 (known critical errors in hpcstruct for CUDA binaries)",
var/spack/repos/builtin/packages/hpctoolkit/package.py:        "^hip@5.3:", when="@:2022.12", msg="rocm 5.3 requires hpctoolkit 2023.03.01 or later"
var/spack/repos/builtin/packages/hpctoolkit/package.py:    conflicts("^hip@6:", when="@:2023", msg="rocm 6.0 requires hpctoolkit 2024.01.1 or later")
var/spack/repos/builtin/packages/hpctoolkit/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/hpctoolkit/package.py:            args.append("--with-cuda=%s" % spec["cuda"].prefix)
var/spack/repos/builtin/packages/hpctoolkit/package.py:        if spec.satisfies("+opencl"):
var/spack/repos/builtin/packages/hpctoolkit/package.py:            args.append("--with-opencl=%s" % spec["opencl-c-headers"].prefix)
var/spack/repos/builtin/packages/hpctoolkit/package.py:        if spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/hpctoolkit/package.py:                    "--with-rocm-hip=%s" % spec["hip"].prefix,
var/spack/repos/builtin/packages/hpctoolkit/package.py:                    "--with-rocm-hsa=%s" % spec["hsa-rocr-dev"].prefix,
var/spack/repos/builtin/packages/hpctoolkit/package.py:                    "--with-rocm-tracer=%s" % spec["roctracer-dev"].prefix,
var/spack/repos/builtin/packages/hpctoolkit/package.py:                    "--with-rocm-profiler=%s" % spec["rocprofiler-dev"].prefix,
var/spack/repos/builtin/packages/hpctoolkit/package.py:            "-Dopencl=" + ("enabled" if spec.satisfies("+opencl") else "disabled"),
var/spack/repos/builtin/packages/hpctoolkit/package.py:            "-Dcuda=" + ("enabled" if spec.satisfies("+cuda") else "disabled"),
var/spack/repos/builtin/packages/hpctoolkit/package.py:            "-Drocm=" + ("enabled" if spec.satisfies("+rocm") else "disabled"),
var/spack/repos/builtin/packages/hpctoolkit/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/hpctoolkit/package.py:            cfg["properties"]["prefix_cuda"] = f"'''{spec['cuda'].prefix}'''"
var/spack/repos/builtin/packages/hpctoolkit/package.py:        if spec.satisfies("+opencl"):
var/spack/repos/builtin/packages/hpctoolkit/package.py:            cfg["properties"]["prefix_opencl"] = f"'''{spec['opencl-c-headers'].prefix}'''"
var/spack/repos/builtin/packages/hpctoolkit/package.py:        if spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/hpctoolkit/package.py:            cfg["properties"]["prefix_rocm_hip"] = f"'''{spec['hip'].prefix}'''"
var/spack/repos/builtin/packages/hpctoolkit/package.py:            cfg["properties"]["prefix_rocm_hsa"] = f"'''{spec['hsa-rocr-dev'].prefix}'''"
var/spack/repos/builtin/packages/hpctoolkit/package.py:            cfg["properties"]["prefix_rocm_tracer"] = f"'''{spec['roctracer-dev'].prefix}'''"
var/spack/repos/builtin/packages/hpctoolkit/package.py:            cfg["properties"]["prefix_rocm_profiler"] = f"'''{spec['rocprofiler-dev'].prefix}'''"
var/spack/repos/builtin/packages/intel-oneapi-dnn/package.py:            return self.component_prefix.cpu_dpcpp_gpu_dpcpp
var/spack/repos/builtin/packages/intel-oneapi-dnn/package.py:            dirs = [self.component_prefix.cpu_dpcpp_gpu_dpcpp.include]
var/spack/repos/builtin/packages/hipblas/remove-hipblas-clients-file-installation.patch:-rocm_install(
var/spack/repos/builtin/packages/hipblas/remove-hipblas-clients-file-installation.patch:-rocm_install(
var/spack/repos/builtin/packages/hipblas/remove-hipblas-clients-file-installation-6.0.patch:-  rocm_install(
var/spack/repos/builtin/packages/hipblas/remove-hipblas-clients-file-installation-6.0.patch:-  rocm_install(
var/spack/repos/builtin/packages/hipblas/package.py:class Hipblas(CMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/hipblas/package.py:    homepage = "https://github.com/ROCm/hipBLAS"
var/spack/repos/builtin/packages/hipblas/package.py:    git = "https://github.com/ROCm/hipBLAS.git"
var/spack/repos/builtin/packages/hipblas/package.py:    url = "https://github.com/ROCm/hipBLAS/archive/rocm-6.0.2.tar.gz"
var/spack/repos/builtin/packages/hipblas/package.py:    tags = ["rocm"]
var/spack/repos/builtin/packages/hipblas/package.py:    # default to an 'auto' variant until amdgpu_targets can be given a better default than 'none'
var/spack/repos/builtin/packages/hipblas/package.py:    amdgpu_targets = ROCmPackage.amdgpu_targets
var/spack/repos/builtin/packages/hipblas/package.py:        "amdgpu_target",
var/spack/repos/builtin/packages/hipblas/package.py:        description="AMD GPU architecture",
var/spack/repos/builtin/packages/hipblas/package.py:        values=spack.variant.DisjointSetsOfValues(("auto",), ("none",), amdgpu_targets)
var/spack/repos/builtin/packages/hipblas/package.py:    variant("rocm", default=True, description="Enable ROCm support")
var/spack/repos/builtin/packages/hipblas/package.py:    conflicts("+cuda +rocm", msg="CUDA and ROCm support are mutually exclusive")
var/spack/repos/builtin/packages/hipblas/package.py:    conflicts("~cuda ~rocm", msg="CUDA or ROCm support is required")
var/spack/repos/builtin/packages/hipblas/package.py:    depends_on("rocm-cmake@5.2.0:", type="build", when="@5.2.0:5.7")
var/spack/repos/builtin/packages/hipblas/package.py:    depends_on("rocm-cmake@4.5.0:", type="build")
var/spack/repos/builtin/packages/hipblas/package.py:        depends_on(f"rocm-cmake@{ver}", when=f"+rocm @{ver}")
var/spack/repos/builtin/packages/hipblas/package.py:        depends_on(f"rocm-openmp-extras@{ver}", type="test", when=f"+rocm @{ver}")
var/spack/repos/builtin/packages/hipblas/package.py:    depends_on("hip +cuda", when="+cuda")
var/spack/repos/builtin/packages/hipblas/package.py:        depends_on(f"rocsolver@{ver}", when=f"+rocm @{ver}")
var/spack/repos/builtin/packages/hipblas/package.py:        depends_on(f"rocblas@{ver}", when=f"+rocm @{ver}")
var/spack/repos/builtin/packages/hipblas/package.py:    for tgt in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/hipblas/package.py:        depends_on(f"rocblas amdgpu_target={tgt}", when=f"+rocm amdgpu_target={tgt}")
var/spack/repos/builtin/packages/hipblas/package.py:        depends_on(f"rocsolver amdgpu_target={tgt}", when=f"+rocm amdgpu_target={tgt}")
var/spack/repos/builtin/packages/hipblas/package.py:            self.define_from_variant("USE_CUDA", "cuda"),
var/spack/repos/builtin/packages/hipblas/package.py:        # FindHIP.cmake is still used for +cuda
var/spack/repos/builtin/packages/hipblas/package.py:        if self.spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/py-pyopencl/package.py:class PyPyopencl(PythonPackage):
var/spack/repos/builtin/packages/py-pyopencl/package.py:    """Python wrapper for OpenCL."""
var/spack/repos/builtin/packages/py-pyopencl/package.py:    homepage = "https://documen.tician.de/pyopencl/"
var/spack/repos/builtin/packages/py-pyopencl/package.py:    pypi = "pyopencl/pyopencl-2020.2.2.tar.gz"
var/spack/repos/builtin/packages/py-pyopencl/package.py:    depends_on("opencl", type=("build", "link", "run"))
var/spack/repos/builtin/packages/py-hpccm/package.py:    homepage = "https://github.com/NVIDIA/hpc-container-maker"
var/spack/repos/builtin/packages/spiner/package.py:class Spiner(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/spiner/package.py:    # Currently the raw cuda backend of ports-of-call is not supported.
var/spack/repos/builtin/packages/spiner/package.py:    for _flag in list(CudaPackage.cuda_arch_values):
var/spack/repos/builtin/packages/spiner/package.py:        depends_on("kokkos@3.3.00: cuda_arch=" + _flag, when="+cuda+kokkos cuda_arch=" + _flag)
var/spack/repos/builtin/packages/spiner/package.py:    for _flag in ("~cuda", "+cuda", "~openmp", "+openmp"):
var/spack/repos/builtin/packages/spiner/package.py:    depends_on("kokkos@3.3.00: ~shared+wrapper+cuda_lambda+cuda_constexpr", when="+cuda+kokkos")
var/spack/repos/builtin/packages/spiner/package.py:    conflicts("+cuda", when="~kokkos")
var/spack/repos/builtin/packages/spiner/package.py:    conflicts("cuda_arch=none", when="+cuda", msg="CUDA architecture is required")
var/spack/repos/builtin/packages/spiner/package.py:            use_cuda_option = "SPINER_TEST_USE_CUDA"
var/spack/repos/builtin/packages/spiner/package.py:            use_cuda_option = "SPINER_USE_CUDA"
var/spack/repos/builtin/packages/spiner/package.py:            self.define_from_variant(use_cuda_option, "cuda"),
var/spack/repos/builtin/packages/spiner/package.py:        if "+cuda" in self.spec:
var/spack/repos/builtin/packages/spiner/package.py:                self.define("CMAKE_CUDA_ARCHITECTURES", self.spec.variants["cuda_arch"].value)
var/spack/repos/builtin/packages/parthenon/package.py:        "pressure", default=False, description="Registry pressure check for Kokkos CUDA kernels"
var/spack/repos/builtin/packages/draco/package.py:    variant("cuda", default=False, description="Enable Cuda/GPU support")
var/spack/repos/builtin/packages/draco/package.py:    depends_on("cuda@11.0:", when="+cuda")
var/spack/repos/builtin/packages/draco/package.py:    conflicts("+cuda", when="@:7.6")
var/spack/repos/builtin/packages/draco/package.py:    patch("d770-nocuda.patch", when="@7.7.0")
var/spack/repos/builtin/packages/draco/package.py:                "-DUSE_CUDA={0}".format("ON" if "+cuda" in self.spec else "OFF"),
var/spack/repos/builtin/packages/draco/d770-nocuda.patch:@@ -741,6 +741,18 @@ macro(dbsSetupCuda)
var/spack/repos/builtin/packages/draco/d770-nocuda.patch:     # User option to disable Cuda, even when it is available.
var/spack/repos/builtin/packages/draco/d770-nocuda.patch:     option(USE_CUDA "Use Cuda?" ON)
var/spack/repos/builtin/packages/draco/d770-nocuda.patch:+  set( HAVE_CUDA ${HAVE_CUDA} CACHE BOOL
var/spack/repos/builtin/packages/draco/d770-nocuda.patch:+    "Should we build CUDA portions of this project?" FORCE )
var/spack/repos/builtin/packages/draco/d770-nocuda.patch:+  if( HAVE_CUDA AND USE_CUDA )
var/spack/repos/builtin/packages/draco/d770-nocuda.patch:+    # Use this string in 'project(foo ${CUDA_DBS_STRING})' commands to enable
var/spack/repos/builtin/packages/draco/d770-nocuda.patch:+    # cuda per project.
var/spack/repos/builtin/packages/draco/d770-nocuda.patch:+    set( CUDA_DBS_STRING "CUDA" CACHE STRING
var/spack/repos/builtin/packages/draco/d770-nocuda.patch:+      "If CUDA is available, this variable is 'CUDA'")
var/spack/repos/builtin/packages/draco/d770-nocuda.patch:     set( COMPILE_WITH_CUDA LINK_LANGUAGE CUDA )
var/spack/repos/builtin/packages/draco/d770-nocuda.patch:@@ -752,14 +764,7 @@ macro(dbsSetupCuda)
var/spack/repos/builtin/packages/draco/d770-nocuda.patch:         "CUDACXX=${CMAKE_CUDA_COMPILER}")
var/spack/repos/builtin/packages/draco/d770-nocuda.patch:-  set( HAVE_CUDA ${HAVE_CUDA} CACHE BOOL
var/spack/repos/builtin/packages/draco/d770-nocuda.patch:-    "Should we build CUDA portions of this project?" FORCE )
var/spack/repos/builtin/packages/draco/d770-nocuda.patch:-  if( ${HAVE_CUDA} )
var/spack/repos/builtin/packages/draco/d770-nocuda.patch:-    set( CUDA_DBS_STRING "CUDA" CACHE STRING
var/spack/repos/builtin/packages/draco/d770-nocuda.patch:-      "If CUDA is available, this variable is 'CUDA'")
var/spack/repos/builtin/packages/draco/d710.patch:@@ -107,8 +107,9 @@ set( WITH_CUDA "@WITH_CUDA@" )
var/spack/repos/builtin/packages/onednn/package.py:        "gpu_runtime",
var/spack/repos/builtin/packages/onednn/package.py:        description="Runtime to use for GPU engines",
var/spack/repos/builtin/packages/onednn/package.py:    depends_on("opencl@1.2:", when="gpu_runtime=ocl")
var/spack/repos/builtin/packages/onednn/package.py:    depends_on("opencl@1.2:", when="gpu_runtime=sycl")
var/spack/repos/builtin/packages/onednn/package.py:    depends_on("oneapi-level-zero", when="gpu_runtime=sycl")
var/spack/repos/builtin/packages/onednn/package.py:            "-DDNNL_GPU_RUNTIME={0}".format(self.spec.variants["gpu_runtime"].value.upper()),
var/spack/repos/builtin/packages/onednn/package.py:        if self.spec.satisfies("gpu_runtime=ocl"):
var/spack/repos/builtin/packages/onednn/package.py:            args.append("-DOPENCLROOT=" + self.spec["opencl"].prefix)
var/spack/repos/builtin/packages/babelstream/package.py:class Babelstream(CMakePackage, CudaPackage, ROCmPackage, MakefilePackage):
var/spack/repos/builtin/packages/babelstream/package.py:    """Measure memory transfer rates to/from global device memory on GPUs.
var/spack/repos/builtin/packages/babelstream/package.py:        # Also supported variants are cuda and rocm (for HIP)
var/spack/repos/builtin/packages/babelstream/package.py:        variant("ocl", default=False, description="Enable OpenCL support")
var/spack/repos/builtin/packages/babelstream/package.py:        variant("acc", default=False, description="Enable OpenACC support")
var/spack/repos/builtin/packages/babelstream/package.py:        values=("nvidia", "intel"),
var/spack/repos/builtin/packages/babelstream/package.py:        description="Offloading to NVIDIA GPU or not",
var/spack/repos/builtin/packages/babelstream/package.py:        values=("cuda", "rocm"),
var/spack/repos/builtin/packages/babelstream/package.py:        default="cuda",
var/spack/repos/builtin/packages/babelstream/package.py:            - CUDA (via https://github.com/NVIDIA/thrust)\
var/spack/repos/builtin/packages/babelstream/package.py:            - ROCM (via https://github.com/ROCmSoftwarePlatform/rocThrust)",
var/spack/repos/builtin/packages/babelstream/package.py:        values=("cuda", "omp", "tbb"),
var/spack/repos/builtin/packages/babelstream/package.py:        default="cuda",
var/spack/repos/builtin/packages/babelstream/package.py:    # CUDA conflict
var/spack/repos/builtin/packages/babelstream/package.py:        "cuda_arch=none",
var/spack/repos/builtin/packages/babelstream/package.py:        when="+cuda",
var/spack/repos/builtin/packages/babelstream/package.py:        msg="CUDA requires architecture to be specfied by cuda_arch=",
var/spack/repos/builtin/packages/babelstream/package.py:        "cuda_memory_mode",
var/spack/repos/builtin/packages/babelstream/package.py:        when="+cuda",
var/spack/repos/builtin/packages/babelstream/package.py:        description="Enable MEM Target for CUDA",
var/spack/repos/builtin/packages/babelstream/package.py:        values=("cpu", "nvidia"),
var/spack/repos/builtin/packages/babelstream/package.py:        description="Enable RAJA Target [CPU or NVIDIA] / Offload with custom settings for OpenMP",
var/spack/repos/builtin/packages/babelstream/package.py:    depends_on("cuda", when="thrust_submodel=cuda")
var/spack/repos/builtin/packages/babelstream/package.py:    depends_on("cuda", when="+raja raja_offload=nvidia")
var/spack/repos/builtin/packages/babelstream/package.py:    depends_on("rocthrust", when="thrust_submodel=rocm")
var/spack/repos/builtin/packages/babelstream/package.py:    cuda_archs = CudaPackage.cuda_arch_values
var/spack/repos/builtin/packages/babelstream/package.py:    for sm_ in cuda_archs:
var/spack/repos/builtin/packages/babelstream/package.py:            "kokkos +cuda +wrapper cuda_arch={0}".format(sm_),
var/spack/repos/builtin/packages/babelstream/package.py:            when="kokkos_backend=cuda cuda_arch={0}".format(sm_),
var/spack/repos/builtin/packages/babelstream/package.py:            "raja +cuda cuda_arch={0}".format(sm_),
var/spack/repos/builtin/packages/babelstream/package.py:            when="raja_offload=nvidia cuda_arch={0}".format(sm_),
var/spack/repos/builtin/packages/babelstream/package.py:    # OpenCL Dependency
var/spack/repos/builtin/packages/babelstream/package.py:        values=("amd", "cuda", "intel", "pocl", "none"),
var/spack/repos/builtin/packages/babelstream/package.py:        description="Enable Backend Target for OpenCL",
var/spack/repos/builtin/packages/babelstream/package.py:        values=("cuda", "omp", "none"),
var/spack/repos/builtin/packages/babelstream/package.py:        msg="OpenCL implementation requires backend to be specfied by ocl_backend=",
var/spack/repos/builtin/packages/babelstream/package.py:    # depends_on("rocm-opencl@6.0.2", when="+ocl ocl_backend=amd")
var/spack/repos/builtin/packages/babelstream/package.py:    depends_on("cuda", when="+ocl ocl_backend=cuda")
var/spack/repos/builtin/packages/babelstream/package.py:    depends_on("cuda", when="+sycl2020 sycl2020_offload=nvidia")
var/spack/repos/builtin/packages/babelstream/package.py:        "cuda_extra_flags",
var/spack/repos/builtin/packages/babelstream/package.py:        description="Additional CUDA Compiler flags to be provided",
var/spack/repos/builtin/packages/babelstream/package.py:    depends_on("opencl-c-headers", when="+ocl")
var/spack/repos/builtin/packages/babelstream/package.py:            "OpenACC",
var/spack/repos/builtin/packages/babelstream/package.py:            "OpenACCArray",
var/spack/repos/builtin/packages/babelstream/package.py:            "CUDA",
var/spack/repos/builtin/packages/babelstream/package.py:            "CUDAKernel",
var/spack/repos/builtin/packages/babelstream/package.py:            "cuda",
var/spack/repos/builtin/packages/babelstream/package.py:        # for +acc and +thrust the CudaPackage appends +cuda variant too so we need
var/spack/repos/builtin/packages/babelstream/package.py:        # to filter cuda from list e.g. we choose 'thrust'
var/spack/repos/builtin/packages/babelstream/package.py:        # from the list of ['cuda', 'thrust']
var/spack/repos/builtin/packages/babelstream/package.py:            model_names = [elem for elem in model_names if (elem != "cuda" and elem != "rocm")]
var/spack/repos/builtin/packages/babelstream/package.py:            elif "sycl2020" in model_names[0]:  # this is for nvidia offload
var/spack/repos/builtin/packages/babelstream/package.py:            elif "rocm" in model_names[0]:
var/spack/repos/builtin/packages/babelstream/package.py:             gpu       - Globally set the target device to an NVIDIA GPU
var/spack/repos/builtin/packages/babelstream/package.py:         register_flag_optional(CUDA_ARCH
var/spack/repos/builtin/packages/babelstream/package.py:        "[PGI/NVHPC only] Only applicable if `TARGET_DEVICE` is set to `gpu`.
var/spack/repos/builtin/packages/babelstream/package.py:         Nvidia architecture in ccXY format, for example, sm_70 becomes cc70,
var/spack/repos/builtin/packages/babelstream/package.py:         will be passed in via `-gpu=` (e.g `cc70`)
var/spack/repos/builtin/packages/babelstream/package.py:                target_device = "gpu" if "cuda_arch" in self.spec.variants else "multicore"
var/spack/repos/builtin/packages/babelstream/package.py:                if "cuda_arch" in self.spec.variants:
var/spack/repos/builtin/packages/babelstream/package.py:                    cuda_arch_list = self.spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/babelstream/package.py:                    cuda_arch = "cc" + cuda_arch_list[0]
var/spack/repos/builtin/packages/babelstream/package.py:                    #     "-DCXX_EXTRA_FLAGS=" + "-target=" + target_device + "-gpu=" + cuda_arch
var/spack/repos/builtin/packages/babelstream/package.py:                    args.append("-DCUDA_ARCH=" + cuda_arch)
var/spack/repos/builtin/packages/babelstream/package.py:                cuda_arch = "cc" + self.spec.variants["cuda_arch"].value[0]
var/spack/repos/builtin/packages/babelstream/package.py:                args.append("-DNVHPC_OFFLOAD=" + cuda_arch)
var/spack/repos/builtin/packages/babelstream/package.py:        #             CUDA
var/spack/repos/builtin/packages/babelstream/package.py:        if self.spec.satisfies("+cuda~kokkos~acc~omp~thrust~raja"):
var/spack/repos/builtin/packages/babelstream/package.py:            # Set up the cuda macros needed by the build
var/spack/repos/builtin/packages/babelstream/package.py:            cuda_arch_list = self.spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/babelstream/package.py:            # "-DCUDA_ARCH" requires sm_
var/spack/repos/builtin/packages/babelstream/package.py:            cuda_arch = "sm_" + cuda_arch_list[0]
var/spack/repos/builtin/packages/babelstream/package.py:            args.append("-DCUDA_ARCH=" + cuda_arch)
var/spack/repos/builtin/packages/babelstream/package.py:            cuda_dir = self.spec["cuda"].prefix
var/spack/repos/builtin/packages/babelstream/package.py:            cuda_comp = cuda_dir + "/bin/nvcc"
var/spack/repos/builtin/packages/babelstream/package.py:            args.append("-DCMAKE_CUDA_COMPILER=" + cuda_comp)
var/spack/repos/builtin/packages/babelstream/package.py:            args.append("-DMEM=" + self.spec.variants["cuda_memory_mode"].value.upper())
var/spack/repos/builtin/packages/babelstream/package.py:            if self.spec.variants["cuda_extra_flags"].value != "none":
var/spack/repos/builtin/packages/babelstream/package.py:                args.append("-DCUDA_EXTRA_FLAGS=" + self.spec.variants["cuda_extra_flags"].value)
var/spack/repos/builtin/packages/babelstream/package.py:                if "cuda_arch" in self.spec.variants:
var/spack/repos/builtin/packages/babelstream/package.py:                        cuda_arch = "cc" + self.spec.variants["cuda_arch"].value[0]
var/spack/repos/builtin/packages/babelstream/package.py:                        offload_args = " -mp=gpu;" + "-gpu=" + cuda_arch + " "
var/spack/repos/builtin/packages/babelstream/package.py:                        cuda_arch = "sm_" + self.spec.variants["cuda_arch"].value[0]
var/spack/repos/builtin/packages/babelstream/package.py:                        offload_args = "-fopenmp;--offload-arch=" + cuda_arch
var/spack/repos/builtin/packages/babelstream/package.py:                elif ("amdgpu_target" in self.spec.variants) and (
var/spack/repos/builtin/packages/babelstream/package.py:                    self.spec.variants["amdgpu_target"].value != "none"
var/spack/repos/builtin/packages/babelstream/package.py:                        ";--offload-arch=" + self.spec.variants["amdgpu_target"].value[0]
var/spack/repos/builtin/packages/babelstream/package.py:            if self.spec.variants["sycl2020_offload"].value == "nvidia":
var/spack/repos/builtin/packages/babelstream/package.py:                cuda_dir = self.spec["cuda"].prefix
var/spack/repos/builtin/packages/babelstream/package.py:                cuda_arch = "sm_" + self.spec.variants["cuda_arch"].value[0]
var/spack/repos/builtin/packages/babelstream/package.py:                    + "-fsycl;-fsycl-targets=nvptx64-nvidia-cuda;"
var/spack/repos/builtin/packages/babelstream/package.py:                    + " --cuda-path="
var/spack/repos/builtin/packages/babelstream/package.py:                    + cuda_dir
var/spack/repos/builtin/packages/babelstream/package.py:        #             HIP(ROCM)
var/spack/repos/builtin/packages/babelstream/package.py:            offload_arch = str(self.spec.variants["amdgpu_target"].value[0])
var/spack/repos/builtin/packages/babelstream/package.py:        #             OpenCL (ocl)
var/spack/repos/builtin/packages/babelstream/package.py:            if "cuda" in self.spec.variants["ocl_backend"].value:
var/spack/repos/builtin/packages/babelstream/package.py:                cuda_dir = self.spec["cuda"].prefix
var/spack/repos/builtin/packages/babelstream/package.py:                args.append("-DOpenCL_LIBRARY=" + cuda_dir + "/lib64/libOpenCL.so")
var/spack/repos/builtin/packages/babelstream/package.py:                rocm_dir = self.spec["rocm-opencl"].prefix
var/spack/repos/builtin/packages/babelstream/package.py:                args.append("-DOpenCL_LIBRARY=" + rocm_dir + "/lib64/libOpenCL.so")
var/spack/repos/builtin/packages/babelstream/package.py:                    + "/linux/lib/libOpenCL.so"
var/spack/repos/builtin/packages/babelstream/package.py:                args.append("-DOpenCL_LIBRARY=" + intel_lib)
var/spack/repos/builtin/packages/babelstream/package.py:                pocl_lib = self.spec["pocl"].prefix + "/lib64/libOpenCL.so"
var/spack/repos/builtin/packages/babelstream/package.py:                args.append("-DOpenCL_LIBRARY=" + pocl_lib)
var/spack/repos/builtin/packages/babelstream/package.py:            if "nvidia" in self.spec.variants["raja_offload"].value:
var/spack/repos/builtin/packages/babelstream/package.py:                cuda_comp = self.spec["cuda"].prefix + "/bin/nvcc"
var/spack/repos/builtin/packages/babelstream/package.py:                args.append("-DTARGET=NVIDIA")
var/spack/repos/builtin/packages/babelstream/package.py:                cuda_arch = "sm_" + self.spec.variants["cuda_arch"].value[0]
var/spack/repos/builtin/packages/babelstream/package.py:                args.append("-DCUDA_ARCH=" + cuda_arch)
var/spack/repos/builtin/packages/babelstream/package.py:                args.append("-DENABLE_CUDA=ON")
var/spack/repos/builtin/packages/babelstream/package.py:                args.append("-DCUDA_TOOLKIT_ROOT_DIR=" + self.spec["cuda"].prefix)
var/spack/repos/builtin/packages/babelstream/package.py:                if self.spec.variants["cuda_extra_flags"].value != "none":
var/spack/repos/builtin/packages/babelstream/package.py:                        "-DCMAKE_CUDA_FLAGS=" + self.spec.variants["cuda_extra_flags"].value
var/spack/repos/builtin/packages/babelstream/package.py:            if "cuda" in self.spec.variants["thrust_submodel"].value:
var/spack/repos/builtin/packages/babelstream/package.py:                # this model uses CMAKE_CUDA_ARCHITECTURES which only requires number of cuda_arch
var/spack/repos/builtin/packages/babelstream/package.py:                args.append("-DCUDA_ARCH=" + self.spec.variants["cuda_arch"].value[0])
var/spack/repos/builtin/packages/babelstream/package.py:                cuda_dir = self.spec["cuda"].prefix
var/spack/repos/builtin/packages/babelstream/package.py:                cuda_comp = cuda_dir + "/bin/nvcc"
var/spack/repos/builtin/packages/babelstream/package.py:                args.append("-DCMAKE_CUDA_COMPILER=" + cuda_comp)
var/spack/repos/builtin/packages/babelstream/package.py:                # args.append("-DCMAKE_CUDA_COMPILER=" + spack_cxx)
var/spack/repos/builtin/packages/babelstream/package.py:                # args.append("-DCMAKE_CUDA_FLAGS=-ccbin " + spack_cc)
var/spack/repos/builtin/packages/babelstream/package.py:                if self.spec.variants["cuda_extra_flags"].value != "none":
var/spack/repos/builtin/packages/babelstream/package.py:                        "-DCUDA_EXTRA_FLAGS=" + self.spec.variants["cuda_extra_flags"].value
var/spack/repos/builtin/packages/babelstream/package.py:            if "rocm" in self.spec.variants["thrust_submodel"].value:
var/spack/repos/builtin/packages/babelstream/package.py:        # kokkos implementation is versatile and it could use cuda or omp architectures as backend
var/spack/repos/builtin/packages/babelstream/package.py:        # The usage should be spack install babelstream +kokkos backend=[cuda or omp or none]
var/spack/repos/builtin/packages/babelstream/package.py:            if "cuda" in self.spec.variants["kokkos_backend"].value:
var/spack/repos/builtin/packages/babelstream/package.py:                # args.append("-DCMAKE_CXX_COMPILER=" + self.spec["cuda"].nvcc)
var/spack/repos/builtin/packages/babelstream/package.py:                args.append("-DKokkos_ENABLE_CUDA=ON")
var/spack/repos/builtin/packages/babelstream/package.py:                int_cuda_arch = int(self.spec.variants["cuda_arch"].value[0])
var/spack/repos/builtin/packages/babelstream/package.py:                if int_cuda_arch in (30, 32, 35, 37):
var/spack/repos/builtin/packages/babelstream/package.py:                    args.append("-D" + "Kokkos_ARCH_KEPLER" + str(int_cuda_arch) + "=ON")
var/spack/repos/builtin/packages/babelstream/package.py:                if int_cuda_arch in (50, 52, 53):
var/spack/repos/builtin/packages/babelstream/package.py:                    args.append("-D" + "Kokkos_ARCH_MAXWELL" + str(int_cuda_arch) + "=ON")
var/spack/repos/builtin/packages/babelstream/package.py:                if int_cuda_arch in (60, 61):
var/spack/repos/builtin/packages/babelstream/package.py:                    args.append("-D" + "Kokkos_ARCH_PASCAL" + str(int_cuda_arch) + "=ON")
var/spack/repos/builtin/packages/babelstream/package.py:                if int_cuda_arch in (70, 72):
var/spack/repos/builtin/packages/babelstream/package.py:                    args.append("-D" + "Kokkos_ARCH_VOLTA" + str(int_cuda_arch) + "=ON")
var/spack/repos/builtin/packages/babelstream/package.py:                if int_cuda_arch == 75:
var/spack/repos/builtin/packages/babelstream/package.py:                if int_cuda_arch == 80:
var/spack/repos/builtin/packages/babelstream/package.py:            "OPENACC_FLAG": "",
var/spack/repos/builtin/packages/babelstream/package.py:            "CUDA_FLAG": "",
var/spack/repos/builtin/packages/babelstream/package.py:            "arm": ["CUDA", "CUDAKernel", "OpenACC", "OpenACCArray"],
var/spack/repos/builtin/packages/babelstream/package.py:            "aocc": ["CUDA", "CUDAKernel"],
var/spack/repos/builtin/packages/babelstream/package.py:            "cce": ["CUDA", "CUDAKernel"],
var/spack/repos/builtin/packages/babelstream/package.py:            "gcc": ["CUDA", "CUDAKernel"],
var/spack/repos/builtin/packages/babelstream/package.py:            "oneapi": ["CUDA", "CUDAKernel", "OpenACC", "OpenACCArray"],
var/spack/repos/builtin/packages/babelstream/package.py:            "fj": ["CUDA", "CUDAKernel", "OpenACC"],
var/spack/repos/builtin/packages/babelstream/package.py:            config["OPENACC_FLAG"] = "-fopenacc"
var/spack/repos/builtin/packages/babelstream/package.py:            config["OPENACC_FLAG"] = "-fopenacc"
var/spack/repos/builtin/packages/babelstream/package.py:            config["OPENACC_FLAG"] = "-h acc"  # for cpu only -h omp
var/spack/repos/builtin/packages/babelstream/package.py:            config["OPENACC_FLAG"] = "-fopenacc"
var/spack/repos/builtin/packages/babelstream/package.py:            TARGET = "gpu"  # target = "multicore"
var/spack/repos/builtin/packages/babelstream/package.py:            if "cuda_arch" in self.spec.variants:
var/spack/repos/builtin/packages/babelstream/package.py:                cuda_arch_list = self.spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/babelstream/package.py:                cuda_arch = "cc" + cuda_arch_list[0]
var/spack/repos/builtin/packages/babelstream/package.py:            GPUFLAG = " -gpu=" + cuda_arch
var/spack/repos/builtin/packages/babelstream/package.py:            # this is to allow apples-to-apples comparison with DC in non-DC GPU impls
var/spack/repos/builtin/packages/babelstream/package.py:            # MANAGED = "-DUSE_MANAGED -gpu=managed"
var/spack/repos/builtin/packages/babelstream/package.py:            DEVICE = "-DUSE_DEVICE -cuda -gpu=nomanaged"
var/spack/repos/builtin/packages/babelstream/package.py:            config["DOCONCURRENT_FLAG"] = GPUFLAG + " -stdpar=" + TARGET + " " + DEVICE
var/spack/repos/builtin/packages/babelstream/package.py:            config["ARRAY_FLAG"] = GPUFLAG + " -stdpar=" + TARGET + " " + MANAGED
var/spack/repos/builtin/packages/babelstream/package.py:            config["OPENMP_FLAG"] = GPUFLAG + " -mp=" + TARGET + " " + MANAGED
var/spack/repos/builtin/packages/babelstream/package.py:            config["OPENACC_FLAG"] = GPUFLAG + " -acc=" + TARGET + " " + MANAGED
var/spack/repos/builtin/packages/babelstream/package.py:            config["CUDA_FLAG"] = GPUFLAG + " -cuda -acc=gpu" + " " + MANAGED
var/spack/repos/builtin/packages/dla-future/package.py:class DlaFuture(CMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/dla-future/package.py:    variant("mpi_gpu_aware", default=False, when="@0.5.0:", description="Use GPU-aware MPI.")
var/spack/repos/builtin/packages/dla-future/package.py:    conflicts("+mpi_gpu_aware", when="~cuda ~rocm", msg="GPU-aware MPI requires +cuda or +rocm")
var/spack/repos/builtin/packages/dla-future/package.py:        "mpi_gpu_force_contiguous",
var/spack/repos/builtin/packages/dla-future/package.py:        when="@0.5.0: +mpi_gpu_aware",
var/spack/repos/builtin/packages/dla-future/package.py:        description="Force GPU communication buffers to be contiguous before communicating.",
var/spack/repos/builtin/packages/dla-future/package.py:    depends_on("umpire~cuda", when="~cuda")
var/spack/repos/builtin/packages/dla-future/package.py:    depends_on("umpire~rocm", when="~rocm")
var/spack/repos/builtin/packages/dla-future/package.py:    depends_on("umpire+cuda~shared", when="+cuda")
var/spack/repos/builtin/packages/dla-future/package.py:    depends_on("umpire+rocm~shared", when="+rocm")
var/spack/repos/builtin/packages/dla-future/package.py:    depends_on("pika +cuda", when="+cuda")
var/spack/repos/builtin/packages/dla-future/package.py:    depends_on("pika +rocm", when="+rocm")
var/spack/repos/builtin/packages/dla-future/package.py:    conflicts("^pika cxxstd=20", when="+cuda")
var/spack/repos/builtin/packages/dla-future/package.py:    depends_on("whip +cuda", when="+cuda")
var/spack/repos/builtin/packages/dla-future/package.py:    depends_on("whip +rocm", when="+rocm")
var/spack/repos/builtin/packages/dla-future/package.py:    depends_on("rocblas", when="+rocm")
var/spack/repos/builtin/packages/dla-future/package.py:    depends_on("rocsolver", when="+rocm")
var/spack/repos/builtin/packages/dla-future/package.py:    depends_on("rocprim", when="@:0.3 +rocm")
var/spack/repos/builtin/packages/dla-future/package.py:    depends_on("rocthrust", when="@:0.3 +rocm")
var/spack/repos/builtin/packages/dla-future/package.py:    conflicts("^fmt@10:", when="@:0.3.0 +cuda ^cuda@:11.2")
var/spack/repos/builtin/packages/dla-future/package.py:    conflicts("^fmt@10:", when="@:0.3.0 %gcc@9 +cuda ^cuda@:11.2 ^umpire@2022.10:")
var/spack/repos/builtin/packages/dla-future/package.py:    conflicts("+cuda", when="+rocm")
var/spack/repos/builtin/packages/dla-future/package.py:    with when("+rocm"):
var/spack/repos/builtin/packages/dla-future/package.py:        for arch in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/dla-future/package.py:            depends_on(f"pika amdgpu_target={arch}", when=f"amdgpu_target={arch}")
var/spack/repos/builtin/packages/dla-future/package.py:            depends_on(f"rocsolver amdgpu_target={arch}", when=f"amdgpu_target={arch}")
var/spack/repos/builtin/packages/dla-future/package.py:            depends_on(f"rocblas amdgpu_target={arch}", when=f"amdgpu_target={arch}")
var/spack/repos/builtin/packages/dla-future/package.py:            depends_on(f"whip amdgpu_target={arch}", when=f"amdgpu_target={arch}")
var/spack/repos/builtin/packages/dla-future/package.py:            depends_on(f"umpire amdgpu_target={arch}", when=f"amdgpu_target={arch}")
var/spack/repos/builtin/packages/dla-future/package.py:    with when("@:0.3 +rocm"):
var/spack/repos/builtin/packages/dla-future/package.py:        for arch in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/dla-future/package.py:            depends_on(f"rocprim amdgpu_target={arch}", when=f"amdgpu_target={arch}")
var/spack/repos/builtin/packages/dla-future/package.py:            depends_on(f"rocthrust amdgpu_target={arch}", when=f"amdgpu_target={arch}")
var/spack/repos/builtin/packages/dla-future/package.py:    with when("+cuda"):
var/spack/repos/builtin/packages/dla-future/package.py:        for arch in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/dla-future/package.py:            depends_on(f"pika cuda_arch={arch}", when=f"cuda_arch={arch}")
var/spack/repos/builtin/packages/dla-future/package.py:            depends_on(f"umpire cuda_arch={arch}", when=f"cuda_arch={arch}")
var/spack/repos/builtin/packages/dla-future/package.py:    patch("hip_complex_operator_overloads.patch", when="+rocm")
var/spack/repos/builtin/packages/dla-future/package.py:        args.append(self.define_from_variant("DLAF_WITH_MPI_GPU_AWARE", "mpi_gpu_aware"))
var/spack/repos/builtin/packages/dla-future/package.py:                "DLAF_WITH_MPI_GPU_FORCE_CONTIGUOUS", "mpi_gpu_force_contiguous"
var/spack/repos/builtin/packages/dla-future/package.py:        # CUDA/HIP
var/spack/repos/builtin/packages/dla-future/package.py:        args.append(self.define_from_variant("DLAF_WITH_CUDA", "cuda"))
var/spack/repos/builtin/packages/dla-future/package.py:        args.append(self.define_from_variant("DLAF_WITH_HIP", "rocm"))
var/spack/repos/builtin/packages/dla-future/package.py:        if spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/dla-future/package.py:            archs = spec.variants["amdgpu_target"].value
var/spack/repos/builtin/packages/dla-future/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/dla-future/package.py:            archs = spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/dla-future/package.py:                args.append(self.define("CMAKE_CUDA_ARCHITECTURES", arch_str))
var/spack/repos/builtin/packages/dla-future/hip_complex_operator_overloads.patch:diff --git a/src/lapack/gpu/add.cu b/src/lapack/gpu/add.cu
var/spack/repos/builtin/packages/dla-future/hip_complex_operator_overloads.patch:--- a/src/lapack/gpu/add.cu
var/spack/repos/builtin/packages/dla-future/hip_complex_operator_overloads.patch:+++ b/src/lapack/gpu/add.cu
var/spack/repos/builtin/packages/amber/package.py:class Amber(Package, CudaPackage):
var/spack/repos/builtin/packages/amber/package.py:    # Workaround to modify the AmberTools script when using the NVIDIA
var/spack/repos/builtin/packages/amber/package.py:    # Workaround to use NVIDIA compilers to build the bundled Boost
var/spack/repos/builtin/packages/amber/package.py:    # Cuda dependencies
var/spack/repos/builtin/packages/amber/package.py:    depends_on("cuda@:11.1", when="@20:+cuda")  # when='AmberTools@21:'
var/spack/repos/builtin/packages/amber/package.py:    depends_on("cuda@:10.2.89", when="@18+cuda")
var/spack/repos/builtin/packages/amber/package.py:    depends_on("cuda@7.5.18", when="@:16+cuda")
var/spack/repos/builtin/packages/amber/package.py:        # CUDA
var/spack/repos/builtin/packages/amber/package.py:        if self.spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/amber/package.py:            env.set("CUDA_HOME", self.spec["cuda"].prefix)
var/spack/repos/builtin/packages/amber/package.py:        # CUDA
var/spack/repos/builtin/packages/amber/package.py:        if self.spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/amber/package.py:            conf(*(base_args + ["-cuda", compiler]))
var/spack/repos/builtin/packages/amber/package.py:        # CUDA + MPI
var/spack/repos/builtin/packages/amber/package.py:        if self.spec.satisfies("+cuda") and self.spec.satisfies("+mpi"):
var/spack/repos/builtin/packages/amber/package.py:            conf(*(base_args + ["-cuda", "-mpi", compiler]))
var/spack/repos/builtin/packages/amber/package.py:        # CUDA
var/spack/repos/builtin/packages/amber/package.py:        if self.spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/amber/package.py:            env.prepend_path("LD_LIBRARY_PATH", self.spec["cuda"].prefix.lib)
var/spack/repos/builtin/packages/py-tensorflow/absl_neon.patch:+ // https://llvm.org/docs/CompileCudaWithLLVM.html#detecting-clang-vs-nvcc-from-code
var/spack/repos/builtin/packages/py-tensorflow/absl_neon.patch:+-#elif defined(__ARM_NEON) && !defined(__CUDA_ARCH__)
var/spack/repos/builtin/packages/py-tensorflow/absl_neon.patch:++#elif defined(__ARM_NEON) && !(defined(__NVCC__) && defined(__CUDACC__))
var/spack/repos/builtin/packages/py-tensorflow/null_linker_bin_path.patch:diff -ru a/third_party/gpus/cuda_configure.bzl b/third_party/gpus/cuda_configure.bzl
var/spack/repos/builtin/packages/py-tensorflow/null_linker_bin_path.patch:--- a/third_party/gpus/cuda_configure.bzl	2021-05-12 13:26:41.000000000 +0000
var/spack/repos/builtin/packages/py-tensorflow/null_linker_bin_path.patch:+++ b/third_party/gpus/cuda_configure.bzl	2021-10-28 21:38:06.949271099 +0000
var/spack/repos/builtin/packages/py-tensorflow/null_linker_bin_path.patch:-        cuda_defines["%{linker_bin_path}"] = ""
var/spack/repos/builtin/packages/py-tensorflow/null_linker_bin_path.patch:-        cuda_defines["%{linker_bin_path}"] = host_compiler_prefix
var/spack/repos/builtin/packages/py-tensorflow/null_linker_bin_path.patch:+    cuda_defines["%{linker_bin_path}"] = ""
var/spack/repos/builtin/packages/py-tensorflow/null_linker_bin_path.patch:     cuda_defines["%{extra_no_canonical_prefixes_flags}"] = ""
var/spack/repos/builtin/packages/py-tensorflow/null_linker_bin_path.patch:     cuda_defines["%{unfiltered_compile_flags}"] = ""
var/spack/repos/builtin/packages/py-tensorflow/package.py:rocm_dependencies = [
var/spack/repos/builtin/packages/py-tensorflow/package.py:    "llvm-amdgpu",
var/spack/repos/builtin/packages/py-tensorflow/package.py:    "rocminfo",
var/spack/repos/builtin/packages/py-tensorflow/package.py:    "rocm-core",
var/spack/repos/builtin/packages/py-tensorflow/package.py:class PyTensorflow(Package, CudaPackage, ROCmPackage, PythonExtension):
var/spack/repos/builtin/packages/py-tensorflow/package.py:        "2.16.1-rocm-enhanced",
var/spack/repos/builtin/packages/py-tensorflow/package.py:        url="https://github.com/ROCm/tensorflow-upstream/archive/refs/tags/v2.16.1-rocm-enhanced.tar.gz",
var/spack/repos/builtin/packages/py-tensorflow/package.py:        "2.14-rocm-enhanced",
var/spack/repos/builtin/packages/py-tensorflow/package.py:        git="https://github.com/ROCm/tensorflow-upstream.git",
var/spack/repos/builtin/packages/py-tensorflow/package.py:        branch="r2.14-rocm-enhanced-nohipblaslt-build",
var/spack/repos/builtin/packages/py-tensorflow/package.py:        "2.11.0-rocm-enhanced",
var/spack/repos/builtin/packages/py-tensorflow/package.py:        url="https://github.com/ROCmSoftwarePlatform/tensorflow-upstream/archive/refs/tags/v2.11.0-rocm-enhanced.tar.gz",
var/spack/repos/builtin/packages/py-tensorflow/package.py:        "2.7.4-rocm-enhanced",
var/spack/repos/builtin/packages/py-tensorflow/package.py:        url="https://github.com/ROCmSoftwarePlatform/tensorflow-upstream/archive/refs/tags/v2.7.4-rocm-enhanced.tar.gz",
var/spack/repos/builtin/packages/py-tensorflow/package.py:    variant("opencl", default=False, description="Build with OpenCL SYCL support")
var/spack/repos/builtin/packages/py-tensorflow/package.py:    variant("cuda", default=sys.platform != "darwin", description="Build with CUDA support")
var/spack/repos/builtin/packages/py-tensorflow/package.py:        "nccl", default=sys.platform.startswith("linux"), description="Enable NVIDIA NCCL support"
var/spack/repos/builtin/packages/py-tensorflow/package.py:    # depends_on('computecpp', when='+opencl+computecpp')
var/spack/repos/builtin/packages/py-tensorflow/package.py:    # depends_on('trisycl',    when='+opencl~computepp')
var/spack/repos/builtin/packages/py-tensorflow/package.py:    with when("+cuda"):
var/spack/repos/builtin/packages/py-tensorflow/package.py:        # https://www.tensorflow.org/install/source#gpu
var/spack/repos/builtin/packages/py-tensorflow/package.py:        depends_on("cuda@12.3:", when="@2.16:")
var/spack/repos/builtin/packages/py-tensorflow/package.py:        depends_on("cuda@12.2:", when="@2.15:")
var/spack/repos/builtin/packages/py-tensorflow/package.py:        depends_on("cuda@11.8:", when="@2.12:")
var/spack/repos/builtin/packages/py-tensorflow/package.py:        depends_on("cuda@11.2:", when="@2.5:")
var/spack/repos/builtin/packages/py-tensorflow/package.py:        depends_on("cuda@11.0:", when="@2.4:")
var/spack/repos/builtin/packages/py-tensorflow/package.py:        depends_on("cuda@10.1:", when="@2.1:")
var/spack/repos/builtin/packages/py-tensorflow/package.py:        depends_on("cuda@:11.7.0", when="@:2.9")
var/spack/repos/builtin/packages/py-tensorflow/package.py:        depends_on("cuda@:11.4", when="@2.4:2.7")
var/spack/repos/builtin/packages/py-tensorflow/package.py:        depends_on("cuda@:10.2", when="@:2.3")
var/spack/repos/builtin/packages/py-tensorflow/package.py:    depends_on("nccl", when="+nccl+cuda")
var/spack/repos/builtin/packages/py-tensorflow/package.py:    with when("+rocm"):
var/spack/repos/builtin/packages/py-tensorflow/package.py:        for pkg_dep in rocm_dependencies:
var/spack/repos/builtin/packages/py-tensorflow/package.py:    conflicts("+opencl", when="platform=windows")
var/spack/repos/builtin/packages/py-tensorflow/package.py:    conflicts("+computecpp", when="~opencl")
var/spack/repos/builtin/packages/py-tensorflow/package.py:        "+cuda",
var/spack/repos/builtin/packages/py-tensorflow/package.py:        when="+rocm",
var/spack/repos/builtin/packages/py-tensorflow/package.py:        msg="CUDA / ROCm are mututally exclusive. At most 1 GPU platform can be configured",
var/spack/repos/builtin/packages/py-tensorflow/package.py:    conflicts("+cuda", when="platform=darwin", msg="There is no GPU support for macOS")
var/spack/repos/builtin/packages/py-tensorflow/package.py:        "cuda_arch=none",
var/spack/repos/builtin/packages/py-tensorflow/package.py:        when="+cuda",
var/spack/repos/builtin/packages/py-tensorflow/package.py:        msg="Must specify CUDA compute capabilities of your GPU, see https://developer.nvidia.com/cuda-gpus",
var/spack/repos/builtin/packages/py-tensorflow/package.py:    conflicts("cuda_arch=20", msg="TensorFlow only supports compute capabilities >= 3.5")
var/spack/repos/builtin/packages/py-tensorflow/package.py:    conflicts("cuda_arch=30", msg="TensorFlow only supports compute capabilities >= 3.5")
var/spack/repos/builtin/packages/py-tensorflow/package.py:    conflicts("cuda_arch=32", msg="TensorFlow only supports compute capabilities >= 3.5")
var/spack/repos/builtin/packages/py-tensorflow/package.py:    conflicts("+tensorrt", when="~cuda")
var/spack/repos/builtin/packages/py-tensorflow/package.py:    conflicts("+nccl", when="~cuda~rocm")
var/spack/repos/builtin/packages/py-tensorflow/package.py:        "+nccl", when="platform=darwin", msg="Currently NCCL is only supported on Linux platform"
var/spack/repos/builtin/packages/py-tensorflow/package.py:        "~rocm",
var/spack/repos/builtin/packages/py-tensorflow/package.py:        when="@2.7.4-rocm-enhanced,2.11.0-rocm-enhanced,2.14-rocm-enhanced,2.16.1-rocm-enhanced",
var/spack/repos/builtin/packages/py-tensorflow/package.py:    conflicts("+rocm", when="@:2.7.4-a,2.7.4.0:2.11.0-a,2.11.0.0:2.14-a,2.14-z:2.16.1-a,2.16.1-z:")
var/spack/repos/builtin/packages/py-tensorflow/package.py:    # E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register
var/spack/repos/builtin/packages/py-tensorflow/package.py:        "https://github.com/ROCm/tensorflow-upstream/commit/fd6b0a4356c66f5f30cedbc62b24f18d9e32806f.patch?full_index=1",
var/spack/repos/builtin/packages/py-tensorflow/package.py:        when="@2.16.1-rocm-enhanced +rocm",
var/spack/repos/builtin/packages/py-tensorflow/package.py:        "https://github.com/ROCm/tensorflow-upstream/commit/c467913bf4411ce2681391f37a9adf6031d23c2c.patch?full_index=1",
var/spack/repos/builtin/packages/py-tensorflow/package.py:        when="@2.16.1-rocm-enhanced +rocm",
var/spack/repos/builtin/packages/py-tensorflow/package.py:        "https://github.com/ROCm/tensorflow-upstream/commit/f4f4e8698b90755b0b5ea2d9da1933b0b988b111.patch?full_index=1",
var/spack/repos/builtin/packages/py-tensorflow/package.py:        when="@2.16.1-rocm-enhanced: +rocm",
var/spack/repos/builtin/packages/py-tensorflow/package.py:        "https://github.com/ROCm/tensorflow-upstream/commit/8b7fcccb2914078737689347540cb79ace579bbb.patch?full_index=1",
var/spack/repos/builtin/packages/py-tensorflow/package.py:        when="@2.16.1-rocm-enhanced +rocm",
var/spack/repos/builtin/packages/py-tensorflow/package.py:        # Do you wish to build TensorFlow with OpenCL SYCL support?
var/spack/repos/builtin/packages/py-tensorflow/package.py:        if "+opencl" in spec:
var/spack/repos/builtin/packages/py-tensorflow/package.py:            env.set("TF_NEED_OPENCL_SYCL", "1")
var/spack/repos/builtin/packages/py-tensorflow/package.py:            env.set("TF_NEED_OPENCL", "1")
var/spack/repos/builtin/packages/py-tensorflow/package.py:            env.set("TF_NEED_OPENCL_SYCL", "0")
var/spack/repos/builtin/packages/py-tensorflow/package.py:            env.set("TF_NEED_OPENCL", "0")
var/spack/repos/builtin/packages/py-tensorflow/package.py:        # Do you wish to build TensorFlow with ROCm support?
var/spack/repos/builtin/packages/py-tensorflow/package.py:        if "+rocm" in spec:
var/spack/repos/builtin/packages/py-tensorflow/package.py:            env.set("TF_NEED_ROCM", "1")
var/spack/repos/builtin/packages/py-tensorflow/package.py:            env.set("LLVM_PATH", spec["llvm-amdgpu"].prefix)
var/spack/repos/builtin/packages/py-tensorflow/package.py:            for pkg_dep in rocm_dependencies:
var/spack/repos/builtin/packages/py-tensorflow/package.py:            env.set("TF_ROCM_AMDGPU_TARGETS", ",".join(self.spec.variants["amdgpu_target"].value))
var/spack/repos/builtin/packages/py-tensorflow/package.py:            env.set("TF_NEED_ROCM", "0")
var/spack/repos/builtin/packages/py-tensorflow/package.py:        # Do you wish to build TensorFlow with CUDA support?
var/spack/repos/builtin/packages/py-tensorflow/package.py:        if "+cuda" in spec:
var/spack/repos/builtin/packages/py-tensorflow/package.py:            env.set("TF_NEED_CUDA", "1")
var/spack/repos/builtin/packages/py-tensorflow/package.py:            env.set("CUDA_NVCC", "1")
var/spack/repos/builtin/packages/py-tensorflow/package.py:            # Do you want to use clang as CUDA compiler?
var/spack/repos/builtin/packages/py-tensorflow/package.py:            env.set("TF_CUDA_CLANG", "0")
var/spack/repos/builtin/packages/py-tensorflow/package.py:            cuda_paths = [spec["cuda"].prefix, spec["cudnn"].prefix]
var/spack/repos/builtin/packages/py-tensorflow/package.py:                cuda_paths.append(spec["tensorrt"].prefix)
var/spack/repos/builtin/packages/py-tensorflow/package.py:            # Please specify the CUDA SDK version you want to use
var/spack/repos/builtin/packages/py-tensorflow/package.py:            env.set("TF_CUDA_VERSION", spec["cuda"].version.up_to(2))
var/spack/repos/builtin/packages/py-tensorflow/package.py:            if "+nccl" in spec:
var/spack/repos/builtin/packages/py-tensorflow/package.py:                cuda_paths.append(spec["nccl"].prefix)
var/spack/repos/builtin/packages/py-tensorflow/package.py:                # Please specify the locally installed NCCL version to use
var/spack/repos/builtin/packages/py-tensorflow/package.py:                env.set("TF_NCCL_VERSION", spec["nccl"].version.up_to(1))
var/spack/repos/builtin/packages/py-tensorflow/package.py:                # Please specify the location where NCCL is installed
var/spack/repos/builtin/packages/py-tensorflow/package.py:                env.set("NCCL_INSTALL_PATH", spec["nccl"].prefix)
var/spack/repos/builtin/packages/py-tensorflow/package.py:                env.set("NCCL_HDR_PATH", spec["nccl"].prefix.include)
var/spack/repos/builtin/packages/py-tensorflow/package.py:                env.unset("TF_NCCL_VERSION")
var/spack/repos/builtin/packages/py-tensorflow/package.py:            # look for CUDA libraries and headers
var/spack/repos/builtin/packages/py-tensorflow/package.py:            env.set("TF_CUDA_PATHS", ",".join(cuda_paths))
var/spack/repos/builtin/packages/py-tensorflow/package.py:            # Please specify the location where CUDA toolkit is installed
var/spack/repos/builtin/packages/py-tensorflow/package.py:            env.set("CUDA_TOOLKIT_PATH", spec["cuda"].prefix)
var/spack/repos/builtin/packages/py-tensorflow/package.py:            # Please specify a list of comma-separated CUDA compute
var/spack/repos/builtin/packages/py-tensorflow/package.py:            # https://developer.nvidia.com/cuda-gpus.
var/spack/repos/builtin/packages/py-tensorflow/package.py:            capabilities = CudaPackage.compute_capabilities(spec.variants["cuda_arch"].value)
var/spack/repos/builtin/packages/py-tensorflow/package.py:            env.set("TF_CUDA_COMPUTE_CAPABILITIES", ",".join(capabilities))
var/spack/repos/builtin/packages/py-tensorflow/package.py:            env.set("HERMETIC_CUDA_COMPUTE_CAPABILITIES", ",".join(capabilities))
var/spack/repos/builtin/packages/py-tensorflow/package.py:            env.set("TF_NEED_CUDA", "0")
var/spack/repos/builtin/packages/py-tensorflow/package.py:        if spec.satisfies("~opencl"):
var/spack/repos/builtin/packages/py-tensorflow/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/py-tensorflow/package.py:            libs = spec["cuda"].libs.directories
var/spack/repos/builtin/packages/py-tensorflow/package.py:            if "+nccl" in spec:
var/spack/repos/builtin/packages/py-tensorflow/package.py:                libs.extend(spec["nccl"].libs.directories)
var/spack/repos/builtin/packages/py-tensorflow/package.py:        if spec.satisfies("@2.16.1-rocm-enhanced +rocm"):
var/spack/repos/builtin/packages/py-tensorflow/package.py:            if os.path.exists(spec["llvm-amdgpu"].prefix.bin.clang):
var/spack/repos/builtin/packages/py-tensorflow/package.py:                    "/usr/lib/llvm-17/bin/clang", spec["llvm-amdgpu"].prefix.bin.clang, ".bazelrc"
var/spack/repos/builtin/packages/py-tensorflow/package.py:                    spec["llvm-amdgpu"].prefix.llvm.bin.clang,
var/spack/repos/builtin/packages/py-tensorflow/package.py:        if "+cuda" in spec:
var/spack/repos/builtin/packages/py-tensorflow/package.py:            args.append("--config=cuda")
var/spack/repos/builtin/packages/py-tensorflow/package.py:                args.append("--config=cuda_wheel")
var/spack/repos/builtin/packages/py-tensorflow/package.py:        if "+rocm" in spec:
var/spack/repos/builtin/packages/py-tensorflow/package.py:            args.append("--config=rocm")
var/spack/repos/builtin/packages/py-tensorflow/package.py:        if "~nccl" in spec:
var/spack/repos/builtin/packages/py-tensorflow/package.py:            args.append("--config=nonccl")
var/spack/repos/builtin/packages/relion/package.py:class Relion(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/relion/package.py:    variant("cuda", default=True, description="enable compute on gpu")
var/spack/repos/builtin/packages/relion/package.py:    variant("double-gpu", default=False, description="double precision gpu")
var/spack/repos/builtin/packages/relion/package.py:    variant("altcpu", default=False, description="Use CPU acceleration", when="~cuda")
var/spack/repos/builtin/packages/relion/package.py:    depends_on("cuda", when="+cuda")
var/spack/repos/builtin/packages/relion/package.py:    depends_on("cuda@9:", when="@3: +cuda")
var/spack/repos/builtin/packages/relion/package.py:            "-DDoublePrec_GPU=%s" % ("+double-gpu" in self.spec),
var/spack/repos/builtin/packages/relion/package.py:        if "+cuda" in self.spec:
var/spack/repos/builtin/packages/relion/package.py:            carch = self.spec.variants["cuda_arch"].value[0]
var/spack/repos/builtin/packages/relion/package.py:            # relion+cuda requires selecting cuda_arch
var/spack/repos/builtin/packages/relion/package.py:                raise ValueError("Must select a value for cuda_arch")
var/spack/repos/builtin/packages/relion/package.py:                args += ["-DCUDA=ON", "-DCudaTexture=ON", "-DCUDA_ARCH=%s" % (carch)]
var/spack/repos/builtin/packages/relion/package.py:        # Remove flags not recognized by the NVIDIA compilers
var/spack/repos/builtin/packages/relion/0002-Simple-patch-to-fix-intel-mkl-linking.patch: 	if(CUDA_FOUND)
var/spack/repos/builtin/packages/gloo/package.py:class Gloo(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/gloo/package.py:            self.define_from_variant("USE_CUDA", "cuda"),
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch: gpu/impl/PQCodeDistances.cu              | 567 -----------------------------
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch: gpu/impl/PQScanMultiPassNoPrecomputed.cu | 597 -------------------------------
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch: gpu/test/Makefile                        |  10 +-
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch: gpu/test/demo_ivfpq_indexing_gpu.cpp     |   1 +
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch: delete mode 100644 gpu/impl/PQCodeDistances.cu
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch: delete mode 100644 gpu/impl/PQScanMultiPassNoPrecomputed.cu
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch:diff --git a/gpu/impl/PQCodeDistances.cu b/gpu/impl/PQCodeDistances.cu
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch:--- a/gpu/impl/PQCodeDistances.cu
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch:-#include <faiss/gpu/impl/PQCodeDistances.cuh>
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch:-#include <faiss/gpu/impl/BroadcastSum.cuh>
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch:-#include <faiss/gpu/impl/Distance.cuh>
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch:-#include <faiss/gpu/impl/L2Norm.cuh>
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch:-#include <faiss/gpu/utils/ConversionOperators.cuh>
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch:-#include <faiss/gpu/utils/DeviceDefs.cuh>
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch:-#include <faiss/gpu/utils/DeviceUtils.h>
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch:-#include <faiss/gpu/utils/Float16.cuh>
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch:-#include <faiss/gpu/utils/MatrixMult.cuh>
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch:-#include <faiss/gpu/utils/PtxUtils.cuh>
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch:-#include <faiss/gpu/utils/StaticUtils.h>
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch:-#include <faiss/gpu/utils/Transpose.cuh>
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch:-namespace faiss { namespace gpu {
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch:-                  cudaStream_t stream) {
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch:-  CUDA_TEST_ERROR();
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch:-                     cudaStream_t stream) {
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch:-                   cudaStream_t stream) {
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch:-  CUDA_TEST_ERROR();
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch:diff --git a/gpu/impl/PQScanMultiPassNoPrecomputed.cu b/gpu/impl/PQScanMultiPassNoPrecomputed.cu
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch:--- a/gpu/impl/PQScanMultiPassNoPrecomputed.cu
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch:-#include <faiss/gpu/impl/PQScanMultiPassNoPrecomputed.cuh>
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch:-#include <faiss/gpu/GpuResources.h>
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch:-#include <faiss/gpu/impl/PQCodeDistances.cuh>
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch:-#include <faiss/gpu/impl/PQCodeLoad.cuh>
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch:-#include <faiss/gpu/impl/IVFUtils.cuh>
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch:-#include <faiss/gpu/utils/ConversionOperators.cuh>
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch:-#include <faiss/gpu/utils/DeviceTensor.cuh>
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch:-#include <faiss/gpu/utils/DeviceUtils.h>
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch:-#include <faiss/gpu/utils/Float16.cuh>
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch:-#include <faiss/gpu/utils/LoadStoreOperators.cuh>
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch:-#include <faiss/gpu/utils/NoTypeTensor.cuh>
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch:-#include <faiss/gpu/utils/StaticUtils.h>
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch:-#include <faiss/gpu/utils/HostTensor.cuh>
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch:-namespace faiss { namespace gpu {
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch:-                 cudaStream_t stream) {
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch:-  CUDA_TEST_ERROR();
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch:-                                     GpuResources* res) {
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch:-  CUDA_VERIFY(cudaMemsetAsync(prefixSumOffsetSpace1.data(),
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch:-  CUDA_VERIFY(cudaMemsetAsync(prefixSumOffsetSpace2.data(),
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch:diff --git a/gpu/test/Makefile b/gpu/test/Makefile
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch:--- a/gpu/test/Makefile
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch:+++ b/gpu/test/Makefile
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch:@@ -17,14 +17,18 @@ TESTS_BIN = $(TESTS_OBJ:.o=) $(CUDA_TESTS_OBJ:.o=)
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch: # test_gpu_index.py test_pytorch_faiss.py
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch: run: $(TESTS_BIN) $(CUDA_TESTS_BIN)
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch: 	for t in $(TESTS_BIN) $(CUDA_TESTS_BIN); do ./$$t || exit; done
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch: $(CUDA_TESTS_OBJ): %.o: %.cu gtest
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch:@@ -33,7 +37,7 @@ demo_ivfpq_indexing_gpu: demo_ivfpq_indexing_gpu.o ../../libfaiss.a
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch: demo_ivfpq_indexing_gpu.o: demo_ivfpq_indexing_gpu.cpp
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch:diff --git a/gpu/test/demo_ivfpq_indexing_gpu.cpp b/gpu/test/demo_ivfpq_indexing_gpu.cpp
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch:--- a/gpu/test/demo_ivfpq_indexing_gpu.cpp
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch:+++ b/gpu/test/demo_ivfpq_indexing_gpu.cpp
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch: #include <faiss/gpu/StandardGpuResources.h>
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch: #include <faiss/gpu/GpuIndexIVFPQ.h>
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch:+#include <faiss/gpu/GpuCloner.h>
var/spack/repos/builtin/packages/faiss/fixes-in-v1.6.3.patch: #include <faiss/gpu/GpuAutoTune.h>
var/spack/repos/builtin/packages/faiss/package.py:class Faiss(AutotoolsPackage, CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/faiss/package.py:    are implemented on the GPU. It is developed by Facebook AI Research.
var/spack/repos/builtin/packages/faiss/package.py:    # for v1.6.3, GPU build has a bug (two files need to be deleted)
var/spack/repos/builtin/packages/faiss/package.py:    # also, some include paths in gpu/tests/Makefile are missing
var/spack/repos/builtin/packages/faiss/package.py:            self.define_from_variant("FAISS_ENABLE_GPU", "cuda"),
var/spack/repos/builtin/packages/faiss/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/faiss/package.py:            key = "CMAKE_CUDA_ARCHITECTURES"
var/spack/repos/builtin/packages/faiss/package.py:            args.append(self.define_from_variant(key, "cuda_arch"))
var/spack/repos/builtin/packages/faiss/package.py:            # 'CMAKE_CUDA_STANDARD', 'cudastd'))
var/spack/repos/builtin/packages/faiss/package.py:        args.extend(self.with_or_without("cuda", activation_value="prefix"))
var/spack/repos/builtin/packages/faiss/package.py:        # GPU tests
var/spack/repos/builtin/packages/faiss/package.py:        if self.spec.satisfies("+tests+cuda"):
var/spack/repos/builtin/packages/faiss/package.py:            with working_dir(os.path.join("gpu", "test")):
var/spack/repos/builtin/packages/faiss/package.py:                make("demo_ivfpq_indexing_gpu")
var/spack/repos/builtin/packages/faiss/package.py:            # rename the exec to keep consistent with gpu tests
var/spack/repos/builtin/packages/faiss/package.py:        # GPU tests
var/spack/repos/builtin/packages/faiss/package.py:        if self.spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/faiss/package.py:            with working_dir(os.path.join("gpu", "test")):
var/spack/repos/builtin/packages/faiss/package.py:                _prefix_and_install("TestGpuIndexFlat")
var/spack/repos/builtin/packages/faiss/package.py:                _prefix_and_install("TestGpuIndexBinaryFlat")
var/spack/repos/builtin/packages/faiss/package.py:                _prefix_and_install("TestGpuIndexIVFFlat")
var/spack/repos/builtin/packages/faiss/package.py:                _prefix_and_install("TestGpuIndexIVFPQ")
var/spack/repos/builtin/packages/faiss/package.py:                _prefix_and_install("TestGpuMemoryException")
var/spack/repos/builtin/packages/faiss/package.py:                _prefix_and_install("TestGpuSelect")
var/spack/repos/builtin/packages/faiss/package.py:                _prefix_and_install("demo_ivfpq_indexing_gpu")
var/spack/repos/builtin/packages/faiss/fixes-in-v1.5.3.patch: gpu/test/Makefile | 4 ++++
var/spack/repos/builtin/packages/faiss/fixes-in-v1.5.3.patch:diff --git a/gpu/test/Makefile b/gpu/test/Makefile
var/spack/repos/builtin/packages/faiss/fixes-in-v1.5.3.patch:--- a/gpu/test/Makefile
var/spack/repos/builtin/packages/faiss/fixes-in-v1.5.3.patch:+++ b/gpu/test/Makefile
var/spack/repos/builtin/packages/faiss/fixes-in-v1.5.3.patch:@@ -17,6 +17,10 @@ TESTS_BIN = $(TESTS_OBJ:.o=) $(CUDA_TESTS_OBJ:.o=)
var/spack/repos/builtin/packages/faiss/fixes-in-v1.5.3.patch: # test_gpu_index.py test_pytorch_faiss.py
var/spack/repos/builtin/packages/faiss/fixes-in-v1.5.3.patch: run: $(TESTS_BIN) $(CUDA_TESTS_BIN)
var/spack/repos/builtin/packages/faiss/fixes-in-v1.5.3.patch: 	for t in $(TESTS_BIN) $(CUDA_TESTS_BIN); do ./$$t || exit; done
var/spack/repos/builtin/packages/py-horovod/package.py:class PyHorovod(PythonPackage, CudaPackage):
var/spack/repos/builtin/packages/py-horovod/package.py:        default="nccl",
var/spack/repos/builtin/packages/py-horovod/package.py:        description="Framework to use for GPU/CPU operations",
var/spack/repos/builtin/packages/py-horovod/package.py:        values=("nccl", "mpi", "gloo", "ccl"),
var/spack/repos/builtin/packages/py-horovod/package.py:    variant("cuda", default=True, description="Build with CUDA")
var/spack/repos/builtin/packages/py-horovod/package.py:    variant("rocm", default=False, description="Build with ROCm")
var/spack/repos/builtin/packages/py-horovod/package.py:    depends_on("nccl@2:", when="tensor_ops=nccl")
var/spack/repos/builtin/packages/py-horovod/package.py:        "cuda_arch=none",
var/spack/repos/builtin/packages/py-horovod/package.py:        when="+cuda",
var/spack/repos/builtin/packages/py-horovod/package.py:        msg="Must specify CUDA compute capabilities of your GPU, see "
var/spack/repos/builtin/packages/py-horovod/package.py:        "https://developer.nvidia.com/cuda-gpus",
var/spack/repos/builtin/packages/py-horovod/package.py:        "tensor_ops=nccl", when="~cuda~rocm", msg="NCCL requires either CUDA or ROCm support"
var/spack/repos/builtin/packages/py-horovod/package.py:        if "tensor_ops=nccl" in self.spec:
var/spack/repos/builtin/packages/py-horovod/package.py:            env.set("HOROVOD_GPU_ALLREDUCE", "NCCL")
var/spack/repos/builtin/packages/py-horovod/package.py:            env.set("HOROVOD_GPU_ALLGATHER", "NCCL")
var/spack/repos/builtin/packages/py-horovod/package.py:            env.set("HOROVOD_GPU_BROADCAST", "NCCL")
var/spack/repos/builtin/packages/py-horovod/package.py:            env.set("HOROVOD_NCCL_HOME", self.spec["nccl"].prefix)
var/spack/repos/builtin/packages/py-horovod/package.py:            env.set("HOROVOD_NCCL_INCLUDE", self.spec["nccl"].headers.directories[0])
var/spack/repos/builtin/packages/py-horovod/package.py:            env.set("HOROVOD_NCCL_LIB", self.spec["nccl"].libs.directories[0])
var/spack/repos/builtin/packages/py-horovod/package.py:            if "+cuda" in self.spec:
var/spack/repos/builtin/packages/py-horovod/package.py:                env.set("HOROVOD_GPU", "CUDA")
var/spack/repos/builtin/packages/py-horovod/package.py:                env.set("HOROVOD_CUDA_HOME", self.spec["cuda"].prefix)
var/spack/repos/builtin/packages/py-horovod/package.py:                cuda_cc_list = ",".join(self.spec.variants["cuda_arch"].value)
var/spack/repos/builtin/packages/py-horovod/package.py:                env.set("HOROVOD_BUILD_CUDA_CC_LIST", cuda_cc_list)
var/spack/repos/builtin/packages/py-horovod/package.py:                env.set("HOROVOD_CUDA_INCLUDE", self.spec["cuda"].headers.directories[0])
var/spack/repos/builtin/packages/py-horovod/package.py:                env.set("HOROVOD_CUDA_LIB", self.spec["cuda"].libs.directories[0])
var/spack/repos/builtin/packages/py-horovod/package.py:            elif "+rocm" in self.spec:
var/spack/repos/builtin/packages/py-horovod/package.py:                env.set("HOROVOD_GPU", "ROCM")
var/spack/repos/builtin/packages/py-horovod/package.py:                # env.set('HOROVOD_ROCM_HOME', self.spec['rocm'].prefix)
var/spack/repos/builtin/packages/mallocmc/package.py:    many core accelerators. Currently, it supports NVIDIA GPUs of
var/spack/repos/builtin/packages/mallocmc/package.py:    depends_on("cuda@5.0:", type="link")
var/spack/repos/builtin/packages/py-mpi4jax/package.py:class PyMpi4jax(PythonPackage, CudaPackage):
var/spack/repos/builtin/packages/py-mpi4jax/package.py:        if "+cuda" in self.spec:
var/spack/repos/builtin/packages/py-mpi4jax/package.py:            env.set("CUDA_PATH", self.spec["cuda"].prefix)
var/spack/repos/builtin/packages/ams/package.py:class Ams(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/ams/package.py:    depends_on("caliper+cuda", when="+caliper +cuda")
var/spack/repos/builtin/packages/ams/package.py:    depends_on("faiss+cuda", when="+faiss +cuda")
var/spack/repos/builtin/packages/ams/package.py:    depends_on("mfem+cuda", when="+examples +cuda")
var/spack/repos/builtin/packages/ams/package.py:    depends_on("py-torch+cuda", when="+torch +cuda")
var/spack/repos/builtin/packages/ams/package.py:    depends_on("py-torch~cuda", when="+torch ~cuda")
var/spack/repos/builtin/packages/ams/package.py:    depends_on("caliper ~cuda", when="+caliper ~cuda")
var/spack/repos/builtin/packages/ams/package.py:    depends_on("faiss ~cuda", when="+faiss ~cuda")
var/spack/repos/builtin/packages/ams/package.py:    depends_on("mfem ~cuda", when="+examples ~cuda")
var/spack/repos/builtin/packages/ams/package.py:    with when("+cuda"):
var/spack/repos/builtin/packages/ams/package.py:        cuda_archs = CudaPackage.cuda_arch_values
var/spack/repos/builtin/packages/ams/package.py:            depends_on("mfem+cuda")
var/spack/repos/builtin/packages/ams/package.py:            for sm_ in cuda_archs:
var/spack/repos/builtin/packages/ams/package.py:                    "mfem +cuda cuda_arch={0}".format(sm_), when="cuda_arch={0}".format(sm_)
var/spack/repos/builtin/packages/ams/package.py:            depends_on("py-torch+cuda")
var/spack/repos/builtin/packages/ams/package.py:            for sm_ in cuda_archs:
var/spack/repos/builtin/packages/ams/package.py:                    "py-torch +cuda cuda_arch={0}".format(sm_), when="cuda_arch={0}".format(sm_)
var/spack/repos/builtin/packages/ams/package.py:            depends_on("caliper+cuda", when="+caliper")
var/spack/repos/builtin/packages/ams/package.py:            for sm_ in cuda_archs:
var/spack/repos/builtin/packages/ams/package.py:                    "caliper +cuda cuda_arch={0}".format(sm_), when="cuda_arch={0}".format(sm_)
var/spack/repos/builtin/packages/ams/package.py:        depends_on("umpire+cuda")
var/spack/repos/builtin/packages/ams/package.py:        for sm_ in cuda_archs:
var/spack/repos/builtin/packages/ams/package.py:            depends_on("umpire +cuda cuda_arch={0}".format(sm_), when="cuda_arch={0}".format(sm_))
var/spack/repos/builtin/packages/ams/package.py:            depends_on("faiss+cuda", when="+faiss")
var/spack/repos/builtin/packages/ams/package.py:            for sm_ in cuda_archs:
var/spack/repos/builtin/packages/ams/package.py:                    "umpire +cuda cuda_arch={0}".format(sm_), when="cuda_arch={0}".format(sm_)
var/spack/repos/builtin/packages/ams/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/ams/package.py:            args.append("-DWITH_CUDA=On")
var/spack/repos/builtin/packages/ams/package.py:            cuda_arch = spec.variants["cuda_arch"].value[0]
var/spack/repos/builtin/packages/ams/package.py:            args.append("-DAMS_CUDA_ARCH={0}".format(cuda_arch))
var/spack/repos/builtin/packages/hipblaslt/001_Set_LLVM_Paths_And_Add_Includes.patch: if(NOT BUILD_CUDA)
var/spack/repos/builtin/packages/hipblaslt/001_Set_LLVM_Paths_And_Add_Includes.patch:       globalParameters["AssemblerPath"] = locateExe(globalParameters["ROCmBinPath"], "clang++.exe")
var/spack/repos/builtin/packages/hipblaslt/001_Set_LLVM_Paths_And_Add_Includes.patch:-      globalParameters["AssemblerPath"] = locateExe(os.path.join(globalParameters["ROCmPath"], "llvm/bin"), "clang++")
var/spack/repos/builtin/packages/hipblaslt/001_Set_LLVM_Paths_And_Add_Includes.patch:   globalParameters["ROCmSMIPath"] = locateExe(globalParameters["ROCmBinPath"], "rocm-smi")
var/spack/repos/builtin/packages/hipblaslt/001_Set_LLVM_Paths_And_Add_Includes.patch:       globalParameters["ClangOffloadBundlerPath"] = locateExe(globalParameters["ROCmBinPath"], "clang-offload-bundler.exe")
var/spack/repos/builtin/packages/hipblaslt/001_Set_LLVM_Paths_And_Add_Includes.patch:-      globalParameters["ClangOffloadBundlerPath"] = locateExe(os.path.join(globalParameters["ROCmPath"], "llvm/bin"), "clang-offload-bundler")
var/spack/repos/builtin/packages/hipblaslt/001_Set_LLVM_Paths_And_Add_Includes.patch:   if "ROCmAgentEnumeratorPath" in config:
var/spack/repos/builtin/packages/hipblaslt/001_Set_LLVM_Paths_And_Add_Includes.patch:     globalParameters["ROCmAgentEnumeratorPath"] = config["ROCmAgentEnumeratorPath"]
var/spack/repos/builtin/packages/hipblaslt/001_Set_LLVM_Paths_And_Add_Includes.patch:@@ -32,7 +32,9 @@ if ! [ -z ${ROCM_PATH+x} ]; then
var/spack/repos/builtin/packages/hipblaslt/001_Set_LLVM_Paths_And_Add_Includes.patch:     rocm_path=${ROCM_PATH}
var/spack/repos/builtin/packages/hipblaslt/001_Set_LLVM_Paths_And_Add_Includes.patch:-toolchain=${rocm_path}/llvm/bin/clang++
var/spack/repos/builtin/packages/hipblaslt/0001-Set-LLVM_Path-Add-Hiblas-Include-to-CmakeLists-6.1.Patch: if(NOT BUILD_CUDA)
var/spack/repos/builtin/packages/hipblaslt/0001-Set-LLVM_Path-Add-Hiblas-Include-to-CmakeLists-6.1.Patch:       globalParameters["AssemblerPath"] = locateExe(globalParameters["ROCmBinPath"], "clang++.exe")
var/spack/repos/builtin/packages/hipblaslt/0001-Set-LLVM_Path-Add-Hiblas-Include-to-CmakeLists-6.1.Patch:-      globalParameters["AssemblerPath"] = locateExe(os.path.join(globalParameters["ROCmPath"], "llvm/bin"), "clang++")
var/spack/repos/builtin/packages/hipblaslt/0001-Set-LLVM_Path-Add-Hiblas-Include-to-CmakeLists-6.1.Patch:   globalParameters["ROCmSMIPath"] = locateExe(globalParameters["ROCmBinPath"], "rocm-smi")
var/spack/repos/builtin/packages/hipblaslt/0001-Set-LLVM_Path-Add-Hiblas-Include-to-CmakeLists-6.1.Patch:-  globalParameters["ROCmLdPath"]  = locateExe(os.path.join(globalParameters["ROCmPath"], "llvm/bin"), "ld.lld")
var/spack/repos/builtin/packages/hipblaslt/0001-Set-LLVM_Path-Add-Hiblas-Include-to-CmakeLists-6.1.Patch:+  globalParameters["ROCmLdPath"]  = locateExe(os.path.join(globalParameters["LLVMPath"], "bin"), "ld.lld")
var/spack/repos/builtin/packages/hipblaslt/0001-Set-LLVM_Path-Add-Hiblas-Include-to-CmakeLists-6.1.Patch:   globalParameters["ExtractKernelPath"] = locateExe(os.path.join(globalParameters["ROCmPath"], "hip/bin"), "extractkernel")
var/spack/repos/builtin/packages/hipblaslt/0001-Set-LLVM_Path-Add-Hiblas-Include-to-CmakeLists-6.1.Patch:       globalParameters["ClangOffloadBundlerPath"] = locateExe(globalParameters["ROCmBinPath"], "clang-offload-bundler.exe")
var/spack/repos/builtin/packages/hipblaslt/0001-Set-LLVM_Path-Add-Hiblas-Include-to-CmakeLists-6.1.Patch:-      globalParameters["ClangOffloadBundlerPath"] = locateExe(os.path.join(globalParameters["ROCmPath"], "llvm/bin"), "clang-offload-bundler")
var/spack/repos/builtin/packages/hipblaslt/0001-Set-LLVM_Path-Add-Hiblas-Include-to-CmakeLists-6.1.Patch:   if "ROCmAgentEnumeratorPath" in config:
var/spack/repos/builtin/packages/hipblaslt/0001-Set-LLVM_Path-Add-Hiblas-Include-to-CmakeLists-6.1.Patch:     globalParameters["ROCmAgentEnumeratorPath"] = config["ROCmAgentEnumeratorPath"]
var/spack/repos/builtin/packages/hipblaslt/0001-Set-LLVM_Path-Add-Hiblas-Include-to-CmakeLists-6.1.Patch:@@ -32,7 +32,7 @@ if ! [ -z ${ROCM_PATH+x} ]; then
var/spack/repos/builtin/packages/hipblaslt/0001-Set-LLVM_Path-Add-Hiblas-Include-to-CmakeLists-6.1.Patch:     rocm_path=${ROCM_PATH}
var/spack/repos/builtin/packages/hipblaslt/0001-Set-LLVM_Path-Add-Hiblas-Include-to-CmakeLists-6.1.Patch:-toolchain=${rocm_path}/llvm/bin/clang++
var/spack/repos/builtin/packages/hipblaslt/package.py:    homepage = "https://github.com/ROCm/hipBLASLt"
var/spack/repos/builtin/packages/hipblaslt/package.py:    url = "https://github.com/ROCm/hipBLASLt/archive/refs/tags/rocm-6.1.2.tar.gz"
var/spack/repos/builtin/packages/hipblaslt/package.py:    git = "https://github.com/ROCm/hipBLASLt.git"
var/spack/repos/builtin/packages/hipblaslt/package.py:    amdgpu_targets = ROCmPackage.amdgpu_targets
var/spack/repos/builtin/packages/hipblaslt/package.py:        "amdgpu_target",
var/spack/repos/builtin/packages/hipblaslt/package.py:        description="AMD GPU architecture",
var/spack/repos/builtin/packages/hipblaslt/package.py:        values=auto_or_any_combination_of(*amdgpu_targets),
var/spack/repos/builtin/packages/hipblaslt/package.py:        depends_on(f"rocm-openmp-extras@{ver}", type="test", when=f"@{ver}")
var/spack/repos/builtin/packages/hipblaslt/package.py:        if "auto" not in self.spec.variants["amdgpu_target"]:
var/spack/repos/builtin/packages/hipblaslt/package.py:            args.append(self.define_from_variant("AMDGPU_TARGETS", "amdgpu_target"))
var/spack/repos/builtin/packages/hipblaslt/package.py:                self.define("ROCM_OPENMP_EXTRAS_DIR", self.spec["rocm-openmp-extras"].prefix)
var/spack/repos/builtin/packages/eospac/package.py:    # GPU offload is only available for version 6.5+
var/spack/repos/builtin/packages/eospac/package.py:    variant("offload", default=False, description="Build GPU offload library instead of standard")
var/spack/repos/builtin/packages/hipsolver/001-suite-sparse-include-path.patch:@@ -88,6 +88,7 @@ include( ROCMPackageConfigHelpers )
var/spack/repos/builtin/packages/hipsolver/001-suite-sparse-include-path.patch: include( ROCMInstallSymlinks )
var/spack/repos/builtin/packages/hipsolver/001-suite-sparse-include-path.patch: include( ROCMClients )
var/spack/repos/builtin/packages/hipsolver/001-suite-sparse-include-path.patch: include( ROCMHeaderWrapper )
var/spack/repos/builtin/packages/hipsolver/001-suite-sparse-include-path.patch: rocm_setup_version( VERSION ${VERSION_STRING} )
var/spack/repos/builtin/packages/hipsolver/001-suite-sparse-include-path.patch:@@ -135,7 +135,7 @@ if( NOT USE_CUDA )
var/spack/repos/builtin/packages/hipsolver/package.py:class Hipsolver(CMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/hipsolver/package.py:    homepage = "https://github.com/ROCm/hipSOLVER"
var/spack/repos/builtin/packages/hipsolver/package.py:    git = "https://github.com/ROCm/hipSOLVER.git"
var/spack/repos/builtin/packages/hipsolver/package.py:    url = "https://github.com/ROCm/hipSOLVER/archive/rocm-6.1.2.tar.gz"
var/spack/repos/builtin/packages/hipsolver/package.py:    tags = ["rocm"]
var/spack/repos/builtin/packages/hipsolver/package.py:    # default to an 'auto' variant until amdgpu_targets can be given a better default than 'none'
var/spack/repos/builtin/packages/hipsolver/package.py:    amdgpu_targets = ROCmPackage.amdgpu_targets
var/spack/repos/builtin/packages/hipsolver/package.py:        "amdgpu_target",
var/spack/repos/builtin/packages/hipsolver/package.py:        description="AMD GPU architecture",
var/spack/repos/builtin/packages/hipsolver/package.py:        values=spack.variant.DisjointSetsOfValues(("auto",), ("none",), amdgpu_targets)
var/spack/repos/builtin/packages/hipsolver/package.py:    variant("rocm", default=True, description="Enable ROCm support")
var/spack/repos/builtin/packages/hipsolver/package.py:    conflicts("+cuda +rocm", msg="CUDA and ROCm support are mutually exclusive")
var/spack/repos/builtin/packages/hipsolver/package.py:    conflicts("~cuda ~rocm", msg="CUDA or ROCm support is required")
var/spack/repos/builtin/packages/hipsolver/package.py:    depends_on("rocm-cmake@5.2.0:", type="build", when="@5.2.0:")
var/spack/repos/builtin/packages/hipsolver/package.py:    depends_on("rocm-cmake@4.5.0:", type="build")
var/spack/repos/builtin/packages/hipsolver/package.py:    depends_on("hip +cuda", when="+cuda")
var/spack/repos/builtin/packages/hipsolver/package.py:        depends_on(f"rocblas@{ver}", when=f"+rocm @{ver}")
var/spack/repos/builtin/packages/hipsolver/package.py:        depends_on(f"rocsolver@{ver}", when=f"+rocm @{ver}")
var/spack/repos/builtin/packages/hipsolver/package.py:    for tgt in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/hipsolver/package.py:        depends_on(f"rocblas amdgpu_target={tgt}", when=f"+rocm amdgpu_target={tgt}")
var/spack/repos/builtin/packages/hipsolver/package.py:        depends_on(f"rocsolver amdgpu_target={tgt}", when=f"+rocm amdgpu_target={tgt}")
var/spack/repos/builtin/packages/hipsolver/package.py:        args.append(self.define_from_variant("USE_CUDA", "cuda"))
var/spack/repos/builtin/packages/hipsolver/package.py:        # FindHIP.cmake is still used for +cuda
var/spack/repos/builtin/packages/hipsolver/package.py:        if self.spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/hipsolver/0001-suite-sparse-include-path-6.1.1.patch:@@ -88,6 +88,7 @@ include( ROCMPackageConfigHelpers )
var/spack/repos/builtin/packages/hipsolver/0001-suite-sparse-include-path-6.1.1.patch: include( ROCMInstallSymlinks )
var/spack/repos/builtin/packages/hipsolver/0001-suite-sparse-include-path-6.1.1.patch: include( ROCMClients )
var/spack/repos/builtin/packages/hipsolver/0001-suite-sparse-include-path-6.1.1.patch: include( ROCMHeaderWrapper )
var/spack/repos/builtin/packages/hipsolver/0001-suite-sparse-include-path-6.1.1.patch: rocm_setup_version( VERSION ${VERSION_STRING} )
var/spack/repos/builtin/packages/hipsolver/0001-suite-sparse-include-path-6.1.1.patch:@@ -135,7 +135,7 @@ if( NOT USE_CUDA )
var/spack/repos/builtin/packages/omniperf/package.py:    homepage = "https://github.com/ROCm/omniperf"
var/spack/repos/builtin/packages/omniperf/package.py:    git = "https://github.com/ROCm/omniperf.git"
var/spack/repos/builtin/packages/omniperf/package.py:    url = "https://github.com/ROCm/omniperf/archive/refs/tags/rocm-6.2.1.tar.gz"
var/spack/repos/builtin/packages/omniperf/package.py:    tags = ["rocm"]
var/spack/repos/builtin/packages/omniperf/package.py:    # VERSION.sha is not in the auto-generated ROCm release tarball
var/spack/repos/builtin/packages/heffte/package.py:class Heffte(CMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/heffte/package.py:    depends_on("cmake@3.21:", when="@2.4.0:+rocm", type=("build", "run"))
var/spack/repos/builtin/packages/heffte/package.py:    depends_on("py-numba", when="+python+cuda", type=("build", "run"))
var/spack/repos/builtin/packages/heffte/package.py:    conflicts("^openmpi~cuda", when="+cuda")  # +cuda requires CUDA enabled OpenMPI
var/spack/repos/builtin/packages/heffte/package.py:    conflicts("~cuda~rocm", when="+magma")  # magma requires CUDA or HIP
var/spack/repos/builtin/packages/heffte/package.py:    conflicts("+rocm", when="@:2.1.0")  # heffte+rocm is in in development in spack
var/spack/repos/builtin/packages/heffte/package.py:    depends_on("cuda@8.0:", when="+cuda", type=("build", "run"))
var/spack/repos/builtin/packages/heffte/package.py:    depends_on("hip@3.8.0:", when="+rocm", type=("build", "run"))
var/spack/repos/builtin/packages/heffte/package.py:    depends_on("rocfft@3.8.0:", when="+rocm", type=("build", "run"))
var/spack/repos/builtin/packages/heffte/package.py:    depends_on("hip@5.2.3:", when="@2.4.0:+rocm", type=("build", "run"))
var/spack/repos/builtin/packages/heffte/package.py:    depends_on("rocfft@5.2.3:", when="@2.4.0:+rocm", type=("build", "run"))
var/spack/repos/builtin/packages/heffte/package.py:    depends_on("magma@2.5.3:", when="+cuda+magma", type=("build", "run"))
var/spack/repos/builtin/packages/heffte/package.py:    depends_on("magma+rocm@2.6.1:", when="+magma+rocm @2.1:", type=("build", "run"))
var/spack/repos/builtin/packages/heffte/package.py:    depends_on("rocblas@3.8:", when="+magma+rocm", type=("build", "run"))
var/spack/repos/builtin/packages/heffte/package.py:    depends_on("rocsparse@3.8:", when="+magma+rocm", type=("build", "run"))
var/spack/repos/builtin/packages/heffte/package.py:    depends_on("hipblas@3.8:", when="+magma+rocm", type=("build", "run"))
var/spack/repos/builtin/packages/heffte/package.py:    depends_on("hipsparse@3.8:", when="+magma+rocm", type=("build", "run"))
var/spack/repos/builtin/packages/heffte/package.py:            self.define_from_variant("Heffte_ENABLE_CUDA", "cuda"),
var/spack/repos/builtin/packages/heffte/package.py:            self.define_from_variant("Heffte_ENABLE_ROCM", "rocm"),
var/spack/repos/builtin/packages/heffte/package.py:        if self.spec.satisfies("+cuda") and self.spec.satisfies("@:2.3.0"):
var/spack/repos/builtin/packages/heffte/package.py:            cuda_arch = self.spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/heffte/package.py:            if len(cuda_arch) > 0 or cuda_arch[0] != "none":
var/spack/repos/builtin/packages/heffte/package.py:                for nvflag in self.cuda_flags(cuda_arch):
var/spack/repos/builtin/packages/heffte/package.py:                args.append("-DCUDA_NVCC_FLAGS={0}".format(nvcc_flags))
var/spack/repos/builtin/packages/heffte/package.py:                archs = ";".join(cuda_arch)
var/spack/repos/builtin/packages/heffte/package.py:                args.append("-DCMAKE_CUDA_ARCHITECTURES=%s" % archs)
var/spack/repos/builtin/packages/heffte/package.py:        if self.spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/heffte/package.py:            rocm_arch = self.spec.variants["amdgpu_target"].value
var/spack/repos/builtin/packages/heffte/package.py:            if "none" not in rocm_arch:
var/spack/repos/builtin/packages/heffte/package.py:                args.append("-DCMAKE_CXX_FLAGS={0}".format(self.hip_flags(rocm_arch)))
var/spack/repos/builtin/packages/heffte/package.py:            # See https://github.com/ROCm/rocFFT/issues/322
var/spack/repos/builtin/packages/heffte/package.py:                args.append(self.define("__skip_rocmclang", "ON"))
var/spack/repos/builtin/packages/heffte/package.py:        if self.spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/heffte/package.py:                        self.spec["llvm-amdgpu"].prefix.lib.cmake.AMDDeviceLibs,
var/spack/repos/builtin/packages/heffte/cmake-magma-v230.patch:     if (@Heffte_ENABLE_CUDA@)
var/spack/repos/builtin/packages/heffte/cmake-magma-v230.patch:         list(FILTER CUDA_CUBLAS_LIBRARIES EXCLUDE REGEX "-NOTFOUND$") # work-around CMake 3.10 + CUDA 10
var/spack/repos/builtin/packages/heffte/cmake-magma-v230.patch:-        target_link_libraries(Heffte Heffte::MAGMA INTERFACE ${CUDA_CUBLAS_LIBRARIES})
var/spack/repos/builtin/packages/heffte/cmake-magma-v230.patch:+        target_link_libraries(Heffte::MAGMA INTERFACE ${CUDA_CUBLAS_LIBRARIES})
var/spack/repos/builtin/packages/heffte/cmake-magma-v230.patch:     if (@Heffte_ENABLE_ROCM@)
var/spack/repos/builtin/packages/py-tfdlpack/package.py:    variant("cuda", default=True, description="Build with CUDA support")
var/spack/repos/builtin/packages/py-tfdlpack/package.py:    depends_on("cuda", when="+cuda")
var/spack/repos/builtin/packages/py-tfdlpack/package.py:        return [self.define_from_variant("USE_CUDA", "cuda")]
var/spack/repos/builtin/packages/py-tfdlpack/package.py:        # Prevent TensorFlow from taking over the whole GPU
var/spack/repos/builtin/packages/py-tfdlpack/package.py:        env.set("TF_FORCE_GPU_ALLOW_GROWTH", "true")
var/spack/repos/builtin/packages/py-torch-sparse/package.py:            if "+cuda" in self.spec["py-torch"]:
var/spack/repos/builtin/packages/py-torch-sparse/package.py:                env.set("FORCE_CUDA", 1)
var/spack/repos/builtin/packages/py-torch-sparse/package.py:                env.set("FORCE_ONLY_CUDA", 0)
var/spack/repos/builtin/packages/py-torch-sparse/package.py:                env.set("FORCE_CUDA", 0)
var/spack/repos/builtin/packages/py-torch-sparse/package.py:                env.set("FORCE_ONLY_CUDA", 0)
var/spack/repos/builtin/packages/py-torch-sparse/package.py:            if "+cuda" in self.spec["py-torch"]:
var/spack/repos/builtin/packages/py-torch-sparse/package.py:                env.set("FORCE_CUDA", 1)
var/spack/repos/builtin/packages/py-torch-sparse/package.py:                env.set("FORCE_CUDA", 0)
var/spack/repos/builtin/packages/plumed/package.py:        values=("none", "cpu", "cuda", "opencl"),
var/spack/repos/builtin/packages/plumed/package.py:    depends_on("arrayfire+cuda", when="arrayfire=cuda")
var/spack/repos/builtin/packages/plumed/package.py:    depends_on("arrayfire+opencl", when="arrayfire=opencl")
var/spack/repos/builtin/packages/plumed/package.py:                "--enable-af_cuda={0}".format("yes" if "arrayfire=cuda" in spec else "no"),
var/spack/repos/builtin/packages/starpu/package.py:    one implementation for CPUs, and one implementation for CUDA).
var/spack/repos/builtin/packages/starpu/package.py:    variant("cuda", default=False, description="Enable CUDA support")
var/spack/repos/builtin/packages/starpu/package.py:    variant("opencl", default=False, description="Enable OpenCL support")
var/spack/repos/builtin/packages/starpu/package.py:    depends_on("hwloc+cuda", when="+cuda")
var/spack/repos/builtin/packages/starpu/package.py:    depends_on("cuda", when="+cuda~simgrid")
var/spack/repos/builtin/packages/starpu/package.py:                "--%s-opencl"
var/spack/repos/builtin/packages/starpu/package.py:                % ("disable" if "~opencl" in spec or "+simgrid" in spec else "enable"),
var/spack/repos/builtin/packages/starpu/package.py:                "--%s-cuda" % ("disable" if "~cuda" in spec or "+simgrid" in spec else "enable"),
var/spack/repos/builtin/packages/py-torch-nvidia-apex/package.py:class PyTorchNvidiaApex(PythonPackage, CudaPackage):
var/spack/repos/builtin/packages/py-torch-nvidia-apex/package.py:    homepage = "https://github.com/nvidia/apex/"
var/spack/repos/builtin/packages/py-torch-nvidia-apex/package.py:    git = "https://github.com/nvidia/apex/"
var/spack/repos/builtin/packages/py-torch-nvidia-apex/package.py:    url = "https://github.com/NVIDIA/apex/archive/refs/tags/24.04.01.tar.gz"
var/spack/repos/builtin/packages/py-torch-nvidia-apex/package.py:    variant("cuda", default=True, description="Build with CUDA")
var/spack/repos/builtin/packages/py-torch-nvidia-apex/package.py:        "permutation_search_cuda", default=False, description="Build permutation search module"
var/spack/repos/builtin/packages/py-torch-nvidia-apex/package.py:    variant("focal_loss_cuda", default=False, description="Build focal loss module")
var/spack/repos/builtin/packages/py-torch-nvidia-apex/package.py:    variant("peer_memory_cuda", default=False, description="Build peer memory module")
var/spack/repos/builtin/packages/py-torch-nvidia-apex/package.py:    variant("nccl_p2p_cuda", default=False, description="Build with nccl p2p")
var/spack/repos/builtin/packages/py-torch-nvidia-apex/package.py:        "+peer_memory_cuda+nccl_p2p_cuda",
var/spack/repos/builtin/packages/py-torch-nvidia-apex/package.py:        msg="+fast_bottleneck requires both +peer_memory_cuda and +nccl_p2p_cuda to be enabled.",
var/spack/repos/builtin/packages/py-torch-nvidia-apex/package.py:    requires("^nccl@2.10:", when="+nccl_p2p_cuda")
var/spack/repos/builtin/packages/py-torch-nvidia-apex/package.py:        for _arch in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/py-torch-nvidia-apex/package.py:            depends_on(f"py-torch+cuda cuda_arch={_arch}", when=f"+cuda cuda_arch={_arch}")
var/spack/repos/builtin/packages/py-torch-nvidia-apex/package.py:    depends_on("cuda@9:", when="+cuda")
var/spack/repos/builtin/packages/py-torch-nvidia-apex/package.py:    # https://github.com/NVIDIA/apex/issues/1498
var/spack/repos/builtin/packages/py-torch-nvidia-apex/package.py:    # https://github.com/NVIDIA/apex/pull/1499
var/spack/repos/builtin/packages/py-torch-nvidia-apex/package.py:        "cuda_arch=none",
var/spack/repos/builtin/packages/py-torch-nvidia-apex/package.py:        when="+cuda",
var/spack/repos/builtin/packages/py-torch-nvidia-apex/package.py:        msg="Must specify CUDA compute capabilities of your GPU, see "
var/spack/repos/builtin/packages/py-torch-nvidia-apex/package.py:        "https://developer.nvidia.com/cuda-gpus",
var/spack/repos/builtin/packages/py-torch-nvidia-apex/package.py:    def torch_cuda_arch_list(self, env):
var/spack/repos/builtin/packages/py-torch-nvidia-apex/package.py:        if self.spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/py-torch-nvidia-apex/package.py:            torch_cuda_arch = ";".join(
var/spack/repos/builtin/packages/py-torch-nvidia-apex/package.py:                "{0:.1f}".format(float(i) / 10.0) for i in self.spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/py-torch-nvidia-apex/package.py:            env.set("TORCH_CUDA_ARCH_LIST", torch_cuda_arch)
var/spack/repos/builtin/packages/py-torch-nvidia-apex/package.py:        if self.spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/py-torch-nvidia-apex/package.py:            env.set("CUDA_HOME", self.spec["cuda"].prefix)
var/spack/repos/builtin/packages/py-torch-nvidia-apex/package.py:            self.torch_cuda_arch_list(env)
var/spack/repos/builtin/packages/py-torch-nvidia-apex/package.py:            env.unset("CUDA_HOME")
var/spack/repos/builtin/packages/py-torch-nvidia-apex/package.py:        self.torch_cuda_arch_list(env)
var/spack/repos/builtin/packages/py-torch-nvidia-apex/package.py:            if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/py-torch-nvidia-apex/package.py:                args.append("--cuda_ext")
var/spack/repos/builtin/packages/py-torch-nvidia-apex/package.py:        if spec.satisfies("+permutation_search_cuda"):
var/spack/repos/builtin/packages/py-torch-nvidia-apex/package.py:        if spec.satisfies("+focal_loss_cuda"):
var/spack/repos/builtin/packages/py-torch-nvidia-apex/package.py:        if spec.satisfies("+peer_memory_cuda"):
var/spack/repos/builtin/packages/py-torch-nvidia-apex/package.py:        if spec.satisfies("+nccl_p2p_cuda"):
var/spack/repos/builtin/packages/py-torch-nvidia-apex/package.py:            args.append("--nccl_p2p")
var/spack/repos/builtin/packages/py-torch-nvidia-apex/package.py:            if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/py-torch-nvidia-apex/package.py:                global_options += " --cuda_ext"
var/spack/repos/builtin/packages/py-torch-nvidia-apex/package.py:        if spec.satisfies("+permutation_search_cuda"):
var/spack/repos/builtin/packages/py-torch-nvidia-apex/package.py:        if spec.satisfies("+focal_loss_cuda"):
var/spack/repos/builtin/packages/py-torch-nvidia-apex/package.py:        if spec.satisfies("+peer_memory_cuda"):
var/spack/repos/builtin/packages/py-torch-nvidia-apex/package.py:        if spec.satisfies("+nccl_p2p_cuda"):
var/spack/repos/builtin/packages/py-torch-nvidia-apex/package.py:            global_options += " --nccl_p2p"
var/spack/repos/builtin/packages/py-torch-nvidia-apex/1499.patch:@@ -31,7 +31,7 @@ if not torch.cuda.is_available():
var/spack/repos/builtin/packages/py-torch-nvidia-apex/1499.patch:           'and, if the CUDA version is >= 11.0, Ampere (compute capability 8.0).\n'
var/spack/repos/builtin/packages/py-torch-nvidia-apex/1499.patch:           'export TORCH_CUDA_ARCH_LIST="compute capability" before running setup.py.\n')
var/spack/repos/builtin/packages/py-torch-nvidia-apex/1499.patch:-    if os.environ.get("TORCH_CUDA_ARCH_LIST", None) is None:
var/spack/repos/builtin/packages/py-torch-nvidia-apex/1499.patch:+    if os.environ.get("TORCH_CUDA_ARCH_LIST", None) is None and cpp_extension.CUDA_HOME is not None:
var/spack/repos/builtin/packages/py-torch-nvidia-apex/1499.patch:         _, bare_metal_major, _ = get_cuda_bare_metal_version(cpp_extension.CUDA_HOME)
var/spack/repos/builtin/packages/py-torch-nvidia-apex/1499.patch:             os.environ["TORCH_CUDA_ARCH_LIST"] = "6.0;6.1;6.2;7.0;7.5;8.0"
var/spack/repos/builtin/packages/slate/package.py:class Slate(CMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/slate/package.py:    depends_on("blaspp ~cuda", when="~cuda")
var/spack/repos/builtin/packages/slate/package.py:    depends_on("blaspp +cuda", when="+cuda")
var/spack/repos/builtin/packages/slate/package.py:    depends_on("blaspp ~rocm", when="~rocm")
var/spack/repos/builtin/packages/slate/package.py:    depends_on("lapackpp ~cuda", when="~cuda")
var/spack/repos/builtin/packages/slate/package.py:    depends_on("lapackpp +cuda", when="+cuda")
var/spack/repos/builtin/packages/slate/package.py:    depends_on("lapackpp ~rocm", when="~rocm")
var/spack/repos/builtin/packages/slate/package.py:    for val in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/slate/package.py:        depends_on("blaspp +cuda cuda_arch=%s" % val, when="cuda_arch=%s" % val)
var/spack/repos/builtin/packages/slate/package.py:        depends_on("lapackpp +cuda cuda_arch=%s" % val, when="cuda_arch=%s" % val)
var/spack/repos/builtin/packages/slate/package.py:    for val in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/slate/package.py:        depends_on("blaspp +rocm amdgpu_target=%s" % val, when="amdgpu_target=%s" % val)
var/spack/repos/builtin/packages/slate/package.py:        depends_on("lapackpp +rocm amdgpu_target=%s" % val, when="amdgpu_target=%s" % val)
var/spack/repos/builtin/packages/slate/package.py:    depends_on("hipify-clang", when="@:2021.05.02 +rocm ^hip@5:")
var/spack/repos/builtin/packages/slate/package.py:    depends_on("comgr", when="+rocm")
var/spack/repos/builtin/packages/slate/package.py:    depends_on("rocblas", when="+rocm")
var/spack/repos/builtin/packages/slate/package.py:    depends_on("rocsolver", when="+rocm")
var/spack/repos/builtin/packages/slate/package.py:        "+rocm", when="@:2020.10.00", msg="ROCm support requires SLATE 2021.05.01 or greater"
var/spack/repos/builtin/packages/slate/package.py:    backend_msg = "SLATE supports only one GPU backend at a time"
var/spack/repos/builtin/packages/slate/package.py:    conflicts("+rocm", when="+cuda", msg=backend_msg)
var/spack/repos/builtin/packages/slate/package.py:    conflicts("+rocm", when="+sycl", msg=backend_msg)
var/spack/repos/builtin/packages/slate/package.py:    conflicts("+cuda", when="+sycl", msg=backend_msg)
var/spack/repos/builtin/packages/slate/package.py:    conflicts("^hip@5.6.0:", when="@:2023.08.25", msg="Incompatible version of HIP/ROCm")
var/spack/repos/builtin/packages/slate/package.py:        backend_config = "-Duse_cuda=%s" % ("+cuda" in spec)
var/spack/repos/builtin/packages/slate/package.py:            if "+cuda" in spec:
var/spack/repos/builtin/packages/slate/package.py:                backend = "cuda"
var/spack/repos/builtin/packages/slate/package.py:            if "+rocm" in spec:
var/spack/repos/builtin/packages/slate/package.py:            backend_config = "-Dgpu_backend=%s" % backend
var/spack/repos/builtin/packages/slate/package.py:        if "+cuda" in spec:
var/spack/repos/builtin/packages/slate/package.py:            archs = ";".join(spec.variants["cuda_arch"].value)
var/spack/repos/builtin/packages/slate/package.py:            config.append("-DCMAKE_CUDA_ARCHITECTURES=%s" % archs)
var/spack/repos/builtin/packages/slate/package.py:        if "+rocm" in spec:
var/spack/repos/builtin/packages/slate/package.py:            archs = ";".join(spec.variants["amdgpu_target"].value)
var/spack/repos/builtin/packages/slate/package.py:            if self.spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/slate/package.py:                deps += " rocblas hip llvm-amdgpu comgr hsa-rocr-dev rocsolver "
var/spack/repos/builtin/packages/r-factoextra/package.py:    depends_on("r-ggpubr@0.1.5:", type=("build", "run"))
var/spack/repos/builtin/packages/bigdft-futile/package.py:class BigdftFutile(AutotoolsPackage, CudaPackage):
var/spack/repos/builtin/packages/bigdft-futile/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/bigdft-futile/package.py:            args.append("--enable-opencl")
var/spack/repos/builtin/packages/bigdft-futile/package.py:            args.append(f"--with-ocl-path={spec['cuda'].prefix}")
var/spack/repos/builtin/packages/bigdft-futile/package.py:            args.append("--enable-cuda-gpu")
var/spack/repos/builtin/packages/bigdft-futile/package.py:            args.append(f"--with-cuda-path={spec['cuda'].prefix}")
var/spack/repos/builtin/packages/trilinos/fix_cxx14_cuda11.patch:+# if CUDA_VERSION < 11000
var/spack/repos/builtin/packages/trilinos/fix_cxx14_cuda11.patch:   // cudaPointerAttributes::type doesn't exist yet in CUDA 9.2.  We
var/spack/repos/builtin/packages/trilinos/fix_cxx14_cuda11.patch:+# else // >=CUDA-11
var/spack/repos/builtin/packages/trilinos/fix_cxx14_cuda11.patch:   const enum cudaMemoryType theType = attr.type;
var/spack/repos/builtin/packages/trilinos/fix_cxx14_cuda11.patch:   if (theType == cudaMemoryTypeManaged) {
var/spack/repos/builtin/packages/trilinos/fix_cxx14_cuda11.patch:+# endif // < CUDA-11
var/spack/repos/builtin/packages/trilinos/fix_cxx14_cuda11.patch: #else // NOT HAVE_TPETRACORE_CUDA
var/spack/repos/builtin/packages/trilinos/fix_cxx14_cuda11.patch: #endif // HAVE_TPETRACORE_CUDA
var/spack/repos/builtin/packages/trilinos/package.py:class Trilinos(CMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/trilinos/package.py:        "cuda_constexpr",
var/spack/repos/builtin/packages/trilinos/package.py:        description="Enable relaxed constexpr functions for CUDA build",
var/spack/repos/builtin/packages/trilinos/package.py:    variant("cuda_rdc", default=False, description="Turn on RDC for CUDA build")
var/spack/repos/builtin/packages/trilinos/package.py:    variant("rocm_rdc", default=False, description="Turn on RDC for ROCm build")
var/spack/repos/builtin/packages/trilinos/package.py:    variant("uvm", default=False, when="@13.2: +cuda", description="Turn on UVM for CUDA build")
var/spack/repos/builtin/packages/trilinos/package.py:    variant("wrapper", default=False, description="Use nvcc-wrapper for CUDA build")
var/spack/repos/builtin/packages/trilinos/package.py:        conflicts("+cuda")
var/spack/repos/builtin/packages/trilinos/package.py:        conflicts("+rocm")
var/spack/repos/builtin/packages/trilinos/package.py:    conflicts("cxxstd=11", when="+wrapper ^cuda@6.5.14")
var/spack/repos/builtin/packages/trilinos/package.py:    conflicts("cxxstd=14", when="+wrapper ^cuda@6.5.14:8.0.61")
var/spack/repos/builtin/packages/trilinos/package.py:    conflicts("cxxstd=17", when="+wrapper ^cuda@6.5.14:10.2.89")
var/spack/repos/builtin/packages/trilinos/package.py:    # CUDA without wrapper requires clang
var/spack/repos/builtin/packages/trilinos/package.py:        when="+cuda~wrapper",
var/spack/repos/builtin/packages/trilinos/package.py:        msg="trilinos~wrapper+cuda can only be built with the Clang compiler",
var/spack/repos/builtin/packages/trilinos/package.py:    conflicts("+cuda_rdc", when="~cuda")
var/spack/repos/builtin/packages/trilinos/package.py:    conflicts("+rocm_rdc", when="~rocm")
var/spack/repos/builtin/packages/trilinos/package.py:    conflicts("+wrapper", when="~cuda")
var/spack/repos/builtin/packages/trilinos/package.py:    # Old trilinos fails with new CUDA (see #27180)
var/spack/repos/builtin/packages/trilinos/package.py:    conflicts("@:13.0.1 +cuda", when="^cuda@11:")
var/spack/repos/builtin/packages/trilinos/package.py:    # Build hangs with CUDA 11.6 (see #28439)
var/spack/repos/builtin/packages/trilinos/package.py:    conflicts("+cuda +stokhos", when="^cuda@11.6:")
var/spack/repos/builtin/packages/trilinos/package.py:    # superlu-dist defines a macro EMPTY which conflicts with a header in cuda
var/spack/repos/builtin/packages/trilinos/package.py:    conflicts("+cuda +stokhos +superlu-dist")
var/spack/repos/builtin/packages/trilinos/package.py:    for a in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/trilinos/package.py:        arch_str = "+cuda cuda_arch={0}".format(a)
var/spack/repos/builtin/packages/trilinos/package.py:    for a in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/trilinos/package.py:        arch_str = "+rocm amdgpu_target={0}".format(a)
var/spack/repos/builtin/packages/trilinos/package.py:    depends_on("hwloc+cuda", when="@13: +kokkos+cuda")
var/spack/repos/builtin/packages/trilinos/package.py:    # avoid calling deprecated functions with CUDA-11
var/spack/repos/builtin/packages/trilinos/package.py:    patch("fix_cxx14_cuda11.patch", when="@13.0.0:13.0.1 cxxstd=14 ^cuda@11:")
var/spack/repos/builtin/packages/trilinos/package.py:        when="@15.0.0 ^hip@6.0 +rocm",
var/spack/repos/builtin/packages/trilinos/package.py:            if spec.satisfies("platform=linux ~cuda"):
var/spack/repos/builtin/packages/trilinos/package.py:                # CUDA is enabled (possibly only with MPI as well) the linker
var/spack/repos/builtin/packages/trilinos/package.py:        if "+cuda" in self.spec:
var/spack/repos/builtin/packages/trilinos/package.py:            # it relies on blocking CUDA kernel launch. This is needed
var/spack/repos/builtin/packages/trilinos/package.py:            # in case the dependent app also run a CUDA backend via Trilinos
var/spack/repos/builtin/packages/trilinos/package.py:            env.set("CUDA_LAUNCH_BLOCKING", "1")
var/spack/repos/builtin/packages/trilinos/package.py:        if "+cuda" in spec and "+wrapper" in spec:
var/spack/repos/builtin/packages/trilinos/package.py:        if "+rocm" in spec:
var/spack/repos/builtin/packages/trilinos/package.py:            ("CUDA", "cuda", "cuda"),
var/spack/repos/builtin/packages/trilinos/package.py:                    define_kok_enable("CUDA"),
var/spack/repos/builtin/packages/trilinos/package.py:            if "+cuda" in spec:
var/spack/repos/builtin/packages/trilinos/package.py:                        define_kok_enable("CUDA_UVM", use_uvm),
var/spack/repos/builtin/packages/trilinos/package.py:                        define_kok_enable("CUDA_LAMBDA", True),
var/spack/repos/builtin/packages/trilinos/package.py:                        define_kok_enable("CUDA_CONSTEXPR", "cuda_constexpr"),
var/spack/repos/builtin/packages/trilinos/package.py:                        define_kok_enable("CUDA_RELOCATABLE_DEVICE_CODE", "cuda_rdc"),
var/spack/repos/builtin/packages/trilinos/package.py:                arch_map = Kokkos.spack_cuda_arch_map
var/spack/repos/builtin/packages/trilinos/package.py:                    for arch in spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/trilinos/package.py:            if "+rocm" in spec:
var/spack/repos/builtin/packages/trilinos/package.py:                        define_kok_enable("ROCM", False),
var/spack/repos/builtin/packages/trilinos/package.py:                        define_kok_enable("HIP_RELOCATABLE_DEVICE_CODE", "rocm_rdc"),
var/spack/repos/builtin/packages/trilinos/package.py:                amdgpu_arch_map = Kokkos.amdgpu_arch_map
var/spack/repos/builtin/packages/trilinos/package.py:                for amd_target in spec.variants["amdgpu_target"].value:
var/spack/repos/builtin/packages/trilinos/package.py:                        arch = amdgpu_arch_map[amd_target]
var/spack/repos/builtin/packages/trilinos/package.py:        if "+cuda" in self.spec:
var/spack/repos/builtin/packages/trilinos/package.py:            # it relies on blocking CUDA kernel launch.
var/spack/repos/builtin/packages/trilinos/package.py:            env.set("CUDA_LAUNCH_BLOCKING", "1")
var/spack/repos/builtin/packages/trilinos/0001-use-the-gcnArchName-inplace-of-gcnArch-as-gcnArch-is.patch: deprecated from rocm-6.0.0
var/spack/repos/builtin/packages/enzo/package.py:        values=("warn", "debug", "cudadebug", "high", "aggressive"),
var/spack/repos/builtin/packages/intel-oneapi-compilers/package.py:        "nvidia-plugin": {
var/spack/repos/builtin/packages/intel-oneapi-compilers/package.py:            "url": "https://developer.codeplay.com/api/v1/products/download?product=oneapi&variant=nvidia&version=2024.2.1&filters[]=12.0&filters[]=linux",
var/spack/repos/builtin/packages/intel-oneapi-compilers/package.py:        "nvidia-plugin": {
var/spack/repos/builtin/packages/intel-oneapi-compilers/package.py:            "url": "https://developer.codeplay.com/api/v1/products/download?product=oneapi&variant=nvidia&version=2024.2.0&filters[]=12.0&filters[]=linux",
var/spack/repos/builtin/packages/intel-oneapi-compilers/package.py:    # Add the nvidia variant
var/spack/repos/builtin/packages/intel-oneapi-compilers/package.py:    variant("nvidia", default=False, description="Install NVIDIA plugin for OneAPI")
var/spack/repos/builtin/packages/intel-oneapi-compilers/package.py:    conflicts("@:2022.2.1", when="+nvidia", msg="Codeplay NVIDIA plugin requires newer release")
var/spack/repos/builtin/packages/intel-oneapi-compilers/package.py:        if "nvidia-plugin" in v:
var/spack/repos/builtin/packages/intel-oneapi-compilers/package.py:                name="nvidia-plugin-installer",
var/spack/repos/builtin/packages/intel-oneapi-compilers/package.py:                placement="nvidia-plugin-installer",
var/spack/repos/builtin/packages/intel-oneapi-compilers/package.py:                **v["nvidia-plugin"],
var/spack/repos/builtin/packages/intel-oneapi-compilers/package.py:        # install nvidia-plugin
var/spack/repos/builtin/packages/intel-oneapi-compilers/package.py:        if self.spec.satisfies("+nvidia"):
var/spack/repos/builtin/packages/intel-oneapi-compilers/package.py:            nvidia_script = find("nvidia-plugin-installer", "*")
var/spack/repos/builtin/packages/intel-oneapi-compilers/package.py:            if nvidia_script:
var/spack/repos/builtin/packages/intel-oneapi-compilers/package.py:                    # For NVIDIA plugin installer
var/spack/repos/builtin/packages/intel-oneapi-compilers/package.py:                    bash(nvidia_script[0], "-y", "--install-dir", self.prefix)
var/spack/repos/builtin/packages/py-torch/package.py:class PyTorch(PythonPackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/py-torch/package.py:    """Tensors and Dynamic neural networks in Python with strong GPU acceleration."""
var/spack/repos/builtin/packages/py-torch/package.py:    variant("cuda", default=not is_darwin, description="Use CUDA")
var/spack/repos/builtin/packages/py-torch/package.py:    variant("rocm", default=False, description="Use ROCm")
var/spack/repos/builtin/packages/py-torch/package.py:    variant("cudnn", default=not is_darwin, description="Use cuDNN", when="+cuda")
var/spack/repos/builtin/packages/py-torch/package.py:    variant("magma", default=not is_darwin, description="Use MAGMA", when="+cuda")
var/spack/repos/builtin/packages/py-torch/package.py:    variant("nccl", default=True, description="Use NCCL", when="+cuda platform=linux")
var/spack/repos/builtin/packages/py-torch/package.py:    variant("nccl", default=True, description="Use NCCL", when="+rocm platform=linux")
var/spack/repos/builtin/packages/py-torch/package.py:    conflicts("+cuda+rocm")
var/spack/repos/builtin/packages/py-torch/package.py:    conflicts("+tensorpipe", when="+rocm ^hip@:5.1", msg="TensorPipe not supported until ROCm 5.2")
var/spack/repos/builtin/packages/py-torch/package.py:        "cuda_arch=none",
var/spack/repos/builtin/packages/py-torch/package.py:        when="+cuda",
var/spack/repos/builtin/packages/py-torch/package.py:        msg="Must specify CUDA compute capabilities of your GPU, see "
var/spack/repos/builtin/packages/py-torch/package.py:        "https://developer.nvidia.com/cuda-gpus",
var/spack/repos/builtin/packages/py-torch/package.py:    depends_on("gloo+cuda", when="@1.6:+gloo+cuda")
var/spack/repos/builtin/packages/py-torch/package.py:    depends_on("nccl", when="+nccl+cuda")
var/spack/repos/builtin/packages/py-torch/package.py:        # cmake/public/cuda.cmake
var/spack/repos/builtin/packages/py-torch/package.py:        depends_on("cuda@11:", when="@2.4:+cuda")
var/spack/repos/builtin/packages/py-torch/package.py:        depends_on("cuda@11:12.3", when="@2.0:2.3+cuda")
var/spack/repos/builtin/packages/py-torch/package.py:        depends_on("cuda@10.2:12.3", when="@1.11:1+cuda")
var/spack/repos/builtin/packages/py-torch/package.py:        # https://discuss.pytorch.org/t/compiling-1-10-1-from-source-with-gcc-11-and-cuda-11-5/140971
var/spack/repos/builtin/packages/py-torch/package.py:        depends_on("cuda@10.2:11.4", when="@1.10+cuda")
var/spack/repos/builtin/packages/py-torch/package.py:        depends_on("cuda@9.2:11.4", when="@1.6:1.9+cuda")
var/spack/repos/builtin/packages/py-torch/package.py:        depends_on("cuda@9:11.4", when="@:1.5+cuda")
var/spack/repos/builtin/packages/py-torch/package.py:    depends_on("magma+cuda", when="+magma+cuda")
var/spack/repos/builtin/packages/py-torch/package.py:    depends_on("magma+rocm", when="+magma+rocm")
var/spack/repos/builtin/packages/py-torch/package.py:    with when("+rocm"):
var/spack/repos/builtin/packages/py-torch/package.py:        depends_on("rccl", when="+nccl")
var/spack/repos/builtin/packages/py-torch/package.py:        depends_on("rocminfo")
var/spack/repos/builtin/packages/py-torch/package.py:    # Fixes build error when ROCm is enabled for pytorch-1.5 release
var/spack/repos/builtin/packages/py-torch/package.py:    patch("rocm.patch", when="@1.5+rocm")
var/spack/repos/builtin/packages/py-torch/package.py:    # Cherry-pick a patch to allow earlier versions of PyTorch to work with CUDA 11.4
var/spack/repos/builtin/packages/py-torch/package.py:        when="@:1.9.1 ^cuda@11.4.100:",
var/spack/repos/builtin/packages/py-torch/package.py:    def torch_cuda_arch_list(self, env):
var/spack/repos/builtin/packages/py-torch/package.py:        if "+cuda" in self.spec:
var/spack/repos/builtin/packages/py-torch/package.py:            torch_cuda_arch = CudaPackage.compute_capabilities(
var/spack/repos/builtin/packages/py-torch/package.py:                self.spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/py-torch/package.py:            env.set("TORCH_CUDA_ARCH_LIST", ";".join(torch_cuda_arch))
var/spack/repos/builtin/packages/py-torch/package.py:        enable_or_disable("cuda")
var/spack/repos/builtin/packages/py-torch/package.py:        if "+cuda" in self.spec:
var/spack/repos/builtin/packages/py-torch/package.py:            env.set("CUDA_TOOLKIT_ROOT_DIR", self.spec["cuda"].prefix)  # Linux/macOS
var/spack/repos/builtin/packages/py-torch/package.py:            env.set("CUDA_HOME", self.spec["cuda"].prefix)  # Linux/macOS
var/spack/repos/builtin/packages/py-torch/package.py:            env.set("CUDA_PATH", self.spec["cuda"].prefix)  # Windows
var/spack/repos/builtin/packages/py-torch/package.py:            self.torch_cuda_arch_list(env)
var/spack/repos/builtin/packages/py-torch/package.py:                        env.set("CMAKE_CUDA_FLAGS", "=-Xcompiler={0}".format(flag))
var/spack/repos/builtin/packages/py-torch/package.py:        enable_or_disable("rocm")
var/spack/repos/builtin/packages/py-torch/package.py:        if "+rocm" in self.spec:
var/spack/repos/builtin/packages/py-torch/package.py:            env.set("PYTORCH_ROCM_ARCH", ";".join(self.spec.variants["amdgpu_target"].value))
var/spack/repos/builtin/packages/py-torch/package.py:            if "+nccl" in self.spec:
var/spack/repos/builtin/packages/py-torch/package.py:            # cmake/Modules_CUDA_fix/FindCUDNN.cmake
var/spack/repos/builtin/packages/py-torch/package.py:        enable_or_disable("nccl")
var/spack/repos/builtin/packages/py-torch/package.py:        if "+cuda+nccl" in self.spec:
var/spack/repos/builtin/packages/py-torch/package.py:            env.set("NCCL_LIB_DIR", self.spec["nccl"].libs.directories[0])
var/spack/repos/builtin/packages/py-torch/package.py:            env.set("NCCL_INCLUDE_DIR", self.spec["nccl"].prefix.include)
var/spack/repos/builtin/packages/py-torch/package.py:        env.set("USE_SYSTEM_NCCL", "ON")
var/spack/repos/builtin/packages/py-torch/package.py:        self.torch_cuda_arch_list(env)
var/spack/repos/builtin/packages/py-torch/package.py:        if "+rocm" in self.spec:
var/spack/repos/builtin/packages/py-torch/xnnpack.patch:@@ -710,7 +717,7 @@ ELSEIF(USE_CUDA)
var/spack/repos/builtin/packages/py-torch/rocm.patch:diff --git a/aten/src/ATen/cuda/nvrtc_stub/ATenNVRTC.h b/aten/src/ATen/cuda/nvrtc_stub/ATenNVRTC.h
var/spack/repos/builtin/packages/py-torch/rocm.patch:--- a/aten/src/ATen/cuda/nvrtc_stub/ATenNVRTC.h
var/spack/repos/builtin/packages/py-torch/rocm.patch:+++ b/aten/src/ATen/cuda/nvrtc_stub/ATenNVRTC.h
var/spack/repos/builtin/packages/py-torch/rocm.patch:@@ -67,6 +67,14 @@ namespace at { namespace cuda {
var/spack/repos/builtin/packages/py-torch/rocm.patch:+// HIP from ROCm 3.5 on renamed hipOccupancyMaxActiveBlocksPerMultiprocessor
var/spack/repos/builtin/packages/py-torch/rocm.patch:@@ -76,7 +84,7 @@ namespace at { namespace cuda {
var/spack/repos/builtin/packages/py-torch/rocm.patch:diff --git a/aten/src/ATen/native/cuda/SoftMax.cu b/aten/src/ATen/native/cuda/SoftMax.cu
var/spack/repos/builtin/packages/py-torch/rocm.patch:--- a/aten/src/ATen/native/cuda/SoftMax.cu
var/spack/repos/builtin/packages/py-torch/rocm.patch:+++ b/aten/src/ATen/native/cuda/SoftMax.cu
var/spack/repos/builtin/packages/py-torch/rocm.patch:   cudaOccupancyMaxActiveBlocksPerMultiprocessor(&max_blocks,
var/spack/repos/builtin/packages/py-torch/rocm.patch:diff --git a/torch/csrc/jit/codegen/fuser/cuda/fused_kernel.cpp b/torch/csrc/jit/codegen/fuser/cuda/fused_kernel.cpp
var/spack/repos/builtin/packages/py-torch/rocm.patch:--- a/torch/csrc/jit/codegen/fuser/cuda/fused_kernel.cpp
var/spack/repos/builtin/packages/py-torch/rocm.patch:+++ b/torch/csrc/jit/codegen/fuser/cuda/fused_kernel.cpp
var/spack/repos/builtin/packages/py-torch/rocm.patch:@@ -140,10 +140,10 @@ FusedKernelCUDA::FusedKernelCUDA(
var/spack/repos/builtin/packages/py-torch/rocm.patch:-  AT_CUDA_DRIVER_CHECK(nvrtc().cuOccupancyMaxActiveBlocksPerMultiprocessor(
var/spack/repos/builtin/packages/py-torch/rocm.patch:+  AT_CUDA_DRIVER_CHECK(nvrtc().hipOccupancyMaxActiveBlocksPerMultiprocessor(
var/spack/repos/builtin/packages/py-torch/rocm.patch:diff --git a/torch/utils/hipify/cuda_to_hip_mappings.py b/torch/utils/hipify/cuda_to_hip_mappings.py
var/spack/repos/builtin/packages/py-torch/rocm.patch:--- a/torch/utils/hipify/cuda_to_hip_mappings.py
var/spack/repos/builtin/packages/py-torch/rocm.patch:+++ b/torch/utils/hipify/cuda_to_hip_mappings.py
var/spack/repos/builtin/packages/ecp-data-vis-sdk/package.py:class EcpDataVisSdk(BundlePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/ecp-data-vis-sdk/package.py:    cuda_arch_variants = ["cuda_arch={0}".format(x) for x in CudaPackage.cuda_arch_values]
var/spack/repos/builtin/packages/ecp-data-vis-sdk/package.py:    amdgpu_target_variants = ["amdgpu_target={0}".format(x) for x in ROCmPackage.amdgpu_targets]
var/spack/repos/builtin/packages/ecp-data-vis-sdk/package.py:        propagate=["cuda", "hdf5", "sz", "zfp", "fortran"] + cuda_arch_variants,
var/spack/repos/builtin/packages/ecp-data-vis-sdk/package.py:    # hdf5-vfd-gds needs cuda@11.7.1 or later, only enable when 11.7.1+ available.
var/spack/repos/builtin/packages/ecp-data-vis-sdk/package.py:    depends_on("hdf5-vfd-gds@1.0.2:", when="+cuda+hdf5 ^cuda@11.7.1: ^hdf5@1.14:")
var/spack/repos/builtin/packages/ecp-data-vis-sdk/package.py:    for cuda_arch in cuda_arch_variants:
var/spack/repos/builtin/packages/ecp-data-vis-sdk/package.py:            "hdf5-vfd-gds@1.0.2: {0}".format(cuda_arch),
var/spack/repos/builtin/packages/ecp-data-vis-sdk/package.py:            when="+cuda+hdf5 {0} ^cuda@11.7.1: ^hdf5@1.14:".format(cuda_arch),
var/spack/repos/builtin/packages/ecp-data-vis-sdk/package.py:    conflicts("~cuda", when="^hdf5-vfd-gds@1.0.2:")
var/spack/repos/builtin/packages/ecp-data-vis-sdk/package.py:        propagate=["adios2", "cuda"] + cuda_arch_variants,
var/spack/repos/builtin/packages/ecp-data-vis-sdk/package.py:    depends_on("ascent+openmp", when="~rocm+ascent")
var/spack/repos/builtin/packages/ecp-data-vis-sdk/package.py:    depends_on("ascent~openmp", when="+rocm+ascent")
var/spack/repos/builtin/packages/ecp-data-vis-sdk/package.py:    # ParaView needs @5.11: in order to use CUDA/ROCM, therefore it is the minimum
var/spack/repos/builtin/packages/ecp-data-vis-sdk/package.py:    # required version since GPU capability is desired for ECP
var/spack/repos/builtin/packages/ecp-data-vis-sdk/package.py:        propagate=["adios2", "cuda", "hdf5", "rocm"] + amdgpu_target_variants + cuda_arch_variants,
var/spack/repos/builtin/packages/ecp-data-vis-sdk/package.py:        propagate=["cuda", "rocm"] + cuda_arch_variants + amdgpu_target_variants,
var/spack/repos/builtin/packages/ecp-data-vis-sdk/package.py:    depends_on("vtk-m+openmp", when="~rocm+vtkm")
var/spack/repos/builtin/packages/ecp-data-vis-sdk/package.py:    depends_on("vtk-m~openmp", when="+rocm+vtkm")
var/spack/repos/builtin/packages/ecp-data-vis-sdk/package.py:    dav_sdk_depends_on("zfp", when="+zfp", propagate=["cuda"] + cuda_arch_variants)
var/spack/repos/builtin/packages/julia/package.py:        " targets=amdgpu,bpf,nvptx,webassembly"
var/spack/repos/builtin/packages/julia/package.py:    # suite-sparse@7.2.1 sometimes builds cuda stub libraries and Julia build
var/spack/repos/builtin/packages/julia/package.py:    patch("julia-1.10-rm-suite-sparse-cuda-stubs.patch", when="@1.10.0:1.10")
var/spack/repos/builtin/packages/julia/julia-1.10-rm-suite-sparse-cuda-stubs.patch:-$(eval $(call symlink_system_library,LIBSUITESPARSE,libcholmod_cuda))
var/spack/repos/builtin/packages/julia/julia-1.10-rm-suite-sparse-cuda-stubs.patch:-$(eval $(call symlink_system_library,LIBSUITESPARSE,libspqr_cuda))
var/spack/repos/builtin/packages/libx11/package.py:        # -Werror flags are not properly interpreted by the NVIDIA compiler
var/spack/repos/builtin/packages/elbencho/package.py:    variant("cuda", default=True, description="Enable CUDA support", when="+cufile")
var/spack/repos/builtin/packages/elbencho/package.py:    variant("cuda", default=False, description="Enable CUDA support")
var/spack/repos/builtin/packages/elbencho/package.py:    variant("cufile", default=False, description="GPU Direct Storage")
var/spack/repos/builtin/packages/elbencho/package.py:    depends_on("cuda", when="+cuda")
var/spack/repos/builtin/packages/elbencho/package.py:    conflicts("+cufile", when="~cuda")
var/spack/repos/builtin/packages/elbencho/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/elbencho/package.py:            os.environ["CUDA_SUPPORT"] = "1"
var/spack/repos/builtin/packages/care/package.py:class Care(CachedCMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/care/package.py:    depends_on("cmake@3.21:", type="build", when="@0.12.0:+rocm")
var/spack/repos/builtin/packages/care/package.py:    depends_on("cmake@3.9:", type="build", when="+cuda")
var/spack/repos/builtin/packages/care/package.py:    conflicts("^blt@:0.3.6", when="+rocm")
var/spack/repos/builtin/packages/care/package.py:    conflicts("+openmp", when="+rocm")
var/spack/repos/builtin/packages/care/package.py:    conflicts("+openmp", when="+cuda")
var/spack/repos/builtin/packages/care/package.py:    with when("+cuda"):
var/spack/repos/builtin/packages/care/package.py:        depends_on("umpire+cuda")
var/spack/repos/builtin/packages/care/package.py:        depends_on("raja+cuda")
var/spack/repos/builtin/packages/care/package.py:        depends_on("chai+cuda")
var/spack/repos/builtin/packages/care/package.py:        for sm_ in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/care/package.py:            depends_on("umpire+cuda cuda_arch={0}".format(sm_), when="cuda_arch={0}".format(sm_))
var/spack/repos/builtin/packages/care/package.py:            depends_on("raja+cuda cuda_arch={0}".format(sm_), when="cuda_arch={0}".format(sm_))
var/spack/repos/builtin/packages/care/package.py:            depends_on("chai+cuda cuda_arch={0}".format(sm_), when="cuda_arch={0}".format(sm_))
var/spack/repos/builtin/packages/care/package.py:    with when("+rocm"):
var/spack/repos/builtin/packages/care/package.py:        depends_on("umpire+rocm")
var/spack/repos/builtin/packages/care/package.py:        depends_on("raja+rocm")
var/spack/repos/builtin/packages/care/package.py:        depends_on("chai+rocm")
var/spack/repos/builtin/packages/care/package.py:        for arch_ in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/care/package.py:                "umpire+rocm amdgpu_target={0}".format(arch_),
var/spack/repos/builtin/packages/care/package.py:                when="amdgpu_target={0}".format(arch_),
var/spack/repos/builtin/packages/care/package.py:                "raja+rocm amdgpu_target={0}".format(arch_), when="amdgpu_target={0}".format(arch_)
var/spack/repos/builtin/packages/care/package.py:                "chai+rocm amdgpu_target={0}".format(arch_), when="amdgpu_target={0}".format(arch_)
var/spack/repos/builtin/packages/care/package.py:        if spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/care/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/care/package.py:            entries.append(cmake_cache_option("ENABLE_CUDA", True))
var/spack/repos/builtin/packages/care/package.py:            entries.append(cmake_cache_option("CUDA_SEPARABLE_COMPILATION", True))
var/spack/repos/builtin/packages/care/package.py:            entries.append(cmake_cache_string("NVTOOLSEXT_DIR", spec["cuda"].prefix))
var/spack/repos/builtin/packages/care/package.py:            entries.append(cmake_cache_option("ENABLE_CUDA", False))
var/spack/repos/builtin/packages/care/package.py:        if spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/care/package.py:            archs = self.spec.variants["amdgpu_target"].value
var/spack/repos/builtin/packages/care/package.py:                    cmake_cache_string("HIP_HIPCC_FLAGS", "--amdgpu-target={0}".format(arch_str))
var/spack/repos/builtin/packages/cusz/package.py:class Cusz(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/cusz/package.py:    """A GPU accelerated error-bounded lossy compression for scientific data"""
var/spack/repos/builtin/packages/cusz/package.py:    conflicts("~cuda")
var/spack/repos/builtin/packages/cusz/package.py:    conflicts("cuda_arch=none", when="+cuda")
var/spack/repos/builtin/packages/cusz/package.py:    # these version of Cuda provide the CUB headers, but not CUB cmake configuration that we use.
var/spack/repos/builtin/packages/cusz/package.py:    conflicts("^cuda@11.0.2:11.2.2")
var/spack/repos/builtin/packages/cusz/package.py:    depends_on("cub", when="^cuda@:10.2.89")
var/spack/repos/builtin/packages/cusz/package.py:        cuda_arch = self.spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/cusz/package.py:        args = ["-DBUILD_TESTING=OFF", ("-DCMAKE_CUDA_ARCHITECTURES=%s" % cuda_arch)]
var/spack/repos/builtin/packages/nlcglib/package.py:class Nlcglib(CMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/nlcglib/package.py:    with when("@1.1: +cuda"):
var/spack/repos/builtin/packages/nlcglib/package.py:            "gpu_direct",
var/spack/repos/builtin/packages/nlcglib/package.py:            description="Enable GPU direct. Required to support distributed wave-functions.",
var/spack/repos/builtin/packages/nlcglib/package.py:    depends_on("kokkos~cuda~rocm", when="~cuda~rocm")
var/spack/repos/builtin/packages/nlcglib/package.py:        conflicts("+rocm")
var/spack/repos/builtin/packages/nlcglib/package.py:    with when("+rocm"):
var/spack/repos/builtin/packages/nlcglib/package.py:        variant("magma", default=True, description="Use magma eigenvalue solver (AMDGPU)")
var/spack/repos/builtin/packages/nlcglib/package.py:        depends_on("magma+rocm", when="+magma")
var/spack/repos/builtin/packages/nlcglib/package.py:        depends_on("kokkos+rocm")
var/spack/repos/builtin/packages/nlcglib/package.py:    with when("+cuda"):
var/spack/repos/builtin/packages/nlcglib/package.py:        depends_on("kokkos+cuda_lambda+wrapper", when="%gcc")
var/spack/repos/builtin/packages/nlcglib/package.py:        depends_on("kokkos+cuda")
var/spack/repos/builtin/packages/nlcglib/package.py:        for arch in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/nlcglib/package.py:            depends_on(f"kokkos cuda_arch={arch}", when=f"cuda_arch={arch}")
var/spack/repos/builtin/packages/nlcglib/package.py:            self.define_from_variant("USE_ROCM", "rocm"),
var/spack/repos/builtin/packages/nlcglib/package.py:            self.define_from_variant("USE_GPU_DIRECT", "gpu_direct"),
var/spack/repos/builtin/packages/nlcglib/package.py:            self.define_from_variant("USE_CUDA", "cuda"),
var/spack/repos/builtin/packages/nlcglib/package.py:        if "+cuda%gcc" in self.spec:
var/spack/repos/builtin/packages/nlcglib/package.py:        if "+cuda" in self.spec:
var/spack/repos/builtin/packages/nlcglib/package.py:            cuda_archs = self.spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/nlcglib/package.py:                cuda_flags = " ".join(
var/spack/repos/builtin/packages/nlcglib/package.py:                    ["-gencode arch=compute_{0},code=sm_{0}".format(x) for x in cuda_archs]
var/spack/repos/builtin/packages/nlcglib/package.py:                options += [self.define("CMAKE_CUDA_FLAGS", cuda_flags)]
var/spack/repos/builtin/packages/nlcglib/package.py:                options += [self.define("CMAKE_CUDA_ARCHITECTURES", cuda_archs)]
var/spack/repos/builtin/packages/nlcglib/package.py:        if "^cuda+allow-unsupported-compilers" in self.spec:
var/spack/repos/builtin/packages/nlcglib/package.py:            options += [self.define("CMAKE_CUDA_FLAGS", "--allow-unsupported-compiler")]
var/spack/repos/builtin/packages/nlcglib/package.py:        if "+rocm" in self.spec:
var/spack/repos/builtin/packages/nlcglib/package.py:            archs = ",".join(self.spec.variants["amdgpu_target"].value)
var/spack/repos/builtin/packages/nlcglib/package.py:            options.append("-DHIP_HCC_FLAGS=--amdgpu-target={0}".format(archs))
var/spack/repos/builtin/packages/nlcglib/package.py:                "-DCMAKE_CXX_FLAGS=--amdgpu-target={0} --offload-arch={0}".format(archs)
var/spack/repos/builtin/packages/ethminer/package.py:    """The ethminer is an Ethereum GPU mining worker."""
var/spack/repos/builtin/packages/ethminer/package.py:    variant("opencl", default=True, description="Enable OpenCL mining.")
var/spack/repos/builtin/packages/ethminer/package.py:    variant("cuda", default=False, description="Enable CUDA mining.")
var/spack/repos/builtin/packages/ethminer/package.py:    depends_on("cuda", when="+cuda")
var/spack/repos/builtin/packages/ethminer/package.py:    depends_on("mesa", when="+opencl")
var/spack/repos/builtin/packages/ethminer/package.py:            self.define_from_variant("ETHASHCL", "opencl"),
var/spack/repos/builtin/packages/ethminer/package.py:            self.define_from_variant("ETHASHCUDA", "cuda"),
var/spack/repos/builtin/packages/nekrs/package.py:class Nekrs(Package, CMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/nekrs/package.py:    like GPUs"""
var/spack/repos/builtin/packages/nekrs/package.py:    variant("opencl", default=False, description="Activates support for OpenCL")
var/spack/repos/builtin/packages/nekrs/package.py:        if "+cuda" in spec:
var/spack/repos/builtin/packages/nekrs/package.py:            cuda_dir = spec["cuda"].prefix
var/spack/repos/builtin/packages/nekrs/package.py:            # Run-time CUDA compiler:
var/spack/repos/builtin/packages/nekrs/package.py:            env.set("OCCA_CUDA_COMPILER", join_path(cuda_dir, "bin", "nvcc"))
var/spack/repos/builtin/packages/nekrs/package.py:        if "+cuda" in spec:
var/spack/repos/builtin/packages/nekrs/package.py:            cuda_dir = spec["cuda"].prefix
var/spack/repos/builtin/packages/nekrs/package.py:            # Run-time CUDA compiler:
var/spack/repos/builtin/packages/nekrs/package.py:            s_env.set("OCCA_CUDA_COMPILER", join_path(cuda_dir, "bin", "nvcc"))
var/spack/repos/builtin/packages/nekrs/package.py:        # For the cuda, openmp, and opencl variants, set the environment
var/spack/repos/builtin/packages/nekrs/package.py:        # variable OCCA_{CUDA,OPENMP,OPENCL}_ENABLED only if the variant is
var/spack/repos/builtin/packages/nekrs/package.py:        if "+cuda" in spec:
var/spack/repos/builtin/packages/nekrs/package.py:            cuda_dir = spec["cuda"].prefix
var/spack/repos/builtin/packages/nekrs/package.py:            cuda_libs_list = ["libcuda", "libcudart", "libOpenCL"]
var/spack/repos/builtin/packages/nekrs/package.py:            cuda_libs = find_libraries(cuda_libs_list, cuda_dir, shared=True, recursive=True)
var/spack/repos/builtin/packages/nekrs/package.py:            env.set("OCCA_INCLUDE_PATH", cuda_dir.include)
var/spack/repos/builtin/packages/nekrs/package.py:            env.set("OCCA_LIBRARY_PATH", ":".join(cuda_libs.directories))
var/spack/repos/builtin/packages/nekrs/package.py:            env.set("OCCA_CUDA_ENABLED", "1")
var/spack/repos/builtin/packages/nekrs/package.py:            env.set("OCCA_CUDA_ENABLED", "0")
var/spack/repos/builtin/packages/nekrs/package.py:        env.set("OCCA_OPENCL_ENABLED", "1" if "+opencl" in spec else "0")
var/spack/repos/builtin/packages/nekrs/package.py:        env.set("OCCA_HIP_ENABLED", "1" if "+rocm" in spec else "0")
var/spack/repos/builtin/packages/nekrs/package.py:            self.define_from_variant("ENABLE_CUDA", "cuda"),
var/spack/repos/builtin/packages/nekrs/package.py:            self.define_from_variant("ENABLE_OPENCL", "opencl"),
var/spack/repos/builtin/packages/nekrs/package.py:            self.define_from_variant("ENABLE_HIP", "rocm"),
var/spack/repos/builtin/packages/nimrod-aai/package.py:    solves with device accelerated computing through OpenACC and abstract types
var/spack/repos/builtin/packages/nimrod-aai/package.py:    variant("openacc", default=False, description="Whether to enable OpenACC")
var/spack/repos/builtin/packages/nimrod-aai/package.py:        "openacc_autocompare", default=False, description="Whether to enable OpenACC autocompare"
var/spack/repos/builtin/packages/nimrod-aai/package.py:    variant("openacc_cc", default="native", description="OpenACC compute capability")
var/spack/repos/builtin/packages/nimrod-aai/package.py:            self.define_from_variant("ENABLE_OPENACC", "openacc"),
var/spack/repos/builtin/packages/nimrod-aai/package.py:        if "+openacc" in self.spec:
var/spack/repos/builtin/packages/nimrod-aai/package.py:                self.define_from_variant("ENABLE_OPENACC_AUTOCOMPARE", "openacc_autocompare"),
var/spack/repos/builtin/packages/nimrod-aai/package.py:                self.define_from_variant("OPENACC_CC", "openacc_cc"),
var/spack/repos/builtin/packages/chai/package.py:class Chai(CachedCMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/chai/package.py:        description="Build with CUDA_SEPARABLE_COMPILATION flag on ",
var/spack/repos/builtin/packages/chai/package.py:    depends_on("cmake@3.9:", type="build", when="+cuda")
var/spack/repos/builtin/packages/chai/package.py:    conflicts("^blt@:0.3.6", when="+rocm")
var/spack/repos/builtin/packages/chai/package.py:    with when("+cuda"):
var/spack/repos/builtin/packages/chai/package.py:        depends_on("umpire+cuda")
var/spack/repos/builtin/packages/chai/package.py:        for sm_ in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/chai/package.py:            depends_on("umpire+cuda cuda_arch={0}".format(sm_), when="cuda_arch={0}".format(sm_))
var/spack/repos/builtin/packages/chai/package.py:    with when("+rocm"):
var/spack/repos/builtin/packages/chai/package.py:        depends_on("umpire+rocm")
var/spack/repos/builtin/packages/chai/package.py:        for arch in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/chai/package.py:                "umpire+rocm amdgpu_target={0}".format(arch), when="amdgpu_target={0}".format(arch)
var/spack/repos/builtin/packages/chai/package.py:        with when("+cuda"):
var/spack/repos/builtin/packages/chai/package.py:            depends_on("raja+cuda")
var/spack/repos/builtin/packages/chai/package.py:            for sm_ in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/chai/package.py:                depends_on("raja+cuda cuda_arch={0}".format(sm_), when="cuda_arch={0}".format(sm_))
var/spack/repos/builtin/packages/chai/package.py:        with when("+rocm"):
var/spack/repos/builtin/packages/chai/package.py:            depends_on("raja+rocm")
var/spack/repos/builtin/packages/chai/package.py:            for arch in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/chai/package.py:                    "raja+rocm amdgpu_target={0}".format(arch),
var/spack/repos/builtin/packages/chai/package.py:                    when="amdgpu_target={0}".format(arch),
var/spack/repos/builtin/packages/chai/package.py:        if spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/chai/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/chai/package.py:            entries.append(cmake_cache_option("ENABLE_CUDA", True))
var/spack/repos/builtin/packages/chai/package.py:                entries.append(cmake_cache_option("CMAKE_CUDA_SEPARABLE_COMPILATION", True))
var/spack/repos/builtin/packages/chai/package.py:                entries.append(cmake_cache_option("CUDA_SEPARABLE_COMPILATION", True))
var/spack/repos/builtin/packages/chai/package.py:            entries.append(cmake_cache_option("ENABLE_CUDA", False))
var/spack/repos/builtin/packages/chai/package.py:        if spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/py-gputil/package.py:class PyGputil(PythonPackage):
var/spack/repos/builtin/packages/py-gputil/package.py:    """GPUtil is a Python module for getting the GPU status from NVIDA GPUs
var/spack/repos/builtin/packages/py-gputil/package.py:    using nvidia-smi."""
var/spack/repos/builtin/packages/py-gputil/package.py:    homepage = "https://github.com/anderskm/gputil"
var/spack/repos/builtin/packages/py-gputil/package.py:    pypi = "GPUtil/GPUtil-1.4.0.tar.gz"
var/spack/repos/builtin/packages/ffte/package.py:    hardware, MPI, and CUDA Fortran is also included."""
var/spack/repos/builtin/packages/ffte/package.py:    variant("cuda", default=False, description="Use CUDA Fortran")
var/spack/repos/builtin/packages/ffte/package.py:    requires("%nvhpc", when="+cuda", msg="ffte+cuda must use NVHPC compiler")
var/spack/repos/builtin/packages/ffte/package.py:            for spc, vrf, rs, ip in (("+vector", vf, "v", 2), ("+cuda", cuf, "cu", 3)):
var/spack/repos/builtin/packages/ffte/package.py:        # enable CUDA Fortran in NVHPC
var/spack/repos/builtin/packages/ffte/package.py:            env["FFLAGS"] = "-Mcuda"
var/spack/repos/builtin/packages/popt/package.py:        # Remove flags not recognized by the NVIDIA compilers
var/spack/repos/builtin/packages/meme/package.py:        # Remove flags not recognized by the NVIDIA compiler
var/spack/repos/builtin/packages/mlperf-deepcam/package.py:class MlperfDeepcam(Package, CudaPackage):
var/spack/repos/builtin/packages/mlperf-deepcam/package.py:    depends_on("py-pycuda", type=("build", "run"))
var/spack/repos/builtin/packages/mlperf-deepcam/package.py:    depends_on("py-torch+cuda", when="+cuda", type=("build", "run"))
var/spack/repos/builtin/packages/mlperf-deepcam/package.py:    depends_on("py-torch~cuda~nccl", when="~cuda", type=("build", "run"))
var/spack/repos/builtin/packages/intel-gtpin/package.py:    includes a binary instrumentation engine for Intel(R) GPUs EUs, along
var/spack/repos/builtin/packages/intel-gtpin/package.py:    data at the finest granularity of the specific GPU EU instruction.
var/spack/repos/builtin/packages/intel-gtpin/package.py:    regular, real-world GPU applications, as well as on pre-captured
var/spack/repos/builtin/packages/intel-gtpin/package.py:    analysis of the code that is executing on the GPU EUs. GTPin opens
var/spack/repos/builtin/packages/intel-gtpin/package.py:    analysis on an Intel(R) GPU, with greater efficiency than other
var/spack/repos/builtin/packages/intel-gtpin/package.py:    develop their own analysis tools. GTPin can analyze any GPU
var/spack/repos/builtin/packages/intel-gtpin/package.py:    application executes on the GPU.
var/spack/repos/builtin/packages/libfabric/package.py:class Libfabric(AutotoolsPackage, CudaPackage):
var/spack/repos/builtin/packages/libfabric/package.py:    # Fix for the inline assembly problem for the Nvidia compilers
var/spack/repos/builtin/packages/libfabric/package.py:        if self.spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/libfabric/package.py:            args.append(f"--with-cuda={self.spec['cuda'].prefix}")
var/spack/repos/builtin/packages/libfabric/nvhpc-symver.patch:Subject: [PATCH] Fix inline assembly for the Nvidia HPC compilers
var/spack/repos/builtin/packages/libfabric/nvhpc-symver.patch:Fixes a problem with the Nvidia compiler, which does not emit
var/spack/repos/builtin/packages/py-chainer/package.py:    It also supports CUDA/cuDNN using CuPy for high performance training
var/spack/repos/builtin/packages/ont-guppy/package.py:#  - the newest non-cuda version should be set as 'preferred_ver'
var/spack/repos/builtin/packages/ont-guppy/package.py:#  - a cuda dependency must be set for each new cuda version
var/spack/repos/builtin/packages/ont-guppy/package.py:    "6.1.7-cuda": {
var/spack/repos/builtin/packages/ont-guppy/package.py:            "https://cdn.oxfordnanoportal.com/software/analysis/ont-guppy_6.1.7_linuxaarch64_cuda10.tar.gz",
var/spack/repos/builtin/packages/ont-guppy/package.py:    depends_on("cuda@11.0.0:", when="@6.1.7-cuda", type="run")
var/spack/repos/builtin/packages/dealii/package.py:class Dealii(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/dealii/package.py:    variant("cgal", default=True, when="@9.4:~cuda", description="Compile with CGAL")
var/spack/repos/builtin/packages/dealii/package.py:    depends_on("cuda@8:", when="+cuda")
var/spack/repos/builtin/packages/dealii/package.py:    depends_on("cmake@3.9:", when="+cuda", type="build")
var/spack/repos/builtin/packages/dealii/package.py:    depends_on("kokkos@3.7:+cuda+cuda_lambda+wrapper", when="@9.5:+kokkos~trilinos+cuda")
var/spack/repos/builtin/packages/dealii/package.py:    for _arch in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/dealii/package.py:        arch_str = f"+cuda cuda_arch={_arch}"
var/spack/repos/builtin/packages/dealii/package.py:    # deal.II's own CUDA backend does not support CUDA version 12.0 or newer.
var/spack/repos/builtin/packages/dealii/package.py:    conflicts("+cuda ^cuda@12:")
var/spack/repos/builtin/packages/dealii/package.py:        # CUDA
var/spack/repos/builtin/packages/dealii/package.py:        options.append(self.define_from_variant("DEAL_II_WITH_CUDA", "cuda"))
var/spack/repos/builtin/packages/dealii/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/dealii/package.py:            if not spec.satisfies("^cuda@9:"):
var/spack/repos/builtin/packages/dealii/package.py:            cuda_arch = spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/dealii/package.py:            if cuda_arch != "none":
var/spack/repos/builtin/packages/dealii/package.py:                if len(cuda_arch) > 1:
var/spack/repos/builtin/packages/dealii/package.py:                    raise InstallError("deal.II only supports compilation for a single GPU!")
var/spack/repos/builtin/packages/dealii/package.py:                flags = "-arch=sm_{0}".format(cuda_arch[0])
var/spack/repos/builtin/packages/dealii/package.py:                # with: flags = ' '.join(self.cuda_flags(cuda_arch))
var/spack/repos/builtin/packages/dealii/package.py:                options.append(self.define("DEAL_II_CUDA_FLAGS", flags))
var/spack/repos/builtin/packages/dealii/package.py:            if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/dealii/package.py:                            "DEAL_II_MPI_WITH_CUDA_SUPPORT", spec["mpi"].satisfies("+cuda")
var/spack/repos/builtin/packages/dealii/package.py:                        self.define("CUDA_HOST_COMPILER", spec["mpi"].mpicxx),
var/spack/repos/builtin/packages/dealii/package.py:        if spec.satisfies("+cuda") and spec.satisfies("+mpi"):
var/spack/repos/builtin/packages/dealii/package.py:            env.set("CUDAHOSTCXX", spec["mpi"].mpicxx)
var/spack/repos/builtin/packages/celeritas/package.py:class Celeritas(CMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/celeritas/package.py:    high-performance (GPU-targeted) simulation of high-energy physics
var/spack/repos/builtin/packages/celeritas/package.py:    # Note: cuda and rocm variants are defined by mixin classes
var/spack/repos/builtin/packages/celeritas/package.py:    depends_on("cmake@3.18:", type="build", when="+cuda+vecgeom")
var/spack/repos/builtin/packages/celeritas/package.py:    depends_on("cmake@3.22:", type="build", when="+rocm")
var/spack/repos/builtin/packages/celeritas/package.py:    depends_on("vecgeom +cuda", when="+vecgeom +cuda")
var/spack/repos/builtin/packages/celeritas/package.py:    conflicts("+rocm", when="+cuda", msg="AMD and NVIDIA accelerators are incompatible")
var/spack/repos/builtin/packages/celeritas/package.py:    conflicts("+rocm", when="+vecgeom", msg="HIP support is only available with ORANGE")
var/spack/repos/builtin/packages/celeritas/package.py:    conflicts("^vecgeom+shared@1.2.0", when="+vecgeom +cuda")
var/spack/repos/builtin/packages/celeritas/package.py:            from_variant("Celeritas_USE_HIP", "rocm"),
var/spack/repos/builtin/packages/celeritas/package.py:        for pkg in ["CUDA", "Geant4", "HepMC3", "OpenMP", "ROOT", "SWIG", "VecGeom"]:
var/spack/repos/builtin/packages/slurm/package.py:    variant("rsmi", default=False, description="Enable ROCm SMI support")
var/spack/repos/builtin/packages/slurm/package.py:    depends_on("cuda", when="+nvml")
var/spack/repos/builtin/packages/slurm/package.py:    depends_on("rocm-smi-lib", when="+rsmi")
var/spack/repos/builtin/packages/slurm/package.py:            args.append(f"--with-nvml={spec['cuda'].prefix}")
var/spack/repos/builtin/packages/slurm/package.py:            args.append(f"--with-rsmi={spec['rocm-smi-lib'].prefix}")
var/spack/repos/builtin/packages/raja/package.py:class Raja(CachedCMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/raja/package.py:    # Backward compatibility is stopped from ROCm 6.0
var/spack/repos/builtin/packages/raja/package.py:    conflicts("^blt@:0.3.6", when="+rocm")
var/spack/repos/builtin/packages/raja/package.py:    depends_on("cmake@3.23:", when="@2022.10.0:2024.02.2+rocm", type="build")
var/spack/repos/builtin/packages/raja/package.py:    depends_on("cmake@3.20:", when="@:2022.03+rocm", type="build")
var/spack/repos/builtin/packages/raja/package.py:    depends_on("rocprim", when="+rocm")
var/spack/repos/builtin/packages/raja/package.py:    with when("+rocm @0.12.0:"):
var/spack/repos/builtin/packages/raja/package.py:        depends_on("camp+rocm")
var/spack/repos/builtin/packages/raja/package.py:        for arch in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/raja/package.py:                "camp+rocm amdgpu_target={0}".format(arch), when="amdgpu_target={0}".format(arch)
var/spack/repos/builtin/packages/raja/package.py:    with when("+cuda @0.12.0:"):
var/spack/repos/builtin/packages/raja/package.py:        depends_on("camp+cuda")
var/spack/repos/builtin/packages/raja/package.py:        for sm_ in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/raja/package.py:            depends_on("camp +cuda cuda_arch={0}".format(sm_), when="cuda_arch={0}".format(sm_))
var/spack/repos/builtin/packages/raja/package.py:    conflicts("+omptarget +rocm")
var/spack/repos/builtin/packages/raja/package.py:    conflicts("+sycl +rocm")
var/spack/repos/builtin/packages/raja/package.py:        if spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/raja/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/raja/package.py:            entries.append(cmake_cache_option("ENABLE_CUDA", True))
var/spack/repos/builtin/packages/raja/package.py:            entries.append(cmake_cache_option("ENABLE_CUDA", False))
var/spack/repos/builtin/packages/raja/package.py:        if spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/raja/package.py:                if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/raja/package.py:                    entries.append(cmake_cache_string("CMAKE_CUDA_STANDARD", "14"))
var/spack/repos/builtin/packages/raja/package.py:                    "BLT_OPENMP_COMPILE_FLAGS", "-fopenmp;-fopenmp-targets=nvptx64-nvidia-cuda"
var/spack/repos/builtin/packages/raja/package.py:                    "BLT_OPENMP_LINK_FLAGS", "-fopenmp;-fopenmp-targets=nvptx64-nvidia-cuda"
var/spack/repos/builtin/packages/raja/package.py:                    "test-algorithm-sort-Cuda.exe",
var/spack/repos/builtin/packages/raja/package.py:                    "test-algorithm-stable-sort-Cuda.exe",
var/spack/repos/builtin/packages/raja/package.py:                if spec.satisfies("+cuda %clang@12.0.0:13.9.999"):
var/spack/repos/builtin/packages/raja/package.py:                if spec.satisfies("+cuda %xl@16.1.1.12"):
var/spack/repos/builtin/packages/raja/package.py:                            "test-algorithm-sort-Cuda.exe;test-algorithm-stable-sort-Cuda.exe",
var/spack/repos/builtin/packages/mfem/mfem-4.3-hypre-2.23.0.patch: #ifdef HYPRE_USING_CUDA
var/spack/repos/builtin/packages/mfem/mfem-4.3-hypre-2.23.0.patch:+#if defined(hypre_IntArrayData) && defined(HYPRE_USING_GPU)
var/spack/repos/builtin/packages/mfem/mfem-4.3-hypre-2.23.0.patch: #ifndef HYPRE_USING_CUDA
var/spack/repos/builtin/packages/mfem/mfem-4.7.patch:    void ClearCuSparse() { ClearGPUSparse(); }
var/spack/repos/builtin/packages/mfem/mfem-4.7.patch: #ifdef MFEM_USE_CUDA
var/spack/repos/builtin/packages/mfem/mfem-4.7.patch: #include <cuda.h>
var/spack/repos/builtin/packages/mfem/mfem-4.7.patch: constexpr const char * device_name = "cuda";
var/spack/repos/builtin/packages/mfem/mfem-4.7.patch: #ifdef MFEM_USE_CUDA
var/spack/repos/builtin/packages/mfem/mfem-4.7.patch:    auto err = cudaHostGetFlags(&flags, h_p);
var/spack/repos/builtin/packages/mfem/mfem-4.7.patch:+   cudaGetLastError(); // also resets last error
var/spack/repos/builtin/packages/mfem/mfem-4.7.patch:    if (err == cudaSuccess) { return true; }
var/spack/repos/builtin/packages/mfem/mfem-4.7.patch:    else if (err == cudaErrorInvalidValue) { return false; }
var/spack/repos/builtin/packages/mfem/package.py:class Mfem(Package, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/mfem/package.py:    # Note: "+cuda" and "cuda_arch" variants are added by the CudaPackage
var/spack/repos/builtin/packages/mfem/package.py:    # Note: "+rocm" and "amdgpu_target" variants are added by the ROCmPackage
var/spack/repos/builtin/packages/mfem/package.py:    variant("amgx", default=False, description="Enable NVIDIA AmgX solver support")
var/spack/repos/builtin/packages/mfem/package.py:    conflicts("+cuda", when="@:3")
var/spack/repos/builtin/packages/mfem/package.py:    conflicts("+rocm", when="@:4.1")
var/spack/repos/builtin/packages/mfem/package.py:    conflicts("+cuda+rocm")
var/spack/repos/builtin/packages/mfem/package.py:    conflicts("+amgx", when="~cuda")
var/spack/repos/builtin/packages/mfem/package.py:    conflicts("+mpi~cuda ^hypre+cuda")
var/spack/repos/builtin/packages/mfem/package.py:    conflicts("+mpi ^hypre+cuda", when="@:4.2")
var/spack/repos/builtin/packages/mfem/package.py:    conflicts("+mpi~rocm ^hypre+rocm")
var/spack/repos/builtin/packages/mfem/package.py:    conflicts("+mpi ^hypre+rocm", when="@:4.3")
var/spack/repos/builtin/packages/mfem/package.py:    depends_on("hipsparse", when="@4.4.0:+rocm")
var/spack/repos/builtin/packages/mfem/package.py:    # If hypre is built with +cuda, propagate cuda_arch
var/spack/repos/builtin/packages/mfem/package.py:    requires("^hypre@2.22.1:", when="+mpi+cuda ^hypre+cuda")
var/spack/repos/builtin/packages/mfem/package.py:    for sm_ in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/mfem/package.py:        requires(f"^hypre cuda_arch={sm_}", when=f"+mpi+cuda cuda_arch={sm_} ^hypre+cuda")
var/spack/repos/builtin/packages/mfem/package.py:    # If hypre is built with +rocm, propagate amdgpu_target
var/spack/repos/builtin/packages/mfem/package.py:    requires("^hypre@2.23.0: ", when="+mpi+rocm ^hypre+rocm")
var/spack/repos/builtin/packages/mfem/package.py:    for gfx in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/mfem/package.py:        requires(f"^hypre amdgpu_target={gfx}", when=f"+mpi+rocm amdgpu_target={gfx} ^hypre+rocm")
var/spack/repos/builtin/packages/mfem/package.py:    for sm_ in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/mfem/package.py:            "sundials@5.4.0:+cuda cuda_arch={0}".format(sm_),
var/spack/repos/builtin/packages/mfem/package.py:            when="@4.2.0:+sundials+cuda cuda_arch={0}".format(sm_),
var/spack/repos/builtin/packages/mfem/package.py:    for gfx in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/mfem/package.py:            "sundials@5.7.0:+rocm amdgpu_target={0}".format(gfx),
var/spack/repos/builtin/packages/mfem/package.py:            when="@4.6.0:+sundials+rocm amdgpu_target={0}".format(gfx),
var/spack/repos/builtin/packages/mfem/package.py:    # If superlu-dist is built with +cuda, propagate cuda_arch
var/spack/repos/builtin/packages/mfem/package.py:    for sm_ in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/mfem/package.py:            f"^superlu-dist cuda_arch={sm_}",
var/spack/repos/builtin/packages/mfem/package.py:            when=f"+superlu-dist+cuda cuda_arch={sm_} ^superlu-dist+cuda",
var/spack/repos/builtin/packages/mfem/package.py:    # If superlu-dist is built with +rocm, propagate amdgpu_target
var/spack/repos/builtin/packages/mfem/package.py:    for gfx in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/mfem/package.py:            f"^superlu-dist+rocm amdgpu_target={gfx}",
var/spack/repos/builtin/packages/mfem/package.py:            when=f"+superlu-dist+rocm amdgpu_target={gfx} ^superlu-dist+rocm",
var/spack/repos/builtin/packages/mfem/package.py:    for sm_ in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/mfem/package.py:            "strumpack+cuda cuda_arch={0}".format(sm_),
var/spack/repos/builtin/packages/mfem/package.py:            when="+strumpack+cuda cuda_arch={0}".format(sm_),
var/spack/repos/builtin/packages/mfem/package.py:    for gfx in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/mfem/package.py:            "strumpack+rocm amdgpu_target={0}".format(gfx),
var/spack/repos/builtin/packages/mfem/package.py:            when="+strumpack+rocm amdgpu_target={0}".format(gfx),
var/spack/repos/builtin/packages/mfem/package.py:    # If petsc is built with +cuda, propagate cuda_arch to petsc and slepc
var/spack/repos/builtin/packages/mfem/package.py:    for sm_ in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/mfem/package.py:        requires(f"^petsc cuda_arch={sm_}", when=f"+cuda+petsc cuda_arch={sm_} ^petsc+cuda")
var/spack/repos/builtin/packages/mfem/package.py:        depends_on(f"slepc+cuda cuda_arch={sm_}", when=f"+cuda+slepc cuda_arch={sm_} ^petsc+cuda")
var/spack/repos/builtin/packages/mfem/package.py:    # If petsc is built with +rocm, propagate amdgpu_target to petsc and slepc
var/spack/repos/builtin/packages/mfem/package.py:    for gfx in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/mfem/package.py:            f"^petsc amdgpu_target={gfx}", when=f"+rocm+petsc amdgpu_target={gfx} ^petsc+rocm"
var/spack/repos/builtin/packages/mfem/package.py:            f"slepc+rocm amdgpu_target={gfx}", when=f"+rocm+slepc amdgpu_target={gfx} ^petsc+rocm"
var/spack/repos/builtin/packages/mfem/package.py:    for sm_ in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/mfem/package.py:            "ginkgo+cuda cuda_arch={0}".format(sm_), when="+ginkgo+cuda cuda_arch={0}".format(sm_)
var/spack/repos/builtin/packages/mfem/package.py:    for gfx in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/mfem/package.py:            "ginkgo+rocm amdgpu_target={0}".format(gfx),
var/spack/repos/builtin/packages/mfem/package.py:            when="+ginkgo+rocm amdgpu_target={0}".format(gfx),
var/spack/repos/builtin/packages/mfem/package.py:    for sm_ in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/mfem/package.py:            "hiop+cuda cuda_arch={0}".format(sm_), when="+hiop+cuda cuda_arch={0}".format(sm_)
var/spack/repos/builtin/packages/mfem/package.py:    for gfx in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/mfem/package.py:            "hiop+rocm amdgpu_target={0}".format(gfx),
var/spack/repos/builtin/packages/mfem/package.py:            when="+hiop+rocm amdgpu_target={0}".format(gfx),
var/spack/repos/builtin/packages/mfem/package.py:    conflicts("+strumpack ^strumpack+cuda", when="~cuda")
var/spack/repos/builtin/packages/mfem/package.py:    depends_on("occa+cuda", when="+occa+cuda")
var/spack/repos/builtin/packages/mfem/package.py:    # TODO: propagate "+rocm" variant to occa when it is supported
var/spack/repos/builtin/packages/mfem/package.py:    for sm_ in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/mfem/package.py:            "raja+cuda cuda_arch={0}".format(sm_), when="+raja+cuda cuda_arch={0}".format(sm_)
var/spack/repos/builtin/packages/mfem/package.py:    for gfx in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/mfem/package.py:            "raja+rocm amdgpu_target={0}".format(gfx),
var/spack/repos/builtin/packages/mfem/package.py:            when="+raja+rocm amdgpu_target={0}".format(gfx),
var/spack/repos/builtin/packages/mfem/package.py:    for sm_ in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/mfem/package.py:            "libceed+cuda cuda_arch={0}".format(sm_),
var/spack/repos/builtin/packages/mfem/package.py:            when="+libceed+cuda cuda_arch={0}".format(sm_),
var/spack/repos/builtin/packages/mfem/package.py:    for gfx in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/mfem/package.py:            "libceed+rocm amdgpu_target={0}".format(gfx),
var/spack/repos/builtin/packages/mfem/package.py:            when="+libceed+rocm amdgpu_target={0}".format(gfx),
var/spack/repos/builtin/packages/mfem/package.py:    for sm_ in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/mfem/package.py:            "umpire+cuda cuda_arch={0}".format(sm_), when="+umpire+cuda cuda_arch={0}".format(sm_)
var/spack/repos/builtin/packages/mfem/package.py:    for gfx in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/mfem/package.py:            "umpire+rocm amdgpu_target={0}".format(gfx),
var/spack/repos/builtin/packages/mfem/package.py:            when="+umpire+rocm amdgpu_target={0}".format(gfx),
var/spack/repos/builtin/packages/mfem/package.py:    # AmgX: propagate the cuda_arch and mpi settings:
var/spack/repos/builtin/packages/mfem/package.py:    for sm_ in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/mfem/package.py:            "amgx+mpi cuda_arch={0}".format(sm_), when="+amgx+mpi cuda_arch={0}".format(sm_)
var/spack/repos/builtin/packages/mfem/package.py:            "amgx~mpi cuda_arch={0}".format(sm_), when="+amgx~mpi cuda_arch={0}".format(sm_)
var/spack/repos/builtin/packages/mfem/package.py:    patch("mfem-4.3-cusparse-11.4.patch", when="@4.3.0+cuda")
var/spack/repos/builtin/packages/mfem/package.py:        xcompiler = "" if "~cuda" in spec else "-Xcompiler="
var/spack/repos/builtin/packages/mfem/package.py:            "MFEM_USE_CUDA=%s" % yes_no("+cuda"),
var/spack/repos/builtin/packages/mfem/package.py:            "MFEM_USE_HIP=%s" % yes_no("+rocm"),
var/spack/repos/builtin/packages/mfem/package.py:            if "+cuda" in spec:
var/spack/repos/builtin/packages/mfem/package.py:        cuda_arch = None if "~cuda" in spec else spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/mfem/package.py:            if "+cuda" in spec:
var/spack/repos/builtin/packages/mfem/package.py:                    "-x=cu --expt-extended-lambda -arch=sm_%s" % cuda_arch,
var/spack/repos/builtin/packages/mfem/package.py:            hypre_gpu_libs = ""
var/spack/repos/builtin/packages/mfem/package.py:            if "+cuda" in hypre:
var/spack/repos/builtin/packages/mfem/package.py:                hypre_gpu_libs = " -lcusparse -lcurand -lcublas"
var/spack/repos/builtin/packages/mfem/package.py:            elif "+rocm" in hypre:
var/spack/repos/builtin/packages/mfem/package.py:                hypre_rocm_libs = LibraryList([])
var/spack/repos/builtin/packages/mfem/package.py:                    hypre_rocm_libs += hypre["rocsparse"].libs
var/spack/repos/builtin/packages/mfem/package.py:                    hypre_rocm_libs += hypre["rocrand"].libs
var/spack/repos/builtin/packages/mfem/package.py:                hypre_gpu_libs = " " + ld_flags_from_library_list(hypre_rocm_libs)
var/spack/repos/builtin/packages/mfem/package.py:                "HYPRE_LIB=%s%s" % (ld_flags_from_library_list(all_hypre_libs), hypre_gpu_libs),
var/spack/repos/builtin/packages/mfem/package.py:            if "+cuda" in strumpack:
var/spack/repos/builtin/packages/mfem/package.py:                # assuming also ("+cuda" in spec)
var/spack/repos/builtin/packages/mfem/package.py:        if "+cuda" in spec:
var/spack/repos/builtin/packages/mfem/package.py:                "CUDA_CXX=%s" % join_path(spec["cuda"].prefix, "bin", "nvcc"),
var/spack/repos/builtin/packages/mfem/package.py:                "CUDA_ARCH=sm_%s" % cuda_arch,
var/spack/repos/builtin/packages/mfem/package.py:            # Check if we are using a CUDA installation where the math libs are
var/spack/repos/builtin/packages/mfem/package.py:            cuda_libs = find_optional_library(culibs, spec["cuda"].prefix)
var/spack/repos/builtin/packages/mfem/package.py:            if not cuda_libs:
var/spack/repos/builtin/packages/mfem/package.py:                p0 = os.path.realpath(join_path(spec["cuda"].prefix, "bin", "nvcc"))
var/spack/repos/builtin/packages/mfem/package.py:                    cuda_libs = find_optional_library(culibs, join_path(p1, "math_libs"))
var/spack/repos/builtin/packages/mfem/package.py:                    if cuda_libs:
var/spack/repos/builtin/packages/mfem/package.py:                if not cuda_libs:
var/spack/repos/builtin/packages/mfem/package.py:                    raise InstallError("Required CUDA libraries not found: %s" % culibs)
var/spack/repos/builtin/packages/mfem/package.py:                options += ["CUDA_LIB=%s" % ld_flags_from_library_list(cuda_libs)]
var/spack/repos/builtin/packages/mfem/package.py:        if "+rocm" in spec:
var/spack/repos/builtin/packages/mfem/package.py:            amdgpu_target = ",".join(spec.variants["amdgpu_target"].value)
var/spack/repos/builtin/packages/mfem/package.py:            options += ["HIP_CXX=%s" % spec["hip"].hipcc, "HIP_ARCH=%s" % amdgpu_target]
var/spack/repos/builtin/packages/mfem/package.py:            #       "HIP_FLAGS=-xhip --offload-arch=%s" % amdgpu_target,
var/spack/repos/builtin/packages/mfem/package.py:            if "^hipsparse" in spec:  # hipsparse is needed @4.4.0:+rocm
var/spack/repos/builtin/packages/mfem/package.py:                # $(HIP_DIR)/lib, so we set HIP_DIR to be $ROCM_PATH when using
var/spack/repos/builtin/packages/mfem/package.py:                    options += ["HIP_DIR=%s" % env["ROCM_PATH"]]
var/spack/repos/builtin/packages/mfem/package.py:                # petsc+rocm needs the rocthrust header path
var/spack/repos/builtin/packages/mfem/package.py:                # rocthrust [via petsc+rocm] has a dependency on rocprim
var/spack/repos/builtin/packages/mfem/package.py:                # superlu-dist+rocm needs the hipblas header path
var/spack/repos/builtin/packages/mfem/package.py:            if hiop.satisfies("@0.6:+cuda"):
var/spack/repos/builtin/packages/mfem/package.py:        if "+cuda" in spec and "+cuda" in spec["sundials"]:
var/spack/repos/builtin/packages/mfem/package.py:            sun_comps += ",nveccuda"
var/spack/repos/builtin/packages/mfem/package.py:        if "+rocm" in spec and "+rocm" in spec["sundials"]:
var/spack/repos/builtin/packages/mfem/package.py:        return "-Wl," if "~cuda" in self.spec else "-Xlinker="
var/spack/repos/builtin/packages/mfem/test_builds.sh:cuda_arch="70"
var/spack/repos/builtin/packages/mfem/test_builds.sh:rocm_arch="gfx908"
var/spack/repos/builtin/packages/mfem/test_builds.sh:backends_specs='^occa~cuda ^raja~openmp'
var/spack/repos/builtin/packages/mfem/test_builds.sh:petsc_spec_cuda='^petsc+cuda+mumps'
var/spack/repos/builtin/packages/mfem/test_builds.sh:petsc_spec_rocm='^petsc+rocm+mumps'
var/spack/repos/builtin/packages/mfem/test_builds.sh:# strumpack spec without cuda (use version > 6.3.1)
var/spack/repos/builtin/packages/mfem/test_builds.sh:strumpack_spec='^strumpack~slate~openmp~cuda'
var/spack/repos/builtin/packages/mfem/test_builds.sh:strumpack_cuda_spec='^strumpack+cuda~slate~openmp'
var/spack/repos/builtin/packages/mfem/test_builds.sh:strumpack_rocm_spec='^strumpack+rocm~slate~openmp~cuda'
var/spack/repos/builtin/packages/mfem/test_builds.sh:# superlu specs with cpu, cuda and rocm
var/spack/repos/builtin/packages/mfem/test_builds.sh:# - v8.2.1 on CPU and GPU stalls in ex11p; works when superlu::PARMETIS is
var/spack/repos/builtin/packages/mfem/test_builds.sh:superlu_cuda_spec='^superlu-dist@8.1.2+cuda'
var/spack/repos/builtin/packages/mfem/test_builds.sh:superlu_rocm_spec='^superlu-dist@8.1.2+rocm'
var/spack/repos/builtin/packages/mfem/test_builds.sh:builds_cuda=(
var/spack/repos/builtin/packages/mfem/test_builds.sh:    # hypre without cuda:
var/spack/repos/builtin/packages/mfem/test_builds.sh:    ${mfem}'+cuda cuda_arch='"${cuda_arch}"
var/spack/repos/builtin/packages/mfem/test_builds.sh:    # hypre with cuda:
var/spack/repos/builtin/packages/mfem/test_builds.sh:    ${mfem}'+cuda cuda_arch='"${cuda_arch} ^hypre+cuda"
var/spack/repos/builtin/packages/mfem/test_builds.sh:    # hypre with cuda:
var/spack/repos/builtin/packages/mfem/test_builds.sh:    # TODO: restore '+libceed' when the libCEED CUDA unit tests take less time.
var/spack/repos/builtin/packages/mfem/test_builds.sh:    ${mfem}'+cuda+raja+occa cuda_arch='"${cuda_arch}"' \
var/spack/repos/builtin/packages/mfem/test_builds.sh:        ^raja+cuda~openmp ^hypre+cuda'
var/spack/repos/builtin/packages/mfem/test_builds.sh:    # hypre without cuda:
var/spack/repos/builtin/packages/mfem/test_builds.sh:    # NOTE: PETSc tests may need PETSC_OPTIONS="-use_gpu_aware_mpi 0"
var/spack/repos/builtin/packages/mfem/test_builds.sh:    # TODO: restore '+libceed' when the libCEED CUDA unit tests take less time.
var/spack/repos/builtin/packages/mfem/test_builds.sh:    ${mfem}'+cuda+openmp+raja+occa cuda_arch='"${cuda_arch}"' \
var/spack/repos/builtin/packages/mfem/test_builds.sh:        ^raja+cuda+openmp'" $strumpack_cuda_spec"' \
var/spack/repos/builtin/packages/mfem/test_builds.sh:        '"$superlu_cuda_spec $petsc_spec_cuda $conduit_spec"
var/spack/repos/builtin/packages/mfem/test_builds.sh:    ${mfem}'+cuda cuda_arch='"${cuda_arch}"' +raja+umpire'
var/spack/repos/builtin/packages/mfem/test_builds.sh:    ${mfem}'+cuda cuda_arch='"${cuda_arch}"' +hiop'
var/spack/repos/builtin/packages/mfem/test_builds.sh:    # hypre with cuda:
var/spack/repos/builtin/packages/mfem/test_builds.sh:    # TODO: restore '+libceed' when the libCEED CUDA unit tests take less time.
var/spack/repos/builtin/packages/mfem/test_builds.sh:    # TODO: add back "+petsc+slepc $petsc_spec_cuda" when it works.
var/spack/repos/builtin/packages/mfem/test_builds.sh:    # NOTE: PETSc tests may need PETSC_OPTIONS="-use_gpu_aware_mpi 0"
var/spack/repos/builtin/packages/mfem/test_builds.sh:    # TODO: add back "+sundials" when it's supported with '^hypre+cuda'.
var/spack/repos/builtin/packages/mfem/test_builds.sh:    ${mfem}'+cuda+openmp+raja+occa cuda_arch='"${cuda_arch}"' \
var/spack/repos/builtin/packages/mfem/test_builds.sh:        ^raja+cuda+openmp ^hypre+cuda \
var/spack/repos/builtin/packages/mfem/test_builds.sh:        '" $strumpack_cuda_spec $superlu_cuda_spec $conduit_spec"
var/spack/repos/builtin/packages/mfem/test_builds.sh:    ${mfem}'+cuda cuda_arch='"${cuda_arch}"' +raja+umpire ^hypre+cuda'
var/spack/repos/builtin/packages/mfem/test_builds.sh:    ${mfem}'+cuda cuda_arch='"${cuda_arch}"' +hiop ^hypre+cuda'
var/spack/repos/builtin/packages/mfem/test_builds.sh:    ${mfem}' precision=single +cuda cuda_arch='"${cuda_arch}"' ^hypre+cuda'
var/spack/repos/builtin/packages/mfem/test_builds.sh:    # hypre without cuda:
var/spack/repos/builtin/packages/mfem/test_builds.sh:    ${mfem_dev}'+cuda cuda_arch='"${cuda_arch}"
var/spack/repos/builtin/packages/mfem/test_builds.sh:    # hypre with cuda:
var/spack/repos/builtin/packages/mfem/test_builds.sh:    ${mfem_dev}'+cuda cuda_arch='"${cuda_arch} ^hypre+cuda"
var/spack/repos/builtin/packages/mfem/test_builds.sh:    # hypre with cuda:
var/spack/repos/builtin/packages/mfem/test_builds.sh:    # TODO: restore '+libceed' when the libCEED CUDA unit tests take less time.
var/spack/repos/builtin/packages/mfem/test_builds.sh:    ${mfem_dev}'+cuda+raja+occa cuda_arch='"${cuda_arch}"' \
var/spack/repos/builtin/packages/mfem/test_builds.sh:        ^raja+cuda~openmp ^hypre+cuda'
var/spack/repos/builtin/packages/mfem/test_builds.sh:    # hypre without cuda:
var/spack/repos/builtin/packages/mfem/test_builds.sh:    # NOTE: PETSc tests may need PETSC_OPTIONS="-use_gpu_aware_mpi 0"
var/spack/repos/builtin/packages/mfem/test_builds.sh:    # TODO: restore '+libceed' when the libCEED CUDA unit tests take less time.
var/spack/repos/builtin/packages/mfem/test_builds.sh:    ${mfem_dev}'+cuda+openmp+raja+occa cuda_arch='"${cuda_arch}"' \
var/spack/repos/builtin/packages/mfem/test_builds.sh:        ^raja+cuda+openmp'" $strumpack_cuda_spec"' \
var/spack/repos/builtin/packages/mfem/test_builds.sh:        '"$superlu_cuda_spec $petsc_spec_cuda $conduit_spec"
var/spack/repos/builtin/packages/mfem/test_builds.sh:    ${mfem_dev}'+cuda cuda_arch='"${cuda_arch}"' +raja+umpire'
var/spack/repos/builtin/packages/mfem/test_builds.sh:    ${mfem_dev}'+cuda cuda_arch='"${cuda_arch}"' +hiop'
var/spack/repos/builtin/packages/mfem/test_builds.sh:    # hypre with cuda:
var/spack/repos/builtin/packages/mfem/test_builds.sh:    # TODO: restore '+libceed' when the libCEED CUDA unit tests take less time.
var/spack/repos/builtin/packages/mfem/test_builds.sh:    # TODO: add back "+petsc+slepc $petsc_spec_cuda" when it works.
var/spack/repos/builtin/packages/mfem/test_builds.sh:    # NOTE: PETSc tests may need PETSC_OPTIONS="-use_gpu_aware_mpi 0"
var/spack/repos/builtin/packages/mfem/test_builds.sh:    # TODO: add back "+sundials" when it's supported with '^hypre+cuda'.
var/spack/repos/builtin/packages/mfem/test_builds.sh:    ${mfem_dev}'+cuda+openmp+raja+occa cuda_arch='"${cuda_arch}"' \
var/spack/repos/builtin/packages/mfem/test_builds.sh:        ^raja+cuda+openmp ^hypre+cuda \
var/spack/repos/builtin/packages/mfem/test_builds.sh:        '"$strumpack_cuda_spec $superlu_cuda_spec $conduit_spec"
var/spack/repos/builtin/packages/mfem/test_builds.sh:    ${mfem_dev}'+cuda cuda_arch='"${cuda_arch}"' +raja+umpire ^hypre+cuda'
var/spack/repos/builtin/packages/mfem/test_builds.sh:    ${mfem_dev}'+cuda cuda_arch='"${cuda_arch}"' +hiop ^hypre+cuda'
var/spack/repos/builtin/packages/mfem/test_builds.sh:    ${mfem_dev}' precision=single +cuda cuda_arch='"${cuda_arch}"' ^hypre+cuda'
var/spack/repos/builtin/packages/mfem/test_builds.sh:builds_rocm=(
var/spack/repos/builtin/packages/mfem/test_builds.sh:    # hypre without rocm:
var/spack/repos/builtin/packages/mfem/test_builds.sh:    ${mfem}'+rocm amdgpu_target='"${rocm_arch}"
var/spack/repos/builtin/packages/mfem/test_builds.sh:    # hypre with rocm:
var/spack/repos/builtin/packages/mfem/test_builds.sh:    ${mfem}'+rocm amdgpu_target='"${rocm_arch} ^hypre+rocm"
var/spack/repos/builtin/packages/mfem/test_builds.sh:    # hypre with rocm:
var/spack/repos/builtin/packages/mfem/test_builds.sh:    ${mfem}'+rocm+raja+occa+libceed amdgpu_target='"${rocm_arch}"' \
var/spack/repos/builtin/packages/mfem/test_builds.sh:        ^raja+rocm~openmp ^occa~cuda~openmp ^hypre+rocm'
var/spack/repos/builtin/packages/mfem/test_builds.sh:    # hypre without rocm:
var/spack/repos/builtin/packages/mfem/test_builds.sh:    ${mfem}'+rocm+openmp+raja+occa+libceed amdgpu_target='"${rocm_arch}"' \
var/spack/repos/builtin/packages/mfem/test_builds.sh:        ^raja+rocm~openmp ^occa~cuda'" $strumpack_rocm_spec"' \
var/spack/repos/builtin/packages/mfem/test_builds.sh:        '"$superlu_rocm_spec $petsc_spec_rocm $conduit_spec"
var/spack/repos/builtin/packages/mfem/test_builds.sh:    ${mfem}'+rocm amdgpu_target='"${rocm_arch}"' +raja+umpire'
var/spack/repos/builtin/packages/mfem/test_builds.sh:    ${mfem}'+rocm amdgpu_target='"${rocm_arch}"' +hiop'
var/spack/repos/builtin/packages/mfem/test_builds.sh:    # hypre with rocm:
var/spack/repos/builtin/packages/mfem/test_builds.sh:    # TODO: add back "+petsc+slepc $petsc_spec_rocm" when it works.
var/spack/repos/builtin/packages/mfem/test_builds.sh:    # TODO: add back "+sundials" when it's supported with '^hypre+rocm'.
var/spack/repos/builtin/packages/mfem/test_builds.sh:    ${mfem}'+rocm+openmp+raja+occa+libceed amdgpu_target='"${rocm_arch}"' \
var/spack/repos/builtin/packages/mfem/test_builds.sh:        ^raja+rocm~openmp ^occa~cuda ^hypre+rocm \
var/spack/repos/builtin/packages/mfem/test_builds.sh:        '"$strumpack_rocm_spec $superlu_rocm_spec $conduit_spec"
var/spack/repos/builtin/packages/mfem/test_builds.sh:    ${mfem}'+rocm amdgpu_target='"${rocm_arch}"' +raja+umpire ^hypre+rocm'
var/spack/repos/builtin/packages/mfem/test_builds.sh:    ${mfem}'+rocm amdgpu_target='"${rocm_arch}"' +hiop ^hypre+rocm'
var/spack/repos/builtin/packages/mfem/test_builds.sh:    ${mfem}' precision=single +rocm amdgpu_target='"${rocm_arch}"' ^hypre+rocm'
var/spack/repos/builtin/packages/mfem/test_builds.sh:# run_builds=("${builds_cuda[@]}")
var/spack/repos/builtin/packages/mfem/test_builds.sh:# run_builds=("${builds_rocm[@]}")
var/spack/repos/builtin/packages/mfem/test_builds.sh:# PETSc CUDA tests on Lassen need this:
var/spack/repos/builtin/packages/mfem/test_builds.sh:# export PETSC_OPTIONS="-use_gpu_aware_mpi 0"
var/spack/repos/builtin/packages/mfem/mfem-4.3-cusparse-11.4.patch:+#endif // MFEM_USE_CUDA
var/spack/repos/builtin/packages/mfem/mfem-4.3-cusparse-11.4.patch:-#if CUDA_VERSION >= 11020
var/spack/repos/builtin/packages/mfem/mfem-4.3-cusparse-11.4.patch:-                              vecX_descr, &beta, vecY_descr, CUDA_R_64F,
var/spack/repos/builtin/packages/mfem/mfem-4.3-cusparse-11.4.patch:-#elif CUDA_VERSION >= 10010
var/spack/repos/builtin/packages/mfem/mfem-4.3-cusparse-11.4.patch:                               vecX_descr, &beta, vecY_descr, CUDA_R_64F,
var/spack/repos/builtin/packages/mfem/mfem-4.3-cusparse-11.4.patch:-#if CUDA_VERSION >= 11020
var/spack/repos/builtin/packages/mfem/mfem-4.3-cusparse-11.4.patch:-                   vecX_descr, &beta, vecY_descr, CUDA_R_64F, CUSPARSE_SPMV_CSR_ALG1, dBuffer);
var/spack/repos/builtin/packages/mfem/mfem-4.3-cusparse-11.4.patch:-#elif CUDA_VERSION >= 10010
var/spack/repos/builtin/packages/mfem/mfem-4.3-cusparse-11.4.patch:+#if CUDA_VERSION >= 10010
var/spack/repos/builtin/packages/mfem/mfem-4.3-cusparse-11.4.patch:-                   vecX_descr, &beta, vecY_descr, CUDA_R_64F, CUSPARSE_CSRMV_ALG1, dBuffer);
var/spack/repos/builtin/packages/mfem/mfem-4.3-cusparse-11.4.patch:+                   vecX_descr, &beta, vecY_descr, CUDA_R_64F, MFEM_CUSPARSE_ALG, dBuffer);
var/spack/repos/builtin/packages/mfem/mfem-4.3-cusparse-11.4.patch:+#endif // CUDA_VERSION >= 10010
var/spack/repos/builtin/packages/mfem/mfem-4.3-cusparse-11.4.patch:+#endif // MFEM_USE_CUDA
var/spack/repos/builtin/packages/mfem/mfem-4.5.patch:-   ifeq ($(MFEM_USE_CUDA)$(MFEM_USE_HIP),NONO)
var/spack/repos/builtin/packages/mfem/mfem-4.5.patch:-   else ifeq ($(MFEM_USE_CUDA),YES)
var/spack/repos/builtin/packages/mfem/mfem-4.5.patch:+   ifeq ($(MFEM_USE_CUDA),YES)
var/spack/repos/builtin/packages/mfem/mfem-4.5.patch:       XLINKER = $(CUDA_XLINKER)
var/spack/repos/builtin/packages/py-nvidia-modulus/package.py:class PyNvidiaModulus(PythonPackage):
var/spack/repos/builtin/packages/py-nvidia-modulus/package.py:    homepage = "https://github.com/NVIDIA/modulus"
var/spack/repos/builtin/packages/py-nvidia-modulus/package.py:    url = "https://github.com/NVIDIA/modulus/archive/refs/tags/v0.5.0.tar.gz"
var/spack/repos/builtin/packages/py-nvidia-modulus/package.py:        # https://github.com/NVIDIA/modulus/issues/383
var/spack/repos/builtin/packages/py-nvidia-modulus/package.py:        depends_on("py-nvidia-dali@1.16.0:")
var/spack/repos/builtin/packages/nvptx-tools/package.py:    of GCC that enables offloading of OpenMP/OpenACC code to NVIDIA
var/spack/repos/builtin/packages/nvptx-tools/package.py:    GPUs."""
var/spack/repos/builtin/packages/nvptx-tools/package.py:    depends_on("cuda")
var/spack/repos/builtin/packages/nvptx-tools/package.py:        cuda_dir = self.spec["cuda"].prefix
var/spack/repos/builtin/packages/nvptx-tools/package.py:            "--with-cuda-driver-include={0}".format(cuda_dir.include),
var/spack/repos/builtin/packages/nvptx-tools/package.py:            "--with-cuda-driver-lib={0}".format(cuda_dir.lib64),
var/spack/repos/builtin/packages/double-batched-fft-library/package.py:    targeting Graphics Processing Units; supporting OpenCL,
var/spack/repos/builtin/packages/double-batched-fft-library/package.py:    variant("opencl", default=True, when="~sycl", description="Build bbfft-opencl")
var/spack/repos/builtin/packages/double-batched-fft-library/package.py:    depends_on("opencl", when="+opencl")
var/spack/repos/builtin/packages/double-batched-fft-library/package.py:    patch("0001-Add-CPATH-and-LIBRARY_PATHs-to-OpenCL-search-paths.patch", when="@:0.3.6")
var/spack/repos/builtin/packages/double-batched-fft-library/package.py:            self.define_from_variant("BUILD_OPENCL", "opencl"),
var/spack/repos/builtin/packages/double-batched-fft-library/0001-Add-CPATH-and-LIBRARY_PATHs-to-OpenCL-search-paths.patch:Subject: [PATCH] Add CPATH and LIBRARY_PATHs to OpenCL search paths
var/spack/repos/builtin/packages/double-batched-fft-library/0001-Add-CPATH-and-LIBRARY_PATHs-to-OpenCL-search-paths.patch: cmake/FindOpenCL.cmake | 5 +++++
var/spack/repos/builtin/packages/double-batched-fft-library/0001-Add-CPATH-and-LIBRARY_PATHs-to-OpenCL-search-paths.patch:diff --git a/cmake/FindOpenCL.cmake b/cmake/FindOpenCL.cmake
var/spack/repos/builtin/packages/double-batched-fft-library/0001-Add-CPATH-and-LIBRARY_PATHs-to-OpenCL-search-paths.patch:--- a/cmake/FindOpenCL.cmake
var/spack/repos/builtin/packages/double-batched-fft-library/0001-Add-CPATH-and-LIBRARY_PATHs-to-OpenCL-search-paths.patch:+++ b/cmake/FindOpenCL.cmake
var/spack/repos/builtin/packages/double-batched-fft-library/0001-Add-CPATH-and-LIBRARY_PATHs-to-OpenCL-search-paths.patch:     find_path(OpenCL_INCLUDE_DIR NAMES CL/cl.h OpenCL/cl.h
var/spack/repos/builtin/packages/double-batched-fft-library/0001-Add-CPATH-and-LIBRARY_PATHs-to-OpenCL-search-paths.patch:             ENV OpenCL_ROOT
var/spack/repos/builtin/packages/double-batched-fft-library/0001-Add-CPATH-and-LIBRARY_PATHs-to-OpenCL-search-paths.patch:             ENV CUDA_PATH
var/spack/repos/builtin/packages/double-batched-fft-library/0001-Add-CPATH-and-LIBRARY_PATHs-to-OpenCL-search-paths.patch:         find_library(OpenCL_LIBRARY NAMES OpenCL
var/spack/repos/builtin/packages/double-batched-fft-library/0001-Add-CPATH-and-LIBRARY_PATHs-to-OpenCL-search-paths.patch:                 ENV OpenCL_ROOT
var/spack/repos/builtin/packages/double-batched-fft-library/0001-Add-CPATH-and-LIBRARY_PATHs-to-OpenCL-search-paths.patch:                 ENV CUDA_PATH
var/spack/repos/builtin/packages/double-batched-fft-library/0001-Add-CPATH-and-LIBRARY_PATHs-to-OpenCL-search-paths.patch:         find_library(OpenCL_LIBRARY NAMES OpenCL
var/spack/repos/builtin/packages/double-batched-fft-library/0001-Add-CPATH-and-LIBRARY_PATHs-to-OpenCL-search-paths.patch:                 ENV OpenCL_ROOT
var/spack/repos/builtin/packages/double-batched-fft-library/0001-Add-CPATH-and-LIBRARY_PATHs-to-OpenCL-search-paths.patch:                 ENV CUDA_PATH
var/spack/repos/builtin/packages/boinc-client/package.py:class BoincClient(AutotoolsPackage):
var/spack/repos/builtin/packages/boinc-client/package.py:    supports virtualized, parallel, and GPU-based
var/spack/repos/builtin/packages/rocfft/package.py:    homepage = "https://github.com/ROCm/rocFFT/"
var/spack/repos/builtin/packages/rocfft/package.py:    git = "https://github.com/ROCm/rocFFT.git"
var/spack/repos/builtin/packages/rocfft/package.py:    url = "https://github.com/ROCm/rocfft/archive/rocm-6.1.1.tar.gz"
var/spack/repos/builtin/packages/rocfft/package.py:    tags = ["rocm"]
var/spack/repos/builtin/packages/rocfft/package.py:    amdgpu_targets = ROCmPackage.amdgpu_targets
var/spack/repos/builtin/packages/rocfft/package.py:        "amdgpu_target",
var/spack/repos/builtin/packages/rocfft/package.py:        description="AMD GPU architecture",
var/spack/repos/builtin/packages/rocfft/package.py:        values=auto_or_any_combination_of(*amdgpu_targets),
var/spack/repos/builtin/packages/rocfft/package.py:        "amdgpu_target_sram_ecc",
var/spack/repos/builtin/packages/rocfft/package.py:        description="AMD GPU architecture",
var/spack/repos/builtin/packages/rocfft/package.py:        values=auto_or_any_combination_of(*amdgpu_targets),
var/spack/repos/builtin/packages/rocfft/package.py:    depends_on("rocm-openmp-extras", type="test")
var/spack/repos/builtin/packages/rocfft/package.py:        depends_on(f"rocm-cmake@{ver}:", type="build", when=f"@{ver}")
var/spack/repos/builtin/packages/rocfft/package.py:        "0005-Fix-clients-tests-include-rocrand-fftw-include-dir-rocm-6.0.0.patch", when="@5.7.0:"
var/spack/repos/builtin/packages/rocfft/package.py:    # https://github.com/ROCm/rocFFT/pull/449)
var/spack/repos/builtin/packages/rocfft/package.py:        "https://github.com/ROCm/rocFFT/commit/0ec78f1daac2d7fa1415f4deff0d129252c1c9de.patch?full_index=1",
var/spack/repos/builtin/packages/rocfft/package.py:            env.set("CC", f"{self.spec['llvm-amdgpu'].prefix}/bin/clang")
var/spack/repos/builtin/packages/rocfft/package.py:            env.set("CXX", f"{self.spec['llvm-amdgpu'].prefix}/bin/clang++")
var/spack/repos/builtin/packages/rocfft/package.py:        tgt = self.spec.variants["amdgpu_target"]
var/spack/repos/builtin/packages/rocfft/package.py:            args.append(self.define_from_variant("AMDGPU_TARGETS", "amdgpu_target"))
var/spack/repos/builtin/packages/rocfft/package.py:        # See https://github.com/ROCm/rocFFT/issues/322
var/spack/repos/builtin/packages/rocfft/package.py:            args.append(self.define("__skip_rocmclang", "ON"))
var/spack/repos/builtin/packages/rocfft/0005-Fix-clients-tests-include-rocrand-fftw-include-dir-rocm-6.0.0.patch: include( ROCMInstallTargets )
var/spack/repos/builtin/packages/rocmlir/package.py:class Rocmlir(CMakePackage):
var/spack/repos/builtin/packages/rocmlir/package.py:    homepage = "https://github.com/ROCm/rocMLIR"
var/spack/repos/builtin/packages/rocmlir/package.py:    git = "https://github.com/ROCm/rocMLIR.git"
var/spack/repos/builtin/packages/rocmlir/package.py:    url = "https://github.com/ROCm/rocMLIR/archive/refs/tags/rocm-6.2.1.tar.gz"
var/spack/repos/builtin/packages/rocmlir/package.py:                "${ROCM_PATH}/bin",
var/spack/repos/builtin/packages/rocmlir/package.py:                self.spec["rocminfo"].prefix.bin,
var/spack/repos/builtin/packages/rocmlir/package.py:        depends_on(f"llvm-amdgpu@{ver}", when=f"@{ver}")
var/spack/repos/builtin/packages/rocmlir/package.py:        depends_on(f"rocm-cmake@{ver}", type="build", when=f"@{ver}")
var/spack/repos/builtin/packages/rocmlir/package.py:        depends_on(f"rocminfo@{ver}", type="build", when=f"@{ver}")
var/spack/repos/builtin/packages/rocmlir/package.py:                "CMAKE_CXX_COMPILER", "{0}/bin/clang++".format(spec["llvm-amdgpu"].prefix)
var/spack/repos/builtin/packages/rocmlir/package.py:            self.define("CMAKE_C_COMPILER", "{0}/bin/clang".format(spec["llvm-amdgpu"].prefix)),
var/spack/repos/builtin/packages/hipace/package.py:        values=("omp", "cuda", "hip", "sycl", "noacc"),
var/spack/repos/builtin/packages/hipace/package.py:    with when("compute=cuda"):
var/spack/repos/builtin/packages/hipace/package.py:        depends_on("cuda@:12.1", when="%gcc@:12.2")
var/spack/repos/builtin/packages/hipace/package.py:        depends_on("cuda@:12.0", when="%gcc@:12.1")
var/spack/repos/builtin/packages/hipace/package.py:        depends_on("cuda@:12.1", when="%gcc@:12.2")
var/spack/repos/builtin/packages/hipace/package.py:        depends_on("cuda@:11.8", when="%gcc@:9")
var/spack/repos/builtin/packages/hipace/package.py:        depends_on("cuda@:10", when="%gcc@:8")
var/spack/repos/builtin/packages/hipace/package.py:        depends_on("cuda@9.2.88:")
var/spack/repos/builtin/packages/miopen-hip/package.py:    homepage = "https://github.com/ROCm/MIOpen"
var/spack/repos/builtin/packages/miopen-hip/package.py:    git = "https://github.com/ROCm/MIOpen.git"
var/spack/repos/builtin/packages/miopen-hip/package.py:    url = "https://github.com/ROCm/MIOpen/archive/rocm-6.1.2.tar.gz"
var/spack/repos/builtin/packages/miopen-hip/package.py:    tags = ["rocm"]
var/spack/repos/builtin/packages/miopen-hip/package.py:        "https://github.com/ROCm/MIOpen/commit/f60aa1ff89f8fb596b4a6a4c70aa7d557803db87.patch?full_index=1",
var/spack/repos/builtin/packages/miopen-hip/package.py:        depends_on(f"rocm-cmake@{ver}:", type="build", when=f"@{ver}")
var/spack/repos/builtin/packages/miopen-hip/package.py:        depends_on(f"rocm-clang-ocl@{ver}", when=f"@{ver}")
var/spack/repos/builtin/packages/miopen-hip/package.py:        depends_on(f"rocmlir@{ver}", when=f"@{ver}")
var/spack/repos/builtin/packages/miopen-hip/package.py:            env.set("CC", f"{self.spec['llvm-amdgpu'].prefix}/bin/clang")
var/spack/repos/builtin/packages/miopen-hip/package.py:            env.set("CXX", f"{self.spec['llvm-amdgpu'].prefix}/bin/clang++")
var/spack/repos/builtin/packages/miopen-hip/package.py:        return self.spec["llvm-amdgpu"].prefix.amdgcn.bitcode
var/spack/repos/builtin/packages/miopen-hip/package.py:                "CMAKE_CXX_COMPILER", "{0}/bin/clang++".format(spec["llvm-amdgpu"].prefix)
var/spack/repos/builtin/packages/miopen-hip/package.py:                "MIOPEN_AMDGCN_ASSEMBLER", "{0}/bin/clang".format(spec["llvm-amdgpu"].prefix)
var/spack/repos/builtin/packages/miopen-hip/0002-add-include-dir-miopen-hip-6.0.0.patch: # Workaround : change in rocm-cmake was causing linking error so had to add ${CMAKE_DL_LIBS} 
var/spack/repos/builtin/packages/chameleon/package.py:class Chameleon(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/chameleon/package.py:    """Dense Linear Algebra for Scalable Multi-core Architectures and GPGPUs"""
var/spack/repos/builtin/packages/chameleon/package.py:    variant("cuda", default=False, when="runtime=starpu", description="Enable CUDA")
var/spack/repos/builtin/packages/chameleon/package.py:        depends_on("starpu~cuda", when="~cuda")
var/spack/repos/builtin/packages/chameleon/package.py:        depends_on("starpu+cuda", when="+cuda")
var/spack/repos/builtin/packages/chameleon/package.py:            depends_on("cuda", when="+cuda")
var/spack/repos/builtin/packages/chameleon/package.py:            self.define_from_variant("CHAMELEON_USE_CUDA", "cuda"),
var/spack/repos/builtin/packages/tau/tau-rocm-disable-llvm-plugin.patch:             tauoptions="${tauoptions}-rocm"
var/spack/repos/builtin/packages/tau/tau-rocm-disable-llvm-plugin.patch:             tauoptions="${tauoptions}-rocm"
var/spack/repos/builtin/packages/tau/package.py:    variant("cuda", default=False, description="Activates CUDA support")
var/spack/repos/builtin/packages/tau/package.py:    variant("rocm", default=False, description="Activates ROCm support", when="@2.28:")
var/spack/repos/builtin/packages/tau/package.py:        description="Activates ROCm rocprofiler support",
var/spack/repos/builtin/packages/tau/package.py:        "roctracer", default=False, description="Activates ROCm roctracer support", when="@2.28.1:"
var/spack/repos/builtin/packages/tau/package.py:        "rocprofv2", default=False, description="Activates ROCm rocprofiler support", when="@2.34:"
var/spack/repos/builtin/packages/tau/package.py:    variant("opencl", default=False, description="Activates OpenCL support")
var/spack/repos/builtin/packages/tau/package.py:    depends_on("cuda", when="+cuda")
var/spack/repos/builtin/packages/tau/package.py:    depends_on("hsa-rocr-dev", when="+rocm")
var/spack/repos/builtin/packages/tau/package.py:    depends_on("rocm-smi-lib", when="@2.32.1: +rocm")
var/spack/repos/builtin/packages/tau/package.py:    depends_on("rocm-core", when="@2.34: +rocm")
var/spack/repos/builtin/packages/tau/package.py:    requires("+rocm", when="+rocprofiler", msg="Rocprofiler requires ROCm")
var/spack/repos/builtin/packages/tau/package.py:    requires("+rocm", when="+roctracer", msg="Roctracer requires ROCm")
var/spack/repos/builtin/packages/tau/package.py:        when="+rocm",
var/spack/repos/builtin/packages/tau/package.py:        msg="Using ROCm, select either +rocprofiler, +roctracer or +rocprofv2",
var/spack/repos/builtin/packages/tau/package.py:    patch("tau-rocm-disable-llvm-plugin.patch", when="@2.33.2 +rocm")
var/spack/repos/builtin/packages/tau/package.py:        if "+cuda" in spec:
var/spack/repos/builtin/packages/tau/package.py:            options.append("-cuda=%s" % spec["cuda"].prefix)
var/spack/repos/builtin/packages/tau/package.py:        if "+opencl" in spec:
var/spack/repos/builtin/packages/tau/package.py:            options.append("-opencl")
var/spack/repos/builtin/packages/tau/package.py:        if "+rocm" in spec:
var/spack/repos/builtin/packages/tau/package.py:            options.append("-rocm=%s" % spec["hsa-rocr-dev"].prefix)
var/spack/repos/builtin/packages/tau/package.py:                options.append("-rocmsmi=%s" % spec["rocm-smi-lib"].prefix)
var/spack/repos/builtin/packages/tau/package.py:                options.append("-rocm-core=%s" % spec["rocm-core"].prefix)
var/spack/repos/builtin/packages/tau/package.py:        if "+cuda" in self.spec:
var/spack/repos/builtin/packages/tau/package.py:            env.append_path("PATH", self.spec["cuda"].prefix.bin)
var/spack/repos/builtin/packages/tau/package.py:    cuda_test = join_path("examples", "gpu", "cuda", "dataElem_um")
var/spack/repos/builtin/packages/tau/package.py:    level_zero_test = join_path("examples", "gpu", "oneapi", "complex_mult")
var/spack/repos/builtin/packages/tau/package.py:    rocm_test = join_path("examples", "gpu", "hip", "vectorAdd")
var/spack/repos/builtin/packages/tau/package.py:        if "+cuda" in self.spec:
var/spack/repos/builtin/packages/tau/package.py:            cache_extra_test_sources(self, self.cuda_test)
var/spack/repos/builtin/packages/tau/package.py:        if "+rocm" in self.spec:
var/spack/repos/builtin/packages/tau/package.py:            cache_extra_test_sources(self, self.rocm_test)
var/spack/repos/builtin/packages/tau/package.py:    def _run_rocm_test(self, test_name, purpose, work_dir):
var/spack/repos/builtin/packages/tau/package.py:                flags = ["-T", "mpi", "-rocm"]
var/spack/repos/builtin/packages/tau/package.py:                mpirun("-np", "4", self.prefix.bin.tau_exec, *flags, "./gpu-stream-hip")
var/spack/repos/builtin/packages/tau/package.py:                flags = ["-T", "serial", "-rocm"]
var/spack/repos/builtin/packages/tau/package.py:                tau_exec(*flags, "./gpu-stream-hip")
var/spack/repos/builtin/packages/tau/package.py:    def test_rocm(self):
var/spack/repos/builtin/packages/tau/package.py:        """rocm test"""
var/spack/repos/builtin/packages/tau/package.py:        # make is unable to find rocm_agent_enumerator
var/spack/repos/builtin/packages/tau/package.py:        if "+rocm" in self.spec and (
var/spack/repos/builtin/packages/tau/package.py:            rocm_test_dir = join_path(self.test_suite.current_test_cache_dir, self.rocm_test)
var/spack/repos/builtin/packages/tau/package.py:            self._run_rocm_test("test_rocm", "Testing rocm", rocm_test_dir)
var/spack/repos/builtin/packages/kokkos-kernels/package.py:class KokkosKernels(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/kokkos-kernels/package.py:        "cuda": (False, "enable Cuda backend"),
var/spack/repos/builtin/packages/kokkos-kernels/package.py:        "execspace_cuda": (
var/spack/repos/builtin/packages/kokkos-kernels/package.py:            "Whether to pre instantiate kernels for the execution space Kokkos::Cuda",
var/spack/repos/builtin/packages/kokkos-kernels/package.py:            "cuda",
var/spack/repos/builtin/packages/kokkos-kernels/package.py:        "memspace_cudauvmspace": (
var/spack/repos/builtin/packages/kokkos-kernels/package.py:            "Whether to pre instantiate kernels for the memory space Kokkos::CudaUVMSpace",
var/spack/repos/builtin/packages/kokkos-kernels/package.py:            "cuda",
var/spack/repos/builtin/packages/kokkos-kernels/package.py:        "memspace_cudaspace": (
var/spack/repos/builtin/packages/kokkos-kernels/package.py:            "Whether to pre instantiate kernels for the memory space Kokkos::CudaSpace",
var/spack/repos/builtin/packages/kokkos-kernels/package.py:            "cuda",
var/spack/repos/builtin/packages/kokkos-kernels/package.py:    depends_on("kokkos+cuda_lambda", when="@4.0.00:+cuda")
var/spack/repos/builtin/packages/kokkos-kernels/package.py:        "cublas": (False, "cuda", None, "@3.0.00:", "Link to CUDA BLAS library"),
var/spack/repos/builtin/packages/kokkos-kernels/package.py:        "cusparse": (False, "cuda", None, "@3.0.00:", "Link to CUDA sparse library"),
var/spack/repos/builtin/packages/kokkos-kernels/package.py:        "cusolver": (False, "cuda", None, "@4.3.00:", "Link to CUDA solver library"),
var/spack/repos/builtin/packages/kokkos-kernels/package.py:        if spec.satisfies("^kokkos+rocm"):
var/spack/repos/builtin/packages/py-keras/package.py:        # requirements-tensorflow-cuda.txt
var/spack/repos/builtin/packages/py-keras/package.py:        # requirements-jax-cuda.txt
var/spack/repos/builtin/packages/py-keras/package.py:        # requirements-torch-cuda.txt
var/spack/repos/builtin/packages/qmcpack/package.py:class Qmcpack(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/qmcpack/package.py:        "combination with CUDA, only AFQMC will have CUDA.",
var/spack/repos/builtin/packages/qmcpack/package.py:    # Notes about CUDA-centric peculiarities:
var/spack/repos/builtin/packages/qmcpack/package.py:    # cuda variant implies mixed precision variant by default, but there is
var/spack/repos/builtin/packages/qmcpack/package.py:    # variant('+mixed', default=True, when='+cuda', description="...")
var/spack/repos/builtin/packages/qmcpack/package.py:    # cuda+afqmc variant will not build the legacy CUDA code in real-space
var/spack/repos/builtin/packages/qmcpack/package.py:    # worth fixing since the legacy CUDA code, will be superseded
var/spack/repos/builtin/packages/qmcpack/package.py:        when="+cuda@:3.4.0",
var/spack/repos/builtin/packages/qmcpack/package.py:        msg="QMCPACK CUDA+SOA variant does not exist prior to v. 3.5.0.",
var/spack/repos/builtin/packages/qmcpack/package.py:        "cuda_arch=none",
var/spack/repos/builtin/packages/qmcpack/package.py:        when="+cuda",
var/spack/repos/builtin/packages/qmcpack/package.py:        msg="A value for cuda_arch must be specified. Add cuda_arch=XX",
var/spack/repos/builtin/packages/qmcpack/package.py:        # When '-DQMC_CUDA=1', CMake automatically sets:
var/spack/repos/builtin/packages/qmcpack/package.py:        # There is a double-precision CUDA path, but it is not as well
var/spack/repos/builtin/packages/qmcpack/package.py:        if "+cuda" in spec:
var/spack/repos/builtin/packages/qmcpack/package.py:            # Cannot support both CUDA builds at the same time, see
var/spack/repos/builtin/packages/qmcpack/package.py:                args.append("-DENABLE_CUDA=1")
var/spack/repos/builtin/packages/qmcpack/package.py:                args.append("-DQMC_CUDA=1")
var/spack/repos/builtin/packages/qmcpack/package.py:            cuda_arch_list = spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/qmcpack/package.py:            cuda_arch = cuda_arch_list[0]
var/spack/repos/builtin/packages/qmcpack/package.py:            if len(cuda_arch_list) > 1:
var/spack/repos/builtin/packages/qmcpack/package.py:                    "QMCPACK only supports compilation for a single " "GPU architecture at a time"
var/spack/repos/builtin/packages/qmcpack/package.py:                args.append("-DCMAKE_CUDA_ARCHITECTURES={0}".format(cuda_arch))
var/spack/repos/builtin/packages/qmcpack/package.py:                args.append("-DCUDA_ARCH=sm_{0}".format(cuda_arch))
var/spack/repos/builtin/packages/qmcpack/package.py:            args.append("-DQMC_CUDA=0")
var/spack/repos/builtin/packages/qmcpack/package.py:        # Mixed-precision versues double-precision CPU and GPU code
var/spack/repos/builtin/packages/oce/package.py:            self.define("OCE_WITH_OPENCL", False),
var/spack/repos/builtin/packages/clinfo/package.py:    """Print all known information about all available OpenCL platforms and
var/spack/repos/builtin/packages/clinfo/package.py:    depends_on("opencl")
var/spack/repos/builtin/packages/rocm-core/package.py:class RocmCore(CMakePackage):
var/spack/repos/builtin/packages/rocm-core/package.py:    """rocm-core is a utility which can be used to get ROCm release version.
var/spack/repos/builtin/packages/rocm-core/package.py:    It also provides the Lmod modules files for the ROCm release.
var/spack/repos/builtin/packages/rocm-core/package.py:    getROCmVersion function provides the ROCm version."""
var/spack/repos/builtin/packages/rocm-core/package.py:    homepage = "https://github.com/ROCm/rocm-core"
var/spack/repos/builtin/packages/rocm-core/package.py:    url = "https://github.com/ROCm/rocm-core/archive/refs/tags/rocm-6.0.0.tar.gz"
var/spack/repos/builtin/packages/rocm-core/package.py:    tags = ["rocm"]
var/spack/repos/builtin/packages/rocm-core/package.py:    libraries = ["librocm-core"]
var/spack/repos/builtin/packages/rocm-core/package.py:        depends_on("llvm-amdgpu", when=f"@{ver}+asan")
var/spack/repos/builtin/packages/rocm-core/package.py:            env.set("CC", self.spec["llvm-amdgpu"].prefix + "/bin/clang")
var/spack/repos/builtin/packages/rocm-core/package.py:            env.set("CXX", self.spec["llvm-amdgpu"].prefix + "/bin/clang++")
var/spack/repos/builtin/packages/rocm-core/package.py:        args = [self.define("ROCM_VERSION", self.spec.version)]
var/spack/repos/builtin/packages/spiral-package-simt/package.py:    threads, is used to generate code for GPUs and multi-threading aplications."""
var/spack/repos/builtin/packages/hip-examples/0001-add-inc-and-lib-paths-to-openmp-helloworld.patch:+CXXFLAGS += -I$(ROCM_OPENMP_EXTRAS_DIR)/include
var/spack/repos/builtin/packages/hip-examples/0001-add-inc-and-lib-paths-to-openmp-helloworld.patch:+CXXFLAGS += -L$(ROCM_OPENMP_EXTRAS_DIR)/lib
var/spack/repos/builtin/packages/hip-examples/package.py:    homepage = "https://github.com/ROCm/HIP-Examples/"
var/spack/repos/builtin/packages/hip-examples/package.py:    git = "https://github.com/ROCm/HIP-Examples.git"
var/spack/repos/builtin/packages/hip-examples/package.py:    url = "https://github.com/ROCm/HIP-Examples/archive/rocm-5.4.3.tar.gz"
var/spack/repos/builtin/packages/hip-examples/package.py:    tags = ["rocm"]
var/spack/repos/builtin/packages/hip-examples/package.py:        depends_on("rocm-openmp-extras@" + ver, when="@" + ver)
var/spack/repos/builtin/packages/hip-examples/package.py:        os.environ["ROCM_OPENMP_EXTRAS_DIR"] = self.spec["rocm-openmp-extras"].prefix
var/spack/repos/builtin/packages/hip-examples/package.py:        os.environ["LD_LIBRARY_PATH"] = self.spec["rocm-openmp-extras"].prefix.lib
var/spack/repos/builtin/packages/hip-examples/package.py:        os.environ["LD_LIBRARY_PATH"] = self.spec["rocm-openmp-extras"].prefix.lib
var/spack/repos/builtin/packages/hip-examples/0002-add-fpic-compile-to-add4.patch: all:  gpu-stream-hip
var/spack/repos/builtin/packages/arbor/package.py:class Arbor(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/arbor/package.py:        "gpu_rng",
var/spack/repos/builtin/packages/arbor/package.py:        description="Use GPU generated random numbers -- not bitwise equal to CPU version",
var/spack/repos/builtin/packages/arbor/package.py:        when="+cuda",
var/spack/repos/builtin/packages/arbor/package.py:    with when("+cuda"):
var/spack/repos/builtin/packages/arbor/package.py:        depends_on("cuda@10:")
var/spack/repos/builtin/packages/arbor/package.py:        depends_on("cuda@11:", when="@0.7.1:")
var/spack/repos/builtin/packages/arbor/package.py:        if self.spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/arbor/package.py:                    self.define("ARB_GPU", "cuda"),
var/spack/repos/builtin/packages/arbor/package.py:                    self.define_from_variant("ARB_USE_GPU_RNG", "gpu_rng"),
var/spack/repos/builtin/packages/dlib/package.py:class Dlib(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/dlib/package.py:    depends_on("cuda@7.5:", when="+cuda")
var/spack/repos/builtin/packages/dlib/package.py:    depends_on("cudnn", when="+cuda")
var/spack/repos/builtin/packages/dlib/package.py:    # depends on the deprecated FindCUDA module dependency as of 19.24.4
var/spack/repos/builtin/packages/dlib/package.py:    # when cuda is enabled
var/spack/repos/builtin/packages/dlib/package.py:    depends_on("cmake@:3.26", when="+cuda")
var/spack/repos/builtin/packages/dlib/package.py:            self.define_from_variant("DLIB_USE_CUDA", "cuda"),
var/spack/repos/builtin/packages/dlib/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/dlib/package.py:                    "DLIB_USE_CUDA_COMPUTE_CAPABILITIES", self.spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/py-torchaudio/package.py:    conflicts("^cuda@12.5:", when="@:2.1")
var/spack/repos/builtin/packages/py-torchaudio/package.py:        if "+cuda" in self.spec["py-torch"]:
var/spack/repos/builtin/packages/py-torchaudio/package.py:            env.set("USE_CUDA", 1)
var/spack/repos/builtin/packages/py-torchaudio/package.py:            env.set("USE_CUDA", 0)
var/spack/repos/builtin/packages/py-torchaudio/package.py:        if "+rocm" in self.spec["py-torch"]:
var/spack/repos/builtin/packages/py-torchaudio/package.py:            env.set("USE_ROCM", 1)
var/spack/repos/builtin/packages/py-torchaudio/package.py:            env.set("USE_ROCM", 0)
var/spack/repos/builtin/packages/accfft/package.py:class Accfft(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/accfft/package.py:    """AccFFT extends existing FFT libraries for CUDA-enabled
var/spack/repos/builtin/packages/accfft/package.py:    Graphics Processing Units (GPUs) to distributed memory clusters
var/spack/repos/builtin/packages/accfft/package.py:            self.define("BUILD_GPU", str(spec.satisfies("+cuda")).lower()),
var/spack/repos/builtin/packages/accfft/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/accfft/package.py:            cuda_arch = [x for x in spec.variants["cuda_arch"].value if x]
var/spack/repos/builtin/packages/accfft/package.py:            if cuda_arch:
var/spack/repos/builtin/packages/accfft/package.py:                args.append(f"-DCUDA_NVCC_FLAGS={' '.join(self.cuda_flags(cuda_arch))}")
var/spack/repos/builtin/packages/opensubdiv/package.py:class Opensubdiv(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/opensubdiv/package.py:    evaluation on massively parallel CPU and GPU architectures.
var/spack/repos/builtin/packages/opensubdiv/package.py:        args.append("-DNO_OPENCL=1")  # disable OpenCL
var/spack/repos/builtin/packages/opensubdiv/package.py:        if "+cuda" in spec:
var/spack/repos/builtin/packages/opensubdiv/package.py:            args.append("-DNO_CUDA=0")
var/spack/repos/builtin/packages/opensubdiv/package.py:            cuda_arch = [x for x in spec.variants["cuda_arch"].value if x]
var/spack/repos/builtin/packages/opensubdiv/package.py:            if cuda_arch:
var/spack/repos/builtin/packages/opensubdiv/package.py:                    "-DOSD_CUDA_NVCC_FLAGS={0}".format(" ".join(self.cuda_flags(cuda_arch)))
var/spack/repos/builtin/packages/opensubdiv/package.py:                args.append("-DOSD_CUDA_NVCC_FLAGS=")
var/spack/repos/builtin/packages/opensubdiv/package.py:            args.append("-DNO_CUDA=1")
var/spack/repos/builtin/packages/rocblas/package.py:    homepage = "https://github.com/ROCm/rocBLAS/"
var/spack/repos/builtin/packages/rocblas/package.py:    git = "https://github.com/ROCm/rocBLAS.git"
var/spack/repos/builtin/packages/rocblas/package.py:    url = "https://github.com/ROCm/rocBLAS/archive/rocm-6.1.1.tar.gz"
var/spack/repos/builtin/packages/rocblas/package.py:    tags = ["rocm"]
var/spack/repos/builtin/packages/rocblas/package.py:    amdgpu_targets = ROCmPackage.amdgpu_targets
var/spack/repos/builtin/packages/rocblas/package.py:        "amdgpu_target",
var/spack/repos/builtin/packages/rocblas/package.py:        description="AMD GPU architecture",
var/spack/repos/builtin/packages/rocblas/package.py:        values=auto_or_any_combination_of(*amdgpu_targets),
var/spack/repos/builtin/packages/rocblas/package.py:    # https://github.com/ROCm/HIP/issues/2678
var/spack/repos/builtin/packages/rocblas/package.py:    # https://github.com/ROCm/hipamd/blob/rocm-5.2.x/include/hip/amd_detail/host_defines.h#L50
var/spack/repos/builtin/packages/rocblas/package.py:        depends_on(f"rocm-openmp-extras@{ver}", type="test", when=f"@{ver}")
var/spack/repos/builtin/packages/rocblas/package.py:        depends_on(f"rocm-smi-lib@{ver}", type="test", when=f"@{ver}")
var/spack/repos/builtin/packages/rocblas/package.py:    depends_on("rocm-cmake@master", type="build", when="@master:")
var/spack/repos/builtin/packages/rocblas/package.py:        depends_on(f"llvm-amdgpu@{ver}", type="build", when=f"@{ver}")
var/spack/repos/builtin/packages/rocblas/package.py:        depends_on(f"rocminfo@{ver}", type="build", when=f"@{ver}")
var/spack/repos/builtin/packages/rocblas/package.py:        depends_on(f"rocm-cmake@{ver}", type="build", when=f"@{ver}")
var/spack/repos/builtin/packages/rocblas/package.py:            git="https://github.com/ROCm/Tensile.git",
var/spack/repos/builtin/packages/rocblas/package.py:            git="https://github.com/ROCm/Tensile.git",
var/spack/repos/builtin/packages/rocblas/package.py:    patch("0007-add-rocm-openmp-extras-include-dir.patch", when="@5.6:5.7")
var/spack/repos/builtin/packages/rocblas/package.py:            env.set("CC", f"{self.spec['llvm-amdgpu'].prefix}/bin/clang")
var/spack/repos/builtin/packages/rocblas/package.py:            env.set("CXX", f"{self.spec['llvm-amdgpu'].prefix}/bin/clang++")
var/spack/repos/builtin/packages/rocblas/package.py:                    self.define("ROCM_OPENMP_EXTRAS_DIR", self.spec["rocm-openmp-extras"].prefix)
var/spack/repos/builtin/packages/rocblas/package.py:            # https://github.com/ROCm/Tensile/blob/93e10678a0ced7843d9332b80bc17ebf9a166e8e/Tensile/Parallel.py#L38
var/spack/repos/builtin/packages/rocblas/package.py:        # See https://github.com/ROCm/rocBLAS/commit/c1895ba4bb3f4f5947f3818ebd155cf71a27b634
var/spack/repos/builtin/packages/rocblas/package.py:        if "auto" not in self.spec.variants["amdgpu_target"]:
var/spack/repos/builtin/packages/rocblas/package.py:            args.append(self.define_from_variant("AMDGPU_TARGETS", "amdgpu_target"))
var/spack/repos/builtin/packages/rocblas/package.py:        # See https://github.com/ROCm/rocBLAS/issues/1196
var/spack/repos/builtin/packages/rocblas/package.py:            args.append(self.define("__skip_rocmclang", "ON"))
var/spack/repos/builtin/packages/rocblas/0007-add-rocm-openmp-extras-include-dir.patch:+    $<BUILD_INTERFACE:${ROCM_OPENMP_EXTRAS_DIR}/include>
var/spack/repos/builtin/packages/rocblas/0007-add-rocm-openmp-extras-include-dir.patch:+    $<BUILD_INTERFACE:${ROCM_OPENMP_EXTRAS_DIR}/include>
var/spack/repos/builtin/packages/profugusmc/package.py:class Profugusmc(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/profugusmc/package.py:    variant("cuda", default=False, description="Enable CUDA")
var/spack/repos/builtin/packages/profugusmc/package.py:    depends_on("cuda", when="+cuda")
var/spack/repos/builtin/packages/heimdall/package.py:class Heimdall(AutotoolsPackage, CudaPackage):
var/spack/repos/builtin/packages/heimdall/package.py:    """GPU accelerated transient detection pipeline"""
var/spack/repos/builtin/packages/heimdall/package.py:    conflicts("~cuda", msg="You must specify +cuda")
var/spack/repos/builtin/packages/heimdall/package.py:    conflicts("cuda@11.8")
var/spack/repos/builtin/packages/heimdall/package.py:    conflicts("cuda_arch=none", msg="You must specify the CUDA architecture")
var/spack/repos/builtin/packages/heimdall/package.py:    depends_on("cuda")
var/spack/repos/builtin/packages/heimdall/package.py:    # Pass the cuda architecture to DEDISP and PSRDADA for building
var/spack/repos/builtin/packages/heimdall/package.py:    for arch in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/heimdall/package.py:        depends_on(f"dedisp cuda_arch={arch}", when=f"cuda_arch={arch}")
var/spack/repos/builtin/packages/heimdall/package.py:        depends_on(f"psrdada cuda_arch={arch}", when=f"cuda_arch={arch}")
var/spack/repos/builtin/packages/heimdall/package.py:        env.prepend_path("LD_LIBRARY_PATH", self.spec["cuda"].prefix.lib)
var/spack/repos/builtin/packages/heimdall/package.py:            f"--with-cuda-dir={self.spec['cuda'].prefix}",
var/spack/repos/builtin/packages/py-pennylane-lightning-kokkos/package.py:class PyPennylaneLightningKokkos(CMakePackage, PythonExtension, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/py-pennylane-lightning-kokkos/package.py:        "cuda": [False, "Whether to build CUDA backend"],
var/spack/repos/builtin/packages/py-pennylane-lightning-kokkos/package.py:        "rocm": [False, "Whether to build HIP backend"],
var/spack/repos/builtin/packages/py-pennylane-lightning-kokkos/package.py:    # CUDA
var/spack/repos/builtin/packages/py-pennylane-lightning-kokkos/package.py:    for val in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/py-pennylane-lightning-kokkos/package.py:        depends_on("kokkos cuda_arch={0}".format(val), when="cuda_arch={0}".format(val))
var/spack/repos/builtin/packages/py-pennylane-lightning-kokkos/package.py:    depends_on("kokkos+wrapper", when="%gcc+cuda")
var/spack/repos/builtin/packages/py-pennylane-lightning-kokkos/package.py:    # ROCm
var/spack/repos/builtin/packages/py-pennylane-lightning-kokkos/package.py:    for val in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/py-pennylane-lightning-kokkos/package.py:        depends_on("kokkos amdgpu_target={0}".format(val), when="amdgpu_target={0}".format(val))
var/spack/repos/builtin/packages/py-pennylane-lightning-kokkos/package.py:        "+cuda",
var/spack/repos/builtin/packages/py-pennylane-lightning-kokkos/package.py:        when="+rocm",
var/spack/repos/builtin/packages/py-pennylane-lightning-kokkos/package.py:        msg="CUDA and ROCm are not compatible in PennyLane-Lightning-Kokkos.",
var/spack/repos/builtin/packages/py-pennylane-lightning-kokkos/package.py:        if "+rocm" in self.spec:
var/spack/repos/builtin/packages/visit/package.py:    depends_on("vtk-m@1.7.0+testlib~cuda", when="@3.3.0:3.3.2+vtkm")
var/spack/repos/builtin/packages/visit/package.py:    depends_on("vtk-h@0.8.1+shared~mpi~openmp~cuda", when="@3.3.0:3.3.2+vtkm")
var/spack/repos/builtin/packages/visit/package.py:    depends_on("vtk-m@1.9.0+testlib~cuda", when="@3.3.3:+vtkm")
var/spack/repos/builtin/packages/visit/package.py:            if self.spec.satisfies("^vtkm+rocm"):
var/spack/repos/builtin/packages/ginkgo/CAS-HIP-NVCC-1.2.0.patch:-if(GINKGO_BUILD_CUDA)
var/spack/repos/builtin/packages/ginkgo/CAS-HIP-NVCC-1.2.0.patch:+if(GINKGO_BUILD_CUDA OR (GINKGO_BUILD_HIP AND GINKGO_HIP_PLATFORM STREQUAL "nvcc"))
var/spack/repos/builtin/packages/ginkgo/CAS-HIP-NVCC-1.2.0.patch:     enable_language(CUDA)
var/spack/repos/builtin/packages/ginkgo/CAS-HIP-NVCC-1.2.0.patch:         include(CudaArchitectureSelector RESULT_VARIABLE GINKGO_CAS_FILE)
var/spack/repos/builtin/packages/ginkgo/package.py:class Ginkgo(CMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/ginkgo/package.py:    depends_on("cmake@3.18:", type="build", when="+cuda@1.7.0:")
var/spack/repos/builtin/packages/ginkgo/package.py:    depends_on("cmake@3.21:", type="build", when="+rocm@1.8.0:")
var/spack/repos/builtin/packages/ginkgo/package.py:    depends_on("cuda@9:", when="+cuda @:1.4.0")
var/spack/repos/builtin/packages/ginkgo/package.py:    depends_on("cuda@9.2:", when="+cuda @1.5.0:")
var/spack/repos/builtin/packages/ginkgo/package.py:    depends_on("cuda@10.1:", when="+cuda @1.7.0:")
var/spack/repos/builtin/packages/ginkgo/package.py:    depends_on("rocthrust", when="+rocm")
var/spack/repos/builtin/packages/ginkgo/package.py:    depends_on("hipsparse", when="+rocm")
var/spack/repos/builtin/packages/ginkgo/package.py:    depends_on("hipblas", when="+rocm")
var/spack/repos/builtin/packages/ginkgo/package.py:    depends_on("rocrand", when="+rocm")
var/spack/repos/builtin/packages/ginkgo/package.py:    depends_on("hiprand", when="+rocm")
var/spack/repos/builtin/packages/ginkgo/package.py:    depends_on("hipfft", when="+rocm")
var/spack/repos/builtin/packages/ginkgo/package.py:    depends_on("rocprim", when="+rocm")
var/spack/repos/builtin/packages/ginkgo/package.py:    conflicts("+rocm", when="@:1.1.1")
var/spack/repos/builtin/packages/ginkgo/package.py:    # ROCm 4.1.0 breaks platform settings which breaks Ginkgo's HIP support.
var/spack/repos/builtin/packages/ginkgo/package.py:    # Ginkgo 1.6.0 start relying on ROCm 4.5.0
var/spack/repos/builtin/packages/ginkgo/package.py:    # Probably fixed in NVIDIA/cccl#1528 which hopefully comes with the next CUDA release
var/spack/repos/builtin/packages/ginkgo/package.py:    conflicts("^cuda@12.4", when="+cuda", msg="CCCL 2.3 bug causes build failure.")
var/spack/repos/builtin/packages/ginkgo/package.py:    patch("thrust-count-header.patch", when="+rocm @1.5.0")
var/spack/repos/builtin/packages/ginkgo/package.py:        when="+rocm @1.8.0",
var/spack/repos/builtin/packages/ginkgo/package.py:            from_variant("GINKGO_BUILD_CUDA", "cuda"),
var/spack/repos/builtin/packages/ginkgo/package.py:            from_variant("GINKGO_BUILD_HIP", "rocm"),
var/spack/repos/builtin/packages/ginkgo/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/ginkgo/package.py:            archs = spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/ginkgo/package.py:                args.append("-DGINKGO_CUDA_ARCHITECTURES={0}".format(arch_str))
var/spack/repos/builtin/packages/ginkgo/package.py:        if spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/ginkgo/package.py:            args.append("-DHIP_CLANG_PATH={0}/bin".format(spec["llvm-amdgpu"].prefix))
var/spack/repos/builtin/packages/ginkgo/package.py:            args.append("-DHIP_CLANG_INCLUDE_PATH={0}/include".format(spec["llvm-amdgpu"].prefix))
var/spack/repos/builtin/packages/ginkgo/package.py:            archs = self.spec.variants["amdgpu_target"].value
var/spack/repos/builtin/packages/ginkgo/package.py:                args.append("-DGINKGO_HIP_AMDGPU={0}".format(arch_str))
var/spack/repos/builtin/packages/ginkgo/package.py:        if self.spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/ginkgo/package.py:    def test_install_cuda(self):
var/spack/repos/builtin/packages/ginkgo/package.py:        """build, run and check results of test_install_cuda"""
var/spack/repos/builtin/packages/ginkgo/package.py:        if not self.spec.satisfies("@1.4.0: +cuda"):
var/spack/repos/builtin/packages/ginkgo/package.py:            raise SkipTest("Test is only available for v1.4.0: +cuda")
var/spack/repos/builtin/packages/ginkgo/package.py:        self._build_and_run_test("test_install_cuda")
var/spack/repos/builtin/packages/ginkgo/package.py:        if not self.spec.satisfies("@1.4.0: +rocm"):
var/spack/repos/builtin/packages/ginkgo/package.py:            raise SkipTest("Test is only available for v1.4.0: +rocm")
var/spack/repos/builtin/packages/ginkgo/1.4.0_skip_invalid_smoke_tests.patch: #if defined(HAS_HIP) || defined(HAS_CUDA)
var/spack/repos/builtin/packages/ginkgo/1.4.0_skip_invalid_smoke_tests.patch: #if defined(HAS_CUDA)
var/spack/repos/builtin/packages/ginkgo/1.4.0_skip_invalid_smoke_tests.patch:-    auto exec = gko::CudaExecutor::create(0, gko::ReferenceExecutor::create());
var/spack/repos/builtin/packages/ginkgo/1.4.0_skip_invalid_smoke_tests.patch:+    auto extra_info = "(CUDA)";
var/spack/repos/builtin/packages/ginkgo/1.4.0_skip_invalid_smoke_tests.patch:+    using exec_type = gko::CudaExecutor;
var/spack/repos/builtin/packages/ginkgo/1.4.0_skip_invalid_smoke_tests.patch:+#if defined(HAS_CUDA) || defined(HAS_HIP)
var/spack/repos/builtin/packages/ginkgo/1.4.0_skip_invalid_smoke_tests.patch:-#if defined(HAS_CUDA)
var/spack/repos/builtin/packages/ginkgo/1.4.0_skip_invalid_smoke_tests.patch:-    auto extra_info = "(CUDA)";
var/spack/repos/builtin/packages/hwloc/package.py:class Hwloc(AutotoolsPackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/hwloc/package.py:    interfaces, InfiniBand HCAs or GPUs. It primarily aims at
var/spack/repos/builtin/packages/hwloc/package.py:    variant("opencl", default=False, description="Support an OpenCL library at run time")
var/spack/repos/builtin/packages/hwloc/package.py:    variant("rocm", default=False, description="Support ROCm devices")
var/spack/repos/builtin/packages/hwloc/package.py:    depends_on("cuda", when="+nvml")
var/spack/repos/builtin/packages/hwloc/package.py:    depends_on("cuda", when="+cuda")
var/spack/repos/builtin/packages/hwloc/package.py:    with when("+rocm"):
var/spack/repos/builtin/packages/hwloc/package.py:        depends_on("rocm-smi-lib")
var/spack/repos/builtin/packages/hwloc/package.py:        depends_on("rocm-opencl", when="+opencl")
var/spack/repos/builtin/packages/hwloc/package.py:        # variant of llvm-amdgpu depends on hwloc.
var/spack/repos/builtin/packages/hwloc/package.py:        depends_on("llvm-amdgpu", when="+opencl")
var/spack/repos/builtin/packages/hwloc/package.py:        # If OpenCL is not enabled, disable it since hwloc might
var/spack/repos/builtin/packages/hwloc/package.py:        # pick up an OpenCL library at build time that is then
var/spack/repos/builtin/packages/hwloc/package.py:        # The OpenCl variant allows OpenCl providers such as
var/spack/repos/builtin/packages/hwloc/package.py:        # 'cuda' and 'rocm-opencl' to be used.
var/spack/repos/builtin/packages/hwloc/package.py:        if "+opencl" not in self.spec:
var/spack/repos/builtin/packages/hwloc/package.py:            args.append("--disable-opencl")
var/spack/repos/builtin/packages/hwloc/package.py:        # If ROCm libraries are found in system /opt/rocm
var/spack/repos/builtin/packages/hwloc/package.py:        # librocm_smi support.
var/spack/repos/builtin/packages/hwloc/package.py:        # OpenMPI due to lack of rpath to librocm_smi
var/spack/repos/builtin/packages/hwloc/package.py:        if "+rocm" not in self.spec:
var/spack/repos/builtin/packages/hwloc/package.py:        if self.spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/hwloc/package.py:            args.append("--with-rocm={0}".format(self.spec["hip"].prefix))
var/spack/repos/builtin/packages/hwloc/package.py:            args.append("--with-rocm-version={0}".format(self.spec["hip"].version))
var/spack/repos/builtin/packages/hwloc/package.py:        args.extend(self.enable_or_disable("cuda"))
var/spack/repos/builtin/packages/hwloc/package.py:        if self.spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/hwloc/package.py:            args.append("--with-cuda={0}".format(self.spec["cuda"].prefix))
var/spack/repos/builtin/packages/hwloc/package.py:            args.append("--with-cuda-version={0}".format(self.spec["cuda"].version))
var/spack/repos/builtin/packages/bison/package.py:    # The NVIDIA compilers do not currently support some GNU builtins.
var/spack/repos/builtin/packages/libxcb/package.py:        # -Werror flags are not properly interpreted by the NVIDIA compiler
var/spack/repos/builtin/packages/vecmem/package.py:class Vecmem(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/vecmem/package.py:            self.define_from_variant("VECMEM_BUILD_CUDA_LIBRARY", "cuda"),
var/spack/repos/builtin/packages/vecmem/package.py:        if "+cuda" in self.spec:
var/spack/repos/builtin/packages/vecmem/package.py:            cuda_arch_list = self.spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/vecmem/package.py:            cuda_arch = cuda_arch_list[0]
var/spack/repos/builtin/packages/vecmem/package.py:            if cuda_arch != "none":
var/spack/repos/builtin/packages/vecmem/package.py:                args.append("-DCUDA_FLAGS=-arch=sm_{0}".format(cuda_arch))
var/spack/repos/builtin/packages/libmolgrid/package.py:    depends_on("cuda@11")
var/spack/repos/builtin/packages/xsdk/package.py:    if accl_name == "cuda":
var/spack/repos/builtin/packages/xsdk/package.py:        accl_arch_name = "cuda_arch"
var/spack/repos/builtin/packages/xsdk/package.py:        accl_arch_values = list(deepcopy(CudaPackage.cuda_arch_values))
var/spack/repos/builtin/packages/xsdk/package.py:    elif accl_name == "rocm":
var/spack/repos/builtin/packages/xsdk/package.py:        accl_arch_name = "amdgpu_target"
var/spack/repos/builtin/packages/xsdk/package.py:        accl_arch_values = list(deepcopy(ROCmPackage.amdgpu_targets))
var/spack/repos/builtin/packages/xsdk/package.py:    # require ~cuda when xsdk~cuda (and '?cuda' not used)
var/spack/repos/builtin/packages/xsdk/package.py:        # if '?cuda' skip adding '~cuda' dep
var/spack/repos/builtin/packages/xsdk/package.py:    # require +cuda when xsdk+cuda, and match the arch
var/spack/repos/builtin/packages/xsdk/package.py:def xsdk_depends_on(spec, cuda_var="", rocm_var="", **kwargs):
var/spack/repos/builtin/packages/xsdk/package.py:    Wrapper for depends_on which can handle propagating cuda and rocm
var/spack/repos/builtin/packages/xsdk/package.py:    Currently, it propagates +cuda_var when xsdk+cuda and rocm_var
var/spack/repos/builtin/packages/xsdk/package.py:    when xsdk+rocm. When xsdk~[cuda|rocm], then ~[cuda|rocm]_var is
var/spack/repos/builtin/packages/xsdk/package.py:    [cuda|rocm]_var can be an array of variant strings or just a single
var/spack/repos/builtin/packages/xsdk/package.py:    if bool(cuda_var):
var/spack/repos/builtin/packages/xsdk/package.py:        xsdk_depends_on_accl("cuda", cuda_var, spec, **kwargs)
var/spack/repos/builtin/packages/xsdk/package.py:    if bool(rocm_var):
var/spack/repos/builtin/packages/xsdk/package.py:        xsdk_depends_on_accl("rocm", rocm_var, spec, **kwargs)
var/spack/repos/builtin/packages/xsdk/package.py:    if not bool(cuda_var) and not bool(rocm_var):
var/spack/repos/builtin/packages/xsdk/package.py:class Xsdk(BundlePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/xsdk/package.py:        "hypre@2.30.0+superlu-dist+shared", when="@1.0.0", cuda_var="cuda", rocm_var="rocm"
var/spack/repos/builtin/packages/xsdk/package.py:    xsdk_depends_on("hypre@2.26.0+superlu-dist+shared", when="@0.8.0", cuda_var="cuda")
var/spack/repos/builtin/packages/xsdk/package.py:        cuda_var="cuda",
var/spack/repos/builtin/packages/xsdk/package.py:        rocm_var="rocm",
var/spack/repos/builtin/packages/xsdk/package.py:        cuda_var="cuda",
var/spack/repos/builtin/packages/xsdk/package.py:        rocm_var="rocm",
var/spack/repos/builtin/packages/xsdk/package.py:    xsdk_depends_on("superlu-dist@8.2.1", when="@1.0.0", cuda_var="cuda", rocm_var="rocm")
var/spack/repos/builtin/packages/xsdk/package.py:    xsdk_depends_on("trilinos +superlu-dist", when="@1.0.0: +trilinos ~cuda ~rocm")
var/spack/repos/builtin/packages/xsdk/package.py:        cuda_var="cuda",
var/spack/repos/builtin/packages/xsdk/package.py:        rocm_var="rocm",
var/spack/repos/builtin/packages/xsdk/package.py:        cuda_var="cuda",
var/spack/repos/builtin/packages/xsdk/package.py:        rocm_var="rocm",
var/spack/repos/builtin/packages/xsdk/package.py:        cuda_var=["cuda", "?magma"],
var/spack/repos/builtin/packages/xsdk/package.py:        rocm_var=["rocm", "?magma"],
var/spack/repos/builtin/packages/xsdk/package.py:        cuda_var=["cuda", "?magma"],
var/spack/repos/builtin/packages/xsdk/package.py:        rocm_var=["rocm", "?magma"],
var/spack/repos/builtin/packages/xsdk/package.py:    xsdk_depends_on("magma@2.7.1", when="@1.0.0", cuda_var="?cuda", rocm_var="?rocm")
var/spack/repos/builtin/packages/xsdk/package.py:    xsdk_depends_on("magma@2.7.0", when="@0.8.0", cuda_var="?cuda", rocm_var="?rocm")
var/spack/repos/builtin/packages/xsdk/package.py:    xsdk_depends_on("amrex@23.08+sundials", when="@1.0.0 +amrex", cuda_var="cuda", rocm_var="rocm")
var/spack/repos/builtin/packages/xsdk/package.py:    xsdk_depends_on("amrex@22.09+sundials", when="@0.8.0 +amrex", cuda_var="cuda", rocm_var="rocm")
var/spack/repos/builtin/packages/xsdk/package.py:    xsdk_depends_on("slepc@3.20.0", when="@1.0.0", cuda_var="cuda", rocm_var="rocm")
var/spack/repos/builtin/packages/xsdk/package.py:    xsdk_depends_on("slepc@3.18.1", when="@0.8.0", cuda_var="cuda", rocm_var="rocm")
var/spack/repos/builtin/packages/xsdk/package.py:    xsdk_depends_on("strumpack ~cuda", when="~cuda +strumpack")
var/spack/repos/builtin/packages/xsdk/package.py:    xsdk_depends_on("strumpack@7.2.0", when="@1.0.0 +strumpack", cuda_var=["cuda"])
var/spack/repos/builtin/packages/xsdk/package.py:    xsdk_depends_on("strumpack@7.0.1", when="@0.8.0 +strumpack", cuda_var=["cuda"])
var/spack/repos/builtin/packages/xsdk/package.py:        "tasmanian@8.0+mpi+blas" + tasmanian_openmp, when="@1.0.0", cuda_var=["cuda", "?magma"]
var/spack/repos/builtin/packages/xsdk/package.py:        cuda_var=["cuda", "?magma"],
var/spack/repos/builtin/packages/xsdk/package.py:        "ginkgo@1.7.0 +mpi ~openmp", when="@1.0.0 +ginkgo", cuda_var="cuda", rocm_var="rocm"
var/spack/repos/builtin/packages/xsdk/package.py:        "ginkgo@1.5.0 +mpi ~openmp", when="@0.8.0 +ginkgo", cuda_var="cuda", rocm_var="rocm"
var/spack/repos/builtin/packages/xsdk/package.py:        cuda_var=["cuda", "?magma"],
var/spack/repos/builtin/packages/xsdk/package.py:        rocm_var=["rocm", "?magma"],
var/spack/repos/builtin/packages/xsdk/package.py:        cuda_var=["cuda", "?magma"],
var/spack/repos/builtin/packages/xsdk/package.py:        rocm_var=["rocm", "?magma"],
var/spack/repos/builtin/packages/xsdk/package.py:    xsdk_depends_on("slate@2023.08.25", when="@1.0.0 +slate", cuda_var="cuda")
var/spack/repos/builtin/packages/xsdk/package.py:    xsdk_depends_on("slate@2022.07.00", when="@0.8.0 +slate", cuda_var="cuda")
var/spack/repos/builtin/packages/xsdk/package.py:    xsdk_depends_on("exago@1.6.0~ipopt+hiop+raja", when="@1.0.0 +exago +raja", cuda_var="cuda")
var/spack/repos/builtin/packages/xsdk/package.py:    xsdk_depends_on("exago@1.5.0~ipopt+hiop+raja", when="@0.8.0 +exago +raja", cuda_var="cuda")
var/spack/repos/builtin/packages/xsdk/package.py:    xsdk_depends_on("hiop@1.0.0+raja", when="@1.0.0 +hiop +raja", cuda_var="cuda")
var/spack/repos/builtin/packages/xsdk/package.py:    xsdk_depends_on("hiop@0.7.1+raja", when="@0.8.0 +hiop +raja", cuda_var="cuda")
var/spack/repos/builtin/packages/boost/boost_1.63.0_pgi.patch:-    || (BOOST_MPL_CFG_GCC != 0) || (BOOST_MPL_CFG_GPU != 0)
var/spack/repos/builtin/packages/boost/boost_1.63.0_pgi.patch:+    || (BOOST_MPL_CFG_GCC != 0) || (BOOST_MPL_CFG_GPU != 0) || defined(__PGI)
var/spack/repos/builtin/packages/boost/package.py:    # Patch to override the PGI toolset when using the NVIDIA compilers
var/spack/repos/builtin/packages/boost/package.py:    # Fix float128 support when building with CUDA and Cray compiler
var/spack/repos/builtin/packages/boost/package.py:        # Disable SSSE3 and AVX2 when using the NVIDIA compiler
var/spack/repos/builtin/packages/boost/xl_1_62_0_le.patch:-#    if defined __GCCXML__ || defined __CUDACC__ || defined __PATHSCALE__ || defined __DMC__ || defined __CODEGEARC__ || defined __BORLANDC__ || defined __MWERKS__ || ( defined __SUNPRO_CC && __SUNPRO_CC < 0x5120 ) || defined __HP_aCC && !defined __EDG__ || defined __MRC__ || defined __SC__ || defined __IBMCPP__ || defined __PGI
var/spack/repos/builtin/packages/boost/xl_1_62_0_le.patch:+#    if defined __GCCXML__ || defined __CUDACC__ || defined __PATHSCALE__ || defined __DMC__ || defined __CODEGEARC__ || defined __BORLANDC__ || defined __MWERKS__ || ( defined __SUNPRO_CC && __SUNPRO_CC < 0x5120 ) || defined __HP_aCC && !defined __EDG__ || defined __MRC__ || defined __SC__ || (defined __IBMCPP__ && !defined __ibmxl__ ) || defined __PGI
var/spack/repos/builtin/packages/boost/xl_1_62_0_le.patch:-#    if defined _MSC_VER && _MSC_VER >= 1400 && (defined(__INTELLISENSE__) || !(defined __EDG__ || defined __GCCXML__ || defined __CUDACC__ || defined __PATHSCALE__ || defined __clang__ || defined __DMC__ || defined __CODEGEARC__ || defined __BORLANDC__ || defined __MWERKS__ || defined __SUNPRO_CC || defined __HP_aCC || defined __MRC__ || defined __SC__ || defined __IBMCPP__ || defined __PGI))
var/spack/repos/builtin/packages/boost/xl_1_62_0_le.patch:+#    if defined _MSC_VER && _MSC_VER >= 1400 && (defined(__INTELLISENSE__) || !(defined __EDG__ || defined __GCCXML__ || defined __CUDACC__ || defined __PATHSCALE__ || defined __clang__ || defined __DMC__ || defined __CODEGEARC__ || defined __BORLANDC__ || defined __MWERKS__ || defined __SUNPRO_CC || defined __HP_aCC || defined __MRC__ || defined __SC__ || (defined __IBMCPP__  && !defined __ibmxl__ ) || defined __PGI))
var/spack/repos/builtin/packages/hypre-cmake/package.py:class HypreCmake(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/hypre-cmake/package.py:    conflicts("+cuda", when="+int64")
var/spack/repos/builtin/packages/hypre-cmake/package.py:    conflicts("+unified_memory", when="~cuda")
var/spack/repos/builtin/packages/hypre-cmake/package.py:            from_variant("HYPRE_WITH_CUDA", "cuda"),
var/spack/repos/builtin/packages/hypre-cmake/package.py:        if self.spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/hypre-cmake/package.py:            env.set("CUDA_HOME", self.spec["cuda"].prefix)
var/spack/repos/builtin/packages/hypre-cmake/package.py:            env.set("CUDA_PATH", self.spec["cuda"].prefix)
var/spack/repos/builtin/packages/hypre-cmake/package.py:            cuda_arch = self.spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/hypre-cmake/package.py:            if cuda_arch:
var/spack/repos/builtin/packages/hypre-cmake/package.py:                arch_sorted = list(sorted(cuda_arch, reverse=True))
var/spack/repos/builtin/packages/hypre-cmake/package.py:                env.set("HYPRE_CUDA_SM", arch_sorted[0])
var/spack/repos/builtin/packages/hypre-cmake/package.py:            # In CUDA builds hypre currently doesn't handle flags correctly
var/spack/repos/builtin/packages/hypre-cmake/package.py:            r"LIBS = -L$(HYPRE_DIR)/lib64 -lHYPRE -lm $(CUDA_LIBS) $(DOMP_LIBS)",
var/spack/repos/builtin/packages/libxml2/package.py:        # Remove flags not recognized by the NVIDIA compiler
var/spack/repos/builtin/packages/cmake/pgi-cxx-ansi.patch:From: Tin Huynh <ahuynh@nvidia.com>
var/spack/repos/builtin/packages/hipcc/package.py:    homepage = "https://github.com/ROCm/hipcc"
var/spack/repos/builtin/packages/hipcc/package.py:    git = "https://github.com/ROCm/hipcc.git"
var/spack/repos/builtin/packages/hipcc/package.py:            url = "https://github.com/ROCm/HIPCC/archive/rocm-{0}.tar.gz"
var/spack/repos/builtin/packages/hipcc/package.py:            url = "https://github.com/ROCm/llvm-project/archive/rocm-{0}.tar.gz"
var/spack/repos/builtin/packages/hipcc/package.py:    patch("0001-Update-the-ROCMINFO-HIPCLANG-PATHS-inside-hipcc-6.2.0.patch", when="@6.2:")
var/spack/repos/builtin/packages/hipcc/0014-remove-compiler-rt-linkage-for-host.6.1.patch:             $targetsStr = $ENV{HCC_AMDGPU_TARGET};
var/spack/repos/builtin/packages/hipcc/0014-remove-compiler-rt-linkage-for-host.6.1.patch:             # Else try using rocm_agent_enumerator
var/spack/repos/builtin/packages/hipcc/0014-remove-compiler-rt-linkage-for-host.6.1.patch:-            $ROCM_AGENT_ENUM = "${ROCM_PATH}/bin/rocm_agent_enumerator";
var/spack/repos/builtin/packages/hipcc/0014-remove-compiler-rt-linkage-for-host.6.1.patch:+            $ROCMINFO_PATH = $ENV{'ROCMINFO_PATH'} // $ROCMINFO_PATH;
var/spack/repos/builtin/packages/hipcc/0014-remove-compiler-rt-linkage-for-host.6.1.patch:+            $ROCM_AGENT_ENUM = "${ROCMINFO_PATH}/bin/rocm_agent_enumerator";
var/spack/repos/builtin/packages/hipcc/0014-remove-compiler-rt-linkage-for-host.6.1.patch:             $targetsStr = `${ROCM_AGENT_ENUM} -t GPU`;
var/spack/repos/builtin/packages/hipcc/0014-remove-compiler-rt-linkage-for-host.6.0.patch:             $targetsStr = $ENV{HCC_AMDGPU_TARGET};
var/spack/repos/builtin/packages/hipcc/0014-remove-compiler-rt-linkage-for-host.6.0.patch:             # Else try using rocm_agent_enumerator
var/spack/repos/builtin/packages/hipcc/0014-remove-compiler-rt-linkage-for-host.6.0.patch:-            $ROCM_AGENT_ENUM = "${ROCM_PATH}/bin/rocm_agent_enumerator";
var/spack/repos/builtin/packages/hipcc/0014-remove-compiler-rt-linkage-for-host.6.0.patch:+            $ROCMINFO_PATH = $ENV{'ROCMINFO_PATH'} // $ROCMINFO_PATH;
var/spack/repos/builtin/packages/hipcc/0014-remove-compiler-rt-linkage-for-host.6.0.patch:+            $ROCM_AGENT_ENUM = "${ROCMINFO_PATH}/bin/rocm_agent_enumerator";
var/spack/repos/builtin/packages/hipcc/0014-remove-compiler-rt-linkage-for-host.6.0.patch:             $targetsStr = `${ROCM_AGENT_ENUM} -t GPU`;
var/spack/repos/builtin/packages/hipcc/0001-Update-the-ROCMINFO-HIPCLANG-PATHS-inside-hipcc-6.2.0.patch:Subject: [PATCH] Use the rocminfo_path and hipclang_path inside the hipcc.pl
var/spack/repos/builtin/packages/hipcc/0001-Update-the-ROCMINFO-HIPCLANG-PATHS-inside-hipcc-6.2.0.patch:             $targetsStr = $ENV{HCC_AMDGPU_TARGET};
var/spack/repos/builtin/packages/hipcc/0001-Update-the-ROCMINFO-HIPCLANG-PATHS-inside-hipcc-6.2.0.patch:             # Else try using rocm_agent_enumerator
var/spack/repos/builtin/packages/hipcc/0001-Update-the-ROCMINFO-HIPCLANG-PATHS-inside-hipcc-6.2.0.patch:-            $ROCM_AGENT_ENUM = "${ROCM_PATH}/bin/rocm_agent_enumerator";
var/spack/repos/builtin/packages/hipcc/0001-Update-the-ROCMINFO-HIPCLANG-PATHS-inside-hipcc-6.2.0.patch:+            $ROCMINFO_PATH = $ENV{'ROCMINFO_PATH'} // $ROCMINFO_PATH;
var/spack/repos/builtin/packages/hipcc/0001-Update-the-ROCMINFO-HIPCLANG-PATHS-inside-hipcc-6.2.0.patch:+            $ROCM_AGENT_ENUM = "${ROCMINFO_PATH}/bin/rocm_agent_enumerator";
var/spack/repos/builtin/packages/hipcc/0001-Update-the-ROCMINFO-HIPCLANG-PATHS-inside-hipcc-6.2.0.patch:             $targetsStr = `${ROCM_AGENT_ENUM} -t GPU`;
var/spack/repos/builtin/packages/hipcc/0001-Update-the-ROCMINFO-HIPCLANG-PATHS-inside-hipcc-6.2.0.patch:       // Else try using rocm_agent_enumerator
var/spack/repos/builtin/packages/hipcc/0001-Update-the-ROCMINFO-HIPCLANG-PATHS-inside-hipcc-6.2.0.patch:       string ROCM_AGENT_ENUM;
var/spack/repos/builtin/packages/hipcc/0001-Update-the-ROCMINFO-HIPCLANG-PATHS-inside-hipcc-6.2.0.patch:-      ROCM_AGENT_ENUM = roccmPath + "/bin/rocm_agent_enumerator";
var/spack/repos/builtin/packages/hipcc/0001-Update-the-ROCMINFO-HIPCLANG-PATHS-inside-hipcc-6.2.0.patch:+      ROCM_AGENT_ENUM = string(getenv("ROCMINFO_PATH")) + "/bin/rocm_agent_enumerator";
var/spack/repos/builtin/packages/hipcc/0001-Update-the-ROCMINFO-HIPCLANG-PATHS-inside-hipcc-6.2.0.patch:       targetsStr = ROCM_AGENT_ENUM +" -t GPU";
var/spack/repos/builtin/packages/py-pycuda/package.py:class PyPycuda(PythonPackage):
var/spack/repos/builtin/packages/py-pycuda/package.py:    """PyCUDA gives you easy, Pythonic access to Nvidia's CUDA parallel
var/spack/repos/builtin/packages/py-pycuda/package.py:    homepage = "https://mathema.tician.de/software/pycuda/"
var/spack/repos/builtin/packages/py-pycuda/package.py:    pypi = "pycuda/pycuda-2019.1.2.tar.gz"
var/spack/repos/builtin/packages/py-pycuda/package.py:    depends_on("cuda")
var/spack/repos/builtin/packages/py-pycuda/package.py:    depends_on("cuda@:8.0.61", when="@2016.1.2")
var/spack/repos/builtin/packages/ambertools/package.py:            self.define("CUDA", False),
var/spack/repos/builtin/packages/nvtop/package.py:class Nvtop(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/nvtop/package.py:    for AMD and NVIDIA GPUS. It can handle multiple GPUs and print
var/spack/repos/builtin/packages/nvtop/package.py:        values=("nvidia", "amd", "intel"),
var/spack/repos/builtin/packages/nvtop/package.py:        default="nvidia,amd,intel",
var/spack/repos/builtin/packages/nvtop/package.py:        description="Which GPU vendors to build support for",
var/spack/repos/builtin/packages/nvtop/package.py:            self.define("NVIDIA_SUPPORT", self.spec.satisfies("support=nvidia")),
var/spack/repos/builtin/packages/nvtop/package.py:            self.define("AMDGPU_SUPPORT", self.spec.satisfies("support=amd")),
var/spack/repos/builtin/packages/libgpuarray/package.py:class Libgpuarray(CMakePackage):
var/spack/repos/builtin/packages/libgpuarray/package.py:    """Make a common GPU ndarray(n dimensions array) that can be reused by all
var/spack/repos/builtin/packages/libgpuarray/package.py:    homepage = "https://github.com/Theano/libgpuarray"
var/spack/repos/builtin/packages/libgpuarray/package.py:    url = "https://github.com/Theano/libgpuarray/archive/v0.6.1.tar.gz"
var/spack/repos/builtin/packages/libgpuarray/package.py:    depends_on("cuda")
var/spack/repos/builtin/packages/adios2/package.py:class Adios2(CMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/adios2/package.py:    # Standalone CUDA support
var/spack/repos/builtin/packages/adios2/package.py:    depends_on("cuda", when="+cuda ~kokkos")
var/spack/repos/builtin/packages/adios2/package.py:        depends_on("kokkos +cuda +wrapper", when="+cuda")
var/spack/repos/builtin/packages/adios2/package.py:        depends_on("kokkos +rocm", when="+rocm")
var/spack/repos/builtin/packages/adios2/package.py:    # Propagate CUDA target to kokkos for +cuda
var/spack/repos/builtin/packages/adios2/package.py:    for cuda_arch in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/adios2/package.py:            "kokkos cuda_arch=%s" % cuda_arch, when="+kokkos +cuda cuda_arch=%s" % cuda_arch
var/spack/repos/builtin/packages/adios2/package.py:    # Propagate AMD GPU target to kokkos for +rocm
var/spack/repos/builtin/packages/adios2/package.py:    for amdgpu_value in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/adios2/package.py:            "kokkos amdgpu_target=%s" % amdgpu_value,
var/spack/repos/builtin/packages/adios2/package.py:            when="+kokkos +rocm amdgpu_target=%s" % amdgpu_value,
var/spack/repos/builtin/packages/adios2/package.py:    conflicts("+cuda", when="@:2.7")
var/spack/repos/builtin/packages/adios2/package.py:    conflicts("+rocm", when="@:2.8")
var/spack/repos/builtin/packages/adios2/package.py:    conflicts("+cuda", when="+sycl")
var/spack/repos/builtin/packages/adios2/package.py:    conflicts("+rocm", when="+cuda")
var/spack/repos/builtin/packages/adios2/package.py:    conflicts("+rocm", when="+sycl")
var/spack/repos/builtin/packages/adios2/package.py:    conflicts("+rocm", when="~kokkos", msg="ADIOS2 does not support HIP without Kokkos")
var/spack/repos/builtin/packages/adios2/package.py:    # ROCM: enable support for rocm >= 6
var/spack/repos/builtin/packages/adios2/package.py:    patch("2.10-enable-rocm6.patch", when="@2.9.1:2.10.1")
var/spack/repos/builtin/packages/adios2/package.py:            self.define("ADIOS2_USE_CUDA", self.spec.satisfies("+cuda ~kokkos")),
var/spack/repos/builtin/packages/adios2/package.py:            self.define("Kokkos_ENABLE_CUDA", self.spec.satisfies("+cuda +kokkos")),
var/spack/repos/builtin/packages/adios2/package.py:            self.define("Kokkos_ENABLE_HIP", self.spec.satisfies("+rocm")),
var/spack/repos/builtin/packages/adios2/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/adios2/package.py:            args.append(self.builder.define_cuda_architectures(self))
var/spack/repos/builtin/packages/adios2/package.py:        if spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/adios2/cmake-update-findmpi.patch:+    # When this is consumed for compiling CUDA, use '-Xcompiler' to wrap '-pthread'.
var/spack/repos/builtin/packages/adios2/cmake-update-findmpi.patch:+    string(REPLACE "-pthread" "$<$<COMPILE_LANGUAGE:CUDA>:SHELL:-Xcompiler >-pthread"
var/spack/repos/builtin/packages/adios2/2.10-enable-rocm6.patch:Subject: [PATCH] kokkos: support ROCM >=6
var/spack/repos/builtin/packages/adios2/2.10-enable-rocm6.patch:@@ -85,7 +85,11 @@ bool IsGPUbuffer(const void *ptr)
var/spack/repos/builtin/packages/adios2/2.10-enable-rocm6.patch:+#if defined(ROCM_VERSION_MAJOR) && ROCM_VERSION_MAJOR < 6
var/spack/repos/builtin/packages/roctracer-dev-api/package.py:    package, mainly to avoid circular dependencies in the ROCm ecosystem.
var/spack/repos/builtin/packages/roctracer-dev-api/package.py:    homepage = "https://github.com/ROCm/roctracer"
var/spack/repos/builtin/packages/roctracer-dev-api/package.py:    git = "https://github.com/ROCm/roctracer.git"
var/spack/repos/builtin/packages/roctracer-dev-api/package.py:    url = "https://github.com/ROCm/roctracer/archive/refs/tags/rocm-6.1.2.tar.gz"
var/spack/repos/builtin/packages/roctracer-dev-api/package.py:    tags = ["rocm"]
var/spack/repos/builtin/packages/hpx5/package.py:    variant("cuda", default=False, description="Enable CUDA support")
var/spack/repos/builtin/packages/hpx5/package.py:    variant("opencl", default=False, description="Enable OpenCL support")
var/spack/repos/builtin/packages/hpx5/package.py:    depends_on("hwloc +cuda", when="+cuda")
var/spack/repos/builtin/packages/hpx5/package.py:    # Note: We could disable CUDA support via "hwloc ~cuda"
var/spack/repos/builtin/packages/hpx5/package.py:    depends_on("opencl", when="+opencl")
var/spack/repos/builtin/packages/hpx5/package.py:        if spec.satisfies("+opencl"):
var/spack/repos/builtin/packages/hpx5/package.py:            args += ["--enable-opencl"]
var/spack/repos/builtin/packages/hpx5/package.py:                args += ["--with-opencl=pocl"]
var/spack/repos/builtin/packages/hpx5/package.py:                args += ["--with-opencl=system"]
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.3.patch: # all find_packages relevant to this build will be in ROCM path hence appending it to CMAKE_PREFIX_PATH 
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.3.patch: set(ROCM_PATH "/opt/rocm" CACHE PATH "ROCM install path")
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.3.patch:-set(CMAKE_INSTALL_PREFIX "/opt/rocm" CACHE PATH "CMAKE installation directory")
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.3.patch:-set(CMAKE_PACKAGING_INSTALL_PREFIX "/opt/rocm" CACHE PATH "Prefix used in built packages")
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.3.patch: list(APPEND CMAKE_PREFIX_PATH "${ROCM_PATH}")
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.3.patch:-set(ROCR_INC_DIR "${ROCM_PATH}/include" CACHE PATH "Contains header files exported by ROC Runtime" FORCE)
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.3.patch:-set(ROCR_LIB_DIR "${ROCM_PATH}/lib" CACHE PATH "Contains library files exported by ROC Runtime" FORCE)
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.3.patch:-set(HIP_INC_DIR "${ROCM_PATH}" CACHE PATH "Contains header files exported by ROC Runtime")
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.3.patch:-set(ROCT_INC_DIR "${ROCM_PATH}/include" CACHE PATH "Contains header files exported by ROC Trunk" FORCE)
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.3.patch:+set(ROCT_INC_DIR "${ROCM_PATH}/include" CACHE PATH "Contains header files exported by ROC Trunk")
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.3.patch: set(RVS_ROCMSMI "0" CACHE STRING "1 = use local rocm_smi_lib")
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.3.patch:   set(ROCT_LIB_DIR "${ROCM_PATH}/lib64" CACHE PATH "Contains library files exported by ROC Trunk")
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.3.patch:-  set(ROCBLAS_INC_DIR "${ROCM_PATH}/include")
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.3.patch:-  set(ROCBLAS_LIB_DIR "${ROCM_PATH}/lib")
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.3.patch: if (RVS_ROCMSMI EQUAL 1)
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.3.patch:     set(ROCM_SMI_LIB_DIR "${ROCM_PATH}/rocm_smi/lib")
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.3.patch:-    set(ROCM_SMI_INC_DIR "${ROCM_PATH}/include")
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.3.patch:-    set(ROCM_SMI_LIB_DIR "${ROCM_PATH}/lib")
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.3.patch:+    set(ROCM_SMI_INC_DIR "${ROCM_SMI_DIR}/include")
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.3.patch:+    set(ROCM_SMI_LIB_DIR "${ROCM_SMI_DIR}/lib")
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.3.patch: set(ROCM_SMI_LIB "rocm_smi64" CACHE STRING "rocm_smi library name")
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.3.patch:@@ -142,16 +142,16 @@ if(DEFINED RVS_ROCMSMI)
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.3.patch:@@ -146,7 +146,7 @@ if(DEFINED RVS_ROCMSMI)
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.3.patch:@@ -149,7 +149,7 @@ if(DEFINED RVS_ROCMSMI)
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.3.patch:-target_link_libraries(${RVS_TARGET} ${PROJECT_LINK_LIBS} ${HIP_HCC_LIB} ${ROCBLAS_LIB} ${ROCM_SMI_LIB})
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.3.patch:+target_link_libraries(${RVS_TARGET} ${PROJECT_LINK_LIBS} ${HIP_INC_DIR}/lib/ ${HIP_HCC_LIB} ${ROCBLAS_LIB} ${ROCM_SMI_LIB})
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.3.patch:@@ -143,7 +143,7 @@ if(DEFINED RVS_ROCMSMI)
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.3.patch:@@ -146,7 +146,7 @@ if(DEFINED RVS_ROCMSMI)
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.3.patch:-  ${ROCM_SMI_INC_DIR} ${ROCR_INC_DIR} ${ROCBLAS_INC_DIR} ${HIP_INC_DIR}
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.3.patch:+  ${ROCM_SMI_INC_DIR} ${HIP_PATH} ${ROCBLAS_INC_DIR} ${HIP_PATH}
var/spack/repos/builtin/packages/rocm-validation-suite/package.py:class RocmValidationSuite(CMakePackage):
var/spack/repos/builtin/packages/rocm-validation-suite/package.py:    """The ROCm Validation Suite (RVS) is a system administrators
var/spack/repos/builtin/packages/rocm-validation-suite/package.py:    common problems affecting AMD GPU(s) running in a high-performance
var/spack/repos/builtin/packages/rocm-validation-suite/package.py:    computing environment, enabled using the ROCm software stack on a
var/spack/repos/builtin/packages/rocm-validation-suite/package.py:    homepage = "https://github.com/ROCm/ROCmValidationSuite"
var/spack/repos/builtin/packages/rocm-validation-suite/package.py:    url = "https://github.com/ROCm/ROCmValidationSuite/archive/rocm-6.1.1.tar.gz"
var/spack/repos/builtin/packages/rocm-validation-suite/package.py:    tags = ["rocm"]
var/spack/repos/builtin/packages/rocm-validation-suite/package.py:    # Replacing ROCM_PATH with corresponding package prefix path.
var/spack/repos/builtin/packages/rocm-validation-suite/package.py:    # It expects rocm components headers and libraries in /opt/rocm
var/spack/repos/builtin/packages/rocm-validation-suite/package.py:    patch("009-replacing-rocm-path-with-package-path.patch", when="@6.0")
var/spack/repos/builtin/packages/rocm-validation-suite/package.py:    patch("009-replacing-rocm-path-with-package-path-6.1.patch", when="@6.1")
var/spack/repos/builtin/packages/rocm-validation-suite/package.py:        depends_on(f"rocminfo@{ver}", when=f"@{ver}")
var/spack/repos/builtin/packages/rocm-validation-suite/package.py:        depends_on(f"rocm-smi-lib@{ver}", when=f"@{ver}")
var/spack/repos/builtin/packages/rocm-validation-suite/package.py:                r"@ROCM_PATH@/bin", self.spec.prefix.bin, "rvs/conf/deviceid.sh.in", string=True
var/spack/repos/builtin/packages/rocm-validation-suite/package.py:                r"@ROCM_PATH@/rvs", self.spec.prefix.rvs, "rvs/conf/deviceid.sh.in", string=True
var/spack/repos/builtin/packages/rocm-validation-suite/package.py:                "@ROCM_PATH@/rvs", self.spec.prefix.bin, "rvs/conf/deviceid.sh.in", string=True
var/spack/repos/builtin/packages/rocm-validation-suite/package.py:            self.define("ROCM_SMI_DIR", self.spec["rocm-smi-lib"].prefix),
var/spack/repos/builtin/packages/rocm-validation-suite/package.py:                    f"-I{self.spec['rocm-smi-lib'].prefix.include} "
var/spack/repos/builtin/packages/rocm-validation-suite/package.py:                    f"-L{self.spec['rocm-smi-lib'].prefix.lib} "
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch: gpup.so/CMakeLists.txt         |  8 ++++----
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch: # all find_packages relevant to this build will be in ROCM path hence appending it to CMAKE_PREFIX_PATH 
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch: set(ROCM_PATH "/opt/rocm" CACHE PATH "ROCM install path")
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch:-set(CMAKE_INSTALL_PREFIX "/opt/rocm" CACHE PATH "CMAKE installation directory")
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch:-set(CPACK_PACKAGING_INSTALL_PREFIX "/opt/rocm" CACHE PATH "Prefix used in built packages")
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch: list(APPEND CMAKE_PREFIX_PATH "${ROCM_PATH}")
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch:-set(ROCR_INC_DIR "${ROCM_PATH}/include" CACHE PATH "Contains header files exported by ROC Runtime" FORCE)
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch:-set(ROCR_LIB_DIR "${ROCM_PATH}/lib" CACHE PATH "Contains library files exported by ROC Runtime" FORCE)
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch:-set(HIP_INC_DIR "${ROCM_PATH}" CACHE PATH "Contains header files exported by ROC Runtime")
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch:-set(ROCT_INC_DIR "${ROCM_PATH}/include" CACHE PATH "Contains header files exported by ROC Trunk" FORCE)
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch:+set(ROCT_INC_DIR "${ROCM_PATH}/include" CACHE PATH "Contains header files exported by ROC Trunk")
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch: add_definitions(-DROCM_PATH="${ROCM_PATH}")
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch:-  set(ROCBLAS_INC_DIR "${ROCM_PATH}/include")
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch:-  set(ROCBLAS_LIB_DIR "${ROCM_PATH}/lib")
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch: if (RVS_ROCMSMI EQUAL 1)
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch:     set(ROCM_SMI_LIB_DIR "${ROCM_PATH}/rocm_smi/lib")
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch:-    set(ROCM_SMI_INC_DIR "${ROCM_PATH}/include")
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch:-    set(ROCM_SMI_LIB_DIR "${ROCM_PATH}/lib")
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch:+    set(ROCM_SMI_INC_DIR "${ROCM_SMI_DIR}/include")
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch:+    set(ROCM_SMI_LIB_DIR "${ROCM_SMI_DIR}/lib")
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch: set(ROCM_SMI_LIB "rocm_smi64" CACHE STRING "rocm_smi library name")
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch:@@ -133,16 +133,16 @@ if(DEFINED RVS_ROCMSMI)
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch:+link_directories(${RVS_LIB_DIR} ${HIP_PATH}/lib/ ${ROCBLAS_LIB_DIR} ${ASAN_LIB_PATH} ${ROCM_SMI_LIB_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch:+link_directories(${UT_LIB} ${RVS_LIB_DIR} ${ROCM_SMI_LIB_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch:+link_directories(${RVS_LIB_DIR} ${ROCR_LIB_DIR} ${ROCBLAS_LIB_DIR} ${ROCM_SMI_LIB_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch:@@ -118,11 +118,11 @@ if(DEFINED RVS_ROCMSMI)
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch:-include_directories(./ ../ ${ROCM_SMI_INC_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch:+include_directories(./ ../ ${ROCM_SMI_INC_DIR} ${YAML_CPP_INCLUDE_DIRS})
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch: link_directories(${RVS_LIB_DIR} ${ROCM_SMI_LIB_DIR} ${ASAN_LIB_PATH})
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch:+set (PROJECT_LINK_LIBS rvslib libpthread.so libpci.so libm.so librocm_smi64.so ${ROCBLAS_LIB_DIR}/librocblas.so ${HSAKMT_LIB_DIR}/libhsakmt.a ${HSA_PATH}/lib/libhsa-runtime64.so)
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch:-target_link_libraries(${RVS_TARGET} ${PROJECT_LINK_LIBS} ${ROCM_SMI_LIB})
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch:diff --git a/gpup.so/CMakeLists.txt b/gpup.so/CMakeLists.txt
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch:--- a/gpup.so/CMakeLists.txt
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch:+++ b/gpup.so/CMakeLists.txt
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch:+link_directories(${RVS_LIB_DIR} ${ASAN_LIB_PATH} ${ROCM_SMI_LIB_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch:@@ -137,17 +137,17 @@ if(DEFINED RVS_ROCMSMI)
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch:+link_directories(${RVS_LIB_DIR} ${ROCR_LIB_DIR} ${HIP_INC_DIR}/lib/ ${ROCBLAS_LIB_DIR} ${ASAN_LIB_PATH} ${ROCM_SMI_LIB_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch:@@ -140,7 +140,7 @@ if(DEFINED RVS_ROCMSMI)
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch:@@ -159,7 +159,7 @@ include_directories(./ ../ ${ROCM_SMI_INC_DIR} ${ROCBLAS_INC_DIR} ${ROCR_INC_DIR
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch: link_directories(${RVS_LIB_DIR} ${ROCR_LIB_DIR} ${ROCBLAS_LIB_DIR} ${ROCM_SMI_LIB_DIR} ${ASAN_LIB_PATH})
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch:+set (PROJECT_LINK_LIBS rvslib libpthread.so libpci.so libm.so librocm_smi64.so ${ROCBLAS_LIB_DIR}/librocblas.so ${HSAKMT_LIB_DIR}/libhsakmt.a ${HSA_PATH}/lib/libhsa-runtime64.so)
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch:-target_link_libraries(${RVS_TARGET} ${PROJECT_LINK_LIBS} ${HIP_HCC_LIB} ${ROCBLAS_LIB} ${ROCM_SMI_LIB})
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch:@@ -134,7 +134,7 @@ if(DEFINED RVS_ROCMSMI)
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch:+link_directories(${RVS_LIB_DIR} ${ROCR_LIB_DIR} ${HIP_INC_DIR}/lib ${ROCBLAS_LIB_DIR} ${ASAN_LIB_PATH} ${ROCM_SMI_LIB_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch:+link_directories(${RVS_LIB_DIR} ${ROCR_LIB_DIR} ${HSAKMT_LIB_DIR} ${ROCT_LIB_DIR} ${ASAN_LIB_PATH} ${ROCM_SMI_LIB_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch:+link_directories(${RVS_LIB_DIR} ${ROCR_LIB_DIR} ${HSAKMT_LIB_DIR} ${ROCT_LIB_DIR} ${ASAN_LIB_PATH} ${ROCM_SMI_LIB_DIR} )
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch:+link_directories(${RVS_LIB_DIR} ${HSA_PATH}/lib/ ${HSAKMT_LIB_DIR} ${ASAN_LIB_PATH} ${ROCM_SMI_LIB_DIR} ${YAML_CPP_INCLUDE_DIRS})
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch:@@ -137,7 +137,7 @@ if(DEFINED RVS_ROCMSMI)
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch:+link_directories(${RVS_LIB_DIR} ${ROCR_LIB_DIR} ${HIP_INC_DIR}/lib ${ROCBLAS_LIB_DIR} ${ASAN_LIB_PATH} ${ROCM_SMI_LIB_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch:+link_directories(${RVS_LIB_DIR} ${ROCR_LIB_DIR} ${ROCBLAS_LIB_DIR} ${ASAN_LIB_PATH} ${ROCM_SMI_LIB_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch:+link_directories(${RVS_LIB_DIR} ${ASAN_LIB_PATH} ${ASAN_LIB_PATH} ${HSAKMT_LIB_DIR} ${ROCM_SMI_LIB_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch:-link_directories(${CMAKE_CURRENT_BINARY_DIR} ${RVS_LIB_DIR} ${ROCT_LIB_DIR} ${ROCBLAS_LIB_DIR} ${ROCM_SMI_LIB_DIR} ${ASAN_LIB_PATH})
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch:+link_directories(${CMAKE_CURRENT_BINARY_DIR} ${RVS_LIB_DIR} ${RVS_LIB_DIR}/.. ${ASAN_LIB_PATH} ${ROCM_SMI_LIB_DIR} ${YAML_CPP_LIBRARIES} ${ROCT_LIB_DIR} ${ROCBLAS_LIB_DIR} )
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch:-  ${ROCBLAS_LIB} ${ROCM_SMI_LIB} ${ROC_THUNK_NAME} ${CORE_RUNTIME_TARGET} ${PROJECT_LINK_LIBS})
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch:+  ${ROCBLAS_LIB} ${ROCM_SMI_LIB} ${ROC_THUNK_NAME} ${PROJECT_LINK_LIBS} ${CORE_RUNTIME_NAME} ${YAML_CPP_LIB})
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch:@@ -41,7 +41,8 @@ link_directories(${RVS_LIB_DIR} ${ROCBLAS_LIB_DIR} ${ROCM_SMI_LIB_DIR} ${ROCT_LI
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch:-  ${ROCM_SMI_LIB} ${ROCBLAS_LIB} ${ROC_THUNK_NAME} ${CORE_RUNTIME_TARGET})
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch:+  ${ROCM_SMI_LIB} ${ROCBLAS_LIB} ${ROC_THUNK_NAME} ${CORE_RUNTIME_TARGET}
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch: link_directories(${UT_LIB} ${ROCBLAS_LIB_DIR} ${ROCM_SMI_LIB_DIR} ${ROCT_LIB_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch:     ${ROCM_SMI_LIB} ${ROCBLAS_LIB} ${ROC_THUNK_NAME} ${CORE_RUNTIME_TARGET}
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch:-  ${ROCM_SMI_INC_DIR} ${ROCR_INC_DIR} ${ROCBLAS_INC_DIR} ${HIP_INC_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch:+  ${ROCM_SMI_INC_DIR} ${HIP_PATH} ${ROCBLAS_INC_DIR} ${YAML_INC_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch: link_directories(${ASAN_LIB_PATH} ${ROCM_SMI_LIB_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch:+link_directories(${RVS_LIB_DIR} ${ASAN_LIB_PATH} ${ROCM_SMI_LIB_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path.patch:+link_directories(${RVS_LIB_DIR} ${ROCBLAS_LIB_DIR} ${ASAN_LIB_PATH} ${ROCM_SMI_LIB_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.6.patch:Subject: [PATCH] 5.6 Patch to add rocm-smi library and include path
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.6.patch: gpup.so/CMakeLists.txt         |   2 +-
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.6.patch: # all find_packages relevant to this build will be in ROCM path hence appending it to CMAKE_PREFIX_PATH 
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.6.patch: set(ROCM_PATH "/opt/rocm" CACHE PATH "ROCM install path")
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.6.patch:-set(CMAKE_INSTALL_PREFIX "/opt/rocm" CACHE PATH "CMAKE installation directory")
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.6.patch:-set(CMAKE_PACKAGING_INSTALL_PREFIX "/opt/rocm" CACHE PATH "Prefix used in built packages")
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.6.patch: list(APPEND CMAKE_PREFIX_PATH "${ROCM_PATH}")
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.6.patch:-set(ROCR_INC_DIR "${ROCM_PATH}/include" CACHE PATH "Contains header files exported by ROC Runtime" FORCE)
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.6.patch:-set(ROCR_LIB_DIR "${ROCM_PATH}/lib" CACHE PATH "Contains library files exported by ROC Runtime" FORCE)
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.6.patch:-set(HIP_INC_DIR "${ROCM_PATH}" CACHE PATH "Contains header files exported by ROC Runtime")
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.6.patch:-set(ROCT_INC_DIR "${ROCM_PATH}/include" CACHE PATH "Contains header files exported by ROC Trunk" FORCE)
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.6.patch:+set(ROCT_INC_DIR "${ROCM_PATH}/include" CACHE PATH "Contains header files exported by ROC Trunk")
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.6.patch: set(RVS_ROCMSMI "0" CACHE STRING "1 = use local rocm_smi_lib")
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.6.patch:   set(ROCT_LIB_DIR "${ROCM_PATH}/lib64" CACHE PATH "Contains library files exported by ROC Trunk")
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.6.patch:-  set(ROCBLAS_INC_DIR "${ROCM_PATH}/include")
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.6.patch:-  set(ROCBLAS_LIB_DIR "${ROCM_PATH}/lib")
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.6.patch: if (RVS_ROCMSMI EQUAL 1)
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.6.patch:     set(ROCM_SMI_LIB_DIR "${ROCM_PATH}/rocm_smi/lib")
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.6.patch:-    set(ROCM_SMI_INC_DIR "${ROCM_PATH}/include")
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.6.patch:-    set(ROCM_SMI_LIB_DIR "${ROCM_PATH}/lib")
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.6.patch:+    set(ROCM_SMI_INC_DIR "${ROCM_SMI_DIR}/include")
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.6.patch:+    set(ROCM_SMI_LIB_DIR "${ROCM_SMI_DIR}/lib")
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.6.patch: set(ROCM_SMI_LIB "rocm_smi64" CACHE STRING "rocm_smi library name")
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.6.patch:@@ -133,16 +133,16 @@ if(DEFINED RVS_ROCMSMI)
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.6.patch:+link_directories(${RVS_LIB_DIR} ${HIP_PATH}/lib/ ${ROCBLAS_LIB_DIR} ${ASAN_LIB_PATH} ${ROCM_SMI_LIB_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.6.patch:+link_directories(${UT_LIB} ${RVS_LIB_DIR} ${ROCM_SMI_LIB_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.6.patch:+link_directories(${RVS_LIB_DIR} ${ROCR_LIB_DIR} ${ROCBLAS_LIB_DIR} ${ROCM_SMI_LIB_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.6.patch:@@ -122,7 +122,7 @@ include_directories(./ ../ ${ROCM_SMI_INC_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.6.patch: link_directories(${RVS_LIB_DIR} ${ROCM_SMI_LIB_DIR} ${ASAN_LIB_PATH})
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.6.patch:+set (PROJECT_LINK_LIBS rvslibrt rvslib libpthread.so libpci.so libm.so librocm_smi64.so)
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.6.patch:-target_link_libraries(${RVS_TARGET} ${PROJECT_LINK_LIBS} ${ROCM_SMI_LIB})
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.6.patch:diff --git a/gpup.so/CMakeLists.txt b/gpup.so/CMakeLists.txt
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.6.patch:--- a/gpup.so/CMakeLists.txt
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.6.patch:+++ b/gpup.so/CMakeLists.txt
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.6.patch:+link_directories(${RVS_LIB_DIR} ${ASAN_LIB_PATH} ${ROCM_SMI_LIB_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.6.patch:@@ -137,7 +137,7 @@ if(DEFINED RVS_ROCMSMI)
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.6.patch:+link_directories(${RVS_LIB_DIR} ${ROCR_LIB_DIR} ${HIP_INC_DIR}/lib/ ${ROCBLAS_LIB_DIR} ${ASAN_LIB_PATH} ${ROCM_SMI_LIB_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.6.patch:@@ -140,7 +140,7 @@ if(DEFINED RVS_ROCMSMI)
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.6.patch:@@ -159,7 +159,7 @@ include_directories(./ ../ ${ROCM_SMI_INC_DIR} ${ROCBLAS_INC_DIR} ${ROCR_INC_DIR
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.6.patch: link_directories(${RVS_LIB_DIR} ${ROCR_LIB_DIR} ${ROCBLAS_LIB_DIR} ${ROCM_SMI_LIB_DIR} ${ASAN_LIB_PATH})
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.6.patch:+set (PROJECT_LINK_LIBS rvslibrt rvslib libpthread.so libpci.so libm.so librocm_smi64.so)
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.6.patch:-target_link_libraries(${RVS_TARGET} ${PROJECT_LINK_LIBS} ${HIP_HCC_LIB} ${ROCBLAS_LIB} ${ROCM_SMI_LIB})
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.6.patch:+target_link_libraries(${RVS_TARGET} ${PROJECT_LINK_LIBS} ${HIP_INC_DIR}/lib/ ${HIP_HCC_LIB} ${ROCBLAS_LIB} ${ROCM_SMI_LIB_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.6.patch:@@ -134,7 +134,7 @@ if(DEFINED RVS_ROCMSMI)
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.6.patch:+link_directories(${RVS_LIB_DIR} ${ROCR_LIB_DIR} ${HIP_INC_DIR}/lib ${ROCBLAS_LIB_DIR} ${ASAN_LIB_PATH} ${ROCM_SMI_LIB_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.6.patch:+link_directories(${RVS_LIB_DIR} ${ROCR_LIB_DIR} ${HSAKMT_LIB_DIR} ${ROCT_LIB_DIR} ${ASAN_LIB_PATH} ${ROCM_SMI_LIB_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.6.patch:+link_directories(${RVS_LIB_DIR} ${ROCR_LIB_DIR} ${HSAKMT_LIB_DIR} ${ROCT_LIB_DIR} ${ASAN_LIB_PATH} ${ROCM_SMI_LIB_DIR} )
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.6.patch:+link_directories(${RVS_LIB_DIR} ${HSA_PATH}/lib/ ${HSAKMT_LIB_DIR} ${ASAN_LIB_PATH} ${ROCM_SMI_LIB_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.6.patch:@@ -137,7 +137,7 @@ if(DEFINED RVS_ROCMSMI)
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.6.patch:+link_directories(${RVS_LIB_DIR} ${ROCR_LIB_DIR} ${HIP_INC_DIR}/lib ${ROCBLAS_LIB_DIR} ${ASAN_LIB_PATH} ${ROCM_SMI_LIB_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.6.patch:+link_directories(${RVS_LIB_DIR} ${ROCR_LIB_DIR} ${ROCBLAS_LIB_DIR} ${ASAN_LIB_PATH} ${ROCM_SMI_LIB_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.6.patch:+link_directories(${RVS_LIB_DIR} ${ASAN_LIB_PATH} ${ASAN_LIB_PATH} ${HSAKMT_LIB_DIR} ${ROCM_SMI_LIB_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.6.patch:+link_directories(${CMAKE_CURRENT_BINARY_DIR} ${RVS_LIB_DIR} ${ASAN_LIB_PATH} ${ROCM_SMI_LIB_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.6.patch:+link_directories(${UT_LIB} ${ROCM_SMI_LIB_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.6.patch:-  ${ROCM_SMI_INC_DIR} ${ROCR_INC_DIR} ${ROCBLAS_INC_DIR} ${HIP_INC_DIR}
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.6.patch:+  ${ROCM_SMI_INC_DIR} ${HIP_PATH} ${ROCBLAS_INC_DIR}
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.6.patch: link_directories(${ASAN_LIB_PATH} ${ROCM_SMI_LIB_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.6.patch:+link_directories(${RVS_LIB_DIR} ${ASAN_LIB_PATH} ${ROCM_SMI_LIB_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/007-cleanup-path-reference-donot-download-googletest-yaml-library-path_5.6.patch:+link_directories(${RVS_LIB_DIR} ${ROCBLAS_LIB_DIR} ${ASAN_LIB_PATH} ${ROCM_SMI_LIB_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch: gpup.so/CMakeLists.txt         |  6 +++---
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch: # Making ROCM_PATH, CMAKE_INSTALL_PREFIX, CPACK_PACKAGING_INSTALL_PREFIX as CACHE
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch: # all find_packages relevant to this build will be in ROCM path hence appending it to CMAKE_PREFIX_PATH 
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch:-set(ROCM_PATH "/opt/rocm" CACHE PATH "ROCM install path")
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch:-set(CMAKE_INSTALL_PREFIX "/opt/rocm" CACHE PATH "CMAKE installation directory")
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch:-set(CPACK_PACKAGING_INSTALL_PREFIX "/opt/rocm" CACHE PATH "Prefix used in built packages")
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch: list(APPEND CMAKE_PREFIX_PATH "${ROCM_PATH}")
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch:-set(ROCR_INC_DIR "${ROCM_PATH}/include" CACHE PATH "Contains header files exported by ROC Runtime" FORCE)
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch:-set(ROCR_LIB_DIR "${ROCM_PATH}/lib" CACHE PATH "Contains library files exported by ROC Runtime" FORCE)
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch:-set(HIP_INC_DIR "${ROCM_PATH}" CACHE PATH "Contains header files exported by ROC Runtime")
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch:-set(ROCT_INC_DIR "${ROCM_PATH}/include" CACHE PATH "Contains header files exported by ROC Trunk" FORCE)
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch:+set(ROCT_INC_DIR "${ROCM_PATH}/include" CACHE PATH "Contains header files exported by ROC Trunk")
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch: add_definitions(-DROCM_PATH="${ROCM_PATH}")
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch: if(FETCH_ROCMPATH_FROM_ROCMCORE)
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch:-  set(ROCBLAS_INC_DIR "${ROCM_PATH}/include")
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch:-  set(ROCBLAS_LIB_DIR "${ROCM_PATH}/lib")
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch: if (RVS_ROCMSMI EQUAL 1)
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch:     set(ROCM_SMI_LIB_DIR "${ROCM_PATH}/rocm_smi/lib")
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch:-    set(ROCM_SMI_INC_DIR "${ROCM_PATH}/include")
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch:-    set(ROCM_SMI_LIB_DIR "${ROCM_PATH}/lib")
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch:+    set(ROCM_SMI_INC_DIR "${ROCM_SMI_DIR}/include")
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch:+    set(ROCM_SMI_LIB_DIR "${ROCM_SMI_DIR}/lib")
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch: set(ROCM_SMI_LIB "rocm_smi64" CACHE STRING "rocm_smi library name")
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch:@@ -135,16 +135,16 @@ if(DEFINED RVS_ROCMSMI)
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch:+link_directories(${RVS_LIB_DIR} ${HIP_PATH}/lib/ ${ROCBLAS_LIB_DIR} ${ASAN_LIB_PATH} ${ROCM_SMI_LIB_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch:+link_directories(${UT_LIB} ${RVS_LIB_DIR} ${ROCM_SMI_LIB_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch:@@ -128,17 +128,17 @@ if(DEFINED RVS_ROCMSMI)
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch:+link_directories(${RVS_LIB_DIR} ${ROCR_LIB_DIR} ${ROCBLAS_LIB_DIR} ${ROCM_SMI_LIB_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch:@@ -118,11 +118,11 @@ if(DEFINED RVS_ROCMSMI)
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch:-include_directories(./ ../ ${ROCM_SMI_INC_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch:+include_directories(./ ../ ${ROCM_SMI_INC_DIR} ${YAML_CPP_INCLUDE_DIRS})
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch: link_directories(${RVS_LIB_DIR} ${ROCM_SMI_LIB_DIR} ${ASAN_LIB_PATH})
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch:+set (PROJECT_LINK_LIBS rvslib libpthread.so libpci.so libm.so librocm_smi64.so ${ROCBLAS_LIB_DIR}/librocblas.so ${HSAKMT_LIB_DIR}/libhsakmt.a ${HSA_PATH}/lib/libhsa-runtime64.so)
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch:-target_link_libraries(${RVS_TARGET} ${PROJECT_LINK_LIBS} ${ROCM_SMI_LIB})
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch:diff --git a/gpup.so/CMakeLists.txt b/gpup.so/CMakeLists.txt
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch:--- a/gpup.so/CMakeLists.txt
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch:+++ b/gpup.so/CMakeLists.txt
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch:+link_directories(${RVS_LIB_DIR} ${ASAN_LIB_PATH} ${ROCM_SMI_LIB_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch:@@ -137,17 +137,17 @@ if(DEFINED RVS_ROCMSMI)
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch:+link_directories(${RVS_LIB_DIR} ${ROCR_LIB_DIR} ${HIP_INC_DIR}/lib/ ${ROCBLAS_LIB_DIR} ${ASAN_LIB_PATH} ${ROCM_SMI_LIB_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch:@@ -140,7 +140,7 @@ if(DEFINED RVS_ROCMSMI)
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch:@@ -159,7 +159,7 @@ include_directories(./ ../ ${ROCM_SMI_INC_DIR} ${ROCBLAS_INC_DIR} ${ROCR_INC_DIR
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch: link_directories(${RVS_LIB_DIR} ${ROCR_LIB_DIR} ${ROCBLAS_LIB_DIR} ${ROCM_SMI_LIB_DIR} ${ASAN_LIB_PATH})
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch:+set (PROJECT_LINK_LIBS rvslib libpthread.so libpci.so libm.so librocm_smi64.so ${ROCBLAS_LIB_DIR}/librocblas.so ${HSAKMT_LIB_DIR}/libhsakmt.a ${HSA_PATH}/lib/libhsa-runtime64.so)
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch:-target_link_libraries(${RVS_TARGET} ${PROJECT_LINK_LIBS} ${HIP_HCC_LIB} ${ROCBLAS_LIB} ${ROCM_SMI_LIB})
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch:@@ -134,7 +134,7 @@ if(DEFINED RVS_ROCMSMI)
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch:+link_directories(${RVS_LIB_DIR} ${ROCR_LIB_DIR} ${HIP_INC_DIR}/lib ${ROCBLAS_LIB_DIR} ${ASAN_LIB_PATH} ${ROCM_SMI_LIB_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch:+link_directories(${RVS_LIB_DIR} ${ROCR_LIB_DIR} ${HSAKMT_LIB_DIR} ${ROCT_LIB_DIR} ${ASAN_LIB_PATH} ${ROCM_SMI_LIB_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch:+link_directories(${RVS_LIB_DIR} ${ROCR_LIB_DIR} ${HSAKMT_LIB_DIR} ${ROCT_LIB_DIR} ${ASAN_LIB_PATH} ${ROCM_SMI_LIB_DIR} )
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch:+link_directories(${RVS_LIB_DIR} ${HSA_PATH}/lib/ ${HSAKMT_LIB_DIR} ${ASAN_LIB_PATH} ${ROCM_SMI_LIB_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch:@@ -137,7 +137,7 @@ if(DEFINED RVS_ROCMSMI)
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch:+link_directories(${RVS_LIB_DIR} ${ROCR_LIB_DIR} ${HIP_INC_DIR}/lib ${ROCBLAS_LIB_DIR} ${ASAN_LIB_PATH} ${ROCM_SMI_LIB_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch:+link_directories(${RVS_LIB_DIR} ${ROCR_LIB_DIR} ${ROCBLAS_LIB_DIR} ${ASAN_LIB_PATH} ${ROCM_SMI_LIB_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch:+link_directories(${RVS_LIB_DIR} ${ASAN_LIB_PATH} ${ASAN_LIB_PATH} ${HSAKMT_LIB_DIR} ${ROCM_SMI_LIB_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch:-link_directories(${CMAKE_CURRENT_BINARY_DIR} ${RVS_LIB_DIR} ${ROCT_LIB_DIR} ${ROCBLAS_LIB_DIR} ${ROCM_SMI_LIB_DIR} ${ASAN_LIB_PATH})
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch:+link_directories(${CMAKE_CURRENT_BINARY_DIR} ${RVS_LIB_DIR} ${RVS_LIB_DIR}/.. ${ASAN_LIB_PATH} ${ROCM_SMI_LIB_DIR} ${ROCT_LIB_DIR} ${ROCBLAS_LIB_DIR} ${YAML_CPP_LIBRARIES})
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch:-  ${ROCBLAS_LIB} ${ROCM_SMI_LIB} ${ROC_THUNK_NAME} ${CORE_RUNTIME_TARGET} ${ROCM_CORE} ${PROJECT_LINK_LIBS})
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch:+  ${ROCBLAS_LIB} ${ROCM_SMI_LIB} ${ROC_THUNK_NAME} ${CORE_RUNTIME_TARGET} ${ROCM_CORE} ${PROJECT_LINK_LIBS} ${YAML_CPP_LIB})
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch:-  ${ROCM_SMI_INC_DIR} ${ROCR_INC_DIR} ${ROCBLAS_INC_DIR} ${HIP_INC_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch:+  ${ROCM_SMI_INC_DIR} ${HIP_PATH} ${ROCBLAS_INC_DIR} ${YAML_CPP_INCLUDE_DIRS})
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch: link_directories(${ASAN_LIB_PATH} ${ROCM_SMI_LIB_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch:+link_directories(${RVS_LIB_DIR} ${ASAN_LIB_PATH} ${ROCM_SMI_LIB_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch:+link_directories(${RVS_LIB_DIR} ${ROCBLAS_LIB_DIR} ${ASAN_LIB_PATH} ${ROCM_SMI_LIB_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/009-replacing-rocm-path-with-package-path-6.1.patch:@@ -140,7 +140,7 @@ if(DEFINED RVS_ROCMSMI)
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch: # all find_packages relevant to this build will be in ROCM path hence appending it to CMAKE_PREFIX_PATH 
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch: set(ROCM_PATH "/opt/rocm" CACHE PATH "ROCM install path")
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch:-set(CMAKE_INSTALL_PREFIX "/opt/rocm" CACHE PATH "CMAKE installation directory")
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch:-set(CPACK_PACKAGING_INSTALL_PREFIX "/opt/rocm" CACHE PATH "Prefix used in built packages")
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch: list(APPEND CMAKE_PREFIX_PATH "${ROCM_PATH}")
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch:-set(ROCR_INC_DIR "${ROCM_PATH}/include" CACHE PATH "Contains header files exported by ROC Runtime" FORCE)
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch:-set(ROCR_LIB_DIR "${ROCM_PATH}/lib" CACHE PATH "Contains library files exported by ROC Runtime" FORCE)
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch:-set(HIP_INC_DIR "${ROCM_PATH}" CACHE PATH "Contains header files exported by ROC Runtime")
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch:-set(ROCT_INC_DIR "${ROCM_PATH}/include" CACHE PATH "Contains header files exported by ROC Trunk" FORCE)
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch:+set(ROCT_INC_DIR "${ROCM_PATH}/include" CACHE PATH "Contains header files exported by ROC Trunk")
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch: add_definitions(-DROCM_PATH="${ROCM_PATH}")
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch:-  set(ROCBLAS_INC_DIR "${ROCM_PATH}/include")
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch:-  set(ROCBLAS_LIB_DIR "${ROCM_PATH}/lib")
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch: if (RVS_ROCMSMI EQUAL 1)
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch:     set(ROCM_SMI_LIB_DIR "${ROCM_PATH}/rocm_smi/lib")
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch:-    set(ROCM_SMI_INC_DIR "${ROCM_PATH}/include")
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch:-    set(ROCM_SMI_LIB_DIR "${ROCM_PATH}/lib")
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch:+    set(ROCM_SMI_INC_DIR "${ROCM_SMI_DIR}/include")
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch:+    set(ROCM_SMI_LIB_DIR "${ROCM_SMI_DIR}/lib")
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch: set(ROCM_SMI_LIB "rocm_smi64" CACHE STRING "rocm_smi library name")
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch:@@ -133,16 +133,16 @@ if(DEFINED RVS_ROCMSMI)
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch:+link_directories(${RVS_LIB_DIR} ${HIP_PATH}/lib/ ${ROCBLAS_LIB_DIR} ${ASAN_LIB_PATH} ${ROCM_SMI_LIB_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch:+link_directories(${UT_LIB} ${RVS_LIB_DIR} ${ROCM_SMI_LIB_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch:+link_directories(${RVS_LIB_DIR} ${ROCR_LIB_DIR} ${ROCBLAS_LIB_DIR} ${ROCM_SMI_LIB_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch:@@ -118,11 +118,11 @@ if(DEFINED RVS_ROCMSMI)
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch:-include_directories(./ ../ ${ROCM_SMI_INC_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch:+include_directories(./ ../ ${ROCM_SMI_INC_DIR} ${YAML_CPP_INCLUDE_DIRS})
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch: link_directories(${RVS_LIB_DIR} ${ROCM_SMI_LIB_DIR} ${ASAN_LIB_PATH})
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch:+set (PROJECT_LINK_LIBS rvslib libpthread.so libpci.so libm.so librocm_smi64.so ${ROCBLAS_LIB_DIR}/librocblas.so ${HSAKMT_LIB_DIR}/libhsakmt.a ${HSA_PATH}/lib/libhsa-runtime64.so)
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch:-target_link_libraries(${RVS_TARGET} ${PROJECT_LINK_LIBS} ${ROCM_SMI_LIB})
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch:diff --git a/gpup.so/CMakeLists.txt b/gpup.so/CMakeLists.txt
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch:--- a/gpup.so/CMakeLists.txt
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch:+++ b/gpup.so/CMakeLists.txt
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch:+link_directories(${RVS_LIB_DIR} ${ASAN_LIB_PATH} ${ROCM_SMI_LIB_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch:@@ -137,17 +137,17 @@ if(DEFINED RVS_ROCMSMI)
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch:+link_directories(${RVS_LIB_DIR} ${ROCR_LIB_DIR} ${HIP_INC_DIR}/lib/ ${ROCBLAS_LIB_DIR} ${ASAN_LIB_PATH} ${ROCM_SMI_LIB_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch:@@ -140,7 +140,7 @@ if(DEFINED RVS_ROCMSMI)
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch:@@ -159,7 +159,7 @@ include_directories(./ ../ ${ROCM_SMI_INC_DIR} ${ROCBLAS_INC_DIR} ${ROCR_INC_DIR
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch: link_directories(${RVS_LIB_DIR} ${ROCR_LIB_DIR} ${ROCBLAS_LIB_DIR} ${ROCM_SMI_LIB_DIR} ${ASAN_LIB_PATH})
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch:+set (PROJECT_LINK_LIBS rvslib libpthread.so libpci.so libm.so librocm_smi64.so ${ROCBLAS_LIB_DIR}/librocblas.so ${HSAKMT_LIB_DIR}/libhsakmt.a ${HSA_PATH}/lib/libhsa-runtime64.so)
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch:-target_link_libraries(${RVS_TARGET} ${PROJECT_LINK_LIBS} ${HIP_HCC_LIB} ${ROCBLAS_LIB} ${ROCM_SMI_LIB})
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch:@@ -134,7 +134,7 @@ if(DEFINED RVS_ROCMSMI)
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch:+link_directories(${RVS_LIB_DIR} ${ROCR_LIB_DIR} ${HIP_INC_DIR}/lib ${ROCBLAS_LIB_DIR} ${ASAN_LIB_PATH} ${ROCM_SMI_LIB_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch:+link_directories(${RVS_LIB_DIR} ${ROCR_LIB_DIR} ${HSAKMT_LIB_DIR} ${ROCT_LIB_DIR} ${ASAN_LIB_PATH} ${ROCM_SMI_LIB_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch:+link_directories(${RVS_LIB_DIR} ${ROCR_LIB_DIR} ${HSAKMT_LIB_DIR} ${ROCT_LIB_DIR} ${ASAN_LIB_PATH} ${ROCM_SMI_LIB_DIR} )
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch:+link_directories(${RVS_LIB_DIR} ${HSA_PATH}/lib/ ${HSAKMT_LIB_DIR} ${ASAN_LIB_PATH} ${ROCM_SMI_LIB_DIR} ${YAML_CPP_INCLUDE_DIRS})
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch:@@ -137,7 +137,7 @@ if(DEFINED RVS_ROCMSMI)
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch:+link_directories(${RVS_LIB_DIR} ${ROCR_LIB_DIR} ${HIP_INC_DIR}/lib ${ROCBLAS_LIB_DIR} ${ASAN_LIB_PATH} ${ROCM_SMI_LIB_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch:+link_directories(${RVS_LIB_DIR} ${ROCR_LIB_DIR} ${ROCBLAS_LIB_DIR} ${ASAN_LIB_PATH} ${ROCM_SMI_LIB_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch:+link_directories(${RVS_LIB_DIR} ${ASAN_LIB_PATH} ${ASAN_LIB_PATH} ${HSAKMT_LIB_DIR} ${ROCM_SMI_LIB_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch:-link_directories(${CMAKE_CURRENT_BINARY_DIR} ${RVS_LIB_DIR} ${ROCT_LIB_DIR} ${ROCBLAS_LIB_DIR} ${ROCM_SMI_LIB_DIR} ${ASAN_LIB_PATH})
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch:+link_directories(${CMAKE_CURRENT_BINARY_DIR} ${RVS_LIB_DIR} ${RVS_LIB_DIR}/.. ${ASAN_LIB_PATH} ${ROCM_SMI_LIB_DIR} ${YAML_CPP_LIBRARIES} ${ROCT_LIB_DIR} ${ROCBLAS_LIB_DIR} )
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch:-  ${ROCBLAS_LIB} ${ROCM_SMI_LIB} ${ROC_THUNK_NAME} ${CORE_RUNTIME_TARGET} ${PROJECT_LINK_LIBS})
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch:+  ${ROCBLAS_LIB} ${ROCM_SMI_LIB} ${ROC_THUNK_NAME} ${PROJECT_LINK_LIBS} ${CORE_RUNTIME_NAME} ${YAML_CPP_LIB})
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch:@@ -41,7 +41,8 @@ link_directories(${RVS_LIB_DIR} ${ROCBLAS_LIB_DIR} ${ROCM_SMI_LIB_DIR} ${ROCT_LI
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch:-  ${ROCM_SMI_LIB} ${ROCBLAS_LIB} ${ROC_THUNK_NAME} ${CORE_RUNTIME_TARGET})
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch:+  ${ROCM_SMI_LIB} ${ROCBLAS_LIB} ${ROC_THUNK_NAME} ${CORE_RUNTIME_TARGET}
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch: link_directories(${UT_LIB} ${ROCBLAS_LIB_DIR} ${ROCM_SMI_LIB_DIR} ${ROCT_LIB_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch:     ${ROCM_SMI_LIB} ${ROCBLAS_LIB} ${ROC_THUNK_NAME} ${CORE_RUNTIME_TARGET}
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch:-  ${ROCM_SMI_INC_DIR} ${ROCR_INC_DIR} ${ROCBLAS_INC_DIR} ${HIP_INC_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch:+  ${ROCM_SMI_INC_DIR} ${HIP_PATH} ${ROCBLAS_INC_DIR} ${YAML_INC_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch: link_directories(${ASAN_LIB_PATH} ${ROCM_SMI_LIB_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch:+link_directories(${RVS_LIB_DIR} ${ASAN_LIB_PATH} ${ROCM_SMI_LIB_DIR})
var/spack/repos/builtin/packages/rocm-validation-suite/008-correcting-library-and-include-path-WITHOUT-RVS-BUILD-TESTS.patch:+link_directories(${RVS_LIB_DIR} ${ROCBLAS_LIB_DIR} ${ASAN_LIB_PATH} ${ROCM_SMI_LIB_DIR})
var/spack/repos/builtin/packages/tandem/package.py:class Tandem(CMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/tandem/package.py:        depends_on(f"{var} +cuda", when=f"+cuda ^[virtuals=mpi] {var}")
var/spack/repos/builtin/packages/tandem/package.py:        depends_on(f"{var} +rocm", when=f"+rocm ^[virtuals=mpi] {var}")
var/spack/repos/builtin/packages/tandem/package.py:    with when("+cuda"):
var/spack/repos/builtin/packages/tandem/package.py:        for tgt in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/tandem/package.py:            depends_on(f"petsc +cuda cuda_arch={tgt}", when=f"+cuda cuda_arch={tgt}")
var/spack/repos/builtin/packages/tandem/package.py:    with when("+rocm"):
var/spack/repos/builtin/packages/tandem/package.py:        for tgt in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/tandem/package.py:            depends_on(f"petsc +rocm amdgpu_target={tgt}", when=f"+rocm amdgpu_target={tgt}")
var/spack/repos/builtin/packages/tandem/package.py:    # GPU architecture requirements
var/spack/repos/builtin/packages/tandem/package.py:        "cuda_arch=none",
var/spack/repos/builtin/packages/tandem/package.py:        when="+cuda",
var/spack/repos/builtin/packages/tandem/package.py:        msg="A value for cuda_arch must be specified. Add cuda_arch=XX",
var/spack/repos/builtin/packages/tandem/package.py:        "amdgpu_target=none",
var/spack/repos/builtin/packages/tandem/package.py:        when="+rocm",
var/spack/repos/builtin/packages/tandem/package.py:        msg="A value for amdgpu_arch must be specified. Add amdgpu_arch=XX",
var/spack/repos/builtin/packages/ddt/package.py:    and threaded applications on CPUs, GPUs, Intel and Arm. Arm DDT is trusted
var/spack/repos/builtin/packages/amrex/package.py:class Amrex(CMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/amrex/package.py:        depends_on("ascent +cuda", when="+cuda")
var/spack/repos/builtin/packages/amrex/package.py:    for arch in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/amrex/package.py:            "sundials@5.7.0: +ARKODE +CVODE +cuda cuda_arch=%s" % arch,
var/spack/repos/builtin/packages/amrex/package.py:            when="@21.07:22.04 +sundials +cuda cuda_arch=%s" % arch,
var/spack/repos/builtin/packages/amrex/package.py:            "sundials@6.0.0: +ARKODE +CVODE +cuda cuda_arch=%s" % arch,
var/spack/repos/builtin/packages/amrex/package.py:            when="@22.05: +sundials +cuda cuda_arch=%s" % arch,
var/spack/repos/builtin/packages/amrex/package.py:    for tgt in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/amrex/package.py:            "sundials@5.7.0: +ARKODE +CVODE +rocm amdgpu_target=%s" % tgt,
var/spack/repos/builtin/packages/amrex/package.py:            when="@21.07:22.04 +sundials +rocm amdgpu_target=%s" % tgt,
var/spack/repos/builtin/packages/amrex/package.py:            "sundials@6.0.0: +ARKODE +CVODE +rocm amdgpu_target=%s" % tgt,
var/spack/repos/builtin/packages/amrex/package.py:            when="@22.05: +sundials +rocm amdgpu_target=%s" % tgt,
var/spack/repos/builtin/packages/amrex/package.py:    with when("+cuda"):
var/spack/repos/builtin/packages/amrex/package.py:        depends_on("cuda@9.0.0:", when="@:22.04")
var/spack/repos/builtin/packages/amrex/package.py:        depends_on("cuda@10.0.0:", when="@22.05:")
var/spack/repos/builtin/packages/amrex/package.py:        depends_on("cuda@11.0.0:", when="@22.12:")
var/spack/repos/builtin/packages/amrex/package.py:    # cmake @3.17: is necessary to handle cuda @11: correctly
var/spack/repos/builtin/packages/amrex/package.py:    depends_on("cmake@3.17:", type="build", when="^cuda @11:")
var/spack/repos/builtin/packages/amrex/package.py:    depends_on("cmake@3.20:", type="build", when="+rocm")
var/spack/repos/builtin/packages/amrex/package.py:    depends_on("rocrand", type="build", when="+rocm")
var/spack/repos/builtin/packages/amrex/package.py:    depends_on("hiprand", type="build", when="+rocm")
var/spack/repos/builtin/packages/amrex/package.py:    depends_on("rocprim", type="build", when="@21.05: +rocm")
var/spack/repos/builtin/packages/amrex/package.py:        depends_on("hypre@2.19.0:", type="link", when="@21.03: ~cuda")
var/spack/repos/builtin/packages/amrex/package.py:        depends_on("hypre@2.20.0:", type="link", when="@21.03: +cuda")
var/spack/repos/builtin/packages/amrex/package.py:        "+cuda", when="@:19.08", msg="AMReX CUDA support needs AMReX newer than version 19.08"
var/spack/repos/builtin/packages/amrex/package.py:    conflicts("cuda_arch=10", when="+cuda", msg="AMReX only supports compute capabilities >= 3.5")
var/spack/repos/builtin/packages/amrex/package.py:    conflicts("cuda_arch=11", when="+cuda", msg="AMReX only supports compute capabilities >= 3.5")
var/spack/repos/builtin/packages/amrex/package.py:    conflicts("cuda_arch=12", when="+cuda", msg="AMReX only supports compute capabilities >= 3.5")
var/spack/repos/builtin/packages/amrex/package.py:    conflicts("cuda_arch=13", when="+cuda", msg="AMReX only supports compute capabilities >= 3.5")
var/spack/repos/builtin/packages/amrex/package.py:    conflicts("cuda_arch=20", when="+cuda", msg="AMReX only supports compute capabilities >= 3.5")
var/spack/repos/builtin/packages/amrex/package.py:    conflicts("cuda_arch=21", when="+cuda", msg="AMReX only supports compute capabilities >= 3.5")
var/spack/repos/builtin/packages/amrex/package.py:    conflicts("cuda_arch=30", when="+cuda", msg="AMReX only supports compute capabilities >= 3.5")
var/spack/repos/builtin/packages/amrex/package.py:    conflicts("cuda_arch=32", when="+cuda", msg="AMReX only supports compute capabilities >= 3.5")
var/spack/repos/builtin/packages/amrex/package.py:        "+rocm", when="@:20.11", msg="AMReX HIP support needs AMReX newer than version 20.11"
var/spack/repos/builtin/packages/amrex/package.py:        "%rocm@4.2.0:4.2",
var/spack/repos/builtin/packages/amrex/package.py:        when="+rocm",
var/spack/repos/builtin/packages/amrex/package.py:        msg="AMReX does not support rocm-4.2 due to a compiler bug",
var/spack/repos/builtin/packages/amrex/package.py:    # GPU vendor support is mutually exclusive
var/spack/repos/builtin/packages/amrex/package.py:    conflicts("+cuda", when="+rocm", msg="CUDA and HIP support are exclusive")
var/spack/repos/builtin/packages/amrex/package.py:    conflicts("+cuda", when="+sycl", msg="CUDA and SYCL support are exclusive")
var/spack/repos/builtin/packages/amrex/package.py:    conflicts("+rocm", when="+sycl", msg="HIP and SYCL support are exclusive")
var/spack/repos/builtin/packages/amrex/package.py:    def get_cuda_arch_string(self, values):
var/spack/repos/builtin/packages/amrex/package.py:            # Use format x.y instead of CudaPackage xy format
var/spack/repos/builtin/packages/amrex/package.py:        if self.spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/amrex/package.py:            args.append("-DAMReX_GPU_BACKEND=CUDA")
var/spack/repos/builtin/packages/amrex/package.py:            args.append("-DAMReX_CUDA_ERROR_CAPTURE_THIS=ON")
var/spack/repos/builtin/packages/amrex/package.py:            args.append("-DAMReX_CUDA_ERROR_CROSS_EXECUTION_SPACE_CALL=ON")
var/spack/repos/builtin/packages/amrex/package.py:            cuda_arch = self.spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/amrex/package.py:            args.append("-DAMReX_CUDA_ARCH=" + self.get_cuda_arch_string(cuda_arch))
var/spack/repos/builtin/packages/amrex/package.py:        if self.spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/amrex/package.py:            args.append("-DAMReX_GPU_BACKEND=HIP")
var/spack/repos/builtin/packages/amrex/package.py:            targets = self.spec.variants["amdgpu_target"].value
var/spack/repos/builtin/packages/amrex/package.py:            args.append("-DAMReX_GPU_BACKEND=SYCL")
var/spack/repos/builtin/packages/amrex/package.py:            # SYCL GPU backend only supported with Intel's oneAPI or DPC++ compilers
var/spack/repos/builtin/packages/amrex/package.py:                    "AMReX's SYCL GPU Backend requires the oneAPI CXX (icpx) compiler."
var/spack/repos/builtin/packages/amrex/package.py:            self.define_from_variant("ENABLE_CUDA", "cuda"),
var/spack/repos/builtin/packages/amrex/package.py:        if self.spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/amrex/package.py:            cuda_arch = self.spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/amrex/package.py:            args.append("-DCUDA_ARCH=" + self.get_cuda_arch_string(cuda_arch))
var/spack/repos/builtin/packages/amrex/package.py:        if self.spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/amrex/package.py:            args.append("-DCMAKE_CUDA_COMPILER=" + join_path(self.spec["cuda"].prefix.bin, "nvcc"))
var/spack/repos/builtin/packages/gunrock/package.py:class Gunrock(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/gunrock/package.py:    """High-Performance Graph Primitives on GPUs"""
var/spack/repos/builtin/packages/gunrock/package.py:    variant("cuda", default=True, description="Build with Cuda support")
var/spack/repos/builtin/packages/gunrock/package.py:        "mgpu_tests",
var/spack/repos/builtin/packages/gunrock/package.py:            "for single GPU implementations"
var/spack/repos/builtin/packages/gunrock/package.py:        "cuda_verbose_ptxas",
var/spack/repos/builtin/packages/gunrock/package.py:        "cuda_arch=none",
var/spack/repos/builtin/packages/gunrock/package.py:        when="+cuda",
var/spack/repos/builtin/packages/gunrock/package.py:        msg='Must specify CUDA compute capabilities of your GPU. \
var/spack/repos/builtin/packages/gunrock/package.py:            from_variant("GUNROCK_MGPU_TESTS", "mgpu_tests"),
var/spack/repos/builtin/packages/gunrock/package.py:            from_variant("CUDA_VERBOSE_PTXAS", "cuda_verbose_ptxas"),
var/spack/repos/builtin/packages/gunrock/package.py:        # turn off auto detect, which undoes custom cuda arch options
var/spack/repos/builtin/packages/gunrock/package.py:        args.append("-DCUDA_AUTODETECT_GENCODE=OFF")
var/spack/repos/builtin/packages/gunrock/package.py:        cuda_arch_list = self.spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/gunrock/package.py:        if cuda_arch_list[0] != "none":
var/spack/repos/builtin/packages/gunrock/package.py:            for carch in cuda_arch_list:
var/spack/repos/builtin/packages/autodiff/package.py:class Autodiff(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/autodiff/package.py:    conflicts("+cuda", when="@:1.0", msg="CUDA support was added in 1.1.0")
var/spack/repos/builtin/packages/opencl-c-headers/package.py:class OpenclCHeaders(CMakePackage):
var/spack/repos/builtin/packages/opencl-c-headers/package.py:    """OpenCL (Open Computing Language) C header files"""
var/spack/repos/builtin/packages/opencl-c-headers/package.py:    homepage = "https://www.khronos.org/registry/OpenCL/"
var/spack/repos/builtin/packages/opencl-c-headers/package.py:    url = "https://github.com/KhronosGroup/OpenCL-Headers/archive/v2020.06.16.tar.gz"
var/spack/repos/builtin/packages/gromacs-swaxs/package.py:    conflicts("+opencl")
var/spack/repos/builtin/packages/py-torch-spline-conv/package.py:        if "+cuda" in self.spec["py-torch"]:
var/spack/repos/builtin/packages/py-torch-spline-conv/package.py:            env.set("FORCE_CUDA", 1)
var/spack/repos/builtin/packages/py-torch-spline-conv/package.py:            env.set("FORCE_ONLY_CUDA", 0)
var/spack/repos/builtin/packages/py-torch-spline-conv/package.py:            env.set("FORCE_CUDA", 0)
var/spack/repos/builtin/packages/py-torch-spline-conv/package.py:            env.set("FORCE_ONLY_CUDA", 0)
var/spack/repos/builtin/packages/kitty/package.py:    fast, featureful, cross-platform, GPU-based terminal emulator
var/spack/repos/builtin/packages/ipm/package.py:    variant("cuda", default=False, description="Enable CUDA")
var/spack/repos/builtin/packages/ipm/package.py:    depends_on("cuda", when="+cuda")
var/spack/repos/builtin/packages/ipm/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/ipm/package.py:            args.append("--with-cudapath={0}".format(spec["cuda"].prefix))
var/spack/repos/builtin/packages/3dtk/package.py:        "cuda",
var/spack/repos/builtin/packages/3dtk/package.py:        description="Whether to build CUDA accelerated collision detection tools",
var/spack/repos/builtin/packages/3dtk/package.py:    depends_on("cuda", when="+cuda")
var/spack/repos/builtin/packages/3dtk/package.py:            self.define_from_variant("WITH_CUDA", "cuda"),
var/spack/repos/builtin/packages/lc-framework/package.py:class LcFramework(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/lc-framework/package.py:    conflicts("+cuda", when="@:1.2.1")
var/spack/repos/builtin/packages/lc-framework/package.py:    for sm in [i for i in CudaPackage.cuda_arch_values if try_le(i, 60)]:
var/spack/repos/builtin/packages/lc-framework/package.py:            "cuda_arch={sm}".format(sm=sm), when="+cuda", msg="cuda_arch 60 or newer is required"
var/spack/repos/builtin/packages/lc-framework/package.py:    depends_on("libpressio+cuda", when="+cuda+libpressio")
var/spack/repos/builtin/packages/lc-framework/package.py:        if self.spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/lc-framework/package.py:            args.append(self.define_from_variant("LC_BUILD_CUDA", "cuda"))
var/spack/repos/builtin/packages/lc-framework/package.py:            args.append(self.builder.define_cuda_architectures(self))
var/spack/repos/builtin/packages/cosma/package.py:    # We just need the libraries of cuda and rocm, so no need to extend
var/spack/repos/builtin/packages/cosma/package.py:    # CudaPackage or ROCmPackage.
var/spack/repos/builtin/packages/cosma/package.py:    variant("cuda", default=False, description="Build with cuBLAS support")
var/spack/repos/builtin/packages/cosma/package.py:    variant("rocm", default=False, description="Build with rocBLAS support")
var/spack/repos/builtin/packages/cosma/package.py:    variant("gpu_direct", default=False, description="GPU aware MPI")
var/spack/repos/builtin/packages/cosma/package.py:    with when("+cuda"):
var/spack/repos/builtin/packages/cosma/package.py:        variant("nccl", default=False, description="Use cuda nccl")
var/spack/repos/builtin/packages/cosma/package.py:    with when("+rocm"):
var/spack/repos/builtin/packages/cosma/package.py:        variant("rccl", default=False, description="Use rocm rccl")
var/spack/repos/builtin/packages/cosma/package.py:    depends_on("blas", when="~cuda ~rocm")
var/spack/repos/builtin/packages/cosma/package.py:    depends_on("cuda", when="+cuda")
var/spack/repos/builtin/packages/cosma/package.py:    depends_on("rocblas", when="+rocm")
var/spack/repos/builtin/packages/cosma/package.py:    depends_on("nccl", when="+nccl")
var/spack/repos/builtin/packages/cosma/package.py:        depends_on("tiled-mm@2.2:+cuda", when="+cuda")
var/spack/repos/builtin/packages/cosma/package.py:        depends_on("tiled-mm@2.2:+rocm", when="+rocm")
var/spack/repos/builtin/packages/cosma/package.py:        depends_on("tiled-mm@2.0+rocm", when="+rocm")
var/spack/repos/builtin/packages/cosma/package.py:        depends_on("tiled-mm@2.0+cuda", when="+cuda")
var/spack/repos/builtin/packages/cosma/package.py:        if self.spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/cosma/package.py:            env.set("CUDA_PATH", self.spec["cuda"].prefix)
var/spack/repos/builtin/packages/cosma/package.py:            ("+cuda", "CUDA"),
var/spack/repos/builtin/packages/cosma/package.py:            ("+rocm", "ROCM"),
var/spack/repos/builtin/packages/cosma/package.py:            self.define_from_variant("COSMA_WITH_NCCL", "nccl"),
var/spack/repos/builtin/packages/cosma/package.py:            self.define_from_variant("COSMA_WITH_GPU_AWARE_MPI", "gpu_direct"),
var/spack/repos/builtin/packages/cosma/fj-ssl2.patch: set(COSMA_GPU_BACKENDS_LIST "CUDA" "ROCM")
var/spack/repos/builtin/packages/cosma/fj-ssl2.patch:-set(COSMA_BLAS_LIST   "auto" "MKL" "OPENBLAS" "CRAY_LIBSCI" "CUSTOM" "BLIS" "ATLAS" "CUDA" "ROCM" "OFF")
var/spack/repos/builtin/packages/cosma/fj-ssl2.patch:+set(COSMA_BLAS_LIST   "auto" "MKL" "SSL2" "OPENBLAS" "CRAY_LIBSCI" "CUSTOM" "BLIS" "ATLAS" "CUDA" "ROCM" "OFF")
var/spack/repos/builtin/packages/cosma/fj-ssl2.patch:@@ -45,7 +45,7 @@ if (COSMA_BLAS MATCHES "CUDA|ROCM")
var/spack/repos/builtin/packages/cosma/fj-ssl2.patch:   set(COSMA_GPU_BACKEND ${COSMA_BLAS})
var/spack/repos/builtin/packages/rocprofiler-dev/package.py:    homepage = "https://github.com/ROCm/rocprofiler"
var/spack/repos/builtin/packages/rocprofiler-dev/package.py:    git = "https://github.com/ROCm/rocprofiler.git"
var/spack/repos/builtin/packages/rocprofiler-dev/package.py:    url = "https://github.com/ROCm/rocprofiler/archive/refs/tags/rocm-6.1.2.tar.gz"
var/spack/repos/builtin/packages/rocprofiler-dev/package.py:    tags = ["rocm"]
var/spack/repos/builtin/packages/rocprofiler-dev/package.py:        depends_on(f"rocminfo@{ver}", when=f"@{ver}")
var/spack/repos/builtin/packages/rocprofiler-dev/package.py:        depends_on(f"rocm-smi-lib@{ver}", when=f"@{ver}")
var/spack/repos/builtin/packages/rocprofiler-dev/package.py:    # See https://github.com/ROCm/rocprofiler/pull/50
var/spack/repos/builtin/packages/rocprofiler-dev/package.py:            self.define("ROCM_ROOT_DIR", self.spec["hsakmt-roct"].prefix.include),
var/spack/repos/builtin/packages/rocprofiler-dev/0001-Continue-build-in-absence-of-aql-profile-lib.patch: find_library ( FIND_AQL_PROFILE_LIB "libhsa-amd-aqlprofile64.so" HINTS ${CMAKE_INSTALL_PREFIX} PATHS ${ROCM_ROOT_DIR})
var/spack/repos/builtin/packages/namd/package.py:class Namd(MakefilePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/namd/package.py:    variant("single_node_gpu", default=False, description="Single node GPU")
var/spack/repos/builtin/packages/namd/package.py:    conflicts("+rocm", when="+cuda", msg="NAMD supports only one GPU backend at a time")
var/spack/repos/builtin/packages/namd/package.py:    conflicts("+single_node_gpu", when="~cuda~rocm")
var/spack/repos/builtin/packages/namd/package.py:        when="+single_node_gpu",
var/spack/repos/builtin/packages/namd/package.py:        msg="memopt mode is not compatible with GPU-resident builds",
var/spack/repos/builtin/packages/namd/package.py:    depends_on("cuda@6.5.14:7.5.18", when="@2.12 +cuda")
var/spack/repos/builtin/packages/namd/package.py:    depends_on("cuda@8.0.61:", when="@2.13: +cuda")
var/spack/repos/builtin/packages/namd/package.py:        if "+cuda" in spec:
var/spack/repos/builtin/packages/namd/package.py:            self._append_option(opts, "cuda")
var/spack/repos/builtin/packages/namd/package.py:                "^CUDADIR=.*$",
var/spack/repos/builtin/packages/namd/package.py:                "CUDADIR={0}".format(spec["cuda"].prefix),
var/spack/repos/builtin/packages/namd/package.py:                join_path("arch", self.arch + ".cuda"),
var/spack/repos/builtin/packages/namd/package.py:            for cuda_arch in spec.variants["cuda_arch"].value:
var/spack/repos/builtin/packages/namd/package.py:                opts.extend(["--cuda-gencode", f"arch=compute_{cuda_arch},code=sm_{cuda_arch}"])
var/spack/repos/builtin/packages/namd/package.py:            if "+single_node_gpu" in spec:
var/spack/repos/builtin/packages/namd/package.py:                opts.extend(["--with-single-node-cuda"])
var/spack/repos/builtin/packages/namd/package.py:        if "+rocm" in spec:
var/spack/repos/builtin/packages/namd/package.py:            opts.extend(["--rocm-prefix", os.environ["ROCM_PATH"]])
var/spack/repos/builtin/packages/namd/package.py:            if "+single_node_gpu" in spec:
var/spack/repos/builtin/packages/tangram/package.py:    variant("thrust", default=False, description="Enable on-node parallelism with NVidia Thrust")
var/spack/repos/builtin/packages/tangram/package.py:    variant("cuda", default=False, description="Enable GPU parallelism using CUDA")
var/spack/repos/builtin/packages/tangram/package.py:    conflicts("+thrust +cuda")  # We don't have Thrust with CUDA working yet
var/spack/repos/builtin/packages/tangram/package.py:    wonton_depends = ["mpi", "jali", "openmp", "thrust", "kokkos", "cuda"]
var/spack/repos/builtin/packages/autodock-gpu/package.py:class AutodockGpu(MakefilePackage, CudaPackage):
var/spack/repos/builtin/packages/autodock-gpu/package.py:    """AutoDock-GPU: AutoDock for GPUs and other accelerators.
var/spack/repos/builtin/packages/autodock-gpu/package.py:    OpenCL and Cuda accelerated version of AutoDock 4.2.6. It
var/spack/repos/builtin/packages/autodock-gpu/package.py:    git = "https://github.com/ccsb-scripps/AutoDock-GPU.git"
var/spack/repos/builtin/packages/autodock-gpu/package.py:        default="cuda",
var/spack/repos/builtin/packages/autodock-gpu/package.py:        values=("cuda", "oclgpu"),
var/spack/repos/builtin/packages/autodock-gpu/package.py:    variant("overlap", default=False, description="Overlap CPU and GPU operations")
var/spack/repos/builtin/packages/autodock-gpu/package.py:    variant("cuda", default=True, description="Build with CUDA")
var/spack/repos/builtin/packages/autodock-gpu/package.py:    depends_on("cuda")
var/spack/repos/builtin/packages/autodock-gpu/package.py:    conflicts("~cuda")  # the cuda variant is mandatory
var/spack/repos/builtin/packages/autodock-gpu/package.py:    conflicts("+cuda", when="cuda_arch=none")
var/spack/repos/builtin/packages/autodock-gpu/package.py:            "TARGETS={0}".format(" ".join(spec.variants["cuda_arch"].value)),
var/spack/repos/builtin/packages/autodock-gpu/package.py:            "GPU_INCLUDE_PATH={0}".format(spec["cuda"].prefix.include),
var/spack/repos/builtin/packages/autodock-gpu/package.py:            "GPU_LIBRARY_PATH={0}".format(spec["cuda"].libs.directories[0]),
var/spack/repos/builtin/packages/quo-vadis/package.py:        "gpu",
var/spack/repos/builtin/packages/quo-vadis/package.py:        values=("nvidia", "amd", "none"),
var/spack/repos/builtin/packages/quo-vadis/package.py:        description="Build with GPU support",
var/spack/repos/builtin/packages/quo-vadis/package.py:    with when("gpu=nvidia"):
var/spack/repos/builtin/packages/quo-vadis/package.py:        depends_on("cuda")
var/spack/repos/builtin/packages/quo-vadis/package.py:    with when("gpu=amd"):
var/spack/repos/builtin/packages/quo-vadis/package.py:        depends_on("rocm-smi-lib")
var/spack/repos/builtin/packages/quo-vadis/package.py:            self.define("QV_GPU_SUPPORT", not spec.satisfies("gpu=none")),
var/spack/repos/builtin/packages/dihydrogen/package.py:class Dihydrogen(CachedCMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/dihydrogen/package.py:        when="%rocmcc",
var/spack/repos/builtin/packages/dihydrogen/package.py:    conflicts("+nvshmem", when="~cuda", msg="NVSHMEM requires CUDA support.")
var/spack/repos/builtin/packages/dihydrogen/package.py:    conflicts("+cuda", when="+rocm", msg="CUDA and ROCm are mutually exclusive.")
var/spack/repos/builtin/packages/dihydrogen/package.py:        "+cuda",
var/spack/repos/builtin/packages/dihydrogen/package.py:        "+rocm",
var/spack/repos/builtin/packages/dihydrogen/package.py:        msg="DistConv support requires CUDA or ROCm.",
var/spack/repos/builtin/packages/dihydrogen/package.py:    depends_on("cuda@11.0:", when="+cuda")
var/spack/repos/builtin/packages/dihydrogen/package.py:        for arch in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/dihydrogen/package.py:                "hydrogen +cuda cuda_arch={0}".format(arch),
var/spack/repos/builtin/packages/dihydrogen/package.py:                when="+cuda cuda_arch={0}".format(arch),
var/spack/repos/builtin/packages/dihydrogen/package.py:        for val in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/dihydrogen/package.py:                "hydrogen amdgpu_target={0}".format(val),
var/spack/repos/builtin/packages/dihydrogen/package.py:                when="+rocm amdgpu_target={0}".format(val),
var/spack/repos/builtin/packages/dihydrogen/package.py:        depends_on("aluminum +cuda +nccl", when="+distconv +cuda")
var/spack/repos/builtin/packages/dihydrogen/package.py:        depends_on("aluminum +rocm +nccl", when="+distconv +rocm")
var/spack/repos/builtin/packages/dihydrogen/package.py:        depends_on("nvshmem +nccl~ucx", when="+nvshmem")
var/spack/repos/builtin/packages/dihydrogen/package.py:        # CUDA/ROCm arch forwarding
var/spack/repos/builtin/packages/dihydrogen/package.py:        for arch in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/dihydrogen/package.py:                "aluminum +cuda cuda_arch={0}".format(arch),
var/spack/repos/builtin/packages/dihydrogen/package.py:                when="+cuda cuda_arch={0}".format(arch),
var/spack/repos/builtin/packages/dihydrogen/package.py:            # as it should be responsible for its own NCCL dependency.
var/spack/repos/builtin/packages/dihydrogen/package.py:                "nccl cuda_arch={0}".format(arch),
var/spack/repos/builtin/packages/dihydrogen/package.py:                when="+distconv +cuda cuda_arch={0}".format(arch),
var/spack/repos/builtin/packages/dihydrogen/package.py:                "nvshmem +cuda cuda_arch={0}".format(arch),
var/spack/repos/builtin/packages/dihydrogen/package.py:                when="+nvshmem +cuda cuda_arch={0}".format(arch),
var/spack/repos/builtin/packages/dihydrogen/package.py:        # Idenfity versions of cuda_arch that are too old from
var/spack/repos/builtin/packages/dihydrogen/package.py:        # lib/spack/spack/build_systems/cuda.py. We require >=60.
var/spack/repos/builtin/packages/dihydrogen/package.py:        illegal_cuda_arch_values = [
var/spack/repos/builtin/packages/dihydrogen/package.py:        for value in illegal_cuda_arch_values:
var/spack/repos/builtin/packages/dihydrogen/package.py:            conflicts("cuda_arch=" + value)
var/spack/repos/builtin/packages/dihydrogen/package.py:        for val in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/dihydrogen/package.py:                "aluminum amdgpu_target={0}".format(val),
var/spack/repos/builtin/packages/dihydrogen/package.py:                when="+rocm amdgpu_target={0}".format(val),
var/spack/repos/builtin/packages/dihydrogen/package.py:        # CUDA-specific distconv dependencies
var/spack/repos/builtin/packages/dihydrogen/package.py:        depends_on("cudnn", when="+cuda")
var/spack/repos/builtin/packages/dihydrogen/package.py:        # ROCm-specific distconv dependencies
var/spack/repos/builtin/packages/dihydrogen/package.py:        depends_on("hipcub", when="+rocm")
var/spack/repos/builtin/packages/dihydrogen/package.py:        depends_on("miopen-hip", when="+rocm")
var/spack/repos/builtin/packages/dihydrogen/package.py:        depends_on("roctracer-dev", when="+rocm")
var/spack/repos/builtin/packages/dihydrogen/package.py:    def get_cuda_flags(self):
var/spack/repos/builtin/packages/dihydrogen/package.py:        if spec.satisfies("^cuda+allow-unsupported-compilers"):
var/spack/repos/builtin/packages/dihydrogen/package.py:        entries.append(cmake_cache_option("H2_ENABLE_CUDA", "+cuda" in spec))
var/spack/repos/builtin/packages/dihydrogen/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/dihydrogen/package.py:            entries.append(cmake_cache_string("CMAKE_CUDA_STANDARD", "17"))
var/spack/repos/builtin/packages/dihydrogen/package.py:            if not spec.satisfies("cuda_arch=none"):
var/spack/repos/builtin/packages/dihydrogen/package.py:                archs = spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/dihydrogen/package.py:                entries.append(cmake_cache_string("CMAKE_CUDA_ARCHITECTURES", arch_str))
var/spack/repos/builtin/packages/dihydrogen/package.py:            # FIXME: Should this use the "cuda_flags" function of the
var/spack/repos/builtin/packages/dihydrogen/package.py:            # CudaPackage class or something? There might be other
var/spack/repos/builtin/packages/dihydrogen/package.py:            cuda_flags = self.get_cuda_flags()
var/spack/repos/builtin/packages/dihydrogen/package.py:            if len(cuda_flags) > 0:
var/spack/repos/builtin/packages/dihydrogen/package.py:                entries.append(cmake_cache_string("CMAKE_CUDA_FLAGS", " ".join(cuda_flags)))
var/spack/repos/builtin/packages/dihydrogen/package.py:        enable_rocm_var = (
var/spack/repos/builtin/packages/dihydrogen/package.py:            "H2_ENABLE_ROCM" if spec.version < Version("0.3") else "H2_ENABLE_HIP_ROCM"
var/spack/repos/builtin/packages/dihydrogen/package.py:        entries.append(cmake_cache_option(enable_rocm_var, "+rocm" in spec))
var/spack/repos/builtin/packages/dihydrogen/package.py:        if spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/dihydrogen/package.py:            if not spec.satisfies("amdgpu_target=none"):
var/spack/repos/builtin/packages/dihydrogen/package.py:                archs = self.spec.variants["amdgpu_target"].value
var/spack/repos/builtin/packages/dihydrogen/package.py:                entries.append(cmake_cache_string("AMDGPU_TARGETS", arch_str))
var/spack/repos/builtin/packages/dihydrogen/package.py:                entries.append(cmake_cache_string("GPU_TARGETS", arch_str))
var/spack/repos/builtin/packages/dihydrogen/package.py:            if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/rocalution/package.py:    eCosystem Platform ROCm runtime and toolchains, targeting modern
var/spack/repos/builtin/packages/rocalution/package.py:    CPU and GPU platforms. Based on C++ and HIP, it provides a portable,
var/spack/repos/builtin/packages/rocalution/package.py:    homepage = "https://github.com/ROCm/rocALUTION"
var/spack/repos/builtin/packages/rocalution/package.py:    git = "https://github.com/ROCm/rocALUTION.git"
var/spack/repos/builtin/packages/rocalution/package.py:    url = "https://github.com/ROCm/rocALUTION/archive/rocm-6.1.2.tar.gz"
var/spack/repos/builtin/packages/rocalution/package.py:    tags = ["rocm"]
var/spack/repos/builtin/packages/rocalution/package.py:    amdgpu_targets = ROCmPackage.amdgpu_targets
var/spack/repos/builtin/packages/rocalution/package.py:        "amdgpu_target",
var/spack/repos/builtin/packages/rocalution/package.py:        description="AMD GPU architecture",
var/spack/repos/builtin/packages/rocalution/package.py:        values=auto_or_any_combination_of(*amdgpu_targets),
var/spack/repos/builtin/packages/rocalution/package.py:        for tgt in itertools.chain(["auto"], amdgpu_targets):
var/spack/repos/builtin/packages/rocalution/package.py:                f"rocblas@{ver} amdgpu_target={rocblas_tgt}", when=f"@{ver} amdgpu_target={tgt}"
var/spack/repos/builtin/packages/rocalution/package.py:            depends_on(f"rocsparse@{ver} amdgpu_target={tgt}", when=f"@{ver} amdgpu_target={tgt}")
var/spack/repos/builtin/packages/rocalution/package.py:            depends_on(f"rocrand@{ver} amdgpu_target={tgt}", when=f"@{ver} amdgpu_target={tgt}")
var/spack/repos/builtin/packages/rocalution/package.py:        depends_on(f"rocm-cmake@{ver}:", type="build", when=f"@{ver}")
var/spack/repos/builtin/packages/rocalution/package.py:    # Fix build for most Radeon 5000 and Radeon 6000 series GPUs.
var/spack/repos/builtin/packages/rocalution/package.py:            env.set("CC", f"{self.spec['llvm-amdgpu'].prefix}/bin/clang")
var/spack/repos/builtin/packages/rocalution/package.py:            env.set("CXX", f"{self.spec['llvm-amdgpu'].prefix}/bin/clang++")
var/spack/repos/builtin/packages/rocalution/package.py:        if "auto" not in self.spec.variants["amdgpu_target"]:
var/spack/repos/builtin/packages/rocalution/package.py:            args.append(self.define_from_variant("AMDGPU_TARGETS", "amdgpu_target"))
var/spack/repos/builtin/packages/rocalution/package.py:            args.append(self.define("__skip_rocmclang", "ON"))
var/spack/repos/builtin/packages/rocalution/0004-fix-navi-1x.patch:when building for unsupported Navi 1x and Navi 2x GPUs.
var/spack/repos/builtin/packages/cbtf-argonavis-gui/package.py:    depends_on("cuda")
var/spack/repos/builtin/packages/cbtf-argonavis-gui/package.py:    depends_on("openspeedshop-utils+cuda@develop", when="@develop")
var/spack/repos/builtin/packages/cbtf-argonavis-gui/package.py:    depends_on("openspeedshop-utils@2.4.0:+cuda", when="@1.3.0.0:9999")
var/spack/repos/builtin/packages/mvapich2-gdr/package.py:    GPU-enabled HPC and Deep Learning Applications. MVAPICH2-GDR is not
var/spack/repos/builtin/packages/mvapich2-gdr/package.py:    variant("openacc", description="Enable/Disable support for openacc", default=False)
var/spack/repos/builtin/packages/mvapich2-gdr/package.py:    variant("cuda", description="Enable/Disable support for cuda", default=True)
var/spack/repos/builtin/packages/mvapich2-gdr/package.py:    variant("rocm", description="Enable/Disable support for ROCM", default=False)
var/spack/repos/builtin/packages/mvapich2-gdr/package.py:    conflicts("+rocm", when="@:2.3.4", msg="MVAPICH2-GDR only supports ROCm in version >= 2.3.5")
var/spack/repos/builtin/packages/mvapich2-gdr/package.py:    conflicts("+cuda +rocm", msg="MVAPICH2-GDR can only be built with either CUDA or ROCm")
var/spack/repos/builtin/packages/mvapich2-gdr/package.py:    conflicts("~cuda ~rocm", msg="MVAPICH2-GDR must be built with either CUDA or ROCm")
var/spack/repos/builtin/packages/mvapich2-gdr/package.py:    depends_on("cuda@9.2.88:", when="+cuda")
var/spack/repos/builtin/packages/mvapich2-gdr/package.py:    depends_on("hip@3.9.0:", when="+rocm")
var/spack/repos/builtin/packages/mvapich2-gdr/package.py:        if "+openacc" in spec:
var/spack/repos/builtin/packages/mvapich2-gdr/package.py:            opts.append("--enable-openacc")
var/spack/repos/builtin/packages/mvapich2-gdr/package.py:        if "+cuda" in spec:
var/spack/repos/builtin/packages/mvapich2-gdr/package.py:            opts.append("--enable-cuda")
var/spack/repos/builtin/packages/mvapich2-gdr/package.py:            opts.append("--disable-opencl")
var/spack/repos/builtin/packages/mvapich2-gdr/package.py:            opts.append("--with-cuda=" + spec["cuda"].prefix)
var/spack/repos/builtin/packages/mvapich2-gdr/package.py:        if "+rocm" in spec:
var/spack/repos/builtin/packages/mvapich2-gdr/package.py:            opts.append("--enable-rocm")
var/spack/repos/builtin/packages/mvapich2-gdr/package.py:            opts.append("--with-rocm=" + spec["hip"].prefix)
var/spack/repos/builtin/packages/dorado/package.py:class Dorado(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/dorado/package.py:    depends_on("cuda")
var/spack/repos/builtin/packages/caliper/package.py:class Caliper(CachedCMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/caliper/package.py:    conflicts("+rocm", "@:2.7")
var/spack/repos/builtin/packages/caliper/package.py:    conflicts("+rocm+cuda")
var/spack/repos/builtin/packages/caliper/package.py:        if spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/caliper/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/caliper/package.py:            entries.append(cmake_cache_path("CUDA_TOOLKIT_ROOT_DIR", spec["cuda"].prefix))
var/spack/repos/builtin/packages/caliper/package.py:            entries.append(cmake_cache_path("CUPTI_PREFIX", spec["cuda"].prefix))
var/spack/repos/builtin/packages/caliper/package.py:        if spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/ucx/commit-2523555.patch:From: Sergey Oblomov <sergeyo@nvidia.com>
var/spack/repos/builtin/packages/ucx/package.py:class Ucx(AutotoolsPackage, CudaPackage):
var/spack/repos/builtin/packages/ucx/package.py:    variant("rocm", default=False, description="Enable ROCm support")
var/spack/repos/builtin/packages/ucx/package.py:    depends_on("hip", when="+rocm")
var/spack/repos/builtin/packages/ucx/package.py:    depends_on("hsa-rocr-dev", when="+rocm")
var/spack/repos/builtin/packages/ucx/package.py:    conflicts("+gdrcopy", when="~cuda", msg="gdrcopy currently requires cuda support")
var/spack/repos/builtin/packages/ucx/package.py:    conflicts("+rocm", when="+gdrcopy", msg="gdrcopy > 2.0 does not support rocm")
var/spack/repos/builtin/packages/ucx/package.py:        if self.spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/ucx/package.py:            filter_file("$$with_rocm", "${with_rocm[@]}", "configure", string=True)
var/spack/repos/builtin/packages/ucx/package.py:                "-I$with_rocm/include/hip -I$with_rocm/include",
var/spack/repos/builtin/packages/ucx/package.py:                "$ROCM_CPPFLAGS",
var/spack/repos/builtin/packages/ucx/package.py:                "-L$with_rocm/hip/lib -L$with_rocm/lib", "$ROCM_LDFLAGS", "configure", string=True
var/spack/repos/builtin/packages/ucx/package.py:        args += self.with_or_without("cuda", activation_value="prefix")
var/spack/repos/builtin/packages/ucx/package.py:        if "+rocm" in spec:
var/spack/repos/builtin/packages/ucx/package.py:            rocm_flags = " ".join(
var/spack/repos/builtin/packages/ucx/package.py:            args.append("--with-rocm=" + rocm_flags)
var/spack/repos/builtin/packages/ucx/package.py:            args.append("--without-rocm")
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/12.6.2/local_installers/cuda_12.6.2_560.35.03_linux_sbsa.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/12.6.2/local_installers/cuda_12.6.2_560.35.03_linux.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/12.6.1/local_installers/cuda_12.6.1_560.35.03_linux_sbsa.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/12.6.1/local_installers/cuda_12.6.1_560.35.03_linux.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/12.6.0/local_installers/cuda_12.6.0_560.28.03_linux_sbsa.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/12.6.0/local_installers/cuda_12.6.0_560.28.03_linux.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/12.5.1/local_installers/cuda_12.5.1_555.42.06_linux_sbsa.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/12.5.1/local_installers/cuda_12.5.1_555.42.06_linux.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/12.5.0/local_installers/cuda_12.5.0_555.42.02_linux_sbsa.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/12.5.0/local_installers/cuda_12.5.0_555.42.02_linux.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/12.4.1/local_installers/cuda_12.4.1_550.54.15_linux_sbsa.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/12.4.1/local_installers/cuda_12.4.1_550.54.15_linux.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/12.4.1/local_installers/cuda_12.4.1_550.54.15_linux_ppc64le.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/12.4.0/local_installers/cuda_12.4.0_550.54.14_linux_sbsa.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/12.4.0/local_installers/cuda_12.4.0_550.54.14_linux.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/12.4.0/local_installers/cuda_12.4.0_550.54.14_linux_ppc64le.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/12.3.2/local_installers/cuda_12.3.2_545.23.08_linux_sbsa.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/12.3.2/local_installers/cuda_12.3.2_545.23.08_linux.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/12.3.2/local_installers/cuda_12.3.2_545.23.08_linux_ppc64le.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/12.3.1/local_installers/cuda_12.3.1_545.23.08_linux_sbsa.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/12.3.1/local_installers/cuda_12.3.1_545.23.08_linux.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/12.3.1/local_installers/cuda_12.3.1_545.23.08_linux_ppc64le.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/12.3.0/local_installers/cuda_12.3.0_545.23.06_linux_sbsa.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/12.3.0/local_installers/cuda_12.3.0_545.23.06_linux.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/12.3.0/local_installers/cuda_12.3.0_545.23.06_linux_ppc64le.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/12.2.2/local_installers/cuda_12.2.2_535.104.05_linux_sbsa.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/12.2.2/local_installers/cuda_12.2.2_535.104.05_linux.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/12.2.2/local_installers/cuda_12.2.2_535.104.05_linux_ppc64le.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/12.2.1/local_installers/cuda_12.2.1_535.86.10_linux_sbsa.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/12.2.1/local_installers/cuda_12.2.1_535.86.10_linux.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/12.2.1/local_installers/cuda_12.2.1_535.86.10_linux_ppc64le.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/12.2.0/local_installers/cuda_12.2.0_535.54.03_linux_sbsa.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/12.2.0/local_installers/cuda_12.2.0_535.54.03_linux.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/12.2.0/local_installers/cuda_12.2.0_535.54.03_linux_ppc64le.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/12.1.1/local_installers/cuda_12.1.1_530.30.02_linux_sbsa.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/12.1.1/local_installers/cuda_12.1.1_530.30.02_linux.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/12.1.1/local_installers/cuda_12.1.1_530.30.02_linux_ppc64le.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux_sbsa.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux_ppc64le.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/12.0.1/local_installers/cuda_12.0.1_525.85.12_linux_sbsa.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/12.0.1/local_installers/cuda_12.0.1_525.85.12_linux.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/12.0.1/local_installers/cuda_12.0.1_525.85.12_linux_ppc64le.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/12.0.0/local_installers/cuda_12.0.0_525.60.13_linux_sbsa.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/12.0.0/local_installers/cuda_12.0.0_525.60.13_linux.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/12.0.0/local_installers/cuda_12.0.0_525.60.13_linux_ppc64le.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.8.0/local_installers/cuda_11.8.0_520.61.05_linux_sbsa.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.8.0/local_installers/cuda_11.8.0_520.61.05_linux.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.8.0/local_installers/cuda_11.8.0_520.61.05_linux_ppc64le.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.7.1/local_installers/cuda_11.7.1_515.65.01_linux_sbsa.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.7.1/local_installers/cuda_11.7.1_515.65.01_linux.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.7.1/local_installers/cuda_11.7.1_515.65.01_linux_ppc64le.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.7.0/local_installers/cuda_11.7.0_515.43.04_linux_sbsa.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.7.0/local_installers/cuda_11.7.0_515.43.04_linux.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.7.0/local_installers/cuda_11.7.0_515.43.04_linux_ppc64le.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.6.2/local_installers/cuda_11.6.2_510.47.03_linux_sbsa.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.6.2/local_installers/cuda_11.6.2_510.47.03_linux.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.6.2/local_installers/cuda_11.6.2_510.47.03_linux_ppc64le.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.6.1/local_installers/cuda_11.6.1_510.47.03_linux_sbsa.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.6.1/local_installers/cuda_11.6.1_510.47.03_linux.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.6.1/local_installers/cuda_11.6.1_510.47.03_linux_ppc64le.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.6.0/local_installers/cuda_11.6.0_510.39.01_linux_sbsa.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.6.0/local_installers/cuda_11.6.0_510.39.01_linux.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.6.0/local_installers/cuda_11.6.0_510.39.01_linux_ppc64le.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.5.2/local_installers/cuda_11.5.2_495.29.05_linux_sbsa.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.5.2/local_installers/cuda_11.5.2_495.29.05_linux.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.5.2/local_installers/cuda_11.5.2_495.29.05_linux_ppc64le.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.5.1/local_installers/cuda_11.5.1_495.29.05_linux_sbsa.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.5.1/local_installers/cuda_11.5.1_495.29.05_linux.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.5.1/local_installers/cuda_11.5.1_495.29.05_linux_ppc64le.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.5.0/local_installers/cuda_11.5.0_495.29.05_linux_sbsa.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.5.0/local_installers/cuda_11.5.0_495.29.05_linux.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.5.0/local_installers/cuda_11.5.0_495.29.05_linux_ppc64le.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.4.4/local_installers/cuda_11.4.4_470.82.01_linux_sbsa.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.4.4/local_installers/cuda_11.4.4_470.82.01_linux.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.4.4/local_installers/cuda_11.4.4_470.82.01_linux_ppc64le.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.4.3/local_installers/cuda_11.4.3_470.82.01_linux_sbsa.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.4.3/local_installers/cuda_11.4.3_470.82.01_linux.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.4.3/local_installers/cuda_11.4.3_470.82.01_linux_ppc64le.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.4.2/local_installers/cuda_11.4.2_470.57.02_linux_sbsa.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.4.2/local_installers/cuda_11.4.2_470.57.02_linux.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.4.2/local_installers/cuda_11.4.2_470.57.02_linux_ppc64le.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.4.1/local_installers/cuda_11.4.1_470.57.02_linux_sbsa.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.4.1/local_installers/cuda_11.4.1_470.57.02_linux.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.4.1/local_installers/cuda_11.4.1_470.57.02_linux_ppc64le.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.4.0/local_installers/cuda_11.4.0_470.42.01_linux_sbsa.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.4.0/local_installers/cuda_11.4.0_470.42.01_linux.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.4.0/local_installers/cuda_11.4.0_470.42.01_linux_ppc64le.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.3.1/local_installers/cuda_11.3.1_465.19.01_linux_sbsa.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.3.1/local_installers/cuda_11.3.1_465.19.01_linux.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.3.1/local_installers/cuda_11.3.1_465.19.01_linux_ppc64le.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.3.0/local_installers/cuda_11.3.0_465.19.01_linux_sbsa.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.3.0/local_installers/cuda_11.3.0_465.19.01_linux.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.3.0/local_installers/cuda_11.3.0_465.19.01_linux_ppc64le.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.2.2/local_installers/cuda_11.2.2_460.32.03_linux_sbsa.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.2.2/local_installers/cuda_11.2.2_460.32.03_linux.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.2.2/local_installers/cuda_11.2.2_460.32.03_linux_ppc64le.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.2.1/local_installers/cuda_11.2.1_460.32.03_linux_sbsa.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.2.1/local_installers/cuda_11.2.1_460.32.03_linux.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.2.1/local_installers/cuda_11.2.1_460.32.03_linux_ppc64le.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.2.0/local_installers/cuda_11.2.0_460.27.04_linux_sbsa.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.2.0/local_installers/cuda_11.2.0_460.27.04_linux.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.2.0/local_installers/cuda_11.2.0_460.27.04_linux_ppc64le.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.1.1/local_installers/cuda_11.1.1_455.32.00_linux_sbsa.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.1.1/local_installers/cuda_11.1.1_455.32.00_linux.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.1.1/local_installers/cuda_11.1.1_455.32.00_linux_ppc64le.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.1.0/local_installers/cuda_11.1.0_455.23.05_linux_sbsa.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.1.0/local_installers/cuda_11.1.0_455.23.05_linux.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.1.0/local_installers/cuda_11.1.0_455.23.05_linux_ppc64le.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.0.3/local_installers/cuda_11.0.3_450.51.06_linux_sbsa.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.0.3/local_installers/cuda_11.0.3_450.51.06_linux.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.0.3/local_installers/cuda_11.0.3_450.51.06_linux_ppc64le.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.0.2/local_installers/cuda_11.0.2_450.51.05_linux_sbsa.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.0.2/local_installers/cuda_11.0.2_450.51.05_linux.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/11.0.2/local_installers/cuda_11.0.2_450.51.05_linux_ppc64le.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/10.2/Prod/local_installers/cuda_10.2.89_440.33.01_linux.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/10.2/Prod/local_installers/cuda_10.2.89_440.33.01_linux_ppc64le.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/10.1/Prod/local_installers/cuda_10.1.243_418.87.00_linux.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/10.1/Prod/local_installers/cuda_10.1.243_418.87.00_linux_ppc64le.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.nvidia.com/compute/cuda/10.0/Prod/local_installers/cuda_10.0.130_410.48_linux",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.nvidia.com/compute/cuda/9.2/Prod/local_installers/cuda_9.2.88_396.26_linux",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.nvidia.com/compute/cuda/9.1/Prod/local_installers/cuda_9.1.85_387.26_linux",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.nvidia.com/compute/cuda/9.0/Prod/local_installers/cuda_9.0.176_384.81_linux-run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.nvidia.com/compute/cuda/8.0/Prod2/local_installers/cuda_8.0.61_375.26_linux-run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.nvidia.com/compute/cuda/8.0/prod/local_installers/cuda_8.0.44_linux-run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/7.5/Prod/local_installers/cuda_7.5.18_linux.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/6_5/rel/installers/cuda_6.5.14_linux_64.run",
var/spack/repos/builtin/packages/cuda/package.py:            "https://developer.download.nvidia.com/compute/cuda/6_0/rel/installers/cuda_6.0.37_linux_64.run",
var/spack/repos/builtin/packages/cuda/package.py:class Cuda(Package):
var/spack/repos/builtin/packages/cuda/package.py:    """CUDA is a parallel computing platform and programming model invented
var/spack/repos/builtin/packages/cuda/package.py:    by NVIDIA. It enables dramatic increases in computing performance by
var/spack/repos/builtin/packages/cuda/package.py:    harnessing the power of the graphics processing unit (GPU).
var/spack/repos/builtin/packages/cuda/package.py:    to run CUDA. These will need to be installed manually. See:
var/spack/repos/builtin/packages/cuda/package.py:    https://docs.nvidia.com/cuda/ for details."""
var/spack/repos/builtin/packages/cuda/package.py:    homepage = "https://developer.nvidia.com/cuda-zone"
var/spack/repos/builtin/packages/cuda/package.py:    # macOS Mojave drops NVIDIA graphics card support -- official NVIDIA
var/spack/repos/builtin/packages/cuda/package.py:    # https://devtalk.nvidia.com/default/topic/1043070/announcements/faq-about-macos-10-14-mojave-nvidia-drivers/
var/spack/repos/builtin/packages/cuda/package.py:    # Note that a CUDA Toolkit installer does exist for macOS Mojave at
var/spack/repos/builtin/packages/cuda/package.py:    # https://developer.nvidia.com/compute/cuda/10.1/Prod1/local_installers/cuda_10.1.168_mac.dmg,
var/spack/repos/builtin/packages/cuda/package.py:    # macOS NVIDIA drivers at
var/spack/repos/builtin/packages/cuda/package.py:    # https://www.nvidia.com/en-us/drivers/cuda/mac-driver-archive/ mention
var/spack/repos/builtin/packages/cuda/package.py:        "dev", default=False, description="Enable development dependencies, i.e to use cuda-gdb"
var/spack/repos/builtin/packages/cuda/package.py:        description="Allow unsupported host compiler and CUDA version combinations",
var/spack/repos/builtin/packages/cuda/package.py:    # cuda-gdb needed libncurses.so.5 before 11.4.0
var/spack/repos/builtin/packages/cuda/package.py:    # see https://docs.nvidia.com/cuda/archive/11.3.1/cuda-gdb/index.html#common-issues-oss
var/spack/repos/builtin/packages/cuda/package.py:    # see https://docs.nvidia.com/cuda/archive/11.4.0/cuda-gdb/index.html#release-notes
var/spack/repos/builtin/packages/cuda/package.py:    provides("opencl@:1.2", when="@7:")
var/spack/repos/builtin/packages/cuda/package.py:    provides("opencl@:1.1", when="@:6")
var/spack/repos/builtin/packages/cuda/package.py:        match = re.search(r"Cuda compilation tools, release .*?, V(\S+)", output)
var/spack/repos/builtin/packages/cuda/package.py:            # CUDA 9 has a fix for this, but CUDA 8 and lower don't.
var/spack/repos/builtin/packages/cuda/package.py:        env.set("CUDAHOSTCXX", dependent_spec.package.compiler.cxx)
var/spack/repos/builtin/packages/cuda/package.py:        env.set("CUDA_HOME", self.prefix)
var/spack/repos/builtin/packages/cuda/package.py:        env.set("NVHPC_CUDA_HOME", self.prefix)
var/spack/repos/builtin/packages/cuda/package.py:        env.set("CUDA_HOME", self.prefix)
var/spack/repos/builtin/packages/cuda/package.py:        env.set("NVHPC_CUDA_HOME", self.prefix)
var/spack/repos/builtin/packages/cuda/package.py:        if os.path.exists("/tmp/cuda-installer.log"):
var/spack/repos/builtin/packages/cuda/package.py:                os.remove("/tmp/cuda-installer.log")
var/spack/repos/builtin/packages/cuda/package.py:                        "The cuda installer will segfault due to the "
var/spack/repos/builtin/packages/cuda/package.py:                        "presence of /tmp/cuda-installer.log "
var/spack/repos/builtin/packages/cuda/package.py:        runfile = glob(join_path(self.stage.source_path, "cuda*_linux*"))[0]
var/spack/repos/builtin/packages/cuda/package.py:        # Note: NVIDIA does not officially support many newer versions of
var/spack/repos/builtin/packages/cuda/package.py:        # http://docs.nvidia.com/cuda/cuda-installation-guide-linux/#system-requirements
var/spack/repos/builtin/packages/cuda/package.py:        # CUDA 10.1 on ppc64le fails to copy some files, the workaround is adapted from
var/spack/repos/builtin/packages/cuda/package.py:        # https://forums.developer.nvidia.com/t/cuda-10-1-243-10-1-update-2-ppc64le-run-file-installation-issue/82433
var/spack/repos/builtin/packages/cuda/package.py:            # found on the Internet, when people try to install CUDA <= 8 manually.
var/spack/repos/builtin/packages/cuda/package.py:        # CUDA 10.1+ has different cmdline options for the installer
var/spack/repos/builtin/packages/cuda/package.py:            "--toolkit",  # install CUDA Toolkit
var/spack/repos/builtin/packages/cuda/package.py:            os.remove("/tmp/cuda-installer.log")
var/spack/repos/builtin/packages/cuda/package.py:        libs = find_libraries("libcudart", root=self.prefix, shared=True, recursive=True)
var/spack/repos/builtin/packages/cuda/package.py:        # CUDA 10.0 provides Compatability libraries for running newer versions
var/spack/repos/builtin/packages/cuda/package.py:        # of CUDA with older drivers. These do not work with newer drivers.
var/spack/repos/builtin/packages/legion/package.py:class Legion(CMakePackage, ROCmPackage):
var/spack/repos/builtin/packages/legion/package.py:    depends_on("cuda@10.0:11.9", when="+cuda_unsupported_compiler @21.03.0:23.03.0")
var/spack/repos/builtin/packages/legion/package.py:    depends_on("cuda@10.0:11.9", when="+cuda @21.03.0:23.03.0")
var/spack/repos/builtin/packages/legion/package.py:    depends_on("cuda@10.0:", when="+cuda_unsupported_compiler")
var/spack/repos/builtin/packages/legion/package.py:    depends_on("cuda@10.0:", when="+cuda")
var/spack/repos/builtin/packages/legion/package.py:    depends_on("hip@5.1:5.7", when="+rocm @23.03.0:23.12.0")
var/spack/repos/builtin/packages/legion/package.py:    depends_on("hip@5.1:", when="+rocm")
var/spack/repos/builtin/packages/legion/package.py:    # cuda-centric
var/spack/repos/builtin/packages/legion/package.py:    cuda_arch_list = CudaPackage.cuda_arch_values
var/spack/repos/builtin/packages/legion/package.py:    for nvarch in cuda_arch_list:
var/spack/repos/builtin/packages/legion/package.py:            f"kokkos@3.3.01:+cuda+cuda_lambda+wrapper cuda_arch={nvarch}",
var/spack/repos/builtin/packages/legion/package.py:            when=f"%gcc+kokkos+cuda cuda_arch={nvarch}",
var/spack/repos/builtin/packages/legion/package.py:            f"kokkos@3.3.01:+cuda+cuda_lambda~wrapper cuda_arch={nvarch}",
var/spack/repos/builtin/packages/legion/package.py:            when=f"%clang+kokkos+cuda cuda_arch={nvarch}",
var/spack/repos/builtin/packages/legion/package.py:    depends_on("kokkos@3.3.01:~cuda", when="+kokkos~cuda")
var/spack/repos/builtin/packages/legion/package.py:    depends_on("kokkos@3.3.01:~cuda+openmp", when="+kokkos+openmp")
var/spack/repos/builtin/packages/legion/package.py:    patch("hip-offload-arch.patch", when="@23.03.0 +rocm")
var/spack/repos/builtin/packages/legion/package.py:        when="+rocm",
var/spack/repos/builtin/packages/legion/package.py:        default="ROCM",
var/spack/repos/builtin/packages/legion/package.py:        values=("ROCM", "CUDA"),
var/spack/repos/builtin/packages/legion/package.py:        when="+rocm",
var/spack/repos/builtin/packages/legion/package.py:    for arch in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/legion/package.py:        depends_on(f"kokkos@3.3.01:+rocm amdgpu_target={arch}", when=f"+rocm amdgpu_target={arch}")
var/spack/repos/builtin/packages/legion/package.py:    depends_on("kokkos@3.3.01:+rocm", when="+kokkos+rocm")
var/spack/repos/builtin/packages/legion/package.py:    # note: we will be dependent upon spack's latest-and-greatest cuda version...
var/spack/repos/builtin/packages/legion/package.py:    variant("cuda", default=False, description="Enable CUDA support.")
var/spack/repos/builtin/packages/legion/package.py:        "cuda_hijack",
var/spack/repos/builtin/packages/legion/package.py:        description="Hijack application calls into the CUDA runtime (+cuda).",
var/spack/repos/builtin/packages/legion/package.py:        "cuda_arch",
var/spack/repos/builtin/packages/legion/package.py:        values=cuda_arch_list,
var/spack/repos/builtin/packages/legion/package.py:        description="GPU/CUDA architecture to build for.",
var/spack/repos/builtin/packages/legion/package.py:        "cuda_unsupported_compiler",
var/spack/repos/builtin/packages/legion/package.py:    conflicts("+cuda_hijack", when="~cuda")
var/spack/repos/builtin/packages/legion/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/legion/package.py:            cuda_arch = spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/legion/package.py:            options.append("-DLegion_USE_CUDA=ON")
var/spack/repos/builtin/packages/legion/package.py:            options.append("-DLegion_GPU_REDUCTIONS=ON")
var/spack/repos/builtin/packages/legion/package.py:            options.append("-DLegion_CUDA_ARCH=%s" % cuda_arch)
var/spack/repos/builtin/packages/legion/package.py:            if spec.satisfies("+cuda_hijack"):
var/spack/repos/builtin/packages/legion/package.py:                options.append("-DLegion_HIJACK_CUDART=ON")
var/spack/repos/builtin/packages/legion/package.py:                options.append("-DLegion_HIJACK_CUDART=OFF")
var/spack/repos/builtin/packages/legion/package.py:            if spec.satisfies("+cuda_unsupported_compiler"):
var/spack/repos/builtin/packages/legion/package.py:                options.append("-DCUDA_NVCC_FLAGS:STRING=--allow-unsupported-compiler")
var/spack/repos/builtin/packages/legion/package.py:        if spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/legion/package.py:            options.append("-DLegion_GPU_REDUCTIONS=ON")
var/spack/repos/builtin/packages/legion/package.py:            options.append(from_variant("Legion_HIP_ARCH", "amdgpu_target"))
var/spack/repos/builtin/packages/legion/package.py:                options.append(self.define("ROCM_PATH", spec["hip"].prefix))
var/spack/repos/builtin/packages/legion/package.py:            if spec.satisfies("+cuda+cuda_unsupported_compiler ^kokkos%clang +cuda"):
var/spack/repos/builtin/packages/legion/package.py:                # Keep CMake CUDA compiler detection happy
var/spack/repos/builtin/packages/legion/package.py:                    self.define("CMAKE_CUDA_FLAGS", "--allow-unsupported-compiler -std=c++17")
var/spack/repos/builtin/packages/legion/README.md:The default build and install of Legion is suitable for laptop/desktop development needs.  Additional variants will need to be specified to build for distributed memory systems, GPU support, and other features (e.g., debugging, profiling, etc.).  A full list of the currently supported variants is provided below.
var/spack/repos/builtin/packages/legion/README.md:    spack install legion@stable~cuda+hdf5%clang10.0
var/spack/repos/builtin/packages/legion/README.md:will build/install the stable version of Legion without CUDA and with HDF5 support; using Clang 10.0 as the compiler.  See below for more examples of the various package options. 
var/spack/repos/builtin/packages/legion/README.md:### Processor Architecture Support (e.g., GPUs)
var/spack/repos/builtin/packages/legion/README.md:* **`cuda`**: This variant supports `on` or `off` and enables CUDA support within Legion.  `default=off`
var/spack/repos/builtin/packages/legion/README.md:* **`cuda_arch`**: This variant specifics the specific CUDA architecture to support within the Legion build/installation.  Currently this variant must be one of [`60`, `70`, `75`, or `80`].  Where `60` is the Pascal architecture, `70` is for Volta, `75` is for Turing, and `80` is for `Volta`. `default=70`
var/spack/repos/builtin/packages/legion/README.md:* **`cuda_hijack`**:  This variant supports `on` or `off` and determines if the build enables performance enhancements by "*hijacking* entry points into CUDA's runtime API; thus, it obviously implies `+cuda`.This is a performance enhancement and not necessary but suggested for production use cases on NVIDIA-based systems.  `default=off`
var/spack/repos/builtin/packages/legion/README.md:#### Adding GPU Support
var/spack/repos/builtin/packages/legion/README.md:To enable support for NVIDIA GPUs with Legion you can add the `+cuda` variant to the examples provided above.  In addition, you can also use the `cuda_arch` flag to enable specific GPU architectures.  For example, the follow command line installs the stable version of Legion with CUDA support for NVIDIA's Volta architecture:
var/spack/repos/builtin/packages/legion/README.md:`$ spack install legion@stable +cuda cuda_arch=70`
var/spack/repos/builtin/packages/legion/README.md:If you wanted to program tasks using Kokkos support for GPUs you can simply add the `+kokkos` option to the previous command line: 
var/spack/repos/builtin/packages/legion/README.md:`$ spack install legion@stable +cuda cuda_arch=70 +kokkos`
var/spack/repos/builtin/packages/legion/README.md:this will enable Kokkos interoperability in Legion and also build and configure a version of Kokkos with cuda and Volta GPU support.
var/spack/repos/builtin/packages/legion/README.md:`$ spack install legion@stable +cuda cuda_arch=70 +kokkos network=gasnet`
var/spack/repos/builtin/packages/legion/README.md:`$ spack install legion@stable +cuda cuda_arch=70 +kokkos network=gasnet conduit=ibv`
var/spack/repos/builtin/packages/legion/hip-offload-arch.patch:   else ifeq ($(strip $(HIP_TARGET)),CUDA)
var/spack/repos/builtin/packages/cabana/package.py:class Cabana(CMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/cabana/package.py:            elif _kk_version == "-legacy" and _backend not in ["serial", "openmp", "cuda"]:
var/spack/repos/builtin/packages/cabana/package.py:            # Handled separately by Cuda/ROCmPackage below
var/spack/repos/builtin/packages/cabana/package.py:            elif _backend == "cuda" or _backend == "hip":
var/spack/repos/builtin/packages/cabana/package.py:    # Propagate cuda architectures down to Kokkos and optional submodules
var/spack/repos/builtin/packages/cabana/package.py:    for arch in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/cabana/package.py:        cuda_dep = "+cuda cuda_arch={0}".format(arch)
var/spack/repos/builtin/packages/cabana/package.py:        depends_on("kokkos {0}".format(cuda_dep), when=cuda_dep)
var/spack/repos/builtin/packages/cabana/package.py:        depends_on("heffte {0}".format(cuda_dep), when="+heffte {0}".format(cuda_dep))
var/spack/repos/builtin/packages/cabana/package.py:        depends_on("arborx {0}".format(cuda_dep), when="+arborx {0}".format(cuda_dep))
var/spack/repos/builtin/packages/cabana/package.py:        depends_on("hypre {0}".format(cuda_dep), when="+hypre {0}".format(cuda_dep))
var/spack/repos/builtin/packages/cabana/package.py:    for arch in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/cabana/package.py:        rocm_dep = "+rocm amdgpu_target={0}".format(arch)
var/spack/repos/builtin/packages/cabana/package.py:        depends_on("kokkos {0}".format(rocm_dep), when=rocm_dep)
var/spack/repos/builtin/packages/cabana/package.py:        depends_on("heffte {0}".format(rocm_dep), when="+heffte {0}".format(rocm_dep))
var/spack/repos/builtin/packages/cabana/package.py:        depends_on("arborx {0}".format(rocm_dep), when="+arborx {0}".format(rocm_dep))
var/spack/repos/builtin/packages/cabana/package.py:        depends_on("hypre {0}".format(rocm_dep), when="+hypre {0}".format(rocm_dep))
var/spack/repos/builtin/packages/cabana/package.py:    conflicts("+cuda", when="cuda_arch=none")
var/spack/repos/builtin/packages/cabana/package.py:    conflicts("+rocm", when="amdgpu_target=none")
var/spack/repos/builtin/packages/cabana/package.py:    depends_on("kokkos+cuda_lambda@3.7:", when="+cuda")
var/spack/repos/builtin/packages/cabana/package.py:    depends_on("kokkos+cuda_lambda@4.1:", when="+cuda@0.7:")
var/spack/repos/builtin/packages/cabana/package.py:    conflicts("+rocm", when="@:0.2.0")
var/spack/repos/builtin/packages/cabana/package.py:            enable += ["Serial", "OpenMP", "Cuda"]
var/spack/repos/builtin/packages/cabana/package.py:        if self.spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/cudnn/package.py:    """NVIDIA cuDNN is a GPU-accelerated library of primitives for deep
var/spack/repos/builtin/packages/cudnn/package.py:    homepage = "https://developer.nvidia.com/cudnn"
var/spack/repos/builtin/packages/cudnn/package.py:    #     https://developer.nvidia.com/rdp/cudnn-download
var/spack/repos/builtin/packages/cudnn/package.py:    #     https://developer.nvidia.com/rdp/cudnn-archive
var/spack/repos/builtin/packages/cudnn/package.py:        cudnn_ver, cuda_ver = ver.split("-")
var/spack/repos/builtin/packages/cudnn/package.py:        long_ver = "{0}-{1}".format(cudnn_ver, cuda_ver)
var/spack/repos/builtin/packages/cudnn/package.py:            # Add constraints matching CUDA version to cuDNN version
var/spack/repos/builtin/packages/cudnn/package.py:            # cuDNN builds for CUDA 11.x are compatible with all CUDA 11.x:
var/spack/repos/builtin/packages/cudnn/package.py:            # https://docs.nvidia.com/deeplearning/cudnn/support-matrix/index.html#fntarg_2
var/spack/repos/builtin/packages/cudnn/package.py:            if Version(cuda_ver) >= Version("11"):
var/spack/repos/builtin/packages/cudnn/package.py:                cuda_ver = Version(cuda_ver).up_to(1)
var/spack/repos/builtin/packages/cudnn/package.py:            depends_on("cuda@{}".format(cuda_ver), when="@{}".format(long_ver))
var/spack/repos/builtin/packages/cudnn/package.py:        # Munge it to match Nvidia's naming scheme
var/spack/repos/builtin/packages/cudnn/package.py:            # artifact for cuda@10.2 x86_64, but the runtime is only supported
var/spack/repos/builtin/packages/cudnn/package.py:            # for cuda@11.  See
var/spack/repos/builtin/packages/cudnn/package.py:            # https://docs.nvidia.com/deeplearning/cudnn/release-notes/rel_8.html
var/spack/repos/builtin/packages/cudnn/package.py:            cuda = version[4:]
var/spack/repos/builtin/packages/cudnn/package.py:            directory = "{0}/local_installers/{1}".format(directory, cuda)
var/spack/repos/builtin/packages/cudnn/package.py:            cuda = version[4:]
var/spack/repos/builtin/packages/cudnn/package.py:            cuda = version[3:]
var/spack/repos/builtin/packages/cudnn/package.py:            cuda = version[3:]
var/spack/repos/builtin/packages/cudnn/package.py:            cuda = version[2:]
var/spack/repos/builtin/packages/cudnn/package.py:            url = "https://developer.download.nvidia.com/compute/cudnn/redist/cudnn/{0}/cudnn-{0}-{1}_cuda{2}-archive.tar.xz"
var/spack/repos/builtin/packages/cudnn/package.py:            return url.format(sys_key, ver, cuda.up_to(1))
var/spack/repos/builtin/packages/cudnn/package.py:        # 8.5.0 removed minor from cuda version
var/spack/repos/builtin/packages/cudnn/package.py:            url = "https://developer.download.nvidia.com/compute/redist/cudnn/v{0}/cudnn-{1}-{2}_cuda{3}-archive.tar.xz"
var/spack/repos/builtin/packages/cudnn/package.py:            return url.format(directory, sys_key, ver, cuda.up_to(1))
var/spack/repos/builtin/packages/cudnn/package.py:            url = "https://developer.download.nvidia.com/compute/redist/cudnn/v{0}/cudnn-{1}-{2}_cuda{3}-archive.tar.xz"
var/spack/repos/builtin/packages/cudnn/package.py:            return url.format(directory, sys_key, ver, cuda)
var/spack/repos/builtin/packages/cudnn/package.py:            url = "https://developer.download.nvidia.com/compute/redist/cudnn/v{0}/cudnn-{1}-{2}-v{3}.tgz"
var/spack/repos/builtin/packages/cudnn/package.py:            return url.format(directory, cuda, sys_key, ver)
var/spack/repos/builtin/packages/redset/package.py:class Redset(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/redset/package.py:    variant("cuda", default=False, description="Enable CUDA support", when="@0.4:")
var/spack/repos/builtin/packages/redset/package.py:        args.append(self.define_from_variant("ENABLE_CUDA", "cuda"))
var/spack/repos/builtin/packages/libpressio/package.py:class Libpressio(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/libpressio/package.py:    # cufile was only added to the .run file installer for cuda in 11.7.1
var/spack/repos/builtin/packages/libpressio/package.py:    depends_on("cuda@11.7.1:", when="+cuda")
var/spack/repos/builtin/packages/libpressio/package.py:    for cuda_compressor in ["cusz", "mgard", "zfp", "ndzip"]:
var/spack/repos/builtin/packages/libpressio/package.py:            f"~cuda+{cuda_compressor} ^ {cuda_compressor}+cuda",
var/spack/repos/builtin/packages/libpressio/package.py:            msg="compiling a CUDA compressor without a CUDA support makes no sense",
var/spack/repos/builtin/packages/libpressio/package.py:            self.define_from_variant("LIBPRESSIO_HAS_CUFILE", "cuda"),
var/spack/repos/builtin/packages/libpressio/package.py:            self.define_from_variant("LIBPRESSIO_HAS_CUDA", "cuda"),
var/spack/repos/builtin/packages/libpressio/package.py:        # if cuda is backed by the shim, we need to set these linker flags to
var/spack/repos/builtin/packages/libpressio/package.py:        if self.spec.satisfies("+cusz +cuda"):
var/spack/repos/builtin/packages/atmi/0002-Remove-usr-bin-rsync-reference-5.2.0.patch: # Looking for ROCM...
var/spack/repos/builtin/packages/atmi/package.py:    and programming model for heterogeneous CPU-GPU systems. It provides a
var/spack/repos/builtin/packages/atmi/package.py:    consistent, declarative API to create task graphs on CPUs and GPUs
var/spack/repos/builtin/packages/atmi/package.py:    homepage = "https://github.com/ROCm/atmi"
var/spack/repos/builtin/packages/atmi/package.py:    git = "https://github.com/ROCm/atmi.git"
var/spack/repos/builtin/packages/atmi/package.py:    url = "https://github.com/ROCm/atmi/archive/rocm-6.0.0.tar.gz"
var/spack/repos/builtin/packages/atmi/package.py:    tags = ["rocm"]
var/spack/repos/builtin/packages/atmi/package.py:        depends_on(f"rocm-core@{ver}", when=f"@{ver}")
var/spack/repos/builtin/packages/atmi/package.py:        args = [self.define("ROCM_VERSION", self.spec.version)]
var/spack/repos/builtin/packages/gasnet/package.py:class Gasnet(Package, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/gasnet/package.py:        "cuda",
var/spack/repos/builtin/packages/gasnet/package.py:        description="Enables support for the CUDA memory kind in some conduits.\n"
var/spack/repos/builtin/packages/gasnet/package.py:        + "NOTE: Requires CUDA Driver library be present on the build system",
var/spack/repos/builtin/packages/gasnet/package.py:        "+cuda",
var/spack/repos/builtin/packages/gasnet/package.py:        msg="GASNet version 2020.11.0 or newer required for CUDA support",
var/spack/repos/builtin/packages/gasnet/package.py:        "rocm",
var/spack/repos/builtin/packages/gasnet/package.py:        description="Enables support for the ROCm/HIP memory kind in some conduits",
var/spack/repos/builtin/packages/gasnet/package.py:        "+rocm", when="@:2021.8", msg="GASNet version 2021.9.0 or newer required for ROCm support"
var/spack/repos/builtin/packages/gasnet/package.py:        + "memory kind on Intel GPUs in some conduits",
var/spack/repos/builtin/packages/gasnet/package.py:    conflicts("^hip@:4.4.0", when="+rocm")
var/spack/repos/builtin/packages/gasnet/package.py:    conflicts("^hip@6:", when="@:2024.4+rocm")  # Bug 4686
var/spack/repos/builtin/packages/gasnet/package.py:            if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/gasnet/package.py:                options.append("--enable-kind-cuda-uva")
var/spack/repos/builtin/packages/gasnet/package.py:                options.append("--with-cuda-home=" + spec["cuda"].prefix)
var/spack/repos/builtin/packages/gasnet/package.py:            if spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/fftx/package.py:class Fftx(CMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/fftx/package.py:    conflicts("+rocm", when="+cuda", msg="FFTX only supports one GPU backend at a time")
var/spack/repos/builtin/packages/fftx/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/fftx/package.py:            backend = "CUDA"
var/spack/repos/builtin/packages/fftx/package.py:        if spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/fftx/package.py:        if spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/intel-tbb/package.py:        if spec.satisfies("%clang") or spec.satisfies("%apple-clang") or spec.satisfies("%rocmcc"):
var/spack/repos/builtin/packages/dftbplus/package.py:        "gpu",
var/spack/repos/builtin/packages/dftbplus/package.py:        description="Use the MAGMA library " "for GPU accelerated computation",
var/spack/repos/builtin/packages/dftbplus/package.py:    depends_on("magma", when="+gpu")
var/spack/repos/builtin/packages/dftbplus/package.py:        if self.spec.satisfies("+gpu"):
var/spack/repos/builtin/packages/dftbplus/package.py:            mconfig.filter("WITH_GPU := .*", "WITH_GPU := 1")
var/spack/repos/builtin/packages/dftbplus/package.py:            self.define_from_variant("WITH_GPU", "gpu"),
var/spack/repos/builtin/packages/warpx/package.py:    features hybrid GPU/OpenMP/MPI parallelization and load balancing capabilities.
var/spack/repos/builtin/packages/warpx/package.py:        values=("omp", "cuda", "hip", "sycl", "noacc"),
var/spack/repos/builtin/packages/warpx/package.py:        depends_on("ascent +cuda", when="+ascent compute=cuda")
var/spack/repos/builtin/packages/warpx/package.py:    with when("compute=cuda"):
var/spack/repos/builtin/packages/warpx/package.py:        depends_on("amrex +cuda")
var/spack/repos/builtin/packages/warpx/package.py:        depends_on("cuda@9.2.88:")
var/spack/repos/builtin/packages/warpx/package.py:        depends_on("cuda@11.0:", when="@22.01:")
var/spack/repos/builtin/packages/warpx/package.py:        depends_on("amrex +rocm")
var/spack/repos/builtin/packages/warpx/package.py:        depends_on("amrex ~cuda ~openmp ~rocm ~sycl")
var/spack/repos/builtin/packages/warpx/package.py:        depends_on("blaspp +cuda", when="compute=cuda")
var/spack/repos/builtin/packages/warpx/package.py:    # Fix failing 1D CUDA build
var/spack/repos/builtin/packages/opencl-icd-loader/package.py:class OpenclIcdLoader(CMakePackage):
var/spack/repos/builtin/packages/opencl-icd-loader/package.py:    """Khronos official OpenCL ICD Loader"""
var/spack/repos/builtin/packages/opencl-icd-loader/package.py:    homepage = "https://github.com/KhronosGroup/OpenCL-ICD-Loader"
var/spack/repos/builtin/packages/opencl-icd-loader/package.py:    url = "https://github.com/KhronosGroup/OpenCL-ICD-Loader/archive/refs/tags/v2024.05.08.tar.gz"
var/spack/repos/builtin/packages/opencl-icd-loader/package.py:    depends_on("opencl-c-headers@2024.05.08", when="@2024.05.08")
var/spack/repos/builtin/packages/opencl-icd-loader/package.py:    depends_on("opencl-c-headers@2023.12.14", when="@2023.12.14")
var/spack/repos/builtin/packages/opencl-icd-loader/package.py:    depends_on("opencl-c-headers@2023.04.17", when="@2023.04.17")
var/spack/repos/builtin/packages/opencl-icd-loader/package.py:    depends_on("opencl-c-headers@2023.02.06", when="@2023.02.06")
var/spack/repos/builtin/packages/opencl-icd-loader/package.py:    depends_on("opencl-c-headers@2022.09.30", when="@2022.09.30")
var/spack/repos/builtin/packages/opencl-icd-loader/package.py:    depends_on("opencl-c-headers@2022.09.23", when="@2022.09.23")
var/spack/repos/builtin/packages/opencl-icd-loader/package.py:    depends_on("opencl-c-headers@2022.05.18", when="@2022.05.18")
var/spack/repos/builtin/packages/opencl-icd-loader/package.py:    depends_on("opencl-c-headers@2022.01.04", when="@2022.01.04")
var/spack/repos/builtin/packages/opencl-icd-loader/package.py:    depends_on("opencl-c-headers@2021.06.30", when="@2021.06.30")
var/spack/repos/builtin/packages/opencl-icd-loader/package.py:    depends_on("opencl-c-headers@2021.04.29", when="@2021.04.29")
var/spack/repos/builtin/packages/opencl-icd-loader/package.py:    provides("opencl@:3.0")
var/spack/repos/builtin/packages/opencl-icd-loader/package.py:        headers_include_dir = self.spec["opencl-c-headers"].prefix.include
var/spack/repos/builtin/packages/opencl-icd-loader/package.py:        args = [self.define("OPENCL_ICD_LOADER_HEADERS_DIR", headers_include_dir)]
var/spack/repos/builtin/packages/omega-h/package.py:class OmegaH(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/omega-h/package.py:    hardware including GPUs.
var/spack/repos/builtin/packages/omega-h/package.py:    with when("+cuda"):
var/spack/repos/builtin/packages/omega-h/package.py:        depends_on("cuda@11.4:", when="@10.8.3:")
var/spack/repos/builtin/packages/omega-h/package.py:        depends_on("cuda@:11", when="@:10.6")
var/spack/repos/builtin/packages/omega-h/package.py:        # Single, broken CUDA version.
var/spack/repos/builtin/packages/omega-h/package.py:        conflicts("^cuda@11.2", msg="See https://github.com/sandialabs/omega_h/issues/366")
var/spack/repos/builtin/packages/omega-h/package.py:    conflicts("@10.5:10.8.5 +cuda~kokkos")
var/spack/repos/builtin/packages/omega-h/package.py:        if "+cuda" in self.spec:
var/spack/repos/builtin/packages/omega-h/package.py:            args.append("-DOmega_h_USE_CUDA:BOOL=ON")
var/spack/repos/builtin/packages/omega-h/package.py:            cuda_arch_list = self.spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/omega-h/package.py:            cuda_arch = cuda_arch_list[0]
var/spack/repos/builtin/packages/omega-h/package.py:            if cuda_arch != "none":
var/spack/repos/builtin/packages/omega-h/package.py:                    args.append("-DOmega_h_CUDA_ARCH={0}".format(cuda_arch))
var/spack/repos/builtin/packages/omega-h/package.py:                    args.append("-DCMAKE_CUDA_FLAGS=-arch=sm_{0}".format(cuda_arch))
var/spack/repos/builtin/packages/omega-h/package.py:            args.append("-DOmega_h_USE_CUDA:BOOL=OFF")
var/spack/repos/builtin/packages/pocl/package.py:    of the OpenCL standard which can be easily adapted for new targets
var/spack/repos/builtin/packages/pocl/package.py:    GPUs/accelerators."""
var/spack/repos/builtin/packages/pocl/package.py:    # < 3.0 provided full OpenCL 1.2 support and some intermediate level of
var/spack/repos/builtin/packages/pocl/package.py:    # OpenCL 2.0 support.  >= 3.0 provides full OpenCL 3.0 support when using
var/spack/repos/builtin/packages/pocl/package.py:    provides("opencl@2.0", when="^llvm@:13")
var/spack/repos/builtin/packages/pocl/package.py:    provides("opencl@3.0", when="@3: ^llvm@14:")
var/spack/repos/builtin/packages/pocl/package.py:            self.define("INSTALL_OPENCL_HEADERS", True),
var/spack/repos/builtin/packages/pocl/package.py:    def symlink_opencl(self):
var/spack/repos/builtin/packages/pocl/package.py:        os.symlink("CL", self.prefix.include.OpenCL)
var/spack/repos/builtin/packages/pocl/package.py:        # Build and run a small program to test the installed OpenCL library
var/spack/repos/builtin/packages/pocl/package.py:            ldflags = ["-L%s" % spec["pocl"].prefix.lib, "-lOpenCL", "-lpoclu"]
var/spack/repos/builtin/packages/pocl/example1.c:/* example1 - Simple example from OpenCL specification.
var/spack/repos/builtin/packages/pocl/example1.c:#include <CL/opencl.h>
var/spack/repos/builtin/packages/pocl/example1.c:  // get the list of GPU devices associated with context
var/spack/repos/builtin/packages/qtgraph/package.py:    """The baseline library used in the CUDA-centric Open|SpeedShop Graphical
var/spack/repos/builtin/packages/armcomputelibrary/package.py:    for Arm® Cortex®-A, Arm® Neoverse® and Arm® Mali™ GPUs architectures.
var/spack/repos/builtin/packages/armcomputelibrary/package.py:        " used with neon=0 and opencl=1.",
var/spack/repos/builtin/packages/cp2k/package.py:GPU_MAP = {
var/spack/repos/builtin/packages/cp2k/package.py:class Cp2k(MakefilePackage, CMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/cp2k/package.py:        description="Use SPLA off-loading functionality. Only relevant when CUDA or ROCM"
var/spack/repos/builtin/packages/cp2k/package.py:    with when("+cuda"):
var/spack/repos/builtin/packages/cp2k/package.py:            "cuda_arch_35_k20x",
var/spack/repos/builtin/packages/cp2k/package.py:                " different GPU models. Enable this when building"
var/spack/repos/builtin/packages/cp2k/package.py:                " with cuda_arch=35 for a K20x instead of a K40"
var/spack/repos/builtin/packages/cp2k/package.py:            "cuda_fft", default=False, description="Use CUDA also for FFTs in the PW part of CP2K"
var/spack/repos/builtin/packages/cp2k/package.py:            "cuda_blas",
var/spack/repos/builtin/packages/cp2k/package.py:        depends_on("spla+cuda+fortran", when="+cuda")
var/spack/repos/builtin/packages/cp2k/package.py:        depends_on("spla+rocm+fortran", when="+rocm")
var/spack/repos/builtin/packages/cp2k/package.py:        depends_on("cosma+cuda", when="+cuda")
var/spack/repos/builtin/packages/cp2k/package.py:        depends_on("cosma+rocm", when="+rocm")
var/spack/repos/builtin/packages/cp2k/package.py:        depends_on("elpa+cuda", when="+cuda")
var/spack/repos/builtin/packages/cp2k/package.py:        depends_on("elpa~cuda", when="~cuda")
var/spack/repos/builtin/packages/cp2k/package.py:        depends_on("elpa+rocm", when="+rocm")
var/spack/repos/builtin/packages/cp2k/package.py:        depends_on("elpa~rocm", when="~rocm")
var/spack/repos/builtin/packages/cp2k/package.py:            depends_on("dla-future ~cuda", when="~cuda")
var/spack/repos/builtin/packages/cp2k/package.py:            depends_on("dla-future ~rocm", when="~rocm")
var/spack/repos/builtin/packages/cp2k/package.py:            depends_on("dla-future +cuda", when="+cuda")
var/spack/repos/builtin/packages/cp2k/package.py:            depends_on("dla-future +rocm", when="+rocm")
var/spack/repos/builtin/packages/cp2k/package.py:            depends_on("dla-future ~cuda", when="~cuda")
var/spack/repos/builtin/packages/cp2k/package.py:            depends_on("dla-future ~rocm", when="~rocm")
var/spack/repos/builtin/packages/cp2k/package.py:            depends_on("dla-future +cuda", when="+cuda")
var/spack/repos/builtin/packages/cp2k/package.py:            depends_on("dla-future +rocm", when="+rocm")
var/spack/repos/builtin/packages/cp2k/package.py:        depends_on("sirius+cuda", when="+cuda")
var/spack/repos/builtin/packages/cp2k/package.py:        depends_on("sirius+rocm", when="+rocm")
var/spack/repos/builtin/packages/cp2k/package.py:    depends_on("py-numpy", when="@7:+cuda")
var/spack/repos/builtin/packages/cp2k/package.py:    depends_on("python@3.6:", when="@7:+cuda")
var/spack/repos/builtin/packages/cp2k/package.py:        depends_on("dbcsr+cuda", when="+cuda")
var/spack/repos/builtin/packages/cp2k/package.py:        depends_on("dbcsr+rocm", when="+rocm")
var/spack/repos/builtin/packages/cp2k/package.py:    with when("@2022: +rocm"):
var/spack/repos/builtin/packages/cp2k/package.py:    # We only support specific cuda_archs for which we have parameter files
var/spack/repos/builtin/packages/cp2k/package.py:    # for optimal kernels. Note that we don't override the cuda_archs property
var/spack/repos/builtin/packages/cp2k/package.py:    # versions. Instead just mark all unsupported cuda archs as conflicting.
var/spack/repos/builtin/packages/cp2k/package.py:    supported_cuda_arch_list = ("35", "37", "60", "70", "80", "90")
var/spack/repos/builtin/packages/cp2k/package.py:    supported_rocm_arch_list = ("gfx906", "gfx908", "gfx90a", "gfx90a:xnack-", "gfx90a:xnack+")
var/spack/repos/builtin/packages/cp2k/package.py:    cuda_msg = "cp2k only supports cuda_arch {0}".format(supported_cuda_arch_list)
var/spack/repos/builtin/packages/cp2k/package.py:    rocm_msg = "cp2k only supports amdgpu_target {0}".format(supported_rocm_arch_list)
var/spack/repos/builtin/packages/cp2k/package.py:    conflicts("+cuda", when="cuda_arch=none")
var/spack/repos/builtin/packages/cp2k/package.py:    # ROCm already emits an error if +rocm amdgpu_target=none is given
var/spack/repos/builtin/packages/cp2k/package.py:    with when("+cuda"):
var/spack/repos/builtin/packages/cp2k/package.py:        for arch in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/cp2k/package.py:            if arch not in supported_cuda_arch_list:
var/spack/repos/builtin/packages/cp2k/package.py:                conflicts("+cuda", when="cuda_arch={0}".format(arch), msg=cuda_msg)
var/spack/repos/builtin/packages/cp2k/package.py:    with when("+rocm"):
var/spack/repos/builtin/packages/cp2k/package.py:        for arch in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/cp2k/package.py:            if arch not in supported_rocm_arch_list:
var/spack/repos/builtin/packages/cp2k/package.py:                conflicts("+rocm", when="amdgpu_target={0}".format(arch), msg=rocm_msg)
var/spack/repos/builtin/packages/cp2k/package.py:                    r"ELPA_2STAGE_REAL_INTEL_GPU",
var/spack/repos/builtin/packages/cp2k/package.py:                    "ELPA_2STAGE_REAL_INTEL_GPU_SYCL",
var/spack/repos/builtin/packages/cp2k/package.py:        # Patch for resolving .mod file conflicts in ROCm by implementing 'USE, INTRINSIC'
var/spack/repos/builtin/packages/cp2k/package.py:        if self.spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/cp2k/package.py:            "rocmcc": ["-O1"],
var/spack/repos/builtin/packages/cp2k/package.py:        elif spec.satisfies("%aocc") or spec.satisfies("%rocmcc"):
var/spack/repos/builtin/packages/cp2k/package.py:        if spec.satisfies("@7:"):  # recent versions of CP2K use C++14 CUDA code
var/spack/repos/builtin/packages/cp2k/package.py:            if "+cuda" in spec and "+cuda" in elpa:
var/spack/repos/builtin/packages/cp2k/package.py:                cppflags += ["-D__ELPA_NVIDIA_GPU"]
var/spack/repos/builtin/packages/cp2k/package.py:        gpuver = ""
var/spack/repos/builtin/packages/cp2k/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/cp2k/package.py:                "-L{}".format(spec["cuda"].libs.directories[0]),
var/spack/repos/builtin/packages/cp2k/package.py:                "-L{}/stubs".format(spec["cuda"].libs.directories[0]),
var/spack/repos/builtin/packages/cp2k/package.py:                "-lcuda",
var/spack/repos/builtin/packages/cp2k/package.py:                "-lcudart",
var/spack/repos/builtin/packages/cp2k/package.py:                    cppflags += ["-D__OFFLOAD_CUDA"]
var/spack/repos/builtin/packages/cp2k/package.py:                cppflags += ["-D__DBCSR_ACC", "-D__GRID_CUDA", "-DOFFLOAD_TARGET=cuda"]
var/spack/repos/builtin/packages/cp2k/package.py:                if spec.satisfies("+cuda_fft"):
var/spack/repos/builtin/packages/cp2k/package.py:                        cppflags += ["-D__PW_CUDA"]
var/spack/repos/builtin/packages/cp2k/package.py:                if spec.satisfies("+cuda_blas"):
var/spack/repos/builtin/packages/cp2k/package.py:                if spec.satisfies("+cuda_fft"):
var/spack/repos/builtin/packages/cp2k/package.py:                    cppflags += ["-D__PW_CUDA"]
var/spack/repos/builtin/packages/cp2k/package.py:            cuda_arch = spec.variants["cuda_arch"].value[0]
var/spack/repos/builtin/packages/cp2k/package.py:            gpuver = GPU_MAP[cuda_arch]
var/spack/repos/builtin/packages/cp2k/package.py:            if cuda_arch == "35" and spec.satisfies("+cuda_arch_35_k20x"):
var/spack/repos/builtin/packages/cp2k/package.py:                gpuver = "K20X"
var/spack/repos/builtin/packages/cp2k/package.py:        if spec.satisfies("@2022: +rocm"):
var/spack/repos/builtin/packages/cp2k/package.py:            gpuver = GPU_MAP[spec.variants["amdgpu_target"].value[0]]
var/spack/repos/builtin/packages/cp2k/package.py:            if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/cp2k/package.py:                        acc_compiler_var, join_path(spec["cuda"].prefix, "bin", "nvcc")
var/spack/repos/builtin/packages/cp2k/package.py:            if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/cp2k/package.py:            if "+rocm" in spec:
var/spack/repos/builtin/packages/cp2k/package.py:            mkf.write("GPUVER = {0}\n".format(gpuver))
var/spack/repos/builtin/packages/cp2k/package.py:        if "+cuda" in spec and len(spec.variants["cuda_arch"].value) > 1:
var/spack/repos/builtin/packages/cp2k/package.py:            raise InstallError("cp2k supports only one cuda_arch at a time")
var/spack/repos/builtin/packages/cp2k/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/cp2k/package.py:            if (len(spec.variants["cuda_arch"].value) > 1) or spec.satisfies("cuda_arch=none"):
var/spack/repos/builtin/packages/cp2k/package.py:                raise InstallError("CP2K supports only one cuda_arch at a time.")
var/spack/repos/builtin/packages/cp2k/package.py:                gpu_ver = GPU_MAP[spec.variants["cuda_arch"].value[0]]
var/spack/repos/builtin/packages/cp2k/package.py:                    self.define("CP2K_USE_ACCEL", "CUDA"),
var/spack/repos/builtin/packages/cp2k/package.py:                    self.define("CP2K_WITH_GPU", gpu_ver),
var/spack/repos/builtin/packages/cp2k/package.py:        if spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/cp2k/package.py:            if len(spec.variants["amdgpu_target"].value) > 1:
var/spack/repos/builtin/packages/cp2k/package.py:                raise InstallError("CP2K supports only one amdgpu_target at a time.")
var/spack/repos/builtin/packages/cp2k/package.py:                gpu_ver = GPU_MAP[spec.variants["amdgpu_target"].value[0]]
var/spack/repos/builtin/packages/cp2k/package.py:                    self.define("CP2K_WITH_GPU", gpu_ver),
var/spack/repos/builtin/packages/cp2k/package.py:        if "spla" in spec and (spec.satisfies("+cuda") or spec.satisfies("+rocm")):
var/spack/repos/builtin/packages/cp2k/cmake-fixes-2023.2.patch:@@ -49,7 +49,8 @@ if(NOT DEFINED CMAKE_CUDA_STANDARD)
var/spack/repos/builtin/packages/cp2k/cmake-fixes-2023.2.patch:@@ -527,7 +532,7 @@ if(CP2K_USE_ACCEL MATCHES "CUDA")
var/spack/repos/builtin/packages/cp2k/cmake-fixes-2023.2.patch:   set(CP2K_USE_CUDA ON)
var/spack/repos/builtin/packages/cp2k/cmake-fixes-2023.2.patch:-  message(STATUS ``"-- CUDA compiler and libraries found")
var/spack/repos/builtin/packages/cp2k/cmake-fixes-2023.2.patch:+  message(STATUS "-- CUDA compiler and libraries found")
var/spack/repos/builtin/packages/cp2k/cmake-fixes-2023.2.patch:+  if(@CP2K_USE_CUDA@)
var/spack/repos/builtin/packages/cp2k/cmake-fixes-2023.2.patch:+    find_dependency(CUDAToolkit REQUIRED)
var/spack/repos/builtin/packages/cp2k/cmake-fixes-2023.2.patch:-if (@@CP2K_USE_CUDA@)
var/spack/repos/builtin/packages/cp2k/cmake-fixes-2023.2.patch:-  find_package(CUDAToolkit REQUIRED)
var/spack/repos/builtin/packages/neuron/package.py:        "gpu", default=False, description="Enable GPU build (with NVHPC)", when="@9:+coreneuron"
var/spack/repos/builtin/packages/neuron/package.py:    depends_on("cuda", when="+coreneuron+gpu")
var/spack/repos/builtin/packages/neuron/package.py:    gpu_compiler_message = "neuron+gpu needs %nvhpc"
var/spack/repos/builtin/packages/neuron/package.py:    requires("%nvhpc", when="+gpu", msg=gpu_compiler_message)
var/spack/repos/builtin/packages/neuron/package.py:            if spec.satisfies("+gpu"):
var/spack/repos/builtin/packages/neuron/package.py:                nvcc = spec["cuda"].prefix.bin.nvcc
var/spack/repos/builtin/packages/neuron/package.py:                options.append(self.define("CMAKE_CUDA_COMPILER", nvcc))
var/spack/repos/builtin/packages/neuron/package.py:                options.append(self.define("CORENRN_ENABLE_GPU", True))
var/spack/repos/builtin/packages/py-pygpu/package.py:class PyPygpu(PythonPackage):
var/spack/repos/builtin/packages/py-pygpu/package.py:    """Python packge for the libgpuarray C library."""
var/spack/repos/builtin/packages/py-pygpu/package.py:    homepage = "https://github.com/Theano/libgpuarray"
var/spack/repos/builtin/packages/py-pygpu/package.py:    url = "https://github.com/Theano/libgpuarray/archive/v0.6.1.tar.gz"
var/spack/repos/builtin/packages/py-pygpu/package.py:    depends_on("libgpuarray@0.7.6", when="@0.7.6")
var/spack/repos/builtin/packages/py-pygpu/package.py:    depends_on("libgpuarray@0.7.5", when="@0.7.5")
var/spack/repos/builtin/packages/py-pygpu/package.py:    depends_on("libgpuarray")
var/spack/repos/builtin/packages/rocm-device-libs/package.py:class RocmDeviceLibs(CMakePackage):
var/spack/repos/builtin/packages/rocm-device-libs/package.py:    homepage = "https://github.com/ROCm/ROCm-Device-Libs"
var/spack/repos/builtin/packages/rocm-device-libs/package.py:    git = "https://github.com/ROCm/ROCm-Device-Libs.git"
var/spack/repos/builtin/packages/rocm-device-libs/package.py:            url = "https://github.com/ROCm/ROCm-Device-Libs/archive/rocm-{0}.tar.gz"
var/spack/repos/builtin/packages/rocm-device-libs/package.py:            url = "https://github.com/ROCm/llvm-project/archive/rocm-{0}.tar.gz"
var/spack/repos/builtin/packages/rocm-device-libs/package.py:    tags = ["rocm"]
var/spack/repos/builtin/packages/rocm-device-libs/package.py:    depends_on("rocm-cmake@3.5.0:", type="build")
var/spack/repos/builtin/packages/rocm-device-libs/package.py:    # Make sure llvm is not built with rocm-device-libs (that is, it's already
var/spack/repos/builtin/packages/rocm-device-libs/package.py:    # built with rocm-device-libs as an external project).
var/spack/repos/builtin/packages/rocm-device-libs/package.py:    depends_on("llvm-amdgpu ~rocm-device-libs")
var/spack/repos/builtin/packages/rocm-device-libs/package.py:        depends_on(f"llvm-amdgpu@{ver}", when=f"@{ver}")
var/spack/repos/builtin/packages/rocm-device-libs/package.py:        depends_on(f"rocm-core@{ver}", when=f"@{ver}")
var/spack/repos/builtin/packages/rocm-device-libs/package.py:            self.define("LLVM_DIR", spec["llvm-amdgpu"].prefix),
var/spack/repos/builtin/packages/rocm-device-libs/package.py:            self.define("CMAKE_C_COMPILER", spec["llvm-amdgpu"].prefix.bin.clang),
var/spack/repos/builtin/packages/cradl/package.py:    depends_on("py-gputil")
var/spack/repos/builtin/packages/open3d/package.py:class Open3d(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/open3d/package.py:    depends_on("cuda@10.1:", when="+cuda")
var/spack/repos/builtin/packages/open3d/package.py:            self.define_from_variant("BUILD_CUDA_MODULE", "cuda"),
var/spack/repos/builtin/packages/neko/package.py:class Neko(AutotoolsPackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/neko/package.py:    # Requires cuda or rocm enabled MPI
var/spack/repos/builtin/packages/neko/package.py:        args += self.with_or_without("cuda", activation_value="prefix")
var/spack/repos/builtin/packages/neko/package.py:        rocm_fn = lambda x: self.spec["hip"].prefix
var/spack/repos/builtin/packages/neko/package.py:        args += self.with_or_without("hip", variant="rocm", activation_value=rocm_fn)
var/spack/repos/builtin/packages/neko/package.py:        if self.spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/neko/package.py:            cuda_arch_list = self.spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/neko/package.py:            cuda_arch = cuda_arch_list[0]
var/spack/repos/builtin/packages/neko/package.py:            if cuda_arch != "none":
var/spack/repos/builtin/packages/neko/package.py:                args.append(f"CUDA_ARCH=-arch=sm_{cuda_arch}")
var/spack/repos/builtin/packages/neko/package.py:        if self.spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/neko/package.py:            rocm_arch_list = self.spec.variants["amdgpu_target"].value
var/spack/repos/builtin/packages/neko/package.py:            rocm_arch = rocm_arch_list[0]
var/spack/repos/builtin/packages/neko/package.py:            if rocm_arch != "none":
var/spack/repos/builtin/packages/neko/package.py:                args.append(f"HIP_HIPCC_FLAGS=-O3 --offload-arch={rocm_arch}")
var/spack/repos/builtin/packages/bigdft-core/package.py:class BigdftCore(AutotoolsPackage, CudaPackage):
var/spack/repos/builtin/packages/bigdft-core/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/bigdft-core/package.py:            args.append("--enable-opencl")
var/spack/repos/builtin/packages/bigdft-core/package.py:            args.append(f"--with-ocl-path={spec['cuda'].prefix}")
var/spack/repos/builtin/packages/bigdft-core/package.py:            args.append("--enable-cuda-gpu")
var/spack/repos/builtin/packages/bigdft-core/package.py:            args.append(f"--with-cuda-path={spec['cuda'].prefix}")
var/spack/repos/builtin/packages/bigdft-core/package.py:            args.append(f"--with-cuda-libs={spec['cuda'].libs.link_flags}")
var/spack/repos/builtin/packages/magma/magma-2.5.0-cmake.patch:-cuda_add_library( magma_sparse ${libsparse_all} )
var/spack/repos/builtin/packages/magma/magma-2.5.0-cmake.patch:+  cuda_add_library( magma_sparse ${libsparse_all} )
var/spack/repos/builtin/packages/magma/magma-2.5.0-cmake.patch: 	${CUDA_CUDART_LIBRARY}
var/spack/repos/builtin/packages/magma/magma-2.5.0-cmake.patch: 	${CUDA_CUBLAS_LIBRARIES}
var/spack/repos/builtin/packages/magma/magma-2.5.0-cmake.patch: 	${CUDA_cusparse_LIBRARY}
var/spack/repos/builtin/packages/magma/magma-2.5.0-cmake.patch: message( STATUS "    NFLAGS       ${CUDA_NVCC_FLAGS}" )
var/spack/repos/builtin/packages/magma/package.py:class Magma(CMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/magma/package.py:    current "Multicore+GPU" systems.
var/spack/repos/builtin/packages/magma/package.py:    variant("cuda", default=True, description="Build with CUDA")
var/spack/repos/builtin/packages/magma/package.py:    depends_on("cuda@8:", when="@2.5.1: +cuda")  # See PR #14471
var/spack/repos/builtin/packages/magma/package.py:    depends_on("hipblas", when="+rocm")
var/spack/repos/builtin/packages/magma/package.py:    depends_on("hipsparse", when="+rocm")
var/spack/repos/builtin/packages/magma/package.py:    # This ensures that rocm-core matches the hip package version in the case that
var/spack/repos/builtin/packages/magma/package.py:        depends_on(f"rocm-core@{ver}", when=f"@2.8.0: +rocm ^hip@{ver}")
var/spack/repos/builtin/packages/magma/package.py:    conflicts("~cuda", when="~rocm", msg="magma: Either CUDA or HIP support must be enabled")
var/spack/repos/builtin/packages/magma/package.py:    conflicts("+rocm", when="+cuda", msg="magma: CUDA must be disabled to support HIP (ROCm)")
var/spack/repos/builtin/packages/magma/package.py:    conflicts("+rocm", when="@:2.5.4", msg="magma: HIP support starts in version 2.6.0")
var/spack/repos/builtin/packages/magma/package.py:        "cuda_arch=none", when="+cuda", msg="magma: Please indicate a CUDA arch value or values"
var/spack/repos/builtin/packages/magma/package.py:    # currently not compatible with CUDA-11
var/spack/repos/builtin/packages/magma/package.py:    # https://bitbucket.org/icl/magma/issues/22/cuda-11-changes-issue
var/spack/repos/builtin/packages/magma/package.py:    conflicts("^cuda@11:", when="@:2.5.3")
var/spack/repos/builtin/packages/magma/package.py:    # currently not compatible with CUDA-12.6
var/spack/repos/builtin/packages/magma/package.py:    conflicts("^cuda@12.6:", when="@:2.8.0")
var/spack/repos/builtin/packages/magma/package.py:    # Many cuda_arch values are not yet recognized by MAGMA's CMakeLists.txt
var/spack/repos/builtin/packages/magma/package.py:        conflicts(f"cuda_arch={target}")
var/spack/repos/builtin/packages/magma/package.py:    # Some cuda_arch values had support added recently
var/spack/repos/builtin/packages/magma/package.py:    conflicts("cuda_arch=37", when="@:2.5", msg="magma: cuda_arch=37 needs a version > 2.5")
var/spack/repos/builtin/packages/magma/package.py:    conflicts("cuda_arch=60", when="@:2.2", msg="magma: cuda_arch=60 needs a version > 2.2")
var/spack/repos/builtin/packages/magma/package.py:    conflicts("cuda_arch=70", when="@:2.2", msg="magma: cuda_arch=70 needs a version > 2.2")
var/spack/repos/builtin/packages/magma/package.py:    conflicts("cuda_arch=75", when="@:2.5.0", msg="magma: cuda_arch=75 needs a version > 2.5.0")
var/spack/repos/builtin/packages/magma/package.py:    conflicts("cuda_arch=80", when="@:2.5.3", msg="magma: cuda_arch=80 needs a version > 2.5.3")
var/spack/repos/builtin/packages/magma/package.py:    patch("0001-fix-magma-build-error-with-rocm-6.0.0.patch", when="@2.7.2 ^hip@6.0 + rocm")
var/spack/repos/builtin/packages/magma/package.py:    def generate_gpu_config(self):
var/spack/repos/builtin/packages/magma/package.py:        backend = "cuda" if "+cuda" in spec else "hip"
var/spack/repos/builtin/packages/magma/package.py:        gpu_target = ""
var/spack/repos/builtin/packages/magma/package.py:        if "+cuda" in spec:
var/spack/repos/builtin/packages/magma/package.py:            cuda_archs = spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/magma/package.py:            gpu_target = " ".join(f"sm_{i}" for i in cuda_archs)
var/spack/repos/builtin/packages/magma/package.py:            gpu_target = spec.variants["amdgpu_target"].value
var/spack/repos/builtin/packages/magma/package.py:            inc.write(f"GPU_TARGET = {gpu_target}\n")
var/spack/repos/builtin/packages/magma/package.py:            options.append(define("CUDA_NVCC_FLAGS", "-allow-unsupported-compiler"))
var/spack/repos/builtin/packages/magma/package.py:        if "+cuda" in spec:
var/spack/repos/builtin/packages/magma/package.py:            cuda_arch = spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/magma/package.py:            capabilities = " ".join(f"sm{sep}{i}" for i in cuda_arch)
var/spack/repos/builtin/packages/magma/package.py:            options.append(define("GPU_TARGET", capabilities))
var/spack/repos/builtin/packages/magma/package.py:            archs = ";".join("%s" % i for i in cuda_arch)
var/spack/repos/builtin/packages/magma/package.py:            options.append(define("CMAKE_CUDA_ARCHITECTURES", archs))
var/spack/repos/builtin/packages/magma/package.py:        if "+rocm" in spec:
var/spack/repos/builtin/packages/magma/package.py:            # See https://github.com/ROCm/rocFFT/issues/322
var/spack/repos/builtin/packages/magma/package.py:                options.append(define("__skip_rocmclang", True))
var/spack/repos/builtin/packages/magma/package.py:                options.append(define("ROCM_CORE", spec["rocm-core"].prefix))
var/spack/repos/builtin/packages/magma/package.py:            options.append(define("MAGMA_ENABLE_CUDA", True))
var/spack/repos/builtin/packages/magma/ibm-xl.patch:     magma_queue_t queues[MagmaMaxGPUs][2],
var/spack/repos/builtin/packages/magma/ibm-xl.patch:     magma_queue_t queues[MagmaMaxGPUs][2],
var/spack/repos/builtin/packages/magma/0001-fix-magma-build-error-with-rocm-6.0.0.patch:Subject: [PATCH] Fix Build Failure with rocm-6.0.0 . Add extra parameter for
var/spack/repos/builtin/packages/magma/0001-fix-magma-build-error-with-rocm-6.0.0.patch:-                    #ifdef MAGMA_HAVE_CUDA
var/spack/repos/builtin/packages/magma/0001-fix-magma-build-error-with-rocm-6.0.0.patch:                     g_magma_devices[dev].cuda_arch       = prop.major*100 + prop.minor*10;
var/spack/repos/builtin/packages/magma/0001-fix-magma-build-error-with-rocm-6.0.0.patch:+                    #ifdef MAGMA_HAVE_CUDA
var/spack/repos/builtin/packages/magma/0001-fix-magma-build-error-with-rocm-6.0.0.patch:-                    g_magma_devices[dev].cuda_arch       = prop.gcnArch;
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch:     // Half precision in CUDA 
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch:-    #if defined(__cplusplus) && CUDA_VERSION > 7500
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch:+    #if defined(__cplusplus) && CUDA_VERSION >= 7500
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch:     #include <cuda_fp16.h>
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch:+#if (__CUDA_ARCH__ >= 350)
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch:-    double* gputmp = (double*)*tmp_ptr;
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch:-    double* gputree = gputmp + total_size;
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch:-    uint32_t* gpubucketidx = (uint32_t*)(gputree + searchtree_size);
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch:-    int32_t* gpurankout = (int32_t*)(gpubucketidx + 1);
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch:-    int32_t* gpucounts = gpurankout + 1;
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch:-    int32_t* gpulocalcounts = gpucounts + searchtree_width;
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch:+        double* gputmp = (double*)*tmp_ptr;
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch:+        double* gputree = gputmp + total_size;
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch:+        uint32_t* gpubucketidx = (uint32_t*)(gputree + searchtree_size);
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch:+        int32_t* gpurankout = (int32_t*)(gpubucketidx + 1);
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch:+        int32_t* gpucounts = gpurankout + 1;
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch:+        int32_t* gpulocalcounts = gpucounts + searchtree_width;
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch:-    compute_abs<<<num_blocks, block_size, 0, queue->cuda_stream()>>>
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch:-        (val, gputmp, total_size);
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch:-    build_searchtree<<<1, sample_size, 0, queue->cuda_stream()>>>
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch:-        (gputmp, gputree, total_size);
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch:-    count_buckets<<<num_grouped_blocks, block_size, 0, queue->cuda_stream()>>>
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch:-        (gputmp, gputree, gpulocalcounts, total_size, local_work);
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch:-    reduce_counts<<<searchtree_width, num_grouped_blocks, 0, queue->cuda_stream()>>>
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch:-        (gpulocalcounts, gpucounts, num_grouped_blocks);
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch:-    sampleselect_findbucket<<<1, searchtree_width / 2, 0, queue->cuda_stream()>>>
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch:-        (gpucounts, subset_size, gpubucketidx, gpurankout);
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch:-    magma_getvector(1, sizeof(uint32_t), gpubucketidx, 1, &bucketidx, 1, queue);
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch:-    magma_dgetvector(1, gputree + searchtree_width - 1 + bucketidx, 1, thrs, 1, queue);
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch:+        compute_abs<<<num_blocks, block_size, 0, queue->cuda_stream()>>>
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch:+            (val, gputmp, total_size);
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch:+        build_searchtree<<<1, sample_size, 0, queue->cuda_stream()>>>
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch:+            (gputmp, gputree, total_size);
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch:+        count_buckets<<<num_grouped_blocks, block_size, 0, queue->cuda_stream()>>>
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch:+            (gputmp, gputree, gpulocalcounts, total_size, local_work);
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch:+        reduce_counts<<<searchtree_width, num_grouped_blocks, 0, queue->cuda_stream()>>>
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch:+            (gpulocalcounts, gpucounts, num_grouped_blocks);
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch:+        sampleselect_findbucket<<<1, searchtree_width / 2, 0, queue->cuda_stream()>>>
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch:+            (gpucounts, subset_size, gpubucketidx, gpurankout);
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch:+        magma_getvector(1, sizeof(uint32_t), gpubucketidx, 1, &bucketidx, 1, queue);
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch:+        magma_dgetvector(1, gputree + searchtree_width - 1 + bucketidx, 1, thrs, 1, queue);
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch:+        printf("error: this functionality needs CUDA architecture >= 3.5\n");
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch:diff -r 89706c0efbdb src/xhsgetrf_gpu.cpp
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch:--- a/src/xhsgetrf_gpu.cpp	Wed Jan 02 14:17:26 2019 -0500
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch:+++ b/src/xhsgetrf_gpu.cpp	Wed Apr 03 15:50:54 2019 -0700
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch: #include <cuda_fp16.h>
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch:+#if CUDA_VERSION < 9020
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch:+// conversion float to half are not defined for host in CUDA version <9.2
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch:+// thus uses the conversion below when CUDA VERSION is < 9.2.
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch:+// Copyright (c) 1993-2016, NVIDIA CORPORATION. All rights reserved.
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch:+//  * Neither the name of NVIDIA CORPORATION nor the names of its
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch:+    #if CUDA_VERSION >= 9020
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch:diff -r 89706c0efbdb src/xshgetrf_gpu.cpp
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch:--- a/src/xshgetrf_gpu.cpp	Wed Jan 02 14:17:26 2019 -0500
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch:+++ b/src/xshgetrf_gpu.cpp	Wed Apr 03 15:50:54 2019 -0700
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch:-#if CUDA_VERSION >= 7500
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch:+#if CUDA_VERSION >= 9000
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch:+#if CUDA_VERSION < 9020
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch:+// conversion float to half are not defined for host in CUDA version <9.2
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch:+// thus uses the conversion below when CUDA VERSION is < 9.2.
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch:+// Copyright (c) 1993-2016, NVIDIA CORPORATION. All rights reserved.
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch:+//  * Neither the name of NVIDIA CORPORATION nor the names of its
var/spack/repos/builtin/packages/magma/magma-2.5.0.patch:+    #if CUDA_VERSION >= 9020
var/spack/repos/builtin/packages/osu-micro-benchmarks/package.py:class OsuMicroBenchmarks(AutotoolsPackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/osu-micro-benchmarks/package.py:    and can be used for both traditional and GPU-enhanced nodes."""
var/spack/repos/builtin/packages/osu-micro-benchmarks/package.py:        if "+cuda" in spec:
var/spack/repos/builtin/packages/osu-micro-benchmarks/package.py:            config_args.extend(["--enable-cuda", "--with-cuda=%s" % spec["cuda"].prefix])
var/spack/repos/builtin/packages/osu-micro-benchmarks/package.py:            cuda_arch = spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/osu-micro-benchmarks/package.py:            if "none" not in cuda_arch:
var/spack/repos/builtin/packages/osu-micro-benchmarks/package.py:                config_args.append("NVCCFLAGS=" + " ".join(self.cuda_flags(cuda_arch)))
var/spack/repos/builtin/packages/osu-micro-benchmarks/package.py:        if "+rocm" in spec:
var/spack/repos/builtin/packages/osu-micro-benchmarks/package.py:            config_args.extend(["--enable-rocm", "--with-rocm=%s" % spec["hip"].prefix])
var/spack/repos/builtin/packages/osu-micro-benchmarks/package.py:            rocm_arch = spec.variants["amdgpu_target"].value
var/spack/repos/builtin/packages/osu-micro-benchmarks/package.py:            if "none" not in rocm_arch:
var/spack/repos/builtin/packages/osu-micro-benchmarks/package.py:                config_args.append("HCC_AMDGPU_TARGET=" + " ".join(self.hip_flags(rocm_arch)))
var/spack/repos/builtin/packages/dbcsr/package.py:class Dbcsr(CMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/dbcsr/package.py:        "cuda_arch_35_k20x",
var/spack/repos/builtin/packages/dbcsr/package.py:            " different GPU models. Enable this when building"
var/spack/repos/builtin/packages/dbcsr/package.py:            " with cuda_arch=35 for a K20x instead of a K40"
var/spack/repos/builtin/packages/dbcsr/package.py:    variant("opencl", default=False, description="Enable OpenCL backend")
var/spack/repos/builtin/packages/dbcsr/package.py:    variant("g2g", default=False, description="GPU-aware MPI with CUDA/HIP")
var/spack/repos/builtin/packages/dbcsr/package.py:    conflicts("+g2g", when="~cuda ~rocm", msg="GPU-aware MPI requires +cuda or +rocm")
var/spack/repos/builtin/packages/dbcsr/package.py:    depends_on("python@3.6:", type="build", when="+cuda")
var/spack/repos/builtin/packages/dbcsr/package.py:    depends_on("hipblas", when="+rocm")
var/spack/repos/builtin/packages/dbcsr/package.py:    depends_on("opencl", when="+opencl")
var/spack/repos/builtin/packages/dbcsr/package.py:    # We only support specific gpu archs for which we have parameter files
var/spack/repos/builtin/packages/dbcsr/package.py:    # Instead just mark all unsupported cuda archs as conflicting.
var/spack/repos/builtin/packages/dbcsr/package.py:    dbcsr_cuda_archs = ("35", "37", "60", "70", "80", "90")
var/spack/repos/builtin/packages/dbcsr/package.py:    cuda_msg = "dbcsr only supports cuda_arch {0}".format(dbcsr_cuda_archs)
var/spack/repos/builtin/packages/dbcsr/package.py:    for arch in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/dbcsr/package.py:        if arch not in dbcsr_cuda_archs:
var/spack/repos/builtin/packages/dbcsr/package.py:            conflicts("+cuda", when="cuda_arch={0}".format(arch), msg=cuda_msg)
var/spack/repos/builtin/packages/dbcsr/package.py:    conflicts("+cuda", when="cuda_arch=none", msg=cuda_msg)
var/spack/repos/builtin/packages/dbcsr/package.py:    dbcsr_amdgpu_targets = ("gfx906", "gfx910", "gfx90a", "gfx90a:xnack-", "gfx90a:xnack+")
var/spack/repos/builtin/packages/dbcsr/package.py:    amd_msg = f"DBCSR supports these AMD gpu targets:  {', '.join(dbcsr_amdgpu_targets)}"
var/spack/repos/builtin/packages/dbcsr/package.py:    for arch in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/dbcsr/package.py:        if arch not in dbcsr_amdgpu_targets:
var/spack/repos/builtin/packages/dbcsr/package.py:            conflicts("+rocm", when="amdgpu_target={0}".format(arch), msg=amd_msg)
var/spack/repos/builtin/packages/dbcsr/package.py:    accel_msg = "CUDA, ROCm and OpenCL support are mutually exlusive"
var/spack/repos/builtin/packages/dbcsr/package.py:    conflicts("+cuda", when="+rocm", msg=accel_msg)
var/spack/repos/builtin/packages/dbcsr/package.py:    conflicts("+cuda", when="+opencl", msg=accel_msg)
var/spack/repos/builtin/packages/dbcsr/package.py:    conflicts("+rocm", when="+opencl", msg=accel_msg)
var/spack/repos/builtin/packages/dbcsr/package.py:    conflicts("smm=blas", when="+opencl")
var/spack/repos/builtin/packages/dbcsr/package.py:        if "+cuda" in spec and len(spec.variants["cuda_arch"].value) > 1:
var/spack/repos/builtin/packages/dbcsr/package.py:            raise InstallError("dbcsr supports only one cuda_arch at a time")
var/spack/repos/builtin/packages/dbcsr/package.py:        if "+rocm" in spec and len(spec.variants["amdgpu_target"].value) > 1:
var/spack/repos/builtin/packages/dbcsr/package.py:            raise InstallError("DBCSR supports only one amdgpu_arch at a time")
var/spack/repos/builtin/packages/dbcsr/package.py:        if self.spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/dbcsr/package.py:            cuda_arch = self.spec.variants["cuda_arch"].value[0]
var/spack/repos/builtin/packages/dbcsr/package.py:            gpu_map = {
var/spack/repos/builtin/packages/dbcsr/package.py:            gpuver = gpu_map[cuda_arch]
var/spack/repos/builtin/packages/dbcsr/package.py:            if cuda_arch == "35" and self.spec.satisfies("+cuda_arch_35_k20x"):
var/spack/repos/builtin/packages/dbcsr/package.py:                gpuver = "K20X"
var/spack/repos/builtin/packages/dbcsr/package.py:            args += ["-DWITH_GPU=%s" % gpuver, "-DUSE_ACCEL=cuda"]
var/spack/repos/builtin/packages/dbcsr/package.py:        if self.spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/dbcsr/package.py:            amd_arch = self.spec.variants["amdgpu_target"].value[0]
var/spack/repos/builtin/packages/dbcsr/package.py:            gpuver = {
var/spack/repos/builtin/packages/dbcsr/package.py:            args += ["-DWITH_GPU={0}".format(gpuver), "-DUSE_ACCEL=hip"]
var/spack/repos/builtin/packages/dbcsr/package.py:        if self.spec.satisfies("+opencl"):
var/spack/repos/builtin/packages/dbcsr/package.py:            args += ["-DUSE_ACCEL=opencl"]
var/spack/repos/builtin/packages/mxnet/package.py:class Mxnet(CMakePackage, CudaPackage, PythonExtension):
var/spack/repos/builtin/packages/mxnet/package.py:    variant("cuda", default=True, description="Enable CUDA support")
var/spack/repos/builtin/packages/mxnet/package.py:    variant("nccl", default=False, description="Use NVidia NCCL with CUDA")
var/spack/repos/builtin/packages/mxnet/package.py:    depends_on("cuda", when="+cuda")
var/spack/repos/builtin/packages/mxnet/package.py:    depends_on("nccl", when="+nccl")
var/spack/repos/builtin/packages/mxnet/package.py:    conflicts("+cudnn", when="~cuda")
var/spack/repos/builtin/packages/mxnet/package.py:    conflicts("+nccl", when="~cuda")
var/spack/repos/builtin/packages/mxnet/package.py:    patch("cmake_cuda_flags.patch", when="@1.6:1.7")
var/spack/repos/builtin/packages/mxnet/package.py:        if self.spec.satisfies("+nccl ^nccl@2.1:"):
var/spack/repos/builtin/packages/mxnet/package.py:            env.set("NCCL_LAUNCH_MODE", "PARALLEL")
var/spack/repos/builtin/packages/mxnet/package.py:            self.define_from_variant("USE_CUDA", "cuda"),
var/spack/repos/builtin/packages/mxnet/package.py:        if "+cuda" in self.spec:
var/spack/repos/builtin/packages/mxnet/package.py:            if "cuda_arch=none" not in self.spec:
var/spack/repos/builtin/packages/mxnet/package.py:                cuda_arch = ";".join(
var/spack/repos/builtin/packages/mxnet/package.py:                    for i in self.spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/mxnet/package.py:                args.append(self.define("MXNET_CUDA_ARCH", cuda_arch))
var/spack/repos/builtin/packages/mxnet/package.py:                    self.define_from_variant("USE_NCCL", "nccl"),
var/spack/repos/builtin/packages/mxnet/package.py:                    # Workaround for bug in GCC 8+ and CUDA 10 on PowerPC
var/spack/repos/builtin/packages/mxnet/package.py:                    self.define("CMAKE_CUDA_FLAGS", self.compiler.cxx11_flag),
var/spack/repos/builtin/packages/mxnet/package.py:                        "-L" + join_path(self.spec["cuda"].libs.directories[0], "stubs"),
var/spack/repos/builtin/packages/mxnet/cmake_cuda_flags.patch:-  string(APPEND CMAKE_CUDA_FLAGS "${CUDA_ARCH_FLAGS_SPACES}")
var/spack/repos/builtin/packages/mxnet/cmake_cuda_flags.patch:+  string(APPEND CMAKE_CUDA_FLAGS " ${CUDA_ARCH_FLAGS_SPACES}")
var/spack/repos/builtin/packages/amdsmi/package.py:    homepage = "https://github.com/ROCm/amdsmi"
var/spack/repos/builtin/packages/amdsmi/package.py:    url = "https://github.com/ROCm/amdsmi/archive/refs/tags/rocm-6.1.2.tar.gz"
var/spack/repos/builtin/packages/amdsmi/package.py:    tags = ["rocm"]
var/spack/repos/builtin/packages/nvtx/package.py:    git = "https://github.com/NVIDIA/NVTX.git"
var/spack/repos/builtin/packages/nvtx/package.py:    url = "https://github.com/NVIDIA/NVTX/archive/refs/tags/v3.1.0.tar.gz"
var/spack/repos/builtin/packages/py-torchvision/package.py:    depends_on("cuda", when="+nvjpeg")
var/spack/repos/builtin/packages/py-torchvision/package.py:    depends_on("cuda", when="+video_codec")
var/spack/repos/builtin/packages/py-torchvision/package.py:        if "^cuda" in self.spec:
var/spack/repos/builtin/packages/py-torchvision/package.py:            env.set("CUDA_HOME", self.spec["cuda"].prefix)
var/spack/repos/builtin/packages/py-torchvision/package.py:        for gpu in ["cuda", "mps"]:
var/spack/repos/builtin/packages/py-torchvision/package.py:            env.set(f"FORCE_{gpu.upper()}", int(f"+{gpu}" in self.spec["py-torch"]))
var/spack/repos/builtin/packages/aqlprofile/package.py:            "https://repo.radeon.com/rocm/apt/6.2.1/pool/main/h/hsa-amd-aqlprofile/hsa-amd-aqlprofile_1.0.0.60201.60201-112~20.04_amd64.deb",
var/spack/repos/builtin/packages/aqlprofile/package.py:            "https://repo.radeon.com/rocm/yum/6.2.1/main/hsa-amd-aqlprofile-1.0.0.60201.60201-112.el7.x86_64.rpm",
var/spack/repos/builtin/packages/aqlprofile/package.py:            "https://repo.radeon.com/rocm/zyp/6.2.1/main/hsa-amd-aqlprofile-1.0.0.60201.60201-sles155.112.x86_64.rpm",
var/spack/repos/builtin/packages/aqlprofile/package.py:            "https://repo.radeon.com/rocm/apt/6.2/pool/main/h/hsa-amd-aqlprofile/hsa-amd-aqlprofile_1.0.0.60200.60200-66~20.04_amd64.deb",
var/spack/repos/builtin/packages/aqlprofile/package.py:            "https://repo.radeon.com/rocm/yum/6.2/main/hsa-amd-aqlprofile-1.0.0.60200.60200-66.el7.x86_64.rpm",
var/spack/repos/builtin/packages/aqlprofile/package.py:            "https://repo.radeon.com/rocm/zyp/6.2/main/hsa-amd-aqlprofile-1.0.0.60200.60200-sles155.66.x86_64.rpm",
var/spack/repos/builtin/packages/aqlprofile/package.py:            "https://repo.radeon.com/rocm/apt/6.1.2/pool/main/h/hsa-amd-aqlprofile/hsa-amd-aqlprofile_1.0.0.60102.60102-119~20.04_amd64.deb",
var/spack/repos/builtin/packages/aqlprofile/package.py:            "https://repo.radeon.com/rocm/yum/6.1.2/main/hsa-amd-aqlprofile-1.0.0.60102.60102-119.el7.x86_64.rpm",
var/spack/repos/builtin/packages/aqlprofile/package.py:            "https://repo.radeon.com/rocm/zyp/6.1.2/main/hsa-amd-aqlprofile-1.0.0.60102.60102-sles154.119.x86_64.rpm",
var/spack/repos/builtin/packages/aqlprofile/package.py:            "https://repo.radeon.com/rocm/apt/6.1.1/pool/main/h/hsa-amd-aqlprofile/hsa-amd-aqlprofile_1.0.0.60101.60101-90~20.04_amd64.deb",
var/spack/repos/builtin/packages/aqlprofile/package.py:            "https://repo.radeon.com/rocm/yum/6.1.1/main/hsa-amd-aqlprofile-1.0.0.60101.60101-90.el7.x86_64.rpm",
var/spack/repos/builtin/packages/aqlprofile/package.py:            "https://repo.radeon.com/rocm/zyp/6.1.1/main/hsa-amd-aqlprofile-1.0.0.60101.60101-sles154.90.x86_64.rpm",
var/spack/repos/builtin/packages/aqlprofile/package.py:            "https://repo.radeon.com/rocm/apt/6.1/pool/main/h/hsa-amd-aqlprofile/hsa-amd-aqlprofile_1.0.0.60100.60100-82~20.04_amd64.deb",
var/spack/repos/builtin/packages/aqlprofile/package.py:            "https://repo.radeon.com/rocm/yum/6.1/main/hsa-amd-aqlprofile-1.0.0.60100.60100-82.el7.x86_64.rpm",
var/spack/repos/builtin/packages/aqlprofile/package.py:            "https://repo.radeon.com/rocm/zyp/6.1/main/hsa-amd-aqlprofile-1.0.0.60100.60100-sles154.82.x86_64.rpm",
var/spack/repos/builtin/packages/aqlprofile/package.py:            "https://repo.radeon.com/rocm/apt/6.0.2/pool/main/h/hsa-amd-aqlprofile/hsa-amd-aqlprofile_1.0.0.60002.60002-115~20.04_amd64.deb",
var/spack/repos/builtin/packages/aqlprofile/package.py:            "https://repo.radeon.com/rocm/yum/6.0.2/main/hsa-amd-aqlprofile-1.0.0.60002.60002-115.el7.x86_64.rpm",
var/spack/repos/builtin/packages/aqlprofile/package.py:            "https://repo.radeon.com/rocm/zyp/6.0.2/main/hsa-amd-aqlprofile-1.0.0.60002.60002-sles154.115.x86_64.rpm",
var/spack/repos/builtin/packages/aqlprofile/package.py:            "https://repo.radeon.com/rocm/apt/6.0/pool/main/h/hsa-amd-aqlprofile/hsa-amd-aqlprofile_1.0.0.60000.60000-91~20.04_amd64.deb",
var/spack/repos/builtin/packages/aqlprofile/package.py:            "https://repo.radeon.com/rocm/yum/6.0/main/hsa-amd-aqlprofile-1.0.0.60000.60000-91.el7.x86_64.rpm",
var/spack/repos/builtin/packages/aqlprofile/package.py:            "https://repo.radeon.com/rocm/zyp/6.0/main/hsa-amd-aqlprofile-1.0.0.60000.60000-sles154.91.x86_64.rpm",
var/spack/repos/builtin/packages/aqlprofile/package.py:            "https://repo.radeon.com/rocm/apt/5.7.1/pool/main/h/hsa-amd-aqlprofile/hsa-amd-aqlprofile_1.0.0.50701.50701-98~20.04_amd64.deb",
var/spack/repos/builtin/packages/aqlprofile/package.py:            "https://repo.radeon.com/rocm/yum/5.7.1/main/hsa-amd-aqlprofile-1.0.0.50701.50701-98.el7.x86_64.rpm",
var/spack/repos/builtin/packages/aqlprofile/package.py:            "https://repo.radeon.com/rocm/zyp/5.7.1/main/hsa-amd-aqlprofile-1.0.0.50701.50701-sles154.98.x86_64.rpm",
var/spack/repos/builtin/packages/aqlprofile/package.py:            "https://repo.radeon.com/rocm/apt/5.7/pool/main/h/hsa-amd-aqlprofile/hsa-amd-aqlprofile_1.0.0.50700.50700-63~20.04_amd64.deb",
var/spack/repos/builtin/packages/aqlprofile/package.py:            "https://repo.radeon.com/rocm/yum/5.7/main/hsa-amd-aqlprofile-1.0.0.50700.50700-63.el7.x86_64.rpm",
var/spack/repos/builtin/packages/aqlprofile/package.py:            "https://repo.radeon.com/rocm/zyp/5.7/main/hsa-amd-aqlprofile-1.0.0.50700.50700-sles154.63.x86_64.rpm",
var/spack/repos/builtin/packages/aqlprofile/package.py:            "https://repo.radeon.com/rocm/apt/5.6.1/pool/main/h/hsa-amd-aqlprofile/hsa-amd-aqlprofile_1.0.0.50601-93~20.04_amd64.deb",
var/spack/repos/builtin/packages/aqlprofile/package.py:            "https://repo.radeon.com/rocm/yum/5.6.1/main/hsa-amd-aqlprofile-1.0.0.50601-93.el7.x86_64.rpm",
var/spack/repos/builtin/packages/aqlprofile/package.py:            "https://repo.radeon.com/rocm/zyp/5.6.1/main/hsa-amd-aqlprofile-1.0.0.50601-sles154.93.x86_64.rpm",
var/spack/repos/builtin/packages/aqlprofile/package.py:            "https://repo.radeon.com/rocm/apt/5.6/pool/main/h/hsa-amd-aqlprofile/hsa-amd-aqlprofile_1.0.0.50600-67~20.04_amd64.deb",
var/spack/repos/builtin/packages/aqlprofile/package.py:            "https://repo.radeon.com/rocm/yum/5.6/main/hsa-amd-aqlprofile-1.0.0.50600-67.el7.x86_64.rpm",
var/spack/repos/builtin/packages/aqlprofile/package.py:            "https://repo.radeon.com/rocm/zyp/5.6/main/hsa-amd-aqlprofile-1.0.0.50600-sles154.67.x86_64.rpm",
var/spack/repos/builtin/packages/aqlprofile/package.py:            "https://repo.radeon.com/rocm/apt/5.5.1/pool/main/h/hsa-amd-aqlprofile/hsa-amd-aqlprofile_1.0.0.50501-74~20.04_amd64.deb",
var/spack/repos/builtin/packages/aqlprofile/package.py:            "https://repo.radeon.com/rocm/yum/5.5.1/main/hsa-amd-aqlprofile-1.0.0.50501-74.el7.x86_64.rpm",
var/spack/repos/builtin/packages/aqlprofile/package.py:            "https://repo.radeon.com/rocm/zyp/5.5.1/main/hsa-amd-aqlprofile-1.0.0.50501-sles153.74.x86_64.rpm",
var/spack/repos/builtin/packages/aqlprofile/package.py:            "https://repo.radeon.com/rocm/apt/5.5/pool/main/h/hsa-amd-aqlprofile/hsa-amd-aqlprofile_1.0.0.50500-63~20.04_amd64.deb",
var/spack/repos/builtin/packages/aqlprofile/package.py:            "https://repo.radeon.com/rocm/yum/5.5/main/hsa-amd-aqlprofile-1.0.0.50500-63.el7.x86_64.rpm",
var/spack/repos/builtin/packages/aqlprofile/package.py:            "https://repo.radeon.com/rocm/zyp/5.5/main/hsa-amd-aqlprofile-1.0.0.50500-sles153.63.x86_64.rpm",
var/spack/repos/builtin/packages/aqlprofile/package.py:        install_tree(f"opt/rocm-{spec.version}/share/", prefix.share)
var/spack/repos/builtin/packages/aqlprofile/package.py:        install_tree(f"opt/rocm-{spec.version}/lib/", prefix.lib)
var/spack/repos/builtin/packages/py-pyarrow/package.py:class PyPyarrow(PythonPackage, CudaPackage):
var/spack/repos/builtin/packages/py-pyarrow/package.py:        depends_on("arrow+cuda" + v, when="+cuda" + v)
var/spack/repos/builtin/packages/py-pyarrow/package.py:        env.set("PYARROW_WITH_CUDA", self.spec.satisfies("+cuda"))
var/spack/repos/builtin/packages/py-pyarrow/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/py-pyarrow/package.py:            args.append("--with-cuda")
var/spack/repos/builtin/packages/vecgeom/package.py:class Vecgeom(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/vecgeom/package.py:    depends_on("veccore@0.8.1:", when="+cuda")
var/spack/repos/builtin/packages/vecgeom/package.py:    conflicts("+cuda", when="@:1.1.5")
var/spack/repos/builtin/packages/vecgeom/package.py:    # Fix missing CMAKE_CUDA_STANDARD
var/spack/repos/builtin/packages/vecgeom/package.py:        when="@1.1.7 +cuda",
var/spack/repos/builtin/packages/vecgeom/package.py:        when="@1.1.18 +cuda ^cuda@:11.4",
var/spack/repos/builtin/packages/vecgeom/package.py:        if "~cuda" in spec:
var/spack/repos/builtin/packages/vecgeom/package.py:            # Only add vectorization if CUDA is disabled due to nvcc flag
var/spack/repos/builtin/packages/vecgeom/package.py:            args.append(from_variant("VECGEOM_ENABLE_CUDA", "cuda"))
var/spack/repos/builtin/packages/vecgeom/package.py:            if "+cuda" in spec:
var/spack/repos/builtin/packages/vecgeom/package.py:                args.append(define("CMAKE_CUDA_ARCHITECTURES", spec.variants["cuda_arch"].value))
var/spack/repos/builtin/packages/vecgeom/package.py:            args.append(from_variant("CUDA"))
var/spack/repos/builtin/packages/vecgeom/package.py:            if "+cuda" in spec:
var/spack/repos/builtin/packages/vecgeom/package.py:                arch = spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/vecgeom/package.py:                    raise InstallError("Exactly one cuda_arch must be specified")
var/spack/repos/builtin/packages/vecgeom/package.py:                args.append(define("CUDA_ARCH", arch[0]))
var/spack/repos/builtin/packages/bigdft-liborbs/package.py:class BigdftLiborbs(AutotoolsPackage, CudaPackage):
var/spack/repos/builtin/packages/bigdft-liborbs/package.py:        if "+cuda" in spec:
var/spack/repos/builtin/packages/bigdft-liborbs/package.py:            args.append("--enable-opencl")
var/spack/repos/builtin/packages/bigdft-liborbs/package.py:            args.append(f"--with-ocl-path={spec['cuda'].prefix}")
var/spack/repos/builtin/packages/bigdft-liborbs/package.py:            args.append("--enable-cuda-gpu")
var/spack/repos/builtin/packages/bigdft-liborbs/package.py:            args.append(f"--with-cuda-path={spec['cuda'].prefix}")
var/spack/repos/builtin/packages/bigdft-liborbs/package.py:            args.append(f"--with-cuda-libs={spec['cuda'].libs.link_flags}")
var/spack/repos/builtin/packages/timemory/package.py:    for C/C++/Fortran/CUDA/Python"""
var/spack/repos/builtin/packages/timemory/package.py:    variant("nccl", default=False, description="Enable support for wrapping NCCL functions")
var/spack/repos/builtin/packages/timemory/package.py:    variant("cuda", default=False, description="Enable CUDA support")
var/spack/repos/builtin/packages/timemory/package.py:        "likwid_nvmon", default=False, description="Enable LIKWID GPU marker API support (nvmon)"
var/spack/repos/builtin/packages/timemory/package.py:        "cuda_arch",
var/spack/repos/builtin/packages/timemory/package.py:        description="CUDA architecture name",
var/spack/repos/builtin/packages/timemory/package.py:        "cudastd",
var/spack/repos/builtin/packages/timemory/package.py:        description="CUDA language standard",
var/spack/repos/builtin/packages/timemory/package.py:    depends_on("nccl", when="+nccl")
var/spack/repos/builtin/packages/timemory/package.py:    depends_on("cuda", when="+cuda")
var/spack/repos/builtin/packages/timemory/package.py:    depends_on("cuda", when="+cupti")
var/spack/repos/builtin/packages/timemory/package.py:    depends_on("likwid+cuda", when="+likwid+likwid_nvmon")
var/spack/repos/builtin/packages/timemory/package.py:    conflicts("+cupti", when="~cuda", msg="CUPTI requires CUDA")
var/spack/repos/builtin/packages/timemory/package.py:    conflicts("+nccl", when="~gotcha", msg="+nccl requires +gotcha")
var/spack/repos/builtin/packages/timemory/package.py:        "+nccl", when="~shared~static", msg="+nccl requires building shared or static libraries"
var/spack/repos/builtin/packages/timemory/package.py:            self.define_from_variant("CMAKE_CUDA_STANDARD", "cudastd"),
var/spack/repos/builtin/packages/timemory/package.py:            self.define_from_variant("TIMEMORY_BUILD_NCCLP_LIBRARY", "nccl"),
var/spack/repos/builtin/packages/timemory/package.py:            self.define_from_variant("TIMEMORY_USE_CUDA", "cuda"),
var/spack/repos/builtin/packages/timemory/package.py:            self.define_from_variant("TIMEMORY_USE_NCCL", "nccl"),
var/spack/repos/builtin/packages/timemory/package.py:        if "+cuda" in spec:
var/spack/repos/builtin/packages/timemory/package.py:            # newer versions use 'TIMEMORY_CUDA_ARCH'
var/spack/repos/builtin/packages/timemory/package.py:            key = "CUDA_ARCH" if spec.satisfies("@:3.0.1") else "TIMEMORY_CUDA_ARCH"
var/spack/repos/builtin/packages/timemory/package.py:            args.append(self.define_from_variant(key, "cuda_arch"))
var/spack/repos/builtin/packages/timemory/package.py:            args.append(self.define_from_variant("CMAKE_CUDA_STANDARD", "cudastd"))
var/spack/repos/builtin/packages/hipfft/package.py:class Hipfft(CMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/hipfft/package.py:    homepage = "https://github.com/ROCm/hipFFT"
var/spack/repos/builtin/packages/hipfft/package.py:    git = "https://github.com/ROCm/hipFFT.git"
var/spack/repos/builtin/packages/hipfft/package.py:    url = "https://github.com/ROCm/hipfft/archive/rocm-6.1.0.tar.gz"
var/spack/repos/builtin/packages/hipfft/package.py:    tags = ["rocm"]
var/spack/repos/builtin/packages/hipfft/package.py:    # default to an 'auto' variant until amdgpu_targets can be given a better default than 'none'
var/spack/repos/builtin/packages/hipfft/package.py:    amdgpu_targets = ROCmPackage.amdgpu_targets
var/spack/repos/builtin/packages/hipfft/package.py:        "amdgpu_target",
var/spack/repos/builtin/packages/hipfft/package.py:        description="AMD GPU architecture",
var/spack/repos/builtin/packages/hipfft/package.py:        values=spack.variant.DisjointSetsOfValues(("auto",), ("none",), amdgpu_targets)
var/spack/repos/builtin/packages/hipfft/package.py:    variant("rocm", default=True, description="Enable ROCm support")
var/spack/repos/builtin/packages/hipfft/package.py:    conflicts("+cuda +rocm", msg="CUDA and ROCm support are mutually exclusive")
var/spack/repos/builtin/packages/hipfft/package.py:    conflicts("~cuda ~rocm", msg="CUDA or ROCm support is required")
var/spack/repos/builtin/packages/hipfft/package.py:    depends_on("hip +cuda", when="+cuda")
var/spack/repos/builtin/packages/hipfft/package.py:        depends_on(f"rocm-cmake@{ver}:", type="build", when=f"@{ver}")
var/spack/repos/builtin/packages/hipfft/package.py:        depends_on(f"rocfft@{ver}", when=f"+rocm @{ver}")
var/spack/repos/builtin/packages/hipfft/package.py:    for tgt in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/hipfft/package.py:        depends_on(f"rocfft amdgpu_target={tgt}", when=f"+rocm amdgpu_target={tgt}")
var/spack/repos/builtin/packages/hipfft/package.py:    # https://github.com/ROCm/rocFFT/pull/85)
var/spack/repos/builtin/packages/hipfft/package.py:        if self.spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/hipfft/package.py:            args.append(self.define("BUILD_WITH_LIB", "ROCM"))
var/spack/repos/builtin/packages/hipfft/package.py:        elif self.spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/hipfft/package.py:            args.append(self.define("BUILD_WITH_LIB", "CUDA"))
var/spack/repos/builtin/packages/hipfft/package.py:        # FindHIP.cmake is still used for both +rocm and +cuda
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch: shared/gpubuf.h                          |  134 +
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch: create mode 100644 shared/gpubuf.h
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:-#include "../rocFFT/shared/gpubuf.h"
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+#include "../../shared/gpubuf.h"
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:-#include "../rocFFT/shared/gpubuf.h"
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+#include "../../shared/gpubuf.h"
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:   if( NOT BUILD_WITH_LIB STREQUAL "CUDA" )
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:-#include "../../../clients/rocFFT/shared/gpubuf.h"
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+#include "../../../shared/gpubuf.h"
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+#include "gpubuf.h"
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+// execute the GPU transform
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+inline void execute_gpu_fft(Tparams&              params,
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+                            std::vector<gpubuf>&  obuffer,
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+                            std::vector<hostbuf>& gpu_output,
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+    gpubuf_t<callback_test_data> load_cb_data_dev;
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+    gpubuf_t<callback_test_data> store_cb_data_dev;
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+    // if not comparing, then just executing the GPU FFT is all we
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+    // finalize a multi-GPU transform
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+    params.multi_gpu_finalize(obuffer, pobuffer);
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+    ASSERT_TRUE(!gpu_output.empty()) << "no output buffers";
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+    for(unsigned int idx = 0; idx < gpu_output.size(); ++idx)
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+        ASSERT_TRUE(gpu_output[idx].data() != nullptr)
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+        auto hip_status = hipMemcpy(gpu_output[idx].data(),
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+                                    gpu_output[idx].size(),
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+        std::cout << "GPU output:\n";
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+        params.print_obuffer(gpu_output);
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+        std::cout << "flat GPU output:\n";
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+        params.print_obuffer_flat(gpu_output);
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+                                   std::vector<gpubuf>&  obuffer,
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+                                   std::vector<hostbuf>& gpu_output)
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+    // execute GPU transform
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+    execute_gpu_fft(params, pibuffer, pobuffer, obuffer, gpu_output, true);
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+                                       std::vector<hostbuf>& gpu_output,
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+        check_output_strides<Tparams>(gpu_output, params);
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+    // compute GPU output norm
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+    std::shared_future<VectorNorms> gpu_norm = std::async(std::launch::async, [&]() {
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+        return norm(gpu_output,
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+    // compare GPU inverse output to CPU forward input
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+                                gpu_output,
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+        std::cout << "GPU output Linf norm: " << gpu_norm.get().l_inf << "\n";
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+        std::cout << "GPU output L2 norm:   " << gpu_norm.get().l_2 << "\n";
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+        std::cout << "GPU linf norm failures:";
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+    EXPECT_TRUE(std::isfinite(gpu_norm.get().l_inf)) << params.str();
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+    EXPECT_TRUE(std::isfinite(gpu_norm.get().l_2)) << params.str();
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+    // that are too large to fit in the gpu (no plan created with the rocFFT backend)
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+    std::vector<gpubuf> ibuffer(ibuffer_sizes.size());
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+    std::vector<hostbuf> gpu_input_data;
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+    // allocate and populate the input buffer (cpu/gpu)
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+        gpu_input_data = allocate_host_buffer(params.precision, params.itype, ibuffer_sizes_elems);
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+        //generate the input directly on the gpu
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+                hip_status = hipMemcpy(gpu_input_data.at(idx).data(),
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+            copy_buffers(gpu_input_data,
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+        gpu_input_data = allocate_host_buffer(params.precision, params.itype, ibuffer_sizes_elems);
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+        // gets a pre-computed gpu input buffer from the cpu cache
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+        std::vector<hostbuf>* gpu_input = &cpu_input;
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+                         gpu_input_data,
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+            gpu_input = &gpu_input_data;
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+        // Copy input to GPU
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+        for(unsigned int idx = 0; idx < gpu_input->size(); ++idx)
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+                                   gpu_input->at(idx).data(),
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+    std::vector<gpubuf>  obuffer_data;
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+    std::vector<gpubuf>* obuffer = &obuffer_data;
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+    // NOTE: This must happen after input is copied to GPU and input
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+    // scatter data out to multi-GPUs if this is a multi-GPU test
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+    params.multi_gpu_prepare(ibuffer, pibuffer, pobuffer);
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+    // execute GPU transform
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+    std::vector<hostbuf> gpu_output
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+    execute_gpu_fft(params, pibuffer, pobuffer, *obuffer, gpu_output);
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+        check_output_strides<Tparams>(gpu_output, params);
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+    // compute GPU output norm
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+    std::shared_future<VectorNorms> gpu_norm;
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+        gpu_norm = std::async(std::launch::async, [&]() {
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+            return norm(gpu_output,
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+    // Compute the l-infinity and l-2 distance between the CPU and GPU output:
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+                            gpu_output,
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+            params_inverse, ibuffer, pobuffer, pibuffer, gpu_input_data);
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+            std::cout << "GPU output Linf norm: " << gpu_norm.get().l_inf << "\n";
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+            std::cout << "GPU output L2 norm:   " << gpu_norm.get().l_2 << "\n";
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+            std::cout << "GPU linf norm failures:";
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+        EXPECT_TRUE(std::isfinite(gpu_norm.get().l_inf)) << params.str();
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+        EXPECT_TRUE(std::isfinite(gpu_norm.get().l_2)) << params.str();
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+                                            gpu_input_data,
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+#include "../shared/gpubuf.h"
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+    // CUDA also generally allows for effectively unlimited grid X
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+// into the input array of floats/doubles or complex floats/doubles gpu buffers.
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+inline void set_input(std::vector<gpubuf>&       input,
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+    // Return true if the given GPU parameters would produce a valid transform.
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+    // Tests that hit this can't fit on the GPU and should be skipped.
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+    // prepare for multi-GPU transform.  Generated input is in ibuffer.
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+    virtual void multi_gpu_prepare(std::vector<gpubuf>& ibuffer,
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+    // finalize multi-GPU transform.  pobuffers are the pointers
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+    virtual void multi_gpu_finalize(std::vector<gpubuf>& obuffer, std::vector<void*>& pobuffer) {}
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:diff --git a/shared/gpubuf.h b/shared/gpubuf.h
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+++ b/shared/gpubuf.h
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+#ifndef ROCFFT_GPUBUF_H
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+#define ROCFFT_GPUBUF_H
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+// Simple RAII class for GPU buffers.  T is the type of pointer that
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+class gpubuf_t
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+    gpubuf_t() {}
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+    gpubuf_t(gpubuf_t&& other)
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+    gpubuf_t& operator=(gpubuf_t&& other)
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+    gpubuf_t(const gpubuf_t&) = delete;
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+    gpubuf_t& operator=(const gpubuf_t&) = delete;
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+    ~gpubuf_t()
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+    // The GPU buffer
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+// default gpubuf that gives out void* pointers
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+typedef gpubuf_t<> gpubuf;
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+#ifdef __HIP_PLATFORM_NVIDIA__
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+#include "../shared/gpubuf.h"
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+    gpubuf_t<void>          wbuffer;
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+    // scatter data to multiple GPUs and adjust I/O buffers to match
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+    void multi_gpu_prepare(std::vector<gpubuf>& ibuffer,
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+                    multi_gpu_data.emplace_back();
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+                    if(multi_gpu_data.back().alloc(brick_size_bytes) != hipSuccess)
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+                    pbuffer.push_back(multi_gpu_data.back().data());
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+                    // gpubuf
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+    // when preparing for multi-GPU transform, we need to allocate data
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+    // on each GPU.  This vector remembers all of those allocations.
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+    std::vector<gpubuf> multi_gpu_data;
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+    // gather data after multi-GPU FFT for verification
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+    void multi_gpu_finalize(std::vector<gpubuf>& obuffer, std::vector<void*>& pobuffer) override
var/spack/repos/builtin/packages/hipfft/001-remove-submodule-and-sync-shared-files-from-rocFFT.patch:+            // has only one gpubuf
var/spack/repos/builtin/packages/hoomd-blue/package.py:    from a single CPU core to thousands of GPUs.
var/spack/repos/builtin/packages/hoomd-blue/package.py:    variant("cuda", default=True, description="Compile with CUDA Toolkit")
var/spack/repos/builtin/packages/hoomd-blue/package.py:    depends_on("cuda@7.0:", when="+cuda")
var/spack/repos/builtin/packages/hoomd-blue/package.py:        # CUDA support
var/spack/repos/builtin/packages/hoomd-blue/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/hoomd-blue/package.py:            cmake_args.append("-DENABLE_CUDA=ON")
var/spack/repos/builtin/packages/hoomd-blue/package.py:            cmake_args.append("-DENABLE_CUDA=OFF")
var/spack/repos/builtin/packages/hoomd-blue/package.py:        # CUDA-aware MPI library support
var/spack/repos/builtin/packages/hoomd-blue/package.py:        # if '+cuda' in spec and '+mpi' in spec:
var/spack/repos/builtin/packages/hoomd-blue/package.py:        #    cmake_args.append('-DENABLE_MPI_CUDA=ON')
var/spack/repos/builtin/packages/hoomd-blue/package.py:        #    cmake_args.append('-DENABLE_MPI_CUDA=OFF')
var/spack/repos/builtin/packages/hoomd-blue/package.py:        # There may be a bug in the MPI-CUDA code. See:
var/spack/repos/builtin/packages/hoomd-blue/package.py:        cmake_args.append("-DENABLE_MPI_CUDA=OFF")
var/spack/repos/builtin/packages/clfft/package.py:    """a software library containing FFT functions written in OpenCL"""
var/spack/repos/builtin/packages/clfft/package.py:    depends_on("opencl@1.2:")
var/spack/repos/builtin/packages/octopus/package.py:class Octopus(AutotoolsPackage, CudaPackage):
var/spack/repos/builtin/packages/octopus/package.py:        if "+cuda" in spec:
var/spack/repos/builtin/packages/octopus/package.py:            args.append("--enable-cuda")
var/spack/repos/builtin/packages/nvidia-nsight-systems/package.py:            "https://developer.nvidia.com/downloads/assets/tools/secure/nsight-systems/2024_6/nsight-systems-2024.6.1-2024.6.1.90_3490548-0.aarch64.rpm",
var/spack/repos/builtin/packages/nvidia-nsight-systems/package.py:            "https://developer.nvidia.com/downloads/assets/tools/secure/nsight-systems/2024_6/nsight-systems-2024.6.1-2024.6.1.90_3490548-0.x86_64.rpm",
var/spack/repos/builtin/packages/nvidia-nsight-systems/package.py:            "https://developer.download.nvidia.com/devtools/repos/rhel8/arm64/nsight-systems-2024.1.1-2024.1.1.59_3380207-0.aarch64.rpm",
var/spack/repos/builtin/packages/nvidia-nsight-systems/package.py:            "https://developer.download.nvidia.com/devtools/repos/rhel8/ppc64le/nsight-systems-cli-2024.1.1-2024.1.1.59_3380207-0.ppc64le.rpm",
var/spack/repos/builtin/packages/nvidia-nsight-systems/package.py:            "https://developer.download.nvidia.com/devtools/repos/rhel8/x86_64/nsight-systems-2024.1.1-2024.1.1.59_3380207-0.x86_64.rpm",
var/spack/repos/builtin/packages/nvidia-nsight-systems/package.py:class NvidiaNsightSystems(Package):
var/spack/repos/builtin/packages/nvidia-nsight-systems/package.py:    """NVIDIA Nsight™ Systems is a system-wide performance analysis tool designed
var/spack/repos/builtin/packages/nvidia-nsight-systems/package.py:    and GPUs, from large servers to the smallest system on a chip"""
var/spack/repos/builtin/packages/nvidia-nsight-systems/package.py:    homepage = "https://developer.nvidia.com/nsight-systems"
var/spack/repos/builtin/packages/nvidia-nsight-systems/package.py:    url = "https://developer.download.nvidia.com/devtools/repos/"
var/spack/repos/builtin/packages/nvidia-nsight-systems/package.py:    license("NVIDIA Software License Agreement")
var/spack/repos/builtin/packages/nvidia-nsight-systems/package.py:        #     NVIDIA Nsight Systems version 2024.1.1.59-241133802077v0
var/spack/repos/builtin/packages/nvidia-nsight-systems/package.py:        match = re.search(r"NVIDIA Nsight Systems version ((?:[0-9]+.){2}[0-9])", output)
var/spack/repos/builtin/packages/nvidia-nsight-systems/package.py:        if os.path.exists(join_path("opt", "nvidia", "nsight-systems-cli")):
var/spack/repos/builtin/packages/nvidia-nsight-systems/package.py:            base_path = join_path("opt", "nvidia", "nsight-systems-cli")
var/spack/repos/builtin/packages/nvidia-nsight-systems/package.py:        elif os.path.exists(join_path("opt", "nvidia", "nsight-systems")):
var/spack/repos/builtin/packages/nvidia-nsight-systems/package.py:            base_path = join_path("opt", "nvidia", "nsight-systems")
var/spack/repos/builtin/packages/py-gemmforge/package.py:    """GPU-GEMM generator for the Discontinuous Galerkin method"""
var/spack/repos/builtin/packages/comgr/hip-tests.patch:diff -Naurb ROCm-CompilerSupport-rocm-3.10.0.orig/lib/comgr/test/CMakeLists.txt ROCm-CompilerSupport-rocm-3.10.0/lib/comgr/test/CMakeLists.txt
var/spack/repos/builtin/packages/comgr/hip-tests.patch:--- ROCm-CompilerSupport-rocm-3.10.0.orig/lib/comgr/test/CMakeLists.txt	2020-09-16 14:17:12.000000000 -0500
var/spack/repos/builtin/packages/comgr/hip-tests.patch:+++ ROCm-CompilerSupport-rocm-3.10.0/lib/comgr/test/CMakeLists.txt	2020-12-14 10:11:56.609584283 -0600
var/spack/repos/builtin/packages/comgr/hip-tests.patch:-find_package(hip CONFIG PATHS /opt/rocm/hip QUIET)
var/spack/repos/builtin/packages/comgr/package.py:    homepage = "https://github.com/ROCm/ROCm-CompilerSupport"
var/spack/repos/builtin/packages/comgr/package.py:    git = "https://github.com/ROCm/ROCm-CompilerSupport.git"
var/spack/repos/builtin/packages/comgr/package.py:            url = "https://github.com/ROCm/ROCm-CompilerSupport/archive/rocm-{0}.tar.gz"
var/spack/repos/builtin/packages/comgr/package.py:            url = "https://github.com/ROCm/llvm-project/archive/rocm-{0}.tar.gz"
var/spack/repos/builtin/packages/comgr/package.py:    tags = ["rocm"]
var/spack/repos/builtin/packages/comgr/package.py:    # /opt/rocm, and this breaks the build when /opt/rocm exists.
var/spack/repos/builtin/packages/comgr/package.py:    depends_on("rocm-cmake@3.5.0:", type="build")
var/spack/repos/builtin/packages/comgr/package.py:        depends_on(f"llvm-amdgpu@{ver}", when=f"@{ver}")
var/spack/repos/builtin/packages/comgr/package.py:        # aomp may not build rocm-device-libs as part of llvm-amdgpu, so make
var/spack/repos/builtin/packages/comgr/package.py:        depends_on(f"rocm-device-libs@{ver}", when=f"@{ver} ^llvm-amdgpu ~rocm-device-libs")
var/spack/repos/builtin/packages/comgr/package.py:        depends_on(f"rocm-core@{ver}", when=f"@{ver}")
var/spack/repos/builtin/packages/py-torch-scatter/package.py:        if "+cuda" in self.spec["py-torch"]:
var/spack/repos/builtin/packages/py-torch-scatter/package.py:            env.set("FORCE_CUDA", 1)
var/spack/repos/builtin/packages/py-torch-scatter/package.py:            env.set("FORCE_ONLY_CUDA", 0)
var/spack/repos/builtin/packages/py-torch-scatter/package.py:            env.set("FORCE_CUDA", 0)
var/spack/repos/builtin/packages/py-torch-scatter/package.py:            env.set("FORCE_ONLY_CUDA", 0)
var/spack/repos/builtin/packages/claw/package.py:    generates architecture specific code decorated with OpenMP or OpenACC"""
var/spack/repos/builtin/packages/yaksa/package.py:class Yaksa(AutotoolsPackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/yaksa/package.py:    GPUs."""
var/spack/repos/builtin/packages/yaksa/package.py:        config_args += self.with_or_without("cuda", activation_value="prefix")
var/spack/repos/builtin/packages/yaksa/package.py:        if "+cuda" in spec:
var/spack/repos/builtin/packages/yaksa/package.py:            cuda_archs = spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/yaksa/package.py:            if "none" not in cuda_archs:
var/spack/repos/builtin/packages/yaksa/package.py:                config_args.append("--with-cuda-sm={0}".format(",".join(cuda_archs)))
var/spack/repos/builtin/packages/yaksa/package.py:            if "^cuda+allow-unsupported-compilers" in self.spec:
var/spack/repos/builtin/packages/yaksa/package.py:        if "+rocm" in spec:
var/spack/repos/builtin/packages/yaksa/package.py:            rocm_archs = spec.variants["amdgpu_target"].value
var/spack/repos/builtin/packages/yaksa/package.py:            if "none" not in rocm_archs:
var/spack/repos/builtin/packages/yaksa/package.py:                config_args.append("--with-hip-sm={0}".format(",".join(rocm_archs)))
var/spack/repos/builtin/packages/elemental/elemental_cublas.patch:+void GemmUseGPU(int min_M, int min_N, int min_K);
var/spack/repos/builtin/packages/elemental/elemental_cublas.patch:+char gemm_cpu_gpu_switch = 'c';
var/spack/repos/builtin/packages/elemental/elemental_cublas.patch:+void GemmUseGPU(int _min_M, int _min_N, int _min_K) {
var/spack/repos/builtin/packages/elemental/elemental_cublas.patch:+   gemm_cpu_gpu_switch = 'g';
var/spack/repos/builtin/packages/elemental/elemental_cublas.patch:+   gemm_cpu_gpu_switch = 'c';
var/spack/repos/builtin/packages/elemental/elemental_cublas.patch:+        if (gemm_cpu_gpu_switch == 'g' && 
var/spack/repos/builtin/packages/spfft/0001-fix-missing-limits-include.patch: #include "memory/gpu_array_view.hpp"
var/spack/repos/builtin/packages/spfft/package.py:class Spfft(CMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/spfft/package.py:    """Sparse 3D FFT library with MPI, OpenMP, CUDA and ROCm support."""
var/spack/repos/builtin/packages/spfft/package.py:    variant("gpu_direct", default=False, description="GPU aware MPI")
var/spack/repos/builtin/packages/spfft/package.py:    depends_on("cmake@3.21:", type="build", when="@1.1.0: +rocm")
var/spack/repos/builtin/packages/spfft/package.py:    depends_on("cuda@9:10", when="@:0.9.11 +cuda")
var/spack/repos/builtin/packages/spfft/package.py:    depends_on("cuda@9:", when="@0.9.12:1.0.6 +cuda")
var/spack/repos/builtin/packages/spfft/package.py:    depends_on("cuda@11:", when="@1.1.0: +cuda")
var/spack/repos/builtin/packages/spfft/package.py:    # Workaround for compiler bug in ROCm 4.5+ added in SpFFT 1.0.6
var/spack/repos/builtin/packages/spfft/package.py:    conflicts("+rocm", when="@:1.0.5")
var/spack/repos/builtin/packages/spfft/package.py:    with when("+rocm"):
var/spack/repos/builtin/packages/spfft/package.py:        conflicts("^hip@6.0.0:", when="@:1.0.6 +rocm")
var/spack/repos/builtin/packages/spfft/package.py:            self.define_from_variant("SPFFT_GPU_DIRECT", "gpu_direct"),
var/spack/repos/builtin/packages/spfft/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/spfft/package.py:            args += ["-DSPFFT_GPU_BACKEND=CUDA"]
var/spack/repos/builtin/packages/spfft/package.py:            cuda_arch = self.spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/spfft/package.py:            if cuda_arch[0] != "none":
var/spack/repos/builtin/packages/spfft/package.py:                args += [self.define("CMAKE_CUDA_ARCHITECTURES", cuda_arch)]
var/spack/repos/builtin/packages/spfft/package.py:        if spec.satisfies("@1.1.0: +rocm"):
var/spack/repos/builtin/packages/spfft/package.py:            args += ["-DSPFFT_GPU_BACKEND=ROCM"]
var/spack/repos/builtin/packages/spfft/package.py:            rocm_arch = self.spec.variants["amdgpu_target"].value
var/spack/repos/builtin/packages/spfft/package.py:            if rocm_arch[0] != "none":
var/spack/repos/builtin/packages/spfft/package.py:                args += [self.define("CMAKE_HIP_ARCHITECTURES", rocm_arch)]
var/spack/repos/builtin/packages/spfft/package.py:        elif spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/spfft/package.py:            archs = ",".join(self.spec.variants["amdgpu_target"].value)
var/spack/repos/builtin/packages/spfft/package.py:                "-DSPFFT_GPU_BACKEND=ROCM",
var/spack/repos/builtin/packages/spfft/package.py:                "-DHIP_HCC_FLAGS=--amdgpu-target={0}".format(archs),
var/spack/repos/builtin/packages/py-nvidia-ml-py3/package.py:class PyNvidiaMlPy3(PythonPackage):
var/spack/repos/builtin/packages/py-nvidia-ml-py3/package.py:    """Python Bindings for the NVIDIA Management Library."""
var/spack/repos/builtin/packages/py-nvidia-ml-py3/package.py:    homepage = "https://www.nvidia.com/"
var/spack/repos/builtin/packages/py-nvidia-ml-py3/package.py:    pypi = "nvidia-ml-py3/nvidia-ml-py3-7.352.0.tar.gz"
var/spack/repos/builtin/packages/linaro-forge/package.py:    to Intel, 64-bit Arm, AMD, OpenPOWER and Nvidia GPU hardware. Linaro Forge
var/spack/repos/builtin/packages/gpuscout/package.py:class Gpuscout(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/gpuscout/package.py:    """GPUscout: A tool for discovering data movement-related bottlenecks on NVidia GPUs."""
var/spack/repos/builtin/packages/gpuscout/package.py:    homepage = "https://github.com/caps-tum/GPUscout"
var/spack/repos/builtin/packages/gpuscout/package.py:    url = "https://github.com/caps-tum/GPUscout/archive/refs/tags/v0.2.1.tar.gz"
var/spack/repos/builtin/packages/gpuscout/package.py:    git = "https://github.com/caps-tum/GPUscout.git"
var/spack/repos/builtin/packages/gpuscout/package.py:    depends_on("cuda@12:")
var/spack/repos/builtin/packages/bricks/package.py:    variant("cuda", default=False, description="Build bricks with CUDA enabled")
var/spack/repos/builtin/packages/bricks/package.py:    # ECP E4S project builds cmake in their e4s-base-cuda Docker image
var/spack/repos/builtin/packages/bricks/package.py:    depends_on("opencl-clhpp", when="+cuda")
var/spack/repos/builtin/packages/bricks/package.py:    depends_on("cuda", when="+cuda")
var/spack/repos/builtin/packages/bricks/package.py:    patch("bricks-cmakelists-option-opencl.patch")
var/spack/repos/builtin/packages/bricks/package.py:        args = [self.define_from_variant("BRICK_USE_OPENCL", "cuda")]
var/spack/repos/builtin/packages/bricks/package.py:        if self.spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/bricks/package.py:            args.append(f"-DOCL_ROOT:STRING={self.spec['opencl-clhpp'].prefix}")
var/spack/repos/builtin/packages/bricks/bricks-cmakelists-option-opencl.patch:-find_package(OpenCL 2.0)
var/spack/repos/builtin/packages/bricks/bricks-cmakelists-option-opencl.patch:+option(BRICK_USE_OPENCL "Use OpenCL targets" ON)
var/spack/repos/builtin/packages/bricks/bricks-cmakelists-option-opencl.patch:+if (BRICK_USE_OPENCL)
var/spack/repos/builtin/packages/bricks/bricks-cmakelists-option-opencl.patch:+    find_package(OpenCL 2.0)
var/spack/repos/builtin/packages/sollve/package.py:        "<current arch>,NVPTX,AMDGPU,CppBackend",
var/spack/repos/builtin/packages/sollve/package.py:        # TODO: Instead of unconditionally disabling CUDA, add a "cuda" variant
var/spack/repos/builtin/packages/sollve/package.py:                "-DCUDA_TOOLKIT_ROOT_DIR:PATH=IGNORE",
var/spack/repos/builtin/packages/sollve/package.py:                "-DCUDA_SDK_ROOT_DIR:PATH=IGNORE",
var/spack/repos/builtin/packages/sollve/package.py:                "-DCUDA_NVCC_EXECUTABLE:FILEPATH=IGNORE",
var/spack/repos/builtin/packages/sollve/package.py:                "-DLIBOMPTARGET_DEP_CUDA_DRIVER_LIBRARIES:STRING=IGNORE",
var/spack/repos/builtin/packages/cuda-memtest/package.py:class CudaMemtest(CMakePackage):
var/spack/repos/builtin/packages/cuda-memtest/package.py:    """Maintained and updated fork of cuda_memtest.
var/spack/repos/builtin/packages/cuda-memtest/package.py:    original homepage: http://sourceforge.net/projects/cudagpumemtest .
var/spack/repos/builtin/packages/cuda-memtest/package.py:    This software tests GPU memory for hardware errors and soft errors
var/spack/repos/builtin/packages/cuda-memtest/package.py:    using CUDA or OpenCL.
var/spack/repos/builtin/packages/cuda-memtest/package.py:    homepage = "https://github.com/ComputationalRadiationPhysics/cuda_memtest"
var/spack/repos/builtin/packages/cuda-memtest/package.py:    git = "https://github.com/ComputationalRadiationPhysics/cuda_memtest.git"
var/spack/repos/builtin/packages/cuda-memtest/package.py:    depends_on("cuda@5.0:")
var/spack/repos/builtin/packages/py-fastai/package.py:    the Colab runtime to "GPU" to have it run fast!) See the
var/spack/repos/builtin/packages/random123/v1140-hip.patch:@@ -81,7 +81,7 @@ inline R123_CUDA_DEVICE value_type assemble_from_u32(uint32_t *p32){
var/spack/repos/builtin/packages/random123/v1140-hip.patch:-#ifdef __CUDA_ARCH__
var/spack/repos/builtin/packages/random123/v1140-hip.patch:+#if defined(__CUDA_ARCH__) || defined(__HIP_DEVICE_COMPILE__)
var/spack/repos/builtin/packages/random123/v1140-hip.patch: /* CUDA can't handle std::reverse_iterator.  We *could* implement it
var/spack/repos/builtin/packages/random123/v1140-hip.patch:@@ -114,8 +114,8 @@ inline R123_CUDA_DEVICE value_type assemble_from_u32(uint32_t *p32){
var/spack/repos/builtin/packages/random123/v1140-hip.patch:     R123_CUDA_DEVICE reference operator[](size_type i){return v[i];}                     \
var/spack/repos/builtin/packages/random123/v1140-hip.patch:     R123_CUDA_DEVICE const_reference operator[](size_type i) const {return v[i];}        \
var/spack/repos/builtin/packages/random123/v1140-hip.patch:-    R123_CUDA_DEVICE reference at(size_type i){ if(i >=  _N) R123_THROW(std::out_of_range("array index out of range")); return (*this)[i]; } \
var/spack/repos/builtin/packages/random123/v1140-hip.patch:-    R123_CUDA_DEVICE const_reference at(size_type i) const { if(i >=  _N) R123_THROW(std::out_of_range("array index out of range")); return (*this)[i]; } \
var/spack/repos/builtin/packages/random123/v1140-hip.patch:+    R123_CUDA_DEVICE reference at(size_type i){ if(i >=  _N) {R123_THROW(std::out_of_range("array index out of range"));}; return (*this)[i]; } \
var/spack/repos/builtin/packages/random123/v1140-hip.patch:+    R123_CUDA_DEVICE const_reference at(size_type i) const { if(i >=  _N) {R123_THROW(std::out_of_range("array index out of range"));}; return (*this)[i]; } \
var/spack/repos/builtin/packages/random123/v1140-hip.patch:     R123_CUDA_DEVICE size_type size() const { return  _N; }                              \
var/spack/repos/builtin/packages/random123/v1140-hip.patch:     R123_CUDA_DEVICE size_type max_size() const { return _N; }                           \
var/spack/repos/builtin/packages/random123/v1140-hip.patch:     R123_CUDA_DEVICE bool empty() const { return _N==0; };                               \
var/spack/repos/builtin/packages/random123/v1140-hip.patch: // namespace structures in CUDA.
var/spack/repos/builtin/packages/random123/v1140-hip.patch:-#if !defined(__CUDACC__)
var/spack/repos/builtin/packages/random123/v1140-hip.patch:+#if !(defined(__CUDACC__) || defined(__HIPCC__))
var/spack/repos/builtin/packages/random123/v1140-hip.patch: The Random123 library is portable across C, C++, CUDA, OpenCL environments,
var/spack/repos/builtin/packages/random123/v1140-hip.patch: <li>R123_CUDA_DEVICE - which expands to __device__ (or something else with
var/spack/repos/builtin/packages/random123/v1140-hip.patch:   sufficiently similar semantics) when CUDA is in use, and expands
var/spack/repos/builtin/packages/random123/v1140-hip.patch: #include "openclfeatures.h"
var/spack/repos/builtin/packages/random123/v1140-hip.patch: #elif defined(__CUDACC__)
var/spack/repos/builtin/packages/random123/v1140-hip.patch: #define R123_USE_PHILOX_64BIT (R123_USE_64BIT && (R123_USE_MULHILO64_ASM || R123_USE_MULHILO64_MSVC_INTRIN || R123_USE_MULHILO64_CUDA_INTRIN || R123_USE_GNU_UINT128 || R123_USE_MULHILO64_C99 || R123_USE_MULHILO64_OPENCL_INTRIN || R123_USE_MULHILO64_MULHI_INTRIN))
var/spack/repos/builtin/packages/random123/v1140-hip.patch:+#if !(defined(CUDART_VERSION) || defined(HIP_INCLUDE_HIP_HIP_RUNTIME_API_H))
var/spack/repos/builtin/packages/random123/v1140-hip.patch:+#error "why are we in hipfeatures.h if neither CUDART_VERSION NOR HIP_PLATFORM?"
var/spack/repos/builtin/packages/random123/v1140-hip.patch:+#if CUDART_VERSION < 4010 && !defined(HIP_INCLUDE_HIP_HIP_RUNTIME_API_H)
var/spack/repos/builtin/packages/random123/v1140-hip.patch:+#error "CUDA versions earlier than 4.1 produce incorrect results for some templated functions in namespaces.  Random123 is unsupported.  See comments in nvccfeatures.h"
var/spack/repos/builtin/packages/random123/v1140-hip.patch:+// T=uint64_t in examples/uniform.hpp produces -1 for CUDA4.0 and
var/spack/repos/builtin/packages/random123/v1140-hip.patch:+// Thus, we no longer trust CUDA versions earlier than 4.1 even though
var/spack/repos/builtin/packages/random123/v1140-hip.patch:+// we had previously tested and timed Random123 with CUDA 3.x and 4.0.
var/spack/repos/builtin/packages/random123/v1140-hip.patch:+//#ifdef  __CUDA_ARCH__ allows Philox32 and Philox64 to be compiled
var/spack/repos/builtin/packages/random123/v1140-hip.patch:+//for both device and host functions in CUDA by setting compiler flags
var/spack/repos/builtin/packages/random123/v1140-hip.patch:+#if defined(__CUDA_ARCH__) || defined(__HIP_DEVICE_COMPILE__)
var/spack/repos/builtin/packages/random123/v1140-hip.patch:+#ifndef R123_CUDA_DEVICE
var/spack/repos/builtin/packages/random123/v1140-hip.patch:+#define R123_CUDA_DEVICE __host__ __device__
var/spack/repos/builtin/packages/random123/v1140-hip.patch:+#ifndef R123_USE_MULHILO64_CUDA_INTRIN
var/spack/repos/builtin/packages/random123/v1140-hip.patch:+#define R123_USE_MULHILO64_CUDA_INTRIN 1
var/spack/repos/builtin/packages/random123/v1140-hip.patch:+// No exceptions in CUDA, at least upto 4.0
var/spack/repos/builtin/packages/random123/v1140-hip.patch:+#  if defined(__CUDA_ARCH__)
var/spack/repos/builtin/packages/random123/v1140-hip.patch:+#else // ! ( defined(__CUDA_ARCH__) || defined(__HIP_DEVICE_COMPILE__) )
var/spack/repos/builtin/packages/random123/v1140-hip.patch:+// If we're using nvcc not compiling for the CUDA architecture,
var/spack/repos/builtin/packages/random123/v1140-hip.patch:+#endif // __CUDA_ARCH__
var/spack/repos/builtin/packages/random123/v1140-hip.patch:-#if defined(__CUDACC__) || defined(_LIBCPP_HAS_NO_CONSTEXPR)
var/spack/repos/builtin/packages/random123/v1140-hip.patch:+#if defined(__CUDACC__) || defined(_LIBCPP_HAS_NO_CONSTEXPR) || defined(__HIPCC__)
var/spack/repos/builtin/packages/random123/v1140-hip.patch: // Amazing! cuda thinks numeric_limits::max() is a __host__ function, so
var/spack/repos/builtin/packages/vmd/package.py:        url="file://{0}/vmd-1.9.3.bin.LINUXAMD64-CUDA8-OptiX4-OSPRay111p1.opengl.tar.gz".format(
var/spack/repos/builtin/packages/raft/package.py:    depends_on("cuda")
var/spack/repos/builtin/packages/intel-oneapi-vtune/package.py:    cloud, IoT, media, storage, and more.  CPU, GPU, and FPGA: Tune
var/spack/repos/builtin/packages/intel-oneapi-vtune/package.py:    portion. Multilingual: Profile SYCL, C, C++, C#, Fortran, OpenCL
var/spack/repos/builtin/packages/arm-forge/package.py:    through to complex parallel HPC codes with MPI, OpenMP, threads or CUDA."""
var/spack/repos/builtin/packages/resolve/package.py:class Resolve(CMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/resolve/package.py:    """ReSolve is a library of GPU-resident sparse linear solvers. It contains iterative and direct
var/spack/repos/builtin/packages/resolve/package.py:    solvers designed to run on NVIDIA and AMD GPUs, as well as CPU devices."""
var/spack/repos/builtin/packages/resolve/package.py:    with when("+rocm"):
var/spack/repos/builtin/packages/resolve/package.py:        if "+cuda" in spec:
var/spack/repos/builtin/packages/resolve/package.py:            cuda_arch_list = spec.variants["cuda_arch"].value
var/spack/repos/builtin/packages/resolve/package.py:            if cuda_arch_list[0] != "none":
var/spack/repos/builtin/packages/resolve/package.py:                args.append(self.define("CMAKE_CUDA_ARCHITECTURES", cuda_arch_list))
var/spack/repos/builtin/packages/resolve/package.py:                args.append(self.define("CMAKE_CUDA_ARCHITECTURES", "70;75;80"))
var/spack/repos/builtin/packages/resolve/package.py:            args.append(self.define("RESOLVE_USE_CUDA", True))
var/spack/repos/builtin/packages/resolve/package.py:        elif "+rocm" in spec:
var/spack/repos/builtin/packages/resolve/package.py:            rocm_arch_list = spec.variants["amdgpu_target"].value
var/spack/repos/builtin/packages/resolve/package.py:            # `+rocm` conflicts with amdgpu_target == "none"...
var/spack/repos/builtin/packages/resolve/package.py:            # if rocm_arch_list[0] == "none":
var/spack/repos/builtin/packages/resolve/package.py:            #     rocm_arch_list = "gfx90a"
var/spack/repos/builtin/packages/resolve/package.py:            args.append(self.define("GPU_TARGETS", rocm_arch_list))
var/spack/repos/builtin/packages/resolve/package.py:            args.append(self.define("AMDGPU_TARGETS", rocm_arch_list))
var/spack/repos/builtin/packages/mivisionx/0002-add-half-include-path-for-tests.patch:@@ -151,6 +151,10 @@ if (OPENVX_BACKEND_OPENCL_FOUND)
var/spack/repos/builtin/packages/mivisionx/0002-add-half-include-path-for-tests.patch:     include_directories (${OpenCL_INCLUDE_DIRS} ${OpenCL_INCLUDE_DIRS}/Headers )
var/spack/repos/builtin/packages/mivisionx/0002-add-half-include-path-for-tests.patch: include_directories (/opt/rocm/include/mivisionx)
var/spack/repos/builtin/packages/mivisionx/0002-add-half-include-path-for-tests.patch:@@ -50,7 +50,10 @@ if (OPENVX_BACKEND_OPENCL_FOUND)
var/spack/repos/builtin/packages/mivisionx/0002-add-half-include-path-for-tests.patch:     include_directories (${OpenCL_INCLUDE_DIRS} ${OpenCL_INCLUDE_DIRS}/Headers )
var/spack/repos/builtin/packages/mivisionx/0002-add-half-include-path-for-tests.patch:-include_directories (${ROCM_PATH}/include/mivisionx ${PROJECT_SOURCE_DIR} )
var/spack/repos/builtin/packages/mivisionx/0002-add-half-include-path-for-tests.patch:+include_directories (${ROCM_PATH}/include/mivisionx ${PROJECT_SOURCE_DIR} ${HALF_INCLUDE_DIR} )
var/spack/repos/builtin/packages/mivisionx/0002-add-half-include-path-for-tests.patch: link_directories    (${ROCM_PATH}/lib ${PROJECT_SOURCE_DIR}/lib)
var/spack/repos/builtin/packages/mivisionx/0002-add-half-include-path-for-tests.patch:-include_directories(${ROCM_PATH}/${CMAKE_INSTALL_INCLUDEDIR}/mivisionx/rocal)
var/spack/repos/builtin/packages/mivisionx/0002-add-half-include-path-for-tests.patch:+include_directories(${ROCM_PATH}/${CMAKE_INSTALL_INCLUDEDIR}/mivisionx/rocal ${HALF_INCLUDE_DIR})
var/spack/repos/builtin/packages/mivisionx/0002-add-half-include-path-for-tests.patch: link_directories(${ROCM_PATH}/lib/)
var/spack/repos/builtin/packages/mivisionx/0002-add-half-include-path-for-tests.patch:-include_directories(${ROCM_PATH}/${CMAKE_INSTALL_INCLUDEDIR}/mivisionx/rocal)
var/spack/repos/builtin/packages/mivisionx/0002-add-half-include-path-for-tests.patch:+include_directories(${ROCM_PATH}/${CMAKE_INSTALL_INCLUDEDIR}/mivisionx/rocal ${HALF_INCLUDE_DIR})
var/spack/repos/builtin/packages/mivisionx/0002-add-half-include-path-for-tests.patch: link_directories(${ROCM_PATH}/lib/)
var/spack/repos/builtin/packages/mivisionx/0002-add-half-include-path-for-tests-6.1.0.patch:@@ -160,6 +160,10 @@ if (OPENVX_BACKEND_OPENCL_FOUND)
var/spack/repos/builtin/packages/mivisionx/0002-add-half-include-path-for-tests-6.1.0.patch:     include_directories (${OpenCL_INCLUDE_DIRS} ${OpenCL_INCLUDE_DIRS}/Headers )
var/spack/repos/builtin/packages/mivisionx/0002-add-half-include-path-for-tests-6.1.0.patch: include_directories (${ROCM_PATH}/include ${ROCM_PATH}/include/mivisionx)
var/spack/repos/builtin/packages/mivisionx/0002-add-half-include-path-for-tests-6.1.0.patch: set(ROCM_PATH /opt/rocm CACHE PATH "ROCm Installation Path")
var/spack/repos/builtin/packages/mivisionx/0002-add-half-include-path-for-tests-6.1.0.patch:@@ -50,7 +51,10 @@ if (OPENVX_BACKEND_OPENCL_FOUND)
var/spack/repos/builtin/packages/mivisionx/0002-add-half-include-path-for-tests-6.1.0.patch:     include_directories (${OpenCL_INCLUDE_DIRS} ${OpenCL_INCLUDE_DIRS}/Headers )
var/spack/repos/builtin/packages/mivisionx/0002-add-half-include-path-for-tests-6.1.0.patch:-include_directories (${ROCM_PATH}/include ${ROCM_PATH}/include/mivisionx ${PROJECT_SOURCE_DIR} )
var/spack/repos/builtin/packages/mivisionx/0002-add-half-include-path-for-tests-6.1.0.patch:+include_directories (${ROCM_PATH}/include/mivisionx ${PROJECT_SOURCE_DIR} ${HALF_INCLUDE_DIR} )
var/spack/repos/builtin/packages/mivisionx/0002-add-half-include-path-for-tests-6.1.0.patch: link_directories    (${ROCM_PATH}/lib ${PROJECT_SOURCE_DIR}/lib)
var/spack/repos/builtin/packages/mivisionx/0002-add-half-include-path-for-tests-6.1.0.patch: include_directories(${ROCM_PATH}/${CMAKE_INSTALL_INCLUDEDIR})
var/spack/repos/builtin/packages/mivisionx/0002-add-half-include-path-for-tests-6.1.0.patch:-include_directories(${ROCM_PATH}/${CMAKE_INSTALL_INCLUDEDIR}/mivisionx/rocal)
var/spack/repos/builtin/packages/mivisionx/0002-add-half-include-path-for-tests-6.1.0.patch:+include_directories(${ROCM_PATH}/${CMAKE_INSTALL_INCLUDEDIR}/mivisionx/rocal ${HALF_INCLUDE_DIR})
var/spack/repos/builtin/packages/mivisionx/0002-add-half-include-path-for-tests-6.1.0.patch: link_directories(${ROCM_PATH}/lib/)
var/spack/repos/builtin/packages/mivisionx/0002-add-half-include-path-for-tests-6.1.0.patch: include_directories(${ROCM_PATH}/${CMAKE_INSTALL_INCLUDEDIR})
var/spack/repos/builtin/packages/mivisionx/0002-add-half-include-path-for-tests-6.1.0.patch:-include_directories(${ROCM_PATH}/${CMAKE_INSTALL_INCLUDEDIR}/mivisionx/rocal)
var/spack/repos/builtin/packages/mivisionx/0002-add-half-include-path-for-tests-6.1.0.patch:+include_directories(${ROCM_PATH}/${CMAKE_INSTALL_INCLUDEDIR}/mivisionx/rocal ${HALF_INCLUDE_DIR})
var/spack/repos/builtin/packages/mivisionx/0002-add-half-include-path-for-tests-6.1.0.patch: link_directories(${ROCM_PATH}/lib/)
var/spack/repos/builtin/packages/mivisionx/package.py:    homepage = "https://github.com/GPUOpen-ProfessionalCompute-Libraries/MIVisionX"
var/spack/repos/builtin/packages/mivisionx/package.py:    git = "https://github.com/GPUOpen-ProfessionalCompute-Libraries/MIVisionX.git"
var/spack/repos/builtin/packages/mivisionx/package.py:    url = "https://github.com/GPUOpen-ProfessionalCompute-Libraries/MIVisionX/archive/rocm-6.1.2.tar.gz"
var/spack/repos/builtin/packages/mivisionx/package.py:    tags = ["rocm"]
var/spack/repos/builtin/packages/mivisionx/package.py:            return "https://github.com/GPUOpen-ProfessionalCompute-Libraries/MIVisionX/archive/1.7.tar.gz"
var/spack/repos/builtin/packages/mivisionx/package.py:        url = "https://github.com/GPUOpen-ProfessionalCompute-Libraries/MIVisionX/archive/rocm-{0}.tar.gz"
var/spack/repos/builtin/packages/mivisionx/package.py:    # Adding 2 variants OPENCL ,HIP which HIP as default. earlier to 5.0.0,OPENCL
var/spack/repos/builtin/packages/mivisionx/package.py:    variant("opencl", default=False, description="Use OPENCL as the backend")
var/spack/repos/builtin/packages/mivisionx/package.py:        "https://github.com/GPUOpen-ProfessionalCompute-Libraries/MIVisionX/commit/da24882438b91a0ae1feee23206b75c1a1256887.patch?full_index=1",
var/spack/repos/builtin/packages/mivisionx/package.py:    conflicts("+opencl", when="@5.6.0:")
var/spack/repos/builtin/packages/mivisionx/package.py:                r"${ROCM_PATH}/include/miopen/config.h",
var/spack/repos/builtin/packages/mivisionx/package.py:        if self.spec.satisfies("@5.1.3: + opencl"):
var/spack/repos/builtin/packages/mivisionx/package.py:                r"${ROCM_PATH}/include/miopen/config.h",
var/spack/repos/builtin/packages/mivisionx/package.py:                "{0}/include/miopen/config.h".format(self.spec["miopen-opencl"].prefix),
var/spack/repos/builtin/packages/mivisionx/package.py:                r"${ROCM_PATH}/llvm/bin/clang++",
var/spack/repos/builtin/packages/mivisionx/package.py:                "{0}/bin/clang++".format(self.spec["llvm-amdgpu"].prefix),
var/spack/repos/builtin/packages/mivisionx/package.py:                r"${ROCM_PATH}/llvm/bin/clang++",
var/spack/repos/builtin/packages/mivisionx/package.py:                "{0}/bin/clang++".format(self.spec["llvm-amdgpu"].prefix),
var/spack/repos/builtin/packages/mivisionx/package.py:                r"${ROCM_PATH}/llvm/bin/clang++",
var/spack/repos/builtin/packages/mivisionx/package.py:                "{0}/bin/clang++".format(self.spec["llvm-amdgpu"].prefix),
var/spack/repos/builtin/packages/mivisionx/package.py:                r"${ROCM_PATH}/include/mivisionx",
var/spack/repos/builtin/packages/mivisionx/package.py:                r"${ROCM_PATH}/lib",
var/spack/repos/builtin/packages/mivisionx/package.py:                r"${ROCM_PATH}/include/mivisionx",
var/spack/repos/builtin/packages/mivisionx/package.py:                r"${ROCM_PATH}/lib",
var/spack/repos/builtin/packages/mivisionx/package.py:                r"${ROCM_PATH}/include/mivisionx",
var/spack/repos/builtin/packages/mivisionx/package.py:                r"${ROCM_PATH}/lib",
var/spack/repos/builtin/packages/mivisionx/package.py:                r"${ROCM_PATH}/include/mivisionx",
var/spack/repos/builtin/packages/mivisionx/package.py:                r"${ROCM_PATH}/lib",
var/spack/repos/builtin/packages/mivisionx/package.py:                r"${ROCM_PATH}/include/mivisionx",
var/spack/repos/builtin/packages/mivisionx/package.py:                r"/opt/rocm",
var/spack/repos/builtin/packages/mivisionx/package.py:                r"${ROCM_PATH}/${CMAKE_INSTALL_INCLUDEDIR}/mivisionx/rocal",
var/spack/repos/builtin/packages/mivisionx/package.py:                r"${ROCM_PATH}/lib",
var/spack/repos/builtin/packages/mivisionx/package.py:                r"${ROCM_PATH}/${CMAKE_INSTALL_INCLUDEDIR}/mivisionx/rocal",
var/spack/repos/builtin/packages/mivisionx/package.py:                r"${ROCM_PATH}/lib",
var/spack/repos/builtin/packages/mivisionx/package.py:    # OPENCL was default backend.
var/spack/repos/builtin/packages/mivisionx/package.py:    conflicts("+opencl+hip")
var/spack/repos/builtin/packages/mivisionx/package.py:    with when("+opencl"):
var/spack/repos/builtin/packages/mivisionx/package.py:            depends_on(f"rocm-opencl@{ver}", when=f"@{ver}")
var/spack/repos/builtin/packages/mivisionx/package.py:            depends_on(f"miopen-opencl@{ver}", when=f"@{ver}")
var/spack/repos/builtin/packages/mivisionx/package.py:        depends_on(f"rocm-core@{ver}", when=f"@{ver}")
var/spack/repos/builtin/packages/mivisionx/package.py:            env.set("CC", f"{self.spec['llvm-amdgpu'].prefix}/bin/clang")
var/spack/repos/builtin/packages/mivisionx/package.py:            env.set("CXX", f"{self.spec['llvm-amdgpu'].prefix}/bin/clang++")
var/spack/repos/builtin/packages/mivisionx/package.py:        if self.spec.satisfies("+opencl"):
var/spack/repos/builtin/packages/mivisionx/package.py:            args.append(self.define("BACKEND", "OPENCL"))
var/spack/repos/builtin/packages/mivisionx/package.py:        if self.spec.satisfies("~hip~opencl"):
var/spack/repos/builtin/packages/mivisionx/0001-add-half-include-path.patch: if( GPU_SUPPORT AND "${BACKEND}" STREQUAL "HIP")
var/spack/repos/builtin/packages/libxslt/package.py:        # Remove flags not recognized by the NVIDIA compiler
var/spack/repos/builtin/packages/mpilander/package.py:    # variant('cuda', default=False, description='Enable CUDA support')
var/spack/repos/builtin/packages/vtk-m/mr3259-thrust-is_arithmetic-fix.patch:diff --git a/vtkm/exec/cuda/internal/WrappedOperators.h b/vtkm/exec/cuda/internal/WrappedOperators.h
var/spack/repos/builtin/packages/vtk-m/mr3259-thrust-is_arithmetic-fix.patch:--- a/vtkm/exec/cuda/internal/WrappedOperators.h
var/spack/repos/builtin/packages/vtk-m/mr3259-thrust-is_arithmetic-fix.patch:+++ b/vtkm/exec/cuda/internal/WrappedOperators.h
var/spack/repos/builtin/packages/vtk-m/mr3259-thrust-is_arithmetic-fix.patch: #include <thrust/system/cuda/memory.h>
var/spack/repos/builtin/packages/vtk-m/mr3259-thrust-is_arithmetic-fix.patch:+#include <cuda/std/type_traits>
var/spack/repos/builtin/packages/vtk-m/mr3259-thrust-is_arithmetic-fix.patch:+struct is_commutative<vtkm::exec::cuda::internal::WrappedBinaryOperator<T, F>>
var/spack/repos/builtin/packages/vtk-m/mr3259-thrust-is_arithmetic-fix.patch:+  : public ::cuda::std::is_arithmetic<T>
var/spack/repos/builtin/packages/vtk-m/mr3259-thrust-is_arithmetic-fix.patch: struct is_commutative<vtkm::exec::cuda::internal::WrappedBinaryOperator<T, F>>
var/spack/repos/builtin/packages/vtk-m/mr3258-fix-typo-thrust-dependency-with-rocm.patch:-cmake_dependent_option(VTKm_ENABLE_KOKKOS_THRUST "Enable Kokkos thrust support (only valid with CUDA and HIP)"
var/spack/repos/builtin/packages/vtk-m/mr3258-fix-typo-thrust-dependency-with-rocm.patch:-  ON "VTKm_ENABLE_KOKKOS;Kokkos_ENABLE_CUDA OR Kokkos_ENABLE_HIP" OFF)
var/spack/repos/builtin/packages/vtk-m/mr3258-fix-typo-thrust-dependency-with-rocm.patch:+  "Enable Kokkos thrust support (only valid with CUDA and HIP)"
var/spack/repos/builtin/packages/vtk-m/mr3258-fix-typo-thrust-dependency-with-rocm.patch:+  "VTKm_ENABLE_KOKKOS;Kokkos_ENABLE_CUDA OR Kokkos_ENABLE_HIP; NOT Kokkos_ENABLE_HIP AND CMAKE_VERSION VERSION_LESS 3.24"
var/spack/repos/builtin/packages/vtk-m/mr3258-fix-typo-thrust-dependency-with-rocm.patch: # CUDA already provides thrust
var/spack/repos/builtin/packages/vtk-m/mr3258-fix-typo-thrust-dependency-with-rocm.patch:       + CMake 3.13+ (for CUDA support)
var/spack/repos/builtin/packages/vtk-m/mr3258-fix-typo-thrust-dependency-with-rocm.patch:+      + CMake 3.24+ (for ROCM+THRUST support)
var/spack/repos/builtin/packages/vtk-m/mr3258-fix-typo-thrust-dependency-with-rocm.patch:-      target_link_libraries(vtkm_cont PRIVATE $<$<LINK_LANGUAGE:CUDA>:roc::rocthrust>)
var/spack/repos/builtin/packages/vtk-m/package.py:class VtkM(CMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/vtk-m/package.py:    # CudaPackage provides cuda variant
var/spack/repos/builtin/packages/vtk-m/package.py:    # ROCmPackage provides rocm variant
var/spack/repos/builtin/packages/vtk-m/package.py:        "cuda_native", default=True, description="build using native cuda backend", when="+cuda"
var/spack/repos/builtin/packages/vtk-m/package.py:    depends_on("cmake@3.18:", when="+rocm", type="build")  # CMake >= 3.18
var/spack/repos/builtin/packages/vtk-m/package.py:    depends_on("cuda@10.1.0:", when="+cuda_native")
var/spack/repos/builtin/packages/vtk-m/package.py:    # VTK-m native CUDA and Kokkos CUDA backends are not compatible
var/spack/repos/builtin/packages/vtk-m/package.py:    depends_on("kokkos ~cuda", when="+kokkos +cuda +cuda_native")
var/spack/repos/builtin/packages/vtk-m/package.py:    depends_on("kokkos +cuda", when="+kokkos +cuda ~cuda_native")
var/spack/repos/builtin/packages/vtk-m/package.py:    for cuda_arch in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/vtk-m/package.py:            "kokkos cuda_arch=%s" % cuda_arch,
var/spack/repos/builtin/packages/vtk-m/package.py:            when="+kokkos +cuda ~cuda_native cuda_arch=%s" % cuda_arch,
var/spack/repos/builtin/packages/vtk-m/package.py:    depends_on("kokkos +rocm", when="+kokkos +rocm")
var/spack/repos/builtin/packages/vtk-m/package.py:    # Propagate AMD GPU target to kokkos for +rocm
var/spack/repos/builtin/packages/vtk-m/package.py:    for amdgpu_value in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/vtk-m/package.py:            "kokkos amdgpu_target=%s" % amdgpu_value,
var/spack/repos/builtin/packages/vtk-m/package.py:            when="+kokkos +rocm amdgpu_target=%s" % amdgpu_value,
var/spack/repos/builtin/packages/vtk-m/package.py:    depends_on("hip@3.7:", when="+rocm")
var/spack/repos/builtin/packages/vtk-m/package.py:    # CUDA thrust is already include in the CUDA pkg
var/spack/repos/builtin/packages/vtk-m/package.py:    depends_on("rocthrust", when="@2.2: +kokkos+rocm ^cmake@3.24:")
var/spack/repos/builtin/packages/vtk-m/package.py:    # The rocm variant is only valid options for >= 1.7. It would be better if
var/spack/repos/builtin/packages/vtk-m/package.py:    # this could be expressed as a when clause to disable the rocm variant,
var/spack/repos/builtin/packages/vtk-m/package.py:    conflicts("+rocm", when="@:1.6")
var/spack/repos/builtin/packages/vtk-m/package.py:    conflicts("+rocm", when="+cuda")
var/spack/repos/builtin/packages/vtk-m/package.py:    conflicts("+rocm", when="~kokkos", msg="VTK-m does not support HIP without Kokkos")
var/spack/repos/builtin/packages/vtk-m/package.py:    conflicts("+rocm", when="+virtuals", msg="VTK-m does not support virtual functions with ROCm")
var/spack/repos/builtin/packages/vtk-m/package.py:    # Can build +shared+cuda after @1.7:
var/spack/repos/builtin/packages/vtk-m/package.py:    conflicts("+shared", when="@:1.6 +cuda_native")
var/spack/repos/builtin/packages/vtk-m/package.py:    conflicts("+cuda~cuda_native~kokkos", msg="Cannot have +cuda without a cuda device")
var/spack/repos/builtin/packages/vtk-m/package.py:    conflicts("+cuda~cuda_native", when="@:1.5", msg="Cannot have +cuda without a cuda device")
var/spack/repos/builtin/packages/vtk-m/package.py:    conflicts("+cuda", when="cuda_arch=none", msg="vtk-m +cuda requires that cuda_arch be set")
var/spack/repos/builtin/packages/vtk-m/package.py:    patch("vtkm-cuda-swap-conflict-pr2972.patch", when="@1.9 +cuda ^cuda@12:")
var/spack/repos/builtin/packages/vtk-m/package.py:    patch("mr3258-fix-typo-thrust-dependency-with-rocm.patch", when="@2.2:")
var/spack/repos/builtin/packages/vtk-m/package.py:    patch("mr3259-thrust-is_arithmetic-fix.patch", when="@2.0.0:2.2.0 +cuda ^cuda@12.6:")
var/spack/repos/builtin/packages/vtk-m/package.py:        when="@1.6.0:2.1 +cuda ^cuda@12.5:",
var/spack/repos/builtin/packages/vtk-m/package.py:        gpu_name_table = {
var/spack/repos/builtin/packages/vtk-m/package.py:            if "+kokkos" in spec and "+rocm" in spec and spec.satisfies("^kokkos@4:"):
var/spack/repos/builtin/packages/vtk-m/package.py:            # cuda support
var/spack/repos/builtin/packages/vtk-m/package.py:            if "+cuda_native" in spec:
var/spack/repos/builtin/packages/vtk-m/package.py:                options.append("-DVTKm_ENABLE_CUDA:BOOL=ON")
var/spack/repos/builtin/packages/vtk-m/package.py:                options.append("-DCMAKE_CUDA_HOST_COMPILER={0}".format(env["SPACK_CXX"]))
var/spack/repos/builtin/packages/vtk-m/package.py:                    options.append(self.builder.define_cuda_architectures(self))
var/spack/repos/builtin/packages/vtk-m/package.py:                    # VTKm_CUDA_Architecture only accepts a single CUDA arch
var/spack/repos/builtin/packages/vtk-m/package.py:                    num_cuda_arch = spec.variants["cuda_arch"].value[0]
var/spack/repos/builtin/packages/vtk-m/package.py:                    str_cuda_arch = str()
var/spack/repos/builtin/packages/vtk-m/package.py:                        str_cuda_arch = gpu_name_table[num_cuda_arch]
var/spack/repos/builtin/packages/vtk-m/package.py:                            f"cuda_arch={num_cuda_arch} needs cmake>=3.18 & VTK-m>=1.9.0"
var/spack/repos/builtin/packages/vtk-m/package.py:                    options.append(f"-DVTKm_CUDA_Architecture={str_cuda_arch}")
var/spack/repos/builtin/packages/vtk-m/package.py:                options.append("-DVTKm_ENABLE_CUDA:BOOL=OFF")
var/spack/repos/builtin/packages/vtk-m/package.py:            if "+rocm" in spec:
var/spack/repos/builtin/packages/vtk-m/mr3160-rocthrust-fix.patch: cmake_dependent_option(VTKm_ENABLE_GPU_MPI "Enable GPU AWARE MPI support" OFF "VTKm_ENABLE_MPI" OFF)
var/spack/repos/builtin/packages/vtk-m/mr3160-rocthrust-fix.patch:+cmake_dependent_option(VTKm_ENABLE_KOKKOS_THRUST "Enable Kokkos thrust support (only valid with CUDA and HIP)"
var/spack/repos/builtin/packages/vtk-m/mr3160-rocthrust-fix.patch:+  ON "VTKm_ENABLE_KOKKOS;Kokkos_ENABLE_CUDA OR Kokkos_ENABLE_HIP" OFF)
var/spack/repos/builtin/packages/vtk-m/mr3160-rocthrust-fix.patch:-cmake_dependent_option(VTKm_ENABLE_KOKKOS_THRUST "Enable Kokkos thrust support (only valid with CUDA and HIP)"
var/spack/repos/builtin/packages/vtk-m/mr3160-rocthrust-fix.patch:-  ON "VTKm_ENABLE_KOKKOS;Kokkos_ENABLE_CUDA OR Kokkos_ENABLE_HIP" OFF)
var/spack/repos/builtin/packages/vtk-m/vtkm-cuda-swap-conflict-pr2972.patch:diff -ruN spack-src/vtkm/exec/cuda/internal/ExecutionPolicy.h spack-src-patched/vtkm/exec/cuda/internal/ExecutionPolicy.h
var/spack/repos/builtin/packages/vtk-m/vtkm-cuda-swap-conflict-pr2972.patch:--- spack-src/vtkm/exec/cuda/internal/ExecutionPolicy.h	2022-10-11 12:07:59.000000000 -0700
var/spack/repos/builtin/packages/vtk-m/vtkm-cuda-swap-conflict-pr2972.patch:+++ spack-src-patched/vtkm/exec/cuda/internal/ExecutionPolicy.h	2023-07-06 17:23:35.898388363 -0700
var/spack/repos/builtin/packages/vtk-m/vtkm-cuda-swap-conflict-pr2972.patch: #include <vtkm/exec/cuda/internal/ThrustPatches.h>
var/spack/repos/builtin/packages/vtk-m/vtkm-cuda-swap-conflict-pr2972.patch: #include <thrust/system/cuda/execution_policy.h>
var/spack/repos/builtin/packages/vtk-m/vtkm-cuda-swap-conflict-pr2972.patch: #include <thrust/system/cuda/memory.h>
var/spack/repos/builtin/packages/vtk-m/vtkm-cuda-swap-conflict-pr2972.patch: /// Performs a swap operation. Safe to call from cuda code.
var/spack/repos/builtin/packages/vtk-m/vtkm-cuda-swap-conflict-pr2972.patch: #if defined(VTKM_CUDA)
var/spack/repos/builtin/packages/vtk-m/vtkm-cuda-swap-conflict-pr2972.patch:+// CUDA 12 adds a `cub::Swap` function that creates ambiguity with `vtkm::Swap`.
var/spack/repos/builtin/packages/vtk-m/vtkm-cuda-swap-conflict-pr2972.patch:+#if defined(VTKM_CUDA_VERSION_MAJOR) && (VTKM_CUDA_VERSION_MAJOR >= 12) && \
var/spack/repos/builtin/packages/vtk-m/vtkm-cuda-swap-conflict-pr2972.patch:+  defined(VTKM_CUDA_DEVICE_PASS)
var/spack/repos/builtin/packages/beatnik/package.py:class Beatnik(CMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/beatnik/package.py:    # Variants are primarily backends to build on GPU systems and pass the right
var/spack/repos/builtin/packages/beatnik/package.py:    variant("cuda", default=False, description="Use CUDA support from subpackages")
var/spack/repos/builtin/packages/beatnik/package.py:    with when("+cuda"):
var/spack/repos/builtin/packages/beatnik/package.py:        depends_on("mpich +cuda", when="^[virtuals=mpi] mpich")
var/spack/repos/builtin/packages/beatnik/package.py:        depends_on("mvapich +cuda", when="^[virtuals=mpi] mvapich")
var/spack/repos/builtin/packages/beatnik/package.py:        depends_on("mvapich2 +cuda", when="^[virtuals=mpi] mvapich2")
var/spack/repos/builtin/packages/beatnik/package.py:        depends_on("mvapich2-gdr +cuda", when="^[virtuals=mpi] mvapich2-gdr")
var/spack/repos/builtin/packages/beatnik/package.py:        depends_on("openmpi +cuda", when="^[virtuals=mpi] openmpi")
var/spack/repos/builtin/packages/beatnik/package.py:    with when("+rocm"):
var/spack/repos/builtin/packages/beatnik/package.py:        depends_on("mpich +rocm", when="^[virtuals=mpi] mpich")
var/spack/repos/builtin/packages/beatnik/package.py:        depends_on("mvapich2-gdr +rocm", when="^[virtuals=mpi] mvapich2-gdr")
var/spack/repos/builtin/packages/beatnik/package.py:    depends_on("kokkos +cuda +cuda_lambda +cuda_constexpr", when="+cuda")
var/spack/repos/builtin/packages/beatnik/package.py:    depends_on("kokkos +rocm", when="+rocm")
var/spack/repos/builtin/packages/beatnik/package.py:    depends_on("kokkos +wrapper", when="%gcc+cuda")
var/spack/repos/builtin/packages/beatnik/package.py:    depends_on("cabana +cuda", when="+cuda")
var/spack/repos/builtin/packages/beatnik/package.py:    depends_on("cabana +rocm", when="+rocm")
var/spack/repos/builtin/packages/beatnik/package.py:    # backend even when we're compiling for GPUs
var/spack/repos/builtin/packages/beatnik/package.py:    depends_on("heffte +cuda", when="+cuda")
var/spack/repos/builtin/packages/beatnik/package.py:    depends_on("heffte +rocm", when="+rocm")
var/spack/repos/builtin/packages/beatnik/package.py:    # If we're using CUDA or ROCM, require MPIs be GPU-aware
var/spack/repos/builtin/packages/beatnik/package.py:    conflicts("mpich ~cuda", when="+cuda")
var/spack/repos/builtin/packages/beatnik/package.py:    conflicts("mpich ~rocm", when="+rocm")
var/spack/repos/builtin/packages/beatnik/package.py:    conflicts("openmpi ~cuda", when="+cuda")
var/spack/repos/builtin/packages/beatnik/package.py:    conflicts("^spectrum-mpi", when="^cuda@11.3:")  # cuda-aware spectrum is broken with cuda 11.3:
var/spack/repos/builtin/packages/beatnik/package.py:    # Propagate CUDA and AMD GPU targets to cabana
var/spack/repos/builtin/packages/beatnik/package.py:    for cuda_arch in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/beatnik/package.py:        depends_on("cabana cuda_arch=%s" % cuda_arch, when="+cuda cuda_arch=%s" % cuda_arch)
var/spack/repos/builtin/packages/beatnik/package.py:    for amdgpu_value in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/beatnik/package.py:            "cabana +rocm amdgpu_target=%s" % amdgpu_value,
var/spack/repos/builtin/packages/beatnik/package.py:            when="+rocm amdgpu_target=%s" % amdgpu_value,
var/spack/repos/builtin/packages/beatnik/package.py:        # Use hipcc as the c compiler if we are compiling for rocm. Doing it this way
var/spack/repos/builtin/packages/beatnik/package.py:        if self.spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/beatnik/package.py:        # gpu-aware MPI, since cabana and beatnik require it
var/spack/repos/builtin/packages/beatnik/package.py:        if self.spec.satisfies("+rocm ^cray-mpich"):
var/spack/repos/builtin/packages/beatnik/package.py:        elif self.spec.satisfies("+cuda ^cray-mpich"):
var/spack/repos/builtin/packages/beatnik/package.py:                "-DCMAKE_EXE_LINKER_FLAGS=-Wl,-rpath={0} -L{0} -lmpi_gtl_cuda".format(gtl_dir)
var/spack/repos/builtin/packages/rocm-dbgapi/package.py:class RocmDbgapi(CMakePackage):
var/spack/repos/builtin/packages/rocm-dbgapi/package.py:    AMD's commercially available GPU architectures."""
var/spack/repos/builtin/packages/rocm-dbgapi/package.py:    homepage = "https://github.com/ROCm/ROCdbgapi"
var/spack/repos/builtin/packages/rocm-dbgapi/package.py:    git = "https://github.com/ROCm/ROCdbgapi.git"
var/spack/repos/builtin/packages/rocm-dbgapi/package.py:    url = "https://github.com/ROCm/ROCdbgapi/archive/rocm-6.1.2.tar.gz"
var/spack/repos/builtin/packages/rocm-dbgapi/package.py:    tags = ["rocm"]
var/spack/repos/builtin/packages/rocm-dbgapi/package.py:    libraries = ["librocm-dbgapi"]
var/spack/repos/builtin/packages/rocm-dbgapi/package.py:        depends_on(f"rocm-core@{ver}", when=f"@{ver}")
var/spack/repos/builtin/packages/rocm-dbgapi/package.py:            env.set("CC", f"{self.spec['llvm-amdgpu'].prefix}/bin/clang")
var/spack/repos/builtin/packages/rocm-dbgapi/package.py:            env.set("CXX", f"{self.spec['llvm-amdgpu'].prefix}/bin/clang++")
var/spack/repos/builtin/packages/elsi/package.py:class Elsi(CMakePackage, CudaPackage):
var/spack/repos/builtin/packages/elsi/package.py:        depends_on("elpa+cuda", when="+cuda")
var/spack/repos/builtin/packages/elsi/package.py:        depends_on("elpa~cuda", when="~cuda")
var/spack/repos/builtin/packages/elsi/package.py:        depends_on("slepc+cuda", when="+cuda")
var/spack/repos/builtin/packages/elsi/package.py:        depends_on("slepc~cuda", when="~cuda")
var/spack/repos/builtin/packages/elsi/package.py:        depends_on("petsc+cuda", when="+cuda")
var/spack/repos/builtin/packages/elsi/package.py:        depends_on("petsc~cuda", when="~cuda")
var/spack/repos/builtin/packages/elsi/package.py:        depends_on("superlu-dist+cuda", when="+cuda")
var/spack/repos/builtin/packages/elsi/package.py:        depends_on("superlu-dist~cuda", when="~cuda")
var/spack/repos/builtin/packages/elsi/package.py:        depends_on("superlu-dist+cuda", when="+cuda")
var/spack/repos/builtin/packages/elsi/package.py:        depends_on("superlu-dist~cuda", when="~cuda")
var/spack/repos/builtin/packages/elsi/package.py:        conflicts("dla-future~cuda", when="+cuda")
var/spack/repos/builtin/packages/elsi/package.py:        conflicts("dla-future+cuda", when="~cuda")
var/spack/repos/builtin/packages/elsi/package.py:            self.define_from_variant("USE_GPU_CUDA", "cuda"),
var/spack/repos/builtin/packages/elsi/package.py:        if self.spec.satisfies("^elpa+cuda"):
var/spack/repos/builtin/packages/elsi/package.py:            elpa_gpu_string = "nvidia-gpu" if self.spec.satisfies("^elpa@2021:") else "gpu"
var/spack/repos/builtin/packages/elsi/package.py:            args.append(self.define(ELSI_ELPA_GPU_STRING, elpa_gpu_string))
var/spack/repos/builtin/packages/libxc/package.py:class Libxc(AutotoolsPackage, CudaPackage):
var/spack/repos/builtin/packages/libxc/package.py:    conflicts("+shared +cuda", msg="Only ~shared supported with +cuda")
var/spack/repos/builtin/packages/libxc/package.py:    conflicts("+cuda", when="@:4", msg="CUDA support only in libxc 5.0.0 and above")
var/spack/repos/builtin/packages/libxc/package.py:    patch("0002-Mark-xc_erfcx-a-GPU_FUNCTION.patch", when="@5.0.0")
var/spack/repos/builtin/packages/libxc/package.py:        if self.spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/libxc/package.py:            nvcc = self.spec["cuda"].prefix.bin.nvcc
var/spack/repos/builtin/packages/libxc/package.py:            cuda_arch = self.spec.variants["cuda_arch"].value[0]
var/spack/repos/builtin/packages/libxc/package.py:            if cuda_arch != "none":
var/spack/repos/builtin/packages/libxc/package.py:                env.append_flags("CFLAGS", "-arch=sm_{0}".format(cuda_arch))
var/spack/repos/builtin/packages/libxc/package.py:        args += self.enable_or_disable("cuda")
var/spack/repos/builtin/packages/libxc/0002-Mark-xc_erfcx-a-GPU_FUNCTION.patch:Subject: [PATCH] Mark xc_erfcx a GPU_FUNCTION
var/spack/repos/builtin/packages/libxc/0002-Mark-xc_erfcx-a-GPU_FUNCTION.patch:+GPU_FUNCTION
var/spack/repos/builtin/packages/elpa/package.py:class Elpa(AutotoolsPackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/elpa/package.py:            "gpu_streams", default=True, when="+cuda", description="Activates GPU streams support"
var/spack/repos/builtin/packages/elpa/package.py:    depends_on("rocblas", when="+rocm")
var/spack/repos/builtin/packages/elpa/package.py:    conflicts("+mpi", when="+rocm", msg="ROCm support and MPI are not yet compatible")
var/spack/repos/builtin/packages/elpa/package.py:        "+gpu_streams",
var/spack/repos/builtin/packages/elpa/package.py:        msg="GPU streams currently not supported in combination with OpenMP",
var/spack/repos/builtin/packages/elpa/package.py:        cuda_flag = "nvidia-gpu"
var/spack/repos/builtin/packages/elpa/package.py:        if spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/elpa/package.py:            prefix = spec["cuda"].prefix
var/spack/repos/builtin/packages/elpa/package.py:            # Can't yet be changed to the new option --enable-nvidia-gpu-kernels
var/spack/repos/builtin/packages/elpa/package.py:            options.append(f"--enable-{cuda_flag}")
var/spack/repos/builtin/packages/elpa/package.py:            options.append("--with-cuda-path={0}".format(prefix))
var/spack/repos/builtin/packages/elpa/package.py:            options.append("--with-cuda-sdk-path={0}".format(prefix))
var/spack/repos/builtin/packages/elpa/package.py:            if spec.satisfies("+gpu_streams"):
var/spack/repos/builtin/packages/elpa/package.py:                options.append("--enable-gpu-streams=nvidia")
var/spack/repos/builtin/packages/elpa/package.py:            cuda_arch = spec.variants["cuda_arch"].value[0]
var/spack/repos/builtin/packages/elpa/package.py:            if cuda_arch != "none":
var/spack/repos/builtin/packages/elpa/package.py:                    "--with-{0}-compute-capability=sm_{1}".format(cuda_flag.upper(), cuda_arch)
var/spack/repos/builtin/packages/elpa/package.py:            options.append(f"--disable-{cuda_flag}" + kernels)
var/spack/repos/builtin/packages/elpa/package.py:        if spec.satisfies("+rocm"):
var/spack/repos/builtin/packages/elpa/package.py:            # Can't yet be changed to the new option --enable-amd-gpu-kernels
var/spack/repos/builtin/packages/elpa/package.py:            options.append("--enable-amd-gpu")
var/spack/repos/builtin/packages/elpa/package.py:            if spec.satisfies("+gpu_streams"):
var/spack/repos/builtin/packages/elpa/package.py:                options.append("--enable-gpu-streams=amd")
var/spack/repos/builtin/packages/elpa/package.py:            options.append("--disable-amd-gpu" + kernels)
var/spack/repos/builtin/packages/rocprofiler-register/package.py:    homepage = "https://github.com/ROCm/rocprofiler-register"
var/spack/repos/builtin/packages/rocprofiler-register/package.py:    git = "https://github.com/ROCm/rocprofiler-register.git"
var/spack/repos/builtin/packages/rocprofiler-register/package.py:    url = "https://github.com/ROCm/rocprofiler-register/archive/refs/tags/rocm-6.2.0.tar.gz"
var/spack/repos/builtin/packages/rocprofiler-register/package.py:    tags = ["rocm"]
var/spack/repos/builtin/packages/miopengemm/package.py:    """An OpenCL general matrix multiplication (GEMM) API
var/spack/repos/builtin/packages/miopengemm/package.py:    homepage = "https://github.com/ROCm/MIOpenGEMM"
var/spack/repos/builtin/packages/miopengemm/package.py:    git = "https://github.com/ROCm/MIOpenGEMM.git"
var/spack/repos/builtin/packages/miopengemm/package.py:    url = "https://github.com/ROCm/MIOpenGEMM/archive/rocm-6.0.0.tar.gz"
var/spack/repos/builtin/packages/miopengemm/package.py:    tags = ["rocm"]
var/spack/repos/builtin/packages/miopengemm/package.py:            return "https://github.com/ROCm/MIOpenGEMM/archive/1.1.6.tar.gz"
var/spack/repos/builtin/packages/miopengemm/package.py:        url = "https://github.com/ROCm/MIOpenGEMM/archive/rocm-{0}.tar.gz"
var/spack/repos/builtin/packages/miopengemm/package.py:        depends_on(f"rocm-cmake@{ver}", type="build", when=f"@{ver}")
var/spack/repos/builtin/packages/miopengemm/package.py:        depends_on(f"rocm-opencl@{ver}", when=f"@{ver}")
var/spack/repos/builtin/packages/hiprand/package.py:class Hiprand(CMakePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/hiprand/package.py:    homepage = "https://github.com/ROCm/hipRAND"
var/spack/repos/builtin/packages/hiprand/package.py:    git = "https://github.com/ROCm/hipRAND.git"
var/spack/repos/builtin/packages/hiprand/package.py:    url = "https://github.com/ROCm/hipRAND/archive/rocm-6.1.2.tar.gz"
var/spack/repos/builtin/packages/hiprand/package.py:    tags = ["rocm"]
var/spack/repos/builtin/packages/hiprand/package.py:    # default to an 'auto' variant until amdgpu_targets can be given a better default than 'none'
var/spack/repos/builtin/packages/hiprand/package.py:    amdgpu_targets = ROCmPackage.amdgpu_targets
var/spack/repos/builtin/packages/hiprand/package.py:        "amdgpu_target",
var/spack/repos/builtin/packages/hiprand/package.py:        description="AMD GPU architecture",
var/spack/repos/builtin/packages/hiprand/package.py:        values=spack.variant.DisjointSetsOfValues(("auto",), ("none",), amdgpu_targets)
var/spack/repos/builtin/packages/hiprand/package.py:    variant("rocm", default=True, description="Enable ROCm support")
var/spack/repos/builtin/packages/hiprand/package.py:    conflicts("+cuda +rocm", msg="CUDA and ROCm support are mutually exclusive")
var/spack/repos/builtin/packages/hiprand/package.py:    conflicts("~cuda ~rocm", msg="CUDA or ROCm support is required")
var/spack/repos/builtin/packages/hiprand/package.py:    depends_on("rocm-cmake@5.2.0:", type="build", when="@5.2.0:")
var/spack/repos/builtin/packages/hiprand/package.py:    depends_on("rocm-cmake@5.1.0:", type="build")
var/spack/repos/builtin/packages/hiprand/package.py:    depends_on("hip +cuda", when="+cuda")
var/spack/repos/builtin/packages/hiprand/package.py:        depends_on("rocrand@" + ver, when="+rocm @" + ver)
var/spack/repos/builtin/packages/hiprand/package.py:    depends_on("rocrand ~hiprand", when="+rocm")
var/spack/repos/builtin/packages/hiprand/package.py:    for tgt in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/hiprand/package.py:            "rocrand amdgpu_target={0}".format(tgt), when="+rocm amdgpu_target={0}".format(tgt)
var/spack/repos/builtin/packages/hiprand/package.py:        if self.spec.satisfies("+cuda"):
var/spack/repos/builtin/packages/hiprand/package.py:            args.append(self.define("BUILD_WITH_LIB", "CUDA"))
var/spack/repos/builtin/packages/hiprand/package.py:            # FindHIP.cmake is used for +cuda
var/spack/repos/builtin/packages/hiprand/package.py:            args.append(self.define("BUILD_WITH_LIB", "ROCM"))
var/spack/repos/builtin/packages/ceed/package.py:class Ceed(BundlePackage, CudaPackage, ROCmPackage):
var/spack/repos/builtin/packages/ceed/package.py:        depends_on("libceed~cuda", when="~cuda")
var/spack/repos/builtin/packages/ceed/package.py:        for arch in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/ceed/package.py:                "libceed+cuda+magma cuda_arch={0}".format(arch),
var/spack/repos/builtin/packages/ceed/package.py:                when="+cuda cuda_arch={0}".format(arch),
var/spack/repos/builtin/packages/ceed/package.py:        depends_on("libceed~rocm", when="~rocm")
var/spack/repos/builtin/packages/ceed/package.py:        for target in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/ceed/package.py:                "libceed+rocm amdgpu_target={0}".format(target),
var/spack/repos/builtin/packages/ceed/package.py:                when="+rocm amdgpu_target={0}".format(target),
var/spack/repos/builtin/packages/ceed/package.py:    depends_on("libceed@0.8~cuda", when="@4.0.0~cuda")
var/spack/repos/builtin/packages/ceed/package.py:    for arch in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/ceed/package.py:            "libceed@0.8+cuda+magma cuda_arch={0}".format(arch),
var/spack/repos/builtin/packages/ceed/package.py:            when="@4.0.0+cuda cuda_arch={0}".format(arch),
var/spack/repos/builtin/packages/ceed/package.py:    depends_on("libceed@0.8~rocm", when="@4.0.0~rocm")
var/spack/repos/builtin/packages/ceed/package.py:    for target in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/ceed/package.py:            "libceed@0.8+rocm amdgpu_target={0}".format(target),
var/spack/repos/builtin/packages/ceed/package.py:            when="@4.0.0+rocm amdgpu_target={0}".format(target),
var/spack/repos/builtin/packages/ceed/package.py:    depends_on("libceed@0.6~cuda", when="@3.0.0~cuda")
var/spack/repos/builtin/packages/ceed/package.py:    depends_on("libceed@0.6+cuda+magma", when="@3.0.0+cuda")
var/spack/repos/builtin/packages/ceed/package.py:    depends_on("libceed@0.4~cuda", when="@2.0.0~cuda")
var/spack/repos/builtin/packages/ceed/package.py:    depends_on("libceed@0.4+cuda", when="@2.0.0+cuda")
var/spack/repos/builtin/packages/ceed/package.py:    depends_on("libceed@0.2~cuda", when="@1.0.0~cuda")
var/spack/repos/builtin/packages/ceed/package.py:    depends_on("libceed@0.2+cuda", when="@1.0.0+cuda")
var/spack/repos/builtin/packages/ceed/package.py:    depends_on("occa@1.1.0~cuda", when="@5.0.0+occa~cuda")
var/spack/repos/builtin/packages/ceed/package.py:    depends_on("occa@1.1.0+cuda", when="@5.0.0+occa+cuda")
var/spack/repos/builtin/packages/ceed/package.py:    depends_on("occa@1.1.0~cuda", when="@4.0.0+occa~cuda")
var/spack/repos/builtin/packages/ceed/package.py:    depends_on("occa@1.1.0+cuda", when="@4.0.0+occa+cuda")
var/spack/repos/builtin/packages/ceed/package.py:    depends_on("occa@1.0.9~cuda", when="@3.0.0+occa~cuda")
var/spack/repos/builtin/packages/ceed/package.py:    depends_on("occa@1.0.9+cuda", when="@3.0.0+occa+cuda")
var/spack/repos/builtin/packages/ceed/package.py:    depends_on("occa@1.0.8~cuda", when="@2.0.0+occa~cuda")
var/spack/repos/builtin/packages/ceed/package.py:    depends_on("occa@1.0.8+cuda", when="@2.0.0+occa+cuda")
var/spack/repos/builtin/packages/ceed/package.py:    depends_on("occa@1.0.0-alpha.5~cuda", when="@1.0.0+occa~cuda")
var/spack/repos/builtin/packages/ceed/package.py:    depends_on("occa@1.0.0-alpha.5+cuda", when="@1.0.0+occa+cuda")
var/spack/repos/builtin/packages/ceed/package.py:    for arch in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/ceed/package.py:            "nekrs@21.0+cuda cuda_arch={0}".format(arch),
var/spack/repos/builtin/packages/ceed/package.py:            when="@4.0.0:5+nek+cuda cuda_arch={0}".format(arch),
var/spack/repos/builtin/packages/ceed/package.py:    for target in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/ceed/package.py:            "nekrs@21.0+rocm amdgpu_target={0}".format(target),
var/spack/repos/builtin/packages/ceed/package.py:            when="@4.0.0:5+nek+rocm amdgpu_target={0}".format(target),
var/spack/repos/builtin/packages/ceed/package.py:        for arch in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/ceed/package.py:                "petsc+cuda cuda_arch={0}".format(arch), when="+cuda cuda_arch={0}".format(arch)
var/spack/repos/builtin/packages/ceed/package.py:                "ratel+cuda cuda_arch={0}".format(arch), when="+cuda cuda_arch={0}".format(arch)
var/spack/repos/builtin/packages/ceed/package.py:        for target in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/ceed/package.py:                "petsc+rocm amdgpu_target={0}".format(target),
var/spack/repos/builtin/packages/ceed/package.py:                when="+rocm amdgpu_target={0}".format(target),
var/spack/repos/builtin/packages/ceed/package.py:                "ratel+rocm amdgpu_target={0}".format(target),
var/spack/repos/builtin/packages/ceed/package.py:                when="+rocm amdgpu_target={0}".format(target),
var/spack/repos/builtin/packages/ceed/package.py:    for arch in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/ceed/package.py:            "petsc+cuda cuda_arch={0}".format(arch),
var/spack/repos/builtin/packages/ceed/package.py:            when="@4.0.0+petsc+cuda cuda_arch={0}".format(arch),
var/spack/repos/builtin/packages/ceed/package.py:    for target in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/ceed/package.py:            "petsc@3.15.0:3.15+rocm amdgpu_target={0}".format(target),
var/spack/repos/builtin/packages/ceed/package.py:            when="@4.0.0:4+petsc+rocm amdgpu_target={0}".format(target),
var/spack/repos/builtin/packages/ceed/package.py:    depends_on("petsc+cuda", when="@3.0.0+petsc+cuda")
var/spack/repos/builtin/packages/ceed/package.py:    for arch in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/ceed/package.py:            "magma@2.6.2+cuda cuda_arch={0}".format(arch),
var/spack/repos/builtin/packages/ceed/package.py:            when="@5.0.0+cuda cuda_arch={0}".format(arch),
var/spack/repos/builtin/packages/ceed/package.py:    for target in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/ceed/package.py:            "magma@2.6.2~cuda+rocm amdgpu_target={0}".format(target),
var/spack/repos/builtin/packages/ceed/package.py:            when="@5.0.0+rocm amdgpu_target={0}".format(target),
var/spack/repos/builtin/packages/ceed/package.py:    for arch in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/ceed/package.py:            "magma@2.5.4 cuda_arch={0}".format(arch), when="@4.0.0+cuda cuda_arch={0}".format(arch)
var/spack/repos/builtin/packages/ceed/package.py:    depends_on("magma@2.5.3", when="@3.0.0+cuda")
var/spack/repos/builtin/packages/ceed/package.py:    depends_on("magma@2.5.0", when="@2.0.0+cuda")
var/spack/repos/builtin/packages/ceed/package.py:    depends_on("magma@2.3.0", when="@1.0.0+cuda")
var/spack/repos/builtin/packages/ceed/package.py:        for arch in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/ceed/package.py:                "mfem+cuda cuda_arch={0}".format(arch), when="+cuda cuda_arch={0}".format(arch)
var/spack/repos/builtin/packages/ceed/package.py:        for target in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/ceed/package.py:                "mfem+rocm amdgpu_target={0}".format(target),
var/spack/repos/builtin/packages/ceed/package.py:                when="+rocm amdgpu_target={0}".format(target),
var/spack/repos/builtin/packages/ceed/package.py:    for arch in CudaPackage.cuda_arch_values:
var/spack/repos/builtin/packages/ceed/package.py:            "mfem@4.2.0+cuda cuda_arch={0}".format(arch),
var/spack/repos/builtin/packages/ceed/package.py:            when="@4.0.0+mfem+cuda cuda_arch={0}".format(arch),
var/spack/repos/builtin/packages/ceed/package.py:    for target in ROCmPackage.amdgpu_targets:
var/spack/repos/builtin/packages/ceed/package.py:            "mfem@4.2.0+rocm amdgpu_target={0}".format(target),
var/spack/repos/builtin/packages/ceed/package.py:            when="@4.0.0+mfem+rocm amdgpu_target={0}".format(target),
var/spack/repos/builtin/packages/ceed/package.py:    depends_on("mfem@4.1.0+cuda", when="@3.0.0+mfem+cuda")
var/spack/repos/builtin.mock/packages/dependency-mv/package.py:    variant("cuda", default=False, description="Build with CUDA")
var/spack/repos/builtin.mock/packages/dependency-mv/package.py:    variant("cuda_arch", values=any_combination_of("10", "11"), when="+cuda")
var/spack/repos/builtin.mock/packages/forward-multi-value/package.py:    variant("cuda", default=False, description="Build with CUDA")
var/spack/repos/builtin.mock/packages/forward-multi-value/package.py:    variant("cuda_arch", values=any_combination_of("10", "11"), when="+cuda")
var/spack/repos/builtin.mock/packages/forward-multi-value/package.py:    requires("^dependency-mv cuda_arch=10", when="+cuda cuda_arch=10 ^dependency-mv+cuda")
var/spack/repos/builtin.mock/packages/forward-multi-value/package.py:    requires("^dependency-mv cuda_arch=11", when="+cuda cuda_arch=11 ^dependency-mv+cuda")
var/spack/repos/builtin.mock/packages/vtk-m/package.py:    variant("cuda", default=False, description="Build with CUDA")
var/spack/repos/builtin.mock/packages/vtk-m/package.py:        "cuda_arch",
var/spack/repos/builtin.mock/packages/vtk-m/package.py:        description="CUDA architecture",
var/spack/repos/builtin.mock/packages/vtk-m/package.py:        when="+cuda",
var/spack/repos/builtin.mock/packages/vtk-m/package.py:    variant("rocm", default=False, description="Enable ROCm support")
var/spack/repos/builtin.mock/packages/vtk-m/package.py:        "amdgpu_target",
var/spack/repos/builtin.mock/packages/vtk-m/package.py:        description="AMD GPU architecture",
var/spack/repos/builtin.mock/packages/vtk-m/package.py:        when="+rocm",
lib/spack/docs/gpu_configuration.rst:Using External GPU Support
lib/spack/docs/gpu_configuration.rst:Many packages come with a ``+cuda`` or ``+rocm`` variant. With no added
lib/spack/docs/gpu_configuration.rst:help with using a system installation of GPU libraries.
lib/spack/docs/gpu_configuration.rst:Using an External ROCm Installation
lib/spack/docs/gpu_configuration.rst:Spack breaks down ROCm into many separate component packages. The following
lib/spack/docs/gpu_configuration.rst:is an example ``packages.yaml`` that organizes a consistent set of ROCm
lib/spack/docs/gpu_configuration.rst:       compiler: [rocmcc@=5.3.0]
lib/spack/docs/gpu_configuration.rst:       variants: amdgpu_target=gfx90a
lib/spack/docs/gpu_configuration.rst:         prefix: /opt/rocm-5.3.0/hip
lib/spack/docs/gpu_configuration.rst:         prefix: /opt/rocm-5.3.0/
lib/spack/docs/gpu_configuration.rst:     llvm-amdgpu:
lib/spack/docs/gpu_configuration.rst:       - spec: llvm-amdgpu@5.3.0
lib/spack/docs/gpu_configuration.rst:         prefix: /opt/rocm-5.3.0/llvm/
lib/spack/docs/gpu_configuration.rst:         prefix: /opt/rocm-5.3.0/
lib/spack/docs/gpu_configuration.rst:         prefix: /opt/rocm-5.3.0/
lib/spack/docs/gpu_configuration.rst:         prefix: /opt/rocm-5.3.0/
lib/spack/docs/gpu_configuration.rst:         prefix: /opt/rocm-5.3.0/
lib/spack/docs/gpu_configuration.rst:         prefix: /opt/rocm-5.3.0/rocprim/
lib/spack/docs/gpu_configuration.rst:       spec: rocmcc@=5.3.0
lib/spack/docs/gpu_configuration.rst:         cc: /opt/rocm-5.3.0/bin/amdclang
lib/spack/docs/gpu_configuration.rst:         cxx: /opt/rocm-5.3.0/bin/amdclang++
lib/spack/docs/gpu_configuration.rst:         fc: /opt/rocm-5.3.0/bin/amdflang
lib/spack/docs/gpu_configuration.rst:- ``spack external find`` can automatically locate some of the ``hip``/``rocm``
lib/spack/docs/gpu_configuration.rst:  guarantees a complementary set if multiple ROCm installations are available.
lib/spack/docs/gpu_configuration.rst:Using an External CUDA Installation
lib/spack/docs/gpu_configuration.rst:CUDA is split into fewer components and is simpler to specify:
lib/spack/docs/gpu_configuration.rst:       - cuda_arch=70
lib/spack/docs/gpu_configuration.rst:     cuda:
lib/spack/docs/gpu_configuration.rst:       - spec: cuda@11.0.2
lib/spack/docs/gpu_configuration.rst:         prefix: /opt/cuda/cuda-11.0.2/
lib/spack/docs/gpu_configuration.rst:where ``/opt/cuda/cuda-11.0.2/lib/`` contains ``libcudart.so``.
lib/spack/docs/packages_yaml.rst:       - one_of: ["+cuda", "+rocm"]
lib/spack/docs/packages_yaml.rst:In the example above, that means you could build ``mpich+cuda`` or ``mpich+rocm`` but not ``mpich+cuda+rocm``.
lib/spack/docs/packages_yaml.rst:       - "+cuda"
lib/spack/docs/packages_yaml.rst:no ``cuda`` variant.
lib/spack/docs/packages_yaml.rst:       require: "+shared +cuda"
lib/spack/docs/packages_yaml.rst:the default requirement will be enforced only if a package has both a ``cuda`` and
lib/spack/docs/packages_yaml.rst:       require: '~cuda'
lib/spack/docs/packages_yaml.rst:you will use ``mvapich2~cuda %gcc`` as an ``mpi`` provider.
lib/spack/docs/contribution_guide.rst:CUDA capable GPU may have the tag ``x86_64-cuda`` to denote that it should only be used for packages that will benefit from
lib/spack/docs/contribution_guide.rst:a CUDA capable resource.
lib/spack/docs/index.rst:   gpu_configuration
lib/spack/docs/features.rst:   $ spack install hdf5@1.10.1 %gcc@4.7.3 +debug ^openmpi+cuda fabrics=auto ^hwloc+gl
lib/spack/docs/containers.rst:Consider, as an example, building a production grade image for a CUDA
lib/spack/docs/containers.rst:images provided by the vendor and regard CUDA as an external package.
lib/spack/docs/containers.rst:Spack doesn't currently provide an official image with CUDA configured
lib/spack/docs/containers.rst:     - gromacs@2019.4+cuda build_type=Release
lib/spack/docs/containers.rst:       cuda:
lib/spack/docs/containers.rst:         - spec: cuda%gcc
lib/spack/docs/containers.rst:           prefix: /usr/local/cuda
lib/spack/docs/containers.rst:         build: custom/cuda-10.1-ubuntu18.04:latest
lib/spack/docs/containers.rst:         final: nvidia/cuda:10.1-base-ubuntu18.04
lib/spack/docs/containers.rst:   FROM custom/cuda-10.1-ubuntu18.04:latest as builder
lib/spack/docs/containers.rst:   &&   echo "  - gromacs@2019.4+cuda build_type=Release" \
lib/spack/docs/containers.rst:   &&   echo "    cuda:" \
lib/spack/docs/containers.rst:   &&   echo "      - spec: cuda%gcc" \
lib/spack/docs/containers.rst:   &&   echo "        prefix: /usr/local/cuda" \
lib/spack/docs/containers.rst:   FROM nvidia/cuda:10.1-base-ubuntu18.04
lib/spack/docs/containers.rst:CUDA
lib/spack/docs/containers.rst:Starting from CUDA 9.0, Nvidia provides minimal CUDA images based on
lib/spack/docs/containers.rst:Ubuntu. Please see `their instructions <https://hub.docker.com/r/nvidia/cuda/>`_.
lib/spack/docs/containers.rst:Avoid double-installing CUDA by adding, e.g.
lib/spack/docs/containers.rst:     cuda:
lib/spack/docs/containers.rst:       - spec: "cuda@9.0.176%gcc@5.4.0 arch=linux-ubuntu16-x86_64"
lib/spack/docs/containers.rst:         prefix: /usr/local/cuda
lib/spack/docs/containers.rst:Users will either need ``nvidia-docker`` or e.g. Singularity to *execute*
lib/spack/docs/packaging_guide.rst:           depends_on("cuda")
lib/spack/docs/packaging_guide.rst:       depends_on("cuda", when="+nvptx")
lib/spack/docs/packaging_guide.rst:and reused by many packages. For instance, packages that depend on ``Cuda`` or ``Rocm``, share
lib/spack/docs/packaging_guide.rst:| :class:`~spack.build_systems.cuda.CudaPackage`                | A helper class for packages that |
lib/spack/docs/packaging_guide.rst:|                                                               | use CUDA                         |
lib/spack/docs/packaging_guide.rst:| :class:`~spack.build_systems.rocm.ROCmPackage`                | A helper class for packages that |
lib/spack/docs/packaging_guide.rst:|                                                               | use ROCm                         |
lib/spack/docs/packaging_guide.rst:   class Cp2k(MakefilePackage, CudaPackage):
lib/spack/docs/packaging_guide.rst:In the example above ``Cp2k`` inherits all the conflicts and variants that ``CudaPackage`` defines.
lib/spack/docs/build_systems/cudapackage.rst:.. _cudapackage:
lib/spack/docs/build_systems/cudapackage.rst:Cuda
lib/spack/docs/build_systems/cudapackage.rst:Different from other packages, ``CudaPackage`` does not represent a build system.
lib/spack/docs/build_systems/cudapackage.rst:Instead its goal is to simplify and unify usage of ``CUDA`` in other packages by providing a `mixin-class <https://en.wikipedia.org/wiki/Mixin>`_.
lib/spack/docs/build_systems/cudapackage.rst:`<https://github.com/spack/spack/blob/develop/lib/spack/spack/build_systems/cuda.py>`__.
lib/spack/docs/build_systems/cudapackage.rst:* **cuda**
lib/spack/docs/build_systems/cudapackage.rst:  This variant is used to enable/disable building with ``CUDA``. The default
lib/spack/docs/build_systems/cudapackage.rst:* **cuda_arch**
lib/spack/docs/build_systems/cudapackage.rst:  Valid values are maintained in the ``cuda_arch_values`` property and
lib/spack/docs/build_systems/cudapackage.rst:  ``CUDA`` dependencies and compiler conflicts.
lib/spack/docs/build_systems/cudapackage.rst:  GPUs and their compute capability versions are listed at
lib/spack/docs/build_systems/cudapackage.rst:  https://developer.nvidia.com/cuda-gpus .
lib/spack/docs/build_systems/cudapackage.rst:base ``CUDA`` conflicts have been included with this package, you may
lib/spack/docs/build_systems/cudapackage.rst:For example, if your package requires ``cuda_arch`` to be specified when
lib/spack/docs/build_systems/cudapackage.rst:``cuda`` is enabled, you can add the following conflict to your package
lib/spack/docs/build_systems/cudapackage.rst:    conflicts("cuda_arch=none", when="+cuda",
lib/spack/docs/build_systems/cudapackage.rst:              msg="CUDA architecture is required")
lib/spack/docs/build_systems/cudapackage.rst:suppose your software does not work with CUDA compute capability versions
lib/spack/docs/build_systems/cudapackage.rst:    unsupported_cuda_archs = [
lib/spack/docs/build_systems/cudapackage.rst:    for value in unsupported_cuda_archs:
lib/spack/docs/build_systems/cudapackage.rst:        conflicts(f"cuda_arch={value}", when="+cuda",
lib/spack/docs/build_systems/cudapackage.rst:                  msg=f"CUDA architecture {value} is not supported")
lib/spack/docs/build_systems/cudapackage.rst:standard CUDA compiler flags.
lib/spack/docs/build_systems/cudapackage.rst:**cuda_flags**
lib/spack/docs/build_systems/cudapackage.rst:    for the chosen ``cuda_arch`` value(s).  The flags are intended to
lib/spack/docs/build_systems/cudapackage.rst:    be passed to the CUDA compiler driver (i.e., ``nvcc``).
lib/spack/docs/build_systems/cudapackage.rst:    class MyCudaPackage(CMakePackage, CudaPackage):
lib/spack/docs/build_systems/cudapackage.rst:            if spec.satisfies("+cuda"):
lib/spack/docs/build_systems/cudapackage.rst:                # Set up the cuda macros needed by the build
lib/spack/docs/build_systems/cudapackage.rst:                args.append("-DWITH_CUDA=ON")
lib/spack/docs/build_systems/cudapackage.rst:                cuda_arch_list = spec.variants["cuda_arch"].value
lib/spack/docs/build_systems/cudapackage.rst:                cuda_arch = cuda_arch_list[0]
lib/spack/docs/build_systems/cudapackage.rst:                if cuda_arch != "none":
lib/spack/docs/build_systems/cudapackage.rst:                    args.append(f"-DCUDA_FLAGS=-arch=sm_{cuda_arch}")
lib/spack/docs/build_systems/cudapackage.rst:                # Ensure build with cuda is disabled
lib/spack/docs/build_systems/cudapackage.rst:                args.append("-DWITH_CUDA=OFF")
lib/spack/docs/build_systems/cudapackage.rst:assuming only the ``WITH_CUDA`` and ``CUDA_FLAGS`` flags are required.
lib/spack/docs/build_systems/cudapackage.rst:This example also illustrates how to check for the ``cuda`` variant using
lib/spack/docs/build_systems/cudapackage.rst:``self.spec`` and how to retrieve the ``cuda_arch`` variant's value, which
lib/spack/docs/build_systems/cudapackage.rst:is a list, using ``self.spec.variants["cuda_arch"].value``.
lib/spack/docs/build_systems/cudapackage.rst:With over 70 packages using ``CudaPackage`` as of January 2021 there are
lib/spack/docs/build_systems/rocmpackage.rst:.. _rocmpackage:
lib/spack/docs/build_systems/rocmpackage.rst:ROCm
lib/spack/docs/build_systems/rocmpackage.rst:The ``ROCmPackage`` is not a build system but a helper package. Like ``CudaPackage``,
lib/spack/docs/build_systems/rocmpackage.rst:packages using GPUs though for AMD in this case.
lib/spack/docs/build_systems/rocmpackage.rst:`<https://github.com/spack/spack/blob/develop/lib/spack/spack/build_systems/rocm.py>`__.
lib/spack/docs/build_systems/rocmpackage.rst:* **rocm**
lib/spack/docs/build_systems/rocmpackage.rst:  This variant is used to enable/disable building with ``rocm``.
lib/spack/docs/build_systems/rocmpackage.rst:* **amdgpu_target**
lib/spack/docs/build_systems/rocmpackage.rst:  This variant supports the optional specification of the AMD GPU architecture.
lib/spack/docs/build_systems/rocmpackage.rst:  Valid values are the names of the GPUs (e.g., ``gfx701``), which are maintained
lib/spack/docs/build_systems/rocmpackage.rst:  in the ``amdgpu_targets`` property.
lib/spack/docs/build_systems/rocmpackage.rst:This package defines basic ``rocm`` dependencies, including ``llvm`` and ``hip``.
lib/spack/docs/build_systems/rocmpackage.rst:already requires that the ``amdgpu_target`` always be specified for ``rocm``
lib/spack/docs/build_systems/rocmpackage.rst:builds. It also defines a conflict that prevents builds with an ``amdgpu_target``
lib/spack/docs/build_systems/rocmpackage.rst:when ``rocm`` is disabled.
lib/spack/docs/build_systems/rocmpackage.rst:    ``--amdgpu-target`` build option for ``hipcc``.
lib/spack/docs/build_systems/rocmpackage.rst:    class MyRocmPackage(CMakePackage, ROCmPackage):
lib/spack/docs/build_systems/rocmpackage.rst:        # Ensure +rocm and amdgpu_targets are passed to dependencies
lib/spack/docs/build_systems/rocmpackage.rst:        depends_on("mydeppackage", when="+rocm")
lib/spack/docs/build_systems/rocmpackage.rst:        for val in ROCmPackage.amdgpu_targets:
lib/spack/docs/build_systems/rocmpackage.rst:            depends_on(f"mydeppackage amdgpu_target={val}",
lib/spack/docs/build_systems/rocmpackage.rst:                       when=f"amdgpu_target={val}")
lib/spack/docs/build_systems/rocmpackage.rst:            if spec.satisfies("+rocm"):
lib/spack/docs/build_systems/rocmpackage.rst:                rocm_archs = spec.variants["amdgpu_target"].value
lib/spack/docs/build_systems/rocmpackage.rst:                if "none" not in rocm_archs:
lib/spack/docs/build_systems/rocmpackage.rst:                    args.append(f"-DHIP_HIPCC_FLAGS=--amdgpu-target={','.join(rocm_archs}")
lib/spack/docs/build_systems/rocmpackage.rst:macros are required to be set and the only dependency needing rocm options
lib/spack/docs/build_systems/rocmpackage.rst:This example also illustrates how to check for the ``rocm`` variant using
lib/spack/docs/build_systems/rocmpackage.rst:``self.spec`` and how to retrieve the ``amdgpu_target`` variant's value
lib/spack/docs/build_systems/rocmpackage.rst:using ``self.spec.variants["amdgpu_target"].value``.
lib/spack/docs/build_systems/rocmpackage.rst:All five packages using ``ROCmPackage`` as of January 2021 also use the
lib/spack/docs/build_systems/rocmpackage.rst::ref:`CudaPackage <cudapackage>`. So it is worth looking at those packages
lib/spack/docs/build_systems/rocmpackage.rst:to get ideas for creating a package that can support both ``cuda`` and
lib/spack/docs/build_systems/rocmpackage.rst:``rocm``.
lib/spack/docs/build_systems/bundlepackage.rst:where ``Xsdk`` also inherits from ``CudaPackage`` and ``RocmPackage`` and
lib/spack/docs/build_systems/intelpackage.rst:* ``intel-gpu-tools`` -- Test suite and low-level tools for the Linux `Direct
lib/spack/docs/build_systems/cachedcmakepackage.rst:   # includes information on mpi and cuda if applicable
lib/spack/docs/config_yaml.rst:   ``libcuda.so``, provided by the CUDA toolkit; it can be used to link against,
lib/spack/docs/config_yaml.rst:   but the library needed at runtime is the one installed with the CUDA driver.
lib/spack/docs/build_systems.rst:   build_systems/cudapackage
lib/spack/docs/build_systems.rst:   build_systems/rocmpackage
lib/spack/external/archspec/json/cpu/microarchitectures.json:      "0x4e": "Nvidia",
lib/spack/spack/container/images.json:    "nvidia/cuda:11.2.1": {
lib/spack/spack/container/images.json:        "template": "container/cuda_11_2_1.dockerfile",
lib/spack/spack/container/images.json:        "image": "nvidia/cuda:11.2.1-devel"
lib/spack/spack/container/images.json:        "image": "nvidia/cuda:11.2.1-base"
lib/spack/spack/detection/common.py:    cuda_re = r"CUDA_PATH[a-zA-Z1-9_]*"
lib/spack/spack/detection/common.py:    add_path = lambda key: re.search(cuda_re, key) or key in path_ext_keys
lib/spack/spack/compilers/rocmcc.py:class Rocmcc(spack.compilers.clang.Clang):
lib/spack/spack/compilers/rocmcc.py:            "cc": "rocmcc/amdclang",
lib/spack/spack/compilers/rocmcc.py:            "cxx": "rocmcc/amdclang++",
lib/spack/spack/compilers/rocmcc.py:            "f77": "rocmcc/amdflang",
lib/spack/spack/compilers/rocmcc.py:            "fc": "rocmcc/amdflang",
lib/spack/spack/compilers/oneapi.py:        "libOpenCL",
lib/spack/spack/compilers/__init__.py:    "rocmcc": "llvm-amdgpu",
lib/spack/spack/compilers/__init__.py:    "llvm-amdgpu": "rocmcc",
lib/spack/spack/package.py:from spack.build_systems.cuda import CudaPackage
lib/spack/spack/package.py:from spack.build_systems.rocm import ROCmPackage
lib/spack/spack/test/data/microarchitectures/microarchitectures.json:      "0x4e": "Nvidia",
lib/spack/spack/test/data/unparse/trilinos.txt:class Trilinos(CMakePackage, CudaPackage):
lib/spack/spack/test/data/unparse/trilinos.txt:    variant("cuda_rdc", default=False, description="turn on RDC for CUDA build")
lib/spack/spack/test/data/unparse/trilinos.txt:    variant("wrapper", default=False, description="Use nvcc-wrapper for CUDA build")
lib/spack/spack/test/data/unparse/trilinos.txt:        conflicts("+cuda")
lib/spack/spack/test/data/unparse/trilinos.txt:    conflicts("cxxstd=11", when="+wrapper ^cuda@6.5.14")
lib/spack/spack/test/data/unparse/trilinos.txt:    conflicts("cxxstd=14", when="+wrapper ^cuda@6.5.14:8.0.61")
lib/spack/spack/test/data/unparse/trilinos.txt:    conflicts("cxxstd=17", when="+wrapper ^cuda@6.5.14:10.2.89")
lib/spack/spack/test/data/unparse/trilinos.txt:    # CUDA without wrapper requires clang
lib/spack/spack/test/data/unparse/trilinos.txt:                "+cuda",
lib/spack/spack/test/data/unparse/trilinos.txt:                msg="trilinos~wrapper+cuda can only be built with the " "Clang compiler",
lib/spack/spack/test/data/unparse/trilinos.txt:    conflicts("+cuda_rdc", when="~cuda")
lib/spack/spack/test/data/unparse/trilinos.txt:    conflicts("+wrapper", when="~cuda")
lib/spack/spack/test/data/unparse/trilinos.txt:    # Old trilinos fails with new CUDA (see #27180)
lib/spack/spack/test/data/unparse/trilinos.txt:    conflicts("@:13.0.1 +cuda", when="^cuda@11:")
lib/spack/spack/test/data/unparse/trilinos.txt:    depends_on("hwloc+cuda", when="@13: +kokkos+cuda")
lib/spack/spack/test/data/unparse/trilinos.txt:    # avoid calling deprecated functions with CUDA-11
lib/spack/spack/test/data/unparse/trilinos.txt:    patch("fix_cxx14_cuda11.patch", when="@13.0.0:13.0.1 cxxstd=14 ^cuda@11:")
lib/spack/spack/test/data/unparse/trilinos.txt:        if "+cuda" in self.spec:
lib/spack/spack/test/data/unparse/trilinos.txt:            # it relies on blocking CUDA kernel launch. This is needed
lib/spack/spack/test/data/unparse/trilinos.txt:            # in case the dependent app also run a CUDA backend via Trilinos
lib/spack/spack/test/data/unparse/trilinos.txt:            env.set("CUDA_LAUNCH_BLOCKING", "1")
lib/spack/spack/test/data/unparse/trilinos.txt:        if "+cuda" in spec and "+wrapper" in spec:
lib/spack/spack/test/data/unparse/trilinos.txt:            ("CUDA", "cuda", "cuda"),
lib/spack/spack/test/data/unparse/trilinos.txt:                    define_kok_enable("CUDA"),
lib/spack/spack/test/data/unparse/trilinos.txt:            if "+cuda" in spec:
lib/spack/spack/test/data/unparse/trilinos.txt:                        define_kok_enable("CUDA_UVM", True),
lib/spack/spack/test/data/unparse/trilinos.txt:                        define_kok_enable("CUDA_LAMBDA", True),
lib/spack/spack/test/data/unparse/trilinos.txt:                        define_kok_enable("CUDA_RELOCATABLE_DEVICE_CODE", "cuda_rdc"),
lib/spack/spack/test/data/unparse/trilinos.txt:                arch_map = Kokkos.spack_cuda_arch_map
lib/spack/spack/test/data/unparse/trilinos.txt:                    for arch in spec.variants["cuda_arch"].value
lib/spack/spack/test/data/unparse/trilinos.txt:        if "+cuda" in self.spec:
lib/spack/spack/test/data/unparse/trilinos.txt:            # it relies on blocking CUDA kernel launch.
lib/spack/spack/test/data/unparse/trilinos.txt:            env.set("CUDA_LAUNCH_BLOCKING", "1")
lib/spack/spack/test/data/unparse/mfem.txt:class Mfem(Package, CudaPackage, ROCmPackage):
lib/spack/spack/test/data/unparse/mfem.txt:    # Note: '+cuda' and 'cuda_arch' variants are added by the CudaPackage
lib/spack/spack/test/data/unparse/mfem.txt:    # Note: '+rocm' and 'amdgpu_target' variants are added by the ROCmPackage
lib/spack/spack/test/data/unparse/mfem.txt:    variant('amgx', default=False, description='Enable NVIDIA AmgX solver support')
lib/spack/spack/test/data/unparse/mfem.txt:    conflicts('+cuda', when='@:3')
lib/spack/spack/test/data/unparse/mfem.txt:    conflicts('+rocm', when='@:4.1')
lib/spack/spack/test/data/unparse/mfem.txt:    conflicts('+cuda+rocm')
lib/spack/spack/test/data/unparse/mfem.txt:    conflicts('+amgx', when='~cuda')
lib/spack/spack/test/data/unparse/mfem.txt:    conflicts('+mpi~cuda ^hypre+cuda')
lib/spack/spack/test/data/unparse/mfem.txt:    for sm_ in CudaPackage.cuda_arch_values:
lib/spack/spack/test/data/unparse/mfem.txt:        depends_on('sundials@5.4.0:+cuda cuda_arch={0}'.format(sm_),
lib/spack/spack/test/data/unparse/mfem.txt:                   when='@4.2.0:+sundials+cuda cuda_arch={0}'.format(sm_))
lib/spack/spack/test/data/unparse/mfem.txt:    for sm_ in CudaPackage.cuda_arch_values:
lib/spack/spack/test/data/unparse/mfem.txt:        depends_on('strumpack+cuda cuda_arch={0}'.format(sm_),
lib/spack/spack/test/data/unparse/mfem.txt:                   when='+strumpack+cuda cuda_arch={0}'.format(sm_))
lib/spack/spack/test/data/unparse/mfem.txt:    conflicts('+strumpack ^strumpack+cuda', when='~cuda')
lib/spack/spack/test/data/unparse/mfem.txt:    depends_on('occa+cuda', when='+occa+cuda')
lib/spack/spack/test/data/unparse/mfem.txt:    # TODO: propagate '+rocm' variant to occa when it is supported
lib/spack/spack/test/data/unparse/mfem.txt:    for sm_ in CudaPackage.cuda_arch_values:
lib/spack/spack/test/data/unparse/mfem.txt:        depends_on('raja+cuda cuda_arch={0}'.format(sm_),
lib/spack/spack/test/data/unparse/mfem.txt:                   when='+raja+cuda cuda_arch={0}'.format(sm_))
lib/spack/spack/test/data/unparse/mfem.txt:    for gfx in ROCmPackage.amdgpu_targets:
lib/spack/spack/test/data/unparse/mfem.txt:        depends_on('raja+rocm amdgpu_target={0}'.format(gfx),
lib/spack/spack/test/data/unparse/mfem.txt:                   when='+raja+rocm amdgpu_target={0}'.format(gfx))
lib/spack/spack/test/data/unparse/mfem.txt:    for sm_ in CudaPackage.cuda_arch_values:
lib/spack/spack/test/data/unparse/mfem.txt:        depends_on('libceed+cuda cuda_arch={0}'.format(sm_),
lib/spack/spack/test/data/unparse/mfem.txt:                   when='+libceed+cuda cuda_arch={0}'.format(sm_))
lib/spack/spack/test/data/unparse/mfem.txt:    for gfx in ROCmPackage.amdgpu_targets:
lib/spack/spack/test/data/unparse/mfem.txt:        depends_on('libceed+rocm amdgpu_target={0}'.format(gfx),
lib/spack/spack/test/data/unparse/mfem.txt:                   when='+libceed+rocm amdgpu_target={0}'.format(gfx))
lib/spack/spack/test/data/unparse/mfem.txt:    for sm_ in CudaPackage.cuda_arch_values:
lib/spack/spack/test/data/unparse/mfem.txt:        depends_on('umpire+cuda cuda_arch={0}'.format(sm_),
lib/spack/spack/test/data/unparse/mfem.txt:                   when='+umpire+cuda cuda_arch={0}'.format(sm_))
lib/spack/spack/test/data/unparse/mfem.txt:    for gfx in ROCmPackage.amdgpu_targets:
lib/spack/spack/test/data/unparse/mfem.txt:        depends_on('umpire+rocm amdgpu_target={0}'.format(gfx),
lib/spack/spack/test/data/unparse/mfem.txt:                   when='+umpire+rocm amdgpu_target={0}'.format(gfx))
lib/spack/spack/test/data/unparse/mfem.txt:    # AmgX: propagate the cuda_arch and mpi settings:
lib/spack/spack/test/data/unparse/mfem.txt:    for sm_ in CudaPackage.cuda_arch_values:
lib/spack/spack/test/data/unparse/mfem.txt:        depends_on('amgx+mpi cuda_arch={0}'.format(sm_),
lib/spack/spack/test/data/unparse/mfem.txt:                   when='+amgx+mpi cuda_arch={0}'.format(sm_))
lib/spack/spack/test/data/unparse/mfem.txt:        depends_on('amgx~mpi cuda_arch={0}'.format(sm_),
lib/spack/spack/test/data/unparse/mfem.txt:                   when='+amgx~mpi cuda_arch={0}'.format(sm_))
lib/spack/spack/test/data/unparse/mfem.txt:    patch('mfem-4.3-cusparse-11.4.patch', when='@4.3.0+cuda')
lib/spack/spack/test/data/unparse/mfem.txt:        if '+cuda' in spec:
lib/spack/spack/test/data/unparse/mfem.txt:        cuda_arch = None if '~cuda' in spec else spec.variants['cuda_arch'].value
lib/spack/spack/test/data/unparse/mfem.txt:            'MFEM_USE_CUDA=%s' % yes_no('+cuda'),
lib/spack/spack/test/data/unparse/mfem.txt:            'MFEM_USE_HIP=%s' % yes_no('+rocm'),
lib/spack/spack/test/data/unparse/mfem.txt:            if '+cuda' in spec:
lib/spack/spack/test/data/unparse/mfem.txt:                    '-x=cu --expt-extended-lambda -arch=sm_%s' % cuda_arch,
lib/spack/spack/test/data/unparse/mfem.txt:            if '+cuda' in strumpack:
lib/spack/spack/test/data/unparse/mfem.txt:                # assuming also ('+cuda' in spec)
lib/spack/spack/test/data/unparse/mfem.txt:        if '+cuda' in spec:
lib/spack/spack/test/data/unparse/mfem.txt:                'CUDA_CXX=%s' % join_path(spec['cuda'].prefix, 'bin', 'nvcc'),
lib/spack/spack/test/data/unparse/mfem.txt:                'CUDA_ARCH=sm_%s' % cuda_arch]
lib/spack/spack/test/data/unparse/mfem.txt:        if '+rocm' in spec:
lib/spack/spack/test/data/unparse/mfem.txt:            amdgpu_target = ','.join(spec.variants['amdgpu_target'].value)
lib/spack/spack/test/data/unparse/mfem.txt:                'HIP_ARCH=%s' % amdgpu_target]
lib/spack/spack/test/data/unparse/mfem.txt:        if '+cuda' in spec and '+cuda' in spec['sundials']:
lib/spack/spack/test/data/unparse/mfem.txt:            sun_comps += ',nveccuda'
lib/spack/spack/test/data/unparse/py-torch.txt:class PyTorch(PythonPackage, CudaPackage):
lib/spack/spack/test/data/unparse/py-torch.txt:    with strong GPU acceleration."""
lib/spack/spack/test/data/unparse/py-torch.txt:        submodules_delete=["third_party/nervanagpu"],
lib/spack/spack/test/data/unparse/py-torch.txt:    variant("cuda", default=not is_darwin, description="Use CUDA")
lib/spack/spack/test/data/unparse/py-torch.txt:    variant("rocm", default=False, description="Use ROCm")
lib/spack/spack/test/data/unparse/py-torch.txt:    variant("nccl", default=not is_darwin, description="Use NCCL")
lib/spack/spack/test/data/unparse/py-torch.txt:    conflicts("+cuda", when="+rocm")
lib/spack/spack/test/data/unparse/py-torch.txt:    conflicts("+cudnn", when="~cuda")
lib/spack/spack/test/data/unparse/py-torch.txt:    conflicts("+magma", when="~cuda")
lib/spack/spack/test/data/unparse/py-torch.txt:    conflicts("+nccl", when="~cuda~rocm")
lib/spack/spack/test/data/unparse/py-torch.txt:    conflicts("+nccl", when="platform=darwin")
lib/spack/spack/test/data/unparse/py-torch.txt:    conflicts("+rocm", when="@:0.4")
lib/spack/spack/test/data/unparse/py-torch.txt:        "cuda_arch=none",
lib/spack/spack/test/data/unparse/py-torch.txt:        when="+cuda",
lib/spack/spack/test/data/unparse/py-torch.txt:        msg="Must specify CUDA compute capabilities of your GPU, see "
lib/spack/spack/test/data/unparse/py-torch.txt:        "https://developer.nvidia.com/cuda-gpus",
lib/spack/spack/test/data/unparse/py-torch.txt:    depends_on("cuda@7.5:", when="+cuda", type=("build", "link", "run"))
lib/spack/spack/test/data/unparse/py-torch.txt:    depends_on("cuda@9:", when="@1.1:+cuda", type=("build", "link", "run"))
lib/spack/spack/test/data/unparse/py-torch.txt:    depends_on("cuda@9.2:", when="@1.6:+cuda", type=("build", "link", "run"))
lib/spack/spack/test/data/unparse/py-torch.txt:    depends_on("nccl", when="+nccl")
lib/spack/spack/test/data/unparse/py-torch.txt:    # Fixes build error when ROCm is enabled for pytorch-1.5 release
lib/spack/spack/test/data/unparse/py-torch.txt:    patch("rocm.patch", when="@1.5.0:1.5+rocm")
lib/spack/spack/test/data/unparse/py-torch.txt:    patch("cusparseGetErrorString.patch", when="@0.4.1:1.0^cuda@10.1.243:")
lib/spack/spack/test/data/unparse/py-torch.txt:        enable_or_disable("cuda")
lib/spack/spack/test/data/unparse/py-torch.txt:        if "+cuda" in self.spec:
lib/spack/spack/test/data/unparse/py-torch.txt:            # cmake/public/cuda.cmake
lib/spack/spack/test/data/unparse/py-torch.txt:            # cmake/Modules_CUDA_fix/upstream/FindCUDA.cmake
lib/spack/spack/test/data/unparse/py-torch.txt:            env.unset("CUDA_ROOT")
lib/spack/spack/test/data/unparse/py-torch.txt:            torch_cuda_arch = ";".join(
lib/spack/spack/test/data/unparse/py-torch.txt:                "{0:.1f}".format(float(i) / 10.0) for i in self.spec.variants["cuda_arch"].value
lib/spack/spack/test/data/unparse/py-torch.txt:            env.set("TORCH_CUDA_ARCH_LIST", torch_cuda_arch)
lib/spack/spack/test/data/unparse/py-torch.txt:        enable_or_disable("rocm")
lib/spack/spack/test/data/unparse/py-torch.txt:            # cmake/Modules_CUDA_fix/FindCUDNN.cmake
lib/spack/spack/test/data/unparse/py-torch.txt:        enable_or_disable("nccl")
lib/spack/spack/test/data/unparse/py-torch.txt:        if "+nccl" in self.spec:
lib/spack/spack/test/data/unparse/py-torch.txt:            env.set("NCCL_LIB_DIR", self.spec["nccl"].libs.directories[0])
lib/spack/spack/test/data/unparse/py-torch.txt:            env.set("NCCL_INCLUDE_DIR", self.spec["nccl"].prefix.include)
lib/spack/spack/test/data/unparse/py-torch.txt:        env.set("USE_SYSTEM_NCCL", "ON")
lib/spack/spack/test/data/unparse/py-torch.txt:        if "+rocm" in self.spec:
lib/spack/spack/test/data/unparse/legion.txt:    depends_on('cuda@10.0:11.9', when='+cuda_unsupported_compiler')
lib/spack/spack/test/data/unparse/legion.txt:    depends_on('cuda@10.0:11.9', when='+cuda')
lib/spack/spack/test/data/unparse/legion.txt:    # cuda-centric
lib/spack/spack/test/data/unparse/legion.txt:    cuda_arch_list = ('60', '70', '75', '80')
lib/spack/spack/test/data/unparse/legion.txt:    for nvarch in cuda_arch_list:
lib/spack/spack/test/data/unparse/legion.txt:        depends_on('kokkos@3.3.01+cuda+cuda_lambda+wrapper cuda_arch={0}'.format(nvarch),
lib/spack/spack/test/data/unparse/legion.txt:                   when='%gcc+kokkos+cuda cuda_arch={0}'.format(nvarch))
lib/spack/spack/test/data/unparse/legion.txt:        depends_on("kokkos@3.3.01+cuda+cuda_lambda~wrapper cuda_arch={0}".format(nvarch),
lib/spack/spack/test/data/unparse/legion.txt:                   when="%clang+kokkos+cuda cuda_arch={0}".format(nvarch))
lib/spack/spack/test/data/unparse/legion.txt:    depends_on('kokkos@3.3.01~cuda', when='+kokkos~cuda')
lib/spack/spack/test/data/unparse/legion.txt:    depends_on("kokkos@3.3.01~cuda+openmp", when='+kokkos+openmp')
lib/spack/spack/test/data/unparse/legion.txt:    # note: we will be dependent upon spack's latest-and-greatest cuda version...
lib/spack/spack/test/data/unparse/legion.txt:    variant('cuda', default=False,
lib/spack/spack/test/data/unparse/legion.txt:            description="Enable CUDA support.")
lib/spack/spack/test/data/unparse/legion.txt:    variant('cuda_hijack', default=False,
lib/spack/spack/test/data/unparse/legion.txt:            description="Hijack application calls into the CUDA runtime (+cuda).")
lib/spack/spack/test/data/unparse/legion.txt:    variant('cuda_arch', default='70',
lib/spack/spack/test/data/unparse/legion.txt:            values=cuda_arch_list,
lib/spack/spack/test/data/unparse/legion.txt:            description="GPU/CUDA architecture to build for.",
lib/spack/spack/test/data/unparse/legion.txt:    variant('cuda_unsupported_compiler', default=False,
lib/spack/spack/test/data/unparse/legion.txt:    conflicts('+cuda_hijack', when='~cuda')
lib/spack/spack/test/data/unparse/legion.txt:        if '+cuda' in spec:
lib/spack/spack/test/data/unparse/legion.txt:            cuda_arch = spec.variants['cuda_arch'].value
lib/spack/spack/test/data/unparse/legion.txt:            options.append('-DLegion_USE_CUDA=ON')
lib/spack/spack/test/data/unparse/legion.txt:            options.append('-DLegion_GPU_REDUCTIONS=ON')
lib/spack/spack/test/data/unparse/legion.txt:            options.append('-DLegion_CUDA_ARCH=%s' % cuda_arch)
lib/spack/spack/test/data/unparse/legion.txt:            if '+cuda_hijack' in spec:
lib/spack/spack/test/data/unparse/legion.txt:                options.append('-DLegion_HIJACK_CUDART=ON')
lib/spack/spack/test/data/unparse/legion.txt:                options.append('-DLegion_HIJACK_CUDART=OFF')
lib/spack/spack/test/data/unparse/legion.txt:            if '+cuda_unsupported_compiler' in spec:
lib/spack/spack/test/data/unparse/legion.txt:                options.append('-DCUDA_NVCC_FLAGS:STRING=--allow-unsupported-compiler')
lib/spack/spack/test/data/unparse/llvm.txt:class Llvm(CMakePackage, CudaPackage):
lib/spack/spack/test/data/unparse/llvm.txt:            "amdgpu",
lib/spack/spack/test/data/unparse/llvm.txt:    depends_on("libelf", when="+cuda")  # libomptarget
lib/spack/spack/test/data/unparse/llvm.txt:    depends_on("libffi", when="+cuda")  # libomptarget
lib/spack/spack/test/data/unparse/llvm.txt:    # cuda_arch value must be specified
lib/spack/spack/test/data/unparse/llvm.txt:    conflicts("cuda_arch=none", when="+cuda", msg="A value for cuda_arch must be specified.")
lib/spack/spack/test/data/unparse/llvm.txt:            if any(x in exe for x in ("vscode", "cpp", "-cl", "-gpu")):
lib/spack/spack/test/data/unparse/llvm.txt:        if "+cuda" in spec:
lib/spack/spack/test/data/unparse/llvm.txt:                    define("CUDA_TOOLKIT_ROOT_DIR", spec["cuda"].prefix),
lib/spack/spack/test/data/unparse/llvm.txt:                        ",".join(spec.variants["cuda_arch"].value),
lib/spack/spack/test/data/unparse/llvm.txt:                        "sm_{0}".format(spec.variants["cuda_arch"].value[-1]),
lib/spack/spack/test/data/unparse/llvm.txt:            # still build libomptarget but disable cuda
lib/spack/spack/test/data/unparse/llvm.txt:                    define("CUDA_TOOLKIT_ROOT_DIR", "IGNORE"),
lib/spack/spack/test/data/unparse/llvm.txt:                    define("CUDA_SDK_ROOT_DIR", "IGNORE"),
lib/spack/spack/test/data/unparse/llvm.txt:                    define("CUDA_NVCC_EXECUTABLE", "IGNORE"),
lib/spack/spack/test/data/unparse/llvm.txt:                    define("LIBOMPTARGET_DEP_CUDA_DRIVER_LIBRARIES", "IGNORE"),
lib/spack/spack/test/data/unparse/llvm.txt:        if "+cuda ~omp_as_runtime" in self.spec:
lib/spack/spack/test/data/unparse/llvm.txt:        "amdgpu": "AMDGPU",
lib/spack/spack/test/data/compiler_verbose_output/gcc-7.3.1.txt:Configured with: ../configure --enable-bootstrap --enable-languages=c,c++,objc,obj-c++,fortran,ada,go,lto --prefix=/usr --mandir=/usr/share/man --infodir=/usr/share/info --with-bugurl=http://bugzilla.redhat.com/bugzilla --enable-shared --enable-threads=posix --enable-checking=release --enable-multilib --with-system-zlib --enable-__cxa_atexit --disable-libunwind-exceptions --enable-gnu-unique-object --enable-linker-build-id --with-gcc-major-version-only --with-linker-hash-style=gnu --enable-plugin --enable-initfini-array --with-isl --enable-libmpx --enable-offload-targets=nvptx-none --without-cuda-driver --enable-gnu-indirect-function --with-tune=generic --with-arch_32=i686 --build=x86_64-redhat-linux
lib/spack/spack/test/cray_manifest.py:    nvidia_compiler = JsonCompilerEntry(
lib/spack/spack/test/cray_manifest.py:        name="nvidia",
lib/spack/spack/test/cray_manifest.py:    compiler = compiler_from_entry(nvidia_compiler.compiler_json(), "/example/file")
lib/spack/spack/test/cray_manifest.py:        compiler=nvidia_compiler.spec_json(),
lib/spack/spack/test/concretize_requirements.py:            "forward-multi-value +cuda cuda_arch=10 ^dependency-mv~cuda",
lib/spack/spack/test/concretize_requirements.py:            ["cuda_arch=10", "^dependency-mv~cuda"],
lib/spack/spack/test/concretize_requirements.py:            ["cuda_arch=11", "^dependency-mv cuda_arch=10", "^dependency-mv cuda_arch=11"],
lib/spack/spack/test/concretize_requirements.py:            "forward-multi-value +cuda cuda_arch=10 ^dependency-mv+cuda",
lib/spack/spack/test/concretize_requirements.py:            ["cuda_arch=10", "^dependency-mv cuda_arch=10"],
lib/spack/spack/test/concretize_requirements.py:            ["cuda_arch=11", "^dependency-mv cuda_arch=11"],
lib/spack/spack/test/concretize_requirements.py:            "forward-multi-value +cuda cuda_arch=11 ^dependency-mv+cuda",
lib/spack/spack/test/concretize_requirements.py:            ["cuda_arch=11", "^dependency-mv cuda_arch=11"],
lib/spack/spack/test/concretize_requirements.py:            ["cuda_arch=10", "^dependency-mv cuda_arch=10"],
lib/spack/spack/test/concretize_requirements.py:            "forward-multi-value +cuda cuda_arch=10,11 ^dependency-mv+cuda",
lib/spack/spack/test/concretize_requirements.py:            ["cuda_arch=10,11", "^dependency-mv cuda_arch=10,11"],
lib/spack/spack/test/build_systems.py:    def test_cmake_std_args_cuda(self, default_mock_concretization):
lib/spack/spack/test/build_systems.py:        s = default_mock_concretization("vtk-m +cuda cuda_arch=70 ^cmake@3.23")
lib/spack/spack/test/build_systems.py:        option = spack.build_systems.cmake.CMakeBuilder.define_cuda_architectures(s.package)
lib/spack/spack/test/build_systems.py:        assert "-DCMAKE_CUDA_ARCHITECTURES:STRING=70" == option
lib/spack/spack/test/build_systems.py:        s = default_mock_concretization("vtk-m +rocm amdgpu_target=gfx900 ^cmake@3.23")
lib/spack/spack/test/cmd/compiler.py:    """Ensure that we'll pick 'clang' over 'clang-gpu' when there is a choice."""
lib/spack/spack/test/cmd/compiler.py:    shutil.copy(clang_path, clang_path.parent / "clang-gpu")
lib/spack/spack/test/cmd/compiler.py:    shutil.copy(clang_path, clang_path.parent / "clang++-gpu")
lib/spack/spack/test/llnl/url.py:        ("cuda_8.0.44_linux.run", "cuda_8.0.44"),
lib/spack/spack/test/llnl/url.py:        ("cuda_6.5.14_linux_64.run", "cuda_6.5.14"),
lib/spack/spack/test/build_environment.py:    cuda_headers = HeaderList(
lib/spack/spack/test/build_environment.py:            "prefix/include/cuda_runtime.h",
lib/spack/spack/test/build_environment.py:            "prefix/include/cuda/atomic",
lib/spack/spack/test/build_environment.py:            "prefix/include/cuda/std/detail/libcxx/include/ctype.h",
lib/spack/spack/test/build_environment.py:    cuda_include_dirs = cuda_headers.directories
lib/spack/spack/test/build_environment.py:    assert posixpath.join("prefix", "include") in cuda_include_dirs
lib/spack/spack/test/build_environment.py:        posixpath.join("prefix", "include", "cuda", "std", "detail", "libcxx", "include")
lib/spack/spack/test/build_environment.py:        not in cuda_include_dirs
lib/spack/spack/cray_manifest.py:compiler_name_translation = {"nvidia": "nvhpc", "rocm": "rocmcc"}
lib/spack/spack/build_systems/rocm.py:# Troubleshooting advice for +rocm builds:
lib/spack/spack/build_systems/rocm.py:#          cc: /opt/rocm/llvm/bin/clang
lib/spack/spack/build_systems/rocm.py:#          cxx: /opt/rocm/llvm/bin/clang++
lib/spack/spack/build_systems/rocm.py:#    It is advisable to replace /rocm/ in the paths above with /rocm-version/
lib/spack/spack/build_systems/rocm.py:#        prefix: /opt/rocm/hip
lib/spack/spack/build_systems/rocm.py:#            c: /opt/rocm/llvm/bin/clang++
lib/spack/spack/build_systems/rocm.py:#            c++: /opt/rocm/llvm/bin/clang++
lib/spack/spack/build_systems/rocm.py:#            hip: /opt/rocm/hip/bin/hipcc
lib/spack/spack/build_systems/rocm.py:#        prefix: /opt/rocm
lib/spack/spack/build_systems/rocm.py:#            c: /opt/rocm/llvm/bin/clang++
lib/spack/spack/build_systems/rocm.py:#            cxx: /opt/rocm/llvm/bin/clang++
lib/spack/spack/build_systems/rocm.py:#    llvm-amdgpu:
lib/spack/spack/build_systems/rocm.py:#      - spec: llvm-amdgpu
lib/spack/spack/build_systems/rocm.py:#        prefix: /opt/rocm/llvm
lib/spack/spack/build_systems/rocm.py:#            c: /opt/rocm/llvm/bin/clang++
lib/spack/spack/build_systems/rocm.py:#            cxx: /opt/rocm/llvm/bin/clang++
lib/spack/spack/build_systems/rocm.py:#    It is advisable to replace /rocm/ in the paths above with /rocm-version/
lib/spack/spack/build_systems/rocm.py:# 3. In part 2, DO NOT list the path to hsa as /opt/rocm/hsa ! You want spack
lib/spack/spack/build_systems/rocm.py:#    to find hsa in /opt/rocm/include/hsa/hsa.h . The directory of
lib/spack/spack/build_systems/rocm.py:#    /opt/rocm/hsa also has an hsa.h file, but it won't be found because spack
lib/spack/spack/build_systems/rocm.py:class ROCmPackage(PackageBase):
lib/spack/spack/build_systems/rocm.py:    """Auxiliary class which contains ROCm variant, dependencies and conflicts
lib/spack/spack/build_systems/rocm.py:    and is meant to unify and facilitate its usage. Closely mimics CudaPackage.
lib/spack/spack/build_systems/rocm.py:    # https://llvm.org/docs/AMDGPUUsage.html
lib/spack/spack/build_systems/rocm.py:    amdgpu_targets = (
lib/spack/spack/build_systems/rocm.py:    variant("rocm", default=False, description="Enable ROCm support")
lib/spack/spack/build_systems/rocm.py:    # possible amd gpu targets for rocm builds
lib/spack/spack/build_systems/rocm.py:        "amdgpu_target",
lib/spack/spack/build_systems/rocm.py:        description="AMD GPU architecture",
lib/spack/spack/build_systems/rocm.py:        values=spack.variant.any_combination_of(*amdgpu_targets),
lib/spack/spack/build_systems/rocm.py:        when="+rocm",
lib/spack/spack/build_systems/rocm.py:    depends_on("llvm-amdgpu", when="+rocm")
lib/spack/spack/build_systems/rocm.py:    depends_on("hsa-rocr-dev", when="+rocm")
lib/spack/spack/build_systems/rocm.py:    depends_on("hip +rocm", when="+rocm")
lib/spack/spack/build_systems/rocm.py:    # need amd gpu type for rocm builds
lib/spack/spack/build_systems/rocm.py:    conflicts("amdgpu_target=none", when="+rocm")
lib/spack/spack/build_systems/rocm.py:    # https://github.com/ROCm-Developer-Tools/HIP/blob/master/bin/hipcc
lib/spack/spack/build_systems/rocm.py:    # we will still need to set the HCC_AMDGPU_TARGET environment flag in the
lib/spack/spack/build_systems/rocm.py:    def hip_flags(amdgpu_target):
lib/spack/spack/build_systems/rocm.py:        archs = ",".join(amdgpu_target)
lib/spack/spack/build_systems/rocm.py:        return "--amdgpu-target={0}".format(archs)
lib/spack/spack/build_systems/rocm.py:        llvm_path = self.spec["llvm-amdgpu"].prefix
lib/spack/spack/build_systems/rocm.py:    # depends_on('hip@:6.0', when='amdgpu_target=gfx701')
lib/spack/spack/build_systems/rocm.py:    depends_on("llvm-amdgpu@5.2.0:", when="amdgpu_target=gfx940")
lib/spack/spack/build_systems/rocm.py:    depends_on("llvm-amdgpu@5.7.0:", when="amdgpu_target=gfx941")
lib/spack/spack/build_systems/rocm.py:    depends_on("llvm-amdgpu@5.7.0:", when="amdgpu_target=gfx942")
lib/spack/spack/build_systems/rocm.py:    depends_on("llvm-amdgpu@5.2.0:", when="amdgpu_target=gfx1036")
lib/spack/spack/build_systems/rocm.py:    depends_on("llvm-amdgpu@5.3.0:", when="amdgpu_target=gfx1100")
lib/spack/spack/build_systems/rocm.py:    depends_on("llvm-amdgpu@5.3.0:", when="amdgpu_target=gfx1101")
lib/spack/spack/build_systems/rocm.py:    depends_on("llvm-amdgpu@5.3.0:", when="amdgpu_target=gfx1102")
lib/spack/spack/build_systems/rocm.py:    depends_on("llvm-amdgpu@5.3.0:", when="amdgpu_target=gfx1103")
lib/spack/spack/build_systems/rocm.py:    # conflicts('%gcc@5:', when='+cuda ^cuda@:7.5' + arch_platform)
lib/spack/spack/build_systems/rocm.py:    # conflicts('platform=darwin', when='+cuda ^cuda@11.0.2:')
lib/spack/spack/build_systems/cmake.py:    def define_cuda_architectures(pkg):
lib/spack/spack/build_systems/cmake.py:        """Returns the str ``-DCMAKE_CUDA_ARCHITECTURES:STRING=(expanded cuda_arch)``.
lib/spack/spack/build_systems/cmake.py:        ``cuda_arch`` is variant composed of a list of target CUDA architectures and
lib/spack/spack/build_systems/cmake.py:        it is declared in the cuda package.
lib/spack/spack/build_systems/cmake.py:        This method is no-op for cmake<3.18 and when ``cuda_arch`` variant is not set.
lib/spack/spack/build_systems/cmake.py:        if "cuda_arch" in pkg.spec.variants and pkg.spec.satisfies("^cmake@3.18:"):
lib/spack/spack/build_systems/cmake.py:                "CMAKE_CUDA_ARCHITECTURES", pkg.spec.variants["cuda_arch"].value
lib/spack/spack/build_systems/cmake.py:        """Returns the str ``-DCMAKE_HIP_ARCHITECTURES:STRING=(expanded amdgpu_target)``.
lib/spack/spack/build_systems/cmake.py:        ``amdgpu_target`` is variant composed of a list of the target HIP
lib/spack/spack/build_systems/cmake.py:        architectures and it is declared in the rocm package.
lib/spack/spack/build_systems/cmake.py:        This method is no-op for cmake<3.18 and when ``amdgpu_target`` variant is
lib/spack/spack/build_systems/cmake.py:        if "amdgpu_target" in pkg.spec.variants and pkg.spec.satisfies("^cmake@3.21:"):
lib/spack/spack/build_systems/cmake.py:                "CMAKE_HIP_ARCHITECTURES", pkg.spec.variants["amdgpu_target"].value
lib/spack/spack/build_systems/cuda.py:class CudaPackage(PackageBase):
lib/spack/spack/build_systems/cuda.py:    """Auxiliary class which contains CUDA variant, dependencies and conflicts
lib/spack/spack/build_systems/cuda.py:    # https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html#gpu-feature-list
lib/spack/spack/build_systems/cuda.py:    # https://developer.nvidia.com/cuda-gpus
lib/spack/spack/build_systems/cuda.py:    # https://en.wikipedia.org/wiki/CUDA#GPUs_supported
lib/spack/spack/build_systems/cuda.py:    cuda_arch_values = (
lib/spack/spack/build_systems/cuda.py:    # FIXME: keep cuda and cuda_arch separate to make usage easier until
lib/spack/spack/build_systems/cuda.py:    # Spack has depends_on(cuda, when='cuda_arch!=None') or alike
lib/spack/spack/build_systems/cuda.py:    variant("cuda", default=False, description="Build with CUDA")
lib/spack/spack/build_systems/cuda.py:        "cuda_arch",
lib/spack/spack/build_systems/cuda.py:        description="CUDA architecture",
lib/spack/spack/build_systems/cuda.py:        values=spack.variant.any_combination_of(*cuda_arch_values),
lib/spack/spack/build_systems/cuda.py:        when="+cuda",
lib/spack/spack/build_systems/cuda.py:    # https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html#nvcc-examples
lib/spack/spack/build_systems/cuda.py:    # https://llvm.org/docs/CompileCudaWithLLVM.html#compiling-cuda-code
lib/spack/spack/build_systems/cuda.py:    def cuda_flags(arch_list):
lib/spack/spack/build_systems/cuda.py:        """Adds a decimal place to each CUDA arch.
lib/spack/spack/build_systems/cuda.py:    depends_on("cuda", when="+cuda")
lib/spack/spack/build_systems/cuda.py:    # CUDA version vs Architecture
lib/spack/spack/build_systems/cuda.py:    # https://en.wikipedia.org/wiki/CUDA#GPUs_supported
lib/spack/spack/build_systems/cuda.py:    # https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#deprecated-features
lib/spack/spack/build_systems/cuda.py:    depends_on("cuda@:6.0", when="cuda_arch=10")
lib/spack/spack/build_systems/cuda.py:    depends_on("cuda@:6.5", when="cuda_arch=11")
lib/spack/spack/build_systems/cuda.py:    depends_on("cuda@2.1:6.5", when="cuda_arch=12")
lib/spack/spack/build_systems/cuda.py:    depends_on("cuda@2.1:6.5", when="cuda_arch=13")
lib/spack/spack/build_systems/cuda.py:    depends_on("cuda@3.0:8.0", when="cuda_arch=20")
lib/spack/spack/build_systems/cuda.py:    depends_on("cuda@3.2:8.0", when="cuda_arch=21")
lib/spack/spack/build_systems/cuda.py:    depends_on("cuda@5.0:10.2", when="cuda_arch=30")
lib/spack/spack/build_systems/cuda.py:    depends_on("cuda@5.0:10.2", when="cuda_arch=32")
lib/spack/spack/build_systems/cuda.py:    depends_on("cuda@5.0:11.8", when="cuda_arch=35")
lib/spack/spack/build_systems/cuda.py:    depends_on("cuda@6.5:11.8", when="cuda_arch=37")
lib/spack/spack/build_systems/cuda.py:    depends_on("cuda@6.0:", when="cuda_arch=50")
lib/spack/spack/build_systems/cuda.py:    depends_on("cuda@6.5:", when="cuda_arch=52")
lib/spack/spack/build_systems/cuda.py:    depends_on("cuda@6.5:", when="cuda_arch=53")
lib/spack/spack/build_systems/cuda.py:    depends_on("cuda@8.0:", when="cuda_arch=60")
lib/spack/spack/build_systems/cuda.py:    depends_on("cuda@8.0:", when="cuda_arch=61")
lib/spack/spack/build_systems/cuda.py:    depends_on("cuda@8.0:", when="cuda_arch=62")
lib/spack/spack/build_systems/cuda.py:    depends_on("cuda@9.0:", when="cuda_arch=70")
lib/spack/spack/build_systems/cuda.py:    depends_on("cuda@9.0:", when="cuda_arch=72")
lib/spack/spack/build_systems/cuda.py:    depends_on("cuda@10.0:", when="cuda_arch=75")
lib/spack/spack/build_systems/cuda.py:    depends_on("cuda@11.0:", when="cuda_arch=80")
lib/spack/spack/build_systems/cuda.py:    depends_on("cuda@11.1:", when="cuda_arch=86")
lib/spack/spack/build_systems/cuda.py:    depends_on("cuda@11.4:", when="cuda_arch=87")
lib/spack/spack/build_systems/cuda.py:    depends_on("cuda@11.8:", when="cuda_arch=89")
lib/spack/spack/build_systems/cuda.py:    depends_on("cuda@12.0:", when="cuda_arch=90")
lib/spack/spack/build_systems/cuda.py:    depends_on("cuda@12.0:", when="cuda_arch=90a")
lib/spack/spack/build_systems/cuda.py:    # From the NVIDIA install guide we know of conflicts for particular
lib/spack/spack/build_systems/cuda.py:    with when("^cuda~allow-unsupported-compilers"):
lib/spack/spack/build_systems/cuda.py:        conflicts("%gcc@:4", when="+cuda ^cuda@11.0:")
lib/spack/spack/build_systems/cuda.py:        conflicts("%gcc@:5", when="+cuda ^cuda@11.4:")
lib/spack/spack/build_systems/cuda.py:        conflicts("%clang@:6", when="+cuda ^cuda@12.2:")
lib/spack/spack/build_systems/cuda.py:        # in order to not constrain future cuda version to old gcc versions,
lib/spack/spack/build_systems/cuda.py:        # each release of a new cuda minor version.
lib/spack/spack/build_systems/cuda.py:        conflicts("%gcc@10:", when="+cuda ^cuda@:11.0")
lib/spack/spack/build_systems/cuda.py:        conflicts("%gcc@11:", when="+cuda ^cuda@:11.4.0")
lib/spack/spack/build_systems/cuda.py:        conflicts("%gcc@11.2:", when="+cuda ^cuda@:11.5")
lib/spack/spack/build_systems/cuda.py:        conflicts("%gcc@12:", when="+cuda ^cuda@:11.8")
lib/spack/spack/build_systems/cuda.py:        conflicts("%gcc@13:", when="+cuda ^cuda@:12.3")
lib/spack/spack/build_systems/cuda.py:        conflicts("%gcc@14:", when="+cuda ^cuda@:12.6")
lib/spack/spack/build_systems/cuda.py:        conflicts("%clang@12:", when="+cuda ^cuda@:11.4.0")
lib/spack/spack/build_systems/cuda.py:        conflicts("%clang@13:", when="+cuda ^cuda@:11.5")
lib/spack/spack/build_systems/cuda.py:        conflicts("%clang@14:", when="+cuda ^cuda@:11.7")
lib/spack/spack/build_systems/cuda.py:        conflicts("%clang@15:", when="+cuda ^cuda@:12.0")
lib/spack/spack/build_systems/cuda.py:        conflicts("%clang@16:", when="+cuda ^cuda@:12.1")
lib/spack/spack/build_systems/cuda.py:        conflicts("%clang@17:", when="+cuda ^cuda@:12.3")
lib/spack/spack/build_systems/cuda.py:        conflicts("%clang@18:", when="+cuda ^cuda@:12.5")
lib/spack/spack/build_systems/cuda.py:        conflicts("%clang@19:", when="+cuda ^cuda@:12.6")
lib/spack/spack/build_systems/cuda.py:        conflicts("%gcc@10", when="+cuda ^cuda@:11.4.0")
lib/spack/spack/build_systems/cuda.py:        conflicts("%gcc@5:", when="+cuda ^cuda@:7.5 target=x86_64:")
lib/spack/spack/build_systems/cuda.py:        conflicts("%gcc@6:", when="+cuda ^cuda@:8 target=x86_64:")
lib/spack/spack/build_systems/cuda.py:        conflicts("%gcc@7:", when="+cuda ^cuda@:9.1 target=x86_64:")
lib/spack/spack/build_systems/cuda.py:        conflicts("%gcc@8:", when="+cuda ^cuda@:10.0.130 target=x86_64:")
lib/spack/spack/build_systems/cuda.py:        conflicts("%gcc@9:", when="+cuda ^cuda@:10.2.89 target=x86_64:")
lib/spack/spack/build_systems/cuda.py:        conflicts("%pgi@:14.8", when="+cuda ^cuda@:7.0.27 target=x86_64:")
lib/spack/spack/build_systems/cuda.py:        conflicts("%pgi@:15.3,15.5:", when="+cuda ^cuda@7.5 target=x86_64:")
lib/spack/spack/build_systems/cuda.py:        conflicts("%pgi@:16.2,16.0:16.3", when="+cuda ^cuda@8 target=x86_64:")
lib/spack/spack/build_systems/cuda.py:        conflicts("%pgi@:15,18:", when="+cuda ^cuda@9.0:9.1 target=x86_64:")
lib/spack/spack/build_systems/cuda.py:        conflicts("%pgi@:16,19:", when="+cuda ^cuda@9.2.88:10.0 target=x86_64:")
lib/spack/spack/build_systems/cuda.py:        conflicts("%pgi@:17,20:", when="+cuda ^cuda@10.1.105:10.2.89 target=x86_64:")
lib/spack/spack/build_systems/cuda.py:        conflicts("%pgi@:17,21:", when="+cuda ^cuda@11.0.2:11.1.0 target=x86_64:")
lib/spack/spack/build_systems/cuda.py:        conflicts("%clang@:3.4", when="+cuda ^cuda@:7.5 target=x86_64:")
lib/spack/spack/build_systems/cuda.py:        conflicts("%clang@:3.7,4:", when="+cuda ^cuda@8.0:9.0 target=x86_64:")
lib/spack/spack/build_systems/cuda.py:        conflicts("%clang@:3.7,4.1:", when="+cuda ^cuda@9.1 target=x86_64:")
lib/spack/spack/build_systems/cuda.py:        conflicts("%clang@:3.7,5.1:", when="+cuda ^cuda@9.2 target=x86_64:")
lib/spack/spack/build_systems/cuda.py:        conflicts("%clang@:3.7,6.1:", when="+cuda ^cuda@10.0.130 target=x86_64:")
lib/spack/spack/build_systems/cuda.py:        conflicts("%clang@:3.7,7.1:", when="+cuda ^cuda@10.1.105 target=x86_64:")
lib/spack/spack/build_systems/cuda.py:        conflicts("%clang@:3.7,8.1:", when="+cuda ^cuda@10.1.105:10.1.243 target=x86_64:")
lib/spack/spack/build_systems/cuda.py:        conflicts("%clang@:3.2,9:", when="+cuda ^cuda@10.2.89 target=x86_64:")
lib/spack/spack/build_systems/cuda.py:        conflicts("%clang@:5", when="+cuda ^cuda@11.0.2: target=x86_64:")
lib/spack/spack/build_systems/cuda.py:        conflicts("%clang@10:", when="+cuda ^cuda@:11.0.3 target=x86_64:")
lib/spack/spack/build_systems/cuda.py:        conflicts("%clang@11:", when="+cuda ^cuda@:11.1.0 target=x86_64:")
lib/spack/spack/build_systems/cuda.py:        # x86_64 vs. ppc64le differ according to NVidia docs
lib/spack/spack/build_systems/cuda.py:        # https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html
lib/spack/spack/build_systems/cuda.py:        # https://docs.nvidia.com/cuda/archive/9.2/cuda-installation-guide-linux/index.html
lib/spack/spack/build_systems/cuda.py:        # https://docs.nvidia.com/cuda/archive/9.1/cuda-installation-guide-linux/index.html
lib/spack/spack/build_systems/cuda.py:        # https://docs.nvidia.com/cuda/archive/9.0/cuda-installation-guide-linux/index.html
lib/spack/spack/build_systems/cuda.py:        # https://docs.nvidia.com/cuda/archive/8.0/cuda-installation-guide-linux/index.html
lib/spack/spack/build_systems/cuda.py:        # information prior to CUDA 9 difficult to find
lib/spack/spack/build_systems/cuda.py:        conflicts("%gcc@6:", when="+cuda ^cuda@:9 target=ppc64le:")
lib/spack/spack/build_systems/cuda.py:        conflicts("%gcc@8:", when="+cuda ^cuda@:10.0.130 target=ppc64le:")
lib/spack/spack/build_systems/cuda.py:        conflicts("%gcc@9:", when="+cuda ^cuda@:10.1.243 target=ppc64le:")
lib/spack/spack/build_systems/cuda.py:        # officially, CUDA 11.0.2 only supports the system GCC 8.3 on ppc64le
lib/spack/spack/build_systems/cuda.py:        conflicts("%pgi", when="+cuda ^cuda@:8 target=ppc64le:")
lib/spack/spack/build_systems/cuda.py:        conflicts("%pgi@:16", when="+cuda ^cuda@:9.1.185 target=ppc64le:")
lib/spack/spack/build_systems/cuda.py:        conflicts("%pgi@:17", when="+cuda ^cuda@:10 target=ppc64le:")
lib/spack/spack/build_systems/cuda.py:        conflicts("%clang@4:", when="+cuda ^cuda@:9.0.176 target=ppc64le:")
lib/spack/spack/build_systems/cuda.py:        conflicts("%clang@5:", when="+cuda ^cuda@:9.1 target=ppc64le:")
lib/spack/spack/build_systems/cuda.py:        conflicts("%clang@6:", when="+cuda ^cuda@:9.2 target=ppc64le:")
lib/spack/spack/build_systems/cuda.py:        conflicts("%clang@7:", when="+cuda ^cuda@10.0.130 target=ppc64le:")
lib/spack/spack/build_systems/cuda.py:        conflicts("%clang@7.1:", when="+cuda ^cuda@:10.1.105 target=ppc64le:")
lib/spack/spack/build_systems/cuda.py:        conflicts("%clang@8.1:", when="+cuda ^cuda@:10.2.89 target=ppc64le:")
lib/spack/spack/build_systems/cuda.py:        conflicts("%clang@:5", when="+cuda ^cuda@11.0.2: target=ppc64le:")
lib/spack/spack/build_systems/cuda.py:        conflicts("%clang@10:", when="+cuda ^cuda@:11.0.2 target=ppc64le:")
lib/spack/spack/build_systems/cuda.py:        conflicts("%clang@11:", when="+cuda ^cuda@:11.1.0 target=ppc64le:")
lib/spack/spack/build_systems/cuda.py:        # exists for Mac OS X. No information prior to CUDA 3.2 or Intel 11.1
lib/spack/spack/build_systems/cuda.py:        conflicts("%intel@:11.0", when="+cuda ^cuda@:3.1")
lib/spack/spack/build_systems/cuda.py:        conflicts("%intel@:12.0", when="+cuda ^cuda@5.5:")
lib/spack/spack/build_systems/cuda.py:        conflicts("%intel@:13.0", when="+cuda ^cuda@6.0:")
lib/spack/spack/build_systems/cuda.py:        conflicts("%intel@:13.2", when="+cuda ^cuda@6.5:")
lib/spack/spack/build_systems/cuda.py:        conflicts("%intel@:14.9", when="+cuda ^cuda@7:")
lib/spack/spack/build_systems/cuda.py:        # Intel 15.x is compatible with CUDA 7 thru current CUDA
lib/spack/spack/build_systems/cuda.py:        conflicts("%intel@16.0:", when="+cuda ^cuda@:8.0.43")
lib/spack/spack/build_systems/cuda.py:        conflicts("%intel@17.0:", when="+cuda ^cuda@:8.0.60")
lib/spack/spack/build_systems/cuda.py:        conflicts("%intel@18.0:", when="+cuda ^cuda@:9.9")
lib/spack/spack/build_systems/cuda.py:        conflicts("%intel@19.0:", when="+cuda ^cuda@:10.0")
lib/spack/spack/build_systems/cuda.py:        conflicts("%intel@19.1:", when="+cuda ^cuda@:10.1")
lib/spack/spack/build_systems/cuda.py:        conflicts("%intel@19.2:", when="+cuda ^cuda@:11.1.0")
lib/spack/spack/build_systems/cuda.py:        conflicts("%intel@2021:", when="+cuda ^cuda@:11.4.0")
lib/spack/spack/build_systems/cuda.py:        conflicts("%gcc@13.2.0", when="+cuda ^cuda@:12.4 target=aarch64:")
lib/spack/spack/build_systems/cuda.py:        conflicts("%xl@:12,14:", when="+cuda ^cuda@:9.1")
lib/spack/spack/build_systems/cuda.py:        conflicts("%xl@:12,14:15,17:", when="+cuda ^cuda@9.2")
lib/spack/spack/build_systems/cuda.py:        conflicts("%xl@:12,17:", when="+cuda ^cuda@:11.1.0")
lib/spack/spack/build_systems/cuda.py:        conflicts("target=ppc64le", when="+cuda ^cuda@12.5:")
lib/spack/spack/build_systems/cuda.py:        # TODO: add missing conflicts for %apple-clang cuda@:10
lib/spack/spack/build_systems/cuda.py:        conflicts("platform=darwin", when="+cuda ^cuda@11.0.2:")
lib/spack/spack/build_systems/cached_cmake.py:        if spec.satisfies("^cuda"):
lib/spack/spack/build_systems/cached_cmake.py:            entries.append("# Cuda")
lib/spack/spack/build_systems/cached_cmake.py:            cudatoolkitdir = spec["cuda"].prefix
lib/spack/spack/build_systems/cached_cmake.py:            entries.append(cmake_cache_path("CUDAToolkit_ROOT", cudatoolkitdir))
lib/spack/spack/build_systems/cached_cmake.py:            entries.append(cmake_cache_path("CMAKE_CUDA_COMPILER", "${CUDAToolkit_ROOT}/bin/nvcc"))
lib/spack/spack/build_systems/cached_cmake.py:            entries.append(cmake_cache_path("CMAKE_CUDA_HOST_COMPILER", "${CMAKE_CXX_COMPILER}"))
lib/spack/spack/build_systems/cached_cmake.py:            # Include the deprecated CUDA_TOOLKIT_ROOT_DIR for supporting BLT packages
lib/spack/spack/build_systems/cached_cmake.py:            entries.append(cmake_cache_path("CUDA_TOOLKIT_ROOT_DIR", cudatoolkitdir))
lib/spack/spack/build_systems/cached_cmake.py:            # CUDA_FLAGS
lib/spack/spack/build_systems/cached_cmake.py:            cuda_flags = []
lib/spack/spack/build_systems/cached_cmake.py:            if not spec.satisfies("cuda_arch=none"):
lib/spack/spack/build_systems/cached_cmake.py:                cuda_archs = ";".join(spec.variants["cuda_arch"].value)
lib/spack/spack/build_systems/cached_cmake.py:                entries.append(cmake_cache_string("CMAKE_CUDA_ARCHITECTURES", cuda_archs))
lib/spack/spack/build_systems/cached_cmake.py:                cuda_flags.append("-Xcompiler {}".format(spec_uses_toolchain(spec)[0]))
lib/spack/spack/build_systems/cached_cmake.py:            entries.append(cmake_cache_string("CMAKE_CUDA_FLAGS", " ".join(cuda_flags)))
lib/spack/spack/build_systems/cached_cmake.py:        if "+rocm" in spec:
lib/spack/spack/build_systems/cached_cmake.py:            entries.append("# ROCm")
lib/spack/spack/build_systems/cached_cmake.py:            llvm_bin = spec["llvm-amdgpu"].prefix.bin
lib/spack/spack/build_systems/cached_cmake.py:            llvm_prefix = spec["llvm-amdgpu"].prefix
lib/spack/spack/build_systems/cached_cmake.py:            # Some ROCm systems seem to point to /<path>/rocm-<ver>/ and
lib/spack/spack/build_systems/cached_cmake.py:            # others point to /<path>/rocm-<ver>/llvm
lib/spack/spack/build_systems/cached_cmake.py:            archs = self.spec.variants["amdgpu_target"].value
lib/spack/spack/build_systems/cached_cmake.py:                entries.append(cmake_cache_string("AMDGPU_TARGETS", arch_str))
lib/spack/spack/build_systems/cached_cmake.py:                entries.append(cmake_cache_string("GPU_TARGETS", arch_str))
lib/spack/spack/build_systems/intel.py:    #  intel-gpu-tools/        intel-mkl-dnn/          intel-tbb/
lib/spack/spack/package_base.py:        class ROCmPackage:
lib/spack/spack/package_base.py:            variant("amdgpu_target", ..., when="+rocm")
lib/spack/spack/package_base.py:            variant("amdgpu_target", ...)
lib/spack/spack/package_base.py:    ``when="+rocm"``, but we can't guarantee that will always happen when a vdef is
lib/spack/spack/package_base.py:    #: runtime. Typically includes stub libraries like libcuda.so. When linking
lib/spack/spack/package_base.py:    #: ``["libcuda.so", "stubs"]`` will ensure libcuda.so and all libraries in the
lib/spack/llnl/util/filesystem.py:    # e.g. in the CUDA Toolkit which ships internal libc++ headers.
lib/spack/llnl/url.py:    * ``cuda_8.0.44_linux.run``
lib/spack/llnl/url.py:    * ``cuda_8.0.44``
.mailmap:James Wynne III       <wynnejr@ornl.gov>                James Wynne III         <wynnejr@gpujake.com>

```
