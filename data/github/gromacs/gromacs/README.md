# https://github.com/gromacs/gromacs

```console
api/gmxapi/include/gmxapi/context.h: * of available GPUs or thread-based processor allocation. See #3650 and #3688.
api/gmxapi/cpp/workflow/tests/CMakeLists.txt:gmx_register_gtest_test(GmxapiInternalInterfaceTests workflow-details-test OPENMP_THREADS 2 INTEGRATION_TEST IGNORE_LEAKS QUICK_GPU_TEST)
api/gmxapi/cpp/tests/CMakeLists.txt:gmx_register_gtest_test(GmxapiExternalInterfaceTests gmxapi-test OPENMP_THREADS 2 INTEGRATION_TEST IGNORE_LEAKS QUICK_GPU_TEST)
api/nblib/gmxcalculatorcpu.cpp:    if (options.useGpu)
api/nblib/gmxcalculatorcpu.cpp:        throw InputException("Use GmxNBForceCalculatorGpu for GPU support");
api/nblib/nbnxmsetuphelpers.cpp:#include "gromacs/gpu_utils/device_stream_manager.h"
api/nblib/nbnxmsetuphelpers.cpp:#include "gromacs/gpu_utils/hostallocator.h"
api/nblib/nbnxmsetuphelpers.cpp:#include "gromacs/nbnxm/gpu_data_mgmt.h"
api/nblib/nbnxmsetuphelpers.cpp:#include "gromacs/nbnxm/nbnxm_gpu.h"
api/nblib/nbnxmsetuphelpers.cpp:struct NbnxmGpu;
api/nblib/nbnxmsetuphelpers.cpp:gmx::NbnxmKernelSetup createKernelSetupGPU(const bool useTabulatedEwaldCorr)
api/nblib/nbnxmsetuphelpers.cpp:    kernelSetup.kernelType         = gmx::NbnxmKernelType::Gpu8x8x8;
api/nblib/nbnxmsetuphelpers.cpp:    stepWorkload.useGpuFBufferOps       = false;
api/nblib/nbnxmsetuphelpers.cpp:    stepWorkload.useGpuXBufferOps       = false;
api/nblib/nbnxmsetuphelpers.cpp:gmx::SimulationWorkload createSimulationWorkloadGpu()
api/nblib/nbnxmsetuphelpers.cpp:    simulationWork.useGpuNonbonded = true;
api/nblib/nbnxmsetuphelpers.cpp:    simulationWork.useGpuUpdate    = false;
api/nblib/nbnxmsetuphelpers.cpp:std::unique_ptr<gmx::nonbonded_verlet_t> createNbnxmGPU(const size_t           numParticleTypes,
api/nblib/nbnxmsetuphelpers.cpp:    gmx::NbnxmKernelSetup kernelSetup = createKernelSetupGPU(options.useTabulatedEwaldCorr);
api/nblib/nbnxmsetuphelpers.cpp:    // nbnxn_atomdata is always initialized with 1 thread if the GPU is used
api/nblib/nbnxmsetuphelpers.cpp:    // multiple energy groups are not supported on the GPU
api/nblib/nbnxmsetuphelpers.cpp:    gmx::NbnxmGpu* nbnxmGpu = gmx::gpu_init(
api/nblib/nbnxmsetuphelpers.cpp:    // minimum iList count for GPU balancing
api/nblib/nbnxmsetuphelpers.cpp:    int iListCount = gmx::gpu_min_ci_balanced(nbnxmGpu);
api/nblib/nbnxmsetuphelpers.cpp:            std::move(pairlistSets), std::move(pairSearch), std::move(atomData), kernelSetup, nbnxmGpu);
api/nblib/nbnxmsetuphelpers.cpp:    // Some parameters must be copied to NbnxmGpu to have a fully constructed nonbonded_verlet_t
api/nblib/nbnxmsetuphelpers.cpp:    gmx::gpu_init_atomdata(nbv->gpuNbv(), &nbv->nbat());
api/nblib/tests/refdata/NBlibTest_SpcMethanolForcesAreCorrectOnGpu.xml:  <Sequence Name="SPC-methanol forces on GPU">
api/nblib/tests/nbnxmsetup.cpp:#if GMX_GPU_CUDA
api/nblib/tests/nbnxmsetup.cpp:TEST(NbnxmSetupTest, canCreateKernelSetupGPU)
api/nblib/tests/nbnxmsetup.cpp:    gmx::NbnxmKernelSetup kernelSetup = createKernelSetupGPU(nbKernelOptions.useTabulatedEwaldCorr);
api/nblib/tests/nbnxmsetup.cpp:    EXPECT_EQ(kernelSetup.kernelType, gmx::NbnxmKernelType::Gpu8x8x8);
api/nblib/tests/nbnxmsetup.cpp:        gmx::SimulationWorkload  simulationWork = createSimulationWorkloadGpu();
api/nblib/tests/nbnxmsetup.cpp:TEST(NbnxmSetupTest, CanCreateNbnxmGPU)
api/nblib/tests/nbnxmsetup.cpp:        gmx::SimulationWorkload  simulationWork      = createSimulationWorkloadGpu();
api/nblib/tests/nbnxmsetup.cpp:        EXPECT_NO_THROW(createNbnxmGPU(
api/nblib/include/nblib/nbnxmsetuphelpers.h://! Creates and returns the kernel setup for GPU
api/nblib/include/nblib/nbnxmsetuphelpers.h:gmx::NbnxmKernelSetup createKernelSetupGPU(const bool useTabulatedEwaldCorr);
api/nblib/include/nblib/nbnxmsetuphelpers.h:gmx::SimulationWorkload createSimulationWorkloadGpu();
api/nblib/include/nblib/nbnxmsetuphelpers.h://! Create nonbonded_verlet_gpu object
api/nblib/include/nblib/nbnxmsetuphelpers.h:std::unique_ptr<gmx::nonbonded_verlet_t> createNbnxmGPU(size_t                 numParticleTypes,
api/nblib/include/nblib/kerneloptions.h:    //! Whether to use a GPU, currently GPUs are not supported
api/nblib/include/nblib/kerneloptions.h:    bool useGpu = false;
api/nblib/CMakeLists.txt:if (GMX_GPU_CUDA)
api/nblib/CMakeLists.txt:    target_link_libraries(nblib PRIVATE CUDA::cudart_static)
api/nblib/CMakeLists.txt:        # We need to PUBLIC link to the stub libraries nvml/cuda to WAR an issue
api/nblib/CMakeLists.txt:        target_link_libraries(nblib PUBLIC CUDA::nvml CUDA::cuda_driver)
api/nblib/systemdescription.h:#include "gromacs/gpu_utils/device_stream_manager.h"
api/nblib/systemdescription.h:#include "gromacs/nbnxm/gpu_data_mgmt.h"
api/nblib/systemdescription.h:#include "gromacs/nbnxm/nbnxm_gpu.h"
api/legacy/include/gromacs/math/units.h:#ifndef M_FLOAT_1_SQRTPI /* used in GPU kernels */
api/legacy/include/gromacs/math/vectypes.h:    // TODO: Use std::is_pointer_v when CUDA 11 is a requirement.
api/legacy/include/gromacs/math/functions.h:#if (defined(__NVCC__) && !defined(__CUDACC_RELAXED_CONSTEXPR__)) || defined(__HIPCC__)
api/legacy/include/gromacs/math/functions.h:/* In CUDA 11, a constexpr function cannot be called from a function with incompatible execution
api/legacy/include/gromacs/math/functions.h:#if (defined(__NVCC__) && !defined(__CUDACC_RELAXED_CONSTEXPR__)) || defined(__HIPCC__)
api/legacy/include/gromacs/math/functions.h:/* In CUDA 11, a constexpr function cannot be called from a function with incompatible execution
api/legacy/include/gromacs/utility/smalloc.h:    // TODO: Use std::is_standard_layout_v when CUDA 11 is a requirement.
api/legacy/include/gromacs/utility/smalloc.h:    // TODO: Use std::is_standard_layout_v when CUDA 11 is a requirement.
api/legacy/include/gromacs/utility/smalloc.h:    // TODO: Use std::is_standard_layout_v when CUDA 11 is a requirement.
api/legacy/include/gromacs/utility/smalloc.h:    // TODO: Use std::is_standard_layout_v when CUDA 11 is a requirement.
api/legacy/include/gromacs/utility/smalloc.h:    // TODO: Use std::is_pod_v and std::is_void_v when CUDA 11 is a requirement.
api/legacy/include/gromacs/utility/smalloc.h:    // TODO: Use std::is_pod_v and std::is_void_v when CUDA 11 is a requirement.
api/legacy/include/gromacs/utility/exceptions.h: * \todo Use std::is_base_of_v when CUDA 11 is a requirement.
api/legacy/include/gromacs/utility/range.h:    // TODO: Use std::is_integral_v when CUDA 11 is a requirement.
api/legacy/include/gromacs/utility/baseversion.h:/*! \brief Return a string describing what kind of GPU suport was configured in the build.
api/legacy/include/gromacs/utility/baseversion.h: * Currently returns correctly for CUDA, OpenCL and SYCL.
api/legacy/include/gromacs/utility/baseversion.h:const char* getGpuImplementationString();
api/legacy/include/gromacs/utility/enumerationhelpers.h:    // TODO: Use std::is_enum_v when CUDA 11 is a requirement.
api/legacy/include/gromacs/utility/arrayref.h:    // TODO: Use std::is_const_v when CUDA 11 is a requirement.
api/legacy/include/gromacs/utility/arrayref.h:     * \todo Use std::is_convertible_v when CUDA 11 is a requirement.
api/legacy/include/gromacs/utility/arrayref.h: * \todo Use std::is_const_v when CUDA 11 is a requirement.
docs/doxygen/Doxyfile-common.cmakein:# CUDA files could be included like this, but currently produce a lot of
docs/doxygen/lib/nbnxm.md:(single-instruction multiple-data) CPU architectures, nor for GPGPU
docs/doxygen/lib/nbnxm.md:a cluster-pair simultaneously. For GPUs there is another layer:
docs/doxygen/lib/nbnxm.md:* GPU: targets GPUs with N=M=8 or N=M=4, depending on
docs/doxygen/lib/nbnxm.md:  `GMX_GPU_NB_CLUSTER_SIZE` compilation option value.
docs/doxygen/lib/modularsimulator.md:  and NVIDIA GPU are at most 1% slower -
docs/doxygen/lib/modularsimulator.md:* Full support of GPU (current implementation does not support
docs/doxygen/lib/modularsimulator.md:GPU update)
docs/doxygen/suppressions.txt:src/gromacs/nbnxm/cuda/nbnxm_cuda_types.h: error: NbnxmGpu: is in internal file(s), but appears in public documentation
docs/dev-manual/naming.rst:Code for GPUs
docs/dev-manual/naming.rst:Rationale: on GPUs, using the right memory space is often performance critical.
docs/dev-manual/naming.rst:* In CUDA device code ``sm_``, ``gm_``, and ``cm_`` prefixes are used for
docs/dev-manual/naming.rst:  register space. Same prefixes are used in OpenCL code, where ``sm_``
docs/dev-manual/naming.rst:* Data transferred to and from host has to live in both CPU and GPU memory
docs/dev-manual/naming.rst:  spaces. Therefore it is typical to have a pointer or container (in CUDA), or
docs/dev-manual/naming.rst:  memory buffer (in OpenCL) in host memory that has a device-based counterpart.
docs/dev-manual/naming.rst:* In case GPU kernel arguments are combined into a structure, it is preferred
docs/dev-manual/naming.rst:* Avoid using uninformative names for CUDA warp, thread, block indexes and
docs/dev-manual/naming.rst:  their OpenCL analogs (i.e. ``threadIndex`` is preferred to ``i`` or
docs/dev-manual/gitlab-ci.rst:Using GPUs in Gitlab-runner
docs/dev-manual/gitlab-ci.rst:difficult to keep up. In the future it might be possible to select GPUs directly
docs/dev-manual/gitlab-ci.rst:well as single or dual GPU devices from Nvidia, AMD, and Intel.
docs/dev-manual/gitlab-ci.rst:Uses NVidia's
docs/dev-manual/gitlab-ci.rst:`HPC Container Maker <https://github.com/NVIDIA/hpc-container-maker/tree/master/docs>`__
docs/dev-manual/known-issues.rst:Issues with GPU timer with OpenCL
docs/dev-manual/known-issues.rst:When building using OpenCL in ``Debug`` mode, it can happen that the GPU timer state gets
docs/dev-manual/known-issues.rst:This seems to be related to the load of other, unrelated tasks on the GPU.
docs/dev-manual/known-issues.rst:GPU emulation does not work
docs/dev-manual/known-issues.rst:The non-bonded GPU emulation mode does not work, at least for builds
docs/dev-manual/known-issues.rst:with GPU support; then a GPU setup call is called.
docs/dev-manual/known-issues.rst:Also dynamic pruning needs to be implemented for GPU emulation.
docs/dev-manual/known-issues.rst:OpenCL on NVIDIA Volta and later broken
docs/dev-manual/known-issues.rst:The OpenCL code produces incorrect results on Volta and Turing GPU architectures
docs/dev-manual/known-issues.rst:from NVIDIA (CC 7.0 and 7.5). This is an issue that affects certain flavors of 
docs/dev-manual/known-issues.rst:work (i.e no separate PME ranks) and more GPUs are detected than
docs/dev-manual/known-issues.rst:this by using ``-gpu_id`` or ``GMX_GPU_ID`` or limiting the number of
docs/dev-manual/known-issues.rst:visible GPUs.
docs/dev-manual/error-handling.rst:* Otherwise, (typically an error returned from system call or GPU SDK, bad user
docs/dev-manual/error-handling.rst:  libraries, GPU SDKs) always check for success/failure. Generally the
docs/dev-manual/error-handling.rst:  non-blocking API calls (e.g. to MPI or GPU SDKs) have been made,
docs/dev-manual/change-management.rst:Good names: documentation_UpdateDevelopersDocsTOGitLab, nbnxm_MakeNbnxmGPUIntoClass, pme_FEPPMEGPU,
docs/dev-manual/testutils.rst:  number of MPI ranks or GPU devices detected.  Explicit information
docs/dev-manual/build-system.rst:.. cmake:: GMX_GPU
docs/dev-manual/build-system.rst:   Choose the backend for GPU offload. Possible values: ``CUDA``, ``OpenCL``, ``SYCL``.
docs/dev-manual/build-system.rst:   Please see the :ref:`Installation guide <gmx-gpu-support>` for more information.
docs/dev-manual/build-system.rst:.. cmake:: GMX_CLANG_CUDA
docs/dev-manual/build-system.rst:   Use clang for compiling CUDA GPU code, both host and device.
docs/dev-manual/build-system.rst:   Please see the :ref:`Installation guide <gmx-gpu-support>` for more information.
docs/dev-manual/build-system.rst:.. cmake:: GMX_CUDA_CLANG_FLAGS
docs/dev-manual/build-system.rst:    Pass additional CUDA-only compiler flags to clang using this variable.
docs/dev-manual/build-system.rst:   Use the NVTX library for annotating |Gromacs| tasks in the NVIDIA tracing tools.
docs/dev-manual/build-system.rst:   Relies on the ``CUDA_HOME`` environment variable to find the library.
docs/dev-manual/build-system.rst:   Use the ROC-TX library for annotating |Gromacs| tasks in the AMD ROCm tracing tools.
docs/dev-manual/build-system.rst:   Relies on the ``ROCM_HOME`` environment variable to find the library.
docs/dev-manual/language-features.rst:* We should be able to use virtually all C++17 features; see "GPU API considerations"
docs/dev-manual/language-features.rst:GPU API considerations
docs/dev-manual/language-features.rst:* Write OpenCL as C (specifically, C99) code. Using C++ in OpenCL kernels
docs/dev-manual/language-features.rst:* Keep in mind that some combinations of CUDA and GCC do not handle the C++17 properly.
docs/dev-manual/language-features.rst:  This makes the code more uniform across all GPU backends. Besides, buffers
docs/user-guide/environment-variables.rst:``GMX_DISABLE_GPU_TIMING``
docs/user-guide/environment-variables.rst:        Disables GPU timings in the log file for OpenCL.
docs/user-guide/environment-variables.rst:``GMX_ENABLE_GPU_TIMING``
docs/user-guide/environment-variables.rst:        Enables GPU timings in the log file for CUDA and SYCL. Note that CUDA
docs/user-guide/environment-variables.rst:        decomposition or with both non-bondeds and PME on the GPU (this is
docs/user-guide/environment-variables.rst:``GMX_DISABLE_ALTERNATING_GPU_WAIT``
docs/user-guide/environment-variables.rst:        GPU tasks completion to overlap to do the reduction of the resulting forces that
docs/user-guide/environment-variables.rst:        sets the number of GPUs required by the test suite. By default, the test suite would
docs/user-guide/environment-variables.rst:        fall-back to using CPU if GPUs could not be detected. Set it to a positive integer value
docs/user-guide/environment-variables.rst:        to ensure that at least this at least this number of usable GPUs are detected. Default:
docs/user-guide/environment-variables.rst:        0 (not testing GPU availability).
docs/user-guide/environment-variables.rst:``GMX_CUDA_GRAPH``
docs/user-guide/environment-variables.rst:        Use CUDA Graphs to schedule a graph on each step rather than multiple
docs/user-guide/environment-variables.rst:        activities scheduled to multiple CUDA streams, if the run conditions allow. Experimental.
docs/user-guide/environment-variables.rst:``GMX_DISABLE_CUDA_TIMING``
docs/user-guide/environment-variables.rst:        Deprecated. Use ``GMX_DISABLE_GPU_TIMING`` instead.
docs/user-guide/environment-variables.rst:``GMX_DISABLE_GPU_DETECTION``
docs/user-guide/environment-variables.rst:        when set, disables GPU detection even if :ref:`gmx mdrun` was compiled
docs/user-guide/environment-variables.rst:        with GPU support.
docs/user-guide/environment-variables.rst:``GMX_DISABLE_GPU_TIMING``
docs/user-guide/environment-variables.rst:        timing of asynchronously executed GPU operations can have a
docs/user-guide/environment-variables.rst:        Timings are disabled by default with CUDA and SYCL.
docs/user-guide/environment-variables.rst:``GMX_DISABLE_STAGED_GPU_TO_CPU_PMEPP_COMM``
docs/user-guide/environment-variables.rst:        Use direct rather than staged GPU communications for PME force
docs/user-guide/environment-variables.rst:        transfers from the PME GPU to the CPU memory of a PP
docs/user-guide/environment-variables.rst:``GMX_EMULATE_GPU``
docs/user-guide/environment-variables.rst:        emulate GPU runs by using algorithmically equivalent CPU reference code instead of
docs/user-guide/environment-variables.rst:        GPU-accelerated functions. As the CPU code is slow, it is intended to be used only for debugging purposes.
docs/user-guide/environment-variables.rst:``GMX_ENABLE_DIRECT_GPU_COMM``
docs/user-guide/environment-variables.rst:        Enable direct GPU communication in multi-rank parallel runs.
docs/user-guide/environment-variables.rst:        Note that domain decomposition with GPU-aware MPI does not support
docs/user-guide/environment-variables.rst:``GMX_ENABLE_STAGED_GPU_TO_CPU_PMEPP_COMM``
docs/user-guide/environment-variables.rst:        Use a staged implementation of GPU communications for PME force
docs/user-guide/environment-variables.rst:        transfers from the PME GPU to the CPU memory of a PP rank for
docs/user-guide/environment-variables.rst:        thread-MPI. The staging is done via a GPU buffer on the PP
docs/user-guide/environment-variables.rst:        GPU. This is expected to be beneficial for servers with direct
docs/user-guide/environment-variables.rst:        communication links between GPUs.
docs/user-guide/environment-variables.rst:        or GPU warp/wave-front width for computing non-bonded interactions. These fillers can
docs/user-guide/environment-variables.rst:``GMX_FORCE_GPU_AWARE_MPI``
docs/user-guide/environment-variables.rst:        Override the result of build- and runtime GPU-aware MPI detection and force the use of
docs/user-guide/environment-variables.rst:        direct GPU MPI communication. Aimed at cases where the user knows that the MPI library is
docs/user-guide/environment-variables.rst:        GPU-aware, but |Gromacs| is not able to detect this. Note that only CUDA and SYCL builds 
docs/user-guide/environment-variables.rst:``GMX_GPU_DD_COMMS``
docs/user-guide/environment-variables.rst:        Removed, use GMX_ENABLE_DIRECT_GPU_COMM instead.
docs/user-guide/environment-variables.rst:``GMX_GPU_DISABLE_COMPATIBILITY_CHECK``
docs/user-guide/environment-variables.rst:        Disables the hardware compatibility check in OpenCL and SYCL. Useful for developers
docs/user-guide/environment-variables.rst:        and allows testing the OpenCL/SYCL kernels on non-supported platforms without source code modification.
docs/user-guide/environment-variables.rst:``GMX_GPU_ID``
docs/user-guide/environment-variables.rst:        set in the same way as ``mdrun -gpu_id``, ``GMX_GPU_ID``
docs/user-guide/environment-variables.rst:        allows the user to specify different GPU IDs for different ranks, which can be useful for selecting different
docs/user-guide/environment-variables.rst:        devices on different compute nodes in a cluster.  Cannot be used in conjunction with ``mdrun -gpu_id``.
docs/user-guide/environment-variables.rst:``GMX_GPU_NB_EWALD_TWINCUT``
docs/user-guide/environment-variables.rst:``GMX_GPU_NB_ANA_EWALD``
docs/user-guide/environment-variables.rst:``GMX_GPU_NB_TAB_EWALD``
docs/user-guide/environment-variables.rst:``GMX_GPU_PME_DECOMPOSITION``
docs/user-guide/environment-variables.rst:        Enable the support for PME decomposition on GPU.
docs/user-guide/environment-variables.rst:        This feature is supported with CUDA and SYCL backends, and allows
docs/user-guide/environment-variables.rst:        using multiple PME ranks with GPU offload, which is expected to improve
docs/user-guide/environment-variables.rst:        performance when scaling over many GPUs. Note: this feature still lacks
docs/user-guide/environment-variables.rst:``GMX_GPU_PME_PP_COMMS``
docs/user-guide/environment-variables.rst:        Removed, use GMX_ENABLE_DIRECT_GPU_COMM instead.
docs/user-guide/environment-variables.rst:``GMX_GPUTASKS``
docs/user-guide/environment-variables.rst:        set in the same way as ``mdrun -gputasks``, ``GMX_GPUTASKS`` allows the mapping
docs/user-guide/environment-variables.rst:        of GPU tasks to GPU device IDs to be different on different ranks, if e.g. the MPI
docs/user-guide/environment-variables.rst:        in conjunction with ``mdrun -gputasks``. Has all the same requirements as ``mdrun -gputasks``.
docs/user-guide/environment-variables.rst:``GMX_HEFFTE_USE_GPU_AWARE``
docs/user-guide/environment-variables.rst:        Sets ``heffte::plan_options::use_gpu_aware`` to ``true`` (the default) or ``false``.
docs/user-guide/environment-variables.rst:        neighbor list balancing parameter used when running on GPU. Sets the
docs/user-guide/environment-variables.rst:        The default value is optimized for supported GPUs
docs/user-guide/environment-variables.rst:        performance gain from adding a GPU accelerator to the current hardware setup -- assuming that this is
docs/user-guide/environment-variables.rst:        (1 for CPU and 2 for GPU) and :mdp:`nstlist` ``- 1``.
docs/user-guide/environment-variables.rst:.. _opencl-management:
docs/user-guide/environment-variables.rst:OpenCL management
docs/user-guide/environment-variables.rst:of the OpenCL_ version of |Gromacs|. They are mostly related to the runtime
docs/user-guide/environment-variables.rst:compilation of OpenCL kernels, but they are also used in device selection.
docs/user-guide/environment-variables.rst:        OpenCL build process is saved to file. Caching has to be
docs/user-guide/environment-variables.rst:            - NVIDIA GPUs: PTX code is saved in the current directory
docs/user-guide/environment-variables.rst:            - AMD GPUs: ``.IL/.ISA`` files will be created for each OpenCL
docs/user-guide/environment-variables.rst:        If defined, the OpenCL build log is always written to the
docs/user-guide/environment-variables.rst:        Use this parameter to force |Gromacs| to load the OpenCL
docs/user-guide/environment-variables.rst:        OpenCL on RDNA-family devices, but is not recommended.
docs/user-guide/environment-variables.rst:        Force the selection of a CPU device instead of a GPU.  This
docs/user-guide/environment-variables.rst:        Enable OpenCL binary caching. Only intended to be used for
docs/user-guide/environment-variables.rst:        Use Intel OpenCL extension to show additional runtime performance
docs/user-guide/environment-variables.rst:        If defined, it enables verbose mode for OpenCL kernel build.
docs/user-guide/environment-variables.rst:        Currently available only for NVIDIA GPUs. See ``GMX_OCL_DUMP_LOG``
docs/user-guide/environment-variables.rst:        for details about how to obtain the OpenCL build log.
docs/user-guide/mdrun-performance.rst:        pthreads, winthreads, CUDA, SYCL, OpenCL, and OpenACC. Some kinds of
docs/user-guide/mdrun-performance.rst:    GPU
docs/user-guide/mdrun-performance.rst:        compute workloads. A GPU is always associated with a
docs/user-guide/mdrun-performance.rst:    CUDA
docs/user-guide/mdrun-performance.rst:        A proprietary parallel computing framework and API developed by NVIDIA
docs/user-guide/mdrun-performance.rst:        |Gromacs| uses CUDA for GPU acceleration support with NVIDIA hardware.
docs/user-guide/mdrun-performance.rst:    OpenCL
docs/user-guide/mdrun-performance.rst:        and accelerator hardware. |Gromacs| uses OpenCL for GPU acceleration
docs/user-guide/mdrun-performance.rst:        on AMD devices (both GPUs and APUs), Intel integrated GPUs, and Apple
docs/user-guide/mdrun-performance.rst:        Silicon integrated GPUs; some NVIDIA hardware is also supported.
docs/user-guide/mdrun-performance.rst:        In |Gromacs|, OpenCL has been deprecated in favor of SYCL.
docs/user-guide/mdrun-performance.rst:        `Intel oneAPI DPC++`_ and AdaptiveCpp_. |Gromacs| uses SYCL for GPU acceleration
docs/user-guide/mdrun-performance.rst:        on AMD and Intel GPUs. There is experimental support for NVIDIA GPUs too.
docs/user-guide/mdrun-performance.rst:        GPU. The PP rank also handles any bonded interactions for the
docs/user-guide/mdrun-performance.rst:        members of its domain. A GPU may perform work for more than
docs/user-guide/mdrun-performance.rst:        PP rank per GPU and for that rank to have thousands of
docs/user-guide/mdrun-performance.rst:performance in GPU accelerated or highly parallel MPI runs.
docs/user-guide/mdrun-performance.rst:Node level parallelization via GPU offloading and thread-MPI
docs/user-guide/mdrun-performance.rst:including OpenMP, GPUs; it is not compatible with MPI and multi-simulation runs.
docs/user-guide/mdrun-performance.rst:Hybrid acceleration means distributing compute work between available CPUs and GPUs
docs/user-guide/mdrun-performance.rst:have been developed with the aim of efficient acceleration both on CPUs and GPUs.
docs/user-guide/mdrun-performance.rst:offloaded to GPUs and carried out simultaneously with remaining CPU work.
docs/user-guide/mdrun-performance.rst:Native GPU acceleration is supported for the most commonly used algorithms in
docs/user-guide/mdrun-performance.rst:For more information about the GPU kernels, please see the :ref:`Installation guide <gmx-gpu-support>`.
docs/user-guide/mdrun-performance.rst:The native GPU acceleration can be turned on or off, either at run-time using the
docs/user-guide/mdrun-performance.rst::ref:`mdrun <gmx mdrun>` ``-nb`` option, or at configuration time using the ``GMX_GPU`` CMake variable.
docs/user-guide/mdrun-performance.rst:To efficiently use all compute resource available, CPU and GPU computation is done simultaneously.
docs/user-guide/mdrun-performance.rst:on the CPU, non-bonded forces are calculated on the GPU. Multiple GPUs, both in a single node as
docs/user-guide/mdrun-performance.rst:well as across multiple nodes, are supported using domain-decomposition. A single GPU is assigned
docs/user-guide/mdrun-performance.rst:to the non-bonded workload of a domain, therefore, the number GPUs used has to match the number
docs/user-guide/mdrun-performance.rst:with a GPU do the calculations on the respective domain.
docs/user-guide/mdrun-performance.rst:With PME electrostatics, :ref:`mdrun <gmx mdrun>` supports automated CPU-GPU load-balancing by
docs/user-guide/mdrun-performance.rst:non-bonded calculations, done on the GPU. At startup a few iterations of tuning are executed
docs/user-guide/mdrun-performance.rst:and PME grid spacing to determine the value that gives optimal CPU-GPU load balance. The cut-off
docs/user-guide/mdrun-performance.rst:While the automated CPU-GPU load balancing always attempts to find the optimal cut-off setting,
docs/user-guide/mdrun-performance.rst:it might not always be possible to balance CPU and GPU workload. This happens when the CPU threads
docs/user-guide/mdrun-performance.rst:finish calculating the bonded forces and PME faster than the GPU the non-bonded force calculation,
docs/user-guide/mdrun-performance.rst:even with the shortest possible cut-off. In such cases the CPU will wait for the GPU and this
docs/user-guide/mdrun-performance.rst:time will show up as ``Wait GPU NB local`` in the cycle and timing summary table at the end
docs/user-guide/mdrun-performance.rst:that uses CUDA/SYCL/OpenCL, OpenMP and the threading platform native to the
docs/user-guide/mdrun-performance.rst:    will start one rank per GPU (if present), and otherwise one rank
docs/user-guide/mdrun-performance.rst:    Can be set to "auto", "cpu", "gpu."
docs/user-guide/mdrun-performance.rst:    Defaults to "auto," which uses a compatible GPU if available.
docs/user-guide/mdrun-performance.rst:    Setting "cpu" requires that no GPU is used. Setting "gpu" requires
docs/user-guide/mdrun-performance.rst:    that a compatible GPU is available and will be used.
docs/user-guide/mdrun-performance.rst:    Can be set to "auto", "cpu", "gpu."
docs/user-guide/mdrun-performance.rst:    Defaults to "auto," which uses a compatible GPU if available.
docs/user-guide/mdrun-performance.rst:    Setting "gpu" requires that a compatible GPU is available.
docs/user-guide/mdrun-performance.rst:    Multiple PME ranks are not supported with PME on GPU, so if a GPU is used
docs/user-guide/mdrun-performance.rst:    Can be set to "auto", "cpu", "gpu."
docs/user-guide/mdrun-performance.rst:    Defaults to "auto," which uses a compatible CUDA or SYCL GPU only when one
docs/user-guide/mdrun-performance.rst:    is available, a GPU is handling short-ranged interactions, and the
docs/user-guide/mdrun-performance.rst:    GPU as the short-ranged interactions, and cannot be independently
docs/user-guide/mdrun-performance.rst:    Setting "gpu" requires that a compatible GPU is available and will
docs/user-guide/mdrun-performance.rst:    Can be set to "auto", "cpu", "gpu."
docs/user-guide/mdrun-performance.rst:    Setting "gpu" requires that a compatible CUDA or SYCL GPU is available,
docs/user-guide/mdrun-performance.rst:    Update and constraints on a GPU is currently not supported
docs/user-guide/mdrun-performance.rst:``-gpu_id``
docs/user-guide/mdrun-performance.rst:    A string that specifies the ID numbers of the GPUs that
docs/user-guide/mdrun-performance.rst:    "12" specifies that the GPUs with IDs 1 and 2 (as reported
docs/user-guide/mdrun-performance.rst:    by the GPU runtime) can be used by :ref:`mdrun <gmx mdrun>`. This is useful
docs/user-guide/mdrun-performance.rst:    when sharing a node with other computations, or if a GPU that
docs/user-guide/mdrun-performance.rst:    will utilize all GPUs. When many GPUs are
docs/user-guide/mdrun-performance.rst:    "12,13" would make GPUs 12 and 13 available to :ref:`mdrun <gmx mdrun>`.
docs/user-guide/mdrun-performance.rst:    It could be necessary to use different GPUs on different
docs/user-guide/mdrun-performance.rst:    variable ``GMX_GPU_ID`` can be set differently for the ranks
docs/user-guide/mdrun-performance.rst:    specify both GPU availability and GPU task assignment.
docs/user-guide/mdrun-performance.rst:    The latter is now done with the ``-gputasks`` parameter.
docs/user-guide/mdrun-performance.rst:``-gputasks``
docs/user-guide/mdrun-performance.rst:    A string that specifies the ID numbers of the GPUs to be
docs/user-guide/mdrun-performance.rst:    used by corresponding GPU tasks on this node. For example,
docs/user-guide/mdrun-performance.rst:    "0011" specifies that the first two GPU tasks will use GPU 0,
docs/user-guide/mdrun-performance.rst:    and the other two use GPU 1. When using this option, the
docs/user-guide/mdrun-performance.rst:    ``-nb gpu`` - only the tasks which are set to run on GPUs
docs/user-guide/mdrun-performance.rst:    count for parsing the mapping. See `Assigning tasks to GPUs`_
docs/user-guide/mdrun-performance.rst:    for more details. Note that ``-gpu_id`` and
docs/user-guide/mdrun-performance.rst:    ``-gputasks`` can not be used at the same time!
docs/user-guide/mdrun-performance.rst:    of GPU task ("PP") could be run on any rank. Now that there is some
docs/user-guide/mdrun-performance.rst:    support for running PME on GPUs, the number of GPU tasks
docs/user-guide/mdrun-performance.rst:    (and the number of GPU IDs expected in the ``-gputasks`` string)
docs/user-guide/mdrun-performance.rst:    still have to be the same in this case, as using multiple GPUs
docs/user-guide/mdrun-performance.rst:    The order of GPU tasks per rank in the string is PP first,
docs/user-guide/mdrun-performance.rst:    PME second. The order of ranks with different kinds of GPU tasks
docs/user-guide/mdrun-performance.rst:    run on the same GPU as the short-ranged work, or on the CPU,
docs/user-guide/mdrun-performance.rst:    The GPU task assignment (whether manually set, or automated),
docs/user-guide/mdrun-performance.rst:      gmx mdrun -gputasks 0001 -nb gpu -pme gpu -npme 1 -ntmpi 4
docs/user-guide/mdrun-performance.rst:      On host tcbl14 2 GPUs selected for this run.
docs/user-guide/mdrun-performance.rst:      Mapping of GPU IDs to the 4 GPU tasks in the 4 ranks on this node:
docs/user-guide/mdrun-performance.rst:    on GPU 0, and 1 rank to compute PME on GPU 1.
docs/user-guide/mdrun-performance.rst:    The detailed indexing of the GPUs is also reported in the log file.
docs/user-guide/mdrun-performance.rst:    For more information about GPU tasks, please refer to
docs/user-guide/mdrun-performance.rst:    :ref:`Types of GPU tasks<gmx-gpu-tasks>`.
docs/user-guide/mdrun-performance.rst:    Allows choosing whether to execute the 3D FFT computation on a CPU or GPU.
docs/user-guide/mdrun-performance.rst:    Can be set to "auto", "cpu", "gpu.".
docs/user-guide/mdrun-performance.rst:    When PME is offloaded to a GPU ``-pmefft gpu`` is the default,
docs/user-guide/mdrun-performance.rst:    and the entire PME calculation is executed on the GPU. However,
docs/user-guide/mdrun-performance.rst:    in some cases, e.g. with a relatively slow or older generation GPU
docs/user-guide/mdrun-performance.rst:    combined with fast CPU cores in a run, moving some work off of the GPU
docs/user-guide/mdrun-performance.rst:to compatible GPUs. Details will vary with hardware
docs/user-guide/mdrun-performance.rst:    gmx mdrun -ntmpi 4 -nb gpu -pme cpu
docs/user-guide/mdrun-performance.rst:powerful compared to the GPUs. The bonded part of force calculation
docs/user-guide/mdrun-performance.rst:will automatically be assigned to the GPU, since the long-range
docs/user-guide/mdrun-performance.rst:    gmx mdrun -ntmpi 1 -nb gpu -pme gpu -bonded gpu -update gpu
docs/user-guide/mdrun-performance.rst:on a GPU will do so. This may be optimal on hardware where the CPUs
docs/user-guide/mdrun-performance.rst:are extremely weak compared to the GPUs.
docs/user-guide/mdrun-performance.rst:    gmx mdrun -ntmpi 4 -nb gpu -pme cpu -gputasks 0011
docs/user-guide/mdrun-performance.rst:to GPUs with IDs 0 and 1. The CPU cores available will be split evenly between
docs/user-guide/mdrun-performance.rst:nonbonded force calculations to GPU 0, and the last two ranks offloading to GPU 1.
docs/user-guide/mdrun-performance.rst:on hardware where the CPUs are relatively powerful compared to the GPUs.
docs/user-guide/mdrun-performance.rst:    gmx mdrun -ntmpi 4 -nb gpu -pme gpu -npme 1 -gputasks 0001
docs/user-guide/mdrun-performance.rst:short-range non-bonded calculations to the GPU with ID 0, the 4th (PME) thread
docs/user-guide/mdrun-performance.rst:offloads its calculations to the GPU with ID 1.
docs/user-guide/mdrun-performance.rst:    gmx mdrun -ntmpi 4 -nb gpu -pme gpu -npme 1 -gputasks 0011
docs/user-guide/mdrun-performance.rst:calculations to GPU 0. The GPU with ID 1 calculates the short-ranged forces of
docs/user-guide/mdrun-performance.rst:of the individual GPUs and the system composition.
docs/user-guide/mdrun-performance.rst:    gmx mdrun -gpu_id 12
docs/user-guide/mdrun-performance.rst:Starts :ref:`mdrun <gmx mdrun>` using GPUs with IDs 1 and 2 (e.g. because
docs/user-guide/mdrun-performance.rst:GPU 0 is dedicated to running a display). This requires
docs/user-guide/mdrun-performance.rst:presence of GPUs, network, and simulation algorithm, but
docs/user-guide/mdrun-performance.rst:    load between ranks and/or GPUs to maximize throughput. Some
docs/user-guide/mdrun-performance.rst:    performance. If available, using ``-bonded gpu`` is expected
docs/user-guide/mdrun-performance.rst:    DLB is not compatible with GPU-resident parallelization (with ``-update gpu``)
docs/user-guide/mdrun-performance.rst:    mpirun -np 4 gmx_mpi mdrun -ntomp 6 -nb gpu -gputasks 00
docs/user-guide/mdrun-performance.rst:and both ranks on a node sharing GPU with ID 0.
docs/user-guide/mdrun-performance.rst:    mpirun -np 8 gmx_mpi mdrun -ntomp 3 -gputasks 0000
docs/user-guide/mdrun-performance.rst:and all four ranks on a node sharing GPU with ID 0.
docs/user-guide/mdrun-performance.rst:    mpirun -np 20 gmx_mpi mdrun -ntomp 4 -gputasks 00
docs/user-guide/mdrun-performance.rst:suitable when there are ten nodes, each with one GPU, and each node
docs/user-guide/mdrun-performance.rst:    mpirun -np 10 gmx_mpi mdrun -gpu_id 1
docs/user-guide/mdrun-performance.rst:suitable when there are ten nodes, each with two GPUs, but another
docs/user-guide/mdrun-performance.rst:job on each node is using GPU 0. The job scheduler should set the
docs/user-guide/mdrun-performance.rst:    mpirun -np 20 gmx_mpi mdrun -gpu_id 01
docs/user-guide/mdrun-performance.rst:GPUs, but there is no need to specify ``-gpu_id`` for the
docs/user-guide/mdrun-performance.rst:normal case where all the GPUs on the node are available
docs/user-guide/mdrun-performance.rst:limit which is around ~100 atoms/core or ~10000 atoms/GPU. Simulations
docs/user-guide/mdrun-performance.rst:the update run on a GPU when using domain decomposition, the system
docs/user-guide/mdrun-performance.rst:* Launch GPU operations
docs/user-guide/mdrun-performance.rst:* Wait GPU nonlocal
docs/user-guide/mdrun-performance.rst:* Wait GPU local
docs/user-guide/mdrun-performance.rst:* Wait PME GPU spread
docs/user-guide/mdrun-performance.rst:* Wait PME GPU gather
docs/user-guide/mdrun-performance.rst:* Reduce PME GPU Force
docs/user-guide/mdrun-performance.rst:* Launch non-bonded GPU tasks
docs/user-guide/mdrun-performance.rst:* Launch PME GPU tasks
docs/user-guide/mdrun-performance.rst:.. _gmx-mdrun-on-gpu:
docs/user-guide/mdrun-performance.rst:Running :ref:`mdrun <gmx mdrun>` with GPUs
docs/user-guide/mdrun-performance.rst:.. _gmx-gpu-tasks:
docs/user-guide/mdrun-performance.rst:Types of GPU tasks
docs/user-guide/mdrun-performance.rst:To better understand the later sections on different GPU use cases for
docs/user-guide/mdrun-performance.rst:calculation of :ref:`short range<gmx-gpu-pp>`, :ref:`PME<gmx-gpu-pme>`,
docs/user-guide/mdrun-performance.rst::ref:`bonded interactions<gmx-gpu-bonded>` and
docs/user-guide/mdrun-performance.rst::ref:`update and constraints <gmx-gpu-update>`
docs/user-guide/mdrun-performance.rst:we first introduce the concept of different GPU tasks. When thinking about
docs/user-guide/mdrun-performance.rst:Additionally, with GPU accelerators used as *co-processors*, some of the work
docs/user-guide/mdrun-performance.rst:Right now, |Gromacs| supports GPU accelerator offload of two tasks:
docs/user-guide/mdrun-performance.rst:the short-range :ref:`nonbonded interactions in real space <gmx-gpu-pp>`,
docs/user-guide/mdrun-performance.rst:and :ref:`PME <gmx-gpu-pme>`.
docs/user-guide/mdrun-performance.rst:|Gromacs| supports two major offload modes: force-offload and GPU-resident.
docs/user-guide/mdrun-performance.rst:on the CPU (hence requiring per-step data movement). In the GPU-resident mode
docs/user-guide/mdrun-performance.rst:The force-offload mode is the more broadly supported GPU-acceleration mode
docs/user-guide/mdrun-performance.rst:with short-range nonbonded offload supported on a wide range of GPU accelerators
docs/user-guide/mdrun-performance.rst:(NVIDIA, AMD, and Intel). This is compatible with the grand majority of
docs/user-guide/mdrun-performance.rst:PME work to GPU accelerators has some restrictions in terms of feature and parallelization
docs/user-guide/mdrun-performance.rst:compatibility (please see the :ref:`section below <gmx-pme-gpu-limitations>`).
docs/user-guide/mdrun-performance.rst:Offloading (most types of) bonded interactions is supported in CUDA and SYCL.
docs/user-guide/mdrun-performance.rst:The GPU-resident mode is supported with CUDA and SYCL, but it has additional limitations as
docs/user-guide/mdrun-performance.rst:described in :ref:`the GPU update section <gmx-gpu-update>`.
docs/user-guide/mdrun-performance.rst:.. _gmx-gpu-pp:
docs/user-guide/mdrun-performance.rst:GPU computation of short range nonbonded interactions
docs/user-guide/mdrun-performance.rst:Using the GPU for the short-ranged nonbonded interactions provides
docs/user-guide/mdrun-performance.rst:Here, the GPU acts as an accelerator that can effectively parallelize
docs/user-guide/mdrun-performance.rst:.. _gmx-gpu-pme:
docs/user-guide/mdrun-performance.rst:GPU accelerated calculation of PME (not for AMD HIP)
docs/user-guide/mdrun-performance.rst:to the GPU, to further reduce the load on the CPU and improve usage overlap between
docs/user-guide/mdrun-performance.rst:CPU and GPU. Here, the solving of PME will be performed in addition to the calculation
docs/user-guide/mdrun-performance.rst:of the short range interactions on the same GPU as the short range interactions.
docs/user-guide/mdrun-performance.rst:.. _gmx-pme-gpu-limitations:
docs/user-guide/mdrun-performance.rst:- Only a PME order of 4 is supported on GPUs.
docs/user-guide/mdrun-performance.rst:- Multiple ranks (hence multiple GPUs) computing PME have limited support:
docs/user-guide/mdrun-performance.rst:  CUDA from the 2022 release and full GPU PME decomposition since the
docs/user-guide/mdrun-performance.rst:  2023 release with CUDA or SYCL (when |Gromacs| is built with
docs/user-guide/mdrun-performance.rst:- LJ PME is not supported on GPUs.
docs/user-guide/mdrun-performance.rst:- When |Gromacs| is built without a GPU FFT library (``-DGMX_GPU_FFT_LIBRARY=none``),
docs/user-guide/mdrun-performance.rst:.. _gmx-gpu-bonded:
docs/user-guide/mdrun-performance.rst:GPU accelerated calculation of bonded interactions (CUDA and SYCL)
docs/user-guide/mdrun-performance.rst:workload to a compatible GPU. This is treated as part of the PP
docs/user-guide/mdrun-performance.rst:a GPU. Typically, there is a performance advantage to offloading
docs/user-guide/mdrun-performance.rst:bonded interactions in particular when the amount of CPU resources per GPU
docs/user-guide/mdrun-performance.rst:cores assigned to a GPU in a run) or when there are other computations on the CPU.
docs/user-guide/mdrun-performance.rst:.. _gmx-gpu-update:
docs/user-guide/mdrun-performance.rst:GPU accelerated calculation of constraints and coordinate update (CUDA and SYCL only)
docs/user-guide/mdrun-performance.rst:.. TODO again, extend this with information on when is GPU update supported
docs/user-guide/mdrun-performance.rst:constraint calculation on a GPU.
docs/user-guide/mdrun-performance.rst:This parallelization mode is referred to as "GPU-resident" as all force and coordinate
docs/user-guide/mdrun-performance.rst:data can remain resident on the GPU for a number of steps (typically between temperature/pressure coupling or
docs/user-guide/mdrun-performance.rst:The GPU-resident mode allows executing all (supported) computation of a simulation step on the GPU. 
docs/user-guide/mdrun-performance.rst:This has the benefit that there is less coupling between CPU host and GPU and
docs/user-guide/mdrun-performance.rst:on typical MD steps data does not need to be transferred between CPU and GPU
docs/user-guide/mdrun-performance.rst:every step between the CPU and GPU.
docs/user-guide/mdrun-performance.rst:The GPU-resident scheme however is still able to carry out part of the computation
docs/user-guide/mdrun-performance.rst:on the CPU concurrently with GPU calculation.
docs/user-guide/mdrun-performance.rst:ported to GPUs. At the same time, it also allows improving performance by making 
docs/user-guide/mdrun-performance.rst:relative performance if the CPU cores paired in a simulation with a GPU.
docs/user-guide/mdrun-performance.rst:GPU-resident mode is enabled by default (when supported) with an automatic
docs/user-guide/mdrun-performance.rst:Using this parallelization mode is typically advantageous in cases where a fast GPU is
docs/user-guide/mdrun-performance.rst:used with a slower CPU, in particular if there is only single simulation assigned to a GPU.
docs/user-guide/mdrun-performance.rst:However, in typical throughput cases where multiple runs are assigned to each GPU,
docs/user-guide/mdrun-performance.rst:Assigning tasks to GPUs
docs/user-guide/mdrun-performance.rst:calculations can be combined on the same or different GPUs, according to the information
docs/user-guide/mdrun-performance.rst:It is possible to assign the calculation of the different computational tasks to the same GPU, meaning
docs/user-guide/mdrun-performance.rst:  Two different types of assignable GPU accelerated tasks are available, (short-range) nonbonded and PME.
docs/user-guide/mdrun-performance.rst:  Each PP rank has a nonbnonded task that can be offloaded to a GPU.
docs/user-guide/mdrun-performance.rst:  PME-only rank), then that task can be offloaded to a GPU. Such a PME
docs/user-guide/mdrun-performance.rst:  task can run wholly on the GPU, or have its latter stages run only on the CPU.
docs/user-guide/mdrun-performance.rst:  Limitations are that PME on GPU does not support PME domain decomposition,
docs/user-guide/mdrun-performance.rst:  so that only one PME task can be offloaded to a single GPU
docs/user-guide/mdrun-performance.rst:  assigned to a separate PME rank, while the nonbonded can be decomposed and offloaded to multiple GPUs.
docs/user-guide/mdrun-performance.rst:  No new assignable GPU tasks are available, but any bonded interactions
docs/user-guide/mdrun-performance.rst:  may run on the same GPU as the short-ranged interactions for a PP task.
docs/user-guide/mdrun-performance.rst:  Update and constraints can run on the same GPU as the short-ranged nonbonded and bonded interactions for a PP task.
docs/user-guide/mdrun-performance.rst:  Communication and auxiliary tasks can also be offloaded in CUDA builds.
docs/user-guide/mdrun-performance.rst:  instead of staging transfers between GPUs though the CPU,
docs/user-guide/mdrun-performance.rst:  direct GPU--GPU communication is possible.
docs/user-guide/mdrun-performance.rst:  which is also offloaded to the GPU.
docs/user-guide/mdrun-performance.rst:  it is also supported using GPU-aware MPI.
docs/user-guide/mdrun-performance.rst:  Direct GPU communication is not enabled by default and can be triggered using
docs/user-guide/mdrun-performance.rst:  the ``GMX_ENABLE_DIRECT_GPU_COMM`` environment variable (will only have an effect
docs/user-guide/mdrun-performance.rst:  Update now runs by default on the GPU with supported simulation settings; note that this is only available with CUDA and SYCL not with OpenCL.
docs/user-guide/mdrun-performance.rst:  PME decomposition support adds additional parallelization-related auxiliary GPU tasks including grid packing and reduction operations
docs/user-guide/mdrun-performance.rst:  as well as distributed GPU FFT computation.
docs/user-guide/mdrun-performance.rst:  Experimental support for CUDA-graphs scheduling has been added, which supports most GPU-resident runs that don't require CPU force computation.
docs/user-guide/mdrun-performance.rst:Performance considerations for GPU tasks
docs/user-guide/mdrun-performance.rst:   have vs the speed and number of GPUs you have.
docs/user-guide/mdrun-performance.rst:#) The GPU-resident parallelization mode (with update/constraints offloaded) is less
docs/user-guide/mdrun-performance.rst:   sensitive to the appropriate CPU-GPU balance than the force-offload mode. 
docs/user-guide/mdrun-performance.rst:#) With slow/old GPUs and/or fast/modern CPUs with many
docs/user-guide/mdrun-performance.rst:   with the GPUs focused on the nonbonded calculation.
docs/user-guide/mdrun-performance.rst:#) With fast/modern GPUs and/or slow/old CPUs with few cores,
docs/user-guide/mdrun-performance.rst:   it generally helps to have the GPU do PME.
docs/user-guide/mdrun-performance.rst:#) Offloading bonded work to a GPU will often not improve simulation performance
docs/user-guide/mdrun-performance.rst:   before the GPU is done with other offloaded work. Therefore,
docs/user-guide/mdrun-performance.rst:   with very few and/or slow CPU cores per GPU, or when the CPU does
docs/user-guide/mdrun-performance.rst:#) On most modern hardware GPU-resident mode (default) is faster than force-offload mode,
docs/user-guide/mdrun-performance.rst:   The only exception may be multi-simulations with a significant number of simulations assigned to each GPU.
docs/user-guide/mdrun-performance.rst:#) Direct GPU communication will in most cases outperform staged communication (both with thread-MPI and MPI).
docs/user-guide/mdrun-performance.rst:   Ideally it should be combined with GPU-resident mode to maximize the benefit.
docs/user-guide/mdrun-performance.rst:.. todo:: we need to be more concrete here, i.e. what machine/software aspects to take into consideration, when will default run mode be using PME-GPU and when will it not, when/how should the user reason about testing different settings than the default.
docs/user-guide/mdrun-performance.rst:Reducing overheads in GPU accelerated runs
docs/user-guide/mdrun-performance.rst:In order for CPU cores and GPU(s) to execute concurrently, tasks are
docs/user-guide/mdrun-performance.rst:launched and executed asynchronously on the GPU(s) while the CPU cores
docs/user-guide/mdrun-performance.rst:Asynchronous task launches are handled by the GPU device driver and
docs/user-guide/mdrun-performance.rst:GPU tasks requires CPU resources that can compete with other CPU tasks
docs/user-guide/mdrun-performance.rst:Delays in CPU execution are caused by the latency of launching GPU tasks,
docs/user-guide/mdrun-performance.rst:The cost of launching GPU work is measured by :ref:`gmx mdrun` and reported in the performance
docs/user-guide/mdrun-performance.rst:summary section of the log file ("Launch PP GPU ops."/"Launch PME GPU ops." rows).
docs/user-guide/mdrun-performance.rst:but in fast-iterating and multi-GPU parallel runs, costs of 10% or larger can be observed.
docs/user-guide/mdrun-performance.rst:and when the CPU is not involved in communication (e.g. with thread-MPI and direct GPU communication enabled) 
docs/user-guide/mdrun-performance.rst:or MPI communication is launched from the CPU (even with GPU-aware MPI), the
docs/user-guide/mdrun-performance.rst:GPU launch cost will compete with other CPU work and therefore represent overheads.
docs/user-guide/mdrun-performance.rst:In OpenCL runs, timing of GPU tasks is by default enabled and,
docs/user-guide/mdrun-performance.rst:In these cases, when more than a few percent of "Launch GPU ops" time is observed,
docs/user-guide/mdrun-performance.rst:it is recommended to turn off timing by setting the ``GMX_DISABLE_GPU_TIMING``
docs/user-guide/mdrun-performance.rst:In parallel runs with many ranks sharing a GPU,
docs/user-guide/mdrun-performance.rst:or MPI ranks per GPU; e.g. most often one rank per thread or core is not optimal.
docs/user-guide/mdrun-performance.rst:The CUDA graphs functionality (added in |Gromacs| 2023) targets reducing such
docs/user-guide/mdrun-performance.rst:overheads and improving GPU work scheduling efficiency and therefore
docs/user-guide/mdrun-performance.rst:running on fast GPUs. Since it is a new feature, in the 2023 release CUDA-graph support
docs/user-guide/mdrun-performance.rst:needs to be triggered using the ``GMX_CUDA_GRAPH`` environment variable.
docs/user-guide/mdrun-performance.rst:The second type of overhead, interference of the GPU runtime or driver with CPU computation,
docs/user-guide/mdrun-performance.rst:is caused by the scheduling and coordination of GPU tasks.
docs/user-guide/mdrun-performance.rst:A separate GPU runtime/driver thread requires CPU resources
docs/user-guide/mdrun-performance.rst:This will leave some CPU resources for the GPU task scheduling
docs/user-guide/mdrun-performance.rst:involves a tradeoff which, with many CPU cores per GPU, may not be significant,
docs/user-guide/mdrun-performance.rst:resource assignment and may outweigh the benefits of reduced GPU scheduling overheads,
docs/user-guide/mdrun-performance.rst:Running the OpenCL version of mdrun
docs/user-guide/mdrun-performance.rst:- GCN-based and CDNA-based AMD GPUs;
docs/user-guide/mdrun-performance.rst:- NVIDIA GPUs prior to Volta;
docs/user-guide/mdrun-performance.rst:- Intel iGPUs.
docs/user-guide/mdrun-performance.rst:Make sure that you have the latest drivers installed. For AMD GPUs,
docs/user-guide/mdrun-performance.rst:the compute-oriented `ROCm`_ stack is recommended;
docs/user-guide/mdrun-performance.rst:alternatively, the AMDGPU-PRO stack is also compatible; using the outdated
docs/user-guide/mdrun-performance.rst:For NVIDIA GPUs, using the proprietary driver is
docs/user-guide/mdrun-performance.rst:provide the OpenCL support.
docs/user-guide/mdrun-performance.rst:For Intel integrated GPUs, the `Neo driver <https://github.com/intel/compute-runtime/releases>`_ is
docs/user-guide/mdrun-performance.rst:The minimum OpenCL version required is |REQUIRED_OPENCL_MIN_VERSION|. See
docs/user-guide/mdrun-performance.rst:also the :ref:`known limitations <opencl-known-limitations>`.
docs/user-guide/mdrun-performance.rst:and regularly tested; NVIDIA Kepler and later (compute capability 3.0)
docs/user-guide/mdrun-performance.rst:The OpenCL GPU kernels are compiled at run time. Hence,
docs/user-guide/mdrun-performance.rst:building the OpenCL program can take a few seconds, introducing a slight
docs/user-guide/mdrun-performance.rst:The same ``-gpu_id`` option (or ``GMX_GPU_ID`` environment variable)
docs/user-guide/mdrun-performance.rst:used to select CUDA or SYCL devices, or to define a mapping of GPUs to PP
docs/user-guide/mdrun-performance.rst:ranks, is used for OpenCL devices.
docs/user-guide/mdrun-performance.rst:Some other :ref:`OpenCL management <opencl-management>` environment
docs/user-guide/mdrun-performance.rst:.. _opencl-known-limitations:
docs/user-guide/mdrun-performance.rst:Known limitations of the OpenCL support
docs/user-guide/mdrun-performance.rst:Limitations in the current OpenCL support of interest to |Gromacs| users:
docs/user-guide/mdrun-performance.rst:- Intel integrated GPUs are supported. Intel CPUs and Xeon Phi are not supported.
docs/user-guide/mdrun-performance.rst:  Set ``-DGMX_GPU_NB_CLUSTER_SIZE=4`` when compiling |Gromacs| to run on consumer
docs/user-guide/mdrun-performance.rst:  Intel GPUs (as opposed to Ponte Vecchio / Data Center Max GPUs).
docs/user-guide/mdrun-performance.rst:  in the NVIDIA OpenCL runtime, with the affected driver versions there is
docs/user-guide/mdrun-performance.rst:  almost no performance gain when using NVIDIA GPUs.
docs/user-guide/mdrun-performance.rst:  The issue affects NVIDIA driver versions up to 349 series, but it
docs/user-guide/mdrun-performance.rst:- On NVIDIA GPUs the OpenCL kernels achieve much lower performance
docs/user-guide/mdrun-performance.rst:  than the equivalent CUDA kernels due to limitations of the NVIDIA OpenCL
docs/user-guide/mdrun-performance.rst:- On the NVIDIA Volta and Turing architectures the OpenCL code is known to produce
docs/user-guide/mdrun-performance.rst:Make sure that you have the latest drivers installed and check the :ref:`installation guide <SYCL GPU acceleration>`
docs/user-guide/mdrun-performance.rst:  - ``SYCL_CACHE_PERSISTENT=1``: enables caching of GPU kernels, reducing :ref:`gmx mdrun` startup time.
docs/user-guide/mdrun-performance.rst:In addition to ``-gpu_id`` option, backend-specific environment variables, like ``SYCL_DEVICE_FILTER``
docs/user-guide/mdrun-performance.rst:or ``ROCR_VISIBLE_DEVICES``, could be used to select GPUs.
docs/user-guide/mdrun-performance.rst:version of the ROCm toolkit and check the :ref:`AMD HIP installation guide <AMD-HIP>`.
docs/user-guide/mdrun-performance.rst:* If you have GPUs that support either CUDA, OpenCL, or SYCL, use them.
docs/user-guide/mdrun-performance.rst:  * Configure with ``-DGMX_GPU=CUDA``, ``-DGMX_GPU=OpenCL``, or ``-DGMX_GPU=SYCL``.
docs/user-guide/mdrun-performance.rst:  * For GPUs, use the newest available SDK for your GPU to take advantage of the
docs/user-guide/mdrun-performance.rst:  * Use a recent GPU driver.
docs/user-guide/mdrun-performance.rst:    However, prefer ``AVX2`` over ``AVX512`` in GPU or highly parallel MPI runs (for more
docs/user-guide/mdrun-performance.rst:  * this is faster, especially with GPUs;
docs/user-guide/mdrun-performance.rst:  * it is necessary to be able to use GPU-resident mode;
docs/user-guide/mdrun-performance.rst:* (Especially) In GPU-resident runs (``-update gpu``):
docs/user-guide/mdrun-performance.rst:    * especially with multi-GPU runs, the automatic increasing of ``nstlist`` at ``mdrun``
docs/user-guide/mdrun-performance.rst:    * odd values of nstlist should be avoided when using CUDA Graphs
docs/user-guide/mdrun-performance.rst:* In multi-GPU runs avoid using as many ranks as cores (or hardware threads) since
docs/user-guide/mdrun-performance.rst:  this introduces a major inefficiency due to overheads associated to GPUs sharing by several MPI ranks.
docs/user-guide/mdrun-performance.rst:  Use at most a few ranks per GPU, 1-3 ranks is generally optimal;
docs/user-guide/mdrun-performance.rst:  with GPU-resident mode and direct GPU communication typically 1 rank/GPU is best.
docs/user-guide/known-issues.rst:Unable to compile with CUDA 11.3
docs/user-guide/known-issues.rst:to compile NVIDIA GPU-enabled |Gromacs| with version 11.3 of the CUDA compiler.
docs/user-guide/known-issues.rst:We recommend using CUDA 11.4 or newer.
docs/user-guide/known-issues.rst:For most cases, we recommend using OpenCL backend (the default) when
docs/user-guide/known-issues.rst:running SYCL build of |Gromacs| on Intel GPUs.
docs/user-guide/known-issues.rst:Unable to build with CUDA 11.5-11.6 and GCC 11 on Ubuntu 22.04
docs/user-guide/known-issues.rst:FFT errors with NVIDIA RTX 40xx-series GPUs and CUDA 11.7 or earlier
docs/user-guide/known-issues.rst:cuFFT library only has full support for RTX 40xx GPUs since version 11.8.
docs/user-guide/known-issues.rst:If you are using older CUDA, you might encounter ``cufftPlanMany R2C plan failure``
docs/user-guide/known-issues.rst:error when running a simulation with PME on such a GPU.
docs/user-guide/known-issues.rst:To resolve, upgrade to CUDA 11.8 or 12.x.
docs/user-guide/known-issues.rst:"Cannot find a working standard library" error with ROCm Clang
docs/user-guide/known-issues.rst:To work around the issue, either avoid ``-update gpu`` (so that it
docs/user-guide/known-issues.rst:Launching multiple instances of GROMACS on the same machine with AMD GPUs
docs/user-guide/known-issues.rst:When |Gromacs| is built with AdaptiveCpp 23.10 or earlier for AMD GPUs,
docs/user-guide/known-issues.rst:launching more than 4 instances of |Gromacs| (even on different GPUs)
docs/user-guide/known-issues.rst:GPU using ``ROCR_VISIBLE_DEVICES`` environment variable. This is already
docs/user-guide/managing-simulations.rst:processors without GPUs (see next section) would lead to binary
docs/user-guide/managing-simulations.rst:* On GPUs, the reduction of e.g. non-bonded forces has a non-deterministic
docs/user-guide/mdp-options.rst:   potential energies to the energy file (not supported on GPUs)
docs/user-guide/mdp-options.rst:      force calculation on the GPU, a value of 20 or 40 often gives
docs/user-guide/mdp-options.rst:   GPUs only support the value 4.
docs/user-guide/mdrun-features.rst:    mpirun -np 32 gmx_mpi mdrun -multidir a b c d -gputasks 0000000011111111
docs/user-guide/mdrun-features.rst:physical nodes and two GPUs per node, there will be 16 MPI ranks per
docs/user-guide/mdrun-features.rst:on a node are mapped to the GPUs with IDs 0 and 1, even though they
docs/user-guide/mdrun-features.rst:GPU. However, the order ``0101010101010101`` could run faster.
docs/install-guide/exotic.rst:SYCL GPU acceleration for AMD and NVIDIA GPUs using Intel oneAPI DPC++
docs/install-guide/exotic.rst:AMD and NVIDIA GPUs can also be used with Intel oneAPI BaseKit and Codeplay oneAPI plugins.
docs/install-guide/exotic.rst:For most users, we recommend using :ref:`CUDA <CUDA GPU acceleration>` for NVIDIA GPUs and
docs/install-guide/exotic.rst::ref:`AdaptiveCpp <SYCL GPU acceleration AMD>` for AMD GPUs instead.
docs/install-guide/exotic.rst:AMD GPUs
docs/install-guide/exotic.rst:After installing Intel oneAPI toolkit 2023.0 or newer, a compatible ROCm version,
docs/install-guide/exotic.rst:            -DGMX_GPU=SYCL -DGMX_SYCL=DPCPP \
docs/install-guide/exotic.rst:            -DGMX_GPU_NB_CLUSTER_SIZE=8 -DGMX_GPU_FFT_LIBRARY=vkfft \
docs/install-guide/exotic.rst:NVIDIA GPUs
docs/install-guide/exotic.rst:After installing Intel oneAPI toolkit 2023.0 or newer, a compatible CUDA version,
docs/install-guide/exotic.rst:and the `Codeplay plugin <https://developer.codeplay.com/products/oneapi/nvidia/home/>`__,
docs/install-guide/exotic.rst:            -DGMX_GPU=SYCL -DGMX_SYCL=DPCPP \
docs/install-guide/exotic.rst:            -DGMX_GPU_NB_CLUSTER_SIZE=8 -DGMX_GPU_FFT_LIBRARY=vkfft \
docs/install-guide/exotic.rst:            -DSYCL_CXX_FLAGS_EXTRA=-fsycl-targets=nvptx64-nvidia-cuda
docs/install-guide/exotic.rst:For more recent NVIDIA GPUs, compiling for a specific compute capability can be
docs/install-guide/exotic.rst:For example for an Ampere architecture GPU such as the NVIDIA A100, set
docs/install-guide/exotic.rst:``-DSYCL_CXX_FLAGS_EXTRA=-fsycl-targets=nvidia_gpu_sm_80``.
docs/install-guide/exotic.rst:SYCL GPU acceleration for NVIDIA GPUs using AdaptiveCpp (hipSYCL)
docs/install-guide/exotic.rst:For most users, we recommend using :ref:`CUDA <CUDA GPU acceleration>` for NVIDIA GPUs.
docs/install-guide/exotic.rst:Build and install AdaptiveCpp_ with CUDA backend (we recommend using the mainline Clang, not the ROCm-bundled one).
docs/install-guide/exotic.rst:Then, use the following command to build |Gromacs| (make sure to use the same compiler and set target GPU architecture
docs/install-guide/exotic.rst:            -DGMX_GPU=SYCL -DGMX_SYCL=ACPP -DHIPSYCL_TARGETS='cuda:sm_XY'
docs/install-guide/index.rst:* ``-DGMX_GPU=CUDA`` to build with NVIDIA CUDA support enabled.
docs/install-guide/index.rst:* ``-DGMX_GPU=OpenCL`` to build with OpenCL_ support enabled.
docs/install-guide/index.rst:* ``-DGMX_GPU=SYCL`` to build with SYCL_ support enabled (using `Intel oneAPI DPC++`_ by default).
docs/install-guide/index.rst:* ``-DGMX_SYCL=ACPP`` to build with SYCL_ support using AdaptiveCpp_ (hipSYCL), requires ``-DGMX_GPU=SYCL``.
docs/install-guide/index.rst:.. _gmx-gpu-support:
docs/install-guide/index.rst:GPU support
docs/install-guide/index.rst:|Gromacs| has excellent support for NVIDIA GPUs supported via CUDA.
docs/install-guide/index.rst:On Linux, NVIDIA CUDA_ toolkit with minimum version |GMX_CUDA_MINIMUM_REQUIRED_VERSION|
docs/install-guide/index.rst:is required, and the latest version is strongly encouraged. NVIDIA GPUs with at
docs/install-guide/index.rst:least NVIDIA compute capability |GMX_CUDA_MINIMUM_REQUIRED_COMPUTE_CAPABILITY| are
docs/install-guide/index.rst:get the latest CUDA version and driver that supports your hardware, but
docs/install-guide/index.rst:beware of possible performance regressions in newer CUDA versions on
docs/install-guide/index.rst:While some CUDA compilers (nvcc) might not
docs/install-guide/index.rst:OpenCL_ support as a portable GPU backend. The minimum OpenCL version required is
docs/install-guide/index.rst:|REQUIRED_OPENCL_MIN_VERSION| and only 64-bit implementations are supported.
docs/install-guide/index.rst:The current OpenCL implementation is recommended for
docs/install-guide/index.rst:use with GCN-based AMD GPUs, and on Linux we recommend the ROCm runtime.
docs/install-guide/index.rst:Intel integrated GPUs are supported with the Neo drivers.
docs/install-guide/index.rst:OpenCL is also supported with NVIDIA GPUs, but using
docs/install-guide/index.rst:the latest NVIDIA driver (which includes the NVIDIA OpenCL runtime) is
docs/install-guide/index.rst:to the NVIDIA OpenCL runtime).
docs/install-guide/index.rst:It is not possible to support both Intel and other vendors' GPUs with OpenCL.
docs/install-guide/index.rst:A 64-bit implementation of OpenCL is required and therefore OpenCL is only
docs/install-guide/index.rst:Please note that OpenCL backend does not support the following GPUs:
docs/install-guide/index.rst:* NVIDIA Volta (CC 7.0, e.g., Tesla V100 or GTX 1630) or newer,
docs/install-guide/index.rst:near feature parity with the CUDA backend as well as broad platform support
docs/install-guide/index.rst:in both aspects more versatile than the OpenCL_ backend
docs/install-guide/index.rst:(notable exception is the Apple Silicon GPU which is only supported in OpenCL).
docs/install-guide/index.rst:compiler for Intel GPUs, or with AdaptiveCpp_ compiler and ROCm runtime for
docs/install-guide/index.rst:AMD GPUs (GFX9, CDNA 1/2, and RDNA1/2/3). Using other devices supported by
docs/install-guide/index.rst:It is not possible to configure several GPU backends in the same build
docs/install-guide/index.rst:GPU-aware MPI support
docs/install-guide/index.rst:In simulations using multiple GPUs, an MPI implementation with GPU support
docs/install-guide/index.rst:distinct GPU memory spaces without staging through CPU memory, often
docs/install-guide/index.rst:current support for this in |Gromacs| is with a CUDA build targeting
docs/install-guide/index.rst:Nvidia GPUs using "CUDA-aware" MPI libraries.  For
docs/install-guide/index.rst:more details, see `Introduction to CUDA-aware MPI
docs/install-guide/index.rst:<https://developer.nvidia.com/blog/introduction-cuda-aware-mpi/>`_.
docs/install-guide/index.rst:To use CUDA-aware MPI for direct GPU communication we recommend
docs/install-guide/index.rst:(>=1.10), since most |Gromacs| internal testing on CUDA-aware support has
docs/install-guide/index.rst:been performed using these versions. OpenMPI with CUDA-aware support can
docs/install-guide/index.rst:<https://www.open-mpi.org/faq/?category=buildcuda>`_.
docs/install-guide/index.rst:For GPU-aware MPI support of Intel GPUs, use Intel MPI no earlier than
docs/install-guide/index.rst:(setting environment variable ``ONEAPI_DEVICE_SELECTOR=level_zero:gpu`` will typically suffice)
docs/install-guide/index.rst:and GPU-aware support in the MPI runtime `selected
docs/install-guide/index.rst:<https://www.intel.com/content/www/us/en/develop/documentation/mpi-developer-reference-linux/top/environment-variable-reference/gpu-support.html>`_.
docs/install-guide/index.rst:For GPU-aware MPI support on AMD GPUs, several MPI implementations with UCX support
docs/install-guide/index.rst:Other MPI flavors such as Cray MPICH are also GPU-aware and compatible with ROCm.
docs/install-guide/index.rst:With ``GMX_MPI=ON``, |Gromacs| attempts to automatically detect GPU support
docs/install-guide/index.rst:in the underlying MPI library at compile time, and enables direct GPU
docs/install-guide/index.rst:|Gromacs| may fail to detect existing GPU-aware MPI support, in which case
docs/install-guide/index.rst:it can be manually enabled by setting environment variable ``GMX_FORCE_GPU_AWARE_MPI=1``
docs/install-guide/index.rst:With PME GPU offload support using CUDA, a GPU-based FFT library
docs/install-guide/index.rst:is required. The CUDA-based GPU FFT library cuFFT is part of the
docs/install-guide/index.rst:CUDA toolkit (required for all CUDA builds) and therefore no additional
docs/install-guide/index.rst:software component is needed when building with CUDA GPU acceleration.
docs/install-guide/index.rst:To target either Intel CPUs or GPUs, use OneAPI MKL (>=2021.3) by setting up the environment,
docs/install-guide/index.rst:Then run CMake with setting ``-DGMX_FFT_LIBRARY=mkl`` and/or ``-DGMX_GPU_FFT_LIBRARY=mkl``.
docs/install-guide/index.rst:or closed-source oneMKL using Intel DPC++ and Codeplay's plugins for NVIDIA and AMD GPUs.
docs/install-guide/index.rst:To use, Intel DPC++ must be installed (>= 2023.2.0), along with Codeplay's plugins for NVIDIA
docs/install-guide/index.rst:and AMD GPUs as required, and CUDA and/or ROCm as required. The enviroment should be initialized
docs/install-guide/index.rst:Then, when building |Gromacs|, set ``-DGMX_GPU_FFT_LIBRARY=ONEMKL``.
docs/install-guide/index.rst:Generally MKL will provide better performance on Intel GPUs, however
docs/install-guide/index.rst:     cmake -DGMX_GPU_FFT_LIBRARY=BBFFT \
docs/install-guide/index.rst:Decomposition of PME work to multiple GPUs is supported with NVIDIA
docs/install-guide/index.rst:GPUs when using a CUDA build. This requires building |Gromacs| with
docs/install-guide/index.rst:the NVIDIA `cuFFTMp (cuFFT Multi-process) library
docs/install-guide/index.rst:<https://docs.nvidia.com/hpc-sdk/cufftmp>`_, shipped with the NVIDIA
docs/install-guide/index.rst:          -DcuFFTMp_ROOT=<path to NVIDIA HPC SDK math_libs folder>
docs/install-guide/index.rst:<https://docs.nvidia.com/hpc-sdk/cufftmp/usage/requirements.html>`_
docs/install-guide/index.rst:are met before trying to use GPU PME decomposition feature.  In
docs/install-guide/index.rst:<https://developer.nvidia.com/nvshmem>`_, and it is vital that the
docs/install-guide/index.rst:the NVIDIA HPC SDK include two versions of NVSHMEM, where the cuFFTMp
docs/install-guide/index.rst:``Linux_x86_64/<SDK_version>/comm_libs/<CUDA_version>/nvshmem_cufftmp_compat``. If
docs/install-guide/index.rst:``Linux_x86_64/<SDK_version>/comm_libs/<CUDA_version>/nvshmem``. The
docs/install-guide/index.rst:<https://docs.nvidia.com/hpc-sdk/nvshmem/api/faq.html#general-faqs>`_ for
docs/install-guide/index.rst:Decomposition of PME work to multiple GPUs is supported with PME
docs/install-guide/index.rst:offloaded to any vendor's GPU when building |Gromacs| linked to the
docs/install-guide/index.rst:`heFFTe library <https://icl.utk.edu/fft/>`_. HeFFTe uses GPU-aware MPI
docs/install-guide/index.rst:nodes. It requires a CUDA build to target NVIDIA GPUs and a SYCL build
docs/install-guide/index.rst:to target Intel or AMD GPUs. To enable heFFTe support, use the
docs/install-guide/index.rst:GPU-aware MPI library that will be used by |Gromacs|, and with support
docs/install-guide/index.rst:C++ compiler and standard library also. When targeting Intel GPUs, add
docs/install-guide/index.rst:folder>``. When targeting AMD GPUs, add ``-DHeffte_ENABLE_ROCM=ON
docs/install-guide/index.rst:-DHeffte_ROCM_ROOT=<path to ROCm folder>``.
docs/install-guide/index.rst:`VkFFT <https://github.com/DTolm/VkFFT>`_ is a multi-backend GPU-accelerated multidimensional
docs/install-guide/index.rst:|Gromacs| includes VkFFT support with two goals: portability across GPU platforms
docs/install-guide/index.rst:and performance improvements. VkFFT can be used with OpenCL and SYCL backends:
docs/install-guide/index.rst:  NVIDIA GPUs with AdaptiveCpp_ and `Intel oneAPI DPC++`_; it generally outperforms rocFFT hence it is recommended as
docs/install-guide/index.rst:* For OpenCL builds, VkFFT provides an alternative to ClFFT. It is
docs/install-guide/index.rst:        cmake -DGMX_GPU_FFT_LIBRARY=VKFFT
docs/install-guide/index.rst:        cmake -DGMX_GPU_FFT_LIBRARY=VKFFT \
docs/install-guide/index.rst:    cmake .. -DGMX_GPU=CUDA -DGMX_MPI=ON \
docs/install-guide/index.rst:can be used to build with CUDA GPUs, MPI and install in a custom
docs/install-guide/index.rst:   the default ``AVX2_128``. With GPU offload, however, ``AVX2_256``
docs/install-guide/index.rst:   Additionally, with GPU accelerated runs ``AVX2_256`` can also be
docs/install-guide/index.rst:11. ``ARM_NEON_ASIMD`` 64-bit ARMv8 and later. For maximum performance on NVIDIA 
docs/install-guide/index.rst:    CMAKE_PREFIX_PATH=/opt/fftw:/opt/cuda cmake ..
docs/install-guide/index.rst:``-DCMAKE_PREFIX_PATH=/opt/fftw:/opt/cuda``.
docs/install-guide/index.rst:.. _CUDA GPU acceleration:
docs/install-guide/index.rst:CUDA GPU acceleration
docs/install-guide/index.rst:If you have the CUDA_ Toolkit installed, you can use ``cmake`` with:
docs/install-guide/index.rst:    cmake .. -DGMX_GPU=CUDA -DCUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda
docs/install-guide/index.rst:e.g. with the advanced option ``CUDA_HOST_COMPILER``.
docs/install-guide/index.rst:By default, code will be generated for the most common CUDA architectures.
docs/install-guide/index.rst:can result in the default build not being able to use some GPUs.
docs/install-guide/index.rst:binary size and build time, you can alter the target CUDA architectures.
docs/install-guide/index.rst:This can be done either with the ``GMX_CUDA_TARGET_SM`` or
docs/install-guide/index.rst:``GMX_CUDA_TARGET_COMPUTE`` CMake variables, which take a semicolon delimited
docs/install-guide/index.rst:string with the two digit suffixes of CUDA (virtual) architectures names, for
docs/install-guide/index.rst:instance "60;75;86". For details, see the `"Options for steering GPU
docs/install-guide/index.rst:<https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/#options-for-steering-cuda-compilation>`_.
docs/install-guide/index.rst:The GPU acceleration has been tested on AMD64/x86-64 platforms with
docs/install-guide/index.rst:Experimental support is available for compiling CUDA code, both for host and
docs/install-guide/index.rst:A CUDA toolkit is still required but it is used only for GPU device code
docs/install-guide/index.rst:generation and to link against the CUDA runtime library.
docs/install-guide/index.rst:The clang CUDA support simplifies compilation and provides benefits for development
docs/install-guide/index.rst:(e.g. allows the use code sanitizers in CUDA host-code).
docs/install-guide/index.rst:Additionally, using clang for both CPU and GPU compilation can be beneficial
docs/install-guide/index.rst:to avoid compatibility issues between the GNU toolchain and the CUDA toolkit.
docs/install-guide/index.rst:clang for CUDA can be triggered using the ``GMX_CLANG_CUDA=ON`` CMake option.
docs/install-guide/index.rst:Target architectures can be selected with  ``GMX_CUDA_TARGET_SM``,
docs/install-guide/index.rst:(hence GMX_CUDA_TARGET_COMPUTE is ignored).
docs/install-guide/index.rst:OpenCL GPU acceleration
docs/install-guide/index.rst:The primary targets of the |Gromacs| OpenCL support is accelerating
docs/install-guide/index.rst:discrete GPUs and APUs (integrated CPU+GPU chips), and for Intel we
docs/install-guide/index.rst:target the integrated GPUs found on modern workstation and mobile
docs/install-guide/index.rst:hardware. The |Gromacs| OpenCL on NVIDIA GPUs works, but performance
docs/install-guide/index.rst:To build |Gromacs| with OpenCL_ support enabled, two components are
docs/install-guide/index.rst:required: the OpenCL_ headers and the wrapper library that acts
docs/install-guide/index.rst:The additional, runtime-only dependency is the vendor-specific GPU driver
docs/install-guide/index.rst:for the device targeted. This also contains the OpenCL_ compiler.
docs/install-guide/index.rst:As the GPU compute kernels are compiled  on-demand at run time,
docs/install-guide/index.rst:repositories (e.g. ``opencl-headers`` and ``ocl-icd-libopencl1`` on Debian/Ubuntu).
docs/install-guide/index.rst:Only the compatibility with the required OpenCL_ version |REQUIRED_OPENCL_MIN_VERSION|
docs/install-guide/index.rst:To trigger an OpenCL_ build the following CMake flags must be set
docs/install-guide/index.rst:    cmake .. -DGMX_GPU=OpenCL
docs/install-guide/index.rst:To build with support for Intel integrated GPUs, it is required
docs/install-guide/index.rst:to add ``-DGMX_GPU_NB_CLUSTER_SIZE=4`` to the cmake command line,
docs/install-guide/index.rst:so that the GPU kernels match the characteristics of the hardware.
docs/install-guide/index.rst:On Mac OS, an AMD GPU can be used only with OS version 10.10.4 and
docs/install-guide/index.rst:    cmake .. -DGMX_GPU=OpenCL -DclFFT_ROOT_DIR=/path/to/your/clFFT \
docs/install-guide/index.rst:.. _SYCL GPU acceleration:
docs/install-guide/index.rst:SYCL GPU acceleration
docs/install-guide/index.rst:implementations targeting different hardware platforms (similar to OpenCL_).
docs/install-guide/index.rst:* Intel GPUs using `Intel oneAPI DPC++`_ (both OpenCL and LevelZero backends),
docs/install-guide/index.rst:* AMD GPUs with AdaptiveCpp_ (previously known as hipSYCL),
docs/install-guide/index.rst:* AMD GPUs with oneAPI with `Codeplay AMD plugin <https://developer.codeplay.com/products/oneapi/amd/home/>`_,
docs/install-guide/index.rst:* NVIDIA GPUs with either AdaptiveCpp_ or oneAPI with `Codeplay NVIDIA plugin <https://developer.codeplay.com/products/oneapi/nvidia/home/>`_.
docs/install-guide/index.rst:GPU vendor  AdaptiveCpp_ (hipSYCL)  `Intel oneAPI DPC++`_
docs/install-guide/index.rst:NVIDIA      experimental            experimental (requires `Codeplay plugin <https://developer.codeplay.com/products/oneapi/nvidia/home/>`__)
docs/install-guide/index.rst:The SYCL_ support in |Gromacs| is intended to replace OpenCL_ as an acceleration mechanism for AMD and Intel hardware.
docs/install-guide/index.rst:For NVIDIA GPUs, we strongly advise using CUDA.
docs/install-guide/index.rst:Apple M1/M2 GPUs are not supported with SYCL but can be used with OpenCL_.
docs/install-guide/index.rst:are less mature than either OpenCL or CUDA. Please, pay extra attention
docs/install-guide/index.rst:SYCL GPU acceleration for Intel GPUs
docs/install-guide/index.rst:            -DGMX_GPU=SYCL -DGMX_SYCL=DPCPP
docs/install-guide/index.rst:When compiling for Intel Data Center GPU Max (also knows as Ponte Vecchio / PVC),
docs/install-guide/index.rst:            -DGMX_GPU=SYCL -DGMX_SYCL=DPCPP \
docs/install-guide/index.rst:            -DGMX_GPU_NB_NUM_CLUSTER_PER_CELL_X=1 -DGMX_GPU_NB_CLUSTER_SIZE=8
docs/install-guide/index.rst:.. _SYCL GPU acceleration AMD:
docs/install-guide/index.rst:SYCL GPU acceleration for AMD GPUs
docs/install-guide/index.rst:and ROCm 5.3-5.7 is recommended. The earliest supported version is hipSYCL 0.9.4.
docs/install-guide/index.rst:with ROCm for building both AdaptiveCpp and |Gromacs|. Mainline Clang releases can also work.
docs/install-guide/index.rst:that the proper Clang is used (assuming ``ROCM_PATH``
docs/install-guide/index.rst:is set correctly, e.g. to ``/opt/rocm`` in the case of default installation):
docs/install-guide/index.rst:   cmake .. -DCMAKE_C_COMPILER=${ROCM_PATH}/llvm/bin/clang \
docs/install-guide/index.rst:            -DCMAKE_CXX_COMPILER=${ROCM_PATH}/llvm/bin/clang++ \
docs/install-guide/index.rst:            -DLLVM_DIR=${ROCM_PATH}/llvm/lib/cmake/llvm/
docs/install-guide/index.rst:If ROCm 5.0 or earlier is used, AdaptiveCpp might require
docs/install-guide/index.rst:`additional build flags <https://github.com/AdaptiveCpp/AdaptiveCpp/blob/v0.9.4/doc/install-rocm.md>`_.
docs/install-guide/index.rst:Using hipSYCL 0.9.4 with ROCm 5.7+ / Clang 17+ might also require
docs/install-guide/index.rst:   cmake .. -DCMAKE_C_COMPILER=${ROCM_PATH}/llvm/bin/clang \
docs/install-guide/index.rst:            -DCMAKE_CXX_COMPILER=${ROCM_PATH}/llvm/bin/clang++ \
docs/install-guide/index.rst:            -DGMX_GPU=SYCL -DGMX_SYCL=ACPP -DHIPSYCL_TARGETS='hip:gfxXYZ'
docs/install-guide/index.rst:If you have multiple AMD GPUs of different generations in the same system
docs/install-guide/index.rst:(e.g., integrated APU and a discrete GPU) the ROCm runtime requires code to be available
docs/install-guide/index.rst:when compiling to avoid ROCm crashes at initialization.
docs/install-guide/index.rst:By default, `VkFFT <https://github.com/DTolm/VkFFT>`_  is used to perform FFT on GPU.
docs/install-guide/index.rst:You can switch to rocFFT by passing ``-DGMX_GPU_FFT_LIBRARY=rocFFT`` CMake flag.
docs/install-guide/index.rst:on most consumer GPUs.
docs/install-guide/index.rst:AMD GPUs can also be targeted via `Intel oneAPI DPC++`_; please refer to
docs/install-guide/index.rst:SYCL GPU compilation options
docs/install-guide/index.rst:``-DGMX_GPU_NB_CLUSTER_SIZE``
docs/install-guide/index.rst:      most Intel GPUs except Data Center MAX (Ponte Vecchio), for which 8
docs/install-guide/index.rst:      which is the only supported value for AMD and NVIDIA devices.
docs/install-guide/index.rst:``-DGMX_GPU_NB_NUM_CLUSTER_PER_CELL_X``, ``-DGMX_GPU_NB_NUM_CLUSTER_PER_CELL_Y``, ``-DGMX_GPU_NB_NUM_CLUSTER_PER_CELL_Z``
docs/install-guide/index.rst:      grid cell, default 2. When targeting Intel Ponte Vecchio GPUs,
docs/install-guide/index.rst:      set ``-DGMX_GPU_NB_NUM_CLUSTER_PER_CELL_X=1`` and leave the
docs/install-guide/index.rst:``-DGMX_GPU_NB_DISABLE_CLUSTER_PAIR_SPLIT``
docs/install-guide/index.rst:     Disables cluster pair splitting in the GPU non-bonded kernels.
docs/install-guide/index.rst:     on GPUs with 64-wide execution like AMD GCN and CDNA family.
docs/install-guide/index.rst:     This option is automatically enabled in all builds that target GCN or CDNA GPUs (but not RDNA).
docs/install-guide/index.rst:AMD HIP GPU acceleration
docs/install-guide/index.rst:HIP is the AMD interoperability layer for the `ROCm`_ toolkit used to target AMD devices.
docs/install-guide/index.rst:In order to use HIP as the device backend, you need to have the `ROCm`_ toolkit installed, including
docs/install-guide/index.rst:the `rocPrim`_ libraries. The minimum version required by |Gromacs| is ROCm 5.2, but we recommend a
docs/install-guide/index.rst:   cmake .. -DCMAKE_HIP_COMPILER=${ROCM_PATH}/bin/amdclang++ \
docs/install-guide/index.rst:            -DCMAKE_PREFIX_PATH=${ROCM_PATH} \
docs/install-guide/index.rst:            -DGMX_GPU=HIP
docs/install-guide/index.rst:``-DGMX_GPU_NB_DISABLE_CLUSTER_PAIR_SPLIT=ON``
docs/install-guide/index.rst:Portability of binaries across GPUs is generally better, targeting
docs/install-guide/index.rst:multiple generations of GPUs from the same vendor is in most cases
docs/install-guide/index.rst:CUDA_ builds will by default be able to run on any NVIDIA GPU
docs/install-guide/index.rst:supported by the CUDA toolkit used since the |Gromacs| build
docs/install-guide/index.rst:With SYCL_ multiple target architectures of the same GPU vendor
docs/install-guide/index.rst:can be selected when using AdaptiveCpp_ (i.e. only AMD or only NVIDIA).
docs/install-guide/index.rst:With OpenCL_, due to just-in-time compilation of GPU code for
docs/install-guide/index.rst:NVIDIA Grace
docs/install-guide/index.rst:CUDA versions 11.0 and 11.7,
docs/install-guide/index.rst:hipSYCL 0.9.4 with ROCm 5.3,
docs/install-guide/index.rst:which uses GitLab runner on a local k8s x86 cluster with NVIDIA,
docs/install-guide/index.rst:AMD, and Intel GPU support.
docs/release-notes/2026/major/highlights.rst:without GPUs, all enabled and automated by default. In addition,
docs/release-notes/2021/2021.2.rst:Removed a potential race condition with GPU update
docs/release-notes/2021/2021.2.rst:using GPU update with dipole moment calculation.
docs/release-notes/2021/2021.2.rst:Fix Apple OpenCL build
docs/release-notes/2021/2021.2.rst:Allow PME on CPU in runs with domain decomposition and GPU update
docs/release-notes/2021/2021.2.rst:decomposition and GPU update to use the CPU for PME (as long as combined
docs/release-notes/2021/2021.5.rst:The linear dHdL contribution from PME, when PME was calculated on GPU or on a separate PME
docs/release-notes/2021/2021.5.rst:with PME on GPU or using a separate PME rank.
docs/release-notes/2021/2021.5.rst:Performance improvements when running on Ampere-class Nvidia GPUs
docs/release-notes/2021/2021.6.rst:Fix missing synchronization in CUDA update kernels
docs/release-notes/2021/2021.6.rst:When using GPU update with SETTLE or LINCS constraints, virial calculations
docs/release-notes/2021/2021.6.rst:could have been incorrect on Volta and newer NVIDIA GPUs, which in turn
docs/release-notes/2021/2021.6.rst:would lead to incorrect pressure. The GPU update is not enabled by default,
docs/release-notes/2021/2021.6.rst:- Look for the line "GPU support:        CUDA";
docs/release-notes/2021/2021.6.rst:- Look for the line "PP task will update and constrain coordinates on the GPU";
docs/release-notes/2021/2021.6.rst:- Check whether any GPU the value of "compute cap." 7.0 or higher in the "GPU Info:" section.
docs/release-notes/2021/2021.6.rst:that allow offloading of the update and constraint calculations to GPUs are affected.
docs/release-notes/2021/2021.7.rst:Add missing net charge term when running PME on a GPU
docs/release-notes/2021/2021.7.rst:When PME was running on a GPU, the term due to a net charge of the system was missing.
docs/release-notes/2021/major/deprecated-functionality.rst:OpenCL to be removed as a GPU framework
docs/release-notes/2021/major/deprecated-functionality.rst::issue:`3818` Work is underway for ports to AMD and Intel GPUs, and it
docs/release-notes/2021/major/deprecated-functionality.rst:OpenCL port. Nvidia GPUs are targeted by the CUDA port, and no changes
docs/release-notes/2021/major/deprecated-functionality.rst:of an emerging GPU vendor in HPC needing OpenCL support, we will
docs/release-notes/2021/major/deprecated-functionality.rst:remove the OpenCL port once AMD and Intel support is established in
docs/release-notes/2021/major/bugs-fixed.rst:The exclusion forces in CUDA and OpenCL kernels were computed incorrectly
docs/release-notes/2021/major/performance.rst:Extend supported use-cases for GPU version of update and constraints
docs/release-notes/2021/major/performance.rst:GPU version of update and constraints can now be used for FEP, except mass and constraints
docs/release-notes/2021/major/performance.rst:Support for offloading PME to GPU when doing Coulomb FEP
docs/release-notes/2021/major/performance.rst:PME calculations can be offloaded to GPU when doing Coulomb free-energy perturbations.
docs/release-notes/2021/major/performance.rst:Allow offloading GPU update and constraints without direct GPU communication
docs/release-notes/2021/major/performance.rst:constraints to a GPU with CUDA without requiring the (experimental) direct GPU
docs/release-notes/2021/major/performance.rst:Tune CUDA short-range nonbonded kernel parameters on NVIDIA Volta and Ampere A100
docs/release-notes/2021/major/performance.rst:Recent compilers allowed re-tuning the nonbonded kernel defaults on NVIDIA Volta and
docs/release-notes/2021/major/performance.rst:Ampere A100GPUs which improves performance of the Ewald kernels, especially those that
docs/release-notes/2021/major/miscellaneous.rst:Uniform and manual CMake GPU-support configuration
docs/release-notes/2021/major/miscellaneous.rst:The GPU accelerations setup has been changed to be uniform for CUDA and OpenCL. Either
docs/release-notes/2021/major/miscellaneous.rst:option is now enabled by setting GMX_GPU to CUDA or OpenCL in the CMake configuration.
docs/release-notes/2021/major/miscellaneous.rst:based on the build host. In particular, this means that CUDA will not be enabled unless
docs/release-notes/2021/major/miscellaneous.rst:the GMX_GPU option is explicitly enabled, and CMake will no longer perform the extra
docs/release-notes/2021/major/miscellaneous.rst:steps of trying to detect hardware and propose to install CUDA if hardware is available.
docs/release-notes/2021/major/miscellaneous.rst:different accelerator APIs targeting e.g. NVIDIA hardware.
docs/release-notes/2021/major/miscellaneous.rst:Unification of several CUDA and OpenCL environment variables
docs/release-notes/2021/major/miscellaneous.rst:The environment variables that had exactly the same meaning in OpenCL and CUDA were unified:
docs/release-notes/2021/major/miscellaneous.rst:* GMX_CUDA_NB_ANA_EWALD and GMX_OCL_NB_ANA_EWALD into GMX_GPU_NB_ANA_EWALD
docs/release-notes/2021/major/miscellaneous.rst:* GMX_CUDA_NB_TAB_EWALD and GMX_OCL_NB_TAB_EWALD into GMX_GPU_NB_TAB_EWALD
docs/release-notes/2021/major/miscellaneous.rst:* GMX_CUDA_NB_EWALD_TWINCUT and GMX_OCL_NB_EWALD_TWINCUT into GMX_GPU_NB_EWALD_TWINCUT
docs/release-notes/2021/major/highlights.rst:without GPUs, all enabled and automated by default. In addition,
docs/release-notes/2021/major/highlights.rst:* Support PME offloading to GPU for free energy simulations
docs/release-notes/2021/2021.4.rst:Fixed bug with GPU LINCS occasionally shifting atoms in wrong direction
docs/release-notes/2021/2021.4.rst:Due to missing blocking synchronizations in the CUDA version of LINCS,
docs/release-notes/2021/2021.4.rst:The use of Mixed mode PME (``-pme gpu -pmefft cpu``) led to incorrect
docs/release-notes/2025/major/performance.rst:when running on GPU and slightly reducing the CPU usage when using
docs/release-notes/2025/major/portability.rst:Added support to compile |Gromacs| using AMD HIP as GPU backend
docs/release-notes/2025/major/portability.rst:It is now possible to use AMD HIP directly as the GPU backend to run
docs/release-notes/2025/major/portability.rst:Added support for the oneMKL interface library for GPU FFTs
docs/release-notes/2025/major/portability.rst:This enables cross-vendor support for GPU FFTs to the |Gromacs|
docs/release-notes/2025/major/portability.rst:Intel DPC++ and Codeplay's plugins for NVIDIA and AMD GPUs.
docs/release-notes/2025/major/highlights.rst:without GPUs, all enabled and automated by default. In addition,
docs/release-notes/2020/2020.4.rst:Bug fix for the GPU version of LINCS in multiple domain case
docs/release-notes/2020/2020.4.rst:CUDA 11.0 supported
docs/release-notes/2020/2020.4.rst:A build with CUDA 11.0 now configures and passes tests.
docs/release-notes/2020/2020.4.rst:Building with CUDA 11.0 means that hardware with CC 3.0 is no longer supported,
docs/release-notes/2020/2020.3.rst:Updated message on using GPU with non-dynamical integrator
docs/release-notes/2020/2020.3.rst:The GPU implementation of PME and bonded forces requires dynamical integrator.
docs/release-notes/2020/2020.3.rst:The message that informs user why using GPU for PME or bonded forces is not
docs/release-notes/2020/2020.7.rst:Fixed bug with GPU LINCS occasionally shifting atoms in wrong direction
docs/release-notes/2020/2020.7.rst:Due to missing blocking synchronizations in the CUDA version of LINCS,
docs/release-notes/2020/2020.2.rst:The velocities of the center of mass are now removed correctly in case of -update gpu
docs/release-notes/2020/2020.2.rst:In case of GPU update, they should be copied back to the GPU memory after they were updated
docs/release-notes/2020/major/performance.rst:performance when non-perturbed non-bondeds are offloaded to a GPU. In that case
docs/release-notes/2020/major/performance.rst:Update and constraints can run on a GPU
docs/release-notes/2020/major/performance.rst:update and constraints can be offloaded to a GPU with CUDA. Thus all compute
docs/release-notes/2020/major/performance.rst:better performance when using a fast GPU combined with a slow CPU.
docs/release-notes/2020/major/performance.rst:By default, update will run on the CPU, to use GPU in single rank simulations,
docs/release-notes/2020/major/performance.rst:one can use new '-update gpu' command line option.
docs/release-notes/2020/major/performance.rst:GPU Direct Communications
docs/release-notes/2020/major/performance.rst:When running on multiple GPUs with CUDA, communication operations can
docs/release-notes/2020/major/performance.rst:now be performed directly between GPU memory spaces (automatically
docs/release-notes/2020/major/performance.rst:shell: GMX_GPU_DD_COMMS (for halo exchange communications between PP
docs/release-notes/2020/major/performance.rst:tasks); GMX_GPU_PME_PP_COMMS (for communications between PME and PP
docs/release-notes/2020/major/performance.rst:tasks); GMX_FORCE_UPDATE_DEFAULT_GPU can also be set in
docs/release-notes/2020/major/performance.rst:order to combine with the new GPU update feature (above). The
docs/release-notes/2020/major/performance.rst:resident on the GPU across most timesteps, avoiding expensive data
docs/release-notes/2020/major/performance.rst:Bonded kernels on GPU have been fused
docs/release-notes/2020/major/performance.rst:Instead of launching one GPU kernel for each listed interaction type there is now one
docs/release-notes/2020/major/performance.rst:GPU kernel that handles all listed interactions. This improves the performance when
docs/release-notes/2020/major/performance.rst:running bonded calculations on a GPU.
docs/release-notes/2020/major/performance.rst:Modern CPUs and GPUs can take a few seconds to ramp up their clock speeds.
docs/release-notes/2020/major/portability.rst:Enabled PME offload support with OpenCL on NVIDIA and Intel GPUs
docs/release-notes/2020/major/portability.rst:Thanks to portability improvements, the previously disabled PME OpenCL offload
docs/release-notes/2020/major/portability.rst:is now enabled also on NVIDIA and Intel GPUs.
docs/release-notes/2020/major/highlights.rst:without GPUs, all enabled and automated by default. In addition,
docs/release-notes/2020/major/highlights.rst:* Running almost the entire simulation step on a single CUDA compatible GPU for supported
docs/release-notes/2020/2020.1.rst:Avoid mdrun assertion failure when running with shells and update on a GPU
docs/release-notes/2020/2020.1.rst:when attempting to run with shells and update on a GPU.
docs/release-notes/2020/2020.1.rst:Fix too small pairlist buffer on Intel GPUs
docs/release-notes/2020/2020.1.rst:The pairlist buffer generated for Intel GPUs was slightly too small,
docs/release-notes/2020/2020.1.rst:Document known issues with OpenCL on Volta and Turing
docs/release-notes/2019/2019.4.rst:Fix incorrect LJ cut-off on GPU when rvdw < rcoulomb
docs/release-notes/2019/2019.4.rst:non-bonded interactions on the GPU. This only affected energy minimization,
docs/release-notes/2019/2019.4.rst:Fix (unlikely) missing bonded forces with CUDA GPUs and domain decomposition
docs/release-notes/2019/2019.4.rst:Forces could be missing for bonded interactions computed on CUDA GPUs with
docs/release-notes/2019/2019.4.rst:Disable PME OpenCL on Apple
docs/release-notes/2019/2019.4.rst:The Apple OpenCL compilers fail to produce a functional clFFT build.
docs/release-notes/2019/2019.4.rst:The OpenCL PME support is therefore disabled on Apple platforms.
docs/release-notes/2019/2019.2.rst:prefer AVX2 over AVX512 in GPU-offload or highly parallel MPI cases.
docs/release-notes/2019/2019.2.rst:A :ref:`fix <release-notes-2019-1-gpu>` made to GPU kernels in 2019.1 was
docs/release-notes/2019/2019.1.rst:.. _release-notes-2019-1-gpu:
docs/release-notes/2019/2019.1.rst:Fix incorrect LJ repulsion force switching on GPUs
docs/release-notes/2019/2019.1.rst:When using a CUDA or OpenCL GPU, the coefficient for the second order
docs/release-notes/2019/2019.1.rst:With MSVC, disabled internal clFFT fallback used for OpenCL support
docs/release-notes/2019/2019.1.rst:|Gromacs| requires MSVC 2017, and the |Gromacs| OpenCL build requires
docs/release-notes/2019/2019.1.rst:Explicitly require 64-bit platforms for OpenCL
docs/release-notes/2019/2019.1.rst:A 64-bit OpenCL runtime is required by |Gromacs|.
docs/release-notes/2019/2019.1.rst:All known OpenCL implementations on 64-bit platforms are 64-bit
docs/release-notes/2019/2019.1.rst:(and there are no known 32-bit platforms with 64-bit OpenCL),
docs/release-notes/2019/2019.1.rst:hence we require a 64-bit platform at configure-time in OpenCL builds.
docs/release-notes/2019/2019.5.rst:When building GPU enabled versions of |Gromacs| with clang as either host only or host
docs/release-notes/2019/2019.5.rst:Fix performance issue with bonded interactions in wrong GPU stream
docs/release-notes/2019/major/removed-functionality.rst:NVML support removed on NVIDIA GPUs
docs/release-notes/2019/major/removed-functionality.rst:NVML support (for reporting GPU application clocks  or changing these
docs/release-notes/2019/major/removed-functionality.rst:Support for CUDA compute capability 2.x removed
docs/release-notes/2019/major/removed-functionality.rst:The Fermi-era GPUs (cira 2010) are no longer in widespread use, are
docs/release-notes/2019/major/performance.rst:PME on GPU when running free energy perturbations not involving charges
docs/release-notes/2019/major/performance.rst:PME can now be run on a GPU when doing free energy perturbations
docs/release-notes/2019/major/performance.rst:PME long-ranged interaction GPU offload now available with OpenCL
docs/release-notes/2019/major/performance.rst:On supported devices from all supported vendors (AMD, Intel, NVIDIA),
docs/release-notes/2019/major/performance.rst:it is now possible to offload PME tasks to the GPU using OpenCL. This
docs/release-notes/2019/major/performance.rst:works in the same way as the former CUDA offload. A single GPU can
docs/release-notes/2019/major/performance.rst:GPU will be about as fast as the 2018 version that needed many more
docs/release-notes/2019/major/performance.rst:CPU cores to balance the GPU. Performance on hardware that had good
docs/release-notes/2019/major/performance.rst:balance of GPU and CPU also shows minor improvements, and the capacity
docs/release-notes/2019/major/performance.rst:for hardware with strong GPUs to run effective simulations is now
docs/release-notes/2019/major/performance.rst:Intel integrated GPUs are now supported for GPU offload with OpenCL
docs/release-notes/2019/major/performance.rst:On Intel CPUs with integrated GPUs, it is now possible to offload nonbonded tasks
docs/release-notes/2019/major/performance.rst:to the GPU the same way as offload is done to other GPU architectures.
docs/release-notes/2019/major/performance.rst:Bonded interactions are now supported for CUDA GPU offload
docs/release-notes/2019/major/performance.rst:NVIDIA GPUs with CUDA, with and without domain decomposition.
docs/release-notes/2019/major/performance.rst:Added code generation support for NVIDIA Turing GPUs
docs/release-notes/2019/major/performance.rst:With CUDA 10.0 NVIDIA Turing GPUs can be directly targeted by the nvcc
docs/release-notes/2019/major/performance.rst:by default when using CUDA 10 (or later).
docs/release-notes/2019/major/portability.rst:Increased the minimum CUDA version required
docs/release-notes/2019/major/portability.rst:We now require CUDA 7.0, whose features help keep the code more
docs/release-notes/2019/major/portability.rst:CUDA 9.0.
docs/release-notes/2019/major/portability.rst:Updated the OpenCL requirement to version 1.2
docs/release-notes/2019/major/portability.rst:We now require at least OpenCL version 1.2 both for API and kernels. All
docs/release-notes/2019/major/highlights.rst:without GPUs, all enabled and automated by default. We are extremely
docs/release-notes/2019/major/highlights.rst:* Intel integrated GPUs are now supported with OpenCL for offloading
docs/release-notes/2019/major/highlights.rst:* PME long-ranged interactions can now also run on a single AMD GPU
docs/release-notes/2019/major/highlights.rst:  using OpenCL, which means many fewer CPU cores are needed for good
docs/release-notes/2019/2019.6.rst:compiled without GPU support.
docs/release-notes/2019/2019.6.rst:Avoid cryptic GPU detection errors when devices are unavailable or out of memory
docs/release-notes/2024/2024.1.rst:was executed on a GPU.
docs/release-notes/2024/2024.1.rst:NVIDIA/AMD GPUs using Intel oneAPI. No user-facing impact expected.
docs/release-notes/2024/2024.2.rst:This was caused by a GPU buffer re-allocation while an operation on the buffer
docs/release-notes/2024/2024.2.rst:Fix performance regression for some cases on latest NVIDIA GPUs
docs/release-notes/2024/2024.2.rst:calculations on NVIDIA GPUs which improved performance for a range of
docs/release-notes/2024/2024.2.rst:Fix crash in NbnxmSetupTest.CanCreateNbnxmGPU
docs/release-notes/2024/2024.2.rst:``NbnxmSetupTest.CanCreateNbnxmGPU`` could crash in GPU builds
docs/release-notes/2024/2024.3.rst:causing up to 50% worse NBNxM kernel performance on Intel Data Center GPU Max
docs/release-notes/2024/2024.3.rst:to avoid that. Arc GPUs were not affected.
docs/release-notes/2024/major/deprecated-functionality.rst:GMX_OPENCL_NB_CLUSTER_SIZE CMake variable deprecated in favor of GMX_GPU_NB_CLUSTER_SIZE
docs/release-notes/2024/major/deprecated-functionality.rst:Both OpenCL and SYCL support different cluster sizes, so GMX_GPU_NB_CLUSTER_SIZE should
docs/release-notes/2024/major/deprecated-functionality.rst:OpenCL to be removed as a GPU framework
docs/release-notes/2024/major/deprecated-functionality.rst::issue:`3818` Work is underway for ports to AMD and Intel GPUs, and it
docs/release-notes/2024/major/deprecated-functionality.rst:OpenCL port. Nvidia GPUs are targeted by the CUDA port, and no changes
docs/release-notes/2024/major/deprecated-functionality.rst:of an emerging GPU vendor in HPC needing OpenCL support, we will
docs/release-notes/2024/major/deprecated-functionality.rst:remove the OpenCL port once AMD and Intel support is established in
docs/release-notes/2024/major/features.rst:Basic support has been added for GPU tracing libraries so wallcycle main and sub-regions
docs/release-notes/2024/major/performance.rst:This can lead to a few percent performance loss, in particular when using GPUs.
docs/release-notes/2024/major/performance.rst:With wall potentials, bonded interactions can now be run on GPUs
docs/release-notes/2024/major/performance.rst:HeFFTe multi-GPU FFT plan options are now configurable
docs/release-notes/2024/major/performance.rst:``GMX_HEFFTE_USE_GPU_AWARE``, ``GMX_HEFFTE_USE_PENCILS``, and
docs/release-notes/2024/major/performance.rst:quality of implementation of e.g. the GPU-aware MPI library, as well
docs/release-notes/2024/major/performance.rst:as the layout and number of the GPUs participating in the 3D-FFT.
docs/release-notes/2024/major/highlights.rst:without GPUs, all enabled and automated by default. In addition,
docs/release-notes/2024/major/highlights.rst:* Configurable HeFFTe multi-GPU FFT options lets users fine-tune the settings for
docs/release-notes/2024/2024.4.rst:Fix missing non-bonded interactions close to cut-off with GPUs
docs/release-notes/2024/2024.4.rst:The GPU rolling pruning employed with the dual pair list setup used
docs/release-notes/2024/2024.4.rst:Affected simulations: all GPU accelerated runs using |Gromacs| versions
docs/release-notes/2024/2024.4.rst:Forbid the usage of triangle constraints with -update gpu
docs/release-notes/2024/2024.4.rst:Prevent using update on GPU if there are triangle constraints.
docs/release-notes/2024/2024.4.rst:``gmx_mpi mdrun`` could hang when using GPUs and separate PME ranks 
docs/release-notes/2024/2024.4.rst:A logic error in task assignment for ``-nb gpu -pme cpu`` and separate PME ranks
docs/release-notes/2024/2024.4.rst:When building GROMACS for Windows with CUDA support, the checks for testing
docs/release-notes/2024/2024.4.rst:CUDA is not compatible anymore with old compute architectures.
docs/release-notes/2023/2023.1.rst:Only runs with >=3 GPUs and with direct GPU communication enabled
docs/release-notes/2023/2023.1.rst:(``GMX_ENABLE_DIRECT_GPU_COMM`` env. var.) are affected.
docs/release-notes/2023/2023.1.rst:For some SYCL targets (most notably, hipSYCL for AMD GPUs with ROCm 5.x),
docs/release-notes/2023/2023.1.rst:Now, bonded force calculation on GPU is expected to be up to 3 times faster.
docs/release-notes/2023/2023.1.rst:release in the the NVIDIA HPC SDK version 23.3, which is required for
docs/release-notes/2023/2023.1.rst:NVIDIA Hopper GPU support. We have now added default support to the
docs/release-notes/2023/2023.4.rst:Missing force contribution on neighbor search steps with GPU update
docs/release-notes/2023/2023.4.rst:when `nstlist` is not a multiple of `nstpcouple`) and GPU update
docs/release-notes/2023/2023.4.rst:is used, there could have been a race between GPU buffer clearing and
docs/release-notes/2023/2023.4.rst:This should not have ever happened unless the GPU was heavily
docs/release-notes/2023/2023.3.rst:Fix missing force buffer clearing with GPU DD and CPU bonded interactions
docs/release-notes/2023/2023.3.rst:In simulations with domain decomposition using direct GPU communication for halo exchange
docs/release-notes/2023/2023.3.rst:(feature enabled with the GMX_ENABLE_DIRECT_GPU_COMM variable), a missing force buffer clearing prior to
docs/release-notes/2023/2023.3.rst:dynamic load balancing support with GPU-resident simulations that use GPU halo exchange.
docs/release-notes/2023/2023.3.rst:Add workaround for OpenCL bug on AppleSilicon GPUs
docs/release-notes/2023/2023.3.rst:After a resource leak was fixed in 2023.2, the OpenCL became broken
docs/release-notes/2023/2023.3.rst:on M1 Macs (and likely other AppleSilicon GPUs).
docs/release-notes/2023/2023.3.rst:affected OpenCL builds of |Gromacs|. 
docs/release-notes/2023/2023.3.rst:CUDA Graph fixes related to neighbour search steps
docs/release-notes/2023/2023.3.rst:When using the experimental CUDA Graphs feature, previously the code
docs/release-notes/2023/2023.3.rst:ensuring that CUDA Graphs are suitably updated on virial steps; runs
docs/release-notes/2023/2023.3.rst:where CUDA graphs are active on steps immediately preceding NS steps -
docs/release-notes/2023/2023.3.rst:Work around the performance regression on AMD MI250X with ROCm 5.5 or newer
docs/release-notes/2023/2023.3.rst:With ROCm 5.5 and 5.6, some NBNXM kernels experienced up to 23% performance
docs/release-notes/2023/2023.3.rst:regression on MI250X compared to ROCm 5.3. We backported two patches from
docs/release-notes/2023/2023.3.rst:a slowdown around 2% with ROCm 5.5+.
docs/release-notes/2023/2023.2.rst:Fix for crash when CUDA Graphs are enabled on multi-GPU
docs/release-notes/2023/2023.2.rst:non-default CUDA Graphs experimental feature was enabled on multi-GPU,
docs/release-notes/2023/2023.2.rst:required for the CUDA graphs codepath. This version fixes the issue by
docs/release-notes/2023/2023.2.rst:Fix resource leak in OpenCL
docs/release-notes/2023/2023.2.rst:``gmx mdrun`` built with OpenCL was slowly leaking memory when
docs/release-notes/2023/2023.2.rst:running on GPUs. That's fixed now.
docs/release-notes/2023/major/deprecated-functionality.rst:GMX_OPENCL_NB_CLUSTER_SIZE CMake variable deprecated in favor of GMX_GPU_NB_CLUSTER_SIZE
docs/release-notes/2023/major/deprecated-functionality.rst:Both OpenCL and SYCL support different cluster sizes, so GMX_GPU_NB_CLUSTER_SIZE should
docs/release-notes/2023/major/deprecated-functionality.rst:OpenCL to be removed as a GPU framework
docs/release-notes/2023/major/deprecated-functionality.rst::issue:`3818` Work is underway for ports to AMD and Intel GPUs, and it
docs/release-notes/2023/major/deprecated-functionality.rst:OpenCL port. Nvidia GPUs are targeted by the CUDA port, and no changes
docs/release-notes/2023/major/deprecated-functionality.rst:of an emerging GPU vendor in HPC needing OpenCL support, we will
docs/release-notes/2023/major/deprecated-functionality.rst:remove the OpenCL port once AMD and Intel support is established in
docs/release-notes/2023/major/performance.rst:Update will run on GPU by default
docs/release-notes/2023/major/performance.rst:The mdrun ``-update auto`` will by default map to GPU if supported.
docs/release-notes/2023/major/performance.rst:of both GPU runs and parallel runs.
docs/release-notes/2023/major/performance.rst:This can improve performance in simulations using GPUs in particular.
docs/release-notes/2023/major/performance.rst:PME decomposition support with CUDA and SYCL backends
docs/release-notes/2023/major/performance.rst:PME decomposition support has been added to CUDA and SYCL
docs/release-notes/2023/major/performance.rst:backends. With PME offloaded to the GPU, the number of PME ranks can
docs/release-notes/2023/major/performance.rst:implementation requires building |Gromacs| with GPU-aware MPI and
docs/release-notes/2023/major/performance.rst:CUDA build configuration, or with :ref:`heFFTe <heffte installation>`
docs/release-notes/2023/major/performance.rst:in either a CUDA or SYCL build configuration.
docs/release-notes/2023/major/performance.rst:GPU-based PME decomposition support still lacks substantial testing,
docs/release-notes/2023/major/performance.rst:equivalent runs using a single PME GPU). This feature can be enabled
docs/release-notes/2023/major/performance.rst:using the ``GMX_GPU_PME_DECOMPOSITION`` environment variable. The
docs/release-notes/2023/major/performance.rst:CUDA Graphs for GPU-resident Steps
docs/release-notes/2023/major/performance.rst:New CUDA functionality has been introduced, allowing GPU activities
docs/release-notes/2023/major/performance.rst:to be launched as a single CUDA graph on each step rather than multiple
docs/release-notes/2023/major/performance.rst:activities scheduled to multiple CUDA streams. It only works for those
docs/release-notes/2023/major/performance.rst:cases which already support GPU-resident steps (where all force and
docs/release-notes/2023/major/performance.rst:update calculations are GPU-accelerated). This offers performance
docs/release-notes/2023/major/performance.rst:and GPU side scheduling overheads. The feature can optionally be
docs/release-notes/2023/major/performance.rst:activated via the ``GMX_CUDA_GRAPH`` environment variable. 
docs/release-notes/2023/major/performance.rst:For AMD GPUs, VkFFT has been integrated to provide performance improvements.
docs/release-notes/2023/major/performance.rst:single rank or single separate PME rank) and can be enabled with ``-DGMX_GPU_FFT_LIBRARY=VKFFT``
docs/release-notes/2023/major/portability.rst:Initial support for Apple silicon GPUs
docs/release-notes/2023/major/portability.rst:We now recognize Apple-designed GPUs as a supported architecture
docs/release-notes/2023/major/portability.rst:in the OpenCL backend.
docs/release-notes/2023/major/portability.rst:VkFFT support for improved portability and performance on GPUs with OpenCL and SYCL
docs/release-notes/2023/major/portability.rst:Support for the VkFFT GPU FFT library was added with two goals:
docs/release-notes/2023/major/portability.rst:improved portability across GPU platforms and better performance.
docs/release-notes/2023/major/portability.rst:VkFFT can be used with OpenCL and SYCL.
docs/release-notes/2023/major/portability.rst:NVIDIA GPUs, and it is a better-performing alternative recommended 
docs/release-notes/2023/major/portability.rst:For OpenCL builds, VkFFT provides an alternative to ClFFT
docs/release-notes/2023/major/portability.rst:``-DGMX_GPU_FFT_LIBRARY=VKFFT``.
docs/release-notes/2023/major/portability.rst:PME GPU offload on macOS
docs/release-notes/2023/major/portability.rst:Until now, PME calculations could not be offloaded to the GPU on
docs/release-notes/2023/major/portability.rst:OpenCL drivers at runtime. To overcome this incompatibility, we
docs/release-notes/2023/major/highlights.rst:without GPUs, all enabled and automated by default. In addition,
docs/release-notes/2023/major/highlights.rst:* The SYCL GPU implementation, which is the GPU portability layer that
docs/release-notes/2023/major/highlights.rst:  supports all major GPU platforms, has received major extensions
docs/release-notes/2023/major/highlights.rst:  in practice, the |Gromacs| GPU portability layer
docs/release-notes/2023/major/highlights.rst:  oneAPI DPC++, IntelLLVM) and regularly tested on multiple GPU backends.
docs/release-notes/2023/major/highlights.rst:  * SYCL supports more GPU offload features: bonded forces and
docs/release-notes/2023/major/highlights.rst:    direct GPU-GPU communication with GPU-aware MPI.
docs/release-notes/2023/major/highlights.rst:    and Intel for production as well as NVIDIA GPUs (not for production).
docs/release-notes/2023/major/highlights.rst:  PME calculation to multiple GPUs, including the FFT computation; when combined with
docs/release-notes/2023/major/highlights.rst:* CUDA Graph support has been added to execute GPU-resident single-/multi-GPU
docs/release-notes/2023/major/highlights.rst:  simulations using thread-MPI entirely on the GPU to improve performance
docs/release-notes/2023/major/highlights.rst:* Apple M1/M2 GPUs are now supported via the OpenCL GPU backend.
docs/release-notes/2023/2023.5.rst:was executed on a GPU.
docs/release-notes/2023/2023.5.rst:Fix crashes with GPU direct communication for some atypical run configurations
docs/release-notes/2023/2023.5.rst:When GPU direct communication was used in combination with atypical
docs/release-notes/2023/2023.5.rst:run conditions (e.g. many thread-MPI tasks on each GPU), crashes could
docs/release-notes/2016/2016.2.rst:Corrected CUDA sm_60 performance
docs/release-notes/2016/2016.4.rst:Supported CUDA 9/Volta for nonbonded kernels
docs/release-notes/2016/2016.4.rst:Implemented production-quality support for Volta GPUs and CUDA 9.
docs/release-notes/2016/2016.4.rst:synchrony, without disturbing support for older GPUs and/or
docs/release-notes/2016/2016.4.rst:CUDA. Further improvements may be seen (e.g. in the 2017 release).
docs/release-notes/2016/2016.3.rst:Fixed ``gmx tune_pme`` detection of GPU support
docs/release-notes/2016/2016.3.rst:AMD GPUs using Mesa 17.0+ and LLVM 4.0+ run |Gromacs| using OpenCL.
docs/release-notes/2016/major/bugs-fixed.rst:and PME kernels get 3% and 1% slower, respectively.  On CUDA, RF and
docs/release-notes/2016/major/bugs-fixed.rst:Properly reset CUDA application clocks
docs/release-notes/2016/major/bugs-fixed.rst:Changed to use only legal characters in OpenCL cache filename
docs/release-notes/2016/major/bugs-fixed.rst:The option to cache JIT-compiled OpenCL short-ranged kernels needed to
docs/release-notes/2016/major/performance.rst:GPU improvements
docs/release-notes/2016/major/performance.rst:up to 5% increase in CUDA performance, so depending on parameters and compilers
docs/release-notes/2016/major/performance.rst:an 5-20% GPU kernel performance increase is expected.
docs/release-notes/2016/major/performance.rst:These benefits are seen with CUDA 7.5 (which is now the version we recommend);
docs/release-notes/2016/major/performance.rst:Even larger improvements in OpenCL performance on AMD devices are
docs/release-notes/2016/major/performance.rst:with recent AMD OpenCL compilers. 
docs/release-notes/2016/major/performance.rst:Note that due to limitations of the NVIDIA OpenCL compiler CUDA is still superior
docs/release-notes/2016/major/performance.rst:in performance on NVIDIA GPUs. Hence, it is recommended to use CUDA-based GPU acceleration
docs/release-notes/2016/major/performance.rst:on NVIDIA hardware.
docs/release-notes/2016/major/performance.rst:Improved support for OpenCL devices
docs/release-notes/2016/major/performance.rst:The OpenCL support is now fully compatible with all intra- and
docs/release-notes/2016/major/performance.rst:inter-node parallelization mode, including MPI, thread-MPI, and GPU
docs/release-notes/2016/major/performance.rst:that in the CUDA code) that had been disabled was found to be useful
docs/release-notes/2016/major/performance.rst:Added Lennard-Jones combination-rule kernels for GPUs
docs/release-notes/2016/major/performance.rst:Implemented LJ combination-rule parameter lookup in the CUDA and
docs/release-notes/2016/major/performance.rst:OpenCL kernels for both geometric and Lorentz-Berthelot combination
docs/release-notes/2016/major/performance.rst:Added support for CUDA CC 6.0/6.1
docs/release-notes/2016/major/performance.rst:by the CUDA 8.0 compiler.
docs/release-notes/2016/major/performance.rst:Improved GPU pair-list splitting to improve performance
docs/release-notes/2016/major/performance.rst:Instead of splitting the GPU lists (to generate more work units) based
docs/release-notes/2016/major/performance.rst:Improved CUDA GPU memory configuration
docs/release-notes/2016/major/performance.rst:but their value for simulations using GPU offload are much higher,
docs/release-notes/2016/major/new-features.rst:Changed to support only CUDA 5.0 and more recent versions
docs/release-notes/2016/major/miscellaneous.rst:Work around glibc 2.23 with CUDA
docs/release-notes/2016/major/miscellaneous.rst:versions of CUDA with all gcc compiler versions. The |Gromacs| build
docs/release-notes/2016/major/miscellaneous.rst:_FORCE_INLINE preprocessor define to CUDA compilation.
docs/release-notes/2016/major/miscellaneous.rst:Split NBNXN CUDA kernels into four compilation units
docs/release-notes/2016/major/miscellaneous.rst:The CUDA nonbonded kernels are now built in four different compilation units
docs/release-notes/2016/major/miscellaneous.rst:GMX_CUDA_NB_SINGLE_COMPILATION_UNIT cmake option.
docs/release-notes/2016/major/miscellaneous.rst:Disable static libcudart on OS X
docs/release-notes/2016/major/miscellaneous.rst:libcudart by default, but this breaks builds at least
docs/release-notes/2016/major/miscellaneous.rst:Disabled NVIDIA JIT cache with OpenCL
docs/release-notes/2016/major/miscellaneous.rst:The NVIDIA JIT caching is known to be broken with OpenCL compilation in
docs/release-notes/2016/major/miscellaneous.rst:the JIT caching when running on NVIDIA GPUs. AMD GPUs are unaffected.
docs/release-notes/2016/major/highlights.rst:  without GPUs. CPU-side SIMD and threading enhancements will
docs/release-notes/2016/major/highlights.rst:  make GPU-accelerated simulations faster even if we'd left the GPU
docs/release-notes/2016/major/highlights.rst:  code alone! Thanks to these and additional GPU kernel improvements,
docs/release-notes/2016/major/highlights.rst:  in GPU-accelerated runs expect around 15% improvement
docs/release-notes/2016/major/highlights.rst:  constraints by a factor of 3 to 5 - which has a strong effect for GPU runs.
docs/release-notes/2016/major/highlights.rst:* OpenCL GPU support is now available with all combinations of MPI,
docs/release-notes/2016/major/highlights.rst:  thread-MPI and GPU sharing (ie. the same as CUDA). Kernel performance
docs/release-notes/2016/major/highlights.rst:  has improved by up to 60%. AMD GPUs benefit the most, OpenCL on NVIDIA is
docs/release-notes/2016/2016.1.rst:Under certain conditions, especially with (shared) GPUs, DLB can
docs/release-notes/2016/2016.1.rst:Improved the accuracy of timing for dynamic load balancing with GPUs
docs/release-notes/2016/2016.1.rst:With OpenCL, the time for the local non-bonded to finish on the GPU
docs/release-notes/2016/2016.1.rst:was ignored in the dynamic load balancing. This change lets OpenCL
docs/release-notes/2016/2016.1.rst:take the same code path as CUDA.
docs/release-notes/2016/2016.1.rst:One internal heuristic parameter was far too small for both CUDA and
docs/release-notes/2016/2016.1.rst:OpenCL, which is now fixed.
docs/release-notes/2016/2016.1.rst:Corrected kernel launch bounds for Tesla P100 GPUs
docs/release-notes/2016/2016.1.rst:in reduced occupancy on sm_60 GPU, and thus improves performance.
docs/release-notes/2016/2016.1.rst:Approaches for reducing overhead for GPU runs are now documented.
docs/release-notes/2016/2016.1.rst:OpenCL, mdrun-only, and "make check". A "quick and dirty" cluster
docs/release-notes/2016/2016.1.rst:OpenCL error string are now written, instead of cryptic error codes
docs/release-notes/2016/2016.5.rst:Required -ntmpi with setting -ntomp with GPUs
docs/release-notes/2016/2016.5.rst:With GPUs and thread-MPI, setting only ``-ntomp`` could lead to
docs/release-notes/2016/2016.5.rst:Now with GPUs and thread-MPI the user is required to set ``-ntmpi`` when
docs/release-notes/2022/2022.3.rst:Energy minimization would not converge with GPU and without DD
docs/release-notes/2022/2022.3.rst:when using a GPU for the nonbonded interactions and not using domain
docs/release-notes/2022/2022.3.rst:Remove unnecessary memory re-allocations for GPU update runs
docs/release-notes/2022/2022.3.rst:An issue has been fixed where GPU memory allocations were repeatedly
docs/release-notes/2022/2022.3.rst:being performed unecessarily for runs with GPU update enabled,
docs/release-notes/2022/2022.4.rst:Incorrect foreign energy differences for 1-4 interactions on a GPU
docs/release-notes/2022/2022.4.rst:When running free-energy calculations using a GPU without domain decomposition,
docs/release-notes/2022/2022.4.rst:AMD RDNA devices are now properly marked as "unsupported" with OpenCL
docs/release-notes/2022/2022.4.rst:AMD RDNA GPUs (Radeon RX 5000, 6000, and 7000 series) never worked
docs/release-notes/2022/2022.4.rst:correctly with OpenCL, usually with a simulation crashing quickly.
docs/release-notes/2022/2022.4.rst:Nvidia GPUs were correct but may not have achieved best possible
docs/release-notes/2022/2022.4.rst:Added compilation support for new CUDA architectures
docs/release-notes/2022/2022.4.rst:The list of NVIDIA CUDA architectures for which code is directly
docs/release-notes/2022/2022.6.rst:Fix CUDA PME spread in multi-GPU runs (with >=3 GPUs)
docs/release-notes/2022/2022.6.rst:Runs with >=3 GPUs with direct GPU communication enabled
docs/release-notes/2022/2022.6.rst:(``GMX_ENABLE_DIRECT_GPU_COMM`` env. var.) are affected.
docs/release-notes/2022/2022.6.rst:Fix missing synchronization in GPU PME pipelining
docs/release-notes/2022/2022.6.rst:forces/energies to be produced when GPU PME pipelining was used.
docs/release-notes/2022/2022.6.rst:Only runs with >=3 GPUs and with direct GPU communication enabled 
docs/release-notes/2022/2022.6.rst:(``GMX_ENABLE_DIRECT_GPU_COMM`` env. var.) are affected.
docs/release-notes/2022/2022.5.rst:Add missing net charge term when running PME on a GPU
docs/release-notes/2022/2022.5.rst:When PME was running on a GPU, the term due to a net charge of the system was missing.
docs/release-notes/2022/2022.5.rst:or GPU on a trajectory produced without domain decompostion and GPU,
docs/release-notes/2022/2022.5.rst:h-bonds constrained only and domain decomposition on GPU.
docs/release-notes/2022/2022.1.rst:Fix missing B State pinning for PME GPU
docs/release-notes/2022/2022.1.rst:The PME memory is now correctly pinned when using GPU PME.
docs/release-notes/2022/2022.1.rst:Only allow 1D PME GPU decomposition
docs/release-notes/2022/2022.2.rst:Fix missing CPU-GPU synchronization when doing free-energy calculations
docs/release-notes/2022/2022.2.rst:When GPU halo exchange with direct communication is enabled, CPU based 
docs/release-notes/2022/2022.2.rst:This issue can cause incorect output when GPU direct communication is enabled 
docs/release-notes/2022/2022.2.rst:using GMX_ENABLE_DIRECT_GPU_COMM environment variable and simulation contains 
docs/release-notes/2022/2022.2.rst:Fix missing PME mesh dV/dlambda with PME on GPU on a seperate PME rank
docs/release-notes/2022/2022.2.rst:When doing free-energy calculations with PME running on GPU on a separate
docs/release-notes/2022/2022.2.rst:the padding area of the buffer on the GPU must be set to
docs/release-notes/2022/2022.2.rst:zero. Previously, a dependency was missing such that, with GPU direct
docs/release-notes/2022/2022.2.rst:communications enabled via the GMX_ENABLE_DIRECT_GPU_COMM, it was
docs/release-notes/2022/2022.2.rst:Warn when using gcc version 7 with CUDA builds
docs/release-notes/2022/2022.2.rst:hard for |Gromacs| to check whether CUDA's ``nvcc`` compiler will
docs/release-notes/2022/2022.2.rst:slow CUDA kernels. Now |Gromacs| assumes all nvcc flags are valid in
docs/release-notes/2022/major/deprecated-functionality.rst:GMX_OPENCL_NB_CLUSTER_SIZE CMake variable deprecated in favor of GMX_GPU_NB_CLUSTER_SIZE
docs/release-notes/2022/major/deprecated-functionality.rst:Both OpenCL and SYCL support different cluster sizes, so GMX_GPU_NB_CLUSTER_SIZE should
docs/release-notes/2022/major/deprecated-functionality.rst:OpenCL to be removed as a GPU framework
docs/release-notes/2022/major/deprecated-functionality.rst::issue:`3818` Work is underway for ports to AMD and Intel GPUs, and it
docs/release-notes/2022/major/deprecated-functionality.rst:OpenCL port. Nvidia GPUs are targeted by the CUDA port, and no changes
docs/release-notes/2022/major/deprecated-functionality.rst:of an emerging GPU vendor in HPC needing OpenCL support, we will
docs/release-notes/2022/major/deprecated-functionality.rst:remove the OpenCL port once AMD and Intel support is established in
docs/release-notes/2022/major/features.rst:Replica-exchange molecular dynamics simulations with GPU update
docs/release-notes/2022/major/features.rst:Replica-exchange molecular dynamics now works with GPU update.
docs/release-notes/2022/major/removed-functionality.rst:* ``GMX_CUDA_NB_ANA_EWALD`` and ``GMX_OCL_NB_ANA_EWALD`` (use ``GMX_GPU_NB_ANA_EWALD``)
docs/release-notes/2022/major/removed-functionality.rst:* ``GMX_CUDA_NB_TAB_EWALD`` and ``GMX_OCL_NB_TAB_EWALD`` (use ``GMX_GPU_NB_TAB_EWALD``)
docs/release-notes/2022/major/removed-functionality.rst:* ``GMX_CUDA_NB_EWALD_TWINCUT`` and ``GMX_OCL_NB_EWALD_TWINCUT`` (use ``GMX_GPU_NB_EWALD_TWINCUT``)
docs/release-notes/2022/major/performance.rst:GPU direct communication with CUDA-aware MPI
docs/release-notes/2022/major/performance.rst:Direct GPU communication support has been extended to simulations that use
docs/release-notes/2022/major/performance.rst:a CUDA-aware library-MPI when running on NVIDIA GPUs. Detection of CUDA-aware MPI
docs/release-notes/2022/major/performance.rst:primarily with OpenMPI but any CUDA-aware MPI implementation should be suitable,
docs/release-notes/2022/major/performance.rst:CUDA-aware MPI support still lacks substantial testing, hence it is included
docs/release-notes/2022/major/performance.rst:default, but it can be enabled using the GMX_ENABLE_DIRECT_GPU_COMM environment
docs/release-notes/2022/major/performance.rst:case when using GPUs, where the performance improvement of free-energy runs is
docs/release-notes/2022/major/performance.rst:PME-PP GPU Direct Communication Pipelining
docs/release-notes/2022/major/performance.rst:For multi-GPU runs with direct PME-PP GPU communication enabled, the
docs/release-notes/2022/major/performance.rst:are shared between multiple GPUs, e.g. PCIe within multi-GPU servers
docs/release-notes/2022/major/performance.rst:When running with a single MPI rank with PME and without GPU, mdrun
docs/release-notes/2022/major/performance.rst:Restricted GPU support with multiple time stepping
docs/release-notes/2022/major/performance.rst:GPUs can be used in combination with MTS, but for now this is limited
docs/release-notes/2022/major/performance.rst:components are are calculated every step (which can be on the GPU).
docs/release-notes/2022/major/performance.rst:PME decomposition support in mixed mode with CUDA and process-MPI
docs/release-notes/2022/major/performance.rst:PME decomposition is supported now in mixed mode with CUDA backend. 
docs/release-notes/2022/major/performance.rst:and underlying MPI implementation is CUDA-aware. This feature lacks substantial testing
docs/release-notes/2022/major/performance.rst:and has been disabled by default but can be enabled by setting GMX_GPU_PME_DECOMPOSITION=1 
docs/release-notes/2022/major/performance.rst:Performance improvements when running on Ampere-class Nvidia GPUs
docs/release-notes/2022/major/highlights.rst:without GPUs, all enabled and automated by default. In addition,
docs/release-notes/2022/major/highlights.rst:  calculations up to three times as fast when using GPUs
docs/release-notes/2022/major/highlights.rst:* Added AMD GPU support with SYCL via hipSYCL_
docs/release-notes/2022/major/highlights.rst:* More GPU offload features supported with SYCL (PME, GPU update). 
docs/release-notes/2022/major/highlights.rst:* Improved parallelization with GPU-accelerated runs using CUDA and extended GPU direct communication to support multi-node simulation using CUDA-aware MPI.
docs/release-notes/2018/2018.6.rst:.. _release-notes-2018-6-gpu:
docs/release-notes/2018/2018.6.rst:Fix incorrect LJ repulsion force switching on GPUs
docs/release-notes/2018/2018.6.rst:When using a CUDA or OpenCL GPU, the coefficient for the second order
docs/release-notes/2018/2018.3.rst:Multi-domain GPU runs can no longer miss pair interactions
docs/release-notes/2018/2018.3.rst:With systems with empty space in the unit cell, GPU runs with domain
docs/release-notes/2018/2018.3.rst: - This bug only affects simulations running on GPUs with domain decomposition
docs/release-notes/2018/2018.3.rst:**This is a critical fix and users of 2018.x series that run on GPUs should
docs/release-notes/2018/2018.3.rst:Correctly specified that PME on GPUs is only supported for dynamical integrators
docs/release-notes/2018/2018.3.rst:Previously PME on GPU support could run (but fail) for energy
docs/release-notes/2018/2018.3.rst:Disable single compilation unit with CUDA 9.0
docs/release-notes/2018/2018.3.rst:Avoid aborting mdrun when GPU sanity check detects errors
docs/release-notes/2018/2018.3.rst:Improve OpenCL kernel performance on AMD Vega GPUs
docs/release-notes/2018/2018.3.rst:The OpenCL kernel optimization flags did not explicitly turn off denorm handling
docs/release-notes/2018/2018.3.rst:on both for consistency with CUDA and performance reasons.
docs/release-notes/2018/2018.3.rst:On AMD Vega GPUs (with ROCm) kernel performance improves by up to 30%.
docs/release-notes/2018/2018.5.rst:Make large PME grids work on GPU
docs/release-notes/2018/2018.5.rst:with a cryptic CUDA error.
docs/release-notes/2018/2018.5.rst:Added code generation support for NVIDIA Turing GPUs
docs/release-notes/2018/2018.5.rst:With CUDA 10.0 NVIDIA Turing GPUs can be directly targeted by the nvcc
docs/release-notes/2018/2018.5.rst:by default when using CUDA 10 (or later).
docs/release-notes/2018/2018.1.rst:Fixed box scaling in PME mixed mode using both GPU and CPU
docs/release-notes/2018/2018.1.rst:Re-enabled GPU support with walls and 1 energy group
docs/release-notes/2018/2018.1.rst:With a single non-bonded energy group and walls, we can now use a GPU
docs/release-notes/2018/2018.1.rst:Improved mdrun handling when GPUs are present but unavailable
docs/release-notes/2018/2018.1.rst:PME on Fermi-era GPUs on large systems now works
docs/release-notes/2018/2018.1.rst:On older GPUs, it was possible to run into a hardware size limitation
docs/release-notes/2018/2018.1.rst:Tests for GPU utility functionality are now more robust
docs/release-notes/2018/2018.1.rst:Non-GPU builds, and GPU builds that find incompatible or otherwise
docs/release-notes/2018/2018.1.rst:Fixed clang 6 with CUDA 9
docs/release-notes/2018/2018.1.rst:In particular, for handling options to mdrun relating to GPUs and
docs/release-notes/2018/2018.7.rst:A :ref:`fix <release-notes-2018-6-gpu>` made to GPU kernels in 2018.6 was
docs/release-notes/2018/major/removed-features.rst:Removed hybrid GPU+CPU nonbonded mode
docs/release-notes/2018/major/removed-features.rst:Updated application clock handling on Pascal+ GPUs
docs/release-notes/2018/major/bugs-fixed.rst:Added check for GPU detection support before detecting GPU devices
docs/release-notes/2018/major/bugs-fixed.rst:When a CUDA-enabled binary was run on a node with no CUDA driver
docs/release-notes/2018/major/bugs-fixed.rst:available, a note was issued that the version of the CUDA driver is
docs/release-notes/2018/major/bugs-fixed.rst:Changed to require ``-ntmpi`` when setting ``-ntomp`` and using GPUs
docs/release-notes/2018/major/bugs-fixed.rst:With GPUs and thread-MPI, setting only ``gmx mdrun -ntomp`` could lead
docs/release-notes/2018/major/bugs-fixed.rst:to oversubscription of the hardware threads.  Now, with GPUs and
docs/release-notes/2018/major/features.rst:When short-ranged interactions are running on the GPU, the dynamic pruning is overlapped
docs/release-notes/2018/major/features.rst:Changed handling of :ref:`gmx mdrun` -gpu_id
docs/release-notes/2018/major/features.rst:As more code is able to be offloaded to the GPU, task assignment has
docs/release-notes/2018/major/features.rst:-gpu_id command-line option now merely enables the user to restrict
docs/release-notes/2018/major/features.rst:which of the detected GPUs are available to the automated task
docs/release-notes/2018/major/features.rst:assignment scheme, somewhat like the ``CUDA_VISIBLE_DEVICES`` environment
docs/release-notes/2018/major/features.rst:``gmx mdrun -gputasks`` is available and documented in the user guide.
docs/release-notes/2018/major/performance.rst:Implemented support for PME long-ranged interactions on GPUs
docs/release-notes/2018/major/performance.rst:A single GPU can now be used to accelerate the computation of the
docs/release-notes/2018/major/performance.rst:GPU will be about as fast as the 2016 version that needed many more
docs/release-notes/2018/major/performance.rst:CPU cores to balance the GPU. Performance on hardware that had good
docs/release-notes/2018/major/performance.rst:balance of GPU and CPU also shows minor improvements, and the capacity
docs/release-notes/2018/major/performance.rst:for hardware with strong GPUs to run effective simulations is now
docs/release-notes/2018/major/performance.rst:Currently, the GPU used for PME must be either the same GPU as used
docs/release-notes/2018/major/performance.rst:simulation, or any GPU used from a PME-only rank. mdrun -pme gpu now
docs/release-notes/2018/major/performance.rst:requires that PME runs on a GPU, if supported. All CUDA versions and
docs/release-notes/2018/major/performance.rst:including CUDA 9.0 and Volta GPUs. However, not all combinations
docs/release-notes/2018/major/performance.rst:of features are supported with PME on GPUs - notably FEP calculations
docs/release-notes/2018/major/performance.rst:improves performance, which can otherwise be rate limiting in GPU-accelerated
docs/release-notes/2018/major/performance.rst:Tweaked conditional in the nonbonded GPU kernels
docs/release-notes/2018/major/performance.rst:GPU compilers miss an easy optimization of a loop invariant in the
docs/release-notes/2018/major/portability.rst:Enabled compiling CUDA device code with clang
docs/release-notes/2018/major/portability.rst:clang can be used as a device compiler by setting GMX_CLANG_CUDA=ON. A
docs/release-notes/2018/major/portability.rst:CUDA toolkit (>=7.0) is also needed. Note that the resulting runtime
docs/release-notes/2018/major/portability.rst:official NVIDIA CUDA compiler (nvcc).
docs/release-notes/2018/major/portability.rst:Increased the oldest cmake, compiler and CUDA versions required
docs/release-notes/2018/major/portability.rst:C++11 support. We now also require CUDA-6.5 and CMake-3.4.3.
docs/release-notes/2018/major/portability.rst:Added check that CUDA available hardware and compiled code are compatible
docs/release-notes/2018/major/portability.rst:not embed code compatible with the GPU device it tries to use nor does
docs/release-notes/2018/major/portability.rst:Additionally, if the user manually sets GMX_CUDA_TARGET_COMPUTE=20 and
docs/release-notes/2018/major/portability.rst:Fixed OpenCL compiles on Mac OS
docs/release-notes/2018/major/portability.rst:We now compile a trivial CUDA program during a run of CMake to catch
docs/release-notes/2018/major/miscellaneous.rst:especially with GPUs. Now this is printed to md.log and stderr.
docs/release-notes/2018/major/miscellaneous.rst:Changed to no longer allow multiple energy groups for GPU runs
docs/release-notes/2018/major/highlights.rst:without GPUs, and all enabled and automated by default. We are
docs/release-notes/2018/major/highlights.rst:* PME long-ranged interactions can now run on a single GPU, which
docs/release-notes/2018/2018.2.rst:Prevented OpenCL timing memory leak
docs/release-notes/2018/2018.2.rst:When using OpenCL builds and timing, a memory leak would lead to all system memory being used up.
docs/release-notes/2018/2018.2.rst:Fixed CUDA compilation on Windows.
docs/release-notes/2018/2018.2.rst:* Reporting about GPU detection has improved.
docs/CMakeLists.txt:            GMX_CUDA_MINIMUM_REQUIRED_COMPUTE_CAPABILITY
docs/CMakeLists.txt:            GMX_CUDA_MINIMUM_REQUIRED_VERSION
docs/CMakeLists.txt:            REQUIRED_OPENCL_MIN_VERSION
docs/nblib/guide-to-writing-MD-programs.rst:| ``useGpu``           | Bool | Use GPU for non-bonded computations   |
docs/nblib/listed-data-format.rst:  Type aggregates also likely simplify an efficient GPU implementation of listed forces.
docs/reference-manual/algorithms/molecular-dynamics.rst:or CUDA on GPUs. At neighbor search steps, a pair list is created with a
docs/reference-manual/algorithms/molecular-dynamics.rst:interaction kernel. On the GPU this pruning is overlapped with the
docs/reference-manual/algorithms/molecular-dynamics.rst:(GPU CUDA kernels and AVX 256-bit single precision kernels) or
docs/reference-manual/algorithms/parallelization-domain-decomp.rst:work on GPUs, we overlap communication and work on the CPU with
docs/reference-manual/algorithms/parallelization-domain-decomp.rst:calculation on the GPU. Therefore we measure from the last communication
docs/reference-manual/algorithms/parallelization-domain-decomp.rst:before the force calculation to when the CPU or GPU is finished,
docs/reference-manual/special/nnpot.rst:environment variable ``GMX_NN_DEVICE``. For now, only ``cpu`` and ``cuda`` are supported.
docs/reference-manual/special/nnpot.rst:For ``cuda``, a CUDA-aware LibTorch installation should be installed, and the corresponding
docs/reference-manual/special/nnpot.rst:CUDA installation should be available in your ``PATH``. 
docs/conf.cmakein.py:    ("GMX_CUDA_MINIMUM_REQUIRED_VERSION", "@GMX_CUDA_MINIMUM_REQUIRED_VERSION@"),
docs/conf.cmakein.py:        "GMX_CUDA_MINIMUM_REQUIRED_COMPUTE_CAPABILITY",
docs/conf.cmakein.py:        "@GMX_CUDA_MINIMUM_REQUIRED_COMPUTE_CAPABILITY@",
docs/conf.cmakein.py:    ("REQUIRED_OPENCL_MIN_VERSION", "@REQUIRED_OPENCL_MIN_VERSION@"),
docs/conf.cmakein.py:.. _CUDA: https://developer.nvidia.com/cuda-zone
docs/conf.cmakein.py:.. _OpenCL: https://www.khronos.org/opencl/
docs/conf.cmakein.py:.. _ROCm: https://rocm.docs.amd.com/en/latest/index.html
docs/conf.cmakein.py:.. _rocPrim: https://rocm.docs.amd.com/projects/rocPRIM/en/latest/index.html
tests/CMakeLists.txt:            LABELS "SlowGpuTest"
admin/clang-tidy.sh:    $RUN_CLANG_TIDY `cat $tmpdir/filelist_clangtidy` -header-filter=${HEADER_FILTER} -j $concurrency -fix -quiet -extra-arg=--cuda-host-only -extra-arg=-nocudainc>$tmpdir/clang-tidy.out 2>&1
admin/gitlab-ci/python-gmxapi.matrix/python-gmxapi-current-gromacs.gitlab-ci.yml:#   GPU: unspecified
admin/gitlab-ci/python-gmxapi.matrix/python-gmxapi-current-gromacs.gitlab-ci.yml:  image: ${CI_REGISTRY_IMAGE}/ci-ubuntu-22.04-llvm-14-cuda-12.1.0
admin/gitlab-ci/python-gmxapi.matrix/python-gmxapi-current-gromacs-mpi.gitlab-ci.yml:#   GPU: unspecified
admin/gitlab-ci/gromacs.gitlab-ci.yml:#   GPU: no
admin/gitlab-ci/gromacs.gitlab-ci.yml:    CMAKE_GPU_OPTIONS: "-DGMX_GPU=OFF"
admin/gitlab-ci/documentation.gitlab-ci.yml:#   GPU: no
admin/gitlab-ci/documentation.gitlab-ci.yml:    CMAKE_GPU_OPTIONS: "-DGMX_GPU=OFF"
admin/gitlab-ci/documentation.gitlab-ci.yml:    - echo $CMAKE_GPU_OPTIONS
admin/gitlab-ci/documentation.gitlab-ci.yml:      $CMAKE_GPU_OPTIONS
admin/gitlab-ci/documentation.gitlab-ci.yml:    CMAKE_GPU_OPTIONS: "-DGMX_GPU=OFF"
admin/gitlab-ci/documentation.gitlab-ci.yml:    - echo $CMAKE_GPU_OPTIONS
admin/gitlab-ci/documentation.gitlab-ci.yml:    - echo $CMAKE_GPU_OPTIONS
admin/gitlab-ci/documentation.gitlab-ci.yml:      $CMAKE_GPU_OPTIONS
admin/gitlab-ci/api-client.matrix/gromacs-main-mpi.gitlab-ci.yml:#   GPU: unspecified
admin/gitlab-ci/api-client.matrix/gromacs-main.gitlab-ci.yml:#   GPU: unspecified
admin/gitlab-ci/api-client.matrix/gromacs-main.gitlab-ci.yml:  image: ${CI_REGISTRY_IMAGE}/ci-ubuntu-22.04-gcc-11-adaptivecpp-23.10.0-rocm-5.7.1
admin/gitlab-ci/api-client.matrix/gromacs-main.gitlab-ci.yml:    - job: gromacs:acpp-rocm:build
admin/gitlab-ci/api-client.matrix/gromacs-main.gitlab-ci.yml:  image: ${CI_REGISTRY_IMAGE}/ci-ubuntu-22.04-llvm-14-cuda-12.1.0
admin/gitlab-ci/gromacs.matrix.gitlab-ci.yml:    CMAKE_GPU_OPTIONS: "-DGMX_GPU=OFF"
admin/gitlab-ci/gromacs.matrix.gitlab-ci.yml:    CMAKE_GPU_OPTIONS: "-DGMX_GPU=OFF"
admin/gitlab-ci/gromacs.matrix.gitlab-ci.yml:    - echo $CMAKE_GPU_OPTIONS
admin/gitlab-ci/gromacs.matrix.gitlab-ci.yml:      $CMAKE_GPU_OPTIONS
admin/gitlab-ci/gromacs.matrix.gitlab-ci.yml:  - local: '/admin/gitlab-ci/gromacs.matrix/gromacs.clang-14-cuda-12.1-release.gitlab-ci.yml'
admin/gitlab-ci/gromacs.matrix.gitlab-ci.yml:  - local: '/admin/gitlab-ci/gromacs.matrix/gromacs.clang-18-cuda-12.6.0.gitlab-ci.yml'
admin/gitlab-ci/gromacs.matrix.gitlab-ci.yml:  - local: '/admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml'
admin/gitlab-ci/gromacs.matrix.gitlab-ci.yml:  - local: '/admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0-mpi.gitlab-ci.yml'
admin/gitlab-ci/gromacs.matrix.gitlab-ci.yml:  - local: '/admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1.gitlab-ci.yml'
admin/gitlab-ci/gromacs.matrix.gitlab-ci.yml:  - local: '/admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-cudagraphs.gitlab-ci.yml'
admin/gitlab-ci/gromacs.matrix.gitlab-ci.yml:  - local: '/admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-release.gitlab-ci.yml'
admin/gitlab-ci/gromacs.matrix.gitlab-ci.yml:  - local: '/admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-nvshmem.gitlab-ci.yml'
admin/gitlab-ci/gromacs.matrix.gitlab-ci.yml:  - local: '/admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-opencl-release.gitlab-ci.yml'
admin/gitlab-ci/gromacs.matrix.gitlab-ci.yml:  - local: '/admin/gitlab-ci/gromacs.matrix/gromacs.nvcxx-24.7-cuda-12.5.1.gitlab-ci.yml'
admin/gitlab-ci/gromacs.matrix.gitlab-ci.yml:  - local: '/admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.0-opencl.gitlab-ci.yml'
admin/gitlab-ci/gromacs.matrix.gitlab-ci.yml:  - local: '/admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.0-opencl-release.gitlab-ci.yml'
admin/gitlab-ci/gromacs.matrix.gitlab-ci.yml:  - local: '/admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.2-nvidia-nightly.gitlab-ci.yml'
admin/gitlab-ci/gromacs.matrix.gitlab-ci.yml:  - local: '/admin/gitlab-ci/gromacs.matrix/gromacs.amdclang-rocm-5.7.1.gitlab-ci.yml'
admin/gitlab-ci/gromacs.matrix.gitlab-ci.yml:  - local: '/admin/gitlab-ci/gromacs.matrix/gromacs.gcc-11-rocm-6.2.2.gitlab-ci.yml'
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.0-opencl.gitlab-ci.yml:# Test goal: Newest ICPX build with OpenCL
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.0-opencl.gitlab-ci.yml:#   GPU: OpenCL
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.0-opencl.gitlab-ci.yml:gromacs:oneapi-2024.0-opencl:configure:
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.0-opencl.gitlab-ci.yml:   - .use-opencl
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.0-opencl.gitlab-ci.yml:gromacs:oneapi-2024.0-opencl:build:
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.0-opencl.gitlab-ci.yml:    - job: gromacs:oneapi-2024.0-opencl:configure
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.0-opencl.gitlab-ci.yml:gromacs:oneapi-2024.0-opencl:test:
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.0-opencl.gitlab-ci.yml:    - job: gromacs:oneapi-2024.0-opencl:build
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.0-opencl.gitlab-ci.yml:gromacs:oneapi-2024.0-opencl:regressiontest:
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.0-opencl.gitlab-ci.yml:    - job: gromacs:oneapi-2024.0-opencl:build
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.2-amd-nightly.gitlab-ci.yml:# Test goal: Newest oneAPI with SYCL with tests on AMD GPUs
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.2-amd-nightly.gitlab-ci.yml:#   GPU: SYCL DPC++ (newest supported AMD backend)
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.2-amd-nightly.gitlab-ci.yml:#   GPU: Codeplay plugin for ROCm 6.1.3
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.2-amd-nightly.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-oneapi-2024.2-cuda-12.0.1-rocm-6.1.3:latest
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.2-amd-nightly.gitlab-ci.yml:    CMAKE_EXTRA_OPTIONS: "-DSYCL_CXX_FLAGS_EXTRA=-fsycl-targets=amd_gpu_gfx1034 -DGMX_GPU_NB_CLUSTER_SIZE=8 -DGMX_GPU_FFT_LIBRARY=vkfft -DGMX_SYCL_ENABLE_GRAPHS=ON"
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.2-amd-nightly.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-oneapi-2024.2-cuda-12.0.1-rocm-6.1.3:latest
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.2-amd-nightly.gitlab-ci.yml:    - !reference [.rules:skip-if-single-amd-gpu-unavailable, rules]
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.2-amd-nightly.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-oneapi-2024.2-cuda-12.0.1-rocm-6.1.3:latest
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.2-amd-nightly.gitlab-ci.yml:    SYCL_DEVICE_FILTER: "ext_oneapi_hip:gpu"
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.2-amd-nightly.gitlab-ci.yml:    GMX_TEST_LABELS: "QuickGpuTest|SlowGpuTest"
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.2-amd-nightly.gitlab-ci.yml:    GPU_VENDOR: "AMD"
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.2-amd-nightly.gitlab-ci.yml:    GPU_COUNT: 1
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.2-amd-nightly.gitlab-ci.yml:    - $GITLAB_RUNNER_TAG_1X_AMD_GPU
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.2-amd-nightly.gitlab-ci.yml:    - !reference [.rules:skip-if-single-amd-gpu-unavailable, rules]
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.2-amd-nightly.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-oneapi-2024.2-cuda-12.0.1-rocm-6.1.3:latest
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.2-amd-nightly.gitlab-ci.yml:    SYCL_DEVICE_FILTER: "ext_oneapi_hip:gpu"
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.2-amd-nightly.gitlab-ci.yml:    GPU_VENDOR: "AMD"
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.2-amd-nightly.gitlab-ci.yml:    GPU_COUNT: 1
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.2-amd-nightly.gitlab-ci.yml:    - $GITLAB_RUNNER_TAG_1X_AMD_GPU
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-11-cp2k-9.1-nightly.gitlab-ci.yml:#   GPU: no
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-11-cp2k-9.1-nightly.gitlab-ci.yml:    CMAKE_GPU_OPTIONS: "-DGMX_GPU=OFF"
admin/gitlab-ci/gromacs.matrix/gromacs.clang-14-cxx-20.gitlab-ci.yml:# Test goal: Simple Clang C++20 build with libc++ and no GPU
admin/gitlab-ci/gromacs.matrix/gromacs.clang-14-cxx-20.gitlab-ci.yml:#   GPU: no
admin/gitlab-ci/gromacs.matrix/gromacs.clang-14-cxx-20.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-llvm-14-cuda-12.1.0
admin/gitlab-ci/gromacs.matrix/gromacs.clang-14-cxx-20.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-llvm-14-cuda-12.1.0
admin/gitlab-ci/gromacs.matrix/gromacs.clang-14-cxx-20.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-llvm-14-cuda-12.1.0
admin/gitlab-ci/gromacs.matrix/gromacs.clang-14-cxx-20.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-llvm-14-cuda-12.1.0
admin/gitlab-ci/gromacs.matrix/gromacs.clang-UBSAN.gitlab-ci.yml:#   GPU: no
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-coverage.gitlab-ci.yml:#   GPU: no
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-coverage.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-12-rocm-5.4.1-2
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-coverage.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-12-rocm-5.4.1-2
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-coverage.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-12-rocm-5.4.1-2
admin/gitlab-ci/gromacs.matrix/gromacs.clang-18-cuda-12.6.0.gitlab-ci.yml:# Test goal: Clang-CUDA build
admin/gitlab-ci/gromacs.matrix/gromacs.clang-18-cuda-12.6.0.gitlab-ci.yml:#   GPU: Clang CUDA
admin/gitlab-ci/gromacs.matrix/gromacs.clang-18-cuda-12.6.0.gitlab-ci.yml:#   HW: NVIDIA GPU
admin/gitlab-ci/gromacs.matrix/gromacs.clang-18-cuda-12.6.0.gitlab-ci.yml:#   GPU: Clang CUDA 12.6.0,
admin/gitlab-ci/gromacs.matrix/gromacs.clang-18-cuda-12.6.0.gitlab-ci.yml:gromacs:clang-18-cuda-12.6.0:configure:
admin/gitlab-ci/gromacs.matrix/gromacs.clang-18-cuda-12.6.0.gitlab-ci.yml:    - .use-cuda
admin/gitlab-ci/gromacs.matrix/gromacs.clang-18-cuda-12.6.0.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-24.04-llvm-18-cuda-12.6.0-adaptivecpp-24.02.0-rocm-6.2
admin/gitlab-ci/gromacs.matrix/gromacs.clang-18-cuda-12.6.0.gitlab-ci.yml:    CMAKE_EXTRA_OPTIONS: "-DGMX_CLANG_CUDA=ON"
admin/gitlab-ci/gromacs.matrix/gromacs.clang-18-cuda-12.6.0.gitlab-ci.yml:gromacs:clang-18-cuda-12.6.0:build:
admin/gitlab-ci/gromacs.matrix/gromacs.clang-18-cuda-12.6.0.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-24.04-llvm-18-cuda-12.6.0-adaptivecpp-24.02.0-rocm-6.2
admin/gitlab-ci/gromacs.matrix/gromacs.clang-18-cuda-12.6.0.gitlab-ci.yml:    - job: gromacs:clang-18-cuda-12.6.0:configure
admin/gitlab-ci/gromacs.matrix/gromacs.clang-18-cuda-12.6.0.gitlab-ci.yml:gromacs:clang-18-cuda-12.6.0:test:
admin/gitlab-ci/gromacs.matrix/gromacs.clang-18-cuda-12.6.0.gitlab-ci.yml:    - !reference [.rules:skip-if-single-nvidia-gpu-unavailable, rules]
admin/gitlab-ci/gromacs.matrix/gromacs.clang-18-cuda-12.6.0.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-24.04-llvm-18-cuda-12.6.0-adaptivecpp-24.02.0-rocm-6.2
admin/gitlab-ci/gromacs.matrix/gromacs.clang-18-cuda-12.6.0.gitlab-ci.yml:    GMX_TEST_LABELS: "QuickGpuTest|SlowGpuTest"
admin/gitlab-ci/gromacs.matrix/gromacs.clang-18-cuda-12.6.0.gitlab-ci.yml:    GPU_VENDOR: "NVIDIA"
admin/gitlab-ci/gromacs.matrix/gromacs.clang-18-cuda-12.6.0.gitlab-ci.yml:    GPU_COUNT: 1
admin/gitlab-ci/gromacs.matrix/gromacs.clang-18-cuda-12.6.0.gitlab-ci.yml:    - $GITLAB_RUNNER_TAG_1X_NVIDIA_GPU
admin/gitlab-ci/gromacs.matrix/gromacs.clang-18-cuda-12.6.0.gitlab-ci.yml:    - job: gromacs:clang-18-cuda-12.6.0:build
admin/gitlab-ci/gromacs.matrix/gromacs.clang-18-cuda-12.6.0.gitlab-ci.yml:gromacs:clang-18-cuda-12.6.0:regressiontest:
admin/gitlab-ci/gromacs.matrix/gromacs.clang-18-cuda-12.6.0.gitlab-ci.yml:    - !reference [.rules:skip-if-single-nvidia-gpu-unavailable, rules]
admin/gitlab-ci/gromacs.matrix/gromacs.clang-18-cuda-12.6.0.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-24.04-llvm-18-cuda-12.6.0-adaptivecpp-24.02.0-rocm-6.2
admin/gitlab-ci/gromacs.matrix/gromacs.clang-18-cuda-12.6.0.gitlab-ci.yml:    GPU_VENDOR: NVIDIA
admin/gitlab-ci/gromacs.matrix/gromacs.clang-18-cuda-12.6.0.gitlab-ci.yml:    GPU_COUNT: 1
admin/gitlab-ci/gromacs.matrix/gromacs.clang-18-cuda-12.6.0.gitlab-ci.yml:    - $GITLAB_RUNNER_TAG_1X_NVIDIA_GPU
admin/gitlab-ci/gromacs.matrix/gromacs.clang-18-cuda-12.6.0.gitlab-ci.yml:    - job: gromacs:clang-18-cuda-12.6.0:build
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-11-rocm-6.2.2.gitlab-ci.yml:#   GPU: HIP
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-11-rocm-6.2.2.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-24.04-gcc-11-rocm-6.2.2
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-11-rocm-6.2.2.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-24.04-gcc-11-rocm-6.2.2
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-11-rocm-6.2.2.gitlab-ci.yml:    - !reference [.rules:skip-if-single-amd-gpu-unavailable, rules]
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-11-rocm-6.2.2.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-24.04-gcc-11-rocm-6.2.2
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-11-rocm-6.2.2.gitlab-ci.yml:    GMX_TEST_LABELS: "QuickGpuTest|SlowGpuTest"
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-11-rocm-6.2.2.gitlab-ci.yml:    GPU_VENDOR: "AMD"
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-11-rocm-6.2.2.gitlab-ci.yml:    GPU_COUNT: 1         
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-11-rocm-6.2.2.gitlab-ci.yml:    - $GITLAB_RUNNER_TAG_1X_AMD_GPU
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-11-rocm-6.2.2.gitlab-ci.yml:    - !reference [.rules:skip-if-single-amd-gpu-unavailable, rules]
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-11-rocm-6.2.2.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-24.04-gcc-11-rocm-6.2.2
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-11-rocm-6.2.2.gitlab-ci.yml:    - $GITLAB_RUNNER_TAG_1X_AMD_GPU
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-11-rocm-6.2.2.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-24.04-gcc-11-rocm-6.2.2
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-11-rocm-6.2.2.gitlab-ci.yml:    CMAKE_EXTRA_OPTIONS: "-DGMX_HIP_TARGET_ARCH=gfx906 -DGMX_GPU_FFT_LIBRARY=ROCFFT"
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-11-rocm-6.2.2.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-24.04-gcc-11-rocm-6.2.2
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-11-rocm-6.2.2.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-24.04-gcc-11-rocm-6.2.2
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-11-rocm-6.2.2.gitlab-ci.yml:    GPU_VENDOR: "AMD"
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-11-rocm-6.2.2.gitlab-ci.yml:    GPU_COUNT: 2
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-11-rocm-6.2.2.gitlab-ci.yml:    GMX_TEST_LABELS: "QuickGpuTest|SlowGpuTest"
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-11-rocm-6.2.2.gitlab-ci.yml:    - amd-gpu-gfx906
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-11-rocm-6.2.2.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-24.04-gcc-11-rocm-6.2.2
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-11-rocm-6.2.2.gitlab-ci.yml:    GPU_VENDOR: "AMD"
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-11-rocm-6.2.2.gitlab-ci.yml:    GPU_COUNT: 2
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-11-rocm-6.2.2.gitlab-ci.yml:    - amd-gpu-gfx906
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:# Test goal: old versions of GCC with CUDA; GPU communications with tMPI (default)
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:#   OS: Ubuntu oldest supported on NGC with the chosen CUDA
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:#   Compiler: GCC oldest supported with the chosen CUDA
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:#   GPU: CUDA oldest supported
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:#   HW: NVIDIA GPU, dual NVIDIA GPU
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:#   Features: GPU direct communications + update (unit tests)
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:#   Features: GPU direct communications + update (regression tests with dual GPU)
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:#   Features: GPU update (regression tests with dual GPU)
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:#   GPU: CUDA 12.1.0
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:#   GPU direct communication: on / default
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:#   Parallelism nt/ntomp: 4/1 (regression tests with dual GPU)
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:gromacs:gcc-12-cuda-12.1.0:configure:
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:    - .use-cuda
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-12-cuda-12.1.0
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:gromacs:gcc-12-cuda-12.1.0:build:
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-12-cuda-12.1.0
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:    - job: gromacs:gcc-12-cuda-12.1.0:configure
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:gromacs:gcc-12-cuda-12.1.0:test:
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:    - !reference [.rules:skip-if-single-nvidia-gpu-unavailable, rules]
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-12-cuda-12.1.0
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:    GMX_TEST_LABELS: "QuickGpuTest|SlowGpuTest"
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:    GPU_VENDOR: "NVIDIA"
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:    GPU_COUNT: 1
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:    - $GITLAB_RUNNER_TAG_1X_NVIDIA_GPU
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:    - job: gromacs:gcc-12-cuda-12.1.0:build
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:gromacs:gcc-12-cuda-12.1.0:test-gpucommupd:
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:    - !reference [.rules:skip-if-single-nvidia-gpu-unavailable, rules]
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-12-cuda-12.1.0
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:    GMX_TEST_LABELS: "QuickGpuTest|SlowGpuTest"
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:    GPU_VENDOR: "NVIDIA"
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:    GPU_COUNT: 1
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:    - $GITLAB_RUNNER_TAG_1X_NVIDIA_GPU
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:    - job: gromacs:gcc-12-cuda-12.1.0:build
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:gromacs:gcc-12-cuda-12.1.0:test-gpucommupd-cuda-sanitizer:
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:    - !reference [.rules:skip-if-single-nvidia-gpu-unavailable, rules]
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-12-cuda-12.1.0
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:    GPU_VENDOR: "NVIDIA"
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:    GPU_COUNT: 1
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:    - bash -x admin/ci-scripts/gromacs-base-cuda-sanitizer-test.sh
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:    - $GITLAB_RUNNER_TAG_1X_NVIDIA_GPU
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:    - job: gromacs:gcc-12-cuda-12.1.0:build
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:gromacs:gcc-12-cuda-12.1.0:regressiontest:
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:    - !reference [.rules:skip-if-single-nvidia-gpu-unavailable, rules]
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-12-cuda-12.1.0
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:    GPU_VENDOR: "NVIDIA"
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:    GPU_COUNT: 1
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:    - $GITLAB_RUNNER_TAG_1X_NVIDIA_GPU
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:    - job: gromacs:gcc-12-cuda-12.1.0:build
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:gromacs:gcc-12-cuda-12.1.0:regressiontest-gpucommupd-tMPI:
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:    - !reference [.rules:skip-if-dual-nvidia-gpus-unavailable, rules]
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-12-cuda-12.1.0
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:    GMX_ENABLE_DIRECT_GPU_COMM: 1
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:    GPU_VENDOR: "NVIDIA"
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:    GPU_COUNT: 2
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:    - $GITLAB_RUNNER_TAG_2X_NVIDIA_GPU
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:    - job: gromacs:gcc-12-cuda-12.1.0:build
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:gromacs:gcc-12-cuda-12.1.0:regressiontest-upd-tMPI:
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:    - !reference [.rules:skip-if-dual-nvidia-gpus-unavailable, rules]
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-12-cuda-12.1.0
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:    GPU_VENDOR: "NVIDIA"
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:    GPU_COUNT: 2
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:    - $GITLAB_RUNNER_TAG_2X_NVIDIA_GPU
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0.gitlab-ci.yml:    - job: gromacs:gcc-12-cuda-12.1.0:build
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.0-opencl-release.gitlab-ci.yml:# Test goal: Newest ICPX build with OpenCL, release build
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.0-opencl-release.gitlab-ci.yml:#   GPU: OpenCL
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.0-opencl-release.gitlab-ci.yml:gromacs:oneapi-2024.0-opencl:release:configure:
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.0-opencl-release.gitlab-ci.yml:   - .use-opencl
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.0-opencl-release.gitlab-ci.yml:gromacs:oneapi-2024.0-opencl:release:build:
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.0-opencl-release.gitlab-ci.yml:    - .use-opencl
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.0-opencl-release.gitlab-ci.yml:    - job: gromacs:oneapi-2024.0-opencl:release:configure
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.0-opencl-release.gitlab-ci.yml:gromacs:oneapi-2024.0-opencl:release:test:
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.0-opencl-release.gitlab-ci.yml:    - job: gromacs:oneapi-2024.0-opencl:release:configure
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.0-opencl-release.gitlab-ci.yml:    - job: gromacs:oneapi-2024.0-opencl:release:build
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.0-opencl-release.gitlab-ci.yml:gromacs:oneapi-2024.0-opencl:release:regressiontest:
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.0-opencl-release.gitlab-ci.yml:    - job: gromacs:oneapi-2024.0-opencl:release:build
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-23.10.gitlab-ci.yml:# Test goal: build with AdaptiveCpp/hipSYCL (ROCm backend) to check SYCL code compatibility
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-23.10.gitlab-ci.yml:#   GPU: AdaptiveCpp
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-23.10.gitlab-ci.yml:#   Compiler: AMD Clang 16 from ROCm 5.7.1
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-23.10.gitlab-ci.yml:gromacs:acpp-rocm:configure:
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-23.10.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-11-adaptivecpp-23.10.0-rocm-5.7.1
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-23.10.gitlab-ci.yml:    CMAKE_GPU_OPTIONS: "-DGMX_GPU=SYCL -DGMX_SYCL=ACPP -DACPP_TARGETS=hip:gfx906,gfx1034 -DGMX_GPU_FFT_LIBRARY=vkFFT"
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-23.10.gitlab-ci.yml:gromacs:acpp-rocm:build:
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-23.10.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-11-adaptivecpp-23.10.0-rocm-5.7.1
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-23.10.gitlab-ci.yml:    - job: gromacs:acpp-rocm:configure
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-23.10.gitlab-ci.yml:gromacs:acpp-rocm:test:
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-23.10.gitlab-ci.yml:    - !reference [.rules:skip-if-single-amd-gpu-unavailable, rules]
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-23.10.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-11-adaptivecpp-23.10.0-rocm-5.7.1
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-23.10.gitlab-ci.yml:    GMX_TEST_LABELS: "QuickGpuTest|SlowGpuTest"
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-23.10.gitlab-ci.yml:    GPU_VENDOR: "AMD"
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-23.10.gitlab-ci.yml:    GPU_COUNT: 1
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-23.10.gitlab-ci.yml:    - $GITLAB_RUNNER_TAG_1X_AMD_GPU
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-23.10.gitlab-ci.yml:    - job: gromacs:acpp-rocm:build
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-23.10.gitlab-ci.yml:gromacs:acpp-rocm:regressiontest:
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-23.10.gitlab-ci.yml:    - !reference [.rules:skip-if-single-amd-gpu-unavailable, rules]
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-23.10.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-11-adaptivecpp-23.10.0-rocm-5.7.1
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-23.10.gitlab-ci.yml:    GPU_VENDOR: "AMD"
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-23.10.gitlab-ci.yml:    GPU_COUNT: 1
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-23.10.gitlab-ci.yml:    - $GITLAB_RUNNER_TAG_1X_AMD_GPU
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-23.10.gitlab-ci.yml:    - job: gromacs:acpp-rocm:build
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-23.10.gitlab-ci.yml:gromacs:acpp-rocm-nightly:configure:
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-23.10.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-11-adaptivecpp-23.10.0-rocm-5.7.1
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-23.10.gitlab-ci.yml:    CMAKE_GPU_OPTIONS: "-DGMX_GPU=SYCL -DGMX_SYCL=ACPP -DACPP_TARGETS='hip:gfx906'"
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-23.10.gitlab-ci.yml:gromacs:acpp-rocm-nightly:build:
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-23.10.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-11-adaptivecpp-23.10.0-rocm-5.7.1
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-23.10.gitlab-ci.yml:    - job: gromacs:acpp-rocm-nightly:configure
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-23.10.gitlab-ci.yml:# This is a temporary job that runs tests on an old GPU that
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-23.10.gitlab-ci.yml:gromacs:acpp-rocm-nightly:test:
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-23.10.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-11-adaptivecpp-23.10.0-rocm-5.7.1
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-23.10.gitlab-ci.yml:    GMX_TEST_LABELS: "QuickGpuTest|SlowGpuTest"
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-23.10.gitlab-ci.yml:    GPU_VENDOR: "AMD"
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-23.10.gitlab-ci.yml:    GPU_COUNT: 2
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-23.10.gitlab-ci.yml:    - amd-gpu-gfx906
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-23.10.gitlab-ci.yml:    - job: gromacs:acpp-rocm-nightly:build
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-23.10.gitlab-ci.yml:gromacs:acpp-rocm-nightly:regressiontest:
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-23.10.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-11-adaptivecpp-23.10.0-rocm-5.7.1
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-23.10.gitlab-ci.yml:    GPU_VENDOR: "AMD"
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-23.10.gitlab-ci.yml:    GPU_COUNT: 2
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-23.10.gitlab-ci.yml:    - amd-gpu-gfx906
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-23.10.gitlab-ci.yml:    - job: gromacs:acpp-rocm-nightly:build
admin/gitlab-ci/gromacs.matrix/gromacs.clang-ASAN.gitlab-ci.yml:#   GPU: no
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cxx-20.gitlab-ci.yml:# Test goal: Simple GCC C++20 build with libstdc++ debug checks and no GPU
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cxx-20.gitlab-ci.yml:#   GPU: no
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cxx-20.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-12-rocm-5.4.1-2
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cxx-20.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-12-rocm-5.4.1-2
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cxx-20.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-12-rocm-5.4.1-2
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cxx-20.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-12-rocm-5.4.1-2
admin/gitlab-ci/gromacs.matrix/gromacs.nvcxx-24.7-cuda-12.5.1.gitlab-ci.yml:# Test goal: CUDA GPU communications with OpenMPI
admin/gitlab-ci/gromacs.matrix/gromacs.nvcxx-24.7-cuda-12.5.1.gitlab-ci.yml:#   GPU: CUDA newest supported with nvc++
admin/gitlab-ci/gromacs.matrix/gromacs.nvcxx-24.7-cuda-12.5.1.gitlab-ci.yml:#   HW: NVIDIA GPU
admin/gitlab-ci/gromacs.matrix/gromacs.nvcxx-24.7-cuda-12.5.1.gitlab-ci.yml:#   Features: GPU direct communications (manual) + PME decomposition
admin/gitlab-ci/gromacs.matrix/gromacs.nvcxx-24.7-cuda-12.5.1.gitlab-ci.yml:#   GPU: CUDA 12.5.1
admin/gitlab-ci/gromacs.matrix/gromacs.nvcxx-24.7-cuda-12.5.1.gitlab-ci.yml:#   Parallelism np/ntomp: 4/1 (regression tests with a GPU)
admin/gitlab-ci/gromacs.matrix/gromacs.nvcxx-24.7-cuda-12.5.1.gitlab-ci.yml:gromacs:nvcxx-24.7-cuda-12.5.1:configureMPI:
admin/gitlab-ci/gromacs.matrix/gromacs.nvcxx-24.7-cuda-12.5.1.gitlab-ci.yml:    - .use-cuda
admin/gitlab-ci/gromacs.matrix/gromacs.nvcxx-24.7-cuda-12.5.1.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-13-cuda-12.5.1
admin/gitlab-ci/gromacs.matrix/gromacs.nvcxx-24.7-cuda-12.5.1.gitlab-ci.yml:                          -DGMX_USE_CUFFTMP=ON -DcuFFTMp_ROOT=/opt/nvidia/hpc_sdk/Linux_x86_64/24.7/math_libs/12.5
admin/gitlab-ci/gromacs.matrix/gromacs.nvcxx-24.7-cuda-12.5.1.gitlab-ci.yml:                          -DCMAKE_CXX_COMPILER=/opt/nvidia/hpc_sdk/Linux_x86_64/24.7/compilers/bin/nvc++
admin/gitlab-ci/gromacs.matrix/gromacs.nvcxx-24.7-cuda-12.5.1.gitlab-ci.yml:                          -DCMAKE_C_COMPILER=/opt/nvidia/hpc_sdk/Linux_x86_64/24.7/compilers/bin/nvc"
admin/gitlab-ci/gromacs.matrix/gromacs.nvcxx-24.7-cuda-12.5.1.gitlab-ci.yml:gromacs:nvcxx-24.7-cuda-12.5.1:buildMPI:
admin/gitlab-ci/gromacs.matrix/gromacs.nvcxx-24.7-cuda-12.5.1.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-13-cuda-12.5.1
admin/gitlab-ci/gromacs.matrix/gromacs.nvcxx-24.7-cuda-12.5.1.gitlab-ci.yml:    - job: gromacs:nvcxx-24.7-cuda-12.5.1:configureMPI
admin/gitlab-ci/gromacs.matrix/gromacs.nvcxx-24.7-cuda-12.5.1.gitlab-ci.yml:gromacs:nvcxx-24.7-cuda-12.5.1:testMPI:
admin/gitlab-ci/gromacs.matrix/gromacs.nvcxx-24.7-cuda-12.5.1.gitlab-ci.yml:    - !reference [.rules:skip-if-single-nvidia-gpu-unavailable, rules]
admin/gitlab-ci/gromacs.matrix/gromacs.nvcxx-24.7-cuda-12.5.1.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-13-cuda-12.5.1
admin/gitlab-ci/gromacs.matrix/gromacs.nvcxx-24.7-cuda-12.5.1.gitlab-ci.yml:    GMX_ENABLE_DIRECT_GPU_COMM: 1
admin/gitlab-ci/gromacs.matrix/gromacs.nvcxx-24.7-cuda-12.5.1.gitlab-ci.yml:    GMX_GPU_PME_DECOMPOSITION: 1
admin/gitlab-ci/gromacs.matrix/gromacs.nvcxx-24.7-cuda-12.5.1.gitlab-ci.yml:    # Location of CUDA FFT library, required for preload to avoid cuFFTMp on incompatible devices 
admin/gitlab-ci/gromacs.matrix/gromacs.nvcxx-24.7-cuda-12.5.1.gitlab-ci.yml:    CUFFTLIB: "/opt/nvidia/hpc_sdk/Linux_x86_64/24.7/math_libs/lib64/libcufft.so"
admin/gitlab-ci/gromacs.matrix/gromacs.nvcxx-24.7-cuda-12.5.1.gitlab-ci.yml:    # cuFFTMp uses NVSHMEM, which in turn will use NCCL on single-node when available
admin/gitlab-ci/gromacs.matrix/gromacs.nvcxx-24.7-cuda-12.5.1.gitlab-ci.yml:    # But NCCL has extra restrictions (such as only 1 rank per GPU) so we disable for CI
admin/gitlab-ci/gromacs.matrix/gromacs.nvcxx-24.7-cuda-12.5.1.gitlab-ci.yml:    NVSHMEM_DISABLE_NCCL: 1
admin/gitlab-ci/gromacs.matrix/gromacs.nvcxx-24.7-cuda-12.5.1.gitlab-ci.yml:    # GPUs having 4GB vidmem also this size is overkill for PME decomp tests, hence we
admin/gitlab-ci/gromacs.matrix/gromacs.nvcxx-24.7-cuda-12.5.1.gitlab-ci.yml:    # set CUDA Module load to be lazy in order to save on CUDA libraries memory footprint
admin/gitlab-ci/gromacs.matrix/gromacs.nvcxx-24.7-cuda-12.5.1.gitlab-ci.yml:    # as without lazy loading whole cuFFTMp library may get loaded into GPU RAM.
admin/gitlab-ci/gromacs.matrix/gromacs.nvcxx-24.7-cuda-12.5.1.gitlab-ci.yml:    CUDA_MODULE_LOADING: LAZY
admin/gitlab-ci/gromacs.matrix/gromacs.nvcxx-24.7-cuda-12.5.1.gitlab-ci.yml:    GMX_TEST_LABELS: "QuickGpuTest|SlowGpuTest"
admin/gitlab-ci/gromacs.matrix/gromacs.nvcxx-24.7-cuda-12.5.1.gitlab-ci.yml:    GPU_VENDOR: "NVIDIA"
admin/gitlab-ci/gromacs.matrix/gromacs.nvcxx-24.7-cuda-12.5.1.gitlab-ci.yml:    GPU_COUNT: 1
admin/gitlab-ci/gromacs.matrix/gromacs.nvcxx-24.7-cuda-12.5.1.gitlab-ci.yml:    - $GITLAB_RUNNER_TAG_1X_NVIDIA_GPU
admin/gitlab-ci/gromacs.matrix/gromacs.nvcxx-24.7-cuda-12.5.1.gitlab-ci.yml:    - job: gromacs:nvcxx-24.7-cuda-12.5.1:buildMPI
admin/gitlab-ci/gromacs.matrix/gromacs.nvcxx-24.7-cuda-12.5.1.gitlab-ci.yml:gromacs:nvcxx-24.7-cuda-12.5.1:regressiontest-gpucommupd-MPI:
admin/gitlab-ci/gromacs.matrix/gromacs.nvcxx-24.7-cuda-12.5.1.gitlab-ci.yml:  # Test parallelism GPU: direct communications, update
admin/gitlab-ci/gromacs.matrix/gromacs.nvcxx-24.7-cuda-12.5.1.gitlab-ci.yml:    - !reference [.rules:skip-if-single-nvidia-gpu-unavailable, rules]
admin/gitlab-ci/gromacs.matrix/gromacs.nvcxx-24.7-cuda-12.5.1.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-13-cuda-12.5.1
admin/gitlab-ci/gromacs.matrix/gromacs.nvcxx-24.7-cuda-12.5.1.gitlab-ci.yml:    GMX_ENABLE_DIRECT_GPU_COMM: 1
admin/gitlab-ci/gromacs.matrix/gromacs.nvcxx-24.7-cuda-12.5.1.gitlab-ci.yml:    GPU_VENDOR: "NVIDIA"
admin/gitlab-ci/gromacs.matrix/gromacs.nvcxx-24.7-cuda-12.5.1.gitlab-ci.yml:    GPU_COUNT: 1
admin/gitlab-ci/gromacs.matrix/gromacs.nvcxx-24.7-cuda-12.5.1.gitlab-ci.yml:    - $GITLAB_RUNNER_TAG_1X_NVIDIA_GPU
admin/gitlab-ci/gromacs.matrix/gromacs.nvcxx-24.7-cuda-12.5.1.gitlab-ci.yml:    - job: gromacs:nvcxx-24.7-cuda-12.5.1:buildMPI
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.2-nvidia-nightly.gitlab-ci.yml:# Test goal: Newest oneAPI with SYCL with tests on NVIDIA GPUs
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.2-nvidia-nightly.gitlab-ci.yml:#   GPU: SYCL DPC++ (newest supported CUDA backend)
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.2-nvidia-nightly.gitlab-ci.yml:#   GPU: Codeplay plugin for CUDA 12.0.1
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.2-nvidia-nightly.gitlab-ci.yml:gromacs:oneapi-2024.2-nvidia-nightly:configure:
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.2-nvidia-nightly.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-oneapi-2024.2-cuda-12.0.1-rocm-6.1.3:latest
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.2-nvidia-nightly.gitlab-ci.yml:    CMAKE_EXTRA_OPTIONS: "-DSYCL_CXX_FLAGS_EXTRA=-fsycl-targets=nvptx64-nvidia-cuda -DGMX_GPU_NB_CLUSTER_SIZE=8 -DGMX_GPU_FFT_LIBRARY=VkFFT -DGMX_SYCL_ENABLE_GRAPHS=ON"
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.2-nvidia-nightly.gitlab-ci.yml:gromacs:oneapi-2024.2-nvidia-nightly:build:
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.2-nvidia-nightly.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-oneapi-2024.2-cuda-12.0.1-rocm-6.1.3:latest
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.2-nvidia-nightly.gitlab-ci.yml:    - job: gromacs:oneapi-2024.2-nvidia-nightly:configure
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.2-nvidia-nightly.gitlab-ci.yml:gromacs:oneapi-2024.2-nvidia-nightly:test:
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.2-nvidia-nightly.gitlab-ci.yml:    - !reference [.rules:skip-if-single-nvidia-gpu-unavailable, rules]
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.2-nvidia-nightly.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-oneapi-2024.2-cuda-12.0.1-rocm-6.1.3:latest
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.2-nvidia-nightly.gitlab-ci.yml:    GMX_TEST_LABELS: "QuickGpuTest|SlowGpuTest"
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.2-nvidia-nightly.gitlab-ci.yml:    GPU_VENDOR: "NVIDIA"
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.2-nvidia-nightly.gitlab-ci.yml:    GPU_COUNT: 1
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.2-nvidia-nightly.gitlab-ci.yml:    - $GITLAB_RUNNER_TAG_1X_NVIDIA_GPU
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.2-nvidia-nightly.gitlab-ci.yml:    - job: gromacs:oneapi-2024.2-nvidia-nightly:build
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.2-nvidia-nightly.gitlab-ci.yml:gromacs:oneapi-2024.2-nvidia-nightly:regressiontest:
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.2-nvidia-nightly.gitlab-ci.yml:    - !reference [.rules:skip-if-single-nvidia-gpu-unavailable, rules]
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.2-nvidia-nightly.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-oneapi-2024.2-cuda-12.0.1-rocm-6.1.3:latest
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.2-nvidia-nightly.gitlab-ci.yml:    GPU_VENDOR: "NVIDIA"
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.2-nvidia-nightly.gitlab-ci.yml:    GPU_COUNT: 1
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.2-nvidia-nightly.gitlab-ci.yml:    - $GITLAB_RUNNER_TAG_1X_NVIDIA_GPU
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.2-nvidia-nightly.gitlab-ci.yml:    - job: gromacs:oneapi-2024.2-nvidia-nightly:build
admin/gitlab-ci/gromacs.matrix/gromacs.amdclang-rocm-5.7.1.gitlab-ci.yml:#   GPU: HIP
admin/gitlab-ci/gromacs.matrix/gromacs.amdclang-rocm-5.7.1.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-11-adaptivecpp-23.10.0-rocm-5.7.1
admin/gitlab-ci/gromacs.matrix/gromacs.amdclang-rocm-5.7.1.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-11-adaptivecpp-23.10.0-rocm-5.7.1
admin/gitlab-ci/gromacs.matrix/gromacs.amdclang-rocm-5.7.1.gitlab-ci.yml:    - !reference [.rules:skip-if-single-amd-gpu-unavailable, rules]    
admin/gitlab-ci/gromacs.matrix/gromacs.amdclang-rocm-5.7.1.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-11-adaptivecpp-23.10.0-rocm-5.7.1
admin/gitlab-ci/gromacs.matrix/gromacs.amdclang-rocm-5.7.1.gitlab-ci.yml:    GMX_TEST_LABELS: "QuickGpuTest|SlowGpuTest"
admin/gitlab-ci/gromacs.matrix/gromacs.amdclang-rocm-5.7.1.gitlab-ci.yml:    GPU_VENDOR: "AMD"
admin/gitlab-ci/gromacs.matrix/gromacs.amdclang-rocm-5.7.1.gitlab-ci.yml:    GPU_COUNT: 1         
admin/gitlab-ci/gromacs.matrix/gromacs.amdclang-rocm-5.7.1.gitlab-ci.yml:    - $GITLAB_RUNNER_TAG_1X_AMD_GPU
admin/gitlab-ci/gromacs.matrix/gromacs.amdclang-rocm-5.7.1.gitlab-ci.yml:    - !reference [.rules:skip-if-single-amd-gpu-unavailable, rules]        
admin/gitlab-ci/gromacs.matrix/gromacs.amdclang-rocm-5.7.1.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-11-adaptivecpp-23.10.0-rocm-5.7.1
admin/gitlab-ci/gromacs.matrix/gromacs.amdclang-rocm-5.7.1.gitlab-ci.yml:    GPU_VENDOR: "AMD"
admin/gitlab-ci/gromacs.matrix/gromacs.amdclang-rocm-5.7.1.gitlab-ci.yml:    GPU_COUNT: 1             
admin/gitlab-ci/gromacs.matrix/gromacs.amdclang-rocm-5.7.1.gitlab-ci.yml:    - $GITLAB_RUNNER_TAG_1X_AMD_GPU
admin/gitlab-ci/gromacs.matrix/gromacs.clang-14-cuda-12.1-release.gitlab-ci.yml:# Test goal: old Clang as host compiler with old CUDA
admin/gitlab-ci/gromacs.matrix/gromacs.clang-14-cuda-12.1-release.gitlab-ci.yml:#   OS: Ubuntu oldest supported (except where constrained by CUDA version)
admin/gitlab-ci/gromacs.matrix/gromacs.clang-14-cuda-12.1-release.gitlab-ci.yml:#   GPU: CUDA oldest supported
admin/gitlab-ci/gromacs.matrix/gromacs.clang-14-cuda-12.1-release.gitlab-ci.yml:#   HW: NVIDIA GPU
admin/gitlab-ci/gromacs.matrix/gromacs.clang-14-cuda-12.1-release.gitlab-ci.yml:#   GPU: CUDA 12.1
admin/gitlab-ci/gromacs.matrix/gromacs.clang-14-cuda-12.1-release.gitlab-ci.yml:gromacs:clang-14-cuda.12.1:release:configure:
admin/gitlab-ci/gromacs.matrix/gromacs.clang-14-cuda-12.1-release.gitlab-ci.yml:    - .use-cuda
admin/gitlab-ci/gromacs.matrix/gromacs.clang-14-cuda-12.1-release.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-llvm-14-cuda-12.1.0
admin/gitlab-ci/gromacs.matrix/gromacs.clang-14-cuda-12.1-release.gitlab-ci.yml:gromacs:clang-14-cuda.12.1:release:build:
admin/gitlab-ci/gromacs.matrix/gromacs.clang-14-cuda-12.1-release.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-llvm-14-cuda-12.1.0
admin/gitlab-ci/gromacs.matrix/gromacs.clang-14-cuda-12.1-release.gitlab-ci.yml:    - job: gromacs:clang-14-cuda.12.1:release:configure
admin/gitlab-ci/gromacs.matrix/gromacs.clang-14-cuda-12.1-release.gitlab-ci.yml:gromacs:clang-14-cuda.12.1:release:test:
admin/gitlab-ci/gromacs.matrix/gromacs.clang-14-cuda-12.1-release.gitlab-ci.yml:    - !reference [.rules:skip-if-single-nvidia-gpu-unavailable, rules]
admin/gitlab-ci/gromacs.matrix/gromacs.clang-14-cuda-12.1-release.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-llvm-14-cuda-12.1.0
admin/gitlab-ci/gromacs.matrix/gromacs.clang-14-cuda-12.1-release.gitlab-ci.yml:    GMX_TEST_LABELS: "QuickGpuTest|SlowGpuTest"
admin/gitlab-ci/gromacs.matrix/gromacs.clang-14-cuda-12.1-release.gitlab-ci.yml:    GPU_VENDOR: "NVIDIA"
admin/gitlab-ci/gromacs.matrix/gromacs.clang-14-cuda-12.1-release.gitlab-ci.yml:    GPU_COUNT: 1
admin/gitlab-ci/gromacs.matrix/gromacs.clang-14-cuda-12.1-release.gitlab-ci.yml:    - $GITLAB_RUNNER_TAG_1X_NVIDIA_GPU
admin/gitlab-ci/gromacs.matrix/gromacs.clang-14-cuda-12.1-release.gitlab-ci.yml:    - job: gromacs:clang-14-cuda.12.1:release:configure
admin/gitlab-ci/gromacs.matrix/gromacs.clang-14-cuda-12.1-release.gitlab-ci.yml:    - job: gromacs:clang-14-cuda.12.1:release:build
admin/gitlab-ci/gromacs.matrix/gromacs.clang-14-cuda-12.1-release.gitlab-ci.yml:gromacs:clang-14-cuda.12.1:release:regressiontest:
admin/gitlab-ci/gromacs.matrix/gromacs.clang-14-cuda-12.1-release.gitlab-ci.yml:    - !reference [.rules:skip-if-single-nvidia-gpu-unavailable, rules]
admin/gitlab-ci/gromacs.matrix/gromacs.clang-14-cuda-12.1-release.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-llvm-14-cuda-12.1.0
admin/gitlab-ci/gromacs.matrix/gromacs.clang-14-cuda-12.1-release.gitlab-ci.yml:    GPU_VENDOR: "NVIDIA"
admin/gitlab-ci/gromacs.matrix/gromacs.clang-14-cuda-12.1-release.gitlab-ci.yml:    GPU_COUNT: 1
admin/gitlab-ci/gromacs.matrix/gromacs.clang-14-cuda-12.1-release.gitlab-ci.yml:    - $GITLAB_RUNNER_TAG_1X_NVIDIA_GPU
admin/gitlab-ci/gromacs.matrix/gromacs.clang-14-cuda-12.1-release.gitlab-ci.yml:    - job: gromacs:clang-14-cuda.12.1:release:build
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-cudagraphs.gitlab-ci.yml:# Test goal: CUDA Graphs codepath
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-cudagraphs.gitlab-ci.yml:#   OS: Ubuntu oldest supported on NGC with the chosen CUDA
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-cudagraphs.gitlab-ci.yml:#   GPU: CUDA newest supported with its newest supported gcc
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-cudagraphs.gitlab-ci.yml:#   HW: NVIDIA GPU
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-cudagraphs.gitlab-ci.yml:#   Features: Mdrun using CUDA Graphs (unit tests)
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-cudagraphs.gitlab-ci.yml:#   GPU: CUDA 12.5.1
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-cudagraphs.gitlab-ci.yml:gromacs:gcc-13-cuda-12.5.1:configure:
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-cudagraphs.gitlab-ci.yml:    - .use-cuda
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-cudagraphs.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-13-cuda-12.5.1
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-cudagraphs.gitlab-ci.yml:gromacs:gcc-13-cuda-12.5.1:build:
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-cudagraphs.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-13-cuda-12.5.1
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-cudagraphs.gitlab-ci.yml:    - job: gromacs:gcc-13-cuda-12.5.1:configure
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-cudagraphs.gitlab-ci.yml:gromacs:gcc-13-cuda-12.5.1:test-cudagraph:
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-cudagraphs.gitlab-ci.yml:    - !reference [.rules:skip-if-single-nvidia-gpu-unavailable, rules]
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-cudagraphs.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-13-cuda-12.5.1
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-cudagraphs.gitlab-ci.yml:    GMX_ENABLE_DIRECT_GPU_COMM: 1
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-cudagraphs.gitlab-ci.yml:    GMX_CUDA_GRAPH: 1
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-cudagraphs.gitlab-ci.yml:    GMX_TEST_LABELS: "QuickGpuTest|SlowGpuTest"
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-cudagraphs.gitlab-ci.yml:    GPU_VENDOR: "NVIDIA"
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-cudagraphs.gitlab-ci.yml:    GPU_COUNT: 1
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-cudagraphs.gitlab-ci.yml:    - $GITLAB_RUNNER_TAG_1X_NVIDIA_GPU
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-cudagraphs.gitlab-ci.yml:    - job: gromacs:gcc-13-cuda-12.5.1:build
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-cudagraphs.gitlab-ci.yml:gromacs:gcc-13-cuda-12.5.1:regressiontest-cudagraph:
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-cudagraphs.gitlab-ci.yml:    - !reference [.rules:skip-if-single-nvidia-gpu-unavailable, rules]
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-cudagraphs.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-13-cuda-12.5.1
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-cudagraphs.gitlab-ci.yml:    GMX_CUDA_GRAPH: 1
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-cudagraphs.gitlab-ci.yml:    GPU_VENDOR: "NVIDIA"
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-cudagraphs.gitlab-ci.yml:    GPU_COUNT: 1
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-cudagraphs.gitlab-ci.yml:    - $GITLAB_RUNNER_TAG_1X_NVIDIA_GPU
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-cudagraphs.gitlab-ci.yml:    - job: gromacs:gcc-13-cuda-12.5.1:build
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-cudagraphs.gitlab-ci.yml:gromacs:gcc-13-cuda-12.5.1:regressiontest-cudagraph-gpucomm:
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-cudagraphs.gitlab-ci.yml:    - !reference [.rules:skip-if-dual-nvidia-gpus-unavailable, rules]
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-cudagraphs.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-13-cuda-12.5.1
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-cudagraphs.gitlab-ci.yml:    GMX_CUDA_GRAPH: 1
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-cudagraphs.gitlab-ci.yml:    GMX_ENABLE_DIRECT_GPU_COMM: 1
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-cudagraphs.gitlab-ci.yml:    GPU_VENDOR: "NVIDIA"
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-cudagraphs.gitlab-ci.yml:    GPU_COUNT: 2
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-cudagraphs.gitlab-ci.yml:    - $GITLAB_RUNNER_TAG_2X_NVIDIA_GPU
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-cudagraphs.gitlab-ci.yml:    - job: gromacs:gcc-13-cuda-12.5.1:build
admin/gitlab-ci/gromacs.matrix/gromacs.clang-19-release.gitlab-ci.yml:#   GPU: no
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-24.02.gitlab-ci.yml:# Test goal: build with AdaptiveCpp (ROCm and NVIDIA backends) to check SYCL code compatibility
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-24.02.gitlab-ci.yml:#   GPU: AdaptiveCpp
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-24.02.gitlab-ci.yml:#   GPU: AdaptiveCpp 24.02 + CUDA 12.6 + ROCm 6.2
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-24.02.gitlab-ci.yml:#   MPI: CUDA-aware MPICH (ROCm build); threadMPI (CUDA build)
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-24.02.gitlab-ci.yml:gromacs:acpp-rocm-mpi:configure:
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-24.02.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-24.04-llvm-18-cuda-12.6.0-adaptivecpp-24.02.0-rocm-6.2
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-24.02.gitlab-ci.yml:    CMAKE_GPU_OPTIONS: "-DGMX_GPU=SYCL -DGMX_SYCL=ACPP -DACPP_TARGETS=hip:gfx906,gfx1034 -DGMX_GPU_FFT_LIBRARY=vkFFT"
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-24.02.gitlab-ci.yml:gromacs:acpp-rocm-mpi:build:
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-24.02.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-24.04-llvm-18-cuda-12.6.0-adaptivecpp-24.02.0-rocm-6.2
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-24.02.gitlab-ci.yml:    - job: gromacs:acpp-rocm-mpi:configure
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-24.02.gitlab-ci.yml:gromacs:acpp-rocm-mpi:test:
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-24.02.gitlab-ci.yml:    - !reference [.rules:skip-if-dual-amd-gpus-unavailable, rules]
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-24.02.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-24.04-llvm-18-cuda-12.6.0-adaptivecpp-24.02.0-rocm-6.2
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-24.02.gitlab-ci.yml:    GMX_TEST_LABELS: "QuickGpuTest|SlowGpuTest"
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-24.02.gitlab-ci.yml:    GMX_ENABLE_DIRECT_GPU_COMM: 1
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-24.02.gitlab-ci.yml:    GMX_FORCE_GPU_AWARE_MPI: 1
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-24.02.gitlab-ci.yml:    UCX_TLS: "sm,self,rocm_copy"  # rocm_ipc does not work well enough
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-24.02.gitlab-ci.yml:    GPU_VENDOR: "AMD"
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-24.02.gitlab-ci.yml:    GPU_COUNT: 2
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-24.02.gitlab-ci.yml:    - $GITLAB_RUNNER_TAG_2X_AMD_GPU
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-24.02.gitlab-ci.yml:    - job: gromacs:acpp-rocm-mpi:build
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-24.02.gitlab-ci.yml:gromacs:acpp-rocm-mpi:regressiontest:
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-24.02.gitlab-ci.yml:    - !reference [.rules:skip-if-dual-amd-gpus-unavailable, rules]
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-24.02.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-24.04-llvm-18-cuda-12.6.0-adaptivecpp-24.02.0-rocm-6.2
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-24.02.gitlab-ci.yml:    GMX_ENABLE_DIRECT_GPU_COMM: 1
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-24.02.gitlab-ci.yml:    GMX_FORCE_GPU_AWARE_MPI: 1
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-24.02.gitlab-ci.yml:    UCX_TLS: "sm,self,rocm_copy"  # rocm_ipc does not work well enough
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-24.02.gitlab-ci.yml:    GPU_VENDOR: "AMD"
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-24.02.gitlab-ci.yml:    GPU_COUNT: 2
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-24.02.gitlab-ci.yml:    - $GITLAB_RUNNER_TAG_2X_AMD_GPU
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-24.02.gitlab-ci.yml:    - job: gromacs:acpp-rocm-mpi:build
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-24.02.gitlab-ci.yml:gromacs:acpp-nvidia:configure:
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-24.02.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-24.04-llvm-18-cuda-12.6.0-adaptivecpp-24.02.0-rocm-6.2
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-24.02.gitlab-ci.yml:    CMAKE_GPU_OPTIONS: "-DGMX_GPU=SYCL -DGMX_SYCL=ACPP -DACPP_TARGETS='cuda:sm_75' -DGMX_GPU_FFT_LIBRARY=VkFFT"
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-24.02.gitlab-ci.yml:gromacs:acpp-nvidia:build:
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-24.02.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-24.04-llvm-18-cuda-12.6.0-adaptivecpp-24.02.0-rocm-6.2
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-24.02.gitlab-ci.yml:    - job: gromacs:acpp-nvidia:configure
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-24.02.gitlab-ci.yml:gromacs:acpp-nvidia:test:
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-24.02.gitlab-ci.yml:    - !reference [.rules:skip-if-single-nvidia-gpu-unavailable, rules]
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-24.02.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-24.04-llvm-18-cuda-12.6.0-adaptivecpp-24.02.0-rocm-6.2
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-24.02.gitlab-ci.yml:    GMX_TEST_LABELS: "QuickGpuTest|SlowGpuTest"
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-24.02.gitlab-ci.yml:    GPU_VENDOR: "NVIDIA"
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-24.02.gitlab-ci.yml:    GPU_COUNT: 1
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-24.02.gitlab-ci.yml:    - $GITLAB_RUNNER_TAG_1X_NVIDIA_GPU
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-24.02.gitlab-ci.yml:    - job: gromacs:acpp-nvidia:build
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-24.02.gitlab-ci.yml:gromacs:acpp-nvidia:regressiontest:
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-24.02.gitlab-ci.yml:    - !reference [.rules:skip-if-single-nvidia-gpu-unavailable, rules]
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-24.02.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-24.04-llvm-18-cuda-12.6.0-adaptivecpp-24.02.0-rocm-6.2
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-24.02.gitlab-ci.yml:    GPU_VENDOR: "NVIDIA"
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-24.02.gitlab-ci.yml:    GPU_COUNT: 1
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-24.02.gitlab-ci.yml:    - $GITLAB_RUNNER_TAG_1X_NVIDIA_GPU
admin/gitlab-ci/gromacs.matrix/gromacs.acpp-24.02.gitlab-ci.yml:    - job: gromacs:acpp-nvidia:build
admin/gitlab-ci/gromacs.matrix/gromacs.clang-TSAN.gitlab-ci.yml:#   GPU: None
admin/gitlab-ci/gromacs.matrix/gromacs.clang-static-analyzer.gitlab-ci.yml:#   GPU: no
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.0-sycl.gitlab-ci.yml:#   GPU: SYCL with Intel GPUs
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.0-sycl.gitlab-ci.yml:    - !reference [.rules:skip-if-single-intel-gpu-unavailable, rules]
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.0-sycl.gitlab-ci.yml:    GMX_TEST_LABELS: "QuickGpuTest|SlowGpuTest"
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.0-sycl.gitlab-ci.yml:    SYCL_DEVICE_FILTER: "opencl:gpu"
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.0-sycl.gitlab-ci.yml:    GPU_VENDOR: "INTEL"
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.0-sycl.gitlab-ci.yml:    GPU_COUNT: 1
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.0-sycl.gitlab-ci.yml:    - $GITLAB_RUNNER_TAG_1X_INTEL_GPU
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.0-sycl.gitlab-ci.yml:    - !reference [.rules:skip-if-single-intel-gpu-unavailable, rules]
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.0-sycl.gitlab-ci.yml:    GMX_TEST_LABELS: "QuickGpuTest|SlowGpuTest"
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.0-sycl.gitlab-ci.yml:    SYCL_DEVICE_FILTER: "opencl:gpu"
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.0-sycl.gitlab-ci.yml:    GPU_VENDOR: "INTEL"
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.0-sycl.gitlab-ci.yml:    GPU_COUNT: 1
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.0-sycl.gitlab-ci.yml:    - $GITLAB_RUNNER_TAG_1X_INTEL_GPU
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.0-sycl.gitlab-ci.yml:    - !reference [.rules:skip-if-dual-intel-gpus-unavailable, rules]
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.0-sycl.gitlab-ci.yml:    SYCL_DEVICE_FILTER: "opencl:gpu"
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.0-sycl.gitlab-ci.yml:    GPU_VENDOR: "INTEL"
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.0-sycl.gitlab-ci.yml:    GPU_COUNT: 2
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.0-sycl.gitlab-ci.yml:    - $GITLAB_RUNNER_TAG_2X_INTEL_GPU
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.0-sycl.gitlab-ci.yml:    - !reference [.rules:skip-if-single-intel-gpu-unavailable, rules]
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.0-sycl.gitlab-ci.yml:    SYCL_DEVICE_FILTER: "opencl:gpu"
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.0-sycl.gitlab-ci.yml:    GPU_VENDOR: "INTEL"
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.0-sycl.gitlab-ci.yml:    GPU_COUNT: 1
admin/gitlab-ci/gromacs.matrix/gromacs.oneapi-2024.0-sycl.gitlab-ci.yml:    - $GITLAB_RUNNER_TAG_1X_INTEL_GPU
admin/gitlab-ci/gromacs.matrix/gromacs.clang-19-mpi.gitlab-ci.yml:#   GPU: no
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-nvshmem.gitlab-ci.yml:# Test goal: CUDA GPU communications with OpenMPI,NVSHMEM
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-nvshmem.gitlab-ci.yml:#   OS: Ubuntu oldest supported on NGC with the chosen CUDA
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-nvshmem.gitlab-ci.yml:#   GPU: NVHPC SDK newest supported with its newest supported CUDA and gcc
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-nvshmem.gitlab-ci.yml:#   HW: dual NVIDIA GPU (CC 7.0 or newer required)
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-nvshmem.gitlab-ci.yml:#   Features: NVSHMEM, GPU direct communications (manual) + update (regression tests with dual GPU)
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-nvshmem.gitlab-ci.yml:#   GPU: NVHPC SDK 24.7, CUDA 12.5.1
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-nvshmem.gitlab-ci.yml:#   Parallelism np/ntomp: 4/1 (regression tests with dual GPU)
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-nvshmem.gitlab-ci.yml:gromacs:gcc-13-cuda-12.5.1:configureMPI-NVSHMEM:
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-nvshmem.gitlab-ci.yml:    - .use-cuda
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-nvshmem.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-13-cuda-12.5.1
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-nvshmem.gitlab-ci.yml:    CMAKE_EXTRA_OPTIONS: "-DGMX_NVSHMEM=ON -DNVSHMEM_ROOT=/opt/nvidia/hpc_sdk/Linux_x86_64/2024/comm_libs/nvshmem"
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-nvshmem.gitlab-ci.yml:gromacs:gcc-13-cuda-12.5.1:buildMPI-NVSHMEM:
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-nvshmem.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-13-cuda-12.5.1
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-nvshmem.gitlab-ci.yml:    CMAKE_EXTRA_OPTIONS: "-DGMX_NVSHMEM=ON -DNVSHMEM_ROOT=/opt/nvidia/hpc_sdk/Linux_x86_64/2024/comm_libs/nvshmem"
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-nvshmem.gitlab-ci.yml:    LD_LIBRARY_FLAGS: /opt/nvidia/hpc_sdk/Linux_x86_64/2024/comm_libs/nvshmem
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-nvshmem.gitlab-ci.yml:    - job: gromacs:gcc-13-cuda-12.5.1:configureMPI-NVSHMEM
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-nvshmem.gitlab-ci.yml:gromacs:gcc-13-cuda-12.5.1:testMPI-NVSHMEM:
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-nvshmem.gitlab-ci.yml:    - !reference [.rules:skip-if-dual-nvidia-gpus-unavailable, rules]
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-nvshmem.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-13-cuda-12.5.1
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-nvshmem.gitlab-ci.yml:    GMX_ENABLE_DIRECT_GPU_COMM: 1
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-nvshmem.gitlab-ci.yml:    # if NVSHMEM uses NCCL it requires more than 4 GB of GPU RAM as min memory,
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-nvshmem.gitlab-ci.yml:    # in CI we've GPUs like T400 which have 4 GB RAM so we disable NVSHMEM's
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-nvshmem.gitlab-ci.yml:    # NCCL usage, as NCCL isn't required in our NVSHMEM usage.
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-nvshmem.gitlab-ci.yml:    NVSHMEM_DISABLE_NCCL: 1
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-nvshmem.gitlab-ci.yml:    GMX_TEST_LABELS: "QuickGpuTest|SlowGpuTest"
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-nvshmem.gitlab-ci.yml:    GPU_VENDOR: "NVIDIA"
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-nvshmem.gitlab-ci.yml:    GPU_COUNT: 2
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-nvshmem.gitlab-ci.yml:    - $GITLAB_RUNNER_TAG_2X_NVIDIA_GPU
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-nvshmem.gitlab-ci.yml:    - job: gromacs:gcc-13-cuda-12.5.1:buildMPI-NVSHMEM
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-nvshmem.gitlab-ci.yml:gromacs:gcc-13-cuda-12.5.1:regressiontest-gpucommupd-MPI-NVSHMEM:
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-nvshmem.gitlab-ci.yml:  # Test parallelism GPU: direct communications, update
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-nvshmem.gitlab-ci.yml:    - !reference [.rules:skip-if-dual-nvidia-gpus-unavailable, rules]
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-nvshmem.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-13-cuda-12.5.1
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-nvshmem.gitlab-ci.yml:    GMX_ENABLE_DIRECT_GPU_COMM: 1
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-nvshmem.gitlab-ci.yml:    # if NVSHMEM uses NCCL it requires more than 4 GB of GPU RAM as min memory,
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-nvshmem.gitlab-ci.yml:    # in CI we've GPUs like T400 which have 4 GB RAM so we disable NVSHMEM's
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-nvshmem.gitlab-ci.yml:    # NCCL usage, as NCCL isn't required in our NVSHMEM usage.
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-nvshmem.gitlab-ci.yml:    NVSHMEM_DISABLE_NCCL: 1
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-nvshmem.gitlab-ci.yml:    GPU_VENDOR: "NVIDIA"
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-nvshmem.gitlab-ci.yml:    GPU_COUNT: 2
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-nvshmem.gitlab-ci.yml:    - $GITLAB_RUNNER_TAG_2X_NVIDIA_GPU
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-nvshmem.gitlab-ci.yml:    - job: gromacs:gcc-13-cuda-12.5.1:buildMPI-NVSHMEM
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-nvshmem.gitlab-ci.yml:gromacs:gcc-13-cuda-12.5.1:regressiontest-gpucommupd-RF-MPI-NVSHMEM:
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-nvshmem.gitlab-ci.yml:  # Test parallelism GPU: direct communications, update, NVSHMEM
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-nvshmem.gitlab-ci.yml:    - !reference [.rules:skip-if-dual-nvidia-gpus-unavailable, rules]
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-nvshmem.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-13-cuda-12.5.1
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-nvshmem.gitlab-ci.yml:    GMX_ENABLE_DIRECT_GPU_COMM: 1
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-nvshmem.gitlab-ci.yml:    # if NVSHMEM uses NCCL it requires more than 4 GB of GPU RAM as min memory,
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-nvshmem.gitlab-ci.yml:    # in CI we've GPUs like T400 which have 4 GB RAM so we disable NVSHMEM's
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-nvshmem.gitlab-ci.yml:    # NCCL usage, as NCCL isn't required in our NVSHMEM usage.
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-nvshmem.gitlab-ci.yml:    NVSHMEM_DISABLE_NCCL: 1
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-nvshmem.gitlab-ci.yml:    GPU_VENDOR: "NVIDIA"
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-nvshmem.gitlab-ci.yml:    GPU_COUNT: 2
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-nvshmem.gitlab-ci.yml:    - $GITLAB_RUNNER_TAG_2X_NVIDIA_GPU
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-nvshmem.gitlab-ci.yml:    - job: gromacs:gcc-13-cuda-12.5.1:buildMPI-NVSHMEM
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-release.gitlab-ci.yml:# Test goal: GCC with newest CUDA
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-release.gitlab-ci.yml:#   OS: Ubuntu oldest supported on NGC with the chosen CUDA
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-release.gitlab-ci.yml:#   GPU: CUDA newest supported with its newest supported gcc
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-release.gitlab-ci.yml:#   HW: NVIDIA GPU
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-release.gitlab-ci.yml:#   GPU: CUDA 12.5.1
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-release.gitlab-ci.yml:gromacs:gcc-13-cuda-12.5.1:release:configure:
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-release.gitlab-ci.yml:    - .use-cuda
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-release.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-13-cuda-12.5.1
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-release.gitlab-ci.yml:gromacs:gcc-13-cuda-12.5.1:release:build:
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-release.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-13-cuda-12.5.1
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-release.gitlab-ci.yml:    - job: gromacs:gcc-13-cuda-12.5.1:release:configure
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-release.gitlab-ci.yml:gromacs:gcc-13-cuda-12.5.1:release:test:
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-release.gitlab-ci.yml:    - !reference [.rules:skip-if-single-nvidia-gpu-unavailable, rules]
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-release.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-13-cuda-12.5.1
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-release.gitlab-ci.yml:    GMX_TEST_LABELS: "QuickGpuTest|SlowGpuTest"
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-release.gitlab-ci.yml:    GPU_VENDOR: "NVIDIA"
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-release.gitlab-ci.yml:    GPU_COUNT: 1
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-release.gitlab-ci.yml:    - $GITLAB_RUNNER_TAG_1X_NVIDIA_GPU
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-release.gitlab-ci.yml:    - job: gromacs:gcc-13-cuda-12.5.1:release:configure
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1-release.gitlab-ci.yml:    - job: gromacs:gcc-13-cuda-12.5.1:release:build
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1.gitlab-ci.yml:# Test goal: CUDA GPU communications with OpenMPI
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1.gitlab-ci.yml:#   OS: Ubuntu oldest supported on NGC with the chosen CUDA
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1.gitlab-ci.yml:#   GPU: NVHPC SDK newest supported with its newest supported CUDA and gcc
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1.gitlab-ci.yml:#   HW: dual NVIDIA GPU
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1.gitlab-ci.yml:#   Features: GPU direct communications (manual) + update (regression tests with dual GPU)
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1.gitlab-ci.yml:#   GPU: NVHPC SDK 24.7, CUDA 12.5.1
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1.gitlab-ci.yml:#   Parallelism np/ntomp: 4/1 (regression tests with dual GPU)
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1.gitlab-ci.yml:gromacs:gcc-13-cuda-12.5.1:configureMPI:
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1.gitlab-ci.yml:    - .use-cuda
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-13-cuda-12.5.1
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1.gitlab-ci.yml:    CMAKE_EXTRA_OPTIONS: "-DGMX_USE_CUFFTMP=ON -DcuFFTMp_ROOT=/opt/nvidia/hpc_sdk/Linux_x86_64/2024/math_libs"
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1.gitlab-ci.yml:gromacs:gcc-13-cuda-12.5.1:buildMPI:
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-13-cuda-12.5.1
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1.gitlab-ci.yml:    - job: gromacs:gcc-13-cuda-12.5.1:configureMPI
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1.gitlab-ci.yml:gromacs:gcc-13-cuda-12.5.1:testMPI:
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1.gitlab-ci.yml:    - !reference [.rules:skip-if-single-nvidia-gpu-unavailable, rules]
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-13-cuda-12.5.1
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1.gitlab-ci.yml:    GMX_ENABLE_DIRECT_GPU_COMM: 1
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1.gitlab-ci.yml:    GMX_GPU_PME_DECOMPOSITION: 1
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1.gitlab-ci.yml:    # cuFFTMp uses NVSHMEM, which in turn will use NCCL on single-node when available
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1.gitlab-ci.yml:    # But NCCL has extra restrictions (such as only 1 rank per GPU) so we disable for CI
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1.gitlab-ci.yml:    NVSHMEM_DISABLE_NCCL: 1
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1.gitlab-ci.yml:    # GPUs having 4GB vidmem also this size is overkill for PME decomp tests, hence we
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1.gitlab-ci.yml:    # set CUDA Module load to be lazy in order to save on CUDA libraries memory footprint
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1.gitlab-ci.yml:    # as without lazy loading whole cuFFTMp library may get loaded into GPU RAM.
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1.gitlab-ci.yml:    CUDA_MODULE_LOADING: LAZY
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1.gitlab-ci.yml:    GMX_TEST_LABELS: "QuickGpuTest|SlowGpuTest"
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1.gitlab-ci.yml:    GPU_VENDOR: "NVIDIA"
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1.gitlab-ci.yml:    GPU_COUNT: 1
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1.gitlab-ci.yml:    - $GITLAB_RUNNER_TAG_1X_NVIDIA_GPU
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1.gitlab-ci.yml:    - job: gromacs:gcc-13-cuda-12.5.1:buildMPI
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1.gitlab-ci.yml:gromacs:gcc-13-cuda-12.5.1:regressiontest-gpucommupd-MPI:
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1.gitlab-ci.yml:  # Test parallelism GPU: direct communications, update
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1.gitlab-ci.yml:    - !reference [.rules:skip-if-dual-nvidia-gpus-unavailable, rules]
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-13-cuda-12.5.1
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1.gitlab-ci.yml:    GMX_ENABLE_DIRECT_GPU_COMM: 1
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1.gitlab-ci.yml:    GPU_VENDOR: "NVIDIA"
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1.gitlab-ci.yml:    GPU_COUNT: 2
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1.gitlab-ci.yml:    - $GITLAB_RUNNER_TAG_2X_NVIDIA_GPU
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-13-cuda-12.5.1.gitlab-ci.yml:    - job: gromacs:gcc-13-cuda-12.5.1:buildMPI
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-opencl-release.gitlab-ci.yml:# Test goal: Release build with GCC and OpenCL
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-opencl-release.gitlab-ci.yml:#   GPU: OpenCL
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-opencl-release.gitlab-ci.yml:#   HW: AMD GPU
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-opencl-release.gitlab-ci.yml:gromacs:gcc-12-opencl:release:configure:
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-opencl-release.gitlab-ci.yml:    - .use-opencl
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-opencl-release.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-12-rocm-5.4.1-2
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-opencl-release.gitlab-ci.yml:gromacs:gcc-12-opencl:release:build:
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-opencl-release.gitlab-ci.yml:  # Test using configuration: gromacs:gcc-12-opencl:release:configure
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-opencl-release.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-12-rocm-5.4.1-2
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-opencl-release.gitlab-ci.yml:    - job: gromacs:gcc-12-opencl:release:configure
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-opencl-release.gitlab-ci.yml:gromacs:gcc-12-opencl:release:test:
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-opencl-release.gitlab-ci.yml:    - !reference [.rules:skip-if-single-amd-gpu-unavailable, rules]
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-opencl-release.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-12-rocm-5.4.1-2
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-opencl-release.gitlab-ci.yml:    LD_LIBRARY_PATH: "/opt/rocm/opencl/lib"
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-opencl-release.gitlab-ci.yml:    GMX_TEST_LABELS: "QuickGpuTest|SlowGpuTest"
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-opencl-release.gitlab-ci.yml:    GPU_VENDOR: "AMD"
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-opencl-release.gitlab-ci.yml:    GPU_COUNT: 1
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-opencl-release.gitlab-ci.yml:    - $GITLAB_RUNNER_TAG_1X_AMD_GPU
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-opencl-release.gitlab-ci.yml:    - job: gromacs:gcc-12-opencl:release:configure
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-opencl-release.gitlab-ci.yml:    - job: gromacs:gcc-12-opencl:release:build
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-opencl-release.gitlab-ci.yml:gromacs:gcc-12-opencl:release:regressiontest:
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-opencl-release.gitlab-ci.yml:    - !reference [.rules:skip-if-single-amd-gpu-unavailable, rules]
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-opencl-release.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-12-rocm-5.4.1-2
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-opencl-release.gitlab-ci.yml:    LD_LIBRARY_PATH: "/opt/rocm/opencl/lib"
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-opencl-release.gitlab-ci.yml:    GPU_VENDOR: "AMD"
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-opencl-release.gitlab-ci.yml:    GPU_COUNT: 1
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-opencl-release.gitlab-ci.yml:    - $GITLAB_RUNNER_TAG_1X_AMD_GPU
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-opencl-release.gitlab-ci.yml:    - job: gromacs:gcc-12-opencl:release:build
admin/gitlab-ci/gromacs.matrix/gromacs.clang-14.gitlab-ci.yml:# Test goal: Simple Clang build with no GPU
admin/gitlab-ci/gromacs.matrix/gromacs.clang-14.gitlab-ci.yml:#   GPU: no
admin/gitlab-ci/gromacs.matrix/gromacs.clang-14.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-llvm-14-cuda-12.1.0
admin/gitlab-ci/gromacs.matrix/gromacs.clang-14.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-llvm-14-cuda-12.1.0
admin/gitlab-ci/gromacs.matrix/gromacs.clang-14.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-llvm-14-cuda-12.1.0
admin/gitlab-ci/gromacs.matrix/gromacs.clang-14.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-llvm-14-cuda-12.1.0
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12.gitlab-ci.yml:# Test goal: newest GCC with OpenCL
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12.gitlab-ci.yml:#   GPU: OpenCL
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12.gitlab-ci.yml:#   HW: AMD GPU
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12.gitlab-ci.yml:    - .use-opencl
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-12-rocm-5.4.1-2
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-12-rocm-5.4.1-2
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12.gitlab-ci.yml:    - !reference [.rules:skip-if-single-amd-gpu-unavailable, rules]
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-12-rocm-5.4.1-2
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12.gitlab-ci.yml:    LD_LIBRARY_PATH: "/opt/rocm/opencl/lib"
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12.gitlab-ci.yml:    GMX_TEST_LABELS: "QuickGpuTest|SlowGpuTest"
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12.gitlab-ci.yml:    GPU_VENDOR: "AMD"
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12.gitlab-ci.yml:    GPU_COUNT: 1
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12.gitlab-ci.yml:    - $GITLAB_RUNNER_TAG_1X_AMD_GPU
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12.gitlab-ci.yml:    - !reference [.rules:skip-if-single-amd-gpu-unavailable, rules]
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-12-rocm-5.4.1-2
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12.gitlab-ci.yml:    LD_LIBRARY_PATH: "/opt/rocm/opencl/lib"
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12.gitlab-ci.yml:    GPU_VENDOR: "AMD"
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12.gitlab-ci.yml:    GPU_COUNT: 1
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12.gitlab-ci.yml:    - $GITLAB_RUNNER_TAG_1X_AMD_GPU
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-14-cp2k-2024.2-nightly.gitlab-ci.yml:#   GPU: no
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-14-cp2k-2024.2-nightly.gitlab-ci.yml:    CMAKE_GPU_OPTIONS: "-DGMX_GPU=OFF"
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0-mpi.gitlab-ci.yml:# Test goal: old versions of GCC with CUDA; GPU communications with OpenMPI
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0-mpi.gitlab-ci.yml:#   OS: Ubuntu oldest supported on NGC with the chosen CUDA
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0-mpi.gitlab-ci.yml:#   Compiler: GCC oldest supported with the chosen CUDA
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0-mpi.gitlab-ci.yml:#   GPU: CUDA oldest supported
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0-mpi.gitlab-ci.yml:#   HW: NVIDIA GPU, single NVIDIA GPU
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0-mpi.gitlab-ci.yml:#   Features: GPU direct communications + update (unit tests), HeFFTe for GPU PME decomposition
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0-mpi.gitlab-ci.yml:#   GPU: CUDA 12.1.0
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0-mpi.gitlab-ci.yml:gromacs:gcc-12-cuda-12.1.0:configureMPI:
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0-mpi.gitlab-ci.yml:    - .use-cuda
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0-mpi.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-12-cuda-12.1.0
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0-mpi.gitlab-ci.yml:gromacs:gcc-12-cuda-12.1.0:buildMPI:
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0-mpi.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-12-cuda-12.1.0
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0-mpi.gitlab-ci.yml:    - job: gromacs:gcc-12-cuda-12.1.0:configureMPI
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0-mpi.gitlab-ci.yml:gromacs:gcc-12-cuda-12.1.0:testMPI:
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0-mpi.gitlab-ci.yml:    - !reference [.rules:skip-if-single-nvidia-gpu-unavailable, rules]
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0-mpi.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-gcc-12-cuda-12.1.0
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0-mpi.gitlab-ci.yml:    GMX_ENABLE_DIRECT_GPU_COMM: 1
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0-mpi.gitlab-ci.yml:    GMX_GPU_PME_DECOMPOSITION: 1
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0-mpi.gitlab-ci.yml:    GMX_TEST_LABELS: "QuickGpuTest|SlowGpuTest"
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0-mpi.gitlab-ci.yml:    GPU_VENDOR: "NVIDIA"
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0-mpi.gitlab-ci.yml:    GPU_COUNT: 1
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0-mpi.gitlab-ci.yml:    - $GITLAB_RUNNER_TAG_1X_NVIDIA_GPU
admin/gitlab-ci/gromacs.matrix/gromacs.gcc-12-cuda-12.1.0-mpi.gitlab-ci.yml:    - job: gromacs:gcc-12-cuda-12.1.0:buildMPI
admin/gitlab-ci/global.gitlab-ci.yml:.use-cuda:
admin/gitlab-ci/global.gitlab-ci.yml:    CMAKE_GPU_OPTIONS: -DGMX_GPU=CUDA
admin/gitlab-ci/global.gitlab-ci.yml:    CMAKE_GPU_OPTIONS: -DGMX_GPU=HIP -DCMAKE_PREFIX_PATH=/opt/rocm -DCMAKE_HIP_COMPILER=/opt/rocm/bin/amdclang++
admin/gitlab-ci/global.gitlab-ci.yml:.use-opencl:
admin/gitlab-ci/global.gitlab-ci.yml:    CMAKE_GPU_OPTIONS: -DGMX_GPU=OpenCL
admin/gitlab-ci/global.gitlab-ci.yml:    CMAKE_GPU_OPTIONS: -DGMX_GPU=SYCL
admin/gitlab-ci/global.gitlab-ci.yml:    CMAKE_COMPILER_SCRIPT: -DCMAKE_C_COMPILER=/opt/rocm/bin/amdclang -DCMAKE_CXX_COMPILER=/opt/rocm/bin/amdclang++
admin/gitlab-ci/lint.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-llvm-18-cuda-12.1.0
admin/gitlab-ci/lint.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-llvm-18-cuda-12.1.0
admin/gitlab-ci/lint.gitlab-ci.yml:clang-tidy:configure-schedule-cuda:
admin/gitlab-ci/lint.gitlab-ci.yml:    - .use-cuda
admin/gitlab-ci/lint.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-llvm-18-cuda-12.1.0
admin/gitlab-ci/lint.gitlab-ci.yml:    CMAKE_EXTRA_OPTIONS: -DCLANG_TIDY=clang-tidy-$COMPILER_MAJOR_VERSION -DGMX_CLANG_TIDY=ON -DGMX_COMPILER_WARNINGS=ON -DGMX_CLANG_CUDA=ON
admin/gitlab-ci/lint.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-llvm-18-cuda-12.1.0
admin/gitlab-ci/lint.gitlab-ci.yml:clang-tidy:build-cuda:
admin/gitlab-ci/lint.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-llvm-18-cuda-12.1.0
admin/gitlab-ci/lint.gitlab-ci.yml:    - job: clang-tidy:configure-schedule-cuda
admin/gitlab-ci/lint.gitlab-ci.yml:    # several of our source files. Since we don't need the clang-tidy+cuda
admin/gitlab-ci/lint.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-llvm-18-cuda-12.1.0
admin/gitlab-ci/lint.gitlab-ci.yml:  image: ${CI_REGISTRY}/gromacs/gromacs/ci-ubuntu-22.04-llvm-18-cuda-12.1.0
admin/gitlab-ci/rules.gitlab-ci.yml:# Rule matching if user selected there are no Nvidia GPUs, it skips executing
admin/gitlab-ci/rules.gitlab-ci.yml:.rules:skip-if-single-nvidia-gpu-unavailable:
admin/gitlab-ci/rules.gitlab-ci.yml:    - if: $GITLAB_RUNNER_MAX_AVAILABLE_NVIDIA_GPUS == "0"
admin/gitlab-ci/rules.gitlab-ci.yml:# Rule matching if user	selected there is max 1 Nvidia GPU, it skips executing
admin/gitlab-ci/rules.gitlab-ci.yml:.rules:skip-if-dual-nvidia-gpus-unavailable:
admin/gitlab-ci/rules.gitlab-ci.yml:    - if: $GITLAB_RUNNER_MAX_AVAILABLE_NVIDIA_GPUS =~ /[01]/
admin/gitlab-ci/rules.gitlab-ci.yml:# Rule matching if user	selected there are no AMD GPUs, it skips executing
admin/gitlab-ci/rules.gitlab-ci.yml:.rules:skip-if-single-amd-gpu-unavailable:
admin/gitlab-ci/rules.gitlab-ci.yml:    - if: $GITLAB_RUNNER_MAX_AVAILABLE_AMD_GPUS == "0"
admin/gitlab-ci/rules.gitlab-ci.yml:# Rule matching if user	selected there is max 1 AMD GPU, it skips executing
admin/gitlab-ci/rules.gitlab-ci.yml:.rules:skip-if-dual-amd-gpus-unavailable:
admin/gitlab-ci/rules.gitlab-ci.yml:    - if: $GITLAB_RUNNER_MAX_AVAILABLE_AMD_GPUS =~ /[01]/
admin/gitlab-ci/rules.gitlab-ci.yml:# Rule matching if user	selected there are no Intel GPUs, it skips executing
admin/gitlab-ci/rules.gitlab-ci.yml:.rules:skip-if-single-intel-gpu-unavailable:
admin/gitlab-ci/rules.gitlab-ci.yml:    - if: $GITLAB_RUNNER_MAX_AVAILABLE_INTEL_GPUS == "0"
admin/gitlab-ci/rules.gitlab-ci.yml:# Rule matching if user	selected there is max 1 Intel GPU, it skips executing
admin/gitlab-ci/rules.gitlab-ci.yml:.rules:skip-if-dual-intel-gpus-unavailable:
admin/gitlab-ci/rules.gitlab-ci.yml:    - if: $GITLAB_RUNNER_MAX_AVAILABLE_INTEL_GPUS =~ /[01]/
admin/ci-scripts/build-and-test-sample_restraint.sh:        --mca opal_warn_on_missing_libcuda 0 \
admin/ci-scripts/gromacs-base-cuda-sanitizer-test.sh:# If $GMX_TEST_REQUIRED_NUMBER_OF_DEVICES is not set and we have GPUs, set it
admin/ci-scripts/gromacs-base-cuda-sanitizer-test.sh:if [ -z "${GMX_TEST_REQUIRED_NUMBER_OF_DEVICES}" ] && [ -n "${GPU_VENDOR}" ] ; then
admin/ci-scripts/gromacs-base-cuda-sanitizer-test.sh:    if grep -q 'NVIDIA' <<< "${GPU_VENDOR}"; then
admin/ci-scripts/gromacs-base-cuda-sanitizer-test.sh:        echo "export GMX_TEST_REQUIRED_NUMBER_OF_DEVICES=\"${GPU_COUNT}\"";
admin/ci-scripts/gromacs-base-cuda-sanitizer-test.sh:        export GMX_TEST_REQUIRED_NUMBER_OF_DEVICES="${GPU_COUNT}";
admin/ci-scripts/gromacs-base-cuda-sanitizer-test.sh:if grep -qF 'NVIDIA' <<< "$GPU_VENDOR"; then
admin/ci-scripts/gromacs-base-cuda-sanitizer-test.sh:    nvidia-smi -L && nvidia-smi || true
admin/ci-scripts/gromacs-base-cuda-sanitizer-test.sh:    echo "Can not use CUDA Compute Sanitizer without an NVIDIA GPU"
admin/ci-scripts/gromacs-base-cuda-sanitizer-test.sh:# Common flags: non-zero exit code on error; require that CUDA is actually used in the tests; trace child processes.
admin/ci-scripts/gromacs-base-cuda-sanitizer-test.sh:TEST_LABELS='QuickGpuTest'
admin/ci-scripts/gromacs-base-cuda-sanitizer-test.sh:    echo "Running CUDA Compute Sanitizer in ${TOOL} mode"
admin/ci-scripts/gromacs-base-cuda-sanitizer-test.sh:      # We get warnings from the Pack kernel in DomDecMpiTests with CUDA 11.0, which is harmless:
admin/ci-scripts/gromacs-base-cuda-sanitizer-test.sh:      --overwrite MemoryCheckType=CudaSanitizer \
admin/ci-scripts/gromacs-cp2k-configure.sh:echo $CMAKE_GPU_OPTIONS
admin/ci-scripts/gromacs-cp2k-configure.sh:      $CMAKE_GPU_OPTIONS \
admin/ci-scripts/gromacs-base-configure.sh:echo $CMAKE_GPU_OPTIONS
admin/ci-scripts/gromacs-base-configure.sh:      $CMAKE_GPU_OPTIONS \
admin/ci-scripts/gromacs-base-test.sh:# If $GMX_TEST_REQUIRED_NUMBER_OF_DEVICES is not set and we have GPUs, set it
admin/ci-scripts/gromacs-base-test.sh:if [[ -z "$GMX_TEST_REQUIRED_NUMBER_OF_DEVICES" ]] && [[ -n "$GPU_VENDOR" ]] ; then
admin/ci-scripts/gromacs-base-test.sh:    echo "export GMX_TEST_REQUIRED_NUMBER_OF_DEVICES=\"$GPU_COUNT\"";
admin/ci-scripts/gromacs-base-test.sh:    export GMX_TEST_REQUIRED_NUMBER_OF_DEVICES="$GPU_COUNT";
admin/ci-scripts/gromacs-base-test.sh:if grep -qF 'NVIDIA' <<< "$GPU_VENDOR"; then
admin/ci-scripts/gromacs-base-test.sh:    nvidia-smi -L && nvidia-smi || true;
admin/ci-scripts/gromacs-base-test.sh:        echo "DUE TO LIMITATIONS OF CUFFTMP, THIS JOB RUNS IN DIFFERENT CONFIGURATIONS DEPENDING ON THE VERSION OF GPU AVAILABLE. Now running:" 
admin/ci-scripts/gromacs-base-test.sh:        computeCapability=`nvidia-smi -i 0 --query-gpu=compute_cap --format=csv | tail -1 | sed 's/\.//g'`    
admin/ci-scripts/gromacs-base-test.sh:            unset GMX_GPU_PME_DECOMPOSITION
admin/ci-scripts/gromacs-base-test.sh:            export LD_PRELOAD=$CUFFTLIB #TODO remove this when cuFFTMp is fixed regarding "regular" ffts for older GPUs #3884
admin/ci-scripts/gromacs-base-test.sh:    if [[ "$GMX_ENABLE_NVSHMEM" != "" ]] && [[ "$GPU_COUNT" -eq "2" ]]
admin/ci-scripts/gromacs-base-test.sh:        # In CI with dual GPUs NVSHMEM cannot support more than 2 MPI processes/GPU
admin/ci-scripts/gromacs-base-test.sh:        echo "Disabling MdrunMultiSim tests as with GMX_ENABLE_NVSHMEM it does not work on dual GPU setup without MPS"
admin/ci-scripts/gromacs-base-test.sh:    export CUDA_DEVICE_MAX_CONNECTIONS=2  # default is 8
admin/ci-scripts/gromacs-base-test.sh:if grep -qF 'AMD' <<< "$GPU_VENDOR"; then
admin/ci-scripts/gromacs-base-test.sh:    rocm-smi || true;
admin/ci-scripts/gromacs-base-test.sh:if grep -qF 'INTEL' <<< "$GPU_VENDOR"; then
admin/ci-scripts/build-and-test-py-gmxapi.sh:      --mca opal_warn_on_missing_libcuda 0 \
admin/containers/buildall.sh:args[${#args[@]}]="--ubuntu 22.04 --gcc 12 --clfft --mpi openmpi --rocm 5.4.1 --hdf5"
admin/containers/buildall.sh:args[${#args[@]}]="--ubuntu 22.04 --gcc 13 --cuda 12.5.1 --clfft --mpi openmpi --nvhpcsdk 24.7"
admin/containers/buildall.sh:args[${#args[@]}]="--ubuntu 22.04 --gcc 12 --cuda 12.1.0 --clfft --mpi openmpi --heffte v2.4.0"
admin/containers/buildall.sh:args[${#args[@]}]="--ubuntu 22.04 --llvm 18 --cuda 12.1.0"
admin/containers/buildall.sh:args[${#args[@]}]="--ubuntu 22.04 --llvm 14 --cuda 12.1.0 --clfft --mpi openmpi"
admin/containers/buildall.sh:args[${#args[@]}]="--oneapi 2024.2 --ubuntu 22.04 --rocm 6.1.3 --cuda 12.0.1 --oneapi-plugin-amd --oneapi-plugin-nvidia"
admin/containers/buildall.sh:args[${#args[@]}]="--ubuntu 24.04 --llvm 18 --cuda 12.6.0 --adaptivecpp 24.02.0 --rocm 6.2 --mpi mpich"
admin/containers/buildall.sh:args[${#args[@]}]="--ubuntu 22.04 --adaptivecpp 23.10.0 --rocm 5.7.1"
admin/containers/buildall.sh:args[${#args[@]}]="--ubuntu 24.04 --rocm 6.2.2 --mpi --plumed"
admin/containers/scripted_gmx_docker_builds.py:Based on the example script provided by the NVidia HPCCM repository.
admin/containers/scripted_gmx_docker_builds.py:    `NVidia HPC Container Maker <https://github.com/NVIDIA/hpc-container-maker>`__
admin/containers/scripted_gmx_docker_builds.py:    * Gaurav Garg <gaugarg@nvidia.com>
admin/containers/scripted_gmx_docker_builds.py:_opencl_extra_packages = [
admin/containers/scripted_gmx_docker_builds.py:    "nvidia-opencl-dev",
admin/containers/scripted_gmx_docker_builds.py:    # The following require apt_ppas=['ppa:intel-opencl/intel-opencl'] on Ubuntu prior to 22.04
admin/containers/scripted_gmx_docker_builds.py:    "intel-opencl-icd",
admin/containers/scripted_gmx_docker_builds.py:    "ocl-icd-libopencl1",
admin/containers/scripted_gmx_docker_builds.py:    "ocl-icd-opencl-dev",
admin/containers/scripted_gmx_docker_builds.py:    "opencl-headers",
admin/containers/scripted_gmx_docker_builds.py:_rocm_extra_packages = [
admin/containers/scripted_gmx_docker_builds.py:    #             apt_keys=['http://repo.radeon.com/rocm/rocm.gpg.key'],
admin/containers/scripted_gmx_docker_builds.py:    #             apt_repositories=['deb [arch=amd64] http://repo.radeon.com/rocm/apt/X.Y.Z/ ubuntu main']
admin/containers/scripted_gmx_docker_builds.py:_rocm_version_dependent_packages = [
admin/containers/scripted_gmx_docker_builds.py:    #             apt_keys=['http://repo.radeon.com/rocm/rocm.gpg.key'],
admin/containers/scripted_gmx_docker_builds.py:    #             apt_repositories=['deb [arch=amd64] http://repo.radeon.com/rocm/apt/X.Y.Z/ ubuntu main']
admin/containers/scripted_gmx_docker_builds.py:    "rocm-dev",
admin/containers/scripted_gmx_docker_builds.py:    "rocm-opencl",
admin/containers/scripted_gmx_docker_builds.py:    "intel-opencl-icd",
admin/containers/scripted_gmx_docker_builds.py:    "intel-level-zero-gpu",
admin/containers/scripted_gmx_docker_builds.py:    # Check if we use CUDA images or plain linux images
admin/containers/scripted_gmx_docker_builds.py:    if args.cuda is not None:
admin/containers/scripted_gmx_docker_builds.py:        cuda_version_tag = "nvidia/cuda:" + args.cuda + "-devel"
admin/containers/scripted_gmx_docker_builds.py:            cuda_version_tag += "-centos" + args.centos
admin/containers/scripted_gmx_docker_builds.py:            cuda_version_tag += "-ubuntu" + args.ubuntu
admin/containers/scripted_gmx_docker_builds.py:        base_image_tag = cuda_version_tag
admin/containers/scripted_gmx_docker_builds.py:def get_opencl_packages(args) -> typing.List[str]:
admin/containers/scripted_gmx_docker_builds.py:        return _opencl_extra_packages
admin/containers/scripted_gmx_docker_builds.py:def get_rocm_packages(args) -> typing.List[str]:
admin/containers/scripted_gmx_docker_builds.py:    if args.rocm is None:
admin/containers/scripted_gmx_docker_builds.py:        packages = _rocm_extra_packages
admin/containers/scripted_gmx_docker_builds.py:        packages.extend(get_rocm_version_dependent_packages(args))
admin/containers/scripted_gmx_docker_builds.py:def get_rocm_version_dependent_packages(args) -> typing.List[str]:
admin/containers/scripted_gmx_docker_builds.py:    rocm_version = args.rocm
admin/containers/scripted_gmx_docker_builds.py:    rocm_version_parsed = [int(i) for i in rocm_version.split(".")]
admin/containers/scripted_gmx_docker_builds.py:    if len(rocm_version_parsed) < 3:
admin/containers/scripted_gmx_docker_builds.py:        rocm_version = rocm_version + ".0"
admin/containers/scripted_gmx_docker_builds.py:    for entry in _rocm_version_dependent_packages:
admin/containers/scripted_gmx_docker_builds.py:        packages.append(entry + rocm_version)
admin/containers/scripted_gmx_docker_builds.py:def get_rocm_repository(args) -> "hpccm.building_blocks.base":
admin/containers/scripted_gmx_docker_builds.py:        raise RuntimeError("ROCm only supported on Ubuntu")
admin/containers/scripted_gmx_docker_builds.py:        rocm_version = [int(i) for i in args.rocm.split(".")]
admin/containers/scripted_gmx_docker_builds.py:        if rocm_version[0] < 5 or (rocm_version[0] == 5 and rocm_version[1] < 3):
admin/containers/scripted_gmx_docker_builds.py:        apt_keys=["http://repo.radeon.com/rocm/rocm.gpg.key"],
admin/containers/scripted_gmx_docker_builds.py:            f"deb [arch=amd64 signed-by=/usr/share/keyrings/rocm.gpg.gpg] http://repo.radeon.com/rocm/apt/{args.rocm}/ {dist_string} main"
admin/containers/scripted_gmx_docker_builds.py:    if args.cuda is not None:
admin/containers/scripted_gmx_docker_builds.py:    if args.cuda is not None or args.rocm is not None:
admin/containers/scripted_gmx_docker_builds.py:            if args.rocm is not None:
admin/containers/scripted_gmx_docker_builds.py:                configure_opts.append("--with-rocm=/opt/rocm")
admin/containers/scripted_gmx_docker_builds.py:            use_cuda = args.cuda is not None
admin/containers/scripted_gmx_docker_builds.py:                cuda=use_cuda,
admin/containers/scripted_gmx_docker_builds.py:                use_cuda = args.cuda is not None
admin/containers/scripted_gmx_docker_builds.py:                    cuda=use_cuda,
admin/containers/scripted_gmx_docker_builds.py:                use_cuda = args.cuda is not None
admin/containers/scripted_gmx_docker_builds.py:                use_rocm = args.rocm is not None
admin/containers/scripted_gmx_docker_builds.py:                    cuda=use_cuda,
admin/containers/scripted_gmx_docker_builds.py:                    rocm=use_rocm,
admin/containers/scripted_gmx_docker_builds.py:        backend_version = {"nvidia": args.cuda, "amd": args.rocm}[variant]
admin/containers/scripted_gmx_docker_builds.py:    if args.oneapi_plugin_nvidia:
admin/containers/scripted_gmx_docker_builds.py:        _add_plugin("nvidia")
admin/containers/scripted_gmx_docker_builds.py:        # Need to make ROCm libraries discoverable
admin/containers/scripted_gmx_docker_builds.py:                    "echo '/opt/rocm/lib/' > /etc/ld.so.conf.d/rocm.conf",
admin/containers/scripted_gmx_docker_builds.py:                "-D Heffte_ENABLE_CUDA=ON",
admin/containers/scripted_gmx_docker_builds.py:            cuda_multi=False,
admin/containers/scripted_gmx_docker_builds.py:    if args.rocm is None:
admin/containers/scripted_gmx_docker_builds.py:        raise RuntimeError("AdaptiveCpp requires the ROCm packages")
admin/containers/scripted_gmx_docker_builds.py:        # We're using ROCm LLVM in this case, which is not compatible with CUDA
admin/containers/scripted_gmx_docker_builds.py:        if args.cuda is not None:
admin/containers/scripted_gmx_docker_builds.py:                "Can not build AdaptiveCpp with CUDA and no upstream LLVM"
admin/containers/scripted_gmx_docker_builds.py:            "-DCMAKE_C_COMPILER=/opt/rocm/bin/amdclang",
admin/containers/scripted_gmx_docker_builds.py:            "-DCMAKE_CXX_COMPILER=/opt/rocm/bin/amdclang++",
admin/containers/scripted_gmx_docker_builds.py:            "-DLLVM_DIR=/opt/rocm/llvm/lib/cmake/llvm",
admin/containers/scripted_gmx_docker_builds.py:            "-DWITH_OPENCL_BACKEND=OFF",
admin/containers/scripted_gmx_docker_builds.py:        "-DCMAKE_PREFIX_PATH=/opt/rocm/lib/cmake",
admin/containers/scripted_gmx_docker_builds.py:        "-DWITH_ROCM_BACKEND=ON",
admin/containers/scripted_gmx_docker_builds.py:    if args.cuda is not None:
admin/containers/scripted_gmx_docker_builds.py:            "-DCUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda",
admin/containers/scripted_gmx_docker_builds.py:            "-DWITH_CUDA_BACKEND=ON",
admin/containers/scripted_gmx_docker_builds.py:        "--cuda",  # Build with CUDA support
admin/containers/scripted_gmx_docker_builds.py:        "--cmake-opt=-DLIBOMPTARGET_BUILD_AMDGPU_PLUGIN=FALSE",
admin/containers/scripted_gmx_docker_builds.py:        "--cmake-opt=-DLIBOMPTARGET_BUILD_CUDA_PLUGIN=FALSE",
admin/containers/scripted_gmx_docker_builds.py:        # Help CMake find CUDA Driver stub, see https://github.com/opencv/opencv/issues/6577
admin/containers/scripted_gmx_docker_builds.py:        "--cmake-opt=-DCMAKE_LIBRARY_PATH=/usr/local/cuda/targets/x86_64-linux/lib/stubs/",
admin/containers/scripted_gmx_docker_builds.py:    if args.rocm is not None:
admin/containers/scripted_gmx_docker_builds.py:        llvm_stage += get_rocm_repository(args)
admin/containers/scripted_gmx_docker_builds.py:            ospackages=get_rocm_packages(args), aptitude=True
admin/containers/scripted_gmx_docker_builds.py:    if args.rocm is not None:
admin/containers/scripted_gmx_docker_builds.py:        building_blocks["rocm"] = [
admin/containers/scripted_gmx_docker_builds.py:            get_rocm_repository(args),
admin/containers/scripted_gmx_docker_builds.py:            hpccm.building_blocks.packages(ospackages=get_rocm_packages(args)),
admin/containers/scripted_gmx_docker_builds.py:        + get_opencl_packages(args)
admin/containers/scripted_gmx_docker_builds.py:            "22.04": "deb [signed-by=/usr/share/keyrings/intel-graphics.gpg arch=amd64] https://repositories.intel.com/gpu/ubuntu jammy client",
admin/containers/scripted_gmx_docker_builds.py:            apt_keys=["https://repositories.intel.com/gpu/intel-graphics.key"],
admin/containers/scripted_gmx_docker_builds.py:            ospackages=os_packages, apt_ppas=["ppa:intel-opencl/intel-opencl"]
admin/containers/scripted_gmx_docker_builds.py:        # use aptitude to resolve the conflicts between Ubuntu ROCm packages and AMD ROCm packages.
admin/containers/scripted_gmx_docker_builds.py:    if args.cuda is not None and args.llvm is not None:
admin/containers/scripted_gmx_docker_builds.py:        # Hack to tell clang what version of CUDA we're using
admin/containers/scripted_gmx_docker_builds.py:        # based on https://github.com/llvm/llvm-project/blob/1fdec59bffc11ae37eb51a1b9869f0696bfd5312/clang/lib/Driver/ToolChains/Cuda.cpp#L43
admin/containers/scripted_gmx_docker_builds.py:        cuda_version_split = args.cuda.split(".")
admin/containers/scripted_gmx_docker_builds.py:        # LLVM requires having the version in x.y.z format, while args.cuda be be either x.y or x.y.z
admin/containers/scripted_gmx_docker_builds.py:        cuda_version_str = "{}.{}.{}".format(
admin/containers/scripted_gmx_docker_builds.py:            cuda_version_split[0],
admin/containers/scripted_gmx_docker_builds.py:            cuda_version_split[1],
admin/containers/scripted_gmx_docker_builds.py:            cuda_version_split[2] if len(cuda_version_split) > 2 else 0,
admin/containers/scripted_gmx_docker_builds.py:        building_blocks["cuda-clang-workaround"] = hpccm.primitives.shell(
admin/containers/scripted_gmx_docker_builds.py:                f'echo "CUDA Version {cuda_version_str}" > /usr/local/cuda/version.txt'
admin/containers/scripted_gmx_docker_builds.py:            "/opt/nvidia/hpc_sdk/Linux_x86_64/"
admin/containers/utility.py:    * Gaurav Garg <gaugarg@nvidia.com>
admin/containers/utility.py:    "--cuda",
admin/containers/utility.py:    help="Select a CUDA version for a base Linux image from NVIDIA.",
admin/containers/utility.py:    "--rocm",
admin/containers/utility.py:    "--oneapi-plugin-nvidia",
admin/containers/utility.py:    help="Install Codeplay oneAPI NVIDIA plugin.",
admin/containers/utility.py:    help="Select NVIDIA HPC SDK version.",
admin/containers/utility.py:        <distro>-<version>-<compiler>-<major version>[-<gpusdk>-<version>][-<use case>]
admin/containers/utility.py:    for gpusdk in ("cuda", "adaptivecpp"):
admin/containers/utility.py:        version = getattr(configuration, gpusdk, None)
admin/containers/utility.py:            elements.append(gpusdk + "-" + version)
admin/containers/utility.py:    if configuration.rocm is not None:
admin/containers/utility.py:        elements.append("rocm-" + configuration.rocm)
CMakeLists.txt:set(GMX_CUDA_MINIMUM_REQUIRED_VERSION 12.1)
CMakeLists.txt:set(GMX_CUDA_MINIMUM_REQUIRED_COMPUTE_CAPABILITY 5.0)
CMakeLists.txt:    GMX_GPU
CMakeLists.txt:    "Framework for GPU acceleration"
CMakeLists.txt:    OFF CUDA OpenCL SYCL HIP)
CMakeLists.txt:if(GMX_GPU STREQUAL SYCL)
CMakeLists.txt:        "SYCL implementation to use for GPU acceleration"
CMakeLists.txt:    "Use HeFFTe for distributed FFT support. Used with CUDA backend"
CMakeLists.txt:    "GMX_GPU STREQUAL CUDA OR GMX_GPU STREQUAL HIP OR GMX_GPU STREQUAL SYCL;GMX_MPI")
CMakeLists.txt:    "Use cuFFTMp for distributed FFT support. Used with CUDA backend"
CMakeLists.txt:    "GMX_GPU STREQUAL CUDA;GMX_MPI")
CMakeLists.txt:# Here the default GPU FFT library is set up depending
CMakeLists.txt:# Configuration    |  Default GPU FFT library
CMakeLists.txt:# CUDA             | cuFFT
CMakeLists.txt:# OpenCL           | VkFFT or clFFT according to toolchain
CMakeLists.txt:if (GMX_GPU)
CMakeLists.txt:    if (GMX_GPU STREQUAL CUDA)
CMakeLists.txt:        set(GMX_GPU_FFT_LIBRARY_DEFAULT "cuFFT")
CMakeLists.txt:    elseif(GMX_GPU STREQUAL HIP)
CMakeLists.txt:        set(GMX_GPU_FFT_LIBRARY_DEFAULT "VkFFT")
CMakeLists.txt:    elseif(GMX_GPU STREQUAL OPENCL)
CMakeLists.txt:            set(GMX_GPU_FFT_LIBRARY_DEFAULT "VkFFT")
CMakeLists.txt:            set(GMX_GPU_FFT_LIBRARY_DEFAULT "clFFT")
CMakeLists.txt:    elseif(GMX_GPU STREQUAL SYCL)
CMakeLists.txt:            set(GMX_GPU_FFT_LIBRARY_DEFAULT "VkFFT")
CMakeLists.txt:            set(GMX_GPU_FFT_LIBRARY_DEFAULT "MKL")
CMakeLists.txt:        GMX_GPU_FFT_LIBRARY
CMakeLists.txt:        "GPU FFT library"
CMakeLists.txt:        "${GMX_GPU_FFT_LIBRARY_DEFAULT}"
CMakeLists.txt:    foreach (_gpu_fft_library cuFFT clFFT VkFFT MKL oneMKL rocFFT BBFFT)
CMakeLists.txt:        string(TOUPPER "${_gpu_fft_library}" _gpu_fft_library_upper)
CMakeLists.txt:        if (GMX_GPU_FFT_LIBRARY STREQUAL ${_gpu_fft_library_upper})
CMakeLists.txt:            if (NOT GMX_GPU_FFT_QUIET_AFTER_FIRST_RUN)
CMakeLists.txt:                message(STATUS "Selected GPU FFT library - ${_gpu_fft_library}")
CMakeLists.txt:            set(GMX_GPU_FFT_QUIET_AFTER_FIRST_RUN TRUE CACHE INTERNAL "Be quiet during future runs of cmake")
CMakeLists.txt:        set("GMX_GPU_FFT_${_gpu_fft_library_upper}" "${_value}" CACHE INTERNAL "Use ${_gpu_fft_library} library for FFTs on GPUs")
CMakeLists.txt:gmx_dependent_option(GMX_NVSHMEM "Build a parallel NVSHMEM multi-GPU code of GROMACS" OFF "GMX_GPU STREQUAL CUDA;GMX_MPI")
CMakeLists.txt:if(GMX_GPU)
CMakeLists.txt:    string(TOUPPER "${GMX_GPU}" _gmx_gpu_uppercase)
CMakeLists.txt:    if(${_gmx_gpu_uppercase} STREQUAL "CUDA")
CMakeLists.txt:        include(gmxManageCuda)
CMakeLists.txt:    elseif(${_gmx_gpu_uppercase} STREQUAL "HIP")
CMakeLists.txt:    elseif(${_gmx_gpu_uppercase} STREQUAL "OPENCL")
CMakeLists.txt:        message(STATUS "GPU support with OpenCL is deprecated. It is still fully supported (and " 
CMakeLists.txt:            "recommended for Apple GPUs). Please use CUDA for running on NVIDIA GPUs and "
CMakeLists.txt:            "SYCL for running on Intel and AMD GPUs.")
CMakeLists.txt:        include(gmxManageOpenCL)
CMakeLists.txt:    elseif(${_gmx_gpu_uppercase} STREQUAL "SYCL")
CMakeLists.txt:        message(WARNING "To use GPU acceleration efficiently, mdrun requires OpenMP multi-threading, which is currently not enabled.")
CMakeLists.txt:    if (GMX_OPENCL_NB_CLUSTER_SIZE)
CMakeLists.txt:        message(WARNING "GMX_OPENCL_NB_CLUSTER_SIZE is deprecated, use GMX_GPU_NB_CLUSTER_SIZE instead")
CMakeLists.txt:    if (GMX_OPENCL_NB_CLUSTER_SIZE AND GMX_GPU_NB_CLUSTER_SIZE)
CMakeLists.txt:        if (NOT ${GMX_OPENCL_NB_CLUSTER_SIZE} EQUAL ${GMX_GPU_NB_CLUSTER_SIZE})
CMakeLists.txt:            message(FATAL_ERROR "Mismatching values passed to GMX_OPENCL_NB_CLUSTER_SIZE and GMX_GPU_NB_CLUSTER_SIZE; the former is deprecated, use only the latter!")
CMakeLists.txt:    # Only OpenCL and SYCL support changing the default cluster size
CMakeLists.txt:    if (${_gmx_gpu_uppercase} STREQUAL "CUDA" OR ${_gmx_gpu_uppercase} STREQUAL "HIP")
CMakeLists.txt:        if (GMX_GPU_NB_CLUSTER_SIZE AND NOT "${GMX_GPU_NB_CLUSTER_SIZE}" EQUAL 8)
CMakeLists.txt:            message(FATAL_ERROR "Changing GMX_GPU_NB_CLUSTER_SIZE is not supported in CUDA (the default GMX_GPU_NB_CLUSTER_SIZE=8 is used)")
CMakeLists.txt:        set(GMX_GPU_NB_CLUSTER_SIZE 8 CACHE STRING "Cluster size used by the nonbonded kernel.")
CMakeLists.txt:        # use the legacy GMX_OPENCL_NB_CLUSTER_SIZE variable if set, otherwise set the defaults
CMakeLists.txt:        if (GMX_OPENCL_NB_CLUSTER_SIZE)
CMakeLists.txt:            set(_gmx_gpu_nb_cluster_size_value ${GMX_OPENCL_NB_CLUSTER_SIZE})
CMakeLists.txt:            # default cluster size is 8 with OpenCL and 4 with SYCL for now
CMakeLists.txt:            if(${_gmx_gpu_uppercase} STREQUAL "OPENCL")
CMakeLists.txt:                set(_gmx_gpu_nb_cluster_size_value 8)
CMakeLists.txt:            elseif(GMX_GPU_SYCL)
CMakeLists.txt:                    set(_gmx_gpu_nb_cluster_size_value 8)
CMakeLists.txt:                    set(_gmx_gpu_nb_cluster_size_value 4)
CMakeLists.txt:        set(GMX_GPU_NB_CLUSTER_SIZE ${_gmx_gpu_nb_cluster_size_value} CACHE STRING "Cluster size used by the nonbonded kernel. Set to 4 for Intel GPUs.")
CMakeLists.txt:        mark_as_advanced(GMX_GPU_NB_CLUSTER_SIZE)
CMakeLists.txt:    if (NOT (${_gmx_gpu_uppercase} STREQUAL "SYCL" OR ${_gmx_gpu_uppercase} STREQUAL "HIP"))
CMakeLists.txt:        if (GMX_GPU_NB_DISABLE_CLUSTER_PAIR_SPLIT)
CMakeLists.txt:            message(FATAL_ERROR "Disabling cluster pair splitting is only supported with SYCL or HIP, set GMX_GPU_NB_DISABLE_CLUSTER_PAIR_SPLIT=off otherwise")
CMakeLists.txt:set(GMX_GPU_NB_NUM_CLUSTER_PER_CELL_X 2 CACHE STRING "Number of clusters along X in a pair-search grid cell for GPU lists")
CMakeLists.txt:set(GMX_GPU_NB_NUM_CLUSTER_PER_CELL_Y 2 CACHE STRING "Number of clusters along Y in a pair-search grid cell for GPU lists")
CMakeLists.txt:set(GMX_GPU_NB_NUM_CLUSTER_PER_CELL_Z 2 CACHE STRING "Number of clusters along Z in a pair-search grid cell for GPU lists")
CMakeLists.txt:mark_as_advanced(GMX_GPU_NB_NUM_CLUSTER_PER_CELL_X)
CMakeLists.txt:mark_as_advanced(GMX_GPU_NB_NUM_CLUSTER_PER_CELL_Y)
CMakeLists.txt:mark_as_advanced(GMX_GPU_NB_NUM_CLUSTER_PER_CELL_Z)
CMakeLists.txt:# For build with CUDA/SYCL and Lib-MPI, check if underlying MPI implementation is GPU-aware
CMakeLists.txt:# GPU-aware MPI allows direct GPU communication without staging data through host
CMakeLists.txt:if((GMX_GPU_CUDA OR GMX_GPU_HIP OR GMX_GPU_SYCL) AND GMX_LIB_MPI)
CMakeLists.txt:    include(gmxManageGpuAwareMpi)
CMakeLists.txt:    set(MPI_SUPPORTS_CUDA_AWARE_DETECTION 0)
CMakeLists.txt:if(GMX_GPU_CUDA AND NOT GMX_GPU_FFT_CUFFT)
CMakeLists.txt:    message(FATAL_ERROR "The CUDA build only supports cuFFT GPU FFT library")
CMakeLists.txt:    if(NOT GMX_GPU_CUDA AND NOT GMX_GPU_HIP AND NOT GMX_GPU_SYCL)
CMakeLists.txt:        message(FATAL_ERROR "HeFFTe support requires a CUDA, HIP or SYCL build")
CMakeLists.txt:    if(GMX_GPU_CUDA)
CMakeLists.txt:        find_package(Heffte 2.2.0 REQUIRED CUDA)
CMakeLists.txt:    elseif(GMX_GPU_HIP)
CMakeLists.txt:        find_package(Heffte 2.2.0 REQUIRED ROCM)
CMakeLists.txt:    elseif(GMX_GPU_SYCL)
CMakeLists.txt:        if(GMX_GPU_FFT_ROCFFT)
CMakeLists.txt:            find_package(Heffte 2.2.0 REQUIRED ROCM)
CMakeLists.txt:        elseif(GMX_GPU_FFT_CUFFT)
CMakeLists.txt:            find_package(Heffte 2.2.0 REQUIRED CUDA)
CMakeLists.txt:                message(WARNING "HeFFTe with cuFFT backend should be used in a CUDA build")
CMakeLists.txt:        elseif(GMX_GPU_FFT_MKL)
CMakeLists.txt:            message(FATAL_ERROR "Your GPU FFT library is incompatible for use in a GROMACS SYCL build with heFFTe. "
CMakeLists.txt:                    "Use -DGMX_GPU_FFT_LIBRARY=rocFFT or -DGMX_GPU_FFT_LIBRARY=MKL or -DGMX_GPU_FFT_LIBRARY=CUFFT")
CMakeLists.txt:    if(NOT GMX_GPU_CUDA)
CMakeLists.txt:        message(FATAL_ERROR "cuFFTMp support requires a CUDA build")
CMakeLists.txt:    find_package(cuFFTMp REQUIRED CUDA)
cmake/gmxManageOneMKL.cmake:# Manage OneMKL, GPU FFT library used with SYCL.
cmake/gmxManageOneMKL.cmake:    if(GMX_DPCPP_HAVE_CUDA_TARGET)
cmake/gmxManageOneMKL.cmake:            message(WARNING "GROMACS SYCL is targetting NVIDIA GPU, but oneMKL interface library was not built with the cuFFT backend.")
cmake/gmxManageOneMKL.cmake:            message(WARNING "GROMACS SYCL is targetting AMD GPU, but oneMKL interface library was not built with the rocFFT backend.")
cmake/gmxManageOneMKL.cmake:        if(NOT TARGET MKL::onemkl_dft_mklgpu)
cmake/gmxManageOneMKL.cmake:            message(WARNING "GROMACS SYCL is targetting Intel GPU, but oneMKL interface library was not built with the MKLGPU backend.")
cmake/gmxManageGpuAwareMpi.cmake:# - Define function to check if underlying MPI is GPU-aware
cmake/gmxManageGpuAwareMpi.cmake:#  GMX_TEST_GPU_AWARE_MPI()
cmake/gmxManageGpuAwareMpi.cmake:#  GMX_TEST_GPU_AWARE_MPI(BACKEND) puts HAVE_MPI_EXT and MPI_SUPPORTS_${BACKEND}_AWARE_DETECTION
cmake/gmxManageGpuAwareMpi.cmake:#  variables in cache. Possible values of "BACKEND" are: cuda, hip, ze (LevelZero).
cmake/gmxManageGpuAwareMpi.cmake:function(GMX_TEST_GPU_AWARE_MPI BACKEND)
cmake/gmxManageGpuAwareMpi.cmake:      if (GMX_GPU_CUDA AND ("${BACKEND_UPPER}" STREQUAL "CUDA"))
cmake/gmxManageGpuAwareMpi.cmake:        "for better multi-GPU performance consider using a more recent CUDA-aware MPI.")
cmake/gmxManageGpuAwareMpi.cmake:# Test if GPU-aware MPI detection is supported
cmake/gmxManageGpuAwareMpi.cmake:if (GMX_GPU_CUDA)
cmake/gmxManageGpuAwareMpi.cmake:  gmx_test_gpu_aware_mpi(cuda)
cmake/gmxManageGpuAwareMpi.cmake:  set(MPI_SUPPORTS_ROCM_AWARE_DETECTION FALSE)
cmake/gmxManageGpuAwareMpi.cmake:if (GMX_GPU_HIP)
cmake/gmxManageGpuAwareMpi.cmake:  set(MPI_SUPPORTS_CUDA_AWARE_DETECTION FALSE)
cmake/gmxManageGpuAwareMpi.cmake:  gmx_test_gpu_aware_mpi(hip) # MPICH has MPIX_Query_hip_support
cmake/gmxManageGpuAwareMpi.cmake:  gmx_test_gpu_aware_mpi(rocm) # OpenMPI has MPIX_Query_rocm_support
cmake/gmxManageGpuAwareMpi.cmake:if(GMX_GPU_SYCL)
cmake/gmxManageGpuAwareMpi.cmake:  gmx_test_gpu_aware_mpi(cuda)
cmake/gmxManageGpuAwareMpi.cmake:  gmx_test_gpu_aware_mpi(hip) # MPICH has MPIX_Query_hip_support
cmake/gmxManageGpuAwareMpi.cmake:  gmx_test_gpu_aware_mpi(rocm) # OpenMPI has MPIX_Query_rocm_support
cmake/gmxManageGpuAwareMpi.cmake:  gmx_test_gpu_aware_mpi(ze)
cmake/gmxManageHip.cmake:# We need to set the gpu targets to a dummy value before looking up the ROCM library to avoid the "feature" to autodetect
cmake/gmxManageHip.cmake:set(AMDGPU_TARGETS "gfx90a")
cmake/gmxManageHip.cmake:find_package(HIP REQUIRED CONFIG PATHS $ENV{ROCM_PATH} "/opt/rocm")
cmake/gmxManageHip.cmake:    message(FATAL_ERROR "The found HIP version ${HIP_VERSION} is less than the required version ${REQUIRED_HIP_VERSION}. Please update your ROCm stack")
cmake/gmxManageHip.cmake:set(GMX_GPU_HIP ON)
cmake/gmxManageHip.cmake:if(GMX_GPU_FFT_VKFFT)
cmake/gmxManageHip.cmake:elseif(GMX_GPU_FFT_ROCFFT OR GMX_USE_Heffte)
cmake/gmxManageHip.cmake:    message(FATAL_ERROR "The configured GPU FFT library ${GMX_GPU_FFT_LIBRARY} can not be used together with the HIP backend") 
cmake/gmxManageHip.cmake:        message(WARNING "The HIP_HIPCONFIG_EXECUTABLE variable has not been set by find_package(HIP), meaning GROMACS can not properly present the information about the ROCm toolkit")
cmake/gmxManageSyclOneApi.cmake:if(NOT GMX_GPU_SYCL OR GMX_SYCL_ACPP OR NOT GMX_SYCL_DPCPP)
cmake/gmxManageSyclOneApi.cmake:#    warning GMX_DPCPP_TEST_HAVE_CUDA_TARGET
cmake/gmxManageSyclOneApi.cmake:    foreach (target IN ITEMS CUDA HIP HIP_WAVE32 HIP_WAVE64 INTEL)
cmake/gmxManageSyclOneApi.cmake:if(NOT GMX_DPCPP_HAVE_CUDA_TARGET AND NOT GMX_DPCPP_HAVE_HIP_TARGET AND NOT GMX_DPCPP_HAVE_INTEL_TARGET)
cmake/gmxManageSyclOneApi.cmake:    message(WARNING "SYCL oneAPI has no known GPU targets enabled!")
cmake/gmxManageSyclOneApi.cmake:if(GMX_DPCPP_HAVE_CUDA_TARGET OR GMX_DPCPP_HAVE_HIP_TARGET)
cmake/gmxManageSyclOneApi.cmake:    # When compiling for NVIDIA/AMD, Intel LLVM produces tons of harmless warnings, ignore them
cmake/gmxManageSyclOneApi.cmake:    # Set GMX_GPU_NB_DISABLE_CLUSTER_PAIR_SPLIT when targeting only devices with 64-wide execution
cmake/gmxManageSyclOneApi.cmake:    if (GMX_DPCPP_HAVE_CUDA_TARGET OR GMX_DPCPP_HAVE_INTEL_TARGET OR GMX_DPCPP_HAVE_HIP_WAVE32_TARGET)
cmake/gmxManageSyclOneApi.cmake:        set(_have_subgroup_not_64 ON) # We have AMD RDNA, NVIDIA, or Intel target(s)
cmake/gmxManageSyclOneApi.cmake:        option(GMX_GPU_NB_DISABLE_CLUSTER_PAIR_SPLIT
cmake/gmxManageSyclOneApi.cmake:            "Disable NBNXM GPU cluster pair splitting. Only supported with SYCL and 64-wide GPU architectures (like AMD GCN/CDNA)."
cmake/gmxManageSyclOneApi.cmake:        mark_as_advanced(GMX_GPU_NB_DISABLE_CLUSTER_PAIR_SPLIT)
cmake/gmxManageSyclOneApi.cmake:        set(GMX_HAVE_GPU_GRAPH_SUPPORT ON)
cmake/gmxManageSyclOneApi.cmake:    set(GMX_HAVE_GPU_GRAPH_SUPPORT OFF)
cmake/gmxManageSyclOneApi.cmake:if(GMX_GPU_FFT_VKFFT)
cmake/gmxManageSyclOneApi.cmake:if(GMX_GPU_FFT_MKL)
cmake/gmxManageSyclOneApi.cmake:    list(APPEND GMX_EXTRA_LIBRARIES "${mkl_sycl_PATH};OpenCL")
cmake/gmxManageSyclOneApi.cmake:if(GMX_GPU_FFT_ONEMKL)
cmake/gmxManageSyclOneApi.cmake:if(GMX_GPU_FFT_BBFFT)
cmake/gmxManageNvccConfig.cmake:# Manage CUDA nvcc compilation configuration, try to be smart to ease the users'
cmake/gmxManageNvccConfig.cmake:# - use the CUDA_HOST_COMPILER if defined by the user, otherwise
cmake/gmxManageNvccConfig.cmake:# - check if nvcc works with CUDA_HOST_COMPILER and the generated nvcc and C++ flags
cmake/gmxManageNvccConfig.cmake:#   * CUDA_HOST_COMPILER_OPTIONS    - the full host-compiler related option list passed to nvcc
cmake/gmxManageNvccConfig.cmake:# Note that from CMake 2.8.10 FindCUDA defines CUDA_HOST_COMPILER internally,
cmake/gmxManageNvccConfig.cmake:# glibc 2.23 changed string.h in a way that breaks CUDA compilation in
cmake/gmxManageNvccConfig.cmake:# feature and performance of memcpy variants is unimportant for CUDA
cmake/gmxManageNvccConfig.cmake:        message(STATUS "Adding work-around for issue compiling CUDA code with glibc 2.23 string.h")
cmake/gmxManageNvccConfig.cmake:        list(APPEND CUDA_HOST_COMPILER_OPTIONS "-D_FORCE_INLINES")
cmake/gmxManageNvccConfig.cmake:        set(CUDA_HOST_COMPILER_OPTIONS ${CUDA_HOST_COMPILER_OPTIONS} PARENT_SCOPE)
cmake/gmxManageNvccConfig.cmake:gmx_check_if_changed(CUDA_HOST_COMPILER_CHANGED CMAKE_CUDA_HOST_COMPILER)
cmake/gmxManageNvccConfig.cmake:if(CUDA_HOST_COMPILER_CHANGED)
cmake/gmxManageNvccConfig.cmake:    set(CUDA_HOST_COMPILER_OPTIONS "")
cmake/gmxManageNvccConfig.cmake:        list(APPEND CUDA_HOST_COMPILER_OPTIONS "-D__STRICT_ANSI__")
cmake/gmxManageNvccConfig.cmake:    set(CUDA_HOST_COMPILER_OPTIONS "${CUDA_HOST_COMPILER_OPTIONS}"
cmake/gmxManageNvccConfig.cmake:    mark_as_advanced(CMAKE_CUDA_HOST_COMPILER  CUDA_HOST_COMPILER_OPTIONS)
cmake/gmxManageNvccConfig.cmake:# CUDA source file is built. Set the CMake variable GMX_NVCC_WORKS
cmake/gmxManageNvccConfig.cmake:if((_cuda_nvcc_executable_or_flags_changed OR CUDA_HOST_COMPILER_CHANGED OR NOT GMX_NVCC_WORKS))
cmake/gmxManageNvccConfig.cmake:    message(STATUS "Check for working NVCC/C++ compiler combination with nvcc '${CUDA_NVCC_EXECUTABLE}'")
cmake/gmxManageNvccConfig.cmake:    execute_process(COMMAND ${CUDA_NVCC_EXECUTABLE} --compiler-bindir=${CUDA_HOST_COMPILER} -c ${CUDA_NVCC_FLAGS} ${CUDA_NVCC_FLAGS_${_build_type}} ${CMAKE_SOURCE_DIR}/cmake/TestCUDA.cu
cmake/gmxManageNvccConfig.cmake:        RESULT_VARIABLE _cuda_test_res
cmake/gmxManageNvccConfig.cmake:        OUTPUT_VARIABLE _cuda_test_out
cmake/gmxManageNvccConfig.cmake:        ERROR_VARIABLE  _cuda_test_err
cmake/gmxManageNvccConfig.cmake:    if(${_cuda_test_res})
cmake/gmxManageNvccConfig.cmake:        message(STATUS "${CUDAToolkit_NVCC_EXECUTABLE} standard output: '${_cuda_test_out}'")
cmake/gmxManageNvccConfig.cmake:        message(STATUS "${CUDAToolkit_NVCC_EXECUTABLE} standard error:  '${_cuda_test_err}'")
cmake/gmxManageNvccConfig.cmake:        if(${_cuda_test_err} MATCHES "nsupported")
cmake/gmxManageNvccConfig.cmake:            message(FATAL_ERROR "NVCC/C++ compiler combination does not seem to be supported. CUDA frequently does not support the latest versions of the host compiler, so you might want to try an earlier C++ compiler version and make sure your CUDA compiler and driver are as recent as possible. Set the GMX_NVCC_WORKS CMake cache variable to bypass this check if you know what you are doing.")
cmake/gmxManageNvccConfig.cmake:            message(FATAL_ERROR "CUDA compiler does not seem to be functional or is not compatible with the host compiler. Set the GMX_NVCC_WORKS CMake cache variable to bypass this check if you know what you are doing.")
cmake/gmxManageNvccConfig.cmake:    elseif(NOT GMX_CUDA_TEST_COMPILER_QUIETLY)
cmake/gmxManageNvccConfig.cmake:            COMMAND ${CUDAToolkit_NVCC_EXECUTABLE} ${ARGN} -ccbin ${CMAKE_CUDA_HOST_COMPILER} "${CMAKE_SOURCE_DIR}/cmake/TestCUDA.cu"
cmake/gmxManageNvccConfig.cmake:            RESULT_VARIABLE _cuda_success
cmake/gmxManageNvccConfig.cmake:        if (_cuda_success EQUAL 0)
cmake/gmxManageNvccConfig.cmake:            set(CCBIN "--compiler-bindir=${CMAKE_CUDA_HOST_COMPILER}")
cmake/gmxManageNvccConfig.cmake:                COMMAND ${CUDAToolkit_NVCC_EXECUTABLE} ${ARGN} ${CCBIN} "${CMAKE_SOURCE_DIR}/cmake/TestCUDA.cu"
cmake/gmxManageNvccConfig.cmake:                RESULT_VARIABLE _cuda_success
cmake/gmxManageNvccConfig.cmake:            if (_cuda_success EQUAL 0)
cmake/gmxManageNvccConfig.cmake:# If any of these manual override variables for target CUDA GPU architectures
cmake/gmxManageNvccConfig.cmake:set(GMX_CUDA_NVCC_GENCODE_FLAGS)
cmake/gmxManageNvccConfig.cmake:if (GMX_CUDA_TARGET_SM OR GMX_CUDA_TARGET_COMPUTE)
cmake/gmxManageNvccConfig.cmake:    set(_target_sm_list ${GMX_CUDA_TARGET_SM})
cmake/gmxManageNvccConfig.cmake:        gmx_add_nvcc_flag_if_supported(GMX_CUDA_NVCC_GENCODE_FLAGS NVCC_HAS_GENCODE_COMPUTE_AND_SM_${_target} "--generate-code=arch=compute_${_target},code=sm_${_target}")
cmake/gmxManageNvccConfig.cmake:            message(FATAL_ERROR "Your choice of ${_target} in GMX_CUDA_TARGET_SM was not accepted by nvcc, please choose a target that it accepts")
cmake/gmxManageNvccConfig.cmake:    set(_target_compute_list ${GMX_CUDA_TARGET_COMPUTE})
cmake/gmxManageNvccConfig.cmake:        gmx_add_nvcc_flag_if_supported(GMX_CUDA_NVCC_GENCODE_FLAGS NVCC_HAS_GENCODE_COMPUTE_${_target} --generate-code=arch=compute_${_target},code=compute_${_target})
cmake/gmxManageNvccConfig.cmake:            message(FATAL_ERROR "Your choice of ${_target} in GMX_CUDA_TARGET_COMPUTE was not accepted by nvcc, please choose a target that it accepts")
cmake/gmxManageNvccConfig.cmake:    # TODO: Note that this SM arch populating code can be replaced with `CUDA_ARCHITECTURES=all` once we upgrade cmake to >= 3.23
cmake/gmxManageNvccConfig.cmake:    # Set the CUDA GPU architectures to compile for:
cmake/gmxManageNvccConfig.cmake:    # - with CUDA >=11.0        CC 8.0 is supported
cmake/gmxManageNvccConfig.cmake:    gmx_add_nvcc_flag_if_supported(GMX_CUDA_NVCC_GENCODE_FLAGS NVCC_HAS_GENCODE_COMPUTE_AND_SM_50 --generate-code=arch=compute_50,code=sm_50)
cmake/gmxManageNvccConfig.cmake:    gmx_add_nvcc_flag_if_supported(GMX_CUDA_NVCC_GENCODE_FLAGS NVCC_HAS_GENCODE_COMPUTE_AND_SM_52 --generate-code=arch=compute_52,code=sm_52)
cmake/gmxManageNvccConfig.cmake:    gmx_add_nvcc_flag_if_supported(GMX_CUDA_NVCC_GENCODE_FLAGS NVCC_HAS_GENCODE_COMPUTE_AND_SM_60 --generate-code=arch=compute_60,code=sm_60)
cmake/gmxManageNvccConfig.cmake:    gmx_add_nvcc_flag_if_supported(GMX_CUDA_NVCC_GENCODE_FLAGS NVCC_HAS_GENCODE_COMPUTE_AND_SM_61 --generate-code=arch=compute_61,code=sm_61)
cmake/gmxManageNvccConfig.cmake:    gmx_add_nvcc_flag_if_supported(GMX_CUDA_NVCC_GENCODE_FLAGS NVCC_HAS_GENCODE_COMPUTE_AND_SM_70 --generate-code=arch=compute_70,code=sm_70)
cmake/gmxManageNvccConfig.cmake:    gmx_add_nvcc_flag_if_supported(GMX_CUDA_NVCC_GENCODE_FLAGS NVCC_HAS_GENCODE_COMPUTE_AND_SM_75 --generate-code=arch=compute_75,code=sm_75)
cmake/gmxManageNvccConfig.cmake:    gmx_add_nvcc_flag_if_supported(GMX_CUDA_NVCC_GENCODE_FLAGS NVCC_HAS_GENCODE_COMPUTE_AND_SM_80 --generate-code=arch=compute_80,code=sm_80)
cmake/gmxManageNvccConfig.cmake:    gmx_add_nvcc_flag_if_supported(GMX_CUDA_NVCC_GENCODE_FLAGS NVCC_HAS_GENCODE_COMPUTE_AND_SM_86 --generate-code=arch=compute_86,code=sm_86)
cmake/gmxManageNvccConfig.cmake:    gmx_add_nvcc_flag_if_supported(GMX_CUDA_NVCC_GENCODE_FLAGS NVCC_HAS_GENCODE_COMPUTE_AND_SM_89 --generate-code=arch=compute_89,code=sm_89)
cmake/gmxManageNvccConfig.cmake:    gmx_add_nvcc_flag_if_supported(GMX_CUDA_NVCC_GENCODE_FLAGS NVCC_HAS_GENCODE_COMPUTE_AND_SM_90 --generate-code=arch=compute_90,code=sm_90)
cmake/gmxManageNvccConfig.cmake:    gmx_add_nvcc_flag_if_supported(GMX_CUDA_NVCC_FLAGS NVCC_HAS_WARNING_NO_DEPRECATED_GPU_TARGETS -Wno-deprecated-gpu-targets)
cmake/gmxManageNvccConfig.cmake:    gmx_add_nvcc_flag_if_supported(GMX_CUDA_NVCC_GENCODE_FLAGS NVCC_HAS_GENCODE_COMPUTE_53 --generate-code=arch=compute_53,code=sm_53)
cmake/gmxManageNvccConfig.cmake:    gmx_add_nvcc_flag_if_supported(GMX_CUDA_NVCC_GENCODE_FLAGS NVCC_HAS_GENCODE_COMPUTE_80 --generate-code=arch=compute_80,code=sm_80)
cmake/gmxManageNvccConfig.cmake:if (GMX_CUDA_TARGET_SM)
cmake/gmxManageNvccConfig.cmake:    set_property(CACHE GMX_CUDA_TARGET_SM PROPERTY HELPSTRING "List of CUDA GPU architecture codes to compile for (without the sm_ prefix)")
cmake/gmxManageNvccConfig.cmake:    set_property(CACHE GMX_CUDA_TARGET_SM PROPERTY TYPE STRING)
cmake/gmxManageNvccConfig.cmake:if (GMX_CUDA_TARGET_COMPUTE)
cmake/gmxManageNvccConfig.cmake:    set_property(CACHE GMX_CUDA_TARGET_COMPUTE PROPERTY HELPSTRING "List of CUDA virtual architecture codes to compile for (without the compute_ prefix)")
cmake/gmxManageNvccConfig.cmake:    set_property(CACHE GMX_CUDA_TARGET_COMPUTE PROPERTY TYPE STRING)
cmake/gmxManageNvccConfig.cmake:# assemble the CUDA flags
cmake/gmxManageNvccConfig.cmake:gmx_add_nvcc_flag_if_supported(GMX_CUDA_NVCC_FLAGS NVCC_HAS_USE_FAST_MATH -use_fast_math)
cmake/gmxManageNvccConfig.cmake:gmx_add_nvcc_flag_if_supported(GMX_CUDA_NVCC_FLAGS NVCC_HAS_PTXAS_WARN_DOUBLE_USAGE -Xptxas=-warn-double-usage)
cmake/gmxManageNvccConfig.cmake:gmx_add_nvcc_flag_if_supported(GMX_CUDA_NVCC_FLAGS NVCC_HAS_PTXAS_WERROR -Xptxas=-Werror)
cmake/gmxManageNvccConfig.cmake:# strip gencode/arch from GMX_CUDA_NVCC_GENCODE_FLAGS only keep the arch numbers
cmake/gmxManageNvccConfig.cmake:string(REGEX REPLACE "([A-Za-z=_-])" ""  GMX_CUDA_NVCC_GENCODE_FLAGS "${GMX_CUDA_NVCC_GENCODE_FLAGS}")
cmake/gmxManageNvccConfig.cmake:string(REGEX REPLACE "([0-9]+,)" ""  GMX_CUDA_NVCC_GENCODE_FLAGS "${GMX_CUDA_NVCC_GENCODE_FLAGS}")
cmake/gmxManageNvccConfig.cmake:    if(NOT ${OpenMP_CXX_FLAGS} IN_LIST GMX_CUDA_NVCC_FLAGS)
cmake/gmxManageNvccConfig.cmake:        list(APPEND CUDA_HOST_COMPILER_OPTIONS -Xcompiler=${OpenMP_CXX_FLAGS})
cmake/gmxManageNvccConfig.cmake:    if(NOT ${CMAKE_CXX_FLAGS} IN_LIST GMX_CUDA_NVCC_FLAGS)
cmake/gmxManageNvccConfig.cmake:            list(APPEND CUDA_HOST_COMPILER_OPTIONS -Xcompiler="${item}")
cmake/gmxManageNvccConfig.cmake:        if(NOT GMXC_CXXFLAGS IN_LIST GMX_CUDA_NVCC_FLAGS)
cmake/gmxManageNvccConfig.cmake:                list(APPEND CUDA_HOST_COMPILER_OPTIONS -Xcompiler="${item}")
cmake/gmxManageNvccConfig.cmake:# assemble the CUDA host compiler flags
cmake/gmxManageNvccConfig.cmake:list(APPEND GMX_CUDA_NVCC_FLAGS "${CUDA_HOST_COMPILER_OPTIONS}")
cmake/gmxManageNvccConfig.cmake:# Disable cudafe warnings with nvc++ as a host compiler - warning #177-D
cmake/gmxManageNvccConfig.cmake:gmx_add_nvcc_flag_if_supported(GMX_CUDA_NVCC_FLAGS NVCC_HAS_DIAG_SUPPRESS_177 -diag-suppress=177)
cmake/gmxManageNvccConfig.cmake:gmx_check_if_changed(_cuda_nvcc_executable_or_flags_changed CUDAToolkit_NVCC_EXECUTABLE CUDA_NVCC_FLAGS CUDA_NVCC_FLAGS_${_build_type})
cmake/gmxManageCuda.cmake:set(GMX_GPU_CUDA ON)
cmake/gmxManageCuda.cmake:option(GMX_CLANG_CUDA "Use clang for CUDA" OFF)
cmake/gmxManageCuda.cmake:    message(FATAL_ERROR "CUDA acceleration is not available in double precision")
cmake/gmxManageCuda.cmake:set(CMAKE_CUDA_STANDARD ${CMAKE_CXX_STANDARD})
cmake/gmxManageCuda.cmake:set(CMAKE_CUDA_STANDARD_REQUIRED ON)
cmake/gmxManageCuda.cmake:find_package(CUDAToolkit ${GMX_CUDA_MINIMUM_REQUIRED_VERSION} REQUIRED)
cmake/gmxManageCuda.cmake:if(CUDAToolkit_VERSION GREATER_EQUAL 11.1)
cmake/gmxManageCuda.cmake:  set(GMX_HAVE_GPU_GRAPH_SUPPORT ON)
cmake/gmxManageCuda.cmake:# Try to execute ${CUDAToolkit_NVCC_EXECUTABLE} --version and set the output
cmake/gmxManageCuda.cmake:macro(get_cuda_compiler_info COMPILER_INFO DEVICE_COMPILER_FLAGS HOST_COMPILER_FLAGS)
cmake/gmxManageCuda.cmake:    if(NOT GMX_CLANG_CUDA)
cmake/gmxManageCuda.cmake:        if(CUDAToolkit_NVCC_EXECUTABLE)
cmake/gmxManageCuda.cmake:            execute_process(COMMAND ${CUDAToolkit_NVCC_EXECUTABLE} --version
cmake/gmxManageCuda.cmake:                SET(${COMPILER_INFO} "${CUDAToolkit_NVCC_EXECUTABLE} ${_nvcc_info_singleline}")
cmake/gmxManageCuda.cmake:                if(CUDA_PROPAGATE_HOST_FLAGS)
cmake/gmxManageCuda.cmake:                SET(_compiler_flags "${CUDA_NVCC_FLAGS_${_build_type}}")
cmake/gmxManageCuda.cmake:                SET(${DEVICE_COMPILER_FLAGS} "${CUDA_NVCC_FLAGS}${CUDA_NVCC_FLAGS_${_build_type}}")
cmake/gmxManageCuda.cmake:        # CXX compiler is the CUDA compiler
cmake/gmxManageCuda.cmake:        set(${COMPILER_FLAGS} "${CMAKE_CXX_FLAGS} ${CMAKE_CXX_FLAGS_${_build_type}} ${GMX_CUDA_CLANG_FLAGS}")
cmake/gmxManageCuda.cmake:if(GMX_CLANG_CUDA)
cmake/gmxManageCuda.cmake:    include(gmxManageClangCudaConfig)
cmake/gmxManageCuda.cmake:    list(APPEND GMX_EXTRA_LIBRARIES ${GMX_CUDA_CLANG_LINK_LIBS})
cmake/gmxManageCuda.cmake:    link_directories("${GMX_CUDA_CLANG_LINK_DIRS}")
cmake/gmxManageCuda.cmake:    # Using NVIDIA compiler
cmake/gmxManageCuda.cmake:    if(NOT CUDAToolkit_NVCC_EXECUTABLE)
cmake/gmxManageCuda.cmake:        message(FATAL_ERROR "nvcc is required for a CUDA build, please set CUDAToolkit_ROOT appropriately")
cmake/gmxManageCuda.cmake:    if(NOT CMAKE_CUDA_HOST_COMPILER)
cmake/gmxManageCuda.cmake:        set(CMAKE_CUDA_HOST_COMPILER ${CMAKE_CXX_COMPILER})
cmake/gmxManageCuda.cmake:# We make sure to call get_cuda_compiler_info() before reaching this line,
cmake/gmxManageCuda.cmake:# before the call to enable_language(CUDA).
cmake/gmxManageCuda.cmake:enable_language(CUDA)
cmake/gmxManageCuda.cmake:option(GMX_CUDA_NB_SINGLE_COMPILATION_UNIT "Whether to compile the CUDA non-bonded module using a single compilation unit." OFF)
cmake/gmxManageCuda.cmake:mark_as_advanced(GMX_CUDA_NB_SINGLE_COMPILATION_UNIT)
cmake/FindEXTRAE.cmake:# cudatrace: CUDA
cmake/FindEXTRAE.cmake:# cudampitrace: CUDA + MPI
cmake/FindEXTRAE.cmake:# cudaompitrace: CUDA+OPENMP+MPI (in the dev version)
cmake/FindEXTRAE.cmake:# cudaomptrace: CUDA + OPENMP
cmake/FindEXTRAE.cmake:# cudapttrace: CUDA + pthreads
cmake/FindEXTRAE.cmake:# cudaptmpitrace: CUDA + pthreads + MPI (unsupported combination in Gromacs)
cmake/FindEXTRAE.cmake:  if (GMX_GPU)
cmake/FindEXTRAE.cmake:    set (extraelib "cuda${extraelib}")
cmake/FindEXTRAE.cmake:    if (GMX_GPU)
cmake/FindEXTRAE.cmake:      set (extraelib "cuda${extraelib}")
cmake/FindEXTRAE.cmake:# library with CUDA only support
cmake/FindEXTRAE.cmake:elseif (GMX_GPU)
cmake/FindEXTRAE.cmake:    set (extraelib "cuda${extraelib}")
cmake/FindHip.cmake:        "$ENV{ROCM_PATH}/hip"
cmake/FindHip.cmake:        /opt/rocm/
cmake/FindHip.cmake:            "$ENV{ROCM_PATH}/hip"
cmake/FindHip.cmake:            /opt/rocm/
cmake/FindHip.cmake:            "$ENV{ROCM_PATH}/hip"
cmake/FindHip.cmake:            /opt/rocm/
cmake/gmxManageSyclAdaptiveCpp.cmake:if(NOT GMX_GPU_SYCL OR GMX_SYCL_DPCPP OR NOT GMX_SYCL_ACPP)
cmake/gmxManageSyclAdaptiveCpp.cmake:# -Wno-unknown-cuda-version because Clang often complains about the newest CUDA, despite working fine with it.
cmake/gmxManageSyclAdaptiveCpp.cmake:set(ACPP_EXTRA_ARGS "-Wno-unknown-cuda-version -Wno-unknown-attributes ${SYCL_CXX_FLAGS_EXTRA}")
cmake/gmxManageSyclAdaptiveCpp.cmake:check_cxx_compiler_flag("-fgpu-inline-threshold=1" HAS_GPU_INLINE_THRESHOLD)
cmake/gmxManageSyclAdaptiveCpp.cmake:if(${HAS_GPU_INLINE_THRESHOLD})
cmake/gmxManageSyclAdaptiveCpp.cmake:    list(APPEND ACPP_EXTRA_COMPILE_OPTIONS -fgpu-inline-threshold=99999)
cmake/gmxManageSyclAdaptiveCpp.cmake:        foreach (target IN ITEMS CUDA HIP HIP_WAVE32 HIP_WAVE64 SPIRV GENERIC)
cmake/gmxManageSyclAdaptiveCpp.cmake:if(NOT GMX_ACPP_HAVE_CUDA_TARGET AND NOT GMX_ACPP_HAVE_HIP_TARGET)
cmake/gmxManageSyclAdaptiveCpp.cmake:    message(WARNING "AdaptiveCpp/hipSYCL has no GPU targets set! Please, specify target hardware with -DHIPSYCL_TARGETS CMake option")
cmake/gmxManageSyclAdaptiveCpp.cmake:if(GMX_ACPP_HAVE_CUDA_TARGET AND GMX_ACPP_HAVE_HIP_TARGET)
cmake/gmxManageSyclAdaptiveCpp.cmake:    message(FATAL_ERROR "AdaptiveCpp/hipSYCL cannot have both CUDA and HIP targets active! This would require explicit multipass mode which both decreases performance on NVIDIA devices and is no longer supported in clang. Compile only for either CUDA or HIP targets.")
cmake/gmxManageSyclAdaptiveCpp.cmake:if(GMX_GPU_FFT_VKFFT)
cmake/gmxManageSyclAdaptiveCpp.cmake:        option(GMX_GPU_NB_DISABLE_CLUSTER_PAIR_SPLIT
cmake/gmxManageSyclAdaptiveCpp.cmake:            "Disable NBNXM GPU cluster pair splitting. Only supported with SYCL and 64-wide GPU architectures (like AMD GCN/CDNA)."
cmake/gmxManageSyclAdaptiveCpp.cmake:        mark_as_advanced(GMX_GPU_NB_DISABLE_CLUSTER_PAIR_SPLIT)
cmake/gmxManageSyclAdaptiveCpp.cmake:    "Enable compiling kernels for AMD RDNA GPUs (gfx1xxx). When OFF, only CDNA and GCN are supported. Only used with AdaptiveCpp/hipSYCL."
cmake/gmxManageSyclAdaptiveCpp.cmake:if (GMX_ACPP_HAVE_HIP_TARGET AND GMX_GPU_FFT_ROCFFT)
cmake/gmxManageSyclAdaptiveCpp.cmake:    # default ROCm distribution that supports the version of
cmake/gmxManageSyclAdaptiveCpp.cmake:    # default ROCm installation used by hipSYCL, which can be used
cmake/gmxManageSyclAdaptiveCpp.cmake:        # Starting from AdaptiveCpp 24.02, the file is "etc/AdaptiveCpp/acpp-rocm.json"
cmake/gmxManageSyclAdaptiveCpp.cmake:            NAMES acpp-rocm.json syclcc.json
cmake/gmxManageSyclAdaptiveCpp.cmake:    if(HIPSYCL_SYCLCC_ROCM_PATH AND NOT ACPP_ROCM_PATH)
cmake/gmxManageSyclAdaptiveCpp.cmake:        set(ACPP_ROCM_PATH ${HIPSYCL_SYCLCC_ROCM_PATH})
cmake/gmxManageSyclAdaptiveCpp.cmake:    if (ACPP_JSON AND NOT ACPP_ROCM_PATH)
cmake/gmxManageSyclAdaptiveCpp.cmake:        string(JSON ACPP_ROCM_PATH_VALUE GET ${ACPP_JSON_CONTENTS} "default-rocm-path")
cmake/gmxManageSyclAdaptiveCpp.cmake:        set(ACPP_ROCM_PATH "${ACPP_ROCM_PATH_VALUE}" CACHE PATH "The default ROCm used by AdaptiveCpp/hipSYCL" FORCE)
cmake/gmxManageSyclAdaptiveCpp.cmake:    if(ACPP_ROCM_PATH)
cmake/gmxManageSyclAdaptiveCpp.cmake:        # from the ROCm distribution used by AdaptiveCpp.
cmake/gmxManageSyclAdaptiveCpp.cmake:        set(hip_DIR ${ACPP_ROCM_PATH}/lib/cmake/hip)
cmake/gmxManageSyclAdaptiveCpp.cmake:        set(AMDDeviceLibs_DIR ${ACPP_ROCM_PATH}/lib/cmake/AMDDeviceLibs)
cmake/gmxManageSyclAdaptiveCpp.cmake:        set(amd_comgr_DIR ${ACPP_ROCM_PATH}/lib/cmake/amd_comgr)
cmake/gmxManageSyclAdaptiveCpp.cmake:        set(hsa-runtime64_DIR ${ACPP_ROCM_PATH}/lib/cmake/hsa-runtime64)
cmake/gmxManageSyclAdaptiveCpp.cmake:        set(ROCclr_DIR ${ACPP_ROCM_PATH}/lib/cmake/rocclr)
cmake/gmxManageSyclAdaptiveCpp.cmake:        set(rocfft_DIR ${ACPP_ROCM_PATH}/lib/cmake/rocfft)
cmake/gmxManageSyclAdaptiveCpp.cmake:    # Find rocFFT, either from the ROCm used by AdaptiveCpp, or as otherwise found on the system
cmake/gmxManageSyclAdaptiveCpp.cmake:    find_package(rocfft ${FIND_ROCFFT_QUIETLY} CONFIG HINTS ${ACPP_ROCM_PATH} PATHS /opt/rocm)
cmake/gmxCFlags.cmake:# Prepare some local variables so CUDA and non-CUDA code in targets
cmake/gmxCFlags.cmake:# or gmx_device_target_compile_options(name_of_variable) because CUDA
cmake/gmxCFlags.cmake:            $<$<LINK_LANGUAGE:CUDA>:MPI::MPI_CXX>
cmake/gmxCFlags.cmake:# The approach taken by FindCUDA.cmake is to require that the compiler
cmake/gmxCFlags.cmake:# the point of calling cuda_add_library, which does not create a
cmake/gmxCFlags.cmake:# when we use native CUDA language support in our CMake.
cmake/gmxCFlags.cmake:        # CUDA headers issue lots of warnings when compiled with
cmake/gmxCFlags.cmake:        # to have FindCUDA.cmake treat CUDA internal headers with
cmake/gmxCFlags.cmake:        # form of FindCUDA.cmake. That creates its own problems,
cmake/gmxCFlags.cmake:        # e.g. #ifdef GMX_MPI rather than #if GMX_MPI in CUDA source
cmake/gmxCFlags.cmake:        # CUDA compilation.
cmake/gmxCFlags.cmake:    # Only C++ compilation is supported with CUDA code in GROMACS
cmake/gmxCFlags.cmake:        if (GMX_GPU_HIP)
cmake/gmxCFlags.cmake:            # Problematic with CUDA
cmake/gmxCFlags.cmake:        if(GMX_GPU)
cmake/gmxCFlags.cmake:            string(TOUPPER "${GMX_GPU}" _gmx_gpu_uppercase)
cmake/gmxCFlags.cmake:            if(${_gmx_gpu_uppercase} STREQUAL "CUDA")
cmake/gmxCFlags.cmake:    if(GMX_GPU_SYCL)
cmake/gmxCFlags.cmake:        # Intel oneAPI compiler warns about compiling kernels with non-32 subgroups for CUDA devices.
cmake/gmxCFlags.cmake:        gmx_target_warning_suppression(${target} "-Wno-cuda-compat" HAS_WARNING_NO_CUDA_COMPAT)
cmake/AdaptiveCppTest/main.cpp:#    warning GMX_SYCL_TEST_HAVE_CUDA_TARGET
cmake/AdaptiveCppTest/CMakeLists.txt:# - "GMX_ACPP_TEST_HAVE_CUDA_TARGET" for CUDA
cmake/gmxManageClangCudaConfig.cmake:function (gmx_test_clang_cuda_support)
cmake/gmxManageClangCudaConfig.cmake:        message(FATAL_ERROR "Clang is required with GMX_CLANG_CUDA=ON!")
cmake/gmxManageClangCudaConfig.cmake:    # (GPU code) in the object file generated during compilation.
cmake/gmxManageClangCudaConfig.cmake:    # CHECK_CXX_SOURCE_COMPILES("int main() { int c; cudaGetDeviceCount(&c); return 0; }" _CLANG_CUDA_COMPILES)
cmake/gmxManageClangCudaConfig.cmake:if (GMX_CUDA_TARGET_COMPUTE)
cmake/gmxManageClangCudaConfig.cmake:    message(WARNING "Values passed in GMX_CUDA_TARGET_COMPUTE will be ignored; clang will by default include PTX in the binary.")
cmake/gmxManageClangCudaConfig.cmake:# Clang 17 supports CUDA SDK 12.1, which GROMACS requires,
cmake/gmxManageClangCudaConfig.cmake:set(_cuda_version_warning "")
cmake/gmxManageClangCudaConfig.cmake:if ((CMAKE_CXX_COMPILER_VERSION VERSION_GREATER_EQUAL 19.1) AND (CUDAToolkit_VERSION VERSION_LESS_EQUAL 12.5))
cmake/gmxManageClangCudaConfig.cmake:elseif ((CMAKE_CXX_COMPILER_VERSION VERSION_GREATER_EQUAL 18.1) AND (CUDAToolkit_VERSION VERSION_LESS_EQUAL 12.3))
cmake/gmxManageClangCudaConfig.cmake:elseif ((CMAKE_CXX_COMPILER_VERSION VERSION_GREATER_EQUAL 17.0) AND (CUDAToolkit_VERSION VERSION_LESS_EQUAL 12.1))
cmake/gmxManageClangCudaConfig.cmake:    set(_cuda_version_warning "officially incompatible")
cmake/gmxManageClangCudaConfig.cmake:else (CUDAToolkit_VERSION VERSION_GREATER 12.5)
cmake/gmxManageClangCudaConfig.cmake:    # We don't know the future; so far Clang 19 states that CUDA 7.0-12.5 are supported.
cmake/gmxManageClangCudaConfig.cmake:    set(_cuda_version_warning "possibly incompatible")
cmake/gmxManageClangCudaConfig.cmake:if(NOT CUDA_CLANG_WARNING_DISPLAYED STREQUAL _cuda_version_warning)
cmake/gmxManageClangCudaConfig.cmake:    message(NOTICE "Using ${_cuda_version_warning} version of CUDA ${CUDAToolkit_VERSION} "
cmake/gmxManageClangCudaConfig.cmake:    message(NOTICE "If Clang fails to recognize CUDA version, consider creating doing "
cmake/gmxManageClangCudaConfig.cmake:      "`echo \"CUDA Version ${CUDAToolkit_VERSION}\" | sudo tee \"${CUDAToolkit_TARGET_DIR}/version.txt\"`")
cmake/gmxManageClangCudaConfig.cmake:set(CUDA_CLANG_WARNING_DISPLAYED "${_cuda_version_warning}" CACHE INTERNAL
cmake/gmxManageClangCudaConfig.cmake:    "Don't warn about this Clang CUDA compatibility issue again" FORCE)
cmake/gmxManageClangCudaConfig.cmake:if(CUDA_CLANG_WARNING_DISPLAYED)
cmake/gmxManageClangCudaConfig.cmake:    list(APPEND _CUDA_CLANG_FLAGS "-Wno-unknown-cuda-version")
cmake/gmxManageClangCudaConfig.cmake:if (GMX_CUDA_TARGET_SM)
cmake/gmxManageClangCudaConfig.cmake:    set(_CUDA_CLANG_GENCODE_FLAGS)
cmake/gmxManageClangCudaConfig.cmake:    set(_target_sm_list ${GMX_CUDA_TARGET_SM})
cmake/gmxManageClangCudaConfig.cmake:        list(APPEND _CUDA_CLANG_GENCODE_FLAGS "${_target};")
cmake/gmxManageClangCudaConfig.cmake:    list(APPEND _CUDA_CLANG_GENCODE_FLAGS "50;")
cmake/gmxManageClangCudaConfig.cmake:    list(APPEND _CUDA_CLANG_GENCODE_FLAGS "52;")
cmake/gmxManageClangCudaConfig.cmake:    list(APPEND _CUDA_CLANG_GENCODE_FLAGS "60;")
cmake/gmxManageClangCudaConfig.cmake:    list(APPEND _CUDA_CLANG_GENCODE_FLAGS "61;")
cmake/gmxManageClangCudaConfig.cmake:    list(APPEND _CUDA_CLANG_GENCODE_FLAGS "70;")
cmake/gmxManageClangCudaConfig.cmake:    list(APPEND _CUDA_CLANG_GENCODE_FLAGS "75;")
cmake/gmxManageClangCudaConfig.cmake:    list(APPEND _CUDA_CLANG_GENCODE_FLAGS "80;")
cmake/gmxManageClangCudaConfig.cmake:    list(APPEND _CUDA_CLANG_GENCODE_FLAGS "86;")
cmake/gmxManageClangCudaConfig.cmake:        list(APPEND _CUDA_CLANG_GENCODE_FLAGS "87;")
cmake/gmxManageClangCudaConfig.cmake:        list(APPEND _CUDA_CLANG_GENCODE_FLAGS "89;")
cmake/gmxManageClangCudaConfig.cmake:        list(APPEND _CUDA_CLANG_GENCODE_FLAGS "90;")
cmake/gmxManageClangCudaConfig.cmake:if (GMX_CUDA_TARGET_SM)
cmake/gmxManageClangCudaConfig.cmake:    set_property(CACHE GMX_CUDA_TARGET_SM PROPERTY HELPSTRING "List of CUDA GPU architecture codes to compile for (without the sm_ prefix)")
cmake/gmxManageClangCudaConfig.cmake:    set_property(CACHE GMX_CUDA_TARGET_SM PROPERTY TYPE STRING)
cmake/gmxManageClangCudaConfig.cmake:list(APPEND _CUDA_CLANG_FLAGS "-ffast-math" "-fcuda-flush-denormals-to-zero")
cmake/gmxManageClangCudaConfig.cmake:# CUDA toolkit
cmake/gmxManageClangCudaConfig.cmake:list(APPEND _CUDA_CLANG_FLAGS "--cuda-path=${CUDAToolkit_TARGET_DIR}")
cmake/gmxManageClangCudaConfig.cmake:set(GMX_CUDA_CLANG_FLAGS ${_CUDA_CLANG_FLAGS})
cmake/gmxManageClangCudaConfig.cmake:if (CUDA_USE_STATIC_CUDA_RUNTIME)
cmake/gmxManageClangCudaConfig.cmake:    set(GMX_CUDA_CLANG_LINK_LIBS "cudart_static")
cmake/gmxManageClangCudaConfig.cmake:    set(GMX_CUDA_CLANG_LINK_LIBS "cudart")
cmake/gmxManageClangCudaConfig.cmake:set(GMX_CUDA_CLANG_LINK_LIBS "${GMX_CUDA_CLANG_LINK_LIBS}" "dl" "rt")
cmake/gmxManageClangCudaConfig.cmake:set(GMX_CUDA_CLANG_LINK_DIRS "${CUDAToolkit_LIBRARY_DIR}")
cmake/gmxManageClangCudaConfig.cmake:set(CMAKE_CUDA_COMPILER ${CMAKE_CXX_COMPILER})
cmake/gmxManageClangCudaConfig.cmake:gmx_test_clang_cuda_support()
cmake/gmxManageOpenMP.cmake:                "This might hurt your performance a lot, in particular with GPUs. "
cmake/gmxManageVkFft.cmake:# Manage VkFFT, GPU FFT library used with OpenCL and SYCL.
cmake/gmxManageVkFft.cmake:    # The "-Wcast-qual" warning appears when compiling VkFFT for OpenCL, but not for HIP. It cannot be suppressed.
cmake/gmxManageVkFft.cmake:    if (BACKEND_NAME STREQUAL "CUDA")
cmake/gmxManageVkFft.cmake:        # This is not ideal, because it uses some random version of CUDA. See #4621.
cmake/gmxManageVkFft.cmake:        find_package(CUDAToolkit REQUIRED)
cmake/gmxManageVkFft.cmake:        target_link_libraries(VkFFT INTERFACE CUDA::cuda_driver CUDA::nvrtc)
cmake/gmxManageVkFft.cmake:        list(APPEND GMX_PUBLIC_LIBRARIES CUDA::cuda_driver) # Workaround for #4902, #4922
cmake/gmxManageVkFft.cmake:            target_link_libraries(VkFFT INTERFACE CUDA::cudart) # Needed only with DPC++
cmake/gmxManageVkFft.cmake:            # https://github.com/ROCm-Developer-Tools/HIP/issues/3131
cmake/gmxManageVkFft.cmake:            # Once we require ROCm 5.6 or newer, we can simply do
cmake/gmxManageVkFft.cmake:    elseif(BACKEND_NAME STREQUAL "OpenCL")
cmake/gmxManageVkFft.cmake:        # The "-Wcast-qual" warning appears when compiling VkFFT for OpenCL, but not for HIP.
cmake/gmxManageVkFft.cmake:if (GMX_GPU_CUDA)
cmake/gmxManageVkFft.cmake:    gmx_manage_vkfft("CUDA")
cmake/gmxManageVkFft.cmake:elseif(GMX_GPU_HIP)
cmake/gmxManageVkFft.cmake:elseif(GMX_GPU_OPENCL)
cmake/gmxManageVkFft.cmake:    gmx_manage_vkfft("OpenCL")
cmake/gmxManageVkFft.cmake:elseif((GMX_SYCL_ACPP AND GMX_ACPP_HAVE_CUDA_TARGET) OR (GMX_SYCL_DPCPP AND GMX_DPCPP_HAVE_CUDA_TARGET))
cmake/gmxManageVkFft.cmake:    gmx_manage_vkfft("CUDA")
cmake/gmxManageVkFft.cmake:     message(FATAL_ERROR "VkFFT can only be used with CUDA, HIP or OpenCL backend")
cmake/gmxBuildTypeReference.cmake:    set(GMX_GPU OFF CACHE BOOL "Disabled for regressiontests reference builds" FORCE)
cmake/gmxManageSycl.cmake:set(GMX_GPU_SYCL ON)
cmake/gmxManageSycl.cmake:if (GMX_GPU_FFT_CUFFT AND GMX_USE_HEFFTE)
cmake/gmxManageSycl.cmake:if(NOT ${_sycl_has_valid_fft} AND NOT GMX_GPU_FFT_LIBRARY STREQUAL "NONE")
cmake/gmxManageSycl.cmake:    if (GMX_GPU_FFT_CUFFT OR GMX_GPU_FFT_CLFFT)
cmake/gmxManageSycl.cmake:    elseif (GMX_SYCL_ACPP AND GMX_GPU_FFT_MKL)
cmake/gmxManageSycl.cmake:    message(FATAL_ERROR "The selected GPU FFT library ${GMX_GPU_FFT_LIBRARY} is not compatible.${_hint}")
cmake/gmxManageSycl.cmake:    message(WARNING "Building SYCL version without GPU FFT library.  Will not be able to perform FFTs on a GPU, which is not good for performance.")
cmake/gmxManageNvshmem.cmake:    if(NOT GMX_GPU_CUDA)
cmake/gmxManageNvshmem.cmake:        message(FATAL_ERROR "NVSHMEM support requires a CUDA build")
cmake/gmxManageNvshmem.cmake:    if (GMX_CLANG_CUDA)
cmake/gmxManageNvshmem.cmake:        message(FATAL_ERROR "NVSHMEM is not supported with Clang CUDA build")
cmake/gmxManageNvshmem.cmake:    set_target_properties(nvshmem_host_lib PROPERTIES IMPORTED_LINK_INTERFACE_LANGUAGES CUDA)
cmake/gmxManageNvshmem.cmake:    target_link_libraries(nvshmem_host_lib INTERFACE CUDA::nvml CUDA::cuda_driver)
cmake/gmxManageNvshmem.cmake:    set_target_properties(nvshmem_device_lib PROPERTIES IMPORTED_LINK_INTERFACE_LANGUAGES CUDA)
cmake/gmxManageNvshmem.cmake:    # we filter all the archs below SM 70 from GMX_CUDA_NVCC_GENCODE_FLAGS
cmake/gmxManageNvshmem.cmake:    string(REPLACE "35;" ""  GMX_CUDA_NVCC_GENCODE_FLAGS "${GMX_CUDA_NVCC_GENCODE_FLAGS}")
cmake/gmxManageNvshmem.cmake:    string(REPLACE "37;" ""  GMX_CUDA_NVCC_GENCODE_FLAGS "${GMX_CUDA_NVCC_GENCODE_FLAGS}")
cmake/gmxManageNvshmem.cmake:    string(REPLACE "50;" ""  GMX_CUDA_NVCC_GENCODE_FLAGS "${GMX_CUDA_NVCC_GENCODE_FLAGS}")
cmake/gmxManageNvshmem.cmake:    string(REPLACE "52;" ""  GMX_CUDA_NVCC_GENCODE_FLAGS "${GMX_CUDA_NVCC_GENCODE_FLAGS}")
cmake/gmxManageNvshmem.cmake:    string(REPLACE "53;" ""  GMX_CUDA_NVCC_GENCODE_FLAGS "${GMX_CUDA_NVCC_GENCODE_FLAGS}")
cmake/gmxManageNvshmem.cmake:    string(REPLACE "60;" ""  GMX_CUDA_NVCC_GENCODE_FLAGS "${GMX_CUDA_NVCC_GENCODE_FLAGS}")
cmake/gmxManageNvshmem.cmake:    string(REPLACE "61;" ""  GMX_CUDA_NVCC_GENCODE_FLAGS "${GMX_CUDA_NVCC_GENCODE_FLAGS}")
cmake/FindcuFFTMp.cmake:# - Find cuFFTMp, NVIDIA's FFT library for multi-GPU multi-node
cmake/TestCUDA.cu:/* GCC 11.2 and CUDA 11.5 from Ubuntu 22.04 repos have troubles with some
cmake/gmxManageOpenCL.cmake:# OpenCL required version: 1.2 or newer
cmake/gmxManageOpenCL.cmake:set(REQUIRED_OPENCL_MIN_VERSION_MAJOR 1)
cmake/gmxManageOpenCL.cmake:set(REQUIRED_OPENCL_MIN_VERSION_MINOR 2)
cmake/gmxManageOpenCL.cmake:set(REQUIRED_OPENCL_MIN_VERSION ${REQUIRED_OPENCL_MIN_VERSION_MAJOR}.${REQUIRED_OPENCL_MIN_VERSION_MINOR})
cmake/gmxManageOpenCL.cmake:set(GMX_GPU_OPENCL ON)
cmake/gmxManageOpenCL.cmake:    message(FATAL_ERROR "OpenCL acceleration is not available in double precision")
cmake/gmxManageOpenCL.cmake:# for some reason FindOpenCL checks CUDA_PATH but not CUDA_HOME
cmake/gmxManageOpenCL.cmake:set(CMAKE_PREFIX_PATH ${CMAKE_PREFIX_PATH};$ENV{CUDA_HOME})
cmake/gmxManageOpenCL.cmake:find_package(OpenCL)
cmake/gmxManageOpenCL.cmake:if (OpenCL_FOUND)
cmake/gmxManageOpenCL.cmake:    if (OpenCL_VERSION_STRING VERSION_LESS REQUIRED_OPENCL_MIN_VERSION)
cmake/gmxManageOpenCL.cmake:        message(FATAL_ERROR "OpenCL " "${OpenCL_VERSION_STRING}" " is not supported. OpenCL version " "${REQUIRED_OPENCL_MIN_VERSION}" " or newer is required.")
cmake/gmxManageOpenCL.cmake:    message(FATAL_ERROR "OpenCL not found.")
cmake/gmxManageOpenCL.cmake:    set(OpenCL_DEFINITIONS ${OpenCL_DEFINITIONS} " -Wno-comment")
cmake/gmxManageOpenCL.cmake:        message(WARNING "OS X prior to version 10.10.4 produces incorrect AMD OpenCL code at runtime. You will not be able to use AMD GPUs on this host unless you upgrade your operating system.")
cmake/gmxManageOpenCL.cmake:if (GMX_GPU_FFT_VKFFT)
cmake/gmxManageOpenCL.cmake:    # Use VkFFT with OpenCL back end as header-only library
cmake/gmxManageOpenCL.cmake:elseif(NOT GMX_GPU_FFT_CLFFT)
cmake/gmxManageOpenCL.cmake:    message(FATAL_ERROR "In the OpenCL build, only -DGMX_GPU_FFT_LIBRARY=VkFFT and -DGMX_GPU_FFT_LIBRARY=clFFT are supported")
cmake/gmxManageOpenCL.cmake:add_definitions(${OpenCL_DEFINITIONS})
cmake/gmxManageOpenCL.cmake:math(EXPR REQUIRED_OPENCL_MIN_VERSION_DEF "100 * ${REQUIRED_OPENCL_MIN_VERSION_MAJOR} + 10 * ${REQUIRED_OPENCL_MIN_VERSION_MINOR}" OUTPUT_FORMAT DECIMAL)
cmake/gmxManageOpenCL.cmake:add_definitions("-DCL_TARGET_OPENCL_VERSION=${REQUIRED_OPENCL_MIN_VERSION_DEF}")
cmake/gmxManageOpenCL.cmake:include_directories(SYSTEM ${OpenCL_INCLUDE_DIRS})
cmake/gmxManageOpenCL.cmake:# Ensure the OpenCL implementation is 64-bit, because we only support that;
cmake/gmxManageOpenCL.cmake:# (originally in pme-gpu-types.h) are relieved.
cmake/gmxManageOpenCL.cmake:    message(FATAL_ERROR "The OpenCL implementation is only supported on 64-bit platforms.")
cmake/gmxManageOpenCL.cmake:set(GMX_INSTALL_OCLDIR       ${GMX_INSTALL_GMXDATADIR}/opencl)
cmake/gmxManageOpenCL.cmake:# super-cluster size <8 is not supported (does not work with cluster size 4 on Intel, untested on NVIDIA)
cmake/gmxManageOpenCL.cmake:if (GMX_GPU_NB_NUM_CLUSTER_PER_CELL_X AND NOT "${GMX_GPU_NB_NUM_CLUSTER_PER_CELL_X}" EQUAL 2)
cmake/gmxManageOpenCL.cmake:    message(FATAL_ERROR "Changing GMX_GPU_NB_NUM_CLUSTER_PER_CELL_X is not supported with OpenCL")
cmake/gmxManageOpenCL.cmake:if (GMX_GPU_NB_NUM_CLUSTER_PER_CELL_Y AND NOT "${GMX_GPU_NB_NUM_CLUSTER_PER_CELL_Y}" EQUAL 2)
cmake/gmxManageOpenCL.cmake:    message(FATAL_ERROR "Changing GMX_GPU_NB_NUM_CLUSTER_PER_CELL_Y is not supported with OpenCL")
cmake/gmxManageOpenCL.cmake:if (GMX_GPU_NB_NUM_CLUSTER_PER_CELL_Z AND NOT "${GMX_GPU_NB_NUM_CLUSTER_PER_CELL_Z}" EQUAL 2)
cmake/gmxManageOpenCL.cmake:    message(FATAL_ERROR "Changing GMX_GPU_NB_NUM_CLUSTER_PER_CELL_Z is not supported with OpenCL")
cmake/FindclFFT.cmake:# - Find clFFT, AMD's OpenCL FFT library
cmake/gmxManageHipccConfig.cmake:    set(AMDGPU_TARGETS ${_all_accepted_architectures} PARENT_SCOPE)
cmake/gmxManageHipccConfig.cmake:# https://github.com/ROCm/ROCR-Runtime/blob/rocm-5.7.x/src/core/runtime/isa.cpp and https://rocm.docs.amd.com/_/downloads/HIP/en/latest/pdf/
cmake/gmxManageHipccConfig.cmake:        option(GMX_GPU_NB_DISABLE_CLUSTER_PAIR_SPLIT
cmake/gmxManageHipccConfig.cmake:                "Disable NBNXM GPU cluster pair splitting. Only supported with HIP and 64-wide GPU architectures (like AMD GCN/CDNA)."
cmake/gmxManageHipccConfig.cmake:        mark_as_advanced(GMX_GPU_NB_DISABLE_CLUSTER_PAIR_SPLIT)
cmake/gmxManageHipccConfig.cmake:        "Enable compiling kernels for AMD RDNA GPUs (gfx1xxx). When OFF, only CDNA and GCN are supported. Only used with HIP."
cmake/gmxManageHipccConfig.cmake:gmx_hip_check_single_flag("-fno-gpu-rdc")
cmake/gmxManageHipccConfig.cmake:gmx_hip_check_single_flag("-fcuda-flush-denormals-to-zero")
cmake/gmxManageHipccConfig.cmake:# currently ROCm 6.2.x spams warnings about missing occupancy targets during the
cmake/gmxManageHipccConfig.cmake:# compilation of the GPU kernels. We silence this warning here to prevent builds 
cmake/gmxManageHipccConfig.cmake:# We are not using the wrapper that comes with the ROCm toolkit but are
.gitlab-ci.yml:# For GPU builds, we use specific tags (see variables below)
.gitlab-ci.yml:  # Where to run GPU jobs
.gitlab-ci.yml:  GITLAB_RUNNER_TAG_1X_NVIDIA_GPU: "scilifelab-k8s-1x-nvidia-gpu"
.gitlab-ci.yml:  GITLAB_RUNNER_TAG_2X_NVIDIA_GPU: "scilifelab-k8s-2x-nvidia-gpu"
.gitlab-ci.yml:  GITLAB_RUNNER_TAG_1X_AMD_GPU:    "scilifelab-k8s-1x-amd-gpu"
.gitlab-ci.yml:  GITLAB_RUNNER_TAG_2X_AMD_GPU:    "scilifelab-k8s-2x-amd-gpu"
.gitlab-ci.yml:  GITLAB_RUNNER_TAG_1X_INTEL_GPU:  "scilifelab-k8s-1x-intel-gpu"
.gitlab-ci.yml:  GITLAB_RUNNER_TAG_2X_INTEL_GPU:  "scilifelab-k8s-2x-intel-gpu"
.gitlab-ci.yml:  # If you set these to 0 or 1, we will skip GPU tests that cannot run
.gitlab-ci.yml:  GITLAB_RUNNER_MAX_AVAILABLE_NVIDIA_GPUS: 2
.gitlab-ci.yml:  GITLAB_RUNNER_MAX_AVAILABLE_AMD_GPUS:    2
.gitlab-ci.yml:  GITLAB_RUNNER_MAX_AVAILABLE_INTEL_GPUS:  2
src/.clang-tidy:# Especially the GPU code uses this in many places.
src/.clang-tidy:# Those warnings are in the gpu code and we shouldn't to worry about them.
src/programs/mdrun/tests/pmetest.cpp: * It runs the input system with PME for several steps (on CPU and GPU, if available),
src/programs/mdrun/tests/pmetest.cpp:    const bool  commandLineTargetsPmeOnGpu = (pmeOptionArgument == "gpu");
src/programs/mdrun/tests/pmetest.cpp:    if (commandLineTargetsPmeOnGpu)
src/programs/mdrun/tests/pmetest.cpp:                          "it targets GPU execution, but no compatible devices were detected");
src/programs/mdrun/tests/pmetest.cpp:                const bool pmeDecompositionActive = (getenv("GMX_GPU_PME_DECOMPOSITION") != nullptr);
src/programs/mdrun/tests/pmetest.cpp:                GpuAwareMpiStatus gpuAwareMpiStatus = s_hwinfo->minGpuAwareMpiStatus;
src/programs/mdrun/tests/pmetest.cpp:                const bool        gpuAwareMpiActive = gpuAwareMpiStatus == GpuAwareMpiStatus::Forced
src/programs/mdrun/tests/pmetest.cpp:                                               || gpuAwareMpiStatus == GpuAwareMpiStatus::Supported;
src/programs/mdrun/tests/pmetest.cpp:                messages.appendIf(!gpuAwareMpiActive,
src/programs/mdrun/tests/pmetest.cpp:                                  "it targets PME decomposition, which requires GPU-aware MPI, but "
src/programs/mdrun/tests/pmetest.cpp:        const bool                      commandLineTargetsPmeFftOnGpu =
src/programs/mdrun/tests/pmetest.cpp:                !pmeFftOptionArgument.has_value() || pmeFftOptionArgument.value() == "gpu";
src/programs/mdrun/tests/pmetest.cpp:        static constexpr bool sc_gpuBuildSyclWithoutGpuFft =
src/programs/mdrun/tests/pmetest.cpp:                (GMX_GPU_SYCL != 0) && (GMX_GPU_FFT_MKL == 0) && (GMX_GPU_FFT_ROCFFT == 0)
src/programs/mdrun/tests/pmetest.cpp:                && (GMX_GPU_FFT_VKFFT == 0) && (GMX_GPU_FFT_BBFFT == 0)
src/programs/mdrun/tests/pmetest.cpp:                && (GMX_GPU_FFT_ONEMKL == 0); // NOLINT(misc-redundant-expression)
src/programs/mdrun/tests/pmetest.cpp:        messages.appendIf(commandLineTargetsPmeFftOnGpu && sc_gpuBuildSyclWithoutGpuFft,
src/programs/mdrun/tests/pmetest.cpp:                          "it targets GPU execution of FFT work, which is not supported in the "
src/programs/mdrun/tests/pmetest.cpp:        messages.appendIf(!pme_gpu_supports_build(&errorMessage), errorMessage);
src/programs/mdrun/tests/pmetest.cpp:        // A check on whether the .tpr is supported for PME on GPUs is
src/programs/mdrun/tests/pmetest.cpp:        // ranks when not targeting GPUs.
src/programs/mdrun/tests/pmetest.cpp:        { PmeTestFlavor::Basic, "-notunepme -npme 0 -pme gpu -pmefft cpu" },
src/programs/mdrun/tests/pmetest.cpp:        { PmeTestFlavor::Basic, "-notunepme -npme 0 -pme gpu -pmefft gpu" },
src/programs/mdrun/tests/pmetest.cpp:        { PmeTestFlavor::Basic, "-notunepme -npme 0 -pme gpu -pmefft auto" },
src/programs/mdrun/tests/pmetest.cpp:        { PmeTestFlavor::WithWalls, "-notunepme -npme 0 -pme gpu -pmefft cpu" },
src/programs/mdrun/tests/pmetest.cpp:        { PmeTestFlavor::WithWalls, "-notunepme -npme 0 -pme gpu -pmefft gpu" },
src/programs/mdrun/tests/pmetest.cpp:        { PmeTestFlavor::Basic, "-notunepme -npme 1 -pme gpu -pmefft cpu" },
src/programs/mdrun/tests/pmetest.cpp:        { PmeTestFlavor::Basic, "-notunepme -npme 1 -pme gpu -pmefft gpu" },
src/programs/mdrun/tests/pmetest.cpp:        { PmeTestFlavor::Basic, "-notunepme -npme 1 -pme gpu -pmefft auto" },
src/programs/mdrun/tests/pmetest.cpp:        { PmeTestFlavor::WithWalls, "-notunepme -npme 1 -pme gpu -pmefft cpu" },
src/programs/mdrun/tests/pmetest.cpp:        { PmeTestFlavor::WithWalls, "-notunepme -npme 1 -pme gpu -pmefft gpu" },
src/programs/mdrun/tests/pmetest.cpp:        { PmeTestFlavor::Basic, "-tunepme -npme 0 -pme gpu -pmefft cpu" },
src/programs/mdrun/tests/pmetest.cpp:        { PmeTestFlavor::Basic, "-tunepme -npme 0 -pme gpu -pmefft gpu" },
src/programs/mdrun/tests/refdata/MdrunTest_WritesHelp.xml:    [-pinoffset &lt;int&gt;] [-pinstride &lt;int&gt;] [-gpu_id &lt;string&gt;]
src/programs/mdrun/tests/refdata/MdrunTest_WritesHelp.xml:    [-gputasks &lt;string&gt;] [-[no]ddcheck] [-rdd &lt;real&gt;] [-rcon &lt;real&gt;]
src/programs/mdrun/tests/refdata/MdrunTest_WritesHelp.xml: -gpu_id &lt;string&gt;
src/programs/mdrun/tests/refdata/MdrunTest_WritesHelp.xml:           List of unique GPU device IDs available to use
src/programs/mdrun/tests/refdata/MdrunTest_WritesHelp.xml: -gputasks &lt;string&gt;
src/programs/mdrun/tests/refdata/MdrunTest_WritesHelp.xml:           List of GPU device IDs, mapping each task on a node to a device.
src/programs/mdrun/tests/refdata/MdrunTest_WritesHelp.xml:           Calculate non-bonded interactions on: auto, cpu, gpu
src/programs/mdrun/tests/refdata/MdrunTest_WritesHelp.xml:           Optimize PME load between PP/PME ranks or GPU/CPU
src/programs/mdrun/tests/refdata/MdrunTest_WritesHelp.xml:           Perform PME calculations on: auto, cpu, gpu
src/programs/mdrun/tests/refdata/MdrunTest_WritesHelp.xml:           Perform PME FFT calculations on: auto, cpu, gpu
src/programs/mdrun/tests/refdata/MdrunTest_WritesHelp.xml:           Perform bonded calculations on: auto, cpu, gpu
src/programs/mdrun/tests/refdata/MdrunTest_WritesHelp.xml:           Perform update and constraints on: auto, cpu, gpu
src/programs/mdrun/tests/normalmodes.cpp:// The time for OpenCL kernel compilation means these tests might time
src/programs/mdrun/tests/normalmodes.cpp:// OpenCL builds. However, once that compilation is cached for the
src/programs/mdrun/tests/exactcontinuation.cpp: * with GPU and at non pair-search steps.
src/programs/mdrun/tests/exactcontinuation.cpp:        // is explicitly disabled, or if GPU update was requested (see #4711).
src/programs/mdrun/tests/exactcontinuation.cpp:        // Forces and update on GPUs are generally result in different
src/programs/mdrun/tests/exactcontinuation.cpp:// TODO The time for OpenCL kernel compilation means these tests time
src/programs/mdrun/tests/exactcontinuation.cpp:#if !GMX_GPU_OPENCL
src/programs/mdrun/tests/multiple_time_stepping.cpp:    // when comparing between forces CPU and GPU, due to summation order differences
src/programs/mdrun/tests/multiple_time_stepping.cpp:INSTANTIATE_TEST_SUITE_P(MultipleTimeSteppingIsNearSingleTimeSteppingPull,
src/programs/mdrun/tests/freeenergy.cpp:// TODO: The time for OpenCL kernel compilation means these tests time
src/programs/mdrun/tests/freeenergy.cpp:#if !GMX_GPU_OPENCL
src/programs/mdrun/tests/domain_decomposition.cpp:    Gpu,
src/programs/mdrun/tests/domain_decomposition.cpp:    static constexpr gmx::EnumerationArray<OffloadFlavor, const char*> s_names = { "cpu", "gpu" };
src/programs/mdrun/tests/domain_decomposition.cpp:    const bool haveAnyGpuWork = (pmeFlavor == PmeFlavor::Gpu || updateFlavor == UpdateFlavor::Gpu
src/programs/mdrun/tests/domain_decomposition.cpp:                                 || nonbondedFlavor == NonbondedFlavor::Gpu);
src/programs/mdrun/tests/domain_decomposition.cpp:    errorReasons.appendIf(electrostaticsFlavor != ElectrostaticsFlavor::Pme && pmeFlavor == PmeFlavor::Gpu,
src/programs/mdrun/tests/domain_decomposition.cpp:                          "Cannot offload PME to GPU when PME is not used");
src/programs/mdrun/tests/domain_decomposition.cpp:    // GPU-related
src/programs/mdrun/tests/domain_decomposition.cpp:    errorReasons.appendIf(haveAnyGpuWork && (GMX_GPU == 0),
src/programs/mdrun/tests/domain_decomposition.cpp:                          "Cannot use GPU offload in non-GPU build");
src/programs/mdrun/tests/domain_decomposition.cpp:#if GMX_GPU
src/programs/mdrun/tests/domain_decomposition.cpp:    errorReasons.appendIf(haveAnyGpuWork && !haveCompatibleDevices,
src/programs/mdrun/tests/domain_decomposition.cpp:                          "Cannot use GPU offload without a compatible GPU");
src/programs/mdrun/tests/domain_decomposition.cpp:    errorReasons.appendIf((GMX_GPU_OPENCL || GMX_GPU_HIP) && updateFlavor == UpdateFlavor::Gpu,
src/programs/mdrun/tests/domain_decomposition.cpp:                          "GPU Update not supported with OpenCL");
src/programs/mdrun/tests/domain_decomposition.cpp:    errorReasons.appendIf(GMX_GPU_HIP && pmeFlavor == PmeFlavor::Gpu, "HIP PME not implemented yet");
src/programs/mdrun/tests/domain_decomposition.cpp:    errorReasons.appendIf(updateFlavor == UpdateFlavor::Gpu && pmeFlavor == PmeFlavor::Cpu
src/programs/mdrun/tests/domain_decomposition.cpp:                          "Can not use GPU update and CPU PME on a separate rank");
src/programs/mdrun/tests/domain_decomposition.cpp:    errorReasons.appendIf(haveAnyGpuWork && nonbondedFlavor == NonbondedFlavor::Cpu,
src/programs/mdrun/tests/domain_decomposition.cpp:                          "Cannot offload PME or Update to GPU without offloading Nonbondeds");
src/programs/mdrun/tests/domain_decomposition.cpp:    if ((getenv("GMX_GPU_PME_DECOMPOSITION")) == nullptr)
src/programs/mdrun/tests/domain_decomposition.cpp:                pmeFlavor == PmeFlavor::Gpu && separatePmeRankFlavor == SeparatePmeRankFlavor::Two,
src/programs/mdrun/tests/domain_decomposition.cpp:                "Cannot use more than one separate PME rank with GPU PME");
src/programs/mdrun/tests/domain_decomposition.cpp:                                  && pmeFlavor == PmeFlavor::Gpu,
src/programs/mdrun/tests/domain_decomposition.cpp:                          "Cannot use GPU PME offload with multiple PME+PP ranks");
src/programs/mdrun/tests/domain_decomposition.cpp:#if GMX_GPU
src/programs/mdrun/tests/domain_decomposition.cpp:constexpr std::array<OffloadFlavor, 2> sc_offloadFlavors{ OffloadFlavor::Cpu, OffloadFlavor::Gpu };
src/programs/mdrun/tests/domain_decomposition.cpp:// We would have skippen GPU tests anyway, but why even bother instantiating them
src/programs/mdrun/tests/periodicactions_constraints.cpp:// TODO The time for OpenCL kernel compilation means these tests time
src/programs/mdrun/tests/periodicactions_constraints.cpp:#if !GMX_GPU_OPENCL
src/programs/mdrun/tests/CMakeLists.txt:gmx_register_gtest_test(${testname} ${exename} OPENMP_THREADS 2 INTEGRATION_TEST IGNORE_LEAKS QUICK_GPU_TEST)
src/programs/mdrun/tests/CMakeLists.txt:gmx_register_gtest_test(${testname} ${exename} OPENMP_THREADS 2 INTEGRATION_TEST IGNORE_LEAKS QUICK_GPU_TEST)
src/programs/mdrun/tests/CMakeLists.txt:gmx_register_gtest_test(${testname} ${exename} OPENMP_THREADS 2 INTEGRATION_TEST IGNORE_LEAKS SLOW_GPU_TEST)
src/programs/mdrun/tests/CMakeLists.txt:gmx_register_gtest_test(MdrunTestsOneRank ${exename} MPI_RANKS 1 OPENMP_THREADS 2 INTEGRATION_TEST SLOW_TEST IGNORE_LEAKS SLOW_GPU_TEST)
src/programs/mdrun/tests/CMakeLists.txt:gmx_register_gtest_test(MdrunTestsTwoRanks ${exename} MPI_RANKS 2 OPENMP_THREADS 2 INTEGRATION_TEST SLOW_TEST IGNORE_LEAKS SLOW_GPU_TEST)
src/programs/mdrun/tests/CMakeLists.txt:gmx_register_gtest_test(${testname} ${exename} OPENMP_THREADS 2 INTEGRATION_TEST IGNORE_LEAKS SLOW_GPU_TEST)
src/programs/mdrun/tests/CMakeLists.txt:gmx_register_gtest_test(Minimize1RankTests ${exename} MPI_RANKS 1 OPENMP_THREADS 2 INTEGRATION_TEST IGNORE_LEAKS QUICK_GPU_TEST)
src/programs/mdrun/tests/CMakeLists.txt:gmx_register_gtest_test(Minimize2RankTests ${exename} MPI_RANKS 2 OPENMP_THREADS 2 INTEGRATION_TEST IGNORE_LEAKS QUICK_GPU_TEST)
src/programs/mdrun/tests/CMakeLists.txt:gmx_register_gtest_test(${testname} ${exename} OPENMP_THREADS 2 INTEGRATION_TEST IGNORE_LEAKS SLOW_GPU_TEST)
src/programs/mdrun/tests/CMakeLists.txt:gmx_register_gtest_test(${testname} ${exename} INTEGRATION_TEST IGNORE_LEAKS QUICK_GPU_TEST)
src/programs/mdrun/tests/CMakeLists.txt:gmx_register_gtest_test(${testname} ${exename} MPI_RANKS 2 OPENMP_THREADS 2 INTEGRATION_TEST IGNORE_LEAKS QUICK_GPU_TEST)
src/programs/mdrun/tests/CMakeLists.txt:gmx_register_gtest_test(${testname} ${exename} MPI_RANKS 4 INTEGRATION_TEST IGNORE_LEAKS QUICK_GPU_TEST)
src/programs/mdrun/tests/CMakeLists.txt:gmx_register_gtest_test(${testname} ${exename} MPI_RANKS 4 INTEGRATION_TEST IGNORE_LEAKS QUICK_GPU_TEST)
src/programs/mdrun/tests/CMakeLists.txt:gmx_register_gtest_test(${testname} ${exename} MPI_RANKS 4 INTEGRATION_TEST IGNORE_LEAKS SLOW_GPU_TEST)
src/programs/mdrun/tests/CMakeLists.txt:gmx_register_gtest_test(MdrunMpi1RankPmeTests ${exename} MPI_RANKS 1 OPENMP_THREADS 2 INTEGRATION_TEST IGNORE_LEAKS SLOW_GPU_TEST)
src/programs/mdrun/tests/CMakeLists.txt:gmx_register_gtest_test(MdrunMpi2RankPmeTests ${exename} MPI_RANKS 2 OPENMP_THREADS 2 INTEGRATION_TEST IGNORE_LEAKS SLOW_GPU_TEST)
src/programs/mdrun/tests/CMakeLists.txt:    gmx_register_gtest_test(${testname} ${exename} MPI_RANKS 1 SLOW_TEST IGNORE_LEAKS QUICK_GPU_TEST)
src/programs/mdrun/tests/CMakeLists.txt:    gmx_register_gtest_test(${testname} ${exename} MPI_RANKS 2 SLOW_TEST IGNORE_LEAKS QUICK_GPU_TEST)
src/programs/mdrun/tests/CMakeLists.txt:    gmx_register_gtest_test(${testname} ${exename} MPI_RANKS 1 SLOW_TEST IGNORE_LEAKS SLOW_GPU_TEST)
src/programs/mdrun/tests/CMakeLists.txt:    gmx_register_gtest_test(${testname} ${exename} MPI_RANKS 2 SLOW_TEST IGNORE_LEAKS SLOW_GPU_TEST)
src/programs/mdrun/tests/CMakeLists.txt:    gmx_register_gtest_test(${testname} ${exename} MPI_RANKS 1 SLOW_TEST IGNORE_LEAKS SLOW_GPU_TEST)
src/programs/mdrun/tests/CMakeLists.txt:    gmx_register_gtest_test(${testname} ${exename} MPI_RANKS 2 SLOW_TEST IGNORE_LEAKS SLOW_GPU_TEST)
src/programs/mdrun/tests/CMakeLists.txt:gmx_register_gtest_test(${testname} ${exename} OPENMP_THREADS 2 INTEGRATION_TEST IGNORE_LEAKS SLOW_GPU_TEST)
src/programs/mdrun/tests/CMakeLists.txt:gmx_register_gtest_test(${testname} ${exename} OPENMP_THREADS 2 INTEGRATION_TEST IGNORE_LEAKS QUICK_GPU_TEST)
src/programs/mdrun/tests/CMakeLists.txt:gmx_register_gtest_test(${testname} ${exename} OPENMP_THREADS 2 INTEGRATION_TEST IGNORE_LEAKS QUICK_GPU_TEST)
src/programs/mdrun/tests/CMakeLists.txt:gmx_register_gtest_test(${testname} ${exename} OPENMP_THREADS 2 INTEGRATION_TEST IGNORE_LEAKS SLOW_GPU_TEST)
src/programs/mdrun/tests/CMakeLists.txt:gmx_register_gtest_test(${testname} ${exename} MPI_RANKS 2 INTEGRATION_TEST IGNORE_LEAKS QUICK_GPU_TEST)
src/programs/mdrun/tests/simple_mdrun.cpp:// The time for OpenCL kernel compilation means these tests might time
src/programs/mdrun/tests/simple_mdrun.cpp:// OpenCL builds. However, once that compilation is cached for the
src/programs/mdrun/tests/replicaexchange_equivalence.cpp: * GPU support:        disabled
src/programs/mdrun/tests/periodicactions_coupling.cpp:// TODO The time for OpenCL kernel compilation means these tests time
src/programs/mdrun/tests/periodicactions_coupling.cpp:#if !GMX_GPU_OPENCL
src/programs/mdrun/tests/termination.cpp:    if (std::getenv("GMX_CUDA_GRAPH") != nullptr)
src/programs/mdrun/tests/termination.cpp:        GTEST_SKIP() << "Test is unstable with CUDA graphs";
src/programs/mdrun/tests/rerun.cpp:// TODO The time for OpenCL kernel compilation means these tests time
src/programs/mdrun/tests/rerun.cpp:#if !GMX_GPU_OPENCL
src/programs/mdrun/tests/rerun.cpp:// TODO The time for OpenCL kernel compilation means these tests time
src/programs/mdrun/tests/rerun.cpp:#if !GMX_GPU_OPENCL
src/programs/mdrun/tests/checkpoint.cpp:#if !GMX_GPU_OPENCL
src/programs/mdrun/tests/periodicactions_basic.cpp:// TODO The time for OpenCL kernel compilation means these tests time
src/programs/mdrun/tests/periodicactions_basic.cpp:#if !GMX_GPU_OPENCL
src/programs/mdrun/tests/minimize.cpp:// The time for OpenCL kernel compilation means these tests might time
src/programs/mdrun/tests/minimize.cpp:// OpenCL builds. However, once that compilation is cached for the
src/programs/mdrun/tests/expandedensemble.cpp:        // is explicitly disabled, or if GPU update was requested (see
src/programs/mdrun/tests/simulator.cpp:// TODO: The time for OpenCL kernel compilation means these tests time
src/programs/mdrun/tests/simulator.cpp:#if !GMX_GPU_OPENCL && GMX_DOUBLE
src/buildinfo.h.cmakein:/** CUDA compiler version information */
src/buildinfo.h.cmakein:#define CUDA_COMPILER_INFO "@CUDA_COMPILER_INFO@"
src/buildinfo.h.cmakein:/** CUDA compiler flags (device flags, plus host flags if propagated)*/
src/buildinfo.h.cmakein:#define CUDA_DEVICE_COMPILER_FLAGS "@CUDA_DEVICE_COMPILER_FLAGS@"
src/buildinfo.h.cmakein:#define CUDA_HOST_COMPILER_FLAGS @CUDA_HOST_COMPILER_FLAGS@
src/buildinfo.h.cmakein:#define CUDA_COMPILER_FLAGS CUDA_DEVICE_COMPILER_FLAGS CUDA_HOST_COMPILER_FLAGS
src/buildinfo.h.cmakein:/** OpenCL include dir */
src/buildinfo.h.cmakein:#define OPENCL_INCLUDE_DIR "@OpenCL_INCLUDE_DIR@"
src/buildinfo.h.cmakein:/** OpenCL library */
src/buildinfo.h.cmakein:#define OPENCL_LIBRARY "@OpenCL_LIBRARY@"
src/buildinfo.h.cmakein:/** OpenCL version */
src/buildinfo.h.cmakein:#define OPENCL_VERSION_STRING "@OpenCL_VERSION_STRING@"
src/external/vkfft/vkFFT.h:#include <cuda.h>
src/external/vkfft/vkFFT.h:#include <cuda_runtime.h>
src/external/vkfft/vkFFT.h:#include <cuda_runtime_api.h>
src/external/vkfft/vkFFT.h:#ifndef CUDA_TOOLKIT_ROOT_DIR
src/external/vkfft/vkFFT.h:#define CUDA_TOOLKIT_ROOT_DIR ""
src/external/vkfft/vkFFT.h:#ifndef CL_USE_DEPRECATED_OPENCL_1_2_APIS
src/external/vkfft/vkFFT.h:#define CL_USE_DEPRECATED_OPENCL_1_2_APIS
src/external/vkfft/vkFFT.h:#include <OpenCL/opencl.h>
src/external/vkfft/vkFFT/vkFFT_CodeGen/vkFFT_KernelsLevel0/vkFFT_KernelUtils.h:#extension GL_ARB_gpu_shader_fp64 : enable\n\
src/external/vkfft/vkFFT/vkFFT_CodeGen/vkFFT_KernelsLevel0/vkFFT_KernelUtils.h:#extension GL_ARB_gpu_shader_int64 : enable\n\n");
src/external/vkfft/vkFFT/vkFFT_CodeGen/vkFFT_KernelsLevel0/vkFFT_KernelUtils.h:#include <%s/include/cuda_fp16.h>\n", CUDA_TOOLKIT_ROOT_DIR);
src/external/vkfft/vkFFT/vkFFT_CodeGen/vkFFT_KernelsLevel0/vkFFT_KernelUtils.h:#ifdef VKFFT_OLD_ROCM
src/external/vkfft/vkFFT/vkFFT_CodeGen/vkFFT_KernelsLevel0/vkFFT_KernelUtils.h:#pragma OPENCL EXTENSION cl_khr_fp64 : enable\n\n");
src/external/vkfft/vkFFT/vkFFT_CodeGen/vkFFT_KernelsLevel0/vkFFT_KernelUtils.h:#pragma OPENCL EXTENSION cl_khr_fp16 : enable\n\n");
src/external/vkfft/vkFFT/vkFFT_CodeGen/vkFFT_KernelsLevel1/PrePostProcessing/vkFFT_R2R.h:#if(!((VKFFT_BACKEND==3)||(VKFFT_BACKEND==4)||(VKFFT_BACKEND==5)))//OpenCL, Level Zero and Metal are  not handling barrier with thread-conditional writes to local memory - so this is a work-around
src/external/vkfft/vkFFT/vkFFT_CodeGen/vkFFT_KernelsLevel1/PrePostProcessing/vkFFT_R2R.h:#if(((VKFFT_BACKEND==3)||(VKFFT_BACKEND==4)||(VKFFT_BACKEND==5)))//OpenCL, Level Zero and Metal are  not handling barrier with thread-conditional writes to local memory - so this is a work-around
src/external/vkfft/vkFFT/vkFFT_AppManagement/vkFFT_RunApp.h:        cudaError_t res = cudaSuccess;
src/external/vkfft/vkFFT/vkFFT_AppManagement/vkFFT_RunApp.h:            res = cudaEventSynchronize(app->configuration.stream_event[s]);
src/external/vkfft/vkFFT/vkFFT_AppManagement/vkFFT_RunApp.h:            if (res != cudaSuccess) return VKFFT_ERROR_FAILED_TO_SYNCHRONIZE;
src/external/vkfft/vkFFT/vkFFT_AppManagement/vkFFT_InitializeApp.h:		case 0x10DE://NVIDIA
src/external/vkfft/vkFFT/vkFFT_AppManagement/vkFFT_InitializeApp.h:		case 0x10DE://Nvidia
src/external/vkfft/vkFFT/vkFFT_AppManagement/vkFFT_InitializeApp.h:	case 0x10DE://NVIDIA
src/external/vkfft/vkFFT/vkFFT_AppManagement/vkFFT_InitializeApp.h:	CUresult res = CUDA_SUCCESS;
src/external/vkfft/vkFFT/vkFFT_AppManagement/vkFFT_InitializeApp.h:	cudaError_t res_t = cudaSuccess;
src/external/vkfft/vkFFT/vkFFT_AppManagement/vkFFT_InitializeApp.h:	if (res != CUDA_SUCCESS) {
src/external/vkfft/vkFFT/vkFFT_AppManagement/vkFFT_InitializeApp.h:	if (res != CUDA_SUCCESS) {
src/external/vkfft/vkFFT/vkFFT_AppManagement/vkFFT_InitializeApp.h:	if (res != CUDA_SUCCESS) {
src/external/vkfft/vkFFT/vkFFT_AppManagement/vkFFT_InitializeApp.h:	if (res != CUDA_SUCCESS) {
src/external/vkfft/vkFFT/vkFFT_AppManagement/vkFFT_InitializeApp.h:	if (res != CUDA_SUCCESS) {
src/external/vkfft/vkFFT/vkFFT_AppManagement/vkFFT_InitializeApp.h:	if (res != CUDA_SUCCESS) {
src/external/vkfft/vkFFT/vkFFT_AppManagement/vkFFT_InitializeApp.h:	if (res != CUDA_SUCCESS) {
src/external/vkfft/vkFFT/vkFFT_AppManagement/vkFFT_InitializeApp.h:	if (res != CUDA_SUCCESS) {
src/external/vkfft/vkFFT/vkFFT_AppManagement/vkFFT_InitializeApp.h:	if (res != CUDA_SUCCESS) {
src/external/vkfft/vkFFT/vkFFT_AppManagement/vkFFT_InitializeApp.h:	if (res != CUDA_SUCCESS) {
src/external/vkfft/vkFFT/vkFFT_AppManagement/vkFFT_InitializeApp.h:	if (res != CUDA_SUCCESS) {
src/external/vkfft/vkFFT/vkFFT_AppManagement/vkFFT_InitializeApp.h:	if (res != CUDA_SUCCESS) {
src/external/vkfft/vkFFT/vkFFT_AppManagement/vkFFT_InitializeApp.h:	if (res != CUDA_SUCCESS) {
src/external/vkfft/vkFFT/vkFFT_AppManagement/vkFFT_InitializeApp.h:	//we don't need this in CUDA
src/external/vkfft/vkFFT/vkFFT_AppManagement/vkFFT_InitializeApp.h:		app->configuration.stream_event = (cudaEvent_t*)malloc(app->configuration.num_streams * sizeof(cudaEvent_t));
src/external/vkfft/vkFFT/vkFFT_AppManagement/vkFFT_InitializeApp.h:			res_t = cudaEventCreate(&app->configuration.stream_event[i]);
src/external/vkfft/vkFFT/vkFFT_AppManagement/vkFFT_InitializeApp.h:			if (res_t != cudaSuccess) {
src/external/vkfft/vkFFT/vkFFT_AppManagement/vkFFT_InitializeApp.h:	case 0x10DE://NVIDIA
src/external/vkfft/vkFFT/vkFFT_AppManagement/vkFFT_InitializeApp.h:		//The dummy kernel approach (above) does not work for some DCT-IV kernels (like 256x256x256). They refuse to have more than 256 threads. I will just force OpenCL thread limits for now.
src/external/vkfft/vkFFT/vkFFT_AppManagement/vkFFT_InitializeApp.h:	case 0x10DE://NVIDIA
src/external/vkfft/vkFFT/vkFFT_AppManagement/vkFFT_DeleteApp.h:		cudaError_t res_t = cudaSuccess;
src/external/vkfft/vkFFT/vkFFT_AppManagement/vkFFT_DeleteApp.h:				res_t = cudaEventDestroy(app->configuration.stream_event[i]);
src/external/vkfft/vkFFT/vkFFT_AppManagement/vkFFT_DeleteApp.h:				if (res_t == cudaSuccess) app->configuration.stream_event[i] = 0;
src/external/vkfft/vkFFT/vkFFT_AppManagement/vkFFT_DeleteApp.h:			cudaError_t res_t = cudaSuccess;
src/external/vkfft/vkFFT/vkFFT_AppManagement/vkFFT_DeleteApp.h:				res_t = cudaFree(app->configuration.tempBuffer[0]);
src/external/vkfft/vkFFT/vkFFT_AppManagement/vkFFT_DeleteApp.h:				if (res_t == cudaSuccess) app->configuration.tempBuffer[0] = 0;
src/external/vkfft/vkFFT/vkFFT_AppManagement/vkFFT_DeleteApp.h:					cudaError_t res_t = cudaSuccess;
src/external/vkfft/vkFFT/vkFFT_AppManagement/vkFFT_DeleteApp.h:					res_t = cudaFree(app->bufferRaderUintLUT[i][j]);
src/external/vkfft/vkFFT/vkFFT_AppManagement/vkFFT_DeleteApp.h:					if (res_t == cudaSuccess) app->bufferRaderUintLUT[i][j] = 0;
src/external/vkfft/vkFFT/vkFFT_AppManagement/vkFFT_DeleteApp.h:			cudaError_t res_t = cudaSuccess;
src/external/vkfft/vkFFT/vkFFT_AppManagement/vkFFT_DeleteApp.h:				res_t = cudaFree(app->bufferBluestein[i]);
src/external/vkfft/vkFFT/vkFFT_AppManagement/vkFFT_DeleteApp.h:				if (res_t == cudaSuccess) app->bufferBluestein[i] = 0;
src/external/vkfft/vkFFT/vkFFT_AppManagement/vkFFT_DeleteApp.h:				res_t = cudaFree(app->bufferBluesteinFFT[i]);
src/external/vkfft/vkFFT/vkFFT_AppManagement/vkFFT_DeleteApp.h:				if (res_t == cudaSuccess) app->bufferBluesteinFFT[i] = 0;
src/external/vkfft/vkFFT/vkFFT_AppManagement/vkFFT_DeleteApp.h:				res_t = cudaFree(app->bufferBluesteinIFFT[i]);
src/external/vkfft/vkFFT/vkFFT_AppManagement/vkFFT_DeleteApp.h:				if (res_t == cudaSuccess) app->bufferBluesteinIFFT[i] = 0;
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_HostFunctions/vkFFT_ManageLUT.h:	cudaError_t res = cudaSuccess;
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_HostFunctions/vkFFT_ManageLUT.h:							res = cudaMalloc((void**)&axis->bufferLUT, axis->bufferLUTSize);
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_HostFunctions/vkFFT_ManageLUT.h:							if (res != cudaSuccess) {
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_HostFunctions/vkFFT_ManageLUT.h:							res = cudaMalloc((void**)&axis->bufferLUT, axis->bufferLUTSize);
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_HostFunctions/vkFFT_ManageLUT.h:							if (res != cudaSuccess) {
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_HostFunctions/vkFFT_ManageLUT.h:	cudaError_t res = cudaSuccess;
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_HostFunctions/vkFFT_ManageLUT.h:			res = cudaMalloc((void**)&app->bufferRaderUintLUT[axis->specializationConstants.axis_id][axis->specializationConstants.axis_upload_id], app->bufferRaderUintLUTSize[axis->specializationConstants.axis_id][axis->specializationConstants.axis_upload_id]);
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_HostFunctions/vkFFT_ManageLUT.h:			if (res != cudaSuccess) {
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_HostFunctions/vkFFT_ManageLUT.h:	cudaError_t res = cudaSuccess;
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_HostFunctions/vkFFT_ManageLUT.h:				res = cudaMalloc((void**)&axis->bufferLUT, axis->bufferLUTSize);
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_HostFunctions/vkFFT_ManageLUT.h:				if (res != cudaSuccess) {
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_HostFunctions/vkFFT_ManageLUT.h:				res = cudaMalloc((void**)&axis->bufferLUT, axis->bufferLUTSize);
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_HostFunctions/vkFFT_ManageLUT.h:				if (res != cudaSuccess) {
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_HostFunctions/vkFFT_RecursiveFFTGenerators.h:	cudaError_t res = cudaSuccess;
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_HostFunctions/vkFFT_RecursiveFFTGenerators.h:	res = cudaMalloc((void**)&app->bufferBluestein[axis_id], bufferSize);
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_HostFunctions/vkFFT_RecursiveFFTGenerators.h:	if (res != cudaSuccess) return VKFFT_ERROR_FAILED_TO_ALLOCATE;
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_HostFunctions/vkFFT_RecursiveFFTGenerators.h:		res = cudaMalloc((void**)&app->bufferBluesteinFFT[axis_id], bufferSize);
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_HostFunctions/vkFFT_RecursiveFFTGenerators.h:		if (res != cudaSuccess) return VKFFT_ERROR_FAILED_TO_ALLOCATE;
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_HostFunctions/vkFFT_RecursiveFFTGenerators.h:		res = cudaMalloc((void**)&app->bufferBluesteinIFFT[axis_id], bufferSize);
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_HostFunctions/vkFFT_RecursiveFFTGenerators.h:		if (res != cudaSuccess) return VKFFT_ERROR_FAILED_TO_ALLOCATE;
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_HostFunctions/vkFFT_RecursiveFFTGenerators.h:		kernelPreparationConfiguration.queue = app->configuration.queue; //to allocate memory for LUT, we have to pass a queue, vkGPU->fence, commandPool and physicalDevice pointers 
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_HostFunctions/vkFFT_RecursiveFFTGenerators.h:			res = cudaDeviceSynchronize();
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_HostFunctions/vkFFT_RecursiveFFTGenerators.h:			if (res != cudaSuccess) {
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_HostFunctions/vkFFT_RecursiveFFTGenerators.h:			res = cudaDeviceSynchronize();
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_HostFunctions/vkFFT_RecursiveFFTGenerators.h:			if (res != cudaSuccess) {
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_HostFunctions/vkFFT_RecursiveFFTGenerators.h:			res = cudaDeviceSynchronize();
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_HostFunctions/vkFFT_RecursiveFFTGenerators.h:			if (res != cudaSuccess) {
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_HostFunctions/vkFFT_RecursiveFFTGenerators.h:				kernelPreparationConfiguration.queue = app->configuration.queue; //to allocate memory for LUT, we have to pass a queue, vkGPU->fence, commandPool and physicalDevice pointers 
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_HostFunctions/vkFFT_RecursiveFFTGenerators.h:				cudaError_t res = cudaSuccess;
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_HostFunctions/vkFFT_RecursiveFFTGenerators.h:				res = cudaMalloc(&bufferRaderFFT, bufferSize);
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_HostFunctions/vkFFT_RecursiveFFTGenerators.h:				if (res != cudaSuccess) return VKFFT_ERROR_FAILED_TO_ALLOCATE;
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_HostFunctions/vkFFT_RecursiveFFTGenerators.h:				res = cudaDeviceSynchronize();
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_HostFunctions/vkFFT_RecursiveFFTGenerators.h:				if (res != cudaSuccess) {
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_HostFunctions/vkFFT_RecursiveFFTGenerators.h:				cudaFree(bufferRaderFFT);
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_HostFunctions/vkFFT_Scheduler.h:			rader_min_registers = (rader_min_registers / 2 + scale_registers_rader) * 2;//min number of registers for Rader (can be more than min_registers_per_thread, but min_registers_per_thread should be at least 4 for Nvidiaif you have >256 threads)
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_API_handles/vkFFT_ManageMemory.h:	cudaError_t res = cudaSuccess;
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_API_handles/vkFFT_ManageMemory.h:	res = cudaMemcpy(buffer, cpu_arr, transferSize, cudaMemcpyHostToDevice);
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_API_handles/vkFFT_ManageMemory.h:	if (res != cudaSuccess) {
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_API_handles/vkFFT_ManageMemory.h:	cudaError_t res = cudaSuccess;
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_API_handles/vkFFT_ManageMemory.h:	res = cudaMemcpy(cpu_arr, buffer, transferSize, cudaMemcpyDeviceToHost);
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_API_handles/vkFFT_ManageMemory.h:	if (res != cudaSuccess) {
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_API_handles/vkFFT_DeletePlan.h:	CUresult res = CUDA_SUCCESS;
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_API_handles/vkFFT_DeletePlan.h:	cudaError_t res_t = cudaSuccess;
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_API_handles/vkFFT_DeletePlan.h:		res_t = cudaFree(axis->bufferLUT);
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_API_handles/vkFFT_DeletePlan.h:		if (res_t == cudaSuccess) axis->bufferLUT = 0;
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_API_handles/vkFFT_DeletePlan.h:		if (res == CUDA_SUCCESS) axis->VkFFTModule = 0;
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_API_handles/vkFFT_DispatchPlan.h:				CUresult result = CUDA_SUCCESS;
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_API_handles/vkFFT_DispatchPlan.h:						if (result != CUDA_SUCCESS) {
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_API_handles/vkFFT_DispatchPlan.h:				if (result != CUDA_SUCCESS) {
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_API_handles/vkFFT_DispatchPlan.h:						cudaError_t res2 = cudaEventRecord(app->configuration.stream_event[app->configuration.streamID], app->configuration.stream[app->configuration.streamID]);
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_API_handles/vkFFT_DispatchPlan.h:						if (res2 != cudaSuccess) return VKFFT_ERROR_FAILED_TO_EVENT_RECORD;
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_API_handles/vkFFT_CompileKernel.h:	cudaError_t res = cudaSuccess;
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_API_handles/vkFFT_CompileKernel.h:#if (CUDA_VERSION >= 11030)
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_API_handles/vkFFT_CompileKernel.h:		sprintf(opts[0], "--gpu-architecture=sm_%" PRIu64 "%" PRIu64 "", app->configuration.computeCapabilityMajor, app->configuration.computeCapabilityMinor);
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_API_handles/vkFFT_CompileKernel.h:		sprintf(opts[0], "--gpu-architecture=compute_%" PRIu64 "%" PRIu64 "", app->configuration.computeCapabilityMajor, app->configuration.computeCapabilityMinor);
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_API_handles/vkFFT_CompileKernel.h:#if (CUDA_VERSION >= 11030)
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_API_handles/vkFFT_CompileKernel.h:#if (CUDA_VERSION >= 11030)
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_API_handles/vkFFT_CompileKernel.h:#if (CUDA_VERSION >= 11030)
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_API_handles/vkFFT_CompileKernel.h:#if (CUDA_VERSION >= 11030)
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_API_handles/vkFFT_CompileKernel.h:	if (result2 != CUDA_SUCCESS) {
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_API_handles/vkFFT_CompileKernel.h:	if (result2 != CUDA_SUCCESS) {
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_API_handles/vkFFT_CompileKernel.h:		if (result2 != CUDA_SUCCESS) {
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_API_handles/vkFFT_CompileKernel.h:		if (result2 != CUDA_SUCCESS) {
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_Plans/vkFFT_Plan_FFT.h:	cudaError_t res = cudaSuccess;
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_Plans/vkFFT_Plan_FFT.h:		res = cudaMalloc(app->configuration.tempBuffer, app->configuration.tempBufferSize[0]);
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_Plans/vkFFT_Plan_FFT.h:		if (res != cudaSuccess) {
src/external/vkfft/vkFFT/vkFFT_PlanManagement/vkFFT_Plans/vkFFT_Plan_R2C.h:	cudaError_t res = cudaSuccess;
src/external/vkfft/vkFFT/vkFFT_Structs/vkFFT_Structs.h:#include <cuda.h>
src/external/vkfft/vkFFT/vkFFT_Structs/vkFFT_Structs.h:#include <cuda_runtime.h>
src/external/vkfft/vkFFT/vkFFT_Structs/vkFFT_Structs.h:#include <cuda_runtime_api.h>
src/external/vkfft/vkFFT/vkFFT_Structs/vkFFT_Structs.h:#ifndef CL_USE_DEPRECATED_OPENCL_1_2_APIS
src/external/vkfft/vkFFT/vkFFT_Structs/vkFFT_Structs.h:#define CL_USE_DEPRECATED_OPENCL_1_2_APIS
src/external/vkfft/vkFFT/vkFFT_Structs/vkFFT_Structs.h:#include <OpenCL/opencl.h>
src/external/vkfft/vkFFT/vkFFT_Structs/vkFFT_Structs.h:	CUdevice* device;//pointer to CUDA device, obtained from cuDeviceGet
src/external/vkfft/vkFFT/vkFFT_Structs/vkFFT_Structs.h:	//CUcontext* context;//pointer to CUDA context, obtained from cuDeviceGet
src/external/vkfft/vkFFT/vkFFT_Structs/vkFFT_Structs.h:	cudaStream_t* stream;//pointer to streams (can be more than 1), where to execute the kernels
src/external/vkfft/vkFFT/vkFFT_Structs/vkFFT_Structs.h:	uint64_t num_streams;//try to submit CUDA kernels in multiple streams for asynchronous execution. Default 0, set to >=1 if you pass values in the stream pointer.
src/external/vkfft/vkFFT/vkFFT_Structs/vkFFT_Structs.h:	uint64_t coalescedMemory;//in bytes, for Nvidia and AMD is equal to 32, Intel is equal 64, scaled for half precision. Gonna work regardles, but if specified by user correctly, the performance will be higher.
src/external/vkfft/vkFFT/vkFFT_Structs/vkFFT_Structs.h:	uint64_t fixMaxRadixBluestein;//controls the padding of sequences in Bluestein convolution. If specified, padded sequence will be made of up to fixMaxRadixBluestein primes. Default: 2 for CUDA and Vulkan/OpenCL/HIP up to 1048576 combined dimension FFT system, 7 for Vulkan/OpenCL/HIP past after. Min = 2, Max = 13.
src/external/vkfft/vkFFT/vkFFT_Structs/vkFFT_Structs.h:	uint64_t fixMinRaderPrimeFFT;//start FFT convolution version of Rader for radix primes from this number. Better than direct multiplication version for almost all primes (except small ones, like 17-23 on some GPUs). Must be bigger or equal to fixMinRaderPrimeMult. Deafult 29 on AMD and 17 on other GPUs. 
src/external/vkfft/vkFFT/vkFFT_Structs/vkFFT_Structs.h:	uint64_t fixMaxRaderPrimeFFT;//switch to Bluestein's algorithm for radix primes from this number. Switch may happen earlier if prime can't fit in shared memory. Default is 16384, which is bigger than most current GPU's shared memory.
src/external/vkfft/vkFFT/vkFFT_Structs/vkFFT_Structs.h:	uint64_t registerBoost; //specify if register file size is bigger than shared memory and can be used to extend it X times (on Nvidia 256KB register file can be used instead of 32KB of shared memory, set this constant to 4 to emulate 128KB of shared memory). Default 1
src/external/vkfft/vkFFT/vkFFT_Structs/vkFFT_Structs.h:	uint64_t devicePageSize;//in KB, the size of a page on the GPU. Setting to 0 disables local buffer split in pages
src/external/vkfft/vkFFT/vkFFT_Structs/vkFFT_Structs.h:	uint64_t computeCapabilityMajor; // CUDA/HIP compute capability of the device
src/external/vkfft/vkFFT/vkFFT_Structs/vkFFT_Structs.h:	uint64_t computeCapabilityMinor; // CUDA/HIP compute capability of the device
src/external/vkfft/vkFFT/vkFFT_Structs/vkFFT_Structs.h:	uint64_t vendorID; // vendorID 0x10DE - NVIDIA, 0x8086 - Intel, 0x1002 - AMD, etc.
src/external/vkfft/vkFFT/vkFFT_Structs/vkFFT_Structs.h:	cudaEvent_t* stream_event;//Filled at app creation
src/external/vkfft/vkFFT/vkFFT_Structs/vkFFT_Structs.h:	void* saveApplicationString;//memory array(uint32_t* for Vulkan, char* for CUDA/HIP/OpenCL) through which user can access VkFFT generated binaries. (will be allocated by VkFFT, deallocated with deleteVkFFT call)
src/external/thread_mpi/include/thread_mpi/atomic/gcc_intrinsics.h:#if !defined(__INTEL_COMPILER) && !defined(__CUDACC__)
src/external/googletest/googlemock/test/gmock-matchers-comparisons_test.cc:TEST(MatcherInterfaceTest, CanBeImplementedUsingPublishedAPI) {
src/external/clFFT/README.Gromacs:src/statTimer/statisticalTimer.GPU.cpp
src/external/clFFT/ReleaseNotes.txt:The clFFT library is an open source OpenCL library
src/external/clFFT/src/statTimer/statisticalTimer.GPU.h:#ifndef _STATISTICALTIMER_GPU_H_
src/external/clFFT/src/statTimer/statisticalTimer.GPU.h:#define _STATISTICALTIMER_GPU_H_
src/external/clFFT/src/statTimer/statisticalTimer.GPU.h: * \file clfft.StatisticalTimer.GPU.h
src/external/clFFT/src/statTimer/statisticalTimer.GPU.h:class GpuStatTimer : public baseStatTimer
src/external/clFFT/src/statTimer/statisticalTimer.GPU.h:	 * \fn GpuStatTimer()
src/external/clFFT/src/statTimer/statisticalTimer.GPU.h:	GpuStatTimer( );
src/external/clFFT/src/statTimer/statisticalTimer.GPU.h:	 * \fn ~GpuStatTimer()
src/external/clFFT/src/statTimer/statisticalTimer.GPU.h:	~GpuStatTimer( );
src/external/clFFT/src/statTimer/statisticalTimer.GPU.h:	 * \fn GpuStatTimer(const StatisticalTimer& )
src/external/clFFT/src/statTimer/statisticalTimer.GPU.h:	GpuStatTimer( const GpuStatTimer& );
src/external/clFFT/src/statTimer/statisticalTimer.GPU.h:	GpuStatTimer& operator=( const GpuStatTimer& );
src/external/clFFT/src/statTimer/statisticalTimer.GPU.h:	friend std::ostream& operator<<( std::ostream& os, const GpuStatTimer& s );
src/external/clFFT/src/statTimer/statisticalTimer.GPU.h:	void queryOpenCL( size_t id );
src/external/clFFT/src/statTimer/statisticalTimer.GPU.h:	static GpuStatTimer& getInstance( );
src/external/clFFT/src/statTimer/statisticalTimer.GPU.h:#endif // _STATISTICALTIMER_GPU_H_
src/external/clFFT/src/statTimer/statisticalTimer.GPU.cpp:#include "statisticalTimer.GPU.h"
src/external/clFFT/src/statTimer/statisticalTimer.GPU.cpp:GpuStatTimer&
src/external/clFFT/src/statTimer/statisticalTimer.GPU.cpp:GpuStatTimer::getInstance( )
src/external/clFFT/src/statTimer/statisticalTimer.GPU.cpp:	static	GpuStatTimer	timer;
src/external/clFFT/src/statTimer/statisticalTimer.GPU.cpp:GpuStatTimer::GpuStatTimer( ): nEvents( 0 ), nSamples( 0 ), currID( 0 ), currSample( 0 ), currRecord( 0 )
src/external/clFFT/src/statTimer/statisticalTimer.GPU.cpp:GpuStatTimer::~GpuStatTimer( )
src/external/clFFT/src/statTimer/statisticalTimer.GPU.cpp:GpuStatTimer::Clear( )
src/external/clFFT/src/statTimer/statisticalTimer.GPU.cpp:GpuStatTimer::Reserve( size_t nE, size_t nS )
src/external/clFFT/src/statTimer/statisticalTimer.GPU.cpp:GpuStatTimer::Reset( )
src/external/clFFT/src/statTimer/statisticalTimer.GPU.cpp:GpuStatTimer::setNormalize( bool norm )
src/external/clFFT/src/statTimer/statisticalTimer.GPU.cpp:GpuStatTimer::Start( size_t id )
src/external/clFFT/src/statTimer/statisticalTimer.GPU.cpp:GpuStatTimer::Stop( size_t id )
src/external/clFFT/src/statTimer/statisticalTimer.GPU.cpp:GpuStatTimer::AddSample( clfftPlanHandle plHandle, FFTPlan* plan, cl_kernel kern, cl_uint numEvents, cl_event* ev,
src/external/clFFT/src/statTimer/statisticalTimer.GPU.cpp:GpuStatTimer::getUniqueID( const std::string& label, cl_uint groupID )
src/external/clFFT/src/statTimer/statisticalTimer.GPU.cpp:void GpuStatTimer::ReleaseEvents()
src/external/clFFT/src/statTimer/statisticalTimer.GPU.cpp:void GpuStatTimer::queryOpenCL( size_t id )
src/external/clFFT/src/statTimer/statisticalTimer.GPU.cpp:GpuStatTimer::getMean( size_t id )
src/external/clFFT/src/statTimer/statisticalTimer.GPU.cpp:	//	Prep the data; query openCL for the timer information
src/external/clFFT/src/statTimer/statisticalTimer.GPU.cpp:	queryOpenCL( id );
src/external/clFFT/src/statTimer/statisticalTimer.GPU.cpp:GpuStatTimer::getVariance( size_t id )
src/external/clFFT/src/statTimer/statisticalTimer.GPU.cpp:GpuStatTimer::getStdDev( size_t id )
src/external/clFFT/src/statTimer/statisticalTimer.GPU.cpp:GpuStatTimer::getAverageTime( size_t id )
src/external/clFFT/src/statTimer/statisticalTimer.GPU.cpp:GpuStatTimer::getMinimumTime( size_t id )
src/external/clFFT/src/statTimer/statisticalTimer.GPU.cpp:	//	Prep the data; query openCL for the timer information
src/external/clFFT/src/statTimer/statisticalTimer.GPU.cpp:	queryOpenCL( id );
src/external/clFFT/src/statTimer/statisticalTimer.GPU.cpp:GpuStatTimer::pruneOutliers( size_t id , cl_double multiple )
src/external/clFFT/src/statTimer/statisticalTimer.GPU.cpp:GpuStatTimer::pruneOutliers( cl_double multiple )
src/external/clFFT/src/statTimer/statisticalTimer.GPU.cpp:GpuStatTimer::Print( )
src/external/clFFT/src/statTimer/statisticalTimer.GPU.cpp:operator<<( std::ostream& os, const GpuStatTimer& st )
src/external/clFFT/src/statTimer/CMakeLists.txt:								statisticalTimer.GPU.cpp
src/external/clFFT/src/statTimer/CMakeLists.txt:								statisticalTimer.GPU.h
src/external/clFFT/src/statTimer/CMakeLists.txt:# Include standard OpenCL headers
src/external/clFFT/src/statTimer/CMakeLists.txt:include_directories( ${OpenCL_INCLUDE_DIRS} ${PROJECT_BINARY_DIR}/include ../include )
src/external/clFFT/src/statTimer/CMakeLists.txt:target_link_libraries( StatTimer ${OpenCL_LIBRARIES} )
src/external/clFFT/src/statTimer/statisticalTimer.extern.cpp:#include "statisticalTimer.GPU.h"
src/external/clFFT/src/statTimer/statisticalTimer.extern.cpp:	return	&GpuStatTimer::getInstance( );
src/external/clFFT/src/statTimer/statisticalTimer.extern.h:	CLFFT_GPU			= 1,
src/external/clFFT/src/include/clAmdFft.h:/*!  @brief clAmdFft error codes definition, incorporating OpenCL error definitions
src/external/clFFT/src/include/clAmdFft.h: *   This enumeration is a superset of the OpenCL error codes.  For example, CL_OUT_OF_HOST_MEMORY,
src/external/clFFT/src/include/clAmdFft.h: *   which is defined in cl.h is aliased as CLFFT_OUT_OF_HOST_MEMORY.  The set of basic OpenCL
src/external/clFFT/src/include/clAmdFft.h:	 *  @return Enum describing error condition; superset of OpenCL error codes
src/external/clFFT/src/include/clAmdFft.h:	 *  @return Enum describing error condition; superset of OpenCL error codes
src/external/clFFT/src/include/clAmdFft.h:	 *  @return Enum describing error condition; superset of OpenCL error codes
src/external/clFFT/src/include/clAmdFft.h:	 *  @return Enum describing error condition; superset of OpenCL error codes
src/external/clFFT/src/include/clAmdFft.h:	 *  @param[in] context Client is responsible for providing an OpenCL context for the plan
src/external/clFFT/src/include/clAmdFft.h:	 *  @return Enum describing error condition; superset of OpenCL error codes
src/external/clFFT/src/include/clAmdFft.h:	 *  @return Enum describing error condition; superset of OpenCL error codes
src/external/clFFT/src/include/clAmdFft.h:	 *  no more changes to the plan's parameters are expected, and the OpenCL kernels should be compiled.  This optional function
src/external/clFFT/src/include/clAmdFft.h:	 *  @return Enum describing error condition; superset of OpenCL error codes
src/external/clFFT/src/include/clAmdFft.h:	 *  @return Enum describing error condition; superset of OpenCL error codes
src/external/clFFT/src/include/clAmdFft.h:	/*! @brief Retrieve the OpenCL context of a previously created plan.
src/external/clFFT/src/include/clAmdFft.h:	 *  @return Enum describing error condition; superset of OpenCL error codes
src/external/clFFT/src/include/clAmdFft.h:	 *  @return Enum describing error condition; superset of OpenCL error codes
src/external/clFFT/src/include/clAmdFft.h:	 *  @return Enum describing error condition; superset of OpenCL error codes
src/external/clFFT/src/include/clAmdFft.h:	 *  @return Enum describing error condition; superset of OpenCL error codes
src/external/clFFT/src/include/clAmdFft.h:	 *  @return Enum describing error condition; superset of OpenCL error codes
src/external/clFFT/src/include/clAmdFft.h:	 *  @return Enum describing error condition; superset of OpenCL error codes
src/external/clFFT/src/include/clAmdFft.h:	 *  @return Enum describing error condition; superset of OpenCL error codes
src/external/clFFT/src/include/clAmdFft.h:	 *  @return Enum describing error condition; superset of OpenCL error codes
src/external/clFFT/src/include/clAmdFft.h:	 *  @return Enum describing error condition; superset of OpenCL error codes
src/external/clFFT/src/include/clAmdFft.h:	 *  @return Enum describing error condition; superset of OpenCL error codes
src/external/clFFT/src/include/clAmdFft.h:	 *  @return Enum describing error condition; superset of OpenCL error codes
src/external/clFFT/src/include/clAmdFft.h:	 *  only enqueues the OpenCL kernels for execution. The synchronization step has to be managed by the user.
src/external/clFFT/src/include/clAmdFft.h:	 *  @return Enum describing error condition; superset of OpenCL error codes
src/external/clFFT/src/include/clFFT.h:	#include <OpenCL/cl.h>
src/external/clFFT/src/include/clFFT.h:/*!  @brief clfft error codes definition(incorporating OpenCL error definitions)
src/external/clFFT/src/include/clFFT.h: *   This enumeration is a superset of the OpenCL error codes.  For example, CL_OUT_OF_HOST_MEMORY,
src/external/clFFT/src/include/clFFT.h: *   which is defined in cl.h is aliased as CLFFT_OUT_OF_HOST_MEMORY.  The set of basic OpenCL
src/external/clFFT/src/include/clFFT.h:	                       *  <p> debugFlags can be set to CLFFT_DUMP_PROGRAMS, in which case the dynamically generated OpenCL kernels are
src/external/clFFT/src/include/clFFT.h:	 *  @return Enum describes the error condition; superset of OpenCL error codes
src/external/clFFT/src/include/clFFT.h:	 *  @return Enum describing error condition; superset of OpenCL error codes
src/external/clFFT/src/include/clFFT.h:	 *  @return Enum describing error condition; superset of OpenCL error codes
src/external/clFFT/src/include/clFFT.h:	 *  @return Enum describing error condition; superset of OpenCL error codes
src/external/clFFT/src/include/clFFT.h:	 *  @param[in] context Client is responsible for providing an OpenCL context for the plan
src/external/clFFT/src/include/clFFT.h:	 *  @return Enum describing error condition; superset of OpenCL error codes
src/external/clFFT/src/include/clFFT.h:	 *  @return Enum describing error condition; superset of OpenCL error codes
src/external/clFFT/src/include/clFFT.h:	 *  no more change to the parameters of the plan is expected, and the OpenCL kernels can be compiled.  This optional function
src/external/clFFT/src/include/clFFT.h:	 *  allows the client application to perform the OpenCL kernel compilation when the application is initialized instead of during the first
src/external/clFFT/src/include/clFFT.h:	 *  @return Enum describing error condition; superset of OpenCL error codes
src/external/clFFT/src/include/clFFT.h:	 *  @return Enum describing error condition; superset of OpenCL error codes
src/external/clFFT/src/include/clFFT.h:	/*! @brief Retrieve the OpenCL context of a previously created plan.
src/external/clFFT/src/include/clFFT.h:	 *  @return Enum describing error condition; superset of OpenCL error codes
src/external/clFFT/src/include/clFFT.h:	 *  @return Enum describing error condition; superset of OpenCL error codes
src/external/clFFT/src/include/clFFT.h:	 *  @return Enum describing error condition; superset of OpenCL error codes
src/external/clFFT/src/include/clFFT.h:	 *  @return Enum describing error condition; superset of OpenCL error codes
src/external/clFFT/src/include/clFFT.h:	 *  @return Enum describing error condition; superset of OpenCL error codes
src/external/clFFT/src/include/clFFT.h:	 *  @return Enum describing error condition; superset of OpenCL error codes
src/external/clFFT/src/include/clFFT.h:	 *  @return Enum describing error condition; superset of OpenCL error codes
src/external/clFFT/src/include/clFFT.h:	 *  @return Enum describing error condition; superset of OpenCL error codes
src/external/clFFT/src/include/clFFT.h:	 *  @return Enum describing error condition; superset of OpenCL error codes
src/external/clFFT/src/include/clFFT.h:	 *  @return Enum describing error condition; superset of OpenCL error codes
src/external/clFFT/src/include/clFFT.h:	 *  @return Enum describing error condition; superset of OpenCL error codes
src/external/clFFT/src/include/clFFT.h:	 *  only enqueues the OpenCL kernels for execution. The synchronization step must be managed by the user.
src/external/clFFT/src/include/clFFT.h:	 * 	the devices included in the OpenCL context associated with the plan
src/external/clFFT/src/include/clFFT.h:	 *  @return Enum describing error condition; superset of OpenCL error codes
src/external/clFFT/src/CMakeLists.txt:        message(FATAL_ERROR "Cannot use clFFT for OpenCL support unless dlopen is available")
src/external/clFFT/src/library/enqueue.cpp:    OPENCL_V( fftRepo.getclProgram( this->getGenerator(), this->getSignatureData(), prog, this->plan->bakeDevice, this->plan->context ), _T( "fftRepo.getclProgram failed" ) );
src/external/clFFT/src/library/enqueue.cpp:    OPENCL_V( fftRepo.getclKernel( prog, dir, kern, kernelLock), _T( "fftRepo.getclKernels failed" ) );
src/external/clFFT/src/library/enqueue.cpp:        //	::clSetKernelArg() is not thread safe, according to the openCL spec for the same cl_kernel object
src/external/clFFT/src/library/enqueue.cpp:        OPENCL_V( clSetKernelArg( kern, uarg++, sizeof( cl_mem ), (void*)&this->plan->const_buffer ), _T( "clSetKernelArg failed" ) );
src/external/clFFT/src/library/enqueue.cpp:        OPENCL_V( clSetKernelArg( kern, uarg++, sizeof( cl_mem ), (void*)&inputBuff[i] ), _T( "clSetKernelArg failed" ) );
src/external/clFFT/src/library/enqueue.cpp:        OPENCL_V( clSetKernelArg( kern, uarg++, sizeof( cl_mem ), (void*)&outputBuff[o] ), _T( "clSetKernelArg failed" ) );
src/external/clFFT/src/library/enqueue.cpp:		OPENCL_V( clSetKernelArg( kern, uarg++, sizeof( cl_mem ), (void*)&this->plan->precallUserData ), _T( "clSetKernelArg failed" ) );
src/external/clFFT/src/library/enqueue.cpp:			OPENCL_V( clSetKernelArg( kern, uarg++, sizeof( cl_mem ), (void*)&this->plan->postcallUserData ), _T( "clSetKernelArg failed" ) );
src/external/clFFT/src/library/enqueue.cpp:			OPENCL_V( clSetKernelArg( kern, uarg++, localmemSize, NULL ), _T( "clSetKernelArg failed" ) );
src/external/clFFT/src/library/enqueue.cpp:        OPENCL_V( result, _T("Work size too large for clEnqueNDRangeKernel()"));
src/external/clFFT/src/library/enqueue.cpp:        OPENCL_V( result, _T("FFTAction::getWorkSizes failed"));
src/external/clFFT/src/library/enqueue.cpp:    OPENCL_V( call_status, _T( "clEnqueueNDRangeKernel failed" ) );
src/external/clFFT/src/library/enqueue.cpp:    OPENCL_V( fftRepo.getProgramCode( gen, data, kernel, device, context ), _T( "fftRepo.getProgramCode failed." ) );
src/external/clFFT/src/library/enqueue.cpp:				OPENCL_V( writeKernel( plHandle, this->getGenerator(), this->getSignatureData(), fftPlan->context, fftPlan->bakeDevice ), _T( "writeKernel failed." ) );
src/external/clFFT/src/library/enqueue.cpp:            OPENCL_V( fftRepo.getProgramCode( this->getGenerator(), this->getSignatureData(), programCode, q_device, fftPlan->context  ), _T( "fftRepo.getProgramCode failed." ) );
src/external/clFFT/src/library/enqueue.cpp:            OPENCL_V( status, _T( "clCreateProgramWithSource failed." ) );
src/external/clFFT/src/library/enqueue.cpp:                    OPENCL_V( clGetProgramBuildInfo( program, q_device, CL_PROGRAM_BUILD_LOG, 0, NULL, &buildLogSize ),
src/external/clFFT/src/library/enqueue.cpp:                    OPENCL_V( clGetProgramBuildInfo( program, q_device, CL_PROGRAM_BUILD_LOG, buildLogSize, &buildLog[ 0 ], NULL ),
src/external/clFFT/src/library/enqueue.cpp:                OPENCL_V( status, _T( "clBuildProgram failed" ) );
src/external/clFFT/src/library/enqueue.cpp:                OPENCL_V( fftRepo.getProgramEntryPoint( this->getGenerator(), this->getSignatureData(), CLFFT_FORWARD, entryPoint, q_device, fftPlan->context ), _T( "fftRepo.getProgramEntryPoint failed." ) );
src/external/clFFT/src/library/enqueue.cpp:                OPENCL_V( status, _T( "clCreateKernel failed" ) );
src/external/clFFT/src/library/enqueue.cpp:                OPENCL_V( fftRepo.getProgramEntryPoint( this->getGenerator(), this->getSignatureData(), CLFFT_BACKWARD, entryPoint, q_device, fftPlan->context ), _T( "fftRepo.getProgramEntryPoint failed." ) );
src/external/clFFT/src/library/enqueue.cpp:                OPENCL_V( status, _T( "clCreateKernel failed" ) );
src/external/clFFT/src/library/generator.transpose.cpp:								// clKernWrite( transKernel, 0 ) << "#pragma OPENCL EXTENSION cl_amd_printf : enable\n" << std::endl;
src/external/clFFT/src/library/generator.transpose.cpp:		clKernWrite(transKernel, 3) << "#pragma OPENCL EXTENSION cl_khr_fp64 : enable" << std::endl;
src/external/clFFT/src/library/generator.transpose.cpp:		clKernWrite(transKernel, 3) << "#pragma OPENCL EXTENSION cl_amd_fp64 : enable" << std::endl;
src/external/clFFT/src/library/generator.transpose.cpp:								// clKernWrite( transKernel, 0 ) << "#pragma OPENCL EXTENSION cl_amd_printf : enable\n" << std::endl;
src/external/clFFT/src/library/generator.transpose.cpp:		clKernWrite(transKernel, 3) << "#pragma OPENCL EXTENSION cl_khr_fp64 : enable" << std::endl;
src/external/clFFT/src/library/generator.transpose.cpp:		clKernWrite(transKernel, 3) << "#pragma OPENCL EXTENSION cl_amd_fp64 : enable" << std::endl;
src/external/clFFT/src/library/generator.transpose.cpp:								// clKernWrite( transKernel, 0 ) << "#pragma OPENCL EXTENSION cl_amd_printf : enable\n" << std::endl;
src/external/clFFT/src/library/generator.transpose.cpp:		clKernWrite(transKernel, 3) << "#pragma OPENCL EXTENSION cl_khr_fp64 : enable" << std::endl;
src/external/clFFT/src/library/generator.transpose.cpp:		clKernWrite(transKernel, 3) << "#pragma OPENCL EXTENSION cl_amd_fp64 : enable" << std::endl;
src/external/clFFT/src/library/generator.transpose.cpp:								// clKernWrite( transKernel, 0 ) << "#pragma OPENCL EXTENSION cl_amd_printf : enable\n" << std::endl;
src/external/clFFT/src/library/generator.transpose.cpp:		clKernWrite(transKernel, 3) << "#pragma OPENCL EXTENSION cl_khr_fp64 : enable" << std::endl;
src/external/clFFT/src/library/generator.transpose.cpp:		clKernWrite(transKernel, 3) << "#pragma OPENCL EXTENSION cl_amd_fp64 : enable" << std::endl;
src/external/clFFT/src/library/plan.cpp:	OPENCL_V( ::clGetDeviceInfo( device, CL_DEVICE_EXTENSIONS, 0, NULL, &deviceExtSize ),
src/external/clFFT/src/library/plan.cpp:	OPENCL_V( ::clGetDeviceInfo( device, CL_DEVICE_EXTENSIONS, deviceExtSize, &szDeviceExt[ 0 ], NULL ),
src/external/clFFT/src/library/plan.cpp:	OPENCL_V( fftRepo.createPlan( plHandle, fftPlan ), _T( "fftRepo.insertPlan failed" ) );
src/external/clFFT/src/library/plan.cpp:	OPENCL_V(fftPlan->SetEnvelope(), _T("SetEnvelope failed"));
src/external/clFFT/src/library/plan.cpp:	// Detect OpenCL devices
src/external/clFFT/src/library/plan.cpp:	OPENCL_V( ::clGetContextInfo( context, CL_CONTEXT_DEVICES, 0, NULL, &deviceListSize ),
src/external/clFFT/src/library/plan.cpp:	OPENCL_V( ::clGetContextInfo( context, CL_CONTEXT_DEVICES, deviceListSize, &fftPlan->devices[ 0 ], NULL ),
src/external/clFFT/src/library/plan.cpp:	OPENCL_V( fftRepo.getPlan( *plHandle, fftPlan, planLock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/plan.cpp:		OPENCL_V( fftRepo.getPlan( *plHandle, fftPlan, planLock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/plan.cpp:			OPENCL_V( err, "FFTGeneratedStockhamAction() failed");
src/external/clFFT/src/library/plan.cpp:			OPENCL_V( err, "FFTGeneratedTransposeGCNAction() failed");
src/external/clFFT/src/library/plan.cpp:			OPENCL_V( err, "FFTGeneratedCopyAction() failed");
src/external/clFFT/src/library/plan.cpp:			OPENCL_V( CLFFT_NOTIMPLEMENTED, "selectAction() failed");
src/external/clFFT/src/library/plan.cpp:	//	We do not currently support multi-GPU transforms
src/external/clFFT/src/library/plan.cpp:	OPENCL_V( fftRepo.getPlan( plHandle, fftPlan, planLock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/plan.cpp:	if( NULL != fftPlan->intBuffer ) { OPENCL_V( clReleaseMemObject( fftPlan->intBuffer ), _T( "Failed to release internal temporary buffer" ) ); fftPlan->intBuffer = NULL; }
src/external/clFFT/src/library/plan.cpp:	if( NULL != fftPlan->intBufferRC ) { OPENCL_V( clReleaseMemObject( fftPlan->intBufferRC ), _T( "Failed to release internal temporary buffer" ) ); fftPlan->intBufferRC = NULL; }
src/external/clFFT/src/library/plan.cpp:	if( NULL != fftPlan->intBufferC2R ) { OPENCL_V( clReleaseMemObject( fftPlan->intBufferC2R ), _T( "Failed to release internal temporary buffer" ) ); fftPlan->intBufferC2R = NULL; }
src/external/clFFT/src/library/plan.cpp:        OPENCL_V( err, _T( "FFTGeneratedCopyAction() failed" ) );
src/external/clFFT/src/library/plan.cpp:	//	depends on the GPU caps -- especially the amount of LDS
src/external/clFFT/src/library/plan.cpp:	OPENCL_V(fftPlan->GetMax1DLength (&Large1DThreshold), _T("GetMax1DLength failed"));
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftCreateDefaultPlanInternal( &fftPlan->planTX, fftPlan->context, CLFFT_2D, clLengths ),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V( fftRepo.getPlan( fftPlan->planTX, trans1Plan, trans1Lock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftBakePlan(fftPlan->planTX, numQueues, commQueueFFT, NULL, NULL ),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftCreateDefaultPlanInternal( &fftPlan->planX, fftPlan->context, CLFFT_1D, &clLengths[1] ),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V( fftRepo.getPlan( fftPlan->planX, row1Plan, row1Lock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftBakePlan(fftPlan->planX, numQueues, commQueueFFT, NULL, NULL ),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftCreateDefaultPlanInternal( &fftPlan->planTY, fftPlan->context, CLFFT_2D, &clLengths[1] ),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V( fftRepo.getPlan( fftPlan->planTY, trans2Plan, trans2Lock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftBakePlan(fftPlan->planTY, numQueues, commQueueFFT, NULL, NULL ),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftCreateDefaultPlanInternal( &fftPlan->planY, fftPlan->context, CLFFT_1D, &clLengths[0] ),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V( fftRepo.getPlan( fftPlan->planY, row2Plan, row2Lock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftBakePlan(fftPlan->planY, numQueues, commQueueFFT, NULL, NULL ),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftCreateDefaultPlanInternal( &fftPlan->planTZ, fftPlan->context, CLFFT_2D, clLengths ),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V( fftRepo.getPlan( fftPlan->planTZ, trans3Plan, trans3Lock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftBakePlan(fftPlan->planTZ, numQueues, commQueueFFT, NULL, NULL ),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftCreateDefaultPlanInternal( &fftPlan->planTX, fftPlan->context, CLFFT_2D, clLengths ),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V( fftRepo.getPlan( fftPlan->planTX, trans1Plan, trans1Lock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftBakePlan(fftPlan->planTX, numQueues, commQueueFFT, NULL, NULL ),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftCreateDefaultPlanInternal( &fftPlan->planX, fftPlan->context, CLFFT_1D, &clLengths[1] ),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V( fftRepo.getPlan( fftPlan->planX, row1Plan, row1Lock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftBakePlan(fftPlan->planX, numQueues, commQueueFFT, NULL, NULL ),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftCreateDefaultPlanInternal( &fftPlan->planTY, fftPlan->context, CLFFT_2D, &clLengths[1] ),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V( fftRepo.getPlan( fftPlan->planTY, trans2Plan, trans2Lock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftSetPlanLength( fftPlan->planTY, CLFFT_2D, transLengths ),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftBakePlan(fftPlan->planTY, numQueues, commQueueFFT, NULL, NULL ),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftCreateDefaultPlanInternal( &fftPlan->planY, fftPlan->context, CLFFT_1D, &clLengths[0] ),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V( fftRepo.getPlan( fftPlan->planY, row2Plan, row2Lock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftBakePlan(fftPlan->planY, numQueues, commQueueFFT, NULL, NULL ),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftCreateDefaultPlanInternal( &fftPlan->planTZ, fftPlan->context, CLFFT_2D, clLengths ),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V( fftRepo.getPlan( fftPlan->planTZ, trans3Plan, trans3Lock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftSetPlanLength( fftPlan->planTZ, CLFFT_2D, transLengths ),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftBakePlan(fftPlan->planTZ, numQueues, commQueueFFT, NULL, NULL ),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftCreateDefaultPlanInternal(&fftPlan->planX, fftPlan->context, CLFFT_1D, &clLengths[1]),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(fftRepo.getPlan(fftPlan->planX, colTPlan, colLock), _T("fftRepo.getPlan failed"));
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftBakePlan(fftPlan->planX, numQueues, commQueueFFT, NULL, NULL), _T("BakePlan large1d first column plan failed"));
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftCreateDefaultPlanInternal(&fftPlan->planY, fftPlan->context, CLFFT_1D, &clLengths[0]),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(fftRepo.getPlan(fftPlan->planY, col2Plan, rowLock), _T("fftRepo.getPlan failed"));
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftBakePlan(fftPlan->planY, numQueues, commQueueFFT, NULL, NULL ), _T( "BakePlan large1d second column plan failed" ) );
src/external/clFFT/src/library/plan.cpp:						OPENCL_V(clfftCreateDefaultPlanInternal(&fftPlan->planRCcopy, fftPlan->context, CLFFT_1D, &fftPlan->length[0]),
src/external/clFFT/src/library/plan.cpp:						OPENCL_V(fftRepo.getPlan(fftPlan->planRCcopy, copyPlan, copyLock), _T("fftRepo.getPlan failed"));
src/external/clFFT/src/library/plan.cpp:						OPENCL_V(clfftBakePlan(fftPlan->planRCcopy, numQueues, commQueueFFT, NULL, NULL), _T("BakePlan large1d RC copy plan failed"));
src/external/clFFT/src/library/plan.cpp:						OPENCL_V(clfftCreateDefaultPlanInternal(&fftPlan->planRCcopy, fftPlan->context, CLFFT_1D, &fftPlan->length[0]),
src/external/clFFT/src/library/plan.cpp:						OPENCL_V(fftRepo.getPlan(fftPlan->planRCcopy, copyPlan, copyLock), _T("fftRepo.getPlan failed"));
src/external/clFFT/src/library/plan.cpp:						OPENCL_V(clfftBakePlan(fftPlan->planRCcopy, numQueues, commQueueFFT, NULL, NULL), _T("BakePlan large1d RC copy plan failed"));
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftCreateDefaultPlanInternal( &fftPlan->planX, fftPlan->context, CLFFT_1D, &clLengths[1] ),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V( fftRepo.getPlan( fftPlan->planX, colTPlan, colLock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftBakePlan(fftPlan->planX, numQueues, commQueueFFT, NULL, NULL ), _T( "BakePlan large1d first column plan failed" ) );
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftCreateDefaultPlanInternal( &fftPlan->planY, fftPlan->context, CLFFT_1D,  &clLengths[0] ),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V( fftRepo.getPlan( fftPlan->planY, col2Plan, rowLock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftBakePlan(fftPlan->planY, numQueues, commQueueFFT, NULL, NULL ), _T( "BakePlan large1d second column plan failed" ) );
src/external/clFFT/src/library/plan.cpp:						OPENCL_V(clfftCreateDefaultPlanInternal( &fftPlan->planTX, fftPlan->context, CLFFT_2D, len ),
src/external/clFFT/src/library/plan.cpp:						OPENCL_V( fftRepo.getPlan( fftPlan->planTX, trans1Plan, trans1Lock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/plan.cpp:						OPENCL_V(clfftBakePlan(fftPlan->planTX, numQueues, commQueueFFT, NULL, NULL ),
src/external/clFFT/src/library/plan.cpp:						OPENCL_V(clfftCreateDefaultPlanInternal( &fftPlan->planX, fftPlan->context, CLFFT_1D, &clLengths[0] ),
src/external/clFFT/src/library/plan.cpp:						OPENCL_V( fftRepo.getPlan( fftPlan->planX, rowPlan, rowLock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/plan.cpp:						OPENCL_V(clfftBakePlan(fftPlan->planX, numQueues, commQueueFFT, NULL, NULL ), _T( "BakePlan large1d first row plan failed" ) );
src/external/clFFT/src/library/plan.cpp:						OPENCL_V(clfftCreateDefaultPlanInternal( &fftPlan->planY, fftPlan->context, CLFFT_1D,  &clLengths[1] ),
src/external/clFFT/src/library/plan.cpp:						OPENCL_V( fftRepo.getPlan( fftPlan->planY, col2Plan, colLock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/plan.cpp:						OPENCL_V(clfftBakePlan(fftPlan->planY, numQueues, commQueueFFT, NULL, NULL ), _T( "BakePlan large1d second column plan failed" ) );
src/external/clFFT/src/library/plan.cpp:						OPENCL_V(clfftCreateDefaultPlanInternal( &fftPlan->planCopy, fftPlan->context, CLFFT_1D,  &clLengths[0] ),
src/external/clFFT/src/library/plan.cpp:						OPENCL_V( fftRepo.getPlan( fftPlan->planCopy, copyPlan, copyLock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/plan.cpp:						OPENCL_V(clfftBakePlan(fftPlan->planCopy, numQueues, commQueueFFT, NULL, NULL ), _T( "BakePlan large1d copy plan failed" ) );
src/external/clFFT/src/library/plan.cpp:						OPENCL_V(clfftCreateDefaultPlanInternal( &fftPlan->planX, fftPlan->context, CLFFT_1D, &clLengths[1] ),
src/external/clFFT/src/library/plan.cpp:						OPENCL_V( fftRepo.getPlan( fftPlan->planX, colTPlan, colLock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/plan.cpp:						OPENCL_V(clfftBakePlan(fftPlan->planX, numQueues, commQueueFFT, NULL, NULL ), _T( "BakePlan large1d first column plan failed" ) );
src/external/clFFT/src/library/plan.cpp:						OPENCL_V(clfftCreateDefaultPlanInternal( &fftPlan->planY, fftPlan->context, CLFFT_1D,  &clLengths[0] ),
src/external/clFFT/src/library/plan.cpp:						OPENCL_V( fftRepo.getPlan( fftPlan->planY, col2Plan, rowLock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/plan.cpp:						OPENCL_V(clfftBakePlan(fftPlan->planY, numQueues, commQueueFFT, NULL, NULL ), _T( "BakePlan large1d second column plan failed" ) );
src/external/clFFT/src/library/plan.cpp:							OPENCL_V(clfftCreateDefaultPlanInternal( &fftPlan->planTZ, fftPlan->context, CLFFT_2D, clLengths ),
src/external/clFFT/src/library/plan.cpp:							OPENCL_V( fftRepo.getPlan( fftPlan->planTZ, trans3Plan, trans3Lock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/plan.cpp:							OPENCL_V(clfftBakePlan(fftPlan->planTZ, numQueues, commQueueFFT, NULL, NULL ),
src/external/clFFT/src/library/plan.cpp:						OPENCL_V(clfftCreateDefaultPlanInternal(&fftPlan->planTX, fftPlan->context, CLFFT_2D, clLengths),
src/external/clFFT/src/library/plan.cpp:						OPENCL_V(fftRepo.getPlan(fftPlan->planTX, trans1Plan, trans1Lock), _T("fftRepo.getPlan failed"));
src/external/clFFT/src/library/plan.cpp:						OPENCL_V(clfftBakePlan(fftPlan->planTX, numQueues, commQueueFFT, NULL, NULL),
src/external/clFFT/src/library/plan.cpp:						OPENCL_V(clfftCreateDefaultPlanInternal(&fftPlan->planTY, fftPlan->context, CLFFT_2D, clLengths),
src/external/clFFT/src/library/plan.cpp:						OPENCL_V(fftRepo.getPlan(fftPlan->planTY, trans2Plan, trans2Lock), _T("fftRepo.getPlan failed"));
src/external/clFFT/src/library/plan.cpp:						OPENCL_V(clfftBakePlan(fftPlan->planTY, numQueues, commQueueFFT, NULL, NULL),
src/external/clFFT/src/library/plan.cpp:                OPENCL_V( err, "FFTGeneratedTransposeXXXAction failed");
src/external/clFFT/src/library/plan.cpp:                bool isnvidia = false;
src/external/clFFT/src/library/plan.cpp:                for (size_t Idx = 0; !isnvidia && Idx < numQueues; Idx++)
src/external/clFFT/src/library/plan.cpp:                    isnvidia |= (strncmp(Vendor, "NVIDIA", 6) == 0);
src/external/clFFT/src/library/plan.cpp:                // nvidia gpus are failing when doing transpose for 2D FFTs
src/external/clFFT/src/library/plan.cpp:                if (isnvidia) break;
src/external/clFFT/src/library/plan.cpp:				OPENCL_V(clfftCreateDefaultPlanInternal( &fftPlan->planX, fftPlan->context, CLFFT_1D, &fftPlan->length[ DimX ] ),
src/external/clFFT/src/library/plan.cpp:				OPENCL_V( fftRepo.getPlan( fftPlan->planX, rowPlan, rowLock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/plan.cpp:				OPENCL_V(clfftBakePlan(fftPlan->planX, numQueues, commQueueFFT, NULL, NULL ),
src/external/clFFT/src/library/plan.cpp:				OPENCL_V(clfftCreateDefaultPlanInternal( &fftPlan->planTX, fftPlan->context, CLFFT_2D, clLengths ),
src/external/clFFT/src/library/plan.cpp:				OPENCL_V( fftRepo.getPlan( fftPlan->planTX, transPlanX, transLockX ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/plan.cpp:				OPENCL_V(clfftBakePlan(fftPlan->planTX, numQueues, commQueueFFT, NULL, NULL ),
src/external/clFFT/src/library/plan.cpp:				OPENCL_V(clfftCreateDefaultPlanInternal( &fftPlan->planY, fftPlan->context, CLFFT_1D, &fftPlan->length[ DimY ] ),
src/external/clFFT/src/library/plan.cpp:				OPENCL_V( fftRepo.getPlan( fftPlan->planY, colPlan, colLock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/plan.cpp:				OPENCL_V(clfftBakePlan(fftPlan->planY, numQueues, commQueueFFT, NULL, NULL ),
src/external/clFFT/src/library/plan.cpp:				OPENCL_V(clfftCreateDefaultPlanInternal( &fftPlan->planTY, fftPlan->context, CLFFT_2D, clLengthsY ),
src/external/clFFT/src/library/plan.cpp:				OPENCL_V( fftRepo.getPlan( fftPlan->planTY, transPlanY, transLockY ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/plan.cpp:				OPENCL_V(clfftBakePlan(fftPlan->planTY, numQueues, commQueueFFT, NULL, NULL ),
src/external/clFFT/src/library/plan.cpp:				OPENCL_V(clfftCreateDefaultPlanInternal( &fftPlan->planX, fftPlan->context, CLFFT_1D, &fftPlan->length[ DimX ] ),
src/external/clFFT/src/library/plan.cpp:				OPENCL_V( fftRepo.getPlan( fftPlan->planX, rowPlan, rowLock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/plan.cpp:				OPENCL_V(clfftBakePlan(fftPlan->planX, numQueues, commQueueFFT, NULL, NULL ), _T( "BakePlan for planX failed" ) );
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftCreateDefaultPlanInternal( &fftPlan->planTX, fftPlan->context, CLFFT_2D, transLengths ),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V( fftRepo.getPlan( fftPlan->planTX, trans1Plan, trans1Lock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftSetPlanLength( fftPlan->planTX, CLFFT_2D, transLengths ),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftBakePlan(fftPlan->planTX, numQueues, commQueueFFT, NULL, NULL ),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftCreateDefaultPlanInternal( &fftPlan->planY, fftPlan->context, CLFFT_1D, &fftPlan->length[ DimY ] ),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V( fftRepo.getPlan( fftPlan->planY, colPlan, colLock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftBakePlan(fftPlan->planY, numQueues, commQueueFFT, NULL, NULL ),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftCreateDefaultPlanInternal( &fftPlan->planTY, fftPlan->context, CLFFT_2D, trans2Lengths ),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V( fftRepo.getPlan( fftPlan->planTY, trans2Plan, trans2Lock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftSetPlanLength( fftPlan->planTY, CLFFT_2D, trans2Lengths ),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftBakePlan(fftPlan->planTY, numQueues, commQueueFFT, NULL, NULL ),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftCreateDefaultPlanInternal( &fftPlan->planY, fftPlan->context, CLFFT_1D, &fftPlan->length[ DimY ] ),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V( fftRepo.getPlan( fftPlan->planY, colPlan, colLock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftBakePlan(fftPlan->planY, numQueues, commQueueFFT, NULL, NULL ), _T( "BakePlan for planY failed" ) );
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftCreateDefaultPlanInternal( &fftPlan->planTY, fftPlan->context, CLFFT_2D, transLengths ),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V( fftRepo.getPlan( fftPlan->planTY, trans1Plan, trans1Lock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftSetPlanLength( fftPlan->planTY, CLFFT_2D, transLengths ),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftBakePlan(fftPlan->planTY, numQueues, commQueueFFT, NULL, NULL ),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftCreateDefaultPlanInternal( &fftPlan->planY, fftPlan->context, CLFFT_1D, &fftPlan->length[ DimY ] ),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V( fftRepo.getPlan( fftPlan->planY, colPlan, colLock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftBakePlan(fftPlan->planY, numQueues, commQueueFFT, NULL, NULL ), _T( "BakePlan for planY failed" ) );
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftCreateDefaultPlanInternal( &fftPlan->planTX, fftPlan->context, CLFFT_2D, trans2Lengths ),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V( fftRepo.getPlan( fftPlan->planTX, trans2Plan, trans2Lock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftSetPlanLength( fftPlan->planTX, CLFFT_2D, trans2Lengths ),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftBakePlan(fftPlan->planTX, numQueues, commQueueFFT, NULL, NULL ),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftCreateDefaultPlanInternal( &fftPlan->planX, fftPlan->context, CLFFT_1D, &fftPlan->length[ DimX ] ),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V( fftRepo.getPlan( fftPlan->planX, rowPlan, rowLock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftBakePlan(fftPlan->planX, numQueues, commQueueFFT, NULL, NULL ), _T( "BakePlan for planX failed" ) );
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftCreateDefaultPlanInternal( &fftPlan->planY, fftPlan->context, CLFFT_1D, &fftPlan->length[ DimY ] ),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V( fftRepo.getPlan( fftPlan->planY, colPlan, colLock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftBakePlan(fftPlan->planY, numQueues, commQueueFFT, NULL, NULL ), _T( "BakePlan for planY failed" ) );
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftCreateDefaultPlanInternal( &fftPlan->planX, fftPlan->context, CLFFT_1D, &fftPlan->length[ DimX ] ),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V( fftRepo.getPlan( fftPlan->planX, rowPlan, rowLock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftBakePlan(fftPlan->planX, numQueues, commQueueFFT, NULL, NULL ), _T( "BakePlan for planX failed" ) );
src/external/clFFT/src/library/plan.cpp:				OPENCL_V(clfftCreateDefaultPlanInternal( &fftPlan->planX, fftPlan->context, CLFFT_1D, &fftPlan->length[ DimX ] ),
src/external/clFFT/src/library/plan.cpp:				OPENCL_V( fftRepo.getPlan( fftPlan->planX, rowPlan, rowLock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/plan.cpp:				OPENCL_V(clfftBakePlan(fftPlan->planX, numQueues, commQueueFFT, NULL, NULL ), _T( "BakePlan for planX failed" ) );
src/external/clFFT/src/library/plan.cpp:				OPENCL_V(clfftCreateDefaultPlanInternal( &fftPlan->planY, fftPlan->context, CLFFT_1D, &fftPlan->length[ DimY ] ),
src/external/clFFT/src/library/plan.cpp:				OPENCL_V( fftRepo.getPlan( fftPlan->planY, colPlan, colLock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/plan.cpp:				OPENCL_V(clfftBakePlan(fftPlan->planY, numQueues, commQueueFFT, NULL, NULL ), _T( "BakePlan for planY failed" ) );
src/external/clFFT/src/library/plan.cpp:				OPENCL_V(clfftCreateDefaultPlanInternal( &fftPlan->planX, fftPlan->context, CLFFT_2D, clLengths ),
src/external/clFFT/src/library/plan.cpp:				OPENCL_V( fftRepo.getPlan( fftPlan->planX, xyPlan, rowLock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/plan.cpp:				OPENCL_V(clfftBakePlan(fftPlan->planX, numQueues, commQueueFFT, NULL, NULL ), _T( "BakePlan 3D->2D planX failed" ) );
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftCreateDefaultPlanInternal( &fftPlan->planTX, fftPlan->context, CLFFT_2D, transLengths ),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V( fftRepo.getPlan( fftPlan->planTX, trans1Plan, trans1Lock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftSetPlanLength( fftPlan->planTX, CLFFT_2D, transLengths ),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftBakePlan(fftPlan->planTX, numQueues, commQueueFFT, NULL, NULL ),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftCreateDefaultPlanInternal( &fftPlan->planZ, fftPlan->context, CLFFT_1D, &fftPlan->length[ DimZ ] ),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V( fftRepo.getPlan( fftPlan->planZ, colPlan, colLock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftBakePlan(fftPlan->planZ, numQueues, commQueueFFT, NULL, NULL ),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftCreateDefaultPlanInternal( &fftPlan->planTY, fftPlan->context, CLFFT_2D, trans2Lengths ),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V( fftRepo.getPlan( fftPlan->planTY, trans2Plan, trans2Lock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftSetPlanLength( fftPlan->planTY, CLFFT_2D, trans2Lengths ),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftBakePlan(fftPlan->planTY, numQueues, commQueueFFT, NULL, NULL ),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftCreateDefaultPlanInternal( &fftPlan->planZ, fftPlan->context, CLFFT_1D, clLengths ),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V( fftRepo.getPlan( fftPlan->planZ, colPlan, colLock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftBakePlan(fftPlan->planZ, numQueues, commQueueFFT, NULL, NULL ), _T( "BakePlan 3D->1D planZ failed" ) );
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftCreateDefaultPlanInternal( &fftPlan->planTZ, fftPlan->context, CLFFT_2D, transLengths ),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V( fftRepo.getPlan( fftPlan->planTZ, trans1Plan, trans1Lock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftSetPlanLength( fftPlan->planTZ, CLFFT_2D, transLengths ),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftBakePlan(fftPlan->planTZ, numQueues, commQueueFFT, NULL, NULL ),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftCreateDefaultPlanInternal( &fftPlan->planZ, fftPlan->context, CLFFT_1D, &fftPlan->length[ DimZ ] ),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V( fftRepo.getPlan( fftPlan->planZ, colPlan, colLock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftBakePlan(fftPlan->planZ, numQueues, commQueueFFT, NULL, NULL ), _T( "BakePlan for planZ failed" ) );
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftCreateDefaultPlanInternal( &fftPlan->planTX, fftPlan->context, CLFFT_2D, trans2Lengths ),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V( fftRepo.getPlan( fftPlan->planTX, trans2Plan, trans2Lock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftSetPlanLength( fftPlan->planTX, CLFFT_2D, trans2Lengths ),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftBakePlan(fftPlan->planTX, numQueues, commQueueFFT, NULL, NULL ),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftCreateDefaultPlanInternal( &fftPlan->planX, fftPlan->context, CLFFT_2D, clLengths ),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V( fftRepo.getPlan( fftPlan->planX, rowPlan, rowLock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftBakePlan(fftPlan->planX, numQueues, commQueueFFT, NULL, NULL ), _T( "BakePlan for planX failed" ) );
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftCreateDefaultPlanInternal( &fftPlan->planZ, fftPlan->context, CLFFT_1D, clLengths ),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V( fftRepo.getPlan( fftPlan->planZ, colPlan, colLock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftBakePlan(fftPlan->planZ, numQueues, commQueueFFT, NULL, NULL ), _T( "BakePlan 3D->1D planZ failed" ) );
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftCreateDefaultPlanInternal( &fftPlan->planX, fftPlan->context, CLFFT_2D, clLengths ),
src/external/clFFT/src/library/plan.cpp:					OPENCL_V( fftRepo.getPlan( fftPlan->planX, xyPlan, rowLock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/plan.cpp:					OPENCL_V(clfftBakePlan(fftPlan->planX, numQueues, commQueueFFT, NULL, NULL ), _T( "BakePlan 3D->2D planX failed" ) );
src/external/clFFT/src/library/plan.cpp:				OPENCL_V(clfftCreateDefaultPlanInternal( &fftPlan->planX, fftPlan->context, CLFFT_2D, clLengths ),
src/external/clFFT/src/library/plan.cpp:				OPENCL_V( fftRepo.getPlan( fftPlan->planX, xyPlan, rowLock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/plan.cpp:				OPENCL_V(clfftBakePlan(fftPlan->planX, numQueues, commQueueFFT, NULL, NULL ), _T( "BakePlan 3D->2D planX failed" ) );
src/external/clFFT/src/library/plan.cpp:				OPENCL_V(clfftCreateDefaultPlanInternal( &fftPlan->planZ, fftPlan->context, CLFFT_1D, clLengths ),
src/external/clFFT/src/library/plan.cpp:				OPENCL_V( fftRepo.getPlan( fftPlan->planZ, colPlan, colLock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/plan.cpp:				OPENCL_V(clfftBakePlan(fftPlan->planZ, numQueues, commQueueFFT, NULL, NULL ), _T( "BakePlan 3D->1D planZ failed" ) );
src/external/clFFT/src/library/plan.cpp:	OPENCL_V( fftPlan->AllocateBuffers (), _T("AllocateBuffers() failed"));
src/external/clFFT/src/library/plan.cpp:	OPENCL_V( fftRepo.getPlan( in_plHandle, in_fftPlan, in_planLock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/plan.cpp:	OPENCL_V( clfftCreateDefaultPlan( out_plHandle, new_context, in_fftPlan->dim, &in_fftPlan->length[ 0 ] ),
src/external/clFFT/src/library/plan.cpp:	OPENCL_V( fftRepo.getPlan( *out_plHandle, out_fftPlan, out_planLock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/plan.cpp:	OPENCL_V(clEnqueueWriteBuffer( *commQueueFFT,
src/external/clFFT/src/library/plan.cpp:	OPENCL_V( fftRepo.getPlan( *plHandle, fftPlan, planLock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/plan.cpp://	This routine will query the OpenCL context for it's devices
src/external/clFFT/src/library/plan.cpp:		//	First time, query OpenCL for the device info
src/external/clFFT/src/library/plan.cpp:		OPENCL_V( ::clGetContextInfo( context, CL_CONTEXT_DEVICES, 0, NULL, &deviceListSize ),
src/external/clFFT/src/library/plan.cpp:		OPENCL_V( ::clGetContextInfo( context, CL_CONTEXT_DEVICES, deviceListSize, &devices[ 0 ], NULL ),
src/external/clFFT/src/library/plan.cpp:		OPENCL_V( ::clGetDeviceInfo( devices[0], CL_DEVICE_VERSION, 0, NULL, &deviceVersionSize ),
src/external/clFFT/src/library/plan.cpp:		OPENCL_V( ::clGetDeviceInfo( devices[0], CL_DEVICE_VERSION, deviceVersionSize, &szDeviceVersion[ 0 ], NULL ),
src/external/clFFT/src/library/plan.cpp:		char openclstr[11]="OpenCL 1.0";
src/external/clFFT/src/library/plan.cpp:		if (!strncmp((const char*)&szDeviceVersion[ 0 ], openclstr, 10))
src/external/clFFT/src/library/plan.cpp:			OPENCL_V( ::clGetContextInfo( context, CL_CONTEXT_NUM_DEVICES, sizeof( cContextDevices ), &cContextDevices, NULL ),
src/external/clFFT/src/library/plan.cpp:			OPENCL_V( ::clGetDeviceInfo( devId, CL_DEVICE_LOCAL_MEM_SIZE, sizeof( cl_ulong ), &memsize, NULL ),
src/external/clFFT/src/library/plan.cpp:			OPENCL_V( ::clGetDeviceInfo( devId, CL_DEVICE_MAX_WORK_ITEM_DIMENSIONS, sizeof( unsigned int ), &maxdim, NULL ),
src/external/clFFT/src/library/plan.cpp:			OPENCL_V( ::clGetDeviceInfo( devId, CL_DEVICE_MAX_WORK_GROUP_SIZE, sizeof( size_t ), &temp[0], NULL ),
src/external/clFFT/src/library/plan.cpp:			OPENCL_V( ::clGetDeviceInfo( devId, CL_DEVICE_MAX_WORK_ITEM_SIZES, sizeof( temp ), &temp[0], NULL ),
src/external/clFFT/src/library/generator.stockham.cpp:        if (strncmp(nameVendor, "NVIDIA",6)!=0)
src/external/clFFT/src/library/generator.stockham.cpp:    OPENCL_V(this->plan->GetEnvelope (& pEnvelope), _T("GetEnvelope failed"));
src/external/clFFT/src/library/generator.stockham.cpp:	OPENCL_V(this->GetEnvelope (& pEnvelope), _T("GetEnvelope failed"));
src/external/clFFT/src/library/generator.stockham.cpp:    OPENCL_V( status, _T( "clGetCommandQueueInfo failed" ) );
src/external/clFFT/src/library/generator.stockham.cpp:    OPENCL_V( status, _T( "clGetCommandQueueInfo failed" ) );
src/external/clFFT/src/library/generator.stockham.cpp:    OPENCL_V( fftRepo.setProgramCode( this->getGenerator(), this->getSignatureData(), programCode, Device, QueueContext ), _T( "fftRepo.setclString() failed!" ) );
src/external/clFFT/src/library/generator.stockham.cpp:    OPENCL_V( fftRepo.setProgramEntryPoints( this->getGenerator(), this->getSignatureData(), "fft_fwd", "fft_back", Device, QueueContext ), _T( "fftRepo.setProgramEntryPoint() failed!" ) );
src/external/clFFT/src/library/private.h://	This is used to either wrap an OpenCL function call, or to explicitly check a variable for an OpenCL error condition.
src/external/clFFT/src/library/private.h:#define OPENCL_V( fn, msg ) \
src/external/clFFT/src/library/private.h:			terr << _T( "OPENCL_V< " ); \
src/external/clFFT/src/library/private.h:#define OPENCL_V( fn, msg ) \
src/external/clFFT/src/library/private.h:*  @return Enum describing error condition; superset of OpenCL error codes
src/external/clFFT/src/library/private.h:*  @return Enum describing error condition; superset of OpenCL error codes
src/external/clFFT/src/library/generator.copy.cpp:    OPENCL_V(this->plan->GetEnvelope (& pEnvelope), _T("GetEnvelope failed"));
src/external/clFFT/src/library/generator.copy.cpp:	OPENCL_V( status, _T( "clGetCommandQueueInfo failed" ) );
src/external/clFFT/src/library/generator.copy.cpp:    OPENCL_V( status, _T( "clGetCommandQueueInfo failed" ) );
src/external/clFFT/src/library/generator.copy.cpp:  OPENCL_V( fftRepo.setProgramCode( this->getGenerator(), this->getSignatureData(), programCode, Device, QueueContext ), _T( "fftRepo.setclString() failed!" ) );
src/external/clFFT/src/library/generator.copy.cpp:  OPENCL_V( fftRepo.setProgramEntryPoints( this->getGenerator(), this->getSignatureData(), "copy_general", "copy_general", Device, QueueContext ), _T( "fftRepo.setProgramEntryPoint() failed!" ) );
src/external/clFFT/src/library/generator.copy.cpp:  OPENCL_V( fftRepo.setProgramEntryPoints( this->getGenerator(), this->getSignatureData(), "copy_c2h", "copy_h2c", Device, QueueContext ), _T( "fftRepo.setProgramEntryPoint() failed!" ) );
src/external/clFFT/src/library/repo.cpp:GpuStatTimer* FFTRepo::pStatTimer	= NULL;
src/external/clFFT/src/library/CMakeLists.txt:# Include standard OpenCL headers
src/external/clFFT/src/library/CMakeLists.txt:include_directories( ${OpenCL_INCLUDE_DIRS} ${PROJECT_BINARY_DIR}/include ../include )
src/external/clFFT/src/library/CMakeLists.txt:target_link_libraries( clFFT ${OpenCL_LIBRARIES} ${CMAKE_DL_LIBS} )
src/external/clFFT/src/library/action.transpose.cpp:    OPENCL_V(this->plan->GetEnvelope(&pEnvelope), _T("GetEnvelope failed"));
src/external/clFFT/src/library/action.transpose.cpp://	OpenCL does not take unicode strings as input, so this routine returns only ASCII strings
src/external/clFFT/src/library/action.transpose.cpp:        OPENCL_V(clfft_transpose_generator::genTransposeKernelLeadingDimensionBatched(this->signature, programCode, lwSize, reShapeFactor), _T("genTransposeKernel() failed!"));
src/external/clFFT/src/library/action.transpose.cpp:		OPENCL_V(clfft_transpose_generator::genTransposeKernelBatched(this->signature, programCode, lwSize, reShapeFactor), _T("genTransposeKernel() failed!"));
src/external/clFFT/src/library/action.transpose.cpp:			OPENCL_V(clfft_transpose_generator::genSwapKernel(this->signature, programCode, kernelFuncName, lwSize, reShapeFactor), _T("genSwapKernel() failed!"));
src/external/clFFT/src/library/action.transpose.cpp:			OPENCL_V(clfft_transpose_generator::genSwapKernelGeneral(this->signature, programCode, kernelFuncName, lwSize, reShapeFactor), _T("genSwapKernel() failed!"));
src/external/clFFT/src/library/action.transpose.cpp:		OPENCL_V(clfft_transpose_generator::genSwapKernelGeneral(this->signature, programCode, kernelFuncName, lwSize, reShapeFactor), _T("genSwapKernel() failed!"));
src/external/clFFT/src/library/action.transpose.cpp:    OPENCL_V(status, _T("clGetCommandQueueInfo failed"));
src/external/clFFT/src/library/action.transpose.cpp:    OPENCL_V(status, _T("clGetCommandQueueInfo failed"));
src/external/clFFT/src/library/action.transpose.cpp:    OPENCL_V(fftRepo.setProgramCode(Transpose_NONSQUARE, this->getSignatureData(), programCode, Device, QueueContext), _T("fftRepo.setclString() failed!"));
src/external/clFFT/src/library/action.transpose.cpp:            OPENCL_V(fftRepo.setProgramEntryPoints(Transpose_NONSQUARE, this->getSignatureData(), "transpose_nonsquare_tw_fwd", "transpose_nonsquare_tw_back", Device, QueueContext), _T("fftRepo.setProgramEntryPoint() failed!"));
src/external/clFFT/src/library/action.transpose.cpp:            OPENCL_V(fftRepo.setProgramEntryPoints(Transpose_NONSQUARE, this->getSignatureData(), "transpose_nonsquare", "transpose_nonsquare", Device, QueueContext), _T("fftRepo.setProgramEntryPoint() failed!"));
src/external/clFFT/src/library/action.transpose.cpp:			OPENCL_V(fftRepo.setProgramEntryPoints(Transpose_NONSQUARE, this->getSignatureData(), "transpose_square_tw_fwd", "transpose_square_tw_back", Device, QueueContext), _T("fftRepo.setProgramEntryPoint() failed!"));
src/external/clFFT/src/library/action.transpose.cpp:			OPENCL_V(fftRepo.setProgramEntryPoints(Transpose_NONSQUARE, this->getSignatureData(), "transpose_square", "transpose_square", Device, QueueContext), _T("fftRepo.setProgramEntryPoint() failed!"));
src/external/clFFT/src/library/action.transpose.cpp:        OPENCL_V(fftRepo.setProgramEntryPoints(Transpose_NONSQUARE, this->getSignatureData(), "transpose_square", "transpose_square", Device, QueueContext), _T("fftRepo.setProgramEntryPoint() failed!"));
src/external/clFFT/src/library/action.transpose.cpp:            OPENCL_V(fftRepo.setProgramEntryPoints(Transpose_NONSQUARE, this->getSignatureData(), kernelFwdFuncName.c_str(), kernelBwdFuncName.c_str(), Device, QueueContext), _T("fftRepo.setProgramEntryPoint() failed!"));
src/external/clFFT/src/library/action.transpose.cpp:            OPENCL_V(fftRepo.setProgramEntryPoints(Transpose_NONSQUARE, this->getSignatureData(), kernelFuncName.c_str(), kernelFuncName.c_str(), Device, QueueContext), _T("fftRepo.setProgramEntryPoint() failed!"));
src/external/clFFT/src/library/action.transpose.cpp:	OPENCL_V(this->plan->GetEnvelope(&pEnvelope), _T("GetEnvelope failed"));
src/external/clFFT/src/library/action.transpose.cpp://	OpenCL does not take unicode strings as input, so this routine returns only ASCII strings
src/external/clFFT/src/library/action.transpose.cpp:	OPENCL_V(clfft_transpose_generator::genTransposeKernelBatched(this->signature, programCode, lwSize, reShapeFactor), _T("GenerateTransposeKernel() failed!"));
src/external/clFFT/src/library/action.transpose.cpp:	OPENCL_V(status, _T("clGetCommandQueueInfo failed"));
src/external/clFFT/src/library/action.transpose.cpp:	OPENCL_V(status, _T("clGetCommandQueueInfo failed"));
src/external/clFFT/src/library/action.transpose.cpp:	OPENCL_V(fftRepo.setProgramCode(Transpose_SQUARE, this->getSignatureData(), programCode, Device, QueueContext), _T("fftRepo.setclString() failed!"));
src/external/clFFT/src/library/action.transpose.cpp:		OPENCL_V(fftRepo.setProgramEntryPoints(Transpose_SQUARE, this->getSignatureData(), "transpose_square_tw_fwd", "transpose_square_tw_back", Device, QueueContext), _T("fftRepo.setProgramEntryPoint() failed!"));
src/external/clFFT/src/library/action.transpose.cpp:		OPENCL_V(fftRepo.setProgramEntryPoints(Transpose_SQUARE, this->getSignatureData(), "transpose_square", "transpose_square", Device, QueueContext), _T("fftRepo.setProgramEntryPoint() failed!"));
src/external/clFFT/src/library/mainpage.h:@mainpage OpenCL Fast Fourier Transforms (FFTs)
src/external/clFFT/src/library/mainpage.h:The clFFT library is an OpenCL library implementation of discrete Fast Fourier Transforms. The library:
src/external/clFFT/src/library/mainpage.h:@li works on CPU or GPU backends.
src/external/clFFT/src/library/mainpage.h:After you use the library, the @ref clfftTeardown() method must be called. This function instructs clFFT to release all resources allocated internally, and resets acquired references to any OpenCL objects.
src/external/clFFT/src/library/mainpage.h:Currently, you must manage the multi-device operation. You can create OpenCL contexts that are
src/external/clFFT/src/library/mainpage.h:@subsection Object OpenCL object creation
src/external/clFFT/src/library/mainpage.h:Your application must allocate and manage OpenCL objects, such as contexts,  *cl_mem* buffers and command queues. All the clFFT interfaces that interact with OpenCL objects take those objects as references through the API.
src/external/clFFT/src/library/mainpage.h:Specifically, the plan creation function @ref clfftCreateDefaultPlan() takes an OpenCL context as a parameter reference, increments the reference count on that object, and keeps the object alive until the corresponding plan is destroyed by the call @ref clfftDestroyPlan().
src/external/clFFT/src/library/mainpage.h:explicitly flush the command queues that are passed by reference to it. It pushes the transform work onto the command queues and returns the modified queues to the client. The client is free to issue its own blocking logic by using OpenCL synchronization mechanisms or push further work onto the queue to continue processing.
src/external/clFFT/src/library/mainpage.h:If the variable CLFFT_CACHE_PATH is defined, the library caches OpenCL binaries. This enables a subsequent run of the application with the same type of transforms to avoid the expensive compilation step. Instead, the stored binaries are loaded and executed. The CLFFT_CACHE_PATH must point to a folder location where the library can store binaries. 
src/external/clFFT/src/library/mainpage.h:<li> The OpenCL context that executes the transform
src/external/clFFT/src/library/mainpage.h:<li> The OpenCL handles to the input and output data buffers.
src/external/clFFT/src/library/mainpage.h:<li> The OpenCL handle to a temporary scratch buffer (if needed).
src/external/clFFT/src/library/mainpage.h:Both *CLFFT_SINGLE* and *CLFFT_DOUBLE* precisions are supported by the library for all supported radices. For both these enums the math functions of the host computer are used to produce the sine and cosine tables that are used by the OpenCL kernel.
src/external/clFFT/src/library/mainpage.h:The efficiency of clFFT is improved by utilizing transforms in batches. Sending as much data as possible in a single transform call leverages the parallel compute capabilities of OpenCL devices (and GPU devices in particular), and minimizes the penalty of transfer overhead. It is best to think of an OpenCL device as a high-throughput, high-latency device. Using a networking analogy as an example, this approach is similar to having a massively high-bandwidth pipe with very high ping response times. If the client is ready to send data to the device for compute, it should be sent in as few API calls as possible and this can be done by batching. clFFT plans have a parameter @ref clfftSetPlanBatchSize() to describe the number of transforms being batched, and another parameter @ref clfftSetPlanDistance() to describe how those batches are laid out and spaced in memory. 1D, 2D, or 3D transforms can be batched.
src/external/clFFT/src/library/mainpage.h:			       OpenCL context must be provided when the plan is created; it cannot be 
src/external/clFFT/src/library/mainpage.h:                         exact OpenCL kernels that perform the specified FFT on the provided OpenCL device.
src/external/clFFT/src/library/mainpage.h:                        executing benchmark kernels on the OpenCL device context to maximize the runtime 
src/external/clFFT/src/library/mainpage.h:	<li> Execute the OpenCL FFT kernels as many times as needed. </li>
src/external/clFFT/src/library/mainpage.h:		         execute a forward or reverse transform; also, provide the OpenCL *cl_mem* 
src/external/clFFT/src/library/mainpage.h:		         @ref clfftEnqueueTransform() performs one or more calls to the OpenCL function 
src/external/clFFT/src/library/mainpage.h:                                       compute kernel(s) are added to the OpenCL context queue to be executed 
src/external/clFFT/src/library/mainpage.h:		         An OpenCL event handle is returned to the caller. If multiple NDRangeKernel 
src/external/clFFT/src/library/mainpage.h:		<li>  Add any application OpenCL tasks to the OpenCL context queue. For example, if the    		        next step in the application process is to apply a filter to the transformed data, the 
src/external/clFFT/src/library/mainpage.h:		<li>  If the application accessed the transformed data directly, it calls one of the OpenCL 
src/external/clFFT/src/library/mainpage.h:		        functions for synchronizing the host computer execution with the OpenCL device 
src/external/clFFT/src/library/mainpage.h:The callback feature of clFFT has the ability to invoke user provided OpenCL inline functions to pre-process or post-process data from within the FFT kernel. The inline OpenCL callback function is passed as a string to the library. It is then incorporated into the generated FFT kernel. This eliminates the need for an additional kernel launch to carry out the pre/post processing tasks, thus improving overall performance.
src/external/clFFT/src/library/plan.h:	// TODO:  These arbitrary parameters should be tuned for the type of GPU
src/external/clFFT/src/library/plan.h:			//  The # of dimensions is arbitrary, but limited by the OpenCL implementation
src/external/clFFT/src/library/plan.h:			//  local data storage (LDS).  This # is best for Evergreen gpus,
src/external/clFFT/src/library/plan.h:			//  This tuning parameter is a good value for Evergreen gpus,
src/external/clFFT/src/library/plan.h:	 *	including all information that is used to generate the OpenCL kernel.
src/external/clFFT/src/library/plan.h:// An action basically implements some OpenCL related actions, for instance:
src/external/clFFT/src/library/plan.h://	This will depend on the GPU(s) in the OpenCL context.
src/external/clFFT/src/library/plan.h:	// TODO, change this logic for handling multiple GPUs/devices
src/external/clFFT/src/library/fft_binary_lookup.h:#include <OpenCL/cl.h>
src/external/clFFT/src/library/fft_binary_lookup.h://      name and the OpenCL context and device shall form a unique
src/external/clFFT/src/library/generator.stockham.h:								"#pragma OPENCL EXTENSION cl_khr_fp64 : enable\n"
src/external/clFFT/src/library/generator.stockham.h:								"#pragma OPENCL EXTENSION cl_amd_fp64 : enable\n"
src/external/clFFT/src/library/generator.transpose.gcn.cpp:// clfft.generator.Transpose.cpp : Dynamic run-time generator of openCL transpose kernels
src/external/clFFT/src/library/generator.transpose.gcn.cpp:    // clKernWrite( transKernel, 0 ) << "#pragma OPENCL EXTENSION cl_amd_printf : enable\n" << std::endl;
src/external/clFFT/src/library/generator.transpose.gcn.cpp:        clKernWrite( transKernel, 3 ) << "#pragma OPENCL EXTENSION cl_khr_fp64 : enable" << std::endl;
src/external/clFFT/src/library/generator.transpose.gcn.cpp:        clKernWrite( transKernel, 3 ) <<  "#pragma OPENCL EXTENSION cl_amd_fp64 : enable" << std::endl;
src/external/clFFT/src/library/generator.transpose.gcn.cpp:    OPENCL_V( this->plan->GetEnvelope( &pEnvelope ), _T( "GetEnvelope failed" ) );
src/external/clFFT/src/library/generator.transpose.gcn.cpp://	OpenCL does not take unicode strings as input, so this routine returns only ASCII strings
src/external/clFFT/src/library/generator.transpose.gcn.cpp:	OPENCL_V( CalculateBlockSize(this->signature.fft_precision, loopCount, blockSize), _T("CalculateBlockSize() failed!") );
src/external/clFFT/src/library/generator.transpose.gcn.cpp:    OPENCL_V( genTransposeKernel( this->signature, programCode, lwSize, reShapeFactor, loopCount, blockSize ), _T( "GenerateTransposeKernel() failed!" ) );
src/external/clFFT/src/library/generator.transpose.gcn.cpp:    OPENCL_V( status, _T( "clGetCommandQueueInfo failed" ) );
src/external/clFFT/src/library/generator.transpose.gcn.cpp:    OPENCL_V( status, _T( "clGetCommandQueueInfo failed" ) );
src/external/clFFT/src/library/generator.transpose.gcn.cpp:    OPENCL_V( fftRepo.setProgramCode( Transpose_GCN, this->getSignatureData(), programCode, Device, QueueContext ), _T( "fftRepo.setclString() failed!" ) );
src/external/clFFT/src/library/generator.transpose.gcn.cpp:        OPENCL_V( fftRepo.setProgramEntryPoints( Transpose_GCN, this->getSignatureData(), "transpose_gcn_tw_fwd", "transpose_gcn_tw_back", Device, QueueContext ), _T( "fftRepo.setProgramEntryPoint() failed!" ) );
src/external/clFFT/src/library/generator.transpose.gcn.cpp:        OPENCL_V( fftRepo.setProgramEntryPoints( Transpose_GCN, this->getSignatureData(), "transpose_gcn", "transpose_gcn", Device, QueueContext ), _T( "fftRepo.setProgramEntryPoint() failed!" ) );
src/external/clFFT/src/library/generator.transpose.gcn.cpp:	OPENCL_V( CalculateBlockSize(this->signature.fft_precision, loopCount, blockSize), _T("CalculateBlockSize() failed!") );
src/external/clFFT/src/library/transform.cpp:	//	We do not currently support multiple command queues, which is necessary to support multi-gpu operations
src/external/clFFT/src/library/transform.cpp:	OPENCL_V( fftRepo.getPlan( plHandle, fftPlan, planLock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/transform.cpp:		OPENCL_V( clfftBakePlan( plHandle, numQueuesAndEvents, commQueues, NULL, NULL ), _T( "Failed to bake plan" ) );
src/external/clFFT/src/library/transform.cpp:		OPENCL_V( status, _T("Creating the intermediate buffer for large1D Failed") );
src/external/clFFT/src/library/transform.cpp:		OPENCL_V( status, _T("Creating the intermediate buffer for large1D RC Failed") );
src/external/clFFT/src/library/transform.cpp:		OPENCL_V( status, _T("Creating the intermediate buffer for large1D YZ C2R Failed") );
src/external/clFFT/src/library/transform.cpp:	//	depends on the GPU caps -- especially the amount of LDS
src/external/clFFT/src/library/transform.cpp:	OPENCL_V(fftPlan->GetMax1DLength (&Large1DThreshold), _T("GetMax1DLength failed"));
src/external/clFFT/src/library/transform.cpp:					OPENCL_V( clfftEnqueueTransform( fftPlan->planTX, dir, numQueuesAndEvents, commQueues, numWaitEvents,
src/external/clFFT/src/library/transform.cpp:								OPENCL_V( clEnqueueReadBuffer( *commQueues, localIntBuffer, CL_TRUE, 0, buffSizeBytes_complex, &temp[ 0 ], 0,
src/external/clFFT/src/library/transform.cpp:					OPENCL_V( clfftEnqueueTransform( fftPlan->planX, dir, numQueuesAndEvents, commQueues, 1,
src/external/clFFT/src/library/transform.cpp:								OPENCL_V( clEnqueueReadBuffer( *commQueues, *mybuffers, CL_TRUE, 0, 536870912, &temp[ 0 ], 0,
src/external/clFFT/src/library/transform.cpp:					OPENCL_V( clfftEnqueueTransform( fftPlan->planTY, dir, numQueuesAndEvents, commQueues, 1,
src/external/clFFT/src/library/transform.cpp:								OPENCL_V( clEnqueueReadBuffer( *commQueues, localIntBuffer, CL_TRUE, 0, buffSizeBytes_complex, &temp[ 0 ], 0,
src/external/clFFT/src/library/transform.cpp:					OPENCL_V( clfftEnqueueTransform( fftPlan->planY, dir, numQueuesAndEvents, commQueues, 1,
src/external/clFFT/src/library/transform.cpp:								OPENCL_V( clEnqueueReadBuffer( *commQueues, localIntBuffer, CL_TRUE, 0, buffSizeBytes_complex, &temp[ 0 ], 0,
src/external/clFFT/src/library/transform.cpp:					OPENCL_V( clfftEnqueueTransform( fftPlan->planTZ, dir, numQueuesAndEvents, commQueues, 1,
src/external/clFFT/src/library/transform.cpp:				OPENCL_V( clfftEnqueueTransform( fftPlan->planX, CLFFT_FORWARD, numQueuesAndEvents, commQueues, numWaitEvents,
src/external/clFFT/src/library/transform.cpp:				OPENCL_V(clfftEnqueueTransform(fftPlan->planY, CLFFT_FORWARD, numQueuesAndEvents, commQueues, 1, &colOutEvents,
src/external/clFFT/src/library/transform.cpp:				OPENCL_V(clfftEnqueueTransform(fftPlan->planRCcopy, CLFFT_FORWARD, numQueuesAndEvents, commQueues, 1, &copyInEvents,
src/external/clFFT/src/library/transform.cpp:					OPENCL_V(clfftEnqueueTransform(fftPlan->planRCcopy, CLFFT_BACKWARD, numQueuesAndEvents, commQueues, numWaitEvents,
src/external/clFFT/src/library/transform.cpp:					OPENCL_V(clfftEnqueueTransform(fftPlan->planX, CLFFT_BACKWARD, numQueuesAndEvents, commQueues, 1,
src/external/clFFT/src/library/transform.cpp:					OPENCL_V(clfftEnqueueTransform(fftPlan->planX, CLFFT_BACKWARD, numQueuesAndEvents, commQueues, numWaitEvents,
src/external/clFFT/src/library/transform.cpp:				OPENCL_V( clfftEnqueueTransform( fftPlan->planY, CLFFT_BACKWARD, numQueuesAndEvents, commQueues, 1, &colOutEvents,
src/external/clFFT/src/library/transform.cpp:					OPENCL_V(clEnqueueWriteBuffer( *commQueues,
src/external/clFFT/src/library/transform.cpp:						OPENCL_V( clfftEnqueueTransform( fftPlan->planTX, dir, numQueuesAndEvents, commQueues, numWaitEvents,
src/external/clFFT/src/library/transform.cpp:						OPENCL_V( clfftEnqueueTransform( fftPlan->planTX, dir, numQueuesAndEvents, commQueues, numWaitEvents,
src/external/clFFT/src/library/transform.cpp:								OPENCL_V( clEnqueueReadBuffer( *commQueues, localIntBuffer, CL_TRUE, 0, buffSizeBytes_complex, &temp[ 0 ], 0,
src/external/clFFT/src/library/transform.cpp:						OPENCL_V( clfftEnqueueTransform( fftPlan->planX, dir, numQueuesAndEvents, commQueues, 1,
src/external/clFFT/src/library/transform.cpp:						OPENCL_V( clfftEnqueueTransform( fftPlan->planX, dir, numQueuesAndEvents, commQueues, 1,
src/external/clFFT/src/library/transform.cpp:								OPENCL_V( clEnqueueReadBuffer( *commQueues, *mybuffers, CL_TRUE, 0, 536870912, &temp[ 0 ], 0,
src/external/clFFT/src/library/transform.cpp:						OPENCL_V( clfftEnqueueTransform( fftPlan->planTY, dir, numQueuesAndEvents, commQueues, 1,
src/external/clFFT/src/library/transform.cpp:						OPENCL_V( clfftEnqueueTransform( fftPlan->planTY, dir, numQueuesAndEvents, commQueues, 1,
src/external/clFFT/src/library/transform.cpp:								OPENCL_V( clEnqueueReadBuffer( *commQueues, localIntBuffer, CL_TRUE, 0, buffSizeBytes_complex, &temp[ 0 ], 0,
src/external/clFFT/src/library/transform.cpp:						OPENCL_V( clfftEnqueueTransform( fftPlan->planY, dir, numQueuesAndEvents, commQueues, 1,
src/external/clFFT/src/library/transform.cpp:						OPENCL_V( clfftEnqueueTransform( fftPlan->planY, dir, numQueuesAndEvents, commQueues, 1,
src/external/clFFT/src/library/transform.cpp:								OPENCL_V( clEnqueueReadBuffer( *commQueues, localIntBuffer, CL_TRUE, 0, buffSizeBytes_complex, &temp[ 0 ], 0,
src/external/clFFT/src/library/transform.cpp:						OPENCL_V( clfftEnqueueTransform( fftPlan->planTZ, dir, numQueuesAndEvents, commQueues, 1,
src/external/clFFT/src/library/transform.cpp:						OPENCL_V( clfftEnqueueTransform( fftPlan->planTZ, dir, numQueuesAndEvents, commQueues, 1,
src/external/clFFT/src/library/transform.cpp:							OPENCL_V( clfftEnqueueTransform( fftPlan->planTX, dir, numQueuesAndEvents, commQueues, numWaitEvents,
src/external/clFFT/src/library/transform.cpp:									OPENCL_V( clEnqueueReadBuffer( *commQueues, localIntBuffer, CL_TRUE, 0, buffSizeBytes_complex, &temp[ 0 ], 0,
src/external/clFFT/src/library/transform.cpp:							OPENCL_V( clfftEnqueueTransform( fftPlan->planX, dir, numQueuesAndEvents, commQueues, 1,
src/external/clFFT/src/library/transform.cpp:									OPENCL_V( clEnqueueReadBuffer( *commQueues, localIntBuffer, CL_TRUE, 0, buffSizeBytes_complex, &temp[ 0 ], 0,
src/external/clFFT/src/library/transform.cpp:							OPENCL_V( clfftEnqueueTransform( fftPlan->planY, dir, numQueuesAndEvents, commQueues, 1, &rowXOutEvents,
src/external/clFFT/src/library/transform.cpp:									OPENCL_V( clEnqueueReadBuffer( *commQueues, localIntBuffer, CL_TRUE, 0, buffSizeBytes_complex, &temp[ 0 ], 0,
src/external/clFFT/src/library/transform.cpp:							OPENCL_V( clfftEnqueueTransform( fftPlan->planCopy, dir, numQueuesAndEvents, commQueues, 1, &colYOutEvents,
src/external/clFFT/src/library/transform.cpp:							OPENCL_V( clfftEnqueueTransform( fftPlan->planX, dir, numQueuesAndEvents, commQueues, numWaitEvents,
src/external/clFFT/src/library/transform.cpp:							OPENCL_V( clEnqueueReadBuffer( *commQueues, localIntBuffer, CL_TRUE, 0, buffSizeBytes_complex, &temp[ 0 ], 1,
src/external/clFFT/src/library/transform.cpp:								OPENCL_V( clfftEnqueueTransform( fftPlan->planY, dir, numQueuesAndEvents, commQueues, 1, &colOutEvents,
src/external/clFFT/src/library/transform.cpp:									OPENCL_V( clfftEnqueueTransform( fftPlan->planTZ, dir, numQueuesAndEvents, commQueues, 1, &rowYOutEvents,
src/external/clFFT/src/library/transform.cpp:									OPENCL_V( clfftEnqueueTransform( fftPlan->planTZ, dir, numQueuesAndEvents, commQueues, 1, &rowYOutEvents,
src/external/clFFT/src/library/transform.cpp:									OPENCL_V( clfftEnqueueTransform( fftPlan->planY, dir, numQueuesAndEvents, commQueues, 1, &colOutEvents,
src/external/clFFT/src/library/transform.cpp:									OPENCL_V( clEnqueueReadBuffer( *commQueues, clInputBuffers[0], CL_TRUE, 0, buffSizeBytes_complex, &temp[ 0 ], 1,
src/external/clFFT/src/library/transform.cpp:								OPENCL_V( clEnqueueReadBuffer( *commQueues, localIntBuffer, CL_TRUE, 0, buffSizeBytes_complex, &temp[ 0 ], 1,
src/external/clFFT/src/library/transform.cpp:									OPENCL_V( clfftEnqueueTransform( fftPlan->planY, dir, numQueuesAndEvents, commQueues, 1, &colOutEvents,
src/external/clFFT/src/library/transform.cpp:									OPENCL_V( clEnqueueReadBuffer( *commQueues, clOutputBuffers[0], CL_TRUE, 0, buffSizeBytes_complex, &temp[ 0 ], 1,
src/external/clFFT/src/library/transform.cpp:						OPENCL_V( clfftEnqueueTransform( fftPlan->planX, dir, numQueuesAndEvents, commQueues, numWaitEvents,
src/external/clFFT/src/library/transform.cpp:						OPENCL_V( clEnqueueReadBuffer( *commQueues, clOutputBuffers[0], CL_TRUE, 0, buffSizeBytes_complex, &temp[ 0 ], 1,
src/external/clFFT/src/library/transform.cpp:						OPENCL_V( clfftEnqueueTransform( fftPlan->planY, dir, numQueuesAndEvents, commQueues, 1, &colOutEvents,
src/external/clFFT/src/library/transform.cpp:				OPENCL_V(clfftEnqueueTransform(fftPlan->planTX, dir, numQueuesAndEvents, commQueues, numWaitEvents,
src/external/clFFT/src/library/transform.cpp:				OPENCL_V(clfftEnqueueTransform(fftPlan->planTY, dir, numQueuesAndEvents, commQueues, 1,
src/external/clFFT/src/library/transform.cpp:			OPENCL_V( clEnqueueReadBuffer( *commQueues, clInputBuffers[0], CL_TRUE, 0, buffSizeBytes, &output2[ 0 ], 0,
src/external/clFFT/src/library/transform.cpp:				OPENCL_V( clEnqueueReadBuffer( *commQueues, clOutputBuffers[0], CL_TRUE, 0, buffSizeBytes, &output2[ 0 ], 0,
src/external/clFFT/src/library/transform.cpp:				OPENCL_V( clfftEnqueueTransform( fftPlan->planX, dir, numQueuesAndEvents, commQueues, numWaitEvents,
src/external/clFFT/src/library/transform.cpp:				OPENCL_V( clEnqueueReadBuffer( *commQueues, mybuffers[0], CL_TRUE, 0, buffSizeBytes*2, &output2[ 0 ], 0,
src/external/clFFT/src/library/transform.cpp:					OPENCL_V( clfftEnqueueTransform( fftPlan->planTX, dir, numQueuesAndEvents, commQueues, 1, &rowOutEvents,
src/external/clFFT/src/library/transform.cpp:					OPENCL_V( clEnqueueReadBuffer( *commQueues, localIntBuffer, CL_TRUE, 0, buffSizeBytes*2, &output2[0], 0,
src/external/clFFT/src/library/transform.cpp:						OPENCL_V( clfftEnqueueTransform( fftPlan->planY, dir, numQueuesAndEvents, commQueues, 1, &transXOutEvents,
src/external/clFFT/src/library/transform.cpp:						OPENCL_V( clEnqueueReadBuffer( *commQueues, localIntBuffer, CL_TRUE, 0, buffSizeBytes*2, &output2[0], 0,
src/external/clFFT/src/library/transform.cpp:						OPENCL_V( clfftEnqueueTransform( fftPlan->planTY, dir, numQueuesAndEvents, commQueues, 1, &colOutEvents,
src/external/clFFT/src/library/transform.cpp:						OPENCL_V( clEnqueueReadBuffer( *commQueues, mybuffers[0], CL_TRUE, 0, buffSizeBytes*2, &output2[0], 0,
src/external/clFFT/src/library/transform.cpp:						OPENCL_V( clfftEnqueueTransform( fftPlan->planY, dir, numQueuesAndEvents, commQueues, 1, &transXOutEvents,
src/external/clFFT/src/library/transform.cpp:					OPENCL_V( clfftEnqueueTransform( fftPlan->planTX, dir, numQueuesAndEvents, commQueues, 1, &rowOutEvents,
src/external/clFFT/src/library/transform.cpp:						OPENCL_V( clfftEnqueueTransform( fftPlan->planY, dir, numQueuesAndEvents, commQueues, 1, &transXOutEvents,
src/external/clFFT/src/library/transform.cpp:						OPENCL_V( clfftEnqueueTransform( fftPlan->planTY, dir, numQueuesAndEvents, commQueues, 1, &colOutEvents,
src/external/clFFT/src/library/transform.cpp:						OPENCL_V( clfftEnqueueTransform( fftPlan->planY, dir, numQueuesAndEvents, commQueues, 1, &transXOutEvents,
src/external/clFFT/src/library/transform.cpp:						OPENCL_V( clfftEnqueueTransform( fftPlan->planX, dir, numQueuesAndEvents, commQueues, numWaitEvents,
src/external/clFFT/src/library/transform.cpp:						OPENCL_V( clfftEnqueueTransform( fftPlan->planY, dir, numQueuesAndEvents, commQueues, 1, &rowOutEvents,
src/external/clFFT/src/library/transform.cpp:						OPENCL_V( clfftEnqueueTransform( fftPlan->planX, dir, numQueuesAndEvents, commQueues, numWaitEvents,
src/external/clFFT/src/library/transform.cpp:						OPENCL_V( clfftEnqueueTransform( fftPlan->planY, dir, numQueuesAndEvents, commQueues, 1, &rowOutEvents,
src/external/clFFT/src/library/transform.cpp:							OPENCL_V( clfftEnqueueTransform( fftPlan->planX, dir, numQueuesAndEvents, commQueues, numWaitEvents,
src/external/clFFT/src/library/transform.cpp:							OPENCL_V( clEnqueueReadBuffer( *commQueues, mybuffers[0], CL_TRUE, 0, buffSizeBytes*2, &output2[ 0 ], 0,
src/external/clFFT/src/library/transform.cpp:							OPENCL_V( clfftEnqueueTransform( fftPlan->planTX, dir, numQueuesAndEvents, commQueues, 1, &rowOutEvents,
src/external/clFFT/src/library/transform.cpp:							OPENCL_V( clEnqueueReadBuffer( *commQueues, localIntBuffer, CL_TRUE, 0, buffSizeBytes*2, &output2[0], 0,
src/external/clFFT/src/library/transform.cpp:							OPENCL_V( clfftEnqueueTransform( fftPlan->planY, dir, numQueuesAndEvents, commQueues, 1, &transXOutEvents,
src/external/clFFT/src/library/transform.cpp:							OPENCL_V( clEnqueueReadBuffer( *commQueues, localIntBuffer, CL_TRUE, 0, buffSizeBytes*2, &output2[0], 0,
src/external/clFFT/src/library/transform.cpp:							OPENCL_V( clfftEnqueueTransform( fftPlan->planTY, dir, numQueuesAndEvents, commQueues, 1, &colOutEvents,
src/external/clFFT/src/library/transform.cpp:							OPENCL_V( clEnqueueReadBuffer( *commQueues, mybuffers[0], CL_TRUE, 0, buffSizeBytes*2, &output2[0], 0,
src/external/clFFT/src/library/transform.cpp:								OPENCL_V( clfftEnqueueTransform( fftPlan->planX, CLFFT_FORWARD, numQueuesAndEvents, commQueues, numWaitEvents,
src/external/clFFT/src/library/transform.cpp:								OPENCL_V( clfftEnqueueTransform( fftPlan->planY, CLFFT_FORWARD, numQueuesAndEvents, commQueues, 1, &rowOutEvents,
src/external/clFFT/src/library/transform.cpp:								OPENCL_V( clfftEnqueueTransform( fftPlan->planX, CLFFT_FORWARD, numQueuesAndEvents, commQueues, numWaitEvents,
src/external/clFFT/src/library/transform.cpp:								OPENCL_V( clfftEnqueueTransform( fftPlan->planY, CLFFT_FORWARD, numQueuesAndEvents, commQueues, 1, &rowOutEvents,
src/external/clFFT/src/library/transform.cpp:							OPENCL_V( clfftEnqueueTransform( fftPlan->planTY, dir, numQueuesAndEvents, commQueues, numWaitEvents, 
src/external/clFFT/src/library/transform.cpp:							OPENCL_V( clEnqueueReadBuffer( *commQueues, mybuffers[0], CL_TRUE, 0, buffSizeBytes*2, &output2[0], 0,
src/external/clFFT/src/library/transform.cpp:							OPENCL_V( clfftEnqueueTransform( fftPlan->planY, dir, numQueuesAndEvents, commQueues, 1, &transYOutEvents, 
src/external/clFFT/src/library/transform.cpp:							OPENCL_V( clEnqueueReadBuffer( *commQueues, mybuffers[0], CL_TRUE, 0, buffSizeBytes*2, &output2[ 0 ], 0,
src/external/clFFT/src/library/transform.cpp:							OPENCL_V( clfftEnqueueTransform( fftPlan->planTX, dir, numQueuesAndEvents, commQueues, 1, &rowOutEvents,
src/external/clFFT/src/library/transform.cpp:							OPENCL_V( clEnqueueReadBuffer( *commQueues, localIntBuffer, CL_TRUE, 0, buffSizeBytes*2, &output2[0], 0,
src/external/clFFT/src/library/transform.cpp:								OPENCL_V( clfftEnqueueTransform( fftPlan->planX, dir, numQueuesAndEvents, commQueues, 1, &transXOutEvents,
src/external/clFFT/src/library/transform.cpp:								OPENCL_V( clfftEnqueueTransform( fftPlan->planX, dir, numQueuesAndEvents, commQueues, 1, &transXOutEvents,
src/external/clFFT/src/library/transform.cpp:							OPENCL_V( clEnqueueReadBuffer( *commQueues, localIntBuffer, CL_TRUE, 0, buffSizeBytes*2, &output2[0], 0,
src/external/clFFT/src/library/transform.cpp:							OPENCL_V( clfftEnqueueTransform( fftPlan->planY, CLFFT_BACKWARD, numQueuesAndEvents, commQueues, numWaitEvents,
src/external/clFFT/src/library/transform.cpp:							OPENCL_V( clfftEnqueueTransform( fftPlan->planX, CLFFT_BACKWARD, numQueuesAndEvents, commQueues, 1, &rowOutEvents,
src/external/clFFT/src/library/transform.cpp:						OPENCL_V( clfftEnqueueTransform( fftPlan->planX, dir, numQueuesAndEvents, commQueues, numWaitEvents,
src/external/clFFT/src/library/transform.cpp:							OPENCL_V( clfftEnqueueTransform( fftPlan->planY, dir, numQueuesAndEvents, commQueues, 1, &rowOutEvents,
src/external/clFFT/src/library/transform.cpp:							OPENCL_V( clfftEnqueueTransform( fftPlan->planY, dir, numQueuesAndEvents, commQueues, 1, &rowOutEvents,
src/external/clFFT/src/library/transform.cpp:							OPENCL_V( clEnqueueReadBuffer( *commQueues, clOutputBuffers[0], CL_TRUE, 0, buffSizeBytes, &output2[ 0 ], 1,
src/external/clFFT/src/library/transform.cpp:					OPENCL_V( clfftEnqueueTransform( fftPlan->planX, dir, numQueuesAndEvents, commQueues, numWaitEvents,
src/external/clFFT/src/library/transform.cpp:					OPENCL_V( clEnqueueReadBuffer( *commQueues, mybuffers[0], CL_TRUE, 0, buffSizeBytes*2, &output2[ 0 ], 0,
src/external/clFFT/src/library/transform.cpp:					OPENCL_V( clfftEnqueueTransform( fftPlan->planTX, dir, numQueuesAndEvents, commQueues, 1, &rowOutEvents,
src/external/clFFT/src/library/transform.cpp:					OPENCL_V( clEnqueueReadBuffer( *commQueues, localIntBuffer, CL_TRUE, 0, buffSizeBytes*2, &output2[0], 0,
src/external/clFFT/src/library/transform.cpp:					OPENCL_V( clfftEnqueueTransform( fftPlan->planZ, dir, numQueuesAndEvents, commQueues, 1, &transXOutEvents,
src/external/clFFT/src/library/transform.cpp:					OPENCL_V( clEnqueueReadBuffer( *commQueues, localIntBuffer, CL_TRUE, 0, buffSizeBytes*2, &output2[0], 0,
src/external/clFFT/src/library/transform.cpp:					OPENCL_V( clfftEnqueueTransform( fftPlan->planTY, dir, numQueuesAndEvents, commQueues, 1, &colOutEvents,
src/external/clFFT/src/library/transform.cpp:					OPENCL_V( clEnqueueReadBuffer( *commQueues, mybuffers[0], CL_TRUE, 0, buffSizeBytes*2, &output2[0], 0,
src/external/clFFT/src/library/transform.cpp:					OPENCL_V( clfftEnqueueTransform( fftPlan->planX, CLFFT_FORWARD, numQueuesAndEvents, commQueues, numWaitEvents,
src/external/clFFT/src/library/transform.cpp:					OPENCL_V( clfftEnqueueTransform( fftPlan->planZ, CLFFT_FORWARD, numQueuesAndEvents, commQueues, 1, &rowOutEvents,
src/external/clFFT/src/library/transform.cpp:					OPENCL_V( clfftEnqueueTransform( fftPlan->planTZ, dir, numQueuesAndEvents, commQueues, numWaitEvents, 
src/external/clFFT/src/library/transform.cpp:					OPENCL_V( clEnqueueReadBuffer( *commQueues, mybuffers[0], CL_TRUE, 0, buffSizeBytes*2, &output2[0], 0,
src/external/clFFT/src/library/transform.cpp:					OPENCL_V( clfftEnqueueTransform( fftPlan->planZ, dir, numQueuesAndEvents, commQueues, 1, &transZOutEvents, 
src/external/clFFT/src/library/transform.cpp:					OPENCL_V( clEnqueueReadBuffer( *commQueues, mybuffers[0], CL_TRUE, 0, buffSizeBytes*2, &output2[ 0 ], 0,
src/external/clFFT/src/library/transform.cpp:					OPENCL_V( clfftEnqueueTransform( fftPlan->planTX, dir, numQueuesAndEvents, commQueues, 1, &rowOutEvents,
src/external/clFFT/src/library/transform.cpp:					OPENCL_V( clEnqueueReadBuffer( *commQueues, localIntBuffer, CL_TRUE, 0, buffSizeBytes*2, &output2[0], 0,
src/external/clFFT/src/library/transform.cpp:						OPENCL_V( clfftEnqueueTransform( fftPlan->planX, dir, numQueuesAndEvents, commQueues, 1, &transXOutEvents,
src/external/clFFT/src/library/transform.cpp:						OPENCL_V( clfftEnqueueTransform( fftPlan->planX, dir, numQueuesAndEvents, commQueues, 1, &transXOutEvents,
src/external/clFFT/src/library/transform.cpp:					OPENCL_V( clEnqueueReadBuffer( *commQueues, localIntBuffer, CL_TRUE, 0, buffSizeBytes*2, &output2[0], 0,
src/external/clFFT/src/library/transform.cpp:					OPENCL_V( clfftEnqueueTransform( fftPlan->planZ, CLFFT_BACKWARD, numQueuesAndEvents, commQueues, numWaitEvents,
src/external/clFFT/src/library/transform.cpp:					OPENCL_V( clfftEnqueueTransform( fftPlan->planX, CLFFT_BACKWARD, numQueuesAndEvents, commQueues, 1, &rowOutEvents,
src/external/clFFT/src/library/transform.cpp:					OPENCL_V( clfftEnqueueTransform( fftPlan->planX, dir, numQueuesAndEvents, commQueues, numWaitEvents,
src/external/clFFT/src/library/transform.cpp:					OPENCL_V( clfftEnqueueTransform( fftPlan->planZ, dir, numQueuesAndEvents, commQueues, 1, &rowOutEvents,
src/external/clFFT/src/library/transform.cpp:					OPENCL_V( clEnqueueReadBuffer( *commQueues, clOutputBuffers[0], CL_TRUE, 0, buffSizeBytes, &output3[ 0 ], 0,
src/external/clFFT/src/library/transform.cpp:					OPENCL_V( clfftEnqueueTransform( fftPlan->planX, dir, numQueuesAndEvents, commQueues, numWaitEvents,
src/external/clFFT/src/library/transform.cpp:					OPENCL_V( clEnqueueReadBuffer( *commQueues, clOutputBuffers[0], CL_TRUE, 0, buffSizeBytes, &output3[ 0 ], 0,
src/external/clFFT/src/library/transform.cpp:					OPENCL_V( clfftEnqueueTransform( fftPlan->planZ, dir, numQueuesAndEvents, commQueues, 1, &rowOutEvents,
src/external/clFFT/src/library/transform.cpp:					OPENCL_V( clEnqueueReadBuffer( *commQueues, clOutputBuffers[0], CL_TRUE, 0, buffSizeBytes, &output3[ 0 ], 1,
src/external/clFFT/src/library/repo.h:#include "../statTimer/statisticalTimer.GPU.h"
src/external/clFFT/src/library/repo.h:		//	is not guaranteed, and openCL might already have cleaned itself up.  When clFFT tries to free its resources, an access
src/external/clFFT/src/library/repo.h:	//	Our runtime library can instrument kernel timings with a GPU timer available in a shared module
src/external/clFFT/src/library/repo.h:	static GpuStatTimer* pStatTimer;
src/external/clFFT/src/library/fft_binary_lookup.cpp:        // FIXME: emit an internal message for OPENCL errors
src/external/clFFT/src/library/clFFT.pc.in:Description: Open source OpenCL FFT library
src/external/clFFT/src/library/accessors.cpp:	OPENCL_V( fftRepo.getPlan( plHandle, fftPlan, planLock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/accessors.cpp:	OPENCL_V( fftRepo.getPlan( plHandle, fftPlan, planLock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/accessors.cpp:	OPENCL_V( fftRepo.getPlan( plHandle, fftPlan, planLock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/accessors.cpp:	OPENCL_V( fftRepo.getPlan( plHandle, fftPlan, planLock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/accessors.cpp:	OPENCL_V( fftRepo.getPlan( plHandle, fftPlan, planLock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/accessors.cpp:	OPENCL_V( fftRepo.getPlan( plHandle, fftPlan, planLock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/accessors.cpp:	OPENCL_V( fftRepo.getPlan( plHandle, fftPlan, planLock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/accessors.cpp:	OPENCL_V( fftRepo.getPlan( plHandle, fftPlan, planLock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/accessors.cpp:	OPENCL_V( fftRepo.getPlan( plHandle, fftPlan, planLock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/accessors.cpp:	OPENCL_V( fftRepo.getPlan( plHandle, fftPlan, planLock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/accessors.cpp:	OPENCL_V( fftRepo.getPlan( plHandle, fftPlan, planLock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/accessors.cpp:	OPENCL_V( fftRepo.getPlan( plHandle, fftPlan, planLock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/accessors.cpp:	OPENCL_V( fftRepo.getPlan( plHandle, fftPlan, planLock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/accessors.cpp:	OPENCL_V( fftRepo.getPlan( plHandle, fftPlan, planLock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/accessors.cpp:	OPENCL_V( fftRepo.getPlan( plHandle, fftPlan, planLock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/accessors.cpp:	OPENCL_V( fftRepo.getPlan( plHandle, fftPlan, planLock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/accessors.cpp:	OPENCL_V( fftRepo.getPlan( plHandle, fftPlan, planLock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/accessors.cpp:	OPENCL_V( fftRepo.getPlan( plHandle, fftPlan, planLock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/accessors.cpp:	OPENCL_V( fftRepo.getPlan( plHandle, fftPlan, planLock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/accessors.cpp:	OPENCL_V( fftRepo.getPlan( plHandle, fftPlan, planLock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/accessors.cpp:	OPENCL_V( fftRepo.getPlan( plHandle, fftPlan, planLock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/accessors.cpp:	OPENCL_V( fftRepo.getPlan( plHandle, fftPlan, planLock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/accessors.cpp:	OPENCL_V( fftRepo.getPlan( plHandle, fftPlan, planLock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/accessors.cpp:	OPENCL_V( fftRepo.getPlan( plHandle, fftPlan, planLock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/accessors.cpp:	OPENCL_V( repo.getPlan( plHandle, plan, lock ), _T( "repo.getPlan failed" ) );
src/external/clFFT/src/library/accessors.cpp:	OPENCL_V( fftRepo.getPlan( plHandle, fftPlan, planLock ), _T( "fftRepo.getPlan failed" ) );
src/external/clFFT/src/library/lifetime.cpp:			fftRepo.pStatTimer = reinterpret_cast< GpuStatTimer* > ( pfGetStatTimer( CLFFT_GPU ) );
src/external/colvars/colvaratoms.h: *          branch prediction is broken (or further migration to GPU code).
src/external/colvars/colvarmodule_refs.h:    "  title = {Scalable molecular dynamics on {CPU} and {GPU} architectures with {NAMD}},\n"
src/gromacs/math/utilities.cpp:#elif GMX_GPU_SYCL
src/gromacs/math/include/gromacs/math/arrayrefwithpadding.h:     * \todo Use std::is_same_v when CUDA 11 is a requirement.
src/gromacs/math/include/gromacs/math/paddedvector.h:     * \todo Use std::is_same_v when CUDA 11 is a requirement.
src/gromacs/math/include/gromacs/math/paddedvector.h:     * \todo Use std::is_same_v when CUDA 11 is a requirement.
src/gromacs/mdlib/mdatoms.cpp:#include "gromacs/gpu_utils/hostallocator.h"
src/gromacs/mdlib/mdatoms.cpp:std::unique_ptr<MDAtoms> makeMDAtoms(FILE* fp, const gmx_mtop_t& mtop, const t_inputrec& ir, const bool rankHasPmeGpuTask)
src/gromacs/mdlib/mdatoms.cpp:    // GPU transfers may want to use a suitable pinning mode.
src/gromacs/mdlib/mdatoms.cpp:    if (rankHasPmeGpuTask)
src/gromacs/mdlib/leapfrog_gpu.h: * \brief Declarations for GPU implementation of Leap-Frog.
src/gromacs/mdlib/leapfrog_gpu.h:#ifndef GMX_MDLIB_LEAPFROG_GPU_H
src/gromacs/mdlib/leapfrog_gpu.h:#define GMX_MDLIB_LEAPFROG_GPU_H
src/gromacs/mdlib/leapfrog_gpu.h:#if GMX_GPU_CUDA
src/gromacs/mdlib/leapfrog_gpu.h:#    include "gromacs/gpu_utils/gputraits.cuh"
src/gromacs/mdlib/leapfrog_gpu.h:#if GMX_GPU_SYCL
src/gromacs/mdlib/leapfrog_gpu.h:#    include "gromacs/gpu_utils/gputraits_sycl.h"
src/gromacs/mdlib/leapfrog_gpu.h:#include "gromacs/gpu_utils/devicebuffer_datatype.h"
src/gromacs/mdlib/leapfrog_gpu.h:#include "gromacs/gpu_utils/hostallocator.h"
src/gromacs/mdlib/leapfrog_gpu.h:class LeapFrogGpu
src/gromacs/mdlib/leapfrog_gpu.h:    LeapFrogGpu(const DeviceContext& deviceContext, const DeviceStream& deviceStream, int numTempScaleValues);
src/gromacs/mdlib/leapfrog_gpu.h:    ~LeapFrogGpu();
src/gromacs/mdlib/leapfrog_gpu.h:     * Updates coordinates and velocities on the GPU. The current coordinates are saved for constraints.
src/gromacs/mdlib/leapfrog_gpu.h:     * to the GPU.
src/gromacs/mdlib/leapfrog_gpu.h:    //! GPU context object
src/gromacs/mdlib/leapfrog_gpu.h:    //! GPU stream
src/gromacs/mdlib/leapfrog_gpu.h:    //! 1/mass for all atoms (GPU)
src/gromacs/mdlib/gpuforcereduction_impl_internal.cu: * \brief Implements GPU Force Reduction using CUDA
src/gromacs/mdlib/gpuforcereduction_impl_internal.cu: * \author Alan Gray <alang@nvidia.com>
src/gromacs/mdlib/gpuforcereduction_impl_internal.cu:#include "gpuforcereduction_impl_internal.h"
src/gromacs/mdlib/gpuforcereduction_impl_internal.cu:#include "gromacs/gpu_utils/cudautils.cuh"
src/gromacs/mdlib/gpuforcereduction_impl_internal.cu:#include "gromacs/gpu_utils/devicebuffer.h"
src/gromacs/mdlib/gpuforcereduction_impl_internal.cu:#include "gromacs/gpu_utils/typecasts_cuda_hip.h"
src/gromacs/mdlib/gpuforcereduction_impl_internal.cu:#include "gromacs/gpu_utils/vectype_ops_cuda.h"
src/gromacs/mdlib/gpuforcereduction_impl_internal.cu:    // map particle-level parallelism to 1D CUDA thread and block index
src/gromacs/mdlib/gpuforcereduction_impl_internal.cu:    const auto kernelArgs = prepareGpuKernelArguments(kernelFn,
src/gromacs/mdlib/gpuforcereduction_impl_internal.cu:    launchGpuKernel(kernelFn, config, deviceStream, nullptr, "Force Reduction", kernelArgs);
src/gromacs/mdlib/leapfrog_gpu_internal.h: * \brief Declarations for backend specific GPU functions for Leap-Frog.
src/gromacs/mdlib/leapfrog_gpu_internal.h:#ifndef GMX_MDLIB_LEAPFROG_GPU_INTERNAL_H
src/gromacs/mdlib/leapfrog_gpu_internal.h:#define GMX_MDLIB_LEAPFROG_GPU_INTERNAL_H
src/gromacs/mdlib/leapfrog_gpu_internal.h:#include "gromacs/gpu_utils/devicebuffer_datatype.h"
src/gromacs/mdlib/leapfrog_gpu_internal.h:#include "gromacs/mdlib/leapfrog_gpu.h"
src/gromacs/mdlib/leapfrog_gpu_internal.h:/*! \brief Backend-specific function to launch GPU Leap Frog kernel.
src/gromacs/mdlib/force.h:     * of whether the PME-mesh contribution is computed on a separate PME rank or on a GPU.
src/gromacs/mdlib/calc_verletbuf.h:    Gpu                   /* GPU (8x2x)8x4 list */
src/gromacs/mdlib/mdgraph_gpu_impl.cu: * \author Alan Gray <alang@nvidia.com>
src/gromacs/mdlib/mdgraph_gpu_impl.cu:#include "mdgraph_gpu_impl.h"
src/gromacs/mdlib/mdgraph_gpu_impl.cu:#include "gromacs/gpu_utils/device_stream.h"
src/gromacs/mdlib/mdgraph_gpu_impl.cu:#include "gromacs/gpu_utils/gpueventsynchronizer.h"
src/gromacs/mdlib/mdgraph_gpu_impl.cu:#if GMX_HAVE_GPU_GRAPH_SUPPORT
src/gromacs/mdlib/mdgraph_gpu_impl.cu:MdGpuGraph::Impl::Impl(const DeviceStreamManager& deviceStreamManager,
src/gromacs/mdlib/mdgraph_gpu_impl.cu:    haveGpuPmeOnThisPpRank_(simulationWork.haveGpuPmeOnPpRank()),
src/gromacs/mdlib/mdgraph_gpu_impl.cu:    helperEvent_           = std::make_unique<GpuEventSynchronizer>();
src/gromacs/mdlib/mdgraph_gpu_impl.cu:    ppTaskCompletionEvent_ = std::make_unique<GpuEventSynchronizer>();
src/gromacs/mdlib/mdgraph_gpu_impl.cu:    // Avoid update for graphs involving inter-GPU transfers if running on old driver
src/gromacs/mdlib/mdgraph_gpu_impl.cu:    // since, due to a performance bug, the updated graph implements these as GPU-initiated
src/gromacs/mdlib/mdgraph_gpu_impl.cu:    cudaError_t stat          = cudaDriverGetVersion(&driverVersion);
src/gromacs/mdlib/mdgraph_gpu_impl.cu:    CU_RET_ERR(stat, "cudaDriverGetVersion in MD graph definition finalization failed.");
src/gromacs/mdlib/mdgraph_gpu_impl.cu:MdGpuGraph::Impl::~Impl()
src/gromacs/mdlib/mdgraph_gpu_impl.cu:    cudaError_t stat = cudaDeviceSynchronize();
src/gromacs/mdlib/mdgraph_gpu_impl.cu:    CU_RET_ERR(stat, "cudaDeviceSynchronize during MD graph cleanup failed.");
src/gromacs/mdlib/mdgraph_gpu_impl.cu:        stat = cudaGraphDestroy(graph_);
src/gromacs/mdlib/mdgraph_gpu_impl.cu:        CU_RET_ERR(stat, "cudaGraphDestroy during MD graph cleanup failed.");
src/gromacs/mdlib/mdgraph_gpu_impl.cu:        stat = cudaGraphExecDestroy(instance_);
src/gromacs/mdlib/mdgraph_gpu_impl.cu:        CU_RET_ERR(stat, "cudaGraphExecDestroy during MD graph cleanup failed.");
src/gromacs/mdlib/mdgraph_gpu_impl.cu:void MdGpuGraph::Impl::enqueueEventFromAllPpRanksToRank0Stream(GpuEventSynchronizer* event,
src/gromacs/mdlib/mdgraph_gpu_impl.cu:                     sizeof(GpuEventSynchronizer*), //NOLINT(bugprone-sizeof-expression)
src/gromacs/mdlib/mdgraph_gpu_impl.cu:            GpuEventSynchronizer* eventToEnqueue;
src/gromacs/mdlib/mdgraph_gpu_impl.cu:                     sizeof(GpuEventSynchronizer*), //NOLINT(bugprone-sizeof-expression)
src/gromacs/mdlib/mdgraph_gpu_impl.cu:void MdGpuGraph::Impl::enqueueRank0EventToAllPpStreams(GpuEventSynchronizer* event, const DeviceStream& stream)
src/gromacs/mdlib/mdgraph_gpu_impl.cu:        MPI_Bcast(&event, sizeof(GpuEventSynchronizer*), MPI_BYTE, 0, mpiComm_);
src/gromacs/mdlib/mdgraph_gpu_impl.cu:void MdGpuGraph::Impl::reset()
src/gromacs/mdlib/mdgraph_gpu_impl.cu:void MdGpuGraph::Impl::disableForDomainIfAnyPpRankHasCpuForces(bool disableGraphAcrossAllPpRanks)
src/gromacs/mdlib/mdgraph_gpu_impl.cu:bool MdGpuGraph::Impl::captureThisStep(bool canUseGraphThisStep)
src/gromacs/mdlib/mdgraph_gpu_impl.cu:void MdGpuGraph::Impl::setUsedGraphLastStep(bool usedGraphLastStep)
src/gromacs/mdlib/mdgraph_gpu_impl.cu:void MdGpuGraph::Impl::startRecord(GpuEventSynchronizer* xReadyOnDeviceEvent)
src/gromacs/mdlib/mdgraph_gpu_impl.cu:    wallcycle_sub_start(wcycle_, WallCycleSubCounter::MdGpuGraphWaitBeforeCapture);
src/gromacs/mdlib/mdgraph_gpu_impl.cu:    wallcycle_sub_stop(wcycle_, WallCycleSubCounter::MdGpuGraphWaitBeforeCapture);
src/gromacs/mdlib/mdgraph_gpu_impl.cu:    wallcycle_start(wcycle_, WallCycleCounter::MdGpuGraph);
src/gromacs/mdlib/mdgraph_gpu_impl.cu:    wallcycle_sub_start(wcycle_, WallCycleSubCounter::MdGpuGraphCapture);
src/gromacs/mdlib/mdgraph_gpu_impl.cu:        cudaError_t stat = cudaStreamBeginCapture(
src/gromacs/mdlib/mdgraph_gpu_impl.cu:                cudaStreamCaptureModeGlobal);
src/gromacs/mdlib/mdgraph_gpu_impl.cu:        CU_RET_ERR(stat, "cudaStreamBeginCapture in MD graph definition initialization failed.");
src/gromacs/mdlib/mdgraph_gpu_impl.cu:    // GPU-side sync is only required to define the graph, and its
src/gromacs/mdlib/mdgraph_gpu_impl.cu:void MdGpuGraph::Impl::endRecord()
src/gromacs/mdlib/mdgraph_gpu_impl.cu:    if (haveGpuPmeOnThisPpRank_)
src/gromacs/mdlib/mdgraph_gpu_impl.cu:    // GPU-side sync is only required to define the graph, and its
src/gromacs/mdlib/mdgraph_gpu_impl.cu:            cudaError_t stat = cudaGraphDestroy(graph_);
src/gromacs/mdlib/mdgraph_gpu_impl.cu:            CU_RET_ERR(stat, "cudaGraphDestroy in MD graph definition finalization failed.");
src/gromacs/mdlib/mdgraph_gpu_impl.cu:        cudaError_t stat = cudaStreamEndCapture(
src/gromacs/mdlib/mdgraph_gpu_impl.cu:        CU_RET_ERR(stat, "cudaStreamEndCapture in MD graph definition finalization failed.");
src/gromacs/mdlib/mdgraph_gpu_impl.cu:    wallcycle_sub_stop(wcycle_, WallCycleSubCounter::MdGpuGraphCapture);
src/gromacs/mdlib/mdgraph_gpu_impl.cu:    wallcycle_stop(wcycle_, WallCycleCounter::MdGpuGraph);
src/gromacs/mdlib/mdgraph_gpu_impl.cu:void MdGpuGraph::Impl::createExecutableGraph(bool forceGraphReinstantiation)
src/gromacs/mdlib/mdgraph_gpu_impl.cu:    wallcycle_start(wcycle_, WallCycleCounter::MdGpuGraph);
src/gromacs/mdlib/mdgraph_gpu_impl.cu:    wallcycle_sub_start(wcycle_, WallCycleSubCounter::MdGpuGraphInstantiateOrUpdate);
src/gromacs/mdlib/mdgraph_gpu_impl.cu:#    if CUDART_VERSION >= 12000
src/gromacs/mdlib/mdgraph_gpu_impl.cu:            cudaGraphExecUpdateResultInfo updateResultInfo_out;
src/gromacs/mdlib/mdgraph_gpu_impl.cu:            cudaError_t stat = cudaGraphExecUpdate(instance_, graph_, &updateResultInfo_out);
src/gromacs/mdlib/mdgraph_gpu_impl.cu:                    (updateResultInfo_out.result == cudaGraphExecUpdateErrorTopologyChanged);
src/gromacs/mdlib/mdgraph_gpu_impl.cu:            cudaGraphNode_t           hErrorNode_out;
src/gromacs/mdlib/mdgraph_gpu_impl.cu:            cudaGraphExecUpdateResult updateResult_out;
src/gromacs/mdlib/mdgraph_gpu_impl.cu:            cudaError_t stat = cudaGraphExecUpdate(instance_, graph_, &hErrorNode_out, &updateResult_out);
src/gromacs/mdlib/mdgraph_gpu_impl.cu:            if ((stat == cudaErrorGraphExecUpdateFailure)
src/gromacs/mdlib/mdgraph_gpu_impl.cu:                // different ordering, which in a minority of cases CUDA wrongly interprets as being
src/gromacs/mdlib/mdgraph_gpu_impl.cu:                stat = cudaSuccess;
src/gromacs/mdlib/mdgraph_gpu_impl.cu:                cudaGetLastError();
src/gromacs/mdlib/mdgraph_gpu_impl.cu:            CU_RET_ERR(stat, "cudaGraphExecUpdate in MD graph definition finalization failed.");
src/gromacs/mdlib/mdgraph_gpu_impl.cu:                cudaError_t stat = cudaGraphExecDestroy(instance_);
src/gromacs/mdlib/mdgraph_gpu_impl.cu:                CU_RET_ERR(stat, "cudaGraphExecDestroy in MD graph definition finalization failed.");
src/gromacs/mdlib/mdgraph_gpu_impl.cu:            // Instantiate using existing CUDA stream priorities for relative node priorities within graph
src/gromacs/mdlib/mdgraph_gpu_impl.cu:            cudaError_t stat = cudaGraphInstantiate(&instance_,
src/gromacs/mdlib/mdgraph_gpu_impl.cu:#    if CUDART_VERSION >= 12000
src/gromacs/mdlib/mdgraph_gpu_impl.cu:                                                    cudaGraphInstantiateFlagUseNodePriority
src/gromacs/mdlib/mdgraph_gpu_impl.cu:            CU_RET_ERR(stat, "cudaGraphInstantiate in MD graph definition finalization failed.");
src/gromacs/mdlib/mdgraph_gpu_impl.cu:    wallcycle_sub_stop(wcycle_, WallCycleSubCounter::MdGpuGraphInstantiateOrUpdate);
src/gromacs/mdlib/mdgraph_gpu_impl.cu:    wallcycle_stop(wcycle_, WallCycleCounter::MdGpuGraph);
src/gromacs/mdlib/mdgraph_gpu_impl.cu:void MdGpuGraph::Impl::launchGraphMdStep(GpuEventSynchronizer* xUpdatedOnDeviceEvent)
src/gromacs/mdlib/mdgraph_gpu_impl.cu:    wallcycle_sub_start(wcycle_, WallCycleSubCounter::MdGpuGraphWaitBeforeLaunch);
src/gromacs/mdlib/mdgraph_gpu_impl.cu:    wallcycle_sub_stop(wcycle_, WallCycleSubCounter::MdGpuGraphWaitBeforeLaunch);
src/gromacs/mdlib/mdgraph_gpu_impl.cu:    wallcycle_start(wcycle_, WallCycleCounter::MdGpuGraph);
src/gromacs/mdlib/mdgraph_gpu_impl.cu:    wallcycle_sub_start(wcycle_, WallCycleSubCounter::MdGpuGraphLaunch);
src/gromacs/mdlib/mdgraph_gpu_impl.cu:    // alternate stream to allow overlap of extra artificial inter-GPU
src/gromacs/mdlib/mdgraph_gpu_impl.cu:    // waits on all GPU streams, across all GPUs, from the previous step. First sync locally on each
src/gromacs/mdlib/mdgraph_gpu_impl.cu:    // GPU to the local launchStream, and then sync each local launchStream to the main launchStream
src/gromacs/mdlib/mdgraph_gpu_impl.cu:        if (haveGpuPmeOnThisPpRank_)
src/gromacs/mdlib/mdgraph_gpu_impl.cu:        // Sync remote GPUs to main rank 0 GPU which will launch graph
src/gromacs/mdlib/mdgraph_gpu_impl.cu:        cudaError_t stat = cudaGraphLaunch(instance_, thisLaunchStream->stream());
src/gromacs/mdlib/mdgraph_gpu_impl.cu:        CU_RET_ERR(stat, "cudaGraphLaunch in MD graph definition finalization failed.");
src/gromacs/mdlib/mdgraph_gpu_impl.cu:    wallcycle_sub_stop(wcycle_, WallCycleSubCounter::MdGpuGraphLaunch);
src/gromacs/mdlib/mdgraph_gpu_impl.cu:    wallcycle_stop(wcycle_, WallCycleCounter::MdGpuGraph);
src/gromacs/mdlib/mdgraph_gpu_impl.cu:void MdGpuGraph::Impl::setAlternateStepPpTaskCompletionEvent(GpuEventSynchronizer* event)
src/gromacs/mdlib/mdgraph_gpu_impl.cu:GpuEventSynchronizer* MdGpuGraph::Impl::getPpTaskCompletionEvent()
src/gromacs/mdlib/mdgraph_gpu_impl.cu:MdGpuGraph::MdGpuGraph(const DeviceStreamManager& deviceStreamManager,
src/gromacs/mdlib/mdgraph_gpu_impl.cu:MdGpuGraph::~MdGpuGraph() = default;
src/gromacs/mdlib/mdgraph_gpu_impl.cu:void MdGpuGraph::reset()
src/gromacs/mdlib/mdgraph_gpu_impl.cu:void MdGpuGraph::disableForDomainIfAnyPpRankHasCpuForces(bool disableGraphAcrossAllPpRanks)
src/gromacs/mdlib/mdgraph_gpu_impl.cu:bool MdGpuGraph::captureThisStep(bool canUseGraphThisStep)
src/gromacs/mdlib/mdgraph_gpu_impl.cu:void MdGpuGraph::setUsedGraphLastStep(bool usedGraphLastStep)
src/gromacs/mdlib/mdgraph_gpu_impl.cu:void MdGpuGraph::startRecord(GpuEventSynchronizer* xReadyOnDeviceEvent)
src/gromacs/mdlib/mdgraph_gpu_impl.cu:void MdGpuGraph::endRecord()
src/gromacs/mdlib/mdgraph_gpu_impl.cu:void MdGpuGraph::createExecutableGraph(bool forceGraphReinstantiation)
src/gromacs/mdlib/mdgraph_gpu_impl.cu:void MdGpuGraph::launchGraphMdStep(GpuEventSynchronizer* xUpdatedOnDeviceEvent)
src/gromacs/mdlib/mdgraph_gpu_impl.cu:bool MdGpuGraph::useGraphThisStep() const
src/gromacs/mdlib/mdgraph_gpu_impl.cu:bool MdGpuGraph::graphIsCapturingThisStep() const
src/gromacs/mdlib/mdgraph_gpu_impl.cu:void MdGpuGraph::setAlternateStepPpTaskCompletionEvent(GpuEventSynchronizer* event)
src/gromacs/mdlib/mdgraph_gpu_impl.cu:GpuEventSynchronizer* MdGpuGraph::getPpTaskCompletionEvent()
src/gromacs/mdlib/mdgraph_gpu_impl.cu:#endif // GMX_HAVE_GPU_GRAPH_SUPPORT
src/gromacs/mdlib/settle_gpu_internal_sycl.cpp: * \brief SYCL-specific routines for the GPU implementation of SETTLE constraints algorithm.
src/gromacs/mdlib/settle_gpu_internal_sycl.cpp:#include "gromacs/gpu_utils/devicebuffer.h"
src/gromacs/mdlib/settle_gpu_internal_sycl.cpp:#include "gromacs/gpu_utils/sycl_kernel_utils.h"
src/gromacs/mdlib/settle_gpu_internal_sycl.cpp:#include "settle_gpu_internal.h"
src/gromacs/mdlib/settle_gpu_internal_sycl.cpp:void launchSettleGpuKernel(const int                          numSettles,
src/gromacs/mdlib/leapfrog_gpu.cpp: * \brief Implements Leap-Frog using CUDA
src/gromacs/mdlib/leapfrog_gpu.cpp: * This file contains backend-agnostic code for Leap-Frog integrator class on GPU,
src/gromacs/mdlib/leapfrog_gpu.cpp:#include "leapfrog_gpu.h"
src/gromacs/mdlib/leapfrog_gpu.cpp:#include "gromacs/gpu_utils/devicebuffer.h"
src/gromacs/mdlib/leapfrog_gpu.cpp:#include "gromacs/mdlib/leapfrog_gpu_internal.h"
src/gromacs/mdlib/leapfrog_gpu.cpp:void LeapFrogGpu::integrate(DeviceBuffer<Float3>              d_x,
src/gromacs/mdlib/leapfrog_gpu.cpp:                           GpuApiCallBehavior::Async,
src/gromacs/mdlib/leapfrog_gpu.cpp:                   "in GPU version of Leap-Frog integrator.");
src/gromacs/mdlib/leapfrog_gpu.cpp:LeapFrogGpu::LeapFrogGpu(const DeviceContext& deviceContext,
src/gromacs/mdlib/leapfrog_gpu.cpp:LeapFrogGpu::~LeapFrogGpu()
src/gromacs/mdlib/leapfrog_gpu.cpp:void LeapFrogGpu::set(const int                            numAtoms,
src/gromacs/mdlib/leapfrog_gpu.cpp:            &d_inverseMasses_, inverseMasses.data(), 0, numAtoms_, deviceStream_, GpuApiCallBehavior::Sync, nullptr);
src/gromacs/mdlib/leapfrog_gpu.cpp:                           GpuApiCallBehavior::Sync,
src/gromacs/mdlib/resethandler.cpp:#include "gromacs/gpu_utils/gpu_utils.h"
src/gromacs/mdlib/resethandler.cpp:#include "gromacs/nbnxm/gpu_data_mgmt.h"
src/gromacs/mdlib/resethandler.cpp:        if (nbv && nbv->useGpu())
src/gromacs/mdlib/resethandler.cpp:            gpu_reset_timings(nbv);
src/gromacs/mdlib/resethandler.cpp:        if (pme_gpu_task_enabled(pme))
src/gromacs/mdlib/resethandler.cpp:            pme_gpu_reset_timings(pme);
src/gromacs/mdlib/resethandler.cpp:        if ((nbv && nbv->useGpu()) || pme_gpu_task_enabled(pme))
src/gromacs/mdlib/resethandler.cpp:            resetGpuProfiler();
src/gromacs/mdlib/tests/settletestrunners.h: * class is used to unify the interfaces for CPU and GPU implementations of the
src/gromacs/mdlib/tests/settletestrunners.h: * GPU version of SETTLE is only available with CUDA.
src/gromacs/mdlib/tests/settletestrunners.h:#define GPU_SETTLE_SUPPORTED (GMX_GPU_CUDA || GMX_GPU_SYCL)
src/gromacs/mdlib/tests/settletestrunners.h:// Runner for the GPU implementation of SETTLE.
src/gromacs/mdlib/tests/settletestrunners.h:    /*! \brief Apply SETTLE using GPU version of the algorithm
src/gromacs/mdlib/tests/settletestrunners.h:     * Initializes SETTLE object, copied data to the GPU, applies algorithm, copies the data back,
src/gromacs/mdlib/tests/settletestrunners.h:     * \returns A string with GPU description.
src/gromacs/mdlib/tests/constrtestdata.h: * declares CPU- and GPU-based functions used to apply SHAKE or LINCS on the
src/gromacs/mdlib/tests/leapfrogtestdata.cpp:#include "gromacs/gpu_utils/gpu_utils.h"
src/gromacs/mdlib/tests/settletestrunners_gpu.cpp: * \brief Defines the runner for CUDA version of SETTLE.
src/gromacs/mdlib/tests/settletestrunners_gpu.cpp:#if GPU_SETTLE_SUPPORTED
src/gromacs/mdlib/tests/settletestrunners_gpu.cpp:#    include "gromacs/gpu_utils/devicebuffer.h"
src/gromacs/mdlib/tests/settletestrunners_gpu.cpp:#include "gromacs/gpu_utils/gputraits.h"
src/gromacs/mdlib/tests/settletestrunners_gpu.cpp:#include "gromacs/mdlib/settle_gpu.h"
src/gromacs/mdlib/tests/settletestrunners_gpu.cpp:#if GPU_SETTLE_SUPPORTED
src/gromacs/mdlib/tests/settletestrunners_gpu.cpp:    auto settleGpu = std::make_unique<SettleGpu>(testData->mtop_, deviceContext, deviceStream);
src/gromacs/mdlib/tests/settletestrunners_gpu.cpp:    settleGpu->set(*testData->idef_);
src/gromacs/mdlib/tests/settletestrunners_gpu.cpp:    copyToDeviceBuffer(&d_x, h_x, 0, numAtoms, deviceStream, GpuApiCallBehavior::Sync, nullptr);
src/gromacs/mdlib/tests/settletestrunners_gpu.cpp:    copyToDeviceBuffer(&d_xp, h_xp, 0, numAtoms, deviceStream, GpuApiCallBehavior::Sync, nullptr);
src/gromacs/mdlib/tests/settletestrunners_gpu.cpp:        copyToDeviceBuffer(&d_v, h_v, 0, numAtoms, deviceStream, GpuApiCallBehavior::Sync, nullptr);
src/gromacs/mdlib/tests/settletestrunners_gpu.cpp:    settleGpu->apply(
src/gromacs/mdlib/tests/settletestrunners_gpu.cpp:    copyFromDeviceBuffer(h_xp, &d_xp, 0, numAtoms, deviceStream, GpuApiCallBehavior::Sync, nullptr);
src/gromacs/mdlib/tests/settletestrunners_gpu.cpp:        copyFromDeviceBuffer(h_v, &d_v, 0, numAtoms, deviceStream, GpuApiCallBehavior::Sync, nullptr);
src/gromacs/mdlib/tests/settletestrunners_gpu.cpp:#else // GPU_SETTLE_SUPPORTED
src/gromacs/mdlib/tests/settletestrunners_gpu.cpp:    FAIL() << "Dummy SETTLE GPU function was called instead of the real one in the SETTLE test.";
src/gromacs/mdlib/tests/settletestrunners_gpu.cpp:#endif // GPU_SETTLE_SUPPORTED
src/gromacs/mdlib/tests/constrtestrunners_gpu.cpp: * \brief Subroutines to run LINCS on GPU
src/gromacs/mdlib/tests/constrtestrunners_gpu.cpp: * Copies data to GPU, runs LINCS and copies the results back.
src/gromacs/mdlib/tests/constrtestrunners_gpu.cpp:#if GPU_CONSTRAINTS_SUPPORTED
src/gromacs/mdlib/tests/constrtestrunners_gpu.cpp:#    include "gromacs/gpu_utils/devicebuffer.h"
src/gromacs/mdlib/tests/constrtestrunners_gpu.cpp:#include "gromacs/gpu_utils/gputraits.h"
src/gromacs/mdlib/tests/constrtestrunners_gpu.cpp:#include "gromacs/mdlib/lincs_gpu.h"
src/gromacs/mdlib/tests/constrtestrunners_gpu.cpp:#if GPU_CONSTRAINTS_SUPPORTED
src/gromacs/mdlib/tests/constrtestrunners_gpu.cpp:    auto lincsGpu = std::make_unique<LincsGpu>(
src/gromacs/mdlib/tests/constrtestrunners_gpu.cpp:    lincsGpu->set(*testData->idef_, testData->numAtoms_, testData->invmass_);
src/gromacs/mdlib/tests/constrtestrunners_gpu.cpp:    copyToDeviceBuffer(&d_x, h_x, 0, numAtoms, deviceStream, GpuApiCallBehavior::Sync, nullptr);
src/gromacs/mdlib/tests/constrtestrunners_gpu.cpp:    copyToDeviceBuffer(&d_xp, h_xp, 0, numAtoms, deviceStream, GpuApiCallBehavior::Sync, nullptr);
src/gromacs/mdlib/tests/constrtestrunners_gpu.cpp:        copyToDeviceBuffer(&d_v, h_v, 0, numAtoms, deviceStream, GpuApiCallBehavior::Sync, nullptr);
src/gromacs/mdlib/tests/constrtestrunners_gpu.cpp:    lincsGpu->apply(
src/gromacs/mdlib/tests/constrtestrunners_gpu.cpp:    copyFromDeviceBuffer(h_xp, &d_xp, 0, numAtoms, deviceStream, GpuApiCallBehavior::Sync, nullptr);
src/gromacs/mdlib/tests/constrtestrunners_gpu.cpp:        copyFromDeviceBuffer(h_v, &d_v, 0, numAtoms, deviceStream, GpuApiCallBehavior::Sync, nullptr);
src/gromacs/mdlib/tests/constrtestrunners_gpu.cpp:#else // GPU_CONSTRAINTS_SUPPORTED
src/gromacs/mdlib/tests/constrtestrunners_gpu.cpp:    FAIL() << "Dummy LINCS GPU function was called instead of the real one.";
src/gromacs/mdlib/tests/constrtestrunners_gpu.cpp:#endif // GPU_CONSTRAINTS_SUPPORTED
src/gromacs/mdlib/tests/constrtestdata.cpp: *       reduction, etc. on the GPU.
src/gromacs/mdlib/tests/mdgpugraph.cpp: * \brief Tests for MD GPU graph
src/gromacs/mdlib/tests/mdgpugraph.cpp: * \author Alan Gray <alang@nvidia.com>
src/gromacs/mdlib/tests/mdgpugraph.cpp:#if GMX_HAVE_GPU_GRAPH_SUPPORT
src/gromacs/mdlib/tests/mdgpugraph.cpp:#    include "gromacs/gpu_utils/device_stream.h"
src/gromacs/mdlib/tests/mdgpugraph.cpp:#    include "gromacs/gpu_utils/device_stream_manager.h"
src/gromacs/mdlib/tests/mdgpugraph.cpp:#    include "gromacs/gpu_utils/devicebuffer.h"
src/gromacs/mdlib/tests/mdgpugraph.cpp:#    include "gromacs/gpu_utils/gpueventsynchronizer.h"
src/gromacs/mdlib/tests/mdgpugraph.cpp:#    include "gromacs/gpu_utils/hostallocator.h"
src/gromacs/mdlib/tests/mdgpugraph.cpp:#    include "gromacs/mdlib/mdgraph_gpu.h"
src/gromacs/mdlib/tests/mdgpugraph.cpp: * useMdGpuGraph is true to enable the graph, useGpuUpdate is true to
src/gromacs/mdlib/tests/mdgpugraph.cpp: * tests, and useGpuPme is false since interoperation with the PME
src/gromacs/mdlib/tests/mdgpugraph.cpp:SimulationWorkload makeSimulationWorkForMdGpuGraph()
src/gromacs/mdlib/tests/mdgpugraph.cpp:    simulationWork.useMdGpuGraph             = true;
src/gromacs/mdlib/tests/mdgpugraph.cpp:    simulationWork.useGpuUpdate              = true;
src/gromacs/mdlib/tests/mdgpugraph.cpp:    simulationWork.useGpuPme                 = false;
src/gromacs/mdlib/tests/mdgpugraph.cpp:TEST(MdGraphTest, MdGpuGraphExecutesActivities)
src/gromacs/mdlib/tests/mdgpugraph.cpp:        GTEST_SKIP() << "No compatible GPUs to test on.";
src/gromacs/mdlib/tests/mdgpugraph.cpp:#    if GMX_GPU_SYCL
src/gromacs/mdlib/tests/mdgpugraph.cpp:            GTEST_SKIP() << "Device does not support GPU Graphs";
src/gromacs/mdlib/tests/mdgpugraph.cpp:        SimulationWorkload   simulationWork = makeSimulationWorkForMdGpuGraph();
src/gromacs/mdlib/tests/mdgpugraph.cpp:        GpuEventSynchronizer xReadyOnDeviceEvent;
src/gromacs/mdlib/tests/mdgpugraph.cpp:        GpuEventSynchronizer xUpdatedOnDeviceEvent;
src/gromacs/mdlib/tests/mdgpugraph.cpp:        gmx::MdGpuGraph      mdGpuGraph(
src/gromacs/mdlib/tests/mdgpugraph.cpp:        gmx::MdGpuGraph mdGpuGraphAlternate(
src/gromacs/mdlib/tests/mdgpugraph.cpp:        mdGpuGraph.setAlternateStepPpTaskCompletionEvent(mdGpuGraphAlternate.getPpTaskCompletionEvent());
src/gromacs/mdlib/tests/mdgpugraph.cpp:            // Set output to 1 on GPU
src/gromacs/mdlib/tests/mdgpugraph.cpp:                               GpuApiCallBehavior::Sync,
src/gromacs/mdlib/tests/mdgpugraph.cpp:            if (useGraph && mdGpuGraph.captureThisStep(true)) // denote start of graph region
src/gromacs/mdlib/tests/mdgpugraph.cpp:                mdGpuGraph.setUsedGraphLastStep(usedGraphLastStep);
src/gromacs/mdlib/tests/mdgpugraph.cpp:                mdGpuGraph.startRecord(&xReadyOnDeviceEvent);
src/gromacs/mdlib/tests/mdgpugraph.cpp:            // Clear output on GPU in update stream, which will be automatically forked from
src/gromacs/mdlib/tests/mdgpugraph.cpp:            copyBetweenDeviceBuffers(&d_output, &d_staging, 1, stream, GpuApiCallBehavior::Async, nullptr);
src/gromacs/mdlib/tests/mdgpugraph.cpp:            if (mdGpuGraph.graphIsCapturingThisStep()) // denote end of graph region
src/gromacs/mdlib/tests/mdgpugraph.cpp:                mdGpuGraph.endRecord();
src/gromacs/mdlib/tests/mdgpugraph.cpp:                mdGpuGraph.createExecutableGraph(forceGraphReinstantiation);
src/gromacs/mdlib/tests/mdgpugraph.cpp:                                 GpuApiCallBehavior::Sync,
src/gromacs/mdlib/tests/mdgpugraph.cpp:            if (mdGpuGraph.useGraphThisStep())
src/gromacs/mdlib/tests/mdgpugraph.cpp:            if (mdGpuGraph.useGraphThisStep())
src/gromacs/mdlib/tests/mdgpugraph.cpp:                mdGpuGraph.launchGraphMdStep(&xUpdatedOnDeviceEvent);
src/gromacs/mdlib/tests/mdgpugraph.cpp:                                     GpuApiCallBehavior::Sync,
src/gromacs/mdlib/tests/mdgpugraph.cpp:TEST(MdGraphTest, MdGpuGraphCaptureAndUsageConsistency)
src/gromacs/mdlib/tests/mdgpugraph.cpp:        GTEST_SKIP() << "No compatible GPUs to test on.";
src/gromacs/mdlib/tests/mdgpugraph.cpp:#    if GMX_GPU_SYCL
src/gromacs/mdlib/tests/mdgpugraph.cpp:            GTEST_SKIP() << "Device does not support GPU Graphs";
src/gromacs/mdlib/tests/mdgpugraph.cpp:        SimulationWorkload   simulationWork = makeSimulationWorkForMdGpuGraph();
src/gromacs/mdlib/tests/mdgpugraph.cpp:        GpuEventSynchronizer xReadyOnDeviceEvent;
src/gromacs/mdlib/tests/mdgpugraph.cpp:        GpuEventSynchronizer xUpdatedOnDeviceEvent;
src/gromacs/mdlib/tests/mdgpugraph.cpp:        gmx::MdGpuGraph      mdGpuGraph(
src/gromacs/mdlib/tests/mdgpugraph.cpp:        gmx::MdGpuGraph mdGpuGraphAlternate(
src/gromacs/mdlib/tests/mdgpugraph.cpp:        mdGpuGraph.setAlternateStepPpTaskCompletionEvent(mdGpuGraphAlternate.getPpTaskCompletionEvent());
src/gromacs/mdlib/tests/mdgpugraph.cpp:        mdGpuGraph.setUsedGraphLastStep(usedGraphLastStep);
src/gromacs/mdlib/tests/mdgpugraph.cpp:        ASSERT_FALSE(mdGpuGraph.captureThisStep(canUseGraphThisStep));
src/gromacs/mdlib/tests/mdgpugraph.cpp:        ASSERT_FALSE(mdGpuGraph.graphIsCapturingThisStep());
src/gromacs/mdlib/tests/mdgpugraph.cpp:        ASSERT_FALSE(mdGpuGraph.useGraphThisStep());
src/gromacs/mdlib/tests/mdgpugraph.cpp:        ASSERT_TRUE(mdGpuGraph.captureThisStep(canUseGraphThisStep));
src/gromacs/mdlib/tests/mdgpugraph.cpp:        mdGpuGraph.startRecord(&xReadyOnDeviceEvent);
src/gromacs/mdlib/tests/mdgpugraph.cpp:        ASSERT_TRUE(mdGpuGraph.graphIsCapturingThisStep());
src/gromacs/mdlib/tests/mdgpugraph.cpp:        ASSERT_TRUE(mdGpuGraph.useGraphThisStep());
src/gromacs/mdlib/tests/mdgpugraph.cpp:        mdGpuGraph.endRecord();
src/gromacs/mdlib/tests/mdgpugraph.cpp:        mdGpuGraph.createExecutableGraph(forceGraphReinstantiation);
src/gromacs/mdlib/tests/mdgpugraph.cpp:        mdGpuGraph.launchGraphMdStep(&xUpdatedOnDeviceEvent);
src/gromacs/mdlib/tests/mdgpugraph.cpp:        ASSERT_FALSE(mdGpuGraph.captureThisStep(canUseGraphThisStep));
src/gromacs/mdlib/tests/mdgpugraph.cpp:        ASSERT_FALSE(mdGpuGraph.graphIsCapturingThisStep());
src/gromacs/mdlib/tests/mdgpugraph.cpp:        ASSERT_TRUE(mdGpuGraph.useGraphThisStep());
src/gromacs/mdlib/tests/mdgpugraph.cpp:        mdGpuGraph.launchGraphMdStep(&xUpdatedOnDeviceEvent);
src/gromacs/mdlib/tests/mdgpugraph.cpp:        ASSERT_FALSE(mdGpuGraph.captureThisStep(canUseGraphThisStep));
src/gromacs/mdlib/tests/mdgpugraph.cpp:        ASSERT_FALSE(mdGpuGraph.graphIsCapturingThisStep());
src/gromacs/mdlib/tests/mdgpugraph.cpp:        ASSERT_FALSE(mdGpuGraph.useGraphThisStep());
src/gromacs/mdlib/tests/mdgpugraph.cpp:        ASSERT_FALSE(mdGpuGraph.captureThisStep(canUseGraphThisStep));
src/gromacs/mdlib/tests/mdgpugraph.cpp:        ASSERT_FALSE(mdGpuGraph.graphIsCapturingThisStep());
src/gromacs/mdlib/tests/mdgpugraph.cpp:        ASSERT_TRUE(mdGpuGraph.useGraphThisStep());
src/gromacs/mdlib/tests/mdgpugraph.cpp:        mdGpuGraph.reset();
src/gromacs/mdlib/tests/mdgpugraph.cpp:        ASSERT_FALSE(mdGpuGraph.graphIsCapturingThisStep());
src/gromacs/mdlib/tests/mdgpugraph.cpp:        ASSERT_FALSE(mdGpuGraph.useGraphThisStep());
src/gromacs/mdlib/tests/mdgpugraph.cpp:#endif // GMX_HAVE_GPU_GRAPH_SUPPORT
src/gromacs/mdlib/tests/langevintestrunners.h: * class is used to unify the interfaces for CPU and GPU implementations of the
src/gromacs/mdlib/tests/calc_verletbuf.cpp: * larger than four. This only matters for GPU runs with the optimal pairlist and cluster sizes
src/gromacs/mdlib/tests/calc_verletbuf.cpp: * determined at runtime to fit the current GPU architecture.
src/gromacs/mdlib/tests/leapfrogtestrunners_gpu.cpp: * \brief Runner for GPU version of the integrator
src/gromacs/mdlib/tests/leapfrogtestrunners_gpu.cpp: * Handles GPU data management and actual numerical integration.
src/gromacs/mdlib/tests/leapfrogtestrunners_gpu.cpp:#if GPU_LEAPFROG_SUPPORTED
src/gromacs/mdlib/tests/leapfrogtestrunners_gpu.cpp:#    include "gromacs/gpu_utils/devicebuffer.h"
src/gromacs/mdlib/tests/leapfrogtestrunners_gpu.cpp:#include "gromacs/gpu_utils/gputraits.h"
src/gromacs/mdlib/tests/leapfrogtestrunners_gpu.cpp:#include "gromacs/mdlib/leapfrog_gpu.h"
src/gromacs/mdlib/tests/leapfrogtestrunners_gpu.cpp:#if GPU_LEAPFROG_SUPPORTED
src/gromacs/mdlib/tests/leapfrogtestrunners_gpu.cpp:    copyToDeviceBuffer(&d_x, h_x, 0, numAtoms, deviceStream, GpuApiCallBehavior::Sync, nullptr);
src/gromacs/mdlib/tests/leapfrogtestrunners_gpu.cpp:    copyToDeviceBuffer(&d_xp, h_xp, 0, numAtoms, deviceStream, GpuApiCallBehavior::Sync, nullptr);
src/gromacs/mdlib/tests/leapfrogtestrunners_gpu.cpp:    copyToDeviceBuffer(&d_v, h_v, 0, numAtoms, deviceStream, GpuApiCallBehavior::Sync, nullptr);
src/gromacs/mdlib/tests/leapfrogtestrunners_gpu.cpp:    copyToDeviceBuffer(&d_f, h_f, 0, numAtoms, deviceStream, GpuApiCallBehavior::Sync, nullptr);
src/gromacs/mdlib/tests/leapfrogtestrunners_gpu.cpp:            std::make_unique<LeapFrogGpu>(deviceContext, deviceStream, testData->numTCoupleGroups_);
src/gromacs/mdlib/tests/leapfrogtestrunners_gpu.cpp:    copyFromDeviceBuffer(h_xp, &d_x, 0, numAtoms, deviceStream, GpuApiCallBehavior::Sync, nullptr);
src/gromacs/mdlib/tests/leapfrogtestrunners_gpu.cpp:    copyFromDeviceBuffer(h_v, &d_v, 0, numAtoms, deviceStream, GpuApiCallBehavior::Sync, nullptr);
src/gromacs/mdlib/tests/leapfrogtestrunners_gpu.cpp:#else // GPU_LEAPFROG_SUPPORTED
src/gromacs/mdlib/tests/leapfrogtestrunners_gpu.cpp:    FAIL() << "Dummy Leap-Frog GPU function was called instead of the real one.";
src/gromacs/mdlib/tests/leapfrogtestrunners_gpu.cpp:#endif // GPU_LEAPFROG_SUPPORTED
src/gromacs/mdlib/tests/settletestrunners.cpp: * Also adds stub for the GPU version to keep the compiler happy.
src/gromacs/mdlib/tests/CMakeLists.txt:    GPU_CPP_SOURCE_FILES
src/gromacs/mdlib/tests/CMakeLists.txt:        constrtestrunners_gpu.cpp
src/gromacs/mdlib/tests/CMakeLists.txt:        leapfrogtestrunners_gpu.cpp
src/gromacs/mdlib/tests/CMakeLists.txt:        mdgpugraph.cpp
src/gromacs/mdlib/tests/CMakeLists.txt:        settletestrunners_gpu.cpp
src/gromacs/mdlib/tests/constrtestrunners.h: * GPU version of constraints is only available with CUDA and SYCL.
src/gromacs/mdlib/tests/constrtestrunners.h:#define GPU_CONSTRAINTS_SUPPORTED (GMX_GPU_CUDA || GMX_GPU_SYCL)
src/gromacs/mdlib/tests/constrtestrunners.h:     * \return "<algorithm> on <device>", depending on the actual implementation used. E.g., "LINCS on #0: NVIDIA GeForce GTX 1660 SUPER".
src/gromacs/mdlib/tests/constrtestrunners.h:// Runner for the GPU implementation of LINCS constraints algorithm.
src/gromacs/mdlib/tests/constrtestrunners.h:    /*! \brief Apply LINCS constraints to the test data on the GPU.
src/gromacs/mdlib/tests/constrtestrunners.h:     * \return "LINCS_GPU" string;
src/gromacs/mdlib/tests/leapfrog.cpp:    // If supported, add runners for the GPU version for each available GPU
src/gromacs/mdlib/tests/leapfrog.cpp:    const bool addGpuRunners = GPU_LEAPFROG_SUPPORTED;
src/gromacs/mdlib/tests/leapfrog.cpp:    if (addGpuRunners)
src/gromacs/mdlib/tests/settle.cpp: * and virial updates. The CPU and GPU versions are tested, if the code was
src/gromacs/mdlib/tests/settle.cpp: * compiled with CUDA support and there is a CUDA-capable GPU in the system.
src/gromacs/mdlib/tests/settle.cpp: * The test also compares the results from the CPU and GPU versions of the
src/gromacs/mdlib/tests/settle.cpp: * \todo The CPU and GPU versions are tested against each other. This
src/gromacs/mdlib/tests/settle.cpp: *       reference values. Also, these test will dry-run on a CUDA
src/gromacs/mdlib/tests/settle.cpp: *       build if no CUDA-capable GPU is available.
src/gromacs/mdlib/tests/settle.cpp:    // If supported, add runners for the GPU version for each available GPU
src/gromacs/mdlib/tests/settle.cpp:    const bool addGpuRunners = GPU_SETTLE_SUPPORTED;
src/gromacs/mdlib/tests/settle.cpp:    if (addGpuRunners)
src/gromacs/mdlib/tests/settle.cpp:// The test will cycle through all available runners, including CPU and, if applicable, GPU implementations of SETTLE.
src/gromacs/mdlib/tests/leapfrogtestdata.h:#include "gromacs/gpu_utils/gpu_utils.h"
src/gromacs/mdlib/tests/leapfrogtestrunners.h: * class is used to unify the interfaces for CPU and GPU implementations of the
src/gromacs/mdlib/tests/leapfrogtestrunners.h: * LeapFrog is available with CUDA and SYCL.
src/gromacs/mdlib/tests/leapfrogtestrunners.h:#define GPU_LEAPFROG_SUPPORTED (GMX_GPU_CUDA || GMX_GPU_SYCL)
src/gromacs/mdlib/tests/leapfrogtestrunners.h:    /*! \brief Integrate on the GPU for a given number of steps.
src/gromacs/mdlib/tests/leapfrogtestrunners.h:     * Copies data from CPU to GPU, integrates the equation of motion
src/gromacs/mdlib/tests/leapfrogtestrunners.h:     * \returns A string with GPU description.
src/gromacs/mdlib/tests/settletestdata.cpp:#include "gromacs/gpu_utils/gpu_utils.h"
src/gromacs/mdlib/tests/constrtestrunners.cpp: * CPU-based implementation and a stub for GPU-based implementation.
src/gromacs/mdlib/tests/constr.cpp: *       reduction, etc. on the GPU.
src/gromacs/mdlib/tests/constr.cpp:    //! Before any test is run, work out whether any compatible GPUs exist.
src/gromacs/mdlib/tests/constr.cpp:        // If supported, add runners for the GPU version of LINCS for each available GPU
src/gromacs/mdlib/tests/constr.cpp:        const bool addGpuRunners = GPU_CONSTRAINTS_SUPPORTED;
src/gromacs/mdlib/tests/constr.cpp:        if (addGpuRunners)
src/gromacs/mdlib/mdatoms.h:#include "gromacs/gpu_utils/hostallocator.h"
src/gromacs/mdlib/mdatoms.h: * efficient PME on GPU transfers. The mdatoms_ member should be
src/gromacs/mdlib/mdatoms.h:    //! Memory for chargeA that can be set up for efficient GPU transfer.
src/gromacs/mdlib/mdatoms.h:    //! Memory for chargeB that can be set up for efficient GPU transfer.
src/gromacs/mdlib/mdatoms.h:    makeMDAtoms(FILE* fp, const gmx_mtop_t& mtop, const t_inputrec& ir, bool rankHasPmeGpuTask);
src/gromacs/mdlib/mdatoms.h:std::unique_ptr<MDAtoms> makeMDAtoms(FILE* fp, const gmx_mtop_t& mtop, const t_inputrec& ir, bool useGpuForPme);
src/gromacs/mdlib/lincs_gpu.h: * \brief Declares the class for GPU implementation of LINCS.
src/gromacs/mdlib/lincs_gpu.h:#ifndef GMX_MDLIB_LINCS_GPU_CUH
src/gromacs/mdlib/lincs_gpu.h:#define GMX_MDLIB_LINCS_GPU_CUH
src/gromacs/mdlib/lincs_gpu.h:#include "gromacs/gpu_utils/device_context.h"
src/gromacs/mdlib/lincs_gpu.h:#include "gromacs/gpu_utils/device_stream.h"
src/gromacs/mdlib/lincs_gpu.h:#include "gromacs/gpu_utils/devicebuffer_datatype.h"
src/gromacs/mdlib/lincs_gpu.h:#include "gromacs/gpu_utils/gputraits.h"
src/gromacs/mdlib/lincs_gpu.h:/* \brief LINCS parameters and GPU pointers
src/gromacs/mdlib/lincs_gpu.h: * to the GPU as a single structure.
src/gromacs/mdlib/lincs_gpu.h:struct LincsGpuKernelParameters
src/gromacs/mdlib/lincs_gpu.h:    //! 1/mass for all atoms (GPU)
src/gromacs/mdlib/lincs_gpu.h:    //! Scaled virial tensor (6 floats: [XX, XY, XZ, YY, YZ, ZZ], GPU)
src/gromacs/mdlib/lincs_gpu.h:    //! List of constrained atoms (GPU memory)
src/gromacs/mdlib/lincs_gpu.h:    //! Equilibrium distances for the constraints (GPU)
src/gromacs/mdlib/lincs_gpu.h:    //! Number of constraints, coupled with the current one (GPU)
src/gromacs/mdlib/lincs_gpu.h:    //! List of coupled with the current one (GPU)
src/gromacs/mdlib/lincs_gpu.h:    //! Mass factors (GPU)
src/gromacs/mdlib/lincs_gpu.h:/*! \internal \brief Class with interfaces and data for GPU version of LINCS. */
src/gromacs/mdlib/lincs_gpu.h:class LincsGpu
src/gromacs/mdlib/lincs_gpu.h:    LincsGpu(int                  numIterations,
src/gromacs/mdlib/lincs_gpu.h:    ~LincsGpu();
src/gromacs/mdlib/lincs_gpu.h:     * Applies LINCS to coordinates and velocities, stored on GPU.
src/gromacs/mdlib/lincs_gpu.h:     * \param[in]     d_x               Coordinates before timestep (in GPU memory)
src/gromacs/mdlib/lincs_gpu.h:     * \param[in,out] d_xp              Coordinates after timestep (in GPU memory). The
src/gromacs/mdlib/lincs_gpu.h:     * \param[in,out] d_v               Velocities to update (in GPU memory, can be nullptr
src/gromacs/mdlib/lincs_gpu.h:     * Updates the constraints data and copies it to the GPU. Should be
src/gromacs/mdlib/lincs_gpu.h:     * are handled by a single GPU. Triangles are not handled as special case.
src/gromacs/mdlib/lincs_gpu.h:     * by the GPU LINCS code.
src/gromacs/mdlib/lincs_gpu.h:    //! GPU context object
src/gromacs/mdlib/lincs_gpu.h:    //! GPU stream
src/gromacs/mdlib/lincs_gpu.h:    //! Parameters and pointers, passed to the GPU kernel
src/gromacs/mdlib/lincs_gpu.h:    LincsGpuKernelParameters kernelParams_;
src/gromacs/mdlib/lincs_gpu.h:     * If the new number of constraints is larger then previous maximum, the GPU data arrays are
src/gromacs/mdlib/lincs_gpu.h:     * If the new number of atoms is larger then previous maximum, the GPU array with masses is
src/gromacs/mdlib/lincs_gpu.h:#endif // GMX_MDLIB_LINCS_GPU_CUH
src/gromacs/mdlib/mdgraph_gpu.h: * \author Alan Gray <alang@nvidia.com>
src/gromacs/mdlib/mdgraph_gpu.h:class GpuEventSynchronizer;
src/gromacs/mdlib/mdgraph_gpu.h:class MdGpuGraph
src/gromacs/mdlib/mdgraph_gpu.h:     *  GPU activities corresponding to a MD step, allowing these
src/gromacs/mdlib/mdgraph_gpu.h:     *  artificial GPU-side synchronizations to be overlapped with
src/gromacs/mdlib/mdgraph_gpu.h:    MdGpuGraph(const DeviceStreamManager& deviceStreamManager,
src/gromacs/mdlib/mdgraph_gpu.h:    ~MdGpuGraph();
src/gromacs/mdlib/mdgraph_gpu.h:    void startRecord(GpuEventSynchronizer* xReadyOnDeviceEvent);
src/gromacs/mdlib/mdgraph_gpu.h:    void launchGraphMdStep(GpuEventSynchronizer* xUpdatedOnDeviceEvent);
src/gromacs/mdlib/mdgraph_gpu.h:    void setAlternateStepPpTaskCompletionEvent(GpuEventSynchronizer* event);
src/gromacs/mdlib/mdgraph_gpu.h:    GpuEventSynchronizer* getPpTaskCompletionEvent();
src/gromacs/mdlib/gpuforcereduction_impl_internal.h:#include "gromacs/gpu_utils/devicebuffer_datatype.h"
src/gromacs/mdlib/gpuforcereduction_impl_internal.h:#include "gromacs/gpu_utils/gputraits.h"
src/gromacs/mdlib/gpuforcereduction_impl_internal.h:/*! \brief Backend-specific function to launch GPU Force Reduction kernel.
src/gromacs/mdlib/gpuforcereduction_impl_stubs.cpp: * \brief May be used to implement force reduction interfaces for non-GPU builds.
src/gromacs/mdlib/gpuforcereduction_impl_stubs.cpp: * \author Alan Gray <alang@nvidia.com>
src/gromacs/mdlib/gpuforcereduction_impl_stubs.cpp:#include "gromacs/gpu_utils/devicebuffer_datatype.h"
src/gromacs/mdlib/gpuforcereduction_impl_stubs.cpp:#include "gpuforcereduction.h"
src/gromacs/mdlib/gpuforcereduction_impl_stubs.cpp:class GpuEventSynchronizer;
src/gromacs/mdlib/gpuforcereduction_impl_stubs.cpp:#if !HAVE_GPU_FORCE_REDUCTION
src/gromacs/mdlib/gpuforcereduction_impl_stubs.cpp:class GpuForceReduction::Impl
src/gromacs/mdlib/gpuforcereduction_impl_stubs.cpp:GpuForceReduction::GpuForceReduction(const DeviceContext& /* deviceContext */,
src/gromacs/mdlib/gpuforcereduction_impl_stubs.cpp:void GpuForceReduction::reinit(DeviceBuffer<RVec> /*baseForcePtr*/,
src/gromacs/mdlib/gpuforcereduction_impl_stubs.cpp:                               GpuEventSynchronizer* /*completionMarker*/)
src/gromacs/mdlib/gpuforcereduction_impl_stubs.cpp:void GpuForceReduction::registerNbnxmForce(DeviceBuffer<RVec> /* forcePtr */)
src/gromacs/mdlib/gpuforcereduction_impl_stubs.cpp:void GpuForceReduction::registerRvecForce(DeviceBuffer<gmx::RVec> /* forcePtr */)
src/gromacs/mdlib/gpuforcereduction_impl_stubs.cpp:void GpuForceReduction::registerForcesReadyNvshmemFlags(DeviceBuffer<uint64_t> /* forceSyncObjPtr */)
src/gromacs/mdlib/gpuforcereduction_impl_stubs.cpp:void GpuForceReduction::addDependency(GpuEventSynchronizer* const /* dependency */)
src/gromacs/mdlib/gpuforcereduction_impl_stubs.cpp:void GpuForceReduction::execute()
src/gromacs/mdlib/gpuforcereduction_impl_stubs.cpp:GpuForceReduction::~GpuForceReduction() = default;
src/gromacs/mdlib/gpuforcereduction_impl_stubs.cpp:#endif /* !HAVE_GPU_FORCE_REDUCTION */
src/gromacs/mdlib/calc_verletbuf.cpp:    if (nbnxnKernelType == gmx::NbnxmKernelType::Gpu8x8x8)
src/gromacs/mdlib/calc_verletbuf.cpp:        // Use the default GPU 8x8x8 pairlist layout here, as the results are
src/gromacs/mdlib/calc_verletbuf.cpp:        listSetup.cluster_size_i = gmx::sc_gpuClusterSize(gmx::PairlistType::Hierarchical8x8x8);
src/gromacs/mdlib/calc_verletbuf.cpp:        listSetup.cluster_size_j = gmx::sc_gpuClusterSize(gmx::PairlistType::Hierarchical8x8x8);
src/gromacs/mdlib/calc_verletbuf.cpp:    if (listType == ListSetupType::Gpu)
src/gromacs/mdlib/calc_verletbuf.cpp:        nbnxnKernelType = gmx::NbnxmKernelType::Gpu8x8x8;
src/gromacs/mdlib/settle_gpu_internal.cu: * \brief CUDA-specific routines for the GPU implementation of SETTLE constraints algorithm.
src/gromacs/mdlib/settle_gpu_internal.cu:#include "settle_gpu_internal.h"
src/gromacs/mdlib/settle_gpu_internal.cu:#include "gromacs/gpu_utils/cuda_arch_utils.cuh"
src/gromacs/mdlib/settle_gpu_internal.cu:#include "gromacs/gpu_utils/cudautils.cuh"
src/gromacs/mdlib/settle_gpu_internal.cu:#include "gromacs/gpu_utils/devicebuffer.h"
src/gromacs/mdlib/settle_gpu_internal.cu:#include "gromacs/gpu_utils/gputraits.h"
src/gromacs/mdlib/settle_gpu_internal.cu:#include "gromacs/gpu_utils/typecasts_cuda_hip.h"
src/gromacs/mdlib/settle_gpu_internal.cu:#include "gromacs/gpu_utils/vectype_ops_cuda.h"
src/gromacs/mdlib/settle_gpu_internal.cu:#include "gromacs/pbcutil/pbc_aiuc_cuda.cuh"
src/gromacs/mdlib/settle_gpu_internal.cu://! Number of CUDA threads in a block
src/gromacs/mdlib/settle_gpu_internal.cu: * Returns pointer to a CUDA kernel based on provided booleans.
src/gromacs/mdlib/settle_gpu_internal.cu: * \return                      Pointer to CUDA kernel
src/gromacs/mdlib/settle_gpu_internal.cu:void launchSettleGpuKernel(const int                          numSettles,
src/gromacs/mdlib/settle_gpu_internal.cu:    const auto kernelArgs = prepareGpuKernelArguments(kernelPtr,
src/gromacs/mdlib/settle_gpu_internal.cu:    launchGpuKernel(kernelPtr,
src/gromacs/mdlib/forcerec.cpp:#include "gromacs/ewald/pme_pp_comm_gpu.h"
src/gromacs/mdlib/forcerec.cpp:#include "gromacs/gpu_utils/gpu_utils.h"
src/gromacs/mdlib/forcerec.cpp:#include "gromacs/listed_forces/listed_forces_gpu.h"
src/gromacs/mdlib/forcerec.cpp:#include "gpuforcereduction.h"
src/gromacs/mdlib/forcerec.cpp:#include "mdgraph_gpu.h"
src/gromacs/mdlib/update_constrain_gpu_impl.cpp:#include "update_constrain_gpu_impl.h"
src/gromacs/mdlib/update_constrain_gpu_impl.cpp:#include "gromacs/gpu_utils/device_context.h"
src/gromacs/mdlib/update_constrain_gpu_impl.cpp:#include "gromacs/gpu_utils/device_stream.h"
src/gromacs/mdlib/update_constrain_gpu_impl.cpp:#include "gromacs/gpu_utils/devicebuffer.h"
src/gromacs/mdlib/update_constrain_gpu_impl.cpp:#include "gromacs/gpu_utils/gpueventsynchronizer.h"
src/gromacs/mdlib/update_constrain_gpu_impl.cpp:#include "gromacs/gpu_utils/gputraits.h"
src/gromacs/mdlib/update_constrain_gpu_impl.cpp:#include "gromacs/mdlib/leapfrog_gpu.h"
src/gromacs/mdlib/update_constrain_gpu_impl.cpp:#include "gromacs/mdlib/update_constrain_gpu.h"
src/gromacs/mdlib/update_constrain_gpu_impl.cpp:#include "gromacs/mdlib/update_constrain_gpu_internal.h"
src/gromacs/mdlib/update_constrain_gpu_impl.cpp:static constexpr bool sc_haveGpuConstraintSupport = (GMX_GPU_CUDA != 0) || (GMX_GPU_SYCL != 0);
src/gromacs/mdlib/update_constrain_gpu_impl.cpp:void UpdateConstrainGpu::Impl::integrate(GpuEventSynchronizer*             fReadyOnDevice,
src/gromacs/mdlib/update_constrain_gpu_impl.cpp:    wallcycle_start_nocount(wcycle_, WallCycleCounter::LaunchGpuPp);
src/gromacs/mdlib/update_constrain_gpu_impl.cpp:    wallcycle_sub_start(wcycle_, WallCycleSubCounter::LaunchGpuUpdateConstrain);
src/gromacs/mdlib/update_constrain_gpu_impl.cpp:        if (sc_haveGpuConstraintSupport)
src/gromacs/mdlib/update_constrain_gpu_impl.cpp:            lincsGpu_->apply(d_x0_, d_x_, updateVelocities, d_v_, 1.0 / dt, computeVirial, virial, pbcAiuc_);
src/gromacs/mdlib/update_constrain_gpu_impl.cpp:            settleGpu_->apply(d_x0_, d_x_, updateVelocities, d_v_, 1.0 / dt, computeVirial, virial, pbcAiuc_);
src/gromacs/mdlib/update_constrain_gpu_impl.cpp:    wallcycle_sub_stop(wcycle_, WallCycleSubCounter::LaunchGpuUpdateConstrain);
src/gromacs/mdlib/update_constrain_gpu_impl.cpp:    wallcycle_stop(wcycle_, WallCycleCounter::LaunchGpuPp);
src/gromacs/mdlib/update_constrain_gpu_impl.cpp:void UpdateConstrainGpu::Impl::scaleCoordinates(const Matrix3x3& scalingMatrix)
src/gromacs/mdlib/update_constrain_gpu_impl.cpp:    wallcycle_start_nocount(wcycle_, WallCycleCounter::LaunchGpuPp);
src/gromacs/mdlib/update_constrain_gpu_impl.cpp:    wallcycle_sub_start(wcycle_, WallCycleSubCounter::LaunchGpuUpdateConstrain);
src/gromacs/mdlib/update_constrain_gpu_impl.cpp:    wallcycle_sub_stop(wcycle_, WallCycleSubCounter::LaunchGpuUpdateConstrain);
src/gromacs/mdlib/update_constrain_gpu_impl.cpp:    wallcycle_stop(wcycle_, WallCycleCounter::LaunchGpuPp);
src/gromacs/mdlib/update_constrain_gpu_impl.cpp:void UpdateConstrainGpu::Impl::scaleVelocities(const Matrix3x3& scalingMatrix)
src/gromacs/mdlib/update_constrain_gpu_impl.cpp:    wallcycle_start_nocount(wcycle_, WallCycleCounter::LaunchGpuPp);
src/gromacs/mdlib/update_constrain_gpu_impl.cpp:    wallcycle_sub_start(wcycle_, WallCycleSubCounter::LaunchGpuUpdateConstrain);
src/gromacs/mdlib/update_constrain_gpu_impl.cpp:    wallcycle_sub_stop(wcycle_, WallCycleSubCounter::LaunchGpuUpdateConstrain);
src/gromacs/mdlib/update_constrain_gpu_impl.cpp:    wallcycle_stop(wcycle_, WallCycleCounter::LaunchGpuPp);
src/gromacs/mdlib/update_constrain_gpu_impl.cpp:UpdateConstrainGpu::Impl::Impl(const t_inputrec&    ir,
src/gromacs/mdlib/update_constrain_gpu_impl.cpp:    integrator_ = std::make_unique<LeapFrogGpu>(deviceContext_, deviceStream_, numTempScaleValues);
src/gromacs/mdlib/update_constrain_gpu_impl.cpp:    if (sc_haveGpuConstraintSupport)
src/gromacs/mdlib/update_constrain_gpu_impl.cpp:        lincsGpu_ = std::make_unique<LincsGpu>(ir.nLincsIter, ir.nProjOrder, deviceContext_, deviceStream_);
src/gromacs/mdlib/update_constrain_gpu_impl.cpp:        settleGpu_ = std::make_unique<SettleGpu>(mtop, deviceContext_, deviceStream_);
src/gromacs/mdlib/update_constrain_gpu_impl.cpp:UpdateConstrainGpu::Impl::~Impl()
src/gromacs/mdlib/update_constrain_gpu_impl.cpp:void UpdateConstrainGpu::Impl::set(DeviceBuffer<Float3>          d_x,
src/gromacs/mdlib/update_constrain_gpu_impl.cpp:    wallcycle_start(wcycle_, WallCycleCounter::GpuSetConstr);
src/gromacs/mdlib/update_constrain_gpu_impl.cpp:    if (sc_haveGpuConstraintSupport)
src/gromacs/mdlib/update_constrain_gpu_impl.cpp:        wallcycle_sub_start(wcycle_, WallCycleSubCounter::GpuSetLincs);
src/gromacs/mdlib/update_constrain_gpu_impl.cpp:        lincsGpu_->set(idef, numAtoms_, md.invmass);
src/gromacs/mdlib/update_constrain_gpu_impl.cpp:        wallcycle_sub_stop(wcycle_, WallCycleSubCounter::GpuSetLincs);
src/gromacs/mdlib/update_constrain_gpu_impl.cpp:        wallcycle_sub_start(wcycle_, WallCycleSubCounter::GpuSetSettle);
src/gromacs/mdlib/update_constrain_gpu_impl.cpp:        settleGpu_->set(idef);
src/gromacs/mdlib/update_constrain_gpu_impl.cpp:        wallcycle_sub_stop(wcycle_, WallCycleSubCounter::GpuSetSettle);
src/gromacs/mdlib/update_constrain_gpu_impl.cpp:    wallcycle_stop(wcycle_, WallCycleCounter::GpuSetConstr);
src/gromacs/mdlib/update_constrain_gpu_impl.cpp:void UpdateConstrainGpu::Impl::setPbc(const PbcType pbcType, const matrix box)
src/gromacs/mdlib/update_constrain_gpu_impl.cpp:GpuEventSynchronizer* UpdateConstrainGpu::Impl::xUpdatedOnDeviceEvent()
src/gromacs/mdlib/update_constrain_gpu_impl.cpp:UpdateConstrainGpu::UpdateConstrainGpu(const t_inputrec&    ir,
src/gromacs/mdlib/update_constrain_gpu_impl.cpp:UpdateConstrainGpu::~UpdateConstrainGpu() = default;
src/gromacs/mdlib/update_constrain_gpu_impl.cpp:void UpdateConstrainGpu::integrate(GpuEventSynchronizer*             fReadyOnDevice,
src/gromacs/mdlib/update_constrain_gpu_impl.cpp:void UpdateConstrainGpu::scaleCoordinates(const gmx::Matrix3x3& scalingMatrix)
src/gromacs/mdlib/update_constrain_gpu_impl.cpp:void UpdateConstrainGpu::scaleVelocities(const gmx::Matrix3x3& scalingMatrix)
src/gromacs/mdlib/update_constrain_gpu_impl.cpp:void UpdateConstrainGpu::set(DeviceBuffer<Float3>          d_x,
src/gromacs/mdlib/update_constrain_gpu_impl.cpp:void UpdateConstrainGpu::setPbc(const PbcType pbcType, const matrix box)
src/gromacs/mdlib/update_constrain_gpu_impl.cpp:GpuEventSynchronizer* UpdateConstrainGpu::xUpdatedOnDeviceEvent()
src/gromacs/mdlib/update_constrain_gpu_impl.cpp:bool UpdateConstrainGpu::isNumCoupledConstraintsSupported(const gmx_mtop_t& mtop)
src/gromacs/mdlib/update_constrain_gpu_impl.cpp:    return LincsGpu::isNumCoupledConstraintsSupported(mtop);
src/gromacs/mdlib/update_constrain_gpu_impl.cpp:bool UpdateConstrainGpu::areConstraintsSupported()
src/gromacs/mdlib/update_constrain_gpu_impl.cpp:    return sc_haveGpuConstraintSupport;
src/gromacs/mdlib/lincs_gpu_internal.h: * \brief Declare backend-specific LINCS GPU functions
src/gromacs/mdlib/lincs_gpu_internal.h: * \author Alan Gray <alang@nvidia.com>
src/gromacs/mdlib/lincs_gpu_internal.h:#ifndef GMX_MDLIB_LINCS_GPU_INTERNAL_H
src/gromacs/mdlib/lincs_gpu_internal.h:#define GMX_MDLIB_LINCS_GPU_INTERNAL_H
src/gromacs/mdlib/lincs_gpu_internal.h:#include "gromacs/gpu_utils/devicebuffer_datatype.h"
src/gromacs/mdlib/lincs_gpu_internal.h:#include "gromacs/gpu_utils/gputraits.h"
src/gromacs/mdlib/lincs_gpu_internal.h:struct LincsGpuKernelParameters;
src/gromacs/mdlib/lincs_gpu_internal.h://! Number of threads in a GPU block
src/gromacs/mdlib/lincs_gpu_internal.h:void launchLincsGpuKernel(LincsGpuKernelParameters*   kernelParams,
src/gromacs/mdlib/lincs_gpu_internal.h:#endif // GMX_MDLIB_LINCS_GPU_INTERNAL_H
src/gromacs/mdlib/CMakeLists.txt:    ${CMAKE_CURRENT_SOURCE_DIR}/gpuforcereduction_impl.cpp
src/gromacs/mdlib/CMakeLists.txt:    ${CMAKE_CURRENT_SOURCE_DIR}/gpuforcereduction_impl_internal_sycl.cpp
src/gromacs/mdlib/CMakeLists.txt:    ${CMAKE_CURRENT_SOURCE_DIR}/leapfrog_gpu.cpp
src/gromacs/mdlib/CMakeLists.txt:    ${CMAKE_CURRENT_SOURCE_DIR}/leapfrog_gpu_internal_sycl.cpp
src/gromacs/mdlib/CMakeLists.txt:    ${CMAKE_CURRENT_SOURCE_DIR}/lincs_gpu.cpp
src/gromacs/mdlib/CMakeLists.txt:    ${CMAKE_CURRENT_SOURCE_DIR}/lincs_gpu_internal_sycl.cpp
src/gromacs/mdlib/CMakeLists.txt:    ${CMAKE_CURRENT_SOURCE_DIR}/mdgraph_gpu_impl_sycl.cpp
src/gromacs/mdlib/CMakeLists.txt:    ${CMAKE_CURRENT_SOURCE_DIR}/settle_gpu.cpp
src/gromacs/mdlib/CMakeLists.txt:    ${CMAKE_CURRENT_SOURCE_DIR}/settle_gpu_internal_sycl.cpp
src/gromacs/mdlib/CMakeLists.txt:    ${CMAKE_CURRENT_SOURCE_DIR}/update_constrain_gpu_impl.cpp
src/gromacs/mdlib/CMakeLists.txt:    ${CMAKE_CURRENT_SOURCE_DIR}/update_constrain_gpu_internal_sycl.cpp)
src/gromacs/mdlib/CMakeLists.txt:if(GMX_GPU_CUDA)
src/gromacs/mdlib/CMakeLists.txt:       gpuforcereduction_impl.cpp
src/gromacs/mdlib/CMakeLists.txt:       gpuforcereduction_impl_internal.cu
src/gromacs/mdlib/CMakeLists.txt:       leapfrog_gpu.cpp
src/gromacs/mdlib/CMakeLists.txt:       leapfrog_gpu_internal.cu
src/gromacs/mdlib/CMakeLists.txt:       lincs_gpu.cpp
src/gromacs/mdlib/CMakeLists.txt:       settle_gpu.cpp
src/gromacs/mdlib/CMakeLists.txt:       settle_gpu_internal.cu
src/gromacs/mdlib/CMakeLists.txt:       update_constrain_gpu_impl.cpp
src/gromacs/mdlib/CMakeLists.txt:       update_constrain_gpu_internal.cu
src/gromacs/mdlib/CMakeLists.txt:       mdgraph_gpu_impl.cu
src/gromacs/mdlib/CMakeLists.txt:    # the CI build that does clang-tidy analysis of a CUDA build.
src/gromacs/mdlib/CMakeLists.txt:       gmx_add_libgromacs_sources(lincs_gpu_internal.cu)
src/gromacs/mdlib/CMakeLists.txt:    _gmx_add_files_to_property(CUDA_SOURCES
src/gromacs/mdlib/CMakeLists.txt:       gpuforcereduction_impl.cpp
src/gromacs/mdlib/CMakeLists.txt:       leapfrog_gpu.cpp
src/gromacs/mdlib/CMakeLists.txt:       lincs_gpu.cpp
src/gromacs/mdlib/CMakeLists.txt:       settle_gpu.cpp
src/gromacs/mdlib/CMakeLists.txt:       update_constrain_gpu_impl.cpp
src/gromacs/mdlib/CMakeLists.txt:if(GMX_GPU_SYCL)
src/gromacs/mdlib/CMakeLists.txt:        gpuforcereduction_impl.cpp
src/gromacs/mdlib/CMakeLists.txt:        gpuforcereduction_impl_internal_sycl.cpp
src/gromacs/mdlib/CMakeLists.txt:        leapfrog_gpu.cpp
src/gromacs/mdlib/CMakeLists.txt:        leapfrog_gpu_internal_sycl.cpp
src/gromacs/mdlib/CMakeLists.txt:        lincs_gpu.cpp
src/gromacs/mdlib/CMakeLists.txt:        lincs_gpu_internal_sycl.cpp
src/gromacs/mdlib/CMakeLists.txt:        mdgraph_gpu_impl_sycl.cpp
src/gromacs/mdlib/CMakeLists.txt:        settle_gpu.cpp
src/gromacs/mdlib/CMakeLists.txt:        settle_gpu_internal_sycl.cpp
src/gromacs/mdlib/CMakeLists.txt:        update_constrain_gpu_impl.cpp
src/gromacs/mdlib/CMakeLists.txt:        update_constrain_gpu_internal_sycl.cpp
src/gromacs/mdlib/CMakeLists.txt:        gpuforcereduction_impl.cpp
src/gromacs/mdlib/CMakeLists.txt:        gpuforcereduction_impl_internal_sycl.cpp
src/gromacs/mdlib/CMakeLists.txt:        leapfrog_gpu.cpp
src/gromacs/mdlib/CMakeLists.txt:        leapfrog_gpu_internal_sycl.cpp
src/gromacs/mdlib/CMakeLists.txt:        lincs_gpu.cpp
src/gromacs/mdlib/CMakeLists.txt:        lincs_gpu_internal_sycl.cpp
src/gromacs/mdlib/CMakeLists.txt:        mdgraph_gpu_impl_sycl.cpp
src/gromacs/mdlib/CMakeLists.txt:        settle_gpu.cpp
src/gromacs/mdlib/CMakeLists.txt:        settle_gpu_internal_sycl.cpp
src/gromacs/mdlib/CMakeLists.txt:        update_constrain_gpu_impl.cpp
src/gromacs/mdlib/CMakeLists.txt:        update_constrain_gpu_internal_sycl.cpp
src/gromacs/mdlib/mdgraph_gpu_impl.cpp: * \brief May be used to implement MD graph CUDA interfaces for non-GPU builds.
src/gromacs/mdlib/mdgraph_gpu_impl.cpp: * Needed to satisfy compiler on systems, where CUDA is not available.
src/gromacs/mdlib/mdgraph_gpu_impl.cpp: * \author Alan Gray <alang@nvidia.com>
src/gromacs/mdlib/mdgraph_gpu_impl.cpp:#include "gromacs/gpu_utils/device_stream_manager.h"
src/gromacs/mdlib/mdgraph_gpu_impl.cpp:#include "gromacs/mdlib/mdgraph_gpu.h"
src/gromacs/mdlib/mdgraph_gpu_impl.cpp:class GpuEventSynchronizer;
src/gromacs/mdlib/mdgraph_gpu_impl.cpp:#if !GMX_HAVE_GPU_GRAPH_SUPPORT
src/gromacs/mdlib/mdgraph_gpu_impl.cpp:class MdGpuGraph::Impl
src/gromacs/mdlib/mdgraph_gpu_impl.cpp:MdGpuGraph::MdGpuGraph(const DeviceStreamManager& /* deviceStreamManager */,
src/gromacs/mdlib/mdgraph_gpu_impl.cpp:MdGpuGraph::~MdGpuGraph() = default;
src/gromacs/mdlib/mdgraph_gpu_impl.cpp:void MdGpuGraph::reset()
src/gromacs/mdlib/mdgraph_gpu_impl.cpp:void MdGpuGraph::disableForDomainIfAnyPpRankHasCpuForces(bool /* disableGraphAcrossAllPpRanks */)
src/gromacs/mdlib/mdgraph_gpu_impl.cpp:bool MdGpuGraph::captureThisStep(bool /* canUseGraphThisStep */)
src/gromacs/mdlib/mdgraph_gpu_impl.cpp:void MdGpuGraph::setUsedGraphLastStep(bool /* usedGraphLastStep */)
src/gromacs/mdlib/mdgraph_gpu_impl.cpp:void MdGpuGraph::startRecord(GpuEventSynchronizer* /* xReadyOnDeviceEvent */)
src/gromacs/mdlib/mdgraph_gpu_impl.cpp:void MdGpuGraph::endRecord()
src/gromacs/mdlib/mdgraph_gpu_impl.cpp:void MdGpuGraph::createExecutableGraph(bool /* forceGraphReinstantiation */)
src/gromacs/mdlib/mdgraph_gpu_impl.cpp:void MdGpuGraph::launchGraphMdStep(GpuEventSynchronizer* /* xUpdatedOnDeviceEvent */)
src/gromacs/mdlib/mdgraph_gpu_impl.cpp:bool MdGpuGraph::useGraphThisStep() const
src/gromacs/mdlib/mdgraph_gpu_impl.cpp:bool MdGpuGraph::graphIsCapturingThisStep() const
src/gromacs/mdlib/mdgraph_gpu_impl.cpp:void MdGpuGraph::setAlternateStepPpTaskCompletionEvent(GpuEventSynchronizer* /* event */)
src/gromacs/mdlib/mdgraph_gpu_impl.cpp:GpuEventSynchronizer* MdGpuGraph::getPpTaskCompletionEvent()
src/gromacs/mdlib/mdgraph_gpu_impl.cpp:#endif // !GMX_HAVE_GPU_GRAPH_SUPPORT
src/gromacs/mdlib/constraint_gpu_helpers.cpp:#include "constraint_gpu_helpers.h"
src/gromacs/mdlib/lincs_gpu_internal_sycl.cpp:#include "gromacs/gpu_utils/devicebuffer.h"
src/gromacs/mdlib/lincs_gpu_internal_sycl.cpp:#include "gromacs/gpu_utils/gmxsycl.h"
src/gromacs/mdlib/lincs_gpu_internal_sycl.cpp:#include "gromacs/gpu_utils/sycl_kernel_utils.h"
src/gromacs/mdlib/lincs_gpu_internal_sycl.cpp:#include "gromacs/mdlib/lincs_gpu.h"
src/gromacs/mdlib/lincs_gpu_internal_sycl.cpp:#include "lincs_gpu_internal.h"
src/gromacs/mdlib/lincs_gpu_internal_sycl.cpp: * In GPU version, one thread is responsible for all computations for one constraint. The blocks are
src/gromacs/mdlib/lincs_gpu_internal_sycl.cpp:    /* Shared local memory buffer. Corresponds to sh_r, sm_rhs, and sm_threadVirial in CUDA.
src/gromacs/mdlib/lincs_gpu_internal_sycl.cpp:void launchLincsGpuKernel(LincsGpuKernelParameters*   kernelParams,
src/gromacs/mdlib/update_constrain_gpu_impl.h: * \brief Declares GPU implementation class for update and constraints.
src/gromacs/mdlib/update_constrain_gpu_impl.h:#ifndef GMX_MDLIB_UPDATE_CONSTRAIN_GPU_IMPL_H
src/gromacs/mdlib/update_constrain_gpu_impl.h:#define GMX_MDLIB_UPDATE_CONSTRAIN_GPU_IMPL_H
src/gromacs/mdlib/update_constrain_gpu_impl.h:#include "gromacs/gpu_utils/gpueventsynchronizer.h"
src/gromacs/mdlib/update_constrain_gpu_impl.h:#include "gromacs/mdlib/leapfrog_gpu.h"
src/gromacs/mdlib/update_constrain_gpu_impl.h:#include "gromacs/mdlib/lincs_gpu.h"
src/gromacs/mdlib/update_constrain_gpu_impl.h:#include "gromacs/mdlib/settle_gpu.h"
src/gromacs/mdlib/update_constrain_gpu_impl.h:#include "gromacs/mdlib/update_constrain_gpu.h"
src/gromacs/mdlib/update_constrain_gpu_impl.h:class GpuEventSynchronizer;
src/gromacs/mdlib/update_constrain_gpu_impl.h:/*! \internal \brief Class with interfaces and data for GPU version of Update-Constraint. */
src/gromacs/mdlib/update_constrain_gpu_impl.h:class UpdateConstrainGpu::Impl
src/gromacs/mdlib/update_constrain_gpu_impl.h:     * \param[in] deviceContext       GPU device context.
src/gromacs/mdlib/update_constrain_gpu_impl.h:     * \param[in] deviceStream        GPU stream to use.
src/gromacs/mdlib/update_constrain_gpu_impl.h:    void integrate(GpuEventSynchronizer*             fReadyOnDevice,
src/gromacs/mdlib/update_constrain_gpu_impl.h:    /*! \brief Scale coordinates on the GPU for the pressure coupling.
src/gromacs/mdlib/update_constrain_gpu_impl.h:    /*! \brief Scale velocities on the GPU for the pressure coupling.
src/gromacs/mdlib/update_constrain_gpu_impl.h:    GpuEventSynchronizer* xUpdatedOnDeviceEvent();
src/gromacs/mdlib/update_constrain_gpu_impl.h:     * by the GPU LINCS code.
src/gromacs/mdlib/update_constrain_gpu_impl.h:    //! GPU context object
src/gromacs/mdlib/update_constrain_gpu_impl.h:    //! GPU stream
src/gromacs/mdlib/update_constrain_gpu_impl.h:    //! 1/mass for all atoms (GPU)
src/gromacs/mdlib/update_constrain_gpu_impl.h:    std::unique_ptr<LeapFrogGpu> integrator_;
src/gromacs/mdlib/update_constrain_gpu_impl.h:    //! LINCS GPU object to use for non-water constraints
src/gromacs/mdlib/update_constrain_gpu_impl.h:    std::unique_ptr<LincsGpu> lincsGpu_;
src/gromacs/mdlib/update_constrain_gpu_impl.h:    //! SETTLE GPU object for water constrains
src/gromacs/mdlib/update_constrain_gpu_impl.h:    std::unique_ptr<SettleGpu> settleGpu_;
src/gromacs/mdlib/update_constrain_gpu_impl.h:    GpuEventSynchronizer xUpdatedOnDeviceEvent_;
src/gromacs/mdlib/update_constrain_gpu_impl.h:#endif // GMX_MDLIB_UPDATE_CONSTRAIN_GPU_IMPL_H
src/gromacs/mdlib/sim_util.cpp:#include "gromacs/domdec/gpuhaloexchange.h"
src/gromacs/mdlib/sim_util.cpp:#include "gromacs/ewald/pme_coordinate_receiver_gpu.h"
src/gromacs/mdlib/sim_util.cpp:#include "gromacs/ewald/pme_pp_comm_gpu.h"
src/gromacs/mdlib/sim_util.cpp:#include "gromacs/gpu_utils/devicebuffer_datatype.h"
src/gromacs/mdlib/sim_util.cpp:#include "gromacs/gpu_utils/gpu_utils.h"
src/gromacs/mdlib/sim_util.cpp:#include "gromacs/listed_forces/listed_forces_gpu.h"
src/gromacs/mdlib/sim_util.cpp:#include "gromacs/mdtypes/state_propagator_data_gpu.h"
src/gromacs/mdlib/sim_util.cpp:#include "gromacs/nbnxm/gpu_data_mgmt.h"
src/gromacs/mdlib/sim_util.cpp:#include "gromacs/nbnxm/nbnxm_gpu.h"
src/gromacs/mdlib/sim_util.cpp:#include "gromacs/timing/gpu_timing.h"
src/gromacs/mdlib/sim_util.cpp:#include "gpuforcereduction.h"
src/gromacs/mdlib/sim_util.cpp:class GpuEventSynchronizer;
src/gromacs/mdlib/sim_util.cpp:static const bool c_disableAlternatingWait = (getenv("GMX_DISABLE_ALTERNATING_GPU_WAIT") != nullptr);
src/gromacs/mdlib/sim_util.cpp:                                   bool             useGpuPmePpComms,
src/gromacs/mdlib/sim_util.cpp:                                   bool             receivePmeForceToGpu,
src/gromacs/mdlib/sim_util.cpp:    gmx_pme_receive_f(fr->pmePpCommGpu.get(),
src/gromacs/mdlib/sim_util.cpp:                      useGpuPmePpComms,
src/gromacs/mdlib/sim_util.cpp:                      receivePmeForceToGpu,
src/gromacs/mdlib/sim_util.cpp:    /* GPU kernel launch overhead is already timed separately */
src/gromacs/mdlib/sim_util.cpp:    if (!nbv->useGpu())
src/gromacs/mdlib/sim_util.cpp:/*! \brief Launch the prepare_step and spread stages of PME GPU.
src/gromacs/mdlib/sim_util.cpp: * \param[in]  useMdGpuGraph        Whether MD GPU Graph is in use.
src/gromacs/mdlib/sim_util.cpp:static inline void launchPmeGpuSpread(gmx_pme_t*            pmedata,
src/gromacs/mdlib/sim_util.cpp:                                      GpuEventSynchronizer* xReadyOnDevice,
src/gromacs/mdlib/sim_util.cpp:                                      bool                  useMdGpuGraph,
src/gromacs/mdlib/sim_util.cpp:    wallcycle_start(wcycle, WallCycleCounter::PmeGpuMesh);
src/gromacs/mdlib/sim_util.cpp:    pme_gpu_prepare_computation(pmedata, box, wcycle, stepWork);
src/gromacs/mdlib/sim_util.cpp:    bool                      useGpuDirectComm         = false;
src/gromacs/mdlib/sim_util.cpp:    PmeCoordinateReceiverGpu* pmeCoordinateReceiverGpu = nullptr;
src/gromacs/mdlib/sim_util.cpp:    pme_gpu_launch_spread(
src/gromacs/mdlib/sim_util.cpp:            pmedata, xReadyOnDevice, wcycle, lambdaQ, useGpuDirectComm, pmeCoordinateReceiverGpu, useMdGpuGraph);
src/gromacs/mdlib/sim_util.cpp:    wallcycle_stop(wcycle, WallCycleCounter::PmeGpuMesh);
src/gromacs/mdlib/sim_util.cpp:/*! \brief Launch the FFT and gather stages of PME GPU
src/gromacs/mdlib/sim_util.cpp:static void launchPmeGpuFftAndGather(gmx_pme_t*          pmedata,
src/gromacs/mdlib/sim_util.cpp:    wallcycle_start_nocount(wcycle, WallCycleCounter::PmeGpuMesh);
src/gromacs/mdlib/sim_util.cpp:    pme_gpu_launch_complex_transforms(pmedata, wcycle, stepWork);
src/gromacs/mdlib/sim_util.cpp:    pme_gpu_launch_gather(pmedata, wcycle, lambdaQ, stepWork.computeVirial);
src/gromacs/mdlib/sim_util.cpp:    wallcycle_stop(wcycle, WallCycleCounter::PmeGpuMesh);
src/gromacs/mdlib/sim_util.cpp: * Blocks until PME GPU tasks are completed, and gets the output forces and virial/energy
src/gromacs/mdlib/sim_util.cpp:static void pmeGpuWaitAndReduce(gmx_pme_t*          pme,
src/gromacs/mdlib/sim_util.cpp:    wallcycle_start_nocount(wcycle, WallCycleCounter::PmeGpuMesh);
src/gromacs/mdlib/sim_util.cpp:    pme_gpu_wait_and_reduce(pme, stepWork, wcycle, forceWithVirial, enerd, lambdaQ);
src/gromacs/mdlib/sim_util.cpp:    wallcycle_stop(wcycle, WallCycleCounter::PmeGpuMesh);
src/gromacs/mdlib/sim_util.cpp: *  Polling wait for either of the PME or nonbonded GPU tasks.
src/gromacs/mdlib/sim_util.cpp: * Instead of a static order in waiting for GPU tasks, this function
src/gromacs/mdlib/sim_util.cpp: * one of the reductions, regardless of the GPU task completion order.
src/gromacs/mdlib/sim_util.cpp:static void alternatePmeNbGpuWaitReduce(nonbonded_verlet_t* nbv,
src/gromacs/mdlib/sim_util.cpp:    bool isPmeGpuDone = false;
src/gromacs/mdlib/sim_util.cpp:    bool isNbGpuDone  = false;
src/gromacs/mdlib/sim_util.cpp:    ArrayRef<const RVec> pmeGpuForces;
src/gromacs/mdlib/sim_util.cpp:    while (!isPmeGpuDone || !isNbGpuDone)
src/gromacs/mdlib/sim_util.cpp:        if (!isPmeGpuDone)
src/gromacs/mdlib/sim_util.cpp:            wallcycle_start_nocount(wcycle, WallCycleCounter::PmeGpuMesh);
src/gromacs/mdlib/sim_util.cpp:            GpuTaskCompletion completionType =
src/gromacs/mdlib/sim_util.cpp:                    (isNbGpuDone) ? GpuTaskCompletion::Wait : GpuTaskCompletion::Check;
src/gromacs/mdlib/sim_util.cpp:            isPmeGpuDone = pme_gpu_try_finish_task(
src/gromacs/mdlib/sim_util.cpp:            wallcycle_stop(wcycle, WallCycleCounter::PmeGpuMesh);
src/gromacs/mdlib/sim_util.cpp:        if (!isNbGpuDone)
src/gromacs/mdlib/sim_util.cpp:            GpuTaskCompletion completionType =
src/gromacs/mdlib/sim_util.cpp:                    (isPmeGpuDone) ? GpuTaskCompletion::Wait : GpuTaskCompletion::Check;
src/gromacs/mdlib/sim_util.cpp:            // To get the wcycle call count right, when in GpuTaskCompletion::Check mode,
src/gromacs/mdlib/sim_util.cpp:            // GpuTaskCompletion::Wait mode the timing is expected to be done in the caller.
src/gromacs/mdlib/sim_util.cpp:            wallcycle_start_nocount(wcycle, WallCycleCounter::WaitGpuNbL);
src/gromacs/mdlib/sim_util.cpp:            isNbGpuDone = gpu_try_finish_task(
src/gromacs/mdlib/sim_util.cpp:                    nbv->gpuNbv(),
src/gromacs/mdlib/sim_util.cpp:            wallcycle_stop(wcycle, WallCycleCounter::WaitGpuNbL);
src/gromacs/mdlib/sim_util.cpp:            if (isNbGpuDone)
src/gromacs/mdlib/sim_util.cpp:                wallcycle_increment_event_count(wcycle, WallCycleCounter::WaitGpuNbL);
src/gromacs/mdlib/sim_util.cpp:        && (domainWork.haveCpuLocalForceWork || !stepWork.useGpuFBufferOps
src/gromacs/mdlib/sim_util.cpp:            || (havePpDomainDecomposition && !stepWork.useGpuFHalo)))
src/gromacs/mdlib/sim_util.cpp:/* \brief Launch end-of-step GPU tasks: buffer clearing and rolling pruning.
src/gromacs/mdlib/sim_util.cpp:static void launchGpuEndOfStepTasks(nonbonded_verlet_t*          nbv,
src/gromacs/mdlib/sim_util.cpp:                                    ListedForcesGpu*             listedForcesGpu,
src/gromacs/mdlib/sim_util.cpp:    if (runScheduleWork.simulationWork.useGpuNonbonded && runScheduleWork.stepWork.computeNonbondedForces)
src/gromacs/mdlib/sim_util.cpp:         * clear kernel launches can leave the GPU idle while it could be running
src/gromacs/mdlib/sim_util.cpp:        if (nbv->isDynamicPruningStepGpu(step))
src/gromacs/mdlib/sim_util.cpp:            nbv->dispatchPruneKernelGpu(step);
src/gromacs/mdlib/sim_util.cpp:        /* now clear the GPU outputs while we finish the step on the CPU */
src/gromacs/mdlib/sim_util.cpp:        wallcycle_start_nocount(wcycle, WallCycleCounter::LaunchGpuPp);
src/gromacs/mdlib/sim_util.cpp:        wallcycle_sub_start_nocount(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
src/gromacs/mdlib/sim_util.cpp:        gpu_clear_outputs(nbv->gpuNbv(), runScheduleWork.stepWork.computeVirial);
src/gromacs/mdlib/sim_util.cpp:        wallcycle_sub_stop(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
src/gromacs/mdlib/sim_util.cpp:        wallcycle_stop(wcycle, WallCycleCounter::LaunchGpuPp);
src/gromacs/mdlib/sim_util.cpp:    if (runScheduleWork.stepWork.haveGpuPmeOnThisRank)
src/gromacs/mdlib/sim_util.cpp:        wallcycle_start_nocount(wcycle, WallCycleCounter::PmeGpuMesh);
src/gromacs/mdlib/sim_util.cpp:        bool gpuGraphWithSeparatePmeRank = false;
src/gromacs/mdlib/sim_util.cpp:        pme_gpu_reinit_computation(pmedata, gpuGraphWithSeparatePmeRank, wcycle);
src/gromacs/mdlib/sim_util.cpp:        wallcycle_stop(wcycle, WallCycleCounter::PmeGpuMesh);
src/gromacs/mdlib/sim_util.cpp:    if (runScheduleWork.domainWork.haveGpuBondedWork && runScheduleWork.stepWork.computeEnergy)
src/gromacs/mdlib/sim_util.cpp:        listedForcesGpu->waitAccumulateEnergyTerms(enerd);
src/gromacs/mdlib/sim_util.cpp:        listedForcesGpu->clearEnergies();
src/gromacs/mdlib/sim_util.cpp:/*! \brief Compute the number of times the "local coordinates ready on device" GPU event will be used as a synchronization point.
src/gromacs/mdlib/sim_util.cpp: * When some work is offloaded to GPU, force calculation should wait for the atom coordinates to
src/gromacs/mdlib/sim_util.cpp: * or from the GPU integration at the end of the previous step.
src/gromacs/mdlib/sim_util.cpp: * \param pmeSendCoordinatesFromGpu Whether peer-to-peer communication is used for PME coordinates.
src/gromacs/mdlib/sim_util.cpp:                                                          bool pmeSendCoordinatesFromGpu)
src/gromacs/mdlib/sim_util.cpp:        if (pmeSendCoordinatesFromGpu)
src/gromacs/mdlib/sim_util.cpp:                       "GPU PME PP communications require having a separate PME rank");
src/gromacs/mdlib/sim_util.cpp:            // Event is consumed by gmx_pme_send_coordinates for GPU PME PP Communications
src/gromacs/mdlib/sim_util.cpp:        if (stepWork.haveGpuPmeOnThisRank)
src/gromacs/mdlib/sim_util.cpp:            // Event is consumed by launchPmeGpuSpread
src/gromacs/mdlib/sim_util.cpp:        if (stepWork.computeNonbondedForces && stepWork.useGpuXBufferOps)
src/gromacs/mdlib/sim_util.cpp:            // Event is consumed by convertCoordinatesGpu
src/gromacs/mdlib/sim_util.cpp:    if (stepWork.useGpuXHalo)
src/gromacs/mdlib/sim_util.cpp:        // Event is consumed by communicateGpuHaloCoordinates
src/gromacs/mdlib/sim_util.cpp:    if (stepWork.clearGpuFBufferEarly && simulationWork.useGpuUpdate)
src/gromacs/mdlib/sim_util.cpp:/*! \brief Compute the number of times the "local forces ready on device" GPU event will be used as a synchronization point.
src/gromacs/mdlib/sim_util.cpp: * \param useOrEmulateGpuNb Whether GPU non-bonded calculations are used or emulated.
src/gromacs/mdlib/sim_util.cpp: * \param alternateGpuWait Whether alternating wait/reduce scheme is used.
src/gromacs/mdlib/sim_util.cpp:                                                          bool useOrEmulateGpuNb,
src/gromacs/mdlib/sim_util.cpp:                                                          bool alternateGpuWait)
src/gromacs/mdlib/sim_util.cpp:    bool eventUsedInGpuForceReduction =
src/gromacs/mdlib/sim_util.cpp:             || (simulationWork.havePpDomainDecomposition && !simulationWork.useGpuHaloExchange));
src/gromacs/mdlib/sim_util.cpp:    bool gpuForceReductionUsed = useOrEmulateGpuNb && !alternateGpuWait && stepWork.useGpuFBufferOps
src/gromacs/mdlib/sim_util.cpp:    if (gpuForceReductionUsed && eventUsedInGpuForceReduction)
src/gromacs/mdlib/sim_util.cpp:    bool gpuForceHaloUsed = simulationWork.havePpDomainDecomposition && stepWork.computeForces
src/gromacs/mdlib/sim_util.cpp:                            && stepWork.useGpuFHalo;
src/gromacs/mdlib/sim_util.cpp:    if (gpuForceHaloUsed)
src/gromacs/mdlib/sim_util.cpp:/*! \brief Setup for the local GPU force reduction:
src/gromacs/mdlib/sim_util.cpp: * \param [in] stateGpu            GPU state propagator object
src/gromacs/mdlib/sim_util.cpp: * \param [in] gpuForceReduction   GPU force reduction object
src/gromacs/mdlib/sim_util.cpp: * \param [in] pmePpCommGpu        PME-PP GPU communication object
src/gromacs/mdlib/sim_util.cpp:static void setupLocalGpuForceReduction(const MdrunScheduleWorkload& runScheduleWork,
src/gromacs/mdlib/sim_util.cpp:                                        StatePropagatorDataGpu*      stateGpu,
src/gromacs/mdlib/sim_util.cpp:                                        GpuForceReduction*           gpuForceReduction,
src/gromacs/mdlib/sim_util.cpp:                                        PmePpCommGpu*                pmePpCommGpu,
src/gromacs/mdlib/sim_util.cpp:               "GPU force reduction is not compatible with MTS");
src/gromacs/mdlib/sim_util.cpp:    // (re-)initialize local GPU force reduction
src/gromacs/mdlib/sim_util.cpp:    gpuForceReduction->reinit(stateGpu->getForces(),
src/gromacs/mdlib/sim_util.cpp:                              stateGpu->fReducedOnDevice(AtomLocality::Local));
src/gromacs/mdlib/sim_util.cpp:    gpuForceReduction->registerNbnxmForce(gpu_get_f(nbv->gpuNbv()));
src/gromacs/mdlib/sim_util.cpp:    GpuEventSynchronizer* pmeSynchronizer     = nullptr;
src/gromacs/mdlib/sim_util.cpp:    if (runScheduleWork.simulationWork.haveGpuPmeOnPpRank())
src/gromacs/mdlib/sim_util.cpp:        pmeForcePtr = pme_gpu_get_device_f(pmedata);
src/gromacs/mdlib/sim_util.cpp:            pmeSynchronizer     = pme_gpu_get_f_ready_synchronizer(pmedata);
src/gromacs/mdlib/sim_util.cpp:    else if (runScheduleWork.simulationWork.useGpuPmePpCommunication)
src/gromacs/mdlib/sim_util.cpp:        pmeForcePtr = pmePpCommGpu->getGpuForceStagingPtr();
src/gromacs/mdlib/sim_util.cpp:            pmeSynchronizer = pmePpCommGpu->getForcesReadySynchronizer();
src/gromacs/mdlib/sim_util.cpp:        gpuForceReduction->registerRvecForce(pmeForcePtr);
src/gromacs/mdlib/sim_util.cpp:            DeviceBuffer<uint64_t> forcesReadyNvshmemFlags = pmePpCommGpu->getGpuForcesSyncObj();
src/gromacs/mdlib/sim_util.cpp:            gpuForceReduction->registerForcesReadyNvshmemFlags(forcesReadyNvshmemFlags);
src/gromacs/mdlib/sim_util.cpp:        if (!runScheduleWork.simulationWork.useGpuPmePpCommunication || GMX_THREAD_MPI)
src/gromacs/mdlib/sim_util.cpp:            GMX_ASSERT(pmeSynchronizer != nullptr, "PME force ready cuda event should not be NULL");
src/gromacs/mdlib/sim_util.cpp:            gpuForceReduction->addDependency(pmeSynchronizer);
src/gromacs/mdlib/sim_util.cpp:            && !runScheduleWork.simulationWork.useGpuHaloExchange))
src/gromacs/mdlib/sim_util.cpp:        gpuForceReduction->addDependency(stateGpu->fReadyOnDevice(AtomLocality::Local));
src/gromacs/mdlib/sim_util.cpp:    if (runScheduleWork.simulationWork.useGpuHaloExchange)
src/gromacs/mdlib/sim_util.cpp:        gpuForceReduction->addDependency(dd->gpuHaloExchange[0][0]->getForcesReadyOnDeviceEvent());
src/gromacs/mdlib/sim_util.cpp:/*! \brief Setup for the non-local GPU force reduction:
src/gromacs/mdlib/sim_util.cpp: * \param [in] stateGpu            GPU state propagator object
src/gromacs/mdlib/sim_util.cpp: * \param [in] gpuForceReduction   GPU force reduction object
src/gromacs/mdlib/sim_util.cpp:static void setupNonLocalGpuForceReduction(const MdrunScheduleWorkload& runScheduleWork,
src/gromacs/mdlib/sim_util.cpp:                                           StatePropagatorDataGpu*      stateGpu,
src/gromacs/mdlib/sim_util.cpp:                                           GpuForceReduction*           gpuForceReduction,
src/gromacs/mdlib/sim_util.cpp:    // (re-)initialize non-local GPU force reduction
src/gromacs/mdlib/sim_util.cpp:    gpuForceReduction->reinit(stateGpu->getForces(),
src/gromacs/mdlib/sim_util.cpp:                              stateGpu->fReducedOnDevice(AtomLocality::NonLocal));
src/gromacs/mdlib/sim_util.cpp:    gpuForceReduction->registerNbnxmForce(gpu_get_f(nbv->gpuNbv()));
src/gromacs/mdlib/sim_util.cpp:        gpuForceReduction->addDependency(stateGpu->fReadyOnDevice(AtomLocality::NonLocal));
src/gromacs/mdlib/sim_util.cpp:    StatePropagatorDataGpu* stateGpu = fr->stateGpu;
src/gromacs/mdlib/sim_util.cpp:    if (needStateGpu(simulationWork))
src/gromacs/mdlib/sim_util.cpp:        stateGpu->reinit(mdatoms.homenr,
src/gromacs/mdlib/sim_util.cpp:    if (simulationWork.haveGpuPmeOnPpRank())
src/gromacs/mdlib/sim_util.cpp:        GMX_ASSERT(needStateGpu(simulationWork), "StatePropagatorDataGpu is needed");
src/gromacs/mdlib/sim_util.cpp:        // TODO: This should be moved into PME setup function ( pme_gpu_prepare_computation(...) )
src/gromacs/mdlib/sim_util.cpp:        pme_gpu_set_device_x(fr->pmedata, stateGpu->getCoordinates());
src/gromacs/mdlib/sim_util.cpp:    /* initialize the GPU nbnxm atom data and bonded data structures */
src/gromacs/mdlib/sim_util.cpp:    if (simulationWork.useGpuNonbonded)
src/gromacs/mdlib/sim_util.cpp:        // Note: cycle counting only nononbondeds, GPU listed forces counts internally
src/gromacs/mdlib/sim_util.cpp:        wallcycle_start_nocount(wcycle, WallCycleCounter::LaunchGpuPp);
src/gromacs/mdlib/sim_util.cpp:        wallcycle_sub_start_nocount(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
src/gromacs/mdlib/sim_util.cpp:        gpu_init_atomdata(nbv->gpuNbv(), &nbv->nbat());
src/gromacs/mdlib/sim_util.cpp:        wallcycle_sub_stop(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
src/gromacs/mdlib/sim_util.cpp:        wallcycle_stop(wcycle, WallCycleCounter::LaunchGpuPp);
src/gromacs/mdlib/sim_util.cpp:        if (fr->listedForcesGpu)
src/gromacs/mdlib/sim_util.cpp:             * interactions to the GPU, where the grid order is
src/gromacs/mdlib/sim_util.cpp:            fr->listedForcesGpu->updateInteractionListsAndDeviceBuffers(
src/gromacs/mdlib/sim_util.cpp:                    nbv->getGridIndices(), top.idef, gpuGetNBAtomData(nbv->gpuNbv()));
src/gromacs/mdlib/sim_util.cpp:    /* Note that with a GPU the launch overhead of the list transfer is not timed separately */
src/gromacs/mdlib/sim_util.cpp:    nbv->setupGpuShortRangeWork(fr->listedForcesGpu.get(), InteractionLocality::Local);
src/gromacs/mdlib/sim_util.cpp:    if (simulationWork.useGpuXBufferOpsWhenAllowed)
src/gromacs/mdlib/sim_util.cpp:        nbv->atomdata_init_copy_x_to_nbat_x_gpu();
src/gromacs/mdlib/sim_util.cpp:    if (simulationWork.useGpuFBufferOpsWhenAllowed)
src/gromacs/mdlib/sim_util.cpp:        // with MPI, direct GPU communication, and separate PME ranks we need
src/gromacs/mdlib/sim_util.cpp:        bool delaySetupLocalGpuForceReduction = GMX_MPI && simulationWork.useGpuPmePpCommunication;
src/gromacs/mdlib/sim_util.cpp:        if (!delaySetupLocalGpuForceReduction)
src/gromacs/mdlib/sim_util.cpp:            setupLocalGpuForceReduction(runScheduleWork,
src/gromacs/mdlib/sim_util.cpp:                                        stateGpu,
src/gromacs/mdlib/sim_util.cpp:                                        fr->gpuForceReduction[AtomLocality::Local].get(),
src/gromacs/mdlib/sim_util.cpp:                                        fr->pmePpCommGpu.get(),
src/gromacs/mdlib/sim_util.cpp:            setupNonLocalGpuForceReduction(runScheduleWork,
src/gromacs/mdlib/sim_util.cpp:                                           stateGpu,
src/gromacs/mdlib/sim_util.cpp:                                           fr->gpuForceReduction[AtomLocality::NonLocal].get(),
src/gromacs/mdlib/sim_util.cpp:        /* Note that with a GPU the launch overhead of the list transfer is not timed separately */
src/gromacs/mdlib/sim_util.cpp:        nbv->setupGpuShortRangeWork(fr->listedForcesGpu.get(), InteractionLocality::NonLocal);
src/gromacs/mdlib/sim_util.cpp:        // TODO refactor this GPU halo exchange re-initialisation
src/gromacs/mdlib/sim_util.cpp:        // to location in do_md where GPU halo exchange is
src/gromacs/mdlib/sim_util.cpp:        // constructed at partitioning, after above stateGpu
src/gromacs/mdlib/sim_util.cpp:        if (simulationWork.useGpuHaloExchange)
src/gromacs/mdlib/sim_util.cpp:            reinitGpuHaloExchange(*cr, stateGpu->getCoordinates(), stateGpu->getForces());
src/gromacs/mdlib/sim_util.cpp:    StatePropagatorDataGpu* stateGpu = fr->stateGpu;
src/gromacs/mdlib/sim_util.cpp:    const bool pmeSendCoordinatesFromGpu =
src/gromacs/mdlib/sim_util.cpp:            simulationWork.useGpuPmePpCommunication && !stepWork.doNeighborSearch;
src/gromacs/mdlib/sim_util.cpp:    const bool reinitGpuPmePpComms = simulationWork.useGpuPmePpCommunication && stepWork.doNeighborSearch;
src/gromacs/mdlib/sim_util.cpp:        // in order for nvshmem collective calls in StatePropagatorDataGpu::Impl::reinit
src/gromacs/mdlib/sim_util.cpp:                                 simulationWork.useGpuPmePpCommunication,
src/gromacs/mdlib/sim_util.cpp:                                 reinitGpuPmePpComms,
src/gromacs/mdlib/sim_util.cpp:                                 pmeSendCoordinatesFromGpu,
src/gromacs/mdlib/sim_util.cpp:                                 stepWork.useGpuPmeFReduction,
src/gromacs/mdlib/sim_util.cpp:                                 simulationWork.useMdGpuGraph,
src/gromacs/mdlib/sim_util.cpp:    auto* localXReadyOnDevice = (stepWork.haveGpuPmeOnThisRank || stepWork.useGpuXBufferOps
src/gromacs/mdlib/sim_util.cpp:                                 || simulationWork.useGpuUpdate || pmeSendCoordinatesFromGpu)
src/gromacs/mdlib/sim_util.cpp:                                        ? stateGpu->getCoordinatesReadyOnDeviceEvent(
src/gromacs/mdlib/sim_util.cpp:    if (stepWork.clearGpuFBufferEarly)
src/gromacs/mdlib/sim_util.cpp:        // GPU Force halo exchange will set a subset of local atoms with remote non-local data.
src/gromacs/mdlib/sim_util.cpp:        // which is satisfied when localXReadyOnDevice has been marked for GPU update case.
src/gromacs/mdlib/sim_util.cpp:        GpuEventSynchronizer* dependency = simulationWork.useGpuUpdate ? localXReadyOnDevice : nullptr;
src/gromacs/mdlib/sim_util.cpp:        stateGpu->clearForcesOnGpu(AtomLocality::Local, dependency);
src/gromacs/mdlib/sim_util.cpp:    GMX_ASSERT(simulationWork.useGpuHaloExchange
src/gromacs/mdlib/sim_util.cpp:                       == ((cr->dd != nullptr) && (!cr->dd->gpuHaloExchange[0].empty())),
src/gromacs/mdlib/sim_util.cpp:               "The GPU halo exchange is active, but it has not been constructed.");
src/gromacs/mdlib/sim_util.cpp:    bool gmx_used_in_debug haveCopiedXFromGpu = false;
src/gromacs/mdlib/sim_util.cpp:    // Copy coordinate from the GPU if update is on the GPU and there
src/gromacs/mdlib/sim_util.cpp:    if (simulationWork.useGpuUpdate && !stepWork.doNeighborSearch
src/gromacs/mdlib/sim_util.cpp:        stateGpu->copyCoordinatesFromGpu(x.unpaddedArrayRef(), AtomLocality::Local);
src/gromacs/mdlib/sim_util.cpp:        haveCopiedXFromGpu = true;
src/gromacs/mdlib/sim_util.cpp:    if (stepWork.haveGpuPmeOnThisRank || stepWork.useGpuXBufferOps || pmeSendCoordinatesFromGpu)
src/gromacs/mdlib/sim_util.cpp:        GMX_ASSERT(stateGpu != nullptr, "stateGpu should not be null");
src/gromacs/mdlib/sim_util.cpp:                        simulationWork, stepWork, pmeSendCoordinatesFromGpu);
src/gromacs/mdlib/sim_util.cpp:        if (!simulationWork.useGpuUpdate || stepWork.doNeighborSearch)
src/gromacs/mdlib/sim_util.cpp:            stateGpu->copyCoordinatesToGpu(x.unpaddedArrayRef(),
src/gromacs/mdlib/sim_util.cpp:        else if (simulationWork.useGpuUpdate)
src/gromacs/mdlib/sim_util.cpp:            stateGpu->setXUpdatedOnDeviceEventExpectedConsumptionCount(
src/gromacs/mdlib/sim_util.cpp:        if (!pmeSendCoordinatesFromGpu && simulationWork.useGpuUpdate)
src/gromacs/mdlib/sim_util.cpp:            GMX_ASSERT(haveCopiedXFromGpu,
src/gromacs/mdlib/sim_util.cpp:            stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
src/gromacs/mdlib/sim_util.cpp:                                 simulationWork.useGpuPmePpCommunication,
src/gromacs/mdlib/sim_util.cpp:                                 reinitGpuPmePpComms,
src/gromacs/mdlib/sim_util.cpp:                                 pmeSendCoordinatesFromGpu,
src/gromacs/mdlib/sim_util.cpp:                                 stepWork.useGpuPmeFReduction,
src/gromacs/mdlib/sim_util.cpp:                                 pmeSendCoordinatesFromGpu ? localXReadyOnDevice : nullptr,
src/gromacs/mdlib/sim_util.cpp:                                 simulationWork.useMdGpuGraph,
src/gromacs/mdlib/sim_util.cpp:    if (simulationWork.useGpuFBufferOpsWhenAllowed && stepWork.doNeighborSearch)
src/gromacs/mdlib/sim_util.cpp:        // with MPI, direct GPU communication, and separate PME ranks we need
src/gromacs/mdlib/sim_util.cpp:        bool doSetupLocalGpuForceReduction = GMX_MPI && simulationWork.useGpuPmePpCommunication;
src/gromacs/mdlib/sim_util.cpp:        if (doSetupLocalGpuForceReduction)
src/gromacs/mdlib/sim_util.cpp:            setupLocalGpuForceReduction(runScheduleWork,
src/gromacs/mdlib/sim_util.cpp:                                        stateGpu,
src/gromacs/mdlib/sim_util.cpp:                                        fr->gpuForceReduction[AtomLocality::Local].get(),
src/gromacs/mdlib/sim_util.cpp:                                        fr->pmePpCommGpu.get(),
src/gromacs/mdlib/sim_util.cpp:    if (stepWork.haveGpuPmeOnThisRank)
src/gromacs/mdlib/sim_util.cpp:        launchPmeGpuSpread(fr->pmedata,
src/gromacs/mdlib/sim_util.cpp:                           simulationWork.useMdGpuGraph,
src/gromacs/mdlib/sim_util.cpp:        if (stepWork.useGpuXBufferOps)
src/gromacs/mdlib/sim_util.cpp:            GMX_ASSERT(stateGpu, "stateGpu should be valid when buffer ops are offloaded");
src/gromacs/mdlib/sim_util.cpp:            nbv->convertCoordinatesGpu(AtomLocality::Local, stateGpu->getCoordinates(), localXReadyOnDevice);
src/gromacs/mdlib/sim_util.cpp:            if (simulationWork.useGpuUpdate)
src/gromacs/mdlib/sim_util.cpp:                GMX_ASSERT(stateGpu, "need a valid stateGpu object");
src/gromacs/mdlib/sim_util.cpp:                GMX_ASSERT(haveCopiedXFromGpu,
src/gromacs/mdlib/sim_util.cpp:                stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
src/gromacs/mdlib/sim_util.cpp:    if (simulationWork.useGpuNonbonded && (stepWork.computeNonbondedForces || domainWork.haveGpuBondedWork))
src/gromacs/mdlib/sim_util.cpp:        ddBalanceRegionHandler.openBeforeForceComputationGpu();
src/gromacs/mdlib/sim_util.cpp:        wallcycle_start(wcycle, WallCycleCounter::LaunchGpuPp);
src/gromacs/mdlib/sim_util.cpp:        wallcycle_sub_start(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
src/gromacs/mdlib/sim_util.cpp:        gpu_upload_shiftvec(nbv->gpuNbv(), &nbv->nbat());
src/gromacs/mdlib/sim_util.cpp:        if (!stepWork.useGpuXBufferOps)
src/gromacs/mdlib/sim_util.cpp:            gpu_copy_xq_to_gpu(nbv->gpuNbv(), &nbv->nbat(), AtomLocality::Local);
src/gromacs/mdlib/sim_util.cpp:        wallcycle_sub_stop(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
src/gromacs/mdlib/sim_util.cpp:        wallcycle_stop(wcycle, WallCycleCounter::LaunchGpuPp);
src/gromacs/mdlib/sim_util.cpp:        // with X buffer ops offloaded to the GPU on all but the search steps
src/gromacs/mdlib/sim_util.cpp:        if (domainWork.haveGpuBondedWork && !simulationWork.havePpDomainDecomposition)
src/gromacs/mdlib/sim_util.cpp:            fr->listedForcesGpu->setPbcAndlaunchKernel(fr->pbcType, box, fr->bMolPBC, stepWork);
src/gromacs/mdlib/sim_util.cpp:        /* launch local nonbonded work on GPU */
src/gromacs/mdlib/sim_util.cpp:        wallcycle_start_nocount(wcycle, WallCycleCounter::LaunchGpuPp);
src/gromacs/mdlib/sim_util.cpp:        wallcycle_sub_start_nocount(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
src/gromacs/mdlib/sim_util.cpp:        wallcycle_sub_stop(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
src/gromacs/mdlib/sim_util.cpp:        wallcycle_stop(wcycle, WallCycleCounter::LaunchGpuPp);
src/gromacs/mdlib/sim_util.cpp:    if (stepWork.haveGpuPmeOnThisRank)
src/gromacs/mdlib/sim_util.cpp:        // In PME GPU and mixed mode we launch FFT / gather after the
src/gromacs/mdlib/sim_util.cpp:        // X copy/transform to allow overlap as well as after the GPU NB
src/gromacs/mdlib/sim_util.cpp:        launchPmeGpuFftAndGather(fr->pmedata,
src/gromacs/mdlib/sim_util.cpp:        GpuEventSynchronizer* gpuCoordinateHaloLaunched = nullptr;
src/gromacs/mdlib/sim_util.cpp:            if (stepWork.useGpuXHalo)
src/gromacs/mdlib/sim_util.cpp:                gpuCoordinateHaloLaunched = communicateGpuHaloCoordinates(*cr, box, localXReadyOnDevice);
src/gromacs/mdlib/sim_util.cpp:                    stateGpu->copyCoordinatesFromGpu(
src/gromacs/mdlib/sim_util.cpp:                            x.unpaddedArrayRef(), AtomLocality::NonLocal, gpuCoordinateHaloLaunched);
src/gromacs/mdlib/sim_util.cpp:                if (simulationWork.useGpuUpdate)
src/gromacs/mdlib/sim_util.cpp:                    GMX_ASSERT(haveCopiedXFromGpu,
src/gromacs/mdlib/sim_util.cpp:                            (stepWork.computePmeOnSeparateRank && !pmeSendCoordinatesFromGpu);
src/gromacs/mdlib/sim_util.cpp:                        stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
src/gromacs/mdlib/sim_util.cpp:        if (stepWork.useGpuXBufferOps)
src/gromacs/mdlib/sim_util.cpp:            if (!stepWork.useGpuXHalo)
src/gromacs/mdlib/sim_util.cpp:                stateGpu->copyCoordinatesToGpu(x.unpaddedArrayRef(), AtomLocality::NonLocal);
src/gromacs/mdlib/sim_util.cpp:            GpuEventSynchronizer* xReadyOnDeviceEvent = stateGpu->getCoordinatesReadyOnDeviceEvent(
src/gromacs/mdlib/sim_util.cpp:                    AtomLocality::NonLocal, simulationWork, stepWork, gpuCoordinateHaloLaunched);
src/gromacs/mdlib/sim_util.cpp:            if (stepWork.useGpuXHalo && domainWork.haveCpuNonLocalForceWork)
src/gromacs/mdlib/sim_util.cpp:                /* We already enqueued an event for Gpu Halo exchange completion into the
src/gromacs/mdlib/sim_util.cpp:            nbv->convertCoordinatesGpu(
src/gromacs/mdlib/sim_util.cpp:                    AtomLocality::NonLocal, stateGpu->getCoordinates(), xReadyOnDeviceEvent);
src/gromacs/mdlib/sim_util.cpp:        if (simulationWork.useGpuNonbonded)
src/gromacs/mdlib/sim_util.cpp:            if (!stepWork.useGpuXBufferOps)
src/gromacs/mdlib/sim_util.cpp:                wallcycle_start(wcycle, WallCycleCounter::LaunchGpuPp);
src/gromacs/mdlib/sim_util.cpp:                wallcycle_sub_start(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
src/gromacs/mdlib/sim_util.cpp:                gpu_copy_xq_to_gpu(nbv->gpuNbv(), &nbv->nbat(), AtomLocality::NonLocal);
src/gromacs/mdlib/sim_util.cpp:                wallcycle_sub_stop(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
src/gromacs/mdlib/sim_util.cpp:                wallcycle_stop(wcycle, WallCycleCounter::LaunchGpuPp);
src/gromacs/mdlib/sim_util.cpp:            if (domainWork.haveGpuBondedWork)
src/gromacs/mdlib/sim_util.cpp:                fr->listedForcesGpu->setPbcAndlaunchKernel(fr->pbcType, box, fr->bMolPBC, stepWork);
src/gromacs/mdlib/sim_util.cpp:            /* launch non-local nonbonded tasks on GPU */
src/gromacs/mdlib/sim_util.cpp:            wallcycle_start_nocount(wcycle, WallCycleCounter::LaunchGpuPp);
src/gromacs/mdlib/sim_util.cpp:            wallcycle_sub_start(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
src/gromacs/mdlib/sim_util.cpp:            wallcycle_sub_stop(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
src/gromacs/mdlib/sim_util.cpp:            wallcycle_stop(wcycle, WallCycleCounter::LaunchGpuPp);
src/gromacs/mdlib/sim_util.cpp:    if (simulationWork.useGpuNonbonded && stepWork.computeNonbondedForces)
src/gromacs/mdlib/sim_util.cpp:        wallcycle_start_nocount(wcycle, WallCycleCounter::LaunchGpuPp);
src/gromacs/mdlib/sim_util.cpp:        wallcycle_sub_start_nocount(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
src/gromacs/mdlib/sim_util.cpp:            gpu_launch_cpyback(nbv->gpuNbv(), &nbv->nbat(), stepWork, AtomLocality::NonLocal);
src/gromacs/mdlib/sim_util.cpp:        gpu_launch_cpyback(nbv->gpuNbv(), &nbv->nbat(), stepWork, AtomLocality::Local);
src/gromacs/mdlib/sim_util.cpp:        wallcycle_sub_stop(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
src/gromacs/mdlib/sim_util.cpp:        if (domainWork.haveGpuBondedWork && stepWork.computeEnergy)
src/gromacs/mdlib/sim_util.cpp:            fr->listedForcesGpu->launchEnergyTransfer();
src/gromacs/mdlib/sim_util.cpp:        wallcycle_stop(wcycle, WallCycleCounter::LaunchGpuPp);
src/gromacs/mdlib/sim_util.cpp:    // For the rest of the CPU tasks that depend on GPU-update produced coordinates,
src/gromacs/mdlib/sim_util.cpp:    if (simulationWork.useGpuUpdate && !stepWork.doNeighborSearch)
src/gromacs/mdlib/sim_util.cpp:                || (stepWork.computePmeOnSeparateRank && !pmeSendCoordinatesFromGpu);
src/gromacs/mdlib/sim_util.cpp:            GMX_ASSERT(haveCopiedXFromGpu,
src/gromacs/mdlib/sim_util.cpp:            stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
src/gromacs/mdlib/sim_util.cpp:     * with independent GPU work (integration/constraints, x D2H copy).
src/gromacs/mdlib/sim_util.cpp:    const bool useOrEmulateGpuNb = simulationWork.useGpuNonbonded || fr->nbv->emulateGpu();
src/gromacs/mdlib/sim_util.cpp:    if (!useOrEmulateGpuNb)
src/gromacs/mdlib/sim_util.cpp:    if (stepWork.useGpuXHalo && domainWork.haveCpuNonLocalForceWork)
src/gromacs/mdlib/sim_util.cpp:        stateGpu->waitCoordinatesReadyOnHost(AtomLocality::NonLocal);
src/gromacs/mdlib/sim_util.cpp:         * Happens here on the CPU both with and without GPU.
src/gromacs/mdlib/sim_util.cpp:    if (stepWork.computeNonbondedForces && !useOrEmulateGpuNb)
src/gromacs/mdlib/sim_util.cpp:            (stepWork.haveGpuPmeOnThisRank || needToReceivePmeResultsFromSeparateRank);
src/gromacs/mdlib/sim_util.cpp:     * GPU we must wait for the PME calculation (dhdl) results to finish before sampling the
src/gromacs/mdlib/sim_util.cpp:        if (stepWork.haveGpuPmeOnThisRank)
src/gromacs/mdlib/sim_util.cpp:            pmeGpuWaitAndReduce(fr->pmedata,
src/gromacs/mdlib/sim_util.cpp:                                   simulationWork.useGpuPmePpCommunication,
src/gromacs/mdlib/sim_util.cpp:                                   stepWork.useGpuPmeFReduction,
src/gromacs/mdlib/sim_util.cpp:    if (simulationWork.havePpDomainDecomposition && stepWork.computeForces && stepWork.useGpuFHalo
src/gromacs/mdlib/sim_util.cpp:        stateGpu->copyForcesToGpu(forceOutMtsLevel0.forceWithShiftForces().force(), AtomLocality::Local);
src/gromacs/mdlib/sim_util.cpp:    GMX_ASSERT(!(nonbondedAtMtsLevel1 && stepWork.useGpuFBufferOps),
src/gromacs/mdlib/sim_util.cpp:               "The schedule below does not allow for nonbonded MTS with GPU buffer ops");
src/gromacs/mdlib/sim_util.cpp:    GMX_ASSERT(!(nonbondedAtMtsLevel1 && stepWork.useGpuFHalo),
src/gromacs/mdlib/sim_util.cpp:               "The schedule below does not allow for nonbonded MTS with GPU halo exchange");
src/gromacs/mdlib/sim_util.cpp:    // Will store the amount of cycles spent waiting for the GPU that
src/gromacs/mdlib/sim_util.cpp:    float cycles_wait_gpu = 0;
src/gromacs/mdlib/sim_util.cpp:    if (useOrEmulateGpuNb && stepWork.computeNonbondedForces)
src/gromacs/mdlib/sim_util.cpp:            if (simulationWork.useGpuNonbonded)
src/gromacs/mdlib/sim_util.cpp:                cycles_wait_gpu += gpu_wait_finish_task(
src/gromacs/mdlib/sim_util.cpp:                        nbv->gpuNbv(),
src/gromacs/mdlib/sim_util.cpp:            if (stepWork.useGpuFBufferOps)
src/gromacs/mdlib/sim_util.cpp:                    stateGpu->copyForcesToGpu(forceOutMtsLevel0.forceWithShiftForces().force(),
src/gromacs/mdlib/sim_util.cpp:                fr->gpuForceReduction[AtomLocality::NonLocal]->execute();
src/gromacs/mdlib/sim_util.cpp:                if (!stepWork.useGpuFHalo)
src/gromacs/mdlib/sim_util.cpp:                    stateGpu->consumeForcesReducedOnDeviceEvent(AtomLocality::NonLocal);
src/gromacs/mdlib/sim_util.cpp:                    // copy from GPU input for dd_move_f()
src/gromacs/mdlib/sim_util.cpp:                    stateGpu->copyForcesFromGpu(forceOutMtsLevel0.forceWithShiftForces().force(),
src/gromacs/mdlib/sim_util.cpp:            if (fr->nbv->emulateGpu() && stepWork.computeVirial)
src/gromacs/mdlib/sim_util.cpp:    // With both nonbonded and PME offloaded a GPU on the same rank, we use
src/gromacs/mdlib/sim_util.cpp:    // When running free energy perturbations steered by AWH and calculating PME on GPU,
src/gromacs/mdlib/sim_util.cpp:    const bool alternateGpuWait = (!c_disableAlternatingWait && stepWork.haveGpuPmeOnThisRank
src/gromacs/mdlib/sim_util.cpp:                                   && simulationWork.useGpuNonbonded && !simulationWork.havePpDomainDecomposition
src/gromacs/mdlib/sim_util.cpp:                                   && !stepWork.useGpuFBufferOps && !needEarlyPmeResults);
src/gromacs/mdlib/sim_util.cpp:            simulationWork, domainWork, stepWork, useOrEmulateGpuNb, alternateGpuWait);
src/gromacs/mdlib/sim_util.cpp:    // If expectedLocalFReadyOnDeviceConsumptionCount == 0, stateGpu can be uninitialized
src/gromacs/mdlib/sim_util.cpp:        stateGpu->setFReadyOnDeviceEventExpectedConsumptionCount(
src/gromacs/mdlib/sim_util.cpp:         * If we use a GPU this will overlap with GPU work, so in that case
src/gromacs/mdlib/sim_util.cpp:            if (stepWork.useGpuFHalo)
src/gromacs/mdlib/sim_util.cpp:                FixedCapacityVector<GpuEventSynchronizer*, 2> gpuForceHaloDependencies;
src/gromacs/mdlib/sim_util.cpp:                if (domainWork.haveCpuLocalForceWork || stepWork.clearGpuFBufferEarly)
src/gromacs/mdlib/sim_util.cpp:                    gpuForceHaloDependencies.push_back(stateGpu->fReadyOnDevice(AtomLocality::Local));
src/gromacs/mdlib/sim_util.cpp:                gpuForceHaloDependencies.push_back(stateGpu->fReducedOnDevice(AtomLocality::NonLocal));
src/gromacs/mdlib/sim_util.cpp:                communicateGpuHaloForces(*cr, accumulateForces, &gpuForceHaloDependencies);
src/gromacs/mdlib/sim_util.cpp:                if (stepWork.useGpuFBufferOps)
src/gromacs/mdlib/sim_util.cpp:                    stateGpu->waitForcesReadyOnHost(AtomLocality::NonLocal);
src/gromacs/mdlib/sim_util.cpp:    if (alternateGpuWait)
src/gromacs/mdlib/sim_util.cpp:        alternatePmeNbGpuWaitReduce(fr->nbv.get(),
src/gromacs/mdlib/sim_util.cpp:    if (!alternateGpuWait && stepWork.haveGpuPmeOnThisRank && !needEarlyPmeResults)
src/gromacs/mdlib/sim_util.cpp:        pmeGpuWaitAndReduce(fr->pmedata,
src/gromacs/mdlib/sim_util.cpp:    /* Wait for local GPU NB outputs on the non-alternating wait path */
src/gromacs/mdlib/sim_util.cpp:    if (!alternateGpuWait && stepWork.computeNonbondedForces && simulationWork.useGpuNonbonded)
src/gromacs/mdlib/sim_util.cpp:        /* Measured overhead on CUDA and OpenCL with(out) GPU sharing
src/gromacs/mdlib/sim_util.cpp:        const float gpuWaitApiOverheadMargin = 2e6F; /* cycles */
src/gromacs/mdlib/sim_util.cpp:        const float waitCycles               = gpu_wait_finish_task(
src/gromacs/mdlib/sim_util.cpp:                nbv->gpuNbv(),
src/gromacs/mdlib/sim_util.cpp:            DdBalanceRegionWaitedForGpu waitedForGpu = DdBalanceRegionWaitedForGpu::yes;
src/gromacs/mdlib/sim_util.cpp:            if (stepWork.computeForces && waitCycles <= gpuWaitApiOverheadMargin)
src/gromacs/mdlib/sim_util.cpp:                waitedForGpu = DdBalanceRegionWaitedForGpu::no;
src/gromacs/mdlib/sim_util.cpp:            ddBalanceRegionHandler.closeAfterForceComputationGpu(cycles_wait_gpu, waitedForGpu);
src/gromacs/mdlib/sim_util.cpp:    if (fr->nbv->emulateGpu())
src/gromacs/mdlib/sim_util.cpp:    // If on GPU PME-PP comms path, receive forces from PME before GPU buffer ops
src/gromacs/mdlib/sim_util.cpp:    // When running free energy perturbations steered by AWH and calculating PME on GPU,
src/gromacs/mdlib/sim_util.cpp:    if (needToReceivePmeResultsFromSeparateRank && simulationWork.useGpuPmePpCommunication && !needEarlyPmeResults)
src/gromacs/mdlib/sim_util.cpp:                               simulationWork.useGpuPmePpCommunication,
src/gromacs/mdlib/sim_util.cpp:                               stepWork.useGpuPmeFReduction,
src/gromacs/mdlib/sim_util.cpp:    /* Do the nonbonded GPU (or emulation) force buffer reduction
src/gromacs/mdlib/sim_util.cpp:    GMX_ASSERT(!(nonbondedAtMtsLevel1 && stepWork.useGpuFBufferOps),
src/gromacs/mdlib/sim_util.cpp:               "The schedule below does not allow for nonbonded MTS with GPU buffer ops");
src/gromacs/mdlib/sim_util.cpp:    if (useOrEmulateGpuNb && !alternateGpuWait)
src/gromacs/mdlib/sim_util.cpp:        if (stepWork.useGpuFBufferOps)
src/gromacs/mdlib/sim_util.cpp:            // - copy is not perfomed if GPU force halo exchange is active, because it would overwrite the result
src/gromacs/mdlib/sim_util.cpp:            if (domainWork.haveLocalForceContribInCpuBuffer && !stepWork.useGpuFHalo)
src/gromacs/mdlib/sim_util.cpp:                stateGpu->copyForcesToGpu(forceWithShift, AtomLocality::Local);
src/gromacs/mdlib/sim_util.cpp:                fr->gpuForceReduction[AtomLocality::Local]->execute();
src/gromacs/mdlib/sim_util.cpp:            if (!simulationWork.useGpuUpdate
src/gromacs/mdlib/sim_util.cpp:                || (simulationWork.useGpuUpdate && haveDDAtomOrdering(*cr) && simulationWork.useCpuPmePpCommunication)
src/gromacs/mdlib/sim_util.cpp:                    /* We have previously issued force reduction on the GPU, but we will
src/gromacs/mdlib/sim_util.cpp:                    stateGpu->consumeForcesReducedOnDeviceEvent(AtomLocality::Local);
src/gromacs/mdlib/sim_util.cpp:                stateGpu->copyForcesFromGpu(forceWithShift, AtomLocality::Local);
src/gromacs/mdlib/sim_util.cpp:                stateGpu->waitForcesReadyOnHost(AtomLocality::Local);
src/gromacs/mdlib/sim_util.cpp:        stateGpu->setFReadyOnDeviceEventExpectedConsumptionCount(AtomLocality::Local, 1);
src/gromacs/mdlib/sim_util.cpp:    launchGpuEndOfStepTasks(
src/gromacs/mdlib/sim_util.cpp:            nbv, fr->listedForcesGpu.get(), fr->pmedata, enerd, runScheduleWork, step, wcycle);
src/gromacs/mdlib/sim_util.cpp:    // TODO refactor this and unify with above GPU PME-PP / GPU update path call to the same function
src/gromacs/mdlib/sim_util.cpp:    // When running free energy perturbations steered by AWH and calculating PME on GPU,
src/gromacs/mdlib/sim_util.cpp:                               simulationWork.useGpuPmePpCommunication,
src/gromacs/mdlib/sim_util.cpp:    /* In case we don't have constraints and are using GPUs, the next balancing
src/gromacs/mdlib/coupling.h: * /param[out] pressureCouplingMu  Used later by the GPU update to scale the
src/gromacs/mdlib/mdgraph_gpu_impl_sycl.cpp: * \author Alan Gray <alang@nvidia.com>
src/gromacs/mdlib/mdgraph_gpu_impl_sycl.cpp:#include "gromacs/gpu_utils/device_context.h"
src/gromacs/mdlib/mdgraph_gpu_impl_sycl.cpp:#include "gromacs/gpu_utils/device_stream.h"
src/gromacs/mdlib/mdgraph_gpu_impl_sycl.cpp:#include "gromacs/gpu_utils/gmxsycl.h"
src/gromacs/mdlib/mdgraph_gpu_impl_sycl.cpp:#include "gromacs/gpu_utils/gpueventsynchronizer.h"
src/gromacs/mdlib/mdgraph_gpu_impl_sycl.cpp:#include "mdgraph_gpu_impl.h"
src/gromacs/mdlib/mdgraph_gpu_impl_sycl.cpp:#if GMX_HAVE_GPU_GRAPH_SUPPORT
src/gromacs/mdlib/mdgraph_gpu_impl_sycl.cpp:MdGpuGraph::Impl::Impl(const DeviceStreamManager& deviceStreamManager,
src/gromacs/mdlib/mdgraph_gpu_impl_sycl.cpp:    haveGpuPmeOnThisPpRank_(simulationWork.haveGpuPmeOnPpRank()),
src/gromacs/mdlib/mdgraph_gpu_impl_sycl.cpp:    helperEvent_           = std::make_unique<GpuEventSynchronizer>();
src/gromacs/mdlib/mdgraph_gpu_impl_sycl.cpp:    ppTaskCompletionEvent_ = std::make_unique<GpuEventSynchronizer>();
src/gromacs/mdlib/mdgraph_gpu_impl_sycl.cpp:                       "GPU Graphs with multiple ranks require threadMPI with GPU-direct "
src/gromacs/mdlib/mdgraph_gpu_impl_sycl.cpp:                       "GPU Graphs with separate PME rank require threadMPI with GPU-direct "
src/gromacs/mdlib/mdgraph_gpu_impl_sycl.cpp:    // NVIDIA-specific, not used in SYCL
src/gromacs/mdlib/mdgraph_gpu_impl_sycl.cpp:MdGpuGraph::Impl::~Impl()
src/gromacs/mdlib/mdgraph_gpu_impl_sycl.cpp:void MdGpuGraph::Impl::enqueueEventFromAllPpRanksToRank0Stream(GpuEventSynchronizer*, const DeviceStream&)
src/gromacs/mdlib/mdgraph_gpu_impl_sycl.cpp:void MdGpuGraph::Impl::enqueueRank0EventToAllPpStreams(GpuEventSynchronizer*, const DeviceStream&)
src/gromacs/mdlib/mdgraph_gpu_impl_sycl.cpp:void MdGpuGraph::Impl::reset()
src/gromacs/mdlib/mdgraph_gpu_impl_sycl.cpp:void MdGpuGraph::Impl::disableForDomainIfAnyPpRankHasCpuForces(bool disableGraphAcrossAllPpRanks)
src/gromacs/mdlib/mdgraph_gpu_impl_sycl.cpp:bool MdGpuGraph::Impl::captureThisStep(bool canUseGraphThisStep)
src/gromacs/mdlib/mdgraph_gpu_impl_sycl.cpp:void MdGpuGraph::Impl::setUsedGraphLastStep(bool usedGraphLastStep)
src/gromacs/mdlib/mdgraph_gpu_impl_sycl.cpp:void MdGpuGraph::Impl::startRecord(GpuEventSynchronizer* xReadyOnDeviceEvent)
src/gromacs/mdlib/mdgraph_gpu_impl_sycl.cpp:    wallcycle_start(wcycle_, WallCycleCounter::MdGpuGraph);
src/gromacs/mdlib/mdgraph_gpu_impl_sycl.cpp:    wallcycle_sub_start(wcycle_, WallCycleSubCounter::MdGpuGraphCapture);
src/gromacs/mdlib/mdgraph_gpu_impl_sycl.cpp:    if (haveGpuPmeOnThisPpRank_)
src/gromacs/mdlib/mdgraph_gpu_impl_sycl.cpp:void MdGpuGraph::Impl::endRecord()
src/gromacs/mdlib/mdgraph_gpu_impl_sycl.cpp:    wallcycle_sub_stop(wcycle_, WallCycleSubCounter::MdGpuGraphCapture);
src/gromacs/mdlib/mdgraph_gpu_impl_sycl.cpp:    wallcycle_stop(wcycle_, WallCycleCounter::MdGpuGraph);
src/gromacs/mdlib/mdgraph_gpu_impl_sycl.cpp:void MdGpuGraph::Impl::createExecutableGraph(bool forceGraphReinstantiation)
src/gromacs/mdlib/mdgraph_gpu_impl_sycl.cpp:    wallcycle_start(wcycle_, WallCycleCounter::MdGpuGraph);
src/gromacs/mdlib/mdgraph_gpu_impl_sycl.cpp:    wallcycle_sub_start(wcycle_, WallCycleSubCounter::MdGpuGraphInstantiateOrUpdate);
src/gromacs/mdlib/mdgraph_gpu_impl_sycl.cpp:    wallcycle_sub_stop(wcycle_, WallCycleSubCounter::MdGpuGraphInstantiateOrUpdate);
src/gromacs/mdlib/mdgraph_gpu_impl_sycl.cpp:    wallcycle_stop(wcycle_, WallCycleCounter::MdGpuGraph);
src/gromacs/mdlib/mdgraph_gpu_impl_sycl.cpp:void MdGpuGraph::Impl::launchGraphMdStep(GpuEventSynchronizer* xUpdatedOnDeviceEvent)
src/gromacs/mdlib/mdgraph_gpu_impl_sycl.cpp:    wallcycle_start(wcycle_, WallCycleCounter::MdGpuGraph);
src/gromacs/mdlib/mdgraph_gpu_impl_sycl.cpp:    wallcycle_sub_start(wcycle_, WallCycleSubCounter::MdGpuGraphLaunch);
src/gromacs/mdlib/mdgraph_gpu_impl_sycl.cpp:        if (haveGpuPmeOnThisPpRank_)
src/gromacs/mdlib/mdgraph_gpu_impl_sycl.cpp:    wallcycle_sub_stop(wcycle_, WallCycleSubCounter::MdGpuGraphLaunch);
src/gromacs/mdlib/mdgraph_gpu_impl_sycl.cpp:    wallcycle_stop(wcycle_, WallCycleCounter::MdGpuGraph);
src/gromacs/mdlib/mdgraph_gpu_impl_sycl.cpp:void MdGpuGraph::Impl::setAlternateStepPpTaskCompletionEvent(GpuEventSynchronizer* event)
src/gromacs/mdlib/mdgraph_gpu_impl_sycl.cpp:GpuEventSynchronizer* MdGpuGraph::Impl::getPpTaskCompletionEvent()
src/gromacs/mdlib/mdgraph_gpu_impl_sycl.cpp:MdGpuGraph::MdGpuGraph(const DeviceStreamManager& deviceStreamManager,
src/gromacs/mdlib/mdgraph_gpu_impl_sycl.cpp:MdGpuGraph::~MdGpuGraph() = default;
src/gromacs/mdlib/mdgraph_gpu_impl_sycl.cpp:void MdGpuGraph::reset()
src/gromacs/mdlib/mdgraph_gpu_impl_sycl.cpp:void MdGpuGraph::disableForDomainIfAnyPpRankHasCpuForces(bool disableGraphAcrossAllPpRanks)
src/gromacs/mdlib/mdgraph_gpu_impl_sycl.cpp:bool MdGpuGraph::captureThisStep(bool canUseGraphThisStep)
src/gromacs/mdlib/mdgraph_gpu_impl_sycl.cpp:void MdGpuGraph::setUsedGraphLastStep(bool usedGraphLastStep)
src/gromacs/mdlib/mdgraph_gpu_impl_sycl.cpp:void MdGpuGraph::startRecord(GpuEventSynchronizer* xReadyOnDeviceEvent)
src/gromacs/mdlib/mdgraph_gpu_impl_sycl.cpp:void MdGpuGraph::endRecord()
src/gromacs/mdlib/mdgraph_gpu_impl_sycl.cpp:void MdGpuGraph::createExecutableGraph(bool forceGraphReinstantiation)
src/gromacs/mdlib/mdgraph_gpu_impl_sycl.cpp:void MdGpuGraph::launchGraphMdStep(GpuEventSynchronizer* xUpdatedOnDeviceEvent)
src/gromacs/mdlib/mdgraph_gpu_impl_sycl.cpp:bool MdGpuGraph::useGraphThisStep() const
src/gromacs/mdlib/mdgraph_gpu_impl_sycl.cpp:bool MdGpuGraph::graphIsCapturingThisStep() const
src/gromacs/mdlib/mdgraph_gpu_impl_sycl.cpp:void MdGpuGraph::setAlternateStepPpTaskCompletionEvent(GpuEventSynchronizer* event)
src/gromacs/mdlib/mdgraph_gpu_impl_sycl.cpp:GpuEventSynchronizer* MdGpuGraph::getPpTaskCompletionEvent()
src/gromacs/mdlib/mdgraph_gpu_impl_sycl.cpp:#endif // GMX_HAVE_GPU_GRAPH_SUPPORT
src/gromacs/mdlib/leapfrog_gpu_internal.cu: * \brief Implements Leap-Frog using CUDA
src/gromacs/mdlib/leapfrog_gpu_internal.cu: * This file contains CUDA implementation of back-end specific code for Leap-Frog.
src/gromacs/mdlib/leapfrog_gpu_internal.cu:#include "leapfrog_gpu_internal.h"
src/gromacs/mdlib/leapfrog_gpu_internal.cu:#include "gromacs/gpu_utils/cudautils.cuh"
src/gromacs/mdlib/leapfrog_gpu_internal.cu:#include "gromacs/gpu_utils/devicebuffer.h"
src/gromacs/mdlib/leapfrog_gpu_internal.cu:#include "gromacs/gpu_utils/typecasts_cuda_hip.h"
src/gromacs/mdlib/leapfrog_gpu_internal.cu:#include "gromacs/gpu_utils/vectype_ops_cuda.h"
src/gromacs/mdlib/leapfrog_gpu_internal.cu:#include "gromacs/pbcutil/pbc_aiuc_cuda.cuh"
src/gromacs/mdlib/leapfrog_gpu_internal.cu:/*!\brief Number of CUDA threads in a block
src/gromacs/mdlib/leapfrog_gpu_internal.cu: *  The coordinates and velocities are updated on the GPU. Also saves the intermediate values of the coordinates for
src/gromacs/mdlib/leapfrog_gpu_internal.cu: *  Each GPU thread works with a single particle. Empty declaration is needed to
src/gromacs/mdlib/leapfrog_gpu_internal.cu: * Returns pointer to a CUDA kernel based on the number of temperature coupling groups and
src/gromacs/mdlib/leapfrog_gpu_internal.cu: * \return                         Pointer to CUDA kernel
src/gromacs/mdlib/leapfrog_gpu_internal.cu:    const auto kernelArgs = prepareGpuKernelArguments(kernelPtr,
src/gromacs/mdlib/leapfrog_gpu_internal.cu:    launchGpuKernel(kernelPtr, kernelLaunchConfig, deviceStream, nullptr, "leapfrog_kernel", kernelArgs);
src/gromacs/mdlib/update_constrain_gpu.h: * \brief Declaration of high-level functions of GPU implementation of update and constrain class.
src/gromacs/mdlib/update_constrain_gpu.h:#ifndef GMX_MDLIB_UPDATE_CONSTRAIN_GPU_H
src/gromacs/mdlib/update_constrain_gpu.h:#define GMX_MDLIB_UPDATE_CONSTRAIN_GPU_H
src/gromacs/mdlib/update_constrain_gpu.h:#include "gromacs/gpu_utils/devicebuffer_datatype.h"
src/gromacs/mdlib/update_constrain_gpu.h:class GpuEventSynchronizer;
src/gromacs/mdlib/update_constrain_gpu.h:class UpdateConstrainGpu
src/gromacs/mdlib/update_constrain_gpu.h:     * \param[in] deviceContext       GPU device context.
src/gromacs/mdlib/update_constrain_gpu.h:     * \param[in] deviceStream        GPU stream to use.
src/gromacs/mdlib/update_constrain_gpu.h:    UpdateConstrainGpu(const t_inputrec&    ir,
src/gromacs/mdlib/update_constrain_gpu.h:    ~UpdateConstrainGpu();
src/gromacs/mdlib/update_constrain_gpu.h:    void integrate(GpuEventSynchronizer*             fReadyOnDevice,
src/gromacs/mdlib/update_constrain_gpu.h:    /*! \brief Scale coordinates on the GPU for the pressure coupling.
src/gromacs/mdlib/update_constrain_gpu.h:    /*! \brief Scale velocities on the GPU for the pressure coupling.
src/gromacs/mdlib/update_constrain_gpu.h:    GpuEventSynchronizer* xUpdatedOnDeviceEvent();
src/gromacs/mdlib/update_constrain_gpu.h:     * by the GPU LINCS code.
src/gromacs/mdlib/update_constrain_gpu.h:     * Returns whether the constraints are supported by the GPU code.
src/gromacs/mdlib/update_constrain_gpu.h:     * Currently true for CUDA, false for others.
src/gromacs/mdlib/update_constrain_gpu.h:#endif // GMX_MDLIB_UPDATE_CONSTRAIN_GPU_H
src/gromacs/mdlib/constraint_gpu_helpers.h:#ifndef GMX_MDLIB_CONSTRAINT_GPU_HELPERS_H
src/gromacs/mdlib/constraint_gpu_helpers.h:#define GMX_MDLIB_CONSTRAINT_GPU_HELPERS_H
src/gromacs/mdlib/constraint_gpu_helpers.h: * single GPU block. The position \p c of the vector \p numCoupledConstraints should have the number
src/gromacs/mdlib/constraint_gpu_helpers.h: *  This information is used to split the array of constraints between thread blocks on a GPU so there is no
src/gromacs/mdlib/settle_gpu_internal.h: * \brief Declares backend-specific functions for GPU implementation of SETTLE.
src/gromacs/mdlib/settle_gpu_internal.h:#ifndef GMX_MDLIB_SETTLE_GPU_INTERNAL_H
src/gromacs/mdlib/settle_gpu_internal.h:#define GMX_MDLIB_SETTLE_GPU_INTERNAL_H
src/gromacs/mdlib/settle_gpu_internal.h:#include "gromacs/gpu_utils/devicebuffer_datatype.h"
src/gromacs/mdlib/settle_gpu_internal.h:#include "gromacs/gpu_utils/gputraits.h"
src/gromacs/mdlib/settle_gpu_internal.h:#include "gromacs/mdlib/settle_gpu.h"
src/gromacs/mdlib/settle_gpu_internal.h: * Applies SETTLE to coordinates and velocities, stored on GPU. Data at pointers d_xp and
src/gromacs/mdlib/settle_gpu_internal.h: * d_v change in the GPU memory. The results are not automatically copied back to the CPU
src/gromacs/mdlib/settle_gpu_internal.h: * \param[in]     d_x               Coordinates before timestep (in GPU memory)
src/gromacs/mdlib/settle_gpu_internal.h: * \param[in,out] d_xp              Coordinates after timestep (in GPU memory). The
src/gromacs/mdlib/settle_gpu_internal.h: * \param[in,out] d_v               Velocities to update (in GPU memory, can be nullptr
src/gromacs/mdlib/settle_gpu_internal.h:void launchSettleGpuKernel(int                                numSettles,
src/gromacs/mdlib/settle_gpu_internal.h:#endif // GMX_MDLIB_SETTLE_GPU_INTERNAL_H
src/gromacs/mdlib/update_constrain_gpu_internal_sycl.cpp:#include "gromacs/gpu_utils/devicebuffer_sycl.h"
src/gromacs/mdlib/update_constrain_gpu_internal_sycl.cpp:#include "gromacs/gpu_utils/gmxsycl.h"
src/gromacs/mdlib/update_constrain_gpu_internal_sycl.cpp:#include "gromacs/gpu_utils/gputraits_sycl.h"
src/gromacs/mdlib/update_constrain_gpu_internal_sycl.cpp:#include "update_constrain_gpu_internal.h"
src/gromacs/mdlib/lincs_gpu.cpp: * \brief Implementations of LINCS GPU class
src/gromacs/mdlib/lincs_gpu.cpp: * This file contains back-end agnostic implementation of LINCS GPU class.
src/gromacs/mdlib/lincs_gpu.cpp: * \author Alan Gray <alang@nvidia.com>
src/gromacs/mdlib/lincs_gpu.cpp:#include "lincs_gpu.h"
src/gromacs/mdlib/lincs_gpu.cpp:#include "gromacs/gpu_utils/devicebuffer.h"
src/gromacs/mdlib/lincs_gpu.cpp:#include "gromacs/gpu_utils/gputraits.h"
src/gromacs/mdlib/lincs_gpu.cpp:#include "gromacs/mdlib/constraint_gpu_helpers.h"
src/gromacs/mdlib/lincs_gpu.cpp:#include "gromacs/mdlib/lincs_gpu_internal.h"
src/gromacs/mdlib/lincs_gpu.cpp:void LincsGpu::apply(const DeviceBuffer<Float3>& d_x,
src/gromacs/mdlib/lincs_gpu.cpp:    launchLincsGpuKernel(
src/gromacs/mdlib/lincs_gpu.cpp:                             GpuApiCallBehavior::Sync,
src/gromacs/mdlib/lincs_gpu.cpp:LincsGpu::LincsGpu(int                  numIterations,
src/gromacs/mdlib/lincs_gpu.cpp:    GMX_RELEASE_ASSERT(bool(GMX_GPU_CUDA) || bool(GMX_GPU_SYCL),
src/gromacs/mdlib/lincs_gpu.cpp:                       "LINCS GPU is only implemented in CUDA and SYCL.");
src/gromacs/mdlib/lincs_gpu.cpp:                  "Real numbers should be in single precision in GPU code.");
src/gromacs/mdlib/lincs_gpu.cpp:LincsGpu::~LincsGpu()
src/gromacs/mdlib/lincs_gpu.cpp:bool LincsGpu::isNumCoupledConstraintsSupported(const gmx_mtop_t& mtop)
src/gromacs/mdlib/lincs_gpu.cpp:void LincsGpu::set(const InteractionDefinitions& idef, int numAtoms, const ArrayRef<const real> invmass)
src/gromacs/mdlib/lincs_gpu.cpp:    GMX_RELEASE_ASSERT(bool(GMX_GPU_CUDA) || bool(GMX_GPU_SYCL),
src/gromacs/mdlib/lincs_gpu.cpp:                       "LINCS GPU is only implemented in CUDA and SYCL.");
src/gromacs/mdlib/lincs_gpu.cpp:                      "Maximum number of coupled constraints (%d) exceeds the size of the CUDA "
src/gromacs/mdlib/lincs_gpu.cpp:                      "thread block (%d). Most likely, you are trying to use the GPU version of "
src/gromacs/mdlib/lincs_gpu.cpp:    // in the constraints array due to the GPU block borders. This number can be adjusted to improve
src/gromacs/mdlib/lincs_gpu.cpp:    // Copy data to GPU.
src/gromacs/mdlib/lincs_gpu.cpp:                       GpuApiCallBehavior::Sync,
src/gromacs/mdlib/lincs_gpu.cpp:                       GpuApiCallBehavior::Sync,
src/gromacs/mdlib/lincs_gpu.cpp:                       GpuApiCallBehavior::Sync,
src/gromacs/mdlib/lincs_gpu.cpp:                       GpuApiCallBehavior::Sync,
src/gromacs/mdlib/lincs_gpu.cpp:                       GpuApiCallBehavior::Sync,
src/gromacs/mdlib/lincs_gpu.cpp:                       GpuApiCallBehavior::Sync,
src/gromacs/mdlib/settle_gpu.h: * \brief Declares class for GPU implementation of SETTLE
src/gromacs/mdlib/settle_gpu.h:#ifndef GMX_MDLIB_SETTLE_GPU_H
src/gromacs/mdlib/settle_gpu.h:#define GMX_MDLIB_SETTLE_GPU_H
src/gromacs/mdlib/settle_gpu.h:#include "gromacs/gpu_utils/device_context.h"
src/gromacs/mdlib/settle_gpu.h:#include "gromacs/gpu_utils/device_stream.h"
src/gromacs/mdlib/settle_gpu.h:#include "gromacs/gpu_utils/devicebuffer_datatype.h"
src/gromacs/mdlib/settle_gpu.h:#include "gromacs/gpu_utils/gputraits.h"
src/gromacs/mdlib/settle_gpu.h:#include "gromacs/mdlib/constraint_gpu_helpers.h"
src/gromacs/mdlib/settle_gpu.h:/*! \internal \brief Class with interfaces and data for GPU version of SETTLE. */
src/gromacs/mdlib/settle_gpu.h:class SettleGpu
src/gromacs/mdlib/settle_gpu.h:    SettleGpu(const gmx_mtop_t& mtop, const DeviceContext& deviceContext, const DeviceStream& deviceStream);
src/gromacs/mdlib/settle_gpu.h:    ~SettleGpu();
src/gromacs/mdlib/settle_gpu.h:     * Applies SETTLE to coordinates and velocities, stored on GPU. Data at pointers d_xp and
src/gromacs/mdlib/settle_gpu.h:     * d_v change in the GPU memory. The results are not automatically copied back to the CPU
src/gromacs/mdlib/settle_gpu.h:     * \param[in]     d_x               Coordinates before timestep (in GPU memory)
src/gromacs/mdlib/settle_gpu.h:     * \param[in,out] d_xp              Coordinates after timestep (in GPU memory). The
src/gromacs/mdlib/settle_gpu.h:     * \param[in,out] d_v               Velocities to update (in GPU memory, can be nullptr
src/gromacs/mdlib/settle_gpu.h:     * Updates the constraints data and copies it to the GPU. Should be
src/gromacs/mdlib/settle_gpu.h:     * All three atoms from single water molecule should be handled by the same GPU.
src/gromacs/mdlib/settle_gpu.h:    //! GPU context object
src/gromacs/mdlib/settle_gpu.h:    //! GPU stream
src/gromacs/mdlib/settle_gpu.h:    //! Scaled virial tensor (9 reals, GPU)
src/gromacs/mdlib/settle_gpu.h:    //! Scaled virial tensor (9 reals, GPU)
src/gromacs/mdlib/settle_gpu.h:    //! Indexes of atoms (.i for oxygen, .j and.k for hydrogens, GPU)
src/gromacs/mdlib/settle_gpu.h:#endif // GMX_MDLIB_SETTLE_GPU_H
src/gromacs/mdlib/update_constrain_gpu_internal.h: * \brief Declares GPU implementations of backend-specific update-constraints functions.
src/gromacs/mdlib/update_constrain_gpu_internal.h:#ifndef GMX_MDLIB_UPDATE_CONSTRAIN_GPU_INTERNAL_H
src/gromacs/mdlib/update_constrain_gpu_internal.h:#define GMX_MDLIB_UPDATE_CONSTRAIN_GPU_INTERNAL_H
src/gromacs/mdlib/update_constrain_gpu_internal.h:#include "gromacs/gpu_utils/device_stream.h"
src/gromacs/mdlib/update_constrain_gpu_internal.h:#include "gromacs/gpu_utils/devicebuffer_datatype.h"
src/gromacs/mdlib/update_constrain_gpu_internal.h:#include "gromacs/gpu_utils/gputraits.h"
src/gromacs/mdlib/update_constrain_gpu_internal.h:class GpuEventSynchronizer;
src/gromacs/mdlib/update_constrain_gpu_internal.h:#endif // GMX_MDLIB_UPDATE_CONSTRAIN_GPU_INTERNAL_H
src/gromacs/mdlib/mdgraph_gpu_impl.h: * \author Alan Gray <alang@nvidia.com>
src/gromacs/mdlib/mdgraph_gpu_impl.h:#include "gromacs/gpu_utils/device_stream_manager.h"
src/gromacs/mdlib/mdgraph_gpu_impl.h:#include "mdgraph_gpu.h"
src/gromacs/mdlib/mdgraph_gpu_impl.h:class MdGpuGraph::Impl
src/gromacs/mdlib/mdgraph_gpu_impl.h:    void startRecord(GpuEventSynchronizer* xReadyOnDeviceEvent);
src/gromacs/mdlib/mdgraph_gpu_impl.h:    void launchGraphMdStep(GpuEventSynchronizer* xUpdatedOnDeviceEvent);
src/gromacs/mdlib/mdgraph_gpu_impl.h:    void setAlternateStepPpTaskCompletionEvent(GpuEventSynchronizer* event);
src/gromacs/mdlib/mdgraph_gpu_impl.h:    GpuEventSynchronizer* getPpTaskCompletionEvent();
src/gromacs/mdlib/mdgraph_gpu_impl.h:#if GMX_GPU_CUDA
src/gromacs/mdlib/mdgraph_gpu_impl.h:    using Graph         = cudaGraph_t;
src/gromacs/mdlib/mdgraph_gpu_impl.h:    using GraphInstance = cudaGraphExec_t;
src/gromacs/mdlib/mdgraph_gpu_impl.h:#elif GMX_GPU_SYCL && GMX_HAVE_GPU_GRAPH_SUPPORT && defined(SYCL_EXT_ONEAPI_GRAPH) && SYCL_EXT_ONEAPI_GRAPH
src/gromacs/mdlib/mdgraph_gpu_impl.h:#elif GMX_HAVE_GPU_GRAPH_SUPPORT
src/gromacs/mdlib/mdgraph_gpu_impl.h:#    error "Configuration error: GPU Graphs enabled but not supported"
src/gromacs/mdlib/mdgraph_gpu_impl.h:    void enqueueEventFromAllPpRanksToRank0Stream(GpuEventSynchronizer* event, const DeviceStream& stream);
src/gromacs/mdlib/mdgraph_gpu_impl.h:    void enqueueRank0EventToAllPpStreams(GpuEventSynchronizer* event, const DeviceStream& stream);
src/gromacs/mdlib/mdgraph_gpu_impl.h:    //! Whether there is PME GPU task on this PP rank
src/gromacs/mdlib/mdgraph_gpu_impl.h:    bool haveGpuPmeOnThisPpRank_;
src/gromacs/mdlib/mdgraph_gpu_impl.h:    std::unique_ptr<GpuEventSynchronizer> helperEvent_;
src/gromacs/mdlib/mdgraph_gpu_impl.h:    std::unique_ptr<GpuEventSynchronizer> ppTaskCompletionEvent_;
src/gromacs/mdlib/mdgraph_gpu_impl.h:    GpuEventSynchronizer* alternateStepPpTaskCompletionEvent_;
src/gromacs/mdlib/lincs_gpu_internal.cu: * \brief Implements LINCS kernels using CUDA
src/gromacs/mdlib/lincs_gpu_internal.cu: * This file contains CUDA kernels of LINCS constraints algorithm.
src/gromacs/mdlib/lincs_gpu_internal.cu: * \author Alan Gray <alang@nvidia.com>
src/gromacs/mdlib/lincs_gpu_internal.cu:#include "lincs_gpu_internal.h"
src/gromacs/mdlib/lincs_gpu_internal.cu:#include "gromacs/gpu_utils/cuda_arch_utils.cuh"
src/gromacs/mdlib/lincs_gpu_internal.cu:#include "gromacs/gpu_utils/cudautils.cuh"
src/gromacs/mdlib/lincs_gpu_internal.cu:#include "gromacs/gpu_utils/devicebuffer.cuh"
src/gromacs/mdlib/lincs_gpu_internal.cu:#include "gromacs/gpu_utils/gputraits.h"
src/gromacs/mdlib/lincs_gpu_internal.cu:#include "gromacs/gpu_utils/typecasts_cuda_hip.h"
src/gromacs/mdlib/lincs_gpu_internal.cu:#include "gromacs/gpu_utils/vectype_ops_cuda.h"
src/gromacs/mdlib/lincs_gpu_internal.cu:#include "gromacs/mdlib/lincs_gpu.h"
src/gromacs/mdlib/lincs_gpu_internal.cu:#include "gromacs/pbcutil/pbc_aiuc_cuda.cuh"
src/gromacs/mdlib/lincs_gpu_internal.cu: * In GPU version, one thread is responsible for all computations for one constraint. The blocks are
src/gromacs/mdlib/lincs_gpu_internal.cu:        void lincs_kernel(LincsGpuKernelParameters kernelParams,
src/gromacs/mdlib/lincs_gpu_internal.cu: * Returns pointer to a CUDA kernel based on provided booleans.
src/gromacs/mdlib/lincs_gpu_internal.cu: * \return                      Pointer to CUDA kernel
src/gromacs/mdlib/lincs_gpu_internal.cu:void launchLincsGpuKernel(LincsGpuKernelParameters*   kernelParams,
src/gromacs/mdlib/lincs_gpu_internal.cu:    const auto kernelArgs = prepareGpuKernelArguments(kernelPtr,
src/gromacs/mdlib/lincs_gpu_internal.cu:    launchGpuKernel(kernelPtr,
src/gromacs/mdlib/settle_gpu.cpp: * \brief Implements SETTLE using GPU
src/gromacs/mdlib/settle_gpu.cpp: * This file contains implementation for the data management of GPU version of SETTLE constraints algorithm.
src/gromacs/mdlib/settle_gpu.cpp:#include "settle_gpu.h"
src/gromacs/mdlib/settle_gpu.cpp:#include "gromacs/gpu_utils/devicebuffer.h"
src/gromacs/mdlib/settle_gpu.cpp:#include "gromacs/gpu_utils/gputraits.h"
src/gromacs/mdlib/settle_gpu.cpp:#include "gromacs/mdlib/constraint_gpu_helpers.h"
src/gromacs/mdlib/settle_gpu.cpp:#include "gromacs/mdlib/settle_gpu_internal.h"
src/gromacs/mdlib/settle_gpu.cpp:void SettleGpu::apply(const DeviceBuffer<Float3>& d_x,
src/gromacs/mdlib/settle_gpu.cpp:    launchSettleGpuKernel(numSettles_,
src/gromacs/mdlib/settle_gpu.cpp:                h_virialScaled_.data(), &d_virialScaled_, 0, 6, deviceStream_, GpuApiCallBehavior::Sync, nullptr);
src/gromacs/mdlib/settle_gpu.cpp:SettleGpu::SettleGpu(const gmx_mtop_t& mtop, const DeviceContext& deviceContext, const DeviceStream& deviceStream) :
src/gromacs/mdlib/settle_gpu.cpp:                  "Real numbers should be in single precision in GPU code.");
src/gromacs/mdlib/settle_gpu.cpp:SettleGpu::~SettleGpu()
src/gromacs/mdlib/settle_gpu.cpp:void SettleGpu::set(const InteractionDefinitions& idef)
src/gromacs/mdlib/settle_gpu.cpp:                &d_atomIds_, h_atomIds_.data(), 0, numSettles_, deviceStream_, GpuApiCallBehavior::Sync, nullptr);
src/gromacs/mdlib/gpuforcereduction.h: * \brief Declares the GPU Force Reduction
src/gromacs/mdlib/gpuforcereduction.h: * \author Alan Gray <alang@nvidia.com>
src/gromacs/mdlib/gpuforcereduction.h:#ifndef GMX_MDLIB_GPUFORCEREDUCTION_H
src/gromacs/mdlib/gpuforcereduction.h:#define GMX_MDLIB_GPUFORCEREDUCTION_H
src/gromacs/mdlib/gpuforcereduction.h:#include "gromacs/gpu_utils/devicebuffer_datatype.h"
src/gromacs/mdlib/gpuforcereduction.h:class GpuEventSynchronizer;
src/gromacs/mdlib/gpuforcereduction.h:#define HAVE_GPU_FORCE_REDUCTION (GMX_GPU_CUDA || GMX_GPU_SYCL)
src/gromacs/mdlib/gpuforcereduction.h: * \brief Manages the force reduction directly in GPU memory
src/gromacs/mdlib/gpuforcereduction.h: * Manages the reduction of multiple GPU force buffers into a single
src/gromacs/mdlib/gpuforcereduction.h: * GPU force buffer. The reduction involves at least one (input/output)
src/gromacs/mdlib/gpuforcereduction.h:class GpuForceReduction
src/gromacs/mdlib/gpuforcereduction.h:    /*! \brief Creates GPU force reduction object
src/gromacs/mdlib/gpuforcereduction.h:     * \param [in] deviceContext GPU device context
src/gromacs/mdlib/gpuforcereduction.h:    GpuForceReduction(const DeviceContext& deviceContext,
src/gromacs/mdlib/gpuforcereduction.h:    ~GpuForceReduction();
src/gromacs/mdlib/gpuforcereduction.h:    void addDependency(GpuEventSynchronizer* dependency);
src/gromacs/mdlib/gpuforcereduction.h:    /*! \brief Reinitialize the GPU force reduction
src/gromacs/mdlib/gpuforcereduction.h:                GpuEventSynchronizer* completionMarker = nullptr);
src/gromacs/mdlib/gpuforcereduction_impl_internal_sycl.cpp: * \brief Implements GPU Force Reduction using SYCL
src/gromacs/mdlib/gpuforcereduction_impl_internal_sycl.cpp: * \author Alan Gray <alang@nvidia.com>
src/gromacs/mdlib/gpuforcereduction_impl_internal_sycl.cpp:#include "gromacs/gpu_utils/devicebuffer.h"
src/gromacs/mdlib/gpuforcereduction_impl_internal_sycl.cpp:#include "gromacs/gpu_utils/gmxsycl.h"
src/gromacs/mdlib/gpuforcereduction_impl_internal_sycl.cpp:#include "gromacs/gpu_utils/gpu_utils.h"
src/gromacs/mdlib/gpuforcereduction_impl_internal_sycl.cpp:#include "gromacs/gpu_utils/gpueventsynchronizer.h"
src/gromacs/mdlib/gpuforcereduction_impl_internal_sycl.cpp:#include "gpuforcereduction_impl_internal.h"
src/gromacs/mdlib/leapfrog_gpu_internal_sycl.cpp:#include "gromacs/gpu_utils/devicebuffer.h"
src/gromacs/mdlib/leapfrog_gpu_internal_sycl.cpp:#include "gromacs/gpu_utils/gmxsycl.h"
src/gromacs/mdlib/leapfrog_gpu_internal_sycl.cpp:#include "gromacs/mdlib/leapfrog_gpu.h"
src/gromacs/mdlib/leapfrog_gpu_internal_sycl.cpp:#include "leapfrog_gpu_internal.h"
src/gromacs/mdlib/leapfrog_gpu_internal_sycl.cpp: *  The coordinates and velocities are updated on the GPU. Also saves the intermediate values of the coordinates for
src/gromacs/mdlib/leapfrog_gpu_internal_sycl.cpp: *  Each GPU thread works with a single particle.
src/gromacs/mdlib/update_constrain_gpu_internal.cu: * \brief Implements backend-specific functions of the update-constraints in CUDA.
src/gromacs/mdlib/update_constrain_gpu_internal.cu:#include "update_constrain_gpu_internal.h"
src/gromacs/mdlib/update_constrain_gpu_internal.cu:#include "gromacs/gpu_utils/cudautils.cuh"
src/gromacs/mdlib/update_constrain_gpu_internal.cu:#include "gromacs/gpu_utils/typecasts_cuda_hip.h"
src/gromacs/mdlib/update_constrain_gpu_internal.cu:#include "gromacs/gpu_utils/vectype_ops_cuda.h"
src/gromacs/mdlib/update_constrain_gpu_internal.cu:/*!\brief Number of CUDA threads in a block
src/gromacs/mdlib/update_constrain_gpu_internal.cu:    const auto kernelArgs = prepareGpuKernelArguments(
src/gromacs/mdlib/update_constrain_gpu_internal.cu:    launchGpuKernel(scaleCoordinates_kernel,
src/gromacs/mdlib/gpuforcereduction_impl.h: * \brief Declares the GPU Force Reduction
src/gromacs/mdlib/gpuforcereduction_impl.h: * \author Alan Gray <alang@nvidia.com>
src/gromacs/mdlib/gpuforcereduction_impl.h:#ifndef GMX_MDLIB_GPUFORCEREDUCTION_IMPL_H
src/gromacs/mdlib/gpuforcereduction_impl.h:#define GMX_MDLIB_GPUFORCEREDUCTION_IMPL_H
src/gromacs/mdlib/gpuforcereduction_impl.h:#include "gromacs/gpu_utils/device_stream.h"
src/gromacs/mdlib/gpuforcereduction_impl.h:#include "gromacs/gpu_utils/devicebuffer_datatype.h"
src/gromacs/mdlib/gpuforcereduction_impl.h:#include "gromacs/gpu_utils/gputraits.h"
src/gromacs/mdlib/gpuforcereduction_impl.h:#include "gpuforcereduction.h"
src/gromacs/mdlib/gpuforcereduction_impl.h:class GpuForceReduction::Impl
src/gromacs/mdlib/gpuforcereduction_impl.h:    /*! \brief Creates GPU force reduction object
src/gromacs/mdlib/gpuforcereduction_impl.h:     * \param [in] deviceContext GPU device context
src/gromacs/mdlib/gpuforcereduction_impl.h:    void addDependency(GpuEventSynchronizer* dependency);
src/gromacs/mdlib/gpuforcereduction_impl.h:    /*! \brief Reinitialize the GPU force reduction
src/gromacs/mdlib/gpuforcereduction_impl.h:                GpuEventSynchronizer* completionMarker = nullptr);
src/gromacs/mdlib/gpuforcereduction_impl.h:    //! GPU context object
src/gromacs/mdlib/gpuforcereduction_impl.h:    gmx::FixedCapacityVector<GpuEventSynchronizer*, 3> dependencyList_;
src/gromacs/mdlib/gpuforcereduction_impl.h:    GpuEventSynchronizer* completionMarker_ = nullptr;
src/gromacs/mdlib/gpuforcereduction_impl.cpp: * \brief Implements backend-agnostic GPU Force Reduction functions
src/gromacs/mdlib/gpuforcereduction_impl.cpp: * \author Alan Gray <alang@nvidia.com>
src/gromacs/mdlib/gpuforcereduction_impl.cpp:#include "gpuforcereduction_impl.h"
src/gromacs/mdlib/gpuforcereduction_impl.cpp:#include "gromacs/gpu_utils/device_stream.h"
src/gromacs/mdlib/gpuforcereduction_impl.cpp:#include "gromacs/gpu_utils/devicebuffer.h"
src/gromacs/mdlib/gpuforcereduction_impl.cpp:#include "gromacs/gpu_utils/gpueventsynchronizer.h"
src/gromacs/mdlib/gpuforcereduction_impl.cpp:#include "gromacs/mdlib/gpuforcereduction_impl_internal.h"
src/gromacs/mdlib/gpuforcereduction_impl.cpp:GpuForceReduction::Impl::Impl(const DeviceContext& deviceContext,
src/gromacs/mdlib/gpuforcereduction_impl.cpp:void GpuForceReduction::Impl::reinit(DeviceBuffer<Float3>  baseForcePtr,
src/gromacs/mdlib/gpuforcereduction_impl.cpp:                                     GpuEventSynchronizer* completionMarker)
src/gromacs/mdlib/gpuforcereduction_impl.cpp:    wallcycle_start_nocount(wcycle_, WallCycleCounter::LaunchGpuPp);
src/gromacs/mdlib/gpuforcereduction_impl.cpp:                       GpuApiCallBehavior::Async,
src/gromacs/mdlib/gpuforcereduction_impl.cpp:    wallcycle_stop(wcycle_, WallCycleCounter::LaunchGpuPp);
src/gromacs/mdlib/gpuforcereduction_impl.cpp:void GpuForceReduction::Impl::registerNbnxmForce(DeviceBuffer<RVec> forcePtr)
src/gromacs/mdlib/gpuforcereduction_impl.cpp:void GpuForceReduction::Impl::registerRvecForce(DeviceBuffer<RVec> forcePtr)
src/gromacs/mdlib/gpuforcereduction_impl.cpp:void GpuForceReduction::Impl::registerForcesReadyNvshmemFlags(DeviceBuffer<uint64_t> syncObj)
src/gromacs/mdlib/gpuforcereduction_impl.cpp:void GpuForceReduction::Impl::addDependency(GpuEventSynchronizer* dependency)
src/gromacs/mdlib/gpuforcereduction_impl.cpp:void GpuForceReduction::Impl::execute()
src/gromacs/mdlib/gpuforcereduction_impl.cpp:    wallcycle_start_nocount(wcycle_, WallCycleCounter::LaunchGpuPp);
src/gromacs/mdlib/gpuforcereduction_impl.cpp:    wallcycle_sub_start(wcycle_, WallCycleSubCounter::LaunchGpuNBFBufOps);
src/gromacs/mdlib/gpuforcereduction_impl.cpp:    wallcycle_sub_stop(wcycle_, WallCycleSubCounter::LaunchGpuNBFBufOps);
src/gromacs/mdlib/gpuforcereduction_impl.cpp:    wallcycle_stop(wcycle_, WallCycleCounter::LaunchGpuPp);
src/gromacs/mdlib/gpuforcereduction_impl.cpp:GpuForceReduction::Impl::~Impl()
src/gromacs/mdlib/gpuforcereduction_impl.cpp:GpuForceReduction::GpuForceReduction(const DeviceContext& deviceContext,
src/gromacs/mdlib/gpuforcereduction_impl.cpp:void GpuForceReduction::registerNbnxmForce(DeviceBuffer<RVec> forcePtr)
src/gromacs/mdlib/gpuforcereduction_impl.cpp:void GpuForceReduction::registerRvecForce(DeviceBuffer<RVec> forcePtr)
src/gromacs/mdlib/gpuforcereduction_impl.cpp:void GpuForceReduction::addDependency(GpuEventSynchronizer* dependency)
src/gromacs/mdlib/gpuforcereduction_impl.cpp:void GpuForceReduction::reinit(DeviceBuffer<RVec>    baseForcePtr,
src/gromacs/mdlib/gpuforcereduction_impl.cpp:                               GpuEventSynchronizer* completionMarker)
src/gromacs/mdlib/gpuforcereduction_impl.cpp:void GpuForceReduction::execute()
src/gromacs/mdlib/gpuforcereduction_impl.cpp:void GpuForceReduction::registerForcesReadyNvshmemFlags(DeviceBuffer<uint64_t> syncObj)
src/gromacs/mdlib/gpuforcereduction_impl.cpp:GpuForceReduction::~GpuForceReduction() = default;
src/gromacs/mdlib/update_constrain_gpu_impl_stubs.cpp:#include "gromacs/gpu_utils/devicebuffer_datatype.h"
src/gromacs/mdlib/update_constrain_gpu_impl_stubs.cpp:#include "gromacs/mdlib/update_constrain_gpu.h"
src/gromacs/mdlib/update_constrain_gpu_impl_stubs.cpp:class GpuEventSynchronizer;
src/gromacs/mdlib/update_constrain_gpu_impl_stubs.cpp:#if !GMX_GPU_CUDA && !GMX_GPU_SYCL
src/gromacs/mdlib/update_constrain_gpu_impl_stubs.cpp:class UpdateConstrainGpu::Impl
src/gromacs/mdlib/update_constrain_gpu_impl_stubs.cpp:UpdateConstrainGpu::UpdateConstrainGpu(const t_inputrec& /* ir   */,
src/gromacs/mdlib/update_constrain_gpu_impl_stubs.cpp:UpdateConstrainGpu::~UpdateConstrainGpu() = default;
src/gromacs/mdlib/update_constrain_gpu_impl_stubs.cpp:void UpdateConstrainGpu::integrate(GpuEventSynchronizer* /* fReadyOnDevice */,
src/gromacs/mdlib/update_constrain_gpu_impl_stubs.cpp:void UpdateConstrainGpu::scaleCoordinates(const Matrix3x3& /* scalingMatrix */)
src/gromacs/mdlib/update_constrain_gpu_impl_stubs.cpp:void UpdateConstrainGpu::scaleVelocities(const Matrix3x3& /* scalingMatrix */)
src/gromacs/mdlib/update_constrain_gpu_impl_stubs.cpp:void UpdateConstrainGpu::set(DeviceBuffer<RVec> /* d_x */,
src/gromacs/mdlib/update_constrain_gpu_impl_stubs.cpp:void UpdateConstrainGpu::setPbc(const PbcType /* pbcType */, const matrix /* box */)
src/gromacs/mdlib/update_constrain_gpu_impl_stubs.cpp:GpuEventSynchronizer* UpdateConstrainGpu::xUpdatedOnDeviceEvent()
src/gromacs/mdlib/update_constrain_gpu_impl_stubs.cpp:bool UpdateConstrainGpu::isNumCoupledConstraintsSupported(const gmx_mtop_t& /* mtop */)
src/gromacs/mdlib/update_constrain_gpu_impl_stubs.cpp:bool UpdateConstrainGpu::areConstraintsSupported()
src/gromacs/mdlib/update_constrain_gpu_impl_stubs.cpp:#endif /* !GMX_GPU_CUDA && !GMX_GPU_SYCL */
src/gromacs/gmxlib/nrnb.cpp:     * Plain Coulomb runs through the RF kernels, except with GPUs.
src/gromacs/gmxlib/nrnb.cpp:     * The flops are equal for plain-C, x86 SIMD and GPUs, except for:
src/gromacs/gmxlib/nrnb.cpp:     * - GPU always does exclusions, which requires 2-4 flops, but as invsqrt
src/gromacs/utility/baseversion.cpp:const char* getGpuImplementationString()
src/gromacs/utility/baseversion.cpp:    if (GMX_GPU)
src/gromacs/utility/baseversion.cpp:        if (GMX_GPU_CUDA)
src/gromacs/utility/baseversion.cpp:            return "CUDA";
src/gromacs/utility/baseversion.cpp:        else if (GMX_GPU_OPENCL)
src/gromacs/utility/baseversion.cpp:            return "OpenCL";
src/gromacs/utility/baseversion.cpp:        else if (GMX_GPU_HIP)
src/gromacs/utility/baseversion.cpp:        else if (GMX_GPU_SYCL)
src/gromacs/utility/baseversion.cpp:            GMX_RELEASE_ASSERT(false, "Unknown GPU configuration");
src/gromacs/utility/coolstuff.cpp:        { "Today we're not going to optimize our CUDA code, cause that's just a rabbit hole of "
src/gromacs/utility/coolstuff.cpp:          "Jen-Hsun Huang, CEO NVIDIA" },
src/gromacs/utility/hip_version_information.cpp:#if GMX_GPU_HIP
src/gromacs/utility/hip_version_information.cpp:#    include "gromacs/gpu_utils/hiputils.h"
src/gromacs/utility/hip_version_information.cpp:#if GMX_GPU_HIP
src/gromacs/utility/mpiinfo.cpp:GpuAwareMpiStatus checkMpiCudaAwareSupport()
src/gromacs/utility/mpiinfo.cpp:#if MPI_SUPPORTS_CUDA_AWARE_DETECTION
src/gromacs/utility/mpiinfo.cpp:    // With OMPI version <=4.x, this function doesn't check if UCX PML is built with CUDA-support
src/gromacs/utility/mpiinfo.cpp:    // or if CUDA is disabled at runtime.
src/gromacs/utility/mpiinfo.cpp:    GpuAwareMpiStatus status = (MPIX_Query_cuda_support() == 1) ? GpuAwareMpiStatus::Supported
src/gromacs/utility/mpiinfo.cpp:                                                                : GpuAwareMpiStatus::NotSupported;
src/gromacs/utility/mpiinfo.cpp:    GpuAwareMpiStatus status = GpuAwareMpiStatus::NotSupported;
src/gromacs/utility/mpiinfo.cpp:    if (status != GpuAwareMpiStatus::Supported && getenv("GMX_FORCE_GPU_AWARE_MPI") != nullptr)
src/gromacs/utility/mpiinfo.cpp:        status = GpuAwareMpiStatus::Forced;
src/gromacs/utility/mpiinfo.cpp:GpuAwareMpiStatus checkMpiHipAwareSupport()
src/gromacs/utility/mpiinfo.cpp:    GpuAwareMpiStatus status = (MPIX_Query_hip_support() == 1) ? GpuAwareMpiStatus::Supported
src/gromacs/utility/mpiinfo.cpp:                                                               : GpuAwareMpiStatus::NotSupported;
src/gromacs/utility/mpiinfo.cpp:#elif MPI_SUPPORTS_ROCM_AWARE_DETECTION
src/gromacs/utility/mpiinfo.cpp:    GpuAwareMpiStatus status = (MPIX_Query_rocm_support() == 1) ? GpuAwareMpiStatus::Supported
src/gromacs/utility/mpiinfo.cpp:                                                                : GpuAwareMpiStatus::NotSupported;
src/gromacs/utility/mpiinfo.cpp:    GpuAwareMpiStatus status = GpuAwareMpiStatus::NotSupported;
src/gromacs/utility/mpiinfo.cpp:    if (status != GpuAwareMpiStatus::Supported && getenv("GMX_FORCE_GPU_AWARE_MPI") != nullptr)
src/gromacs/utility/mpiinfo.cpp:        status = GpuAwareMpiStatus::Forced;
src/gromacs/utility/mpiinfo.cpp:GpuAwareMpiStatus checkMpiZEAwareSupport()
src/gromacs/utility/mpiinfo.cpp:    GpuAwareMpiStatus status = GpuAwareMpiStatus::NotSupported;
src/gromacs/utility/mpiinfo.cpp:        status = GpuAwareMpiStatus::Supported;
src/gromacs/utility/mpiinfo.cpp:        // If so, then we can decide whether it supports GPU-aware
src/gromacs/utility/mpiinfo.cpp:                // GPU-aware MPI.
src/gromacs/utility/mpiinfo.cpp:                    // Now check whether they did ask for GPU-aware MPI.  Note
src/gromacs/utility/mpiinfo.cpp:                    // because I_MPI_OFFLOAD=0 means no GPU-aware support.
src/gromacs/utility/mpiinfo.cpp:                        status = GpuAwareMpiStatus::Supported;
src/gromacs/utility/mpiinfo.cpp:    if (status != GpuAwareMpiStatus::Supported && getenv("GMX_FORCE_GPU_AWARE_MPI") != nullptr)
src/gromacs/utility/mpiinfo.cpp:        status = GpuAwareMpiStatus::Forced;
src/gromacs/utility/tests/alignedallocator_impl.h: * tests, which is currently needed because gpu_utils is physically
src/gromacs/utility/sycl_version_information.cpp:#if GMX_GPU_SYCL
src/gromacs/utility/sycl_version_information.cpp:#    include "gromacs/gpu_utils/gmxsycl.h"
src/gromacs/utility/cuda_version_information.h:#ifndef GMX_UTILITY_CUDA_VERSION_INFORMATION_H
src/gromacs/utility/cuda_version_information.h:#define GMX_UTILITY_CUDA_VERSION_INFORMATION_H
src/gromacs/utility/cuda_version_information.h://! Returns a string of the CUDA driver version.
src/gromacs/utility/cuda_version_information.h:std::string getCudaDriverVersionString();
src/gromacs/utility/cuda_version_information.h://! Returns a string of the CUDA runtime version.
src/gromacs/utility/cuda_version_information.h:std::string getCudaRuntimeVersionString();
src/gromacs/utility/include/gromacs/utility/allocator.h: * e.g. with SIMD alignment, GPU host-side page locking, or perhaps
src/gromacs/utility/include/gromacs/utility/allocator.h: * transfer to a GPU. That code needs to specify an Allocator that can
src/gromacs/utility/include/gromacs/utility/allocator.h: * that will not know until run time whether a GPU transfer will
src/gromacs/utility/include/gromacs/utility/defaultinitializationallocator.h:     * \todo Use std::is_nothrow_default_constructible_v when CUDA 11 is a requirement.
src/gromacs/utility/include/gromacs/utility/alignedallocator.h: * asynchronous transfer to GPU devices.
src/gromacs/utility/include/gromacs/utility/alignedallocator.h: * asynchronous transfer between a GPU device and the host.  The
src/gromacs/utility/include/gromacs/utility/logger.h:    // Should be explicit, once that works in CUDA.
src/gromacs/utility/include/gromacs/utility/logger.h:    // Both of the below should be explicit, once that works in CUDA.
src/gromacs/utility/include/gromacs/utility/mpiinfo.h:/*! \brief Enum describing GPU-aware support in underlying MPI library.
src/gromacs/utility/include/gromacs/utility/mpiinfo.h:enum class GpuAwareMpiStatus : int
src/gromacs/utility/include/gromacs/utility/mpiinfo.h:    NotSupported = 0, //!< GPU-aware support NOT available or not known.
src/gromacs/utility/include/gromacs/utility/mpiinfo.h:    Forced,           //!< GPU-aware support forced using env variable
src/gromacs/utility/include/gromacs/utility/mpiinfo.h:    Supported,        //!< GPU-aware support available.
src/gromacs/utility/include/gromacs/utility/mpiinfo.h: * Wrapper on top of \c MPIX_Query_cuda_support()
src/gromacs/utility/include/gromacs/utility/mpiinfo.h: * robust enough to detect CUDA-aware support at runtime correctly e.g. when UCX PML is used
src/gromacs/utility/include/gromacs/utility/mpiinfo.h: * or CUDA is disabled at runtime
src/gromacs/utility/include/gromacs/utility/mpiinfo.h: * \returns     CUDA-aware status in MPI implementation */
src/gromacs/utility/include/gromacs/utility/mpiinfo.h:GpuAwareMpiStatus checkMpiCudaAwareSupport();
src/gromacs/utility/include/gromacs/utility/mpiinfo.h: * Wrapper on top of \c MPIX_Query_hip_support() or \c MPIX_Query_rocm_support().
src/gromacs/utility/include/gromacs/utility/mpiinfo.h:GpuAwareMpiStatus checkMpiHipAwareSupport();
src/gromacs/utility/include/gromacs/utility/mpiinfo.h:GpuAwareMpiStatus checkMpiZEAwareSupport();
src/gromacs/utility/CMakeLists.txt:if (GMX_GPU_CUDA)
src/gromacs/utility/CMakeLists.txt:    set_source_files_properties(cuda_version_information.cu PROPERTIES LANGUAGE CUDA)
src/gromacs/utility/CMakeLists.txt:    set_property(GLOBAL APPEND PROPERTY GMX_LIBGROMACS_SOURCES ${CMAKE_CURRENT_SOURCE_DIR}/cuda_version_information.cu)
src/gromacs/utility/CMakeLists.txt:if (GMX_GPU_HIP)
src/gromacs/utility/CMakeLists.txt:if (GMX_GPU_SYCL)
src/gromacs/utility/binaryinformation.cpp:#if GMX_GPU_FFT_ONEMKL
src/gromacs/utility/binaryinformation.cpp:#include "cuda_version_information.h"
src/gromacs/utility/binaryinformation.cpp:#if GMX_GPU_FFT_ONEMKL
src/gromacs/utility/binaryinformation.cpp:#    ifdef ENABLE_MKLGPU_BACKEND
src/gromacs/utility/binaryinformation.cpp:    description += " MKLGPU";
src/gromacs/utility/binaryinformation.cpp://! Construct a string that describes the library that provides GPU FFT support to this build
src/gromacs/utility/binaryinformation.cpp:std::string getGpuFftDescriptionString()
src/gromacs/utility/binaryinformation.cpp:    if (GMX_GPU)
src/gromacs/utility/binaryinformation.cpp:        if (GMX_GPU_FFT_CUFFT)
src/gromacs/utility/binaryinformation.cpp:        else if (GMX_GPU_FFT_CLFFT)
src/gromacs/utility/binaryinformation.cpp:        else if (GMX_GPU_FFT_VKFFT)
src/gromacs/utility/binaryinformation.cpp:        else if (GMX_GPU_FFT_MKL)
src/gromacs/utility/binaryinformation.cpp:        else if (GMX_GPU_FFT_ONEMKL)
src/gromacs/utility/binaryinformation.cpp:        else if (GMX_GPU_FFT_ROCFFT)
src/gromacs/utility/binaryinformation.cpp:        else if (GMX_GPU_FFT_HIPFFT)
src/gromacs/utility/binaryinformation.cpp:        else if (GMX_GPU_FFT_BBFFT)
src/gromacs/utility/binaryinformation.cpp:            /* Some SYCL builds have no support for GPU FFT,
src/gromacs/utility/binaryinformation.cpp:            GMX_RELEASE_ASSERT(GMX_GPU_SYCL,
src/gromacs/utility/binaryinformation.cpp:                               "Only the SYCL build can function without a GPU FFT library");
src/gromacs/utility/binaryinformation.cpp: * that provides multi-GPU FFT support to this build */
src/gromacs/utility/binaryinformation.cpp:std::string getMultiGpuFftDescriptionString()
src/gromacs/utility/binaryinformation.cpp:        if (GMX_GPU_FFT_CUFFT)
src/gromacs/utility/binaryinformation.cpp:            // This could be either in a CUDA or SYCL build, but the
src/gromacs/utility/binaryinformation.cpp:        else if (GMX_GPU_HIP && GMX_GPU_FFT_HIPFFT)
src/gromacs/utility/binaryinformation.cpp:        else if (GMX_GPU_SYCL && GMX_GPU_FFT_MKL)
src/gromacs/utility/binaryinformation.cpp:        else if ((GMX_GPU_SYCL || GMX_GPU_HIP) && GMX_GPU_FFT_ROCFFT)
src/gromacs/utility/binaryinformation.cpp:    std::vector<std::string> gpuAwareBackendsSupported;
src/gromacs/utility/binaryinformation.cpp:    if (gmx::checkMpiCudaAwareSupport() == gmx::GpuAwareMpiStatus::Supported)
src/gromacs/utility/binaryinformation.cpp:        gpuAwareBackendsSupported.emplace_back("CUDA");
src/gromacs/utility/binaryinformation.cpp:    if (gmx::checkMpiHipAwareSupport() == gmx::GpuAwareMpiStatus::Supported)
src/gromacs/utility/binaryinformation.cpp:        gpuAwareBackendsSupported.emplace_back("HIP");
src/gromacs/utility/binaryinformation.cpp:    if (gmx::checkMpiZEAwareSupport() == gmx::GpuAwareMpiStatus::Supported)
src/gromacs/utility/binaryinformation.cpp:        gpuAwareBackendsSupported.emplace_back("LevelZero");
src/gromacs/utility/binaryinformation.cpp:    if (!gpuAwareBackendsSupported.empty())
src/gromacs/utility/binaryinformation.cpp:        writer->writeLine(formatString("MPI library:         MPI (GPU-aware: %s)",
src/gromacs/utility/binaryinformation.cpp:                                       gmx::joinStrings(gpuAwareBackendsSupported, ", ").c_str()));
src/gromacs/utility/binaryinformation.cpp:    writer->writeLine(formatString("GPU support:         %s", getGpuImplementationString()));
src/gromacs/utility/binaryinformation.cpp:#if GMX_GPU
src/gromacs/utility/binaryinformation.cpp:    std::string infoStr = (GMX_GPU_NB_DISABLE_CLUSTER_PAIR_SPLIT) ? " (cluster-pair splitting off)" : "";
src/gromacs/utility/binaryinformation.cpp:    writer->writeLine(formatString("NBNxM GPU setup:     super-cluster %dx%dx%d / cluster %d%s",
src/gromacs/utility/binaryinformation.cpp:                                   GMX_GPU_NB_NUM_CLUSTER_PER_CELL_X,
src/gromacs/utility/binaryinformation.cpp:                                   GMX_GPU_NB_NUM_CLUSTER_PER_CELL_Y,
src/gromacs/utility/binaryinformation.cpp:                                   GMX_GPU_NB_NUM_CLUSTER_PER_CELL_Z,
src/gromacs/utility/binaryinformation.cpp:                                   GMX_GPU_NB_CLUSTER_SIZE,
src/gromacs/utility/binaryinformation.cpp:    writer->writeLine(formatString("GPU FFT library:     %s", getGpuFftDescriptionString().c_str()));
src/gromacs/utility/binaryinformation.cpp:    writer->writeLine(formatString("Multi-GPU FFT:       %s", getMultiGpuFftDescriptionString().c_str()));
src/gromacs/utility/binaryinformation.cpp:#if GMX_GPU_OPENCL
src/gromacs/utility/binaryinformation.cpp:    writer->writeLine(formatString("OpenCL include dir:  %s", OPENCL_INCLUDE_DIR));
src/gromacs/utility/binaryinformation.cpp:    writer->writeLine(formatString("OpenCL library:      %s", OPENCL_LIBRARY));
src/gromacs/utility/binaryinformation.cpp:    writer->writeLine(formatString("OpenCL version:      %s", OPENCL_VERSION_STRING));
src/gromacs/utility/binaryinformation.cpp:#if GMX_GPU_CUDA
src/gromacs/utility/binaryinformation.cpp:    writer->writeLine(formatString("CUDA compiler:       %s", CUDA_COMPILER_INFO));
src/gromacs/utility/binaryinformation.cpp:            "CUDA compiler flags:%s %s", CUDA_COMPILER_FLAGS, CMAKE_BUILD_CONFIGURATION_CXX_FLAGS));
src/gromacs/utility/binaryinformation.cpp:    writer->writeLine("CUDA driver:         " + gmx::getCudaDriverVersionString());
src/gromacs/utility/binaryinformation.cpp:    writer->writeLine("CUDA runtime:        " + gmx::getCudaRuntimeVersionString());
src/gromacs/utility/binaryinformation.cpp:    writer->writeLine(formatString("SYCL GPU flags:      %s", SYCL_ACPP_DEVICE_COMPILER_FLAGS));
src/gromacs/utility/binaryinformation.cpp:#if GMX_GPU_HIP
src/gromacs/utility/cuda_version_information.cu:#include "cuda_version_information.h"
src/gromacs/utility/cuda_version_information.cu:std::string getCudaDriverVersionString()
src/gromacs/utility/cuda_version_information.cu:    int cuda_driver = 0;
src/gromacs/utility/cuda_version_information.cu:    if (cudaDriverGetVersion(&cuda_driver) != cudaSuccess)
src/gromacs/utility/cuda_version_information.cu:    return formatString("%d.%d", cuda_driver / 1000, cuda_driver % 100);
src/gromacs/utility/cuda_version_information.cu:std::string getCudaRuntimeVersionString()
src/gromacs/utility/cuda_version_information.cu:    int cuda_runtime = 0;
src/gromacs/utility/cuda_version_information.cu:    if (cudaRuntimeGetVersion(&cuda_runtime) != cudaSuccess)
src/gromacs/utility/cuda_version_information.cu:    return formatString("%d.%d", cuda_runtime / 1000, cuda_runtime % 100);
src/gromacs/gmxpreprocess/readir.cpp:                    "With Verlet lists the optimal nstlist is >= 10, with GPUs >= 20. Note "
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp: * \brief Implements shared code for GPU halo exchange.
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp: * \author Alan Gray <alang@nvidia.com>
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:#include "gpuhaloexchange_impl_gpu.h"
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:#include "gromacs/domdec/gpuhaloexchange.h"
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:#include "gromacs/gpu_utils/device_context.h"
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:#include "gromacs/gpu_utils/devicebuffer.h"
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:#include "gromacs/gpu_utils/gpueventsynchronizer.h"
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:#include "gromacs/gpu_utils/nvshmem_utils.h"
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:        ((GMX_LIB_MPI != 0) && ((GMX_GPU_CUDA != 0) || (GMX_GPU_SYCL != 0)));
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:constexpr bool supportedThreadMpiBuild = ((GMX_THREAD_MPI != 0) && (GMX_GPU_CUDA != 0));
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:void GpuHaloExchange::Impl::reinitHalo(DeviceBuffer<Float3> d_coordinatesBuffer,
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:                       "Gpu Halo Exchange not supported in this build");
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:    wallcycle_sub_start(wcycle_, WallCycleSubCounter::DDGpu);
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:                   "Size mismatch between domain decomposition communication index array and GPU "
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:                &d_indexMap_, h_indexMap_.data(), 0, newSize, *haloStream_, GpuApiCallBehavior::Async, nullptr);
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:    // Exchange of remote addresses from neighboring ranks is needed only with peer-to-peer copies as cudaMemcpy needs both src/dst pointer
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:    wallcycle_sub_stop(wcycle_, WallCycleSubCounter::DDGpu);
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:void GpuHaloExchange::Impl::reinitNvshmemSignal(const t_commrec& cr, int signalObjOffset)
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:        // As only CUDA DeviceBuffer<> supports pointer updates from host side
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:        // we guard these pointer update code by GMX_GPU_CUDA
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:#if GMX_GPU_CUDA
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:void GpuHaloExchange::Impl::reinitXGridSizeAndDevBarrier()
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:            cuda::barrier<cuda::thread_scope_device> temp_bar(nvshmemHaloExchange_.arriveWaitBarrierVal_);
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:            cudaMemcpyAsync(nvshmemHaloExchange_.d_arriveWaitBarrier_,
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:                            sizeof(cuda::barrier<cuda::thread_scope_device>),
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:                            cudaMemcpyDefault,
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:void GpuHaloExchange::Impl::enqueueWaitRemoteCoordinatesReadyEvent(GpuEventSynchronizer* coordinatesReadyOnDeviceEvent)
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:    GpuEventSynchronizer* remoteCoordinatesReadyOnDeviceEvent;
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:                 sizeof(GpuEventSynchronizer*), //NOLINT(bugprone-sizeof-expression)
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:                 sizeof(GpuEventSynchronizer*), //NOLINT(bugprone-sizeof-expression)
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:GpuEventSynchronizer* GpuHaloExchange::Impl::communicateHaloCoordinates(const matrix box,
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:                                                                        GpuEventSynchronizer* dependencyEvent)
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:    wallcycle_start(wcycle_, WallCycleCounter::LaunchGpuPp);
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:    wallcycle_sub_start(wcycle_, WallCycleSubCounter::LaunchGpuMoveX);
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:    wallcycle_sub_stop(wcycle_, WallCycleSubCounter::LaunchGpuMoveX);
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:    wallcycle_stop(wcycle_, WallCycleCounter::LaunchGpuPp);
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:void GpuHaloExchange::Impl::communicateHaloForces(bool accumulateForces,
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:                                                  FixedCapacityVector<GpuEventSynchronizer*, 2>* dependencyEvents)
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:    wallcycle_start_nocount(wcycle_, WallCycleCounter::LaunchGpuPp);
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:    wallcycle_sub_start(wcycle_, WallCycleSubCounter::LaunchGpuMoveF);
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:    wallcycle_sub_stop(wcycle_, WallCycleSubCounter::LaunchGpuMoveF);
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:    wallcycle_stop(wcycle_, WallCycleCounter::LaunchGpuPp);
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:void GpuHaloExchange::Impl::communicateHaloData(Float3*  sendPtr,
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:        communicateHaloDataGpuAwareMpi(sendPtr, sendSize, sendRank, recvPtr, recvSize, recvRank);
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:void GpuHaloExchange::Impl::communicateHaloDataGpuAwareMpi(Float3* sendPtr,
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:    GMX_RELEASE_ASSERT(supportedLibMpiBuild, "Build does not support GPU-aware MPI");
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:        // activities, to ensure that buffer is up-to-date in GPU memory
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:void GpuHaloExchange::Impl::communicateHaloCoordinatesOutOfPlace(DeviceBuffer<Float3> d_sendPtr,
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:            h_outOfPlaceSendBuffer_.data(), &d_sendPtr, 0, sendSize, *haloStream_, GpuApiCallBehavior::Async, nullptr);
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:                           GpuApiCallBehavior::Async,
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:void GpuHaloExchange::Impl::communicateHaloForcesOutOfPlace(DeviceBuffer<Float3> d_sendPtr,
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:                             GpuApiCallBehavior::Async,
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:            &d_recvBuf_, h_outOfPlaceRecvBuffer_.data(), 0, recvSize, *haloStream_, GpuApiCallBehavior::Async, nullptr);
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:void GpuHaloExchange::Impl::communicateHaloDataPeerToPeer(Float3*  sendPtr,
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:#if GMX_GPU_CUDA
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:    cudaError_t stat;
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:        stat = cudaMemcpyAsync(remotePtr,
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:                               cudaMemcpyDeviceToDevice,
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:        CU_RET_ERR(stat, "cudaMemcpyAsync on GPU Domdec CUDA direct data transfer failed");
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:    GpuEventSynchronizer* haloDataTransferRemote;
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:    GpuEventSynchronizer* haloDataTransferLaunched = (haloType == HaloType::Coordinates)
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:                 sizeof(GpuEventSynchronizer*), //NOLINT(bugprone-sizeof-expression)
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:                 sizeof(GpuEventSynchronizer*), //NOLINT(bugprone-sizeof-expression)
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:GpuEventSynchronizer* GpuHaloExchange::Impl::getForcesReadyOnDeviceEvent()
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:/*! \brief Create GpuHaloExchange object */
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:GpuHaloExchange::Impl::Impl(gmx_domdec_t*        dd,
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:    haloXDataTransferLaunched_(GMX_THREAD_MPI ? new GpuEventSynchronizer() : nullptr),
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:    haloFDataTransferLaunched_(GMX_THREAD_MPI ? new GpuEventSynchronizer() : nullptr),
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:        gmx_fatal(FARGS, "Error: screw is not yet supported in GPU halo exchange\n");
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:GpuHaloExchange::Impl::~Impl()
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:        // happens in destroyGpuHaloExchangeNvshmemBuf() due to it
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:void GpuHaloExchange::Impl::destroyGpuHaloExchangeNvshmemBuf()
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:GpuHaloExchange::GpuHaloExchange(gmx_domdec_t*        dd,
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:GpuHaloExchange::GpuHaloExchange(GpuHaloExchange&&) noexcept = default;
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:GpuHaloExchange& GpuHaloExchange::operator=(GpuHaloExchange&& other) noexcept
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:GpuHaloExchange::~GpuHaloExchange() = default;
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:void GpuHaloExchange::reinitHalo(DeviceBuffer<RVec> d_coordinatesBuffer, DeviceBuffer<RVec> d_forcesBuffer)
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:void GpuHaloExchange::reinitNvshmemSignal(const t_commrec& cr, int signalObjOffset)
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:void GpuHaloExchange::destroyGpuHaloExchangeNvshmemBuf()
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:    impl_->destroyGpuHaloExchangeNvshmemBuf();
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:GpuEventSynchronizer* GpuHaloExchange::communicateHaloCoordinates(const matrix box,
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:                                                                  GpuEventSynchronizer* dependencyEvent)
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:void GpuHaloExchange::communicateHaloForces(bool accumulateForces,
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:                                            FixedCapacityVector<GpuEventSynchronizer*, 2>* dependencyEvents)
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cpp:GpuEventSynchronizer* GpuHaloExchange::getForcesReadyOnDeviceEvent()
src/gromacs/domdec/domdec_internal.h:    /**< The number of MPI ranks sharing the GPU our rank is using */
src/gromacs/domdec/domdec_internal.h:    int nrank_gpu_shared = 0;
src/gromacs/domdec/domdec_internal.h:    /**< The MPI load communicator for ranks sharing a GPU */
src/gromacs/domdec/domdec_internal.h:    MPI_Comm mpi_comm_gpu_shared;
src/gromacs/domdec/domdec.cpp:#include "gromacs/domdec/gpuhaloexchange.h"
src/gromacs/domdec/domdec.cpp:#include "gromacs/gpu_utils/device_stream_manager.h"
src/gromacs/domdec/domdec.cpp:#include "gromacs/gpu_utils/gpu_utils.h"
src/gromacs/domdec/domdec.cpp:#include "gromacs/gpu_utils/nvshmem_utils.h"
src/gromacs/domdec/domdec.cpp:        /* Only ranks with short-ranged tasks (currently) use GPUs.
src/gromacs/domdec/domdec.cpp:         * If we don't have GPUs assigned, there are no resources to share.
src/gromacs/domdec/domdec.cpp:        dd->comm->nrank_gpu_shared = 1;
src/gromacs/domdec/domdec.cpp:    MPI_Comm_split(mpi_comm_pp_physicalnode, uniqueDeviceId, dd->rank, &dd->comm->mpi_comm_gpu_shared);
src/gromacs/domdec/domdec.cpp:    MPI_Comm_size(dd->comm->mpi_comm_gpu_shared, &dd->comm->nrank_gpu_shared);
src/gromacs/domdec/domdec.cpp:    /* Note that some ranks could share a GPU, while others don't */
src/gromacs/domdec/domdec.cpp:    if (dd->comm->nrank_gpu_shared == 1)
src/gromacs/domdec/domdec.cpp:        MPI_Comm_free(&dd->comm->mpi_comm_gpu_shared);
src/gromacs/domdec/domdec.cpp: * \param [in] useGpuForPme         PME offloaded to GPU
src/gromacs/domdec/domdec.cpp: * \param [in] canUseGpuPmeDecomposition         GPU pme decomposition supported
src/gromacs/domdec/domdec.cpp:                                         const bool               useGpuForPme,
src/gromacs/domdec/domdec.cpp:                                         const bool               canUseGpuPmeDecomposition)
src/gromacs/domdec/domdec.cpp:    // Disable DLB when GPU PME decomposition is used
src/gromacs/domdec/domdec.cpp:    // GPU PME decomposition uses an extended halo exchange algorithm without any X/F-redistribution
src/gromacs/domdec/domdec.cpp:    if (useGpuForPme && canUseGpuPmeDecomposition && (options.numPmeRanks == 0 || options.numPmeRanks > 1))
src/gromacs/domdec/domdec.cpp:        std::string reasonStr = "it is not supported with GPU PME decomposition.";
src/gromacs/domdec/domdec.cpp:    /* Initialize to GPU share count to 0, might change later */
src/gromacs/domdec/domdec.cpp:    comm->nrank_gpu_shared = 0;
src/gromacs/domdec/domdec.cpp:                                const bool               useGpuForPme,
src/gromacs/domdec/domdec.cpp:                                const bool               canUseGpuPmeDecomposition)
src/gromacs/domdec/domdec.cpp:            mdlog, options, ddSettings.recordLoad, mdrunOptions, ir, useGpuForPme, canUseGpuPmeDecomposition);
src/gromacs/domdec/domdec.cpp:         bool                              useGpuForNonbonded,
src/gromacs/domdec/domdec.cpp:         bool                              useGpuForPme,
src/gromacs/domdec/domdec.cpp:         bool                              useGpuForUpdate,
src/gromacs/domdec/domdec.cpp:         bool                              useGpuDirectHalo,
src/gromacs/domdec/domdec.cpp:         bool                              canUseGpuPmeDecomposition);
src/gromacs/domdec/domdec.cpp:                                       bool                 useGpuForNonbonded,
src/gromacs/domdec/domdec.cpp:                                       bool                 useGpuForPme,
src/gromacs/domdec/domdec.cpp:                                       bool                 useGpuForUpdate,
src/gromacs/domdec/domdec.cpp:                                       bool                 useGpuDirectHalo,
src/gromacs/domdec/domdec.cpp:                                       bool                 canUseGpuPmeDecomposition) :
src/gromacs/domdec/domdec.cpp:    ddSettings_ = getDDSettings(mdlog_, options_, mdrunOptions, ir_, useGpuForPme, canUseGpuPmeDecomposition);
src/gromacs/domdec/domdec.cpp:            notifiers_, options_, numRanksRequested, useGpuForNonbonded, useGpuForPme, canUseGpuPmeDecomposition);
src/gromacs/domdec/domdec.cpp:    // Now that we know whether GPU-direct halos actually will be used, we might have to modify DLB
src/gromacs/domdec/domdec.cpp:    if (!isDlbDisabled(ddSettings_.initialDlbState) && useGpuForUpdate && useGpuDirectHalo)
src/gromacs/domdec/domdec.cpp:                        "Disabling dynamic load balancing; unsupported with GPU communication + "
src/gromacs/domdec/domdec.cpp:                                                       const bool           useGpuForNonbonded,
src/gromacs/domdec/domdec.cpp:                                                       const bool           useGpuForPme,
src/gromacs/domdec/domdec.cpp:                                                       bool                 useGpuForUpdate,
src/gromacs/domdec/domdec.cpp:                                                       bool                 useGpuDirectHalo,
src/gromacs/domdec/domdec.cpp:                                                       const bool canUseGpuPmeDecomposition) :
src/gromacs/domdec/domdec.cpp:                   useGpuForNonbonded,
src/gromacs/domdec/domdec.cpp:                   useGpuForPme,
src/gromacs/domdec/domdec.cpp:                   useGpuForUpdate,
src/gromacs/domdec/domdec.cpp:                   useGpuDirectHalo,
src/gromacs/domdec/domdec.cpp:                   canUseGpuPmeDecomposition))
src/gromacs/domdec/domdec.cpp:                               bool                           checkGpuDdLimitation)
src/gromacs/domdec/domdec.cpp:        /* The GPU halo communication code currently does not allow multiple
src/gromacs/domdec/domdec.cpp:        if (checkGpuDdLimitation && (!cr->dd->gpuHaloExchange[0].empty()) && d > 0 && np > 1)
src/gromacs/domdec/domdec.cpp:                      bool                           checkGpuDdLimitation)
src/gromacs/domdec/domdec.cpp:    bool bCutoffAllowed = test_dd_cutoff(cr, box, x, cutoffRequested, checkGpuDdLimitation);
src/gromacs/domdec/domdec.cpp:void constructGpuHaloExchange(const t_commrec&                cr,
src/gromacs/domdec/domdec.cpp:                       "GPU halo exchange.");
src/gromacs/domdec/domdec.cpp:                       "GPU halo exchange.");
src/gromacs/domdec/domdec.cpp:        for (int pulse = cr.dd->gpuHaloExchange[d].size(); pulse < cr.dd->comm->cd[d].numPulses(); pulse++)
src/gromacs/domdec/domdec.cpp:            cr.dd->gpuHaloExchange[d].push_back(std::make_unique<gmx::GpuHaloExchange>(
src/gromacs/domdec/domdec.cpp:void reinitGpuHaloExchange(const t_commrec&              cr,
src/gromacs/domdec/domdec.cpp:            cr.dd->gpuHaloExchange[d][pulse]->reinitHalo(d_coordinatesBuffer, d_forcesBuffer);
src/gromacs/domdec/domdec.cpp:            cr.dd->gpuHaloExchange[d][pulse]->reinitNvshmemSignal(cr, numDimsAndPulses++);
src/gromacs/domdec/domdec.cpp:void destroyGpuHaloExchangeNvshmemBuf(const t_commrec& cr)
src/gromacs/domdec/domdec.cpp:                cr.dd->gpuHaloExchange[d][pulse]->destroyGpuHaloExchangeNvshmemBuf();
src/gromacs/domdec/domdec.cpp:GpuEventSynchronizer* communicateGpuHaloCoordinates(const t_commrec&      cr,
src/gromacs/domdec/domdec.cpp:                                                    GpuEventSynchronizer* dependencyEvent)
src/gromacs/domdec/domdec.cpp:    GpuEventSynchronizer* eventPtr = dependencyEvent;
src/gromacs/domdec/domdec.cpp:            eventPtr = cr.dd->gpuHaloExchange[d][pulse]->communicateHaloCoordinates(box, eventPtr);
src/gromacs/domdec/domdec.cpp:void communicateGpuHaloForces(const t_commrec&                                    cr,
src/gromacs/domdec/domdec.cpp:                              gmx::FixedCapacityVector<GpuEventSynchronizer*, 2>* dependencyEvents)
src/gromacs/domdec/domdec.cpp:            cr.dd->gpuHaloExchange[d][pulse]->communicateHaloForces(accumulateForces, dependencyEvents);
src/gromacs/domdec/domdec.cpp:            dependencyEvents->push_back(cr.dd->gpuHaloExchange[d][pulse]->getForcesReadyOnDeviceEvent());
src/gromacs/domdec/dlbtiming.cpp:        isOpen(false), isOpenOnCpu(false), isOpenOnGpu(false), cyclesOpenCpu(0), cyclesLastCpu(0)
src/gromacs/domdec/dlbtiming.cpp:    bool         isOpenOnGpu;   /**< Is the, currently open, region open on the GPU side? */
src/gromacs/domdec/dlbtiming.cpp:        reg->isOpenOnGpu   = false;
src/gromacs/domdec/dlbtiming.cpp:void DDBalanceRegionHandler::openRegionGpuImpl() const
src/gromacs/domdec/dlbtiming.cpp:    GMX_ASSERT(reg->isOpen, "Can only open a GPU region inside an open CPU region");
src/gromacs/domdec/dlbtiming.cpp:    GMX_ASSERT(!reg->isOpenOnGpu, "Can not re-open a GPU balance region");
src/gromacs/domdec/dlbtiming.cpp:    reg->isOpenOnGpu = true;
src/gromacs/domdec/dlbtiming.cpp:    /* If the GPU is busy, don't reopen as we are overlapping with work */
src/gromacs/domdec/dlbtiming.cpp:    if (reg->isOpen && !reg->isOpenOnGpu)
src/gromacs/domdec/dlbtiming.cpp:        if (reg->isOpenOnGpu)
src/gromacs/domdec/dlbtiming.cpp:            /* Store the cycles for estimating the GPU/CPU overlap time */
src/gromacs/domdec/dlbtiming.cpp:void DDBalanceRegionHandler::closeRegionGpuImpl(float waitGpuCyclesInCpuRegion,
src/gromacs/domdec/dlbtiming.cpp:                                                DdBalanceRegionWaitedForGpu waitedForGpu) const
src/gromacs/domdec/dlbtiming.cpp:        GMX_ASSERT(reg->isOpenOnGpu, "Can not close a non-open GPU balance region");
src/gromacs/domdec/dlbtiming.cpp:                   "The GPU region should be closed after closing the CPU region");
src/gromacs/domdec/dlbtiming.cpp:        float waitGpuCyclesEstimate = gmx_cycles_read() - reg->cyclesLastCpu;
src/gromacs/domdec/dlbtiming.cpp:        if (waitedForGpu == DdBalanceRegionWaitedForGpu::no)
src/gromacs/domdec/dlbtiming.cpp:            waitGpuCyclesEstimate *= unknownWaitEstimateFactor;
src/gromacs/domdec/dlbtiming.cpp:        dd_cycles_add(dd_, cyclesCpu + waitGpuCyclesEstimate, ddCyclF);
src/gromacs/domdec/dlbtiming.cpp:        /* Register the total GPU wait time, to redistribute with GPU sharing */
src/gromacs/domdec/dlbtiming.cpp:        dd_cycles_add(dd_, waitGpuCyclesInCpuRegion + waitGpuCyclesEstimate, ddCyclWaitGPU);
src/gromacs/domdec/dlbtiming.cpp:        reg->isOpenOnGpu = false;
src/gromacs/domdec/gpuhaloexchange_impl_gpu.h: * \brief Declares the implementation of GPU Halo Exchange.
src/gromacs/domdec/gpuhaloexchange_impl_gpu.h: * \author Alan Gray <alang@nvidia.com>
src/gromacs/domdec/gpuhaloexchange_impl_gpu.h:#ifndef GMX_DOMDEC_GPUHALOEXCHANGE_IMPL_GPU_H
src/gromacs/domdec/gpuhaloexchange_impl_gpu.h:#define GMX_DOMDEC_GPUHALOEXCHANGE_IMPL_GPU_H
src/gromacs/domdec/gpuhaloexchange_impl_gpu.h:#include "gromacs/domdec/gpuhaloexchange.h"
src/gromacs/domdec/gpuhaloexchange_impl_gpu.h:#include "gromacs/gpu_utils/device_context.h"
src/gromacs/domdec/gpuhaloexchange_impl_gpu.h:#include "gromacs/gpu_utils/gpueventsynchronizer.h"
src/gromacs/domdec/gpuhaloexchange_impl_gpu.h:#include "gromacs/gpu_utils/hostallocator.h"
src/gromacs/domdec/gpuhaloexchange_impl_gpu.h:#    include <cuda/barrier>
src/gromacs/domdec/gpuhaloexchange_impl_gpu.h:#if GMX_GPU_SYCL
src/gromacs/domdec/gpuhaloexchange_impl_gpu.h:#    include "gromacs/gpu_utils/gputraits_sycl.h"
src/gromacs/domdec/gpuhaloexchange_impl_gpu.h:/*! \internal \brief Struct encapsulating data for NVSHMEM Enabled GPU Halo Exchange */
src/gromacs/domdec/gpuhaloexchange_impl_gpu.h:    cuda::barrier<cuda::thread_scope_device>* d_arriveWaitBarrier_ = nullptr;
src/gromacs/domdec/gpuhaloexchange_impl_gpu.h:/*! \internal \brief Class with interfaces and data for GPU Halo Exchange */
src/gromacs/domdec/gpuhaloexchange_impl_gpu.h:class GpuHaloExchange::Impl
src/gromacs/domdec/gpuhaloexchange_impl_gpu.h:    /*! \brief Creates GPU Halo Exchange object.
src/gromacs/domdec/gpuhaloexchange_impl_gpu.h:     * \param [in]    deviceContext            GPU device context
src/gromacs/domdec/gpuhaloexchange_impl_gpu.h:     * (Re-) Initialization for GPU halo exchange
src/gromacs/domdec/gpuhaloexchange_impl_gpu.h:     * \param [in] d_coordinatesBuffer  pointer to coordinates buffer in GPU memory
src/gromacs/domdec/gpuhaloexchange_impl_gpu.h:     * \param [in] d_forcesBuffer       pointer to forces buffer in GPU memory
src/gromacs/domdec/gpuhaloexchange_impl_gpu.h:     * GPU halo exchange of coordinates buffer
src/gromacs/domdec/gpuhaloexchange_impl_gpu.h:    GpuEventSynchronizer* communicateHaloCoordinates(const matrix box, GpuEventSynchronizer* dependencyEvent);
src/gromacs/domdec/gpuhaloexchange_impl_gpu.h:    /*! \brief  GPU halo exchange of force buffer
src/gromacs/domdec/gpuhaloexchange_impl_gpu.h:                               FixedCapacityVector<GpuEventSynchronizer*, 2>* dependencyEvents);
src/gromacs/domdec/gpuhaloexchange_impl_gpu.h:    GpuEventSynchronizer* getForcesReadyOnDeviceEvent();
src/gromacs/domdec/gpuhaloexchange_impl_gpu.h:    void destroyGpuHaloExchangeNvshmemBuf();
src/gromacs/domdec/gpuhaloexchange_impl_gpu.h:    /*! \brief Data transfer wrapper for GPU halo exchange
src/gromacs/domdec/gpuhaloexchange_impl_gpu.h:    /*! \brief Data transfer for GPU halo exchange using peer-to-peer copies
src/gromacs/domdec/gpuhaloexchange_impl_gpu.h:    /*! \brief Data transfer for GPU halo exchange using GPU-aware MPI
src/gromacs/domdec/gpuhaloexchange_impl_gpu.h:    void communicateHaloDataGpuAwareMpi(Float3* sendPtr,
src/gromacs/domdec/gpuhaloexchange_impl_gpu.h:    /*! \brief Transfer GPU coordinate halo data out of place
src/gromacs/domdec/gpuhaloexchange_impl_gpu.h:    /*! \brief Transfer GPU force halo data out of place
src/gromacs/domdec/gpuhaloexchange_impl_gpu.h:    void enqueueWaitRemoteCoordinatesReadyEvent(GpuEventSynchronizer* coordinatesReadyOnDeviceEvent);
src/gromacs/domdec/gpuhaloexchange_impl_gpu.h:    //! remote GPU coordinates buffer pointer for pushing data
src/gromacs/domdec/gpuhaloexchange_impl_gpu.h:    //! remote GPU force buffer pointer for pushing data
src/gromacs/domdec/gpuhaloexchange_impl_gpu.h:    std::unique_ptr<GpuEventSynchronizer> haloXDataTransferLaunched_;
src/gromacs/domdec/gpuhaloexchange_impl_gpu.h:    std::unique_ptr<GpuEventSynchronizer> haloFDataTransferLaunched_;
src/gromacs/domdec/gpuhaloexchange_impl_gpu.h:    //! GPU context object
src/gromacs/domdec/gpuhaloexchange_impl_gpu.h:    //! full coordinates buffer in GPU memory
src/gromacs/domdec/gpuhaloexchange_impl_gpu.h:    //! full forces buffer in GPU memory
src/gromacs/domdec/gpuhaloexchange_impl_gpu.h:    //! An event recorded once the exchanged forces are ready on the GPU
src/gromacs/domdec/gpuhaloexchange_impl_gpu.h:    GpuEventSynchronizer fReadyOnDevice_;
src/gromacs/domdec/gpuhaloexchange_impl_gpu.h:    GpuEventSynchronizer coordinateHaloLaunched_;
src/gromacs/domdec/gpuhaloexchange_impl_gpu.h:#endif // GMX_DOMDEC_GPUHALOEXCHANGE_IMPL_GPU_H
src/gromacs/domdec/tests/haloexchange_mpi.cpp: *  exchange (for both CPU and GPU codepaths) for several 1D and 2D
src/gromacs/domdec/tests/haloexchange_mpi.cpp: * \author Alan Gray <alang@nvidia.com>
src/gromacs/domdec/tests/haloexchange_mpi.cpp:#include "gromacs/domdec/gpuhaloexchange.h"
src/gromacs/domdec/tests/haloexchange_mpi.cpp:#if GMX_GPU_CUDA
src/gromacs/domdec/tests/haloexchange_mpi.cpp:#    include "gromacs/gpu_utils/device_stream.h"
src/gromacs/domdec/tests/haloexchange_mpi.cpp:#    include "gromacs/gpu_utils/devicebuffer.h"
src/gromacs/domdec/tests/haloexchange_mpi.cpp:#include "gromacs/gpu_utils/gpueventsynchronizer.h"
src/gromacs/domdec/tests/haloexchange_mpi.cpp:#include "gromacs/gpu_utils/hostallocator.h"
src/gromacs/domdec/tests/haloexchange_mpi.cpp:/*! \brief Perform GPU halo exchange, including required setup and data transfers
src/gromacs/domdec/tests/haloexchange_mpi.cpp:void gpuHalo(gmx_domdec_t* dd, matrix box, HostVector<RVec>* h_x, int numAtomsTotal)
src/gromacs/domdec/tests/haloexchange_mpi.cpp:#if (GMX_GPU_CUDA && GMX_THREAD_MPI)
src/gromacs/domdec/tests/haloexchange_mpi.cpp:    // Set up GPU hardware environment and assign this MPI rank to a device
src/gromacs/domdec/tests/haloexchange_mpi.cpp:    // Set up GPU buffer and copy input data from host
src/gromacs/domdec/tests/haloexchange_mpi.cpp:    copyToDeviceBuffer(&d_x, h_x->data(), 0, numAtomsTotal, deviceStream, GpuApiCallBehavior::Sync, nullptr);
src/gromacs/domdec/tests/haloexchange_mpi.cpp:    GpuEventSynchronizer coordinatesReadyOnDeviceEvent(numPulses + numExtraConsumptions,
src/gromacs/domdec/tests/haloexchange_mpi.cpp:    std::array<std::vector<GpuHaloExchange>, DIM> gpuHaloExchange;
src/gromacs/domdec/tests/haloexchange_mpi.cpp:            gpuHaloExchange[d].push_back(GpuHaloExchange(
src/gromacs/domdec/tests/haloexchange_mpi.cpp:    // Perform GPU halo exchange
src/gromacs/domdec/tests/haloexchange_mpi.cpp:            gpuHaloExchange[d][pulse].reinitHalo(d_x, nullptr);
src/gromacs/domdec/tests/haloexchange_mpi.cpp:            gpuHaloExchange[d][pulse].communicateHaloCoordinates(box, &coordinatesReadyOnDeviceEvent);
src/gromacs/domdec/tests/haloexchange_mpi.cpp:            h_x->data(), &d_x, 0, numAtomsTotal, deviceStream, GpuApiCallBehavior::Sync, nullptr);
src/gromacs/domdec/tests/haloexchange_mpi.cpp:    if (GMX_GPU_CUDA && GMX_THREAD_MPI) // repeat with GPU halo codepath
src/gromacs/domdec/tests/haloexchange_mpi.cpp:        // Perform GPU halo exchange
src/gromacs/domdec/tests/haloexchange_mpi.cpp:        gpuHalo(&dd, box, &h_x, numAtomsTotal);
src/gromacs/domdec/tests/haloexchange_mpi.cpp:    if (GMX_GPU_CUDA && GMX_THREAD_MPI) // repeat with GPU halo codepath
src/gromacs/domdec/tests/haloexchange_mpi.cpp:        // Perform GPU halo exchange
src/gromacs/domdec/tests/haloexchange_mpi.cpp:        gpuHalo(&dd, box, &h_x, numAtomsTotal);
src/gromacs/domdec/tests/haloexchange_mpi.cpp:    if (GMX_GPU_CUDA && GMX_THREAD_MPI) // repeat with GPU halo codepath
src/gromacs/domdec/tests/haloexchange_mpi.cpp:        // Perform GPU halo exchange
src/gromacs/domdec/tests/haloexchange_mpi.cpp:        gpuHalo(&dd, box, &h_x, numAtomsTotal);
src/gromacs/domdec/tests/haloexchange_mpi.cpp:    if (GMX_GPU_CUDA && GMX_THREAD_MPI) // repeat with GPU halo codepath
src/gromacs/domdec/tests/haloexchange_mpi.cpp:        // Perform GPU halo exchange
src/gromacs/domdec/tests/haloexchange_mpi.cpp:        gpuHalo(&dd, box, &h_x, numAtomsTotal);
src/gromacs/domdec/tests/CMakeLists.txt:        gpu_utils
src/gromacs/domdec/tests/CMakeLists.txt:    GPU_CPP_SOURCE_FILES
src/gromacs/domdec/tests/CMakeLists.txt:            gpu_utils
src/gromacs/domdec/domdec_setup.cpp:         * extendedHaloRegion (used for PME GPU decomposition runs) is also not known
src/gromacs/domdec/domdec_setup.cpp:        bool useGpuPme          = false;
src/gromacs/domdec/domdec_setup.cpp:                    ir.pme_order, ir.nkx, ir.nky, ir.nkz, npme_x, npme_y, extendedHaloRegion, useGpuPme, useThreads, errorsAreFatal))
src/gromacs/domdec/domdec_setup.cpp:                                                        bool useGpuForNonbonded,
src/gromacs/domdec/domdec_setup.cpp:                                                        bool useGpuForPme,
src/gromacs/domdec/domdec_setup.cpp:                                                        bool canUseGpuPmeDecomposition)
src/gromacs/domdec/domdec_setup.cpp:    /* With NB GPUs we don't automatically use PME-only ranks. PME-only CPU ranks can
src/gromacs/domdec/domdec_setup.cpp:     * improve performance with many ranks per GPU, since our OpenMP
src/gromacs/domdec/domdec_setup.cpp:    if (useGpuForNonbonded && options.numPmeRanks < 0)
src/gromacs/domdec/domdec_setup.cpp:                "non-bonded interactions are computed on GPUs");
src/gromacs/domdec/domdec_setup.cpp:    if (useGpuForPme && !canUseGpuPmeDecomposition && (options.numPmeRanks < 0 || options.numPmeRanks > 1))
src/gromacs/domdec/domdec_setup.cpp:                "PME GPU decomposition is not supported for current build configuration, only one "
src/gromacs/domdec/domdec_setup.cpp:                "separate PME-only GPU rank "
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cu: * \brief Implements backend-specific part of GPU halo exchange, namely pack and unpack
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cu: * kernels and the host code for scheduling them, using CUDA.
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cu: * \author Alan Gray <alang@nvidia.com>
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cu: * \author Mahesh Doijade <mdoijade@nvidia.com>
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cu:#include "gpuhaloexchange_impl_gpu.h"
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cu:#include "gromacs/gpu_utils/typecasts_cuda_hip.h"
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cu:#include "gromacs/gpu_utils/vectype_ops_cuda.h"
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cu:#    include <cuda/barrier>
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cu://! Number of CUDA threads in a block
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cu:/*! \brief unpack non-local force data buffer on the GPU using pre-populated "map" containing index
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cu: * on the GPU using pre-populated "map" containing index info and then sending this
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cu:                                               cuda::barrier<cuda::thread_scope_device>* bar,
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cu:    using barrier   = cuda::barrier<cuda::thread_scope_device>;
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cu:void GpuHaloExchange::Impl::launchPackXKernel(const matrix box)
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cu:                    prepareGpuKernelArguments(packNvShmemkernelFn,
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cu:            launchGpuKernel(packNvShmemkernelFn,
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cu:                            "Domdec GPU Apply X Halo Exchange",
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cu:            const auto kernelArgs = prepareGpuKernelArguments(
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cu:            launchGpuKernel(
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cu:                    kernelFn, config, *haloStream_, nullptr, "Domdec GPU Apply X Halo Exchange", kernelArgs);
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cu:void GpuHaloExchange::Impl::launchUnpackFKernel(bool accumulateForces)
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cu:                    prepareGpuKernelArguments(kernelFn,
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cu:            launchGpuKernel(
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cu:                    kernelFn, config, *haloStream_, nullptr, "Domdec GPU Apply F Halo Exchange", kernelArgs);
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cu:                    prepareGpuKernelArguments(kernelFn, config, &d_f, &recvBuf, &indexMap, &fRecvSize_);
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cu:            launchGpuKernel(
src/gromacs/domdec/gpuhaloexchange_impl_gpu.cu:                    kernelFn, config, *haloStream_, nullptr, "Domdec GPU Apply F Halo Exchange", kernelArgs);
src/gromacs/domdec/dlbtiming.h:/*! \brief Tells if we had to wait for a GPU to finish computation */
src/gromacs/domdec/dlbtiming.h:enum class DdBalanceRegionWaitedForGpu
src/gromacs/domdec/dlbtiming.h:    no, //!< The GPU finished computation before the CPU needed the result
src/gromacs/domdec/dlbtiming.h:    yes //!< We had to wait for the GPU to finish computation
src/gromacs/domdec/dlbtiming.h:    void openBeforeForceComputationGpu() const
src/gromacs/domdec/dlbtiming.h:            openRegionGpuImpl();
src/gromacs/domdec/dlbtiming.h:    /*! \brief Close the load balance timing region on the GPU side
src/gromacs/domdec/dlbtiming.h:     * from the GPU. The wait time for these results is estimated, depending
src/gromacs/domdec/dlbtiming.h:     * on the \p waitedForGpu parameter.
src/gromacs/domdec/dlbtiming.h:     * \param[in] waitCyclesGpuInCpuRegion  The time we waited for the GPU earlier, overlapping completely with the open CPU region
src/gromacs/domdec/dlbtiming.h:     * \param[in] waitedForGpu              Tells if we waited for the GPU to finish now
src/gromacs/domdec/dlbtiming.h:    void closeAfterForceComputationGpu(float                       waitCyclesGpuInCpuRegion,
src/gromacs/domdec/dlbtiming.h:                                       DdBalanceRegionWaitedForGpu waitedForGpu) const
src/gromacs/domdec/dlbtiming.h:            closeRegionGpuImpl(waitCyclesGpuInCpuRegion, waitedForGpu);
src/gromacs/domdec/dlbtiming.h:    /*! \brief Open the load balance timing region for the GPU
src/gromacs/domdec/dlbtiming.h:    void openRegionGpuImpl() const;
src/gromacs/domdec/dlbtiming.h:    /*! \brief Close the load balance timing region on the GPU side
src/gromacs/domdec/dlbtiming.h:     * \param[in] waitCyclesGpuInCpuRegion  The time we waited for the GPU earlier, overlapping completely with the open CPU region
src/gromacs/domdec/dlbtiming.h:     * \param[in] waitedForGpu              Tells if we waited for the GPU to finish now
src/gromacs/domdec/dlbtiming.h:    void closeRegionGpuImpl(float waitCyclesGpuInCpuRegion, DdBalanceRegionWaitedForGpu waitedForGpu) const;
src/gromacs/domdec/CMakeLists.txt:list(FILTER DOMDEC_SOURCES EXCLUDE REGEX ".*/gpuhaloexchange_impl_gpu[a-z_]*\.cpp$")
src/gromacs/domdec/CMakeLists.txt:if(GMX_GPU_SYCL)
src/gromacs/domdec/CMakeLists.txt:    file(GLOB DOMDEC_GPU_SOURCES gpuhaloexchange_impl_gpu.cpp gpuhaloexchange_impl_gpu_sycl.cpp)
src/gromacs/domdec/CMakeLists.txt:    _gmx_add_files_to_property(SYCL_SOURCES gpuhaloexchange_impl_gpu.cpp gpuhaloexchange_impl_gpu_sycl.cpp)
src/gromacs/domdec/CMakeLists.txt:if(GMX_GPU_CUDA)
src/gromacs/domdec/CMakeLists.txt:  file(GLOB DOMDEC_GPU_SOURCES gpuhaloexchange_impl_gpu.cpp gpuhaloexchange_impl_gpu.cu)
src/gromacs/domdec/CMakeLists.txt:  _gmx_add_files_to_property(CUDA_SOURCES gpuhaloexchange_impl_gpu.cpp)
src/gromacs/domdec/CMakeLists.txt:set(LIBGROMACS_SOURCES ${LIBGROMACS_SOURCES} ${DOMDEC_SOURCES} ${DOMDEC_GPU_SOURCES} PARENT_SCOPE)
src/gromacs/domdec/CMakeLists.txt:                      gpu_utils
src/gromacs/domdec/gpuhaloexchange_impl_gpu_sycl.cpp: * \brief Implements GPU halo exchange using SYCL.
src/gromacs/domdec/gpuhaloexchange_impl_gpu_sycl.cpp: * \author Alan Gray <alang@nvidia.com>
src/gromacs/domdec/gpuhaloexchange_impl_gpu_sycl.cpp:#include "gromacs/gpu_utils/devicebuffer_sycl.h"
src/gromacs/domdec/gpuhaloexchange_impl_gpu_sycl.cpp:#include "gromacs/gpu_utils/gmxsycl.h"
src/gromacs/domdec/gpuhaloexchange_impl_gpu_sycl.cpp:#include "gromacs/gpu_utils/gputraits_sycl.h"
src/gromacs/domdec/gpuhaloexchange_impl_gpu_sycl.cpp:#include "gpuhaloexchange_impl_gpu.h"
src/gromacs/domdec/gpuhaloexchange_impl_gpu_sycl.cpp:/*! \brief unpack non-local force data buffer on the GPU using pre-populated "map" containing index
src/gromacs/domdec/gpuhaloexchange_impl_gpu_sycl.cpp:void GpuHaloExchange::Impl::launchPackXKernel(const matrix box)
src/gromacs/domdec/gpuhaloexchange_impl_gpu_sycl.cpp:void GpuHaloExchange::Impl::launchUnpackFKernel(bool accumulateForces)
src/gromacs/domdec/gpuhaloexchange_impl.cpp: * \brief May be used to implement Domdec CUDA interfaces for non-GPU builds.
src/gromacs/domdec/gpuhaloexchange_impl.cpp: * Needed to satisfy compiler on systems, where CUDA is not available.
src/gromacs/domdec/gpuhaloexchange_impl.cpp: * \author Alan Gray <alang@nvidia.com>
src/gromacs/domdec/gpuhaloexchange_impl.cpp:#include "gromacs/domdec/gpuhaloexchange.h"
src/gromacs/domdec/gpuhaloexchange_impl.cpp:#include "gromacs/gpu_utils/devicebuffer_datatype.h"
src/gromacs/domdec/gpuhaloexchange_impl.cpp:class GpuEventSynchronizer;
src/gromacs/domdec/gpuhaloexchange_impl.cpp:#if !GMX_GPU_CUDA && !GMX_GPU_SYCL
src/gromacs/domdec/gpuhaloexchange_impl.cpp:class GpuHaloExchange::Impl
src/gromacs/domdec/gpuhaloexchange_impl.cpp:GpuHaloExchange::GpuHaloExchange(gmx_domdec_t* /* dd */,
src/gromacs/domdec/gpuhaloexchange_impl.cpp:               "A CPU stub for GPU Halo Exchange was called insted of the correct implementation.");
src/gromacs/domdec/gpuhaloexchange_impl.cpp:GpuHaloExchange::~GpuHaloExchange() = default;
src/gromacs/domdec/gpuhaloexchange_impl.cpp:GpuHaloExchange::GpuHaloExchange(GpuHaloExchange&&) noexcept = default;
src/gromacs/domdec/gpuhaloexchange_impl.cpp:GpuHaloExchange& GpuHaloExchange::operator=(GpuHaloExchange&& other) noexcept
src/gromacs/domdec/gpuhaloexchange_impl.cpp:void GpuHaloExchange::reinitHalo(DeviceBuffer<RVec> /* d_coordinatesBuffer */,
src/gromacs/domdec/gpuhaloexchange_impl.cpp:               "A CPU stub for GPU Halo Exchange was called insted of the correct implementation.");
src/gromacs/domdec/gpuhaloexchange_impl.cpp:void GpuHaloExchange::reinitNvshmemSignal(const t_commrec& /* cr */, int /* signalObjOffset */)
src/gromacs/domdec/gpuhaloexchange_impl.cpp:               "A CPU stub for GPU Halo Exchange was called insted of the correct implementation.");
src/gromacs/domdec/gpuhaloexchange_impl.cpp:GpuEventSynchronizer* GpuHaloExchange::communicateHaloCoordinates(const matrix /* box */,
src/gromacs/domdec/gpuhaloexchange_impl.cpp:                                                                  GpuEventSynchronizer* /*dependencyEvent*/)
src/gromacs/domdec/gpuhaloexchange_impl.cpp:               "A CPU stub for GPU Halo Exchange exchange was called insted of the correct "
src/gromacs/domdec/gpuhaloexchange_impl.cpp:void GpuHaloExchange::communicateHaloForces(bool /* accumulateForces */,
src/gromacs/domdec/gpuhaloexchange_impl.cpp:                                            FixedCapacityVector<GpuEventSynchronizer*, 2>* /*dependencyEvents*/)
src/gromacs/domdec/gpuhaloexchange_impl.cpp:               "A CPU stub for GPU Halo Exchange was called insted of the correct implementation.");
src/gromacs/domdec/gpuhaloexchange_impl.cpp:GpuEventSynchronizer* GpuHaloExchange::getForcesReadyOnDeviceEvent()
src/gromacs/domdec/gpuhaloexchange_impl.cpp:               "A CPU stub for GPU Halo Exchange was called insted of the correct implementation.");
src/gromacs/domdec/gpuhaloexchange_impl.cpp:void GpuHaloExchange::destroyGpuHaloExchangeNvshmemBuf()
src/gromacs/domdec/gpuhaloexchange_impl.cpp:               "A CPU stub for GPU Halo Exchange was called insted of the correct implementation.");
src/gromacs/domdec/gpuhaloexchange_impl.cpp:#endif // !GMX_GPU_CUDA && !GMX_GPU_SYCL
src/gromacs/domdec/partition.cpp:        if (comm->cycl_n[ddCyclWaitGPU] && comm->nrank_gpu_shared > 1)
src/gromacs/domdec/partition.cpp:            float gpu_wait, gpu_wait_sum;
src/gromacs/domdec/partition.cpp:            gpu_wait = comm->cycl[ddCyclWaitGPU];
src/gromacs/domdec/partition.cpp:                /* We should remove the WaitGPU time of the same MD step
src/gromacs/domdec/partition.cpp:                 * the same on all ranks that share the same GPU.
src/gromacs/domdec/partition.cpp:                 * that changes in the GPU wait time matter a lot here.
src/gromacs/domdec/partition.cpp:                gpu_wait *= (comm->cycl_n[ddCyclF] - 1) / static_cast<float>(comm->cycl_n[ddCyclF]);
src/gromacs/domdec/partition.cpp:            /* Sum the wait times over the ranks that share the same GPU */
src/gromacs/domdec/partition.cpp:            MPI_Allreduce(&gpu_wait, &gpu_wait_sum, 1, MPI_FLOAT, MPI_SUM, comm->mpi_comm_gpu_shared);
src/gromacs/domdec/partition.cpp:            load += -gpu_wait + gpu_wait_sum / comm->nrank_gpu_shared;
src/gromacs/domdec/hashedmap.h:     * \todo Use std::as_const when CUDA 11 is a requirement.
src/gromacs/domdec/domdec_struct.h:#include "gromacs/gpu_utils/hostallocator.h"
src/gromacs/domdec/domdec_struct.h:class GpuHaloExchange;
src/gromacs/domdec/domdec_struct.h:    /* GPU halo exchange objects: this structure supports a vector of pulses for each dimension */
src/gromacs/domdec/domdec_struct.h:    std::vector<std::unique_ptr<gmx::GpuHaloExchange>> gpuHaloExchange[DIM];
src/gromacs/domdec/mdsetup.cpp:                           fr->listedForcesGpu != nullptr,
src/gromacs/domdec/domdec_setup.h: * GPU setup is not compatible with separate PME ranks,
src/gromacs/domdec/domdec_setup.h:                                                        bool useGpuForNonbonded,
src/gromacs/domdec/domdec_setup.h:                                                        bool useGpuForPme,
src/gromacs/domdec/domdec_setup.h:                                                        bool canUseGpuPmeDecomposition);
src/gromacs/domdec/builder.h: * duty(s) of each rank, and then perhaps to be advised of GPU streams
src/gromacs/domdec/builder.h:                               bool                              useGpuForNonbonded,
src/gromacs/domdec/builder.h:                               bool                              useGpuForPme,
src/gromacs/domdec/builder.h:                               bool                              useGpuForUpdate,
src/gromacs/domdec/builder.h:                               bool                              useGpuDirectHalo,
src/gromacs/domdec/builder.h:                               bool                              canUseGpuPmeDecomposition);
src/gromacs/domdec/domdec.h:#include "gromacs/gpu_utils/devicebuffer_datatype.h"
src/gromacs/domdec/domdec.h:class GpuEventSynchronizer;
src/gromacs/domdec/domdec.h: * \param[in] checkGpuDdLimitation Whether to check the GPU DD support limitation
src/gromacs/domdec/domdec.h:                      bool                           checkGpuDdLimitation);
src/gromacs/domdec/domdec.h:/*! \brief Set up communication between PP ranks for averaging GPU
src/gromacs/domdec/domdec.h: * When domains (PP MPI ranks) share a GPU, the individual GPU wait times
src/gromacs/domdec/domdec.h: * GPU finish. Therefore there wait times need to be averaged over the ranks
src/gromacs/domdec/domdec.h: * sharing the same GPU. This function sets up the communication for that.
src/gromacs/domdec/domdec.h: * a PP rank only when that rank is using a GPU.
src/gromacs/domdec/domdec.h:    ddCyclWaitGPU,
src/gromacs/domdec/domdec.h:/*! \brief Construct the GPU halo exchange object(s).
src/gromacs/domdec/domdec.h: * \param[in] deviceStreamManager Manager of the GPU context and streams.
src/gromacs/domdec/domdec.h:void constructGpuHaloExchange(const t_commrec&                cr,
src/gromacs/domdec/domdec.h: * (Re-) Initialization for GPU halo exchange
src/gromacs/domdec/domdec.h: * \param [in] d_coordinatesBuffer  pointer to coordinates buffer in GPU memory
src/gromacs/domdec/domdec.h: * \param [in] d_forcesBuffer       pointer to forces buffer in GPU memory
src/gromacs/domdec/domdec.h:void reinitGpuHaloExchange(const t_commrec&        cr,
src/gromacs/domdec/domdec.h:void destroyGpuHaloExchangeNvshmemBuf(const t_commrec& cr);
src/gromacs/domdec/domdec.h:/*! \brief GPU halo exchange of coordinates buffer.
src/gromacs/domdec/domdec.h:GpuEventSynchronizer* communicateGpuHaloCoordinates(const t_commrec&      cr,
src/gromacs/domdec/domdec.h:                                                    GpuEventSynchronizer* dependencyEvent);
src/gromacs/domdec/domdec.h:/*! \brief  Wait for copy of nonlocal part of coordinate array from GPU to CPU
src/gromacs/domdec/domdec.h:void communicateGpuHaloForces(const t_commrec&                                    cr,
src/gromacs/domdec/domdec.h:                              gmx::FixedCapacityVector<GpuEventSynchronizer*, 2>* dependencyEvents);
src/gromacs/domdec/gpuhaloexchange.h: * \brief Declaration of GPU halo exchange.
src/gromacs/domdec/gpuhaloexchange.h: * \author Alan Gray <alang@nvidia.com>
src/gromacs/domdec/gpuhaloexchange.h:#ifndef GMX_DOMDEC_GPUHALOEXCHANGE_H
src/gromacs/domdec/gpuhaloexchange.h:#define GMX_DOMDEC_GPUHALOEXCHANGE_H
src/gromacs/domdec/gpuhaloexchange.h:#include "gromacs/gpu_utils/devicebuffer_datatype.h"
src/gromacs/domdec/gpuhaloexchange.h:class GpuEventSynchronizer;
src/gromacs/domdec/gpuhaloexchange.h: * \brief Manages GPU Halo Exchange object */
src/gromacs/domdec/gpuhaloexchange.h:class GpuHaloExchange
src/gromacs/domdec/gpuhaloexchange.h:    /*! \brief Creates GPU Halo Exchange object.
src/gromacs/domdec/gpuhaloexchange.h:     * been copied to the GPU (if CPU forces are present), and before
src/gromacs/domdec/gpuhaloexchange.h:     * \param [in]    deviceContext            GPU device context
src/gromacs/domdec/gpuhaloexchange.h:    GpuHaloExchange(gmx_domdec_t*        dd,
src/gromacs/domdec/gpuhaloexchange.h:    ~GpuHaloExchange();
src/gromacs/domdec/gpuhaloexchange.h:    GpuHaloExchange(GpuHaloExchange&& source) noexcept;
src/gromacs/domdec/gpuhaloexchange.h:    GpuHaloExchange& operator=(GpuHaloExchange&& source) noexcept;
src/gromacs/domdec/gpuhaloexchange.h:     * Initialization for GPU halo exchange of coordinates buffer
src/gromacs/domdec/gpuhaloexchange.h:     * \param [in] d_coordinateBuffer   pointer to coordinates buffer in GPU memory
src/gromacs/domdec/gpuhaloexchange.h:     * \param [in] d_forcesBuffer   pointer to coordinates buffer in GPU memory
src/gromacs/domdec/gpuhaloexchange.h:    /*! \brief GPU halo exchange of coordinates buffer.
src/gromacs/domdec/gpuhaloexchange.h:    GpuEventSynchronizer* communicateHaloCoordinates(const matrix box, GpuEventSynchronizer* dependencyEvent);
src/gromacs/domdec/gpuhaloexchange.h:    /*! \brief GPU halo exchange of force buffer.
src/gromacs/domdec/gpuhaloexchange.h:                               FixedCapacityVector<GpuEventSynchronizer*, 2>* dependencyEvents);
src/gromacs/domdec/gpuhaloexchange.h:    GpuEventSynchronizer* getForcesReadyOnDeviceEvent();
src/gromacs/domdec/gpuhaloexchange.h:    void destroyGpuHaloExchangeNvshmemBuf();
src/gromacs/gromacs-hints.in.cmake:@_gmx_cuda_config@
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp: * \brief Implements helper functions for GPU listed forces (bonded)
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:#include "gromacs/gpu_utils/device_context.h"
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:#include "gromacs/gpu_utils/device_stream.h"
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:#include "gromacs/gpu_utils/devicebuffer.h"
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:#include "gromacs/nbnxm/gpu_types_common.h"
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:#include "listed_forces_gpu_impl.h"
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:// Number of GPU threads in a block
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:// ---- ListedForcesGpu::Impl
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:                GMX_RELEASE_ASSERT(false, "Flexible sub-groups only supported for Intel GPUs");
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:ListedForcesGpu::Impl::Impl(const gmx_ffparams_t& ffparams,
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:                       "Only a single energy group is supported with listed forces on GPU");
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:                       "Can't run GPU version of bonded forces in stream that is not valid.");
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:            "Threads per block in GPU bonded must be >= c_numShiftVectors for the virial kernel "
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:                       GpuApiCallBehavior::Sync,
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:    for (int i = 0; i < numFTypesOnGpu; i++)
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:    int fTypeRangeEnd = kernelParams_.fTypeRangeEnd[numFTypesOnGpu - 1];
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:ListedForcesGpu::Impl::~Impl()
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:    for (int fType : fTypesOnGpu)
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:void ListedForcesGpu::Impl::updateHaveInteractions(const InteractionDefinitions& idef)
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:    for (int fType : fTypesOnGpu)
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:        /* Perturbation is not implemented in the GPU bonded kernels.
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:         * interactions on the GPU. */
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:// would be harder to misuse than ListedForcesGpu, and exchange the problem
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:/*! Divides bonded interactions over threads and GPU.
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp: *  The bonded interactions are assigned by interaction type to GPU threads. The interaction
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp: *  data structures on the GPU are also stored in kernelParams_.
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:void ListedForcesGpu::Impl::updateInteractionListsAndDeviceBuffers(ArrayRef<const int> nbnxnAtomOrder,
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:    wallcycle_sub_start(wcycle_, WallCycleSubCounter::GpuBondedListUpdate);
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:    bool haveGpuInteractions = false;
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:    for (int fType : fTypesOnGpu)
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:        /* Perturbation is not implemented in the GPU bonded kernels.
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:         * interactions on the GPU. */
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:            haveGpuInteractions = true;
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:                               GpuApiCallBehavior::Async,
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:        kernelParams_.fTypesOnGpu[fTypesCounter] = fType;
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:                   "Invalid GPU listed forces setup. numBonds must be > 0 if there are threads "
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:                "The bonded interactions must be assigned to the GPU in blocks of sub-group size.");
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:    int fTypeRangeEnd               = kernelParams_.fTypeRangeEnd[numFTypesOnGpu - 1];
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:    GMX_RELEASE_ASSERT(haveGpuInteractions == haveInteractions_,
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:    wallcycle_sub_stop(wcycle_, WallCycleSubCounter::GpuBondedListUpdate);
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:void ListedForcesGpu::Impl::setPbc(PbcType pbcType, const matrix box, bool canMoleculeSpanPbc)
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:bool ListedForcesGpu::Impl::haveInteractions() const
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:void ListedForcesGpu::Impl::launchEnergyTransfer()
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:               "No GPU bonded interactions, so no energies will be computed, so transfer should "
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:    wallcycle_sub_start_nocount(wcycle_, WallCycleSubCounter::LaunchGpuBonded);
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:    copyFromDeviceBuffer(h_vTot, &d_vTot_, 0, F_NRE, deviceStream_, GpuApiCallBehavior::Async, nullptr);
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:    wallcycle_sub_stop(wcycle_, WallCycleSubCounter::LaunchGpuBonded);
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:void ListedForcesGpu::Impl::waitAccumulateEnergyTerms(gmx_enerdata_t* enerd)
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:               "No GPU bonded interactions, so no energies will be computed or transferred, so "
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:    wallcycle_start(wcycle_, WallCycleCounter::WaitGpuBonded);
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:    wallcycle_stop(wcycle_, WallCycleCounter::WaitGpuBonded);
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:    for (int fType : fTypesOnGpu)
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:void ListedForcesGpu::Impl::clearEnergies()
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:    wallcycle_start_nocount(wcycle_, WallCycleCounter::LaunchGpuPp);
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:    wallcycle_sub_start_nocount(wcycle_, WallCycleSubCounter::LaunchGpuBonded);
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:    wallcycle_sub_stop(wcycle_, WallCycleSubCounter::LaunchGpuBonded);
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:    wallcycle_stop(wcycle_, WallCycleCounter::LaunchGpuPp);
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:// ---- ListedForcesGpu
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:ListedForcesGpu::ListedForcesGpu(const gmx_ffparams_t& ffparams,
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:ListedForcesGpu::~ListedForcesGpu() = default;
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:void ListedForcesGpu::updateHaveInteractions(const InteractionDefinitions& idef)
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:void ListedForcesGpu::updateInteractionListsAndDeviceBuffers(ArrayRef<const int> nbnxnAtomOrder,
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:                                                             NBAtomDataGpu* nbnxmAtomDataGpu)
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:            nbnxnAtomOrder, idef, nbnxmAtomDataGpu->xq, nbnxmAtomDataGpu->f, nbnxmAtomDataGpu->fShift);
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:void ListedForcesGpu::setPbc(PbcType pbcType, const matrix box, bool canMoleculeSpanPbc)
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:bool ListedForcesGpu::haveInteractions() const
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:void ListedForcesGpu::setPbcAndlaunchKernel(PbcType                  pbcType,
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:void ListedForcesGpu::launchEnergyTransfer()
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:void ListedForcesGpu::waitAccumulateEnergyTerms(gmx_enerdata_t* enerd)
src/gromacs/listed_forces/listed_forces_gpu_impl_gpu.cpp:void ListedForcesGpu::clearEnergies()
src/gromacs/listed_forces/manage_threading.cpp:#include "gromacs/listed_forces/listed_forces_gpu.h"
src/gromacs/listed_forces/manage_threading.cpp://! Divides bonded interactions over threads and GPU
src/gromacs/listed_forces/manage_threading.cpp:                                        bool                          useGpuForBondeds,
src/gromacs/listed_forces/manage_threading.cpp:    size_t fTypeGpuIndex = 0;
src/gromacs/listed_forces/manage_threading.cpp:        if (useGpuForBondeds && fTypeGpuIndex < gmx::fTypesOnGpu.size()
src/gromacs/listed_forces/manage_threading.cpp:            && gmx::fTypesOnGpu[fTypeGpuIndex] == fType)
src/gromacs/listed_forces/manage_threading.cpp:            fTypeGpuIndex++;
src/gromacs/listed_forces/manage_threading.cpp:            /* Perturbation is not implemented in the GPU bonded kernels.
src/gromacs/listed_forces/manage_threading.cpp:                /* We will assign this interaction type to the GPU */
src/gromacs/listed_forces/manage_threading.cpp:                            bool                          useGpuForBondeds,
src/gromacs/listed_forces/manage_threading.cpp:    divide_bondeds_over_threads(bt, useGpuForBondeds, idef);
src/gromacs/listed_forces/listed_forces_gpu_impl.cpp: * \brief Implements GPU bonded lists for non-GPU builds
src/gromacs/listed_forces/listed_forces_gpu_impl.cpp:#include "gromacs/listed_forces/listed_forces_gpu.h"
src/gromacs/listed_forces/listed_forces_gpu_impl.cpp:struct NBAtomDataGpu;
src/gromacs/listed_forces/listed_forces_gpu_impl.cpp://! Returns whether there are any interactions in ilists suitable for a GPU.
src/gromacs/listed_forces/listed_forces_gpu_impl.cpp:static bool someInteractionsCanRunOnGpu(const InteractionLists& ilists)
src/gromacs/listed_forces/listed_forces_gpu_impl.cpp:    // Perturbation is not implemented in the GPU bonded
src/gromacs/listed_forces/listed_forces_gpu_impl.cpp:    // domain, and work will never run on the GPU. This is
src/gromacs/listed_forces/listed_forces_gpu_impl.cpp:    return std::any_of(fTypesOnGpu.begin(),
src/gromacs/listed_forces/listed_forces_gpu_impl.cpp:                       fTypesOnGpu.end(),
src/gromacs/listed_forces/listed_forces_gpu_impl.cpp://! Returns whether there are any bonded interactions in the global topology suitable for a GPU.
src/gromacs/listed_forces/listed_forces_gpu_impl.cpp:static bool bondedInteractionsCanRunOnGpu(const gmx_mtop_t& mtop)
src/gromacs/listed_forces/listed_forces_gpu_impl.cpp:        if (someInteractionsCanRunOnGpu(moltype.ilist))
src/gromacs/listed_forces/listed_forces_gpu_impl.cpp:        if (someInteractionsCanRunOnGpu(*mtop.intermolecular_ilist))
src/gromacs/listed_forces/listed_forces_gpu_impl.cpp:bool buildSupportsListedForcesGpu(std::string* error)
src/gromacs/listed_forces/listed_forces_gpu_impl.cpp:    errorReasons.startContext("Bonded interactions on GPU are not supported in:");
src/gromacs/listed_forces/listed_forces_gpu_impl.cpp:    errorReasons.appendIf(GMX_GPU_OPENCL, "OpenCL build of GROMACS");
src/gromacs/listed_forces/listed_forces_gpu_impl.cpp:    errorReasons.appendIf(!GMX_GPU, "CPU-only build of GROMACS");
src/gromacs/listed_forces/listed_forces_gpu_impl.cpp:    errorReasons.appendIf(GMX_GPU_HIP, "HIP listed forces not implemented yet");
src/gromacs/listed_forces/listed_forces_gpu_impl.cpp:bool inputSupportsListedForcesGpu(const t_inputrec& ir, const gmx_mtop_t& mtop, std::string* error)
src/gromacs/listed_forces/listed_forces_gpu_impl.cpp:    errorReasons.startContext("Bonded interactions can not be computed on a GPU:");
src/gromacs/listed_forces/listed_forces_gpu_impl.cpp:    errorReasons.appendIf(!bondedInteractionsCanRunOnGpu(mtop),
src/gromacs/listed_forces/listed_forces_gpu_impl.cpp:                          "None of the bonded types are implemented on the GPU.");
src/gromacs/listed_forces/listed_forces_gpu_impl.cpp:            "Cannot compute bonded interactions on a GPU, because GPU implementation requires "
src/gromacs/listed_forces/listed_forces_gpu_impl.cpp:#if !GMX_GPU_CUDA && !GMX_GPU_SYCL
src/gromacs/listed_forces/listed_forces_gpu_impl.cpp:class ListedForcesGpu::Impl
src/gromacs/listed_forces/listed_forces_gpu_impl.cpp:ListedForcesGpu::ListedForcesGpu(const gmx_ffparams_t& /* ffparams */,
src/gromacs/listed_forces/listed_forces_gpu_impl.cpp:ListedForcesGpu::~ListedForcesGpu() = default;
src/gromacs/listed_forces/listed_forces_gpu_impl.cpp:void ListedForcesGpu::updateHaveInteractions(const InteractionDefinitions& /*idef*/) {}
src/gromacs/listed_forces/listed_forces_gpu_impl.cpp:void ListedForcesGpu::updateInteractionListsAndDeviceBuffers(ArrayRef<const int> /* nbnxnAtomOrder */,
src/gromacs/listed_forces/listed_forces_gpu_impl.cpp:                                                             NBAtomDataGpu* /* nbnxmAtomDataGpu */)
src/gromacs/listed_forces/listed_forces_gpu_impl.cpp:void ListedForcesGpu::setPbc(PbcType /* pbcType */, const matrix /* box */, bool /* canMoleculeSpanPbc */)
src/gromacs/listed_forces/listed_forces_gpu_impl.cpp:bool ListedForcesGpu::haveInteractions() const
src/gromacs/listed_forces/listed_forces_gpu_impl.cpp:void ListedForcesGpu::launchKernel(const gmx::StepWorkload& /* stepWork */) {}
src/gromacs/listed_forces/listed_forces_gpu_impl.cpp:void ListedForcesGpu::setPbcAndlaunchKernel(PbcType /* pbcType */,
src/gromacs/listed_forces/listed_forces_gpu_impl.cpp:void ListedForcesGpu::launchEnergyTransfer() {}
src/gromacs/listed_forces/listed_forces_gpu_impl.cpp:void ListedForcesGpu::waitAccumulateEnergyTerms(gmx_enerdata_t* /* enerd */) {}
src/gromacs/listed_forces/listed_forces_gpu_impl.cpp:void ListedForcesGpu::clearEnergies() {}
src/gromacs/listed_forces/listed_forces_gpu_impl.cpp:#endif // !GMX_GPU_CUDA
src/gromacs/listed_forces/listed_forces_gpu.h:#ifndef GMX_LISTED_FORCES_LISTED_FORCES_GPU_H
src/gromacs/listed_forces/listed_forces_gpu.h:#define GMX_LISTED_FORCES_LISTED_FORCES_GPU_H
src/gromacs/listed_forces/listed_forces_gpu.h:#include "gromacs/gpu_utils/devicebuffer_datatype.h"
src/gromacs/listed_forces/listed_forces_gpu.h:struct NBAtomDataGpu;
src/gromacs/listed_forces/listed_forces_gpu.h:/*! \brief The number on bonded function types supported on GPUs */
src/gromacs/listed_forces/listed_forces_gpu.h:static constexpr int numFTypesOnGpu = 8;
src/gromacs/listed_forces/listed_forces_gpu.h:/*! \brief List of all bonded function types supported on GPUs
src/gromacs/listed_forces/listed_forces_gpu.h: * \note This list should be in sync with the actual GPU code.
src/gromacs/listed_forces/listed_forces_gpu.h: * \note Perturbed interactions are not supported on GPUs.
src/gromacs/listed_forces/listed_forces_gpu.h: * \note Currently bonded are only supported with CUDA and SYCL, not with OpenCL.
src/gromacs/listed_forces/listed_forces_gpu.h:constexpr std::array<int, numFTypesOnGpu> fTypesOnGpu = { F_BONDS,  F_ANGLES, F_UREY_BRADLEY,
src/gromacs/listed_forces/listed_forces_gpu.h:/*! \brief Checks whether the GROMACS build allows to compute bonded interactions on a GPU.
src/gromacs/listed_forces/listed_forces_gpu.h: * \param[out] error  If non-null, the diagnostic message when bondeds cannot run on a GPU.
src/gromacs/listed_forces/listed_forces_gpu.h: * \returns true when this build can run bonded interactions on a GPU, false otherwise.
src/gromacs/listed_forces/listed_forces_gpu.h:bool buildSupportsListedForcesGpu(std::string* error);
src/gromacs/listed_forces/listed_forces_gpu.h:/*! \brief Checks whether the input system allows to compute bonded interactions on a GPU.
src/gromacs/listed_forces/listed_forces_gpu.h: * \param[out] error  If non-null, the error message if the input is not supported on GPU.
src/gromacs/listed_forces/listed_forces_gpu.h: * \returns true if PME can run on GPU with this input, false otherwise.
src/gromacs/listed_forces/listed_forces_gpu.h:bool inputSupportsListedForcesGpu(const t_inputrec& ir, const gmx_mtop_t& mtop, std::string* error);
src/gromacs/listed_forces/listed_forces_gpu.h:class ListedForcesGpu
src/gromacs/listed_forces/listed_forces_gpu.h:     * \param[in] deviceContext              GPU device context.
src/gromacs/listed_forces/listed_forces_gpu.h:     * \param[in] deviceStream               GPU device stream.
src/gromacs/listed_forces/listed_forces_gpu.h:    ListedForcesGpu(const gmx_ffparams_t& ffparams,
src/gromacs/listed_forces/listed_forces_gpu.h:    ~ListedForcesGpu();
src/gromacs/listed_forces/listed_forces_gpu.h:    /*! \brief Update flag whether there are bonded interactions suitable for the GPU.
src/gromacs/listed_forces/listed_forces_gpu.h:    /*! \brief Update lists of interactions from idef suitable for the GPU,
src/gromacs/listed_forces/listed_forces_gpu.h:     * stage. Copies the bonded interactions assigned to the GPU
src/gromacs/listed_forces/listed_forces_gpu.h:     * \param[in,out] nbnxmAtomDataGpu Nbnxm GPU atom data (XQ and force buffers).
src/gromacs/listed_forces/listed_forces_gpu.h:                                                NBAtomDataGpu*                nbnxmAtomDataGpu);
src/gromacs/listed_forces/listed_forces_gpu.h:     * assigned to the GPU
src/gromacs/listed_forces/listed_forces_gpu.h:    /*! \brief Launches bonded kernel on a GPU
src/gromacs/listed_forces/listed_forces_gpu.h:    /*! \brief Sets the PBC and launches bonded kernel on a GPU
src/gromacs/listed_forces/listed_forces_gpu.h:#endif // GMX_LISTED_FORCES_LISTED_FORCES_GPU_H
src/gromacs/listed_forces/listed_forces_gpu_internal_sycl.cpp: * \author Jon Vincent <jvincent@nvidia.com>
src/gromacs/listed_forces/listed_forces_gpu_internal_sycl.cpp: * \author Alan Gray <alang@nvidia.com>
src/gromacs/listed_forces/listed_forces_gpu_internal_sycl.cpp:#include "gromacs/gpu_utils/devicebuffer_sycl.h"
src/gromacs/listed_forces/listed_forces_gpu_internal_sycl.cpp:#include "gromacs/gpu_utils/gmxsycl.h"
src/gromacs/listed_forces/listed_forces_gpu_internal_sycl.cpp:#include "gromacs/gpu_utils/sycl_kernel_utils.h"
src/gromacs/listed_forces/listed_forces_gpu_internal_sycl.cpp:#include "gromacs/listed_forces/listed_forces_gpu.h"
src/gromacs/listed_forces/listed_forces_gpu_internal_sycl.cpp:#include "listed_forces_gpu_impl.h"
src/gromacs/listed_forces/listed_forces_gpu_internal_sycl.cpp: * See https://developer.nvidia.com/blog/fast-dynamic-indexing-private-arrays-cuda/
src/gromacs/listed_forces/listed_forces_gpu_internal_sycl.cpp: * - hipSYCL 0.9.4 + Clang 14-15 for AMD (but not NVIDIA),
src/gromacs/listed_forces/listed_forces_gpu_internal_sycl.cpp: * - IntelLLVM 2023-02 for NVIDIA and Arc (but to a much lesser extent PVC).
src/gromacs/listed_forces/listed_forces_gpu_internal_sycl.cpp:    static_assert(gmx::numFTypesOnGpu == 8,
src/gromacs/listed_forces/listed_forces_gpu_internal_sycl.cpp:    constexpr FTypeArray(const T in[gmx::numFTypesOnGpu]) :
src/gromacs/listed_forces/listed_forces_gpu_internal_sycl.cpp:        __builtin_assume(idx >= 0 && idx < gmx::numFTypesOnGpu);
src/gromacs/listed_forces/listed_forces_gpu_internal_sycl.cpp:    T data[gmx::numFTypesOnGpu];
src/gromacs/listed_forces/listed_forces_gpu_internal_sycl.cpp:static inline void harmonic_gpu(const float              kA,
src/gromacs/listed_forces/listed_forces_gpu_internal_sycl.cpp:static inline void bonds_gpu(const int                                  i,
src/gromacs/listed_forces/listed_forces_gpu_internal_sycl.cpp:    harmonic_gpu<calcEner>(
src/gromacs/listed_forces/listed_forces_gpu_internal_sycl.cpp:static float bond_angle_gpu(const sycl::float4        xi,
src/gromacs/listed_forces/listed_forces_gpu_internal_sycl.cpp:static void angles_gpu(const int                                  i,
src/gromacs/listed_forces/listed_forces_gpu_internal_sycl.cpp:    float  theta = bond_angle_gpu<calcVir>(
src/gromacs/listed_forces/listed_forces_gpu_internal_sycl.cpp:    harmonic_gpu<calcEner>(
src/gromacs/listed_forces/listed_forces_gpu_internal_sycl.cpp:static void urey_bradley_gpu(const int                                  i,
src/gromacs/listed_forces/listed_forces_gpu_internal_sycl.cpp:    float  theta = bond_angle_gpu<calcVir>(
src/gromacs/listed_forces/listed_forces_gpu_internal_sycl.cpp:    harmonic_gpu<calcEner>(kthA, th0A, theta, &va, &dVdt);
src/gromacs/listed_forces/listed_forces_gpu_internal_sycl.cpp:    harmonic_gpu<calcEner>(kUBA, r13A, dr, &vbond, &fbond);
src/gromacs/listed_forces/listed_forces_gpu_internal_sycl.cpp:static float dih_angle_gpu(const T                   xi,
src/gromacs/listed_forces/listed_forces_gpu_internal_sycl.cpp:static float dih_angle_gpu_sincos(const T                   xi,
src/gromacs/listed_forces/listed_forces_gpu_internal_sycl.cpp:static void dopdihs_gpu(const float              cpA,
src/gromacs/listed_forces/listed_forces_gpu_internal_sycl.cpp:static void do_dih_fup_gpu(const int                            i,
src/gromacs/listed_forces/listed_forces_gpu_internal_sycl.cpp:static void pdihs_gpu(const int                                  i,
src/gromacs/listed_forces/listed_forces_gpu_internal_sycl.cpp:    float  phi = dih_angle_gpu<calcVir>(
src/gromacs/listed_forces/listed_forces_gpu_internal_sycl.cpp:    dopdihs_gpu(gm_forceparams[type].pdihs.cpA,
src/gromacs/listed_forces/listed_forces_gpu_internal_sycl.cpp:    do_dih_fup_gpu<calcVir>(
src/gromacs/listed_forces/listed_forces_gpu_internal_sycl.cpp:static void rbdihs_gpu(const int                                  i,
src/gromacs/listed_forces/listed_forces_gpu_internal_sycl.cpp:        const float negative_sin_phi = dih_angle_gpu_sincos<calcVir>(
src/gromacs/listed_forces/listed_forces_gpu_internal_sycl.cpp:        do_dih_fup_gpu<calcVir>(
src/gromacs/listed_forces/listed_forces_gpu_internal_sycl.cpp:static void idihs_gpu(const int                                  i,
src/gromacs/listed_forces/listed_forces_gpu_internal_sycl.cpp:    float  phi = dih_angle_gpu<calcVir>(
src/gromacs/listed_forces/listed_forces_gpu_internal_sycl.cpp:    do_dih_fup_gpu<calcVir>(
src/gromacs/listed_forces/listed_forces_gpu_internal_sycl.cpp:static void pairs_gpu(const int                                  i,
src/gromacs/listed_forces/listed_forces_gpu_internal_sycl.cpp:                  const BondedGpuKernelParameters& kernelParams,
src/gromacs/listed_forces/listed_forces_gpu_internal_sycl.cpp:                  const DeviceBuffer<t_iatom>      gm_iatoms_[numFTypesOnGpu],
src/gromacs/listed_forces/listed_forces_gpu_internal_sycl.cpp:    sycl::global_ptr<const t_iatom> gm_iatomsTemp[numFTypesOnGpu];
src/gromacs/listed_forces/listed_forces_gpu_internal_sycl.cpp:    for (int i = 0; i < numFTypesOnGpu; i++)
src/gromacs/listed_forces/listed_forces_gpu_internal_sycl.cpp:        for (int j = 0; j < numFTypesOnGpu; j++)
src/gromacs/listed_forces/listed_forces_gpu_internal_sycl.cpp:                fType                                          = fTypesOnGpu[j];
src/gromacs/listed_forces/listed_forces_gpu_internal_sycl.cpp:                        bonds_gpu<calcVir, calcEner>(
src/gromacs/listed_forces/listed_forces_gpu_internal_sycl.cpp:                        angles_gpu<calcVir, calcEner>(
src/gromacs/listed_forces/listed_forces_gpu_internal_sycl.cpp:                        urey_bradley_gpu<calcVir, calcEner>(
src/gromacs/listed_forces/listed_forces_gpu_internal_sycl.cpp:                        pdihs_gpu<calcVir, calcEner>(
src/gromacs/listed_forces/listed_forces_gpu_internal_sycl.cpp:                        rbdihs_gpu<calcVir, calcEner>(
src/gromacs/listed_forces/listed_forces_gpu_internal_sycl.cpp:                        idihs_gpu<calcVir, calcEner>(
src/gromacs/listed_forces/listed_forces_gpu_internal_sycl.cpp:                        pairs_gpu<calcVir, calcEner>(fTypeTid,
src/gromacs/listed_forces/listed_forces_gpu_internal_sycl.cpp:void ListedForcesGpu::Impl::launchKernel()
src/gromacs/listed_forces/listed_forces_gpu_internal_sycl.cpp:               "Cannot launch bonded GPU kernels unless bonded GPU work was scheduled");
src/gromacs/listed_forces/listed_forces_gpu_internal_sycl.cpp:    wallcycle_start_nocount(wcycle_, WallCycleCounter::LaunchGpuPp);
src/gromacs/listed_forces/listed_forces_gpu_internal_sycl.cpp:    wallcycle_sub_start(wcycle_, WallCycleSubCounter::LaunchGpuBonded);
src/gromacs/listed_forces/listed_forces_gpu_internal_sycl.cpp:    if (kernelParams_.fTypeRangeEnd[numFTypesOnGpu - 1] < 0)
src/gromacs/listed_forces/listed_forces_gpu_internal_sycl.cpp:    wallcycle_sub_stop(wcycle_, WallCycleSubCounter::LaunchGpuBonded);
src/gromacs/listed_forces/listed_forces_gpu_internal_sycl.cpp:    wallcycle_stop(wcycle_, WallCycleCounter::LaunchGpuPp);
src/gromacs/listed_forces/listed_forces_gpu_internal_sycl.cpp:void ListedForcesGpu::launchKernel(const gmx::StepWorkload& stepWork)
src/gromacs/listed_forces/CMakeLists.txt:    listed_forces_gpu_impl.cpp
src/gromacs/listed_forces/CMakeLists.txt:if(GMX_GPU_CUDA)
src/gromacs/listed_forces/CMakeLists.txt:       listed_forces_gpu_impl_gpu.cpp
src/gromacs/listed_forces/CMakeLists.txt:    # the CI build that does clang-tidy analysis of a CUDA build.
src/gromacs/listed_forces/CMakeLists.txt:       gmx_add_libgromacs_sources(listed_forces_gpu_internal.cu)
src/gromacs/listed_forces/CMakeLists.txt:    _gmx_add_files_to_property(CUDA_SOURCES
src/gromacs/listed_forces/CMakeLists.txt:      listed_forces_gpu_impl_gpu.cpp
src/gromacs/listed_forces/CMakeLists.txt:if (GMX_GPU_SYCL)
src/gromacs/listed_forces/CMakeLists.txt:        listed_forces_gpu_impl_gpu.cpp
src/gromacs/listed_forces/CMakeLists.txt:        listed_forces_gpu_internal_sycl.cpp
src/gromacs/listed_forces/CMakeLists.txt:        listed_forces_gpu_impl_gpu.cpp
src/gromacs/listed_forces/CMakeLists.txt:        listed_forces_gpu_internal_sycl.cpp
src/gromacs/listed_forces/listed_forces_gpu_internal.cu: * \brief Implements CUDA bonded functionality
src/gromacs/listed_forces/listed_forces_gpu_internal.cu: * \author Jon Vincent <jvincent@nvidia.com>
src/gromacs/listed_forces/listed_forces_gpu_internal.cu: * \author Alan Gray <alang@nvidia.com>
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:#include "gromacs/gpu_utils/cuda_arch_utils.cuh"
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:#include "gromacs/gpu_utils/cudautils.cuh"
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:#include "gromacs/gpu_utils/typecasts_cuda_hip.h"
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:#include "gromacs/gpu_utils/vectype_ops_cuda.h"
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:#include "gromacs/listed_forces/listed_forces_gpu.h"
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:#include "gromacs/pbcutil/pbc_aiuc_cuda.cuh"
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:#include "listed_forces_gpu_impl.h"
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:/*-------------------------------- CUDA kernels-------------------------------- */
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:#define CUDA_DEG2RAD_F (CUDART_PI_F / 180.0F)
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:/*---------------- BONDED CUDA kernels--------------*/
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:harmonic_gpu(const float kA, const float xA, const float x, float* V, float* F)
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:__device__ __forceinline__ void bonds_gpu(const int i,
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:    harmonic_gpu(d_forceparams[type].harmonic.krA, d_forceparams[type].harmonic.rA, dr, &vbond, &fbond);
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:__device__ __forceinline__ static float bond_angle_gpu(const float4   xi,
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:__device__ __forceinline__ void angles_gpu(const int i,
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:    float  theta = bond_angle_gpu<calcVir>(
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:    harmonic_gpu(d_forceparams[type].harmonic.krA,
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:                 d_forceparams[type].harmonic.rA * CUDA_DEG2RAD_F,
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:__device__ __forceinline__ void urey_bradley_gpu(const int i,
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:    float th0A = d_forceparams[type].u_b.thetaA * CUDA_DEG2RAD_F;
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:    float  theta = bond_angle_gpu<calcVir>(
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:    harmonic_gpu(kthA, th0A, theta, &va, &dVdt);
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:    harmonic_gpu(kUBA, r13A, dr, &vbond, &fbond);
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:__device__ __forceinline__ static float dih_angle_gpu(const T        xi,
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:__device__ __forceinline__ static float dih_angle_gpu_sincos(const T        xi,
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:dopdihs_gpu(const float cpA, const float phiA, const int mult, const float phi, float* v, float* f)
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:    mdphi = mult * phi - phiA * CUDA_DEG2RAD_F;
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:__device__ __forceinline__ static void do_dih_fup_gpu(const int      i,
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:__device__ __forceinline__ void pdihs_gpu(const int i,
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:    float  phi = dih_angle_gpu<calcVir>(
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:    dopdihs_gpu(d_forceparams[type].pdihs.cpA,
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:    do_dih_fup_gpu<calcVir>(
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:__device__ __forceinline__ void rbdihs_gpu(const int i,
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:    float negative_sin_phi = dih_angle_gpu_sincos<calcVir>(
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:    do_dih_fup_gpu<calcVir>(
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:__device__ __forceinline__ static void make_dp_periodic_gpu(float* dp)
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:    if (*dp >= CUDART_PI_F)
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:        *dp -= 2.0F * CUDART_PI_F;
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:    else if (*dp < -CUDART_PI_F)
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:        *dp += 2.0F * CUDART_PI_F;
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:__device__ __forceinline__ void idihs_gpu(const int i,
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:    float  phi = dih_angle_gpu<calcVir>(
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:    float phi0 = pA * CUDA_DEG2RAD_F;
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:    make_dp_periodic_gpu(&dp);
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:    do_dih_fup_gpu<calcVir>(
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:__device__ __forceinline__ void pairs_gpu(const int i,
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:    // TODO this should be made into a separate type, the GPU and CPU sizes should be compared
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:__global__ void bonded_kernel_gpu(BondedGpuKernelParameters kernelParams,
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:                                  BondedGpuKernelBuffers    kernelBuffers,
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:    for (int j = 0; j < numFTypesOnGpu; j++)
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:            fType                   = kernelParams.fTypesOnGpu[j];
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:                    bonds_gpu<calcVir, calcEner>(fTypeTid,
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:                    angles_gpu<calcVir, calcEner>(fTypeTid,
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:                    urey_bradley_gpu<calcVir, calcEner>(fTypeTid,
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:                    pdihs_gpu<calcVir, calcEner>(fTypeTid,
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:                    rbdihs_gpu<calcVir, calcEner>(fTypeTid,
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:                    idihs_gpu<calcVir, calcEner>(fTypeTid,
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:                    pairs_gpu<calcVir, calcEner>(fTypeTid,
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:/*-------------------------------- End CUDA kernels-----------------------------*/
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:void ListedForcesGpu::Impl::launchKernel()
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:               "Cannot launch bonded GPU kernels unless bonded GPU work was scheduled");
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:    wallcycle_start_nocount(wcycle_, WallCycleCounter::LaunchGpuPp);
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:    wallcycle_sub_start(wcycle_, WallCycleSubCounter::LaunchGpuBonded);
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:    int fTypeRangeEnd = kernelParams_.fTypeRangeEnd[numFTypesOnGpu - 1];
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:    auto kernelPtr = bonded_kernel_gpu<calcVir, calcEner>;
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:    const auto kernelArgs = prepareGpuKernelArguments(
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:    launchGpuKernel(kernelPtr,
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:                    "bonded_kernel_gpu<calcVir, calcEner>",
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:    wallcycle_sub_stop(wcycle_, WallCycleSubCounter::LaunchGpuBonded);
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:    wallcycle_stop(wcycle_, WallCycleCounter::LaunchGpuPp);
src/gromacs/listed_forces/listed_forces_gpu_internal.cu:void ListedForcesGpu::launchKernel(const gmx::StepWorkload& stepWork)
src/gromacs/listed_forces/listed_forces.cpp:                         const bool                                useGpu,
src/gromacs/listed_forces/listed_forces.cpp:    setup_bonded_threading(threading_.get(), numAtomsForce, useGpu, *idef_);
src/gromacs/listed_forces/listed_forces.cpp:    GMX_ASSERT(fr->listedForcesGpu != nullptr || workDivision.end(ftype) == iatoms.ssize(),
src/gromacs/listed_forces/manage_threading.h:/*! \brief Divide the listed interactions over the threads and GPU
src/gromacs/listed_forces/manage_threading.h:                            bool                          useGpuForBondeds,
src/gromacs/listed_forces/listed_forces.h:     * \param[in] useGpu         Whether a GPU is used to compute (part of) the listed interactions
src/gromacs/listed_forces/listed_forces.h:               bool                                useGpu,
src/gromacs/listed_forces/listed_forces_gpu_impl.h: * \brief Declares GPU implementation class for GPU bonded
src/gromacs/listed_forces/listed_forces_gpu_impl.h:#ifndef GMX_LISTED_FORCES_LISTED_FORCES_GPU_IMPL_H
src/gromacs/listed_forces/listed_forces_gpu_impl.h:#define GMX_LISTED_FORCES_LISTED_FORCES_GPU_IMPL_H
src/gromacs/listed_forces/listed_forces_gpu_impl.h:#include "gromacs/gpu_utils/gputraits.h"
src/gromacs/listed_forces/listed_forces_gpu_impl.h:#include "gromacs/gpu_utils/hostallocator.h"
src/gromacs/listed_forces/listed_forces_gpu_impl.h:#include "gromacs/listed_forces/listed_forces_gpu.h"
src/gromacs/listed_forces/listed_forces_gpu_impl.h:#if GMX_GPU_SYCL
src/gromacs/listed_forces/listed_forces_gpu_impl.h:#    include "gromacs/gpu_utils/syclutils.h"
src/gromacs/listed_forces/listed_forces_gpu_impl.h:/* \brief Bonded parameters and GPU pointers
src/gromacs/listed_forces/listed_forces_gpu_impl.h: * to the GPU as a single structure.
src/gromacs/listed_forces/listed_forces_gpu_impl.h:struct BondedGpuKernelParameters
src/gromacs/listed_forces/listed_forces_gpu_impl.h:    //! The bonded types on GPU
src/gromacs/listed_forces/listed_forces_gpu_impl.h:    int fTypesOnGpu[numFTypesOnGpu];
src/gromacs/listed_forces/listed_forces_gpu_impl.h:    int numFTypeBonds[numFTypesOnGpu];
src/gromacs/listed_forces/listed_forces_gpu_impl.h:    int fTypeRangeStart[numFTypesOnGpu];
src/gromacs/listed_forces/listed_forces_gpu_impl.h:    int fTypeRangeEnd[numFTypesOnGpu];
src/gromacs/listed_forces/listed_forces_gpu_impl.h:    BondedGpuKernelParameters()
src/gromacs/listed_forces/listed_forces_gpu_impl.h:struct BondedGpuKernelBuffers
src/gromacs/listed_forces/listed_forces_gpu_impl.h:    //! Force parameters (on GPU)
src/gromacs/listed_forces/listed_forces_gpu_impl.h:    //! Total Energy (on GPU)
src/gromacs/listed_forces/listed_forces_gpu_impl.h:    //! Interaction list atoms (on GPU)
src/gromacs/listed_forces/listed_forces_gpu_impl.h:    DeviceBuffer<t_iatom> d_iatoms[numFTypesOnGpu];
src/gromacs/listed_forces/listed_forces_gpu_impl.h:/*! \internal \brief Implements GPU bondeds */
src/gromacs/listed_forces/listed_forces_gpu_impl.h:class ListedForcesGpu::Impl
src/gromacs/listed_forces/listed_forces_gpu_impl.h:    /*! \brief Update flag that tells whether there are bonded interactions suitable for the GPU.
src/gromacs/listed_forces/listed_forces_gpu_impl.h:    /*! \brief Update lists of interactions from idef suitable for the GPU,
src/gromacs/listed_forces/listed_forces_gpu_impl.h:     * stage. Copies the bonded interactions assigned to the GPU
src/gromacs/listed_forces/listed_forces_gpu_impl.h:    /*! \brief Launches bonded kernel on a GPU */
src/gromacs/listed_forces/listed_forces_gpu_impl.h:     * assigned to the GPU */
src/gromacs/listed_forces/listed_forces_gpu_impl.h:    //! GPU context object
src/gromacs/listed_forces/listed_forces_gpu_impl.h:    //! \brief Bonded GPU stream, not owned by this module
src/gromacs/listed_forces/listed_forces_gpu_impl.h:    //! Parameters, passed to the GPU kernel
src/gromacs/listed_forces/listed_forces_gpu_impl.h:    BondedGpuKernelParameters kernelParams_;
src/gromacs/listed_forces/listed_forces_gpu_impl.h:    //! Buffers, used in the GPU kernel
src/gromacs/listed_forces/listed_forces_gpu_impl.h:    BondedGpuKernelBuffers kernelBuffers_;
src/gromacs/listed_forces/listed_forces_gpu_impl.h:    //! GPU kernel launch configuration
src/gromacs/listed_forces/listed_forces_gpu_impl.h:#endif // GMX_LISTED_FORCES_LISTED_FORCES_GPU_IMPL_H
src/gromacs/mdrun/runner.cpp:#include "gromacs/domdec/gpuhaloexchange.h"
src/gromacs/mdrun/runner.cpp:#include "gromacs/ewald/pme_gpu_program.h"
src/gromacs/mdrun/runner.cpp:#include "gromacs/ewald/pme_pp_comm_gpu.h"
src/gromacs/mdrun/runner.cpp:#include "gromacs/gpu_utils/device_stream_manager.h"
src/gromacs/mdrun/runner.cpp:#include "gromacs/gpu_utils/gpu_utils.h"
src/gromacs/mdrun/runner.cpp:#include "gromacs/gpu_utils/gpueventsynchronizer_helpers.h"
src/gromacs/mdrun/runner.cpp:#include "gromacs/gpu_utils/hostallocator.h"
src/gromacs/mdrun/runner.cpp:#include "gromacs/gpu_utils/nvshmem_utils.h"
src/gromacs/mdrun/runner.cpp:#include "gromacs/listed_forces/listed_forces_gpu.h"
src/gromacs/mdrun/runner.cpp:#include "gromacs/mdlib/gpuforcereduction.h"
src/gromacs/mdrun/runner.cpp:#include "gromacs/mdlib/mdgraph_gpu.h"
src/gromacs/mdrun/runner.cpp:#include "gromacs/mdtypes/state_propagator_data_gpu.h"
src/gromacs/mdrun/runner.cpp:#include "gromacs/nbnxm/gpu_data_mgmt.h"
src/gromacs/mdrun/runner.cpp:#include "gromacs/taskassignment/decidegpuusage.h"
src/gromacs/mdrun/runner.cpp:#include "gromacs/taskassignment/usergpuids.h"
src/gromacs/mdrun/runner.cpp:#include "gromacs/timing/gpu_timing.h"
src/gromacs/mdrun/runner.cpp: * the GPU communication flags are set to false in non-tMPI and non-CUDA builds.
src/gromacs/mdrun/runner.cpp: * \param[in]  useGpuForNonbonded   True if the nonbonded task is offloaded in this run.
src/gromacs/mdrun/runner.cpp: * \param[in]  gpuAwareMpiStatus  Minimum level of GPU-aware MPI support across all ranks
src/gromacs/mdrun/runner.cpp:                                                         const bool           useGpuForNonbonded,
src/gromacs/mdrun/runner.cpp:                                                         gmx::GpuAwareMpiStatus gpuAwareMpiStatus)
src/gromacs/mdrun/runner.cpp:    devFlags.enableGpuBufferOps = (GMX_GPU_CUDA || GMX_GPU_SYCL) && useGpuForNonbonded
src/gromacs/mdrun/runner.cpp:                                  && (getenv("GMX_USE_GPU_BUFFER_OPS") != nullptr);
src/gromacs/mdrun/runner.cpp:    if (getenv("GMX_CUDA_GRAPH") != nullptr)
src/gromacs/mdrun/runner.cpp:        if (GMX_HAVE_GPU_GRAPH_SUPPORT)
src/gromacs/mdrun/runner.cpp:            devFlags.enableCudaGraphs = true;
src/gromacs/mdrun/runner.cpp:                            "GMX_CUDA_GRAPH environment variable is detected. "
src/gromacs/mdrun/runner.cpp:                            "The experimental CUDA Graphs feature will be used if run conditions "
src/gromacs/mdrun/runner.cpp:            devFlags.enableCudaGraphs = false;
src/gromacs/mdrun/runner.cpp:            if (GMX_GPU_CUDA)
src/gromacs/mdrun/runner.cpp:                errorReason = "the CUDA version in use is below the minimum requirement (11.1)";
src/gromacs/mdrun/runner.cpp:            else if (GMX_GPU_SYCL)
src/gromacs/mdrun/runner.cpp:                errorReason = "CUDA or SYCL build is required";
src/gromacs/mdrun/runner.cpp:                            "GMX_CUDA_GRAPH environment variable is detected, but %s. GPU Graphs "
src/gromacs/mdrun/runner.cpp:    // Flag use to enable GPU-aware MPI depenendent features such PME GPU decomposition
src/gromacs/mdrun/runner.cpp:    // GPU-aware MPI is marked available if it has been detected by GROMACS or detection fails but
src/gromacs/mdrun/runner.cpp:    devFlags.canUseGpuAwareMpi = false;
src/gromacs/mdrun/runner.cpp:    // Direct GPU comm path is being used with GPU-aware MPI
src/gromacs/mdrun/runner.cpp:    // make sure underlying MPI implementation is GPU-aware
src/gromacs/mdrun/runner.cpp:    if (GMX_LIB_MPI && (GMX_GPU_CUDA || GMX_GPU_SYCL))
src/gromacs/mdrun/runner.cpp:        // Allow overriding the detection for GPU-aware MPI
src/gromacs/mdrun/runner.cpp:        if (getenv("GMX_FORCE_CUDA_AWARE_MPI") != nullptr)
src/gromacs/mdrun/runner.cpp:                            "GMX_FORCE_CUDA_AWARE_MPI environment variable is inactive. "
src/gromacs/mdrun/runner.cpp:                            "Please use GMX_FORCE_GPU_AWARE_MPI instead.");
src/gromacs/mdrun/runner.cpp:        devFlags.canUseGpuAwareMpi = (gpuAwareMpiStatus == gmx::GpuAwareMpiStatus::Supported
src/gromacs/mdrun/runner.cpp:                                      || gpuAwareMpiStatus == gmx::GpuAwareMpiStatus::Forced);
src/gromacs/mdrun/runner.cpp:        if (getenv("GMX_ENABLE_DIRECT_GPU_COMM") != nullptr)
src/gromacs/mdrun/runner.cpp:            if (gpuAwareMpiStatus == gmx::GpuAwareMpiStatus::Forced)
src/gromacs/mdrun/runner.cpp:                // GPU-aware support not detected in MPI library but, user has forced its use
src/gromacs/mdrun/runner.cpp:                                "This run has forced use of 'GPU-aware MPI'. "
src/gromacs/mdrun/runner.cpp:                                "However, GROMACS cannot determine if underlying MPI is GPU-aware. "
src/gromacs/mdrun/runner.cpp:                                "Check the GROMACS install guide for recommendations for GPU-aware "
src/gromacs/mdrun/runner.cpp:                                "GMX_FORCE_GPU_AWARE_MPI environment variable.");
src/gromacs/mdrun/runner.cpp:            if (devFlags.canUseGpuAwareMpi)
src/gromacs/mdrun/runner.cpp:                                "GMX_ENABLE_DIRECT_GPU_COMM environment variable detected, "
src/gromacs/mdrun/runner.cpp:                                "enabling direct GPU communication using GPU-aware MPI.");
src/gromacs/mdrun/runner.cpp:                                "GPU-aware MPI was not detected, will not use direct GPU "
src/gromacs/mdrun/runner.cpp:                                "for GPU-aware support. If you are certain about GPU-aware support "
src/gromacs/mdrun/runner.cpp:                                "GMX_FORCE_GPU_AWARE_MPI environment variable.");
src/gromacs/mdrun/runner.cpp:        else if (gpuAwareMpiStatus == gmx::GpuAwareMpiStatus::Supported)
src/gromacs/mdrun/runner.cpp:            // GPU-aware MPI was detected, let the user know that using it may improve performance
src/gromacs/mdrun/runner.cpp:                            "GPU-aware MPI detected, but by default GROMACS will not "
src/gromacs/mdrun/runner.cpp:                            "make use the direct GPU communication capabilities of MPI. "
src/gromacs/mdrun/runner.cpp:                            "the GMX_ENABLE_DIRECT_GPU_COMM environment variable.");
src/gromacs/mdrun/runner.cpp:        if (getenv("GMX_FORCE_GPU_AWARE_MPI") != nullptr)
src/gromacs/mdrun/runner.cpp:            // Cannot force use of GPU-aware MPI in this build configuration
src/gromacs/mdrun/runner.cpp:                            "A CUDA or SYCL build with an external MPI library is required in "
src/gromacs/mdrun/runner.cpp:                            "order to benefit from GMX_FORCE_GPU_AWARE_MPI. That environment "
src/gromacs/mdrun/runner.cpp:    if (devFlags.enableGpuBufferOps)
src/gromacs/mdrun/runner.cpp:                        "This run uses the 'GPU buffer ops' feature, enabled by the "
src/gromacs/mdrun/runner.cpp:                        "GMX_USE_GPU_BUFFER_OPS environment variable.");
src/gromacs/mdrun/runner.cpp:    // PME decomposition is supported only with CUDA or SYCL and also
src/gromacs/mdrun/runner.cpp:    // needs GPU-aware MPI support for it to work.
src/gromacs/mdrun/runner.cpp:    const bool pmeGpuDecompositionRequested =
src/gromacs/mdrun/runner.cpp:            (pmeRunMode == PmeRunMode::GPU || pmeRunMode == PmeRunMode::Mixed)
src/gromacs/mdrun/runner.cpp:    const bool pmeGpuDecompositionSupported =
src/gromacs/mdrun/runner.cpp:            (devFlags.canUseGpuAwareMpi && (GMX_GPU_CUDA || GMX_GPU_SYCL)
src/gromacs/mdrun/runner.cpp:             && ((pmeRunMode == PmeRunMode::GPU && (GMX_USE_Heffte || GMX_USE_cuFFTMp))
src/gromacs/mdrun/runner.cpp:    const bool forcePmeGpuDecomposition = getenv("GMX_GPU_PME_DECOMPOSITION") != nullptr;
src/gromacs/mdrun/runner.cpp:    if (pmeGpuDecompositionSupported && pmeGpuDecompositionRequested)
src/gromacs/mdrun/runner.cpp:        // PME decomposition is supported only when it is forced using GMX_GPU_PME_DECOMPOSITION
src/gromacs/mdrun/runner.cpp:        if (forcePmeGpuDecomposition)
src/gromacs/mdrun/runner.cpp:                            "This run has requested the 'GPU PME decomposition' feature, enabled "
src/gromacs/mdrun/runner.cpp:                            "by the GMX_GPU_PME_DECOMPOSITION environment variable. "
src/gromacs/mdrun/runner.cpp:                      "Multiple PME tasks were required to run on GPUs, "
src/gromacs/mdrun/runner.cpp:                      "Use GMX_GPU_PME_DECOMPOSITION environment variable to enable it.");
src/gromacs/mdrun/runner.cpp:    if (!pmeGpuDecompositionSupported && pmeGpuDecompositionRequested)
src/gromacs/mdrun/runner.cpp:        if (GMX_GPU_CUDA)
src/gromacs/mdrun/runner.cpp:                      "PME tasks were required to run on more than one CUDA-devices. To enable "
src/gromacs/mdrun/runner.cpp:                      "use MPI with CUDA-aware support and build GROMACS with cuFFTMp support.");
src/gromacs/mdrun/runner.cpp:                    "PME tasks were required to run on GPUs, but that is not implemented with "
src/gromacs/mdrun/runner.cpp:    devFlags.enableGpuPmeDecomposition =
src/gromacs/mdrun/runner.cpp:            forcePmeGpuDecomposition && pmeGpuDecompositionRequested && pmeGpuDecompositionSupported;
src/gromacs/mdrun/runner.cpp:                                  bool                           makeGpuPairList,
src/gromacs/mdrun/runner.cpp:                (makeGpuPairList ? ListSetupType::Gpu : ListSetupType::CpuSimdWhenSupported);
src/gromacs/mdrun/runner.cpp:                fplog, cr, ir, nstlist_cmdline, &mtop, box, effectiveAtomDensity.value(), makeGpuPairList, cpuinfo);
src/gromacs/mdrun/runner.cpp:/*! \brief Return whether GPU acceleration of nonbondeds is supported with the given settings.
src/gromacs/mdrun/runner.cpp:static bool gpuAccelerationOfNonbondedIsUseful(const MDLogger&   mdlog,
src/gromacs/mdrun/runner.cpp:    bool        gpuIsUseful = true;
src/gromacs/mdrun/runner.cpp:        /* The GPU code does not support more than one energy group.
src/gromacs/mdrun/runner.cpp:         * If the user requested GPUs explicitly, a fatal error is given later.
src/gromacs/mdrun/runner.cpp:        gpuIsUseful = false;
src/gromacs/mdrun/runner.cpp:                    "Multiple energy groups is not implemented for GPUs, falling back to the CPU. "
src/gromacs/mdrun/runner.cpp:                    "For better performance, run on the GPU without energy groups and then do "
src/gromacs/mdrun/runner.cpp:    /* There are resource handling issues in the GPU code paths with MTS on anything else than only
src/gromacs/mdrun/runner.cpp:        gpuIsUseful = false;
src/gromacs/mdrun/runner.cpp:                "Multiple time stepping is only supported with GPUs when MTS is only applied to %s "
src/gromacs/mdrun/runner.cpp:        gpuIsUseful = false;
src/gromacs/mdrun/runner.cpp:        warning     = "TPI is not implemented for GPUs.";
src/gromacs/mdrun/runner.cpp:    if (!gpuIsUseful && issueWarning)
src/gromacs/mdrun/runner.cpp:    return gpuIsUseful;
src/gromacs/mdrun/runner.cpp:    else if (strncmp(optionString, "gpu", 3) == 0)
src/gromacs/mdrun/runner.cpp:        returnValue = TaskTarget::Gpu;
src/gromacs/mdrun/runner.cpp:        auto* nbnxn_gpu_timings =
src/gromacs/mdrun/runner.cpp:                (nbv != nullptr && nbv->useGpu()) ? gpu_get_timings(nbv->gpuNbv()) : nullptr;
src/gromacs/mdrun/runner.cpp:        gmx_wallclock_gpu_pme_t pme_gpu_timings = {};
src/gromacs/mdrun/runner.cpp:        if (pme_gpu_task_enabled(pme))
src/gromacs/mdrun/runner.cpp:            pme_gpu_get_timings(pme, &pme_gpu_timings);
src/gromacs/mdrun/runner.cpp:                        nbnxn_gpu_timings,
src/gromacs/mdrun/runner.cpp:                        &pme_gpu_timings);
src/gromacs/mdrun/runner.cpp:    EmulateGpuNonbonded emulateGpuNonbonded =
src/gromacs/mdrun/runner.cpp:            (getenv("GMX_EMULATE_GPU") != nullptr ? EmulateGpuNonbonded::Yes : EmulateGpuNonbonded::No);
src/gromacs/mdrun/runner.cpp:    std::vector<int> userGpuTaskAssignment;
src/gromacs/mdrun/runner.cpp:        userGpuTaskAssignment = parseUserTaskAssignmentString(hw_opt.userGpuTaskAssignment);
src/gromacs/mdrun/runner.cpp:        bool useGpuForNonbonded = false;
src/gromacs/mdrun/runner.cpp:        bool useGpuForPme       = false;
src/gromacs/mdrun/runner.cpp:            // the number of GPUs to choose the number of ranks.
src/gromacs/mdrun/runner.cpp:            auto canUseGpuForNonbonded = buildSupportsNonbondedOnGpu(nullptr);
src/gromacs/mdrun/runner.cpp:            useGpuForNonbonded         = decideWhetherToUseGpusForNonbondedWithThreadMpi(
src/gromacs/mdrun/runner.cpp:                    userGpuTaskAssignment,
src/gromacs/mdrun/runner.cpp:                    emulateGpuNonbonded,
src/gromacs/mdrun/runner.cpp:                    canUseGpuForNonbonded,
src/gromacs/mdrun/runner.cpp:                    gpuAccelerationOfNonbondedIsUseful(mdlog, *inputrec, GMX_THREAD_MPI, doRerun),
src/gromacs/mdrun/runner.cpp:            useGpuForPme = decideWhetherToUseGpusForPmeWithThreadMpi(useGpuForNonbonded,
src/gromacs/mdrun/runner.cpp:                                                                     userGpuTaskAssignment,
src/gromacs/mdrun/runner.cpp:                                                useGpuForNonbonded,
src/gromacs/mdrun/runner.cpp:                                                useGpuForPme,
src/gromacs/mdrun/runner.cpp:    // Note that when bonded interactions run on a GPU they always run
src/gromacs/mdrun/runner.cpp:    bool useGpuForNonbonded = false;
src/gromacs/mdrun/runner.cpp:    bool useGpuForPme       = false;
src/gromacs/mdrun/runner.cpp:    bool useGpuForBonded    = false;
src/gromacs/mdrun/runner.cpp:    bool useGpuForUpdate    = false;
src/gromacs/mdrun/runner.cpp:    bool gpusWereDetected   = hwinfo_->ngpu_compatible_tot > 0;
src/gromacs/mdrun/runner.cpp:        // It's possible that there are different numbers of GPUs on
src/gromacs/mdrun/runner.cpp:        auto canUseGpuForNonbonded = buildSupportsNonbondedOnGpu(nullptr);
src/gromacs/mdrun/runner.cpp:        useGpuForNonbonded         = decideWhetherToUseGpusForNonbonded(
src/gromacs/mdrun/runner.cpp:                userGpuTaskAssignment,
src/gromacs/mdrun/runner.cpp:                emulateGpuNonbonded,
src/gromacs/mdrun/runner.cpp:                canUseGpuForNonbonded,
src/gromacs/mdrun/runner.cpp:                gpuAccelerationOfNonbondedIsUseful(mdlog, *inputrec, !GMX_THREAD_MPI, doRerun),
src/gromacs/mdrun/runner.cpp:                gpusWereDetected);
src/gromacs/mdrun/runner.cpp:        useGpuForPme    = decideWhetherToUseGpusForPme(useGpuForNonbonded,
src/gromacs/mdrun/runner.cpp:                                                    userGpuTaskAssignment,
src/gromacs/mdrun/runner.cpp:                                                    gpusWereDetected);
src/gromacs/mdrun/runner.cpp:        useGpuForBonded = decideWhetherToUseGpusForBonded(
src/gromacs/mdrun/runner.cpp:                useGpuForNonbonded, useGpuForPme, bondedTarget, *inputrec, mtop, domdecOptions.numPmeRanks, gpusWereDetected);
src/gromacs/mdrun/runner.cpp:    const PmeRunMode pmeRunMode = determinePmeRunMode(useGpuForPme, pmeFftTarget, *inputrec);
src/gromacs/mdrun/runner.cpp:    // We are using the minimal supported level of GPU-aware MPI
src/gromacs/mdrun/runner.cpp:                                                                       useGpuForNonbonded,
src/gromacs/mdrun/runner.cpp:                                                                       hwinfo_->minGpuAwareMpiStatus);
src/gromacs/mdrun/runner.cpp:                                                              updateTarget == TaskTarget::Gpu);
src/gromacs/mdrun/runner.cpp:                || (!useGpuForNonbonded && usingFullElectrostatics(inputrec->coulombtype)
src/gromacs/mdrun/runner.cpp:                          useGpuForNonbonded || (emulateGpuNonbonded == EmulateGpuNonbonded::Yes),
src/gromacs/mdrun/runner.cpp:        useGpuForUpdate = decideWhetherToUseGpuForUpdate(useDomainDecomposition,
src/gromacs/mdrun/runner.cpp:                                                         useGpuForNonbonded,
src/gromacs/mdrun/runner.cpp:                                                         gpusWereDetected,
src/gromacs/mdrun/runner.cpp:    const bool canUseDirectGpuComm = decideWhetherDirectGpuCommunicationCanBeUsed(
src/gromacs/mdrun/runner.cpp:    bool useGpuDirectHalo = false;
src/gromacs/mdrun/runner.cpp:    if (useGpuForNonbonded)
src/gromacs/mdrun/runner.cpp:        // domdecOptions.numPmeRanks == -1 results in 0 separate PME ranks when useGpuForNonbonded is true.
src/gromacs/mdrun/runner.cpp:        useGpuDirectHalo = decideWhetherToUseGpuForHalo(havePPDomainDecomposition,
src/gromacs/mdrun/runner.cpp:                                                        useGpuForNonbonded,
src/gromacs/mdrun/runner.cpp:                                                        canUseDirectGpuComm,
src/gromacs/mdrun/runner.cpp:        // The DD builder will disable useGpuDirectHalo if the Y or Z component of any domain is
src/gromacs/mdrun/runner.cpp:        // smaller than twice the communication distance, since GPU-direct communication presently
src/gromacs/mdrun/runner.cpp:        // perform well on multiple GPUs in any case, but it is important that our core functionality
src/gromacs/mdrun/runner.cpp:        // (in particular for testing) does not break depending on GPU direct communication being enabled.
src/gromacs/mdrun/runner.cpp:                useGpuForNonbonded,
src/gromacs/mdrun/runner.cpp:                useGpuForPme,
src/gromacs/mdrun/runner.cpp:                useGpuForUpdate,
src/gromacs/mdrun/runner.cpp:                useGpuDirectHalo,
src/gromacs/mdrun/runner.cpp:                devFlags.enableGpuPmeDecomposition);
src/gromacs/mdrun/runner.cpp:    GpuTaskAssignments gpuTaskAssignments = GpuTaskAssignmentsBuilder::build(
src/gromacs/mdrun/runner.cpp:            userGpuTaskAssignment,
src/gromacs/mdrun/runner.cpp:            useGpuForNonbonded,
src/gromacs/mdrun/runner.cpp:            useGpuForPme,
src/gromacs/mdrun/runner.cpp:    DeviceInformation* deviceInfo = gpuTaskAssignments.initDevice();
src/gromacs/mdrun/runner.cpp:    GMX_RELEASE_ASSERT(sc_gpuClusterSize(pairlistType) >= 4,
src/gromacs/mdrun/runner.cpp:                       "The verlet scheme setup relies on the GPU cluster size to be at least 4");
src/gromacs/mdrun/runner.cpp:        // TODO Pass the GPU streams to ddBuilder to use in buffer
src/gromacs/mdrun/runner.cpp:    const bool useGpuPmeDecomposition = numPmeDomains.x * numPmeDomains.y > 1 && useGpuForPme;
src/gromacs/mdrun/runner.cpp:    GMX_RELEASE_ASSERT(!useGpuPmeDecomposition || devFlags.enableGpuPmeDecomposition,
src/gromacs/mdrun/runner.cpp:                       "GPU PME decomposition works only in the cases where it is supported");
src/gromacs/mdrun/runner.cpp:                                                              useGpuForNonbonded,
src/gromacs/mdrun/runner.cpp:                                                              useGpuForBonded,
src/gromacs/mdrun/runner.cpp:                                                              useGpuForUpdate,
src/gromacs/mdrun/runner.cpp:                                                              useGpuDirectHalo,
src/gromacs/mdrun/runner.cpp:                                                              canUseDirectGpuComm,
src/gromacs/mdrun/runner.cpp:                                                              useGpuPmeDecomposition);
src/gromacs/mdrun/runner.cpp:        && (runScheduleWork.simulationWork.useGpuDirectCommunication
src/gromacs/mdrun/runner.cpp:            || runScheduleWork.simulationWork.useGpuPmeDecomposition
src/gromacs/mdrun/runner.cpp:            || runScheduleWork.simulationWork.useGpuPmePpCommunication
src/gromacs/mdrun/runner.cpp:            || runScheduleWork.simulationWork.useGpuHaloExchange))
src/gromacs/mdrun/runner.cpp:        doubleCheckGpuAwareMpiWillWork(*deviceInfo);
src/gromacs/mdrun/runner.cpp:    if (runScheduleWork.simulationWork.useGpuDirectCommunication && GMX_GPU_CUDA)
src/gromacs/mdrun/runner.cpp:        // Don't enable event counting with GPU Direct comm, see #3988.
src/gromacs/mdrun/runner.cpp:        gmx::internal::disableGpuEventConsumptionCounting();
src/gromacs/mdrun/runner.cpp:    if (isSimulationMainRank && GMX_GPU_SYCL)
src/gromacs/mdrun/runner.cpp:        bool                      haveAnyGpuWork = simWorkload.useGpuPme || simWorkload.useGpuBonded
src/gromacs/mdrun/runner.cpp:                              || simWorkload.useGpuNonbonded || simWorkload.useGpuUpdate;
src/gromacs/mdrun/runner.cpp:        if (haveAnyGpuWork)
src/gromacs/mdrun/runner.cpp:                            "\nNOTE: SYCL GPU support in GROMACS, and the compilers, libraries,\n"
src/gromacs/mdrun/runner.cpp:    gpuTaskAssignments.reportGpuUsage(mdlog, printHostName, pmeRunMode, runScheduleWork.simulationWork);
src/gromacs/mdrun/runner.cpp:        const bool useGpuTiming = decideGpuTimingsUsage();
src/gromacs/mdrun/runner.cpp:                *deviceInfo, runScheduleWork.simulationWork, useGpuTiming);
src/gromacs/mdrun/runner.cpp:    if (!userGpuTaskAssignment.empty())
src/gromacs/mdrun/runner.cpp:        gpuTaskAssignments.logPerformanceHints(mdlog, numAvailableDevices);
src/gromacs/mdrun/runner.cpp:    check_resource_division_efficiency(hwinfo_, gpuTaskAssignments.thisRankHasAnyGpuTask(), cr, mdlog);
src/gromacs/mdrun/runner.cpp:    // Enable Peer access between GPUs where available
src/gromacs/mdrun/runner.cpp:    // any of the GPU communication features are active.
src/gromacs/mdrun/runner.cpp:        && (runScheduleWork.simulationWork.useGpuHaloExchange
src/gromacs/mdrun/runner.cpp:            || runScheduleWork.simulationWork.useGpuPmePpCommunication))
src/gromacs/mdrun/runner.cpp:        setupGpuDevicePeerAccess(gpuTaskAssignments.deviceIdsAssigned(), mdlog);
src/gromacs/mdrun/runner.cpp:    const bool thisRankHasPmeGpuTask = gpuTaskAssignments.thisRankHasPmeGpuTask();
src/gromacs/mdrun/runner.cpp:        if (runScheduleWork.simulationWork.useGpuPmePpCommunication && !thisRankHasDuty(cr, DUTY_PME))
src/gromacs/mdrun/runner.cpp:                    "GPU device stream manager should be valid in order to use PME-PP direct "
src/gromacs/mdrun/runner.cpp:                    "GPU PP-PME stream should be valid in order to use GPU PME-PP direct "
src/gromacs/mdrun/runner.cpp:            fr->pmePpCommGpu = std::make_unique<gmx::PmePpCommGpu>(
src/gromacs/mdrun/runner.cpp:                                 runScheduleWork.simulationWork.useGpuNonbonded,
src/gromacs/mdrun/runner.cpp:        // TODO: Move the logic below to a GPU bonded builder
src/gromacs/mdrun/runner.cpp:        if (runScheduleWork.simulationWork.useGpuBonded)
src/gromacs/mdrun/runner.cpp:                               "GPU device stream manager should be valid in order to use GPU "
src/gromacs/mdrun/runner.cpp:            fr->listedForcesGpu =
src/gromacs/mdrun/runner.cpp:                    std::make_unique<ListedForcesGpu>(mtop.ffparams,
src/gromacs/mdrun/runner.cpp:        mdAtoms = makeMDAtoms(fplog, mtop, *inputrec, thisRankHasPmeGpuTask);
src/gromacs/mdrun/runner.cpp:        if (globalState && thisRankHasPmeGpuTask)
src/gromacs/mdrun/runner.cpp:            // PME on GPU without DD or on a separate PME rank, and because the local state pointer
src/gromacs/mdrun/runner.cpp:    PmeGpuProgramStorage pmeGpuProgram;
src/gromacs/mdrun/runner.cpp:    if (thisRankHasPmeGpuTask)
src/gromacs/mdrun/runner.cpp:                "GPU device stream manager should be initialized in order to use GPU for PME.");
src/gromacs/mdrun/runner.cpp:                           "GPU device should be initialized in order to use GPU for PME.");
src/gromacs/mdrun/runner.cpp:        pmeGpuProgram = buildPmeGpuProgram(deviceStreamManager->context());
src/gromacs/mdrun/runner.cpp:                GMX_RELEASE_ASSERT(!runScheduleWork.simulationWork.useGpuPme
src/gromacs/mdrun/runner.cpp:                                   "Device stream manager should be valid in order to use GPU "
src/gromacs/mdrun/runner.cpp:                        !runScheduleWork.simulationWork.useGpuPme
src/gromacs/mdrun/runner.cpp:                        "GPU PME stream should be valid in order to use GPU version of PME.");
src/gromacs/mdrun/runner.cpp:                const DeviceContext* deviceContext = runScheduleWork.simulationWork.useGpuPme
src/gromacs/mdrun/runner.cpp:                        runScheduleWork.simulationWork.useGpuPme
src/gromacs/mdrun/runner.cpp:                                       pmeGpuProgram.get(),
src/gromacs/mdrun/runner.cpp:            if (runScheduleWork.simulationWork.useGpuFBufferOpsWhenAllowed)
src/gromacs/mdrun/runner.cpp:                fr->gpuForceReduction[gmx::AtomLocality::Local] = std::make_unique<gmx::GpuForceReduction>(
src/gromacs/mdrun/runner.cpp:                    fr->gpuForceReduction[gmx::AtomLocality::NonLocal] =
src/gromacs/mdrun/runner.cpp:                            std::make_unique<gmx::GpuForceReduction>(
src/gromacs/mdrun/runner.cpp:                if (runScheduleWork.simulationWork.useMdGpuGraph)
src/gromacs/mdrun/runner.cpp:                            std::make_unique<gmx::MdGpuGraph>(*fr->deviceStreamManager,
src/gromacs/mdrun/runner.cpp:                            std::make_unique<gmx::MdGpuGraph>(*fr->deviceStreamManager,
src/gromacs/mdrun/runner.cpp:            std::unique_ptr<gmx::StatePropagatorDataGpu> stateGpu;
src/gromacs/mdrun/runner.cpp:            if (gpusWereDetected && gmx::needStateGpu(runScheduleWork.simulationWork))
src/gromacs/mdrun/runner.cpp:                GpuApiCallBehavior transferKind =
src/gromacs/mdrun/runner.cpp:                                ? GpuApiCallBehavior::Async
src/gromacs/mdrun/runner.cpp:                                : GpuApiCallBehavior::Sync;
src/gromacs/mdrun/runner.cpp:                                   "GPU device stream manager should be initialized to use GPU.");
src/gromacs/mdrun/runner.cpp:                stateGpu = std::make_unique<gmx::StatePropagatorDataGpu>(
src/gromacs/mdrun/runner.cpp:                        pme_gpu_get_block_size(fr->pmedata),
src/gromacs/mdrun/runner.cpp:                fr->stateGpu = stateGpu.get();
src/gromacs/mdrun/runner.cpp:            if (fr->pmePpCommGpu)
src/gromacs/mdrun/runner.cpp:                // destroy object since it is no longer required. (This needs to be done while the GPU context still exists.)
src/gromacs/mdrun/runner.cpp:                fr->pmePpCommGpu.reset();
src/gromacs/mdrun/runner.cpp:                        runScheduleWork.simulationWork.useGpuPmePpCommunication,
src/gromacs/mdrun/runner.cpp:            /* stop the GPU profiler (only CUDA);
src/gromacs/mdrun/runner.cpp:            stopGpuProfiler();
src/gromacs/mdrun/runner.cpp:        // before we destroy the GPU context(s)
src/gromacs/mdrun/runner.cpp:        // Pinned buffers are associated with contexts in CUDA.
src/gromacs/mdrun/runner.cpp:        // As soon as we destroy GPU contexts after mdrunner() exits, these lines should go.
src/gromacs/mdrun/runner.cpp:        mdModules_.reset(nullptr); // destruct force providers here as they might also use the GPU
src/gromacs/mdrun/runner.cpp:        fr.reset(nullptr);         // destruct forcerec before gpu
src/gromacs/mdrun/runner.cpp:         * destroying the CUDA context as some tMPI ranks may be sharing
src/gromacs/mdrun/runner.cpp:         * GPU and context.
src/gromacs/mdrun/runner.cpp:         * This is not a concern in OpenCL where we use one context per rank.
src/gromacs/mdrun/runner.cpp:         * Note: it is safe to not call the barrier on the ranks which do not use GPU,
src/gromacs/mdrun/runner.cpp:         * Note that this function needs to be called even if GPUs are not used
src/gromacs/mdrun/runner.cpp:         * in this run because the PME ranks have no knowledge of whether GPUs
src/gromacs/mdrun/runner.cpp:         * that it's not needed anymore (with a shared GPU run).
src/gromacs/mdrun/runner.cpp:        const bool haveDetectedOrForcedCudaAwareMpi =
src/gromacs/mdrun/runner.cpp:                (gmx::checkMpiCudaAwareSupport() == gmx::GpuAwareMpiStatus::Supported
src/gromacs/mdrun/runner.cpp:                 || gmx::checkMpiCudaAwareSupport() == gmx::GpuAwareMpiStatus::Forced);
src/gromacs/mdrun/runner.cpp:        if (!haveDetectedOrForcedCudaAwareMpi)
src/gromacs/mdrun/runner.cpp:            // Don't reset GPU in case of GPU-AWARE MPI
src/gromacs/mdrun/runner.cpp:            // UCX creates GPU buffers which are cleaned-up as part of MPI_Finalize()
src/gromacs/mdrun/mimic.cpp:#include "gromacs/gpu_utils/gpu_utils.h"
src/gromacs/mdrun/mimic.cpp:#include "gromacs/gpu_utils/hostallocator.h"
src/gromacs/mdrun/mimic.cpp:#include "gromacs/listed_forces/listed_forces_gpu.h"
src/gromacs/mdrun/mimic.cpp:                                 runScheduleWork_->simulationWork.useGpuPme);
src/gromacs/mdrun/mimic.cpp:            if (fr_->listedForcesGpu)
src/gromacs/mdrun/mimic.cpp:                fr_->listedForcesGpu->updateHaveInteractions(top_->idef);
src/gromacs/mdrun/legacymdrunoptions.cpp:    // which compatible GPUs are availble for use, or to select a GPU
src/gromacs/mdrun/legacymdrunoptions.cpp:        hw_opt.userGpuTaskAssignment = userGpuTaskAssignment;
src/gromacs/mdrun/legacymdrunoptions.cpp:        const char* env = getenv("GMX_GPU_ID");
src/gromacs/mdrun/legacymdrunoptions.cpp:                gmx_fatal(FARGS, "GMX_GPU_ID and -gpu_id can not be used at the same time");
src/gromacs/mdrun/legacymdrunoptions.cpp:        env = getenv("GMX_GPUTASKS");
src/gromacs/mdrun/legacymdrunoptions.cpp:            if (!hw_opt.userGpuTaskAssignment.empty())
src/gromacs/mdrun/legacymdrunoptions.cpp:                gmx_fatal(FARGS, "GMX_GPUTASKS and -gputasks can not be used at the same time");
src/gromacs/mdrun/legacymdrunoptions.cpp:            hw_opt.userGpuTaskAssignment = env;
src/gromacs/mdrun/legacymdrunoptions.cpp:        if (!hw_opt.devicesSelectedByUser.empty() && !hw_opt.userGpuTaskAssignment.empty())
src/gromacs/mdrun/legacymdrunoptions.cpp:            gmx_fatal(FARGS, "-gpu_id and -gputasks cannot be used at the same time");
src/gromacs/mdrun/legacymdrunoptions.h:    const char* nbpu_opt_choices[5]    = { nullptr, "auto", "cpu", "gpu", nullptr };
src/gromacs/mdrun/legacymdrunoptions.h:    const char* pme_opt_choices[5]     = { nullptr, "auto", "cpu", "gpu", nullptr };
src/gromacs/mdrun/legacymdrunoptions.h:    const char* pme_fft_opt_choices[5] = { nullptr, "auto", "cpu", "gpu", nullptr };
src/gromacs/mdrun/legacymdrunoptions.h:    const char* bonded_opt_choices[5]  = { nullptr, "auto", "cpu", "gpu", nullptr };
src/gromacs/mdrun/legacymdrunoptions.h:    const char* update_opt_choices[5]  = { nullptr, "auto", "cpu", "gpu", nullptr };
src/gromacs/mdrun/legacymdrunoptions.h:    const char* userGpuTaskAssignment  = "";
src/gromacs/mdrun/legacymdrunoptions.h:        { "-gpu_id",
src/gromacs/mdrun/legacymdrunoptions.h:          "List of unique GPU device IDs available to use" },
src/gromacs/mdrun/legacymdrunoptions.h:        { "-gputasks",
src/gromacs/mdrun/legacymdrunoptions.h:          { &userGpuTaskAssignment },
src/gromacs/mdrun/legacymdrunoptions.h:          "List of GPU device IDs, mapping each task on a node to a device. "
src/gromacs/mdrun/legacymdrunoptions.h:          "Optimize PME load between PP/PME ranks or GPU/CPU" },
src/gromacs/mdrun/shellfc.cpp:#include "gromacs/gpu_utils/hostallocator.h"
src/gromacs/mdrun/shellfc.cpp:                                  bool              usingPmeOnGpu)
src/gromacs/mdrun/shellfc.cpp:    if (usingPmeOnGpu)
src/gromacs/mdrun/rerun.cpp:#include "gromacs/gpu_utils/gpu_utils.h"
src/gromacs/mdrun/rerun.cpp:#include "gromacs/listed_forces/listed_forces_gpu.h"
src/gromacs/mdrun/rerun.cpp:                                 runScheduleWork_->simulationWork.useGpuPme);
src/gromacs/mdrun/rerun.cpp:            if (fr_->listedForcesGpu)
src/gromacs/mdrun/rerun.cpp:                fr_->listedForcesGpu->updateHaveInteractions(top_->idef);
src/gromacs/mdrun/runner.h:    /*! \brief Target short-range interactions for "cpu", "gpu", or "auto". Default is "auto".
src/gromacs/mdrun/runner.h:    /*! \brief Target long-range interactions for "cpu", "gpu", or "auto". Default is "auto".
src/gromacs/mdrun/runner.h:    /*! \brief Target long-range interactions FFT/solve stages for "cpu", "gpu", or "auto". Default is "auto".
src/gromacs/mdrun/runner.h:    /*! \brief Target bonded interactions for "cpu", "gpu", or "auto". Default is "auto".
src/gromacs/mdrun/runner.h:    /*! \brief Target update calculation for "cpu", "gpu", or "auto". Default is "auto".
src/gromacs/mdrun/runner.h:     * \param nbpu_opt Target short-range interactions for "cpu", "gpu", or "auto".
src/gromacs/mdrun/runner.h:     * \param pme_opt Target long-range interactions for "cpu", "gpu", or "auto".
src/gromacs/mdrun/runner.h:     * \param pme_fft_opt Target long-range interactions FFT/solve stages for "cpu", "gpu", or "auto".
src/gromacs/mdrun/runner.h:     * \param bonded_opt Target bonded interactions for "cpu", "gpu", or "auto".
src/gromacs/mdrun/runner.h:     * \param[in] update_opt Target update calculation for "cpu", "gpu", or "auto".
src/gromacs/mdrun/minimize.cpp:#include "gromacs/listed_forces/listed_forces_gpu.h"
src/gromacs/mdrun/minimize.cpp:        if (fr->listedForcesGpu)
src/gromacs/mdrun/minimize.cpp:            fr->listedForcesGpu->updateHaveInteractions(top->idef);
src/gromacs/mdrun/shellfc.h: * \param usingPmeOnGpu Set to true if GPU will be used for PME calculations. Necessary for proper buffer initialization.
src/gromacs/mdrun/shellfc.h:                                  bool              usingPmeOnGpu);
src/gromacs/mdrun/md.cpp:#include "gromacs/domdec/gpuhaloexchange.h"
src/gromacs/mdrun/md.cpp:#include "gromacs/gpu_utils/device_stream_manager.h"
src/gromacs/mdrun/md.cpp:#include "gromacs/gpu_utils/gpu_utils.h"
src/gromacs/mdrun/md.cpp:#include "gromacs/gpu_utils/hostallocator.h"
src/gromacs/mdrun/md.cpp:#include "gromacs/listed_forces/listed_forces_gpu.h"
src/gromacs/mdrun/md.cpp:#include "gromacs/mdlib/mdgraph_gpu.h"
src/gromacs/mdrun/md.cpp:#include "gromacs/mdlib/update_constrain_gpu.h"
src/gromacs/mdrun/md.cpp:#include "gromacs/mdtypes/state_propagator_data_gpu.h"
src/gromacs/mdrun/md.cpp:#include "gromacs/nbnxm/gpu_data_mgmt.h"
src/gromacs/mdrun/md.cpp:    /* PME load balancing data for GPU kernels */
src/gromacs/mdrun/md.cpp:    const bool  useGpuForPme       = simulationWork.useGpuPme;
src/gromacs/mdrun/md.cpp:    const bool  useGpuForNonbonded = simulationWork.useGpuNonbonded;
src/gromacs/mdrun/md.cpp:    const bool  useGpuForUpdate    = simulationWork.useGpuUpdate;
src/gromacs/mdrun/md.cpp:                                                useGpuForPme);
src/gromacs/mdrun/md.cpp:                   (simulationWork.useGpuFBufferOpsWhenAllowed || useGpuForUpdate)
src/gromacs/mdrun/md.cpp:    std::unique_ptr<UpdateConstrainGpu> integrator;
src/gromacs/mdrun/md.cpp:    StatePropagatorDataGpu* stateGpu = fr_->stateGpu;
src/gromacs/mdrun/md.cpp:    if (useGpuForUpdate)
src/gromacs/mdrun/md.cpp:                           "groups if using GPU update.\n");
src/gromacs/mdrun/md.cpp:                           "SHAKE is not supported with GPU update.");
src/gromacs/mdrun/md.cpp:        GMX_RELEASE_ASSERT(useGpuForPme || (useGpuForNonbonded && simulationWork.useGpuXBufferOpsWhenAllowed),
src/gromacs/mdrun/md.cpp:                           "the GPU to use GPU update.\n");
src/gromacs/mdrun/md.cpp:                           "Only the md integrator is supported with the GPU update.\n");
src/gromacs/mdrun/md.cpp:                "Nose-Hoover temperature coupling is not supported with the GPU update.\n");
src/gromacs/mdrun/md.cpp:                "with the GPU update.\n");
src/gromacs/mdrun/md.cpp:                           "Virtual sites are not supported with the GPU update.\n");
src/gromacs/mdrun/md.cpp:                           "Essential dynamics is not supported with the GPU update.\n");
src/gromacs/mdrun/md.cpp:                           "Constraints pulling is not supported with the GPU update.\n");
src/gromacs/mdrun/md.cpp:                           "Orientation restraints are not supported with the GPU update.\n");
src/gromacs/mdrun/md.cpp:                "Free energy perturbation of masses and constraints are not supported with the GPU "
src/gromacs/mdrun/md.cpp:                    .appendText("Updating coordinates and applying constraints on the GPU.");
src/gromacs/mdrun/md.cpp:            GMX_LOG(mdLog_.info).asParagraph().appendText("Updating coordinates on the GPU.");
src/gromacs/mdrun/md.cpp:                           "Device stream manager should be initialized in order to use GPU "
src/gromacs/mdrun/md.cpp:                "Update stream should be initialized in order to use GPU "
src/gromacs/mdrun/md.cpp:        integrator = std::make_unique<UpdateConstrainGpu>(
src/gromacs/mdrun/md.cpp:        stateGpu->setXUpdatedOnDeviceEvent(integrator->xUpdatedOnDeviceEvent());
src/gromacs/mdrun/md.cpp:    if (useGpuForPme || simulationWork.useGpuXBufferOpsWhenAllowed || useGpuForUpdate)
src/gromacs/mdrun/md.cpp:    if (useGpuForUpdate)
src/gromacs/mdrun/md.cpp:     * Disable PME tuning with GPU PME decomposition */
src/gromacs/mdrun/md.cpp:                && ir->cutoff_scheme != CutoffScheme::Group && !simulationWork.useGpuPmeDecomposition);
src/gromacs/mdrun/md.cpp:                &pme_loadbal, cr_, mdLog_, *ir, state_->box, *fr_->ic, *fr_->nbv, fr_->pmedata, fr_->nbv->useGpu());
src/gromacs/mdrun/md.cpp:    bool usedMdGpuGraphLastStep = false;
src/gromacs/mdrun/md.cpp:            if (useGpuForUpdate && !bFirstStep)
src/gromacs/mdrun/md.cpp:                stateGpu->copyCoordinatesFromGpu(state_->x, AtomLocality::Local);
src/gromacs/mdrun/md.cpp:                stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
src/gromacs/mdrun/md.cpp:            /* PME grid + cut-off optimization with GPUs or PME nodes */
src/gromacs/mdrun/md.cpp:                           simulationWork.useGpuPmePpCommunication);
src/gromacs/mdrun/md.cpp:        // On search steps, when doing the update on the GPU, copy
src/gromacs/mdrun/md.cpp:        if (useGpuForUpdate && bNS && !bFirstStep && !bExchanged)
src/gromacs/mdrun/md.cpp:            if (usedMdGpuGraphLastStep)
src/gromacs/mdrun/md.cpp:                // Wait on coordinates produced from GPU graph
src/gromacs/mdrun/md.cpp:                stateGpu->waitCoordinatesUpdatedOnDevice();
src/gromacs/mdrun/md.cpp:            stateGpu->copyVelocitiesFromGpu(state_->v, AtomLocality::Local);
src/gromacs/mdrun/md.cpp:            stateGpu->copyCoordinatesFromGpu(state_->x, AtomLocality::Local);
src/gromacs/mdrun/md.cpp:            stateGpu->waitVelocitiesReadyOnHost(AtomLocality::Local);
src/gromacs/mdrun/md.cpp:            stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
src/gromacs/mdrun/md.cpp:            // the GPU Update object should be informed
src/gromacs/mdrun/md.cpp:            if (useGpuForUpdate && (bMainState || bExchanged))
src/gromacs/mdrun/md.cpp:        // Allocate or re-size GPU halo exchange object, if necessary
src/gromacs/mdrun/md.cpp:        if (bNS && simulationWork.havePpDomainDecomposition && simulationWork.useGpuHaloExchange)
src/gromacs/mdrun/md.cpp:                               "GPU device manager has to be initialized to use GPU "
src/gromacs/mdrun/md.cpp:            constructGpuHaloExchange(*cr_, *fr_->deviceStreamManager, wallCycleCounters_);
src/gromacs/mdrun/md.cpp:            if (fr_->listedForcesGpu)
src/gromacs/mdrun/md.cpp:                fr_->listedForcesGpu->updateHaveInteractions(top_->idef);
src/gromacs/mdrun/md.cpp:        MdGpuGraph* mdGraph = simulationWork.useMdGpuGraph ? fr_->mdGraph[step % 2].get() : nullptr;
src/gromacs/mdrun/md.cpp:        if (simulationWork.useMdGpuGraph)
src/gromacs/mdrun/md.cpp:                mdGraph->setUsedGraphLastStep(usedMdGpuGraphLastStep);
src/gromacs/mdrun/md.cpp:                bool canUseMdGpuGraphThisStep =
src/gromacs/mdrun/md.cpp:                if (mdGraph->captureThisStep(canUseMdGpuGraphThisStep))
src/gromacs/mdrun/md.cpp:                    mdGraph->startRecord(stateGpu->getCoordinatesReadyOnDeviceEvent(
src/gromacs/mdrun/md.cpp:        if (!simulationWork.useMdGpuGraph || mdGraph->graphIsCapturingThisStep()
src/gromacs/mdrun/md.cpp:            // Copy coordinate from the GPU for the output/checkpointing if the update is offloaded
src/gromacs/mdrun/md.cpp:            if (useGpuForUpdate && !bNS && !runScheduleWork_->domainWork.haveCpuLocalForceWork
src/gromacs/mdrun/md.cpp:                stateGpu->copyCoordinatesFromGpu(state_->x, AtomLocality::Local);
src/gromacs/mdrun/md.cpp:                stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
src/gromacs/mdrun/md.cpp:            if (useGpuForUpdate && !bNS
src/gromacs/mdrun/md.cpp:                stateGpu->copyVelocitiesFromGpu(state_->v, AtomLocality::Local);
src/gromacs/mdrun/md.cpp:                stateGpu->waitVelocitiesReadyOnHost(AtomLocality::Local);
src/gromacs/mdrun/md.cpp:            // Copy forces for the output if the forces were reduced on the GPU (not the case on virial steps)
src/gromacs/mdrun/md.cpp:            // and update is offloaded hence forces are kept on the GPU for update and have not been
src/gromacs/mdrun/md.cpp:            //       when the forces are ready on the GPU -- the same synchronizer should be used as the one
src/gromacs/mdrun/md.cpp:            //       prior to GPU update.
src/gromacs/mdrun/md.cpp:            if (runScheduleWork_->stepWork.useGpuFBufferOps
src/gromacs/mdrun/md.cpp:                && (simulationWork.useGpuUpdate && !virtualSites_) && do_per_step(step, ir->nstfout))
src/gromacs/mdrun/md.cpp:                stateGpu->copyForcesFromGpu(f.view().force(), AtomLocality::Local);
src/gromacs/mdrun/md.cpp:                stateGpu->waitForcesReadyOnHost(AtomLocality::Local);
src/gromacs/mdrun/md.cpp:            if (!useGpuForUpdate)
src/gromacs/mdrun/md.cpp:                GMX_ASSERT(!useGpuForUpdate, "GPU update is not supported with VVAK integrator.");
src/gromacs/mdrun/md.cpp:                if (useGpuForUpdate)
src/gromacs/mdrun/md.cpp:                        integrator->set(stateGpu->getCoordinates(),
src/gromacs/mdrun/md.cpp:                                        stateGpu->getVelocities(),
src/gromacs/mdrun/md.cpp:                                        stateGpu->getForces(),
src/gromacs/mdrun/md.cpp:                        // Copy data to the GPU after buffers might have been reinitialized
src/gromacs/mdrun/md.cpp:                        stateGpu->copyVelocitiesToGpu(state_->v, AtomLocality::Local);
src/gromacs/mdrun/md.cpp:                    // Copy x to the GPU unless we have already transferred in do_force().
src/gromacs/mdrun/md.cpp:                    // We transfer in do_force() if a GPU force task requires x (PME or x buffer ops).
src/gromacs/mdrun/md.cpp:                    if (!(runScheduleWork_->stepWork.haveGpuPmeOnThisRank
src/gromacs/mdrun/md.cpp:                          || runScheduleWork_->stepWork.useGpuXBufferOps))
src/gromacs/mdrun/md.cpp:                        stateGpu->copyCoordinatesToGpu(state_->x, AtomLocality::Local);
src/gromacs/mdrun/md.cpp:                        stateGpu->consumeCoordinatesCopiedToDeviceEvent(AtomLocality::Local);
src/gromacs/mdrun/md.cpp:                    if ((simulationWork.useGpuPme && simulationWork.useCpuPmePpCommunication)
src/gromacs/mdrun/md.cpp:                        || (!runScheduleWork_->stepWork.useGpuFBufferOps))
src/gromacs/mdrun/md.cpp:                        // rest of the forces computed on the GPU, so the final forces have to be
src/gromacs/mdrun/md.cpp:                        // copied back to the GPU. Or the buffer ops were not offloaded this step,
src/gromacs/mdrun/md.cpp:                        stateGpu->copyForcesToGpu(f.view().force(), AtomLocality::Local);
src/gromacs/mdrun/md.cpp:                            stateGpu->getLocalForcesReadyOnDeviceEvent(
src/gromacs/mdrun/md.cpp:        if (simulationWork.useMdGpuGraph)
src/gromacs/mdrun/md.cpp:            GMX_ASSERT((mdGraph != nullptr), "MD GPU graph does not exist.");
src/gromacs/mdrun/md.cpp:                // update): with PME tuning, since the GPU kernels
src/gromacs/mdrun/md.cpp:            usedMdGpuGraphLastStep = mdGraph->useGraphThisStep();
src/gromacs/mdrun/md.cpp:            if (useGpuForUpdate)
src/gromacs/mdrun/md.cpp:                    stateGpu->copyCoordinatesFromGpu(state_->x, AtomLocality::Local);
src/gromacs/mdrun/md.cpp:                    stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
src/gromacs/mdrun/md.cpp:                    stateGpu->copyVelocitiesFromGpu(state_->v, AtomLocality::Local);
src/gromacs/mdrun/md.cpp:                    stateGpu->waitVelocitiesReadyOnHost(AtomLocality::Local);
src/gromacs/mdrun/md.cpp:                    if (useGpuForUpdate)
src/gromacs/mdrun/md.cpp:                        stateGpu->resetCoordinatesCopiedToDeviceEvent(AtomLocality::Local);
src/gromacs/mdrun/md.cpp:                        stateGpu->copyCoordinatesToGpu(state_->x, AtomLocality::Local);
src/gromacs/mdrun/md.cpp:                        stateGpu->waitCoordinatesCopiedToDevice(AtomLocality::Local);
src/gromacs/mdrun/md.cpp:                            stateGpu->copyVelocitiesToGpu(state_->v, AtomLocality::Local);
src/gromacs/mdrun/md.cpp:        const bool scaleCoordinates = !useGpuForUpdate || bDoReplEx;
src/gromacs/mdrun/md.cpp:        if (useGpuForUpdate
src/gromacs/mdrun/md.cpp:        // any run that uses GPUs must be at least offloading nonbondeds
src/gromacs/mdrun/md.cpp:        const bool usingGpu = simulationWork.useGpuNonbonded;
src/gromacs/mdrun/md.cpp:        if (usingGpu)
src/gromacs/mdrun/md.cpp:            // ensure that GPU errors do not propagate between MD steps
src/gromacs/mdrun/md.cpp:    // This is to free PP ranks gpuhaloexchange symmetric buffer `d_recvBuf_`
src/gromacs/mdrun/md.cpp:        destroyGpuHaloExchangeNvshmemBuf(*cr_);
src/gromacs/mdrun/md.cpp:        pme_loadbal_done(pme_loadbal, fpLog_, mdLog_, fr_->nbv->useGpu());
src/gromacs/tools/tune_pme.cpp:#include "gromacs/taskassignment/usergpuids.h"
src/gromacs/tools/tune_pme.cpp:    eParselogMismatchOfNumberOfPPRanksAndAvailableGPUs,
src/gromacs/tools/tune_pme.cpp:    eParselogGpuProblem,
src/gromacs/tools/tune_pme.cpp:                    return eParselogMismatchOfNumberOfPPRanksAndAvailableGPUs;
src/gromacs/tools/tune_pme.cpp:                else if (str_starts(line, "Some of the requested GPUs do not exist"))
src/gromacs/tools/tune_pme.cpp:                    return eParselogGpuProblem;
src/gromacs/tools/tune_pme.cpp: * GPU support to be available if applicable. */
src/gromacs/tools/tune_pme.cpp:   compatible GPUs from mdrun, if the user of gmx tune-pme has not
src/gromacs/tools/tune_pme.cpp:                              gmx_bool    bNeedGpuSupport)
src/gromacs/tools/tune_pme.cpp:    const char match_nogpu[]   = "GPU support:         disabled";
src/gromacs/tools/tune_pme.cpp:    gmx_bool   bHaveGpuSupport = TRUE;
src/gromacs/tools/tune_pme.cpp:            if (str_starts(line, match_nogpu))
src/gromacs/tools/tune_pme.cpp:                bHaveGpuSupport = FALSE;
src/gromacs/tools/tune_pme.cpp:    if (bNeedGpuSupport && !bHaveGpuSupport)
src/gromacs/tools/tune_pme.cpp:        gmx_fatal(FARGS, "The mdrun executable did not have the expected GPU support.");
src/gromacs/tools/tune_pme.cpp:/* Handles the no-GPU case by emitting an empty string. */
src/gromacs/tools/tune_pme.cpp:static std::string make_gpu_id_command_line(const char* eligible_gpu_ids)
src/gromacs/tools/tune_pme.cpp:    /* If the user has given no eligible GPU IDs, or we're trying the
src/gromacs/tools/tune_pme.cpp:     * to mdrun -gpu_id */
src/gromacs/tools/tune_pme.cpp:    if (eligible_gpu_ids != nullptr)
src/gromacs/tools/tune_pme.cpp:        return gmx::formatString("-gpu_id %s", eligible_gpu_ids);
src/gromacs/tools/tune_pme.cpp:                              const char* eligible_gpu_ids) /* Available GPU IDs for
src/gromacs/tools/tune_pme.cpp:    auto cmd_gpu_ids = make_gpu_id_command_line(eligible_gpu_ids);
src/gromacs/tools/tune_pme.cpp:                cmd_gpu_ids.c_str());
src/gromacs/tools/tune_pme.cpp:                cmd_gpu_ids.c_str());
src/gromacs/tools/tune_pme.cpp:                         const char*     eligible_gpu_ids) /* GPU IDs for
src/gromacs/tools/tune_pme.cpp:                               "The number of PP ranks did not suit the number of GPUs.",
src/gromacs/tools/tune_pme.cpp:                               "Some GPUs were not detected or are incompatible.",
src/gromacs/tools/tune_pme.cpp:            auto cmd_gpu_ids = make_gpu_id_command_line(eligible_gpu_ids);
src/gromacs/tools/tune_pme.cpp:                        cmd_gpu_ids.c_str());
src/gromacs/tools/tune_pme.cpp:        "Basic support for GPU-enabled [TT]mdrun[tt] exists. Give a string containing the IDs",
src/gromacs/tools/tune_pme.cpp:        "of the GPUs that you wish to use in the optimization in the [TT]-gpu_id[tt]",
src/gromacs/tools/tune_pme.cpp:        "command-line argument. This works exactly like [TT]mdrun -gpu_id[tt], does not imply a ",
src/gromacs/tools/tune_pme.cpp:        "and merely declares the eligible set of GPU devices. [TT]gmx-tune_pme[tt] will construct ",
src/gromacs/tools/tune_pme.cpp:        "[TT]-gputasks[tt].[PAR]",
src/gromacs/tools/tune_pme.cpp:    /* IDs of GPUs that are eligible for computation */
src/gromacs/tools/tune_pme.cpp:    char* eligible_gpu_ids = nullptr;
src/gromacs/tools/tune_pme.cpp:        { "-gpu_id",
src/gromacs/tools/tune_pme.cpp:          { &eligible_gpu_ids },
src/gromacs/tools/tune_pme.cpp:          "List of unique GPU device IDs that are eligible for use" },
src/gromacs/tools/tune_pme.cpp:        check_mdrun_works(bThreads, cmd_mpirun, cmd_np, cmd_mdrun, nullptr != eligible_gpu_ids);
src/gromacs/tools/tune_pme.cpp:                     eligible_gpu_ids);
src/gromacs/tools/tune_pme.cpp:                bLaunch, fp, bThreads, cmd_mpirun, cmd_np, cmd_mdrun, cmd_args_launch, simulation_tpr, best_npme, eligible_gpu_ids);
src/gromacs/fft/fft5d.cpp:#include "gromacs/gpu_utils/gpu_utils.h"
src/gromacs/fft/fft5d.cpp:#include "gromacs/gpu_utils/hostallocator.h"
src/gromacs/fft/fft5d.cpp:static constexpr bool allocatePmeGpuMixedMode = (GMX_GPU && !GMX_GPU_OPENCL);
src/gromacs/fft/fft5d.cpp:        // only needed for PME GPU mixed mode
src/gromacs/fft/fft5d.cpp:        if (allocatePmeGpuMixedMode && realGridAllocationPinningPolicy == gmx::PinningPolicy::PinnedIfSupported)
src/gromacs/fft/fft5d.cpp:        // only needed for PME GPU mixed mode
src/gromacs/fft/fft5d.cpp:        if (allocatePmeGpuMixedMode && plan->pinningPolicy == gmx::PinningPolicy::PinnedIfSupported)
src/gromacs/fft/fft5d.cpp:            GMX_ASSERT(GMX_GPU_SYCL || isHostMemoryPinned(plan->lin),
src/gromacs/fft/fft5d.h:#include "gromacs/gpu_utils/hostallocator.h"
src/gromacs/fft/gpu_3dfft_cufftmp.h: *  \brief Declares the GPU 3D FFT routines.
src/gromacs/fft/gpu_3dfft_cufftmp.h: *  \author Gaurav Garg <gaugarg@nvidia.com>
src/gromacs/fft/gpu_3dfft_cufftmp.h:#ifndef GMX_FFT_GPU_3DFFT_CUFFTMP_H
src/gromacs/fft/gpu_3dfft_cufftmp.h:#define GMX_FFT_GPU_3DFFT_CUFFTMP_H
src/gromacs/fft/gpu_3dfft_cufftmp.h:#include "gromacs/gpu_utils/devicebuffer_datatype.h"
src/gromacs/fft/gpu_3dfft_cufftmp.h:#include "gromacs/gpu_utils/gputraits.h"
src/gromacs/fft/gpu_3dfft_cufftmp.h:#include "gromacs/gpu_utils/hostallocator.h"
src/gromacs/fft/gpu_3dfft_cufftmp.h:#include "gpu_3dfft_impl.h"
src/gromacs/fft/gpu_3dfft_cufftmp.h:class Gpu3dFft::ImplCuFftMp : public Gpu3dFft::Impl
src/gromacs/fft/gpu_3dfft_cufftmp.h:    //! \copydoc Gpu3dFft::Impl::Impl
src/gromacs/fft/gpu_3dfft_cufftmp.h:    cudaLibXtDesc* desc_;
src/gromacs/fft/rocfft_common_utils.cpp: *  \brief Implements GPU 3D FFT routines for hipSYCL via rocFFT.
src/gromacs/fft/rocfft_common_utils.cpp:#include "gromacs/gpu_utils/device_stream.h"
src/gromacs/fft/rocfft_common_utils.cpp:#include "gromacs/gpu_utils/devicebuffer.h"
src/gromacs/fft/rocfft_common_utils.cpp:#if (!(GMX_GPU_SYCL && GMX_GPU_FFT_ROCFFT)) && (!(GMX_GPU_HIP && GMX_GPU_FFT_ROCFFT))
src/gromacs/fft/gpu_3dfft_sycl_mkl.h: *  \brief Declares the GPU 3D FFT routines.
src/gromacs/fft/gpu_3dfft_sycl_mkl.h: *  \author Gaurav Garg <gaugarg@nvidia.com>
src/gromacs/fft/gpu_3dfft_sycl_mkl.h:#ifndef GMX_FFT_GPU_3DFFT_SYCL_MKL_H
src/gromacs/fft/gpu_3dfft_sycl_mkl.h:#define GMX_FFT_GPU_3DFFT_SYCL_MKL_H
src/gromacs/fft/gpu_3dfft_sycl_mkl.h:#if GMX_GPU_FFT_MKL
src/gromacs/fft/gpu_3dfft_sycl_mkl.h:#elif GMX_GPU_FFT_ONEMKL
src/gromacs/fft/gpu_3dfft_sycl_mkl.h:#    error Expected GMX_GPU_FFT_ONEMKL or GMX_GPU_FFT_MKL
src/gromacs/fft/gpu_3dfft_sycl_mkl.h:#include "gromacs/gpu_utils/devicebuffer_datatype.h"
src/gromacs/fft/gpu_3dfft_sycl_mkl.h:#include "gromacs/gpu_utils/gmxsycl.h"
src/gromacs/fft/gpu_3dfft_sycl_mkl.h:#include "gromacs/gpu_utils/gputraits.h"
src/gromacs/fft/gpu_3dfft_sycl_mkl.h:#include "gpu_3dfft_impl.h"
src/gromacs/fft/gpu_3dfft_sycl_mkl.h:class Gpu3dFft::ImplSyclMkl : public Gpu3dFft::Impl
src/gromacs/fft/gpu_3dfft_sycl_mkl.h:    //! \copydoc Gpu3dFft::Impl::Impl
src/gromacs/fft/gpu_3dfft_sycl_mkl.h:    //! \copydoc Gpu3dFft::Impl::~Impl
src/gromacs/fft/gpu_3dfft_sycl_mkl.h:    //! \copydoc Gpu3dFft::Impl::perform3dFft
src/gromacs/fft/gpu_3dfft_impl.cpp: *  \brief Implements stub GPU 3D FFT routines for CPU-only builds
src/gromacs/fft/gpu_3dfft_impl.cpp: *  \author Gaurav Garg <gaugarg@nvidia.com>
src/gromacs/fft/gpu_3dfft_impl.cpp:#include "gpu_3dfft_impl.h"
src/gromacs/fft/gpu_3dfft_impl.cpp:#include "gromacs/gpu_utils/devicebuffer.h"
src/gromacs/fft/gpu_3dfft_impl.cpp:Gpu3dFft::Impl::Impl() : complexGrid_(nullptr) {}
src/gromacs/fft/gpu_3dfft_impl.cpp:Gpu3dFft::Impl::Impl(bool performOutOfPlaceFFT) :
src/gromacs/fft/gpu_3dfft_impl.cpp:void Gpu3dFft::Impl::allocateComplexGrid(const ivec           complexGridSizePadded,
src/gromacs/fft/gpu_3dfft_impl.cpp:void Gpu3dFft::Impl::deallocateComplexGrid()
src/gromacs/fft/gpu_3dfft_impl.cpp:Gpu3dFft::Impl::~Impl() = default;
src/gromacs/fft/gpu_3dfft_hip_vkfft.h: *  \brief Declares the HIP GPU 3D FFT routines for VkFFT.
src/gromacs/fft/gpu_3dfft_hip_vkfft.h: *  \author Gaurav Garg <gaugarg@nvidia.com>
src/gromacs/fft/gpu_3dfft_hip_vkfft.h:#ifndef GMX_FFT_GPU_3DFFT_HIP_VKFFT_H
src/gromacs/fft/gpu_3dfft_hip_vkfft.h:#define GMX_FFT_GPU_3DFFT_HIP_VKFFT_H
src/gromacs/fft/gpu_3dfft_hip_vkfft.h:#include "gromacs/gpu_utils/devicebuffer_datatype.h"
src/gromacs/fft/gpu_3dfft_hip_vkfft.h:#include "gromacs/gpu_utils/gputraits.h"
src/gromacs/fft/gpu_3dfft_hip_vkfft.h:#include "gpu_3dfft_impl.h"
src/gromacs/fft/gpu_3dfft_hip_vkfft.h:class Gpu3dFft::ImplHipVkFft : public Gpu3dFft::Impl
src/gromacs/fft/gpu_3dfft_hip_vkfft.h:    //! \copydoc Gpu3dFft::Impl::Impl
src/gromacs/fft/gpu_3dfft_hip_vkfft.h:    //! \copydoc Gpu3dFft::Impl::~Impl
src/gromacs/fft/gpu_3dfft_hip_vkfft.h:    //! \copydoc Gpu3dFft::Impl::perform3dFft
src/gromacs/fft/gpu_3dfft_ocl_vkfft.h: *  \brief Declares the GPU 3D FFT routines.
src/gromacs/fft/gpu_3dfft_ocl_vkfft.h: * This backend for OpenCL provides greater performance than clFFT, but
src/gromacs/fft/gpu_3dfft_ocl_vkfft.h: * GPU-accelerated PME on macOS. OpenCL is deprecated, so we may stop
src/gromacs/fft/gpu_3dfft_ocl_vkfft.h: * maintaining this backend once hipSYCL runs on Apple GPUs.
src/gromacs/fft/gpu_3dfft_ocl_vkfft.h:#ifndef GMX_FFT_GPU_3DFFT_OCL_VKFFT_H
src/gromacs/fft/gpu_3dfft_ocl_vkfft.h:#define GMX_FFT_GPU_3DFFT_OCL_VKFFT_H
src/gromacs/fft/gpu_3dfft_ocl_vkfft.h:#include "gromacs/gpu_utils/devicebuffer_datatype.h"
src/gromacs/fft/gpu_3dfft_ocl_vkfft.h:#include "gromacs/gpu_utils/gputraits.h"
src/gromacs/fft/gpu_3dfft_ocl_vkfft.h:#include "gpu_3dfft_impl.h"
src/gromacs/fft/gpu_3dfft_ocl_vkfft.h:class Gpu3dFft::ImplOclVkfft : public Gpu3dFft::Impl
src/gromacs/fft/gpu_3dfft_ocl_vkfft.h:    //! \copydoc Gpu3dFft::Impl::Impl
src/gromacs/fft/gpu_3dfft_ocl_vkfft.h:    //! \copydoc Gpu3dFft::Impl::~Impl
src/gromacs/fft/gpu_3dfft_ocl_vkfft.h:    //! \copydoc Gpu3dFft::Impl::perform3dFft
src/gromacs/fft/gpu_3dfft_ocl_vkfft.h:#endif // GMX_FFT_GPU_3DFFT_OCL_VKFFT_H
src/gromacs/fft/gpu_3dfft_ocl_vkfft.cpp: * OpenCL VkFFT support to GROMACS is based heavily on the SYCL VkFFT backend.
src/gromacs/fft/gpu_3dfft_ocl_vkfft.cpp: *  \brief Implements GPU 3D FFT routines for OpenCL.
src/gromacs/fft/gpu_3dfft_ocl_vkfft.cpp:#include "gpu_3dfft_ocl_vkfft.h"
src/gromacs/fft/gpu_3dfft_ocl_vkfft.cpp:#include "gromacs/gpu_utils/device_context.h"
src/gromacs/fft/gpu_3dfft_ocl_vkfft.cpp:#include "gromacs/gpu_utils/device_stream.h"
src/gromacs/fft/gpu_3dfft_ocl_vkfft.cpp:#include "gromacs/gpu_utils/devicebuffer.h"
src/gromacs/fft/gpu_3dfft_ocl_vkfft.cpp:#include "gromacs/gpu_utils/gmxopencl.h"
src/gromacs/fft/gpu_3dfft_ocl_vkfft.cpp:class Gpu3dFft::ImplOclVkfft::Impl
src/gromacs/fft/gpu_3dfft_ocl_vkfft.cpp:    //! \copydoc Gpu3dFft::Impl::Impl
src/gromacs/fft/gpu_3dfft_ocl_vkfft.cpp:Gpu3dFft::ImplOclVkfft::Impl::Impl(bool allocateGrids,
src/gromacs/fft/gpu_3dfft_ocl_vkfft.cpp:                       "FFT decomposition not implemented with the OpenCL VkFFT backend");
src/gromacs/fft/gpu_3dfft_ocl_vkfft.cpp:    // Fetch OpenCL device from the command queue; this backend assumes only one GPU is present.
src/gromacs/fft/gpu_3dfft_ocl_vkfft.cpp:    GMX_RELEASE_ASSERT(status == CL_SUCCESS, "Could not fetch OpenCL device for VkFFT");
src/gromacs/fft/gpu_3dfft_ocl_vkfft.cpp:Gpu3dFft::ImplOclVkfft::Impl::~Impl()
src/gromacs/fft/gpu_3dfft_ocl_vkfft.cpp:Gpu3dFft::ImplOclVkfft::ImplOclVkfft(bool                 allocateGrids,
src/gromacs/fft/gpu_3dfft_ocl_vkfft.cpp:    Gpu3dFft::Impl::Impl(performOutOfPlaceFFT),
src/gromacs/fft/gpu_3dfft_ocl_vkfft.cpp:Gpu3dFft::ImplOclVkfft::~ImplOclVkfft()
src/gromacs/fft/gpu_3dfft_ocl_vkfft.cpp:void Gpu3dFft::ImplOclVkfft::perform3dFft(gmx_fft_direction dir, CommandEvent* /*timingEvent*/)
src/gromacs/fft/gpu_3dfft_ocl_vkfft.cpp:    // In the OpenCL backend, `DeviceBuffer` is a wrapper for `cl_mem`.
src/gromacs/fft/gpu_3dfft_sycl_bbfft.cpp: *  \brief Implements GPU 3D FFT routines for SYCL.
src/gromacs/fft/gpu_3dfft_sycl_bbfft.cpp:#include "gpu_3dfft_sycl_bbfft.h"
src/gromacs/fft/gpu_3dfft_sycl_bbfft.cpp:#include "gromacs/gpu_utils/device_stream.h"
src/gromacs/fft/gpu_3dfft_sycl_bbfft.cpp:#include "gromacs/gpu_utils/devicebuffer_sycl.h"
src/gromacs/fft/gpu_3dfft_sycl_bbfft.cpp:Gpu3dFft::ImplSyclBbfft::ImplSyclBbfft(bool allocateRealGrid,
src/gromacs/fft/gpu_3dfft_sycl_bbfft.cpp:    Gpu3dFft::Impl::Impl(performOutOfPlaceFFT), realGrid_(*realGrid->buffer_), queue_(pmeStream.stream())
src/gromacs/fft/gpu_3dfft_sycl_bbfft.cpp:Gpu3dFft::ImplSyclBbfft::~ImplSyclBbfft()
src/gromacs/fft/gpu_3dfft_sycl_bbfft.cpp:void Gpu3dFft::ImplSyclBbfft::perform3dFft(gmx_fft_direction dir, CommandEvent* /*timingEvent*/)
src/gromacs/fft/gpu_3dfft_sycl_bbfft.cpp:            GMX_THROW(NotImplementedError("The chosen 3D-FFT case is not implemented on GPUs"));
src/gromacs/fft/gpu_3dfft_heffte.h: *  \brief Declares the GPU 3D FFT routines.
src/gromacs/fft/gpu_3dfft_heffte.h: *  \author Gaurav Garg <gaugarg@nvidia.com>
src/gromacs/fft/gpu_3dfft_heffte.h:#ifndef GMX_FFT_GPU_3DFFT_HEFFTE_H
src/gromacs/fft/gpu_3dfft_heffte.h:#define GMX_FFT_GPU_3DFFT_HEFFTE_H
src/gromacs/fft/gpu_3dfft_heffte.h:#include "gromacs/gpu_utils/device_stream.h"
src/gromacs/fft/gpu_3dfft_heffte.h:#include "gromacs/gpu_utils/devicebuffer_datatype.h"
src/gromacs/fft/gpu_3dfft_heffte.h:#include "gromacs/gpu_utils/gputraits.h"
src/gromacs/fft/gpu_3dfft_heffte.h:#include "gromacs/gpu_utils/hostallocator.h"
src/gromacs/fft/gpu_3dfft_heffte.h:#include "gpu_3dfft_impl.h"
src/gromacs/fft/gpu_3dfft_heffte.h:class Gpu3dFft::ImplHeFfte : public Gpu3dFft::Impl
src/gromacs/fft/gpu_3dfft_heffte.h:    //! \copydoc Gpu3dFft::Impl::Impl
src/gromacs/fft/gpu_3dfft_heffte.h:    heffte::gpu::vector<std::complex<float>> workspace_;
src/gromacs/fft/gpu_3dfft_heffte.h:#if GMX_GPU_CUDA
src/gromacs/fft/gpu_3dfft_heffte.h:    cudaStream_t pmeRawStream_;
src/gromacs/fft/gpu_3dfft_heffte.h:    heffte::gpu::vector<float> localRealGrid_;
src/gromacs/fft/gpu_3dfft_heffte.h:    heffte::gpu::vector<std::complex<float>> localComplexGrid_;
src/gromacs/fft/gpu_3dfft_heffte.h:#elif GMX_GPU_SYCL
src/gromacs/fft/gpu_3dfft_sycl_rocfft.cpp: *  \brief Implements GPU 3D FFT routines for hipSYCL via rocFFT.
src/gromacs/fft/gpu_3dfft_sycl_rocfft.cpp:#include "gpu_3dfft_sycl_rocfft.h"
src/gromacs/fft/gpu_3dfft_sycl_rocfft.cpp:#include "gromacs/gpu_utils/device_stream.h"
src/gromacs/fft/gpu_3dfft_sycl_rocfft.cpp:#include "gromacs/gpu_utils/devicebuffer.h"
src/gromacs/fft/gpu_3dfft_sycl_rocfft.cpp:#if !defined HIPSYCL_PLATFORM_ROCM
src/gromacs/fft/gpu_3dfft_sycl_rocfft.cpp:#    error Only ROCM platform is supported for 3D FFT with hipSYCL
src/gromacs/fft/gpu_3dfft_sycl_rocfft.cpp:    // The plan creation depends on the identity of the GPU device, so
src/gromacs/fft/gpu_3dfft_sycl_rocfft.cpp:class Gpu3dFft::ImplSyclRocfft::Impl
src/gromacs/fft/gpu_3dfft_sycl_rocfft.cpp:    //! \copydoc Gpu3dFft::Impl::Impl
src/gromacs/fft/gpu_3dfft_sycl_rocfft.cpp:Gpu3dFft::ImplSyclRocfft::Impl::Impl(bool allocateRealGrid,
src/gromacs/fft/gpu_3dfft_sycl_rocfft.cpp:void Gpu3dFft::ImplSyclRocfft::perform3dFft(gmx_fft_direction dir, CommandEvent* /*timingEvent*/)
src/gromacs/fft/gpu_3dfft_sycl_rocfft.cpp:Gpu3dFft::ImplSyclRocfft::ImplSyclRocfft(bool                 allocateRealGrid,
src/gromacs/fft/gpu_3dfft_sycl_rocfft.cpp:    Gpu3dFft::Impl::Impl(performOutOfPlaceFFT),
src/gromacs/fft/gpu_3dfft_sycl_rocfft.cpp:Gpu3dFft::ImplSyclRocfft::~ImplSyclRocfft()
src/gromacs/fft/gpu_3dfft_cufft.cu: *  \brief Implements GPU 3D FFT routines for CUDA.
src/gromacs/fft/gpu_3dfft_cufft.cu:#include "gpu_3dfft_cufft.h"
src/gromacs/fft/gpu_3dfft_cufft.cu:#include "gromacs/gpu_utils/device_stream.h"
src/gromacs/fft/gpu_3dfft_cufft.cu:#include "gromacs/gpu_utils/devicebuffer.h"
src/gromacs/fft/gpu_3dfft_cufft.cu:        // Clang-CUDA does not define __CUDACC_VER_x__, but it's for developers only anyway
src/gromacs/fft/gpu_3dfft_cufft.cu:#if defined(__CUDACC_VER_MAJOR__) && (__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ < 8)
src/gromacs/fft/gpu_3dfft_cufft.cu:                "If you're using RTX 40-series card, consider updating to CUDA 11.8 or newer.\n"; // See #4759
src/gromacs/fft/gpu_3dfft_cufft.cu:Gpu3dFft::ImplCuFft::ImplCuFft(bool allocateRealGrid,
src/gromacs/fft/gpu_3dfft_cufft.cu:    Gpu3dFft::Impl::Impl(performOutOfPlaceFFT), realGrid_(reinterpret_cast<cufftReal*>(*realGrid))
src/gromacs/fft/gpu_3dfft_cufft.cu:    cudaStream_t stream = pmeStream.stream();
src/gromacs/fft/gpu_3dfft_cufft.cu:    GMX_RELEASE_ASSERT(stream, "Can not use the default CUDA stream for PME cuFFT");
src/gromacs/fft/gpu_3dfft_cufft.cu:Gpu3dFft::ImplCuFft::~ImplCuFft()
src/gromacs/fft/gpu_3dfft_cufft.cu:void Gpu3dFft::ImplCuFft::perform3dFft(gmx_fft_direction dir, CommandEvent* /*timingEvent*/)
src/gromacs/fft/gpu_3dfft_hip_vkfft.cpp: *  \brief Implements GPU 3D FFT routines for VkFFT with HIP.
src/gromacs/fft/gpu_3dfft_hip_vkfft.cpp:#include "gpu_3dfft_hip_vkfft.h"
src/gromacs/fft/gpu_3dfft_hip_vkfft.cpp:#include "gromacs/gpu_utils/device_stream.h"
src/gromacs/fft/gpu_3dfft_hip_vkfft.cpp:#include "gromacs/gpu_utils/devicebuffer.h"
src/gromacs/fft/gpu_3dfft_hip_vkfft.cpp:class Gpu3dFft::ImplHipVkFft::Impl
src/gromacs/fft/gpu_3dfft_hip_vkfft.cpp:    //! \copydoc Gpu3dFft::Impl::Impl
src/gromacs/fft/gpu_3dfft_hip_vkfft.cpp:Gpu3dFft::ImplHipVkFft::Impl::Impl(bool allocateRealGrid,
src/gromacs/fft/gpu_3dfft_hip_vkfft.cpp:Gpu3dFft::ImplHipVkFft::Impl::~Impl()
src/gromacs/fft/gpu_3dfft_hip_vkfft.cpp:Gpu3dFft::ImplHipVkFft::ImplHipVkFft(bool                 allocateGrids,
src/gromacs/fft/gpu_3dfft_hip_vkfft.cpp:    Gpu3dFft::Impl::Impl(performOutOfPlaceFFT),
src/gromacs/fft/gpu_3dfft_hip_vkfft.cpp:Gpu3dFft::ImplHipVkFft::~ImplHipVkFft()
src/gromacs/fft/gpu_3dfft_hip_vkfft.cpp:void Gpu3dFft::ImplHipVkFft::perform3dFft(gmx_fft_direction dir, CommandEvent* /*timingEvent*/)
src/gromacs/fft/parallel_3dfft.h:#include "gromacs/gpu_utils/hostallocator.h"
src/gromacs/fft/parallel_3dfft.h: *  \param realGridAllocation  Whether to make real grid use allocation pinned for GPU transfers.
src/gromacs/fft/parallel_3dfft.h: *                             Only used in PME mixed CPU+GPU mode.
src/gromacs/fft/gpu_3dfft_sycl_bbfft.h: *  \brief Declares the GPU 3D FFT routines.
src/gromacs/fft/gpu_3dfft_sycl_bbfft.h:#ifndef GMX_FFT_GPU_3DFFT_SYCL_BBFFT_H
src/gromacs/fft/gpu_3dfft_sycl_bbfft.h:#define GMX_FFT_GPU_3DFFT_SYCL_BBFFT_H
src/gromacs/fft/gpu_3dfft_sycl_bbfft.h:#include "gromacs/gpu_utils/devicebuffer_datatype.h"
src/gromacs/fft/gpu_3dfft_sycl_bbfft.h:#include "gromacs/gpu_utils/gmxsycl.h"
src/gromacs/fft/gpu_3dfft_sycl_bbfft.h:#include "gromacs/gpu_utils/gputraits.h"
src/gromacs/fft/gpu_3dfft_sycl_bbfft.h:#include "gpu_3dfft_impl.h"
src/gromacs/fft/gpu_3dfft_sycl_bbfft.h:class Gpu3dFft::ImplSyclBbfft : public Gpu3dFft::Impl
src/gromacs/fft/gpu_3dfft_sycl_bbfft.h:    //! \copydoc Gpu3dFft::Impl::Impl
src/gromacs/fft/gpu_3dfft_sycl_bbfft.h:    //! \copydoc Gpu3dFft::Impl::~Impl
src/gromacs/fft/gpu_3dfft_sycl_bbfft.h:    //! \copydoc Gpu3dFft::Impl::perform3dFft
src/gromacs/fft/tests/fft.cpp:#include "gromacs/fft/gpu_3dfft.h"
src/gromacs/fft/tests/fft.cpp:#include "gromacs/gpu_utils/clfftinitializer.h"
src/gromacs/fft/tests/fft.cpp:#if GMX_GPU
src/gromacs/fft/tests/fft.cpp:#    include "gromacs/gpu_utils/devicebuffer.h"
src/gromacs/fft/tests/fft.cpp:#if GMX_GPU
src/gromacs/fft/tests/fft.cpp: * For real simulations, we use in-place FFT with BBFFT (see pme_gpu_init_internal)
src/gromacs/fft/tests/fft.cpp:constexpr bool sc_performOutOfPlaceFFT = (GMX_GPU_FFT_BBFFT == 0);
src/gromacs/fft/tests/fft.cpp:#    if GMX_SYCL_DPCPP && (GMX_GPU_FFT_BBFFT)
src/gromacs/fft/tests/fft.cpp:// when run on the GPU vs CPU, because the latter caters to a 5D
src/gromacs/fft/tests/fft.cpp:// the transpose from XYZ to YXZ and back, however on the GPU that
src/gromacs/fft/tests/fft.cpp:#    if GMX_GPU_CUDA
src/gromacs/fft/tests/fft.cpp:#    elif GMX_GPU_HIP
src/gromacs/fft/tests/fft.cpp:#        if GMX_GPU_FFT_VKFFT
src/gromacs/fft/tests/fft.cpp:#    elif GMX_GPU_OPENCL
src/gromacs/fft/tests/fft.cpp:#        if GMX_GPU_FFT_VKFFT
src/gromacs/fft/tests/fft.cpp:#    elif GMX_GPU_SYCL
src/gromacs/fft/tests/fft.cpp:#        if GMX_GPU_FFT_MKL
src/gromacs/fft/tests/fft.cpp:#        elif GMX_GPU_FFT_ONEMKL
src/gromacs/fft/tests/fft.cpp:#        elif GMX_GPU_FFT_BBFFT
src/gromacs/fft/tests/fft.cpp:#        elif GMX_GPU_FFT_ROCFFT
src/gromacs/fft/tests/fft.cpp:#        elif GMX_GPU_FFT_VKFFT
src/gromacs/fft/tests/fft.cpp:        GTEST_SKIP() << "No supported GPU FFT backend detected";
src/gromacs/fft/tests/fft.cpp:        Gpu3dFft           gpu3dFft(backend,
src/gromacs/fft/tests/fft.cpp:                &realGrid, in_.data(), 0, in_.size(), deviceStream, GpuApiCallBehavior::Sync, nullptr);
src/gromacs/fft/tests/fft.cpp:        gpu3dFft.perform3dFft(GMX_FFT_REAL_TO_COMPLEX, timingEvent);
src/gromacs/fft/tests/fft.cpp:                             GpuApiCallBehavior::Sync,
src/gromacs/fft/tests/fft.cpp:                               GpuApiCallBehavior::Sync,
src/gromacs/fft/tests/fft.cpp:        gpu3dFft.perform3dFft(GMX_FFT_COMPLEX_TO_REAL, timingEvent);
src/gromacs/fft/tests/fft.cpp:                             GpuApiCallBehavior::Sync,
src/gromacs/fft/tests/fft.cpp:#endif // GMX_GPU
src/gromacs/fft/tests/CMakeLists.txt:# This test is slow with OpenCL or the OpenCL DPC++ SYCL back end so
src/gromacs/fft/tests/CMakeLists.txt:    GPU_CPP_SOURCE_FILES
src/gromacs/fft/tests/CMakeLists.txt:gmx_register_gtest_test(FFTUnitTests fft-test SLOW_TEST QUICK_GPU_TEST)
src/gromacs/fft/tests/CMakeLists.txt:        gpu_utils
src/gromacs/fft/tests/CMakeLists.txt:    GPU_CPP_SOURCE_FILES
src/gromacs/fft/tests/CMakeLists.txt:            gpu_utils
src/gromacs/fft/tests/fft_mpi.cpp: * \author Gaurav Garg <gaugarg@nvidia.com>
src/gromacs/fft/tests/fft_mpi.cpp:#include "gromacs/fft/gpu_3dfft.h"
src/gromacs/fft/tests/fft_mpi.cpp:#include "gromacs/gpu_utils/clfftinitializer.h"
src/gromacs/fft/tests/fft_mpi.cpp:#if GMX_GPU
src/gromacs/fft/tests/fft_mpi.cpp:#    include "gromacs/gpu_utils/devicebuffer.h"
src/gromacs/fft/tests/fft_mpi.cpp:using GpuFftTestParams = std::tuple<std::tuple<IVec, // size of grid
src/gromacs/fft/tests/fft_mpi.cpp:using GpuFftTestGridParams = std::tuple_element<0, GpuFftTestParams>::type;
src/gromacs/fft/tests/fft_mpi.cpp:static GpuAwareMpiStatus getGpuAwareMpiStatusForFftBackend(const FftBackend fftBackend)
src/gromacs/fft/tests/fft_mpi.cpp:     * by the FFT library, not by the devices. E.g., we can have OpenCL Intel GPUs, which are
src/gromacs/fft/tests/fft_mpi.cpp:     * not supported by GPU-aware Intel MPI (LevelZero backend is required). Or even NVIDIA or AMD
src/gromacs/fft/tests/fft_mpi.cpp:     * GPUs in the same build. We do handle some common cases, however.
src/gromacs/fft/tests/fft_mpi.cpp:        case FftBackend::HeFFTe_CUDA:
src/gromacs/fft/tests/fft_mpi.cpp:        case FftBackend::HeFFTe_Sycl_cuFFT: return checkMpiCudaAwareSupport();
src/gromacs/fft/tests/fft_mpi.cpp:        default: return GpuAwareMpiStatus::NotSupported;
src/gromacs/fft/tests/fft_mpi.cpp:class GpuFftTest3D : public ::testing::Test, public ::testing::WithParamInterface<GpuFftTestParams>
src/gromacs/fft/tests/fft_mpi.cpp:    GpuFftTest3D() = default;
src/gromacs/fft/tests/fft_mpi.cpp:    static void runTest(const GpuFftTestParams& param)
src/gromacs/fft/tests/fft_mpi.cpp:            GTEST_SKIP() << "No compatible GPUs detected";
src/gromacs/fft/tests/fft_mpi.cpp:        GpuFftTestGridParams gridParams;
src/gromacs/fft/tests/fft_mpi.cpp:        const GpuAwareMpiStatus gpuAwareMpiStatus = getGpuAwareMpiStatusForFftBackend(backend);
src/gromacs/fft/tests/fft_mpi.cpp:        if (gpuAwareMpiStatus != GpuAwareMpiStatus::Supported
src/gromacs/fft/tests/fft_mpi.cpp:            && gpuAwareMpiStatus != GpuAwareMpiStatus::Forced)
src/gromacs/fft/tests/fft_mpi.cpp:            GTEST_SKIP() << "Test requires GPU-aware MPI library";
src/gromacs/fft/tests/fft_mpi.cpp:        Gpu3dFft       gpu3dFft(backend,
src/gromacs/fft/tests/fft_mpi.cpp:                &realGrid, in.data(), 0, in.size(), deviceStream, GpuApiCallBehavior::Sync, nullptr);
src/gromacs/fft/tests/fft_mpi.cpp:        gpu3dFft.perform3dFft(GMX_FFT_REAL_TO_COMPLEX, timingEvent);
src/gromacs/fft/tests/fft_mpi.cpp:        gpu3dFft.perform3dFft(GMX_FFT_COMPLEX_TO_REAL, timingEvent);
src/gromacs/fft/tests/fft_mpi.cpp:                             GpuApiCallBehavior::Sync,
src/gromacs/fft/tests/fft_mpi.cpp:TEST_P(GpuFftTest3D, GpuFftDecomposition)
src/gromacs/fft/tests/fft_mpi.cpp:    GpuFftTestParams params = GetParam();
src/gromacs/fft/tests/fft_mpi.cpp:std::vector<GpuFftTestGridParams> const inputGrids{ { IVec{ 5, 6, 9 }, 4, 1 },
src/gromacs/fft/tests/fft_mpi.cpp:#    if GMX_GPU_CUDA
src/gromacs/fft/tests/fft_mpi.cpp:    FftBackend::HeFFTe_CUDA,
src/gromacs/fft/tests/fft_mpi.cpp:#    if GMX_GPU_SYCL && GMX_GPU_FFT_MKL
src/gromacs/fft/tests/fft_mpi.cpp:#    if GMX_GPU_SYCL && GMX_GPU_FFT_ROCFFT
src/gromacs/fft/tests/fft_mpi.cpp:#    if GMX_GPU_SYCL && GMX_GPU_FFT_CUFFT
src/gromacs/fft/tests/fft_mpi.cpp:INSTANTIATE_TEST_SUITE_P(GpuFft,
src/gromacs/fft/tests/fft_mpi.cpp:                         GpuFftTest3D,
src/gromacs/fft/gpu_3dfft_ocl.h: *  \brief Declares the GPU 3D FFT routines.
src/gromacs/fft/gpu_3dfft_ocl.h: *  \author Gaurav Garg <gaugarg@nvidia.com>
src/gromacs/fft/gpu_3dfft_ocl.h:#ifndef GMX_FFT_GPU_3DFFT_OCL_H
src/gromacs/fft/gpu_3dfft_ocl.h:#define GMX_FFT_GPU_3DFFT_OCL_H
src/gromacs/fft/gpu_3dfft_ocl.h:#include "gromacs/gpu_utils/devicebuffer_datatype.h"
src/gromacs/fft/gpu_3dfft_ocl.h:#include "gromacs/gpu_utils/gputraits.h"
src/gromacs/fft/gpu_3dfft_ocl.h:#include "gpu_3dfft_impl.h"
src/gromacs/fft/gpu_3dfft_ocl.h:class Gpu3dFft::ImplOcl : public Gpu3dFft::Impl
src/gromacs/fft/gpu_3dfft_ocl.h:    //! \copydoc Gpu3dFft::Impl::Impl
src/gromacs/fft/gpu_3dfft_ocl.h:    //! \copydoc Gpu3dFft::Impl::~Impl
src/gromacs/fft/gpu_3dfft_ocl.h:    //! \copydoc Gpu3dFft::Impl::perform3dFft
src/gromacs/fft/gpu_3dfft_sycl_vkfft.h: *  \brief Declares the GPU 3D FFT routines.
src/gromacs/fft/gpu_3dfft_sycl_vkfft.h:#ifndef GMX_FFT_GPU_3DFFT_SYCL_VKFFT_H
src/gromacs/fft/gpu_3dfft_sycl_vkfft.h:#define GMX_FFT_GPU_3DFFT_SYCL_VKFFT_H
src/gromacs/fft/gpu_3dfft_sycl_vkfft.h:#include "gromacs/gpu_utils/devicebuffer_datatype.h"
src/gromacs/fft/gpu_3dfft_sycl_vkfft.h:#include "gromacs/gpu_utils/gputraits.h"
src/gromacs/fft/gpu_3dfft_sycl_vkfft.h:#include "gpu_3dfft_impl.h"
src/gromacs/fft/gpu_3dfft_sycl_vkfft.h:class Gpu3dFft::ImplSyclVkfft : public Gpu3dFft::Impl
src/gromacs/fft/gpu_3dfft_sycl_vkfft.h:    //! \copydoc Gpu3dFft::Impl::Impl
src/gromacs/fft/gpu_3dfft_sycl_vkfft.h:    //! \copydoc Gpu3dFft::Impl::~Impl
src/gromacs/fft/gpu_3dfft_sycl_vkfft.h:    //! \copydoc Gpu3dFft::Impl::perform3dFft
src/gromacs/fft/gpu_3dfft_sycl_vkfft.h:#endif // GMX_FFT_GPU_3DFFT_SYCL_VKFFT_H
src/gromacs/fft/gpu_3dfft_hip_rocfft.h: *  \brief Declares the GPU 3D FFT routines for HIP via rocFFT.
src/gromacs/fft/gpu_3dfft_hip_rocfft.h:#ifndef GMX_FFT_GPU_3DFFT_HIP_ROCFFT_H
src/gromacs/fft/gpu_3dfft_hip_rocfft.h:#define GMX_FFT_GPU_3DFFT_HIP_ROCFFT_H
src/gromacs/fft/gpu_3dfft_hip_rocfft.h:#include "gromacs/gpu_utils/devicebuffer_datatype.h"
src/gromacs/fft/gpu_3dfft_hip_rocfft.h:#include "gromacs/gpu_utils/gputraits.h"
src/gromacs/fft/gpu_3dfft_hip_rocfft.h:#include "gpu_3dfft_impl.h"
src/gromacs/fft/gpu_3dfft_hip_rocfft.h:class Gpu3dFft::ImplHipRocfft : public Gpu3dFft::Impl
src/gromacs/fft/gpu_3dfft_hip_rocfft.h:    //! \copydoc Gpu3dFft::Impl::Impl
src/gromacs/fft/gpu_3dfft_hip_rocfft.h:    //! \copydoc Gpu3dFft::Impl::~Impl
src/gromacs/fft/gpu_3dfft_hip_rocfft.h:    //! \copydoc Gpu3dFft::Impl::perform3dFft
src/gromacs/fft/gpu_3dfft_cufftmp.cpp: *  \brief Implements GPU 3D FFT routines using HeFFTe.
src/gromacs/fft/gpu_3dfft_cufftmp.cpp: *  \author Gaurav Garg <gaugarg@nvidia.com>
src/gromacs/fft/gpu_3dfft_cufftmp.cpp:#include "gpu_3dfft_cufftmp.h"
src/gromacs/fft/gpu_3dfft_cufftmp.cpp:#include "gromacs/gpu_utils/device_stream.h"
src/gromacs/fft/gpu_3dfft_cufftmp.cpp:// first introduced in the NVIDIA HPC SDK 23.3 (cuFFT/cuFFTMp version 11.0.5).
src/gromacs/fft/gpu_3dfft_cufftmp.cpp:Gpu3dFft::ImplCuFftMp::ImplCuFftMp(bool                allocateRealGrid,
src/gromacs/fft/gpu_3dfft_cufftmp.cpp:    // Allocate GPU memory
src/gromacs/fft/gpu_3dfft_cufftmp.cpp:Gpu3dFft::ImplCuFftMp::~ImplCuFftMp()
src/gromacs/fft/gpu_3dfft_cufftmp.cpp:void Gpu3dFft::ImplCuFftMp::perform3dFft(gmx_fft_direction dir, CommandEvent* /*timingEvent*/)
src/gromacs/fft/gpu_3dfft_cufftmp.cpp:            GMX_THROW(NotImplementedError("The chosen 3D-FFT case is not implemented on GPUs"));
src/gromacs/fft/gpu_3dfft_sycl.cpp: *  \brief Implements GPU 3D FFT routines for SYCL.
src/gromacs/fft/gpu_3dfft_sycl.cpp:#include "gpu_3dfft_sycl.h"
src/gromacs/fft/gpu_3dfft_sycl.cpp:Gpu3dFft::ImplSycl::ImplSycl(bool /*allocateRealGrid*/,
src/gromacs/fft/gpu_3dfft_sycl.cpp:    GMX_THROW(NotImplementedError("Using SYCL build without GPU 3DFFT support"));
src/gromacs/fft/gpu_3dfft_sycl.cpp:Gpu3dFft::ImplSycl::~ImplSycl() = default;
src/gromacs/fft/gpu_3dfft_sycl.cpp:void Gpu3dFft::ImplSycl::perform3dFft(gmx_fft_direction /*dir*/, CommandEvent* /*timingEvent*/)
src/gromacs/fft/gpu_3dfft_sycl.cpp:    GMX_THROW(NotImplementedError("Using SYCL build without GPU 3DFFT support"));
src/gromacs/fft/CMakeLists.txt:    gmx_add_libgromacs_sources(gpu_3dfft_heffte.cpp)
src/gromacs/fft/CMakeLists.txt:    if (GMX_GPU_SYCL)
src/gromacs/fft/CMakeLists.txt:        _gmx_add_files_to_property(SYCL_SOURCES gpu_3dfft_heffte.cpp)
src/gromacs/fft/CMakeLists.txt:if (GMX_GPU_CUDA)
src/gromacs/fft/CMakeLists.txt:        gmx_add_libgromacs_sources(gpu_3dfft_cufftmp.cpp)
src/gromacs/fft/CMakeLists.txt:        # CUDA-specific sources
src/gromacs/fft/CMakeLists.txt:        gpu_3dfft_cufft.cu
src/gromacs/fft/CMakeLists.txt:        gpu_3dfft_impl.cpp
src/gromacs/fft/CMakeLists.txt:        gpu_3dfft.cpp
src/gromacs/fft/CMakeLists.txt:    _gmx_add_files_to_property(CUDA_SOURCES
src/gromacs/fft/CMakeLists.txt:        # Must add these files so they can include cuda_runtime.h
src/gromacs/fft/CMakeLists.txt:        gpu_3dfft.cpp
src/gromacs/fft/CMakeLists.txt:        gpu_3dfft_impl.cpp
src/gromacs/fft/CMakeLists.txt:        _gmx_add_files_to_property(CUDA_SOURCES
src/gromacs/fft/CMakeLists.txt:            # Must add this files as it requires path set in cuda_include_directories
src/gromacs/fft/CMakeLists.txt:            gpu_3dfft_cufftmp.cpp
src/gromacs/fft/CMakeLists.txt:elseif (GMX_GPU_HIP)
src/gromacs/fft/CMakeLists.txt:    if (GMX_GPU_FFT_VKFFT)
src/gromacs/fft/CMakeLists.txt:            gpu_3dfft_hip_vkfft.cpp
src/gromacs/fft/CMakeLists.txt:            gpu_3dfft_impl.cpp
src/gromacs/fft/CMakeLists.txt:            gpu_3dfft.cpp
src/gromacs/fft/CMakeLists.txt:        gpu_3dfft_hip_vkfft.cpp
src/gromacs/fft/CMakeLists.txt:    elseif(GMX_GPU_FFT_ROCFFT)
src/gromacs/fft/CMakeLists.txt:            gpu_3dfft_hip_rocfft.cpp
src/gromacs/fft/CMakeLists.txt:            gpu_3dfft_impl.cpp
src/gromacs/fft/CMakeLists.txt:            gpu_3dfft.cpp
src/gromacs/fft/CMakeLists.txt:        gpu_3dfft_hip_rocfft.cpp
src/gromacs/fft/CMakeLists.txt:        gpu_3dfft.cpp
src/gromacs/fft/CMakeLists.txt:        gpu_3dfft_impl.cpp
src/gromacs/fft/CMakeLists.txt:elseif (GMX_GPU_OPENCL)
src/gromacs/fft/CMakeLists.txt:    if (GMX_GPU_FFT_VKFFT)
src/gromacs/fft/CMakeLists.txt:            gpu_3dfft_ocl_vkfft.cpp
src/gromacs/fft/CMakeLists.txt:            gpu_3dfft_impl.cpp
src/gromacs/fft/CMakeLists.txt:            gpu_3dfft.cpp
src/gromacs/fft/CMakeLists.txt:    elseif(GMX_GPU_FFT_CLFFT)
src/gromacs/fft/CMakeLists.txt:            gpu_3dfft_ocl.cpp
src/gromacs/fft/CMakeLists.txt:            gpu_3dfft_impl.cpp
src/gromacs/fft/CMakeLists.txt:            gpu_3dfft.cpp
src/gromacs/fft/CMakeLists.txt:elseif (GMX_GPU_SYCL)
src/gromacs/fft/CMakeLists.txt:    set(_sycl_fft_sources gpu_3dfft.cpp gpu_3dfft_impl.cpp gpu_3dfft_sycl.cpp)
src/gromacs/fft/CMakeLists.txt:    if (GMX_GPU_FFT_MKL OR GMX_GPU_FFT_ONEMKL)
src/gromacs/fft/CMakeLists.txt:        list(APPEND _sycl_fft_sources gpu_3dfft_sycl_mkl.cpp)
src/gromacs/fft/CMakeLists.txt:    elseif(GMX_GPU_FFT_BBFFT)
src/gromacs/fft/CMakeLists.txt:        list(APPEND _sycl_fft_sources gpu_3dfft_sycl_bbfft.cpp)
src/gromacs/fft/CMakeLists.txt:    elseif(GMX_GPU_FFT_ROCFFT)
src/gromacs/fft/CMakeLists.txt:        list(APPEND _sycl_fft_sources gpu_3dfft_sycl_rocfft.cpp rocfft_common_utils.cpp)
src/gromacs/fft/CMakeLists.txt:    elseif(GMX_GPU_FFT_VKFFT)
src/gromacs/fft/CMakeLists.txt:        list(APPEND _sycl_fft_sources gpu_3dfft_sycl_vkfft.cpp)
src/gromacs/fft/CMakeLists.txt:                      gpu_utils
src/gromacs/fft/CMakeLists.txt:if (GMX_GPU_FFT_BBFFT)
src/gromacs/fft/CMakeLists.txt:if (GMX_GPU_FFT_VKFFT)
src/gromacs/fft/gpu_3dfft_sycl.h: *  \brief Declares the GPU 3D FFT routines.
src/gromacs/fft/gpu_3dfft_sycl.h: *  \author Gaurav Garg <gaugarg@nvidia.com>
src/gromacs/fft/gpu_3dfft_sycl.h:#ifndef GMX_FFT_GPU_3DFFT_SYCL_H
src/gromacs/fft/gpu_3dfft_sycl.h:#define GMX_FFT_GPU_3DFFT_SYCL_H
src/gromacs/fft/gpu_3dfft_sycl.h:#include "gromacs/gpu_utils/devicebuffer_datatype.h"
src/gromacs/fft/gpu_3dfft_sycl.h:#include "gromacs/gpu_utils/gputraits.h"
src/gromacs/fft/gpu_3dfft_sycl.h:#include "gpu_3dfft_impl.h"
src/gromacs/fft/gpu_3dfft_sycl.h:class Gpu3dFft::ImplSycl : public Gpu3dFft::Impl
src/gromacs/fft/gpu_3dfft_sycl.h:    //! \copydoc Gpu3dFft::Impl::Impl
src/gromacs/fft/gpu_3dfft_sycl.h:    //! \copydoc Gpu3dFft::Impl::~Impl
src/gromacs/fft/gpu_3dfft_sycl.h:    //! \copydoc Gpu3dFft::Impl::perform3dFft
src/gromacs/fft/gpu_3dfft_impl.h: *  \brief Declares the GPU 3D FFT routines.
src/gromacs/fft/gpu_3dfft_impl.h: *  \author Gaurav Garg <gaugarg@nvidia.com>
src/gromacs/fft/gpu_3dfft_impl.h:#ifndef GMX_FFT_GPU_3DFFT_IMPL_H
src/gromacs/fft/gpu_3dfft_impl.h:#define GMX_FFT_GPU_3DFFT_IMPL_H
src/gromacs/fft/gpu_3dfft_impl.h:#include "gromacs/fft/gpu_3dfft.h"
src/gromacs/fft/gpu_3dfft_impl.h:#include "gromacs/gpu_utils/devicebuffer_datatype.h"
src/gromacs/fft/gpu_3dfft_impl.h:#include "gromacs/gpu_utils/gputraits.h"
src/gromacs/fft/gpu_3dfft_impl.h:class Gpu3dFft::Impl
src/gromacs/fft/gpu_3dfft_impl.h:     * Constructs GPU FFT plans for performing 3D FFT on a PME grid.
src/gromacs/fft/gpu_3dfft_impl.h:     * \param[in]  context                   GPU context
src/gromacs/fft/gpu_3dfft_impl.h:     * \param[in]  pmeStream                 GPU stream for PME
src/gromacs/fft/gpu_3dfft_impl.h:    //! \copydoc Gpu3dFft::perform3dFft
src/gromacs/fft/gpu_3dfft_cufft.h: *  \brief Declares the GPU 3D FFT routines.
src/gromacs/fft/gpu_3dfft_cufft.h: *  \author Gaurav Garg <gaugarg@nvidia.com>
src/gromacs/fft/gpu_3dfft_cufft.h:#ifndef GMX_FFT_GPU_3DFFT_CUFFT_H
src/gromacs/fft/gpu_3dfft_cufft.h:#define GMX_FFT_GPU_3DFFT_CUFFT_H
src/gromacs/fft/gpu_3dfft_cufft.h:#include "gromacs/gpu_utils/devicebuffer_datatype.h"
src/gromacs/fft/gpu_3dfft_cufft.h:#include "gromacs/gpu_utils/gputraits.h"
src/gromacs/fft/gpu_3dfft_cufft.h:#include "gpu_3dfft_impl.h"
src/gromacs/fft/gpu_3dfft_cufft.h:class Gpu3dFft::ImplCuFft : public Gpu3dFft::Impl
src/gromacs/fft/gpu_3dfft_cufft.h:    //! \copydoc Gpu3dFft::Impl::Impl
src/gromacs/fft/gpu_3dfft_cufft.h:    //! \copydoc Gpu3dFft::Impl::~Impl
src/gromacs/fft/gpu_3dfft_cufft.h:    //! \copydoc Gpu3dFft::Impl::perform3dFft
src/gromacs/fft/gpu_3dfft_hip_rocfft.cpp: *  \brief Implements GPU 3D FFT routines for HIP via rocFFT.
src/gromacs/fft/gpu_3dfft_hip_rocfft.cpp:#include "gpu_3dfft_hip_rocfft.h"
src/gromacs/fft/gpu_3dfft_hip_rocfft.cpp:#include "gromacs/gpu_utils/device_stream.h"
src/gromacs/fft/gpu_3dfft_hip_rocfft.cpp:#include "gromacs/gpu_utils/devicebuffer.h"
src/gromacs/fft/gpu_3dfft_hip_rocfft.cpp:#    error This file can only be compiled with the HIPCC compiler for ROCm
src/gromacs/fft/gpu_3dfft_hip_rocfft.cpp:    // The plan creation depends on the identity of the GPU device, so
src/gromacs/fft/gpu_3dfft_hip_rocfft.cpp:class Gpu3dFft::ImplHipRocfft::Impl
src/gromacs/fft/gpu_3dfft_hip_rocfft.cpp:    //! \copydoc Gpu3dFft::Impl::Impl
src/gromacs/fft/gpu_3dfft_hip_rocfft.cpp:Gpu3dFft::ImplHipRocfft::Impl::Impl(bool allocateRealGrid,
src/gromacs/fft/gpu_3dfft_hip_rocfft.cpp:void Gpu3dFft::ImplHipRocfft::perform3dFft(gmx_fft_direction dir, CommandEvent* /*timingEvent*/)
src/gromacs/fft/gpu_3dfft_hip_rocfft.cpp:Gpu3dFft::ImplHipRocfft::ImplHipRocfft(bool                 allocateRealGrid,
src/gromacs/fft/gpu_3dfft_hip_rocfft.cpp:    Gpu3dFft::Impl::Impl(performOutOfPlaceFFT),
src/gromacs/fft/gpu_3dfft_hip_rocfft.cpp:Gpu3dFft::ImplHipRocfft::~ImplHipRocfft()
src/gromacs/fft/gpu_3dfft_heffte.cpp: *  \brief Implements GPU 3D FFT routines using HeFFTe.
src/gromacs/fft/gpu_3dfft_heffte.cpp: *  \author Gaurav Garg <gaugarg@nvidia.com>
src/gromacs/fft/gpu_3dfft_heffte.cpp:#include "gpu_3dfft_heffte.h"
src/gromacs/fft/gpu_3dfft_heffte.cpp:#include "gromacs/gpu_utils/device_context.h"
src/gromacs/fft/gpu_3dfft_heffte.cpp:#include "gromacs/gpu_utils/device_stream.h"
src/gromacs/fft/gpu_3dfft_heffte.cpp:#if GMX_GPU_SYCL
src/gromacs/fft/gpu_3dfft_heffte.cpp:#    include "gromacs/gpu_utils/devicebuffer_sycl.h"
src/gromacs/fft/gpu_3dfft_heffte.cpp:#if GMX_GPU_SYCL
src/gromacs/fft/gpu_3dfft_heffte.cpp:        return sycl::backend::cuda;
src/gromacs/fft/gpu_3dfft_heffte.cpp:        return sycl::backend::ext_oneapi_cuda;
src/gromacs/fft/gpu_3dfft_heffte.cpp:        return sycl::backend::opencl;
src/gromacs/fft/gpu_3dfft_heffte.cpp:#elif GMX_GPU_CUDA
src/gromacs/fft/gpu_3dfft_heffte.cpp:cudaStream_t heffteStream(cudaStream_t queue)
src/gromacs/fft/gpu_3dfft_heffte.cpp:                  "Only heFFTe cuFFT backend supported in CUDA build");
src/gromacs/fft/gpu_3dfft_heffte.cpp:Gpu3dFft::ImplHeFfte<backend_tag>::ImplHeFfte(bool                 allocateRealGrid,
src/gromacs/fft/gpu_3dfft_heffte.cpp:    planOptions.use_gpu_aware = boolFromEnvironmentOrDefault("GMX_HEFFTE_USE_GPU_AWARE", true);
src/gromacs/fft/gpu_3dfft_heffte.cpp:                                        workspaceRef = heffte::gpu::vector<std::complex<float>>(
src/gromacs/fft/gpu_3dfft_heffte.cpp:        // extra worker thread to submit tasks to the GPU. No need to do this with DPC++.
src/gromacs/fft/gpu_3dfft_heffte.cpp:                                        workspaceRef = heffte::gpu::vector<std::complex<float>>(
src/gromacs/fft/gpu_3dfft_heffte.cpp:    workspace_ = heffte::gpu::vector<std::complex<float>>(fftPlan_->size_workspace());
src/gromacs/fft/gpu_3dfft_heffte.cpp:#if GMX_GPU_CUDA
src/gromacs/fft/gpu_3dfft_heffte.cpp:    localRealGrid_    = heffte::gpu::vector<float>(fftPlan_->size_inbox());
src/gromacs/fft/gpu_3dfft_heffte.cpp:    localComplexGrid_ = heffte::gpu::vector<std::complex<float>>(fftPlan_->size_outbox());
src/gromacs/fft/gpu_3dfft_heffte.cpp:#elif GMX_GPU_SYCL
src/gromacs/fft/gpu_3dfft_heffte.cpp:#    error "HeFFTe build only supports CUDA and SYCL"
src/gromacs/fft/gpu_3dfft_heffte.cpp:Gpu3dFft::ImplHeFfte<backend_tag>::~ImplHeFfte<backend_tag>()
src/gromacs/fft/gpu_3dfft_heffte.cpp:#if GMX_GPU_SYCL
src/gromacs/fft/gpu_3dfft_heffte.cpp:void Gpu3dFft::ImplHeFfte<backend_tag>::perform3dFft(gmx_fft_direction dir, CommandEvent* /*timingEvent*/)
src/gromacs/fft/gpu_3dfft_heffte.cpp:#if GMX_GPU_CUDA
src/gromacs/fft/gpu_3dfft_heffte.cpp:#elif GMX_GPU_SYCL
src/gromacs/fft/gpu_3dfft_heffte.cpp:            GMX_THROW(NotImplementedError("The chosen 3D-FFT case is not implemented on GPUs"));
src/gromacs/fft/gpu_3dfft_heffte.cpp:#if GMX_GPU_FFT_CUFFT
src/gromacs/fft/gpu_3dfft_heffte.cpp:template class Gpu3dFft::ImplHeFfte<heffte::backend::cufft>;
src/gromacs/fft/gpu_3dfft_heffte.cpp:#if GMX_GPU_FFT_MKL
src/gromacs/fft/gpu_3dfft_heffte.cpp:template class Gpu3dFft::ImplHeFfte<heffte::backend::onemkl>;
src/gromacs/fft/gpu_3dfft_heffte.cpp:#if GMX_GPU_FFT_ROCFFT
src/gromacs/fft/gpu_3dfft_heffte.cpp:template class Gpu3dFft::ImplHeFfte<heffte::backend::rocfft>;
src/gromacs/fft/gpu_3dfft_sycl_vkfft.cpp: *  \brief Implements GPU 3D FFT routines for hipSYCL using vkFFT.
src/gromacs/fft/gpu_3dfft_sycl_vkfft.cpp:#include "gpu_3dfft_sycl_vkfft.h"
src/gromacs/fft/gpu_3dfft_sycl_vkfft.cpp:#include "gromacs/gpu_utils/device_stream.h"
src/gromacs/fft/gpu_3dfft_sycl_vkfft.cpp:#include "gromacs/gpu_utils/devicebuffer.h"
src/gromacs/fft/gpu_3dfft_sycl_vkfft.cpp:#if VKFFT_BACKEND == 1 // CUDA
src/gromacs/fft/gpu_3dfft_sycl_vkfft.cpp:static constexpr auto sc_syclBackend = sycl::backend::cuda;
src/gromacs/fft/gpu_3dfft_sycl_vkfft.cpp:static constexpr auto sc_syclBackend = sycl::backend::ext_oneapi_cuda;
src/gromacs/fft/gpu_3dfft_sycl_vkfft.cpp:class Gpu3dFft::ImplSyclVkfft::Impl
src/gromacs/fft/gpu_3dfft_sycl_vkfft.cpp:    //! \copydoc Gpu3dFft::Impl::Impl
src/gromacs/fft/gpu_3dfft_sycl_vkfft.cpp:Gpu3dFft::ImplSyclVkfft::Impl::Impl(bool allocateGrids,
src/gromacs/fft/gpu_3dfft_sycl_vkfft.cpp:Gpu3dFft::ImplSyclVkfft::Impl::~Impl()
src/gromacs/fft/gpu_3dfft_sycl_vkfft.cpp:Gpu3dFft::ImplSyclVkfft::ImplSyclVkfft(bool                 allocateGrids,
src/gromacs/fft/gpu_3dfft_sycl_vkfft.cpp:    Gpu3dFft::Impl::Impl(performOutOfPlaceFFT),
src/gromacs/fft/gpu_3dfft_sycl_vkfft.cpp:Gpu3dFft::ImplSyclVkfft::~ImplSyclVkfft()
src/gromacs/fft/gpu_3dfft_sycl_vkfft.cpp:void Gpu3dFft::ImplSyclVkfft::perform3dFft(gmx_fft_direction dir, CommandEvent* /*timingEvent*/)
src/gromacs/fft/gpu_3dfft_sycl_mkl.cpp: *  \brief Implements GPU 3D FFT routines for SYCL.
src/gromacs/fft/gpu_3dfft_sycl_mkl.cpp:#include "gpu_3dfft_sycl_mkl.h"
src/gromacs/fft/gpu_3dfft_sycl_mkl.cpp:#include "gromacs/gpu_utils/device_stream.h"
src/gromacs/fft/gpu_3dfft_sycl_mkl.cpp:#include "gromacs/gpu_utils/devicebuffer.h"
src/gromacs/fft/gpu_3dfft_sycl_mkl.cpp:#include "gromacs/gpu_utils/devicebuffer_sycl.h"
src/gromacs/fft/gpu_3dfft_sycl_mkl.cpp:#if (!GMX_FFT_MKL && !GMX_GPU_FFT_ONEMKL)
src/gromacs/fft/gpu_3dfft_sycl_mkl.cpp:#    error Must use MKL library CPU FFT when using MKL for GPU FFT and Intel DPC++ compiler
src/gromacs/fft/gpu_3dfft_sycl_mkl.cpp:#if GMX_GPU_FFT_MKL
src/gromacs/fft/gpu_3dfft_sycl_mkl.cpp:#if GMX_GPU_FFT_MKL && INTEL_MKL_VERSION < 20250000
src/gromacs/fft/gpu_3dfft_sycl_mkl.cpp:// Using oneMKL interface library. (GMX_GPU_FFT_ONEMKL)
src/gromacs/fft/gpu_3dfft_sycl_mkl.cpp:#endif // GMX_GPU_FFT_MKL
src/gromacs/fft/gpu_3dfft_sycl_mkl.cpp:Gpu3dFft::ImplSyclMkl::Descriptor Gpu3dFft::ImplSyclMkl::initDescriptor(const ivec realGridSize)
src/gromacs/fft/gpu_3dfft_sycl_mkl.cpp:Gpu3dFft::ImplSyclMkl::ImplSyclMkl(bool allocateRealGrid,
src/gromacs/fft/gpu_3dfft_sycl_mkl.cpp:    Gpu3dFft::Impl::Impl(performOutOfPlaceFFT),
src/gromacs/fft/gpu_3dfft_sycl_mkl.cpp:    // GROMACS doesn't use ILP64 for non-GPU interfaces (BLAS, FFT). The GPU/oneAPI interface assumes it.
src/gromacs/fft/gpu_3dfft_sycl_mkl.cpp:Gpu3dFft::ImplSyclMkl::~ImplSyclMkl()
src/gromacs/fft/gpu_3dfft_sycl_mkl.cpp:void Gpu3dFft::ImplSyclMkl::perform3dFft(gmx_fft_direction dir, CommandEvent* /*timingEvent*/)
src/gromacs/fft/gpu_3dfft_sycl_mkl.cpp:            GMX_THROW(NotImplementedError("The chosen 3D-FFT case is not implemented on GPUs"));
src/gromacs/fft/gpu_3dfft.cpp: *  \brief Implements stub GPU 3D FFT routines for CPU-only builds
src/gromacs/fft/gpu_3dfft.cpp: *  \author Gaurav Garg <gaugarg@nvidia.com>
src/gromacs/fft/gpu_3dfft.cpp:#include "gpu_3dfft.h"
src/gromacs/fft/gpu_3dfft.cpp:#include "gpu_3dfft_impl.h"
src/gromacs/fft/gpu_3dfft.cpp:#if GMX_GPU_CUDA
src/gromacs/fft/gpu_3dfft.cpp:#    include "gpu_3dfft_cufft.h"
src/gromacs/fft/gpu_3dfft.cpp:#elif GMX_GPU_HIP
src/gromacs/fft/gpu_3dfft.cpp:#    if GMX_GPU_FFT_VKFFT
src/gromacs/fft/gpu_3dfft.cpp:#        include "gpu_3dfft_hip_vkfft.h"
src/gromacs/fft/gpu_3dfft.cpp:#        include "gpu_3dfft_hip_rocfft.h"
src/gromacs/fft/gpu_3dfft.cpp:#elif GMX_GPU_OPENCL
src/gromacs/fft/gpu_3dfft.cpp:#    if GMX_GPU_FFT_VKFFT
src/gromacs/fft/gpu_3dfft.cpp:#        include "gpu_3dfft_ocl_vkfft.h"
src/gromacs/fft/gpu_3dfft.cpp:#        include "gpu_3dfft_ocl.h"
src/gromacs/fft/gpu_3dfft.cpp:#elif GMX_GPU_SYCL
src/gromacs/fft/gpu_3dfft.cpp:#    include "gpu_3dfft_sycl.h"
src/gromacs/fft/gpu_3dfft.cpp:#    if GMX_GPU_FFT_MKL || GMX_GPU_FFT_ONEMKL
src/gromacs/fft/gpu_3dfft.cpp:#        include "gpu_3dfft_sycl_mkl.h"
src/gromacs/fft/gpu_3dfft.cpp:#    elif GMX_GPU_FFT_BBFFT
src/gromacs/fft/gpu_3dfft.cpp:#        include "gpu_3dfft_sycl_bbfft.h"
src/gromacs/fft/gpu_3dfft.cpp:#    elif GMX_GPU_FFT_ROCFFT
src/gromacs/fft/gpu_3dfft.cpp:#        include "gpu_3dfft_sycl_rocfft.h"
src/gromacs/fft/gpu_3dfft.cpp:#    elif GMX_GPU_FFT_VKFFT
src/gromacs/fft/gpu_3dfft.cpp:#        include "gpu_3dfft_sycl_vkfft.h"
src/gromacs/fft/gpu_3dfft.cpp:#    include "gpu_3dfft_heffte.h"
src/gromacs/fft/gpu_3dfft.cpp:#    include "gpu_3dfft_cufftmp.h"
src/gromacs/fft/gpu_3dfft.cpp:Gpu3dFft::Gpu3dFft(FftBackend           backend,
src/gromacs/fft/gpu_3dfft.cpp:#if GMX_GPU_CUDA
src/gromacs/fft/gpu_3dfft.cpp:            impl_ = std::make_unique<Gpu3dFft::ImplCuFft>(allocateRealGrid,
src/gromacs/fft/gpu_3dfft.cpp:            impl_ = std::make_unique<Gpu3dFft::ImplCuFftMp>(allocateRealGrid,
src/gromacs/fft/gpu_3dfft.cpp:            GMX_RELEASE_ASSERT(backend == FftBackend::HeFFTe_CUDA,
src/gromacs/fft/gpu_3dfft.cpp:#elif GMX_GPU_HIP
src/gromacs/fft/gpu_3dfft.cpp:#    if GMX_GPU_FFT_VKFFT
src/gromacs/fft/gpu_3dfft.cpp:            impl_ = std::make_unique<Gpu3dFft::ImplHipVkFft>(allocateRealGrid,
src/gromacs/fft/gpu_3dfft.cpp:#    elif GMX_GPU_FFT_ROCFFT
src/gromacs/fft/gpu_3dfft.cpp:            impl_ = std::make_unique<Gpu3dFft::ImplHipRocfft>(allocateRealGrid,
src/gromacs/fft/gpu_3dfft.cpp:#elif GMX_GPU_OPENCL
src/gromacs/fft/gpu_3dfft.cpp:#    if GMX_GPU_FFT_VKFFT
src/gromacs/fft/gpu_3dfft.cpp:            impl_ = std::make_unique<Gpu3dFft::ImplOclVkfft>(allocateRealGrid,
src/gromacs/fft/gpu_3dfft.cpp:#    elif GMX_GPU_FFT_CLFFT
src/gromacs/fft/gpu_3dfft.cpp:            impl_ = std::make_unique<Gpu3dFft::ImplOcl>(allocateRealGrid,
src/gromacs/fft/gpu_3dfft.cpp:#elif GMX_GPU_SYCL
src/gromacs/fft/gpu_3dfft.cpp:#    if GMX_GPU_FFT_MKL || GMX_GPU_FFT_ONEMKL
src/gromacs/fft/gpu_3dfft.cpp:            impl_ = std::make_unique<Gpu3dFft::ImplSyclMkl>(allocateRealGrid,
src/gromacs/fft/gpu_3dfft.cpp:#    elif GMX_GPU_FFT_BBFFT
src/gromacs/fft/gpu_3dfft.cpp:            impl_ = std::make_unique<Gpu3dFft::ImplSyclBbfft>(allocateRealGrid,
src/gromacs/fft/gpu_3dfft.cpp:#    elif GMX_GPU_FFT_VKFFT
src/gromacs/fft/gpu_3dfft.cpp:            impl_ = std::make_unique<Gpu3dFft::ImplSyclVkfft>(allocateRealGrid,
src/gromacs/fft/gpu_3dfft.cpp:#    elif GMX_GPU_FFT_ROCFFT
src/gromacs/fft/gpu_3dfft.cpp:            impl_ = std::make_unique<Gpu3dFft::ImplSyclRocfft>(allocateRealGrid,
src/gromacs/fft/gpu_3dfft.cpp:            impl_ = std::make_unique<Gpu3dFft::ImplSycl>(allocateRealGrid,
src/gromacs/fft/gpu_3dfft.cpp:        case FftBackend::HeFFTe_CUDA:
src/gromacs/fft/gpu_3dfft.cpp:#    if GMX_GPU_CUDA
src/gromacs/fft/gpu_3dfft.cpp:                               "HeFFTe not compiled with CUDA support");
src/gromacs/fft/gpu_3dfft.cpp:            impl_ = std::make_unique<Gpu3dFft::ImplHeFfte<heffte::backend::cufft>>(
src/gromacs/fft/gpu_3dfft.cpp:                    "HeFFTe_CUDA FFT backend is supported only with GROMACS compiled with CUDA");
src/gromacs/fft/gpu_3dfft.cpp:#    if GMX_GPU_SYCL && GMX_GPU_FFT_MKL
src/gromacs/fft/gpu_3dfft.cpp:            impl_ = std::make_unique<Gpu3dFft::ImplHeFfte<heffte::backend::onemkl>>(
src/gromacs/fft/gpu_3dfft.cpp:                               "HeFFTe multi-GPU FFT backend is supported in GROMACS SYCL "
src/gromacs/fft/gpu_3dfft.cpp:#    if GMX_GPU_SYCL && GMX_GPU_FFT_ROCFFT
src/gromacs/fft/gpu_3dfft.cpp:            impl_ = std::make_unique<Gpu3dFft::ImplHeFfte<heffte::backend::rocfft>>(
src/gromacs/fft/gpu_3dfft.cpp:                               "HeFFTe multi-GPU FFT backend is supported in GROMACS SYCL "
src/gromacs/fft/gpu_3dfft.cpp:#    if GMX_GPU_SYCL && GMX_GPU_FFT_CUFFT
src/gromacs/fft/gpu_3dfft.cpp:            impl_ = std::make_unique<Gpu3dFft::ImplHeFfte<heffte::backend::cufft>>(
src/gromacs/fft/gpu_3dfft.cpp:                               "HeFFTe multi-GPU FFT backend is supported in GROMACS SYCL "
src/gromacs/fft/gpu_3dfft.cpp:Gpu3dFft::~Gpu3dFft() = default;
src/gromacs/fft/gpu_3dfft.cpp:void Gpu3dFft::perform3dFft(gmx_fft_direction dir, CommandEvent* timingEvent)
src/gromacs/fft/gpu_3dfft.cpp:    GMX_RELEASE_ASSERT(impl_ != nullptr, "Cannot run GPU routines in a CPU-only configuration");
src/gromacs/fft/gpu_3dfft_ocl.cpp: *  \brief Implements GPU 3D FFT routines for OpenCL.
src/gromacs/fft/gpu_3dfft_ocl.cpp:#include "gpu_3dfft_ocl.h"
src/gromacs/fft/gpu_3dfft_ocl.cpp:#include "gromacs/gpu_utils/device_context.h"
src/gromacs/fft/gpu_3dfft_ocl.cpp:#include "gromacs/gpu_utils/device_stream.h"
src/gromacs/fft/gpu_3dfft_ocl.cpp:#include "gromacs/gpu_utils/devicebuffer.h"
src/gromacs/fft/gpu_3dfft_ocl.cpp:#include "gromacs/gpu_utils/gmxopencl.h"
src/gromacs/fft/gpu_3dfft_ocl.cpp:    // Supposedly it's just a superset of standard OpenCL errors
src/gromacs/fft/gpu_3dfft_ocl.cpp:Gpu3dFft::ImplOcl::ImplOcl(bool allocateRealGrid,
src/gromacs/fft/gpu_3dfft_ocl.cpp:    Gpu3dFft::Impl::Impl(performOutOfPlaceFFT), realGrid_(*realGrid)
src/gromacs/fft/gpu_3dfft_ocl.cpp:Gpu3dFft::ImplOcl::~ImplOcl()
src/gromacs/fft/gpu_3dfft_ocl.cpp:void Gpu3dFft::ImplOcl::perform3dFft(gmx_fft_direction dir, CommandEvent* timingEvent)
src/gromacs/fft/gpu_3dfft_ocl.cpp:            GMX_THROW(NotImplementedError("The chosen 3D-FFT case is not implemented on GPUs"));
src/gromacs/fft/gpu_3dfft.h: *  \brief Declares the GPU 3D FFT routines.
src/gromacs/fft/gpu_3dfft.h: *  \author Gaurav Garg <gaugarg@nvidia.com>
src/gromacs/fft/gpu_3dfft.h:#ifndef GMX_FFT_GPU_3DFFT_H
src/gromacs/fft/gpu_3dfft.h:#define GMX_FFT_GPU_3DFFT_H
src/gromacs/fft/gpu_3dfft.h:#include "gromacs/gpu_utils/devicebuffer_datatype.h"
src/gromacs/fft/gpu_3dfft.h:#include "gromacs/gpu_utils/gputraits.h"
src/gromacs/fft/gpu_3dfft.h: * Enum specifying all GPU FFT backends supported by GROMACS
src/gromacs/fft/gpu_3dfft.h: * Some of the backends support only single GPU, some only multi-node, multi-GPU
src/gromacs/fft/gpu_3dfft.h:    Cufft,              //!< supports only single-GPU
src/gromacs/fft/gpu_3dfft.h:    OclVkfft,           //!< supports only single-GPU
src/gromacs/fft/gpu_3dfft.h:    Ocl,                //!< supports only single-GPU
src/gromacs/fft/gpu_3dfft.h:    CuFFTMp,            //!< supports only multi-gpu
src/gromacs/fft/gpu_3dfft.h:    HeFFTe_CUDA,        //!< supports only multi-gpu
src/gromacs/fft/gpu_3dfft.h:    HeFFTe_Sycl_OneMkl, //!< supports only multi-gpu
src/gromacs/fft/gpu_3dfft.h:    HeFFTe_Sycl_Rocfft, //!< supports only multi-gpu
src/gromacs/fft/gpu_3dfft.h:    HeFFTe_Sycl_cuFFT,  //!< supports only multi-gpu
src/gromacs/fft/gpu_3dfft.h:    SyclMkl,            //!< supports only single-GPU
src/gromacs/fft/gpu_3dfft.h:    SyclOneMkl,         //!< supports only single-GPU
src/gromacs/fft/gpu_3dfft.h:    SyclRocfft,         //!< supports only single-GPU
src/gromacs/fft/gpu_3dfft.h:    SyclVkfft,          //!< supports only single-GPU
src/gromacs/fft/gpu_3dfft.h:    SyclBbfft,          //!< supports only single-GPU
src/gromacs/fft/gpu_3dfft.h:    HipVkfft,           //!< supports only single-GPU
src/gromacs/fft/gpu_3dfft.h:    HipRocfft,          //!< supports only single-GPU
src/gromacs/fft/gpu_3dfft.h:class Gpu3dFft
src/gromacs/fft/gpu_3dfft.h:     * \param[in] context                    GPU context.
src/gromacs/fft/gpu_3dfft.h:     * \param[in]  pmeStream                 GPU stream for PME.
src/gromacs/fft/gpu_3dfft.h:    Gpu3dFft(FftBackend           backend,
src/gromacs/fft/gpu_3dfft.h:    ~Gpu3dFft();
src/gromacs/fft/gpu_3dfft_sycl_rocfft.h: *  \brief Declares the GPU 3D FFT routines for hipSYCL via rocFFT.
src/gromacs/fft/gpu_3dfft_sycl_rocfft.h:#ifndef GMX_FFT_GPU_3DFFT_SYCL_ROCFFT_H
src/gromacs/fft/gpu_3dfft_sycl_rocfft.h:#define GMX_FFT_GPU_3DFFT_SYCL_ROCFFT_H
src/gromacs/fft/gpu_3dfft_sycl_rocfft.h:#include "gromacs/gpu_utils/devicebuffer_datatype.h"
src/gromacs/fft/gpu_3dfft_sycl_rocfft.h:#include "gromacs/gpu_utils/gputraits.h"
src/gromacs/fft/gpu_3dfft_sycl_rocfft.h:#include "gpu_3dfft_impl.h"
src/gromacs/fft/gpu_3dfft_sycl_rocfft.h: * transforms using rocFFT for hipSYCL targetting ROCm devices.
src/gromacs/fft/gpu_3dfft_sycl_rocfft.h:class Gpu3dFft::ImplSyclRocfft : public Gpu3dFft::Impl
src/gromacs/fft/gpu_3dfft_sycl_rocfft.h:    //! \copydoc Gpu3dFft::Impl::Impl
src/gromacs/fft/gpu_3dfft_sycl_rocfft.h:    //! \copydoc Gpu3dFft::Impl::~Impl
src/gromacs/fft/gpu_3dfft_sycl_rocfft.h:    //! \copydoc Gpu3dFft::Impl::perform3dFft
src/gromacs/CMakeLists.txt:set_property(GLOBAL PROPERTY CUDA_SOURCES)
src/gromacs/CMakeLists.txt:add_subdirectory(gpu_utils)
src/gromacs/CMakeLists.txt:# Mark some shared GPU implementation files to compile with CUDA if needed
src/gromacs/CMakeLists.txt:if (GMX_GPU_CUDA)
src/gromacs/CMakeLists.txt:    get_property(CUDA_SOURCES GLOBAL PROPERTY CUDA_SOURCES)
src/gromacs/CMakeLists.txt:    set_source_files_properties(${CUDA_SOURCES} PROPERTIES LANGUAGE CUDA)
src/gromacs/CMakeLists.txt:if (GMX_GPU_CUDA)
src/gromacs/CMakeLists.txt:        target_link_libraries(libgromacs PRIVATE CUDA::cufft)
src/gromacs/CMakeLists.txt:    if (GMX_CLANG_CUDA)
src/gromacs/CMakeLists.txt:        set_target_properties(libgromacs PROPERTIES CUDA_ARCHITECTURES "${_CUDA_CLANG_GENCODE_FLAGS}")
src/gromacs/CMakeLists.txt:        target_compile_options(libgromacs PRIVATE "$<$<COMPILE_LANGUAGE:CUDA>:${GMX_CUDA_CLANG_FLAGS}>")
src/gromacs/CMakeLists.txt:        target_compile_options(libgromacs PRIVATE "$<$<COMPILE_LANGUAGE:CUDA>:${GMX_CUDA_NVCC_FLAGS}>")
src/gromacs/CMakeLists.txt:        set_target_properties(libgromacs PROPERTIES CUDA_ARCHITECTURES "${GMX_CUDA_NVCC_GENCODE_FLAGS}")
src/gromacs/CMakeLists.txt:        set_target_properties(libgromacs PROPERTIES CUDA_SEPARABLE_COMPILATION ON)
src/gromacs/CMakeLists.txt:        set_target_properties(libgromacs PROPERTIES CUDA_RESOLVE_DEVICE_SYMBOLS ON)
src/gromacs/CMakeLists.txt:        # We need to PUBLIC link to the stub libraries nvml/cuda to WAR an issue
src/gromacs/CMakeLists.txt:        target_link_libraries(libgromacs PUBLIC CUDA::nvml CUDA::cuda_driver)
src/gromacs/CMakeLists.txt:elseif(GMX_GPU_HIP)
src/gromacs/CMakeLists.txt:if(GMX_GPU_FFT_VKFFT)
src/gromacs/CMakeLists.txt:if(GMX_GPU_FFT_ROCFFT)
src/gromacs/CMakeLists.txt:if(GMX_GPU_FFT_ONEMKL)
src/gromacs/CMakeLists.txt:if (GMX_GPU_FFT_CLFFT)
src/gromacs/CMakeLists.txt:    if (NOT GMX_GPU_OPENCL)
src/gromacs/CMakeLists.txt:        message(FATAL_ERROR "clFFT is only supported in OpenCL builds")
src/gromacs/CMakeLists.txt:"An OpenCL build was requested with Visual Studio compiler, but GROMACS
src/gromacs/CMakeLists.txt:clFFT to help with building for OpenCL, but that clFFT has not yet been
src/gromacs/CMakeLists.txt:requires. Thus for now, OpenCL is not available with MSVC and the internal
src/gromacs/CMakeLists.txt:installing a clFFT package, use VkFFT by setting -DGMX_GPU_FFT_LIBRARY=VkFFT,
src/gromacs/CMakeLists.txt:# CUDA runtime headers
src/gromacs/CMakeLists.txt:if (GMX_GPU_CUDA AND CMAKE_CXX_COMPILER_ID MATCHES "Clang")
src/gromacs/CMakeLists.txt:        set(GMX_CUDA_CLANG_FLAGS "${GMX_CUDA_CLANG_FLAGS} ${_compile_flag}")
src/gromacs/CMakeLists.txt:    if (NOT GMX_CLANG_CUDA)
src/gromacs/CMakeLists.txt:        get_property(CUDA_SOURCES GLOBAL PROPERTY CUDA_SOURCES)
src/gromacs/CMakeLists.txt:        if (GMX_CUDA_CLANG_FLAGS)
src/gromacs/CMakeLists.txt:            set_source_files_properties(${CUDA_SOURCES} PROPERTIES COMPILE_FLAGS ${GMX_CUDA_CLANG_FLAGS})
src/gromacs/CMakeLists.txt:if (GMX_GPU_HIP)
src/gromacs/CMakeLists.txt:if (GMX_GPU_SYCL)
src/gromacs/CMakeLists.txt:                      ${OpenCL_LIBRARIES}
src/gromacs/CMakeLists.txt:                      $<BUILD_INTERFACE:gpu_utils>
src/gromacs/CMakeLists.txt:if(GMX_GPU_OPENCL)
src/gromacs/CMakeLists.txt:    file(GLOB OPENCL_INSTALLED_FILES
src/gromacs/CMakeLists.txt:        gpu_utils/vectype_ops.clh
src/gromacs/CMakeLists.txt:        gpu_utils/device_utils.clh
src/gromacs/CMakeLists.txt:    install(FILES ${OPENCL_INSTALLED_FILES}
src/gromacs/CMakeLists.txt:        DESTINATION ${GMX_INSTALL_OCLDIR}/gromacs/gpu_utils
src/gromacs/CMakeLists.txt:    file(GLOB OPENCL_INSTALLED_FILES
src/gromacs/CMakeLists.txt:    install(FILES ${OPENCL_INSTALLED_FILES}
src/gromacs/CMakeLists.txt:    file(GLOB OPENCL_INSTALLED_FILES
src/gromacs/CMakeLists.txt:    install(FILES ${OPENCL_INSTALLED_FILES}
src/gromacs/CMakeLists.txt:    file(GLOB OPENCL_INSTALLED_FILES
src/gromacs/CMakeLists.txt:        nbnxm/opencl/nbnxm_ocl_kernels.cl
src/gromacs/CMakeLists.txt:        nbnxm/opencl/nbnxm_ocl_kernel.clh
src/gromacs/CMakeLists.txt:        nbnxm/opencl/nbnxm_ocl_kernel_pruneonly.clh
src/gromacs/CMakeLists.txt:        nbnxm/opencl/nbnxm_ocl_kernels.clh
src/gromacs/CMakeLists.txt:        nbnxm/opencl/nbnxm_ocl_kernels_fastgen.clh
src/gromacs/CMakeLists.txt:        nbnxm/opencl/nbnxm_ocl_kernels_fastgen_add_twincut.clh
src/gromacs/CMakeLists.txt:        nbnxm/opencl/nbnxm_ocl_kernel_utils.clh
src/gromacs/CMakeLists.txt:        nbnxm/opencl/nbnxm_ocl_consts.h
src/gromacs/CMakeLists.txt:    install(FILES ${OPENCL_INSTALLED_FILES}
src/gromacs/CMakeLists.txt:        DESTINATION ${GMX_INSTALL_OCLDIR}/gromacs/nbnxm/opencl
src/gromacs/CMakeLists.txt:    file(GLOB OPENCL_INSTALLED_FILES
src/gromacs/CMakeLists.txt:        ewald/pme_gpu_calculate_splines.clh
src/gromacs/CMakeLists.txt:        ewald/pme_gpu_types.h
src/gromacs/CMakeLists.txt:    install(FILES ${OPENCL_INSTALLED_FILES}
src/gromacs/ewald/pme_gpu_calculate_splines.clh:#ifndef GMX_EWALD_PME_GPU_CALCULATE_SPLINES_CLH
src/gromacs/ewald/pme_gpu_calculate_splines.clh:#define GMX_EWALD_PME_GPU_CALCULATE_SPLINES_CLH
src/gromacs/ewald/pme_gpu_calculate_splines.clh: * \brief This file defines the PME OpenCL inline device functions for computing splines.
src/gromacs/ewald/pme_gpu_calculate_splines.clh: * This closely mirrors pme_gpu_calculate_splines.cuh (which is used in CUDA kernels), except with no templates.
src/gromacs/ewald/pme_gpu_calculate_splines.clh: * - c_skipNeutralAtoms - same as in pme_gpu_constants.h.
src/gromacs/ewald/pme_gpu_calculate_splines.clh: * which is laid out for GPU spread/gather kernels. The base only corresponds to the atom index within the execution block.
src/gromacs/ewald/pme_gpu_calculate_splines.clh: * \returns Index into theta or dtheta array using GPU layout.
src/gromacs/ewald/pme_gpu_calculate_splines.clh: * which is laid out for GPU spread/gather kernels. The index is wrt to the execution block,
src/gromacs/ewald/pme_gpu_calculate_splines.clh: * \returns Index into theta or dtheta array using GPU layout.
src/gromacs/ewald/pme_gpu_calculate_splines.clh:inline int pme_gpu_check_atom_charge(const float coefficient)
src/gromacs/ewald/pme_pp_comm_gpu.h: * \brief Declaration of GPU PME-PP Communication.
src/gromacs/ewald/pme_pp_comm_gpu.h: * \author Alan Gray <alang@nvidia.com>
src/gromacs/ewald/pme_pp_comm_gpu.h:#ifndef GMX_PME_PP_COMM_GPU_H
src/gromacs/ewald/pme_pp_comm_gpu.h:#define GMX_PME_PP_COMM_GPU_H
src/gromacs/ewald/pme_pp_comm_gpu.h:#include "gromacs/gpu_utils/devicebuffer_datatype.h"
src/gromacs/ewald/pme_pp_comm_gpu.h:#include "gromacs/gpu_utils/hostallocator.h"
src/gromacs/ewald/pme_pp_comm_gpu.h:class GpuEventSynchronizer;
src/gromacs/ewald/pme_pp_comm_gpu.h: * \brief Manages communication related to GPU buffers between this
src/gromacs/ewald/pme_pp_comm_gpu.h:class PmePpCommGpu
src/gromacs/ewald/pme_pp_comm_gpu.h:    /*! \brief Creates PME-PP GPU communication object
src/gromacs/ewald/pme_pp_comm_gpu.h:     * \param[in] deviceContext     GPU context.
src/gromacs/ewald/pme_pp_comm_gpu.h:     * \param[in] deviceStream      GPU stream.
src/gromacs/ewald/pme_pp_comm_gpu.h:     * \param[in] useNvshmem        NVSHMEM enable/disable for GPU comm.
src/gromacs/ewald/pme_pp_comm_gpu.h:    PmePpCommGpu(MPI_Comm                    comm,
src/gromacs/ewald/pme_pp_comm_gpu.h:    ~PmePpCommGpu();
src/gromacs/ewald/pme_pp_comm_gpu.h:     * Pull data from PME GPU directly using CUDA Memory copy.
src/gromacs/ewald/pme_pp_comm_gpu.h:     * \param[in] recvPmeForceToGpu Whether receive is to GPU, otherwise CPU
src/gromacs/ewald/pme_pp_comm_gpu.h:    void receiveForceFromPme(RVec* recvPtr, int recvSize, bool recvPmeForceToGpu);
src/gromacs/ewald/pme_pp_comm_gpu.h:    /*! \brief Push coordinates buffer directly to GPU memory on PME task
src/gromacs/ewald/pme_pp_comm_gpu.h:    void sendCoordinatesToPmeFromGpu(DeviceBuffer<RVec>    sendPtr,
src/gromacs/ewald/pme_pp_comm_gpu.h:                                     GpuEventSynchronizer* coordinatesReadyOnDeviceEvent);
src/gromacs/ewald/pme_pp_comm_gpu.h:    /*! \brief Push coordinates buffer from host memory directly to GPU memory on PME task
src/gromacs/ewald/pme_pp_comm_gpu.h:     * Return pointer to buffer used for staging PME force on GPU
src/gromacs/ewald/pme_pp_comm_gpu.h:    DeviceBuffer<gmx::RVec> getGpuForceStagingPtr();
src/gromacs/ewald/pme_pp_comm_gpu.h:    GpuEventSynchronizer* getForcesReadySynchronizer();
src/gromacs/ewald/pme_pp_comm_gpu.h:    DeviceBuffer<uint64_t> getGpuForcesSyncObj();
src/gromacs/ewald/pme_gpu_internal.cpp: * for performing the PME calculations on GPU.
src/gromacs/ewald/pme_gpu_internal.cpp: * Note that this file is compiled as regular C++ source in OpenCL builds, but
src/gromacs/ewald/pme_gpu_internal.cpp: * it is treated as CUDA source in CUDA-enabled GPU builds.
src/gromacs/ewald/pme_gpu_internal.cpp:#include "pme_gpu_internal.h"
src/gromacs/ewald/pme_gpu_internal.cpp:#include "gromacs/fft/gpu_3dfft.h"
src/gromacs/ewald/pme_gpu_internal.cpp:#include "gromacs/gpu_utils/device_context.h"
src/gromacs/ewald/pme_gpu_internal.cpp:#include "gromacs/gpu_utils/device_stream.h"
src/gromacs/ewald/pme_gpu_internal.cpp:#include "gromacs/gpu_utils/gpu_utils.h"
src/gromacs/ewald/pme_gpu_internal.cpp:#include "gromacs/gpu_utils/hostallocator.h"
src/gromacs/ewald/pme_gpu_internal.cpp:#if GMX_GPU_SYCL
src/gromacs/ewald/pme_gpu_internal.cpp:#    include "gromacs/gpu_utils/syclutils.h"
src/gromacs/ewald/pme_gpu_internal.cpp:#include "gromacs/ewald/pme_coordinate_receiver_gpu.h"
src/gromacs/ewald/pme_gpu_internal.cpp:#include "gromacs/timing/gpu_timing.h"
src/gromacs/ewald/pme_gpu_internal.cpp:#if GMX_GPU_CUDA
src/gromacs/ewald/pme_gpu_internal.cpp:#include "pme_gpu_calculate_splines.h"
src/gromacs/ewald/pme_gpu_internal.cpp:#include "pme_gpu_constants.h"
src/gromacs/ewald/pme_gpu_internal.cpp:#include "pme_gpu_grid.h"
src/gromacs/ewald/pme_gpu_internal.cpp:#include "pme_gpu_program_impl.h"
src/gromacs/ewald/pme_gpu_internal.cpp:#include "pme_gpu_timings.h"
src/gromacs/ewald/pme_gpu_internal.cpp:#include "pme_gpu_types.h"
src/gromacs/ewald/pme_gpu_internal.cpp:#include "pme_gpu_types_host.h"
src/gromacs/ewald/pme_gpu_internal.cpp:#include "pme_gpu_types_host_impl.h"
src/gromacs/ewald/pme_gpu_internal.cpp: * Wrapper for getting a pointer to the plain C++ part of the GPU kernel parameters structure.
src/gromacs/ewald/pme_gpu_internal.cpp: * \param[in] pmeGpu  The PME GPU structure.
src/gromacs/ewald/pme_gpu_internal.cpp:static PmeGpuKernelParamsBase* pme_gpu_get_kernel_params_base_ptr(const PmeGpu* pmeGpu)
src/gromacs/ewald/pme_gpu_internal.cpp:    // reinterpret_cast is needed because the derived CUDA structure is not known in this file
src/gromacs/ewald/pme_gpu_internal.cpp:    auto* kernelParamsPtr = reinterpret_cast<PmeGpuKernelParamsBase*>(pmeGpu->kernelParams.get());
src/gromacs/ewald/pme_gpu_internal.cpp: * The GPU atom data buffers must be padded, which means that
src/gromacs/ewald/pme_gpu_internal.cpp:#if !GMX_GPU_SYCL
src/gromacs/ewald/pme_gpu_internal.cpp:int pme_gpu_get_atom_data_block_size()
src/gromacs/ewald/pme_gpu_internal.cpp:int pme_gpu_get_atoms_per_warp(const PmeGpu* pmeGpu)
src/gromacs/ewald/pme_gpu_internal.cpp:    const int order = pmeGpu->common->pme_order;
src/gromacs/ewald/pme_gpu_internal.cpp:            (pmeGpu->settings.threadsPerAtom == ThreadsPerAtom::Order ? order : order * order);
src/gromacs/ewald/pme_gpu_internal.cpp:    return pmeGpu->programHandle_->warpSize() / threadsPerAtom;
src/gromacs/ewald/pme_gpu_internal.cpp:void pme_gpu_synchronize(const PmeGpu* pmeGpu)
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->archSpecific->pmeStream_.synchronize();
src/gromacs/ewald/pme_gpu_internal.cpp:void pme_gpu_alloc_energy_virial(PmeGpu* pmeGpu)
src/gromacs/ewald/pme_gpu_internal.cpp:            pmeGpu->common->ngrids == 1 || pmeGpu->common->ngrids == 2,
src/gromacs/ewald/pme_gpu_internal.cpp:            "Only one (normal Coulomb PME) or two (FEP coulomb PME) PME grids can be used on GPU");
src/gromacs/ewald/pme_gpu_internal.cpp:    for (int gridIndex = 0; gridIndex < pmeGpu->common->ngrids; gridIndex++)
src/gromacs/ewald/pme_gpu_internal.cpp:        allocateDeviceBuffer(&pmeGpu->kernelParams->constants.d_virialAndEnergy[gridIndex],
src/gromacs/ewald/pme_gpu_internal.cpp:                             pmeGpu->archSpecific->deviceContext_);
src/gromacs/ewald/pme_gpu_internal.cpp:        gmx::changePinningPolicy(&pmeGpu->staging.h_virialAndEnergy[gridIndex],
src/gromacs/ewald/pme_gpu_internal.cpp:        pmeGpu->staging.h_virialAndEnergy[gridIndex].resize(c_virialAndEnergyCount);
src/gromacs/ewald/pme_gpu_internal.cpp:void pme_gpu_free_energy_virial(PmeGpu* pmeGpu)
src/gromacs/ewald/pme_gpu_internal.cpp:    for (int gridIndex = 0; gridIndex < pmeGpu->common->ngrids; gridIndex++)
src/gromacs/ewald/pme_gpu_internal.cpp:        freeDeviceBuffer(&pmeGpu->kernelParams->constants.d_virialAndEnergy[gridIndex]);
src/gromacs/ewald/pme_gpu_internal.cpp:        pmeGpu->staging.h_virialAndEnergy[gridIndex].clear();
src/gromacs/ewald/pme_gpu_internal.cpp:void pme_gpu_clear_energy_virial(const PmeGpu* pmeGpu, const bool gpuGraphWithSeparatePmeRank)
src/gromacs/ewald/pme_gpu_internal.cpp:    for (int gridIndex = 0; gridIndex < pmeGpu->common->ngrids; gridIndex++)
src/gromacs/ewald/pme_gpu_internal.cpp:        clearDeviceBufferAsync(&pmeGpu->kernelParams->constants.d_virialAndEnergy[gridIndex],
src/gromacs/ewald/pme_gpu_internal.cpp:                               pmeGpu->archSpecific->pmeStream_);
src/gromacs/ewald/pme_gpu_internal.cpp:    if (pmeGpu->settings.useGpuForceReduction && gpuGraphWithSeparatePmeRank)
src/gromacs/ewald/pme_gpu_internal.cpp:        // Mark forces ready event after this clearing, otherwise CUDA graph capture fails due to unjoined work
src/gromacs/ewald/pme_gpu_internal.cpp:        pmeGpu->archSpecific->pmeForcesReady.markEvent(pmeGpu->archSpecific->pmeStream_);
src/gromacs/ewald/pme_gpu_internal.cpp:void pme_gpu_realloc_and_copy_bspline_values(PmeGpu* pmeGpu, const int gridIndex)
src/gromacs/ewald/pme_gpu_internal.cpp:            pmeGpu->common->ngrids == 1 || pmeGpu->common->ngrids == 2,
src/gromacs/ewald/pme_gpu_internal.cpp:            "Only one (normal Coulomb PME) or two (FEP coulomb PME) PME grids can be used on GPU");
src/gromacs/ewald/pme_gpu_internal.cpp:    GMX_ASSERT(gridIndex < pmeGpu->common->ngrids,
src/gromacs/ewald/pme_gpu_internal.cpp:                                          pmeGpu->kernelParams->grid.realGridSize[XX],
src/gromacs/ewald/pme_gpu_internal.cpp:                                          pmeGpu->kernelParams->grid.realGridSize[XX]
src/gromacs/ewald/pme_gpu_internal.cpp:                                                  + pmeGpu->kernelParams->grid.realGridSize[YY] };
src/gromacs/ewald/pme_gpu_internal.cpp:    memcpy(&pmeGpu->kernelParams->grid.splineValuesOffset, &splineValuesOffset, sizeof(splineValuesOffset));
src/gromacs/ewald/pme_gpu_internal.cpp:    const int newSplineValuesSize = pmeGpu->kernelParams->grid.realGridSize[XX]
src/gromacs/ewald/pme_gpu_internal.cpp:                                    + pmeGpu->kernelParams->grid.realGridSize[YY]
src/gromacs/ewald/pme_gpu_internal.cpp:                                    + pmeGpu->kernelParams->grid.realGridSize[ZZ];
src/gromacs/ewald/pme_gpu_internal.cpp:    const bool shouldRealloc = (newSplineValuesSize > pmeGpu->archSpecific->splineValuesSize[gridIndex]);
src/gromacs/ewald/pme_gpu_internal.cpp:    reallocateDeviceBuffer(&pmeGpu->kernelParams->grid.d_splineModuli[gridIndex],
src/gromacs/ewald/pme_gpu_internal.cpp:                           &pmeGpu->archSpecific->splineValuesSize[gridIndex],
src/gromacs/ewald/pme_gpu_internal.cpp:                           &pmeGpu->archSpecific->splineValuesCapacity[gridIndex],
src/gromacs/ewald/pme_gpu_internal.cpp:                           pmeGpu->archSpecific->deviceContext_);
src/gromacs/ewald/pme_gpu_internal.cpp:        changePinningPolicy(&pmeGpu->staging.h_splineModuli[gridIndex],
src/gromacs/ewald/pme_gpu_internal.cpp:        pmeGpu->staging.h_splineModuli[gridIndex].resize(newSplineValuesSize);
src/gromacs/ewald/pme_gpu_internal.cpp:        memcpy(pmeGpu->staging.h_splineModuli[gridIndex].data() + splineValuesOffset[i],
src/gromacs/ewald/pme_gpu_internal.cpp:               pmeGpu->common->bsp_mod[i].data(),
src/gromacs/ewald/pme_gpu_internal.cpp:               pmeGpu->common->bsp_mod[i].size() * sizeof(float));
src/gromacs/ewald/pme_gpu_internal.cpp:    copyToDeviceBuffer(&pmeGpu->kernelParams->grid.d_splineModuli[gridIndex],
src/gromacs/ewald/pme_gpu_internal.cpp:                       pmeGpu->staging.h_splineModuli[gridIndex].data(),
src/gromacs/ewald/pme_gpu_internal.cpp:                       pmeGpu->archSpecific->pmeStream_,
src/gromacs/ewald/pme_gpu_internal.cpp:                       pmeGpu->settings.transferKind,
src/gromacs/ewald/pme_gpu_internal.cpp:void pme_gpu_free_bspline_values(PmeGpu* pmeGpu)
src/gromacs/ewald/pme_gpu_internal.cpp:    for (int gridIndex = 0; gridIndex < pmeGpu->common->ngrids; gridIndex++)
src/gromacs/ewald/pme_gpu_internal.cpp:        pmeGpu->staging.h_splineModuli[gridIndex].clear();
src/gromacs/ewald/pme_gpu_internal.cpp:        freeDeviceBuffer(&pmeGpu->kernelParams->grid.d_splineModuli[gridIndex]);
src/gromacs/ewald/pme_gpu_internal.cpp:void pme_gpu_realloc_forces(PmeGpu* pmeGpu)
src/gromacs/ewald/pme_gpu_internal.cpp:            pmeGpu->useNvshmem ? pmeGpu->nvshmemParams->nAtomsAlloc_symmetric : pmeGpu->nAtomsAlloc;
src/gromacs/ewald/pme_gpu_internal.cpp:    GMX_ASSERT(newForcesSize > 0, "Bad number of atoms in PME GPU");
src/gromacs/ewald/pme_gpu_internal.cpp:    reallocateDeviceBuffer(&pmeGpu->kernelParams->atoms.d_forces,
src/gromacs/ewald/pme_gpu_internal.cpp:                           &pmeGpu->archSpecific->forcesSize,
src/gromacs/ewald/pme_gpu_internal.cpp:                           &pmeGpu->archSpecific->forcesSizeAlloc,
src/gromacs/ewald/pme_gpu_internal.cpp:                           pmeGpu->archSpecific->deviceContext_,
src/gromacs/ewald/pme_gpu_internal.cpp:                           pmeGpu->useNvshmem);
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->staging.h_forces.reserveWithPadding(pmeGpu->nAtomsAlloc);
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->staging.h_forces.resizeWithPadding(pmeGpu->kernelParams->atoms.nAtoms);
src/gromacs/ewald/pme_gpu_internal.cpp:void pme_gpu_free_forces(const PmeGpu* pmeGpu)
src/gromacs/ewald/pme_gpu_internal.cpp:    freeDeviceBuffer(&pmeGpu->kernelParams->atoms.d_forces);
src/gromacs/ewald/pme_gpu_internal.cpp:void pme_gpu_copy_input_forces(PmeGpu* pmeGpu)
src/gromacs/ewald/pme_gpu_internal.cpp:    GMX_ASSERT(pmeGpu->kernelParams->atoms.nAtoms > 0, "Bad number of atoms in PME GPU");
src/gromacs/ewald/pme_gpu_internal.cpp:    copyToDeviceBuffer(&pmeGpu->kernelParams->atoms.d_forces,
src/gromacs/ewald/pme_gpu_internal.cpp:                       pmeGpu->staging.h_forces.data(),
src/gromacs/ewald/pme_gpu_internal.cpp:                       pmeGpu->kernelParams->atoms.nAtoms,
src/gromacs/ewald/pme_gpu_internal.cpp:                       pmeGpu->archSpecific->pmeStream_,
src/gromacs/ewald/pme_gpu_internal.cpp:                       pmeGpu->settings.transferKind,
src/gromacs/ewald/pme_gpu_internal.cpp:void pme_gpu_copy_output_forces(PmeGpu* pmeGpu)
src/gromacs/ewald/pme_gpu_internal.cpp:    GMX_ASSERT(pmeGpu->kernelParams->atoms.nAtoms > 0, "Bad number of atoms in PME GPU");
src/gromacs/ewald/pme_gpu_internal.cpp:    copyFromDeviceBuffer(pmeGpu->staging.h_forces.data(),
src/gromacs/ewald/pme_gpu_internal.cpp:                         &pmeGpu->kernelParams->atoms.d_forces,
src/gromacs/ewald/pme_gpu_internal.cpp:                         pmeGpu->kernelParams->atoms.nAtoms,
src/gromacs/ewald/pme_gpu_internal.cpp:                         pmeGpu->archSpecific->pmeStream_,
src/gromacs/ewald/pme_gpu_internal.cpp:                         pmeGpu->settings.transferKind,
src/gromacs/ewald/pme_gpu_internal.cpp:void pme_gpu_realloc_and_copy_input_coefficients(const PmeGpu* pmeGpu,
src/gromacs/ewald/pme_gpu_internal.cpp:    const size_t newCoefficientsSize = pmeGpu->nAtomsAlloc;
src/gromacs/ewald/pme_gpu_internal.cpp:    reallocateDeviceBuffer(&pmeGpu->kernelParams->atoms.d_coefficients[gridIndex],
src/gromacs/ewald/pme_gpu_internal.cpp:                           &pmeGpu->archSpecific->coefficientsSize[gridIndex],
src/gromacs/ewald/pme_gpu_internal.cpp:                           &pmeGpu->archSpecific->coefficientsCapacity[gridIndex],
src/gromacs/ewald/pme_gpu_internal.cpp:                           pmeGpu->archSpecific->deviceContext_);
src/gromacs/ewald/pme_gpu_internal.cpp:        GMX_ASSERT(h_coefficients, "Bad host-side charge buffer in PME GPU");
src/gromacs/ewald/pme_gpu_internal.cpp:        GMX_ASSERT(newCoefficientsSize > 0, "Bad number of atoms in PME GPU");
src/gromacs/ewald/pme_gpu_internal.cpp:        copyToDeviceBuffer(&pmeGpu->kernelParams->atoms.d_coefficients[gridIndex],
src/gromacs/ewald/pme_gpu_internal.cpp:                           pmeGpu->kernelParams->atoms.nAtoms,
src/gromacs/ewald/pme_gpu_internal.cpp:                           pmeGpu->archSpecific->pmeStream_,
src/gromacs/ewald/pme_gpu_internal.cpp:                           pmeGpu->settings.transferKind,
src/gromacs/ewald/pme_gpu_internal.cpp:        const size_t paddingIndex = pmeGpu->kernelParams->atoms.nAtoms;
src/gromacs/ewald/pme_gpu_internal.cpp:        const size_t paddingCount = pmeGpu->nAtomsAlloc - paddingIndex;
src/gromacs/ewald/pme_gpu_internal.cpp:            clearDeviceBufferAsync(&pmeGpu->kernelParams->atoms.d_coefficients[gridIndex],
src/gromacs/ewald/pme_gpu_internal.cpp:                                   pmeGpu->archSpecific->pmeStream_);
src/gromacs/ewald/pme_gpu_internal.cpp:void pme_gpu_free_coefficients(const PmeGpu* pmeGpu)
src/gromacs/ewald/pme_gpu_internal.cpp:    for (int gridIndex = 0; gridIndex < pmeGpu->common->ngrids; gridIndex++)
src/gromacs/ewald/pme_gpu_internal.cpp:        freeDeviceBuffer(&pmeGpu->kernelParams->atoms.d_coefficients[gridIndex]);
src/gromacs/ewald/pme_gpu_internal.cpp:void pme_gpu_realloc_spline_data(PmeGpu* pmeGpu)
src/gromacs/ewald/pme_gpu_internal.cpp:    const int order             = pmeGpu->common->pme_order;
src/gromacs/ewald/pme_gpu_internal.cpp:    const int newSplineDataSize = DIM * order * pmeGpu->nAtomsAlloc;
src/gromacs/ewald/pme_gpu_internal.cpp:    GMX_ASSERT(newSplineDataSize > 0, "Bad number of atoms in PME GPU");
src/gromacs/ewald/pme_gpu_internal.cpp:    const bool shouldRealloc        = (newSplineDataSize > pmeGpu->archSpecific->splineDataSize);
src/gromacs/ewald/pme_gpu_internal.cpp:    int        currentSizeTemp      = pmeGpu->archSpecific->splineDataSize;
src/gromacs/ewald/pme_gpu_internal.cpp:    int        currentSizeTempAlloc = pmeGpu->archSpecific->splineDataSizeAlloc;
src/gromacs/ewald/pme_gpu_internal.cpp:    reallocateDeviceBuffer(&pmeGpu->kernelParams->atoms.d_theta,
src/gromacs/ewald/pme_gpu_internal.cpp:                           pmeGpu->archSpecific->deviceContext_);
src/gromacs/ewald/pme_gpu_internal.cpp:    reallocateDeviceBuffer(&pmeGpu->kernelParams->atoms.d_dtheta,
src/gromacs/ewald/pme_gpu_internal.cpp:                           &pmeGpu->archSpecific->splineDataSize,
src/gromacs/ewald/pme_gpu_internal.cpp:                           &pmeGpu->archSpecific->splineDataSizeAlloc,
src/gromacs/ewald/pme_gpu_internal.cpp:                           pmeGpu->archSpecific->deviceContext_);
src/gromacs/ewald/pme_gpu_internal.cpp:        changePinningPolicy(&pmeGpu->staging.h_theta, gmx::PinningPolicy::PinnedIfSupported);
src/gromacs/ewald/pme_gpu_internal.cpp:        pmeGpu->staging.h_theta.resize(newSplineDataSize);
src/gromacs/ewald/pme_gpu_internal.cpp:        changePinningPolicy(&pmeGpu->staging.h_dtheta, gmx::PinningPolicy::PinnedIfSupported);
src/gromacs/ewald/pme_gpu_internal.cpp:        pmeGpu->staging.h_dtheta.resize(newSplineDataSize);
src/gromacs/ewald/pme_gpu_internal.cpp:void pme_gpu_free_spline_data(PmeGpu* pmeGpu)
src/gromacs/ewald/pme_gpu_internal.cpp:    freeDeviceBuffer(&pmeGpu->kernelParams->atoms.d_theta);
src/gromacs/ewald/pme_gpu_internal.cpp:    freeDeviceBuffer(&pmeGpu->kernelParams->atoms.d_dtheta);
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->staging.h_theta.clear();
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->staging.h_dtheta.clear();
src/gromacs/ewald/pme_gpu_internal.cpp:void pme_gpu_realloc_grid_indices(PmeGpu* pmeGpu)
src/gromacs/ewald/pme_gpu_internal.cpp:    const size_t newIndicesSize = DIM * pmeGpu->nAtomsAlloc;
src/gromacs/ewald/pme_gpu_internal.cpp:    GMX_ASSERT(newIndicesSize > 0, "Bad number of atoms in PME GPU");
src/gromacs/ewald/pme_gpu_internal.cpp:    reallocateDeviceBuffer(&pmeGpu->kernelParams->atoms.d_gridlineIndices,
src/gromacs/ewald/pme_gpu_internal.cpp:                           &pmeGpu->archSpecific->gridlineIndicesSize,
src/gromacs/ewald/pme_gpu_internal.cpp:                           &pmeGpu->archSpecific->gridlineIndicesSizeAlloc,
src/gromacs/ewald/pme_gpu_internal.cpp:                           pmeGpu->archSpecific->deviceContext_);
src/gromacs/ewald/pme_gpu_internal.cpp:    changePinningPolicy(&pmeGpu->staging.h_gridlineIndices, gmx::PinningPolicy::PinnedIfSupported);
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->staging.h_gridlineIndices.resize(newIndicesSize);
src/gromacs/ewald/pme_gpu_internal.cpp:void pme_gpu_free_grid_indices(PmeGpu* pmeGpu)
src/gromacs/ewald/pme_gpu_internal.cpp:    freeDeviceBuffer(&pmeGpu->kernelParams->atoms.d_gridlineIndices);
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->staging.h_gridlineIndices.clear();
src/gromacs/ewald/pme_gpu_internal.cpp:void pme_gpu_realloc_grids(PmeGpu* pmeGpu)
src/gromacs/ewald/pme_gpu_internal.cpp:    auto* kernelParamsPtr = pmeGpu->kernelParams.get();
src/gromacs/ewald/pme_gpu_internal.cpp:    for (int gridIndex = 0; gridIndex < pmeGpu->common->ngrids; gridIndex++)
src/gromacs/ewald/pme_gpu_internal.cpp:        if (pmeGpu->archSpecific->performOutOfPlaceFFT)
src/gromacs/ewald/pme_gpu_internal.cpp:                                   &pmeGpu->archSpecific->realGridSize[gridIndex],
src/gromacs/ewald/pme_gpu_internal.cpp:                                   &pmeGpu->archSpecific->realGridCapacity[gridIndex],
src/gromacs/ewald/pme_gpu_internal.cpp:                                   pmeGpu->archSpecific->deviceContext_);
src/gromacs/ewald/pme_gpu_internal.cpp:            pmeGpu->archSpecific->complexGridSize[gridIndex] = newComplexGridSize;
src/gromacs/ewald/pme_gpu_internal.cpp:                                   &pmeGpu->archSpecific->realGridSize[gridIndex],
src/gromacs/ewald/pme_gpu_internal.cpp:                                   &pmeGpu->archSpecific->realGridCapacity[gridIndex],
src/gromacs/ewald/pme_gpu_internal.cpp:                                   pmeGpu->archSpecific->deviceContext_);
src/gromacs/ewald/pme_gpu_internal.cpp:            pmeGpu->archSpecific->complexGridSize[gridIndex] =
src/gromacs/ewald/pme_gpu_internal.cpp:                    pmeGpu->archSpecific->realGridSize[gridIndex];
src/gromacs/ewald/pme_gpu_internal.cpp:void pme_gpu_free_grids(const PmeGpu* pmeGpu)
src/gromacs/ewald/pme_gpu_internal.cpp:    for (int gridIndex = 0; gridIndex < pmeGpu->common->ngrids; gridIndex++)
src/gromacs/ewald/pme_gpu_internal.cpp:        freeDeviceBuffer(&pmeGpu->kernelParams->grid.d_realGrid[gridIndex]);
src/gromacs/ewald/pme_gpu_internal.cpp:void pme_gpu_reinit_haloexchange(PmeGpu* pmeGpu)
src/gromacs/ewald/pme_gpu_internal.cpp:    int rankX = pmeGpu->common->nodeidX;
src/gromacs/ewald/pme_gpu_internal.cpp:    int sizeX = pmeGpu->common->nnodesX;
src/gromacs/ewald/pme_gpu_internal.cpp:    int rankY = pmeGpu->common->nodeidY;
src/gromacs/ewald/pme_gpu_internal.cpp:    int sizeY = pmeGpu->common->nnodesY;
src/gromacs/ewald/pme_gpu_internal.cpp:    int myGridX = pmeGpu->common->s2g0X[rankX + 1] - pmeGpu->common->s2g0X[rankX];
src/gromacs/ewald/pme_gpu_internal.cpp:    int myGridY = pmeGpu->common->s2g0Y[rankY + 1] - pmeGpu->common->s2g0Y[rankY];
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->haloExchange->gridSizeX = myGridX;
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->haloExchange->gridSizeY = myGridY;
src/gromacs/ewald/pme_gpu_internal.cpp:    // This can be different from pmeGpu->common->gridHalo
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->haloExchange->haloSizeX[gmx::DirectionX::Up]     = 0;
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->haloExchange->haloSizeX[gmx::DirectionX::Down]   = 0;
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->haloExchange->haloSizeX[gmx::DirectionX::Center] = 0;
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->haloExchange->haloSizeY[gmx::DirectionY::Left]   = 0;
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->haloExchange->haloSizeY[gmx::DirectionY::Right]  = 0;
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->haloExchange->haloSizeY[gmx::DirectionY::Center] = 0;
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->haloExchange->ranksX[gmx::DirectionX::Up]     = up;
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->haloExchange->ranksX[gmx::DirectionX::Down]   = down;
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->haloExchange->ranksX[gmx::DirectionX::Center] = rankX;
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->haloExchange->ranksY[gmx::DirectionY::Left]   = left;
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->haloExchange->ranksY[gmx::DirectionY::Right]  = right;
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->haloExchange->ranksY[gmx::DirectionY::Center] = rankY;
src/gromacs/ewald/pme_gpu_internal.cpp:    auto* kernelParamsPtr = pmeGpu->kernelParams.get();
src/gromacs/ewald/pme_gpu_internal.cpp:    if (pmeGpu->common->nnodesX > 1)
src/gromacs/ewald/pme_gpu_internal.cpp:        int downGrid = pmeGpu->common->s2g0X[down + 1] - pmeGpu->common->s2g0X[down];
src/gromacs/ewald/pme_gpu_internal.cpp:        int upGrid   = pmeGpu->common->s2g0X[up + 1] - pmeGpu->common->s2g0X[up];
src/gromacs/ewald/pme_gpu_internal.cpp:        overlapX = pmeGpu->common->gridHalo;
src/gromacs/ewald/pme_gpu_internal.cpp:        pmeGpu->haloExchange->haloSizeX[gmx::DirectionX::Up]     = overlapUp;
src/gromacs/ewald/pme_gpu_internal.cpp:        pmeGpu->haloExchange->haloSizeX[gmx::DirectionX::Down]   = overlapDown;
src/gromacs/ewald/pme_gpu_internal.cpp:        pmeGpu->haloExchange->haloSizeX[gmx::DirectionX::Center] = overlapX;
src/gromacs/ewald/pme_gpu_internal.cpp:            reallocateDeviceBuffer(&pmeGpu->haloExchange->d_sendGrids[i][gmx::DirectionY::Center],
src/gromacs/ewald/pme_gpu_internal.cpp:                                   &pmeGpu->haloExchange->overlapSendSize[i][gmx::DirectionY::Center],
src/gromacs/ewald/pme_gpu_internal.cpp:                                   &pmeGpu->haloExchange->overlapSendCapacity[i][gmx::DirectionY::Center],
src/gromacs/ewald/pme_gpu_internal.cpp:                                   pmeGpu->archSpecific->deviceContext_);
src/gromacs/ewald/pme_gpu_internal.cpp:            reallocateDeviceBuffer(&pmeGpu->haloExchange->d_recvGrids[i][gmx::DirectionY::Center],
src/gromacs/ewald/pme_gpu_internal.cpp:                                   &pmeGpu->haloExchange->overlapRecvSize[i][gmx::DirectionY::Center],
src/gromacs/ewald/pme_gpu_internal.cpp:                                   &pmeGpu->haloExchange->overlapRecvCapacity[i][gmx::DirectionY::Center],
src/gromacs/ewald/pme_gpu_internal.cpp:                                   pmeGpu->archSpecific->deviceContext_);
src/gromacs/ewald/pme_gpu_internal.cpp:    if (pmeGpu->common->nnodesY > 1)
src/gromacs/ewald/pme_gpu_internal.cpp:        int rightGrid = pmeGpu->common->s2g0Y[right + 1] - pmeGpu->common->s2g0Y[right];
src/gromacs/ewald/pme_gpu_internal.cpp:        int leftGrid  = pmeGpu->common->s2g0Y[left + 1] - pmeGpu->common->s2g0Y[left];
src/gromacs/ewald/pme_gpu_internal.cpp:        overlapY = pmeGpu->common->gridHalo;
src/gromacs/ewald/pme_gpu_internal.cpp:        pmeGpu->haloExchange->haloSizeY[gmx::DirectionY::Left]   = overlapLeft;
src/gromacs/ewald/pme_gpu_internal.cpp:        pmeGpu->haloExchange->haloSizeY[gmx::DirectionY::Right]  = overlapRight;
src/gromacs/ewald/pme_gpu_internal.cpp:        pmeGpu->haloExchange->haloSizeY[gmx::DirectionY::Center] = overlapY;
src/gromacs/ewald/pme_gpu_internal.cpp:            reallocateDeviceBuffer(&pmeGpu->haloExchange->d_sendGrids[gmx::DirectionX::Center][i],
src/gromacs/ewald/pme_gpu_internal.cpp:                                   &pmeGpu->haloExchange->overlapSendSize[gmx::DirectionX::Center][i],
src/gromacs/ewald/pme_gpu_internal.cpp:                                   &pmeGpu->haloExchange->overlapSendCapacity[gmx::DirectionX::Center][i],
src/gromacs/ewald/pme_gpu_internal.cpp:                                   pmeGpu->archSpecific->deviceContext_);
src/gromacs/ewald/pme_gpu_internal.cpp:            reallocateDeviceBuffer(&pmeGpu->haloExchange->d_recvGrids[gmx::DirectionX::Center][i],
src/gromacs/ewald/pme_gpu_internal.cpp:                                   &pmeGpu->haloExchange->overlapRecvSize[gmx::DirectionX::Center][i],
src/gromacs/ewald/pme_gpu_internal.cpp:                                   &pmeGpu->haloExchange->overlapRecvCapacity[gmx::DirectionX::Center][i],
src/gromacs/ewald/pme_gpu_internal.cpp:                                   pmeGpu->archSpecific->deviceContext_);
src/gromacs/ewald/pme_gpu_internal.cpp:    if (pmeGpu->common->nnodesX > 1 && pmeGpu->common->nnodesY > 1)
src/gromacs/ewald/pme_gpu_internal.cpp:                reallocateDeviceBuffer(&pmeGpu->haloExchange->d_sendGrids[i][j],
src/gromacs/ewald/pme_gpu_internal.cpp:                                       &pmeGpu->haloExchange->overlapSendSize[i][j],
src/gromacs/ewald/pme_gpu_internal.cpp:                                       &pmeGpu->haloExchange->overlapSendCapacity[i][j],
src/gromacs/ewald/pme_gpu_internal.cpp:                                       pmeGpu->archSpecific->deviceContext_);
src/gromacs/ewald/pme_gpu_internal.cpp:                reallocateDeviceBuffer(&pmeGpu->haloExchange->d_recvGrids[i][j],
src/gromacs/ewald/pme_gpu_internal.cpp:                                       &pmeGpu->haloExchange->overlapRecvSize[i][j],
src/gromacs/ewald/pme_gpu_internal.cpp:                                       &pmeGpu->haloExchange->overlapRecvCapacity[i][j],
src/gromacs/ewald/pme_gpu_internal.cpp:                                       pmeGpu->archSpecific->deviceContext_);
src/gromacs/ewald/pme_gpu_internal.cpp:void pme_gpu_free_haloexchange(const PmeGpu* pmeGpu)
src/gromacs/ewald/pme_gpu_internal.cpp:            freeDeviceBuffer(&pmeGpu->haloExchange->d_sendGrids[i][j]);
src/gromacs/ewald/pme_gpu_internal.cpp:            freeDeviceBuffer(&pmeGpu->haloExchange->d_recvGrids[i][j]);
src/gromacs/ewald/pme_gpu_internal.cpp:void pme_gpu_clear_grids(const PmeGpu* pmeGpu)
src/gromacs/ewald/pme_gpu_internal.cpp:    for (int gridIndex = 0; gridIndex < pmeGpu->common->ngrids; gridIndex++)
src/gromacs/ewald/pme_gpu_internal.cpp:        clearDeviceBufferAsync(&pmeGpu->kernelParams->grid.d_realGrid[gridIndex],
src/gromacs/ewald/pme_gpu_internal.cpp:                               pmeGpu->archSpecific->realGridSize[gridIndex],
src/gromacs/ewald/pme_gpu_internal.cpp:                               pmeGpu->archSpecific->pmeStream_);
src/gromacs/ewald/pme_gpu_internal.cpp:void pme_gpu_realloc_and_copy_fract_shifts(PmeGpu* pmeGpu)
src/gromacs/ewald/pme_gpu_internal.cpp:    pme_gpu_free_fract_shifts(pmeGpu);
src/gromacs/ewald/pme_gpu_internal.cpp:    auto* kernelParamsPtr = pmeGpu->kernelParams.get();
src/gromacs/ewald/pme_gpu_internal.cpp:    const int nx                  = pmeGpu->common->nk[XX];
src/gromacs/ewald/pme_gpu_internal.cpp:    const int ny                  = pmeGpu->common->nk[YY];
src/gromacs/ewald/pme_gpu_internal.cpp:    const int nz                  = pmeGpu->common->nk[ZZ];
src/gromacs/ewald/pme_gpu_internal.cpp:                         pmeGpu->common->fsh.data(),
src/gromacs/ewald/pme_gpu_internal.cpp:                         pmeGpu->archSpecific->deviceContext_,
src/gromacs/ewald/pme_gpu_internal.cpp:                         pmeGpu->archSpecific->pmeStream_);
src/gromacs/ewald/pme_gpu_internal.cpp:                         pmeGpu->common->nn.data(),
src/gromacs/ewald/pme_gpu_internal.cpp:                         pmeGpu->archSpecific->deviceContext_,
src/gromacs/ewald/pme_gpu_internal.cpp:                         pmeGpu->archSpecific->pmeStream_);
src/gromacs/ewald/pme_gpu_internal.cpp:void pme_gpu_free_fract_shifts(const PmeGpu* pmeGpu)
src/gromacs/ewald/pme_gpu_internal.cpp:    auto* kernelParamsPtr = pmeGpu->kernelParams.get();
src/gromacs/ewald/pme_gpu_internal.cpp:#if GMX_GPU_CUDA
src/gromacs/ewald/pme_gpu_internal.cpp:#elif GMX_GPU_OPENCL || GMX_GPU_SYCL
src/gromacs/ewald/pme_gpu_internal.cpp:bool pme_gpu_stream_query(const PmeGpu* pmeGpu)
src/gromacs/ewald/pme_gpu_internal.cpp:    return haveStreamTasksCompleted(pmeGpu->archSpecific->pmeStream_);
src/gromacs/ewald/pme_gpu_internal.cpp:void pme_gpu_copy_input_gather_grid(const PmeGpu* pmeGpu, const float* h_grid, const int gridIndex)
src/gromacs/ewald/pme_gpu_internal.cpp:    copyToDeviceBuffer(&pmeGpu->kernelParams->grid.d_realGrid[gridIndex],
src/gromacs/ewald/pme_gpu_internal.cpp:                       pmeGpu->archSpecific->realGridSize[gridIndex],
src/gromacs/ewald/pme_gpu_internal.cpp:                       pmeGpu->archSpecific->pmeStream_,
src/gromacs/ewald/pme_gpu_internal.cpp:                       pmeGpu->settings.transferKind,
src/gromacs/ewald/pme_gpu_internal.cpp:void pme_gpu_copy_output_spread_grid(const PmeGpu* pmeGpu, float* h_grid, const int gridIndex)
src/gromacs/ewald/pme_gpu_internal.cpp:                         &pmeGpu->kernelParams->grid.d_realGrid[gridIndex],
src/gromacs/ewald/pme_gpu_internal.cpp:                         pmeGpu->archSpecific->realGridSize[gridIndex],
src/gromacs/ewald/pme_gpu_internal.cpp:                         pmeGpu->archSpecific->pmeStream_,
src/gromacs/ewald/pme_gpu_internal.cpp:                         pmeGpu->settings.transferKind,
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->archSpecific->syncSpreadGridD2H.markEvent(pmeGpu->archSpecific->pmeStream_);
src/gromacs/ewald/pme_gpu_internal.cpp:void pme_gpu_copy_output_spread_atom_data(PmeGpu* pmeGpu)
src/gromacs/ewald/pme_gpu_internal.cpp:    auto*     kernelParamsPtr = pmeGpu->kernelParams.get();
src/gromacs/ewald/pme_gpu_internal.cpp:    const int nAtoms          = pmeGpu->kernelParams->atoms.nAtoms;
src/gromacs/ewald/pme_gpu_internal.cpp:    copyFromDeviceBuffer(pmeGpu->staging.h_dtheta.data(),
src/gromacs/ewald/pme_gpu_internal.cpp:                         pmeGpu->archSpecific->splineCountActive,
src/gromacs/ewald/pme_gpu_internal.cpp:                         pmeGpu->archSpecific->pmeStream_,
src/gromacs/ewald/pme_gpu_internal.cpp:                         pmeGpu->settings.transferKind,
src/gromacs/ewald/pme_gpu_internal.cpp:    copyFromDeviceBuffer(pmeGpu->staging.h_theta.data(),
src/gromacs/ewald/pme_gpu_internal.cpp:                         pmeGpu->archSpecific->splineCountActive,
src/gromacs/ewald/pme_gpu_internal.cpp:                         pmeGpu->archSpecific->pmeStream_,
src/gromacs/ewald/pme_gpu_internal.cpp:                         pmeGpu->settings.transferKind,
src/gromacs/ewald/pme_gpu_internal.cpp:    copyFromDeviceBuffer(pmeGpu->staging.h_gridlineIndices.data(),
src/gromacs/ewald/pme_gpu_internal.cpp:                         pmeGpu->archSpecific->pmeStream_,
src/gromacs/ewald/pme_gpu_internal.cpp:                         pmeGpu->settings.transferKind,
src/gromacs/ewald/pme_gpu_internal.cpp:void pme_gpu_copy_input_gather_atom_data(const PmeGpu* pmeGpu)
src/gromacs/ewald/pme_gpu_internal.cpp:    const size_t splineDataSize  = pmeGpu->archSpecific->splineDataSize;
src/gromacs/ewald/pme_gpu_internal.cpp:    auto*        kernelParamsPtr = pmeGpu->kernelParams.get();
src/gromacs/ewald/pme_gpu_internal.cpp:                           pmeGpu->nAtomsAlloc * DIM,
src/gromacs/ewald/pme_gpu_internal.cpp:                           pmeGpu->archSpecific->pmeStream_);
src/gromacs/ewald/pme_gpu_internal.cpp:            &kernelParamsPtr->atoms.d_dtheta, 0, splineDataSize, pmeGpu->archSpecific->pmeStream_);
src/gromacs/ewald/pme_gpu_internal.cpp:            &kernelParamsPtr->atoms.d_theta, 0, splineDataSize, pmeGpu->archSpecific->pmeStream_);
src/gromacs/ewald/pme_gpu_internal.cpp:                       pmeGpu->staging.h_dtheta.data(),
src/gromacs/ewald/pme_gpu_internal.cpp:                       pmeGpu->archSpecific->pmeStream_,
src/gromacs/ewald/pme_gpu_internal.cpp:                       pmeGpu->settings.transferKind,
src/gromacs/ewald/pme_gpu_internal.cpp:                       pmeGpu->staging.h_theta.data(),
src/gromacs/ewald/pme_gpu_internal.cpp:                       pmeGpu->archSpecific->pmeStream_,
src/gromacs/ewald/pme_gpu_internal.cpp:                       pmeGpu->settings.transferKind,
src/gromacs/ewald/pme_gpu_internal.cpp:                       pmeGpu->staging.h_gridlineIndices.data(),
src/gromacs/ewald/pme_gpu_internal.cpp:                       pmeGpu->archSpecific->pmeStream_,
src/gromacs/ewald/pme_gpu_internal.cpp:                       pmeGpu->settings.transferKind,
src/gromacs/ewald/pme_gpu_internal.cpp:void pme_gpu_sync_spread_grid(const PmeGpu* pmeGpu)
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->archSpecific->syncSpreadGridD2H.waitForEvent();
src/gromacs/ewald/pme_gpu_internal.cpp:/*! \brief Internal GPU initialization for PME.
src/gromacs/ewald/pme_gpu_internal.cpp: * \param[in]  pmeGpu         GPU PME data.
src/gromacs/ewald/pme_gpu_internal.cpp: * \param[in]  deviceContext  GPU context.
src/gromacs/ewald/pme_gpu_internal.cpp: * \param[in]  deviceStream   GPU stream.
src/gromacs/ewald/pme_gpu_internal.cpp:static void pme_gpu_init_internal(PmeGpu* pmeGpu, const DeviceContext& deviceContext, const DeviceStream& deviceStream)
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->archSpecific.reset(new PmeGpuSpecific(deviceContext, deviceStream));
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->kernelParams.reset(new PmeGpuKernelParams());
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->archSpecific->performOutOfPlaceFFT =
src/gromacs/ewald/pme_gpu_internal.cpp:            !((pmeGpu->settings.useDecomposition && GMX_USE_cuFFTMp) || GMX_GPU_FFT_BBFFT);
src/gromacs/ewald/pme_gpu_internal.cpp:#if GMX_GPU_CUDA || GMX_GPU_SYCL
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->kernelParams->usePipeline       = char(false);
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->kernelParams->pipelineAtomStart = 0;
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->kernelParams->pipelineAtomEnd   = 0;
src/gromacs/ewald/pme_gpu_internal.cpp:#if GMX_GPU_CUDA
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->maxGridWidthX = deviceContext.deviceInfo().prop.maxGridSize[0];
src/gromacs/ewald/pme_gpu_internal.cpp:    // Use this path for any non-CUDA GPU acceleration
src/gromacs/ewald/pme_gpu_internal.cpp:    // TODO: is there no really global work size limit in OpenCL?
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->maxGridWidthX = INT32_MAX / 2;
src/gromacs/ewald/pme_gpu_internal.cpp:    if (pmeGpu->settings.useDecomposition)
src/gromacs/ewald/pme_gpu_internal.cpp:        pmeGpu->haloExchange = std::make_unique<PmeGpuHaloExchange>();
src/gromacs/ewald/pme_gpu_internal.cpp:                pmeGpu->haloExchange->d_sendGrids[i][j] = nullptr;
src/gromacs/ewald/pme_gpu_internal.cpp:                pmeGpu->haloExchange->d_recvGrids[i][j] = nullptr;
src/gromacs/ewald/pme_gpu_internal.cpp:                pmeGpu->haloExchange->overlapSendSize[i][j] = 0;
src/gromacs/ewald/pme_gpu_internal.cpp:                pmeGpu->haloExchange->overlapRecvSize[i][j] = 0;
src/gromacs/ewald/pme_gpu_internal.cpp:                pmeGpu->haloExchange->overlapSendCapacity[i][j] = 0;
src/gromacs/ewald/pme_gpu_internal.cpp:                pmeGpu->haloExchange->overlapRecvCapacity[i][j] = 0;
src/gromacs/ewald/pme_gpu_internal.cpp: * Wrapper for getting FFT backend depending on Gromacs GPU backend compilation flags.
src/gromacs/ewald/pme_gpu_internal.cpp: * \param[in] pmeGpu  The PME GPU structure.
src/gromacs/ewald/pme_gpu_internal.cpp:static gmx::FftBackend getFftBackend(const PmeGpu* pmeGpu)
src/gromacs/ewald/pme_gpu_internal.cpp:    if (GMX_GPU_CUDA)
src/gromacs/ewald/pme_gpu_internal.cpp:        if (!pmeGpu->settings.useDecomposition)
src/gromacs/ewald/pme_gpu_internal.cpp:                return gmx::FftBackend::HeFFTe_CUDA;
src/gromacs/ewald/pme_gpu_internal.cpp:                        "Gromacs must be built with cuFFTMp or Heffte to enable GPU-based "
src/gromacs/ewald/pme_gpu_internal.cpp:                        "PME decomposition on CUDA-compatible GPUs");
src/gromacs/ewald/pme_gpu_internal.cpp:    else if (GMX_GPU_OPENCL)
src/gromacs/ewald/pme_gpu_internal.cpp:        if (GMX_GPU_FFT_VKFFT)
src/gromacs/ewald/pme_gpu_internal.cpp:            GMX_RELEASE_ASSERT(GMX_GPU_FFT_CLFFT, "Only clFFT and VkFFT are supported with OpenCL");
src/gromacs/ewald/pme_gpu_internal.cpp:    else if (GMX_GPU_SYCL)
src/gromacs/ewald/pme_gpu_internal.cpp:        if (GMX_GPU_FFT_MKL)
src/gromacs/ewald/pme_gpu_internal.cpp:            if (!pmeGpu->settings.useDecomposition)
src/gromacs/ewald/pme_gpu_internal.cpp:                        "GROMACS must be built with HeFFTe to enable fully GPU-offloaded "
src/gromacs/ewald/pme_gpu_internal.cpp:                        "PME decomposition on oneAPI-compatible GPUs"));
src/gromacs/ewald/pme_gpu_internal.cpp:        else if (GMX_GPU_FFT_ONEMKL)
src/gromacs/ewald/pme_gpu_internal.cpp:        else if (GMX_GPU_FFT_BBFFT)
src/gromacs/ewald/pme_gpu_internal.cpp:        else if (GMX_GPU_FFT_ROCFFT)
src/gromacs/ewald/pme_gpu_internal.cpp:            if (!pmeGpu->settings.useDecomposition)
src/gromacs/ewald/pme_gpu_internal.cpp:                        "GROMACS must be built with HeFFTe to enable fully GPU-offloaded "
src/gromacs/ewald/pme_gpu_internal.cpp:                        "PME decomposition on ROCm-compatible GPUs"));
src/gromacs/ewald/pme_gpu_internal.cpp:        else if (GMX_GPU_FFT_CUFFT)
src/gromacs/ewald/pme_gpu_internal.cpp:                if (pmeGpu->settings.useDecomposition)
src/gromacs/ewald/pme_gpu_internal.cpp:                            "GROMACS can only do multi-GPU FFT in SYCL+cuFFT+HeFFTe build"));
src/gromacs/ewald/pme_gpu_internal.cpp:        else if (GMX_GPU_FFT_VKFFT)
src/gromacs/ewald/pme_gpu_internal.cpp:        GMX_RELEASE_ASSERT(false, "Unknown GPU backend");
src/gromacs/ewald/pme_gpu_internal.cpp:void pme_gpu_reinit_3dfft(const PmeGpu* pmeGpu)
src/gromacs/ewald/pme_gpu_internal.cpp:    if (pme_gpu_settings(pmeGpu).performGPUFFT)
src/gromacs/ewald/pme_gpu_internal.cpp:        pmeGpu->archSpecific->fftSetup.resize(0);
src/gromacs/ewald/pme_gpu_internal.cpp:        const bool       performOutOfPlaceFFT = pmeGpu->archSpecific->performOutOfPlaceFFT;
src/gromacs/ewald/pme_gpu_internal.cpp:        MPI_Comm         comm                 = pmeGpu->common->mpiComm;
src/gromacs/ewald/pme_gpu_internal.cpp:        std::vector<int> gridSizesInXForEachRank(pmeGpu->common->nnodesX);
src/gromacs/ewald/pme_gpu_internal.cpp:        std::vector<int> gridSizesInYForEachRank(pmeGpu->common->nnodesY);
src/gromacs/ewald/pme_gpu_internal.cpp:        for (int i = 0; i < pmeGpu->common->nnodesX; ++i)
src/gromacs/ewald/pme_gpu_internal.cpp:            gridSizesInXForEachRank[i] = pmeGpu->common->s2g0X[i + 1] - pmeGpu->common->s2g0X[i];
src/gromacs/ewald/pme_gpu_internal.cpp:        for (int i = 0; i < pmeGpu->common->nnodesY; ++i)
src/gromacs/ewald/pme_gpu_internal.cpp:            gridSizesInYForEachRank[i] = pmeGpu->common->s2g0Y[i + 1] - pmeGpu->common->s2g0Y[i];
src/gromacs/ewald/pme_gpu_internal.cpp:        const bool allocateRealGrid = pmeGpu->settings.useDecomposition;
src/gromacs/ewald/pme_gpu_internal.cpp:        const gmx::FftBackend backend = getFftBackend(pmeGpu);
src/gromacs/ewald/pme_gpu_internal.cpp:        PmeGpuGridParams& grid = pme_gpu_get_kernel_params_base_ptr(pmeGpu)->grid;
src/gromacs/ewald/pme_gpu_internal.cpp:        for (int gridIndex = 0; gridIndex < pmeGpu->common->ngrids; gridIndex++)
src/gromacs/ewald/pme_gpu_internal.cpp:            const bool useDecomposition = pmeGpu->settings.useDecomposition;
src/gromacs/ewald/pme_gpu_internal.cpp:                memcpy(pmeGpu->archSpecific->localRealGridSize, grid.realGridSize, DIM * sizeof(int));
src/gromacs/ewald/pme_gpu_internal.cpp:                memcpy(pmeGpu->archSpecific->localRealGridSizePadded,
src/gromacs/ewald/pme_gpu_internal.cpp:                pmeGpu->archSpecific->d_fftRealGrid[gridIndex] = grid.d_realGrid[gridIndex];
src/gromacs/ewald/pme_gpu_internal.cpp:            pmeGpu->archSpecific->fftSetup.push_back(std::make_unique<gmx::Gpu3dFft>(
src/gromacs/ewald/pme_gpu_internal.cpp:                    pmeGpu->archSpecific->deviceContext_,
src/gromacs/ewald/pme_gpu_internal.cpp:                    pmeGpu->archSpecific->pmeStream_,
src/gromacs/ewald/pme_gpu_internal.cpp:                    pmeGpu->archSpecific->localRealGridSize,
src/gromacs/ewald/pme_gpu_internal.cpp:                    pmeGpu->archSpecific->localRealGridSizePadded,
src/gromacs/ewald/pme_gpu_internal.cpp:                    &(pmeGpu->archSpecific->d_fftRealGrid[gridIndex]),
src/gromacs/ewald/pme_gpu_internal.cpp:        // These values needs to be initialized for unit tests which run pme_gpu_solve even in mixed
src/gromacs/ewald/pme_gpu_internal.cpp:        // mode. In real world cases, pme_gpu_solve is never called in mixed mode.
src/gromacs/ewald/pme_gpu_internal.cpp:        PmeGpuGridParams& grid = pme_gpu_get_kernel_params_base_ptr(pmeGpu)->grid;
src/gromacs/ewald/pme_gpu_internal.cpp:        for (int gridIndex = 0; gridIndex < pmeGpu->common->ngrids; gridIndex++)
src/gromacs/ewald/pme_gpu_internal.cpp:void pme_gpu_destroy_3dfft(const PmeGpu* pmeGpu)
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->archSpecific->fftSetup.resize(0);
src/gromacs/ewald/pme_gpu_internal.cpp:void pme_gpu_getEnergyAndVirial(const gmx_pme_t& pme, const float lambda, PmeOutput* output)
src/gromacs/ewald/pme_gpu_internal.cpp:    const PmeGpu* pmeGpu = pme.gpu;
src/gromacs/ewald/pme_gpu_internal.cpp:    GMX_ASSERT(lambda == 1.0 || pmeGpu->common->ngrids == 2,
src/gromacs/ewald/pme_gpu_internal.cpp:    for (int gridIndex = 0; gridIndex < pmeGpu->common->ngrids; gridIndex++)
src/gromacs/ewald/pme_gpu_internal.cpp:            GMX_ASSERT(std::isfinite(pmeGpu->staging.h_virialAndEnergy[gridIndex][j]),
src/gromacs/ewald/pme_gpu_internal.cpp:                       "PME GPU produces incorrect energy/virial.");
src/gromacs/ewald/pme_gpu_internal.cpp:    for (int gridIndex = 0; gridIndex < pmeGpu->common->ngrids; gridIndex++)
src/gromacs/ewald/pme_gpu_internal.cpp:        if (pmeGpu->common->ngrids == 2)
src/gromacs/ewald/pme_gpu_internal.cpp:                scale * 0.25F * pmeGpu->staging.h_virialAndEnergy[gridIndex][0];
src/gromacs/ewald/pme_gpu_internal.cpp:                scale * 0.25F * pmeGpu->staging.h_virialAndEnergy[gridIndex][1];
src/gromacs/ewald/pme_gpu_internal.cpp:                scale * 0.25F * pmeGpu->staging.h_virialAndEnergy[gridIndex][2];
src/gromacs/ewald/pme_gpu_internal.cpp:                scale * 0.25F * pmeGpu->staging.h_virialAndEnergy[gridIndex][3];
src/gromacs/ewald/pme_gpu_internal.cpp:                scale * 0.25F * pmeGpu->staging.h_virialAndEnergy[gridIndex][3];
src/gromacs/ewald/pme_gpu_internal.cpp:                scale * 0.25F * pmeGpu->staging.h_virialAndEnergy[gridIndex][4];
src/gromacs/ewald/pme_gpu_internal.cpp:                scale * 0.25F * pmeGpu->staging.h_virialAndEnergy[gridIndex][4];
src/gromacs/ewald/pme_gpu_internal.cpp:                scale * 0.25F * pmeGpu->staging.h_virialAndEnergy[gridIndex][5];
src/gromacs/ewald/pme_gpu_internal.cpp:                scale * 0.25F * pmeGpu->staging.h_virialAndEnergy[gridIndex][5];
src/gromacs/ewald/pme_gpu_internal.cpp:        output->coulombEnergy_ += scale * 0.5F * pmeGpu->staging.h_virialAndEnergy[gridIndex][6];
src/gromacs/ewald/pme_gpu_internal.cpp:    if (pmeGpu->common->ngrids > 1)
src/gromacs/ewald/pme_gpu_internal.cpp:                               * (pmeGpu->staging.h_virialAndEnergy[FEP_STATE_B][6]
src/gromacs/ewald/pme_gpu_internal.cpp:                                  - pmeGpu->staging.h_virialAndEnergy[FEP_STATE_A][6]);
src/gromacs/ewald/pme_gpu_internal.cpp: * \param[in]   pmeGpu      PME GPU data structure
src/gromacs/ewald/pme_gpu_internal.cpp:static void pme_gpu_getForceOutput(PmeGpu* pmeGpu, PmeOutput* output)
src/gromacs/ewald/pme_gpu_internal.cpp:    output->haveForceOutput_ = !pmeGpu->settings.useGpuForceReduction;
src/gromacs/ewald/pme_gpu_internal.cpp:        output->forces_ = pmeGpu->staging.h_forces;
src/gromacs/ewald/pme_gpu_internal.cpp:PmeOutput pme_gpu_getOutput(gmx_pme_t* pme, const bool computeEnergyAndVirial, const real lambdaQ)
src/gromacs/ewald/pme_gpu_internal.cpp:    PmeGpu* pmeGpu = pme->gpu;
src/gromacs/ewald/pme_gpu_internal.cpp:    pme_gpu_getForceOutput(pmeGpu, &output);
src/gromacs/ewald/pme_gpu_internal.cpp:        if (pme_gpu_settings(pmeGpu).performGPUSolve)
src/gromacs/ewald/pme_gpu_internal.cpp:            pme_gpu_getEnergyAndVirial(*pme, lambdaQ, &output);
src/gromacs/ewald/pme_gpu_internal.cpp:void pme_gpu_update_input_box(PmeGpu gmx_unused* pmeGpu, const matrix gmx_unused box)
src/gromacs/ewald/pme_gpu_internal.cpp:    GMX_THROW(gmx::NotImplementedError("PME is implemented for single-precision only on GPU"));
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->common->boxScaler->scaleBox(box, scaledBox);
src/gromacs/ewald/pme_gpu_internal.cpp:    auto* kernelParamsPtr              = pme_gpu_get_kernel_params_base_ptr(pmeGpu);
src/gromacs/ewald/pme_gpu_internal.cpp:    /* The GPU recipBox is transposed as compared to the CPU recipBox.
src/gromacs/ewald/pme_gpu_internal.cpp: * (Re-)initializes all the PME GPU data related to the grid size and cut-off.
src/gromacs/ewald/pme_gpu_internal.cpp: * \param[in] pmeGpu            The PME GPU structure.
src/gromacs/ewald/pme_gpu_internal.cpp:static void pme_gpu_reinit_grids(PmeGpu* pmeGpu)
src/gromacs/ewald/pme_gpu_internal.cpp:    auto* kernelParamsPtr = pme_gpu_get_kernel_params_base_ptr(pmeGpu);
src/gromacs/ewald/pme_gpu_internal.cpp:            pmeGpu->common->ngrids == 1 || pmeGpu->common->ngrids == 2,
src/gromacs/ewald/pme_gpu_internal.cpp:            "Only one (normal Coulomb PME) or two (FEP coulomb PME) PME grids can be used on GPU");
src/gromacs/ewald/pme_gpu_internal.cpp:            (M_PI * M_PI) / (pmeGpu->common->ewaldcoeff_q * pmeGpu->common->ewaldcoeff_q);
src/gromacs/ewald/pme_gpu_internal.cpp:        kernelParamsPtr->grid.realGridSize[i] = pmeGpu->common->nk[i];
src/gromacs/ewald/pme_gpu_internal.cpp:        kernelParamsPtr->grid.realGridSizeFP[i]     = static_cast<float>(pmeGpu->common->nk[i]);
src/gromacs/ewald/pme_gpu_internal.cpp:    const bool needWorkaroundForOneMkl = (GMX_GPU_SYCL != 0) && (GMX_SYCL_DPCPP != 0)
src/gromacs/ewald/pme_gpu_internal.cpp:                                         && pme_gpu_settings(pmeGpu).performGPUFFT; // Issue #4219.
src/gromacs/ewald/pme_gpu_internal.cpp:    if (!pme_gpu_settings(pmeGpu).performGPUFFT || needWorkaroundForOneMkl)
src/gromacs/ewald/pme_gpu_internal.cpp:        // This allows for GPU spreading grid and CPU fftgrid to have the same layout, so that we can copy the data directly
src/gromacs/ewald/pme_gpu_internal.cpp:    /* GPU FFT: n real elements correspond to (n / 2 + 1) complex elements in minor dimension */
src/gromacs/ewald/pme_gpu_internal.cpp:    pme_gpu_realloc_and_copy_fract_shifts(pmeGpu);
src/gromacs/ewald/pme_gpu_internal.cpp:    for (int gridIndex = 0; gridIndex < pmeGpu->common->ngrids; gridIndex++)
src/gromacs/ewald/pme_gpu_internal.cpp:        pme_gpu_realloc_and_copy_bspline_values(pmeGpu, gridIndex);
src/gromacs/ewald/pme_gpu_internal.cpp:    pme_gpu_realloc_grids(pmeGpu);
src/gromacs/ewald/pme_gpu_internal.cpp:    if (pmeGpu->settings.useDecomposition)
src/gromacs/ewald/pme_gpu_internal.cpp:        pme_gpu_reinit_haloexchange(pmeGpu);
src/gromacs/ewald/pme_gpu_internal.cpp:    pme_gpu_reinit_3dfft(pmeGpu);
src/gromacs/ewald/pme_gpu_internal.cpp:/* Several GPU functions that refer to the CPU PME data live here.
src/gromacs/ewald/pme_gpu_internal.cpp: * We would like to keep these away from the GPU-framework specific code for clarity,
src/gromacs/ewald/pme_gpu_internal.cpp: * Copies everything useful from the PME CPU to the PME GPU structure.
src/gromacs/ewald/pme_gpu_internal.cpp: * The goal is to minimize interaction with the PME CPU structure in the GPU code.
src/gromacs/ewald/pme_gpu_internal.cpp:static void pme_gpu_copy_common_data_from(const gmx_pme_t* pme)
src/gromacs/ewald/pme_gpu_internal.cpp:    PmeGpu* pmeGpu               = pme->gpu;
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->common->ngrids       = pme->bFEP_q ? 2 : 1;
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->common->epsilon_r    = pme->epsilon_r;
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->common->ewaldcoeff_q = pme->ewaldcoeff_q;
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->common->nk[XX]       = pme->nkx;
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->common->nk[YY]       = pme->nky;
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->common->nk[ZZ]       = pme->nkz;
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->common->pme_order    = pme->pme_order;
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->common->pmegridNk[XX] = pme->nnodes_major > 1 ? pme->pmegrid_nx : pme->nkx;
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->common->pmegridNk[YY] = pme->nnodes_minor > 1 ? pme->pmegrid_ny : pme->nky;
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->common->pmegridNk[ZZ] = pme->nkz;
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->common->ndecompdim    = pme->ndecompdim;
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->common->nodeid        = pme->nodeid;
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->common->nodeidX       = pme->nodeid_major;
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->common->nodeidY       = pme->nodeid_minor;
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->common->nnodes        = pme->nnodes;
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->common->nnodesX       = pme->nnodes_major;
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->common->nnodesY       = pme->nnodes_minor;
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->common->s2g0X         = pme->overlap[0].s2g0;
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->common->s2g1X         = pme->overlap[0].s2g1;
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->common->s2g0Y         = pme->overlap[1].s2g0;
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->common->s2g1Y         = pme->overlap[1].s2g1;
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->common->gridHalo      = pme->pmeGpuGridHalo;
src/gromacs/ewald/pme_gpu_internal.cpp:    if (pmeGpu->common->pme_order != c_pmeGpuOrder)
src/gromacs/ewald/pme_gpu_internal.cpp:        pmeGpu->common->bsp_mod[i].assign(pme->bsp_mod[i].data(),
src/gromacs/ewald/pme_gpu_internal.cpp:                                          pme->bsp_mod[i].data() + pmeGpu->common->nk[i]);
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->common->fsh.resize(0);
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->common->fsh.insert(pmeGpu->common->fsh.end(), pme->fshx.begin(), pme->fshx.end());
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->common->fsh.insert(pmeGpu->common->fsh.end(), pme->fshy.begin(), pme->fshy.end());
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->common->fsh.insert(pmeGpu->common->fsh.end(), pme->fshz.begin(), pme->fshz.end());
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->common->fsh.insert(pmeGpu->common->fsh.end(), pme->fshz.begin(), pme->fshz.end());
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->common->nn.resize(0);
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->common->nn.insert(pmeGpu->common->nn.end(), pme->nnx.begin(), pme->nnx.end());
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->common->nn.insert(pmeGpu->common->nn.end(), pme->nny.begin(), pme->nny.end());
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->common->nn.insert(pmeGpu->common->nn.end(), pme->nnz.begin(), pme->nnz.end());
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->common->runMode       = pme->runMode;
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->common->isRankPmeOnly = !pme->bPPnode;
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->common->boxScaler     = pme->boxScaler.get();
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->common->mpiCommX      = pme->mpi_comm_d[0];
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->common->mpiCommY      = pme->mpi_comm_d[1];
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->common->mpiComm       = pme->mpi_comm;
src/gromacs/ewald/pme_gpu_internal.cpp: * \param[in,out] pmeGpu         The PME GPU structure.
src/gromacs/ewald/pme_gpu_internal.cpp:static void pme_gpu_select_best_performing_pme_spreadgather_kernels(PmeGpu* pmeGpu)
src/gromacs/ewald/pme_gpu_internal.cpp:    if (((GMX_GPU_CUDA != 0) || (GMX_GPU_SYCL != 0))
src/gromacs/ewald/pme_gpu_internal.cpp:        && pmeGpu->kernelParams->atoms.nAtoms > pmeGpu->minParticleCountToRecalculateSplines)
src/gromacs/ewald/pme_gpu_internal.cpp:        pmeGpu->settings.threadsPerAtom     = ThreadsPerAtom::Order;
src/gromacs/ewald/pme_gpu_internal.cpp:        pmeGpu->settings.recalculateSplines = true;
src/gromacs/ewald/pme_gpu_internal.cpp:        pmeGpu->settings.threadsPerAtom     = ThreadsPerAtom::OrderSquared;
src/gromacs/ewald/pme_gpu_internal.cpp:        pmeGpu->settings.recalculateSplines = false;
src/gromacs/ewald/pme_gpu_internal.cpp: * Initializes the PME GPU data at the beginning of the run.
src/gromacs/ewald/pme_gpu_internal.cpp: * TODO: this should become PmeGpu::PmeGpu()
src/gromacs/ewald/pme_gpu_internal.cpp: * \param[in]     deviceContext  The GPU context.
src/gromacs/ewald/pme_gpu_internal.cpp: * \param[in]     deviceStream   The GPU stream.
src/gromacs/ewald/pme_gpu_internal.cpp: * \param[in,out] pmeGpuProgram  The handle to the program/kernel data created outside (e.g. in unit tests/runner)
src/gromacs/ewald/pme_gpu_internal.cpp:static void pme_gpu_init(gmx_pme_t*           pme,
src/gromacs/ewald/pme_gpu_internal.cpp:                         const PmeGpuProgram* pmeGpuProgram)
src/gromacs/ewald/pme_gpu_internal.cpp:    pme->gpu       = new PmeGpu();
src/gromacs/ewald/pme_gpu_internal.cpp:    PmeGpu* pmeGpu = pme->gpu;
src/gromacs/ewald/pme_gpu_internal.cpp:    changePinningPolicy(&pmeGpu->staging.h_forces, pme_get_pinning_policy());
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->common = std::make_shared<PmeShared>();
src/gromacs/ewald/pme_gpu_internal.cpp:    /* These settings are set here for the whole run; dynamic ones are set in pme_gpu_reinit() */
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->settings.useDecomposition = (pme->nnodes != 1);
src/gromacs/ewald/pme_gpu_internal.cpp:    /* TODO: CPU gather with GPU spread is broken due to different theta/dtheta layout. */
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->settings.performGPUGather = true;
src/gromacs/ewald/pme_gpu_internal.cpp:    // By default GPU-side reduction is off (explicitly set here for tests, otherwise reset per-step)
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->settings.useGpuForceReduction = false;
src/gromacs/ewald/pme_gpu_internal.cpp:    pme_gpu_set_testing(pmeGpu, false);
src/gromacs/ewald/pme_gpu_internal.cpp:    GMX_ASSERT(pmeGpuProgram != nullptr, "GPU kernels must be already compiled");
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->programHandle_ = pmeGpuProgram;
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->initializedClfftLibrary_ = std::make_unique<gmx::ClfftInitializer>();
src/gromacs/ewald/pme_gpu_internal.cpp:    pme_gpu_init_internal(pmeGpu, deviceContext, deviceStream);
src/gromacs/ewald/pme_gpu_internal.cpp:    pme_gpu_copy_common_data_from(pme);
src/gromacs/ewald/pme_gpu_internal.cpp:    pme_gpu_alloc_energy_virial(pmeGpu);
src/gromacs/ewald/pme_gpu_internal.cpp:    GMX_ASSERT(pmeGpu->common->epsilon_r != 0.0F, "PME GPU: bad electrostatic coefficient");
src/gromacs/ewald/pme_gpu_internal.cpp:    auto* kernelParamsPtr               = pme_gpu_get_kernel_params_base_ptr(pmeGpu);
src/gromacs/ewald/pme_gpu_internal.cpp:    kernelParamsPtr->constants.elFactor = gmx::c_one4PiEps0 / pmeGpu->common->epsilon_r;
src/gromacs/ewald/pme_gpu_internal.cpp:void pme_gpu_get_real_grid_sizes(const PmeGpu* pmeGpu, gmx::IVec* gridSize, gmx::IVec* paddedGridSize)
src/gromacs/ewald/pme_gpu_internal.cpp:    GMX_ASSERT(pmeGpu != nullptr, "");
src/gromacs/ewald/pme_gpu_internal.cpp:    auto* kernelParamsPtr = pme_gpu_get_kernel_params_base_ptr(pmeGpu);
src/gromacs/ewald/pme_gpu_internal.cpp:void pme_gpu_reinit(gmx_pme_t*           pme,
src/gromacs/ewald/pme_gpu_internal.cpp:                    const PmeGpuProgram* pmeGpuProgram,
src/gromacs/ewald/pme_gpu_internal.cpp:                    const bool           useMdGpuGraph)
src/gromacs/ewald/pme_gpu_internal.cpp:    if (!pme->gpu)
src/gromacs/ewald/pme_gpu_internal.cpp:                           "Device context can not be nullptr when setting up PME on GPU.");
src/gromacs/ewald/pme_gpu_internal.cpp:                           "Device stream can not be nullptr when setting up PME on GPU.");
src/gromacs/ewald/pme_gpu_internal.cpp:        pme_gpu_init(pme, *deviceContext, *deviceStream, pmeGpuProgram);
src/gromacs/ewald/pme_gpu_internal.cpp:        /* After this call nothing in the GPU code should refer to the gmx_pme_t *pme itself - until the next pme_gpu_reinit */
src/gromacs/ewald/pme_gpu_internal.cpp:        pme_gpu_copy_common_data_from(pme);
src/gromacs/ewald/pme_gpu_internal.cpp:    pme->gpu->settings.performGPUFFT   = (pme->gpu->common->runMode == PmeRunMode::GPU);
src/gromacs/ewald/pme_gpu_internal.cpp:    pme->gpu->settings.performGPUSolve = (pme->gpu->common->runMode == PmeRunMode::GPU);
src/gromacs/ewald/pme_gpu_internal.cpp:    pme_gpu_reinit_timings(pme->gpu);
src/gromacs/ewald/pme_gpu_internal.cpp:    pme_gpu_reinit_grids(pme->gpu);
src/gromacs/ewald/pme_gpu_internal.cpp:    pme_gpu_reinit_computation(pme, useMdGpuGraph, nullptr);
src/gromacs/ewald/pme_gpu_internal.cpp:    std::memset(pme->gpu->common->previousBox, 0, sizeof(pme->gpu->common->previousBox));
src/gromacs/ewald/pme_gpu_internal.cpp:void pme_gpu_destroy(PmeGpu* pmeGpu)
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->archSpecific->pmeStream_.synchronize();
src/gromacs/ewald/pme_gpu_internal.cpp:    pme_gpu_free_energy_virial(pmeGpu);
src/gromacs/ewald/pme_gpu_internal.cpp:    pme_gpu_free_bspline_values(pmeGpu);
src/gromacs/ewald/pme_gpu_internal.cpp:    pme_gpu_free_forces(pmeGpu);
src/gromacs/ewald/pme_gpu_internal.cpp:    pme_gpu_free_coefficients(pmeGpu);
src/gromacs/ewald/pme_gpu_internal.cpp:    pme_gpu_free_spline_data(pmeGpu);
src/gromacs/ewald/pme_gpu_internal.cpp:    pme_gpu_free_grid_indices(pmeGpu);
src/gromacs/ewald/pme_gpu_internal.cpp:    pme_gpu_free_fract_shifts(pmeGpu);
src/gromacs/ewald/pme_gpu_internal.cpp:    pme_gpu_free_grids(pmeGpu);
src/gromacs/ewald/pme_gpu_internal.cpp:    if (pmeGpu->settings.useDecomposition)
src/gromacs/ewald/pme_gpu_internal.cpp:        pme_gpu_free_haloexchange(pmeGpu);
src/gromacs/ewald/pme_gpu_internal.cpp:    pme_gpu_destroy_3dfft(pmeGpu);
src/gromacs/ewald/pme_gpu_internal.cpp:    delete pmeGpu;
src/gromacs/ewald/pme_gpu_internal.cpp:void pme_gpu_reinit_atoms(PmeGpu* pmeGpu, const int nAtoms, const real* chargesA, const real* chargesB)
src/gromacs/ewald/pme_gpu_internal.cpp:    auto* kernelParamsPtr         = pme_gpu_get_kernel_params_base_ptr(pmeGpu);
src/gromacs/ewald/pme_gpu_internal.cpp:    const int  blockSize          = pme_gpu_get_atom_data_block_size();
src/gromacs/ewald/pme_gpu_internal.cpp:    const bool haveToRealloc      = (pmeGpu->nAtomsAlloc < nAtomsNewPadded);
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->nAtomsAlloc           = nAtomsNewPadded;
src/gromacs/ewald/pme_gpu_internal.cpp:    const auto atomsPerWarp = pme_gpu_get_atoms_per_warp(pmeGpu);
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->archSpecific->splineCountActive = DIM * nWarps * atomsPerWarp * pmeGpu->common->pme_order;
src/gromacs/ewald/pme_gpu_internal.cpp:    if (pmeGpu->useNvshmem)
src/gromacs/ewald/pme_gpu_internal.cpp:                &pmeGpu->nAtomsAlloc, &pmeGpu->nvshmemParams->nAtomsAlloc_symmetric, 1, MPI_INT, MPI_MAX, MPI_COMM_WORLD);
src/gromacs/ewald/pme_gpu_internal.cpp:        int numPpRanks = pmeGpu->nvshmemParams->ppRanksRef.size();
src/gromacs/ewald/pme_gpu_internal.cpp:        reallocateDeviceBuffer(&pmeGpu->kernelParams->forcesReadyNvshmemFlags,
src/gromacs/ewald/pme_gpu_internal.cpp:                               &pmeGpu->nvshmemParams->forcesReadyNvshmemFlagsSize,
src/gromacs/ewald/pme_gpu_internal.cpp:                               &pmeGpu->nvshmemParams->forcesReadyNvshmemFlagsSizeAlloc,
src/gromacs/ewald/pme_gpu_internal.cpp:                               pmeGpu->archSpecific->deviceContext_,
src/gromacs/ewald/pme_gpu_internal.cpp:        pme_gpu_realloc_forces(pmeGpu);
src/gromacs/ewald/pme_gpu_internal.cpp:        kernelParamsPtr->ppRanksInfoSize = pmeGpu->nvshmemParams->ppRanksRef.size();
src/gromacs/ewald/pme_gpu_internal.cpp:        if (pmeGpu->nvshmemParams->ppRanksFInfo.empty())
src/gromacs/ewald/pme_gpu_internal.cpp:            changePinningPolicy(&pmeGpu->nvshmemParams->ppRanksFInfo, gmx::PinningPolicy::PinnedIfSupported);
src/gromacs/ewald/pme_gpu_internal.cpp:            pmeGpu->nvshmemParams->ppRanksFInfo.resize(kernelParamsPtr->ppRanksInfoSize);
src/gromacs/ewald/pme_gpu_internal.cpp:        // prepare the ppRanksFInfo struct for sending it to gpu.
src/gromacs/ewald/pme_gpu_internal.cpp:        for (const auto& receiver : pmeGpu->nvshmemParams->ppRanksRef)
src/gromacs/ewald/pme_gpu_internal.cpp:            auto& ppRankFInfo_prev = pmeGpu->nvshmemParams->ppRanksFInfo[receiverIndex - 1];
src/gromacs/ewald/pme_gpu_internal.cpp:            pmeGpu->nvshmemParams->ppRanksFInfo[receiverIndex] = { receiver.rankId, receiver.numAtoms, startIndex };
src/gromacs/ewald/pme_gpu_internal.cpp:                               &pmeGpu->nvshmemParams->ppRanksFInfoSize,
src/gromacs/ewald/pme_gpu_internal.cpp:                               &pmeGpu->nvshmemParams->ppRanksFInfoSizeAlloc,
src/gromacs/ewald/pme_gpu_internal.cpp:                               pmeGpu->archSpecific->deviceContext_);
src/gromacs/ewald/pme_gpu_internal.cpp:                           pmeGpu->nvshmemParams->ppRanksFInfo.data(),
src/gromacs/ewald/pme_gpu_internal.cpp:                           pmeGpu->archSpecific->pmeStream_,
src/gromacs/ewald/pme_gpu_internal.cpp:                           GpuApiCallBehavior::Async,
src/gromacs/ewald/pme_gpu_internal.cpp:                               &pmeGpu->nvshmemParams->lastProcessedBlockPerPpRankSize,
src/gromacs/ewald/pme_gpu_internal.cpp:                               &pmeGpu->nvshmemParams->lastProcessedBlockPerPpRankSizeAlloc,
src/gromacs/ewald/pme_gpu_internal.cpp:                               pmeGpu->archSpecific->deviceContext_);
src/gromacs/ewald/pme_gpu_internal.cpp:                               pmeGpu->archSpecific->pmeStream_);
src/gromacs/ewald/pme_gpu_internal.cpp:        pme_gpu_realloc_forces(pmeGpu);
src/gromacs/ewald/pme_gpu_internal.cpp:    pme_gpu_realloc_and_copy_input_coefficients(pmeGpu, reinterpret_cast<const float*>(chargesA), gridIndex);
src/gromacs/ewald/pme_gpu_internal.cpp:        pme_gpu_realloc_and_copy_input_coefficients(
src/gromacs/ewald/pme_gpu_internal.cpp:                pmeGpu, reinterpret_cast<const float*>(chargesB), gridIndex);
src/gromacs/ewald/pme_gpu_internal.cpp:         * conditionals in the GPU kernels */
src/gromacs/ewald/pme_gpu_internal.cpp:        if (pmeGpu->common->ngrids > 1)
src/gromacs/ewald/pme_gpu_internal.cpp:            pme_gpu_realloc_and_copy_input_coefficients(
src/gromacs/ewald/pme_gpu_internal.cpp:                    pmeGpu, reinterpret_cast<const float*>(chargesA), gridIndex);
src/gromacs/ewald/pme_gpu_internal.cpp:        pme_gpu_realloc_spline_data(pmeGpu);
src/gromacs/ewald/pme_gpu_internal.cpp:        pme_gpu_realloc_grid_indices(pmeGpu);
src/gromacs/ewald/pme_gpu_internal.cpp:    else if (pmeGpu->staging.h_forces.size() != pmeGpu->kernelParams->atoms.nAtoms)
src/gromacs/ewald/pme_gpu_internal.cpp:        pmeGpu->staging.h_forces.resizeWithPadding(pmeGpu->kernelParams->atoms.nAtoms);
src/gromacs/ewald/pme_gpu_internal.cpp:    pme_gpu_select_best_performing_pme_spreadgather_kernels(pmeGpu);
src/gromacs/ewald/pme_gpu_internal.cpp: * Returns raw timing event from the corresponding GpuRegionTimer (if timings are enabled).
src/gromacs/ewald/pme_gpu_internal.cpp: * In CUDA result can be nullptr stub, per GpuRegionTimer implementation.
src/gromacs/ewald/pme_gpu_internal.cpp: * \param[in] pmeGpu         The PME GPU data structure.
src/gromacs/ewald/pme_gpu_internal.cpp: * \param[in] pmeStageId     The PME GPU stage gtPME_ index from the enum in src/gromacs/timing/gpu_timing.h
src/gromacs/ewald/pme_gpu_internal.cpp:static CommandEvent* pme_gpu_fetch_timing_event(const PmeGpu* pmeGpu, PmeStage pmeStageId)
src/gromacs/ewald/pme_gpu_internal.cpp:    if (pme_gpu_timings_enabled(pmeGpu))
src/gromacs/ewald/pme_gpu_internal.cpp:        GMX_ASSERT(pmeStageId < PmeStage::Count, "Wrong PME GPU timing event index");
src/gromacs/ewald/pme_gpu_internal.cpp:        timingEvent = pmeGpu->archSpecific->timingEvents[pmeStageId].fetchNextEvent();
src/gromacs/ewald/pme_gpu_internal.cpp:void pme_gpu_3dfft(const PmeGpu* pmeGpu, gmx_fft_direction dir, const int grid_index)
src/gromacs/ewald/pme_gpu_internal.cpp:    pme_gpu_start_timing(pmeGpu, timerId);
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->archSpecific->fftSetup[grid_index]->perform3dFft(
src/gromacs/ewald/pme_gpu_internal.cpp:            dir, pme_gpu_fetch_timing_event(pmeGpu, timerId));
src/gromacs/ewald/pme_gpu_internal.cpp:    pme_gpu_stop_timing(pmeGpu, timerId);
src/gromacs/ewald/pme_gpu_internal.cpp:std::pair<int, int> inline pmeGpuCreateGrid(const PmeGpu* pmeGpu, int blockCount)
src/gromacs/ewald/pme_gpu_internal.cpp:    const int minRowCount = gmx::divideRoundUp<std::intmax_t>(blockCount, pmeGpu->maxGridWidthX);
src/gromacs/ewald/pme_gpu_internal.cpp:    GMX_ASSERT((colCount * minRowCount - blockCount) >= 0, "pmeGpuCreateGrid: totally wrong");
src/gromacs/ewald/pme_gpu_internal.cpp:               "pmeGpuCreateGrid: excessive blocks");
src/gromacs/ewald/pme_gpu_internal.cpp: * \param[in]  pmeGpu                   The PME GPU structure.
src/gromacs/ewald/pme_gpu_internal.cpp: * \return Pointer to CUDA kernel
src/gromacs/ewald/pme_gpu_internal.cpp:static auto selectSplineAndSpreadKernelPtr(const PmeGpu*  pmeGpu,
src/gromacs/ewald/pme_gpu_internal.cpp:    PmeGpuProgramImpl::PmeKernelHandle kernelPtr = nullptr;
src/gromacs/ewald/pme_gpu_internal.cpp:                kernelPtr = pmeGpu->programHandle_->impl_->splineAndSpreadKernelWriteSplinesThPerAtom4Dual;
src/gromacs/ewald/pme_gpu_internal.cpp:                kernelPtr = pmeGpu->programHandle_->impl_->splineAndSpreadKernelWriteSplinesThPerAtom4Single;
src/gromacs/ewald/pme_gpu_internal.cpp:                kernelPtr = pmeGpu->programHandle_->impl_->splineAndSpreadKernelWriteSplinesDual;
src/gromacs/ewald/pme_gpu_internal.cpp:                kernelPtr = pmeGpu->programHandle_->impl_->splineAndSpreadKernelWriteSplinesSingle;
src/gromacs/ewald/pme_gpu_internal.cpp:                kernelPtr = pmeGpu->programHandle_->impl_->splineAndSpreadKernelThPerAtom4Dual;
src/gromacs/ewald/pme_gpu_internal.cpp:                kernelPtr = pmeGpu->programHandle_->impl_->splineAndSpreadKernelThPerAtom4Single;
src/gromacs/ewald/pme_gpu_internal.cpp:                kernelPtr = pmeGpu->programHandle_->impl_->splineAndSpreadKernelDual;
src/gromacs/ewald/pme_gpu_internal.cpp:                kernelPtr = pmeGpu->programHandle_->impl_->splineAndSpreadKernelSingle;
src/gromacs/ewald/pme_gpu_internal.cpp: * \param[in]  pmeGpu                   The PME GPU structure.
src/gromacs/ewald/pme_gpu_internal.cpp: * \return Pointer to CUDA kernel
src/gromacs/ewald/pme_gpu_internal.cpp:static auto selectSplineKernelPtr(const PmeGpu*   pmeGpu,
src/gromacs/ewald/pme_gpu_internal.cpp:    PmeGpuProgramImpl::PmeKernelHandle kernelPtr = nullptr;
src/gromacs/ewald/pme_gpu_internal.cpp:            kernelPtr = pmeGpu->programHandle_->impl_->splineKernelThPerAtom4Dual;
src/gromacs/ewald/pme_gpu_internal.cpp:            kernelPtr = pmeGpu->programHandle_->impl_->splineKernelThPerAtom4Single;
src/gromacs/ewald/pme_gpu_internal.cpp:            kernelPtr = pmeGpu->programHandle_->impl_->splineKernelDual;
src/gromacs/ewald/pme_gpu_internal.cpp:            kernelPtr = pmeGpu->programHandle_->impl_->splineKernelSingle;
src/gromacs/ewald/pme_gpu_internal.cpp: * \param[in]  pmeGpu                   The PME GPU structure.
src/gromacs/ewald/pme_gpu_internal.cpp: * \return Pointer to CUDA kernel
src/gromacs/ewald/pme_gpu_internal.cpp:static auto selectSpreadKernelPtr(const PmeGpu*  pmeGpu,
src/gromacs/ewald/pme_gpu_internal.cpp:    PmeGpuProgramImpl::PmeKernelHandle kernelPtr = nullptr;
src/gromacs/ewald/pme_gpu_internal.cpp:                kernelPtr = pmeGpu->programHandle_->impl_->spreadKernelThPerAtom4Dual;
src/gromacs/ewald/pme_gpu_internal.cpp:                kernelPtr = pmeGpu->programHandle_->impl_->spreadKernelThPerAtom4Single;
src/gromacs/ewald/pme_gpu_internal.cpp:                kernelPtr = pmeGpu->programHandle_->impl_->spreadKernelDual;
src/gromacs/ewald/pme_gpu_internal.cpp:                kernelPtr = pmeGpu->programHandle_->impl_->spreadKernelSingle;
src/gromacs/ewald/pme_gpu_internal.cpp:                kernelPtr = pmeGpu->programHandle_->impl_->splineAndSpreadKernelThPerAtom4Dual;
src/gromacs/ewald/pme_gpu_internal.cpp:                kernelPtr = pmeGpu->programHandle_->impl_->splineAndSpreadKernelThPerAtom4Single;
src/gromacs/ewald/pme_gpu_internal.cpp:                kernelPtr = pmeGpu->programHandle_->impl_->splineAndSpreadKernelDual;
src/gromacs/ewald/pme_gpu_internal.cpp:                kernelPtr = pmeGpu->programHandle_->impl_->splineAndSpreadKernelSingle;
src/gromacs/ewald/pme_gpu_internal.cpp: * Manages synchronization with remote GPU's PP coordinate sender, for a stage of the communication operation.
src/gromacs/ewald/pme_gpu_internal.cpp: * For thread-MPI, an event associated with a stage of the operation is enqueued to the GPU stream that will be used by the consumer.
src/gromacs/ewald/pme_gpu_internal.cpp: * \param[in]  pmeGpu                    The PME GPU structure.
src/gromacs/ewald/pme_gpu_internal.cpp: * \param[in]  pmeCoordinateReceiverGpu  The PME coordinate reciever GPU object
src/gromacs/ewald/pme_gpu_internal.cpp:static int manageSyncWithPpCoordinateSenderGpu(const PmeGpu* pmeGpu,
src/gromacs/ewald/pme_gpu_internal.cpp:                                               gmx::PmeCoordinateReceiverGpu* pmeCoordinateReceiverGpu,
src/gromacs/ewald/pme_gpu_internal.cpp:        GpuEventSynchronizer* event;
src/gromacs/ewald/pme_gpu_internal.cpp:                pmeCoordinateReceiverGpu->receivePpCoordinateSendEvent(pipelineStage);
src/gromacs/ewald/pme_gpu_internal.cpp:                event->enqueueWaitEvent(*(pmeCoordinateReceiverGpu->ppCommStream(senderRank)));
src/gromacs/ewald/pme_gpu_internal.cpp:                event->enqueueWaitEvent(pmeGpu->archSpecific->pmeStream_);
src/gromacs/ewald/pme_gpu_internal.cpp:        senderRank = pmeCoordinateReceiverGpu->waitForCoordinatesFromAnyPpRank();
src/gromacs/ewald/pme_gpu_internal.cpp:void pme_gpu_spread(PmeGpu*                        pmeGpu,
src/gromacs/ewald/pme_gpu_internal.cpp:                    GpuEventSynchronizer*          xReadyOnDevice,
src/gromacs/ewald/pme_gpu_internal.cpp:                    const bool                     useGpuDirectComm,
src/gromacs/ewald/pme_gpu_internal.cpp:                    gmx::PmeCoordinateReceiverGpu* pmeCoordinateReceiverGpu,
src/gromacs/ewald/pme_gpu_internal.cpp:                    const bool                     useMdGpuGraph,
src/gromacs/ewald/pme_gpu_internal.cpp:            pmeGpu->common->ngrids == 1 || pmeGpu->common->ngrids == 2,
src/gromacs/ewald/pme_gpu_internal.cpp:            "Only one (normal Coulomb PME) or two (FEP coulomb PME) PME grids can be used on GPU");
src/gromacs/ewald/pme_gpu_internal.cpp:    auto* kernelParamsPtr = pmeGpu->kernelParams.get();
src/gromacs/ewald/pme_gpu_internal.cpp:    const size_t blockSize = pmeGpu->programHandle_->impl_->spreadWorkGroupSize;
src/gromacs/ewald/pme_gpu_internal.cpp:    const int order = pmeGpu->common->pme_order;
src/gromacs/ewald/pme_gpu_internal.cpp:    GMX_ASSERT(order == c_pmeGpuOrder, "Only PME order 4 is implemented");
src/gromacs/ewald/pme_gpu_internal.cpp:    const bool writeGlobal = pmeGpu->settings.copyAllOutputs;
src/gromacs/ewald/pme_gpu_internal.cpp:            (pmeGpu->settings.threadsPerAtom == ThreadsPerAtom::Order ? order : order * order);
src/gromacs/ewald/pme_gpu_internal.cpp:    const bool recalculateSplines = pmeGpu->settings.recalculateSplines;
src/gromacs/ewald/pme_gpu_internal.cpp:    GMX_ASSERT(!GMX_GPU_OPENCL || pmeGpu->settings.threadsPerAtom == ThreadsPerAtom::OrderSquared,
src/gromacs/ewald/pme_gpu_internal.cpp:               "Only 16 threads per atom supported in OpenCL");
src/gromacs/ewald/pme_gpu_internal.cpp:    GMX_ASSERT(!GMX_GPU_OPENCL || !recalculateSplines,
src/gromacs/ewald/pme_gpu_internal.cpp:               "Recalculating splines not supported in OpenCL");
src/gromacs/ewald/pme_gpu_internal.cpp:    // TODO: also consider using cudaFuncSetCacheConfig() for preferring shared memory on older architectures
src/gromacs/ewald/pme_gpu_internal.cpp:    GMX_ASSERT(xReadyOnDevice != nullptr || pmeGpu->common->isRankPmeOnly
src/gromacs/ewald/pme_gpu_internal.cpp:                       || pme_gpu_settings(pmeGpu).copyAllOutputs,
src/gromacs/ewald/pme_gpu_internal.cpp:               "Need a valid coordinate synchronizer on PP+PME ranks with CUDA.");
src/gromacs/ewald/pme_gpu_internal.cpp:        xReadyOnDevice->enqueueWaitEvent(pmeGpu->archSpecific->pmeStream_);
src/gromacs/ewald/pme_gpu_internal.cpp:        const int blockCount = pmeGpu->nAtomsAlloc / atomsPerBlock;
src/gromacs/ewald/pme_gpu_internal.cpp:        auto      dimGrid    = pmeGpuCreateGrid(pmeGpu, blockCount);
src/gromacs/ewald/pme_gpu_internal.cpp:        if (pmeGpu->common->ngrids == 1)
src/gromacs/ewald/pme_gpu_internal.cpp:        config.blockSize[1] = (pmeGpu->settings.threadsPerAtom == ThreadsPerAtom::Order ? 1 : order);
src/gromacs/ewald/pme_gpu_internal.cpp:        PmeGpuProgramImpl::PmeKernelHandle kernelPtr = nullptr;
src/gromacs/ewald/pme_gpu_internal.cpp:                kernelPtr = selectSplineAndSpreadKernelPtr(pmeGpu,
src/gromacs/ewald/pme_gpu_internal.cpp:                                                           pmeGpu->settings.threadsPerAtom,
src/gromacs/ewald/pme_gpu_internal.cpp:                                                           pmeGpu->common->ngrids);
src/gromacs/ewald/pme_gpu_internal.cpp:                kernelPtr = selectSplineKernelPtr(pmeGpu,
src/gromacs/ewald/pme_gpu_internal.cpp:                                                  pmeGpu->settings.threadsPerAtom,
src/gromacs/ewald/pme_gpu_internal.cpp:                                                  pmeGpu->common->ngrids);
src/gromacs/ewald/pme_gpu_internal.cpp:            kernelPtr = selectSpreadKernelPtr(pmeGpu,
src/gromacs/ewald/pme_gpu_internal.cpp:                                              pmeGpu->settings.threadsPerAtom,
src/gromacs/ewald/pme_gpu_internal.cpp:                                              pmeGpu->common->ngrids);
src/gromacs/ewald/pme_gpu_internal.cpp:        pme_gpu_start_timing(pmeGpu, timingId);
src/gromacs/ewald/pme_gpu_internal.cpp:        auto* timingEvent = pme_gpu_fetch_timing_event(pmeGpu, timingId);
src/gromacs/ewald/pme_gpu_internal.cpp:        kernelParamsPtr->usePipeline = char(computeSplines && spreadCharges && useGpuDirectComm
src/gromacs/ewald/pme_gpu_internal.cpp:                                            && (pmeCoordinateReceiverGpu->ppCommNumSenderRanks() > 1)
src/gromacs/ewald/pme_gpu_internal.cpp:            const int numStagesInPipeline = pmeCoordinateReceiverGpu->ppCommNumSenderRanks();
src/gromacs/ewald/pme_gpu_internal.cpp:            GpuEventSynchronizer* gridsReadyForSpread = &pmeGpu->archSpecific->pmeGridsReadyForSpread;
src/gromacs/ewald/pme_gpu_internal.cpp:            // Sync on grid zeroing is required except when GPU graphs are in use,
src/gromacs/ewald/pme_gpu_internal.cpp:            if (!useMdGpuGraph)
src/gromacs/ewald/pme_gpu_internal.cpp:                gridsReadyForSpread->markEvent(pmeGpu->archSpecific->pmeStream_);
src/gromacs/ewald/pme_gpu_internal.cpp:                wallcycle_start(wcycle, WallCycleCounter::WaitGpuPmePPRecvX);
src/gromacs/ewald/pme_gpu_internal.cpp:                int senderRank = manageSyncWithPpCoordinateSenderGpu(
src/gromacs/ewald/pme_gpu_internal.cpp:                        pmeGpu, pmeCoordinateReceiverGpu, kernelParamsPtr->usePipeline != 0, i);
src/gromacs/ewald/pme_gpu_internal.cpp:                    wallcycle_stop(wcycle, WallCycleCounter::WaitGpuPmePPRecvX);
src/gromacs/ewald/pme_gpu_internal.cpp:                wallcycle_stop(wcycle, WallCycleCounter::WaitGpuPmePPRecvX);
src/gromacs/ewald/pme_gpu_internal.cpp:                wallcycle_start(wcycle, WallCycleCounter::LaunchGpuPme);
src/gromacs/ewald/pme_gpu_internal.cpp:                DeviceStream* launchStream = pmeCoordinateReceiverGpu->ppCommStream(senderRank);
src/gromacs/ewald/pme_gpu_internal.cpp:                if (!useMdGpuGraph)
src/gromacs/ewald/pme_gpu_internal.cpp:                        pmeCoordinateReceiverGpu->ppCommAtomRange(senderRank);
src/gromacs/ewald/pme_gpu_internal.cpp:                auto      dimGrid    = pmeGpuCreateGrid(pmeGpu, blockCount);
src/gromacs/ewald/pme_gpu_internal.cpp:                const auto kernelArgs = prepareGpuKernelArguments(kernelPtr, config, kernelParamsPtr);
src/gromacs/ewald/pme_gpu_internal.cpp:                        prepareGpuKernelArguments(kernelPtr,
src/gromacs/ewald/pme_gpu_internal.cpp:                launchGpuKernel(kernelPtr, config, *launchStream, timingEvent, "PME spline/spread", kernelArgs);
src/gromacs/ewald/pme_gpu_internal.cpp:                wallcycle_stop(wcycle, WallCycleCounter::LaunchGpuPme);
src/gromacs/ewald/pme_gpu_internal.cpp:            wallcycle_start(wcycle, WallCycleCounter::LaunchGpuPme);
src/gromacs/ewald/pme_gpu_internal.cpp:            for (int i = 0; i < pmeCoordinateReceiverGpu->ppCommNumSenderRanks(); i++)
src/gromacs/ewald/pme_gpu_internal.cpp:                pmeCoordinateReceiverGpu->insertAsDependencyIntoStream(i, pmeGpu->archSpecific->pmeStream_);
src/gromacs/ewald/pme_gpu_internal.cpp:            wallcycle_stop(wcycle, WallCycleCounter::LaunchGpuPme);
src/gromacs/ewald/pme_gpu_internal.cpp:            if (useGpuDirectComm) // Sync all PME-PP communications to PME stream
src/gromacs/ewald/pme_gpu_internal.cpp:                wallcycle_start(wcycle, WallCycleCounter::WaitGpuPmePPRecvX);
src/gromacs/ewald/pme_gpu_internal.cpp:                for (int i = 0; i < pmeCoordinateReceiverGpu->ppCommNumSenderRanks(); i++)
src/gromacs/ewald/pme_gpu_internal.cpp:                    manageSyncWithPpCoordinateSenderGpu(pmeGpu, pmeCoordinateReceiverGpu);
src/gromacs/ewald/pme_gpu_internal.cpp:                wallcycle_stop(wcycle, WallCycleCounter::WaitGpuPmePPRecvX);
src/gromacs/ewald/pme_gpu_internal.cpp:            wallcycle_start(wcycle, WallCycleCounter::LaunchGpuPme);
src/gromacs/ewald/pme_gpu_internal.cpp:            const auto kernelArgs = prepareGpuKernelArguments(kernelPtr, config, kernelParamsPtr);
src/gromacs/ewald/pme_gpu_internal.cpp:                    prepareGpuKernelArguments(kernelPtr,
src/gromacs/ewald/pme_gpu_internal.cpp:            launchGpuKernel(kernelPtr,
src/gromacs/ewald/pme_gpu_internal.cpp:                            pmeGpu->archSpecific->pmeStream_,
src/gromacs/ewald/pme_gpu_internal.cpp:            wallcycle_stop(wcycle, WallCycleCounter::LaunchGpuPme);
src/gromacs/ewald/pme_gpu_internal.cpp:        pme_gpu_stop_timing(pmeGpu, timingId);
src/gromacs/ewald/pme_gpu_internal.cpp:    const auto& settings = pmeGpu->settings;
src/gromacs/ewald/pme_gpu_internal.cpp:        pmeGpuGridHaloExchange(pmeGpu, wcycle);
src/gromacs/ewald/pme_gpu_internal.cpp:    wallcycle_start(wcycle, WallCycleCounter::LaunchGpuPme);
src/gromacs/ewald/pme_gpu_internal.cpp:    // full PME GPU decomposition
src/gromacs/ewald/pme_gpu_internal.cpp:    const bool convertPmeToFftGridOnGpu = settings.performGPUFFT && settings.useDecomposition;
src/gromacs/ewald/pme_gpu_internal.cpp:    if (convertPmeToFftGridOnGpu)
src/gromacs/ewald/pme_gpu_internal.cpp:        for (int gridIndex = 0; gridIndex < pmeGpu->common->ngrids; gridIndex++)
src/gromacs/ewald/pme_gpu_internal.cpp:            convertPmeGridToFftGrid<true>(pmeGpu, &pmeGpu->archSpecific->d_fftRealGrid[gridIndex], gridIndex);
src/gromacs/ewald/pme_gpu_internal.cpp:    const bool copyBackGrid = spreadCharges && (!settings.performGPUFFT || settings.copyAllOutputs);
src/gromacs/ewald/pme_gpu_internal.cpp:            for (int gridIndex = 0; gridIndex < pmeGpu->common->ngrids; gridIndex++)
src/gromacs/ewald/pme_gpu_internal.cpp:                convertPmeGridToFftGrid<true>(pmeGpu, h_grid, fftSetup, gridIndex);
src/gromacs/ewald/pme_gpu_internal.cpp:            for (int gridIndex = 0; gridIndex < pmeGpu->common->ngrids; gridIndex++)
src/gromacs/ewald/pme_gpu_internal.cpp:                pme_gpu_copy_output_spread_grid(pmeGpu, h_grid, gridIndex);
src/gromacs/ewald/pme_gpu_internal.cpp:            computeSplines && (!settings.performGPUGather || settings.copyAllOutputs);
src/gromacs/ewald/pme_gpu_internal.cpp:        pme_gpu_copy_output_spread_atom_data(pmeGpu);
src/gromacs/ewald/pme_gpu_internal.cpp:    wallcycle_stop(wcycle, WallCycleCounter::LaunchGpuPme);
src/gromacs/ewald/pme_gpu_internal.cpp:void pme_gpu_solve(PmeGpu* pmeGpu, const int gridIndex, t_complex* h_grid, GridOrdering gridOrdering, bool computeEnergyAndVirial)
src/gromacs/ewald/pme_gpu_internal.cpp:            pmeGpu->common->ngrids == 1 || pmeGpu->common->ngrids == 2,
src/gromacs/ewald/pme_gpu_internal.cpp:            "Only one (normal Coulomb PME) or two (FEP coulomb PME) PME grids can be used on GPU");
src/gromacs/ewald/pme_gpu_internal.cpp:    GMX_ASSERT(gridIndex < pmeGpu->common->ngrids,
src/gromacs/ewald/pme_gpu_internal.cpp:    const auto& settings               = pmeGpu->settings;
src/gromacs/ewald/pme_gpu_internal.cpp:    const bool  copyInputAndOutputGrid = !settings.performGPUFFT || settings.copyAllOutputs;
src/gromacs/ewald/pme_gpu_internal.cpp:    auto* kernelParamsPtr = pmeGpu->kernelParams.get();
src/gromacs/ewald/pme_gpu_internal.cpp:                           pmeGpu->archSpecific->pmeStream_,
src/gromacs/ewald/pme_gpu_internal.cpp:                           pmeGpu->settings.transferKind,
src/gromacs/ewald/pme_gpu_internal.cpp:    const int maxBlockSize = pmeGpu->programHandle_->impl_->solveMaxWorkGroupSize;
src/gromacs/ewald/pme_gpu_internal.cpp:    const int gridLineSize      = pmeGpu->kernelParams->grid.localComplexGridSize[minorDim];
src/gromacs/ewald/pme_gpu_internal.cpp:    if (pmeGpu->common->nnodesY > 1
src/gromacs/ewald/pme_gpu_internal.cpp:        && pmeGpu->kernelParams->grid.complexGridSize[ZZ] >= pmeGpu->common->nnodes)
src/gromacs/ewald/pme_gpu_internal.cpp:        pmeGpu->kernelParams->grid.kOffsets[XX] = 0;
src/gromacs/ewald/pme_gpu_internal.cpp:        pmeGpu->kernelParams->grid.kOffsets[YY] = 0;
src/gromacs/ewald/pme_gpu_internal.cpp:        pmeGpu->kernelParams->grid.kOffsets[ZZ] = pmeGpu->common->nodeid
src/gromacs/ewald/pme_gpu_internal.cpp:                                                  * pmeGpu->kernelParams->grid.complexGridSize[ZZ]
src/gromacs/ewald/pme_gpu_internal.cpp:                                                  / pmeGpu->common->nnodes;
src/gromacs/ewald/pme_gpu_internal.cpp:        pmeGpu->kernelParams->grid.kOffsets[XX] = 0;
src/gromacs/ewald/pme_gpu_internal.cpp:        pmeGpu->kernelParams->grid.kOffsets[YY] = pmeGpu->common->nodeidX
src/gromacs/ewald/pme_gpu_internal.cpp:                                                  * pmeGpu->kernelParams->grid.complexGridSize[YY]
src/gromacs/ewald/pme_gpu_internal.cpp:                                                  / pmeGpu->common->nnodesX;
src/gromacs/ewald/pme_gpu_internal.cpp:        pmeGpu->kernelParams->grid.kOffsets[ZZ] = pmeGpu->common->nodeidY
src/gromacs/ewald/pme_gpu_internal.cpp:                                                  * pmeGpu->kernelParams->grid.complexGridSize[ZZ]
src/gromacs/ewald/pme_gpu_internal.cpp:                                                  / pmeGpu->common->nnodesY;
src/gromacs/ewald/pme_gpu_internal.cpp:    const int warpSize  = pmeGpu->programHandle_->warpSize();
src/gromacs/ewald/pme_gpu_internal.cpp:    static_assert(!GMX_GPU_CUDA || c_solveMaxWarpsPerBlock / 2 >= 4,
src/gromacs/ewald/pme_gpu_internal.cpp:                  "The CUDA solve energy kernels needs at least 4 warps. "
src/gromacs/ewald/pme_gpu_internal.cpp:            (pmeGpu->kernelParams->grid.localComplexGridSize[middleDim] + gridLinesPerBlock - 1)
src/gromacs/ewald/pme_gpu_internal.cpp:    config.gridSize[2] = pmeGpu->kernelParams->grid.localComplexGridSize[majorDim];
src/gromacs/ewald/pme_gpu_internal.cpp:    PmeGpuProgramImpl::PmeKernelHandle kernelPtr = nullptr;
src/gromacs/ewald/pme_gpu_internal.cpp:            kernelPtr = computeEnergyAndVirial ? pmeGpu->programHandle_->impl_->solveYZXEnergyKernelA
src/gromacs/ewald/pme_gpu_internal.cpp:                                               : pmeGpu->programHandle_->impl_->solveYZXKernelA;
src/gromacs/ewald/pme_gpu_internal.cpp:            kernelPtr = computeEnergyAndVirial ? pmeGpu->programHandle_->impl_->solveYZXEnergyKernelB
src/gromacs/ewald/pme_gpu_internal.cpp:                                               : pmeGpu->programHandle_->impl_->solveYZXKernelB;
src/gromacs/ewald/pme_gpu_internal.cpp:            kernelPtr = computeEnergyAndVirial ? pmeGpu->programHandle_->impl_->solveXYZEnergyKernelA
src/gromacs/ewald/pme_gpu_internal.cpp:                                               : pmeGpu->programHandle_->impl_->solveXYZKernelA;
src/gromacs/ewald/pme_gpu_internal.cpp:            kernelPtr = computeEnergyAndVirial ? pmeGpu->programHandle_->impl_->solveXYZEnergyKernelB
src/gromacs/ewald/pme_gpu_internal.cpp:                                               : pmeGpu->programHandle_->impl_->solveXYZKernelB;
src/gromacs/ewald/pme_gpu_internal.cpp:    pme_gpu_start_timing(pmeGpu, timingId);
src/gromacs/ewald/pme_gpu_internal.cpp:    auto* timingEvent = pme_gpu_fetch_timing_event(pmeGpu, timingId);
src/gromacs/ewald/pme_gpu_internal.cpp:    const auto kernelArgs = prepareGpuKernelArguments(kernelPtr, config, kernelParamsPtr);
src/gromacs/ewald/pme_gpu_internal.cpp:            prepareGpuKernelArguments(kernelPtr,
src/gromacs/ewald/pme_gpu_internal.cpp:    launchGpuKernel(kernelPtr, config, pmeGpu->archSpecific->pmeStream_, timingEvent, "PME solve", kernelArgs);
src/gromacs/ewald/pme_gpu_internal.cpp:    pme_gpu_stop_timing(pmeGpu, timingId);
src/gromacs/ewald/pme_gpu_internal.cpp:        copyFromDeviceBuffer(pmeGpu->staging.h_virialAndEnergy[gridIndex].data(),
src/gromacs/ewald/pme_gpu_internal.cpp:                             pmeGpu->archSpecific->pmeStream_,
src/gromacs/ewald/pme_gpu_internal.cpp:                             pmeGpu->settings.transferKind,
src/gromacs/ewald/pme_gpu_internal.cpp:                             pmeGpu->archSpecific->pmeStream_,
src/gromacs/ewald/pme_gpu_internal.cpp:                             pmeGpu->settings.transferKind,
src/gromacs/ewald/pme_gpu_internal.cpp: * \param[in]  pmeGpu                   The PME GPU structure.
src/gromacs/ewald/pme_gpu_internal.cpp: * \return Pointer to CUDA kernel
src/gromacs/ewald/pme_gpu_internal.cpp:inline auto selectGatherKernelPtr(const PmeGpu*  pmeGpu,
src/gromacs/ewald/pme_gpu_internal.cpp:    PmeGpuProgramImpl::PmeKernelHandle kernelPtr = nullptr;
src/gromacs/ewald/pme_gpu_internal.cpp:                kernelPtr = pmeGpu->programHandle_->impl_->gatherKernelReadSplinesThPerAtom4Dual;
src/gromacs/ewald/pme_gpu_internal.cpp:                kernelPtr = pmeGpu->programHandle_->impl_->gatherKernelReadSplinesThPerAtom4Single;
src/gromacs/ewald/pme_gpu_internal.cpp:                kernelPtr = pmeGpu->programHandle_->impl_->gatherKernelReadSplinesDual;
src/gromacs/ewald/pme_gpu_internal.cpp:                kernelPtr = pmeGpu->programHandle_->impl_->gatherKernelReadSplinesSingle;
src/gromacs/ewald/pme_gpu_internal.cpp:                kernelPtr = pmeGpu->programHandle_->impl_->gatherKernelThPerAtom4Dual;
src/gromacs/ewald/pme_gpu_internal.cpp:                kernelPtr = pmeGpu->programHandle_->impl_->gatherKernelThPerAtom4Single;
src/gromacs/ewald/pme_gpu_internal.cpp:                kernelPtr = pmeGpu->programHandle_->impl_->gatherKernelDual;
src/gromacs/ewald/pme_gpu_internal.cpp:                kernelPtr = pmeGpu->programHandle_->impl_->gatherKernelSingle;
src/gromacs/ewald/pme_gpu_internal.cpp:void pme_gpu_gather(PmeGpu*                       pmeGpu,
src/gromacs/ewald/pme_gpu_internal.cpp:            pmeGpu->common->ngrids == 1 || pmeGpu->common->ngrids == 2,
src/gromacs/ewald/pme_gpu_internal.cpp:            "Only one (normal Coulomb PME) or two (FEP coulomb PME) PME grids can be used on GPU");
src/gromacs/ewald/pme_gpu_internal.cpp:    const auto& settings        = pmeGpu->settings;
src/gromacs/ewald/pme_gpu_internal.cpp:    auto*       kernelParamsPtr = pmeGpu->kernelParams.get();
src/gromacs/ewald/pme_gpu_internal.cpp:    wallcycle_start(wcycle, WallCycleCounter::LaunchGpuPme);
src/gromacs/ewald/pme_gpu_internal.cpp:    // full PME GPU decomposition
src/gromacs/ewald/pme_gpu_internal.cpp:    const bool convertFftToPmeGridOnGpu = settings.performGPUFFT && settings.useDecomposition;
src/gromacs/ewald/pme_gpu_internal.cpp:    if (convertFftToPmeGridOnGpu)
src/gromacs/ewald/pme_gpu_internal.cpp:        for (int gridIndex = 0; gridIndex < pmeGpu->common->ngrids; gridIndex++)
src/gromacs/ewald/pme_gpu_internal.cpp:                    pmeGpu, &pmeGpu->archSpecific->d_fftRealGrid[gridIndex], gridIndex);
src/gromacs/ewald/pme_gpu_internal.cpp:    const bool copyBackGrid = (!settings.performGPUFFT || settings.copyAllOutputs);
src/gromacs/ewald/pme_gpu_internal.cpp:            for (int gridIndex = 0; gridIndex < pmeGpu->common->ngrids; gridIndex++)
src/gromacs/ewald/pme_gpu_internal.cpp:                convertPmeGridToFftGrid<false>(pmeGpu, h_grid, fftSetup, gridIndex);
src/gromacs/ewald/pme_gpu_internal.cpp:            for (int gridIndex = 0; gridIndex < pmeGpu->common->ngrids; gridIndex++)
src/gromacs/ewald/pme_gpu_internal.cpp:                pme_gpu_copy_input_gather_grid(pmeGpu, h_grid, gridIndex);
src/gromacs/ewald/pme_gpu_internal.cpp:        pme_gpu_copy_input_gather_atom_data(pmeGpu);
src/gromacs/ewald/pme_gpu_internal.cpp:    wallcycle_stop(wcycle, WallCycleCounter::LaunchGpuPme);
src/gromacs/ewald/pme_gpu_internal.cpp:        pmeGpuGridHaloExchangeReverse(pmeGpu, wcycle);
src/gromacs/ewald/pme_gpu_internal.cpp:    wallcycle_start(wcycle, WallCycleCounter::LaunchGpuPme);
src/gromacs/ewald/pme_gpu_internal.cpp:    const bool   readGlobal = pmeGpu->settings.copyAllOutputs;
src/gromacs/ewald/pme_gpu_internal.cpp:    const size_t blockSize  = pmeGpu->programHandle_->impl_->gatherWorkGroupSize;
src/gromacs/ewald/pme_gpu_internal.cpp:    const int    order      = pmeGpu->common->pme_order;
src/gromacs/ewald/pme_gpu_internal.cpp:    GMX_ASSERT(order == c_pmeGpuOrder, "Only PME order 4 is implemented");
src/gromacs/ewald/pme_gpu_internal.cpp:            (pmeGpu->settings.threadsPerAtom == ThreadsPerAtom::Order ? order : order * order);
src/gromacs/ewald/pme_gpu_internal.cpp:    const bool recalculateSplines = pmeGpu->settings.recalculateSplines;
src/gromacs/ewald/pme_gpu_internal.cpp:    GMX_ASSERT(!GMX_GPU_OPENCL || pmeGpu->settings.threadsPerAtom == ThreadsPerAtom::OrderSquared,
src/gromacs/ewald/pme_gpu_internal.cpp:               "Only 16 threads per atom supported in OpenCL");
src/gromacs/ewald/pme_gpu_internal.cpp:    GMX_ASSERT(!GMX_GPU_OPENCL || !recalculateSplines,
src/gromacs/ewald/pme_gpu_internal.cpp:               "Recalculating splines not supported in OpenCL");
src/gromacs/ewald/pme_gpu_internal.cpp:        const int blockCount = pmeGpu->nAtomsAlloc / atomsPerBlock;
src/gromacs/ewald/pme_gpu_internal.cpp:        auto      dimGrid    = pmeGpuCreateGrid(pmeGpu, blockCount);
src/gromacs/ewald/pme_gpu_internal.cpp:        config.blockSize[1] = (pmeGpu->settings.threadsPerAtom == ThreadsPerAtom::Order ? 1 : order);
src/gromacs/ewald/pme_gpu_internal.cpp:        PmeGpuProgramImpl::PmeKernelHandle kernelPtr =
src/gromacs/ewald/pme_gpu_internal.cpp:                selectGatherKernelPtr(pmeGpu,
src/gromacs/ewald/pme_gpu_internal.cpp:                                      pmeGpu->settings.threadsPerAtom,
src/gromacs/ewald/pme_gpu_internal.cpp:                                      pmeGpu->common->ngrids);
src/gromacs/ewald/pme_gpu_internal.cpp:        // TODO design kernel selection getters and make PmeGpu a friend of PmeGpuProgramImpl
src/gromacs/ewald/pme_gpu_internal.cpp:        pme_gpu_start_timing(pmeGpu, timingId);
src/gromacs/ewald/pme_gpu_internal.cpp:        auto* timingEvent = pme_gpu_fetch_timing_event(pmeGpu, timingId);
src/gromacs/ewald/pme_gpu_internal.cpp:        if (pmeGpu->common->ngrids == 1)
src/gromacs/ewald/pme_gpu_internal.cpp:        if (!computeVirial && pmeGpu->useNvshmem)
src/gromacs/ewald/pme_gpu_internal.cpp:        const auto kernelArgs = prepareGpuKernelArguments(kernelPtr, config, kernelParamsPtr);
src/gromacs/ewald/pme_gpu_internal.cpp:                prepareGpuKernelArguments(kernelPtr,
src/gromacs/ewald/pme_gpu_internal.cpp:        launchGpuKernel(
src/gromacs/ewald/pme_gpu_internal.cpp:                kernelPtr, config, pmeGpu->archSpecific->pmeStream_, timingEvent, "PME gather", kernelArgs);
src/gromacs/ewald/pme_gpu_internal.cpp:        if (!computeVirial && pmeGpu->useNvshmem)
src/gromacs/ewald/pme_gpu_internal.cpp:            auto kernelPtr_     = pmeGpu->programHandle_->impl_->nvshmemSignalKern;
src/gromacs/ewald/pme_gpu_internal.cpp:            const auto kernelArgs_ = prepareGpuKernelArguments(kernelPtr_, config, kernelParamsPtr);
src/gromacs/ewald/pme_gpu_internal.cpp:            launchGpuKernel(
src/gromacs/ewald/pme_gpu_internal.cpp:                    kernelPtr_, config, pmeGpu->archSpecific->pmeStream_, timingEvent, "PME gather", kernelArgs_);
src/gromacs/ewald/pme_gpu_internal.cpp:        pme_gpu_stop_timing(pmeGpu, timingId);
src/gromacs/ewald/pme_gpu_internal.cpp:    if (pmeGpu->settings.useGpuForceReduction)
src/gromacs/ewald/pme_gpu_internal.cpp:        pmeGpu->archSpecific->pmeForcesReady.markEvent(pmeGpu->archSpecific->pmeStream_);
src/gromacs/ewald/pme_gpu_internal.cpp:    else if (pmeGpu->kernelParams->atoms.nAtoms > 0)
src/gromacs/ewald/pme_gpu_internal.cpp:        pme_gpu_copy_output_forces(pmeGpu);
src/gromacs/ewald/pme_gpu_internal.cpp:    wallcycle_stop(wcycle, WallCycleCounter::LaunchGpuPme);
src/gromacs/ewald/pme_gpu_internal.cpp:DeviceBuffer<gmx::RVec> pme_gpu_get_kernelparam_forces(const PmeGpu* pmeGpu)
src/gromacs/ewald/pme_gpu_internal.cpp:    if (pmeGpu && pmeGpu->kernelParams)
src/gromacs/ewald/pme_gpu_internal.cpp:        return pmeGpu->kernelParams->atoms.d_forces;
src/gromacs/ewald/pme_gpu_internal.cpp:void pme_gpu_set_kernelparam_useNvshmem(const PmeGpu* pmeGpu, bool useNvshmem)
src/gromacs/ewald/pme_gpu_internal.cpp:    GMX_ASSERT(pmeGpu && pmeGpu->kernelParams,
src/gromacs/ewald/pme_gpu_internal.cpp:               "PME GPU NVSHMEM support can not be set in non-GPU builds or before the GPU PME was "
src/gromacs/ewald/pme_gpu_internal.cpp:    if (pmeGpu && pmeGpu->kernelParams)
src/gromacs/ewald/pme_gpu_internal.cpp:        pmeGpu->kernelParams->useNvshmem = static_cast<int>(useNvshmem);
src/gromacs/ewald/pme_gpu_internal.cpp:void pme_gpu_set_kernelparam_coordinates(const PmeGpu* pmeGpu, DeviceBuffer<gmx::RVec> d_x)
src/gromacs/ewald/pme_gpu_internal.cpp:    GMX_ASSERT(pmeGpu && pmeGpu->kernelParams,
src/gromacs/ewald/pme_gpu_internal.cpp:               "PME GPU device buffer can not be set in non-GPU builds or before the GPU PME was "
src/gromacs/ewald/pme_gpu_internal.cpp:    GMX_ASSERT(checkDeviceBuffer(d_x, pmeGpu->kernelParams->atoms.nAtoms),
src/gromacs/ewald/pme_gpu_internal.cpp:    pmeGpu->kernelParams->atoms.d_coordinates = d_x;
src/gromacs/ewald/pme_gpu_internal.cpp:GpuEventSynchronizer* pme_gpu_get_forces_ready_synchronizer(const PmeGpu* pmeGpu)
src/gromacs/ewald/pme_gpu_internal.cpp:    if (pmeGpu && pmeGpu->kernelParams)
src/gromacs/ewald/pme_gpu_internal.cpp:        return &pmeGpu->archSpecific->pmeForcesReady;
src/gromacs/ewald/pme.cpp:#include "gromacs/gpu_utils/hostallocator.h"
src/gromacs/ewald/pme.cpp:#include "pme_gpu_internal.h"
src/gromacs/ewald/pme.cpp:bool pme_gpu_supports_build(std::string* error)
src/gromacs/ewald/pme.cpp:    errorReasons.startContext("PME GPU does not support:");
src/gromacs/ewald/pme.cpp:    // OpenCL compiler silently fails on macOS when using clFFT backend.
src/gromacs/ewald/pme.cpp:    errorReasons.appendIf(GMX_GPU_OPENCL && !GMX_GPU_FFT_VKFFT, "macOS build using clFFT.");
src/gromacs/ewald/pme.cpp:    errorReasons.appendIf(!GMX_GPU, "Non-GPU build of GROMACS.");
src/gromacs/ewald/pme.cpp:    errorReasons.appendIf(GMX_GPU_HIP, "HIP API not supported yet");
src/gromacs/ewald/pme.cpp:bool pme_gpu_supports_input(const t_inputrec& ir, std::string* error)
src/gromacs/ewald/pme.cpp:    errorReasons.startContext("PME GPU does not support:");
src/gromacs/ewald/pme.cpp:bool pme_gpu_mixed_mode_supports_input(const t_inputrec& ir, std::string* error)
src/gromacs/ewald/pme.cpp:    errorReasons.startContext("PME GPU in Mixed mode does not support:");
src/gromacs/ewald/pme.cpp: * Finds out if PME with given inputs is possible to run on GPU.
src/gromacs/ewald/pme.cpp: * but it still duplicates the preliminary checks from the above (externally exposed) pme_gpu_supports_input() - just in case.
src/gromacs/ewald/pme.cpp: * \param[out] error        The error message if the input is not supported on GPU.
src/gromacs/ewald/pme.cpp: * \returns                 True if this PME input is possible to run on GPU, false otherwise.
src/gromacs/ewald/pme.cpp:static bool pme_gpu_check_restrictions(const gmx_pme_t* pme, std::string* error)
src/gromacs/ewald/pme.cpp:    errorReasons.startContext("PME GPU does not support:");
src/gromacs/ewald/pme.cpp:    errorReasons.appendIf((!GMX_GPU_CUDA && pme->nnodes != 1 && pme->ndecompdim >= 2),
src/gromacs/ewald/pme.cpp:    errorReasons.appendIf(!GMX_GPU, "Non-GPU build of GROMACS.");
src/gromacs/ewald/pme.cpp:                                bool useGpuPme,
src/gromacs/ewald/pme.cpp:     * this is needed for PME-GPU decomposition
src/gromacs/ewald/pme.cpp:    if (useGpuPme && (numPmeDomainsAlongX > 1 || numPmeDomainsAlongY > 1)
src/gromacs/ewald/pme.cpp:        const auto  allocateRealGridForGpu = (pme.runMode == PmeRunMode::Mixed)
src/gromacs/ewald/pme.cpp:                                allocateRealGridForGpu);
src/gromacs/ewald/pme.cpp:                        PmeGpu*                          pmeGpu,
src/gromacs/ewald/pme.cpp:                        const PmeGpuProgram*             pmeGpuProgram,
src/gromacs/ewald/pme.cpp:        const int pmeGpuGridHalo = numGridLinesForExtendedHaloRegion(
src/gromacs/ewald/pme.cpp:                    "PME GPU haloExtent = %.3f pmeGpuGridHalo = %d\n",
src/gromacs/ewald/pme.cpp:                    pmeGpuGridHalo);
src/gromacs/ewald/pme.cpp:        pme->pmeGpuGridHalo                = pmeGpuGridHalo;
src/gromacs/ewald/pme.cpp:                               pme->pmeGpuGridHalo,
src/gromacs/ewald/pme.cpp:    pme->gpu     = pmeGpu; /* Carrying over the single GPU structure */
src/gromacs/ewald/pme.cpp:    // The way PME decomposition is implemented for GPUs, gridline indices at borders should not be rounded
src/gromacs/ewald/pme.cpp:    // Initial check of validity of the input for running on the GPU
src/gromacs/ewald/pme.cpp:        bool        canRunOnGpu = pme_gpu_check_restrictions(pme.get(), &errorString);
src/gromacs/ewald/pme.cpp:        if (!canRunOnGpu)
src/gromacs/ewald/pme.cpp:        const bool useMdGpuGraph = false; // This will be reset later after PP communication
src/gromacs/ewald/pme.cpp:        pme_gpu_reinit(pme.get(), deviceContext, deviceStream, pmeGpuProgram, useMdGpuGraph);
src/gromacs/ewald/pme.cpp:        GMX_ASSERT(pme->gpu == nullptr, "Should not have PME GPU object when PME is on a CPU.");
src/gromacs/ewald/pme.cpp:                                pme_src->gpu,
src/gromacs/ewald/pme.cpp:        if (!pme_src->gpu && pme_src->nnodes == 1)
src/gromacs/ewald/pme.cpp:               "gmx_pme_do should not be called on the GPU PME run.");
src/gromacs/ewald/pme.cpp:void gmx_pme_destroy(gmx_pme_t* pme, bool destroyGpuData)
src/gromacs/ewald/pme.cpp:    if (pme->gpu != nullptr && destroyGpuData)
src/gromacs/ewald/pme.cpp:        pme_gpu_destroy(pme->gpu);
src/gromacs/ewald/pme.cpp:    if (pme->gpu != nullptr)
src/gromacs/ewald/pme.cpp:                   "B state charges must be specified if running Coulomb FEP on the GPU");
src/gromacs/ewald/pme.cpp:        pme_gpu_reinit_atoms(pme->gpu, numAtoms, chargesA.data(), pme->bFEP_q ? chargesB.data() : nullptr);
src/gromacs/ewald/pme_gpu_program_impl_ocl.cpp: * Implements PmeGpuProgramImpl, which stores permanent PME GPU context-derived data,
src/gromacs/ewald/pme_gpu_program_impl_ocl.cpp:#include "gromacs/gpu_utils/gmxopencl.h"
src/gromacs/ewald/pme_gpu_program_impl_ocl.cpp:#include "gromacs/gpu_utils/ocl_compiler.h"
src/gromacs/ewald/pme_gpu_program_impl_ocl.cpp:#include "pme_gpu_constants.h"
src/gromacs/ewald/pme_gpu_program_impl_ocl.cpp:#include "pme_gpu_internal.h" // for GridOrdering enum
src/gromacs/ewald/pme_gpu_program_impl_ocl.cpp:#include "pme_gpu_program_impl.h"
src/gromacs/ewald/pme_gpu_program_impl_ocl.cpp:#include "pme_gpu_types_host.h"
src/gromacs/ewald/pme_gpu_program_impl_ocl.cpp:PmeGpuProgramImpl::PmeGpuProgramImpl(const DeviceContext& deviceContext) :
src/gromacs/ewald/pme_gpu_program_impl_ocl.cpp:    // but given that we've done no tuning for Intel iGPU, this is as good as anything.
src/gromacs/ewald/pme_gpu_program_impl_ocl.cpp:PmeGpuProgramImpl::~PmeGpuProgramImpl()
src/gromacs/ewald/pme_gpu_program_impl_ocl.cpp:               gmx::formatString("Failed to release PME OpenCL resources %d: %s",
src/gromacs/ewald/pme_gpu_program_impl_ocl.cpp:        const int minKernelWarpSize = c_pmeGpuOrder * c_pmeGpuOrder;
src/gromacs/ewald/pme_gpu_program_impl_ocl.cpp:                    "PME OpenCL kernels require >=%d execution width, but the %s kernel "
src/gromacs/ewald/pme_gpu_program_impl_ocl.cpp:void PmeGpuProgramImpl::compileKernels(const DeviceInformation& deviceInfo)
src/gromacs/ewald/pme_gpu_program_impl_ocl.cpp:                // forwarding PME behavior constants from pme_gpu_constants.h
src/gromacs/ewald/pme_gpu_program_impl_ocl.cpp:                c_pmeGpuOrder,
src/gromacs/ewald/pme_gpu_program_impl_ocl.cpp:                c_pmeGpuOrder * c_pmeGpuOrder,
src/gromacs/ewald/pme_gpu_program_impl_ocl.cpp:            e.prependContext(gmx::formatString("Failed to compile PME kernels for GPU #%s\n",
src/gromacs/ewald/pme_gpu_program_impl_ocl.cpp:                "Failed to create kernels for PME on GPU #%s:\n OpenCL error %d: %s",
src/gromacs/ewald/pme_gpu_program_impl_ocl.cpp:                    "Failed to parse kernels for PME on GPU #%s:\n OpenCL error %d: %s",
src/gromacs/ewald/pme_output.h:    //!< True if forces have been staged other false (when forces are reduced on the GPU).
src/gromacs/ewald/pme_force_sender_gpu_impl_gpu_sycl.cpp:#include "pme_force_sender_gpu_impl.h"
src/gromacs/ewald/pme_force_sender_gpu_impl_gpu_sycl.cpp:void PmeForceSenderGpu::Impl::sendFToPpPeerToPeer(int /*ppRank*/, int /*numAtoms*/, bool /*sendForcesDirectToPpGpu*/)
src/gromacs/ewald/pme_internal.h:struct PmeGpu;
src/gromacs/ewald/pme_internal.h:    int  pmeGpuGridHalo = 0; /* Size of the grid halo region with PME GPU decomposition */
src/gromacs/ewald/pme_internal.h:    enum PmeRunMode runMode; /* Which codepath is the PME runner taking - CPU, GPU, mixed;
src/gromacs/ewald/pme_internal.h:    PmeGpu* gpu; /* A pointer to the GPU data.
src/gromacs/ewald/pme_gpu.cpp: * \brief Implements high-level PME GPU functions which do not require GPU framework-specific code.
src/gromacs/ewald/pme_gpu.cpp:#include "gromacs/ewald/pme_coordinate_receiver_gpu.h"
src/gromacs/ewald/pme_gpu.cpp:#include "pme_gpu_internal.h"
src/gromacs/ewald/pme_gpu.cpp:#include "pme_gpu_settings.h"
src/gromacs/ewald/pme_gpu.cpp:#include "pme_gpu_timings.h"
src/gromacs/ewald/pme_gpu.cpp:#include "pme_gpu_types_host.h"
src/gromacs/ewald/pme_gpu.cpp: * Finds out if PME is currently running on GPU.
src/gromacs/ewald/pme_gpu.cpp: * \todo The GPU module should not be constructed (or at least called)
src/gromacs/ewald/pme_gpu.cpp: * \returns        True if PME runs on GPU currently, false otherwise.
src/gromacs/ewald/pme_gpu.cpp:static inline bool pme_gpu_active(const gmx_pme_t* pme)
src/gromacs/ewald/pme_gpu.cpp:void pme_gpu_reset_timings(const gmx_pme_t* pme)
src/gromacs/ewald/pme_gpu.cpp:    if (pme_gpu_active(pme))
src/gromacs/ewald/pme_gpu.cpp:        pme_gpu_reset_timings(pme->gpu);
src/gromacs/ewald/pme_gpu.cpp:void pme_gpu_get_timings(const gmx_pme_t* pme, gmx_wallclock_gpu_pme_t* timings)
src/gromacs/ewald/pme_gpu.cpp:    if (pme_gpu_active(pme))
src/gromacs/ewald/pme_gpu.cpp:        pme_gpu_get_timings(pme->gpu, timings);
src/gromacs/ewald/pme_gpu.cpp:int pme_gpu_get_block_size(const gmx_pme_t* pme)
src/gromacs/ewald/pme_gpu.cpp:    if (!pme || !pme_gpu_active(pme))
src/gromacs/ewald/pme_gpu.cpp:        return pme_gpu_get_atom_data_block_size();
src/gromacs/ewald/pme_gpu.cpp: * A convenience wrapper for launching either the GPU or CPU FFT.
src/gromacs/ewald/pme_gpu.cpp:void inline parallel_3dfft_execute_gpu_wrapper(gmx_pme_t*             pme,
src/gromacs/ewald/pme_gpu.cpp:    if (pme_gpu_settings(pme->gpu).performGPUFFT)
src/gromacs/ewald/pme_gpu.cpp:        // use a separate sub-counter for GPU FFT launch
src/gromacs/ewald/pme_gpu.cpp:        wallcycle_start(wcycle, WallCycleCounter::LaunchGpuPme);
src/gromacs/ewald/pme_gpu.cpp:        wallcycle_sub_start(wcycle, WallCycleSubCounter::LaunchGpuPmeFft);
src/gromacs/ewald/pme_gpu.cpp:        pme_gpu_3dfft(pme->gpu, dir, gridIndex);
src/gromacs/ewald/pme_gpu.cpp:        wallcycle_sub_stop(wcycle, WallCycleSubCounter::LaunchGpuPmeFft);
src/gromacs/ewald/pme_gpu.cpp:        wallcycle_stop(wcycle, WallCycleCounter::LaunchGpuPme);
src/gromacs/ewald/pme_gpu.cpp:void pme_gpu_prepare_computation(gmx_pme_t*               pme,
src/gromacs/ewald/pme_gpu.cpp:    GMX_ASSERT(pme_gpu_active(pme), "This should be a GPU run of PME but it is not enabled.");
src/gromacs/ewald/pme_gpu.cpp:    PmeGpu* pmeGpu = pme->gpu;
src/gromacs/ewald/pme_gpu.cpp:    pmeGpu->settings.useGpuForceReduction = stepWork.useGpuPmeFReduction;
src/gromacs/ewald/pme_gpu.cpp:            shouldUpdateBox |= (pmeGpu->common->previousBox[i][j] != box[i][j]);
src/gromacs/ewald/pme_gpu.cpp:            pmeGpu->common->previousBox[i][j] = box[i][j];
src/gromacs/ewald/pme_gpu.cpp:        wallcycle_start(wcycle, WallCycleCounter::LaunchGpuPme);
src/gromacs/ewald/pme_gpu.cpp:        pme_gpu_update_input_box(pmeGpu, box);
src/gromacs/ewald/pme_gpu.cpp:        wallcycle_stop(wcycle, WallCycleCounter::LaunchGpuPme);
src/gromacs/ewald/pme_gpu.cpp:        if (!pme_gpu_settings(pmeGpu).performGPUSolve)
src/gromacs/ewald/pme_gpu.cpp:            pmeGpu->common->boxScaler->scaleBox(box, scaledBox);
src/gromacs/ewald/pme_gpu.cpp:void pme_gpu_launch_spread(gmx_pme_t*                     pme,
src/gromacs/ewald/pme_gpu.cpp:                           GpuEventSynchronizer*          xReadyOnDevice,
src/gromacs/ewald/pme_gpu.cpp:                           const bool                     useGpuDirectComm,
src/gromacs/ewald/pme_gpu.cpp:                           gmx::PmeCoordinateReceiverGpu* pmeCoordinateReceiverGpu,
src/gromacs/ewald/pme_gpu.cpp:                           const bool                     useMdGpuGraph)
src/gromacs/ewald/pme_gpu.cpp:    GMX_ASSERT(pme_gpu_active(pme), "This should be a GPU run of PME but it is not enabled.");
src/gromacs/ewald/pme_gpu.cpp:    GMX_ASSERT(pme->doCoulomb, "Only Coulomb PME can be run on GPU.");
src/gromacs/ewald/pme_gpu.cpp:    PmeGpu* pmeGpu = pme->gpu;
src/gromacs/ewald/pme_gpu.cpp:    GMX_ASSERT(pmeGpu->common->ngrids == 1 || (pmeGpu->common->ngrids == 2 && pme->bFEP_q),
src/gromacs/ewald/pme_gpu.cpp:    /* PME on GPU can currently manage two grids:
src/gromacs/ewald/pme_gpu.cpp:    pme_gpu_spread(pmeGpu,
src/gromacs/ewald/pme_gpu.cpp:                   useGpuDirectComm,
src/gromacs/ewald/pme_gpu.cpp:                   pmeCoordinateReceiverGpu,
src/gromacs/ewald/pme_gpu.cpp:                   useMdGpuGraph,
src/gromacs/ewald/pme_gpu.cpp:void pme_gpu_launch_complex_transforms(gmx_pme_t* pme, gmx_wallcycle* wcycle, const gmx::StepWorkload& stepWork)
src/gromacs/ewald/pme_gpu.cpp:    PmeGpu*     pmeGpu   = pme->gpu;
src/gromacs/ewald/pme_gpu.cpp:    const auto& settings = pmeGpu->settings;
src/gromacs/ewald/pme_gpu.cpp:    if (!settings.performGPUFFT)
src/gromacs/ewald/pme_gpu.cpp:        wallcycle_start(wcycle, WallCycleCounter::WaitGpuPmeGridD2hCopy);
src/gromacs/ewald/pme_gpu.cpp:        pme_gpu_sync_spread_grid(pme->gpu);
src/gromacs/ewald/pme_gpu.cpp:        wallcycle_stop(wcycle, WallCycleCounter::WaitGpuPmeGridD2hCopy);
src/gromacs/ewald/pme_gpu.cpp:        for (int gridIndex = 0; gridIndex < pmeGpu->common->ngrids; gridIndex++)
src/gromacs/ewald/pme_gpu.cpp:            parallel_3dfft_execute_gpu_wrapper(pme, gridIndex, GMX_FFT_REAL_TO_COMPLEX, wcycle);
src/gromacs/ewald/pme_gpu.cpp:            if (settings.performGPUSolve)
src/gromacs/ewald/pme_gpu.cpp:                wallcycle_start(wcycle, WallCycleCounter::LaunchGpuPme);
src/gromacs/ewald/pme_gpu.cpp:                pme_gpu_solve(pmeGpu, gridIndex, cfftgrid, gridOrdering, computeEnergyAndVirial);
src/gromacs/ewald/pme_gpu.cpp:                wallcycle_stop(wcycle, WallCycleCounter::LaunchGpuPme);
src/gromacs/ewald/pme_gpu.cpp:            parallel_3dfft_execute_gpu_wrapper(pme, gridIndex, GMX_FFT_COMPLEX_TO_REAL, wcycle);
src/gromacs/ewald/pme_gpu.cpp:void pme_gpu_launch_gather(gmx_pme_t* pme, gmx_wallcycle gmx_unused* wcycle, const real lambdaQ, const bool computeVirial)
src/gromacs/ewald/pme_gpu.cpp:    GMX_ASSERT(pme_gpu_active(pme), "This should be a GPU run of PME but it is not enabled.");
src/gromacs/ewald/pme_gpu.cpp:    if (!pme_gpu_settings(pme->gpu).performGPUGather)
src/gromacs/ewald/pme_gpu.cpp:    pme_gpu_gather(pme->gpu, pme->gridsCoulomb, lambdaQ, wcycle, computeVirial);
src/gromacs/ewald/pme_gpu.cpp:static void pme_gpu_reduce_outputs(const bool            computeEnergyAndVirial,
src/gromacs/ewald/pme_gpu.cpp:    wallcycle_start(wcycle, WallCycleCounter::PmeGpuFReduction);
src/gromacs/ewald/pme_gpu.cpp:    wallcycle_stop(wcycle, WallCycleCounter::PmeGpuFReduction);
src/gromacs/ewald/pme_gpu.cpp:bool pme_gpu_try_finish_task(gmx_pme_t*               pme,
src/gromacs/ewald/pme_gpu.cpp:                             GpuTaskCompletion        completionKind)
src/gromacs/ewald/pme_gpu.cpp:    GMX_ASSERT(pme_gpu_active(pme), "This should be a GPU run of PME but it is not enabled.");
src/gromacs/ewald/pme_gpu.cpp:    GMX_ASSERT(!pme->gpu->settings.useGpuForceReduction,
src/gromacs/ewald/pme_gpu.cpp:               "GPU force reduction should not be active on the pme_gpu_try_finish_task() path");
src/gromacs/ewald/pme_gpu.cpp:    constexpr bool c_streamQuerySupported = GMX_GPU_CUDA;
src/gromacs/ewald/pme_gpu.cpp:    // TODO: implement c_streamQuerySupported with an additional GpuEventSynchronizer per stream (#2521)
src/gromacs/ewald/pme_gpu.cpp:    if ((completionKind == GpuTaskCompletion::Check) && c_streamQuerySupported)
src/gromacs/ewald/pme_gpu.cpp:        wallcycle_start_nocount(wcycle, WallCycleCounter::WaitGpuPmeGather);
src/gromacs/ewald/pme_gpu.cpp:        const bool pmeGpuDone = pme_gpu_stream_query(pme->gpu);
src/gromacs/ewald/pme_gpu.cpp:        wallcycle_stop(wcycle, WallCycleCounter::WaitGpuPmeGather);
src/gromacs/ewald/pme_gpu.cpp:        if (!pmeGpuDone)
src/gromacs/ewald/pme_gpu.cpp:    wallcycle_start(wcycle, WallCycleCounter::WaitGpuPmeGather);
src/gromacs/ewald/pme_gpu.cpp:        pme_gpu_synchronize(pme->gpu);
src/gromacs/ewald/pme_gpu.cpp:    pme_gpu_update_timings(pme->gpu);
src/gromacs/ewald/pme_gpu.cpp:    PmeOutput  output                 = pme_gpu_getOutput(
src/gromacs/ewald/pme_gpu.cpp:            pme, computeEnergyAndVirial, pme->gpu->common->ngrids > 1 ? lambdaQ : 1.0);
src/gromacs/ewald/pme_gpu.cpp:    wallcycle_stop(wcycle, WallCycleCounter::WaitGpuPmeGather);
src/gromacs/ewald/pme_gpu.cpp:    GMX_ASSERT(pme->gpu->settings.useGpuForceReduction == !output.haveForceOutput_,
src/gromacs/ewald/pme_gpu.cpp:    pme_gpu_reduce_outputs(computeEnergyAndVirial, output, wcycle, forceWithVirial, enerd);
src/gromacs/ewald/pme_gpu.cpp:PmeOutput pme_gpu_wait_finish_task(gmx_pme_t*     pme,
src/gromacs/ewald/pme_gpu.cpp:    GMX_ASSERT(pme_gpu_active(pme), "This should be a GPU run of PME but it is not enabled.");
src/gromacs/ewald/pme_gpu.cpp:    wallcycle_start(wcycle, WallCycleCounter::WaitGpuPmeGather);
src/gromacs/ewald/pme_gpu.cpp:    if (!pme->gpu->settings.useGpuForceReduction || computeEnergyAndVirial)
src/gromacs/ewald/pme_gpu.cpp:        pme_gpu_synchronize(pme->gpu);
src/gromacs/ewald/pme_gpu.cpp:    PmeOutput output = pme_gpu_getOutput(
src/gromacs/ewald/pme_gpu.cpp:            pme, computeEnergyAndVirial, pme->gpu->common->ngrids > 1 ? lambdaQ : 1.0);
src/gromacs/ewald/pme_gpu.cpp:    wallcycle_stop(wcycle, WallCycleCounter::WaitGpuPmeGather);
src/gromacs/ewald/pme_gpu.cpp:void pme_gpu_wait_and_reduce(gmx_pme_t*               pme,
src/gromacs/ewald/pme_gpu.cpp:    PmeOutput  output                 = pme_gpu_wait_finish_task(
src/gromacs/ewald/pme_gpu.cpp:            pme, computeEnergyAndVirial, pme->gpu->common->ngrids > 1 ? lambdaQ : 1.0, wcycle);
src/gromacs/ewald/pme_gpu.cpp:    GMX_ASSERT(pme->gpu->settings.useGpuForceReduction == !output.haveForceOutput_,
src/gromacs/ewald/pme_gpu.cpp:    pme_gpu_reduce_outputs(computeEnergyAndVirial, output, wcycle, forceWithVirial, enerd);
src/gromacs/ewald/pme_gpu.cpp:void pme_gpu_reinit_computation(const gmx_pme_t* pme, const bool gpuGraphWithSeparatePmeRank, gmx_wallcycle* wcycle)
src/gromacs/ewald/pme_gpu.cpp:    GMX_ASSERT(pme_gpu_active(pme), "This should be a GPU run of PME but it is not enabled.");
src/gromacs/ewald/pme_gpu.cpp:    wallcycle_start(wcycle, WallCycleCounter::LaunchGpuPme);
src/gromacs/ewald/pme_gpu.cpp:    pme_gpu_update_timings(pme->gpu);
src/gromacs/ewald/pme_gpu.cpp:    pme_gpu_clear_grids(pme->gpu);
src/gromacs/ewald/pme_gpu.cpp:    pme_gpu_clear_energy_virial(pme->gpu, gpuGraphWithSeparatePmeRank);
src/gromacs/ewald/pme_gpu.cpp:    wallcycle_stop(wcycle, WallCycleCounter::LaunchGpuPme);
src/gromacs/ewald/pme_gpu.cpp:DeviceBuffer<gmx::RVec> pme_gpu_get_device_f(const gmx_pme_t* pme)
src/gromacs/ewald/pme_gpu.cpp:    if (!pme || !pme_gpu_active(pme))
src/gromacs/ewald/pme_gpu.cpp:    return pme_gpu_get_kernelparam_forces(pme->gpu);
src/gromacs/ewald/pme_gpu.cpp:void pme_gpu_set_device_x(const gmx_pme_t* pme, DeviceBuffer<gmx::RVec> d_x)
src/gromacs/ewald/pme_gpu.cpp:    GMX_ASSERT(pme_gpu_active(pme), "This should be a GPU run of PME but it is not enabled.");
src/gromacs/ewald/pme_gpu.cpp:    pme_gpu_set_kernelparam_coordinates(pme->gpu, d_x);
src/gromacs/ewald/pme_gpu.cpp:GpuEventSynchronizer* pme_gpu_get_f_ready_synchronizer(const gmx_pme_t* pme)
src/gromacs/ewald/pme_gpu.cpp:    if (!pme || !pme_gpu_active(pme))
src/gromacs/ewald/pme_gpu.cpp:    return pme_gpu_get_forces_ready_synchronizer(pme->gpu);
src/gromacs/ewald/pme_gpu.cpp:void pme_gpu_use_nvshmem(PmeGpu* pmeGpu, bool useNvshmem)
src/gromacs/ewald/pme_gpu.cpp:    pmeGpu->useNvshmem = useNvshmem;
src/gromacs/ewald/pme_gpu.cpp:    pme_gpu_set_kernelparam_useNvshmem(pmeGpu, useNvshmem);
src/gromacs/ewald/pme_gpu.cpp:        pmeGpu->nvshmemParams = std::make_unique<PmeNvshmemHost>();
src/gromacs/ewald/pme_solve.clh: *  \brief Implements PME OpenCL solving kernel.
src/gromacs/ewald/pme_solve.clh: * When including this and other PME OpenCL kernel files, plenty of common
src/gromacs/ewald/pme_solve.clh: * For details, please see how pme_program.cl is compiled in pme_gpu_program_impl_ocl.cpp.
src/gromacs/ewald/pme_solve.clh: * and corresponds to the dimension order of the grid (GridOrdering enum in CUDA kernels);
src/gromacs/ewald/pme_solve.clh:#include "gromacs/gpu_utils/vectype_ops.clh"
src/gromacs/ewald/pme_solve.clh:#include "pme_gpu_types.h"
src/gromacs/ewald/pme_solve.clh: * \param[in]     kernelParams         Input PME GPU data in constant memory.
src/gromacs/ewald/pme_solve.clh:CUSTOMIZED_KERNEL_NAME(pme_solve_kernel)(const struct PmeOpenCLKernelParams kernelParams,
src/gromacs/ewald/pme_solve.clh:    // This is only for reduction below. OpenCL 1.2: all local memory must be declared on kernel scope.
src/gromacs/ewald/pme_solve.clh:        // TODO: implement AMD intrinsics reduction, like with shuffles in CUDA version. #2514
src/gromacs/ewald/pme_solve.clh:#ifdef _NVIDIA_SOURCE_
src/gromacs/ewald/pme_solve.clh:                     * NVIDIA OpenCL of all things fails without the memory barrier here. #2519
src/gromacs/ewald/pme_gpu_settings.h: * \brief Defines the PME GPU settings data structures.
src/gromacs/ewald/pme_gpu_settings.h: * -- PmeGpuSettings -> PmeGpuTasks
src/gromacs/ewald/pme_gpu_settings.h:#ifndef GMX_EWALD_PME_GPU_SETTINGS_H
src/gromacs/ewald/pme_gpu_settings.h:#define GMX_EWALD_PME_GPU_SETTINGS_H
src/gromacs/ewald/pme_gpu_settings.h:#include "gromacs/gpu_utils/gpu_utils.h" // for GpuApiCallBehavior
src/gromacs/ewald/pme_gpu_settings.h: * The PME GPU settings structure, included in the main PME GPU structure by value.
src/gromacs/ewald/pme_gpu_settings.h:struct PmeGpuSettings
src/gromacs/ewald/pme_gpu_settings.h:    /*! \brief A boolean which tells if the solving is performed on GPU. Currently always true */
src/gromacs/ewald/pme_gpu_settings.h:    bool performGPUSolve;
src/gromacs/ewald/pme_gpu_settings.h:    /*! \brief A boolean which tells if the gathering is performed on GPU. Currently always true */
src/gromacs/ewald/pme_gpu_settings.h:    bool performGPUGather;
src/gromacs/ewald/pme_gpu_settings.h:    /*! \brief A boolean which tells if the FFT is performed on GPU. Currently true for a single MPI rank. */
src/gromacs/ewald/pme_gpu_settings.h:    bool performGPUFFT;
src/gromacs/ewald/pme_gpu_settings.h:    /*! \brief True if PME forces are reduced on-GPU, false if reduction is done on the CPU;
src/gromacs/ewald/pme_gpu_settings.h:    bool useGpuForceReduction;
src/gromacs/ewald/pme_gpu_settings.h:    /*! \brief A boolean which tells if any PME GPU stage should copy all of its outputs to the
src/gromacs/ewald/pme_gpu_settings.h:    /*! \brief An enum which tells whether most PME GPU D2H/H2D data transfers should be synchronous. */
src/gromacs/ewald/pme_gpu_settings.h:    GpuApiCallBehavior transferKind;
src/gromacs/ewald/pme_gpu_settings.h:     *  Controls whether we use order (i.e. 4) threads per atom for the GPU
src/gromacs/ewald/pme_gpu_settings.h:     *  Currently ThreadsPerAtom::Order is only supported by CUDA.
src/gromacs/ewald/pme_gpu_settings.h:     * Currently only supported by CUDA.
src/gromacs/ewald/pme_force_sender_gpu_impl.cpp: * \brief May be used to implement PME-PP GPU comm interfaces for non-GPU builds.
src/gromacs/ewald/pme_force_sender_gpu_impl.cpp: * Needed to satisfy compiler on systems, where CUDA is not available.
src/gromacs/ewald/pme_force_sender_gpu_impl.cpp: * \author Alan Gray <alang@nvidia.com>
src/gromacs/ewald/pme_force_sender_gpu_impl.cpp:#include "gromacs/ewald/pme_force_sender_gpu.h"
src/gromacs/ewald/pme_force_sender_gpu_impl.cpp:#include "gromacs/gpu_utils/devicebuffer_datatype.h"
src/gromacs/ewald/pme_force_sender_gpu_impl.cpp:class GpuEventSynchronizer;
src/gromacs/ewald/pme_force_sender_gpu_impl.cpp:#if !GMX_GPU_CUDA && !GMX_GPU_SYCL
src/gromacs/ewald/pme_force_sender_gpu_impl.cpp:class PmeForceSenderGpu::Impl
src/gromacs/ewald/pme_force_sender_gpu_impl.cpp:PmeForceSenderGpu::PmeForceSenderGpu(GpuEventSynchronizer* /*pmeForcesReady */,
src/gromacs/ewald/pme_force_sender_gpu_impl.cpp:               "A CPU stub for PME-PP GPU communication was called instead of the correct "
src/gromacs/ewald/pme_force_sender_gpu_impl.cpp:PmeForceSenderGpu::~PmeForceSenderGpu() = default;
src/gromacs/ewald/pme_force_sender_gpu_impl.cpp:/*!\brief init PME-PP GPU communication stub */
src/gromacs/ewald/pme_force_sender_gpu_impl.cpp:void PmeForceSenderGpu::setForceSendBuffer(DeviceBuffer<RVec> /* d_f */)
src/gromacs/ewald/pme_force_sender_gpu_impl.cpp:               "A CPU stub for PME-PP GPU communication initialization was called instead of the "
src/gromacs/ewald/pme_force_sender_gpu_impl.cpp:void PmeForceSenderGpu::sendFToPpPeerToPeer(int /* ppRank */,
src/gromacs/ewald/pme_force_sender_gpu_impl.cpp:                                            bool /* sendForcesDirectToPpGpu */)
src/gromacs/ewald/pme_force_sender_gpu_impl.cpp:               "A CPU stub for PME-PP GPU communication was called instead of the correct "
src/gromacs/ewald/pme_force_sender_gpu_impl.cpp:void PmeForceSenderGpu::sendFToPpGpuAwareMpi(DeviceBuffer<RVec> /* sendbuf */,
src/gromacs/ewald/pme_force_sender_gpu_impl.cpp:               "A CPU stub for PME-PP GPU communication was called instead of the correct "
src/gromacs/ewald/pme_force_sender_gpu_impl.cpp:void PmeForceSenderGpu::waitForEvents()
src/gromacs/ewald/pme_force_sender_gpu_impl.cpp:               "A CPU stub for PME-PP GPU communication was called instead of the correct "
src/gromacs/ewald/pme_force_sender_gpu_impl.cpp:#endif // !GMX_GPU_CUDA
src/gromacs/ewald/pme.cuh: * \brief This file defines the PME CUDA-specific kernel parameter data structure.
src/gromacs/ewald/pme.cuh: * \todo Rename the file (pme-gpu-types.cuh?), reconsider inheritance approach.
src/gromacs/ewald/pme.cuh:#include "pme_gpu_constants.h"
src/gromacs/ewald/pme.cuh:#include "pme_gpu_internal.h" // for GridOrdering
src/gromacs/ewald/pme.cuh:#include "pme_gpu_types.h"
src/gromacs/ewald/pme.cuh: * An alias for PME parameters in CUDA.
src/gromacs/ewald/pme.cuh: * \todo Remove if we decide to unify CUDA and OpenCL
src/gromacs/ewald/pme.cuh:struct PmeGpuCudaKernelParams : PmeGpuKernelParamsBase
src/gromacs/ewald/pme.cuh:    // Place CUDA-specific stuff here
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu_sycl.cpp:#include "pme_pp_comm_gpu_impl.h"
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu_sycl.cpp:void PmePpCommGpu::Impl::sendCoordinatesToPmePeerToPeer(Float3* /*sendPtr*/,
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu_sycl.cpp:                                                        GpuEventSynchronizer* /*coordinatesReadyOnDeviceEvent*/)
src/gromacs/ewald/pme_gpu_program_impl.cu: * Implements PmeGpuProgramImpl, which stores permanent PME GPU context-derived data,
src/gromacs/ewald/pme_gpu_program_impl.cu:#include "pme_gpu_program_impl.h"
src/gromacs/ewald/pme_gpu_program_impl.cu:#include "pme_gpu_constants.h"
src/gromacs/ewald/pme_gpu_program_impl.cu:#include "pme_gpu_internal.h" // for GridOrdering enum
src/gromacs/ewald/pme_gpu_program_impl.cu:#include "pme_gpu_types_host.h"
src/gromacs/ewald/pme_gpu_program_impl.cu:// These hardcoded spread/gather parameters refer to not-implemented PME GPU 2D decomposition in X/Y
src/gromacs/ewald/pme_gpu_program_impl.cu://! PME CUDA kernels forward declarations. Kernels are documented in their respective files.
src/gromacs/ewald/pme_gpu_program_impl.cu:__global__ void pme_spline_and_spread_kernel(PmeGpuCudaKernelParams kernelParams);
src/gromacs/ewald/pme_gpu_program_impl.cu:pme_spline_and_spread_kernel<c_pmeOrder, true, true, c_wrapX, c_wrapY, 1, true, ThreadsPerAtom::Order>(const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_gpu_program_impl.cu:pme_spline_and_spread_kernel<c_pmeOrder, true, true, c_wrapX, c_wrapY, 1, true, ThreadsPerAtom::OrderSquared>(const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_gpu_program_impl.cu:pme_spline_and_spread_kernel<c_pmeOrder, true, false, c_wrapX, c_wrapY, 1, true, ThreadsPerAtom::Order>(const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_gpu_program_impl.cu:pme_spline_and_spread_kernel<c_pmeOrder, true, false, c_wrapX, c_wrapY, 1, true, ThreadsPerAtom::OrderSquared>(const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_gpu_program_impl.cu:pme_spline_and_spread_kernel<c_pmeOrder, false, true, c_wrapX, c_wrapY, 1, true, ThreadsPerAtom::Order>(const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_gpu_program_impl.cu:pme_spline_and_spread_kernel<c_pmeOrder, false, true, c_wrapX, c_wrapY, 1, true, ThreadsPerAtom::OrderSquared>(const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_gpu_program_impl.cu:pme_spline_and_spread_kernel<c_pmeOrder, true, true, c_wrapX, c_wrapY, 1, false, ThreadsPerAtom::Order>(const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_gpu_program_impl.cu:pme_spline_and_spread_kernel<c_pmeOrder, true, true, c_wrapX, c_wrapY, 1, false, ThreadsPerAtom::OrderSquared>(const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_gpu_program_impl.cu:pme_spline_and_spread_kernel<c_pmeOrder, true, true, c_wrapX, c_wrapY, 2, true, ThreadsPerAtom::Order>(const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_gpu_program_impl.cu:pme_spline_and_spread_kernel<c_pmeOrder, true, true, c_wrapX, c_wrapY, 2, true, ThreadsPerAtom::OrderSquared>(const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_gpu_program_impl.cu:pme_spline_and_spread_kernel<c_pmeOrder, true, false, c_wrapX, c_wrapY, 2, true, ThreadsPerAtom::Order>(const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_gpu_program_impl.cu:pme_spline_and_spread_kernel<c_pmeOrder, true, false, c_wrapX, c_wrapY, 2, true, ThreadsPerAtom::OrderSquared>(const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_gpu_program_impl.cu:pme_spline_and_spread_kernel<c_pmeOrder, false, true, c_wrapX, c_wrapY, 2, true, ThreadsPerAtom::Order>(const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_gpu_program_impl.cu:pme_spline_and_spread_kernel<c_pmeOrder, false, true, c_wrapX, c_wrapY, 2, true, ThreadsPerAtom::OrderSquared>(const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_gpu_program_impl.cu:pme_spline_and_spread_kernel<c_pmeOrder, true, true, c_wrapX, c_wrapY, 2, false, ThreadsPerAtom::Order>(const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_gpu_program_impl.cu:pme_spline_and_spread_kernel<c_pmeOrder, true, true, c_wrapX, c_wrapY, 2, false, ThreadsPerAtom::OrderSquared>(const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_gpu_program_impl.cu:__global__ void pme_solve_kernel(PmeGpuCudaKernelParams kernelParams);
src/gromacs/ewald/pme_gpu_program_impl.cu:extern template __global__ void pme_solve_kernel<GridOrdering::XYZ, false, c_stateA>(const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_gpu_program_impl.cu:extern template __global__ void pme_solve_kernel<GridOrdering::XYZ, true, c_stateA>(const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_gpu_program_impl.cu:extern template __global__ void pme_solve_kernel<GridOrdering::YZX, false, c_stateA>(const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_gpu_program_impl.cu:extern template __global__ void pme_solve_kernel<GridOrdering::YZX, true, c_stateA>(const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_gpu_program_impl.cu:extern template __global__ void pme_solve_kernel<GridOrdering::XYZ, false, c_stateB>(const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_gpu_program_impl.cu:extern template __global__ void pme_solve_kernel<GridOrdering::XYZ, true, c_stateB>(const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_gpu_program_impl.cu:extern template __global__ void pme_solve_kernel<GridOrdering::YZX, false, c_stateB>(const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_gpu_program_impl.cu:extern template __global__ void pme_solve_kernel<GridOrdering::YZX, true, c_stateB>(const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_gpu_program_impl.cu:__global__ void pme_gather_kernel(PmeGpuCudaKernelParams kernelParams);
src/gromacs/ewald/pme_gpu_program_impl.cu:__global__ void nvshmemSignalKernel(PmeGpuCudaKernelParams kernelParams);
src/gromacs/ewald/pme_gpu_program_impl.cu:extern template __global__ void pme_gather_kernel<c_pmeOrder, c_wrapX, c_wrapY, 1, true, ThreadsPerAtom::Order>        (const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_gpu_program_impl.cu:extern template __global__ void pme_gather_kernel<c_pmeOrder, c_wrapX, c_wrapY, 1, false, ThreadsPerAtom::Order>       (const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_gpu_program_impl.cu:extern template __global__ void pme_gather_kernel<c_pmeOrder, c_wrapX, c_wrapY, 1, true, ThreadsPerAtom::OrderSquared> (const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_gpu_program_impl.cu:extern template __global__ void pme_gather_kernel<c_pmeOrder, c_wrapX, c_wrapY, 1, false, ThreadsPerAtom::OrderSquared>(const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_gpu_program_impl.cu:extern template __global__ void pme_gather_kernel<c_pmeOrder, c_wrapX, c_wrapY, 2, true, ThreadsPerAtom::Order>          (const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_gpu_program_impl.cu:extern template __global__ void pme_gather_kernel<c_pmeOrder, c_wrapX, c_wrapY, 2, false, ThreadsPerAtom::Order>         (const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_gpu_program_impl.cu:extern template __global__ void pme_gather_kernel<c_pmeOrder, c_wrapX, c_wrapY, 2, true, ThreadsPerAtom::OrderSquared>   (const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_gpu_program_impl.cu:extern template __global__ void pme_gather_kernel<c_pmeOrder, c_wrapX, c_wrapY, 2, false, ThreadsPerAtom::OrderSquared>  (const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_gpu_program_impl.cu:PmeGpuProgramImpl::PmeGpuProgramImpl(const DeviceContext& deviceContext) :
src/gromacs/ewald/pme_gpu_program_impl.cu:PmeGpuProgramImpl::~PmeGpuProgramImpl() = default;
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl_gpu.cpp: * \brief Implements class which receives coordinates to GPU memory on PME task using CUDA/SYCL.
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl_gpu.cpp: * \author Alan Gray <alang@nvidia.com>
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl_gpu.cpp:#include "gromacs/ewald/pme_force_sender_gpu.h"
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl_gpu.cpp:#include "gromacs/gpu_utils/device_stream.h"
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl_gpu.cpp:#include "gromacs/gpu_utils/devicebuffer.h"
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl_gpu.cpp:#include "gromacs/gpu_utils/gpueventsynchronizer.h"
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl_gpu.cpp:#include "pme_coordinate_receiver_gpu_impl.h"
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl_gpu.cpp:PmeCoordinateReceiverGpu::Impl::Impl(MPI_Comm                     comm,
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl_gpu.cpp:                std::make_unique<GpuEventSynchronizer>(),
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl_gpu.cpp:PmeCoordinateReceiverGpu::Impl::~Impl() = default;
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl_gpu.cpp:void PmeCoordinateReceiverGpu::Impl::reinitCoordinateReceiver(DeviceBuffer<RVec> d_x)
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl_gpu.cpp:        // Need to send address to PP rank only for thread-MPI as PP rank pushes data using cudamemcpy
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl_gpu.cpp:        // Skip receiving x buffer pointer when the PP domain is empty (the matching call in `pmePpCommGpu->reinit(n)` is also conditional)
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl_gpu.cpp:                    GMX_GPU_CUDA,
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl_gpu.cpp:                    "Direct PME-PP communication with threadMPI is only supported with CUDA.");
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl_gpu.cpp:            // Data will be transferred directly from GPU.
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl_gpu.cpp:void PmeCoordinateReceiverGpu::Impl::receiveCoordinatesSynchronizerFromPpPeerToPeer(int ppRank)
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl_gpu.cpp:    GMX_ASSERT(!GMX_GPU_SYCL,
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl_gpu.cpp:              sizeof(GpuEventSynchronizer*), // NOLINT(bugprone-sizeof-expression)
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl_gpu.cpp:/*! \brief Receive coordinate data using GPU-aware MPI */
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl_gpu.cpp:void PmeCoordinateReceiverGpu::Impl::launchReceiveCoordinatesFromPpGpuAwareMpi(DeviceBuffer<RVec> recvbuf,
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl_gpu.cpp:            "launchReceiveCoordinatesFromPpGpuAwareMpi is expected to be called only for Lib-MPI");
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl_gpu.cpp:              eCommType_COORD_GPU,
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl_gpu.cpp:std::tuple<int, GpuEventSynchronizer*> PmeCoordinateReceiverGpu::Impl::receivePpCoordinateSendEvent(int pipelineStage)
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl_gpu.cpp:        // scheduling code to receive a CUDA event, and will be executed
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl_gpu.cpp:        // scheduled GPU-direct comms to initiate out-of-order in their
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl_gpu.cpp:int PmeCoordinateReceiverGpu::Impl::waitForCoordinatesFromAnyPpRank()
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl_gpu.cpp:    // Wait on data from any one of the PP sender GPUs
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl_gpu.cpp:DeviceStream* PmeCoordinateReceiverGpu::Impl::ppCommStream(int senderIndex)
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl_gpu.cpp:std::tuple<int, int> PmeCoordinateReceiverGpu::Impl::ppCommAtomRange(int senderIndex)
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl_gpu.cpp:int PmeCoordinateReceiverGpu::Impl::ppCommNumSenderRanks()
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl_gpu.cpp:void PmeCoordinateReceiverGpu::Impl::insertAsDependencyIntoStream(int senderIndex, const DeviceStream& stream)
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl_gpu.cpp:PmeCoordinateReceiverGpu::PmeCoordinateReceiverGpu(MPI_Comm               comm,
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl_gpu.cpp:PmeCoordinateReceiverGpu::~PmeCoordinateReceiverGpu() = default;
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl_gpu.cpp:void PmeCoordinateReceiverGpu::reinitCoordinateReceiver(DeviceBuffer<RVec> d_x)
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl_gpu.cpp:void PmeCoordinateReceiverGpu::receiveCoordinatesSynchronizerFromPpPeerToPeer(int ppRank)
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl_gpu.cpp:void PmeCoordinateReceiverGpu::launchReceiveCoordinatesFromPpGpuAwareMpi(DeviceBuffer<RVec> recvbuf,
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl_gpu.cpp:    impl_->launchReceiveCoordinatesFromPpGpuAwareMpi(recvbuf, numAtoms, numBytes, ppRank, senderIndex);
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl_gpu.cpp:std::tuple<int, GpuEventSynchronizer*> PmeCoordinateReceiverGpu::receivePpCoordinateSendEvent(int pipelineStage)
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl_gpu.cpp:int PmeCoordinateReceiverGpu::waitForCoordinatesFromAnyPpRank()
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl_gpu.cpp:DeviceStream* PmeCoordinateReceiverGpu::ppCommStream(int senderIndex)
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl_gpu.cpp:std::tuple<int, int> PmeCoordinateReceiverGpu::ppCommAtomRange(int senderIndex)
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl_gpu.cpp:int PmeCoordinateReceiverGpu::ppCommNumSenderRanks()
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl_gpu.cpp:void PmeCoordinateReceiverGpu::insertAsDependencyIntoStream(int senderIndex, const DeviceStream& stream)
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl.h: * \brief Declaration of class which receives coordinates to GPU memory on PME task
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl.h: * \author Alan Gray <alang@nvidia.com>
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl.h:#ifndef GMX_PMECOORDINATERECEIVERGPU_IMPL_H
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl.h:#define GMX_PMECOORDINATERECEIVERGPU_IMPL_H
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl.h:#include "gromacs/ewald/pme_coordinate_receiver_gpu.h"
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl.h:class GpuEventSynchronizer;
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl.h:    GpuEventSynchronizer* sync = nullptr;
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl.h:    std::unique_ptr<GpuEventSynchronizer> ready;
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl.h:/*! \internal \brief Class with interfaces and data for CUDA version of PME coordinate receiving functionality */
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl.h:class PmeCoordinateReceiverGpu::Impl
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl.h:    /*! \brief Creates PME GPU coordinate receiver object
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl.h:     * \param[in] deviceContext   GPU context
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl.h:     * \param[in] d_x   coordinates buffer in GPU memory
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl.h:     * \param[in] recvbuf      coordinates buffer in GPU memory
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl.h:    void launchReceiveCoordinatesFromPpGpuAwareMpi(DeviceBuffer<RVec> recvbuf,
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl.h:    std::tuple<int, GpuEventSynchronizer*> receivePpCoordinateSendEvent(int pipelineStage);
src/gromacs/ewald/pme_gpu_program_impl_hip.cpp: * Implements PmeGpuProgramImpl, which stores permanent PME GPU context-derived data,
src/gromacs/ewald/pme_gpu_program_impl_hip.cpp:#include "pme_gpu_program_impl.h"
src/gromacs/ewald/pme_gpu_program_impl_hip.cpp:PmeGpuProgramImpl::PmeGpuProgramImpl(const DeviceContext& deviceContext) :
src/gromacs/ewald/pme_gpu_program_impl_hip.cpp:PmeGpuProgramImpl::~PmeGpuProgramImpl() {}
src/gromacs/ewald/pme_gpu_grid.cu: * \brief Implements PME GPU halo exchange and PME GPU - Host FFT grid conversion
src/gromacs/ewald/pme_gpu_grid.cu: * \author Gaurav Garg <gaugarg@nvidia.com>
src/gromacs/ewald/pme_gpu_grid.cu:#include "pme_gpu_grid.h"
src/gromacs/ewald/pme_gpu_grid.cu:#include "gromacs/gpu_utils/cudautils.cuh"
src/gromacs/ewald/pme_gpu_grid.cu:#include "gromacs/gpu_utils/devicebuffer.cuh"
src/gromacs/ewald/pme_gpu_grid.cu:#include "pme_gpu_types.h"
src/gromacs/ewald/pme_gpu_grid.cu:#include "pme_gpu_types_host.h"
src/gromacs/ewald/pme_gpu_grid.cu:#include "pme_gpu_types_host_impl.h"
src/gromacs/ewald/pme_gpu_grid.cu: * A CUDA kernel which packs non-contiguous overlap data in all 8 neighboring directions
src/gromacs/ewald/pme_gpu_grid.cu:static __global__ void pmeGpuPackHaloExternal(const float* __restrict__ gm_realGrid,
src/gromacs/ewald/pme_gpu_grid.cu: * A CUDA kernel which assigns data in halo region in all 8 neighboring directions
src/gromacs/ewald/pme_gpu_grid.cu:static __global__ void pmeGpuUnpackHaloExternal(float* __restrict__ gm_realGrid,
src/gromacs/ewald/pme_gpu_grid.cu: * A CUDA kernel which adds grid overlap data received from neighboring ranks
src/gromacs/ewald/pme_gpu_grid.cu:static __global__ void pmeGpuUnpackAndAddHaloInternal(float* __restrict__ gm_realGrid,
src/gromacs/ewald/pme_gpu_grid.cu: * A CUDA kernel which packs non-contiguous overlap data in all 8 neighboring directions
src/gromacs/ewald/pme_gpu_grid.cu:static __global__ void pmeGpuPackHaloInternal(const float* __restrict__ gm_realGrid,
src/gromacs/ewald/pme_gpu_grid.cu: * A CUDA kernel which copies data from pme grid to FFT grid and back
src/gromacs/ewald/pme_gpu_grid.cu: * Launches CUDA kernel to pack non-contiguous external halo data
src/gromacs/ewald/pme_gpu_grid.cu:static void packHaloDataExternal(const PmeGpu*       pmeGpu,
src/gromacs/ewald/pme_gpu_grid.cu:    auto kernelFn   = pmeGpuPackHaloExternal;
src/gromacs/ewald/pme_gpu_grid.cu:    auto kernelArgs = prepareGpuKernelArguments(kernelFn,
src/gromacs/ewald/pme_gpu_grid.cu:    launchGpuKernel(kernelFn,
src/gromacs/ewald/pme_gpu_grid.cu:                    pmeGpu->archSpecific->pmeStream_,
src/gromacs/ewald/pme_gpu_grid.cu:                    "PME Domdec GPU Pack Grid Halo Exchange",
src/gromacs/ewald/pme_gpu_grid.cu: * Launches CUDA kernel to pack non-contiguous internal halo data
src/gromacs/ewald/pme_gpu_grid.cu:static void packHaloDataInternal(const PmeGpu*       pmeGpu,
src/gromacs/ewald/pme_gpu_grid.cu:    auto kernelFn   = pmeGpuPackHaloInternal;
src/gromacs/ewald/pme_gpu_grid.cu:    auto kernelArgs = prepareGpuKernelArguments(kernelFn,
src/gromacs/ewald/pme_gpu_grid.cu:    launchGpuKernel(kernelFn,
src/gromacs/ewald/pme_gpu_grid.cu:                    pmeGpu->archSpecific->pmeStream_,
src/gromacs/ewald/pme_gpu_grid.cu:                    "PME Domdec GPU Pack Grid Halo Exchange",
src/gromacs/ewald/pme_gpu_grid.cu: * Launches CUDA kernel to unpack and reduce overlap data
src/gromacs/ewald/pme_gpu_grid.cu:static void unpackAndAddHaloDataInternal(const PmeGpu*       pmeGpu,
src/gromacs/ewald/pme_gpu_grid.cu:    auto kernelFn = pmeGpuUnpackAndAddHaloInternal;
src/gromacs/ewald/pme_gpu_grid.cu:    auto kernelArgs = prepareGpuKernelArguments(kernelFn,
src/gromacs/ewald/pme_gpu_grid.cu:    launchGpuKernel(kernelFn,
src/gromacs/ewald/pme_gpu_grid.cu:                    pmeGpu->archSpecific->pmeStream_,
src/gromacs/ewald/pme_gpu_grid.cu:                    "PME Domdec GPU Pack Grid Halo Exchange",
src/gromacs/ewald/pme_gpu_grid.cu: * Launches CUDA kernel to initialize overlap data
src/gromacs/ewald/pme_gpu_grid.cu:static void unpackHaloDataExternal(const PmeGpu*       pmeGpu,
src/gromacs/ewald/pme_gpu_grid.cu:    auto kernelFn   = pmeGpuUnpackHaloExternal;
src/gromacs/ewald/pme_gpu_grid.cu:    auto kernelArgs = prepareGpuKernelArguments(kernelFn,
src/gromacs/ewald/pme_gpu_grid.cu:    launchGpuKernel(kernelFn,
src/gromacs/ewald/pme_gpu_grid.cu:                    pmeGpu->archSpecific->pmeStream_,
src/gromacs/ewald/pme_gpu_grid.cu:                    "PME Domdec GPU Pack Grid Halo Exchange",
src/gromacs/ewald/pme_gpu_grid.cu:void pmeGpuGridHaloExchange(const PmeGpu* pmeGpu, gmx_wallcycle* wcycle)
src/gromacs/ewald/pme_gpu_grid.cu:    auto* kernelParamsPtr = pmeGpu->kernelParams.get();
src/gromacs/ewald/pme_gpu_grid.cu:    int overlapX = pmeGpu->haloExchange->haloSizeX[gmx::DirectionX::Center];
src/gromacs/ewald/pme_gpu_grid.cu:    int overlapY = pmeGpu->haloExchange->haloSizeY[gmx::DirectionY::Center];
src/gromacs/ewald/pme_gpu_grid.cu:    int overlapDown = pmeGpu->haloExchange->haloSizeX[gmx::DirectionX::Down];
src/gromacs/ewald/pme_gpu_grid.cu:    int overlapUp   = pmeGpu->haloExchange->haloSizeX[gmx::DirectionX::Up];
src/gromacs/ewald/pme_gpu_grid.cu:    int overlapRight = pmeGpu->haloExchange->haloSizeY[gmx::DirectionY::Right];
src/gromacs/ewald/pme_gpu_grid.cu:    int overlapLeft  = pmeGpu->haloExchange->haloSizeY[gmx::DirectionY::Left];
src/gromacs/ewald/pme_gpu_grid.cu:    int myGridX = pmeGpu->haloExchange->gridSizeX;
src/gromacs/ewald/pme_gpu_grid.cu:    int myGridY = pmeGpu->haloExchange->gridSizeY;
src/gromacs/ewald/pme_gpu_grid.cu:    int sizeX = pmeGpu->common->nnodesX;
src/gromacs/ewald/pme_gpu_grid.cu:    int down  = pmeGpu->haloExchange->ranksX[gmx::DirectionX::Down];
src/gromacs/ewald/pme_gpu_grid.cu:    int up    = pmeGpu->haloExchange->ranksX[gmx::DirectionX::Up];
src/gromacs/ewald/pme_gpu_grid.cu:    int sizeY = pmeGpu->common->nnodesY;
src/gromacs/ewald/pme_gpu_grid.cu:    int right = pmeGpu->haloExchange->ranksY[gmx::DirectionY::Right];
src/gromacs/ewald/pme_gpu_grid.cu:    int left  = pmeGpu->haloExchange->ranksY[gmx::DirectionY::Left];
src/gromacs/ewald/pme_gpu_grid.cu:    for (int gridIndex = 0; gridIndex < pmeGpu->common->ngrids; gridIndex++)
src/gromacs/ewald/pme_gpu_grid.cu:        float*      realGrid = pmeGpu->kernelParams->grid.d_realGrid[gridIndex];
src/gromacs/ewald/pme_gpu_grid.cu:                pmeGpu->haloExchange->d_sendGrids[gmx::DirectionX::Up][gmx::DirectionY::Center];
src/gromacs/ewald/pme_gpu_grid.cu:                pmeGpu->haloExchange->d_sendGrids[gmx::DirectionX::Down][gmx::DirectionY::Center];
src/gromacs/ewald/pme_gpu_grid.cu:        if (pmeGpu->common->nnodesY == 1)
src/gromacs/ewald/pme_gpu_grid.cu:            wallcycle_start(wcycle, WallCycleCounter::LaunchGpuPme);
src/gromacs/ewald/pme_gpu_grid.cu:                    pmeGpu,
src/gromacs/ewald/pme_gpu_grid.cu:                    pmeGpu->haloExchange->d_sendGrids[gmx::DirectionX::Center][gmx::DirectionY::Left],
src/gromacs/ewald/pme_gpu_grid.cu:                    pmeGpu->haloExchange->d_sendGrids[gmx::DirectionX::Center][gmx::DirectionY::Right],
src/gromacs/ewald/pme_gpu_grid.cu:                    pmeGpu->haloExchange->d_sendGrids[gmx::DirectionX::Up][gmx::DirectionY::Left],
src/gromacs/ewald/pme_gpu_grid.cu:                    pmeGpu->haloExchange->d_sendGrids[gmx::DirectionX::Down][gmx::DirectionY::Left],
src/gromacs/ewald/pme_gpu_grid.cu:                    pmeGpu->haloExchange->d_sendGrids[gmx::DirectionX::Up][gmx::DirectionY::Right],
src/gromacs/ewald/pme_gpu_grid.cu:                    pmeGpu->haloExchange->d_sendGrids[gmx::DirectionX::Down][gmx::DirectionY::Right]);
src/gromacs/ewald/pme_gpu_grid.cu:            wallcycle_stop(wcycle, WallCycleCounter::LaunchGpuPme);
src/gromacs/ewald/pme_gpu_grid.cu:        wallcycle_start(wcycle, WallCycleCounter::WaitGpuPmeSpread);
src/gromacs/ewald/pme_gpu_grid.cu:        // Make sure data is ready on GPU before MPI communication.
src/gromacs/ewald/pme_gpu_grid.cu:        pmeGpu->archSpecific->pmeStream_.synchronize();
src/gromacs/ewald/pme_gpu_grid.cu:        wallcycle_stop(wcycle, WallCycleCounter::WaitGpuPmeSpread);
src/gromacs/ewald/pme_gpu_grid.cu:                           pmeGpu->haloExchange->d_recvGrids[gmx::DirectionX::Up][gmx::DirectionY::Center],
src/gromacs/ewald/pme_gpu_grid.cu:                           pmeGpu->common->mpiCommX);
src/gromacs/ewald/pme_gpu_grid.cu:                        pmeGpu->haloExchange->d_recvGrids[gmx::DirectionX::Down][gmx::DirectionY::Center],
src/gromacs/ewald/pme_gpu_grid.cu:                        pmeGpu->common->mpiCommX);
src/gromacs/ewald/pme_gpu_grid.cu:                    pmeGpu->haloExchange->d_sendGrids[gmx::DirectionX::Center][gmx::DirectionY::Right],
src/gromacs/ewald/pme_gpu_grid.cu:                    pmeGpu->haloExchange->d_recvGrids[gmx::DirectionX::Center][gmx::DirectionY::Left],
src/gromacs/ewald/pme_gpu_grid.cu:                    pmeGpu->common->mpiCommY);
src/gromacs/ewald/pme_gpu_grid.cu:                        pmeGpu->haloExchange->d_sendGrids[gmx::DirectionX::Center][gmx::DirectionY::Left],
src/gromacs/ewald/pme_gpu_grid.cu:                        pmeGpu->haloExchange->d_recvGrids[gmx::DirectionX::Center][gmx::DirectionY::Right],
src/gromacs/ewald/pme_gpu_grid.cu:                        pmeGpu->common->mpiCommY);
src/gromacs/ewald/pme_gpu_grid.cu:                    pmeGpu->haloExchange->d_sendGrids[gmx::DirectionX::Down][gmx::DirectionY::Right],
src/gromacs/ewald/pme_gpu_grid.cu:                    pmeGpu->haloExchange->d_recvGrids[gmx::DirectionX::Up][gmx::DirectionY::Left],
src/gromacs/ewald/pme_gpu_grid.cu:                    pmeGpu->common->mpiComm);
src/gromacs/ewald/pme_gpu_grid.cu:                        pmeGpu->haloExchange->d_sendGrids[gmx::DirectionX::Down][gmx::DirectionY::Left],
src/gromacs/ewald/pme_gpu_grid.cu:                        pmeGpu->haloExchange->d_recvGrids[gmx::DirectionX::Up][gmx::DirectionY::Right],
src/gromacs/ewald/pme_gpu_grid.cu:                        pmeGpu->common->mpiComm);
src/gromacs/ewald/pme_gpu_grid.cu:                        pmeGpu->haloExchange->d_sendGrids[gmx::DirectionX::Up][gmx::DirectionY::Right],
src/gromacs/ewald/pme_gpu_grid.cu:                        pmeGpu->haloExchange->d_recvGrids[gmx::DirectionX::Down][gmx::DirectionY::Left],
src/gromacs/ewald/pme_gpu_grid.cu:                        pmeGpu->common->mpiComm);
src/gromacs/ewald/pme_gpu_grid.cu:                        pmeGpu->haloExchange->d_sendGrids[gmx::DirectionX::Up][gmx::DirectionY::Left],
src/gromacs/ewald/pme_gpu_grid.cu:                        pmeGpu->haloExchange->d_recvGrids[gmx::DirectionX::Down][gmx::DirectionY::Right],
src/gromacs/ewald/pme_gpu_grid.cu:                        pmeGpu->common->mpiComm);
src/gromacs/ewald/pme_gpu_grid.cu:        wallcycle_start(wcycle, WallCycleCounter::LaunchGpuPme);
src/gromacs/ewald/pme_gpu_grid.cu:                pmeGpu,
src/gromacs/ewald/pme_gpu_grid.cu:                pmeGpu->haloExchange->d_recvGrids[gmx::DirectionX::Up][gmx::DirectionY::Center],
src/gromacs/ewald/pme_gpu_grid.cu:                pmeGpu->haloExchange->d_recvGrids[gmx::DirectionX::Down][gmx::DirectionY::Center],
src/gromacs/ewald/pme_gpu_grid.cu:                pmeGpu->haloExchange->d_recvGrids[gmx::DirectionX::Center][gmx::DirectionY::Left],
src/gromacs/ewald/pme_gpu_grid.cu:                pmeGpu->haloExchange->d_recvGrids[gmx::DirectionX::Center][gmx::DirectionY::Right],
src/gromacs/ewald/pme_gpu_grid.cu:                pmeGpu->haloExchange->d_recvGrids[gmx::DirectionX::Up][gmx::DirectionY::Left],
src/gromacs/ewald/pme_gpu_grid.cu:                pmeGpu->haloExchange->d_recvGrids[gmx::DirectionX::Down][gmx::DirectionY::Left],
src/gromacs/ewald/pme_gpu_grid.cu:                pmeGpu->haloExchange->d_recvGrids[gmx::DirectionX::Up][gmx::DirectionY::Right],
src/gromacs/ewald/pme_gpu_grid.cu:                pmeGpu->haloExchange->d_recvGrids[gmx::DirectionX::Down][gmx::DirectionY::Right]);
src/gromacs/ewald/pme_gpu_grid.cu:        wallcycle_stop(wcycle, WallCycleCounter::LaunchGpuPme);
src/gromacs/ewald/pme_gpu_grid.cu:    GMX_UNUSED_VALUE(pmeGpu);
src/gromacs/ewald/pme_gpu_grid.cu:void pmeGpuGridHaloExchangeReverse(const PmeGpu* pmeGpu, gmx_wallcycle* wcycle)
src/gromacs/ewald/pme_gpu_grid.cu:    auto* kernelParamsPtr = pmeGpu->kernelParams.get();
src/gromacs/ewald/pme_gpu_grid.cu:    int overlapX = pmeGpu->haloExchange->haloSizeX[gmx::DirectionX::Center];
src/gromacs/ewald/pme_gpu_grid.cu:    int overlapY = pmeGpu->haloExchange->haloSizeY[gmx::DirectionY::Center];
src/gromacs/ewald/pme_gpu_grid.cu:    int overlapDown = pmeGpu->haloExchange->haloSizeX[gmx::DirectionX::Down];
src/gromacs/ewald/pme_gpu_grid.cu:    int overlapUp   = pmeGpu->haloExchange->haloSizeX[gmx::DirectionX::Up];
src/gromacs/ewald/pme_gpu_grid.cu:    int overlapRight = pmeGpu->haloExchange->haloSizeY[gmx::DirectionY::Right];
src/gromacs/ewald/pme_gpu_grid.cu:    int overlapLeft  = pmeGpu->haloExchange->haloSizeY[gmx::DirectionY::Left];
src/gromacs/ewald/pme_gpu_grid.cu:    int myGridX = pmeGpu->haloExchange->gridSizeX;
src/gromacs/ewald/pme_gpu_grid.cu:    int myGridY = pmeGpu->haloExchange->gridSizeY;
src/gromacs/ewald/pme_gpu_grid.cu:    int sizeX = pmeGpu->common->nnodesX;
src/gromacs/ewald/pme_gpu_grid.cu:    int down  = pmeGpu->haloExchange->ranksX[gmx::DirectionX::Down];
src/gromacs/ewald/pme_gpu_grid.cu:    int up    = pmeGpu->haloExchange->ranksX[gmx::DirectionX::Up];
src/gromacs/ewald/pme_gpu_grid.cu:    int sizeY = pmeGpu->common->nnodesY;
src/gromacs/ewald/pme_gpu_grid.cu:    int right = pmeGpu->haloExchange->ranksY[gmx::DirectionY::Right];
src/gromacs/ewald/pme_gpu_grid.cu:    int left  = pmeGpu->haloExchange->ranksY[gmx::DirectionY::Left];
src/gromacs/ewald/pme_gpu_grid.cu:    for (int gridIndex = 0; gridIndex < pmeGpu->common->ngrids; gridIndex++)
src/gromacs/ewald/pme_gpu_grid.cu:        float* realGrid = pmeGpu->kernelParams->grid.d_realGrid[gridIndex];
src/gromacs/ewald/pme_gpu_grid.cu:                pmeGpu->haloExchange->d_sendGrids[gmx::DirectionX::Up][gmx::DirectionY::Center];
src/gromacs/ewald/pme_gpu_grid.cu:                pmeGpu->haloExchange->d_sendGrids[gmx::DirectionX::Down][gmx::DirectionY::Center];
src/gromacs/ewald/pme_gpu_grid.cu:                pmeGpu->haloExchange->d_recvGrids[gmx::DirectionX::Up][gmx::DirectionY::Center];
src/gromacs/ewald/pme_gpu_grid.cu:                pmeGpu->haloExchange->d_recvGrids[gmx::DirectionX::Down][gmx::DirectionY::Center];
src/gromacs/ewald/pme_gpu_grid.cu:            wallcycle_start(wcycle, WallCycleCounter::LaunchGpuPme);
src/gromacs/ewald/pme_gpu_grid.cu:                    pmeGpu,
src/gromacs/ewald/pme_gpu_grid.cu:                    pmeGpu->haloExchange->d_sendGrids[gmx::DirectionX::Center][gmx::DirectionY::Left],
src/gromacs/ewald/pme_gpu_grid.cu:                    pmeGpu->haloExchange->d_sendGrids[gmx::DirectionX::Center][gmx::DirectionY::Right],
src/gromacs/ewald/pme_gpu_grid.cu:                    pmeGpu->haloExchange->d_sendGrids[gmx::DirectionX::Up][gmx::DirectionY::Left],
src/gromacs/ewald/pme_gpu_grid.cu:                    pmeGpu->haloExchange->d_sendGrids[gmx::DirectionX::Down][gmx::DirectionY::Left],
src/gromacs/ewald/pme_gpu_grid.cu:                    pmeGpu->haloExchange->d_sendGrids[gmx::DirectionX::Up][gmx::DirectionY::Right],
src/gromacs/ewald/pme_gpu_grid.cu:                    pmeGpu->haloExchange->d_sendGrids[gmx::DirectionX::Down][gmx::DirectionY::Right]);
src/gromacs/ewald/pme_gpu_grid.cu:            wallcycle_stop(wcycle, WallCycleCounter::LaunchGpuPme);
src/gromacs/ewald/pme_gpu_grid.cu:        wallcycle_start(wcycle, WallCycleCounter::WaitGpuFftToPmeGrid);
src/gromacs/ewald/pme_gpu_grid.cu:        // Make sure data is ready on GPU before MPI communication.
src/gromacs/ewald/pme_gpu_grid.cu:        pmeGpu->archSpecific->pmeStream_.synchronize();
src/gromacs/ewald/pme_gpu_grid.cu:        wallcycle_stop(wcycle, WallCycleCounter::WaitGpuFftToPmeGrid);
src/gromacs/ewald/pme_gpu_grid.cu:                           pmeGpu->common->mpiCommX);
src/gromacs/ewald/pme_gpu_grid.cu:                               pmeGpu->common->mpiCommX);
src/gromacs/ewald/pme_gpu_grid.cu:                    pmeGpu->haloExchange->d_sendGrids[gmx::DirectionX::Center][gmx::DirectionY::Left],
src/gromacs/ewald/pme_gpu_grid.cu:                    pmeGpu->haloExchange->d_recvGrids[gmx::DirectionX::Center][gmx::DirectionY::Right],
src/gromacs/ewald/pme_gpu_grid.cu:                    pmeGpu->common->mpiCommY);
src/gromacs/ewald/pme_gpu_grid.cu:                        pmeGpu->haloExchange->d_sendGrids[gmx::DirectionX::Center][gmx::DirectionY::Right],
src/gromacs/ewald/pme_gpu_grid.cu:                        pmeGpu->haloExchange->d_recvGrids[gmx::DirectionX::Center][gmx::DirectionY::Left],
src/gromacs/ewald/pme_gpu_grid.cu:                        pmeGpu->common->mpiCommY);
src/gromacs/ewald/pme_gpu_grid.cu:                    pmeGpu->haloExchange->d_sendGrids[gmx::DirectionX::Up][gmx::DirectionY::Left],
src/gromacs/ewald/pme_gpu_grid.cu:                    pmeGpu->haloExchange->d_recvGrids[gmx::DirectionX::Down][gmx::DirectionY::Right],
src/gromacs/ewald/pme_gpu_grid.cu:                    pmeGpu->common->mpiComm);
src/gromacs/ewald/pme_gpu_grid.cu:                        pmeGpu->haloExchange->d_sendGrids[gmx::DirectionX::Up][gmx::DirectionY::Right],
src/gromacs/ewald/pme_gpu_grid.cu:                        pmeGpu->haloExchange->d_recvGrids[gmx::DirectionX::Down][gmx::DirectionY::Left],
src/gromacs/ewald/pme_gpu_grid.cu:                        pmeGpu->common->mpiComm);
src/gromacs/ewald/pme_gpu_grid.cu:                        pmeGpu->haloExchange->d_sendGrids[gmx::DirectionX::Down][gmx::DirectionY::Left],
src/gromacs/ewald/pme_gpu_grid.cu:                        pmeGpu->haloExchange->d_recvGrids[gmx::DirectionX::Up][gmx::DirectionY::Right],
src/gromacs/ewald/pme_gpu_grid.cu:                        pmeGpu->common->mpiComm);
src/gromacs/ewald/pme_gpu_grid.cu:                        pmeGpu->haloExchange->d_sendGrids[gmx::DirectionX::Down][gmx::DirectionY::Right],
src/gromacs/ewald/pme_gpu_grid.cu:                        pmeGpu->haloExchange->d_recvGrids[gmx::DirectionX::Up][gmx::DirectionY::Left],
src/gromacs/ewald/pme_gpu_grid.cu:                        pmeGpu->common->mpiComm);
src/gromacs/ewald/pme_gpu_grid.cu:            wallcycle_start(wcycle, WallCycleCounter::LaunchGpuPme);
src/gromacs/ewald/pme_gpu_grid.cu:                    pmeGpu,
src/gromacs/ewald/pme_gpu_grid.cu:                    pmeGpu->haloExchange->d_recvGrids[gmx::DirectionX::Center][gmx::DirectionY::Left],
src/gromacs/ewald/pme_gpu_grid.cu:                    pmeGpu->haloExchange->d_recvGrids[gmx::DirectionX::Center][gmx::DirectionY::Right],
src/gromacs/ewald/pme_gpu_grid.cu:                    pmeGpu->haloExchange->d_recvGrids[gmx::DirectionX::Up][gmx::DirectionY::Left],
src/gromacs/ewald/pme_gpu_grid.cu:                    pmeGpu->haloExchange->d_recvGrids[gmx::DirectionX::Down][gmx::DirectionY::Left],
src/gromacs/ewald/pme_gpu_grid.cu:                    pmeGpu->haloExchange->d_recvGrids[gmx::DirectionX::Up][gmx::DirectionY::Right],
src/gromacs/ewald/pme_gpu_grid.cu:                    pmeGpu->haloExchange->d_recvGrids[gmx::DirectionX::Down][gmx::DirectionY::Right]);
src/gromacs/ewald/pme_gpu_grid.cu:            wallcycle_stop(wcycle, WallCycleCounter::LaunchGpuPme);
src/gromacs/ewald/pme_gpu_grid.cu:    GMX_UNUSED_VALUE(pmeGpu);
src/gromacs/ewald/pme_gpu_grid.cu:void convertPmeGridToFftGrid(const PmeGpu* pmeGpu, float* h_fftRealGrid, gmx_parallel_3dfft* fftSetup, const int gridIndex)
src/gromacs/ewald/pme_gpu_grid.cu:    localPmeSize[XX] = pmeGpu->kernelParams->grid.realGridSizePadded[XX];
src/gromacs/ewald/pme_gpu_grid.cu:    localPmeSize[YY] = pmeGpu->kernelParams->grid.realGridSizePadded[YY];
src/gromacs/ewald/pme_gpu_grid.cu:    localPmeSize[ZZ] = pmeGpu->kernelParams->grid.realGridSizePadded[ZZ];
src/gromacs/ewald/pme_gpu_grid.cu:                                 &pmeGpu->kernelParams->grid.d_realGrid[gridIndex],
src/gromacs/ewald/pme_gpu_grid.cu:                                 pmeGpu->archSpecific->pmeStream_,
src/gromacs/ewald/pme_gpu_grid.cu:                                 pmeGpu->settings.transferKind,
src/gromacs/ewald/pme_gpu_grid.cu:            copyToDeviceBuffer(&pmeGpu->kernelParams->grid.d_realGrid[gridIndex],
src/gromacs/ewald/pme_gpu_grid.cu:                               pmeGpu->archSpecific->pmeStream_,
src/gromacs/ewald/pme_gpu_grid.cu:                               pmeGpu->settings.transferKind,
src/gromacs/ewald/pme_gpu_grid.cu:                prepareGpuKernelArguments(kernelFn,
src/gromacs/ewald/pme_gpu_grid.cu:                                          &pmeGpu->kernelParams->grid.d_realGrid[gridIndex],
src/gromacs/ewald/pme_gpu_grid.cu:        launchGpuKernel(kernelFn,
src/gromacs/ewald/pme_gpu_grid.cu:                        pmeGpu->archSpecific->pmeStream_,
src/gromacs/ewald/pme_gpu_grid.cu:        pmeGpu->archSpecific->syncSpreadGridD2H.markEvent(pmeGpu->archSpecific->pmeStream_);
src/gromacs/ewald/pme_gpu_grid.cu:void convertPmeGridToFftGrid(const PmeGpu* pmeGpu, DeviceBuffer<float>* d_fftRealGrid, const int gridIndex)
src/gromacs/ewald/pme_gpu_grid.cu:    localPmeSize[XX] = pmeGpu->kernelParams->grid.realGridSizePadded[XX];
src/gromacs/ewald/pme_gpu_grid.cu:    localPmeSize[YY] = pmeGpu->kernelParams->grid.realGridSizePadded[YY];
src/gromacs/ewald/pme_gpu_grid.cu:    localPmeSize[ZZ] = pmeGpu->kernelParams->grid.realGridSizePadded[ZZ];
src/gromacs/ewald/pme_gpu_grid.cu:    localFftNData[XX] = pmeGpu->archSpecific->localRealGridSize[XX];
src/gromacs/ewald/pme_gpu_grid.cu:    localFftNData[YY] = pmeGpu->archSpecific->localRealGridSize[YY];
src/gromacs/ewald/pme_gpu_grid.cu:    localFftNData[ZZ] = pmeGpu->archSpecific->localRealGridSize[ZZ];
src/gromacs/ewald/pme_gpu_grid.cu:    localFftSize[XX] = pmeGpu->archSpecific->localRealGridSizePadded[XX];
src/gromacs/ewald/pme_gpu_grid.cu:    localFftSize[YY] = pmeGpu->archSpecific->localRealGridSizePadded[YY];
src/gromacs/ewald/pme_gpu_grid.cu:    localFftSize[ZZ] = pmeGpu->archSpecific->localRealGridSizePadded[ZZ];
src/gromacs/ewald/pme_gpu_grid.cu:                                     &pmeGpu->kernelParams->grid.d_realGrid[gridIndex],
src/gromacs/ewald/pme_gpu_grid.cu:                                     pmeGpu->archSpecific->pmeStream_,
src/gromacs/ewald/pme_gpu_grid.cu:                                     pmeGpu->settings.transferKind,
src/gromacs/ewald/pme_gpu_grid.cu:            copyBetweenDeviceBuffers(&pmeGpu->kernelParams->grid.d_realGrid[gridIndex],
src/gromacs/ewald/pme_gpu_grid.cu:                                     pmeGpu->archSpecific->pmeStream_,
src/gromacs/ewald/pme_gpu_grid.cu:                                     pmeGpu->settings.transferKind,
src/gromacs/ewald/pme_gpu_grid.cu:                prepareGpuKernelArguments(kernelFn,
src/gromacs/ewald/pme_gpu_grid.cu:                                          &pmeGpu->kernelParams->grid.d_realGrid[gridIndex],
src/gromacs/ewald/pme_gpu_grid.cu:        launchGpuKernel(kernelFn,
src/gromacs/ewald/pme_gpu_grid.cu:                        pmeGpu->archSpecific->pmeStream_,
src/gromacs/ewald/pme_gpu_grid.cu:template void convertPmeGridToFftGrid<true>(const PmeGpu*       pmeGpu,
src/gromacs/ewald/pme_gpu_grid.cu:template void convertPmeGridToFftGrid<false>(const PmeGpu*       pmeGpu,
src/gromacs/ewald/pme_gpu_grid.cu:template void convertPmeGridToFftGrid<true>(const PmeGpu*        pmeGpu,
src/gromacs/ewald/pme_gpu_grid.cu:template void convertPmeGridToFftGrid<false>(const PmeGpu*        pmeGpu,
src/gromacs/ewald/pme_program.cl: *  \brief Top-level file for generating PME OpenCL kernels.
src/gromacs/ewald/pme_program.cl: * This file includes all the PME OpenCL kernel files multiple times, with additional defines.
src/gromacs/ewald/pme_program.cl: * For details, please see how pme_program.cl is compiled in pme_gpu_program_impl_ocl.cpp.
src/gromacs/ewald/pme_program.cl:// Assert placeholders, to not rip them out from OpenCL implementation - hopefully they come in handy some day with OpenCL 2
src/gromacs/ewald/pme_program.cl:#define PmeOpenCLKernelParams PmeGpuKernelParamsBase
src/gromacs/ewald/tests/pmetestcommon.cpp:#include "gromacs/ewald/pme_coordinate_receiver_gpu.h"
src/gromacs/ewald/tests/pmetestcommon.cpp:#include "gromacs/ewald/pme_gpu_calculate_splines.h"
src/gromacs/ewald/tests/pmetestcommon.cpp:#include "gromacs/ewald/pme_gpu_constants.h"
src/gromacs/ewald/tests/pmetestcommon.cpp:#include "gromacs/ewald/pme_gpu_internal.h"
src/gromacs/ewald/tests/pmetestcommon.cpp:#include "gromacs/ewald/pme_gpu_staging.h"
src/gromacs/ewald/tests/pmetestcommon.cpp:#include "gromacs/ewald/pme_gpu_types_host.h"
src/gromacs/ewald/tests/pmetestcommon.cpp:#include "gromacs/gpu_utils/device_context.h"
src/gromacs/ewald/tests/pmetestcommon.cpp:#include "gromacs/gpu_utils/gpu_utils.h"
src/gromacs/ewald/tests/pmetestcommon.cpp:#include "gromacs/gpu_utils/hostallocator.h"
src/gromacs/ewald/tests/pmetestcommon.cpp:class GpuEventSynchronizer;
src/gromacs/ewald/tests/pmetestcommon.cpp:    messages.appendIf(!pme_gpu_supports_build(&errorMessage), errorMessage);
src/gromacs/ewald/tests/pmetestcommon.cpp:    messages.appendIf(!pme_gpu_supports_input(inputRec, &errorMessage), errorMessage);
src/gromacs/ewald/tests/pmetestcommon.cpp:                              const PmeGpuProgram* pmeGpuProgram,
src/gromacs/ewald/tests/pmetestcommon.cpp:    // TODO: Need to use proper value when GPU PME decomposition code path is tested
src/gromacs/ewald/tests/pmetestcommon.cpp:                                         pmeGpuProgram,
src/gromacs/ewald/tests/pmetestcommon.cpp:        case CodePath::GPU:
src/gromacs/ewald/tests/pmetestcommon.cpp:            pme_gpu_set_testing(pme->gpu, true);
src/gromacs/ewald/tests/pmetestcommon.cpp:            pme_gpu_update_input_box(pme->gpu, boxTemp);
src/gromacs/ewald/tests/pmetestcommon.cpp://! Make a GPU state-propagator manager
src/gromacs/ewald/tests/pmetestcommon.cpp:std::unique_ptr<StatePropagatorDataGpu> makeStatePropagatorDataGpu(const gmx_pme_t& pme,
src/gromacs/ewald/tests/pmetestcommon.cpp:    return std::make_unique<StatePropagatorDataGpu>(
src/gromacs/ewald/tests/pmetestcommon.cpp:            deviceStream, *deviceContext, GpuApiCallBehavior::Sync, pme_gpu_get_block_size(&pme), false, nullptr);
src/gromacs/ewald/tests/pmetestcommon.cpp:                  StatePropagatorDataGpu*  stateGpu,
src/gromacs/ewald/tests/pmetestcommon.cpp:        case CodePath::GPU:
src/gromacs/ewald/tests/pmetestcommon.cpp:            // TODO: Avoid use of atc in the GPU code path
src/gromacs/ewald/tests/pmetestcommon.cpp:            stateGpu->reinit(atomCount, atomCount, dummyCommrec, 0);
src/gromacs/ewald/tests/pmetestcommon.cpp:            stateGpu->copyCoordinatesToGpu(arrayRefFromArray(coordinates.data(), coordinates.size()),
src/gromacs/ewald/tests/pmetestcommon.cpp:            pme_gpu_set_kernelparam_coordinates(pme->gpu, stateGpu->getCoordinates());
src/gromacs/ewald/tests/pmetestcommon.cpp:        case CodePath::GPU:
src/gromacs/ewald/tests/pmetestcommon.cpp:            pme_gpu_get_real_grid_sizes(pme->gpu, &gridSize, &paddedGridSize);
src/gromacs/ewald/tests/pmetestcommon.cpp: * double precision. GPUs are not used with double precision anyhow. */
src/gromacs/ewald/tests/pmetestcommon.cpp:        case CodePath::GPU:
src/gromacs/ewald/tests/pmetestcommon.cpp:            GpuEventSynchronizer* xReadyOnDevice = nullptr;
src/gromacs/ewald/tests/pmetestcommon.cpp:            bool                           useGpuDirectComm         = false;
src/gromacs/ewald/tests/pmetestcommon.cpp:            gmx::PmeCoordinateReceiverGpu* pmeCoordinateReceiverGpu = nullptr;
src/gromacs/ewald/tests/pmetestcommon.cpp:            pme_gpu_spread(pme->gpu,
src/gromacs/ewald/tests/pmetestcommon.cpp:                           useGpuDirectComm,
src/gromacs/ewald/tests/pmetestcommon.cpp:                           pmeCoordinateReceiverGpu,
src/gromacs/ewald/tests/pmetestcommon.cpp:        case CodePath::GPU:
src/gromacs/ewald/tests/pmetestcommon.cpp:                    pme_gpu_solve(pme->gpu, gridIndex, h_grid, gridOrdering, computeEnergyAndVirial);
src/gromacs/ewald/tests/pmetestcommon.cpp: * double precision. GPUs are not used with double precision anyhow. */
src/gromacs/ewald/tests/pmetestcommon.cpp:        case CodePath::GPU:
src/gromacs/ewald/tests/pmetestcommon.cpp:            PmeOutput  output = pme_gpu_getOutput(pme, computeEnergyAndVirial, lambdaQ);
src/gromacs/ewald/tests/pmetestcommon.cpp:            pme_gpu_gather(pme->gpu, pme->gridsCoulomb, lambdaQ, nullptr, computeEnergyAndVirial);
src/gromacs/ewald/tests/pmetestcommon.cpp:        case CodePath::GPU: pme_gpu_synchronize(pme->gpu); break;
src/gromacs/ewald/tests/pmetestcommon.cpp:    GpuToHost,
src/gromacs/ewald/tests/pmetestcommon.cpp:    HostToGpu
src/gromacs/ewald/tests/pmetestcommon.cpp: * These theta/dtheta buffers are laid out for GPU spread/gather
src/gromacs/ewald/tests/pmetestcommon.cpp: * \returns Index into theta or dtheta array using GPU layout.
src/gromacs/ewald/tests/pmetestcommon.cpp:    if (order != c_pmeGpuOrder)
src/gromacs/ewald/tests/pmetestcommon.cpp:    constexpr int fixedOrder = c_pmeGpuOrder;
src/gromacs/ewald/tests/pmetestcommon.cpp:/*! \brief Rearranges the atom spline data between the GPU and host layouts.
src/gromacs/ewald/tests/pmetestcommon.cpp: * \param[in]  pmeGpu     The PME GPU structure.
src/gromacs/ewald/tests/pmetestcommon.cpp:static void pme_gpu_transform_spline_atom_data(PmeGpu*            pmeGpu,
src/gromacs/ewald/tests/pmetestcommon.cpp:    // The GPU atom spline data is laid out in a different way currently than the CPU one.
src/gromacs/ewald/tests/pmetestcommon.cpp:    // This function converts the data from GPU to CPU layout (in the host memory).
src/gromacs/ewald/tests/pmetestcommon.cpp:    // Ideally we should use similar layouts on CPU and GPU if we care about mixed modes and their
src/gromacs/ewald/tests/pmetestcommon.cpp:    // performance (e.g. spreading on GPU, gathering on CPU).
src/gromacs/ewald/tests/pmetestcommon.cpp:    const auto      atomsPerWarp = pme_gpu_get_atoms_per_warp(pmeGpu);
src/gromacs/ewald/tests/pmetestcommon.cpp:    GMX_RELEASE_ASSERT(atomsPerWarp > 0, "Can not get GPU warp size");
src/gromacs/ewald/tests/pmetestcommon.cpp:    const auto pmeOrder = pmeGpu->common->pme_order;
src/gromacs/ewald/tests/pmetestcommon.cpp:    GMX_ASSERT(pmeOrder == c_pmeGpuOrder, "Only PME order 4 is implemented");
src/gromacs/ewald/tests/pmetestcommon.cpp:            h_splineBuffer  = pmeGpu->staging.h_theta.data();
src/gromacs/ewald/tests/pmetestcommon.cpp:            h_splineBuffer  = pmeGpu->staging.h_dtheta.data();
src/gromacs/ewald/tests/pmetestcommon.cpp:            const auto gpuValueIndex =
src/gromacs/ewald/tests/pmetestcommon.cpp:                       "Atom spline data index out of bounds (while transforming GPU data layout "
src/gromacs/ewald/tests/pmetestcommon.cpp:                case PmeLayoutTransform::GpuToHost:
src/gromacs/ewald/tests/pmetestcommon.cpp:                    cpuSplineBuffer[cpuValueIndex] = h_splineBuffer[gpuValueIndex];
src/gromacs/ewald/tests/pmetestcommon.cpp:                case PmeLayoutTransform::HostToGpu:
src/gromacs/ewald/tests/pmetestcommon.cpp:                    h_splineBuffer[gpuValueIndex] = cpuSplineBuffer[cpuValueIndex];
src/gromacs/ewald/tests/pmetestcommon.cpp:        case CodePath::GPU:
src/gromacs/ewald/tests/pmetestcommon.cpp:            pme_gpu_transform_spline_atom_data(pme->gpu, atc, type, dimIndex, PmeLayoutTransform::HostToGpu);
src/gromacs/ewald/tests/pmetestcommon.cpp:        case CodePath::GPU:
src/gromacs/ewald/tests/pmetestcommon.cpp:            memcpy(pme_gpu_staging(pme->gpu).h_gridlineIndices.data(),
src/gromacs/ewald/tests/pmetestcommon.cpp:        case CodePath::GPU: // intentional absence of break, the grid will be copied from the host buffer in testing mode
src/gromacs/ewald/tests/pmetestcommon.cpp:        case CodePath::GPU:
src/gromacs/ewald/tests/pmetestcommon.cpp:            pme_gpu_transform_spline_atom_data(pme->gpu, atc, type, dimIndex, PmeLayoutTransform::GpuToHost);
src/gromacs/ewald/tests/pmetestcommon.cpp:        case CodePath::GPU:
src/gromacs/ewald/tests/pmetestcommon.cpp:                    reinterpret_cast<IVec*>(pme_gpu_staging(pme->gpu).h_gridlineIndices.data());
src/gromacs/ewald/tests/pmetestcommon.cpp:        case CodePath::GPU: // intentional absence of break
src/gromacs/ewald/tests/pmetestcommon.cpp:        case CodePath::GPU:
src/gromacs/ewald/tests/pmetestcommon.cpp:                    pme_gpu_getEnergyAndVirial(*pme, lambdaQ, &output);
src/gromacs/ewald/tests/pmetestcommon.cpp:    std::optional<int> gpuId = getPmeTestHardwareContexts()[hardwareContextIndex].gpuId();
src/gromacs/ewald/tests/pmetestcommon.cpp:    if (gpuId.has_value())
src/gromacs/ewald/tests/pmetestcommon.cpp:        description = "GPU" + std::to_string(gpuId.value());
src/gromacs/ewald/tests/pmetestcommon.cpp:    codePath_(CodePath::GPU), testDevice_(testDevice)
src/gromacs/ewald/tests/pmetestcommon.cpp:    pmeGpuProgram_ = buildPmeGpuProgram(testDevice_->deviceContext());
src/gromacs/ewald/tests/pmetestcommon.cpp:        case CodePath::GPU: return "GPU (" + testDevice_->description() + ")";
src/gromacs/ewald/tests/pmetestcommon.cpp:std::optional<int> PmeTestHardwareContext::gpuId() const
src/gromacs/ewald/tests/pmetestcommon.cpp:        case CodePath::GPU: return testDevice_->id();
src/gromacs/ewald/tests/pmetestcommon.cpp:    if (codePath_ == CodePath::GPU)
src/gromacs/ewald/tests/pmetestcommon.cpp:        // Add GPU devices
src/gromacs/ewald/tests/pmegathertest.cpp:#include "gromacs/ewald/pme_gpu_internal.h"
src/gromacs/ewald/tests/pmegathertest.cpp:#include "gromacs/mdtypes/state_propagator_data_gpu.h"
src/gromacs/ewald/tests/pmegathertest.cpp:                                                pmeTestHardwareContext.pmeGpuProgram(),
src/gromacs/ewald/tests/pmegathertest.cpp:        std::unique_ptr<StatePropagatorDataGpu> stateGpu =
src/gromacs/ewald/tests/pmegathertest.cpp:                (codePath == CodePath::GPU)
src/gromacs/ewald/tests/pmegathertest.cpp:                        ? makeStatePropagatorDataGpu(*pmeSafe.get(),
src/gromacs/ewald/tests/pmegathertest.cpp:        pmeInitAtoms(pmeSafe.get(), stateGpu.get(), codePath, testSystem.coordinates, testSystem.charges);
src/gromacs/ewald/tests/pmesolvetest.cpp:#include "gromacs/ewald/pme_gpu_internal.h"
src/gromacs/ewald/tests/pmesolvetest.cpp:        messages.appendIf(!pmeTestHardwareContext.gpuId().has_value() && gridOrdering == GridOrdering::XYZ,
src/gromacs/ewald/tests/pmesolvetest.cpp:                                                pmeTestHardwareContext.pmeGpuProgram(),
src/gromacs/ewald/tests/pmetestcommon.h:#include "gromacs/ewald/pme_gpu_internal.h"
src/gromacs/ewald/tests/pmetestcommon.h:#include "gromacs/ewald/pme_gpu_program.h"
src/gromacs/ewald/tests/pmetestcommon.h:#include "gromacs/mdtypes/state_propagator_data_gpu.h"
src/gromacs/ewald/tests/pmetestcommon.h:    //! GPU code path
src/gromacs/ewald/tests/pmetestcommon.h:    GPU,
src/gromacs/ewald/tests/pmetestcommon.h:                              const PmeGpuProgram* pmeGpuProgram,
src/gromacs/ewald/tests/pmetestcommon.h://! Make a GPU state-propagator manager
src/gromacs/ewald/tests/pmetestcommon.h:std::unique_ptr<StatePropagatorDataGpu> makeStatePropagatorDataGpu(const gmx_pme_t& pme,
src/gromacs/ewald/tests/pmetestcommon.h:                  StatePropagatorDataGpu*  stateGpu,
src/gromacs/ewald/tests/pmetestcommon.h:    //! Returns an optional GPU ID, with a valid value when the context is for a GPU
src/gromacs/ewald/tests/pmetestcommon.h:    std::optional<int> gpuId() const;
src/gromacs/ewald/tests/pmetestcommon.h:    //! Pointer to the global test hardware device (if on GPU)
src/gromacs/ewald/tests/pmetestcommon.h:    //! PME GPU program if needed
src/gromacs/ewald/tests/pmetestcommon.h:    PmeGpuProgramStorage pmeGpuProgram_ = nullptr;
src/gromacs/ewald/tests/pmetestcommon.h:    //! Constructor for GPU context
src/gromacs/ewald/tests/pmetestcommon.h:    //! Get the PME GPU program
src/gromacs/ewald/tests/pmetestcommon.h:    const PmeGpuProgram* pmeGpuProgram() const
src/gromacs/ewald/tests/pmetestcommon.h:        return codePath() == CodePath::GPU ? pmeGpuProgram_.get() : nullptr;
src/gromacs/ewald/tests/pmetestcommon.h:        return codePath() == CodePath::GPU ? &testDevice_->deviceContext() : nullptr;
src/gromacs/ewald/tests/pmetestcommon.h:        return codePath() == CodePath::GPU ? &testDevice_->deviceStream() : nullptr;
src/gromacs/ewald/tests/CMakeLists.txt:if (GMX_GPU_FFT_VKFFT)
src/gromacs/ewald/tests/CMakeLists.txt:    GPU_CPP_SOURCE_FILES
src/gromacs/ewald/tests/CMakeLists.txt:        gpu_utils
src/gromacs/ewald/tests/pmesplinespreadtest.cpp:#include "gromacs/ewald/pme_gpu_internal.h"
src/gromacs/ewald/tests/pmesplinespreadtest.cpp:#include "gromacs/mdtypes/state_propagator_data_gpu.h"
src/gromacs/ewald/tests/pmesplinespreadtest.cpp:                                                pmeTestHardwareContext.pmeGpuProgram(),
src/gromacs/ewald/tests/pmesplinespreadtest.cpp:        std::unique_ptr<StatePropagatorDataGpu> stateGpu =
src/gromacs/ewald/tests/pmesplinespreadtest.cpp:                (codePath == CodePath::GPU)
src/gromacs/ewald/tests/pmesplinespreadtest.cpp:                        ? makeStatePropagatorDataGpu(*pmeSafe.get(),
src/gromacs/ewald/tests/pmesplinespreadtest.cpp:        pmeInitAtoms(pmeSafe.get(), stateGpu.get(), codePath, coordinates, charges);
src/gromacs/ewald/pme_spread_sycl.cpp: *  \brief Implements PME GPU spline calculation and charge spreading in SYCL.
src/gromacs/ewald/pme_spread_sycl.cpp:#include "gromacs/gpu_utils/gmxsycl.h"
src/gromacs/ewald/pme_spread_sycl.cpp:#include "gromacs/gpu_utils/gputraits_sycl.h"
src/gromacs/ewald/pme_spread_sycl.cpp:#include "gromacs/gpu_utils/sycl_kernel_utils.h"
src/gromacs/ewald/pme_spread_sycl.cpp:#include "gromacs/gpu_utils/syclutils.h"
src/gromacs/ewald/pme_spread_sycl.cpp:#include "pme_gpu_calculate_splines_sycl.h"
src/gromacs/ewald/pme_spread_sycl.cpp:#include "pme_gpu_types_host.h"
src/gromacs/ewald/pme_spread_sycl.cpp:    const int chargeCheck = pmeGpuCheckAtomCharge(atomCharge);
src/gromacs/ewald/pme_spread_sycl.cpp:                              const PmeGpuPipeliningParams pipeliningParams)
src/gromacs/ewald/pme_spread_sycl.cpp:        pmeGpuStageAtomData<float, atomsPerBlock, 1>(sm_coefficients.get_pointer(),
src/gromacs/ewald/pme_spread_sycl.cpp:             * (the data is assumed to be in GPU global memory with proper layout already,
src/gromacs/ewald/pme_spread_sycl.cpp:            pmeGpuStageAtomData<float, atomsPerBlock, DIM * order>(sm_theta.get_pointer(), gm_theta, itemIdx);
src/gromacs/ewald/pme_spread_sycl.cpp:            pmeGpuStageAtomData<int, atomsPerBlock, DIM>(
src/gromacs/ewald/pme_spread_sycl.cpp:            pmeGpuStageAtomData<float, atomsPerBlock, 1>(sm_coefficients.get_pointer(),
src/gromacs/ewald/pme_spread_sycl.cpp:        auto* params   = reinterpret_cast<PmeGpuKernelParams*>(arg);
src/gromacs/ewald/pme_spread_sycl.cpp:    // SYCL has different multidimensional layout than OpenCL/CUDA.
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl.cpp: * \brief May be used to implement PME-PP GPU comm interfaces for non-GPU builds.
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl.cpp: * Needed to satisfy compiler when compiling without GPU support.
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl.cpp: * \author Alan Gray <alang@nvidia.com>
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl.cpp:#include "gromacs/ewald/pme_coordinate_receiver_gpu.h"
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl.cpp:#include "gromacs/gpu_utils/devicebuffer_datatype.h"
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl.cpp:class GpuEventSynchronizer;
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl.cpp:#if !GMX_GPU_CUDA && !GMX_GPU_SYCL
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl.cpp:class PmeCoordinateReceiverGpu::Impl
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl.cpp:PmeCoordinateReceiverGpu::PmeCoordinateReceiverGpu(MPI_Comm /* comm */,
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl.cpp:               "A CPU stub for PME-PP GPU communication was called instead of the correct "
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl.cpp:PmeCoordinateReceiverGpu::~PmeCoordinateReceiverGpu() = default;
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl.cpp:/*!\brief init PME-PP GPU communication stub */
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl.cpp:void PmeCoordinateReceiverGpu::reinitCoordinateReceiver(DeviceBuffer<RVec> /* d_x */)
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl.cpp:               "A CPU stub for PME-PP GPU communication initialization was called instead of the "
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl.cpp:void PmeCoordinateReceiverGpu::receiveCoordinatesSynchronizerFromPpPeerToPeer(int /* ppRank */)
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl.cpp:               "A CPU stub for PME-PP GPU communication was called instead of the correct "
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl.cpp:void PmeCoordinateReceiverGpu::launchReceiveCoordinatesFromPpGpuAwareMpi(DeviceBuffer<RVec> /* recvbuf */,
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl.cpp:               "A CPU stub for PME-PP GPU communication was called instead of the correct "
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl.cpp:std::tuple<int, GpuEventSynchronizer*> PmeCoordinateReceiverGpu::receivePpCoordinateSendEvent(int /* pipelineStage */)
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl.cpp:               "A CPU stub for PME-PP GPU communication was called instead of the correct "
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl.cpp:int PmeCoordinateReceiverGpu::waitForCoordinatesFromAnyPpRank()
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl.cpp:               "A CPU stub for PME-PP GPU communication was called instead of the correct "
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl.cpp:DeviceStream* PmeCoordinateReceiverGpu::ppCommStream(int /* senderIndex */)
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl.cpp:               "A CPU stub for PME-PP GPU communication was called instead of the correct "
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl.cpp:std::tuple<int, int> PmeCoordinateReceiverGpu::ppCommAtomRange(int /* senderIndex */)
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl.cpp:               "A CPU stub for PME-PP GPU communication was called instead of the correct "
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl.cpp:int PmeCoordinateReceiverGpu::ppCommNumSenderRanks()
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl.cpp:               "A CPU stub for PME-PP GPU communication was called instead of the correct "
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl.cpp:void PmeCoordinateReceiverGpu::insertAsDependencyIntoStream(int /*senderIndex*/, const DeviceStream& /*stream*/)
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl.cpp:               "A CPU stub for PME-PP GPU communication was called instead of the correct "
src/gromacs/ewald/pme_coordinate_receiver_gpu_impl.cpp:#endif // !GMX_GPU_CUDA
src/gromacs/ewald/pme_gpu_grid_sycl.cpp: * \brief Implements PME GPU halo exchange and PME GPU - Host FFT grid conversion
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:#include "gromacs/gpu_utils/gmxsycl.h"
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:#include "gromacs/gpu_utils/syclutils.h"
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:#include "pme_gpu_grid.h"
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:#include "pme_gpu_types.h"
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:#include "pme_gpu_types_host.h"
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:#include "pme_gpu_types_host_impl.h"
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:/*! \brief Submits a GPU grid kernel
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:    // for coalescing. Current value is taken from the CUDA version.
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:    // For Intel GPUs, we force 32-wide execution with
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:void pmeGpuGridHaloExchange(const PmeGpu* pmeGpu, gmx_wallcycle* wcycle)
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:    auto*             kernelParamsPtr = pmeGpu->kernelParams.get();
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:    size_t overlapX = pmeGpu->haloExchange->haloSizeX[gmx::DirectionX::Center];
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:    size_t overlapY = pmeGpu->haloExchange->haloSizeY[gmx::DirectionY::Center];
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:    size_t overlapDown = pmeGpu->haloExchange->haloSizeX[gmx::DirectionX::Down];
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:    size_t overlapUp   = pmeGpu->haloExchange->haloSizeX[gmx::DirectionX::Up];
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:    size_t overlapRight = pmeGpu->haloExchange->haloSizeY[gmx::DirectionY::Right];
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:    size_t overlapLeft  = pmeGpu->haloExchange->haloSizeY[gmx::DirectionY::Left];
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:    size_t myGridX = pmeGpu->haloExchange->gridSizeX;
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:    size_t myGridY = pmeGpu->haloExchange->gridSizeY;
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:    size_t sizeX = pmeGpu->common->nnodesX;
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:    size_t down  = pmeGpu->haloExchange->ranksX[gmx::DirectionX::Down];
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:    size_t up    = pmeGpu->haloExchange->ranksX[gmx::DirectionX::Up];
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:    size_t sizeY = pmeGpu->common->nnodesY;
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:    size_t right = pmeGpu->haloExchange->ranksY[gmx::DirectionY::Right];
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:    size_t left  = pmeGpu->haloExchange->ranksY[gmx::DirectionY::Left];
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:    for (int gridIndex = 0; gridIndex < pmeGpu->common->ngrids; gridIndex++)
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:        DeviceBuffer<float>& realGrid = pmeGpu->kernelParams->grid.d_realGrid[gridIndex];
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:        if (pmeGpu->common->nnodesY == 1)
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:            wallcycle_start(wcycle, WallCycleCounter::LaunchGpuPme);
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                    pmeGpu->archSpecific->pmeStream_,
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                    pmeGpu->haloExchange->d_sendGrids[gmx::DirectionX::Up][gmx::DirectionY::Center].get_pointer(),
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                    pmeGpu->haloExchange->d_sendGrids[gmx::DirectionX::Down][gmx::DirectionY::Center]
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                    pmeGpu->haloExchange->d_sendGrids[gmx::DirectionX::Center][gmx::DirectionY::Left]
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                    pmeGpu->haloExchange
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                    pmeGpu->haloExchange->d_sendGrids[gmx::DirectionX::Up][gmx::DirectionY::Left].get_pointer(),
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                    pmeGpu->haloExchange->d_sendGrids[gmx::DirectionX::Down][gmx::DirectionY::Left].get_pointer(),
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                    pmeGpu->haloExchange->d_sendGrids[gmx::DirectionX::Up][gmx::DirectionY::Right].get_pointer(),
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                    pmeGpu->haloExchange->d_sendGrids[gmx::DirectionX::Down][gmx::DirectionY::Right].get_pointer(),
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                    pmeGpu->haloExchange->d_sendGrids[gmx::DirectionX::Up][gmx::DirectionY::Center]);
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                    pmeGpu->haloExchange->d_sendGrids[gmx::DirectionX::Down][gmx::DirectionY::Center]);
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:            wallcycle_stop(wcycle, WallCycleCounter::LaunchGpuPme);
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:        wallcycle_start(wcycle, WallCycleCounter::WaitGpuPmeSpread);
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:        // Make sure data is ready on GPU before MPI communication.
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:        pmeGpu->archSpecific->pmeStream_.synchronize();
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:        wallcycle_stop(wcycle, WallCycleCounter::WaitGpuPmeSpread);
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                    asMpiPointer(pmeGpu->haloExchange->d_recvGrids[gmx::DirectionX::Up][gmx::DirectionY::Center]),
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                    pmeGpu->common->mpiCommX);
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                        asMpiPointer(pmeGpu->haloExchange->d_recvGrids[gmx::DirectionX::Down][gmx::DirectionY::Center]),
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                        pmeGpu->common->mpiCommX);
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                    asMpiPointer(pmeGpu->haloExchange->d_sendGrids[gmx::DirectionX::Center][gmx::DirectionY::Right]),
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                    asMpiPointer(pmeGpu->haloExchange->d_recvGrids[gmx::DirectionX::Center][gmx::DirectionY::Left]),
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                    pmeGpu->common->mpiCommY);
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                        asMpiPointer(pmeGpu->haloExchange->d_sendGrids[gmx::DirectionX::Center][gmx::DirectionY::Left]),
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                        asMpiPointer(pmeGpu->haloExchange->d_recvGrids[gmx::DirectionX::Center][gmx::DirectionY::Right]),
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                        pmeGpu->common->mpiCommY);
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                    asMpiPointer(pmeGpu->haloExchange->d_sendGrids[gmx::DirectionX::Down][gmx::DirectionY::Right]),
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                    asMpiPointer(pmeGpu->haloExchange->d_recvGrids[gmx::DirectionX::Up][gmx::DirectionY::Left]),
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                    pmeGpu->common->mpiComm);
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                        asMpiPointer(pmeGpu->haloExchange->d_sendGrids[gmx::DirectionX::Down][gmx::DirectionY::Left]),
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                        asMpiPointer(pmeGpu->haloExchange->d_recvGrids[gmx::DirectionX::Up][gmx::DirectionY::Right]),
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                        pmeGpu->common->mpiComm);
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                        asMpiPointer(pmeGpu->haloExchange->d_sendGrids[gmx::DirectionX::Up][gmx::DirectionY::Right]),
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                        asMpiPointer(pmeGpu->haloExchange->d_recvGrids[gmx::DirectionX::Down][gmx::DirectionY::Left]),
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                        pmeGpu->common->mpiComm);
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                        asMpiPointer(pmeGpu->haloExchange->d_sendGrids[gmx::DirectionX::Up][gmx::DirectionY::Left]),
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                        asMpiPointer(pmeGpu->haloExchange->d_recvGrids[gmx::DirectionX::Down][gmx::DirectionY::Right]),
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                        pmeGpu->common->mpiComm);
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:        wallcycle_start(wcycle, WallCycleCounter::LaunchGpuPme);
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                pmeGpu->archSpecific->pmeStream_,
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                pmeGpu->haloExchange->d_recvGrids[gmx::DirectionX::Up][gmx::DirectionY::Center].get_pointer(),
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                pmeGpu->haloExchange->d_recvGrids[gmx::DirectionX::Down][gmx::DirectionY::Center].get_pointer(),
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                pmeGpu->haloExchange->d_recvGrids[gmx::DirectionX::Center][gmx::DirectionY::Left].get_pointer(),
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                pmeGpu->haloExchange->d_recvGrids[gmx::DirectionX::Center][gmx::DirectionY::Right].get_pointer(),
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                pmeGpu->haloExchange->d_recvGrids[gmx::DirectionX::Up][gmx::DirectionY::Left].get_pointer(),
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                pmeGpu->haloExchange->d_recvGrids[gmx::DirectionX::Down][gmx::DirectionY::Left].get_pointer(),
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                pmeGpu->haloExchange->d_recvGrids[gmx::DirectionX::Up][gmx::DirectionY::Right].get_pointer(),
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                pmeGpu->haloExchange->d_recvGrids[gmx::DirectionX::Down][gmx::DirectionY::Right].get_pointer(),
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:        wallcycle_stop(wcycle, WallCycleCounter::LaunchGpuPme);
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:    GMX_UNUSED_VALUE(pmeGpu);
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:void pmeGpuGridHaloExchangeReverse(const PmeGpu* pmeGpu, gmx_wallcycle* wcycle)
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:    auto*             kernelParamsPtr = pmeGpu->kernelParams.get();
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:    size_t overlapX = pmeGpu->haloExchange->haloSizeX[gmx::DirectionX::Center];
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:    size_t overlapY = pmeGpu->haloExchange->haloSizeY[gmx::DirectionY::Center];
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:    size_t overlapDown = pmeGpu->haloExchange->haloSizeX[gmx::DirectionX::Down];
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:    size_t overlapUp   = pmeGpu->haloExchange->haloSizeX[gmx::DirectionX::Up];
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:    size_t overlapRight = pmeGpu->haloExchange->haloSizeY[gmx::DirectionY::Right];
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:    size_t overlapLeft  = pmeGpu->haloExchange->haloSizeY[gmx::DirectionY::Left];
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:    size_t myGridX = pmeGpu->haloExchange->gridSizeX;
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:    size_t myGridY = pmeGpu->haloExchange->gridSizeY;
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:    size_t sizeX = pmeGpu->common->nnodesX;
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:    size_t down  = pmeGpu->haloExchange->ranksX[gmx::DirectionX::Down];
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:    size_t up    = pmeGpu->haloExchange->ranksX[gmx::DirectionX::Up];
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:    size_t sizeY = pmeGpu->common->nnodesY;
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:    size_t right = pmeGpu->haloExchange->ranksY[gmx::DirectionY::Right];
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:    size_t left  = pmeGpu->haloExchange->ranksY[gmx::DirectionY::Left];
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:    for (int gridIndex = 0; gridIndex < pmeGpu->common->ngrids; gridIndex++)
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:        DeviceBuffer<float>& realGrid = pmeGpu->kernelParams->grid.d_realGrid[gridIndex];
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:            wallcycle_start(wcycle, WallCycleCounter::LaunchGpuPme);
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                    pmeGpu->archSpecific->pmeStream_,
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                    pmeGpu->haloExchange->d_sendGrids[gmx::DirectionX::Up][gmx::DirectionY::Center].get_pointer(),
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                    pmeGpu->haloExchange->d_sendGrids[gmx::DirectionX::Down][gmx::DirectionY::Center]
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                    pmeGpu->haloExchange->d_sendGrids[gmx::DirectionX::Center][gmx::DirectionY::Left]
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                    pmeGpu->haloExchange
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                    pmeGpu->haloExchange->d_sendGrids[gmx::DirectionX::Up][gmx::DirectionY::Left].get_pointer(),
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                    pmeGpu->haloExchange->d_sendGrids[gmx::DirectionX::Down][gmx::DirectionY::Left].get_pointer(),
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                    pmeGpu->haloExchange->d_sendGrids[gmx::DirectionX::Up][gmx::DirectionY::Right].get_pointer(),
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                    pmeGpu->haloExchange->d_sendGrids[gmx::DirectionX::Down][gmx::DirectionY::Right].get_pointer(),
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                    pmeGpu->haloExchange->d_sendGrids[gmx::DirectionX::Up][gmx::DirectionY::Center]);
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                    pmeGpu->haloExchange->d_sendGrids[gmx::DirectionX::Down][gmx::DirectionY::Center]);
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                    pmeGpu->haloExchange->d_recvGrids[gmx::DirectionX::Up][gmx::DirectionY::Center]);
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                    pmeGpu->haloExchange->d_recvGrids[gmx::DirectionX::Down][gmx::DirectionY::Center]);
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:            wallcycle_stop(wcycle, WallCycleCounter::LaunchGpuPme);
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:        wallcycle_start(wcycle, WallCycleCounter::WaitGpuFftToPmeGrid);
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:        // Make sure data is ready on GPU before MPI communication.
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:        pmeGpu->archSpecific->pmeStream_.synchronize();
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:        wallcycle_stop(wcycle, WallCycleCounter::WaitGpuFftToPmeGrid);
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                           pmeGpu->common->mpiCommX);
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                               pmeGpu->common->mpiCommX);
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                    asMpiPointer(pmeGpu->haloExchange->d_sendGrids[gmx::DirectionX::Center][gmx::DirectionY::Left]),
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                    asMpiPointer(pmeGpu->haloExchange->d_recvGrids[gmx::DirectionX::Center][gmx::DirectionY::Right]),
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                    pmeGpu->common->mpiCommY);
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                        asMpiPointer(pmeGpu->haloExchange->d_sendGrids[gmx::DirectionX::Center][gmx::DirectionY::Right]),
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                        asMpiPointer(pmeGpu->haloExchange->d_recvGrids[gmx::DirectionX::Center][gmx::DirectionY::Left]),
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                        pmeGpu->common->mpiCommY);
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                    asMpiPointer(pmeGpu->haloExchange->d_sendGrids[gmx::DirectionX::Up][gmx::DirectionY::Left]),
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                    asMpiPointer(pmeGpu->haloExchange->d_recvGrids[gmx::DirectionX::Down][gmx::DirectionY::Right]),
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                    pmeGpu->common->mpiComm);
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                        asMpiPointer(pmeGpu->haloExchange->d_sendGrids[gmx::DirectionX::Up][gmx::DirectionY::Right]),
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                        asMpiPointer(pmeGpu->haloExchange->d_recvGrids[gmx::DirectionX::Down][gmx::DirectionY::Left]),
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                        pmeGpu->common->mpiComm);
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                        asMpiPointer(pmeGpu->haloExchange->d_sendGrids[gmx::DirectionX::Down][gmx::DirectionY::Left]),
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                        asMpiPointer(pmeGpu->haloExchange->d_recvGrids[gmx::DirectionX::Up][gmx::DirectionY::Right]),
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                        pmeGpu->common->mpiComm);
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                        asMpiPointer(pmeGpu->haloExchange->d_sendGrids[gmx::DirectionX::Down][gmx::DirectionY::Right]),
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                        asMpiPointer(pmeGpu->haloExchange->d_recvGrids[gmx::DirectionX::Up][gmx::DirectionY::Left]),
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                        pmeGpu->common->mpiComm);
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:            wallcycle_start(wcycle, WallCycleCounter::LaunchGpuPme);
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                    pmeGpu->archSpecific->pmeStream_,
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                    pmeGpu->haloExchange->d_recvGrids[gmx::DirectionX::Up][gmx::DirectionY::Center].get_pointer(),
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                    pmeGpu->haloExchange->d_recvGrids[gmx::DirectionX::Down][gmx::DirectionY::Center]
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                    pmeGpu->haloExchange->d_recvGrids[gmx::DirectionX::Center][gmx::DirectionY::Left]
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                    pmeGpu->haloExchange
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                    pmeGpu->haloExchange->d_recvGrids[gmx::DirectionX::Up][gmx::DirectionY::Left].get_pointer(),
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                    pmeGpu->haloExchange->d_recvGrids[gmx::DirectionX::Down][gmx::DirectionY::Left].get_pointer(),
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                    pmeGpu->haloExchange->d_recvGrids[gmx::DirectionX::Up][gmx::DirectionY::Right].get_pointer(),
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                    pmeGpu->haloExchange->d_recvGrids[gmx::DirectionX::Down][gmx::DirectionY::Right].get_pointer(),
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:            wallcycle_stop(wcycle, WallCycleCounter::LaunchGpuPme);
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:    GMX_UNUSED_VALUE(pmeGpu);
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:        // for coalescing. Current value is taken from the CUDA version.
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:        // For Intel GPUs, we force 32-wide execution with
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:void convertPmeGridToFftGrid(const PmeGpu* pmeGpu, float* h_fftRealGrid, gmx_parallel_3dfft* fftSetup, const int gridIndex)
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:    const sycl::uint3 localPmeSize  = { pmeGpu->kernelParams->grid.realGridSizePadded[XX],
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                                        pmeGpu->kernelParams->grid.realGridSizePadded[YY],
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                                        pmeGpu->kernelParams->grid.realGridSizePadded[ZZ] };
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                                 &pmeGpu->kernelParams->grid.d_realGrid[gridIndex],
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                                 pmeGpu->archSpecific->pmeStream_,
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                                 pmeGpu->settings.transferKind,
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:            copyToDeviceBuffer(&pmeGpu->kernelParams->grid.d_realGrid[gridIndex],
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                               pmeGpu->archSpecific->pmeStream_,
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                               pmeGpu->settings.transferKind,
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                pmeGpu->archSpecific->pmeStream_,
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                pmeGpu->kernelParams->grid.d_realGrid[gridIndex].get_pointer(),
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:        pmeGpu->archSpecific->syncSpreadGridD2H.markEvent(pmeGpu->archSpecific->pmeStream_);
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:void convertPmeGridToFftGrid(const PmeGpu* pmeGpu, DeviceBuffer<float>* d_fftRealGrid, const int gridIndex)
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:    const sycl::uint3 localPmeSize{ pmeGpu->kernelParams->grid.realGridSizePadded[XX],
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                                    pmeGpu->kernelParams->grid.realGridSizePadded[YY],
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                                    pmeGpu->kernelParams->grid.realGridSizePadded[ZZ] };
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:    const sycl::uint3 localFftNData{ pmeGpu->archSpecific->localRealGridSize[XX],
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                                     pmeGpu->archSpecific->localRealGridSize[YY],
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                                     pmeGpu->archSpecific->localRealGridSize[ZZ] };
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:    const sycl::uint3 localFftSize{ pmeGpu->archSpecific->localRealGridSizePadded[XX],
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                                    pmeGpu->archSpecific->localRealGridSizePadded[YY],
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                                    pmeGpu->archSpecific->localRealGridSizePadded[ZZ] };
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                                     &pmeGpu->kernelParams->grid.d_realGrid[gridIndex],
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                                     pmeGpu->archSpecific->pmeStream_,
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                                     pmeGpu->settings.transferKind,
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:            copyBetweenDeviceBuffers(&pmeGpu->kernelParams->grid.d_realGrid[gridIndex],
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                                     pmeGpu->archSpecific->pmeStream_,
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                                     pmeGpu->settings.transferKind,
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                pmeGpu->archSpecific->pmeStream_,
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:                pmeGpu->kernelParams->grid.d_realGrid[gridIndex].get_pointer(),
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:template void convertPmeGridToFftGrid<true>(const PmeGpu*       pmeGpu,
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:template void convertPmeGridToFftGrid<false>(const PmeGpu*       pmeGpu,
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:template void convertPmeGridToFftGrid<true>(const PmeGpu*        pmeGpu,
src/gromacs/ewald/pme_gpu_grid_sycl.cpp:template void convertPmeGridToFftGrid<false>(const PmeGpu*        pmeGpu,
src/gromacs/ewald/pme_force_sender_gpu_impl.h: * \brief Declaration of class which sends PME Force from GPU memory to PP task
src/gromacs/ewald/pme_force_sender_gpu_impl.h: * \author Alan Gray <alang@nvidia.com>
src/gromacs/ewald/pme_force_sender_gpu_impl.h:#ifndef GMX_PMEFORCESENDERGPU_IMPL_H
src/gromacs/ewald/pme_force_sender_gpu_impl.h:#define GMX_PMEFORCESENDERGPU_IMPL_H
src/gromacs/ewald/pme_force_sender_gpu_impl.h:#include "gromacs/ewald/pme_force_sender_gpu.h"
src/gromacs/ewald/pme_force_sender_gpu_impl.h:#include "gromacs/gpu_utils/devicebuffer_datatype.h"
src/gromacs/ewald/pme_force_sender_gpu_impl.h:#include "gromacs/gpu_utils/gputraits.h"
src/gromacs/ewald/pme_force_sender_gpu_impl.h:class GpuEventSynchronizer;
src/gromacs/ewald/pme_force_sender_gpu_impl.h:/*! \internal \brief Class with interfaces and data for CUDA version of PME Force sending functionality*/
src/gromacs/ewald/pme_force_sender_gpu_impl.h:    std::unique_ptr<GpuEventSynchronizer> event;
src/gromacs/ewald/pme_force_sender_gpu_impl.h:    //! GPU force buffer pointers for remote PP rank
src/gromacs/ewald/pme_force_sender_gpu_impl.h:    Float3* pmeRemoteGpuForcePtr;
src/gromacs/ewald/pme_force_sender_gpu_impl.h:class PmeForceSenderGpu::Impl
src/gromacs/ewald/pme_force_sender_gpu_impl.h:    /*! \brief Creates PME GPU Force sender object
src/gromacs/ewald/pme_force_sender_gpu_impl.h:     * \param[in] pmeForcesReady  Event synchronizer marked when PME forces are ready on the GPU
src/gromacs/ewald/pme_force_sender_gpu_impl.h:     * \param[in] deviceContext   GPU context
src/gromacs/ewald/pme_force_sender_gpu_impl.h:    Impl(GpuEventSynchronizer*  pmeForcesReady,
src/gromacs/ewald/pme_force_sender_gpu_impl.h:     * \param[in] d_f   force buffer in GPU memory
src/gromacs/ewald/pme_force_sender_gpu_impl.h:     * \param[in] sendForcesDirectToPpGpu  whether forces are transferred direct to remote GPU memory
src/gromacs/ewald/pme_force_sender_gpu_impl.h:    void sendFToPpPeerToPeer(int ppRank, int numAtoms, bool sendForcesDirectToPpGpu);
src/gromacs/ewald/pme_force_sender_gpu_impl.h:     * \param[in] sendbuf  force buffer in GPU memory
src/gromacs/ewald/pme_force_sender_gpu_impl.h:    void sendFToPpGpuAwareMpi(DeviceBuffer<RVec> sendbuf, int offset, int numBytes, int ppRank, MPI_Request* request);
src/gromacs/ewald/pme_force_sender_gpu_impl.h:    //! Event indicating when PME forces are ready on the GPU in order for PP stream to sync with the PME stream
src/gromacs/ewald/pme_force_sender_gpu_impl.h:    GpuEventSynchronizer* pmeForcesReady_;
src/gromacs/ewald/pme_force_sender_gpu_impl.h:    //! Whether GPU to CPU communication should be staged as GPU to
src/gromacs/ewald/pme_force_sender_gpu_impl.h:    //! GPU via P2P cudaMemcpy, then local D2H, for thread-MPI This
src/gromacs/ewald/pme_force_sender_gpu_impl.h:    //! GPUs, but direct communication is expected to be advantageous
src/gromacs/ewald/pme_force_sender_gpu_impl.h:    bool stageThreadMpiGpuCpuComm_ = false;
src/gromacs/ewald/pme_solve_sycl.h: *  \brief Implements PME GPU spline calculation and charge spreading in SYCL.
src/gromacs/ewald/pme_solve_sycl.h:#include "gromacs/gpu_utils/syclutils.h"
src/gromacs/ewald/pme_solve_sycl.h:#include "pme_gpu_internal.h"
src/gromacs/ewald/pme_solve_sycl.h:#include "pme_gpu_types.h"
src/gromacs/ewald/pme_solve_sycl.h:struct PmeGpuConstParams;
src/gromacs/ewald/pme_solve_sycl.h:struct PmeGpuGridParams;
src/gromacs/ewald/pme_solve_sycl.h:    PmeGpuConstParams* constParams_ = nullptr;
src/gromacs/ewald/pme_solve_sycl.h:    PmeGpuGridParams* gridParams_ = nullptr;
src/gromacs/ewald/pme_solve_sycl.cpp: *  \brief Implements PME GPU Fourier grid solving in SYCL.
src/gromacs/ewald/pme_solve_sycl.cpp:#include "gromacs/gpu_utils/gmxsycl.h"
src/gromacs/ewald/pme_solve_sycl.cpp:#include "gromacs/gpu_utils/sycl_kernel_utils.h"
src/gromacs/ewald/pme_solve_sycl.cpp:#include "pme_gpu_constants.h"
src/gromacs/ewald/pme_solve_sycl.cpp:        auto* params = reinterpret_cast<PmeGpuKernelParams*>(arg);
src/gromacs/ewald/pme_solve_sycl.cpp:    // SYCL has different multidimensional layout than OpenCL/CUDA.
src/gromacs/ewald/pme_gpu_program.h: * Declares PmeGpuProgram
src/gromacs/ewald/pme_gpu_program.h: * to store data derived from the GPU context or devices for
src/gromacs/ewald/pme_gpu_program.h:#ifndef GMX_EWALD_PME_PME_GPU_PROGRAM_H
src/gromacs/ewald/pme_gpu_program.h:#define GMX_EWALD_PME_PME_GPU_PROGRAM_H
src/gromacs/ewald/pme_gpu_program.h:struct PmeGpuProgramImpl;
src/gromacs/ewald/pme_gpu_program.h: * \brief Stores PME data derived from the GPU context or devices.
src/gromacs/ewald/pme_gpu_program.h:class PmeGpuProgram
src/gromacs/ewald/pme_gpu_program.h:    /*! \brief Construct a PME GPU program.
src/gromacs/ewald/pme_gpu_program.h:     * \param[in] deviceContext  GPU context.
src/gromacs/ewald/pme_gpu_program.h:    explicit PmeGpuProgram(const DeviceContext& deviceContext);
src/gromacs/ewald/pme_gpu_program.h:    ~PmeGpuProgram();
src/gromacs/ewald/pme_gpu_program.h:    std::unique_ptr<PmeGpuProgramImpl> impl_;
src/gromacs/ewald/pme_gpu_program.h:/*! \brief This is an owning handle for the compiled PME GPU kernels.
src/gromacs/ewald/pme_gpu_program.h:using PmeGpuProgramStorage = std::unique_ptr<PmeGpuProgram>;
src/gromacs/ewald/pme_gpu_program.h: * Factory function used to build persistent PME GPU program for the device at once.
src/gromacs/ewald/pme_gpu_program.h:PmeGpuProgramStorage buildPmeGpuProgram(const DeviceContext& /* deviceContext */);
src/gromacs/ewald/pme_gpu_timings.h: *  \brief Defines PME GPU timing functions.
src/gromacs/ewald/pme_gpu_timings.h:#ifndef GMX_EWALD_PME_GPU_TIMINGS_H
src/gromacs/ewald/pme_gpu_timings.h:#define GMX_EWALD_PME_GPU_TIMINGS_H
src/gromacs/ewald/pme_gpu_timings.h:struct gmx_wallclock_gpu_pme_t;
src/gromacs/ewald/pme_gpu_timings.h:struct PmeGpu;
src/gromacs/ewald/pme_gpu_timings.h: * Starts timing the certain PME GPU stage during a single computation (if timings are enabled).
src/gromacs/ewald/pme_gpu_timings.h: * \param[in] pmeGpu         The PME GPU data structure.
src/gromacs/ewald/pme_gpu_timings.h: * \param[in] pmeStageId     The PME GPU stage gtPME_ index from the enum in src/gromacs/timing/gpu_timing.h
src/gromacs/ewald/pme_gpu_timings.h:void pme_gpu_start_timing(const PmeGpu* pmeGpu, PmeStage pmeStageId);
src/gromacs/ewald/pme_gpu_timings.h: * Stops timing the certain PME GPU stage during a single computation (if timings are enabled).
src/gromacs/ewald/pme_gpu_timings.h: * \param[in] pmeGpu         The PME GPU data structure.
src/gromacs/ewald/pme_gpu_timings.h: * \param[in] pmeStageId     The PME GPU stage gtPME_ index from the enum in src/gromacs/timing/gpu_timing.h
src/gromacs/ewald/pme_gpu_timings.h:void pme_gpu_stop_timing(const PmeGpu* pmeGpu, PmeStage pmeStageId);
src/gromacs/ewald/pme_gpu_timings.h: * Tells if CUDA-based performance tracking is enabled for PME.
src/gromacs/ewald/pme_gpu_timings.h: * \param[in] pmeGpu         The PME GPU data structure.
src/gromacs/ewald/pme_gpu_timings.h:bool pme_gpu_timings_enabled(const PmeGpu* pmeGpu);
src/gromacs/ewald/pme_gpu_timings.h: * Finalizes all the active PME GPU stage timings for the current computation. Should be called at the end of every computation.
src/gromacs/ewald/pme_gpu_timings.h: * \param[in] pmeGpu         The PME GPU structure.
src/gromacs/ewald/pme_gpu_timings.h:void pme_gpu_update_timings(const PmeGpu* pmeGpu);
src/gromacs/ewald/pme_gpu_timings.h: * Updates the internal list of active PME GPU stages (if timings are enabled).
src/gromacs/ewald/pme_gpu_timings.h: * \param[in] pmeGpu         The PME GPU data structure.
src/gromacs/ewald/pme_gpu_timings.h:void pme_gpu_reinit_timings(const PmeGpu* pmeGpu);
src/gromacs/ewald/pme_gpu_timings.h: * Resets the PME GPU timings. To be called at the reset MD step.
src/gromacs/ewald/pme_gpu_timings.h: * \param[in] pmeGpu         The PME GPU structure.
src/gromacs/ewald/pme_gpu_timings.h:void pme_gpu_reset_timings(const PmeGpu* pmeGpu);
src/gromacs/ewald/pme_gpu_timings.h: * Copies the PME GPU timings to the gmx_wallclock_gpu_t structure (for log output). To be called at the run end.
src/gromacs/ewald/pme_gpu_timings.h: * \param[in] pmeGpu         The PME GPU structure.
src/gromacs/ewald/pme_gpu_timings.h: * \param[in] timings        The gmx_wallclock_gpu_pme_t structure.
src/gromacs/ewald/pme_gpu_timings.h:void pme_gpu_get_timings(const PmeGpu* pmeGpu, gmx_wallclock_gpu_pme_t* timings);
src/gromacs/ewald/pme_gpu_program_impl.cpp: * Implements PmeGpuProgramImpl for non-GPU builds.
src/gromacs/ewald/pme_gpu_program_impl.cpp:#include "pme_gpu_program_impl.h"
src/gromacs/ewald/pme_gpu_program_impl.cpp:PmeGpuProgramImpl::PmeGpuProgramImpl(const DeviceContext& deviceContext) :
src/gromacs/ewald/pme_gpu_program_impl.cpp:PmeGpuProgramImpl::~PmeGpuProgramImpl() = default;
src/gromacs/ewald/pme_gpu_program.cpp: * Implements PmeGpuProgram, which wrap arounds PmeGpuProgramImpl
src/gromacs/ewald/pme_gpu_program.cpp: * to store permanent PME GPU context-derived data,
src/gromacs/ewald/pme_gpu_program.cpp:#include "pme_gpu_program.h"
src/gromacs/ewald/pme_gpu_program.cpp:#include "pme_gpu_program_impl.h"
src/gromacs/ewald/pme_gpu_program.cpp:PmeGpuProgram::PmeGpuProgram(const DeviceContext& deviceContext) :
src/gromacs/ewald/pme_gpu_program.cpp:    impl_(std::make_unique<PmeGpuProgramImpl>(deviceContext))
src/gromacs/ewald/pme_gpu_program.cpp:PmeGpuProgram::~PmeGpuProgram() = default;
src/gromacs/ewald/pme_gpu_program.cpp:int PmeGpuProgram::warpSize() const
src/gromacs/ewald/pme_gpu_program.cpp:PmeGpuProgramStorage buildPmeGpuProgram(const DeviceContext& deviceContext)
src/gromacs/ewald/pme_gpu_program.cpp:    return std::make_unique<PmeGpuProgram>(deviceContext);
src/gromacs/ewald/pme_force_sender_gpu_impl_gpu.cu: * \brief Implememnts backend-specific code for PME-PP communication using CUDA.
src/gromacs/ewald/pme_force_sender_gpu_impl_gpu.cu: * \author Alan Gray <alang@nvidia.com>
src/gromacs/ewald/pme_force_sender_gpu_impl_gpu.cu:#include "gromacs/gpu_utils/cudautils.cuh"
src/gromacs/ewald/pme_force_sender_gpu_impl_gpu.cu:#include "gromacs/gpu_utils/gpueventsynchronizer.h"
src/gromacs/ewald/pme_force_sender_gpu_impl_gpu.cu:#include "gromacs/gpu_utils/typecasts_cuda_hip.h"
src/gromacs/ewald/pme_force_sender_gpu_impl_gpu.cu:#include "pme_force_sender_gpu_impl.h"
src/gromacs/ewald/pme_force_sender_gpu_impl_gpu.cu:/*! \brief Send PME synchronizer directly using CUDA memory copy */
src/gromacs/ewald/pme_force_sender_gpu_impl_gpu.cu:void PmeForceSenderGpu::Impl::sendFToPpPeerToPeer(int ppRank, int numAtoms, bool sendForcesDirectToPpGpu)
src/gromacs/ewald/pme_force_sender_gpu_impl_gpu.cu:    GMX_ASSERT(GMX_THREAD_MPI, "sendFToPpCudaDirect is expected to be called only for Thread-MPI");
src/gromacs/ewald/pme_force_sender_gpu_impl_gpu.cu:    Float3* pmeRemoteForcePtr = (sendForcesDirectToPpGpu || stageThreadMpiGpuCpuComm_)
src/gromacs/ewald/pme_force_sender_gpu_impl_gpu.cu:                                        ? ppCommManagers_[ppRank].pmeRemoteGpuForcePtr
src/gromacs/ewald/pme_force_sender_gpu_impl_gpu.cu:    // Push data to remote GPU's memory
src/gromacs/ewald/pme_force_sender_gpu_impl_gpu.cu:    cudaError_t stat = cudaMemcpyAsync(asFloat3(pmeRemoteForcePtr),
src/gromacs/ewald/pme_force_sender_gpu_impl_gpu.cu:                                       cudaMemcpyDefault,
src/gromacs/ewald/pme_force_sender_gpu_impl_gpu.cu:    CU_RET_ERR(stat, "cudaMemcpyAsync on Recv from PME CUDA direct data transfer failed");
src/gromacs/ewald/pme_force_sender_gpu_impl_gpu.cu:    if (stageThreadMpiGpuCpuComm_ && !sendForcesDirectToPpGpu)
src/gromacs/ewald/pme_force_sender_gpu_impl_gpu.cu:        // Perform local D2H (from remote GPU memory to remote PP rank's CPU memory)
src/gromacs/ewald/pme_force_sender_gpu_impl_gpu.cu:        stat = cudaMemcpyAsync(ppCommManagers_[ppRank].pmeRemoteCpuForcePtr,
src/gromacs/ewald/pme_force_sender_gpu_impl_gpu.cu:                               ppCommManagers_[ppRank].pmeRemoteGpuForcePtr,
src/gromacs/ewald/pme_force_sender_gpu_impl_gpu.cu:                               cudaMemcpyDefault,
src/gromacs/ewald/pme_force_sender_gpu_impl_gpu.cu:        CU_RET_ERR(stat, "cudaMemcpyAsync on local device to host transfer of PME forces failed");
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cu: * \brief Implements backend-specific part of PME-PP communication using CUDA.
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cu: * \author Alan Gray <alang@nvidia.com>
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cu:#include "gromacs/gpu_utils/cudautils.cuh"
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cu:#include "gromacs/gpu_utils/device_context.h"
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cu:#include "gromacs/gpu_utils/device_stream.h"
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cu:#include "gromacs/gpu_utils/devicebuffer.h"
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cu:#include "gromacs/gpu_utils/gpueventsynchronizer.h"
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cu:#include "gromacs/gpu_utils/typecasts_cuda_hip.h"
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cu:#include "pme_pp_comm_gpu_impl.h"
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cu:void PmePpCommGpu::Impl::sendCoordinatesToPmePeerToPeer(Float3* sendPtr,
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cu:                                                        GpuEventSynchronizer* coordinatesReadyOnDeviceEvent)
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cu:    cudaError_t stat = cudaMemcpyAsync(remotePmeXBuffer_,
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cu:                                       cudaMemcpyDefault,
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cu:    CU_RET_ERR(stat, "cudaMemcpyAsync on Send to PME CUDA direct data transfer failed");
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cu:    GpuEventSynchronizer* pmeSync = &pmeCoordinatesSynchronizer_;
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cu:    MPI_Send(&pmeSync, sizeof(GpuEventSynchronizer*), MPI_BYTE, pmeRank_, 0, comm_);
src/gromacs/ewald/pme_gpu_calculate_splines.cuh:#include "gromacs/gpu_utils/cuda_kernel_utils.cuh"
src/gromacs/ewald/pme_gpu_calculate_splines.cuh:#include "gromacs/gpu_utils/vectype_ops_cuda.h"
src/gromacs/ewald/pme_gpu_calculate_splines.cuh: * which is laid out for GPU spread/gather kernels. The base only corresponds to the atom index within the execution block.
src/gromacs/ewald/pme_gpu_calculate_splines.cuh: * \returns Index into theta or dtheta array using GPU layout.
src/gromacs/ewald/pme_gpu_calculate_splines.cuh: * which is laid out for GPU spread/gather kernels. The index is wrt to the execution block,
src/gromacs/ewald/pme_gpu_calculate_splines.cuh: * \returns Index into theta or dtheta array using GPU layout.
src/gromacs/ewald/pme_gpu_calculate_splines.cuh: * An inline CUDA function for skipping the zero-charge atoms.
src/gromacs/ewald/pme_gpu_calculate_splines.cuh:static bool __device__ __forceinline__ pme_gpu_check_atom_charge(const float coefficient)
src/gromacs/ewald/pme_gpu_calculate_splines.cuh:static __device__ __forceinline__ void pme_gpu_stage_atom_data(T* __restrict__ sm_destination,
src/gromacs/ewald/pme_gpu_calculate_splines.cuh: * PME GPU spline parameter and gridline indices calculation.
src/gromacs/ewald/pme_gpu_calculate_splines.cuh: * \param[in]  kernelParams         Input PME CUDA data in constant memory.
src/gromacs/ewald/pme_gpu_calculate_splines.cuh:static __device__ __forceinline__ void calculate_splines(const PmeGpuCudaKernelParams kernelParams,
src/gromacs/ewald/pme_gpu_calculate_splines.cuh:        const int chargeCheck = pme_gpu_check_atom_charge(atomCharge);
src/gromacs/ewald/pme_gpu_calculate_splines.cuh:            int o = orderIndex; // This is an index that is set once for PME_GPU_PARALLEL_SPLINE == 1
src/gromacs/ewald/CMakeLists.txt:    pme_gpu_program.cpp
src/gromacs/ewald/CMakeLists.txt:    pme_pp_comm_gpu_impl.cpp
src/gromacs/ewald/CMakeLists.txt:    pme_coordinate_receiver_gpu_impl.cpp
src/gromacs/ewald/CMakeLists.txt:    pme_force_sender_gpu_impl.cpp
src/gromacs/ewald/CMakeLists.txt:if (GMX_GPU_CUDA)
src/gromacs/ewald/CMakeLists.txt:        # CUDA-specific sources
src/gromacs/ewald/CMakeLists.txt:        pme_gpu_program_impl.cu
src/gromacs/ewald/CMakeLists.txt:        pme_pp_comm_gpu_impl_gpu.cpp
src/gromacs/ewald/CMakeLists.txt:        pme_pp_comm_gpu_impl_gpu.cu
src/gromacs/ewald/CMakeLists.txt:        pme_force_sender_gpu_impl_gpu.cpp
src/gromacs/ewald/CMakeLists.txt:        pme_force_sender_gpu_impl_gpu.cu
src/gromacs/ewald/CMakeLists.txt:        pme_coordinate_receiver_gpu_impl_gpu.cpp
src/gromacs/ewald/CMakeLists.txt:        pme_gpu_grid.cu
src/gromacs/ewald/CMakeLists.txt:        # GPU-specific sources
src/gromacs/ewald/CMakeLists.txt:        pme_gpu.cpp
src/gromacs/ewald/CMakeLists.txt:        pme_gpu_internal.cpp
src/gromacs/ewald/CMakeLists.txt:        pme_gpu_timings.cpp
src/gromacs/ewald/CMakeLists.txt:    # the CI build that does clang-tidy analysis of a CUDA build.
src/gromacs/ewald/CMakeLists.txt:    _gmx_add_files_to_property(CUDA_SOURCES
src/gromacs/ewald/CMakeLists.txt:        pme_gpu_internal.cpp
src/gromacs/ewald/CMakeLists.txt:        pme_gpu_timings.cpp
src/gromacs/ewald/CMakeLists.txt:        pme_pp_comm_gpu_impl_gpu.cpp
src/gromacs/ewald/CMakeLists.txt:        pme_force_sender_gpu_impl_gpu.cpp
src/gromacs/ewald/CMakeLists.txt:        pme_coordinate_receiver_gpu_impl_gpu.cpp
src/gromacs/ewald/CMakeLists.txt:elseif (GMX_GPU_OPENCL)
src/gromacs/ewald/CMakeLists.txt:        # OpenCL-specific sources
src/gromacs/ewald/CMakeLists.txt:        pme_gpu_program_impl_ocl.cpp
src/gromacs/ewald/CMakeLists.txt:        pme_gpu_ocl_stubs.cpp
src/gromacs/ewald/CMakeLists.txt:        # GPU-specific sources
src/gromacs/ewald/CMakeLists.txt:        pme_gpu.cpp
src/gromacs/ewald/CMakeLists.txt:        pme_gpu_internal.cpp
src/gromacs/ewald/CMakeLists.txt:        pme_gpu_timings.cpp
src/gromacs/ewald/CMakeLists.txt:elseif (GMX_GPU_SYCL)
src/gromacs/ewald/CMakeLists.txt:        # GPU-specific sources
src/gromacs/ewald/CMakeLists.txt:        pme_gpu.cpp
src/gromacs/ewald/CMakeLists.txt:        pme_gpu_grid_sycl.cpp
src/gromacs/ewald/CMakeLists.txt:        pme_gpu_internal.cpp
src/gromacs/ewald/CMakeLists.txt:        pme_gpu_program_impl_sycl.cpp
src/gromacs/ewald/CMakeLists.txt:        pme_gpu_timings.cpp
src/gromacs/ewald/CMakeLists.txt:        pme_pp_comm_gpu_impl_gpu_sycl.cpp
src/gromacs/ewald/CMakeLists.txt:        pme_force_sender_gpu_impl_gpu_sycl.cpp
src/gromacs/ewald/CMakeLists.txt:        pme_pp_comm_gpu_impl_gpu.cpp
src/gromacs/ewald/CMakeLists.txt:        pme_force_sender_gpu_impl_gpu.cpp
src/gromacs/ewald/CMakeLists.txt:        pme_force_sender_gpu_impl_gpu_sycl.cpp
src/gromacs/ewald/CMakeLists.txt:        pme_coordinate_receiver_gpu_impl_gpu.cpp
src/gromacs/ewald/CMakeLists.txt:        pme_gpu_grid_sycl.cpp
src/gromacs/ewald/CMakeLists.txt:        pme_gpu_internal.cpp
src/gromacs/ewald/CMakeLists.txt:        pme_gpu_program.cpp
src/gromacs/ewald/CMakeLists.txt:        pme_gpu_program_impl_sycl.cpp
src/gromacs/ewald/CMakeLists.txt:        pme_gpu_3dfft_sycl.cpp
src/gromacs/ewald/CMakeLists.txt:        pme_gpu_timings.cpp
src/gromacs/ewald/CMakeLists.txt:        pme_pp_comm_gpu_impl_gpu.cpp
src/gromacs/ewald/CMakeLists.txt:        pme_pp_comm_gpu_impl_gpu_sycl.cpp
src/gromacs/ewald/CMakeLists.txt:        pme_force_sender_gpu_impl_gpu.cpp
src/gromacs/ewald/CMakeLists.txt:        pme_force_sender_gpu_impl_gpu_sycl.cpp
src/gromacs/ewald/CMakeLists.txt:        pme_coordinate_receiver_gpu_impl_gpu.cpp
src/gromacs/ewald/CMakeLists.txt:elseif (GMX_GPU_HIP)
src/gromacs/ewald/CMakeLists.txt:        pme_gpu.cpp
src/gromacs/ewald/CMakeLists.txt:        pme_gpu_hip_stubs.cpp
src/gromacs/ewald/CMakeLists.txt:        pme_gpu_program_impl_hip.cpp
src/gromacs/ewald/CMakeLists.txt:        pme_gpu.cpp
src/gromacs/ewald/CMakeLists.txt:        pme_gpu_hip_stubs.cpp
src/gromacs/ewald/CMakeLists.txt:        pme_gpu_program.cpp
src/gromacs/ewald/CMakeLists.txt:        pme_gpu_program_impl_hip.cpp
src/gromacs/ewald/CMakeLists.txt:        pme_pp_comm_gpu_impl.cpp
src/gromacs/ewald/CMakeLists.txt:        pme_coordinate_receiver_gpu_impl.cpp
src/gromacs/ewald/CMakeLists.txt:        pme_force_sender_gpu_impl.cpp
src/gromacs/ewald/CMakeLists.txt:        pme_gpu_program_impl.cpp
src/gromacs/ewald/CMakeLists.txt:    "${CMAKE_CURRENT_SOURCE_DIR}/pme_gpu_calculate_splines.clh"
src/gromacs/ewald/CMakeLists.txt:foreach(VENDOR AMD NVIDIA INTEL APPLE)
src/gromacs/ewald/CMakeLists.txt:        # to avoid pme_gpu_types.h:100:52: warning: padding struct 'struct PmeGpuConstParams' with 4 bytes to align 'd_virialAndEnergy'
src/gromacs/ewald/pme_gpu_constants.h: * \brief This file defines the PME GPU compile-time constants/macros,
src/gromacs/ewald/pme_gpu_constants.h: * As OpenCL C is not aware of constexpr, most of this file is
src/gromacs/ewald/pme_gpu_constants.h: * forwarded to the OpenCL kernel compilation as defines with same
src/gromacs/ewald/pme_gpu_constants.h: * \todo The values are currently common to both CUDA and OpenCL
src/gromacs/ewald/pme_gpu_constants.h: * implementations, but should be reconsidered when we tune the OpenCL
src/gromacs/ewald/pme_gpu_constants.h:#ifndef GMX_EWALD_PME_GPU_CONSTANTS_H
src/gromacs/ewald/pme_gpu_constants.h:#define GMX_EWALD_PME_GPU_CONSTANTS_H
src/gromacs/ewald/pme_gpu_constants.h:#if GMX_GPU_CUDA
src/gromacs/ewald/pme_gpu_constants.h:#    include "gromacs/gpu_utils/cuda_arch_utils.cuh" // for warp_size
src/gromacs/ewald/pme_gpu_constants.h:/* General settings for PME GPU behaviour */
src/gromacs/ewald/pme_gpu_constants.h: *  Note that the GPU code, unlike the CPU, only supports order 4.
src/gromacs/ewald/pme_gpu_constants.h:constexpr int c_pmeGpuOrder = 4;
src/gromacs/ewald/pme_gpu_constants.h:/*! \brief The number of GPU threads used for computing spread/gather
src/gromacs/ewald/pme_gpu_constants.h:     * Only CUDA implements this. See Issue #2516 */
src/gromacs/ewald/pme_gpu_constants.h: * The execution widths for PME GPU kernels, used both on host and device for correct scheduling.
src/gromacs/ewald/pme_gpu_constants.h: * TODO: those were tuned for CUDA with assumption of warp size 32; specialize those for OpenCL
src/gromacs/ewald/pme_gpu_constants.h:#if GMX_GPU_CUDA
src/gromacs/ewald/pme_gpu_constants.h: * do that for OpenCL already.
src/gromacs/ewald/pme_gpu_constants.h: * while both with CUDA and OpenCL we have to treat the device
src/gromacs/ewald/pme_gpu_constants.h://! Gathering min blocks per CUDA multiprocessor (determined empirically to give best performance)
src/gromacs/ewald/pme_gpu_constants.h:static constexpr int c_gatherMinBlocksPerMP = GMX_CUDA_MAX_THREADS_PER_MP / c_gatherMaxThreadsPerBlock;
src/gromacs/ewald/pme_gpu_constants.h:#endif // GMX_GPU_CUDA
src/gromacs/ewald/pme_gpu_program_impl.h: * Declares PmeGpuProgramImpl, which stores PME GPU (compiled) kernel handles.
src/gromacs/ewald/pme_gpu_program_impl.h:#ifndef GMX_EWALD_PME_PME_GPU_PROGRAM_IMPL_H
src/gromacs/ewald/pme_gpu_program_impl.h:#define GMX_EWALD_PME_PME_GPU_PROGRAM_IMPL_H
src/gromacs/ewald/pme_gpu_program_impl.h:#include "gromacs/gpu_utils/device_context.h"
src/gromacs/ewald/pme_gpu_program_impl.h: * PME GPU persistent host program/kernel data, which should be initialized once for the whole execution.
src/gromacs/ewald/pme_gpu_program_impl.h: * Primary purpose of this is to not recompile GPU kernels for each OpenCL unit test,
src/gromacs/ewald/pme_gpu_program_impl.h: * while the relevant GPU context (e.g. cl_context) instance persists.
src/gromacs/ewald/pme_gpu_program_impl.h: * In CUDA, this just assigns the kernel function pointers.
src/gromacs/ewald/pme_gpu_program_impl.h:struct PmeGpuProgramImpl
src/gromacs/ewald/pme_gpu_program_impl.h:     * Only used in OpenCL for JIT compilation.
src/gromacs/ewald/pme_gpu_program_impl.h:#if GMX_GPU_CUDA
src/gromacs/ewald/pme_gpu_program_impl.h:    using PmeKernelHandle = void (*)(const struct PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_gpu_program_impl.h:#elif GMX_GPU_OPENCL
src/gromacs/ewald/pme_gpu_program_impl.h:     * Maximum synchronous GPU thread group execution width.
src/gromacs/ewald/pme_gpu_program_impl.h:     * "Warp" is a CUDA term which we end up reusing in OpenCL kernels as well.
src/gromacs/ewald/pme_gpu_program_impl.h:     * For CUDA, this is a static value that comes from gromacs/gpu_utils/cuda_arch_utils.cuh;
src/gromacs/ewald/pme_gpu_program_impl.h:     * for OpenCL, we have to query it dynamically.
src/gromacs/ewald/pme_gpu_program_impl.h:    PmeGpuProgramImpl() = delete;
src/gromacs/ewald/pme_gpu_program_impl.h:    explicit PmeGpuProgramImpl(const DeviceContext& deviceContext);
src/gromacs/ewald/pme_gpu_program_impl.h:    ~PmeGpuProgramImpl();
src/gromacs/ewald/pme_gpu_program_impl.h:    GMX_DISALLOW_COPY_AND_ASSIGN(PmeGpuProgramImpl);
src/gromacs/ewald/pme_gpu_calculate_splines_sycl.h:#include "gromacs/gpu_utils/gmxsycl.h"
src/gromacs/ewald/pme_gpu_calculate_splines_sycl.h:#include "gromacs/gpu_utils/gputraits_sycl.h"
src/gromacs/ewald/pme_gpu_calculate_splines_sycl.h:#include "gromacs/gpu_utils/sycl_kernel_utils.h"
src/gromacs/ewald/pme_gpu_calculate_splines_sycl.h:#include "pme_gpu_constants.h"
src/gromacs/ewald/pme_gpu_calculate_splines_sycl.h:#include "pme_gpu_types.h"
src/gromacs/ewald/pme_gpu_calculate_splines_sycl.h: * which is laid out for GPU spread/gather kernels. The base only corresponds to the atom index within the execution block.
src/gromacs/ewald/pme_gpu_calculate_splines_sycl.h: * \returns Index into theta or dtheta array using GPU layout.
src/gromacs/ewald/pme_gpu_calculate_splines_sycl.h: * which is laid out for GPU spread/gather kernels. The index is wrt to the execution block,
src/gromacs/ewald/pme_gpu_calculate_splines_sycl.h: * \returns Index into theta or dtheta array using GPU layout.
src/gromacs/ewald/pme_gpu_calculate_splines_sycl.h:static inline bool pmeGpuCheckAtomCharge(const float charge)
src/gromacs/ewald/pme_gpu_calculate_splines_sycl.h:static inline void pmeGpuStageAtomData(sycl::local_ptr<T>              sm_destination,
src/gromacs/ewald/pme_gpu_calculate_splines_sycl.h: * PME GPU spline parameter and gridline indices calculation.
src/gromacs/ewald/pme_gpu_calculate_splines_sycl.h:            // Switch structure inherited from CUDA.
src/gromacs/ewald/pme_gpu_calculate_splines_sycl.h:        const int chargeCheck = pmeGpuCheckAtomCharge(atomCharge);
src/gromacs/ewald/pme_spread.clh: *  \brief Implements PME OpenCL spline parameter computation and charge spread kernels.
src/gromacs/ewald/pme_spread.clh: * When including this and other PME OpenCL kernel files, plenty of common
src/gromacs/ewald/pme_spread.clh: * For details, please see how pme_program.cl is compiled in pme_gpu_program_impl_ocl.cpp.
src/gromacs/ewald/pme_spread.clh:#include "gromacs/gpu_utils/vectype_ops.clh"
src/gromacs/ewald/pme_spread.clh:#include "pme_gpu_calculate_splines.clh"
src/gromacs/ewald/pme_spread.clh:#include "pme_gpu_types.h"
src/gromacs/ewald/pme_spread.clh: * 0: a single GPU thread handles a single dimension of a single particle (calculating and storing
src/gromacs/ewald/pme_spread.clh:#define PME_GPU_PARALLEL_SPLINE 0
src/gromacs/ewald/pme_spread.clh:inline void pme_gpu_stage_atom_data(__local float* __restrict__ sm_destination,
src/gromacs/ewald/pme_spread.clh: * PME GPU spline parameter and gridline indices calculation.
src/gromacs/ewald/pme_spread.clh: * \param[in]  kernelParams             Input PME GPU data in constant memory.
src/gromacs/ewald/pme_spread.clh:gmx_opencl_inline void calculate_splines(const struct PmeOpenCLKernelParams kernelParams,
src/gromacs/ewald/pme_spread.clh:     * With PME_GPU_PARALLEL_SPLINE == 0 it is just a local array of (order) values for some of the threads, which is fine;
src/gromacs/ewald/pme_spread.clh:     * With PME_GPU_PARALLEL_SPLINE == 1 (order) times more threads are involved, so the shared memory is used to avoid overhead.
src/gromacs/ewald/pme_spread.clh:#    if PME_GPU_PARALLEL_SPLINE
src/gromacs/ewald/pme_spread.clh:    const int localCheck = (dimIndex < DIM) && (orderIndex < (PME_GPU_PARALLEL_SPLINE ? order : 1));
src/gromacs/ewald/pme_spread.clh:        const int chargeCheck = pme_gpu_check_atom_charge(sm_coefficients[atomIndexLocal]);
src/gromacs/ewald/pme_spread.clh:            int o = orderIndex; // This is an index that is set once for PME_GPU_PARALLEL_SPLINE == 1
src/gromacs/ewald/pme_spread.clh:#    if !PME_GPU_PARALLEL_SPLINE
src/gromacs/ewald/pme_spread.clh:            // With PME_GPU_PARALLEL_SPLINE == 1, o is already set to orderIndex;
src/gromacs/ewald/pme_spread.clh:            // With PME_GPU_PARALLEL_SPLINE == 0, we loop o over range(order).
src/gromacs/ewald/pme_spread.clh:#    if !PME_GPU_PARALLEL_SPLINE
src/gromacs/ewald/pme_spread.clh: * \param[in]  kernelParams         Input PME GPU data in constant memory.
src/gromacs/ewald/pme_spread.clh:gmx_opencl_inline void spread_charges(const struct PmeOpenCLKernelParams kernelParams,
src/gromacs/ewald/pme_spread.clh:    const int chargeCheck = pme_gpu_check_atom_charge(sm_coefficients[atomIndexLocal]);
src/gromacs/ewald/pme_spread.clh: * \param[in]     kernelParams             Input PME GPU data in constant memory.
src/gromacs/ewald/pme_spread.clh:        pme_spline_and_spread_kernel)(const struct PmeOpenCLKernelParams kernelParams,
src/gromacs/ewald/pme_spread.clh:    pme_gpu_stage_atom_data(sm_coefficients, gm_coefficientsA, 1);
src/gromacs/ewald/pme_spread.clh:        pme_gpu_stage_atom_data(sm_coordinates, gm_coordinates, DIM);
src/gromacs/ewald/pme_spread.clh:#if !defined(_AMD_SOURCE_) && !defined(_NVIDIA_SOURCE_) && !defined(_APPLE_SOURCE_)
src/gromacs/ewald/pme_spread.clh:         * __syncwarp() in CUDA. #2519
src/gromacs/ewald/pme_spread.clh:         * (the data is assumed to be in GPU global memory with proper layout already,
src/gromacs/ewald/pme_spread.clh:        pme_gpu_stage_atom_data(sm_theta, gm_theta, DIM * order);
src/gromacs/ewald/pme_spread.clh:        pme_gpu_stage_atom_data(
src/gromacs/ewald/pme_spread.clh:        pme_gpu_stage_atom_data(sm_coefficients, gm_coefficientsB, 1);
src/gromacs/ewald/pme_gpu_types.h: * \brief Defines the PME GPU data structures
src/gromacs/ewald/pme_gpu_types.h: * (the GPU function parameters used both on host and device sides).
src/gromacs/ewald/pme_gpu_types.h:#ifndef GMX_EWALD_PME_GPU_TYPES_H
src/gromacs/ewald/pme_gpu_types.h:#define GMX_EWALD_PME_GPU_TYPES_H
src/gromacs/ewald/pme_gpu_types.h: * In OpenCL, the structures must be laid out on the host and device exactly the same way.
src/gromacs/ewald/pme_gpu_types.h: * #define GMX_GPU_ALIGNED __attribute__ ((aligned(8)))
src/gromacs/ewald/pme_gpu_types.h: * struct GMX_GPU_ALIGNED PmeGpuConstParams
src/gromacs/ewald/pme_gpu_types.h: * struct GMX_GPU_ALIGNED PmeGpuGridParams
src/gromacs/ewald/pme_gpu_types.h:/*! \brief A workaround to hide DeviceBuffer template from OpenCL kernel compilation
src/gromacs/ewald/pme_gpu_types.h:#ifndef __OPENCL_C_VERSION__
src/gromacs/ewald/pme_gpu_types.h:#    include "gromacs/gpu_utils/devicebuffer.h"
src/gromacs/ewald/pme_gpu_types.h:#    define HIDE_FROM_OPENCL_COMPILER(x) x
src/gromacs/ewald/pme_gpu_types.h:              "DeviceBuffer is defined as an 8 byte stub for OpenCL C");
src/gromacs/ewald/pme_gpu_types.h:              "DeviceBuffer is defined as an 8 byte stub for OpenCL C");
src/gromacs/ewald/pme_gpu_types.h:#    define HIDE_FROM_OPENCL_COMPILER(x) char8
src/gromacs/ewald/pme_gpu_types.h:/* What follows is all the PME GPU function arguments,
src/gromacs/ewald/pme_gpu_types.h: * This is GPU agnostic (float3 replaced by float[3], etc.).
src/gromacs/ewald/pme_gpu_types.h: * The GPU-framework specifics (e.g. cudaTextureObject_t handles) are described
src/gromacs/ewald/pme_gpu_types.h: * in the larger structure PmeGpuCudaKernelParams in the pme.cuh.
src/gromacs/ewald/pme_gpu_types.h: * A GPU data structure for storing the constant PME data.
src/gromacs/ewald/pme_gpu_types.h:struct PmeGpuConstParams
src/gromacs/ewald/pme_gpu_types.h:    /*! \brief Virial and energy GPU array. Size is c_virialAndEnergyCount (7) floats.
src/gromacs/ewald/pme_gpu_types.h:    HIDE_FROM_OPENCL_COMPILER(DeviceBuffer<float>) d_virialAndEnergy[NUMFEPSTATES];
src/gromacs/ewald/pme_gpu_types.h: * A GPU data structure for storing the PME data related to the grid sizes and cut-off.
src/gromacs/ewald/pme_gpu_types.h:struct PmeGpuGridParams
src/gromacs/ewald/pme_gpu_types.h:    HIDE_FROM_OPENCL_COMPILER(DeviceBuffer<float>) d_realGrid[NUMFEPSTATES];
src/gromacs/ewald/pme_gpu_types.h:    HIDE_FROM_OPENCL_COMPILER(DeviceBuffer<float>) d_fftComplexGrid[NUMFEPSTATES];
src/gromacs/ewald/pme_gpu_types.h:    HIDE_FROM_OPENCL_COMPILER(DeviceBuffer<float>) d_splineModuli[NUMFEPSTATES];
src/gromacs/ewald/pme_gpu_types.h:    HIDE_FROM_OPENCL_COMPILER(DeviceBuffer<float>) d_fractShiftsTable;
src/gromacs/ewald/pme_gpu_types.h:    HIDE_FROM_OPENCL_COMPILER(DeviceBuffer<int>) d_gridlineIndicesTable;
src/gromacs/ewald/pme_gpu_types.h: * A GPU data structure for storing the PME data of the atoms, local to this process' domain
src/gromacs/ewald/pme_gpu_types.h:struct PmeGpuAtomParams
src/gromacs/ewald/pme_gpu_types.h:    /*! \brief Global GPU memory array handle with input rvec atom coordinates.
src/gromacs/ewald/pme_gpu_types.h:     * The coordinates themselves change and need to be copied to the GPU for every PME computation,
src/gromacs/ewald/pme_gpu_types.h:    HIDE_FROM_OPENCL_COMPILER(DeviceBuffer<gmx::RVec>) d_coordinates;
src/gromacs/ewald/pme_gpu_types.h:    /*! \brief Global GPU memory array handle with input atom charges in states A and B.
src/gromacs/ewald/pme_gpu_types.h:     * The charges only need to be reallocated and copied to the GPU at DD step.
src/gromacs/ewald/pme_gpu_types.h:    HIDE_FROM_OPENCL_COMPILER(DeviceBuffer<float>) d_coefficients[NUMFEPSTATES];
src/gromacs/ewald/pme_gpu_types.h:    /*! \brief Global GPU memory array handle with input/output rvec atom forces.
src/gromacs/ewald/pme_gpu_types.h:     * The forces change and need to be copied from (and possibly to) the GPU for every PME
src/gromacs/ewald/pme_gpu_types.h:    HIDE_FROM_OPENCL_COMPILER(DeviceBuffer<gmx::RVec>) d_forces;
src/gromacs/ewald/pme_gpu_types.h:    /*! \brief Global GPU memory array handle with ivec atom gridline indices.
src/gromacs/ewald/pme_gpu_types.h:     * Computed on GPU in the spline calculation part.
src/gromacs/ewald/pme_gpu_types.h:    HIDE_FROM_OPENCL_COMPILER(DeviceBuffer<int>) d_gridlineIndices;
src/gromacs/ewald/pme_gpu_types.h:    /* B-spline parameters are computed entirely on GPU for every PME computation, not copied.
src/gromacs/ewald/pme_gpu_types.h:     * Unless we want to try something like GPU spread + CPU gather?
src/gromacs/ewald/pme_gpu_types.h:    /*! \brief Global GPU memory array handle with B-spline values */
src/gromacs/ewald/pme_gpu_types.h:    HIDE_FROM_OPENCL_COMPILER(DeviceBuffer<float>) d_theta;
src/gromacs/ewald/pme_gpu_types.h:    /*! \brief Global GPU memory array handle with B-spline derivative values */
src/gromacs/ewald/pme_gpu_types.h:    HIDE_FROM_OPENCL_COMPILER(DeviceBuffer<float>) d_dtheta;
src/gromacs/ewald/pme_gpu_types.h: * A GPU data structure for storing the PME data which might change for each new PME computation.
src/gromacs/ewald/pme_gpu_types.h:struct PmeGpuDynamicParams
src/gromacs/ewald/pme_gpu_types.h: * A single structure encompassing all the PME data used in GPU kernels on device.
src/gromacs/ewald/pme_gpu_types.h: * GPU framework-specific structure.
src/gromacs/ewald/pme_gpu_types.h:struct PmeGpuKernelParamsBase
src/gromacs/ewald/pme_gpu_types.h:    struct PmeGpuConstParams constants;
src/gromacs/ewald/pme_gpu_types.h:    struct PmeGpuGridParams grid;
src/gromacs/ewald/pme_gpu_types.h:    struct PmeGpuAtomParams atoms;
src/gromacs/ewald/pme_gpu_types.h:     * This should be kept up-to-date by calling pme_gpu_prepare_computation(...)
src/gromacs/ewald/pme_gpu_types.h:    struct PmeGpuDynamicParams current;
src/gromacs/ewald/pme_gpu_types.h:     * char rather than bool to avoid problem with OpenCL compiler */
src/gromacs/ewald/pme_gpu_types.h:    HIDE_FROM_OPENCL_COMPILER(DeviceBuffer<PpRanksSendFInfo>) ppRanksInfo;
src/gromacs/ewald/pme_gpu_types.h:    HIDE_FROM_OPENCL_COMPILER(DeviceBuffer<unsigned int>) lastProcessedBlockPerPpRank;
src/gromacs/ewald/pme_gpu_types.h:    HIDE_FROM_OPENCL_COMPILER(DeviceBuffer<uint64_t>) forcesReadyNvshmemFlags;
src/gromacs/ewald/pme_gpu_types.h:    HIDE_FROM_OPENCL_COMPILER(uint64_t) forcesReadyNvshmemFlagsCounter;
src/gromacs/ewald/pme_gpu_types.h:    /*! \brief whether to use NVSHMEM for GPU comm*/
src/gromacs/ewald/pme_gpu_types.h:    /* These texture objects are only used in CUDA and are related to the grid size. */
src/gromacs/ewald/pme_gpu_types.h:    HIDE_FROM_OPENCL_COMPILER(DeviceTexture) fractShiftsTableTexture;
src/gromacs/ewald/pme_gpu_types.h:    HIDE_FROM_OPENCL_COMPILER(DeviceTexture) gridlineIndicesTableTexture;
src/gromacs/ewald/pme_gather_sycl.h: *  \brief Implements PME GPU gather in SYCL.
src/gromacs/ewald/pme_gather_sycl.h:#include "gromacs/gpu_utils/syclutils.h"
src/gromacs/ewald/pme_gather_sycl.h:#include "pme_gpu_types_host.h"
src/gromacs/ewald/pme_gather_sycl.h:struct PmeGpuGridParams;
src/gromacs/ewald/pme_gather_sycl.h:struct PmeGpuAtomParams;
src/gromacs/ewald/pme_gather_sycl.h:struct PmeGpuDynamicParams;
src/gromacs/ewald/pme_gather_sycl.h:    PmeGpuGridParams*    gridParams_;
src/gromacs/ewald/pme_gather_sycl.h:    PmeGpuAtomParams*    atomParams_;
src/gromacs/ewald/pme_gather_sycl.h:    PmeGpuDynamicParams* dynamicParams_;
src/gromacs/ewald/pme_pp_communication.h:    eCommType_COORD_GPU,
src/gromacs/ewald/pme_pp_communication.h:#define PP_PME_GPUCOMMS (1 << 13)
src/gromacs/ewald/pme_pp_communication.h:// Whether PME forces are transferred directly to remote PP GPU memory in a specific step
src/gromacs/ewald/pme_pp_communication.h:#define PP_PME_RECVFTOGPU (1 << 14)
src/gromacs/ewald/pme_pp_communication.h:// Whether a GPU graph should be used to execute steps in the MD loop if run conditions allow
src/gromacs/ewald/pme_pp_communication.h:#define PP_PME_MDGPUGRAPH (1 << 15)
src/gromacs/ewald/pme_load_balancing.cpp:#include "gromacs/nbnxm/gpu_data_mgmt.h"
src/gromacs/ewald/pme_load_balancing.cpp:                      gmx_bool                       bUseGPU)
src/gromacs/ewald/pme_load_balancing.cpp:    /* Tune with GPUs and/or separate PME ranks.
src/gromacs/ewald/pme_load_balancing.cpp:    pme_lb->bActive = (wallcycle_have_counter() && (bUseGPU || pme_lb->bSepPMERanks));
src/gromacs/ewald/pme_load_balancing.cpp:    /* With GPUs and no separate PME ranks we can't measure the PP/PME
src/gromacs/ewald/pme_load_balancing.cpp:    pme_lb->bBalance = (pme_lb->bActive && (bUseGPU && !pme_lb->bSepPMERanks));
src/gromacs/ewald/pme_load_balancing.cpp:    /* Delay DD load balancing when GPUs are used */
src/gromacs/ewald/pme_load_balancing.cpp:    if (pme_lb->bActive && haveDDAtomOrdering(*cr) && cr->dd->nnodes > 1 && bUseGPU)
src/gromacs/ewald/pme_load_balancing.cpp:         * With GPUs and separate PME nodes, we want to first
src/gromacs/ewald/pme_load_balancing.cpp: * a GPU (PP on GPU, PME on CPU), PP and PME run on different resources
src/gromacs/ewald/pme_load_balancing.cpp:                    const bool checkGpuDdLimitation = true;
src/gromacs/ewald/pme_load_balancing.cpp:                            cr, box, x, pme_lb->setup[pme_lb->cur].rlistOuter, checkGpuDdLimitation);
src/gromacs/ewald/pme_load_balancing.cpp:        const bool checkGpuDdLimitation = true;
src/gromacs/ewald/pme_load_balancing.cpp:        OK = change_dd_cutoff(cr, box, x, pme_lb->setup[pme_lb->cur].rlistOuter, checkGpuDdLimitation);
src/gromacs/ewald/pme_load_balancing.cpp:    gmx::gpu_pme_loadbal_update_param(nbv, *ic);
src/gromacs/ewald/pme_load_balancing.cpp:         * GPU PME, however, currently needs the gmx_pme_reinit always called on load balancing
src/gromacs/ewald/pme_load_balancing.cpp:         * (pme_gpu_reinit might be not sufficiently decoupled from gmx_pme_init).
src/gromacs/ewald/pme_load_balancing.cpp:         * This can lead to a lot of reallocations for PME GPU.
src/gromacs/ewald/pme_load_balancing.cpp:            || pme_gpu_task_enabled(pme_lb->setup[pme_lb->cur].pmedata))
src/gromacs/ewald/pme_load_balancing.cpp: * If the conditions (e.g. DLB off/on, CPU/GPU throttling etc.) changed,
src/gromacs/ewald/pme_load_balancing.cpp:                    bool                           useGpuPmePpCommunication)
src/gromacs/ewald/pme_load_balancing.cpp:     * the CPU and GPU, when used, performance stabilizes.
src/gromacs/ewald/pme_load_balancing.cpp:    /* PME grid + cut-off optimization with GPUs or PME ranks */
src/gromacs/ewald/pme_load_balancing.cpp:                   PME-PP direct GPU communication is active,
src/gromacs/ewald/pme_load_balancing.cpp:                   unreliable due to CPU-GPU asynchronicity in codepath */
src/gromacs/ewald/pme_load_balancing.cpp:                pme_lb->bBalance = useGpuPmePpCommunication
src/gromacs/ewald/pme_load_balancing.cpp:                                       gmx_bool              bNonBondedOnGPU)
src/gromacs/ewald/pme_load_balancing.cpp:    if (pp_ratio > 1.5 && !bNonBondedOnGPU)
src/gromacs/ewald/pme_load_balancing.cpp:void pme_loadbal_done(pme_load_balancing_t* pme_lb, FILE* fplog, const gmx::MDLogger& mdlog, gmx_bool bNonBondedOnGPU)
src/gromacs/ewald/pme_load_balancing.cpp:        print_pme_loadbal_settings(pme_lb, fplog, mdlog, bNonBondedOnGPU);
src/gromacs/ewald/pme_gather.clh: *  \brief Implements PME OpenCL force gathering kernel.
src/gromacs/ewald/pme_gather.clh: * When including this and other PME OpenCL kernel files, plenty of common
src/gromacs/ewald/pme_gather.clh: * For details, please see how pme_program.cl is compiled in pme_gpu_program_impl_ocl.cpp.
src/gromacs/ewald/pme_gather.clh:#include "pme_gpu_calculate_splines.clh"
src/gromacs/ewald/pme_gather.clh:#include "pme_gpu_types.h"
src/gromacs/ewald/pme_gather.clh:    // TODO: implement AMD intrinsics reduction, like with shuffles in CUDA version. #2514
src/gromacs/ewald/pme_gather.clh: * An OpenCL kernel which gathers the atom forces from the grid.
src/gromacs/ewald/pme_gather.clh: * \param[in]     kernelParams         All the PME GPU data.
src/gromacs/ewald/pme_gather.clh:CUSTOMIZED_KERNEL_NAME(pme_gather_kernel)(const struct PmeOpenCLKernelParams kernelParams,
src/gromacs/ewald/pme_gather.clh:    int chargeCheck = pme_gpu_check_atom_charge(gm_coefficientsA[atomIndexGlobal]);
src/gromacs/ewald/pme_gather.clh:#if !defined(_AMD_SOURCE_) && !defined(_NVIDIA_SOURCE_) && !defined(_APPLE_SOURCE_)
src/gromacs/ewald/pme_gather.clh:     * __syncwarp() in CUDA. #2519
src/gromacs/ewald/pme_gather.clh:        chargeCheck = pme_gpu_check_atom_charge(gm_coefficientsB[atomIndexGlobal]);
src/gromacs/ewald/pme_gather.clh:#if !defined(_AMD_SOURCE_) && !defined(_NVIDIA_SOURCE_)
src/gromacs/ewald/pme_gather.clh:         * __syncwarp() in CUDA. #2519
src/gromacs/ewald/pme_force_sender_gpu.h: * \brief Declaration of class which sends PME Force from GPU memory to PP task
src/gromacs/ewald/pme_force_sender_gpu.h: * \author Alan Gray <alang@nvidia.com>
src/gromacs/ewald/pme_force_sender_gpu.h:#ifndef GMX_PMEFORCESENDERGPU_H
src/gromacs/ewald/pme_force_sender_gpu.h:#define GMX_PMEFORCESENDERGPU_H
src/gromacs/ewald/pme_force_sender_gpu.h:#include "gromacs/gpu_utils/devicebuffer_datatype.h"
src/gromacs/ewald/pme_force_sender_gpu.h:class GpuEventSynchronizer;
src/gromacs/ewald/pme_force_sender_gpu.h:class PmeForceSenderGpu
src/gromacs/ewald/pme_force_sender_gpu.h:    /*! \brief Creates PME GPU Force sender object
src/gromacs/ewald/pme_force_sender_gpu.h:     * \param[in] pmeForcesReady  Event synchronizer marked when PME forces are ready on the GPU
src/gromacs/ewald/pme_force_sender_gpu.h:     * \param[in] deviceContext   GPU context
src/gromacs/ewald/pme_force_sender_gpu.h:    PmeForceSenderGpu(GpuEventSynchronizer*  pmeForcesReady,
src/gromacs/ewald/pme_force_sender_gpu.h:    ~PmeForceSenderGpu();
src/gromacs/ewald/pme_force_sender_gpu.h:     * \param[in] d_f   force buffer in GPU memory
src/gromacs/ewald/pme_force_sender_gpu.h:     * \param[in] sendForcesDirectToPpGpu  whether forces are transferred direct to remote GPU memory
src/gromacs/ewald/pme_force_sender_gpu.h:    void sendFToPpPeerToPeer(int ppRank, int numAtoms, bool sendForcesDirectToPpGpu);
src/gromacs/ewald/pme_force_sender_gpu.h:     * \param[in] sendbuf  force buffer in GPU memory
src/gromacs/ewald/pme_force_sender_gpu.h:    void sendFToPpGpuAwareMpi(DeviceBuffer<RVec> sendbuf, int offset, int numBytes, int ppRank, MPI_Request* request);
src/gromacs/ewald/pme_pp.h:class GpuEventSynchronizer;
src/gromacs/ewald/pme_pp.h:class PmePpCommGpu;
src/gromacs/ewald/pme_pp.h:                              bool                           useGpuPmePpComms,
src/gromacs/ewald/pme_pp.h:                              bool                           reinitGpuPmePpComms,
src/gromacs/ewald/pme_pp.h:                              bool                           sendCoordinatesFromGpu,
src/gromacs/ewald/pme_pp.h:                              bool                           receiveForcesToGpu,
src/gromacs/ewald/pme_pp.h:                              GpuEventSynchronizer*          coordinatesReadyOnDeviceEvent,
src/gromacs/ewald/pme_pp.h:                              bool                           useMdGpuGraph,
src/gromacs/ewald/pme_pp.h:void gmx_pme_receive_f(gmx::PmePpCommGpu*    pmePpCommGpu,
src/gromacs/ewald/pme_pp.h:                       bool                  useGpuPmePpComms,
src/gromacs/ewald/pme_pp.h:                       bool                  receivePmeForceToGpu,
src/gromacs/ewald/pme_gpu_types_host_impl.h: * \brief Defines the host-side PME GPU data structure, which is dependent on the GPU types.
src/gromacs/ewald/pme_gpu_types_host_impl.h: * It's included by pointer in the general PmeGpu host structure in pme_gpu_types_host.h.
src/gromacs/ewald/pme_gpu_types_host_impl.h:#ifndef PMEGPUTYPESHOSTIMPL_H
src/gromacs/ewald/pme_gpu_types_host_impl.h:#define PMEGPUTYPESHOSTIMPL_H
src/gromacs/ewald/pme_gpu_types_host_impl.h:#if GMX_GPU_CUDA
src/gromacs/ewald/pme_gpu_types_host_impl.h:#    include "gromacs/gpu_utils/gpuregiontimer.cuh"
src/gromacs/ewald/pme_gpu_types_host_impl.h:#elif GMX_GPU_OPENCL
src/gromacs/ewald/pme_gpu_types_host_impl.h:#    include "gromacs/gpu_utils/gpuregiontimer_ocl.h"
src/gromacs/ewald/pme_gpu_types_host_impl.h:#elif GMX_GPU_SYCL
src/gromacs/ewald/pme_gpu_types_host_impl.h:#    include "gromacs/gpu_utils/gpuregiontimer_sycl.h"
src/gromacs/ewald/pme_gpu_types_host_impl.h:#include "gromacs/fft/gpu_3dfft.h"
src/gromacs/ewald/pme_gpu_types_host_impl.h:#include "gromacs/gpu_utils/gpueventsynchronizer.h"
src/gromacs/ewald/pme_gpu_types_host_impl.h:#include "gromacs/timing/gpu_timing.h" // for gtPME_EVENT_COUNT
src/gromacs/ewald/pme_gpu_types_host_impl.h:class Gpu3dFft;
src/gromacs/ewald/pme_gpu_types_host_impl.h: * Used in GPU implementation of PME halo exchange
src/gromacs/ewald/pme_gpu_types_host_impl.h: * Used in GPU implementation of PME halo exchange
src/gromacs/ewald/pme_gpu_types_host_impl.h: * The main PME CUDA/OpenCL-specific host data structure, included in the PME GPU structure by the archSpecific pointer.
src/gromacs/ewald/pme_gpu_types_host_impl.h:struct PmeGpuSpecific
src/gromacs/ewald/pme_gpu_types_host_impl.h:     * \param[in] deviceContext  GPU device context
src/gromacs/ewald/pme_gpu_types_host_impl.h:     * \param[in] pmeStream      GPU pme stream.
src/gromacs/ewald/pme_gpu_types_host_impl.h:    PmeGpuSpecific(const DeviceContext& deviceContext, const DeviceStream& pmeStream) :
src/gromacs/ewald/pme_gpu_types_host_impl.h:    /*! \brief The GPU stream where everything related to the PME happens. */
src/gromacs/ewald/pme_gpu_types_host_impl.h:    GpuEventSynchronizer pmeForcesReady;
src/gromacs/ewald/pme_gpu_types_host_impl.h:    GpuEventSynchronizer syncSpreadGridD2H;
src/gromacs/ewald/pme_gpu_types_host_impl.h:     * Required only in case of GPU PME pipelining, when we launch Spread kernels in
src/gromacs/ewald/pme_gpu_types_host_impl.h:    GpuEventSynchronizer pmeGridsReadyForSpread;
src/gromacs/ewald/pme_gpu_types_host_impl.h:    /*! \brief A boolean which tells if the GPU timing events are enabled.
src/gromacs/ewald/pme_gpu_types_host_impl.h:     *  False by default, can be enabled by setting the environment variable GMX_ENABLE_GPU_TIMING.
src/gromacs/ewald/pme_gpu_types_host_impl.h:     *  Note: will not be reliable when multiple GPU tasks are running concurrently on the same
src/gromacs/ewald/pme_gpu_types_host_impl.h:     * device context, as CUDA events on multiple streams are untrustworthy.
src/gromacs/ewald/pme_gpu_types_host_impl.h:    std::vector<std::unique_ptr<gmx::Gpu3dFft>> fftSetup;
src/gromacs/ewald/pme_gpu_types_host_impl.h:    gmx::EnumerationArray<PmeStage, GpuRegionTimer> timingEvents;
src/gromacs/ewald/pme_gpu_types_host_impl.h:    /* GPU arrays element counts (not the arrays sizes in bytes!).
src/gromacs/ewald/pme_gpu_types_host_impl.h:     * TODO: these should live in a clean buffered container type, and be refactored in the NB/cudautils as well.
src/gromacs/ewald/pme_gpu_types_host_impl.h:struct PmeGpuHaloExchange
src/gromacs/ewald/pme_load_balancing.h:                      gmx_bool                       bUseGPU);
src/gromacs/ewald/pme_load_balancing.h:                    bool                           useGpuPmePpCommunication);
src/gromacs/ewald/pme_load_balancing.h:void pme_loadbal_done(pme_load_balancing_t* pme_lb, FILE* fplog, const gmx::MDLogger& mdlog, gmx_bool bNonBondedOnGPU);
src/gromacs/ewald/pme_pp_comm_gpu_impl.cpp: * \brief May be used to implement PME-PP GPU comm interfaces for non-GPU builds.
src/gromacs/ewald/pme_pp_comm_gpu_impl.cpp: * Needed to satisfy compiler on systems, where CUDA is not available.
src/gromacs/ewald/pme_pp_comm_gpu_impl.cpp: * \author Alan Gray <alang@nvidia.com>
src/gromacs/ewald/pme_pp_comm_gpu_impl.cpp:#include "gromacs/ewald/pme_pp_comm_gpu.h"
src/gromacs/ewald/pme_pp_comm_gpu_impl.cpp:#include "gromacs/gpu_utils/devicebuffer_datatype.h"
src/gromacs/ewald/pme_pp_comm_gpu_impl.cpp:#include "gromacs/gpu_utils/hostallocator.h"
src/gromacs/ewald/pme_pp_comm_gpu_impl.cpp:class GpuEventSynchronizer;
src/gromacs/ewald/pme_pp_comm_gpu_impl.cpp:#if !GMX_GPU_CUDA && !GMX_GPU_SYCL
src/gromacs/ewald/pme_pp_comm_gpu_impl.cpp:class PmePpCommGpu::Impl
src/gromacs/ewald/pme_pp_comm_gpu_impl.cpp:PmePpCommGpu::PmePpCommGpu(MPI_Comm /* comm */,
src/gromacs/ewald/pme_pp_comm_gpu_impl.cpp:               "A CPU stub for PME-PP GPU communication was called instead of the correct "
src/gromacs/ewald/pme_pp_comm_gpu_impl.cpp:PmePpCommGpu::~PmePpCommGpu() = default;
src/gromacs/ewald/pme_pp_comm_gpu_impl.cpp:/*!\brief init PME-PP GPU communication stub */
src/gromacs/ewald/pme_pp_comm_gpu_impl.cpp:void PmePpCommGpu::reinit(int /* size */)
src/gromacs/ewald/pme_pp_comm_gpu_impl.cpp:               "A CPU stub for PME-PP GPU communication initialization was called instead of the "
src/gromacs/ewald/pme_pp_comm_gpu_impl.cpp:void PmePpCommGpu::receiveForceFromPme(RVec* /* recvPtr */, int /* recvSize */, bool /* receivePmeForceToGpu */)
src/gromacs/ewald/pme_pp_comm_gpu_impl.cpp:               "A CPU stub for PME-PP GPU communication was called instead of the correct "
src/gromacs/ewald/pme_pp_comm_gpu_impl.cpp:void PmePpCommGpu::sendCoordinatesToPmeFromGpu(DeviceBuffer<RVec> /* sendPtr */,
src/gromacs/ewald/pme_pp_comm_gpu_impl.cpp:                                               GpuEventSynchronizer* /* coordinatesOnDeviceEvent */)
src/gromacs/ewald/pme_pp_comm_gpu_impl.cpp:               "A CPU stub for PME-PP GPU communication was called instead of the correct "
src/gromacs/ewald/pme_pp_comm_gpu_impl.cpp:void PmePpCommGpu::sendCoordinatesToPmeFromCpu(RVec* /* sendPtr */, int /* sendSize */)
src/gromacs/ewald/pme_pp_comm_gpu_impl.cpp:               "A CPU stub for PME-PP GPU communication was called instead of the correct "
src/gromacs/ewald/pme_pp_comm_gpu_impl.cpp:DeviceBuffer<gmx::RVec> PmePpCommGpu::getGpuForceStagingPtr()
src/gromacs/ewald/pme_pp_comm_gpu_impl.cpp:               "A CPU stub for PME-PP GPU communication was called instead of the correct "
src/gromacs/ewald/pme_pp_comm_gpu_impl.cpp:GpuEventSynchronizer* PmePpCommGpu::getForcesReadySynchronizer()
src/gromacs/ewald/pme_pp_comm_gpu_impl.cpp:               "A CPU stub for PME-PP GPU communication was called instead of the correct "
src/gromacs/ewald/pme_pp_comm_gpu_impl.cpp:DeviceBuffer<uint64_t> PmePpCommGpu::getGpuForcesSyncObj()
src/gromacs/ewald/pme_pp_comm_gpu_impl.cpp:               "A CPU stub for PME-PP GPU communication was called instead of the correct "
src/gromacs/ewald/pme_pp_comm_gpu_impl.cpp:#endif // !GMX_GPU_CUDA && !GMX_GPU_SYCL
src/gromacs/ewald/pme_gpu_timings.cpp: *  \brief Implements PME GPU timing events wrappers.
src/gromacs/ewald/pme_gpu_timings.cpp:#include "pme_gpu_timings.h"
src/gromacs/ewald/pme_gpu_timings.cpp:#include "pme_gpu_internal.h"
src/gromacs/ewald/pme_gpu_timings.cpp:#include "pme_gpu_types_host.h"
src/gromacs/ewald/pme_gpu_timings.cpp:#include "pme_gpu_types_host_impl.h"
src/gromacs/ewald/pme_gpu_timings.cpp:bool pme_gpu_timings_enabled(const PmeGpu* pmeGpu)
src/gromacs/ewald/pme_gpu_timings.cpp:    return pmeGpu->archSpecific->useTiming;
src/gromacs/ewald/pme_gpu_timings.cpp:void pme_gpu_start_timing(const PmeGpu* pmeGpu, PmeStage pmeStageId)
src/gromacs/ewald/pme_gpu_timings.cpp:    if (pme_gpu_timings_enabled(pmeGpu))
src/gromacs/ewald/pme_gpu_timings.cpp:        GMX_ASSERT(pmeStageId < PmeStage::Count, "Wrong PME GPU timing event index");
src/gromacs/ewald/pme_gpu_timings.cpp:        pmeGpu->archSpecific->timingEvents[pmeStageId].openTimingRegion(pmeGpu->archSpecific->pmeStream_);
src/gromacs/ewald/pme_gpu_timings.cpp:void pme_gpu_stop_timing(const PmeGpu* pmeGpu, PmeStage pmeStageId)
src/gromacs/ewald/pme_gpu_timings.cpp:    if (pme_gpu_timings_enabled(pmeGpu))
src/gromacs/ewald/pme_gpu_timings.cpp:        GMX_ASSERT(pmeStageId < PmeStage::Count, "Wrong PME GPU timing event index");
src/gromacs/ewald/pme_gpu_timings.cpp:        pmeGpu->archSpecific->timingEvents[pmeStageId].closeTimingRegion(pmeGpu->archSpecific->pmeStream_);
src/gromacs/ewald/pme_gpu_timings.cpp:void pme_gpu_get_timings(const PmeGpu* pmeGpu, gmx_wallclock_gpu_pme_t* timings)
src/gromacs/ewald/pme_gpu_timings.cpp:    if (pme_gpu_timings_enabled(pmeGpu))
src/gromacs/ewald/pme_gpu_timings.cpp:        GMX_RELEASE_ASSERT(timings, "Null GPU timing pointer");
src/gromacs/ewald/pme_gpu_timings.cpp:            timings->timing[key].t = pmeGpu->archSpecific->timingEvents[key].getTotalTime();
src/gromacs/ewald/pme_gpu_timings.cpp:            timings->timing[key].c = pmeGpu->archSpecific->timingEvents[key].getCallCount();
src/gromacs/ewald/pme_gpu_timings.cpp:void pme_gpu_update_timings(const PmeGpu* pmeGpu)
src/gromacs/ewald/pme_gpu_timings.cpp:    if (pme_gpu_timings_enabled(pmeGpu))
src/gromacs/ewald/pme_gpu_timings.cpp:        for (const auto& activeTimer : pmeGpu->archSpecific->activeTimers)
src/gromacs/ewald/pme_gpu_timings.cpp:            pmeGpu->archSpecific->timingEvents[activeTimer].getLastRangeTime();
src/gromacs/ewald/pme_gpu_timings.cpp:void pme_gpu_reinit_timings(const PmeGpu* pmeGpu)
src/gromacs/ewald/pme_gpu_timings.cpp:    if (pme_gpu_timings_enabled(pmeGpu))
src/gromacs/ewald/pme_gpu_timings.cpp:        pmeGpu->archSpecific->activeTimers.clear();
src/gromacs/ewald/pme_gpu_timings.cpp:        pmeGpu->archSpecific->activeTimers.insert(PmeStage::SplineAndSpread);
src/gromacs/ewald/pme_gpu_timings.cpp:        const auto& settings = pme_gpu_settings(pmeGpu);
src/gromacs/ewald/pme_gpu_timings.cpp:        if (settings.performGPUFFT)
src/gromacs/ewald/pme_gpu_timings.cpp:            pmeGpu->archSpecific->activeTimers.insert(PmeStage::FftTransformC2R);
src/gromacs/ewald/pme_gpu_timings.cpp:            pmeGpu->archSpecific->activeTimers.insert(PmeStage::FftTransformR2C);
src/gromacs/ewald/pme_gpu_timings.cpp:        if (settings.performGPUSolve)
src/gromacs/ewald/pme_gpu_timings.cpp:            pmeGpu->archSpecific->activeTimers.insert(PmeStage::Solve);
src/gromacs/ewald/pme_gpu_timings.cpp:        if (settings.performGPUGather)
src/gromacs/ewald/pme_gpu_timings.cpp:            pmeGpu->archSpecific->activeTimers.insert(PmeStage::Gather);
src/gromacs/ewald/pme_gpu_timings.cpp:void pme_gpu_reset_timings(const PmeGpu* pmeGpu)
src/gromacs/ewald/pme_gpu_timings.cpp:    if (pme_gpu_timings_enabled(pmeGpu))
src/gromacs/ewald/pme_gpu_timings.cpp:        for (auto key : keysOf(pmeGpu->archSpecific->timingEvents))
src/gromacs/ewald/pme_gpu_timings.cpp:            pmeGpu->archSpecific->timingEvents[key].reset();
src/gromacs/ewald/pme_force_sender_gpu_impl_gpu.cpp: * \brief Implements backend-agnostic part of GPU-direct PME-PP communication.
src/gromacs/ewald/pme_force_sender_gpu_impl_gpu.cpp: * \author Alan Gray <alang@nvidia.com>
src/gromacs/ewald/pme_force_sender_gpu_impl_gpu.cpp:#include "gromacs/gpu_utils/devicebuffer.h"
src/gromacs/ewald/pme_force_sender_gpu_impl_gpu.cpp:#include "gromacs/gpu_utils/gpueventsynchronizer.h"
src/gromacs/ewald/pme_force_sender_gpu_impl_gpu.cpp:#include "pme_force_sender_gpu_impl.h"
src/gromacs/ewald/pme_force_sender_gpu_impl_gpu.cpp:/*! \brief Create PME-PP GPU communication object */
src/gromacs/ewald/pme_force_sender_gpu_impl_gpu.cpp:PmeForceSenderGpu::Impl::Impl(GpuEventSynchronizer*  pmeForcesReady,
src/gromacs/ewald/pme_force_sender_gpu_impl_gpu.cpp:                std::make_unique<GpuEventSynchronizer>(),
src/gromacs/ewald/pme_force_sender_gpu_impl_gpu.cpp:    stageThreadMpiGpuCpuComm_ = (getenv("GMX_ENABLE_STAGED_GPU_TO_CPU_PMEPP_COMM") != nullptr);
src/gromacs/ewald/pme_force_sender_gpu_impl_gpu.cpp:PmeForceSenderGpu::Impl::~Impl() = default;
src/gromacs/ewald/pme_force_sender_gpu_impl_gpu.cpp:void PmeForceSenderGpu::Impl::setForceSendBuffer(DeviceBuffer<Float3> d_f)
src/gromacs/ewald/pme_force_sender_gpu_impl_gpu.cpp:    // data using cudamemcpy
src/gromacs/ewald/pme_force_sender_gpu_impl_gpu.cpp:    GMX_ASSERT(!GMX_GPU_SYCL,
src/gromacs/ewald/pme_force_sender_gpu_impl_gpu.cpp:               "PmeForceSenderGpu does not support SYCL with threadMPI; use libMPI instead.");
src/gromacs/ewald/pme_force_sender_gpu_impl_gpu.cpp:#if GMX_MPI && GMX_GPU_CUDA
src/gromacs/ewald/pme_force_sender_gpu_impl_gpu.cpp:            MPI_Recv(&ppCommManagers_[i].pmeRemoteGpuForcePtr,
src/gromacs/ewald/pme_force_sender_gpu_impl_gpu.cpp:            MPI_Send(&ppCommManagers_[i].event, sizeof(GpuEventSynchronizer*), MPI_BYTE, receiver.rankId, 0, comm_);
src/gromacs/ewald/pme_force_sender_gpu_impl_gpu.cpp:/*! \brief Send PME data directly using GPU-aware MPI */
src/gromacs/ewald/pme_force_sender_gpu_impl_gpu.cpp:void PmeForceSenderGpu::Impl::sendFToPpGpuAwareMpi(DeviceBuffer<RVec> sendbuf,
src/gromacs/ewald/pme_force_sender_gpu_impl_gpu.cpp:    GMX_ASSERT(GMX_LIB_MPI, "sendFToPpCudaMpi is expected to be called only for Lib-MPI");
src/gromacs/ewald/pme_force_sender_gpu_impl_gpu.cpp:    // if using GPU direct comm with GPU-aware MPI, make sure forces are ready on device
src/gromacs/ewald/pme_force_sender_gpu_impl_gpu.cpp:void PmeForceSenderGpu::Impl::waitForEvents()
src/gromacs/ewald/pme_force_sender_gpu_impl_gpu.cpp:PmeForceSenderGpu::PmeForceSenderGpu(GpuEventSynchronizer*  pmeForcesReady,
src/gromacs/ewald/pme_force_sender_gpu_impl_gpu.cpp:PmeForceSenderGpu::~PmeForceSenderGpu() = default;
src/gromacs/ewald/pme_force_sender_gpu_impl_gpu.cpp:void PmeForceSenderGpu::setForceSendBuffer(DeviceBuffer<RVec> d_f)
src/gromacs/ewald/pme_force_sender_gpu_impl_gpu.cpp:void PmeForceSenderGpu::sendFToPpGpuAwareMpi(DeviceBuffer<RVec> sendbuf,
src/gromacs/ewald/pme_force_sender_gpu_impl_gpu.cpp:    impl_->sendFToPpGpuAwareMpi(sendbuf, offset, numBytes, ppRank, request);
src/gromacs/ewald/pme_force_sender_gpu_impl_gpu.cpp:void PmeForceSenderGpu::sendFToPpPeerToPeer(int ppRank, int numAtoms, bool sendForcesDirectToPpGpu)
src/gromacs/ewald/pme_force_sender_gpu_impl_gpu.cpp:    impl_->sendFToPpPeerToPeer(ppRank, numAtoms, sendForcesDirectToPpGpu);
src/gromacs/ewald/pme_force_sender_gpu_impl_gpu.cpp:void PmeForceSenderGpu::waitForEvents()
src/gromacs/ewald/pme_pp_comm_gpu_impl.h: * \brief Declares GPU implementation class for PME-PP communications
src/gromacs/ewald/pme_pp_comm_gpu_impl.h: * \author Alan Gray <alang@nvidia.com>
src/gromacs/ewald/pme_pp_comm_gpu_impl.h:#ifndef GMX_PME_PP_COMM_GPU_IMPL_H
src/gromacs/ewald/pme_pp_comm_gpu_impl.h:#define GMX_PME_PP_COMM_GPU_IMPL_H
src/gromacs/ewald/pme_pp_comm_gpu_impl.h:#include "gromacs/ewald/pme_pp_comm_gpu.h"
src/gromacs/ewald/pme_pp_comm_gpu_impl.h:#include "gromacs/gpu_utils/gpueventsynchronizer.h"
src/gromacs/ewald/pme_pp_comm_gpu_impl.h:#include "gromacs/gpu_utils/gputraits.h"
src/gromacs/ewald/pme_pp_comm_gpu_impl.h:/*! \internal \brief Class with interfaces and data for GPU version of PME-PP Communication */
src/gromacs/ewald/pme_pp_comm_gpu_impl.h:class PmePpCommGpu::Impl
src/gromacs/ewald/pme_pp_comm_gpu_impl.h:    /*! \brief Creates PME-PP GPU communication object.
src/gromacs/ewald/pme_pp_comm_gpu_impl.h:     * \param[in] deviceContext     GPU context.
src/gromacs/ewald/pme_pp_comm_gpu_impl.h:     * \param[in] useNvshmem        NVSHMEM enable/disable for GPU comm.
src/gromacs/ewald/pme_pp_comm_gpu_impl.h:    /*! \brief Pull force buffer directly from GPU memory on PME
src/gromacs/ewald/pme_pp_comm_gpu_impl.h:     * rank to either GPU or CPU memory on PP task using CUDA
src/gromacs/ewald/pme_pp_comm_gpu_impl.h:     * Memory copy or GPU-aware MPI.
src/gromacs/ewald/pme_pp_comm_gpu_impl.h:     * recvPtr should be in GPU or CPU memory if recvPmeForceToGpu
src/gromacs/ewald/pme_pp_comm_gpu_impl.h:     * is true or false, respectively. If receiving to GPU, this
src/gromacs/ewald/pme_pp_comm_gpu_impl.h:     * method should be called before the local GPU buffer
src/gromacs/ewald/pme_pp_comm_gpu_impl.h:     * \param[out] recvPtr CPU or GPU buffer to receive PME force data
src/gromacs/ewald/pme_pp_comm_gpu_impl.h:     * \param[in] receivePmeForceToGpu Whether receive is to GPU, otherwise CPU
src/gromacs/ewald/pme_pp_comm_gpu_impl.h:    void receiveForceFromPme(Float3* recvPtr, int recvSize, bool receivePmeForceToGpu);
src/gromacs/ewald/pme_pp_comm_gpu_impl.h:    /*! \brief Push coordinates buffer directly to GPU memory on PME
src/gromacs/ewald/pme_pp_comm_gpu_impl.h:     * task, from either GPU or CPU memory on PP task using CUDA
src/gromacs/ewald/pme_pp_comm_gpu_impl.h:     * Memory copy or GPU-aware MPI. If sending from GPU, this method should
src/gromacs/ewald/pme_pp_comm_gpu_impl.h:     * be called after the local GPU coordinate buffer operations.
src/gromacs/ewald/pme_pp_comm_gpu_impl.h:    void sendCoordinatesToPme(Float3* sendPtr, int sendSize, GpuEventSynchronizer* coordinatesReadyOnDeviceEvent);
src/gromacs/ewald/pme_pp_comm_gpu_impl.h:     * Return pointer to buffer used for staging PME force on GPU
src/gromacs/ewald/pme_pp_comm_gpu_impl.h:    DeviceBuffer<Float3> getGpuForceStagingPtr();
src/gromacs/ewald/pme_pp_comm_gpu_impl.h:    GpuEventSynchronizer* getForcesReadySynchronizer();
src/gromacs/ewald/pme_pp_comm_gpu_impl.h:     * Return pointer to NVSHMEM sync object used for staging PME force on GPU
src/gromacs/ewald/pme_pp_comm_gpu_impl.h:    DeviceBuffer<uint64_t> getGpuForcesSyncObj();
src/gromacs/ewald/pme_pp_comm_gpu_impl.h:    /*! \brief Receive buffer from GPU memory on PME rank to either
src/gromacs/ewald/pme_pp_comm_gpu_impl.h:     * GPU or CPU memory on PP rank. Data is pushed from PME force
src/gromacs/ewald/pme_pp_comm_gpu_impl.h:     * sender object using CUDA memory copy functionality, and this
src/gromacs/ewald/pme_pp_comm_gpu_impl.h:     * \param[in] receivePmeForceToGpu Whether receive is to GPU, otherwise CPU
src/gromacs/ewald/pme_pp_comm_gpu_impl.h:    void receiveForceFromPmePeerToPeer(bool receivePmeForceToGpu);
src/gromacs/ewald/pme_pp_comm_gpu_impl.h:    /*! \brief Receive buffer from GPU memory on PME rank to either
src/gromacs/ewald/pme_pp_comm_gpu_impl.h:     * GPU or CPU memory on PP rank using GPU-aware MPI. This method
src/gromacs/ewald/pme_pp_comm_gpu_impl.h:     * \param[out] recvPtr CPU or GPU buffer to receive PME force data into
src/gromacs/ewald/pme_pp_comm_gpu_impl.h:    void receiveForceFromPmeGpuAwareMpi(Float3* recvPtr, int recvSize);
src/gromacs/ewald/pme_pp_comm_gpu_impl.h:    /*! \brief Push coordinates buffer directly to GPU memory on PME
src/gromacs/ewald/pme_pp_comm_gpu_impl.h:     * task, from either GPU or CPU memory on PP task using CUDA Memory copy.
src/gromacs/ewald/pme_pp_comm_gpu_impl.h:                                        GpuEventSynchronizer* coordinatesReadyOnDeviceEvent);
src/gromacs/ewald/pme_pp_comm_gpu_impl.h:    /*! \brief Push coordinates buffer directly to GPU memory on PME
src/gromacs/ewald/pme_pp_comm_gpu_impl.h:     * task, from either GPU or CPU memory on PP task using GPU-aware MPI.
src/gromacs/ewald/pme_pp_comm_gpu_impl.h:    void sendCoordinatesToPmeGpuAwareMpi(Float3*               sendPtr,
src/gromacs/ewald/pme_pp_comm_gpu_impl.h:                                         GpuEventSynchronizer* coordinatesReadyOnDeviceEvent);
src/gromacs/ewald/pme_pp_comm_gpu_impl.h:    //! Buffer for staging PME force on GPU
src/gromacs/ewald/pme_pp_comm_gpu_impl.h:    GpuEventSynchronizer forcesReadySynchronizer_;
src/gromacs/ewald/pme_pp_comm_gpu_impl.h:    GpuEventSynchronizer pmeCoordinatesSynchronizer_;
src/gromacs/ewald/pme_pp_comm_gpu_impl.h:    GpuEventSynchronizer* remotePmeForceSendEvent_;
src/gromacs/ewald/pme_pp_comm_gpu_impl.h:    //! Whether GPU to CPU communication should staged through GPU
src/gromacs/ewald/pme_pp_comm_gpu_impl.h:    //! with direct links between GPUs, because it allows the device
src/gromacs/ewald/pme_pp_comm_gpu_impl.h:    bool stageLibMpiGpuCpuComm_ = true;
src/gromacs/ewald/pme_pp_comm_gpu_impl.h:    // Flag on whether to use NVSHMEM for GPU communication
src/gromacs/ewald/pme_gpu_types_host.h: * \brief Defines the host-side PME GPU data structures.
src/gromacs/ewald/pme_gpu_types_host.h: * -- PmeGpuSettings -> PmeGpuTasks
src/gromacs/ewald/pme_gpu_types_host.h: * -- refining GPU notation application (#2053)
src/gromacs/ewald/pme_gpu_types_host.h:#ifndef GMX_EWALD_PME_GPU_TYPES_HOST_H
src/gromacs/ewald/pme_gpu_types_host.h:#define GMX_EWALD_PME_GPU_TYPES_HOST_H
src/gromacs/ewald/pme_gpu_types_host.h:#include "gromacs/ewald/pme_gpu_program.h"
src/gromacs/ewald/pme_gpu_types_host.h:#include "gromacs/gpu_utils/clfftinitializer.h"
src/gromacs/ewald/pme_gpu_types_host.h:#include "gromacs/gpu_utils/hostallocator.h"
src/gromacs/ewald/pme_gpu_types_host.h:#include "pme_force_sender_gpu.h"
src/gromacs/ewald/pme_gpu_types_host.h:#include "pme_gpu_settings.h"
src/gromacs/ewald/pme_gpu_types_host.h:#include "pme_gpu_staging.h"
src/gromacs/ewald/pme_gpu_types_host.h:#if GMX_GPU && !GMX_GPU_HIP
src/gromacs/ewald/pme_gpu_types_host.h:struct PmeGpuSpecific;
src/gromacs/ewald/pme_gpu_types_host.h:struct PmeGpuHaloExchange;
src/gromacs/ewald/pme_gpu_types_host.h:/*! \brief A dummy typedef for the GPU host data placeholder on non-GPU builds */
src/gromacs/ewald/pme_gpu_types_host.h:typedef int PmeGpuSpecific;
src/gromacs/ewald/pme_gpu_types_host.h:typedef int PmeGpuHaloExchange;
src/gromacs/ewald/pme_gpu_types_host.h:#if GMX_GPU_CUDA
src/gromacs/ewald/pme_gpu_types_host.h:struct PmeGpuCudaKernelParams;
src/gromacs/ewald/pme_gpu_types_host.h:/*! \brief A typedef for including the GPU kernel arguments data by pointer */
src/gromacs/ewald/pme_gpu_types_host.h:typedef PmeGpuCudaKernelParams PmeGpuKernelParams;
src/gromacs/ewald/pme_gpu_types_host.h:#elif GMX_GPU_OPENCL || GMX_GPU_SYCL
src/gromacs/ewald/pme_gpu_types_host.h:struct PmeGpuKernelParamsBase;
src/gromacs/ewald/pme_gpu_types_host.h:/*! \brief A typedef for including the GPU kernel arguments data by pointer */
src/gromacs/ewald/pme_gpu_types_host.h:typedef PmeGpuKernelParamsBase PmeGpuKernelParams;
src/gromacs/ewald/pme_gpu_types_host.h:/*! \brief A dummy typedef for the GPU kernel arguments data placeholder on non-GPU builds */
src/gromacs/ewald/pme_gpu_types_host.h:typedef int PmeGpuKernelParams;
src/gromacs/ewald/pme_gpu_types_host.h: * The PME GPU structure for all the data copied directly from the CPU PME structure.
src/gromacs/ewald/pme_gpu_types_host.h: * (pme_gpu_reinit is called at the end of gmx_pme_init).
src/gromacs/ewald/pme_gpu_types_host.h: * Included in the main PME GPU structure by value.
src/gromacs/ewald/pme_gpu_types_host.h: * The main PME GPU host structure, included in the PME CPU structure by pointer.
src/gromacs/ewald/pme_gpu_types_host.h:struct PmeGpu
src/gromacs/ewald/pme_gpu_types_host.h:    //! A handle to the program created by buildPmeGpuProgram()
src/gromacs/ewald/pme_gpu_types_host.h:    const PmeGpuProgram* programHandle_;
src/gromacs/ewald/pme_gpu_types_host.h:    PmeGpuSettings settings;
src/gromacs/ewald/pme_gpu_types_host.h:    PmeGpuStaging staging;
src/gromacs/ewald/pme_gpu_types_host.h:    /*! \brief Kernel scheduling grid width limit in X - derived from deviceinfo compute capability in CUDA.
src/gromacs/ewald/pme_gpu_types_host.h:     * OpenCL seems to not have readily available global work size limit, so we just assign a large arbitrary constant to this instead.
src/gromacs/ewald/pme_gpu_types_host.h:     * TODO: this should be in PmeGpuProgram(Impl)
src/gromacs/ewald/pme_gpu_types_host.h:     * This feature is supported by CUDA and SYCL. Spread pipelining
src/gromacs/ewald/pme_gpu_types_host.h:    /*! \brief A single structure encompassing all the PME data used on GPU.
src/gromacs/ewald/pme_gpu_types_host.h:     * Its value is the only argument to all the PME GPU kernels.
src/gromacs/ewald/pme_gpu_types_host.h:     * \todo Test whether this should be copied to the constant GPU memory once for each computation
src/gromacs/ewald/pme_gpu_types_host.h:    std::shared_ptr<PmeGpuKernelParams> kernelParams;
src/gromacs/ewald/pme_gpu_types_host.h:    /*! \brief The pointer to GPU-framework specific host-side data, such as CUDA streams and events. */
src/gromacs/ewald/pme_gpu_types_host.h:    std::shared_ptr<PmeGpuSpecific> archSpecific; /* FIXME: make it an unique_ptr */
src/gromacs/ewald/pme_gpu_types_host.h:    std::unique_ptr<PmeGpuHaloExchange> haloExchange;
src/gromacs/ewald/pme_gather_sycl.cpp:#include "gromacs/gpu_utils/gmxsycl.h"
src/gromacs/ewald/pme_gather_sycl.cpp:#include "gromacs/gpu_utils/gputraits_sycl.h"
src/gromacs/ewald/pme_gather_sycl.cpp:#include "gromacs/gpu_utils/sycl_kernel_utils.h"
src/gromacs/ewald/pme_gather_sycl.cpp:#include "gromacs/gpu_utils/syclutils.h"
src/gromacs/ewald/pme_gather_sycl.cpp:#include "pme_gpu_calculate_splines_sycl.h"
src/gromacs/ewald/pme_gather_sycl.cpp:#include "pme_gpu_constants.h"
src/gromacs/ewald/pme_gather_sycl.cpp:#include "pme_gpu_types_host.h"
src/gromacs/ewald/pme_gather_sycl.cpp: * \tparam     atomsPerWarp       The number of atoms per GPU warp.
src/gromacs/ewald/pme_gather_sycl.cpp:            pmeGpuStageAtomData<float, atomsPerBlock, 1>(
src/gromacs/ewald/pme_gather_sycl.cpp:            pmeGpuStageAtomData<Float3, atomsPerBlock, 1>(
src/gromacs/ewald/pme_gather_sycl.cpp:        const int chargeCheck = pmeGpuCheckAtomCharge(gm_coefficientsA[atomIndexGlobal]);
src/gromacs/ewald/pme_gather_sycl.cpp:            const bool chargeCheck = pmeGpuCheckAtomCharge(gm_coefficientsB[atomIndexGlobal]);
src/gromacs/ewald/pme_gather_sycl.cpp:        auto* params   = reinterpret_cast<PmeGpuKernelParams*>(arg);
src/gromacs/ewald/pme_gather_sycl.cpp:    // SYCL has different multidimensional layout than OpenCL/CUDA.
src/gromacs/ewald/pme_gpu_ocl_stubs.cpp: * \brief Implements stubs of high-level PME GPU functions for OpenCL.
src/gromacs/ewald/pme_gpu_ocl_stubs.cpp: * \author Gaurav Garg <gaugarg@nvidia.com>
src/gromacs/ewald/pme_gpu_ocl_stubs.cpp:#include "pme_gpu_grid.h"
src/gromacs/ewald/pme_gpu_ocl_stubs.cpp:#include "pme_gpu_types.h"
src/gromacs/ewald/pme_gpu_ocl_stubs.cpp:#include "pme_gpu_types_host.h"
src/gromacs/ewald/pme_gpu_ocl_stubs.cpp:#include "pme_gpu_types_host_impl.h"
src/gromacs/ewald/pme_gpu_ocl_stubs.cpp:void pmeGpuGridHaloExchange(const PmeGpu* /*pmeGpu*/, gmx_wallcycle* /*wcycle*/)
src/gromacs/ewald/pme_gpu_ocl_stubs.cpp:    GMX_THROW(gmx::NotImplementedError("PME decomposition is not implemented in OpenCL"));
src/gromacs/ewald/pme_gpu_ocl_stubs.cpp:void pmeGpuGridHaloExchangeReverse(const PmeGpu* /*pmeGpu*/, gmx_wallcycle* /*wcycle*/)
src/gromacs/ewald/pme_gpu_ocl_stubs.cpp:    GMX_THROW(gmx::NotImplementedError("PME decomposition is not implemented in OpenCL"));
src/gromacs/ewald/pme_gpu_ocl_stubs.cpp:void convertPmeGridToFftGrid(const PmeGpu* /*pmeGpu*/,
src/gromacs/ewald/pme_gpu_ocl_stubs.cpp:    GMX_THROW(gmx::NotImplementedError("PME decomposition is not implemented in OpenCL"));
src/gromacs/ewald/pme_gpu_ocl_stubs.cpp:void convertPmeGridToFftGrid(const PmeGpu* /*pmeGpu*/, DeviceBuffer<float>* /*d_fftRealGrid*/, const int /*gridIndex*/)
src/gromacs/ewald/pme_gpu_ocl_stubs.cpp:    GMX_THROW(gmx::NotImplementedError("PME decomposition is not implemented in OpenCL"));
src/gromacs/ewald/pme_gpu_ocl_stubs.cpp:template void convertPmeGridToFftGrid<true>(const PmeGpu* /*pmeGpu*/,
src/gromacs/ewald/pme_gpu_ocl_stubs.cpp:template void convertPmeGridToFftGrid<false>(const PmeGpu* /*pmeGpu*/,
src/gromacs/ewald/pme_gpu_ocl_stubs.cpp:template void convertPmeGridToFftGrid<true>(const PmeGpu* /*pmeGpu*/,
src/gromacs/ewald/pme_gpu_ocl_stubs.cpp:template void convertPmeGridToFftGrid<false>(const PmeGpu* /*pmeGpu*/,
src/gromacs/ewald/pme_pp.cpp:#include "gromacs/ewald/pme_pp_comm_gpu.h"
src/gromacs/ewald/pme_pp.cpp:#include "gromacs/gpu_utils/hostallocator.h"
src/gromacs/ewald/pme_pp.cpp:#include "gromacs/mdtypes/state_propagator_data_gpu.h"
src/gromacs/ewald/pme_pp.cpp:                                       bool                           useGpuPmePpComms,
src/gromacs/ewald/pme_pp.cpp:                                       bool                           reinitGpuPmePpComms,
src/gromacs/ewald/pme_pp.cpp:                                       bool                           sendCoordinatesFromGpu,
src/gromacs/ewald/pme_pp.cpp:                                       bool                           receiveForcesToGpu,
src/gromacs/ewald/pme_pp.cpp:                                       bool                           useMdGpuGraph,
src/gromacs/ewald/pme_pp.cpp:                                       GpuEventSynchronizer*          coordinatesReadyOnDeviceEvent)
src/gromacs/ewald/pme_pp.cpp:    if (useGpuPmePpComms)
src/gromacs/ewald/pme_pp.cpp:        flags |= PP_PME_GPUCOMMS;
src/gromacs/ewald/pme_pp.cpp:        if (receiveForcesToGpu)
src/gromacs/ewald/pme_pp.cpp:            flags |= PP_PME_RECVFTOGPU;
src/gromacs/ewald/pme_pp.cpp:    if (useMdGpuGraph)
src/gromacs/ewald/pme_pp.cpp:        flags |= PP_PME_MDGPUGRAPH;
src/gromacs/ewald/pme_pp.cpp:            if (reinitGpuPmePpComms)
src/gromacs/ewald/pme_pp.cpp:                fr->pmePpCommGpu->reinit(n);
src/gromacs/ewald/pme_pp.cpp:            if (useGpuPmePpComms && (fr != nullptr))
src/gromacs/ewald/pme_pp.cpp:                if (sendCoordinatesFromGpu)
src/gromacs/ewald/pme_pp.cpp:                               "When sending coordinates from GPU, a synchronization event should "
src/gromacs/ewald/pme_pp.cpp:                    fr->pmePpCommGpu->sendCoordinatesToPmeFromGpu(
src/gromacs/ewald/pme_pp.cpp:                            fr->stateGpu->getCoordinates(), n, coordinatesReadyOnDeviceEvent);
src/gromacs/ewald/pme_pp.cpp:                    fr->pmePpCommGpu->sendCoordinatesToPmeFromCpu(const_cast<gmx::RVec*>(x.data()), n);
src/gromacs/ewald/pme_pp.cpp:    GMX_UNUSED_VALUE(reinitGpuPmePpComms);
src/gromacs/ewald/pme_pp.cpp:    GMX_UNUSED_VALUE(sendCoordinatesFromGpu);
src/gromacs/ewald/pme_pp.cpp:                              bool                           useGpuPmePpComms,
src/gromacs/ewald/pme_pp.cpp:                              bool                           sendCoordinatesFromGpu,
src/gromacs/ewald/pme_pp.cpp:                              bool                           receiveForcesToGpu,
src/gromacs/ewald/pme_pp.cpp:                              GpuEventSynchronizer*          coordinatesReadyOnDeviceEvent,
src/gromacs/ewald/pme_pp.cpp:                              bool                           useMdGpuGraph,
src/gromacs/ewald/pme_pp.cpp:                               useGpuPmePpComms,
src/gromacs/ewald/pme_pp.cpp:                               sendCoordinatesFromGpu,
src/gromacs/ewald/pme_pp.cpp:                               receiveForcesToGpu,
src/gromacs/ewald/pme_pp.cpp:                               useMdGpuGraph,
src/gromacs/ewald/pme_pp.cpp:static void recvFFromPme(gmx::PmePpCommGpu* pmePpCommGpu,
src/gromacs/ewald/pme_pp.cpp:                         bool               useGpuPmePpComms,
src/gromacs/ewald/pme_pp.cpp:                         bool               receivePmeForceToGpu)
src/gromacs/ewald/pme_pp.cpp:    if (useGpuPmePpComms)
src/gromacs/ewald/pme_pp.cpp:        GMX_ASSERT(pmePpCommGpu != nullptr, "Need valid pmePpCommGpu");
src/gromacs/ewald/pme_pp.cpp:        pmePpCommGpu->receiveForceFromPme(static_cast<gmx::RVec*>(recvptr), n, receivePmeForceToGpu);
src/gromacs/ewald/pme_pp.cpp:void gmx_pme_receive_f(gmx::PmePpCommGpu*    pmePpCommGpu,
src/gromacs/ewald/pme_pp.cpp:                       bool                  useGpuPmePpComms,
src/gromacs/ewald/pme_pp.cpp:                       bool                  receivePmeForceToGpu,
src/gromacs/ewald/pme_pp.cpp:    recvFFromPme(pmePpCommGpu, recvptr, natoms, cr, useGpuPmePpComms, receivePmeForceToGpu);
src/gromacs/ewald/pme_pp.cpp:    if (!receivePmeForceToGpu)
src/gromacs/ewald/pme_spread.cu: *  \brief Implements PME GPU spline calculation and charge spreading in CUDA.
src/gromacs/ewald/pme_spread.cu:#include "gromacs/gpu_utils/cuda_kernel_utils.cuh"
src/gromacs/ewald/pme_spread.cu:#include "gromacs/gpu_utils/typecasts_cuda_hip.h"
src/gromacs/ewald/pme_spread.cu:#include "pme_gpu_calculate_splines.cuh"
src/gromacs/ewald/pme_spread.cu: * \param[in]  kernelParams         Input PME CUDA data in constant memory.
src/gromacs/ewald/pme_spread.cu:__device__ __forceinline__ void spread_charges(const PmeGpuCudaKernelParams kernelParams,
src/gromacs/ewald/pme_spread.cu:    const int chargeCheck = pme_gpu_check_atom_charge(*atomCharge);
src/gromacs/ewald/pme_spread.cu: * \param[in]  kernelParams         Input PME CUDA data in constant memory.
src/gromacs/ewald/pme_spread.cu:        void pme_spline_and_spread_kernel(const PmeGpuCudaKernelParams kernelParams)
src/gromacs/ewald/pme_spread.cu:        pme_gpu_stage_atom_data<float, atomsPerBlock, 1>(
src/gromacs/ewald/pme_spread.cu:            pme_gpu_stage_atom_data<float3, atomsPerBlock, 1>(
src/gromacs/ewald/pme_spread.cu:         * (the data is assumed to be in GPU global memory with proper layout already,
src/gromacs/ewald/pme_spread.cu:        pme_gpu_stage_atom_data<float, atomsPerBlock, DIM * order>(sm_theta, kernelParams.atoms.d_theta);
src/gromacs/ewald/pme_spread.cu:        pme_gpu_stage_atom_data<int, atomsPerBlock, DIM>(sm_gridlineIndices,
src/gromacs/ewald/pme_spread.cu:            pme_gpu_stage_atom_data<float, atomsPerBlock, 1>(
src/gromacs/ewald/pme_spread.cu:template __global__ void pme_spline_and_spread_kernel<4, true, true, true, true, 1, true, ThreadsPerAtom::Order>        (const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_spread.cu:template __global__ void pme_spline_and_spread_kernel<4, true, false, true, true, 1, true, ThreadsPerAtom::Order>       (const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_spread.cu:template __global__ void pme_spline_and_spread_kernel<4, false, true, true, true, 1, true, ThreadsPerAtom::Order>       (const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_spread.cu:template __global__ void pme_spline_and_spread_kernel<4, true, true, true, true, 1, false, ThreadsPerAtom::Order>       (const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_spread.cu:template __global__ void pme_spline_and_spread_kernel<4, true, true, true, true, 1, true, ThreadsPerAtom::OrderSquared> (const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_spread.cu:template __global__ void pme_spline_and_spread_kernel<4, true, false, true, true, 1, true, ThreadsPerAtom::OrderSquared>(const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_spread.cu:template __global__ void pme_spline_and_spread_kernel<4, false, true, true, true, 1, true, ThreadsPerAtom::OrderSquared>(const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_spread.cu:template __global__ void pme_spline_and_spread_kernel<4, true, true, true, true, 1, false, ThreadsPerAtom::OrderSquared>(const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_spread.cu:template __global__ void pme_spline_and_spread_kernel<4, true, true, true, true, 2, true, ThreadsPerAtom::Order>        (const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_spread.cu:template __global__ void pme_spline_and_spread_kernel<4, true, false, true, true, 2, true, ThreadsPerAtom::Order>       (const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_spread.cu:template __global__ void pme_spline_and_spread_kernel<4, false, true, true, true, 2, true, ThreadsPerAtom::Order>       (const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_spread.cu:template __global__ void pme_spline_and_spread_kernel<4, true, true, true, true, 2, false, ThreadsPerAtom::Order>       (const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_spread.cu:template __global__ void pme_spline_and_spread_kernel<4, true, true, true, true, 2, true, ThreadsPerAtom::OrderSquared> (const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_spread.cu:template __global__ void pme_spline_and_spread_kernel<4, true, false, true, true, 2, true, ThreadsPerAtom::OrderSquared>(const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_spread.cu:template __global__ void pme_spline_and_spread_kernel<4, false, true, true, true, 2, true, ThreadsPerAtom::OrderSquared>(const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_spread.cu:template __global__ void pme_spline_and_spread_kernel<4, true, true, true, true, 2, false, ThreadsPerAtom::OrderSquared>(const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_gpu_grid.h: * \author Gaurav Garg <gaugarg@nvidia.com>
src/gromacs/ewald/pme_gpu_grid.h:#ifndef GMX_EWALD_PME_GPU_GRID_H
src/gromacs/ewald/pme_gpu_grid.h:#define GMX_EWALD_PME_GPU_GRID_H
src/gromacs/ewald/pme_gpu_grid.h:#include "gromacs/gpu_utils/devicebuffer_datatype.h"
src/gromacs/ewald/pme_gpu_grid.h:struct PmeGpu;
src/gromacs/ewald/pme_gpu_grid.h: * ToDo: Current implementation synchronizes pmeStream to make sure data is ready on GPU after
src/gromacs/ewald/pme_gpu_grid.h: * \param[in]  pmeGpu                 The PME GPU structure.
src/gromacs/ewald/pme_gpu_grid.h:void pmeGpuGridHaloExchange(const PmeGpu* pmeGpu, gmx_wallcycle* wcycle);
src/gromacs/ewald/pme_gpu_grid.h: * ToDo: Current implementation synchronizes pmeStream to make sure data is ready on GPU after FFT
src/gromacs/ewald/pme_gpu_grid.h: * \param[in]  pmeGpu                 The PME GPU structure.
src/gromacs/ewald/pme_gpu_grid.h:void pmeGpuGridHaloExchangeReverse(const PmeGpu* pmeGpu, gmx_wallcycle* wcycle);
src/gromacs/ewald/pme_gpu_grid.h: * \param[in]  pmeGpu                 The PME GPU structure.
src/gromacs/ewald/pme_gpu_grid.h:void convertPmeGridToFftGrid(const PmeGpu* pmeGpu, float* h_fftRealGrid, gmx_parallel_3dfft* fftSetup, int gridIndex);
src/gromacs/ewald/pme_gpu_grid.h: * Copy PME Grid with overlap region to device FFT grid and vice-versa. Used in full GPU PME decomposition
src/gromacs/ewald/pme_gpu_grid.h: * \param[in]  pmeGpu                 The PME GPU structure.
src/gromacs/ewald/pme_gpu_grid.h:void convertPmeGridToFftGrid(const PmeGpu* pmeGpu, DeviceBuffer<float>* d_fftRealGrid, int gridIndex);
src/gromacs/ewald/pme_gpu_grid.h:extern template void convertPmeGridToFftGrid<true>(const PmeGpu* /*pmeGpu*/,
src/gromacs/ewald/pme_gpu_grid.h:extern template void convertPmeGridToFftGrid<false>(const PmeGpu* /*pmeGpu*/,
src/gromacs/ewald/pme_gpu_grid.h:extern template void convertPmeGridToFftGrid<true>(const PmeGpu* /*pmeGpu*/,
src/gromacs/ewald/pme_gpu_grid.h:extern template void convertPmeGridToFftGrid<false>(const PmeGpu* /*pmeGpu*/,
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cpp: * \brief Implements PME-PP communication using GPU direct communication
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cpp: * \author Alan Gray <alang@nvidia.com>
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cpp:#include "gromacs/gpu_utils/device_context.h"
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cpp:#include "gromacs/gpu_utils/device_stream.h"
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cpp:#include "gromacs/gpu_utils/devicebuffer.h"
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cpp:#include "gromacs/gpu_utils/gpueventsynchronizer.h"
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cpp:#include "pme_pp_comm_gpu_impl.h"
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cpp:#if GMX_GPU_CUDA
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cpp:#    include "gromacs/gpu_utils/cudautils.cuh"
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cpp:#    include "gromacs/gpu_utils/typecasts_cuda_hip.h"
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cpp:#if GMX_GPU_SYCL
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cpp:#    include "gromacs/gpu_utils/gmxsycl.h"
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cpp:PmePpCommGpu::Impl::Impl(MPI_Comm                    comm,
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cpp:    stageLibMpiGpuCpuComm_ = (getenv("GMX_DISABLE_STAGED_GPU_TO_CPU_PMEPP_COMM") == nullptr);
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cpp:PmePpCommGpu::Impl::~Impl()
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cpp:void PmePpCommGpu::Impl::reinit(int size)
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cpp:        MPI_Recv(&remotePmeForceSendEvent_, sizeof(GpuEventSynchronizer*), MPI_BYTE, pmeRank_, 0, comm_, MPI_STATUS_IGNORE);
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cpp:void PmePpCommGpu::Impl::receiveForceFromPmePeerToPeer(bool receivePmeForceToGpu)
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cpp:    if (receivePmeForceToGpu)
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cpp:        // Record event to be enqueued in the GPU local buffer operations, to
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cpp:void PmePpCommGpu::Impl::receiveForceFromPmeGpuAwareMpi(Float3* pmeForcePtr, int recvSize)
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cpp:    if (!stageLibMpiGpuCpuComm_)
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cpp:                // Receive data from remote GPU in memory of local GPU
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cpp:            // Receive data from remote GPU in memory of local GPU
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cpp:                                 GpuApiCallBehavior::Sync,
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cpp:void PmePpCommGpu::Impl::receiveForceFromPme(Float3* recvPtr, int recvSize, bool receivePmeForceToGpu)
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cpp:    Float3* pmeForcePtr = receivePmeForceToGpu ? asMpiPointer(d_pmeForces_) : recvPtr;
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cpp:        receiveForceFromPmePeerToPeer(receivePmeForceToGpu);
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cpp:        receiveForceFromPmeGpuAwareMpi(pmeForcePtr, recvSize);
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cpp:void PmePpCommGpu::Impl::sendCoordinatesToPmeGpuAwareMpi(Float3* sendPtr,
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cpp:                                                         GpuEventSynchronizer* coordinatesReadyOnDeviceEvent)
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cpp:    // The corresponding wait for the below non-blocking coordinate send is in receiveForceFromPmeGpuAwareMpi.
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cpp:    MPI_Isend(sendptr_x, sendSize * DIM, MPI_FLOAT, pmeRank_, eCommType_COORD_GPU, comm_, &coordinateSendRequest_);
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cpp:void PmePpCommGpu::Impl::sendCoordinatesToPme(Float3*               sendPtr,
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cpp:                                              GpuEventSynchronizer* coordinatesReadyOnDeviceEvent)
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cpp:        sendCoordinatesToPmeGpuAwareMpi(sendPtr, sendSize, coordinatesReadyOnDeviceEvent);
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cpp:DeviceBuffer<Float3> PmePpCommGpu::Impl::getGpuForceStagingPtr()
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cpp:GpuEventSynchronizer* PmePpCommGpu::Impl::getForcesReadySynchronizer()
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cpp:DeviceBuffer<uint64_t> PmePpCommGpu::Impl::getGpuForcesSyncObj()
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cpp:PmePpCommGpu::PmePpCommGpu(MPI_Comm                    comm,
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cpp:PmePpCommGpu::~PmePpCommGpu() = default;
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cpp:void PmePpCommGpu::reinit(int size)
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cpp:void PmePpCommGpu::receiveForceFromPme(RVec* recvPtr, int recvSize, bool receivePmeForceToGpu)
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cpp:    impl_->receiveForceFromPme(recvPtr, recvSize, receivePmeForceToGpu);
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cpp:void PmePpCommGpu::sendCoordinatesToPmeFromGpu(DeviceBuffer<RVec>    sendPtr,
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cpp:                                               GpuEventSynchronizer* coordinatesReadyOnDeviceEvent)
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cpp:void PmePpCommGpu::sendCoordinatesToPmeFromCpu(RVec* sendPtr, int sendSize)
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cpp:DeviceBuffer<Float3> PmePpCommGpu::getGpuForceStagingPtr()
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cpp:    return impl_->getGpuForceStagingPtr();
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cpp:GpuEventSynchronizer* PmePpCommGpu::getForcesReadySynchronizer()
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cpp:DeviceBuffer<uint64_t> PmePpCommGpu::getGpuForcesSyncObj()
src/gromacs/ewald/pme_pp_comm_gpu_impl_gpu.cpp:    return impl_->getGpuForcesSyncObj();
src/gromacs/ewald/pme_gpu_program_impl_sycl.cpp: * Implements PmeGpuProgramImpl, which stores permanent PME GPU context-derived data,
src/gromacs/ewald/pme_gpu_program_impl_sycl.cpp:#include "gromacs/gpu_utils/gmxsycl.h"
src/gromacs/ewald/pme_gpu_program_impl_sycl.cpp:#include "gromacs/gpu_utils/syclutils.h"
src/gromacs/ewald/pme_gpu_program_impl_sycl.cpp:#include "pme_gpu_constants.h"
src/gromacs/ewald/pme_gpu_program_impl_sycl.cpp:#include "pme_gpu_internal.h" // for GridOrdering enum
src/gromacs/ewald/pme_gpu_program_impl_sycl.cpp:#include "pme_gpu_program_impl.h"
src/gromacs/ewald/pme_gpu_program_impl_sycl.cpp:#include "pme_gpu_types_host.h"
src/gromacs/ewald/pme_gpu_program_impl_sycl.cpp:// These hardcoded spread/gather parameters refer to not-implemented PME GPU 2D decomposition in X/Y
src/gromacs/ewald/pme_gpu_program_impl_sycl.cpp:                GMX_RELEASE_ASSERT(false, "Flexible sub-groups only supported for Intel GPUs");
src/gromacs/ewald/pme_gpu_program_impl_sycl.cpp:static void setKernelPointers(struct PmeGpuProgramImpl* pmeGpuProgram)
src/gromacs/ewald/pme_gpu_program_impl_sycl.cpp:    pmeGpuProgram->splineAndSpreadKernelSingle =
src/gromacs/ewald/pme_gpu_program_impl_sycl.cpp:    pmeGpuProgram->splineAndSpreadKernelThPerAtom4Single =
src/gromacs/ewald/pme_gpu_program_impl_sycl.cpp:    pmeGpuProgram->splineAndSpreadKernelWriteSplinesSingle =
src/gromacs/ewald/pme_gpu_program_impl_sycl.cpp:    pmeGpuProgram->splineAndSpreadKernelWriteSplinesThPerAtom4Single =
src/gromacs/ewald/pme_gpu_program_impl_sycl.cpp:    pmeGpuProgram->splineKernelSingle =
src/gromacs/ewald/pme_gpu_program_impl_sycl.cpp:    pmeGpuProgram->splineKernelThPerAtom4Single =
src/gromacs/ewald/pme_gpu_program_impl_sycl.cpp:    pmeGpuProgram->spreadKernelSingle =
src/gromacs/ewald/pme_gpu_program_impl_sycl.cpp:    pmeGpuProgram->spreadKernelThPerAtom4Single =
src/gromacs/ewald/pme_gpu_program_impl_sycl.cpp:    pmeGpuProgram->splineAndSpreadKernelDual =
src/gromacs/ewald/pme_gpu_program_impl_sycl.cpp:    pmeGpuProgram->splineAndSpreadKernelThPerAtom4Dual =
src/gromacs/ewald/pme_gpu_program_impl_sycl.cpp:    pmeGpuProgram->splineAndSpreadKernelWriteSplinesDual =
src/gromacs/ewald/pme_gpu_program_impl_sycl.cpp:    pmeGpuProgram->splineAndSpreadKernelWriteSplinesThPerAtom4Dual =
src/gromacs/ewald/pme_gpu_program_impl_sycl.cpp:    pmeGpuProgram->splineKernelDual =
src/gromacs/ewald/pme_gpu_program_impl_sycl.cpp:    pmeGpuProgram->splineKernelThPerAtom4Dual =
src/gromacs/ewald/pme_gpu_program_impl_sycl.cpp:    pmeGpuProgram->spreadKernelDual =
src/gromacs/ewald/pme_gpu_program_impl_sycl.cpp:    pmeGpuProgram->spreadKernelThPerAtom4Dual =
src/gromacs/ewald/pme_gpu_program_impl_sycl.cpp:    pmeGpuProgram->gatherKernelSingle =
src/gromacs/ewald/pme_gpu_program_impl_sycl.cpp:    pmeGpuProgram->gatherKernelThPerAtom4Single =
src/gromacs/ewald/pme_gpu_program_impl_sycl.cpp:    pmeGpuProgram->gatherKernelReadSplinesSingle =
src/gromacs/ewald/pme_gpu_program_impl_sycl.cpp:    pmeGpuProgram->gatherKernelReadSplinesThPerAtom4Single =
src/gromacs/ewald/pme_gpu_program_impl_sycl.cpp:    pmeGpuProgram->gatherKernelDual =
src/gromacs/ewald/pme_gpu_program_impl_sycl.cpp:    pmeGpuProgram->gatherKernelThPerAtom4Dual =
src/gromacs/ewald/pme_gpu_program_impl_sycl.cpp:    pmeGpuProgram->gatherKernelReadSplinesDual =
src/gromacs/ewald/pme_gpu_program_impl_sycl.cpp:    pmeGpuProgram->gatherKernelReadSplinesThPerAtom4Dual =
src/gromacs/ewald/pme_gpu_program_impl_sycl.cpp:    pmeGpuProgram->solveXYZKernelA =
src/gromacs/ewald/pme_gpu_program_impl_sycl.cpp:    pmeGpuProgram->solveXYZEnergyKernelA =
src/gromacs/ewald/pme_gpu_program_impl_sycl.cpp:    pmeGpuProgram->solveYZXKernelA =
src/gromacs/ewald/pme_gpu_program_impl_sycl.cpp:    pmeGpuProgram->solveYZXEnergyKernelA =
src/gromacs/ewald/pme_gpu_program_impl_sycl.cpp:    pmeGpuProgram->solveXYZKernelB =
src/gromacs/ewald/pme_gpu_program_impl_sycl.cpp:    pmeGpuProgram->solveXYZEnergyKernelB =
src/gromacs/ewald/pme_gpu_program_impl_sycl.cpp:    pmeGpuProgram->solveYZXKernelB =
src/gromacs/ewald/pme_gpu_program_impl_sycl.cpp:    pmeGpuProgram->solveYZXEnergyKernelB =
src/gromacs/ewald/pme_gpu_program_impl_sycl.cpp:PmeGpuProgramImpl::PmeGpuProgramImpl(const DeviceContext& deviceContext) :
src/gromacs/ewald/pme_gpu_program_impl_sycl.cpp:PmeGpuProgramImpl::~PmeGpuProgramImpl()
src/gromacs/ewald/pme_gather.cu: *  \brief Implements PME force gathering in CUDA.
src/gromacs/ewald/pme_gather.cu:#include "gromacs/gpu_utils/cuda_kernel_utils.cuh"
src/gromacs/ewald/pme_gather.cu:#include "gromacs/gpu_utils/typecasts_cuda_hip.h"
src/gromacs/ewald/pme_gather.cu:#include "pme_gpu_calculate_splines.cuh"
src/gromacs/ewald/pme_gather.cu: * An inline CUDA function: unroll the dynamic index accesses to the constant grid sizes to avoid local memory operations.
src/gromacs/ewald/pme_gather.cu: * \tparam     blockSize          The CUDA block size
src/gromacs/ewald/pme_gather.cu: * \tparam     atomsPerWarp       The number of atoms per GPU warp.
src/gromacs/ewald/pme_gather.cu:__global__ void nvshmemSignalKernel(PmeGpuCudaKernelParams kernelParams);
src/gromacs/ewald/pme_gather.cu: * A CUDA kernel which signals to the corresponding PP ranks of pme forces transfer completion.
src/gromacs/ewald/pme_gather.cu: * \param[in]  kernelParams         All the PME GPU data.
src/gromacs/ewald/pme_gather.cu:__global__ void nvshmemSignalKernel(const PmeGpuCudaKernelParams kernelParams)
src/gromacs/ewald/pme_gather.cu: * A CUDA kernel which gathers the atom forces from the grid.
src/gromacs/ewald/pme_gather.cu: * \param[in]  kernelParams         All the PME GPU data.
src/gromacs/ewald/pme_gather.cu:        void pme_gather_kernel(const PmeGpuCudaKernelParams kernelParams)
src/gromacs/ewald/pme_gather.cu:            pme_gpu_stage_atom_data<float, atomsPerBlock, 1>(sm_coefficients, gm_coefficientsA);
src/gromacs/ewald/pme_gather.cu:            pme_gpu_stage_atom_data<float3, atomsPerBlock, 1>(sm_coordinates, gm_coordinates);
src/gromacs/ewald/pme_gather.cu:    const int chargeCheck = pme_gpu_check_atom_charge(gm_coefficientsA[atomIndexGlobal]);
src/gromacs/ewald/pme_gather.cu:        const int chargeCheck = pme_gpu_check_atom_charge(gm_coefficientsB[atomIndexGlobal]);
src/gromacs/ewald/pme_gather.cu:template __global__ void pme_gather_kernel<4, true, true, 1, true, ThreadsPerAtom::Order>        (const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_gather.cu:template __global__ void pme_gather_kernel<4, true, true, 1, true, ThreadsPerAtom::OrderSquared> (const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_gather.cu:template __global__ void pme_gather_kernel<4, true, true, 1, false, ThreadsPerAtom::Order>       (const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_gather.cu:template __global__ void pme_gather_kernel<4, true, true, 1, false, ThreadsPerAtom::OrderSquared>(const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_gather.cu:template __global__ void pme_gather_kernel<4, true, true, 2, true, ThreadsPerAtom::Order>        (const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_gather.cu:template __global__ void pme_gather_kernel<4, true, true, 2, true, ThreadsPerAtom::OrderSquared> (const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_gather.cu:template __global__ void pme_gather_kernel<4, true, true, 2, false, ThreadsPerAtom::Order>       (const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_gather.cu:template __global__ void pme_gather_kernel<4, true, true, 2, false, ThreadsPerAtom::OrderSquared>(const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_gpu_staging.h: * \brief Defines the host-side PME GPU data structures.
src/gromacs/ewald/pme_gpu_staging.h: * -- PmeGpuSettings -> PmeGpuTasks
src/gromacs/ewald/pme_gpu_staging.h: * -- refining GPU notation application (#2053)
src/gromacs/ewald/pme_gpu_staging.h:#ifndef GMX_EWALD_PME_GPU_STAGING_H
src/gromacs/ewald/pme_gpu_staging.h:#define GMX_EWALD_PME_GPU_STAGING_H
src/gromacs/ewald/pme_gpu_staging.h:#include "gromacs/gpu_utils/hostallocator.h"
src/gromacs/ewald/pme_gpu_staging.h: * The PME GPU intermediate buffers structure, included in the main PME GPU structure by value.
src/gromacs/ewald/pme_gpu_staging.h: * Buffers are managed by the PME GPU module.
src/gromacs/ewald/pme_gpu_staging.h:struct PmeGpuStaging
src/gromacs/ewald/pme_gpu_staging.h:    /*! \brief Virial and energy intermediate host-side buffer. Size is PME_GPU_VIRIAL_AND_ENERGY_COUNT. */
src/gromacs/ewald/pme_only.h:                bool                            useGpuPmePpCommunication,
src/gromacs/ewald/pme_gpu_hip_stubs.cpp: * for performing the PME calculations on GPU.
src/gromacs/ewald/pme_gpu_hip_stubs.cpp:#include "pme_gpu_internal.h"
src/gromacs/ewald/pme_gpu_hip_stubs.cpp:int pme_gpu_get_atoms_per_warp(const PmeGpu* /* pmeGpu */)
src/gromacs/ewald/pme_gpu_hip_stubs.cpp:void pme_gpu_synchronize(const PmeGpu* /* pmeGpu */)
src/gromacs/ewald/pme_gpu_hip_stubs.cpp:void pme_gpu_spread(PmeGpu* /* pmeGpu */,
src/gromacs/ewald/pme_gpu_hip_stubs.cpp:                    GpuEventSynchronizer* /* xReadyOnDevice */,
src/gromacs/ewald/pme_gpu_hip_stubs.cpp:                    bool /* useGpuDirectComm */,
src/gromacs/ewald/pme_gpu_hip_stubs.cpp:                    gmx::PmeCoordinateReceiverGpu* /* pmeCoordinateReceiverGpu */,
src/gromacs/ewald/pme_gpu_hip_stubs.cpp:                    bool /* useMdGpuGraph */,
src/gromacs/ewald/pme_gpu_hip_stubs.cpp:void pme_gpu_solve(PmeGpu* /* pmeGpu */,
src/gromacs/ewald/pme_gpu_hip_stubs.cpp:void pme_gpu_gather(PmeGpu* /* pmeGpu */,
src/gromacs/ewald/pme_gpu_hip_stubs.cpp:void pme_gpu_set_kernelparam_coordinates(const PmeGpu* /* pmeGpu */, DeviceBuffer<gmx::RVec> /* d_x */)
src/gromacs/ewald/pme_gpu_hip_stubs.cpp:DeviceBuffer<gmx::RVec> pme_gpu_get_kernelparam_forces(const PmeGpu* /* pmeGpu */)
src/gromacs/ewald/pme_gpu_hip_stubs.cpp:void pme_gpu_set_kernelparam_useNvshmem(const PmeGpu* /* pmeGpu */, bool /* useNvshmem */)
src/gromacs/ewald/pme_gpu_hip_stubs.cpp:GpuEventSynchronizer* pme_gpu_get_forces_ready_synchronizer(const PmeGpu* /* pmeGpu */)
src/gromacs/ewald/pme_gpu_hip_stubs.cpp:void pme_gpu_getEnergyAndVirial(const gmx_pme_t& /* pme */, float /* lambda */, PmeOutput* /* output */)
src/gromacs/ewald/pme_gpu_hip_stubs.cpp:PmeOutput pme_gpu_getOutput(gmx_pme_t* /* pme */, bool /* computeEnergyAndVirial */, real /* lambdaQ */)
src/gromacs/ewald/pme_gpu_hip_stubs.cpp:void pme_gpu_update_input_box(PmeGpu* /* pmeGpu */, const matrix /* box */)
src/gromacs/ewald/pme_gpu_hip_stubs.cpp:void pme_gpu_get_real_grid_sizes(const PmeGpu* /* pmeGpu */, gmx::IVec* /* gridSize */, gmx::IVec* /* paddedGridSize */)
src/gromacs/ewald/pme_gpu_hip_stubs.cpp:void pme_gpu_reinit(gmx_pme_t* /* pme */,
src/gromacs/ewald/pme_gpu_hip_stubs.cpp:                    const PmeGpuProgram* /* pmeGpuProgram */,
src/gromacs/ewald/pme_gpu_hip_stubs.cpp:                    bool /* useMdGpuGraph */)
src/gromacs/ewald/pme_gpu_hip_stubs.cpp:void pme_gpu_destroy(PmeGpu* /* pmeGpu */)
src/gromacs/ewald/pme_gpu_hip_stubs.cpp:void pme_gpu_reinit_atoms(PmeGpu* /* pmeGpu */,
src/gromacs/ewald/pme_gpu_hip_stubs.cpp:void pme_gpu_sync_spread_grid(const PmeGpu* /* pmeGpu */)
src/gromacs/ewald/pme_gpu_hip_stubs.cpp:void pme_gpu_3dfft(const PmeGpu* /* pmeGpu */, enum gmx_fft_direction /* direction */, int /* gridIndex = 0 */)
src/gromacs/ewald/pme_gpu_hip_stubs.cpp:void pme_gpu_clear_grids(const PmeGpu* /* pmeGpu */)
src/gromacs/ewald/pme_gpu_hip_stubs.cpp:void pme_gpu_clear_energy_virial(const PmeGpu* /* pmeGpu */, bool /* gpuGraphWithSeparatePmeRank */)
src/gromacs/ewald/pme_gpu_hip_stubs.cpp:int pme_gpu_get_atom_data_block_size()
src/gromacs/ewald/pme_gpu_hip_stubs.cpp:void pme_gpu_update_timings(const PmeGpu* /* pmeGpu */)
src/gromacs/ewald/pme_gpu_hip_stubs.cpp:void pme_gpu_reinit_timings(const PmeGpu* /* pmeGpu */)
src/gromacs/ewald/pme_gpu_hip_stubs.cpp:void pme_gpu_reset_timings(const PmeGpu* /* pmeGpu */)
src/gromacs/ewald/pme_gpu_hip_stubs.cpp:void pme_gpu_get_timings(const PmeGpu* /* pmeGpu */, gmx_wallclock_gpu_pme_t* /* timings */)
src/gromacs/ewald/pme_gpu_hip_stubs.cpp:bool pme_gpu_stream_query(const PmeGpu* /* pmeGpu */)
src/gromacs/ewald/pme_gpu_internal.h: * \brief This file contains internal function definitions for performing the PME calculations on GPU.
src/gromacs/ewald/pme_gpu_internal.h: * These are not meant to be exposed outside of the PME GPU code.
src/gromacs/ewald/pme_gpu_internal.h: * As of now, their bodies are still in the common pme_gpu.cpp files.
src/gromacs/ewald/pme_gpu_internal.h:#ifndef GMX_EWALD_PME_GPU_INTERNAL_H
src/gromacs/ewald/pme_gpu_internal.h:#define GMX_EWALD_PME_GPU_INTERNAL_H
src/gromacs/ewald/pme_gpu_internal.h:#include "gromacs/gpu_utils/devicebuffer_datatype.h"
src/gromacs/ewald/pme_gpu_internal.h:#include "gromacs/gpu_utils/gpu_macros.h" // for the GPU_FUNC_ macros
src/gromacs/ewald/pme_gpu_internal.h:#include "pme_gpu_types_host.h"
src/gromacs/ewald/pme_gpu_internal.h:class GpuEventSynchronizer;
src/gromacs/ewald/pme_gpu_internal.h:struct gmx_pme_t; // only used in pme_gpu_reinit
src/gromacs/ewald/pme_gpu_internal.h:struct PmeGpu;
src/gromacs/ewald/pme_gpu_internal.h:class PmeGpuProgram;
src/gromacs/ewald/pme_gpu_internal.h:struct PmeGpuStaging;
src/gromacs/ewald/pme_gpu_internal.h:struct PmeGpuSettings;
src/gromacs/ewald/pme_gpu_internal.h: * The GPU version of PME requires that the coordinates array have a
src/gromacs/ewald/pme_gpu_internal.h: * \returns Number of atoms in a single GPU atom data chunk, which
src/gromacs/ewald/pme_gpu_internal.h:int pme_gpu_get_atom_data_block_size();
src/gromacs/ewald/pme_gpu_internal.h:GPU_FUNC_QUALIFIER int pme_gpu_get_atoms_per_warp(const PmeGpu* GPU_FUNC_ARGUMENT(pmeGpu))
src/gromacs/ewald/pme_gpu_internal.h:        GPU_FUNC_TERM_WITH_RETURN(0);
src/gromacs/ewald/pme_gpu_internal.h: * Synchronizes the current computation, waiting for the GPU kernels/transfers to finish.
src/gromacs/ewald/pme_gpu_internal.h: * \param[in] pmeGpu            The PME GPU structure.
src/gromacs/ewald/pme_gpu_internal.h:GPU_FUNC_QUALIFIER void pme_gpu_synchronize(const PmeGpu* GPU_FUNC_ARGUMENT(pmeGpu)) GPU_FUNC_TERM;
src/gromacs/ewald/pme_gpu_internal.h: * Allocates the fixed size energy and virial buffer both on GPU and CPU.
src/gromacs/ewald/pme_gpu_internal.h: * \param[in,out] pmeGpu            The PME GPU structure.
src/gromacs/ewald/pme_gpu_internal.h:void pme_gpu_alloc_energy_virial(PmeGpu* pmeGpu);
src/gromacs/ewald/pme_gpu_internal.h: * Frees the energy and virial memory both on GPU and CPU.
src/gromacs/ewald/pme_gpu_internal.h: * \param[in] pmeGpu            The PME GPU structure.
src/gromacs/ewald/pme_gpu_internal.h:void pme_gpu_free_energy_virial(PmeGpu* pmeGpu);
src/gromacs/ewald/pme_gpu_internal.h: * Clears the energy and virial memory on GPU with 0.
src/gromacs/ewald/pme_gpu_internal.h: * \param[in] pmeGpu                          The PME GPU structure.
src/gromacs/ewald/pme_gpu_internal.h: * \param[in] gpuGraphWithSeparatePmeRank     Whether MD GPU Graph with separate PME rank is in use.
src/gromacs/ewald/pme_gpu_internal.h:void pme_gpu_clear_energy_virial(const PmeGpu* pmeGpu, bool gpuGraphWithSeparatePmeRank);
src/gromacs/ewald/pme_gpu_internal.h: * Reallocates and copies the pre-computed B-spline values to the GPU.
src/gromacs/ewald/pme_gpu_internal.h: * \param[in,out] pmeGpu             The PME GPU structure.
src/gromacs/ewald/pme_gpu_internal.h:void pme_gpu_realloc_and_copy_bspline_values(PmeGpu* pmeGpu, int gridIndex = 0);
src/gromacs/ewald/pme_gpu_internal.h: * Frees the pre-computed B-spline values on the GPU (and the transfer CPU buffers).
src/gromacs/ewald/pme_gpu_internal.h: * \param[in] pmeGpu             The PME GPU structure.
src/gromacs/ewald/pme_gpu_internal.h:void pme_gpu_free_bspline_values(PmeGpu* pmeGpu);
src/gromacs/ewald/pme_gpu_internal.h: * Reallocates the GPU buffer for the PME forces.
src/gromacs/ewald/pme_gpu_internal.h: * \param[in] pmeGpu             The PME GPU structure.
src/gromacs/ewald/pme_gpu_internal.h:void pme_gpu_realloc_forces(PmeGpu* pmeGpu);
src/gromacs/ewald/pme_gpu_internal.h: * Frees the GPU buffer for the PME forces.
src/gromacs/ewald/pme_gpu_internal.h: * \param[in] pmeGpu             The PME GPU structure.
src/gromacs/ewald/pme_gpu_internal.h:void pme_gpu_free_forces(const PmeGpu* pmeGpu);
src/gromacs/ewald/pme_gpu_internal.h: * Copies the forces from the CPU buffer to the GPU (to reduce them with the PME GPU gathered
src/gromacs/ewald/pme_gpu_internal.h: * \param[in] pmeGpu             The PME GPU structure.
src/gromacs/ewald/pme_gpu_internal.h:void pme_gpu_copy_input_forces(PmeGpu* pmeGpu);
src/gromacs/ewald/pme_gpu_internal.h: * Copies the forces from the GPU to the CPU buffer. To be called after the gathering stage.
src/gromacs/ewald/pme_gpu_internal.h: * \param[in] pmeGpu             The PME GPU structure.
src/gromacs/ewald/pme_gpu_internal.h:void pme_gpu_copy_output_forces(PmeGpu* pmeGpu);
src/gromacs/ewald/pme_gpu_internal.h: * Checks whether work in the PME GPU stream has completed.
src/gromacs/ewald/pme_gpu_internal.h: * \param[in] pmeGpu            The PME GPU structure.
src/gromacs/ewald/pme_gpu_internal.h:bool pme_gpu_stream_query(const PmeGpu* pmeGpu);
src/gromacs/ewald/pme_gpu_internal.h: * Reallocates the buffer on the GPU and copies the charges/coefficients from the CPU buffer.
src/gromacs/ewald/pme_gpu_internal.h: * \param[in] pmeGpu            The PME GPU structure.
src/gromacs/ewald/pme_gpu_internal.h:void pme_gpu_realloc_and_copy_input_coefficients(const PmeGpu* pmeGpu,
src/gromacs/ewald/pme_gpu_internal.h: * Frees the charges/coefficients on the GPU.
src/gromacs/ewald/pme_gpu_internal.h: * \param[in] pmeGpu             The PME GPU structure.
src/gromacs/ewald/pme_gpu_internal.h:void pme_gpu_free_coefficients(const PmeGpu* pmeGpu);
src/gromacs/ewald/pme_gpu_internal.h: * Reallocates the buffers on the GPU and the host for the atoms spline data.
src/gromacs/ewald/pme_gpu_internal.h: * \param[in,out] pmeGpu            The PME GPU structure.
src/gromacs/ewald/pme_gpu_internal.h:void pme_gpu_realloc_spline_data(PmeGpu* pmeGpu);
src/gromacs/ewald/pme_gpu_internal.h: * Frees the buffers on the GPU for the atoms spline data.
src/gromacs/ewald/pme_gpu_internal.h: * \param[in] pmeGpu            The PME GPU structure.
src/gromacs/ewald/pme_gpu_internal.h:void pme_gpu_free_spline_data(PmeGpu* pmeGpu);
src/gromacs/ewald/pme_gpu_internal.h: * Reallocates the buffers on the GPU and the host for the particle gridline indices.
src/gromacs/ewald/pme_gpu_internal.h: * \param[in,out] pmeGpu            The PME GPU structure.
src/gromacs/ewald/pme_gpu_internal.h:void pme_gpu_realloc_grid_indices(PmeGpu* pmeGpu);
src/gromacs/ewald/pme_gpu_internal.h: * Frees the buffer on the GPU for the particle gridline indices.
src/gromacs/ewald/pme_gpu_internal.h: * \param[in] pmeGpu            The PME GPU structure.
src/gromacs/ewald/pme_gpu_internal.h:void pme_gpu_free_grid_indices(PmeGpu* pmeGpu);
src/gromacs/ewald/pme_gpu_internal.h: * Reallocates the real space grid and the complex reciprocal grid (if needed) on the GPU.
src/gromacs/ewald/pme_gpu_internal.h: * \param[in] pmeGpu            The PME GPU structure.
src/gromacs/ewald/pme_gpu_internal.h:void pme_gpu_realloc_grids(PmeGpu* pmeGpu);
src/gromacs/ewald/pme_gpu_internal.h: * Frees the real space grid and the complex reciprocal grid (if needed) on the GPU.
src/gromacs/ewald/pme_gpu_internal.h: * \param[in] pmeGpu            The PME GPU structure.
src/gromacs/ewald/pme_gpu_internal.h:void pme_gpu_free_grids(const PmeGpu* pmeGpu);
src/gromacs/ewald/pme_gpu_internal.h: * \param[in] pmeGpu            The PME GPU structure.
src/gromacs/ewald/pme_gpu_internal.h:void pme_gpu_reinit_haloexchange(PmeGpu* pmeGpu);
src/gromacs/ewald/pme_gpu_internal.h: * \param[in] pmeGpu            The PME GPU structure.
src/gromacs/ewald/pme_gpu_internal.h:void pme_gpu_free_haloexchange(const PmeGpu* pmeGpu);
src/gromacs/ewald/pme_gpu_internal.h: * Clears the real space grid on the GPU.
src/gromacs/ewald/pme_gpu_internal.h: * \param[in] pmeGpu            The PME GPU structure.
src/gromacs/ewald/pme_gpu_internal.h:void pme_gpu_clear_grids(const PmeGpu* pmeGpu);
src/gromacs/ewald/pme_gpu_internal.h: * Reallocates and copies the pre-computed fractional coordinates' shifts to the GPU.
src/gromacs/ewald/pme_gpu_internal.h: * \param[in] pmeGpu            The PME GPU structure.
src/gromacs/ewald/pme_gpu_internal.h:void pme_gpu_realloc_and_copy_fract_shifts(PmeGpu* pmeGpu);
src/gromacs/ewald/pme_gpu_internal.h: * Frees the pre-computed fractional coordinates' shifts on the GPU.
src/gromacs/ewald/pme_gpu_internal.h: * \param[in] pmeGpu            The PME GPU structure.
src/gromacs/ewald/pme_gpu_internal.h:void pme_gpu_free_fract_shifts(const PmeGpu* pmeGpu);
src/gromacs/ewald/pme_gpu_internal.h: * Copies the input real-space grid from the host to the GPU.
src/gromacs/ewald/pme_gpu_internal.h: * \param[in] pmeGpu    The PME GPU structure.
src/gromacs/ewald/pme_gpu_internal.h:void pme_gpu_copy_input_gather_grid(const PmeGpu* pmeGpu, const float* h_grid, int gridIndex = 0);
src/gromacs/ewald/pme_gpu_internal.h: * Copies the output real-space grid from the GPU to the host.
src/gromacs/ewald/pme_gpu_internal.h: * \param[in] pmeGpu    The PME GPU structure.
src/gromacs/ewald/pme_gpu_internal.h:void pme_gpu_copy_output_spread_grid(const PmeGpu* pmeGpu, float* h_grid, int gridIndex = 0);
src/gromacs/ewald/pme_gpu_internal.h: * Copies the spread output spline data and gridline indices from the GPU to the host.
src/gromacs/ewald/pme_gpu_internal.h: * \param[in] pmeGpu    The PME GPU structure.
src/gromacs/ewald/pme_gpu_internal.h:void pme_gpu_copy_output_spread_atom_data(PmeGpu* pmeGpu);
src/gromacs/ewald/pme_gpu_internal.h: * Copies the gather input spline data and gridline indices from the host to the GPU.
src/gromacs/ewald/pme_gpu_internal.h: * \param[in] pmeGpu    The PME GPU structure.
src/gromacs/ewald/pme_gpu_internal.h:void pme_gpu_copy_input_gather_atom_data(const PmeGpu* pmeGpu);
src/gromacs/ewald/pme_gpu_internal.h: * \param[in] pmeGpu  The PME GPU structure.
src/gromacs/ewald/pme_gpu_internal.h:void pme_gpu_sync_spread_grid(const PmeGpu* pmeGpu);
src/gromacs/ewald/pme_gpu_internal.h: * Initializes the CUDA FFT structures.
src/gromacs/ewald/pme_gpu_internal.h: * \param[in] pmeGpu  The PME GPU structure.
src/gromacs/ewald/pme_gpu_internal.h:void pme_gpu_reinit_3dfft(const PmeGpu* pmeGpu);
src/gromacs/ewald/pme_gpu_internal.h: * Destroys the CUDA FFT structures.
src/gromacs/ewald/pme_gpu_internal.h: * \param[in] pmeGpu  The PME GPU structure.
src/gromacs/ewald/pme_gpu_internal.h:void pme_gpu_destroy_3dfft(const PmeGpu* pmeGpu);
src/gromacs/ewald/pme_gpu_internal.h: * A GPU spline computation and charge spreading function.
src/gromacs/ewald/pme_gpu_internal.h: * \param[in]  pmeGpu                    The PME GPU structure.
src/gromacs/ewald/pme_gpu_internal.h: * \param[in]  useGpuDirectComm          Whether direct GPU PME-PP communication is active
src/gromacs/ewald/pme_gpu_internal.h: * \param[in]  pmeCoordinateReceiverGpu  Coordinate receiver object, which must be valid when
src/gromacs/ewald/pme_gpu_internal.h: *                                       direct GPU PME-PP communication is active
src/gromacs/ewald/pme_gpu_internal.h: * \param[in]  useMdGpuGraph             Whether MD GPU Graph is in use.
src/gromacs/ewald/pme_gpu_internal.h:GPU_FUNC_QUALIFIER void pme_gpu_spread(PmeGpu*               GPU_FUNC_ARGUMENT(pmeGpu),
src/gromacs/ewald/pme_gpu_internal.h:                                       GpuEventSynchronizer* GPU_FUNC_ARGUMENT(xReadyOnDevice),
src/gromacs/ewald/pme_gpu_internal.h:                                       gmx::ArrayRef<PmeAndFftGrids> GPU_FUNC_ARGUMENT(h_grids),
src/gromacs/ewald/pme_gpu_internal.h:                                       bool GPU_FUNC_ARGUMENT(computeSplines),
src/gromacs/ewald/pme_gpu_internal.h:                                       bool GPU_FUNC_ARGUMENT(spreadCharges),
src/gromacs/ewald/pme_gpu_internal.h:                                       real GPU_FUNC_ARGUMENT(lambda),
src/gromacs/ewald/pme_gpu_internal.h:                                       bool GPU_FUNC_ARGUMENT(useGpuDirectComm),
src/gromacs/ewald/pme_gpu_internal.h:                                       gmx::PmeCoordinateReceiverGpu* GPU_FUNC_ARGUMENT(pmeCoordinateReceiverGpu),
src/gromacs/ewald/pme_gpu_internal.h:                                       bool           GPU_FUNC_ARGUMENT(useMdGpuGraph),
src/gromacs/ewald/pme_gpu_internal.h:                                       gmx_wallcycle* GPU_FUNC_ARGUMENT(wcycle)) GPU_FUNC_TERM;
src/gromacs/ewald/pme_gpu_internal.h: * \param[in]  pmeGpu          The PME GPU structure.
src/gromacs/ewald/pme_gpu_internal.h:void pme_gpu_3dfft(const PmeGpu* pmeGpu, enum gmx_fft_direction direction, int gridIndex = 0);
src/gromacs/ewald/pme_gpu_internal.h: * A GPU Fourier space solving function.
src/gromacs/ewald/pme_gpu_internal.h: * \param[in]     pmeGpu                  The PME GPU structure.
src/gromacs/ewald/pme_gpu_internal.h:GPU_FUNC_QUALIFIER void pme_gpu_solve(PmeGpu*      GPU_FUNC_ARGUMENT(pmeGpu),
src/gromacs/ewald/pme_gpu_internal.h:                                      int          GPU_FUNC_ARGUMENT(gridIndex),
src/gromacs/ewald/pme_gpu_internal.h:                                      t_complex*   GPU_FUNC_ARGUMENT(h_grid),
src/gromacs/ewald/pme_gpu_internal.h:                                      GridOrdering GPU_FUNC_ARGUMENT(gridOrdering),
src/gromacs/ewald/pme_gpu_internal.h:                                      bool GPU_FUNC_ARGUMENT(computeEnergyAndVirial)) GPU_FUNC_TERM;
src/gromacs/ewald/pme_gpu_internal.h: * A GPU force gathering function.
src/gromacs/ewald/pme_gpu_internal.h: * \param[in] pmeGpu   The PME GPU structure.
src/gromacs/ewald/pme_gpu_internal.h:GPU_FUNC_QUALIFIER void pme_gpu_gather(PmeGpu*                       GPU_FUNC_ARGUMENT(pmeGpu),
src/gromacs/ewald/pme_gpu_internal.h:                                       gmx::ArrayRef<PmeAndFftGrids> GPU_FUNC_ARGUMENT(h_grids),
src/gromacs/ewald/pme_gpu_internal.h:                                       float                         GPU_FUNC_ARGUMENT(lambda),
src/gromacs/ewald/pme_gpu_internal.h:                                       gmx_wallcycle*                GPU_FUNC_ARGUMENT(wcycle),
src/gromacs/ewald/pme_gpu_internal.h:                                       bool GPU_FUNC_ARGUMENT(computeVirial)) GPU_FUNC_TERM;
src/gromacs/ewald/pme_gpu_internal.h: * \param[in] pmeGpu         The PME GPU structure.
src/gromacs/ewald/pme_gpu_internal.h:GPU_FUNC_QUALIFIER void pme_gpu_set_kernelparam_coordinates(const PmeGpu* GPU_FUNC_ARGUMENT(pmeGpu),
src/gromacs/ewald/pme_gpu_internal.h:                                                            DeviceBuffer<gmx::RVec> GPU_FUNC_ARGUMENT(d_x)) GPU_FUNC_TERM;
src/gromacs/ewald/pme_gpu_internal.h: * \param[in] pmeGpu         The PME GPU structure.
src/gromacs/ewald/pme_gpu_internal.h:GPU_FUNC_QUALIFIER DeviceBuffer<gmx::RVec> pme_gpu_get_kernelparam_forces(const PmeGpu* GPU_FUNC_ARGUMENT(pmeGpu))
src/gromacs/ewald/pme_gpu_internal.h:        GPU_FUNC_TERM_WITH_RETURN(DeviceBuffer<gmx::RVec>{});
src/gromacs/ewald/pme_gpu_internal.h:GPU_FUNC_QUALIFIER void pme_gpu_set_kernelparam_useNvshmem(const PmeGpu* GPU_FUNC_ARGUMENT(pmeGpu),
src/gromacs/ewald/pme_gpu_internal.h:                                                           bool GPU_FUNC_ARGUMENT(useNvshmem)) GPU_FUNC_TERM;
src/gromacs/ewald/pme_gpu_internal.h: * \param[in] pmeGpu         The PME GPU structure.
src/gromacs/ewald/pme_gpu_internal.h:GPU_FUNC_QUALIFIER GpuEventSynchronizer* pme_gpu_get_forces_ready_synchronizer(
src/gromacs/ewald/pme_gpu_internal.h:        const PmeGpu* GPU_FUNC_ARGUMENT(pmeGpu)) GPU_FUNC_TERM_WITH_RETURN(nullptr);
src/gromacs/ewald/pme_gpu_internal.h: * Returns the PME GPU settings
src/gromacs/ewald/pme_gpu_internal.h: * \param[in] pmeGpu         The PME GPU structure.
src/gromacs/ewald/pme_gpu_internal.h: * \returns                  The settings for PME on GPU
src/gromacs/ewald/pme_gpu_internal.h:inline const PmeGpuSettings& pme_gpu_settings(const PmeGpu* pmeGpu)
src/gromacs/ewald/pme_gpu_internal.h:    return pmeGpu->settings;
src/gromacs/ewald/pme_gpu_internal.h: * Returns the PME GPU staging object
src/gromacs/ewald/pme_gpu_internal.h: * \param[in] pmeGpu         The PME GPU structure.
src/gromacs/ewald/pme_gpu_internal.h: * \returns                  The staging object for PME on GPU
src/gromacs/ewald/pme_gpu_internal.h:inline PmeGpuStaging& pme_gpu_staging(PmeGpu* pmeGpu)
src/gromacs/ewald/pme_gpu_internal.h:    return pmeGpu->staging;
src/gromacs/ewald/pme_gpu_internal.h: * \param[in] pmeGpu         The PME GPU structure.
src/gromacs/ewald/pme_gpu_internal.h:inline void pme_gpu_set_testing(PmeGpu* pmeGpu, bool testing)
src/gromacs/ewald/pme_gpu_internal.h:    if (pmeGpu)
src/gromacs/ewald/pme_gpu_internal.h:        pmeGpu->settings.copyAllOutputs = testing;
src/gromacs/ewald/pme_gpu_internal.h:        pmeGpu->settings.transferKind = testing ? GpuApiCallBehavior::Sync : GpuApiCallBehavior::Async;
src/gromacs/ewald/pme_gpu_internal.h:/* A block of C++ functions that live in pme_gpu_internal.cpp */
src/gromacs/ewald/pme_gpu_internal.h: * Returns the energy and virial GPU outputs, useful for testing.
src/gromacs/ewald/pme_gpu_internal.h: * It is the caller's responsibility to be aware of whether the GPU
src/gromacs/ewald/pme_gpu_internal.h:GPU_FUNC_QUALIFIER void pme_gpu_getEnergyAndVirial(const gmx_pme_t& GPU_FUNC_ARGUMENT(pme),
src/gromacs/ewald/pme_gpu_internal.h:                                                   float            GPU_FUNC_ARGUMENT(lambda),
src/gromacs/ewald/pme_gpu_internal.h:                                                   PmeOutput* GPU_FUNC_ARGUMENT(output)) GPU_FUNC_TERM;
src/gromacs/ewald/pme_gpu_internal.h: * Returns the GPU outputs (forces, energy and virial)
src/gromacs/ewald/pme_gpu_internal.h:GPU_FUNC_QUALIFIER PmeOutput pme_gpu_getOutput(gmx_pme_t* GPU_FUNC_ARGUMENT(pme),
src/gromacs/ewald/pme_gpu_internal.h:                                               bool       GPU_FUNC_ARGUMENT(computeEnergyAndVirial),
src/gromacs/ewald/pme_gpu_internal.h:                                               real       GPU_FUNC_ARGUMENT(lambdaQ))
src/gromacs/ewald/pme_gpu_internal.h:        GPU_FUNC_TERM_WITH_RETURN(PmeOutput{});
src/gromacs/ewald/pme_gpu_internal.h: * Updates the unit cell parameters. Does not check if update is necessary - that is done in pme_gpu_prepare_computation().
src/gromacs/ewald/pme_gpu_internal.h: * \param[in] pmeGpu         The PME GPU structure.
src/gromacs/ewald/pme_gpu_internal.h:GPU_FUNC_QUALIFIER void pme_gpu_update_input_box(PmeGpu*      GPU_FUNC_ARGUMENT(pmeGpu),
src/gromacs/ewald/pme_gpu_internal.h:                                                 const matrix GPU_FUNC_ARGUMENT(box)) GPU_FUNC_TERM;
src/gromacs/ewald/pme_gpu_internal.h: * Get the normal/padded grid dimensions of the real-space PME grid on GPU. Only used in tests.
src/gromacs/ewald/pme_gpu_internal.h: * \param[in] pmeGpu             The PME GPU structure.
src/gromacs/ewald/pme_gpu_internal.h:GPU_FUNC_QUALIFIER void pme_gpu_get_real_grid_sizes(const PmeGpu* GPU_FUNC_ARGUMENT(pmeGpu),
src/gromacs/ewald/pme_gpu_internal.h:                                                    gmx::IVec*    GPU_FUNC_ARGUMENT(gridSize),
src/gromacs/ewald/pme_gpu_internal.h:                                                    gmx::IVec* GPU_FUNC_ARGUMENT(paddedGridSize)) GPU_FUNC_TERM;
src/gromacs/ewald/pme_gpu_internal.h: * (Re-)initializes the PME GPU data at the beginning of the run or on DLB.
src/gromacs/ewald/pme_gpu_internal.h: * \param[in]     deviceContext     The GPU context.
src/gromacs/ewald/pme_gpu_internal.h: * \param[in]     deviceStream      The GPU stream.
src/gromacs/ewald/pme_gpu_internal.h: * \param[in,out] pmeGpuProgram     The handle to the program/kernel data created outside (e.g. in unit tests/runner)
src/gromacs/ewald/pme_gpu_internal.h: * \param[in]     useMdGpuGraph     Whether MD GPU Graph is in use
src/gromacs/ewald/pme_gpu_internal.h: * \throws gmx::NotImplementedError if this generally valid PME structure is not valid for GPU runs.
src/gromacs/ewald/pme_gpu_internal.h:GPU_FUNC_QUALIFIER void pme_gpu_reinit(gmx_pme_t*           GPU_FUNC_ARGUMENT(pme),
src/gromacs/ewald/pme_gpu_internal.h:                                       const DeviceContext* GPU_FUNC_ARGUMENT(deviceContext),
src/gromacs/ewald/pme_gpu_internal.h:                                       const DeviceStream*  GPU_FUNC_ARGUMENT(deviceStream),
src/gromacs/ewald/pme_gpu_internal.h:                                       const PmeGpuProgram* GPU_FUNC_ARGUMENT(pmeGpuProgram),
src/gromacs/ewald/pme_gpu_internal.h:                                       bool GPU_FUNC_ARGUMENT(useMdGpuGraph)) GPU_FUNC_TERM;
src/gromacs/ewald/pme_gpu_internal.h: * Destroys the PME GPU data at the end of the run.
src/gromacs/ewald/pme_gpu_internal.h: * \param[in] pmeGpu     The PME GPU structure.
src/gromacs/ewald/pme_gpu_internal.h:GPU_FUNC_QUALIFIER void pme_gpu_destroy(PmeGpu* GPU_FUNC_ARGUMENT(pmeGpu)) GPU_FUNC_TERM;
src/gromacs/ewald/pme_gpu_internal.h: * Reallocates the local atoms data (charges, coordinates, etc.). Copies the charges to the GPU.
src/gromacs/ewald/pme_gpu_internal.h: * \param[in] pmeGpu    The PME GPU structure.
src/gromacs/ewald/pme_gpu_internal.h: * decomposition. Should be called before the pme_gpu_set_io_ranges.
src/gromacs/ewald/pme_gpu_internal.h:GPU_FUNC_QUALIFIER void pme_gpu_reinit_atoms(PmeGpu*     GPU_FUNC_ARGUMENT(pmeGpu),
src/gromacs/ewald/pme_gpu_internal.h:                                             int         GPU_FUNC_ARGUMENT(nAtoms),
src/gromacs/ewald/pme_gpu_internal.h:                                             const real* GPU_FUNC_ARGUMENT(chargesA),
src/gromacs/ewald/pme_gpu_internal.h:                                             const real* GPU_FUNC_ARGUMENT(chargesB) = nullptr) GPU_FUNC_TERM;
src/gromacs/ewald/pme_gpu_internal.h: * The PME GPU reinitialization function that is called both at the end of any PME computation and on any load balancing.
src/gromacs/ewald/pme_gpu_internal.h: * \param[in] pmeGpu            The PME GPU structure.
src/gromacs/ewald/pme_gpu_internal.h:void pme_gpu_reinit_computation(const PmeGpu* pmeGpu);
src/gromacs/ewald/pme_gpu_internal.h: * Blocks until PME GPU tasks are completed, and gets the output forces and virial/energy
src/gromacs/ewald/pme_gpu_internal.h:GPU_FUNC_QUALIFIER PmeOutput pme_gpu_wait_finish_task(gmx_pme_t* GPU_FUNC_ARGUMENT(pme),
src/gromacs/ewald/pme_gpu_internal.h:                                                      bool GPU_FUNC_ARGUMENT(computeEnergyAndVirial),
src/gromacs/ewald/pme_gpu_internal.h:                                                      real           GPU_FUNC_ARGUMENT(lambdaQ),
src/gromacs/ewald/pme_gpu_internal.h:                                                      gmx_wallcycle* GPU_FUNC_ARGUMENT(wcycle))
src/gromacs/ewald/pme_gpu_internal.h:        GPU_FUNC_TERM_WITH_RETURN(PmeOutput{});
src/gromacs/ewald/pme_spread_sycl.h: *  \brief Implements PME GPU spline calculation and charge spreading in SYCL.
src/gromacs/ewald/pme_spread_sycl.h:#include "gromacs/gpu_utils/syclutils.h"
src/gromacs/ewald/pme_spread_sycl.h:#include "pme_gpu_types_host.h"
src/gromacs/ewald/pme_spread_sycl.h:struct PmeGpuGridParams;
src/gromacs/ewald/pme_spread_sycl.h:struct PmeGpuAtomParams;
src/gromacs/ewald/pme_spread_sycl.h:struct PmeGpuDynamicParams;
src/gromacs/ewald/pme_spread_sycl.h:struct PmeGpuPipeliningParams
src/gromacs/ewald/pme_spread_sycl.h:    PmeGpuGridParams*      gridParams_;
src/gromacs/ewald/pme_spread_sycl.h:    PmeGpuAtomParams*      atomParams_;
src/gromacs/ewald/pme_spread_sycl.h:    PmeGpuDynamicParams*   dynamicParams_;
src/gromacs/ewald/pme_spread_sycl.h:    PmeGpuPipeliningParams pipeliningParams_;
src/gromacs/ewald/pme_gpu_calculate_splines.h:#ifndef GMX_EWALD_PME_GPU_UTILS_H
src/gromacs/ewald/pme_gpu_calculate_splines.h:#define GMX_EWALD_PME_GPU_UTILS_H
src/gromacs/ewald/pme_gpu_calculate_splines.h: * \brief This file defines small PME GPU inline host/device functions.
src/gromacs/ewald/pme_gpu_calculate_splines.h: * Note that OpenCL device-side functions can't use C++ features, so they are
src/gromacs/ewald/pme_gpu_calculate_splines.h: * located in a similar file pme_gpu_utils.clh.
src/gromacs/ewald/pme_gpu_calculate_splines.h:struct PmeGpu;
src/gromacs/ewald/pme_gpu_calculate_splines.h: * which is laid out for GPU spread/gather kernels. The base only corresponds to the atom index within the execution block.
src/gromacs/ewald/pme_gpu_calculate_splines.h: * \returns Index into theta or dtheta array using GPU layout.
src/gromacs/ewald/pme_gpu_calculate_splines.h: * which is laid out for GPU spread/gather kernels. The index is wrt to the execution block,
src/gromacs/ewald/pme_gpu_calculate_splines.h: * \returns Index into theta or dtheta array using GPU layout.
src/gromacs/ewald/pme_solve.cu: *  \brief Implements PME GPU Fourier grid solving in CUDA.
src/gromacs/ewald/pme_solve.cu:#include "gromacs/gpu_utils/cuda_arch_utils.cuh"
src/gromacs/ewald/pme_solve.cu: * \param[in]  kernelParams             Input PME CUDA data in constant memory.
src/gromacs/ewald/pme_solve.cu:        void pme_solve_kernel(const struct PmeGpuCudaKernelParams kernelParams)
src/gromacs/ewald/pme_solve.cu:            float denom = m2k * float(CUDART_PI_F) * kernelParams.current.boxVolume
src/gromacs/ewald/pme_solve.cu:template __global__ void pme_solve_kernel<GridOrdering::YZX, true, 0>(const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_solve.cu:template __global__ void pme_solve_kernel<GridOrdering::YZX, false, 0>(const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_solve.cu:template __global__ void pme_solve_kernel<GridOrdering::XYZ, true, 0>(const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_solve.cu:template __global__ void pme_solve_kernel<GridOrdering::XYZ, false, 0>(const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_solve.cu:template __global__ void pme_solve_kernel<GridOrdering::YZX, true, 1>(const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_solve.cu:template __global__ void pme_solve_kernel<GridOrdering::YZX, false, 1>(const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_solve.cu:template __global__ void pme_solve_kernel<GridOrdering::XYZ, true, 1>(const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_solve.cu:template __global__ void pme_solve_kernel<GridOrdering::XYZ, false, 1>(const PmeGpuCudaKernelParams);
src/gromacs/ewald/pme_coordinate_receiver_gpu.h: * \brief Declaration of class which receives coordinates to GPU memory on PME task
src/gromacs/ewald/pme_coordinate_receiver_gpu.h: * \author Alan Gray <alang@nvidia.com>
src/gromacs/ewald/pme_coordinate_receiver_gpu.h:#ifndef GMX_PMECOORDINATERECEIVERGPU_H
src/gromacs/ewald/pme_coordinate_receiver_gpu.h:#define GMX_PMECOORDINATERECEIVERGPU_H
src/gromacs/ewald/pme_coordinate_receiver_gpu.h:#include "gromacs/gpu_utils/devicebuffer_datatype.h"
src/gromacs/ewald/pme_coordinate_receiver_gpu.h:class GpuEventSynchronizer;
src/gromacs/ewald/pme_coordinate_receiver_gpu.h:class PmeCoordinateReceiverGpu
src/gromacs/ewald/pme_coordinate_receiver_gpu.h:    /*! \brief Creates PME GPU coordinate receiver object
src/gromacs/ewald/pme_coordinate_receiver_gpu.h:     * For multi-GPU runs, the PME GPU can receive coordinates from
src/gromacs/ewald/pme_coordinate_receiver_gpu.h:     * multiple PP GPUs. Data from these distinct communications can
src/gromacs/ewald/pme_coordinate_receiver_gpu.h:     * \param[in] deviceContext   GPU context
src/gromacs/ewald/pme_coordinate_receiver_gpu.h:    PmeCoordinateReceiverGpu(MPI_Comm comm, const DeviceContext& deviceContext, gmx::ArrayRef<PpRanks> ppRanks);
src/gromacs/ewald/pme_coordinate_receiver_gpu.h:    ~PmeCoordinateReceiverGpu();
src/gromacs/ewald/pme_coordinate_receiver_gpu.h:     * \param[in] d_x   coordinates buffer in GPU memory
src/gromacs/ewald/pme_coordinate_receiver_gpu.h:     * \param[in] recvbuf      coordinates buffer in GPU memory
src/gromacs/ewald/pme_coordinate_receiver_gpu.h:    void launchReceiveCoordinatesFromPpGpuAwareMpi(DeviceBuffer<RVec> recvbuf,
src/gromacs/ewald/pme_coordinate_receiver_gpu.h:    std::tuple<int, GpuEventSynchronizer*> receivePpCoordinateSendEvent(int pipelineStage);
src/gromacs/ewald/pme_only.cpp:#include "gromacs/ewald/pme_coordinate_receiver_gpu.h"
src/gromacs/ewald/pme_only.cpp:#include "gromacs/ewald/pme_force_sender_gpu.h"
src/gromacs/ewald/pme_only.cpp:#include "gromacs/gpu_utils/device_stream_manager.h"
src/gromacs/ewald/pme_only.cpp:#include "gromacs/gpu_utils/hostallocator.h"
src/gromacs/ewald/pme_only.cpp:#include "gromacs/mdtypes/state_propagator_data_gpu.h"
src/gromacs/ewald/pme_only.cpp:#include "pme_gpu_internal.h"
src/gromacs/ewald/pme_only.cpp:    /*! \brief object for receiving coordinates using communications operating on GPU memory space */
src/gromacs/ewald/pme_only.cpp:    std::unique_ptr<gmx::PmeCoordinateReceiverGpu> pmeCoordinateReceiverGpu;
src/gromacs/ewald/pme_only.cpp:    /*! \brief object for sending PME force using communications operating on GPU memory space */
src/gromacs/ewald/pme_only.cpp:    std::unique_ptr<gmx::PmeForceSenderGpu> pmeForceSenderGpu;
src/gromacs/ewald/pme_only.cpp:    /*! \brief whether GPU direct communications are active for PME-PP transfers */
src/gromacs/ewald/pme_only.cpp:    bool useGpuDirectComm = false;
src/gromacs/ewald/pme_only.cpp:    /*! \brief whether GPU direct communications should send forces directly to remote GPU memory */
src/gromacs/ewald/pme_only.cpp:    bool sendForcesDirectToPpGpu = false;
src/gromacs/ewald/pme_only.cpp:    /*! \brief Whether a GPU graph should be used to execute steps in the MD loop if run conditions allow */
src/gromacs/ewald/pme_only.cpp:    bool useMdGpuGraph = false;
src/gromacs/ewald/pme_only.cpp:    /*! \brief Whether a NVSHMEM should be used for GPU communication if run conditions allow */
src/gromacs/ewald/pme_only.cpp:                                   bool                      useGpuForPme)
src/gromacs/ewald/pme_only.cpp:    if (useGpuForPme)
src/gromacs/ewald/pme_only.cpp:        resetGpuProfiler();
src/gromacs/ewald/pme_only.cpp:             * However, in the GPU case, we have to reinitialize it - there's only one GPU structure.
src/gromacs/ewald/pme_only.cpp:             * This should not cause actual GPU reallocations, at least (the allocated buffers are never shrunk).
src/gromacs/ewald/pme_only.cpp:             * So, just some grid size updates in the GPU kernel parameters.
src/gromacs/ewald/pme_only.cpp: * Note that with GPU direct communication the transfer is only initiated, it is the responsibility
src/gromacs/ewald/pme_only.cpp: * \param[in]  useGpuForPme           Flag on whether PME is on GPU.
src/gromacs/ewald/pme_only.cpp: * \param[in]  stateGpu               GPU state propagator object.
src/gromacs/ewald/pme_only.cpp:                                      bool                         useGpuForPme,
src/gromacs/ewald/pme_only.cpp:                                      gmx::StatePropagatorDataGpu* stateGpu,
src/gromacs/ewald/pme_only.cpp:        pme_pp->useGpuDirectComm = ((cnb.flags & PP_PME_GPUCOMMS) != 0);
src/gromacs/ewald/pme_only.cpp:        GMX_ASSERT(!pme_pp->useGpuDirectComm || (pme_pp->pmeForceSenderGpu != nullptr),
src/gromacs/ewald/pme_only.cpp:                   "The use of GPU direct communication for PME-PP is enabled, "
src/gromacs/ewald/pme_only.cpp:                   "but the PME GPU force reciever object does not exist");
src/gromacs/ewald/pme_only.cpp:        pme_pp->sendForcesDirectToPpGpu = ((cnb.flags & PP_PME_RECVFTOGPU) != 0);
src/gromacs/ewald/pme_only.cpp:        pme_pp->useMdGpuGraph = ((cnb.flags & PP_PME_MDGPUGRAPH) != 0);
src/gromacs/ewald/pme_only.cpp:                if (useGpuForPme)
src/gromacs/ewald/pme_only.cpp:                    stateGpu->reinit(nat, nat, cr, pme_pp->peerRankId);
src/gromacs/ewald/pme_only.cpp:                    pme_gpu_set_device_x(pme, stateGpu->getCoordinates());
src/gromacs/ewald/pme_only.cpp:                if (pme_pp->useGpuDirectComm)
src/gromacs/ewald/pme_only.cpp:                    GMX_ASSERT((runMode == PmeRunMode::GPU || runMode == PmeRunMode::Mixed),
src/gromacs/ewald/pme_only.cpp:                               "GPU Direct PME-PP communication has been enabled, "
src/gromacs/ewald/pme_only.cpp:                    pme_pp->pmeCoordinateReceiverGpu->reinitCoordinateReceiver(stateGpu->getCoordinates());
src/gromacs/ewald/pme_only.cpp:                    pme_pp->pmeForceSenderGpu->setForceSendBuffer(pme_gpu_get_device_f(pme));
src/gromacs/ewald/pme_only.cpp:                    if (pme_pp->useGpuDirectComm)
src/gromacs/ewald/pme_only.cpp:                            pme_pp->pmeCoordinateReceiverGpu->receiveCoordinatesSynchronizerFromPpPeerToPeer(
src/gromacs/ewald/pme_only.cpp:                            pme_pp->pmeCoordinateReceiverGpu->launchReceiveCoordinatesFromPpGpuAwareMpi(
src/gromacs/ewald/pme_only.cpp:                                    stateGpu->getCoordinates(),
src/gromacs/ewald/pme_only.cpp:    GMX_UNUSED_VALUE(useGpuForPme);
src/gromacs/ewald/pme_only.cpp:    GMX_UNUSED_VALUE(stateGpu);
src/gromacs/ewald/pme_only.cpp:    if (pme_pp->useGpuDirectComm)
src/gromacs/ewald/pme_only.cpp:        GMX_ASSERT((pme_pp->pmeForceSenderGpu != nullptr),
src/gromacs/ewald/pme_only.cpp:                   "The use of GPU direct communication for PME-PP is enabled, "
src/gromacs/ewald/pme_only.cpp:                   "but the PME GPU force reciever object does not exist");
src/gromacs/ewald/pme_only.cpp:    if (pme_pp->useGpuDirectComm && GMX_THREAD_MPI)
src/gromacs/ewald/pme_only.cpp:            pme_pp->pmeForceSenderGpu->sendFToPpPeerToPeer(
src/gromacs/ewald/pme_only.cpp:                    receiver.rankId, receiver.numAtoms, pme_pp->sendForcesDirectToPpGpu);
src/gromacs/ewald/pme_only.cpp:            pme_pp->pmeForceSenderGpu->waitForEvents();
src/gromacs/ewald/pme_only.cpp:                if (pme_pp->useGpuDirectComm)
src/gromacs/ewald/pme_only.cpp:                    pme_pp->pmeForceSenderGpu->sendFToPpGpuAwareMpi(pme_gpu_get_device_f(&pme),
src/gromacs/ewald/pme_only.cpp:                bool                            useGpuPmePpCommunication,
src/gromacs/ewald/pme_only.cpp:    std::unique_ptr<gmx::StatePropagatorDataGpu> stateGpu;
src/gromacs/ewald/pme_only.cpp:    const bool useGpuForPme = (runMode == PmeRunMode::GPU) || (runMode == PmeRunMode::Mixed);
src/gromacs/ewald/pme_only.cpp:    if (useGpuForPme)
src/gromacs/ewald/pme_only.cpp:                "Device stream manager can not be nullptr when using GPU in PME-only rank.");
src/gromacs/ewald/pme_only.cpp:                           "Device stream can not be nullptr when using GPU in PME-only rank");
src/gromacs/ewald/pme_only.cpp:        if (useGpuPmePpCommunication)
src/gromacs/ewald/pme_only.cpp:            pme_pp->pmeCoordinateReceiverGpu = std::make_unique<gmx::PmeCoordinateReceiverGpu>(
src/gromacs/ewald/pme_only.cpp:            pme_pp->pmeForceSenderGpu = std::make_unique<gmx::PmeForceSenderGpu>(
src/gromacs/ewald/pme_only.cpp:                    pme_gpu_get_f_ready_synchronizer(pmeFromRunner),
src/gromacs/ewald/pme_only.cpp:                pme_gpu_use_nvshmem(pmeFromRunner->gpu, useNvshmem);
src/gromacs/ewald/pme_only.cpp:                pmeFromRunner->gpu->nvshmemParams->ppRanksRef = pme_pp->ppRanks;
src/gromacs/ewald/pme_only.cpp:        stateGpu = std::make_unique<gmx::StatePropagatorDataGpu>(
src/gromacs/ewald/pme_only.cpp:                GpuApiCallBehavior::Async,
src/gromacs/ewald/pme_only.cpp:                pme_gpu_get_block_size(pmeFromRunner),
src/gromacs/ewald/pme_only.cpp:                                             useGpuForPme,
src/gromacs/ewald/pme_only.cpp:                                             stateGpu.get(),
src/gromacs/ewald/pme_only.cpp:                reset_pmeonly_counters(wcycle, walltime_accounting, mynrnb, step, useGpuForPme);
src/gromacs/ewald/pme_only.cpp:        wallcycle_start(wcycle, useGpuForPme ? WallCycleCounter::PmeGpuMesh : WallCycleCounter::PmeMesh);
src/gromacs/ewald/pme_only.cpp:        if (useGpuForPme)
src/gromacs/ewald/pme_only.cpp:            stepWork.useGpuPmeFReduction = pme_pp->useGpuDirectComm;
src/gromacs/ewald/pme_only.cpp:            pme_gpu_prepare_computation(pme, box, wcycle, stepWork);
src/gromacs/ewald/pme_only.cpp:            if (!pme_pp->useGpuDirectComm)
src/gromacs/ewald/pme_only.cpp:                stateGpu->copyCoordinatesToGpu(gmx::ArrayRef<gmx::RVec>(pme_pp->x),
src/gromacs/ewald/pme_only.cpp:            // TODO: with pme on GPU the receive should make a list of synchronizers and pass it here #3157
src/gromacs/ewald/pme_only.cpp:            pme_gpu_launch_spread(pme,
src/gromacs/ewald/pme_only.cpp:                                  pme_pp->useGpuDirectComm,
src/gromacs/ewald/pme_only.cpp:                                  pme_pp->pmeCoordinateReceiverGpu.get(),
src/gromacs/ewald/pme_only.cpp:                                  pme_pp->useMdGpuGraph);
src/gromacs/ewald/pme_only.cpp:            pme_gpu_launch_complex_transforms(pme, wcycle, stepWork);
src/gromacs/ewald/pme_only.cpp:            pme_gpu_launch_gather(pme, wcycle, lambda_q, stepWork.computeVirial);
src/gromacs/ewald/pme_only.cpp:            output = pme_gpu_wait_finish_task(pme, computeEnergyAndVirial, lambda_q, wcycle);
src/gromacs/ewald/pme_only.cpp:                wcycle, useGpuForPme ? WallCycleCounter::PmeGpuMesh : WallCycleCounter::PmeMesh);
src/gromacs/ewald/pme_only.cpp:        if (useGpuForPme && pme_pp->useMdGpuGraph)
src/gromacs/ewald/pme_only.cpp:            pme_gpu_reinit_computation(pme, pme_pp->useMdGpuGraph, wcycle);
src/gromacs/ewald/pme_only.cpp:        if (useGpuForPme && !pme_pp->useMdGpuGraph)
src/gromacs/ewald/pme_only.cpp:            pme_gpu_reinit_computation(pme, pme_pp->useMdGpuGraph, wcycle);
src/gromacs/ewald/pme.h:#include "gromacs/gpu_utils/devicebuffer_datatype.h"
src/gromacs/ewald/pme.h:#include "gromacs/gpu_utils/gpu_macros.h"
src/gromacs/ewald/pme.h:struct PmeGpu;
src/gromacs/ewald/pme.h:struct gmx_wallclock_gpu_pme_t;
src/gromacs/ewald/pme.h:enum class GpuTaskCompletion;
src/gromacs/ewald/pme.h:class PmeGpuProgram;
src/gromacs/ewald/pme.h:class GpuEventSynchronizer;
src/gromacs/ewald/pme.h:class PmeCoordinateReceiverGpu;
src/gromacs/ewald/pme.h:    GPU,   //!< Whole PME computation is done on GPU
src/gromacs/ewald/pme.h:    Mixed, //!< Mixed mode: only spread and gather run on GPU; FFT and solving are done on CPU.
src/gromacs/ewald/pme.h: * This function is only used for PME GPU decomposition case
src/gromacs/ewald/pme.h: * \returns  extended halo region used in GPU PME decomposition
src/gromacs/ewald/pme.h: * The PME GPU restrictions are checked separately during pme_gpu_init().
src/gromacs/ewald/pme.h:                                bool useGpuPme,
src/gromacs/ewald/pme.h: * \todo We should evolve something like a \c GpuManager that holds \c
src/gromacs/ewald/pme.h: * DeviceInformation* and \c PmeGpuProgram* and perhaps other
src/gromacs/ewald/pme.h:                        PmeGpu*                          pmeGpu,
src/gromacs/ewald/pme.h:                        const PmeGpuProgram*             pmeGpuProgram,
src/gromacs/ewald/pme.h:/*! \brief Destroys the PME data structure (including GPU data). */
src/gromacs/ewald/pme.h: * \param destroyGpuData  Set to \c false if \p pme is a copy created by \ref gmx_pme_reinit,
src/gromacs/ewald/pme.h: * and only it should be destroyed, while shared GPU data should be preserved. If \c true,
src/gromacs/ewald/pme.h:void gmx_pme_destroy(gmx_pme_t* pme, bool destroyGpuData);
src/gromacs/ewald/pme.h: * This function updates the local atom data on GPU after DD (charges, coordinates, etc.).
src/gromacs/ewald/pme.h: * state A. Can be nullptr if PME is not performed on the GPU.
src/gromacs/ewald/pme.h:/* A block of PME GPU functions */
src/gromacs/ewald/pme.h:/*! \brief Checks whether the GROMACS build allows to run PME on GPU.
src/gromacs/ewald/pme.h: * pme_gpu_check_restrictions(), except that works with a
src/gromacs/ewald/pme.h: * \param[out] error   If non-null, the error message when PME is not supported on GPU.
src/gromacs/ewald/pme.h: * \returns true if PME can run on GPU on this build, false otherwise.
src/gromacs/ewald/pme.h:bool pme_gpu_supports_build(std::string* error);
src/gromacs/ewald/pme.h:/*! \brief Checks whether the input system allows to run PME on GPU.
src/gromacs/ewald/pme.h: * pme_gpu_check_restrictions(), except that works with a
src/gromacs/ewald/pme.h: * \param[out] error  If non-null, the error message if the input is not supported on GPU.
src/gromacs/ewald/pme.h: * \returns true if PME can run on GPU with this input, false otherwise.
src/gromacs/ewald/pme.h:bool pme_gpu_supports_input(const t_inputrec& ir, std::string* error);
src/gromacs/ewald/pme.h:/*! \brief Checks whether the input system allows to run PME on GPU in Mixed mode.
src/gromacs/ewald/pme.h: * Assumes that the input system is compatible with GPU PME otherwise, that is,
src/gromacs/ewald/pme.h: * before calling this function one should check that \ref pme_gpu_supports_input returns \c true.
src/gromacs/ewald/pme.h: * \returns true if PME can run on GPU in Mixed mode with this input, false otherwise.
src/gromacs/ewald/pme.h:bool pme_gpu_mixed_mode_supports_input(const t_inputrec& ir, std::string* error);
src/gromacs/ewald/pme.h: * Returns the active PME codepath (CPU, GPU, mixed).
src/gromacs/ewald/pme.h: * on a GPU). */
src/gromacs/ewald/pme.h: * Tells if PME is enabled to run on GPU (not necessarily active at the moment).
src/gromacs/ewald/pme.h: * \returns true if PME can run on GPU, false otherwise.
src/gromacs/ewald/pme.h:inline bool pme_gpu_task_enabled(const gmx_pme_t* pme)
src/gromacs/ewald/pme.h: * \param[in] pmeGpu             The PME GPU structure.
src/gromacs/ewald/pme.h:GPU_FUNC_QUALIFIER void pme_gpu_use_nvshmem(PmeGpu* GPU_FUNC_ARGUMENT(pmeGpu),
src/gromacs/ewald/pme.h:                                            bool    GPU_FUNC_ARGUMENT(useNvshmem)) GPU_FUNC_TERM;
src/gromacs/ewald/pme.h: * The GPU version of PME requires that the coordinates array have a
src/gromacs/ewald/pme.h:GPU_FUNC_QUALIFIER int pme_gpu_get_block_size(const gmx_pme_t* GPU_FUNC_ARGUMENT(pme))
src/gromacs/ewald/pme.h:        GPU_FUNC_TERM_WITH_RETURN(0);
src/gromacs/ewald/pme.h:// The following functions are all the PME GPU entry points,
src/gromacs/ewald/pme.h:// currently inlining to nothing on non-CUDA builds.
src/gromacs/ewald/pme.h: * Resets the PME GPU timings. To be called at the reset step.
src/gromacs/ewald/pme.h:GPU_FUNC_QUALIFIER void pme_gpu_reset_timings(const gmx_pme_t* GPU_FUNC_ARGUMENT(pme)) GPU_FUNC_TERM;
src/gromacs/ewald/pme.h: * Copies the PME GPU timings to the gmx_wallclock_gpu_pme_t structure (for log output). To be called at the run end.
src/gromacs/ewald/pme.h: * \param[in] timings           The gmx_wallclock_gpu_pme_t structure.
src/gromacs/ewald/pme.h:GPU_FUNC_QUALIFIER void pme_gpu_get_timings(const gmx_pme_t* GPU_FUNC_ARGUMENT(pme),
src/gromacs/ewald/pme.h:                                            gmx_wallclock_gpu_pme_t* GPU_FUNC_ARGUMENT(timings)) GPU_FUNC_TERM;
src/gromacs/ewald/pme.h:/* The main PME GPU functions */
src/gromacs/ewald/pme.h: * Prepares PME on GPU computation (updating the box if needed)
src/gromacs/ewald/pme.h:GPU_FUNC_QUALIFIER void pme_gpu_prepare_computation(gmx_pme_t*     GPU_FUNC_ARGUMENT(pme),
src/gromacs/ewald/pme.h:                                                    const matrix   GPU_FUNC_ARGUMENT(box),
src/gromacs/ewald/pme.h:                                                    gmx_wallcycle* GPU_FUNC_ARGUMENT(wcycle),
src/gromacs/ewald/pme.h:                                                    const gmx::StepWorkload& GPU_FUNC_ARGUMENT(stepWork)) GPU_FUNC_TERM;
src/gromacs/ewald/pme.h: * Launches first stage of PME on GPU - spreading kernel.
src/gromacs/ewald/pme.h: * \param[in] useGpuDirectComm               Whether direct GPU PME-PP communication is active
src/gromacs/ewald/pme.h: * \param[in]  pmeCoordinateReceiverGpu      Coordinate receiver object, which must be valid when
src/gromacs/ewald/pme.h: *                                           direct GPU PME-PP communication is active
src/gromacs/ewald/pme.h: * \param[in] useMdGpuGraph                  Whether MD GPU Graph is in use.
src/gromacs/ewald/pme.h:GPU_FUNC_QUALIFIER void pme_gpu_launch_spread(gmx_pme_t* GPU_FUNC_ARGUMENT(pme),
src/gromacs/ewald/pme.h:                                              GpuEventSynchronizer* GPU_FUNC_ARGUMENT(xReadyOnDevice),
src/gromacs/ewald/pme.h:                                              gmx_wallcycle* GPU_FUNC_ARGUMENT(wcycle),
src/gromacs/ewald/pme.h:                                              real           GPU_FUNC_ARGUMENT(lambdaQ),
src/gromacs/ewald/pme.h:                                              bool           GPU_FUNC_ARGUMENT(useGpuDirectComm),
src/gromacs/ewald/pme.h:                                              gmx::PmeCoordinateReceiverGpu* GPU_FUNC_ARGUMENT(pmeCoordinateReceiverGpu),
src/gromacs/ewald/pme.h:                                              bool GPU_FUNC_ARGUMENT(useMdGpuGraph)) GPU_FUNC_TERM;
src/gromacs/ewald/pme.h: * Launches middle stages of PME (FFT R2C, solving, FFT C2R) either on GPU or on CPU, depending on the run mode.
src/gromacs/ewald/pme.h:GPU_FUNC_QUALIFIER void
src/gromacs/ewald/pme.h:pme_gpu_launch_complex_transforms(gmx_pme_t*     GPU_FUNC_ARGUMENT(pme),
src/gromacs/ewald/pme.h:                                  gmx_wallcycle* GPU_FUNC_ARGUMENT(wcycle),
src/gromacs/ewald/pme.h:                                  const gmx::StepWorkload& GPU_FUNC_ARGUMENT(stepWork)) GPU_FUNC_TERM;
src/gromacs/ewald/pme.h: * Launches last stage of PME on GPU - force gathering and D2H force transfer.
src/gromacs/ewald/pme.h:GPU_FUNC_QUALIFIER void pme_gpu_launch_gather(gmx_pme_t*     GPU_FUNC_ARGUMENT(pme),
src/gromacs/ewald/pme.h:                                              gmx_wallcycle* GPU_FUNC_ARGUMENT(wcycle),
src/gromacs/ewald/pme.h:                                              real           GPU_FUNC_ARGUMENT(lambdaQ),
src/gromacs/ewald/pme.h:                                              bool GPU_FUNC_ARGUMENT(computeVirial)) GPU_FUNC_TERM;
src/gromacs/ewald/pme.h: * Attempts to complete PME GPU tasks.
src/gromacs/ewald/pme.h: * PME GPU tasks enqueued completed (as pme_gpu_wait_finish_task() does) or only
src/gromacs/ewald/pme.h: * \returns                    True if the PME GPU tasks have completed
src/gromacs/ewald/pme.h:GPU_FUNC_QUALIFIER bool pme_gpu_try_finish_task(gmx_pme_t* GPU_FUNC_ARGUMENT(pme),
src/gromacs/ewald/pme.h:                                                const gmx::StepWorkload& GPU_FUNC_ARGUMENT(stepWork),
src/gromacs/ewald/pme.h:                                                gmx_wallcycle* GPU_FUNC_ARGUMENT(wcycle),
src/gromacs/ewald/pme.h:                                                gmx::ForceWithVirial* GPU_FUNC_ARGUMENT(forceWithVirial),
src/gromacs/ewald/pme.h:                                                gmx_enerdata_t*   GPU_FUNC_ARGUMENT(enerd),
src/gromacs/ewald/pme.h:                                                real              GPU_FUNC_ARGUMENT(lambdaQ),
src/gromacs/ewald/pme.h:                                                GpuTaskCompletion GPU_FUNC_ARGUMENT(completionKind))
src/gromacs/ewald/pme.h:        GPU_FUNC_TERM_WITH_RETURN(false);
src/gromacs/ewald/pme.h: * Blocks until PME GPU tasks are completed, and gets the output forces and virial/energy
src/gromacs/ewald/pme.h:GPU_FUNC_QUALIFIER void pme_gpu_wait_and_reduce(gmx_pme_t* GPU_FUNC_ARGUMENT(pme),
src/gromacs/ewald/pme.h:                                                const gmx::StepWorkload& GPU_FUNC_ARGUMENT(stepWork),
src/gromacs/ewald/pme.h:                                                gmx_wallcycle* GPU_FUNC_ARGUMENT(wcycle),
src/gromacs/ewald/pme.h:                                                gmx::ForceWithVirial* GPU_FUNC_ARGUMENT(forceWithVirial),
src/gromacs/ewald/pme.h:                                                gmx_enerdata_t* GPU_FUNC_ARGUMENT(enerd),
src/gromacs/ewald/pme.h:                                                real GPU_FUNC_ARGUMENT(lambdaQ)) GPU_FUNC_TERM;
src/gromacs/ewald/pme.h: * The PME GPU reinitialization function that is called both at the end of any PME computation and on any load balancing.
src/gromacs/ewald/pme.h: * \param[in] gpuGraphWithSeparatePmeRank    Whether MD GPU Graph with separate PME rank is in use.
src/gromacs/ewald/pme.h:GPU_FUNC_QUALIFIER void pme_gpu_reinit_computation(const gmx_pme_t* GPU_FUNC_ARGUMENT(pme),
src/gromacs/ewald/pme.h:                                                   bool GPU_FUNC_ARGUMENT(gpuGraphWithSeparatePmeRank),
src/gromacs/ewald/pme.h:                                                   gmx_wallcycle* GPU_FUNC_ARGUMENT(wcycle)) GPU_FUNC_TERM;
src/gromacs/ewald/pme.h:GPU_FUNC_QUALIFIER void pme_gpu_set_device_x(const gmx_pme_t* GPU_FUNC_ARGUMENT(pme),
src/gromacs/ewald/pme.h:                                             DeviceBuffer<gmx::RVec> GPU_FUNC_ARGUMENT(d_x)) GPU_FUNC_TERM;
src/gromacs/ewald/pme.h:GPU_FUNC_QUALIFIER DeviceBuffer<gmx::RVec> pme_gpu_get_device_f(const gmx_pme_t* GPU_FUNC_ARGUMENT(pme))
src/gromacs/ewald/pme.h:        GPU_FUNC_TERM_WITH_RETURN(DeviceBuffer<gmx::RVec>{});
src/gromacs/ewald/pme.h:GPU_FUNC_QUALIFIER GpuEventSynchronizer* pme_gpu_get_f_ready_synchronizer(const gmx_pme_t* GPU_FUNC_ARGUMENT(pme))
src/gromacs/ewald/pme.h:        GPU_FUNC_TERM_WITH_RETURN(nullptr);
src/gromacs/hardware/hardwaretopology.cpp:    // able to identify e.g. that the GPU is connected to package 0 while we run on package 1,
src/gromacs/hardware/hardwaretopology.cpp:    std::ifstream procMountsStream(root + "/proc/mounts");
src/gromacs/hardware/hardwaretopology.cpp:    while (!found && std::getline(procMountsStream, line))
src/gromacs/hardware/device_management.h: *  \brief Declares functions to manage GPU resources.
src/gromacs/hardware/device_management.h: *  This has several implementations: one for each supported GPU platform,
src/gromacs/hardware/device_management.h: *  and a stub implementation if the build does not support GPUs.
src/gromacs/hardware/device_management.h:enum class GpuAwareMpiStatus : int;
src/gromacs/hardware/device_management.h:/*! \brief Return whether GPUs can be detected.
src/gromacs/hardware/device_management.h: * GPU usage, GPU detection is not disabled by \c GMX_DISABLE_GPU_DETECTION
src/gromacs/hardware/device_management.h: *                           GPU support and non-nullptr was passed,
src/gromacs/hardware/device_management.h: *                           why GPUs cannot be detected.
src/gromacs/hardware/device_management.h:/*! \brief Return whether GPU detection is enabled
src/gromacs/hardware/device_management.h: * GPU usage and GPU detection is not disabled by \c GMX_DISABLE_GPU_DETECTION
src/gromacs/hardware/device_management.h:/*! \brief Return whether GPU detection is functioning correctly
src/gromacs/hardware/device_management.h: * GPU usage, and a valid device driver, ICD, and/or runtime was detected.
src/gromacs/hardware/device_management.h: * configurations that do not support GPUs, and there will be no
src/gromacs/hardware/device_management.h: *                           GPU support and non-nullptr was passed,
src/gromacs/hardware/device_management.h: *                           why GPUs cannot be detected.
src/gromacs/hardware/device_management.h:/*! \brief Returns an DeviceVendor value corresponding to the input OpenCL vendor name.
src/gromacs/hardware/device_management.h: * OpenCL and SYCL can report the number of Compute Units (CUs) a device has, see
src/gromacs/hardware/device_management.h: * On NVIDIA, that is the number of SMs.
src/gromacs/hardware/device_management.h: * On AMD, that is the number of Compute Units, which are similar to CUDA's SM.
src/gromacs/hardware/device_management.h: * On Intel, that is the number of EUs (XVEs), which are similar to CUDA core. The concept similar
src/gromacs/hardware/device_management.h: * to CUDA SM is called sub-slice (Xe Core, XC), and it contains 16 EUs (Gen9-Gen11, Xe).
src/gromacs/hardware/device_management.h: * This function uses CUDA SM as a reference. To get the number of SM-like units on a device,
src/gromacs/hardware/device_management.h:/*! \brief Find all GPUs in the system.
src/gromacs/hardware/device_management.h: *  Will detect every GPU supported by the device driver in use.
src/gromacs/hardware/device_management.h: *  Note that this function leaves the GPU runtime API error state clean;
src/gromacs/hardware/device_management.h: *  this is implemented ATM in the CUDA flavor. This invalidates any existing
src/gromacs/hardware/device_management.h: *  CUDA streams, allocated memory on GPU, etc.
src/gromacs/hardware/device_management.h: *  \todo:  Check if errors do propagate in OpenCL as they do in CUDA and
src/gromacs/hardware/device_management.h: *  \throws InternalError if a GPU API returns an unexpected failure (because
src/gromacs/hardware/device_management.h: *          the call to canDetectGpus() should always prevent this occuring)
src/gromacs/hardware/device_management.h: * GPUs, based on the previously run compatibility tests.
src/gromacs/hardware/device_management.h: * \return  Vector of DeviceInformations on GPUs recorded as compatible
src/gromacs/hardware/device_management.h:/*! \brief Return a container of the IDs of the compatible GPU ids.
src/gromacs/hardware/device_management.h: * GPUs, based on the previously run compatibility tests.
src/gromacs/hardware/device_management.h: * \return  Vector of compatible GPU ids.
src/gromacs/hardware/device_management.h: * GPUs, based on the previously run compatibility tests.
src/gromacs/hardware/device_management.h:/*! \brief Return whether all compatible devices in \p deviceInfoList support GPU-aware MPI.
src/gromacs/hardware/device_management.h: * \return  Whether all compatible devices in the list support GPU-aware MPI
src/gromacs/hardware/device_management.h:gmx::GpuAwareMpiStatus getMinimalSupportedGpuAwareMpiStatus(
src/gromacs/hardware/device_management.h:/*! \brief Set the active GPU.
src/gromacs/hardware/device_management.h: * This sets the device for which the device information is passed active. Essential in CUDA, where
src/gromacs/hardware/device_management.h: * the device buffers and kernel launches are not connected to the device context. In OpenCL, checks
src/gromacs/hardware/device_management.h:/*! \brief Releases the GPU device used by the active context at the time of calling.
src/gromacs/hardware/device_management.h: * With CUDA, the device is reset and therefore all data uploaded to
src/gromacs/hardware/device_management.h: * the GPU is lost. This must only be called when none of this data is
src/gromacs/hardware/device_management.h: * With other GPU SDKs, does nothing.
src/gromacs/hardware/device_management.h:/*! \brief Formats and returns a device information string for a given GPU.
src/gromacs/hardware/device_management.h: * Given an index *directly* into the array of available GPUs, returns
src/gromacs/hardware/device_management.h: * a formatted info string for the respective GPU which includes ID, name,
src/gromacs/hardware/device_management.h:/*! \brief Return a string describing how compatible the GPU with given \c deviceId is.
src/gromacs/hardware/device_management.h:/*! \brief Return an ID (non-negative integer) for the described GPU that may be unique.
src/gromacs/hardware/device_management.h:/*! Run a possible check that GPU-aware MPI will work on \c deviceInfo
src/gromacs/hardware/device_management.h:void doubleCheckGpuAwareMpiWillWork(const DeviceInformation& deviceInfo);
src/gromacs/hardware/device_information.h: *  \brief Declares the GPU information structure and its helpers
src/gromacs/hardware/device_information.h:#if GMX_GPU_CUDA
src/gromacs/hardware/device_information.h:#    include <cuda_runtime.h>
src/gromacs/hardware/device_information.h:#if GMX_GPU_HIP
src/gromacs/hardware/device_information.h:#if GMX_GPU_OPENCL
src/gromacs/hardware/device_information.h:#    include "gromacs/gpu_utils/gmxopencl.h"
src/gromacs/hardware/device_information.h:#if GMX_GPU_SYCL
src/gromacs/hardware/device_information.h:#    include "gromacs/gpu_utils/gmxsycl.h"
src/gromacs/hardware/device_information.h:static constexpr bool c_binarySupportsGpus = (GMX_GPU != 0);
src/gromacs/hardware/device_information.h://! Possible results of the GPU detection/check.
src/gromacs/hardware/device_information.h:    //! OpenCL device has incompatible cluster size for non-bonded kernels.
src/gromacs/hardware/device_information.h:    //! There are known issues with OpenCL on NVIDIA Volta and newer.
src/gromacs/hardware/device_information.h:    IncompatibleNvidiaVolta,
src/gromacs/hardware/device_information.h:    /*! \brief CUDA devices are busy or unavailable.
src/gromacs/hardware/device_information.h:     * typically due to use of \p cudaComputeModeExclusive, \p cudaComputeModeProhibited modes.
src/gromacs/hardware/device_information.h:     * See \c GMX_CUDA_TARGET_SM and \c GMX_CUDA_TARGET_COMPUTE CMake variables.
src/gromacs/hardware/device_information.h:    //! \brief AMD RDNA devices (gfx10xx, gfx11xx) with 32-wide execution are not supported with OpenCL,
src/gromacs/hardware/device_information.h:/*! \brief Names of the GPU detection/check results
src/gromacs/hardware/device_information.h:    "incompatible (please recompile with correct GMX" "_GPU_NB_CLUSTER_SIZE of 4)",
src/gromacs/hardware/device_information.h:    "incompatible (please use CUDA build for NVIDIA Volta GPUs or newer)",
src/gromacs/hardware/device_information.h:    //! NVIDIA
src/gromacs/hardware/device_information.h:    Nvidia = 1,
src/gromacs/hardware/device_information.h:    //! ID of the device, ie. the index into the device order reported by the GPU runtime.
src/gromacs/hardware/device_information.h:     * \ref DeviceInformation must be serializable in CUDA, so we cannot use \c std::vector here.
src/gromacs/hardware/device_information.h:    gmx::GpuAwareMpiStatus gpuAwareMpiStatus;
src/gromacs/hardware/device_information.h:#if GMX_GPU_CUDA
src/gromacs/hardware/device_information.h:    //! CUDA device properties.
src/gromacs/hardware/device_information.h:    cudaDeviceProp prop;
src/gromacs/hardware/device_information.h:#elif GMX_GPU_HIP
src/gromacs/hardware/device_information.h:#elif GMX_GPU_OPENCL
src/gromacs/hardware/device_information.h:    cl_platform_id oclPlatformId;       //!< OpenCL Platform ID.
src/gromacs/hardware/device_information.h:    cl_device_id   oclDeviceId;         //!< OpenCL Device ID.
src/gromacs/hardware/device_information.h:#elif GMX_GPU_SYCL
src/gromacs/hardware/device_information.h:    //! CUDA CC major for NVIDIA devices, generation code for AMD (gfx90a -> 9), architecture code for Intel (Gen9 -> 9, Xe -> 12)
src/gromacs/hardware/device_information.h:    //! CUDA CC minor for NVIDIA devices, major architecture(?) code for AMD (gfx90a -> 0), release code for Intel
src/gromacs/hardware/device_information.h:    //! CUDA CC minor for NVIDIA devices, device code for AMD (gfx90a -> a -> 10), revision code for Intel
src/gromacs/hardware/device_management_hip.cpp:#include "gromacs/gpu_utils/device_context.h"
src/gromacs/hardware/device_management_hip.cpp:#include "gromacs/gpu_utils/device_stream.h"
src/gromacs/hardware/device_management_hip.cpp:#include "gromacs/gpu_utils/hiputils.h"
src/gromacs/hardware/device_management_hip.cpp:                    "the GPU ID #%d (gcn: %s) detected during detection. "
src/gromacs/hardware/device_management_hip.cpp:                    "Either your GPU is not covered by this, so you'll need to add the "
src/gromacs/hardware/device_management_hip.cpp:                    "in which case you should use VkFFT instead as the GPU FFT library. ",
src/gromacs/hardware/device_management_hip.cpp:    else if (GMX_GPU_NB_DISABLE_CLUSTER_PAIR_SPLIT && deviceInfo.supportedSubGroupSizes[0] == 32)
src/gromacs/hardware/device_management_hip.cpp:/*! \brief Runs GPU compatibility and sanity checks on the indicated device.
src/gromacs/hardware/device_management_hip.cpp: * Runs a series of checks to determine that the given GPU and underlying HIP
src/gromacs/hardware/device_management_hip.cpp: *  As the error handling only permits returning the state of the GPU, this function
src/gromacs/hardware/device_management_hip.cpp:    if (hip_err == hipErrorSharedObjectInitFailed || hip_err == hipErrorNoBinaryForGpu)
src/gromacs/hardware/device_management_hip.cpp:        // launchGpuKernel error is not fatal and should continue with marking the device bad
src/gromacs/hardware/device_management_hip.cpp:            (gmx::checkMpiHipAwareSupport() == gmx::GpuAwareMpiStatus::Supported
src/gromacs/hardware/device_management_hip.cpp:             || gmx::checkMpiHipAwareSupport() == gmx::GpuAwareMpiStatus::Forced);
src/gromacs/hardware/device_management_hip.cpp:             * mismatch, all GPUs being busy in exclusive mode,
src/gromacs/hardware/device_management_hip.cpp:        // Can't detect GPUs
src/gromacs/hardware/device_management_hip.cpp:    const gmx::GpuAwareMpiStatus gpuAwareMpiStatus =
src/gromacs/hardware/device_management_hip.cpp:            GMX_LIB_MPI ? gmx::checkMpiHipAwareSupport() : gmx::GpuAwareMpiStatus::NotSupported;
src/gromacs/hardware/device_management_hip.cpp:        deviceInfoList[i]->gpuAwareMpiStatus = gpuAwareMpiStatus;
src/gromacs/hardware/device_management_hip.cpp:            //    errors that occurred during is_gmx_supported_gpu_id() here,
src/gromacs/hardware/device_management_hip.cpp:            //    but this would be more elegant done within is_gmx_supported_gpu_id()
src/gromacs/hardware/device_management_hip.cpp:        auto message = gmx::formatString("Failed to initialize GPU #%d", deviceId);
src/gromacs/hardware/device_management_hip.cpp:        fprintf(stderr, "Initialized GPU ID #%d: %s\n", deviceId, deviceInfo.prop.name);
src/gromacs/hardware/device_management_hip.cpp:    int gpuid;
src/gromacs/hardware/device_management_hip.cpp:    stat = hipGetDevice(&gpuid);
src/gromacs/hardware/device_management_hip.cpp:            fprintf(stderr, "Cleaning up context on GPU ID #%d.\n", gpuid);
src/gromacs/hardware/device_management_hip.cpp:            gmx_warning("Failed to free GPU #%d. %s", gpuid, gmx::getDeviceErrorString(stat).c_str());
src/gromacs/hardware/device_management_hip.cpp:    bool gpuExists = (deviceInfo.status != DeviceStatus::Nonexistent
src/gromacs/hardware/device_management_hip.cpp:    if (!gpuExists)
src/gromacs/hardware/device_management.cpp:void doubleCheckGpuAwareMpiWillWork(const DeviceInformation& /* deviceInfo */) {}
src/gromacs/hardware/tests/device_management.cpp:#if GMX_GPU_OPENCL
src/gromacs/hardware/tests/device_management.cpp:                    "Device OpenCL platform ID changed after serialization/deserialization for "
src/gromacs/hardware/tests/device_management.cpp:#endif // GMX_GPU_OPENCL
src/gromacs/hardware/tests/CMakeLists.txt:    GPU_CPP_SOURCE_FILES
src/gromacs/hardware/device_management_common.cpp: *         are common for CPU, CUDA and OpenCL.
src/gromacs/hardware/device_management_common.cpp:    if (c_binarySupportsGpus)
src/gromacs/hardware/device_management_common.cpp:        return getenv("GMX_DISABLE_GPU_DETECTION") == nullptr;
src/gromacs/hardware/device_management_common.cpp:        if (strstr(vendorName, "NVIDIA"))
src/gromacs/hardware/device_management_common.cpp:            return DeviceVendor::Nvidia;
src/gromacs/hardware/device_management_common.cpp:        case DeviceVendor::Nvidia:
src/gromacs/hardware/device_management_common.cpp:gmx::GpuAwareMpiStatus getMinimalSupportedGpuAwareMpiStatus(
src/gromacs/hardware/device_management_common.cpp:    gmx::GpuAwareMpiStatus minVal                 = gmx::GpuAwareMpiStatus::Supported;
src/gromacs/hardware/device_management_common.cpp:        // In all cases, the level of GPU-aware support that is
src/gromacs/hardware/device_management_common.cpp:        // Also, by default dpcpp makes the same Intel GPU visible
src/gromacs/hardware/device_management_common.cpp:            minVal                 = std::min(minVal, deviceInfo->gpuAwareMpiStatus);
src/gromacs/hardware/device_management_common.cpp:    // support for GPU-aware MPI.
src/gromacs/hardware/device_management_common.cpp:        minVal = gmx::GpuAwareMpiStatus::NotSupported;
src/gromacs/hardware/device_management_common.cpp:                       "DeviceInformation for OpenCL/SYCL can not be serialized");
src/gromacs/hardware/device_management_common.cpp:                       "DeviceInformation for OpenCL/SYCL can not be deserialized");
src/gromacs/hardware/hw_info.h:enum class GpuAwareMpiStatus : int;
src/gromacs/hardware/hw_info.h:/* Hardware information structure with CPU and GPU information.
src/gromacs/hardware/hw_info.h:    std::vector<std::unique_ptr<DeviceInformation>> deviceInfoList; /* Information about GPUs detected on this physical node */
src/gromacs/hardware/hw_info.h:    int ngpu_compatible_tot;  /* Sum of #GPUs over all nodes */
src/gromacs/hardware/hw_info.h:    int ngpu_compatible_min;  /* Min #GPUs over all nodes */
src/gromacs/hardware/hw_info.h:    int ngpu_compatible_max;  /* Max #GPUs over all nodes */
src/gromacs/hardware/hw_info.h:    gmx_bool bIdenticalGPUs; /* TRUE if all ranks have the same type(s) and order of GPUs */
src/gromacs/hardware/hw_info.h:    gmx::GpuAwareMpiStatus minGpuAwareMpiStatus; /* Lowest support level for GPU-aware MPI (Supported > Forced > Not supported) across all detected devices */
src/gromacs/hardware/hw_info.h:/*! \internal \brief Threading and GPU options, can be set automatically or by the user
src/gromacs/hardware/hw_info.h:    //! Empty, or a string provided by the user declaring (unique) GPU IDs available for mdrun to use.
src/gromacs/hardware/hw_info.h:    //! Empty, or a string provided by the user mapping GPU tasks to devices.
src/gromacs/hardware/hw_info.h:    std::string userGpuTaskAssignment;
src/gromacs/hardware/CMakeLists.txt:if(GMX_GPU_OPENCL)
src/gromacs/hardware/CMakeLists.txt:elseif(GMX_GPU_CUDA)
src/gromacs/hardware/CMakeLists.txt:    _gmx_add_files_to_property(CUDA_SOURCES
src/gromacs/hardware/CMakeLists.txt:elseif(GMX_GPU_HIP)
src/gromacs/hardware/CMakeLists.txt:elseif(GMX_GPU_SYCL)
src/gromacs/hardware/detecthardware.cpp:enum class GpuAwareMpiStatus : int;
src/gromacs/hardware/detecthardware.cpp:/*! \brief Detect GPUs when that makes sense to attempt.
src/gromacs/hardware/detecthardware.cpp:    /* The SYCL and OpenCL support requires us to run detection on all
src/gromacs/hardware/detecthardware.cpp:     * With CUDA we don't need to, and prefer to detect on one rank
src/gromacs/hardware/detecthardware.cpp:     * node making the same GPU API calls. */
src/gromacs/hardware/detecthardware.cpp:    constexpr bool allRanksMustDetectGpus = (GMX_GPU_OPENCL != 0 || GMX_GPU_SYCL != 0);
src/gromacs/hardware/detecthardware.cpp:    bool           gpusCanBeDetected      = false;
src/gromacs/hardware/detecthardware.cpp:    if (isMainRankOfPhysicalNode || allRanksMustDetectGpus)
src/gromacs/hardware/detecthardware.cpp:        gpusCanBeDetected = isDeviceDetectionFunctional(&errorMessage);
src/gromacs/hardware/detecthardware.cpp:        if (!gpusCanBeDetected)
src/gromacs/hardware/detecthardware.cpp:                    "Detection of GPUs failed. The API reported:\n" + errorMessage);
src/gromacs/hardware/detecthardware.cpp:    if (gpusCanBeDetected)
src/gromacs/hardware/detecthardware.cpp:    if (!allRanksMustDetectGpus && (physicalNodeComm.size_ > 1))
src/gromacs/hardware/detecthardware.cpp:    // Collect information about GPU-aware MPI support
src/gromacs/hardware/detecthardware.cpp:    const gmx::GpuAwareMpiStatus gpuAwareMpiStatus =
src/gromacs/hardware/detecthardware.cpp:            getMinimalSupportedGpuAwareMpiStatus(hardwareInfo->deviceInfoList);
src/gromacs/hardware/detecthardware.cpp:    int gpu_hash;
src/gromacs/hardware/detecthardware.cpp:    /* Create a unique hash of the GPU type(s) in this node */
src/gromacs/hardware/detecthardware.cpp:    gpu_hash = 0;
src/gromacs/hardware/detecthardware.cpp:    /* Here it might be better to only loop over the compatible GPU, but we
src/gromacs/hardware/detecthardware.cpp:         * the GPUs affects the hash. Also two identical GPUs won't give
src/gromacs/hardware/detecthardware.cpp:         * a gpu_hash of zero after XORing.
src/gromacs/hardware/detecthardware.cpp:        gpu_hash ^= gmx_string_fullhash_func(deviceInfoString.c_str(), gmx_string_hash_init);
src/gromacs/hardware/detecthardware.cpp:        maxMinLocal[5]  = gpu_hash;
src/gromacs/hardware/detecthardware.cpp:                -static_cast<int>(gpuAwareMpiStatus); // Enum is ordinal, higher values mean better support
src/gromacs/hardware/detecthardware.cpp:    hardwareInfo->ngpu_compatible_tot  = countsReduced[4];
src/gromacs/hardware/detecthardware.cpp:    hardwareInfo->ngpu_compatible_min  = -maxMinReduced[9];
src/gromacs/hardware/detecthardware.cpp:    hardwareInfo->ngpu_compatible_max  = maxMinReduced[3];
src/gromacs/hardware/detecthardware.cpp:    hardwareInfo->bIdenticalGPUs       = (maxMinReduced[5] == -maxMinReduced[11]);
src/gromacs/hardware/detecthardware.cpp:    hardwareInfo->minGpuAwareMpiStatus = static_cast<gmx::GpuAwareMpiStatus>(-maxMinReduced[13]);
src/gromacs/hardware/detecthardware.cpp:    hardwareInfo->ngpu_compatible_tot  = numCompatibleDevices;
src/gromacs/hardware/detecthardware.cpp:    hardwareInfo->ngpu_compatible_min  = numCompatibleDevices;
src/gromacs/hardware/detecthardware.cpp:    hardwareInfo->ngpu_compatible_max  = numCompatibleDevices;
src/gromacs/hardware/detecthardware.cpp:    hardwareInfo->bIdenticalGPUs       = TRUE;
src/gromacs/hardware/detecthardware.cpp:    hardwareInfo->minGpuAwareMpiStatus = gpuAwareMpiStatus;
src/gromacs/hardware/detecthardware.cpp:    // Detect GPUs
src/gromacs/hardware/simd_support.cpp:                    "instructons will perform best on this hardware. For non-GPU runs\n"
src/gromacs/hardware/simd_support.cpp:                    "while AVX2 is often better for runs also using a GPU. Typically\n"
src/gromacs/hardware/simd_support.cpp:        // faster with nonbondeds and PME on a GPU. Don't warn the user.
src/gromacs/hardware/device_management_ocl.cpp: *  \brief Defines the OpenCL implementations of the device management.
src/gromacs/hardware/device_management_ocl.cpp:#include "gromacs/gpu_utils/ocl_compiler.h"
src/gromacs/hardware/device_management_ocl.cpp:#include "gromacs/gpu_utils/oclraii.h"
src/gromacs/hardware/device_management_ocl.cpp:#include "gromacs/gpu_utils/oclutils.h"
src/gromacs/hardware/device_management_ocl.cpp:/*! \brief Return true if executing on compatible OS for AMD OpenCL.
src/gromacs/hardware/device_management_ocl.cpp:/*! \brief Return true if executing on compatible GPU for NVIDIA OpenCL.
src/gromacs/hardware/device_management_ocl.cpp: * There are known issues with OpenCL when running on NVIDIA Volta or newer (CC 7+).
src/gromacs/hardware/device_management_ocl.cpp: * As a workaround, we recommend using CUDA on such hardware.
src/gromacs/hardware/device_management_ocl.cpp:static bool runningOnCompatibleHWForNvidia(const DeviceInformation& deviceInfo)
src/gromacs/hardware/device_management_ocl.cpp: * \param devId OpenCL device ID.
src/gromacs/hardware/device_management_ocl.cpp:        case DeviceVendor::Nvidia: result.push_back(32); return result;
src/gromacs/hardware/device_management_ocl.cpp:/*! \brief Return true if executing on compatible GPU for AMD OpenCL.
src/gromacs/hardware/device_management_ocl.cpp: * There are known issues with OpenCL when running on 32-wide AMD hardware, such as
src/gromacs/hardware/device_management_ocl.cpp: * desktop GPUs with RDNA and RDNA2 architectures (gfx10xx).
src/gromacs/hardware/device_management_ocl.cpp: *  Vendor and OpenCL version support checks are executed an the result
src/gromacs/hardware/device_management_ocl.cpp:    if (getenv("GMX_GPU_DISABLE_COMPATIBILITY_CHECK") != nullptr)
src/gromacs/hardware/device_management_ocl.cpp:                "be removed in release 2022. Please use GMX_GPU_DISABLE_COMPATIBILITY_CHECK "
src/gromacs/hardware/device_management_ocl.cpp:    // OpenCL device version check, ensure >= REQUIRED_OPENCL_MIN_VERSION
src/gromacs/hardware/device_management_ocl.cpp:    constexpr unsigned int minVersionMajor = REQUIRED_OPENCL_MIN_VERSION_MAJOR;
src/gromacs/hardware/device_management_ocl.cpp:    constexpr unsigned int minVersionMinor = REQUIRED_OPENCL_MIN_VERSION_MINOR;
src/gromacs/hardware/device_management_ocl.cpp:    // Based on the OpenCL spec we're checking the version supported by
src/gromacs/hardware/device_management_ocl.cpp:    //      OpenCL<space><major_version.minor_version><space><vendor-specific information>
src/gromacs/hardware/device_management_ocl.cpp:            deviceInfo.device_version, "OpenCL %u.%u", &deviceVersionMajor, &deviceVersionMinor);
src/gromacs/hardware/device_management_ocl.cpp:    /* Only AMD, Intel, NVIDIA, and Apple GPUs are supported for now */
src/gromacs/hardware/device_management_ocl.cpp:        case DeviceVendor::Nvidia:
src/gromacs/hardware/device_management_ocl.cpp:            return runningOnCompatibleHWForNvidia(deviceInfo) ? DeviceStatus::Compatible
src/gromacs/hardware/device_management_ocl.cpp:                                                              : DeviceStatus::IncompatibleNvidiaVolta;
src/gromacs/hardware/device_management_ocl.cpp:            return GMX_GPU_NB_CLUSTER_SIZE == 4 ? DeviceStatus::Compatible
src/gromacs/hardware/device_management_ocl.cpp:/*! \brief Make an error string following an OpenCL API call.
src/gromacs/hardware/device_management_ocl.cpp: *  work correctly even if it is called with no OpenCL failure.
src/gromacs/hardware/device_management_ocl.cpp: * \param[in]  status   OpenCL API status code
src/gromacs/hardware/device_management_ocl.cpp: * \returns             A string describing the OpenCL error.
src/gromacs/hardware/device_management_ocl.cpp:inline std::string makeOpenClInternalErrorString(const char* message, cl_int status)
src/gromacs/hardware/device_management_ocl.cpp:        return gmx::formatString("%sOpenCL error encountered %d: %s",
src/gromacs/hardware/device_management_ocl.cpp: * OpenCL device functions properly.
src/gromacs/hardware/device_management_ocl.cpp: * \param[out] errorMessage    An error message related to a failing OpenCL API call.
src/gromacs/hardware/device_management_ocl.cpp:        errorMessage->assign(makeOpenClInternalErrorString("clCreateContext", status));
src/gromacs/hardware/device_management_ocl.cpp:        errorMessage->assign(makeOpenClInternalErrorString("clCreateCommandQueue", status));
src/gromacs/hardware/device_management_ocl.cpp:        errorMessage->assign(makeOpenClInternalErrorString("clCreateProgramWithSource", status));
src/gromacs/hardware/device_management_ocl.cpp:        errorMessage->assign(makeOpenClInternalErrorString("clBuildProgram", status));
src/gromacs/hardware/device_management_ocl.cpp:        errorMessage->assign(makeOpenClInternalErrorString("clCreateKernel", status));
src/gromacs/hardware/device_management_ocl.cpp:        errorMessage->assign(makeOpenClInternalErrorString("clEnqueueNDRangeKernel", status));
src/gromacs/hardware/device_management_ocl.cpp:/*! \brief Check whether the \c ocl_gpu_device is suitable for use by mdrun
src/gromacs/hardware/device_management_ocl.cpp: * Runs compatibility checks verifying the device OpenCL version requirement
src/gromacs/hardware/device_management_ocl.cpp: * \returns  A DeviceStatus to indicate if the GPU device is supported and if it was able to run
src/gromacs/hardware/device_management_ocl.cpp:static DeviceStatus checkGpu(size_t deviceId, const DeviceInformation& deviceInfo)
src/gromacs/hardware/device_management_ocl.cpp:            errorMessage->assign("No valid OpenCL driver found");
src/gromacs/hardware/device_management_ocl.cpp:        errorMessage->assign("No OpenCL platforms found even though the driver was valid");
src/gromacs/hardware/device_management_ocl.cpp:    cl_device_type  req_dev_type = CL_DEVICE_TYPE_GPU;
src/gromacs/hardware/device_management_ocl.cpp:            // TODO this should have a descriptive error message that we only support one OpenCL platform
src/gromacs/hardware/device_management_ocl.cpp:                    deviceInfoList[device_index]->gpuAwareMpiStatus = gmx::GpuAwareMpiStatus::NotSupported;
src/gromacs/hardware/device_management_ocl.cpp:                            gmx::checkGpu(device_index, *deviceInfoList[device_index]);
src/gromacs/hardware/device_management_ocl.cpp:            /* Dummy sort of devices -  AMD first, then NVIDIA, then Intel */
src/gromacs/hardware/device_management_ocl.cpp:                        if (deviceInfoList[i]->deviceVendor == DeviceVendor::Nvidia)
src/gromacs/hardware/device_management_ocl.cpp:    // If the device is NVIDIA, for safety reasons we disable the JIT
src/gromacs/hardware/device_management_ocl.cpp:    if (deviceInfo.deviceVendor == DeviceVendor::Nvidia)
src/gromacs/hardware/device_management_ocl.cpp:        _putenv("CUDA_CACHE_DISABLE=1");
src/gromacs/hardware/device_management_ocl.cpp:        setenv("CUDA_CACHE_DISABLE", "1", 0);
src/gromacs/hardware/device_management_ocl.cpp:    bool gpuExists = (deviceInfo.status != DeviceStatus::Nonexistent
src/gromacs/hardware/device_management_ocl.cpp:    if (!gpuExists)
src/gromacs/hardware/device_management_ocl.cpp:void doubleCheckGpuAwareMpiWillWork(const DeviceInformation& /* deviceInfo */) {}
src/gromacs/hardware/printhardware.cpp:static constexpr bool bGPUBinary = (GMX_GPU != 0);
src/gromacs/hardware/printhardware.cpp: * Returns the GPU information text, one GPU per line.
src/gromacs/hardware/printhardware.cpp:static std::string sprint_gpus(const std::vector<std::unique_ptr<DeviceInformation>>& deviceInfoList)
src/gromacs/hardware/printhardware.cpp:    std::vector<std::string> gpuStrings(0);
src/gromacs/hardware/printhardware.cpp:        gpuStrings.emplace_back("    " + getDeviceInformationString(*deviceInfo));
src/gromacs/hardware/printhardware.cpp:    return gmx::joinStrings(gpuStrings, "\n");
src/gromacs/hardware/printhardware.cpp:        s += gmx::formatString(", %d compatible GPU%s",
src/gromacs/hardware/printhardware.cpp:                               hwinfo->ngpu_compatible_tot,
src/gromacs/hardware/printhardware.cpp:                               hwinfo->ngpu_compatible_tot == 1 ? "" : "s");
src/gromacs/hardware/printhardware.cpp:    else if (bGPUBinary)
src/gromacs/hardware/printhardware.cpp:            s += gmx::formatString(" (GPU detection failed)");
src/gromacs/hardware/printhardware.cpp:            s += gmx::formatString(" (GPU detection deactivated)");
src/gromacs/hardware/printhardware.cpp:        if (bGPUBinary)
src/gromacs/hardware/printhardware.cpp:            s += gmx::formatString("  Compatible GPUs per node: %2d", hwinfo->ngpu_compatible_min);
src/gromacs/hardware/printhardware.cpp:            if (hwinfo->ngpu_compatible_max > hwinfo->ngpu_compatible_min)
src/gromacs/hardware/printhardware.cpp:                s += gmx::formatString(" - %2d", hwinfo->ngpu_compatible_max);
src/gromacs/hardware/printhardware.cpp:            if (hwinfo->ngpu_compatible_tot > 0)
src/gromacs/hardware/printhardware.cpp:                if (hwinfo->bIdenticalGPUs)
src/gromacs/hardware/printhardware.cpp:                    s += gmx::formatString("  All nodes have identical type(s) of GPUs\n");
src/gromacs/hardware/printhardware.cpp:                    /* This message will also appear with identical GPU types
src/gromacs/hardware/printhardware.cpp:                     * when at least one node has no GPU.
src/gromacs/hardware/printhardware.cpp:                            "  Different nodes have different type(s) and/or order of GPUs\n");
src/gromacs/hardware/printhardware.cpp:    if (bGPUBinary && !hwinfo->deviceInfoList.empty())
src/gromacs/hardware/printhardware.cpp:        s += gmx::formatString("  GPU info:\n");
src/gromacs/hardware/printhardware.cpp:        s += gmx::formatString("    Number of GPUs detected: %d\n",
src/gromacs/hardware/printhardware.cpp:        s += sprint_gpus(hwinfo->deviceInfoList) + "\n";
src/gromacs/hardware/printhardware.cpp:        if constexpr (bGPUBinary)
src/gromacs/hardware/device_management_sycl.cpp:#include "gromacs/gpu_utils/gmxsycl.h"
src/gromacs/hardware/device_management_sycl.cpp:static std::optional<std::tuple<int, int>> parseHardwareVersionNvidia(const std::string& archName)
src/gromacs/hardware/device_management_sycl.cpp:static std::optional<std::tuple<int, int>> getHardwareVersionNvidia(const sycl::device& device)
src/gromacs/hardware/device_management_sycl.cpp:    if (auto result = parseHardwareVersionNvidia(deviceVersion); result.has_value())
src/gromacs/hardware/device_management_sycl.cpp:#if (GMX_SYCL_ACPP && GMX_ACPP_HAVE_CUDA_TARGET) // hipSYCL uses CUDA Runtime API
src/gromacs/hardware/device_management_sycl.cpp:    const int             nativeDeviceId = sycl::get_native<sycl::backend::cuda>(device);
src/gromacs/hardware/device_management_sycl.cpp:    struct cudaDeviceProp prop;
src/gromacs/hardware/device_management_sycl.cpp:    cudaError_t           status = cudaGetDeviceProperties(&prop, nativeDeviceId);
src/gromacs/hardware/device_management_sycl.cpp:    if (status == cudaSuccess)
src/gromacs/hardware/device_management_sycl.cpp:#elif (GMX_SYCL_DPCPP && defined(SYCL_EXT_ONEAPI_BACKEND_CUDA))
src/gromacs/hardware/device_management_sycl.cpp:    // oneAPI uses CUDA Driver API, but does not link the application to it
src/gromacs/hardware/device_management_sycl.cpp:    return parseHardwareVersionNvidia(ccStr);
src/gromacs/hardware/device_management_sycl.cpp:    // prop.major and prop.minor indicate the closest CUDA CC
src/gromacs/hardware/device_management_sycl.cpp:    // Device name might contain the desired string, but it depends on the ROCm version
src/gromacs/hardware/device_management_sycl.cpp:static gmx::GpuAwareMpiStatus getDeviceGpuAwareMpiStatus(const sycl::backend backend)
src/gromacs/hardware/device_management_sycl.cpp:        return gmx::GpuAwareMpiStatus::NotSupported;
src/gromacs/hardware/device_management_sycl.cpp:        case sycl::backend::opencl: return gmx::GpuAwareMpiStatus::NotSupported;
src/gromacs/hardware/device_management_sycl.cpp:        case sycl::backend::ext_oneapi_cuda: return gmx::checkMpiCudaAwareSupport();
src/gromacs/hardware/device_management_sycl.cpp:        case sycl::backend::cuda: return gmx::checkMpiCudaAwareSupport();
src/gromacs/hardware/device_management_sycl.cpp:        default: return gmx::GpuAwareMpiStatus::NotSupported;
src/gromacs/hardware/device_management_sycl.cpp:        if (getenv("GMX_GPU_DISABLE_COMPATIBILITY_CHECK") != nullptr)
src/gromacs/hardware/device_management_sycl.cpp:#if GMX_GPU_NB_CLUSTER_SIZE == 4
src/gromacs/hardware/device_management_sycl.cpp:#elif GMX_GPU_NB_CLUSTER_SIZE == 8
src/gromacs/hardware/device_management_sycl.cpp:        const std::vector<int> compiledNbnxmSubGroupSizes{ 32 }; // Only NVIDIA
src/gromacs/hardware/device_management_sycl.cpp:         * So, the only viable options are CPUs and GPUs. */
src/gromacs/hardware/device_management_sycl.cpp:        else if (!forceCpu && syclDevice.is_gpu())
src/gromacs/hardware/device_management_sycl.cpp: * by different backends (e.g., the same GPU can be accessible via both OpenCL and L0).
src/gromacs/hardware/device_management_sycl.cpp: * the user has opted into using its GPU-aware functionality. Otherwise, we choose the
src/gromacs/hardware/device_management_sycl.cpp: * backend with the most compatible devices. In case of a tie, we choose OpenCL (if
src/gromacs/hardware/device_management_sycl.cpp:    // Prefer L0 backend if GROMACS might be using GPU-aware Intel MPI.
src/gromacs/hardware/device_management_sycl.cpp:    if (const gmx::GpuAwareMpiStatus status = gmx::checkMpiZEAwareSupport();
src/gromacs/hardware/device_management_sycl.cpp:        && ((status == gmx::GpuAwareMpiStatus::Supported) || (status == gmx::GpuAwareMpiStatus::Forced)))
src/gromacs/hardware/device_management_sycl.cpp:        // simulation that will use GPU-aware MPI.
src/gromacs/hardware/device_management_sycl.cpp:        // Count devices provided by OpenCL. Will be zero if no OpenCL devices found.
src/gromacs/hardware/device_management_sycl.cpp:        const int devicesInOpenCL = countDevicesByBackend[sycl::backend::opencl];
src/gromacs/hardware/device_management_sycl.cpp:        if (devicesInOpenCL == backendWithMostDevices->second)
src/gromacs/hardware/device_management_sycl.cpp:            // Prefer OpenCL backend as more stable, if it has as many devices as others
src/gromacs/hardware/device_management_sycl.cpp:            return sycl::backend::opencl;
src/gromacs/hardware/device_management_sycl.cpp:             * For multi-tile Intel GPUs, this corresponds to
src/gromacs/hardware/device_management_sycl.cpp:    std::vector<sycl::device> allDevices = sycl::device::get_devices(sycl::info::device_type::gpu);
src/gromacs/hardware/device_management_sycl.cpp:            // SYCL uses unsigned char (unlike CUDA, ROCM, and OpenCL) which
src/gromacs/hardware/device_management_sycl.cpp:        deviceInfos[i]->gpuAwareMpiStatus = getDeviceGpuAwareMpiStatus(syclDevice.get_backend());
src/gromacs/hardware/device_management_sycl.cpp:        if (deviceInfos[i]->deviceVendor == DeviceVendor::Nvidia)
src/gromacs/hardware/device_management_sycl.cpp:            if (const auto computeCapability = getHardwareVersionNvidia(syclDevice);
src/gromacs/hardware/device_management_sycl.cpp:#if GMX_HAVE_GPU_GRAPH_SUPPORT && defined(SYCL_EXT_ONEAPI_GRAPH) && SYCL_EXT_ONEAPI_GRAPH
src/gromacs/hardware/device_management_sycl.cpp:    if (getenv("GMX_GPU_DISABLE_COMPATIBILITY_CHECK") == nullptr)
src/gromacs/hardware/device_management_sycl.cpp:void doubleCheckGpuAwareMpiWillWork(const DeviceInformation& deviceInfo)
src/gromacs/hardware/device_management_sycl.cpp:            // Trying to use a device from e.g. an OpenCL backend
src/gromacs/hardware/device_management_sycl.cpp:            // treat Intel MPI as GPU aware.
src/gromacs/hardware/device_management_sycl.cpp:                    gmx::InvalidInputError("Intel MPI can only implement GPU-aware operations on "
src/gromacs/hardware/device_management_shared_amd.cpp:#if GMX_GPU_HIP
src/gromacs/hardware/device_management_shared_amd.cpp:#    include "gromacs/gpu_utils/hiputils.h"
src/gromacs/hardware/device_management_shared_amd.cpp:void doubleCheckGpuAwareMpiWillWork(const DeviceInformation& /* deviceInfo */) {}
src/gromacs/hardware/device_management.cu: *  \brief Defines the CUDA implementations of the device management.
src/gromacs/hardware/device_management.cu:#include "gromacs/gpu_utils/cudautils.cuh"
src/gromacs/hardware/device_management.cu:#include "gromacs/gpu_utils/device_context.h"
src/gromacs/hardware/device_management.cu:#include "gromacs/gpu_utils/device_stream.h"
src/gromacs/hardware/device_management.cu: * Max number of devices supported by CUDA (for consistency checking).
src/gromacs/hardware/device_management.cu: * In reality it is 16 with CUDA <=v5.0, but let's stay on the safe side.
src/gromacs/hardware/device_management.cu:static const int c_cudaMaxDeviceCount = 32;
src/gromacs/hardware/device_management.cu:                    "WARNING: The %s binary does not include support for the CUDA architecture of "
src/gromacs/hardware/device_management.cu:                    "the GPU ID #%d (compute capability %d.%d) detected during detection. "
src/gromacs/hardware/device_management.cu:                    "capability >= 3.5, so your GPU "
src/gromacs/hardware/device_management.cu:                    "Consult the install guide for how to use the GMX_CUDA_TARGET_SM and "
src/gromacs/hardware/device_management.cu:                    "GMX_CUDA_TARGET_COMPUTE CMake variables to add this architecture.",
src/gromacs/hardware/device_management.cu:/*! \brief Runs GPU compatibility and sanity checks on the indicated device.
src/gromacs/hardware/device_management.cu: * Runs a series of checks to determine that the given GPU and underlying CUDA
src/gromacs/hardware/device_management.cu: *  As the error handling only permits returning the state of the GPU, this function
src/gromacs/hardware/device_management.cu: *  does not clear the CUDA runtime API status allowing the caller to inspect the error
src/gromacs/hardware/device_management.cu: *  reset the CUDA runtime state.
src/gromacs/hardware/device_management.cu:    cudaError_t cu_err;
src/gromacs/hardware/device_management.cu:    /* both major & minor is 9999 if no CUDA capable devices are present */
src/gromacs/hardware/device_management.cu:    cu_err = cudaSetDevice(deviceInfo.id);
src/gromacs/hardware/device_management.cu:    if (cu_err != cudaSuccess)
src/gromacs/hardware/device_management.cu:    cudaFuncAttributes attributes;
src/gromacs/hardware/device_management.cu:    cu_err = cudaFuncGetAttributes(&attributes, dummy_kernel);
src/gromacs/hardware/device_management.cu:    if (cu_err == cudaErrorInvalidDeviceFunction)
src/gromacs/hardware/device_management.cu:        cudaGetLastError();
src/gromacs/hardware/device_management.cu:    // Avoid triggering an error if GPU devices are in exclusive or prohibited mode;
src/gromacs/hardware/device_management.cu:    // it is enough to check for cudaErrorDevicesUnavailable only here because
src/gromacs/hardware/device_management.cu:    // if we encounter it that will happen in above cudaFuncGetAttributes.
src/gromacs/hardware/device_management.cu:    if (cu_err == cudaErrorDevicesUnavailable)
src/gromacs/hardware/device_management.cu:    else if (cu_err != cudaSuccess)
src/gromacs/hardware/device_management.cu:        const auto          dummyArguments = prepareGpuKernelArguments(dummy_kernel, config);
src/gromacs/hardware/device_management.cu:        launchGpuKernel(dummy_kernel, config, deviceStream, nullptr, "Dummy kernel", dummyArguments);
src/gromacs/hardware/device_management.cu:        // launchGpuKernel error is not fatal and should continue with marking the device bad
src/gromacs/hardware/device_management.cu:    if (cudaDeviceSynchronize() != cudaSuccess)
src/gromacs/hardware/device_management.cu:    // Skip context teardown when using CUDA-aware MPI because this can lead to
src/gromacs/hardware/device_management.cu:    const bool haveDetectedOrForcedCudaAwareMpi =
src/gromacs/hardware/device_management.cu:            (gmx::checkMpiCudaAwareSupport() == gmx::GpuAwareMpiStatus::Supported
src/gromacs/hardware/device_management.cu:             || gmx::checkMpiCudaAwareSupport() == gmx::GpuAwareMpiStatus::Forced);
src/gromacs/hardware/device_management.cu:    if (!haveDetectedOrForcedCudaAwareMpi)
src/gromacs/hardware/device_management.cu:        cu_err = cudaDeviceReset();
src/gromacs/hardware/device_management.cu:        CU_RET_ERR(cu_err, "cudaDeviceReset failed");
src/gromacs/hardware/device_management.cu:    cudaError_t stat;
src/gromacs/hardware/device_management.cu:    stat                      = cudaDriverGetVersion(&driverVersion);
src/gromacs/hardware/device_management.cu:    GMX_ASSERT(stat != cudaErrorInvalidValue,
src/gromacs/hardware/device_management.cu:               "An impossible null pointer was passed to cudaDriverGetVersion");
src/gromacs/hardware/device_management.cu:    GMX_RELEASE_ASSERT(stat == cudaSuccess,
src/gromacs/hardware/device_management.cu:                       ("An unexpected value was returned from cudaDriverGetVersion. "
src/gromacs/hardware/device_management.cu:        // Can't detect GPUs if there is no driver
src/gromacs/hardware/device_management.cu:            errorMessage->assign("No valid CUDA driver found");
src/gromacs/hardware/device_management.cu:    stat = cudaGetDeviceCount(&numDevices);
src/gromacs/hardware/device_management.cu:    if (stat != cudaSuccess)
src/gromacs/hardware/device_management.cu:            /* cudaGetDeviceCount failed which means that there is
src/gromacs/hardware/device_management.cu:             * mismatch, all GPUs being busy in exclusive mode,
src/gromacs/hardware/device_management.cu:             * invalid CUDA_VISIBLE_DEVICES, or some other condition
src/gromacs/hardware/device_management.cu:            errorMessage->assign(cudaGetErrorString(stat));
src/gromacs/hardware/device_management.cu:        // errors. Note that if CUDA_VISIBLE_DEVICES does not contain
src/gromacs/hardware/device_management.cu:        // valid devices, then cudaGetLastError returns the
src/gromacs/hardware/device_management.cu:        // (undocumented) cudaErrorNoDevice, but this should not be a
src/gromacs/hardware/device_management.cu:        // problem as there should be no future CUDA API calls.
src/gromacs/hardware/device_management.cu:        // NVIDIA bug report #2038718 has been filed.
src/gromacs/hardware/device_management.cu:        cudaGetLastError();
src/gromacs/hardware/device_management.cu:        // Can't detect GPUs
src/gromacs/hardware/device_management.cu:    cudaError_t stat = cudaGetDeviceCount(&numDevices);
src/gromacs/hardware/device_management.cu:                          "Invalid call of findDevices() when CUDA API returned an error, perhaps "
src/gromacs/hardware/device_management.cu:    /* things might go horribly wrong if cudart is not compatible with the driver */
src/gromacs/hardware/device_management.cu:    numDevices = std::min(numDevices, c_cudaMaxDeviceCount);
src/gromacs/hardware/device_management.cu:    gmx::ensureNoPendingDeviceError("Trying to find available CUDA devices.");
src/gromacs/hardware/device_management.cu:    const gmx::GpuAwareMpiStatus gpuAwareMpiStatus =
src/gromacs/hardware/device_management.cu:            GMX_LIB_MPI ? gmx::checkMpiCudaAwareSupport() : gmx::GpuAwareMpiStatus::NotSupported;
src/gromacs/hardware/device_management.cu:        cudaDeviceProp prop;
src/gromacs/hardware/device_management.cu:        memset(&prop, 0, sizeof(cudaDeviceProp));
src/gromacs/hardware/device_management.cu:        stat = cudaGetDeviceProperties(&prop, i);
src/gromacs/hardware/device_management.cu:        deviceInfoList[i]->deviceVendor = DeviceVendor::Nvidia;
src/gromacs/hardware/device_management.cu:        deviceInfoList[i]->gpuAwareMpiStatus = gpuAwareMpiStatus;
src/gromacs/hardware/device_management.cu:        const DeviceStatus checkResult = (stat != cudaSuccess) ? DeviceStatus::NonFunctional
src/gromacs/hardware/device_management.cu:            //  - we inspect the CUDA API state to retrieve and record any
src/gromacs/hardware/device_management.cu:            //    errors that occurred during is_gmx_supported_gpu_id() here,
src/gromacs/hardware/device_management.cu:            //    but this would be more elegant done within is_gmx_supported_gpu_id()
src/gromacs/hardware/device_management.cu:            // Here we also clear the CUDA API error state so potential
src/gromacs/hardware/device_management.cu:    stat = cudaPeekAtLastError();
src/gromacs/hardware/device_management.cu:            stat == cudaSuccess,
src/gromacs/hardware/device_management.cu:            ("We promise to return with clean CUDA state, but non-success state encountered. "
src/gromacs/hardware/device_management.cu:    cudaError_t stat;
src/gromacs/hardware/device_management.cu:    stat = cudaSetDevice(deviceId);
src/gromacs/hardware/device_management.cu:    if (stat != cudaSuccess)
src/gromacs/hardware/device_management.cu:        auto message = gmx::formatString("Failed to initialize GPU #%d", deviceId);
src/gromacs/hardware/device_management.cu:        fprintf(stderr, "Initialized GPU ID #%d: %s\n", deviceId, deviceInfo.prop.name);
src/gromacs/hardware/device_management.cu:    cudaError_t stat;
src/gromacs/hardware/device_management.cu:    int gpuid;
src/gromacs/hardware/device_management.cu:    stat = cudaGetDevice(&gpuid);
src/gromacs/hardware/device_management.cu:    if (stat == cudaSuccess)
src/gromacs/hardware/device_management.cu:            fprintf(stderr, "Cleaning up context on GPU ID #%d.\n", gpuid);
src/gromacs/hardware/device_management.cu:        stat = cudaDeviceReset();
src/gromacs/hardware/device_management.cu:        if (stat != cudaSuccess)
src/gromacs/hardware/device_management.cu:            gmx_warning("Failed to free GPU #%d. %s", gpuid, gmx::getDeviceErrorString(stat).c_str());
src/gromacs/hardware/device_management.cu:    bool gpuExists = (deviceInfo.status != DeviceStatus::Nonexistent
src/gromacs/hardware/device_management.cu:    if (!gpuExists)
src/gromacs/hardware/device_management.cu:        return gmx::formatString("#%d: NVIDIA %s, compute cap.: %d.%d, ECC: %3s, stat: %s",
src/gromacs/hardware/device_management.cu:void doubleCheckGpuAwareMpiWillWork(const DeviceInformation& /* deviceInfo */) {}
src/gromacs/gpu_utils/vectype_ops_cuda_base_math.h:#ifndef GMX_GPU_UTILS_VECTYPE_OPS_CUDA_BASE_MATH_H
src/gromacs/gpu_utils/vectype_ops_cuda_base_math.h:#define GMX_GPU_UTILS_VECTYPE_OPS_CUDA_BASE_MATH_H
src/gromacs/gpu_utils/vectype_ops_cuda_base_math.h:#if !defined(__CUDACC__)
src/gromacs/gpu_utils/vectype_ops_cuda_base_math.h:#    error Including header specific for CUDA device code without compiling the file with the correct compiler
src/gromacs/gpu_utils/vectype_ops_cuda_base_math.h:#endif /* GMX_GPU_UTILS_VECTYPE_OPS_CUDA_BASE_MATH_H */
src/gromacs/gpu_utils/gpueventsynchronizer_helpers.cpp: *  \brief Helper functions for a GpuEventSynchronizer class.
src/gromacs/gpu_utils/gpueventsynchronizer_helpers.cpp:#include "gpueventsynchronizer_helpers.h"
src/gromacs/gpu_utils/gpueventsynchronizer_helpers.cpp:#if GMX_GPU_CUDA
src/gromacs/gpu_utils/gpueventsynchronizer_helpers.cpp:// In OpenCL and SYCL builds, g_useEventConsumptionCounting is constexpr true.
src/gromacs/gpu_utils/gpueventsynchronizer_helpers.cpp:void disableGpuEventConsumptionCounting()
src/gromacs/gpu_utils/gpueventsynchronizer_helpers.cpp:    GMX_RELEASE_ASSERT(GMX_GPU_CUDA, "Can only be called in CUDA builds");
src/gromacs/gpu_utils/gpueventsynchronizer_helpers.cpp:#if GMX_GPU_CUDA
src/gromacs/gpu_utils/vectype_ops_cuda.h:#ifndef GMX_GPU_UTILS_VECTYPE_OPS_CUDA_H
src/gromacs/gpu_utils/vectype_ops_cuda.h:#define GMX_GPU_UTILS_VECTYPE_OPS_CUDA_H
src/gromacs/gpu_utils/vectype_ops_cuda.h:#if !defined(__CUDACC__)
src/gromacs/gpu_utils/vectype_ops_cuda.h:#    error Including header specific for CUDA device code without compiling the file with the correct compiler
src/gromacs/gpu_utils/vectype_ops_cuda.h:#include "vectype_ops_cuda_base_math.h"
src/gromacs/gpu_utils/vectype_ops_cuda.h:#include "vectype_ops_cuda_hip_shared_declarations.h"
src/gromacs/gpu_utils/vectype_ops_cuda.h:#include "vectype_ops_cuda_hip_shared_trig_math.h"
src/gromacs/gpu_utils/vectype_ops_cuda.h:#endif /* GMX_GPU_UTILS_VECTYPE_OPS_CUDA_H */
src/gromacs/gpu_utils/pmalloc.cpp: *  \brief Define functions for host-side memory handling when using OpenCL devices or no GPU device.
src/gromacs/gpu_utils/pmalloc.cpp: * This function is a stub for CPU-only and OpenCL builds.
src/gromacs/gpu_utils/pmalloc.cpp: * CUDA, HIP, and SYCL have special implementations in separate files.
src/gromacs/gpu_utils/pmalloc.cpp:     * 4kb page, like CUDA does. */
src/gromacs/gpu_utils/gpu_utils.cu: *  \brief Define functions for detection and initialization for CUDA devices.
src/gromacs/gpu_utils/gpu_utils.cu:#include "gpu_utils.h"
src/gromacs/gpu_utils/gpu_utils.cu:#include <cuda_profiler_api.h>
src/gromacs/gpu_utils/gpu_utils.cu:#include "gromacs/gpu_utils/cudautils.cuh"
src/gromacs/gpu_utils/gpu_utils.cu:#include "gromacs/gpu_utils/device_context.h"
src/gromacs/gpu_utils/gpu_utils.cu:#include "gromacs/gpu_utils/device_stream.h"
src/gromacs/gpu_utils/gpu_utils.cu:static const bool cudaProfilerRun =
src/gromacs/gpu_utils/gpu_utils.cu:    cudaPointerAttributes memoryAttributes;
src/gromacs/gpu_utils/gpu_utils.cu:    cudaError_t           stat = cudaPointerGetAttributes(&memoryAttributes, h_ptr);
src/gromacs/gpu_utils/gpu_utils.cu:        case cudaSuccess:
src/gromacs/gpu_utils/gpu_utils.cu:            // In CUDA 11.0, the field called memoryType in
src/gromacs/gpu_utils/gpu_utils.cu:            // cudaPointerAttributes was replaced by a field called
src/gromacs/gpu_utils/gpu_utils.cu:            // pointer passed to cudaPointerGetAttributes is to
src/gromacs/gpu_utils/gpu_utils.cu:#if CUDART_VERSION < 11 * 1000
src/gromacs/gpu_utils/gpu_utils.cu:            isPinned = (memoryAttributes.type == cudaMemoryTypeHost);
src/gromacs/gpu_utils/gpu_utils.cu:        case cudaErrorInvalidValue:
src/gromacs/gpu_utils/gpu_utils.cu:            // If the buffer was not pinned, then it will not be recognized by CUDA at all
src/gromacs/gpu_utils/gpu_utils.cu:            cudaGetLastError();
src/gromacs/gpu_utils/gpu_utils.cu:        default: CU_RET_ERR(stat, "Unexpected CUDA error");
src/gromacs/gpu_utils/gpu_utils.cu:void startGpuProfiler()
src/gromacs/gpu_utils/gpu_utils.cu:    /* An environment variable set by the CUDA tool in use indicates that
src/gromacs/gpu_utils/gpu_utils.cu:       mdrun is executed in the CUDA profiler.
src/gromacs/gpu_utils/gpu_utils.cu:       be started here. This way we can avoid tracing the CUDA events from the
src/gromacs/gpu_utils/gpu_utils.cu:    if (cudaProfilerRun)
src/gromacs/gpu_utils/gpu_utils.cu:        cudaError_t stat;
src/gromacs/gpu_utils/gpu_utils.cu:        stat = cudaProfilerStart();
src/gromacs/gpu_utils/gpu_utils.cu:        CU_RET_ERR(stat, "cudaProfilerStart failed");
src/gromacs/gpu_utils/gpu_utils.cu:void stopGpuProfiler()
src/gromacs/gpu_utils/gpu_utils.cu:    /* Stopping the nvidia here allows us to eliminate the subsequent
src/gromacs/gpu_utils/gpu_utils.cu:    if (cudaProfilerRun)
src/gromacs/gpu_utils/gpu_utils.cu:        cudaError_t stat;
src/gromacs/gpu_utils/gpu_utils.cu:        stat = cudaProfilerStop();
src/gromacs/gpu_utils/gpu_utils.cu:        CU_RET_ERR(stat, "cudaProfilerStop failed");
src/gromacs/gpu_utils/gpu_utils.cu:void resetGpuProfiler()
src/gromacs/gpu_utils/gpu_utils.cu:    /* With CUDA <=7.5 the profiler can't be properly reset; we can only start
src/gromacs/gpu_utils/gpu_utils.cu:     * TODO: add a stop (or replace it with reset) when this will work correctly in CUDA.
src/gromacs/gpu_utils/gpu_utils.cu:     * stopGpuProfiler();
src/gromacs/gpu_utils/gpu_utils.cu:    if (cudaProfilerRun)
src/gromacs/gpu_utils/gpu_utils.cu:        startGpuProfiler();
src/gromacs/gpu_utils/gpu_utils.cu:/*! \brief Check and act on status returned from peer access CUDA call
src/gromacs/gpu_utils/gpu_utils.cu: * If status is "cudaSuccess", we continue. If
src/gromacs/gpu_utils/gpu_utils.cu: * "cudaErrorPeerAccessAlreadyEnabled", then peer access has already
src/gromacs/gpu_utils/gpu_utils.cu: * been enabled so we ignore. If "cudaErrorInvalidDevice" then the
src/gromacs/gpu_utils/gpu_utils.cu: * run is trying to access an invalid GPU, so we throw an error. If
src/gromacs/gpu_utils/gpu_utils.cu: * "cudaErrorInvalidValue" then there is a problem with the arguments
src/gromacs/gpu_utils/gpu_utils.cu: * to the CUDA call, and we throw an error. These cover all expected
src/gromacs/gpu_utils/gpu_utils.cu: * \param[in] stat           CUDA call return status
src/gromacs/gpu_utils/gpu_utils.cu: * \param[in] gpuA           ID for GPU initiating peer access call
src/gromacs/gpu_utils/gpu_utils.cu: * \param[in] gpuB           ID for remote GPU
src/gromacs/gpu_utils/gpu_utils.cu: * \param[in] cudaCallName   name of CUDA peer access call
src/gromacs/gpu_utils/gpu_utils.cu:static void peerAccessCheckStat(const cudaError_t    stat,
src/gromacs/gpu_utils/gpu_utils.cu:                                const int            gpuA,
src/gromacs/gpu_utils/gpu_utils.cu:                                const int            gpuB,
src/gromacs/gpu_utils/gpu_utils.cu:                                const char*          cudaCallName)
src/gromacs/gpu_utils/gpu_utils.cu:    if (stat == cudaErrorPeerAccessAlreadyEnabled)
src/gromacs/gpu_utils/gpu_utils.cu:        // Now clear the error internally within CUDA:
src/gromacs/gpu_utils/gpu_utils.cu:        cudaGetLastError();
src/gromacs/gpu_utils/gpu_utils.cu:    if ((stat == cudaErrorInvalidDevice) || (stat == cudaErrorInvalidValue))
src/gromacs/gpu_utils/gpu_utils.cu:                gmx::formatString("%s from GPU %d to GPU %d failed", cudaCallName, gpuA, gpuB);
src/gromacs/gpu_utils/gpu_utils.cu:    if (stat != cudaSuccess)
src/gromacs/gpu_utils/gpu_utils.cu:                        "GPU peer access not enabled between GPUs %d and %d due to unexpected "
src/gromacs/gpu_utils/gpu_utils.cu:                        gpuA,
src/gromacs/gpu_utils/gpu_utils.cu:                        gpuB,
src/gromacs/gpu_utils/gpu_utils.cu:                        cudaCallName,
src/gromacs/gpu_utils/gpu_utils.cu:        // Clear the error internally within CUDA
src/gromacs/gpu_utils/gpu_utils.cu:        cudaGetLastError();
src/gromacs/gpu_utils/gpu_utils.cu:void setupGpuDevicePeerAccess(gmx::ArrayRef<const int> gpuIdsToUse, const gmx::MDLogger& mdlog)
src/gromacs/gpu_utils/gpu_utils.cu:    cudaError_t stat;
src/gromacs/gpu_utils/gpu_utils.cu:    // take a note of currently-set GPU
src/gromacs/gpu_utils/gpu_utils.cu:    int currentGpu;
src/gromacs/gpu_utils/gpu_utils.cu:    stat = cudaGetDevice(&currentGpu);
src/gromacs/gpu_utils/gpu_utils.cu:    CU_RET_ERR(stat, "cudaGetDevice in setupGpuDevicePeerAccess failed");
src/gromacs/gpu_utils/gpu_utils.cu:            "Note: Peer access enabled between the following GPU pairs in the node:\n ");
src/gromacs/gpu_utils/gpu_utils.cu:    for (unsigned int i = 0; i < gpuIdsToUse.size(); i++)
src/gromacs/gpu_utils/gpu_utils.cu:        int gpuA = gpuIdsToUse[i];
src/gromacs/gpu_utils/gpu_utils.cu:        stat     = cudaSetDevice(gpuA);
src/gromacs/gpu_utils/gpu_utils.cu:        if (stat != cudaSuccess)
src/gromacs/gpu_utils/gpu_utils.cu:                            "GPU peer access not enabled due to unexpected return value from "
src/gromacs/gpu_utils/gpu_utils.cu:                            "cudaSetDevice(%d). %s",
src/gromacs/gpu_utils/gpu_utils.cu:                            gpuA,
src/gromacs/gpu_utils/gpu_utils.cu:        for (unsigned int j = 0; j < gpuIdsToUse.size(); j++)
src/gromacs/gpu_utils/gpu_utils.cu:                int gpuB          = gpuIdsToUse[j];
src/gromacs/gpu_utils/gpu_utils.cu:                stat              = cudaDeviceCanAccessPeer(&canAccessPeer, gpuA, gpuB);
src/gromacs/gpu_utils/gpu_utils.cu:                peerAccessCheckStat(stat, gpuA, gpuB, mdlog, "cudaDeviceCanAccessPeer");
src/gromacs/gpu_utils/gpu_utils.cu:                    stat = cudaDeviceEnablePeerAccess(gpuB, 0);
src/gromacs/gpu_utils/gpu_utils.cu:                    peerAccessCheckStat(stat, gpuA, gpuB, mdlog, "cudaDeviceEnablePeerAccess");
src/gromacs/gpu_utils/gpu_utils.cu:                    message           = gmx::formatString("%s%d->%d ", message.c_str(), gpuA, gpuB);
src/gromacs/gpu_utils/gpu_utils.cu:    // re-set GPU to that originally set
src/gromacs/gpu_utils/gpu_utils.cu:    stat = cudaSetDevice(currentGpu);
src/gromacs/gpu_utils/gpu_utils.cu:    if (stat != cudaSuccess)
src/gromacs/gpu_utils/gpu_utils.cu:        CU_RET_ERR(stat, "cudaSetDevice in setupGpuDevicePeerAccess failed");
src/gromacs/gpu_utils/gpu_utils.cu:            "An unhandled error from a CUDA operation during the current MD step was detected:";
src/gromacs/gpu_utils/gpu_utils.cu:    gmx::checkDeviceError(cudaGetLastError(), errorPrefix);
src/gromacs/gpu_utils/gputraits_sycl.h:#ifndef GMX_GPU_UTILS_GPUTRAITS_SYCL_H
src/gromacs/gpu_utils/gputraits_sycl.h:#define GMX_GPU_UTILS_GPUTRAITS_SYCL_H
src/gromacs/gpu_utils/gputraits_sycl.h: * \ingroup module_gpu_utils
src/gromacs/gpu_utils/gputraits_sycl.h:#include "gromacs/gpu_utils/gmxsycl.h"
src/gromacs/gpu_utils/gputraits_sycl.h://! \brief Single GPU call timing event, not used with SYCL
src/gromacs/gpu_utils/gputraits_sycl.h: * GPU kernels scheduling description.
src/gromacs/gpu_utils/gputraits_sycl.h: * \note This struct uses CUDA/OpenCL layout, with the first dimension being contiguous.
src/gromacs/gpu_utils/gputraits_sycl.h:    //! Work groups (CUDA blocks) counts
src/gromacs/gpu_utils/gputraits_sycl.h:    //! Per work group (CUDA block) thread counts
src/gromacs/gpu_utils/gputraits_sycl.h: * But our \c prepareGpuKernelArguments and \c launchGpuKernel functions deal
src/gromacs/gpu_utils/gpu_kernel_utils.h:#ifndef GMX_GPU_UTILS_GPU_KERNEL_UTILS_H
src/gromacs/gpu_utils/gpu_kernel_utils.h:#define GMX_GPU_UTILS_GPU_KERNEL_UTILS_H
src/gromacs/gpu_utils/gpu_kernel_utils.h: *  NBNXM GPU kernel utility methods
src/gromacs/gpu_utils/gpu_kernel_utils.h: *  \ingroup module_gpu_utils
src/gromacs/gpu_utils/gpu_kernel_utils.h:#include "gputraits.h"
src/gromacs/gpu_utils/gpu_kernel_utils.h:#if GMX_GPU_SYCL
src/gromacs/gpu_utils/gpu_kernel_utils.h:#elif GMX_GPU_HIP || GMX_GPU_CUDA
src/gromacs/gpu_utils/gpu_kernel_utils.h:#    if GMX_GPU_HIP
src/gromacs/gpu_utils/gpu_kernel_utils.h:#        include "cuda_kernel_utils.cuh"
src/gromacs/gpu_utils/gpu_kernel_utils.h:#    error Including shared gpu kernel utilities header in unsupported build config
src/gromacs/gpu_utils/gpu_kernel_utils.h:#if GMX_GPU_SYCL
src/gromacs/gpu_utils/gpu_kernel_utils.h:static inline GMX_ALWAYS_INLINE float gmxGpuFDim(const float one, const float two)
src/gromacs/gpu_utils/gpu_kernel_utils.h:#if GMX_GPU_SYCL
src/gromacs/gpu_utils/gpu_kernel_utils.h:static inline GMX_ALWAYS_INLINE float gmxGpuExp(const float value)
src/gromacs/gpu_utils/gpu_kernel_utils.h:#if GMX_GPU_SYCL
src/gromacs/gpu_utils/gpu_kernel_utils.h:#elif GMX_GPU_CUDA
src/gromacs/gpu_utils/gpu_kernel_utils.h:#elif GMX_GPU_HIP
src/gromacs/gpu_utils/gpu_kernel_utils.h:static inline GMX_ALWAYS_INLINE float gmxGpuFma(const T valueOne, const T valueTwo, const T valueThree)
src/gromacs/gpu_utils/gpu_kernel_utils.h:#if GMX_GPU_SYCL
src/gromacs/gpu_utils/gpu_kernel_utils.h:#elif GMX_GPU_CUDA
src/gromacs/gpu_utils/gpu_kernel_utils.h:#elif GMX_GPU_HIP
src/gromacs/gpu_utils/gpu_kernel_utils.h:    return gmxGpuFma(t, d1, gmxGpuFma(-t, d0, d0));
src/gromacs/gpu_utils/gpu_kernel_utils.h:#if GMX_GPU_SYCL
src/gromacs/gpu_utils/device_stream_sycl.cpp: * \ingroup module_gpu_utils
src/gromacs/gpu_utils/device_stream_sycl.cpp:#include "gromacs/gpu_utils/device_context.h"
src/gromacs/gpu_utils/device_stream_sycl.cpp:#include "gromacs/gpu_utils/device_stream.h"
src/gromacs/gpu_utils/device_stream_sycl.cpp:    // For simplicity, we assume 0 to be the default priority (guaranteed for CUDA, verified for HIP)
src/gromacs/gpu_utils/device_stream_sycl.cpp:    // In both CUDA and HIP, lower value means higher priority, and values are automatically clamped
src/gromacs/gpu_utils/device_stream_sycl.cpp:    // Prevents use-after-free errors in hipSYCL's CUDA backend during unit tests
src/gromacs/gpu_utils/device_event.cuh: *  \brief Implements a DeviceEvent class for CUDA.
src/gromacs/gpu_utils/device_event.cuh:#ifndef GMX_GPU_UTILS_DEVICE_EVENT_CUH
src/gromacs/gpu_utils/device_event.cuh:#define GMX_GPU_UTILS_DEVICE_EVENT_CUH
src/gromacs/gpu_utils/device_event.cuh:#include "gromacs/gpu_utils/cudautils.cuh"
src/gromacs/gpu_utils/device_event.cuh:#include "gromacs/gpu_utils/device_stream.h"
src/gromacs/gpu_utils/device_event.cuh:#include "gromacs/gpu_utils/gputraits.cuh"
src/gromacs/gpu_utils/device_event.cuh:        cudaError_t stat = cudaEventCreateWithFlags(&event_, cudaEventDisableTiming);
src/gromacs/gpu_utils/device_event.cuh:        if (stat != cudaSuccess)
src/gromacs/gpu_utils/device_event.cuh:            GMX_THROW(gmx::InternalError("cudaEventCreate failed: " + gmx::getDeviceErrorString(stat)));
src/gromacs/gpu_utils/device_event.cuh:    ~DeviceEvent() { cudaEventDestroy(event_); }
src/gromacs/gpu_utils/device_event.cuh:        cudaError_t stat = cudaEventRecord(event_, deviceStream.stream());
src/gromacs/gpu_utils/device_event.cuh:        if (stat != cudaSuccess)
src/gromacs/gpu_utils/device_event.cuh:            GMX_THROW(gmx::InternalError("cudaEventRecord failed: " + gmx::getDeviceErrorString(stat)));
src/gromacs/gpu_utils/device_event.cuh:#    if GMX_HAVE_GPU_GRAPH_SUPPORT
src/gromacs/gpu_utils/device_event.cuh:    //! Marks the synchronization point in the \p stream, for external event while capturing a CUDA graph.
src/gromacs/gpu_utils/device_event.cuh:        cudaError_t stat =
src/gromacs/gpu_utils/device_event.cuh:                cudaEventRecordWithFlags(event_, deviceStream.stream(), cudaEventRecordExternal);
src/gromacs/gpu_utils/device_event.cuh:        if (stat != cudaSuccess)
src/gromacs/gpu_utils/device_event.cuh:            GMX_THROW(gmx::InternalError("cudaEventRecordWithFlags failed: "
src/gromacs/gpu_utils/device_event.cuh:        cudaError_t gmx_used_in_debug stat = cudaEventSynchronize(event_);
src/gromacs/gpu_utils/device_event.cuh:        if (stat != cudaSuccess)
src/gromacs/gpu_utils/device_event.cuh:            GMX_THROW(gmx::InternalError("cudaEventSynchronize failed: " + gmx::getDeviceErrorString(stat)));
src/gromacs/gpu_utils/device_event.cuh:        cudaError_t stat = cudaEventQuery(event_);
src/gromacs/gpu_utils/device_event.cuh:        if (stat != cudaSuccess && stat != cudaErrorNotReady)
src/gromacs/gpu_utils/device_event.cuh:            GMX_THROW(gmx::InternalError("cudaEventQuery failed: " + gmx::getDeviceErrorString(stat)));
src/gromacs/gpu_utils/device_event.cuh:        return (stat == cudaSuccess);
src/gromacs/gpu_utils/device_event.cuh:        cudaError_t stat = cudaStreamWaitEvent(deviceStream.stream(), event_, 0);
src/gromacs/gpu_utils/device_event.cuh:        if (stat != cudaSuccess)
src/gromacs/gpu_utils/device_event.cuh:            GMX_THROW(gmx::InternalError("cudaStreamWaitEvent failed: " + gmx::getDeviceErrorString(stat)));
src/gromacs/gpu_utils/device_event.cuh:#    if GMX_HAVE_GPU_GRAPH_SUPPORT
src/gromacs/gpu_utils/device_event.cuh:    //! Enqueues a wait for the recorded event in stream \p stream, for external event while capturing a CUDA graph.
src/gromacs/gpu_utils/device_event.cuh:        cudaError_t stat = cudaStreamWaitEvent(deviceStream.stream(), event_, cudaEventWaitExternal);
src/gromacs/gpu_utils/device_event.cuh:        if (stat != cudaSuccess)
src/gromacs/gpu_utils/device_event.cuh:            GMX_THROW(gmx::InternalError("cudaStreamWaitEvent failed: " + gmx::getDeviceErrorString(stat)));
src/gromacs/gpu_utils/device_event.cuh:    cudaEvent_t event_;
src/gromacs/gpu_utils/cuda_arch_utils.cuh:#ifndef CUDA_ARCH_UTILS_CUH_
src/gromacs/gpu_utils/cuda_arch_utils.cuh:#define CUDA_ARCH_UTILS_CUH_
src/gromacs/gpu_utils/cuda_arch_utils.cuh: *  \brief CUDA arch dependent definitions.
src/gromacs/gpu_utils/cuda_arch_utils.cuh: * intended to be used instead of __CUDA_ARCH__.
src/gromacs/gpu_utils/cuda_arch_utils.cuh:#ifndef __CUDA_ARCH__
src/gromacs/gpu_utils/cuda_arch_utils.cuh:#    define GMX_PTX_ARCH __CUDA_ARCH__
src/gromacs/gpu_utils/cuda_arch_utils.cuh:/* Until CC 5.2 and likely for the near future all NVIDIA architectures
src/gromacs/gpu_utils/cuda_arch_utils.cuh:/*! \brief Allow disabling CUDA textures using the GMX_DISABLE_CUDA_TEXTURES macro.
src/gromacs/gpu_utils/cuda_arch_utils.cuh:#if defined(GMX_DISABLE_CUDA_TEXTURES) || (defined(__clang__) && defined(__CUDA__)) \
src/gromacs/gpu_utils/cuda_arch_utils.cuh:#    define DISABLE_CUDA_TEXTURES 1
src/gromacs/gpu_utils/cuda_arch_utils.cuh:#    define DISABLE_CUDA_TEXTURES 0
src/gromacs/gpu_utils/cuda_arch_utils.cuh:/*! \brief True if the use of texture fetch in the CUDA kernels is disabled. */
src/gromacs/gpu_utils/cuda_arch_utils.cuh:static const bool c_disableCudaTextures = DISABLE_CUDA_TEXTURES;
src/gromacs/gpu_utils/cuda_arch_utils.cuh:/* CUDA architecture technical characteristics. Needs macros because it is used
src/gromacs/gpu_utils/cuda_arch_utils.cuh:#        define GMX_CUDA_MAX_BLOCKS_PER_MP 16
src/gromacs/gpu_utils/cuda_arch_utils.cuh:#        define GMX_CUDA_MAX_THREADS_PER_MP 2048
src/gromacs/gpu_utils/cuda_arch_utils.cuh:#        define GMX_CUDA_MAX_BLOCKS_PER_MP 16
src/gromacs/gpu_utils/cuda_arch_utils.cuh:#        define GMX_CUDA_MAX_THREADS_PER_MP 1024
src/gromacs/gpu_utils/cuda_arch_utils.cuh:#        define GMX_CUDA_MAX_BLOCKS_PER_MP 16
src/gromacs/gpu_utils/cuda_arch_utils.cuh:#        define GMX_CUDA_MAX_THREADS_PER_MP 1536
src/gromacs/gpu_utils/cuda_arch_utils.cuh:#        define GMX_CUDA_MAX_BLOCKS_PER_MP 32
src/gromacs/gpu_utils/cuda_arch_utils.cuh:#        define GMX_CUDA_MAX_THREADS_PER_MP 2048
src/gromacs/gpu_utils/cuda_arch_utils.cuh:#    define GMX_CUDA_MAX_BLOCKS_PER_MP 0
src/gromacs/gpu_utils/cuda_arch_utils.cuh:#    define GMX_CUDA_MAX_THREADS_PER_MP 0
src/gromacs/gpu_utils/cuda_arch_utils.cuh:// Macro defined for clang CUDA device compilation in the presence of debug symbols
src/gromacs/gpu_utils/cuda_arch_utils.cuh:#if defined(__clang__) && defined(__CUDA__) && defined(__CUDA_ARCH__) && !defined(NDEBUG)
src/gromacs/gpu_utils/cuda_arch_utils.cuh:#endif /* CUDA_ARCH_UTILS_CUH_ */
src/gromacs/gpu_utils/ocl_compiler.cpp: *  \brief Define infrastructure for OpenCL JIT compilation for Gromacs
src/gromacs/gpu_utils/ocl_compiler.cpp:#include "gromacs/gpu_utils/oclutils.h"
src/gromacs/gpu_utils/ocl_compiler.cpp:/*! \brief True if OpenCL binary caching is enabled.
src/gromacs/gpu_utils/ocl_compiler.cpp:/*! \brief Handles writing the OpenCL JIT compilation log to \c fplog.
src/gromacs/gpu_utils/ocl_compiler.cpp: * variable is set or the compilation failed, then the OpenCL
src/gromacs/gpu_utils/ocl_compiler.cpp: * \param program             OpenCL program that was compiled
src/gromacs/gpu_utils/ocl_compiler.cpp: * \param buildFailed         Whether the OpenCL build succeeded
src/gromacs/gpu_utils/ocl_compiler.cpp:        GMX_THROW(InternalError("Could not get OpenCL program build log size, error was "
src/gromacs/gpu_utils/ocl_compiler.cpp:            GMX_THROW(InternalError("Could not get OpenCL program build log, error was "
src/gromacs/gpu_utils/ocl_compiler.cpp:        // In CUDA this is triggered by the -use_fast_math flag, equivalent with
src/gromacs/gpu_utils/ocl_compiler.cpp:    if ((deviceVendor == DeviceVendor::Nvidia) && getenv("GMX_OCL_VERBOSE"))
src/gromacs/gpu_utils/ocl_compiler.cpp:        /* To dump OpenCL build intermediate files, caching must be off */
src/gromacs/gpu_utils/ocl_compiler.cpp:/*! \brief Get the path to the folder storing an OpenCL source file.
src/gromacs/gpu_utils/ocl_compiler.cpp: * By default, this function constructs the full path to the OpenCL from
src/gromacs/gpu_utils/ocl_compiler.cpp: * \return OS-normalized path string to the folder storing OpenCL source file
src/gromacs/gpu_utils/ocl_compiler.cpp:                    formatString("GMX_OCL_FILE_PATH must point to the directory where OpenCL"
src/gromacs/gpu_utils/ocl_compiler.cpp:        GMX_THROW(InternalError("Could not query OpenCL preferred workgroup size, error was "
src/gromacs/gpu_utils/ocl_compiler.cpp:        GMX_THROW(InternalError(formatString("Invalid OpenCL warp size encountered")));
src/gromacs/gpu_utils/ocl_compiler.cpp:        GMX_THROW(InternalError("Could not create OpenCL program to determine warp size, error was "
src/gromacs/gpu_utils/ocl_compiler.cpp:        GMX_THROW(InternalError("Could not build OpenCL program to determine warp size, error was "
src/gromacs/gpu_utils/ocl_compiler.cpp:        GMX_THROW(InternalError("Could not create OpenCL kernel to determine warp size, error was "
src/gromacs/gpu_utils/ocl_compiler.cpp:        GMX_THROW(InternalError("Could not release OpenCL warp-size kernel, error was "
src/gromacs/gpu_utils/ocl_compiler.cpp:        GMX_THROW(InternalError("Could not release OpenCL warp-size program, error was "
src/gromacs/gpu_utils/ocl_compiler.cpp:        case DeviceVendor::Nvidia: return "-D_NVIDIA_SOURCE_";
src/gromacs/gpu_utils/ocl_compiler.cpp: * All OpenCL kernel files are expected to be stored in one single folder.
src/gromacs/gpu_utils/ocl_compiler.cpp:     * OpenCL implementations are happy with. Since the standard still says
src/gromacs/gpu_utils/ocl_compiler.cpp:/*! \brief Builds a string with build options for the OpenCL kernels
src/gromacs/gpu_utils/ocl_compiler.cpp:    GMX_RELEASE_ASSERT(fplog != nullptr, "Need a valid log file for building OpenCL programs");
src/gromacs/gpu_utils/ocl_compiler.cpp:    /* Load OpenCL source files */
src/gromacs/gpu_utils/ocl_compiler.cpp:    /* Create OpenCL program */
src/gromacs/gpu_utils/ocl_compiler.cpp:                    "OpenCL binary cache file %s is present, will load kernels.\n",
src/gromacs/gpu_utils/ocl_compiler.cpp:                    "No OpenCL binary cache file was present for %s, so will compile kernels "
src/gromacs/gpu_utils/ocl_compiler.cpp:        // Compile OpenCL program from source
src/gromacs/gpu_utils/ocl_compiler.cpp:            GMX_THROW(FileIOError(gmx::formatString("Error loading OpenCL code %s",
src/gromacs/gpu_utils/ocl_compiler.cpp:            GMX_THROW(InternalError("Could not create OpenCL program, error was "
src/gromacs/gpu_utils/ocl_compiler.cpp:    /* Build the OpenCL program, keeping the status to potentially
src/gromacs/gpu_utils/ocl_compiler.cpp:        GMX_THROW(InternalError("Could not build OpenCL program, error was "
src/gromacs/gpu_utils/ocl_compiler.cpp:            /* If OpenCL caching is ON, but the current cache is not
src/gromacs/gpu_utils/ocl_compiler.cpp:    if ((deviceVendor == DeviceVendor::Nvidia) && getenv("GMX_OCL_DUMP_INTERM_FILES"))
src/gromacs/gpu_utils/ocl_compiler.cpp:        /* If dumping intermediate files has been requested and this is an NVIDIA card
src/gromacs/gpu_utils/ocl_compiler.cpp:            GMX_THROW(InternalError("Could not get OpenCL device info, error was "
src/gromacs/gpu_utils/nvshmem_utils.cpp: * \author Mahesh Doijade <mdoijade@nvidia.com>
src/gromacs/gpu_utils/nvshmem_utils.cpp: * \ingroup module_gpu_utils
src/gromacs/gpu_utils/nvshmem_utils.cpp:#include "gromacs/gpu_utils/hostallocator.h"
src/gromacs/gpu_utils/nvshmem_utils.cpp:#if GMX_GPU
src/gromacs/gpu_utils/nvshmem_utils.cpp:#    include "gromacs/gpu_utils/devicebuffer.h"
src/gromacs/gpu_utils/nvshmem_utils.cpp:                        "Note: To use multiple processses per GPU NVSHMEM requires MPS enabled "
src/gromacs/gpu_utils/nvshmem_utils.cpp:                        "total active thread percentage of all PEs on the same GPU to be under "
src/gromacs/gpu_utils/nvshmem_utils.cpp:                        "100%% for multi-process GPU sharing.\n");
src/gromacs/gpu_utils/nvshmem_utils.cpp:#if GMX_GPU
src/gromacs/gpu_utils/nvshmem_utils.cpp:                                     GpuApiCallBehavior::Async,
src/gromacs/gpu_utils/vectype_ops_cuda_hip_shared_declarations.h:#ifndef GMX_GPU_UTILS_VECTYPE_OPS_GPU_SHARED_DECLARATIONS_H
src/gromacs/gpu_utils/vectype_ops_cuda_hip_shared_declarations.h:#define GMX_GPU_UTILS_VECTYPE_OPS_GPU_SHARED_DECLARATIONS_H
src/gromacs/gpu_utils/vectype_ops_cuda_hip_shared_declarations.h:#if !defined(__CUDACC__) && !defined(__HIPCC__)
src/gromacs/gpu_utils/vectype_ops_cuda_hip_shared_declarations.h:#    error Including header specific for CUDA or HIP device code without compiling the file with the correct compiler
src/gromacs/gpu_utils/vectype_ops_cuda_hip_shared_declarations.h:#endif /* GMX_GPU_UTILS_VECTYPE_OPS_CUDA_HIP_SHARED_DECLARATIONS_H */
src/gromacs/gpu_utils/clfftinitializer.h: * tears down the clFFT library resources in OpenCL builds,
src/gromacs/gpu_utils/clfftinitializer.h: * clFFT itself is used in the OpenCL implementation of PME
src/gromacs/gpu_utils/clfftinitializer.h: * for 3D R2C/C2R transforms. It is know to work with NVidia
src/gromacs/gpu_utils/clfftinitializer.h: * OpenCL, AMD fglrx and AMDGPU-PRO drivers, and to not work with
src/gromacs/gpu_utils/clfftinitializer.h: * AMD Rocm dev preview as of May 2018 (#2515).
src/gromacs/gpu_utils/clfftinitializer.h:#ifndef GMX_GPU_UTILS_CLFFTINITIALIZER_H
src/gromacs/gpu_utils/clfftinitializer.h:#define GMX_GPU_UTILS_CLFFTINITIALIZER_H
src/gromacs/gpu_utils/gputraits_ocl.h:#ifndef GMX_GPU_UTILS_GPUTRAITS_OCL_H
src/gromacs/gpu_utils/gputraits_ocl.h:#define GMX_GPU_UTILS_GPUTRAITS_OCL_H
src/gromacs/gpu_utils/gputraits_ocl.h: *  \brief Declares the OpenCL type traits.
src/gromacs/gpu_utils/gputraits_ocl.h: * \ingroup module_gpu_utils
src/gromacs/gpu_utils/gputraits_ocl.h:#include "gromacs/gpu_utils/gmxopencl.h"
src/gromacs/gpu_utils/gputraits_ocl.h://! \brief Single GPU call timing event
src/gromacs/gpu_utils/gputraits_ocl.h: * GPU kernels scheduling description. This is same in OpenCL/CUDA.
src/gromacs/gpu_utils/gputraits_ocl.h: * Provides reasonable defaults, one typically only needs to set the GPU stream
src/gromacs/gpu_utils/gputraits_ocl.h:    //! Work groups (CUDA blocks) counts
src/gromacs/gpu_utils/gputraits_ocl.h:    //! Per work group (CUDA block) thread counts
src/gromacs/gpu_utils/gputraits_ocl.h: * Note that OpenCL 2.x might be able to do this, but we use 1.2.
src/gromacs/gpu_utils/device_utils_hip_sycl.h:#ifndef GMX_GPU_UTILS_DEVICE_UTILS_HIP_SYCL_H
src/gromacs/gpu_utils/device_utils_hip_sycl.h:#define GMX_GPU_UTILS_DEVICE_UTILS_HIP_SYCL_H
src/gromacs/gpu_utils/device_utils_hip_sycl.h:#    if GMX_GPU_SYCL
src/gromacs/gpu_utils/device_utils_hip_sycl.h:#        include "gputraits_sycl.h"
src/gromacs/gpu_utils/device_utils_hip_sycl.h:#    elif GMX_GPU_HIP
src/gromacs/gpu_utils/device_utils_hip_sycl.h:#        include "gputraits_hip.h"
src/gromacs/gpu_utils/device_utils_hip_sycl.h: * Ref: https://gpuopen.com/learn/amd-gcn-assembly-cross-lane-operations
src/gromacs/gpu_utils/device_utils_hip_sycl.h: * when operating on float2, in a SIMD2-fashion. Compiler (at least up to ROCm 5.6)
src/gromacs/gpu_utils/device_utils_hip_sycl.h:#endif /* GMX_GPU_UTILS_WAVE_MOVE_DPP_H */
src/gromacs/gpu_utils/oclutils.h: *  \brief Declare utility routines for OpenCL
src/gromacs/gpu_utils/oclutils.h:#ifndef GMX_GPU_UTILS_OCLUTILS_H
src/gromacs/gpu_utils/oclutils.h:#define GMX_GPU_UTILS_OCLUTILS_H
src/gromacs/gpu_utils/oclutils.h:#include "gromacs/gpu_utils/device_context.h"
src/gromacs/gpu_utils/oclutils.h:#include "gromacs/gpu_utils/device_stream.h"
src/gromacs/gpu_utils/oclutils.h:#include "gromacs/gpu_utils/gmxopencl.h"
src/gromacs/gpu_utils/oclutils.h:#include "gromacs/gpu_utils/gputraits_ocl.h"
src/gromacs/gpu_utils/oclutils.h: * \brief OpenCL GPU runtime data
src/gromacs/gpu_utils/oclutils.h:    //! OpenCL program
src/gromacs/gpu_utils/oclutils.h:/*! \brief Pretend to synchronize an OpenCL stream (dummy implementation).
src/gromacs/gpu_utils/oclutils.h: *  \returns  Not implemented in OpenCL.
src/gromacs/gpu_utils/oclutils.h:    GMX_RELEASE_ASSERT(false, "haveStreamTasksCompleted is not implemented for OpenCL");
src/gromacs/gpu_utils/oclutils.h: * A function for setting up a single OpenCL kernel argument.
src/gromacs/gpu_utils/oclutils.h:void inline prepareGpuKernelArgument(cl_kernel kernel, const KernelLaunchConfig& config, size_t argIndex)
src/gromacs/gpu_utils/oclutils.h: * Compile-time recursive function for setting up a single OpenCL kernel argument.
src/gromacs/gpu_utils/oclutils.h:void prepareGpuKernelArgument(cl_kernel                 kernel,
src/gromacs/gpu_utils/oclutils.h:    // (as per section 6.9 of the OpenCL spec).
src/gromacs/gpu_utils/oclutils.h:                  "Invalid type passed to OpenCL kernel functions (see OpenCL spec section 6.9).");
src/gromacs/gpu_utils/oclutils.h:    prepareGpuKernelArgument(kernel, config, argIndex + 1, otherArgsPtrs...);
src/gromacs/gpu_utils/oclutils.h: * A wrapper function for setting up all the OpenCL kernel arguments.
src/gromacs/gpu_utils/oclutils.h: * \returns A handle for the prepared parameter pack to be used with launchGpuKernel() as the last argument
src/gromacs/gpu_utils/oclutils.h: * - currently always nullptr for OpenCL, as it manages kernel/arguments association by itself.
src/gromacs/gpu_utils/oclutils.h:void* prepareGpuKernelArguments(cl_kernel kernel, const KernelLaunchConfig& config, const Args*... argsPtrs)
src/gromacs/gpu_utils/oclutils.h:    prepareGpuKernelArgument(kernel, config, 0, argsPtrs...);
src/gromacs/gpu_utils/oclutils.h:/*! \brief Launches the OpenCL kernel and handles the errors.
src/gromacs/gpu_utils/oclutils.h: * \param[in] deviceStream    GPU stream to launch kernel in
src/gromacs/gpu_utils/oclutils.h: * \param[in] timingEvent     Timing event, fetched from GpuRegionTimer
src/gromacs/gpu_utils/oclutils.h:inline void launchGpuKernel(cl_kernel                 kernel,
src/gromacs/gpu_utils/oclutils.h:        const std::string errorMessage = "GPU kernel (" + std::string(kernelName)
src/gromacs/gpu_utils/nvshmem_utils.h: * \author Mahesh Doijade <mdoijade@nvidia.com>
src/gromacs/gpu_utils/nvshmem_utils.h: * \ingroup module_gpu_utils
src/gromacs/gpu_utils/nvshmem_utils.h:#include "gromacs/gpu_utils/device_stream_manager.h"
src/gromacs/gpu_utils/nvshmem_utils.h:#include "gromacs/gpu_utils/devicebuffer_datatype.h"
src/gromacs/gpu_utils/vectype_ops_cuda_hip_shared_trig_math.h:#ifndef GMX_GPU_UTILS_VECTYPE_OPS_SHARED_GPU_TRIG_MATH_H
src/gromacs/gpu_utils/vectype_ops_cuda_hip_shared_trig_math.h:#define GMX_GPU_UTILS_VECTYPE_OPS_SHARED_GPU_TRIG_MATH_H
src/gromacs/gpu_utils/vectype_ops_cuda_hip_shared_trig_math.h:#if !defined(__CUDACC__) && !defined(__HIPCC__)
src/gromacs/gpu_utils/vectype_ops_cuda_hip_shared_trig_math.h:#    error Including header specific for CUDA or HIP device code without compiling the file with the correct compiler
src/gromacs/gpu_utils/vectype_ops_cuda_hip_shared_trig_math.h:#endif /* GMX_GPU_UTILS_VECTYPE_OPS_SHARED_CUDA_HIP_TRIG_MATH_H */
src/gromacs/gpu_utils/oclutils.cpp: *  \brief Define utility routines for OpenCL
src/gromacs/gpu_utils/oclutils.cpp:#include "gromacs/gpu_utils/gpu_utils.h"
src/gromacs/gpu_utils/oclutils.cpp:        default: return "Unknown OpenCL error: " + std::to_string(static_cast<int32_t>(error));
src/gromacs/gpu_utils/device_stream.h:#ifndef GMX_GPU_UTILS_DEVICE_STREAM_H
src/gromacs/gpu_utils/device_stream.h:#define GMX_GPU_UTILS_DEVICE_STREAM_H
src/gromacs/gpu_utils/device_stream.h: * \ingroup module_gpu_utils
src/gromacs/gpu_utils/device_stream.h:#if GMX_GPU_CUDA
src/gromacs/gpu_utils/device_stream.h:#    include <cuda_runtime.h>
src/gromacs/gpu_utils/device_stream.h:#elif GMX_GPU_HIP
src/gromacs/gpu_utils/device_stream.h:#elif GMX_GPU_OPENCL
src/gromacs/gpu_utils/device_stream.h:#    include "gromacs/gpu_utils/gmxopencl.h"
src/gromacs/gpu_utils/device_stream.h:#elif GMX_GPU_SYCL
src/gromacs/gpu_utils/device_stream.h:#    include "gromacs/gpu_utils/gmxsycl.h"
src/gromacs/gpu_utils/device_stream.h:     * \param[in] priority       Stream priority: high or normal (ignored in OpenCL).
src/gromacs/gpu_utils/device_stream.h:     * \param[in] useTiming      If the timing should be enabled (ignored in CUDA).
src/gromacs/gpu_utils/device_stream.h:#if GMX_GPU_CUDA
src/gromacs/gpu_utils/device_stream.h:    cudaStream_t stream() const;
src/gromacs/gpu_utils/device_stream.h:    cudaStream_t stream_ = nullptr;
src/gromacs/gpu_utils/device_stream.h:#elif GMX_GPU_HIP
src/gromacs/gpu_utils/device_stream.h:#elif GMX_GPU_SYCL
src/gromacs/gpu_utils/device_stream.h:#elif GMX_GPU_OPENCL || defined DOXYGEN
src/gromacs/gpu_utils/device_stream.h:/*! \brief Helper function to flush the commands in OpenCL. No-op in other backends.
src/gromacs/gpu_utils/device_stream.h: * Based on the section 5.13 of the OpenCL 1.2 spec (section 5.15 in OpenCL 3.0 spec), a flush is
src/gromacs/gpu_utils/device_stream.h:#endif // GMX_GPU_UTILS_DEVICE_STREAM_H
src/gromacs/gpu_utils/device_context.cpp: * \ingroup module_gpu_utils
src/gromacs/gpu_utils/pmalloc.cu: *  \brief Define functions for host-side memory handling when using CUDA devices.
src/gromacs/gpu_utils/pmalloc.cu:#include "gromacs/gpu_utils/cudautils.cuh"
src/gromacs/gpu_utils/pmalloc.cu: *  free functions provied by the CUDA library).
src/gromacs/gpu_utils/pmalloc.cu:    cudaError_t stat;
src/gromacs/gpu_utils/pmalloc.cu:    int         flag = cudaHostAllocDefault;
src/gromacs/gpu_utils/pmalloc.cu:    stat = cudaMallocHost(h_ptr, nbytes, flag);
src/gromacs/gpu_utils/pmalloc.cu:    sprintf(strbuf, "cudaMallocHost of size %d bytes failed", static_cast<int>(nbytes));
src/gromacs/gpu_utils/pmalloc.cu: *  memory allocated directly with CUDA API calls.
src/gromacs/gpu_utils/pmalloc.cu:    cudaError_t stat;
src/gromacs/gpu_utils/pmalloc.cu:    stat = cudaFreeHost(h_ptr);
src/gromacs/gpu_utils/pmalloc.cu:    CU_RET_ERR(stat, "cudaFreeHost failed");
src/gromacs/gpu_utils/pmalloc.cu:    // We don't need context for CUDA's pmalloc.
src/gromacs/gpu_utils/pmalloc.cu:    // We don't need context for CUDA's pmalloc, so we have nothing to clear.
src/gromacs/gpu_utils/devicebuffer.h:#ifndef GMX_GPU_UTILS_DEVICEBUFFER_H
src/gromacs/gpu_utils/devicebuffer.h:#define GMX_GPU_UTILS_DEVICEBUFFER_H
src/gromacs/gpu_utils/devicebuffer.h: *  \brief Implements the logic for handling of DeviceBuffer types in OpenCL, CUDA and SYCL.
src/gromacs/gpu_utils/devicebuffer.h: *  Can only be included on GPU build paths.
src/gromacs/gpu_utils/devicebuffer.h:#include "gromacs/gpu_utils/devicebuffer_datatype.h"
src/gromacs/gpu_utils/devicebuffer.h:#if GMX_GPU_CUDA
src/gromacs/gpu_utils/devicebuffer.h:#    include "gromacs/gpu_utils/devicebuffer.cuh"
src/gromacs/gpu_utils/devicebuffer.h:#elif GMX_GPU_HIP
src/gromacs/gpu_utils/devicebuffer.h:#    include "gromacs/gpu_utils/devicebuffer_hip.h"
src/gromacs/gpu_utils/devicebuffer.h:#elif GMX_GPU_OPENCL
src/gromacs/gpu_utils/devicebuffer.h:#    include "gromacs/gpu_utils/devicebuffer_ocl.h"
src/gromacs/gpu_utils/devicebuffer.h:#elif GMX_GPU_SYCL
src/gromacs/gpu_utils/devicebuffer.h:#    include "gromacs/gpu_utils/devicebuffer_sycl.h"
src/gromacs/gpu_utils/devicebuffer.h:#    error "devicebuffer.h included on non-GPU build!"
src/gromacs/gpu_utils/devicebuffer_sycl.cpp: *  \ingroup module_gpu_utils
src/gromacs/gpu_utils/gputraits.cuh:#ifndef GMX_GPU_UTILS_GPUTRAITS_CUH
src/gromacs/gpu_utils/gputraits.cuh:#define GMX_GPU_UTILS_GPUTRAITS_CUH
src/gromacs/gpu_utils/gputraits.cuh: *  \brief Declares the CUDA type traits.
src/gromacs/gpu_utils/gputraits.cuh: * \ingroup module_gpu_utils
src/gromacs/gpu_utils/gputraits.cuh:#include <cuda_runtime.h>
src/gromacs/gpu_utils/gputraits.cuh:using DeviceTexture = cudaTextureObject_t;
src/gromacs/gpu_utils/gputraits.cuh://! \brief Single GPU call timing event - meaningless in CUDA
src/gromacs/gpu_utils/gputraits.cuh: * GPU kernels scheduling description. This is same in OpenCL/CUDA.
src/gromacs/gpu_utils/gputraits.cuh: * Provides reasonable defaults, one typically only needs to set the GPU stream
src/gromacs/gpu_utils/typecasts_cuda_hip.h: *  \brief Declare functions to be used to cast CPU types to compatible GPU types.
src/gromacs/gpu_utils/typecasts_cuda_hip.h:#ifndef GMX_GPU_UTILS_TYPECASTS_CUDA_HIP_H
src/gromacs/gpu_utils/typecasts_cuda_hip.h:#define GMX_GPU_UTILS_TYPECASTS_CUDA_HIP_H
src/gromacs/gpu_utils/typecasts_cuda_hip.h:#endif // GMX_GPU_UTILS_TYPECASTS_CUDA_HIP_H
src/gromacs/gpu_utils/gpueventsynchronizer.h: *  \brief Implements a GpuEventSynchronizer class.
src/gromacs/gpu_utils/gpueventsynchronizer.h:#ifndef GMX_GPU_UTILS_GPUEVENTSYNCHRONIZER_H
src/gromacs/gpu_utils/gpueventsynchronizer.h:#define GMX_GPU_UTILS_GPUEVENTSYNCHRONIZER_H
src/gromacs/gpu_utils/gpueventsynchronizer.h: * A class which allows for CPU thread to mark and wait for certain GPU stream execution point.
src/gromacs/gpu_utils/gpueventsynchronizer.h: * The event can be put into the stream with \ref GpuEventSynchronizer::markEvent and then later
src/gromacs/gpu_utils/gpueventsynchronizer.h: * waited on with \ref GpuEventSynchronizer::waitForEvent or
src/gromacs/gpu_utils/gpueventsynchronizer.h: * \ref GpuEventSynchronizer::enqueueWaitEvent.
src/gromacs/gpu_utils/gpueventsynchronizer.h: * - \ref GpuEventSynchronizer::reset returns object into the initial <em>fully consumed</em> state.
src/gromacs/gpu_utils/gpueventsynchronizer.h: * - \ref GpuEventSynchronizer::consume \em consumes the event, without doing anything else.
src/gromacs/gpu_utils/gpueventsynchronizer.h: * - \ref GpuEventSynchronizer::markEvent enqueues new event into the provided stream, and sets \c to 0.
src/gromacs/gpu_utils/gpueventsynchronizer.h: * - \ref GpuEventSynchronizer::waitForEvent \em consumes the event and blocks the host thread until
src/gromacs/gpu_utils/gpueventsynchronizer.h: * - \ref GpuEventSynchronizer::enqueueWaitEvent \em consumes the event and blocks the inserts
src/gromacs/gpu_utils/gpueventsynchronizer.h: * \ref GpuEventSynchronizer::markEvent must be followed by exactly one
src/gromacs/gpu_utils/gpueventsynchronizer.h: * \ref GpuEventSynchronizer::enqueueWaitEvent or \ref GpuEventSynchronizer::enqueueWaitEvent.
src/gromacs/gpu_utils/gpueventsynchronizer.h:/* With CUDA, we only want to use event counting in known "good" configurations. With OpenCL
src/gromacs/gpu_utils/gpueventsynchronizer.h: * and SYCL, we want it to be enabled always. So, we have a global flag in CUDA build, and
src/gromacs/gpu_utils/gpueventsynchronizer.h:#if GMX_GPU_CUDA
src/gromacs/gpu_utils/gpueventsynchronizer.h:extern bool g_useEventConsumptionCounting; // Defined in gpueventsynchronizer_helpers.h
src/gromacs/gpu_utils/gpueventsynchronizer.h:class GpuEventSynchronizer
src/gromacs/gpu_utils/gpueventsynchronizer.h:    GpuEventSynchronizer(int minConsumptionCount, int maxConsumptionCount) :
src/gromacs/gpu_utils/gpueventsynchronizer.h:    GpuEventSynchronizer() : GpuEventSynchronizer(1, 1) {}
src/gromacs/gpu_utils/gpueventsynchronizer.h:    ~GpuEventSynchronizer() = default;
src/gromacs/gpu_utils/gpueventsynchronizer.h:    GpuEventSynchronizer& operator=(const GpuEventSynchronizer&) = delete;
src/gromacs/gpu_utils/gpueventsynchronizer.h:    GpuEventSynchronizer(const GpuEventSynchronizer&) = delete;
src/gromacs/gpu_utils/gpueventsynchronizer.h:    GpuEventSynchronizer& operator=(GpuEventSynchronizer&&) = delete;
src/gromacs/gpu_utils/gpueventsynchronizer.h:    GpuEventSynchronizer(GpuEventSynchronizer&&) = delete;
src/gromacs/gpu_utils/gpueventsynchronizer.h:#if GMX_HAVE_GPU_GRAPH_SUPPORT && GMX_GPU_CUDA
src/gromacs/gpu_utils/gpueventsynchronizer.h:                "markExternalEventWhileCapturingGraph called without GPU graph support"));
src/gromacs/gpu_utils/gpueventsynchronizer.h:#if GMX_HAVE_GPU_GRAPH_SUPPORT && GMX_GPU_CUDA
src/gromacs/gpu_utils/gpueventsynchronizer.h:                "enqueueExternalWaitEventWhileCapturingGraph called without GPU graph support"));
src/gromacs/gpu_utils/device_stream_manager.cpp: * \brief Implements GPU stream manager.
src/gromacs/gpu_utils/device_stream_manager.cpp: * \ingroup module_gpu_utils
src/gromacs/gpu_utils/device_stream_manager.cpp:#include "gromacs/gpu_utils/device_context.h"
src/gromacs/gpu_utils/device_stream_manager.cpp:#include "gromacs/gpu_utils/device_stream.h"
src/gromacs/gpu_utils/device_stream_manager.cpp: * \brief Impl class to manages the lifetime of the GPU context
src/gromacs/gpu_utils/device_stream_manager.cpp: * If supported by the GPU API, the available runtime and the
src/gromacs/gpu_utils/device_stream_manager.cpp:    //! GPU command streams.
src/gromacs/gpu_utils/device_stream_manager.cpp:        if (simulationWork.useGpuPme)
src/gromacs/gpu_utils/device_stream_manager.cpp:            /* Creating a PME GPU stream:
src/gromacs/gpu_utils/device_stream_manager.cpp:             * - default high priority with CUDA
src/gromacs/gpu_utils/device_stream_manager.cpp:             * - no priorities implemented yet with OpenCL; see #2532
src/gromacs/gpu_utils/device_stream_manager.cpp:        // Update stream is used both for coordinates transfers and for GPU update/constraints
src/gromacs/gpu_utils/device_stream_manager.cpp:        if (simulationWork.useGpuPme || simulationWork.useGpuUpdate || simulationWork.useGpuXBufferOpsWhenAllowed)
src/gromacs/gpu_utils/device_stream_manager.cpp:        if (simulationWork.useGpuPmePpCommunication)
src/gromacs/gpu_utils/device_stream_manager.cpp:                           "GPU non-bonded non-local stream should be valid in order to use GPU "
src/gromacs/gpu_utils/device_stream_manager.cpp:                           "GPU non-bonded local stream should be valid in order to use GPU "
src/gromacs/gpu_utils/pmalloc_sycl.cpp: * Unlike in CUDA, pinning memory in SYCL requires a context. It can be passed explicitly to
src/gromacs/gpu_utils/pmalloc_sycl.cpp:#include "gromacs/gpu_utils/device_context.h"
src/gromacs/gpu_utils/pmalloc_sycl.cpp:#include "gromacs/gpu_utils/gmxsycl.h"
src/gromacs/gpu_utils/gpu_utils_hip.cpp:#include "gromacs/gpu_utils/hiputils.h"
src/gromacs/gpu_utils/gpu_utils_hip.cpp:#include "gpu_utils.h"
src/gromacs/gpu_utils/gpu_utils_hip.cpp:            // If the buffer was not allocated by ROCm, then it will not be
src/gromacs/gpu_utils/gpu_utils_hip.cpp:void startGpuProfiler() {}
src/gromacs/gpu_utils/gpu_utils_hip.cpp:void stopGpuProfiler() {}
src/gromacs/gpu_utils/gpu_utils_hip.cpp:void resetGpuProfiler() {}
src/gromacs/gpu_utils/gpu_utils_hip.cpp: * run is trying to access an invalid GPU, so we throw an error. If
src/gromacs/gpu_utils/gpu_utils_hip.cpp: * \param[in] gpuA           ID for GPU initiating peer access call
src/gromacs/gpu_utils/gpu_utils_hip.cpp: * \param[in] gpuB           ID for remote GPU
src/gromacs/gpu_utils/gpu_utils_hip.cpp:                                const int            gpuA,
src/gromacs/gpu_utils/gpu_utils_hip.cpp:                                const int            gpuB,
src/gromacs/gpu_utils/gpu_utils_hip.cpp:                gmx::formatString("%s from GPU %d to GPU %d failed", hipCallName, gpuA, gpuB);
src/gromacs/gpu_utils/gpu_utils_hip.cpp:                        "GPU peer access not enabled between GPUs %d and %d due to unexpected "
src/gromacs/gpu_utils/gpu_utils_hip.cpp:                        gpuA,
src/gromacs/gpu_utils/gpu_utils_hip.cpp:                        gpuB,
src/gromacs/gpu_utils/gpu_utils_hip.cpp:void setupGpuDevicePeerAccess(gmx::ArrayRef<const int> gpuIdsToUse, const gmx::MDLogger& mdlog)
src/gromacs/gpu_utils/gpu_utils_hip.cpp:    // take a note of currently-set GPU
src/gromacs/gpu_utils/gpu_utils_hip.cpp:    int currentGpu;
src/gromacs/gpu_utils/gpu_utils_hip.cpp:    stat = hipGetDevice(&currentGpu);
src/gromacs/gpu_utils/gpu_utils_hip.cpp:    gmx::checkDeviceError(stat, "hipGetDevice in setupGpuDevicePeerAccess failed");
src/gromacs/gpu_utils/gpu_utils_hip.cpp:            "Note: Peer access enabled between the following GPU pairs in the node:\n ");
src/gromacs/gpu_utils/gpu_utils_hip.cpp:    for (unsigned int i = 0; i < gpuIdsToUse.size(); i++)
src/gromacs/gpu_utils/gpu_utils_hip.cpp:        int gpuA = gpuIdsToUse[i];
src/gromacs/gpu_utils/gpu_utils_hip.cpp:        stat     = hipSetDevice(gpuA);
src/gromacs/gpu_utils/gpu_utils_hip.cpp:                            "GPU peer access not enabled due to unexpected return value from "
src/gromacs/gpu_utils/gpu_utils_hip.cpp:                            gpuA,
src/gromacs/gpu_utils/gpu_utils_hip.cpp:        for (unsigned int j = 0; j < gpuIdsToUse.size(); j++)
src/gromacs/gpu_utils/gpu_utils_hip.cpp:                int gpuB          = gpuIdsToUse[j];
src/gromacs/gpu_utils/gpu_utils_hip.cpp:                stat              = hipDeviceCanAccessPeer(&canAccessPeer, gpuA, gpuB);
src/gromacs/gpu_utils/gpu_utils_hip.cpp:                peerAccessCheckStat(stat, gpuA, gpuB, mdlog, "hipDeviceCanAccessPeer");
src/gromacs/gpu_utils/gpu_utils_hip.cpp:                    stat = hipDeviceEnablePeerAccess(gpuB, 0);
src/gromacs/gpu_utils/gpu_utils_hip.cpp:                    peerAccessCheckStat(stat, gpuA, gpuB, mdlog, "hipDeviceEnablePeerAccess");
src/gromacs/gpu_utils/gpu_utils_hip.cpp:                    message           = gmx::formatString("%s%d->%d ", message.c_str(), gpuA, gpuB);
src/gromacs/gpu_utils/gpu_utils_hip.cpp:    // re-set GPU to that originally set
src/gromacs/gpu_utils/gpu_utils_hip.cpp:    stat = hipSetDevice(currentGpu);
src/gromacs/gpu_utils/gpu_utils_hip.cpp:    gmx::checkDeviceError(stat, "hipSetDevice in setupGpuDevicePeerAccess failed");
src/gromacs/gpu_utils/ocl_caching.h: *  \brief Declare infrastructure for managing caching of OpenCL
src/gromacs/gpu_utils/ocl_caching.h:#ifndef GMX_GPU_UTILS_OCL_CACHING_H
src/gromacs/gpu_utils/ocl_caching.h:#define GMX_GPU_UTILS_OCL_CACHING_H
src/gromacs/gpu_utils/ocl_caching.h:#include "gromacs/gpu_utils/oclutils.h"
src/gromacs/gpu_utils/ocl_caching.h: * \param[in]  context    The OpenCL context
src/gromacs/gpu_utils/ocl_caching.h: * \returns The OpenCL program read from the cache
src/gromacs/gpu_utils/ocl_caching.h: * \throws InternalError  if an OpenCL error was encountered
src/gromacs/gpu_utils/ocl_caching.h:/*! \brief Implement caching of OpenCL binaries
src/gromacs/gpu_utils/ocl_caching.h: * \throws InternalError  if an OpenCL error was encountered
src/gromacs/gpu_utils/tests/nvshmem_test.cpp: * \author Mahesh Doijade <mdoijade@nvidia.com>
src/gromacs/gpu_utils/tests/nvshmem_test.cpp:#    include "gromacs/gpu_utils/gputraits.h"
src/gromacs/gpu_utils/tests/nvshmem_test.cpp:        FAIL() << "No compatible GPUs to test on.";
src/gromacs/gpu_utils/tests/nvshmem_test.cpp:        FAIL() << "NVSHMEM requires minimum Volta or higher GPUs";
src/gromacs/gpu_utils/tests/typecasts_runner.h: * Header for runner for CUDA float3 type layout tests.
src/gromacs/gpu_utils/tests/typecasts_runner.h:#ifndef GMX_GPU_UTILS_TESTS_TYPECASTS_RUNNER_H
src/gromacs/gpu_utils/tests/typecasts_runner.h:#define GMX_GPU_UTILS_TESTS_TYPECASTS_RUNNER_H
src/gromacs/gpu_utils/tests/typecasts_runner.h:#endif // GMX_GPU_UTILS_TESTS_TYPECASTS_RUNNER_H
src/gromacs/gpu_utils/tests/devicetransfers_ocl.cpp: * for GPU host allocator.
src/gromacs/gpu_utils/tests/devicetransfers_ocl.cpp:#include "gromacs/gpu_utils/device_context.h"
src/gromacs/gpu_utils/tests/devicetransfers_ocl.cpp:#include "gromacs/gpu_utils/gmxopencl.h"
src/gromacs/gpu_utils/tests/devicetransfers_ocl.cpp:#include "gromacs/gpu_utils/oclutils.h"
src/gromacs/gpu_utils/tests/device_stream_manager.cpp: * \brief Tests GPU stream manager
src/gromacs/gpu_utils/tests/device_stream_manager.cpp: * \ingroup module_gpu_utils
src/gromacs/gpu_utils/tests/device_stream_manager.cpp:#include "gromacs/gpu_utils/device_stream_manager.h"
src/gromacs/gpu_utils/tests/device_stream_manager.cpp://! GPU device stream names for outputs.
src/gromacs/gpu_utils/tests/device_stream_manager.cpp:/*! \brief Non-GPU builds return nullptr instead of streams,
src/gromacs/gpu_utils/tests/device_stream_manager.cpp:constexpr bool c_canExpectValidStreams = (GMX_GPU != 0);
src/gromacs/gpu_utils/tests/device_stream_manager.cpp:            SCOPED_TRACE("No DD, no PME rank, no GPU update");
src/gromacs/gpu_utils/tests/device_stream_manager.cpp:            simulationWork.useGpuPme                 = false;
src/gromacs/gpu_utils/tests/device_stream_manager.cpp:            simulationWork.useGpuPmePpCommunication  = false;
src/gromacs/gpu_utils/tests/device_stream_manager.cpp:            simulationWork.useGpuUpdate              = false;
src/gromacs/gpu_utils/tests/device_stream_manager.cpp:            SCOPED_TRACE("With DD, no PME rank, no GPU update");
src/gromacs/gpu_utils/tests/device_stream_manager.cpp:            simulationWork.useGpuPme                 = false;
src/gromacs/gpu_utils/tests/device_stream_manager.cpp:            simulationWork.useGpuPmePpCommunication  = false;
src/gromacs/gpu_utils/tests/device_stream_manager.cpp:            simulationWork.useGpuUpdate              = false;
src/gromacs/gpu_utils/tests/device_stream_manager.cpp:            SCOPED_TRACE("No DD, with PME rank, no GPU update");
src/gromacs/gpu_utils/tests/device_stream_manager.cpp:            simulationWork.useGpuPme                 = true;
src/gromacs/gpu_utils/tests/device_stream_manager.cpp:            simulationWork.useGpuPmePpCommunication  = true;
src/gromacs/gpu_utils/tests/device_stream_manager.cpp:            simulationWork.useGpuUpdate              = false;
src/gromacs/gpu_utils/tests/device_stream_manager.cpp:            SCOPED_TRACE("With DD, with PME rank, no GPU update");
src/gromacs/gpu_utils/tests/device_stream_manager.cpp:            simulationWork.useGpuPme                 = true;
src/gromacs/gpu_utils/tests/device_stream_manager.cpp:            simulationWork.useGpuPmePpCommunication  = true;
src/gromacs/gpu_utils/tests/device_stream_manager.cpp:            simulationWork.useGpuUpdate              = false;
src/gromacs/gpu_utils/tests/device_stream_manager.cpp:            SCOPED_TRACE("No DD, no PME rank, with GPU update");
src/gromacs/gpu_utils/tests/device_stream_manager.cpp:            simulationWork.useGpuPme                 = false;
src/gromacs/gpu_utils/tests/device_stream_manager.cpp:            simulationWork.useGpuPmePpCommunication  = false;
src/gromacs/gpu_utils/tests/device_stream_manager.cpp:            simulationWork.useGpuUpdate              = true;
src/gromacs/gpu_utils/tests/device_stream_manager.cpp:            SCOPED_TRACE("With DD, no PME rank, with GPU update");
src/gromacs/gpu_utils/tests/device_stream_manager.cpp:            simulationWork.useGpuPme                 = false;
src/gromacs/gpu_utils/tests/device_stream_manager.cpp:            simulationWork.useGpuPmePpCommunication  = false;
src/gromacs/gpu_utils/tests/device_stream_manager.cpp:            simulationWork.useGpuUpdate              = true;
src/gromacs/gpu_utils/tests/device_stream_manager.cpp:            SCOPED_TRACE("No DD, with PME rank, with GPU update");
src/gromacs/gpu_utils/tests/device_stream_manager.cpp:            simulationWork.useGpuPme                 = true;
src/gromacs/gpu_utils/tests/device_stream_manager.cpp:            simulationWork.useGpuPmePpCommunication  = true;
src/gromacs/gpu_utils/tests/device_stream_manager.cpp:            simulationWork.useGpuUpdate              = true;
src/gromacs/gpu_utils/tests/device_stream_manager.cpp:            SCOPED_TRACE("With DD, with PME rank, with GPU update");
src/gromacs/gpu_utils/tests/device_stream_manager.cpp:            simulationWork.useGpuPme                 = true;
src/gromacs/gpu_utils/tests/device_stream_manager.cpp:            simulationWork.useGpuPmePpCommunication  = true;
src/gromacs/gpu_utils/tests/device_stream_manager.cpp:            simulationWork.useGpuUpdate              = true;
src/gromacs/gpu_utils/tests/devicetransfers.cu: * for GPU host allocator.
src/gromacs/gpu_utils/tests/devicetransfers.cu: * implementation of FindCUDA.cmake and/or nvcc mean that no
src/gromacs/gpu_utils/tests/devicetransfers.cu:#include "gromacs/gpu_utils/cudautils.cuh"
src/gromacs/gpu_utils/tests/devicetransfers.cu:#include "gromacs/gpu_utils/device_context.h"
src/gromacs/gpu_utils/tests/devicetransfers.cu:    cudaError_t status;
src/gromacs/gpu_utils/tests/devicetransfers.cu:    status = cudaGetDevice(&oldDeviceId);
src/gromacs/gpu_utils/tests/devicetransfers.cu:    status = cudaSetDevice(context.deviceInfo().id);
src/gromacs/gpu_utils/tests/devicetransfers.cu:    checkDeviceError(status, "Error while setting device id to the first compatible GPU.");
src/gromacs/gpu_utils/tests/devicetransfers.cu:    status = cudaMalloc(&devicePointer, input.size());
src/gromacs/gpu_utils/tests/devicetransfers.cu:    status = cudaMemcpy(devicePointer, input.data(), input.size(), cudaMemcpyHostToDevice);
src/gromacs/gpu_utils/tests/devicetransfers.cu:    status = cudaMemcpy(output.data(), devicePointer, output.size(), cudaMemcpyDeviceToHost);
src/gromacs/gpu_utils/tests/devicetransfers.cu:    status = cudaFree(devicePointer);
src/gromacs/gpu_utils/tests/devicetransfers.cu:    status = cudaSetDevice(oldDeviceId);
src/gromacs/gpu_utils/tests/devicetransfers_sycl.cpp: * for GPU host allocator.
src/gromacs/gpu_utils/tests/devicetransfers_sycl.cpp:#include "gromacs/gpu_utils/device_context.h"
src/gromacs/gpu_utils/tests/devicetransfers_sycl.cpp:#include "gromacs/gpu_utils/gmxsycl.h"
src/gromacs/gpu_utils/tests/clfftinitializer.cpp:#include "gromacs/gpu_utils/clfftinitializer.h"
src/gromacs/gpu_utils/tests/pinnedmemorychecker.cpp: * Tests for GPU memory status checker isHostMemoryPinned() being correct.
src/gromacs/gpu_utils/tests/pinnedmemorychecker.cpp:#if GMX_GPU_CUDA || GMX_GPU_HIP
src/gromacs/gpu_utils/tests/pinnedmemorychecker.cpp:#    include "gromacs/gpu_utils/gpu_utils.h"
src/gromacs/gpu_utils/tests/pinnedmemorychecker.cpp:#    include "gromacs/gpu_utils/hostallocator.h"
src/gromacs/gpu_utils/tests/pinnedmemorychecker.cpp:#    include "gromacs/gpu_utils/pmalloc.h"
src/gromacs/gpu_utils/tests/pinnedmemorychecker.cpp:    /* Note that this tests can be executed even on hosts with no GPUs.
src/gromacs/gpu_utils/tests/pinnedmemorychecker.cpp:     * However, the checks for pending CUDA errors run cudaGetLastError(...),
src/gromacs/gpu_utils/tests/pinnedmemorychecker.cpp:     * which itself returns cudaErrorNoDevice in this case. This causes the
src/gromacs/gpu_utils/tests/pinnedmemorychecker.cpp:#endif // GMX_GPU_CUDA || GMX_GPU_HIP
src/gromacs/gpu_utils/tests/CMakeLists.txt:# different files and strategies for the different GPU implementation
src/gromacs/gpu_utils/tests/CMakeLists.txt:gmx_add_unit_test(GpuUtilsUnitTests gpu_utils-test HARDWARE_DETECTION
src/gromacs/gpu_utils/tests/CMakeLists.txt:    GPU_CPP_SOURCE_FILES
src/gromacs/gpu_utils/tests/CMakeLists.txt:        gpueventsynchronizer.cpp
src/gromacs/gpu_utils/tests/CMakeLists.txt:    CUDA_CU_SOURCE_FILES
src/gromacs/gpu_utils/tests/CMakeLists.txt:    OPENCL_CPP_SOURCE_FILES
src/gromacs/gpu_utils/tests/CMakeLists.txt:    NON_GPU_CPP_SOURCE_FILES
src/gromacs/gpu_utils/tests/CMakeLists.txt:target_link_libraries(gpu_utils-test PRIVATE gpu_utils)
src/gromacs/gpu_utils/tests/CMakeLists.txt:target_link_libraries(gpu_utils-test PRIVATE math)
src/gromacs/gpu_utils/tests/CMakeLists.txt:    CUDA_CU_SOURCE_FILES
src/gromacs/gpu_utils/tests/CMakeLists.txt:    GPU_CPP_SOURCE_FILES
src/gromacs/gpu_utils/tests/CMakeLists.txt:    target_link_libraries(nvshmem-test PRIVATE gpu_utils)
src/gromacs/gpu_utils/tests/devicetransfers.h: * for GPU host allocator.
src/gromacs/gpu_utils/tests/devicetransfers.h: * implementation of FindCUDA.cmake and/or nvcc mean that no
src/gromacs/gpu_utils/tests/devicetransfers.h: * Thus, this header isolates CUDA-specific functionality to its own
src/gromacs/gpu_utils/tests/devicetransfers.h: * translation unit. The OpenCL and no-GPU implementations do not
src/gromacs/gpu_utils/tests/devicetransfers.h:#ifndef GMX_GPU_UTILS_TESTS_DEVICETRANSFERS_H
src/gromacs/gpu_utils/tests/devicetransfers.h:#define GMX_GPU_UTILS_TESTS_DEVICETRANSFERS_H
src/gromacs/gpu_utils/tests/devicetransfers.h:/*! \brief Helper function for GPU test code to be platform agnostic.
src/gromacs/gpu_utils/tests/devicetransfers.h: * \throws InternalError  Upon any GPU API error condition. */
src/gromacs/gpu_utils/tests/typecasts_runner_hip.cpp:#include "gromacs/gpu_utils/devicebuffer.h"
src/gromacs/gpu_utils/tests/typecasts_runner_hip.cpp:#include "gromacs/gpu_utils/hiputils.h"
src/gromacs/gpu_utils/tests/typecasts_runner_hip.cpp:#include "gromacs/gpu_utils/typecasts_cuda_hip.h"
src/gromacs/gpu_utils/tests/typecasts_runner_hip.cpp:/*! \brief GPU kernel to perform type conversion on the device.
src/gromacs/gpu_utils/tests/typecasts_runner_hip.cpp:            &d_rVecInput, h_rVecInput.data(), 0, numElements, deviceStream, GpuApiCallBehavior::Sync, nullptr);
src/gromacs/gpu_utils/tests/typecasts_runner_hip.cpp:    const auto kernelArgs = prepareGpuKernelArguments(
src/gromacs/gpu_utils/tests/typecasts_runner_hip.cpp:    launchGpuKernel(kernelPtr,
src/gromacs/gpu_utils/tests/typecasts_runner_hip.cpp:            h_float3Output.data(), &d_float3Output, 0, numElements, deviceStream, GpuApiCallBehavior::Sync, nullptr);
src/gromacs/gpu_utils/tests/devicetransfers_hip.cpp: * for GPU host allocator.
src/gromacs/gpu_utils/tests/devicetransfers_hip.cpp:#include "gromacs/gpu_utils/device_context.h"
src/gromacs/gpu_utils/tests/devicetransfers_hip.cpp:#include "gromacs/gpu_utils/hiputils.h"
src/gromacs/gpu_utils/tests/devicetransfers_hip.cpp:    checkDeviceError(status, "Error while setting device id to the first compatible GPU.");
src/gromacs/gpu_utils/tests/device_availability.cpp: * Check if GPUs are available when they should be.
src/gromacs/gpu_utils/tests/device_availability.cpp: * This is to test that CI can detect and use GPUs, when they are available.
src/gromacs/gpu_utils/tests/device_availability.cpp: * Driver and CUDA mismatch can lead to the tests falling back to the CPU
src/gromacs/gpu_utils/tests/device_availability.cpp: * code path quietly, leaving the GPU path untested. This is designed to
src/gromacs/gpu_utils/tests/device_availability.cpp: * fail when GPUs should be available, indicated by setting the environment
src/gromacs/gpu_utils/tests/device_availability.cpp:        ASSERT_TRUE(GMX_GPU) << "GROMACS was compiled without GPU support, yet "
src/gromacs/gpu_utils/tests/device_availability.cpp:        std::string platformString(getGpuImplementationString());
src/gromacs/gpu_utils/tests/device_availability.cpp:                << errorMessage << "GPU device detection is disabled by "
src/gromacs/gpu_utils/tests/device_availability.cpp:                << "GMX_DISABLE_GPU_DETECTION environment variable.";
src/gromacs/gpu_utils/tests/device_availability.cpp:                << "GPU device checks failed with the following message: " << detectionErrorMessage;
src/gromacs/gpu_utils/tests/device_availability.cpp:        // This is to test that the GPU detection test environment is working
src/gromacs/gpu_utils/tests/typecasts.cpp: * Tests for CUDA float3 type layout.
src/gromacs/gpu_utils/tests/typecasts.cpp:#if GMX_GPU_CUDA || GMX_GPU_HIP
src/gromacs/gpu_utils/tests/typecasts.cpp:#    include "gromacs/gpu_utils/device_context.h"
src/gromacs/gpu_utils/tests/typecasts.cpp:TEST(GpuDataTypesCompatibilityTest, RVecAndFloat3Host)
src/gromacs/gpu_utils/tests/typecasts.cpp:TEST(GpuDataTypesCompatibilityTest, RVecAndFloat3Device)
src/gromacs/gpu_utils/tests/typecasts.cpp:#endif // GMX_GPU_CUDA || GMX_GPU_HIP
src/gromacs/gpu_utils/tests/nvshmem_simple_put.cuh: * \author Mahesh Doijade <mdoijade@nvidia.com>
src/gromacs/gpu_utils/tests/nvshmem_simple_put.cuh:#include "gromacs/gpu_utils/cudautils.cuh"
src/gromacs/gpu_utils/tests/devicetransfers.cpp: * for GPU host allocator.
src/gromacs/gpu_utils/tests/devicetransfers.cpp:    // We can't have any valid GPUs for this build configuration.
src/gromacs/gpu_utils/tests/gpueventsynchronizer.cpp: * \brief Tests for GpuEventSynchronizer
src/gromacs/gpu_utils/tests/gpueventsynchronizer.cpp: * \ingroup module_gpu_utils
src/gromacs/gpu_utils/tests/gpueventsynchronizer.cpp:#include "gromacs/gpu_utils/gpueventsynchronizer.h"
src/gromacs/gpu_utils/tests/gpueventsynchronizer.cpp:#include "gromacs/gpu_utils/device_context.h"
src/gromacs/gpu_utils/tests/gpueventsynchronizer.cpp:#include "gromacs/gpu_utils/device_stream.h"
src/gromacs/gpu_utils/tests/gpueventsynchronizer.cpp:#if GMX_GPU
src/gromacs/gpu_utils/tests/gpueventsynchronizer.cpp:TEST(GpuEventSynchronizerTest, BasicFunctionality)
src/gromacs/gpu_utils/tests/gpueventsynchronizer.cpp:            GpuEventSynchronizer gpuEventSynchronizer;
src/gromacs/gpu_utils/tests/gpueventsynchronizer.cpp:            GpuEventSynchronizer gpuEventSynchronizer;
src/gromacs/gpu_utils/tests/gpueventsynchronizer.cpp:            gpuEventSynchronizer.markEvent(streamA);
src/gromacs/gpu_utils/tests/gpueventsynchronizer.cpp:            gpuEventSynchronizer.waitForEvent(); // Should return immediately
src/gromacs/gpu_utils/tests/gpueventsynchronizer.cpp:            GpuEventSynchronizer gpuEventSynchronizer;
src/gromacs/gpu_utils/tests/gpueventsynchronizer.cpp:            EXPECT_FALSE(gpuEventSynchronizer.isMarked());
src/gromacs/gpu_utils/tests/gpueventsynchronizer.cpp:            gpuEventSynchronizer.markEvent(streamA);
src/gromacs/gpu_utils/tests/gpueventsynchronizer.cpp:            EXPECT_TRUE(gpuEventSynchronizer.isMarked());
src/gromacs/gpu_utils/tests/gpueventsynchronizer.cpp:            gpuEventSynchronizer.reset();
src/gromacs/gpu_utils/tests/gpueventsynchronizer.cpp:            EXPECT_FALSE(gpuEventSynchronizer.isMarked());
src/gromacs/gpu_utils/tests/gpueventsynchronizer.cpp:            GpuEventSynchronizer gpuEventSynchronizer;
src/gromacs/gpu_utils/tests/gpueventsynchronizer.cpp:            gpuEventSynchronizer.markEvent(streamA);
src/gromacs/gpu_utils/tests/gpueventsynchronizer.cpp:            // OpenCL standard requires explicit flush in such cases; no-op for CUDA and SYCL
src/gromacs/gpu_utils/tests/gpueventsynchronizer.cpp:            gpuEventSynchronizer.enqueueWaitEvent(streamB);
src/gromacs/gpu_utils/tests/gpueventsynchronizer.cpp:            GpuEventSynchronizer gpuEventSynchronizer;
src/gromacs/gpu_utils/tests/gpueventsynchronizer.cpp:            gpuEventSynchronizer.markEvent(streamA);
src/gromacs/gpu_utils/tests/gpueventsynchronizer.cpp:            gpuEventSynchronizer.waitForEvent();
src/gromacs/gpu_utils/tests/gpueventsynchronizer.cpp:            gpuEventSynchronizer.markEvent(streamB);
src/gromacs/gpu_utils/tests/gpueventsynchronizer.cpp:            gpuEventSynchronizer.waitForEvent();
src/gromacs/gpu_utils/tests/gpueventsynchronizer.cpp:#    if !GMX_GPU_CUDA // CUDA has very lax rules for event consumption. See Issues #2527 and #3988.
src/gromacs/gpu_utils/tests/gpueventsynchronizer.cpp:            GpuEventSynchronizer gpuEventSynchronizer;
src/gromacs/gpu_utils/tests/gpueventsynchronizer.cpp:            EXPECT_THROW(gpuEventSynchronizer.waitForEvent(), gmx::InternalError);
src/gromacs/gpu_utils/tests/gpueventsynchronizer.cpp:            GpuEventSynchronizer gpuEventSynchronizer;
src/gromacs/gpu_utils/tests/gpueventsynchronizer.cpp:            EXPECT_THROW(gpuEventSynchronizer.enqueueWaitEvent(streamA), gmx::InternalError);
src/gromacs/gpu_utils/tests/gpueventsynchronizer.cpp:            GpuEventSynchronizer gpuEventSynchronizer;
src/gromacs/gpu_utils/tests/gpueventsynchronizer.cpp:            gpuEventSynchronizer.markEvent(streamA);
src/gromacs/gpu_utils/tests/gpueventsynchronizer.cpp:            gpuEventSynchronizer.waitForEvent();
src/gromacs/gpu_utils/tests/gpueventsynchronizer.cpp:            EXPECT_THROW(gpuEventSynchronizer.waitForEvent(), gmx::InternalError);
src/gromacs/gpu_utils/tests/gpueventsynchronizer.cpp:            GpuEventSynchronizer gpuEventSynchronizer;
src/gromacs/gpu_utils/tests/gpueventsynchronizer.cpp:            gpuEventSynchronizer.markEvent(streamA);
src/gromacs/gpu_utils/tests/gpueventsynchronizer.cpp:            EXPECT_THROW(gpuEventSynchronizer.markEvent(streamB), gmx::InternalError);
src/gromacs/gpu_utils/tests/gpueventsynchronizer.cpp:            GpuEventSynchronizer gpuEventSynchronizer(0, 2);
src/gromacs/gpu_utils/tests/gpueventsynchronizer.cpp:            gpuEventSynchronizer.markEvent(streamA);
src/gromacs/gpu_utils/tests/gpueventsynchronizer.cpp:            gpuEventSynchronizer.waitForEvent();
src/gromacs/gpu_utils/tests/gpueventsynchronizer.cpp:            gpuEventSynchronizer.waitForEvent();
src/gromacs/gpu_utils/tests/gpueventsynchronizer.cpp:            EXPECT_THROW(gpuEventSynchronizer.waitForEvent(), gmx::InternalError);
src/gromacs/gpu_utils/tests/gpueventsynchronizer.cpp:            GpuEventSynchronizer gpuEventSynchronizer(0, 1);
src/gromacs/gpu_utils/tests/gpueventsynchronizer.cpp:            gpuEventSynchronizer.markEvent(streamA);
src/gromacs/gpu_utils/tests/gpueventsynchronizer.cpp:            gpuEventSynchronizer.markEvent(streamB);
src/gromacs/gpu_utils/tests/gpueventsynchronizer.cpp:            GpuEventSynchronizer gpuEventSynchronizer(2, 2);
src/gromacs/gpu_utils/tests/gpueventsynchronizer.cpp:            gpuEventSynchronizer.markEvent(streamA);
src/gromacs/gpu_utils/tests/gpueventsynchronizer.cpp:            gpuEventSynchronizer.waitForEvent();
src/gromacs/gpu_utils/tests/gpueventsynchronizer.cpp:            EXPECT_THROW(gpuEventSynchronizer.markEvent(streamB), gmx::InternalError);
src/gromacs/gpu_utils/tests/gpueventsynchronizer.cpp:#endif // GMX_GPU
src/gromacs/gpu_utils/tests/typecasts_runner.cu: * Runners for tests of CUDA types compatibility.
src/gromacs/gpu_utils/tests/typecasts_runner.cu:#include "gromacs/gpu_utils/cudautils.cuh"
src/gromacs/gpu_utils/tests/typecasts_runner.cu:#include "gromacs/gpu_utils/devicebuffer.h"
src/gromacs/gpu_utils/tests/typecasts_runner.cu:#include "gromacs/gpu_utils/typecasts_cuda_hip.h"
src/gromacs/gpu_utils/tests/typecasts_runner.cu://! Number of CUDA threads in a block.
src/gromacs/gpu_utils/tests/typecasts_runner.cu:/*! \brief GPU kernel to perform type conversion on the device.
src/gromacs/gpu_utils/tests/typecasts_runner.cu:            &d_rVecInput, h_rVecInput.data(), 0, numElements, deviceStream, GpuApiCallBehavior::Sync, nullptr);
src/gromacs/gpu_utils/tests/typecasts_runner.cu:    const auto kernelArgs = prepareGpuKernelArguments(
src/gromacs/gpu_utils/tests/typecasts_runner.cu:    launchGpuKernel(kernelPtr,
src/gromacs/gpu_utils/tests/typecasts_runner.cu:            h_float3Output.data(), &d_float3Output, 0, numElements, deviceStream, GpuApiCallBehavior::Sync, nullptr);
src/gromacs/gpu_utils/tests/device_buffer.cpp: * \ingroup module_gpu_utils
src/gromacs/gpu_utils/tests/device_buffer.cpp:#if GMX_GPU
src/gromacs/gpu_utils/tests/device_buffer.cpp:#    include "gromacs/gpu_utils/device_context.h"
src/gromacs/gpu_utils/tests/device_buffer.cpp:#    include "gromacs/gpu_utils/device_stream.h"
src/gromacs/gpu_utils/tests/device_buffer.cpp:#    include "gromacs/gpu_utils/devicebuffer.h"
src/gromacs/gpu_utils/tests/device_buffer.cpp:#    include "gromacs/gpu_utils/hostallocator.h"
src/gromacs/gpu_utils/tests/device_buffer.cpp:    for (auto transferKind : { GpuApiCallBehavior::Sync, GpuApiCallBehavior::Async })
src/gromacs/gpu_utils/tests/device_buffer.cpp:        PinningPolicy pinningPolicy = (transferKind == GpuApiCallBehavior::Async)
src/gromacs/gpu_utils/tests/device_buffer.cpp:            if (transferKind == GpuApiCallBehavior::Async)
src/gromacs/gpu_utils/tests/device_buffer.cpp:    for (auto transferKind : { GpuApiCallBehavior::Sync, GpuApiCallBehavior::Async })
src/gromacs/gpu_utils/tests/device_buffer.cpp:        PinningPolicy pinningPolicy = (transferKind == GpuApiCallBehavior::Async)
src/gromacs/gpu_utils/tests/device_buffer.cpp:            // Wait until GPU is done and do the same copying on the CPU, so we can test it works correctly.
src/gromacs/gpu_utils/tests/device_buffer.cpp:            if (transferKind == GpuApiCallBehavior::Async)
src/gromacs/gpu_utils/tests/device_buffer.cpp:            if (transferKind == GpuApiCallBehavior::Async)
src/gromacs/gpu_utils/tests/device_buffer.cpp:            if (transferKind == GpuApiCallBehavior::Async)
src/gromacs/gpu_utils/tests/device_buffer.cpp:#    if GMX_GPU_CUDA || GMX_GPU_SYCL || GMX_GPU_HIP
src/gromacs/gpu_utils/tests/device_buffer.cpp:    for (auto transferKind : { GpuApiCallBehavior::Sync, GpuApiCallBehavior::Async })
src/gromacs/gpu_utils/tests/device_buffer.cpp:        PinningPolicy pinningPolicy = (transferKind == GpuApiCallBehavior::Async)
src/gromacs/gpu_utils/tests/device_buffer.cpp:            if (transferKind == GpuApiCallBehavior::Async)
src/gromacs/gpu_utils/tests/device_buffer.cpp:            if (transferKind == GpuApiCallBehavior::Async)
src/gromacs/gpu_utils/tests/device_buffer.cpp:            if (transferKind == GpuApiCallBehavior::Async)
src/gromacs/gpu_utils/tests/device_buffer.cpp:#    endif // GMX_GPU_CUDA || GMX_GPU_SYCL || GMX_GPU_HIP
src/gromacs/gpu_utils/tests/device_buffer.cpp:#endif // GMX_GPU
src/gromacs/gpu_utils/tests/hostallocator.cpp: * Tests for GPU host allocator.
src/gromacs/gpu_utils/tests/hostallocator.cpp:#include "gromacs/gpu_utils/hostallocator.h"
src/gromacs/gpu_utils/tests/hostallocator.cpp:#include "gromacs/gpu_utils/device_context.h"
src/gromacs/gpu_utils/tests/hostallocator.cpp:#include "gromacs/gpu_utils/gpu_utils.h"
src/gromacs/gpu_utils/tests/hostallocator.cpp: * host-side memory used for GPU transfers. */
src/gromacs/gpu_utils/tests/hostallocator.cpp://! Does a device transfer of \c input to the device in \c gpuInfo, and back to \c output.
src/gromacs/gpu_utils/tests/hostallocator.cpp://! Typed test fixture (no mem/gpu initializtion - much faster)
src/gromacs/gpu_utils/tests/hostallocator.cpp:// Note also that aspects of this code can be tested even when a GPU
src/gromacs/gpu_utils/tests/hostallocator.cpp:// Several tests actually do CUDA transfers. This is not necessary
src/gromacs/gpu_utils/tests/hostallocator.cpp:// relevant to the success of a CUDA transfer. CUDA checks happen only
src/gromacs/gpu_utils/tests/hostallocator.cpp:// during cudaHostRegister and cudaHostUnregister. Such tests are of
src/gromacs/gpu_utils/tests/hostallocator.cpp:#if GMX_GPU_CUDA || GMX_GPU_SYCL || GMX_GPU_HIP
src/gromacs/gpu_utils/tests/hostallocator.cpp:// Policy suitable for pinning is supported for a CUDA, HIP or SYCL build
src/gromacs/gpu_utils/tests/hostallocator.cpp:#if GMX_GPU_CUDA || GMX_GPU_HIP
src/gromacs/gpu_utils/tests/nvshmem_simple_put.cu: * \author Mahesh Doijade <mdoijade@nvidia.com>
src/gromacs/gpu_utils/tests/nvshmem_simple_put.cu:#include "gromacs/gpu_utils/cudautils.cuh"
src/gromacs/gpu_utils/tests/nvshmem_simple_put.cu:#    include "gromacs/gpu_utils/devicebuffer.cuh"
src/gromacs/gpu_utils/tests/nvshmem_simple_put.cu://! Number of CUDA threads in a block.
src/gromacs/gpu_utils/tests/nvshmem_simple_put.cu:/*! \brief GPU kernel to perform nvshmem puts on symmetric buffer.
src/gromacs/gpu_utils/tests/nvshmem_simple_put.cu:#    if __CUDA_ARCH__ >= 700
src/gromacs/gpu_utils/tests/nvshmem_simple_put.cu:            prepareGpuKernelArguments(kernelPtr, kernelLaunchConfig, &destination, &numElements);
src/gromacs/gpu_utils/tests/nvshmem_simple_put.cu:    launchGpuKernel(kernelPtr, kernelLaunchConfig, deviceStream, nullptr, "nvshmemSimplePut", kernelArgs);
src/gromacs/gpu_utils/tests/nvshmem_simple_put.cu:    copyFromDeviceBuffer(&msg, &destination, 0, numElements, deviceStream, GpuApiCallBehavior::Sync, nullptr);
src/gromacs/gpu_utils/device_utils.clh: *  \brief OpenCL device-side utilities.
src/gromacs/gpu_utils/device_utils.clh: *  Implements device-side macros and inline functions to be used in OpenCL kernels.
src/gromacs/gpu_utils/device_utils.clh: *  \ingroup module_gpu_utils
src/gromacs/gpu_utils/device_utils.clh:#ifndef GMX_GPU_UTILS_DEVICE_UTILS_CLH
src/gromacs/gpu_utils/device_utils.clh:#define GMX_GPU_UTILS_DEVICE_UTILS_CLH
src/gromacs/gpu_utils/device_utils.clh:///! As far as we know this should be enough to convince OpenCL C compilers
src/gromacs/gpu_utils/device_utils.clh:#define gmx_opencl_inline static inline
src/gromacs/gpu_utils/device_utils.clh:gmx_opencl_inline void atomicAdd_l_f(volatile __local float* addr, float val)
src/gromacs/gpu_utils/device_utils.clh:gmx_opencl_inline void atomicAdd_l_f3(__local float3* addr, float3 val)
src/gromacs/gpu_utils/device_utils.clh:gmx_opencl_inline void atomicAdd_g_f(volatile __global float* addr, float val)
src/gromacs/gpu_utils/device_utils.clh:gmx_opencl_inline void atomicAdd_g_f3(__global float* addr, const float3 val)
src/gromacs/gpu_utils/device_utils.clh:#endif /* GMX_GPU_UTILS_DEVICE_UTILS_CLH */
src/gromacs/gpu_utils/gpu_utils.cpp: *  \brief Function definitions for non-GPU builds
src/gromacs/gpu_utils/gpu_utils.cpp:#include "gpu_utils.h"
src/gromacs/gpu_utils/gpu_utils.cpp:const char* enumValueToString(GpuApiCallBehavior enumValue)
src/gromacs/gpu_utils/gpu_utils.cpp:    static constexpr gmx::EnumerationArray<GpuApiCallBehavior, const char*> s_gpuApiCallBehaviorNames = {
src/gromacs/gpu_utils/gpu_utils.cpp:    return s_gpuApiCallBehaviorNames[enumValue];
src/gromacs/gpu_utils/gpu_utils.cpp:bool decideGpuTimingsUsage()
src/gromacs/gpu_utils/gpu_utils.cpp:    if (GMX_GPU_CUDA || GMX_GPU_SYCL)
src/gromacs/gpu_utils/gpu_utils.cpp:        /* CUDA: timings are incorrect with multiple streams.
src/gromacs/gpu_utils/gpu_utils.cpp:        return (getenv("GMX_ENABLE_GPU_TIMING") != nullptr);
src/gromacs/gpu_utils/gpu_utils.cpp:    else if (GMX_GPU_OPENCL)
src/gromacs/gpu_utils/gpu_utils.cpp:        return (getenv("GMX_DISABLE_GPU_TIMING") == nullptr);
src/gromacs/gpu_utils/pmalloc_hip.cpp:#include "gromacs/gpu_utils/hiputils.h"
src/gromacs/gpu_utils/device_context.h:#ifndef GMX_GPU_UTILS_DEVICE_CONTEXT_H
src/gromacs/gpu_utils/device_context.h:#define GMX_GPU_UTILS_DEVICE_CONTEXT_H
src/gromacs/gpu_utils/device_context.h: * \ingroup module_gpu_utils
src/gromacs/gpu_utils/device_context.h:#if GMX_GPU_OPENCL
src/gromacs/gpu_utils/device_context.h:#    include "gromacs/gpu_utils/gmxopencl.h"
src/gromacs/gpu_utils/device_context.h:#if GMX_GPU_SYCL
src/gromacs/gpu_utils/device_context.h:#    include "gromacs/gpu_utils/gmxsycl.h"
src/gromacs/gpu_utils/device_context.h:#include "gromacs/gpu_utils/pmalloc.h"
src/gromacs/gpu_utils/device_context.h:#if GMX_GPU_OPENCL
src/gromacs/gpu_utils/device_context.h:    //! OpenCL context object
src/gromacs/gpu_utils/device_context.h:#if GMX_GPU_SYCL
src/gromacs/gpu_utils/device_context.h:#endif // GMX_GPU_UTILS_DEVICE_CONTEXT_H
src/gromacs/gpu_utils/device_context_ocl.cpp: * \brief Implements the DeviceContext for OpenCL
src/gromacs/gpu_utils/device_context_ocl.cpp: * \ingroup module_gpu_utils
src/gromacs/gpu_utils/device_context_ocl.cpp:#include "gromacs/gpu_utils/device_context.h"
src/gromacs/gpu_utils/device_context_ocl.cpp:                "Failed to create OpenCL context on device %s (OpenCL error ID %d).",
src/gromacs/gpu_utils/device_context_ocl.cpp:            std::fprintf(stderr, "Failed to release OpenCL context (OpenCL error ID %d).\n", clError);
src/gromacs/gpu_utils/devicebuffer_hip.h:#ifndef GMX_GPU_UTILS_DEVICEBUFFER_HIP_H
src/gromacs/gpu_utils/devicebuffer_hip.h:#define GMX_GPU_UTILS_DEVICEBUFFER_HIP_H
src/gromacs/gpu_utils/devicebuffer_hip.h:#include "gromacs/gpu_utils/device_context.h"
src/gromacs/gpu_utils/devicebuffer_hip.h:#include "gromacs/gpu_utils/device_stream.h"
src/gromacs/gpu_utils/devicebuffer_hip.h:#include "gromacs/gpu_utils/devicebuffer_datatype.h"
src/gromacs/gpu_utils/devicebuffer_hip.h:#include "gromacs/gpu_utils/gpu_utils.h" //only for GpuApiCallBehavior
src/gromacs/gpu_utils/devicebuffer_hip.h:#include "gromacs/gpu_utils/gputraits_hip.h"
src/gromacs/gpu_utils/devicebuffer_hip.h:#include "gromacs/gpu_utils/hiputils.h"
src/gromacs/gpu_utils/devicebuffer_hip.h: * \param[in]     deviceStream         GPU stream to perform asynchronous copy in.
src/gromacs/gpu_utils/devicebuffer_hip.h:                        GpuApiCallBehavior       transferKind,
src/gromacs/gpu_utils/devicebuffer_hip.h:        case GpuApiCallBehavior::Async:
src/gromacs/gpu_utils/devicebuffer_hip.h:        case GpuApiCallBehavior::Sync:
src/gromacs/gpu_utils/devicebuffer_hip.h: * \param[in]     deviceStream         GPU stream to perform asynchronous copy in.
src/gromacs/gpu_utils/devicebuffer_hip.h:                          GpuApiCallBehavior       transferKind,
src/gromacs/gpu_utils/devicebuffer_hip.h:        case GpuApiCallBehavior::Async:
src/gromacs/gpu_utils/devicebuffer_hip.h:        case GpuApiCallBehavior::Sync:
src/gromacs/gpu_utils/devicebuffer_hip.h: * \param[in]     deviceStream             GPU stream to perform asynchronous copy in.
src/gromacs/gpu_utils/devicebuffer_hip.h:                              GpuApiCallBehavior       transferKind,
src/gromacs/gpu_utils/devicebuffer_hip.h:        case GpuApiCallBehavior::Async:
src/gromacs/gpu_utils/devicebuffer_hip.h:        case GpuApiCallBehavior::Sync:
src/gromacs/gpu_utils/devicebuffer_hip.h: * \param[in]     deviceStream    GPU stream.
src/gromacs/gpu_utils/devicebuffer_hip.h:            deviceBuffer, hostBuffer, 0, numValues, deviceStream, GpuApiCallBehavior::Sync, nullptr);
src/gromacs/gpu_utils/devicebuffer_ocl.h:#ifndef GMX_GPU_UTILS_DEVICEBUFFER_OCL_H
src/gromacs/gpu_utils/devicebuffer_ocl.h:#define GMX_GPU_UTILS_DEVICEBUFFER_OCL_H
src/gromacs/gpu_utils/devicebuffer_ocl.h: *  \brief Implements the DeviceBuffer type and routines for OpenCL.
src/gromacs/gpu_utils/devicebuffer_ocl.h:#include "gromacs/gpu_utils/device_context.h"
src/gromacs/gpu_utils/devicebuffer_ocl.h:#include "gromacs/gpu_utils/device_stream.h"
src/gromacs/gpu_utils/devicebuffer_ocl.h:#include "gromacs/gpu_utils/devicebuffer_datatype.h"
src/gromacs/gpu_utils/devicebuffer_ocl.h:#include "gromacs/gpu_utils/gpu_utils.h" //only for GpuApiCallBehavior
src/gromacs/gpu_utils/devicebuffer_ocl.h:#include "gromacs/gpu_utils/gputraits_ocl.h"
src/gromacs/gpu_utils/devicebuffer_ocl.h:#include "gromacs/gpu_utils/oclutils.h"
src/gromacs/gpu_utils/devicebuffer_ocl.h:                       gmx::formatString("clCreateBuffer failure (OpenCL error %d: %s)",
src/gromacs/gpu_utils/devicebuffer_ocl.h:                           gmx::formatString("clReleaseMemObject failed (OpenCL error %d: %s)",
src/gromacs/gpu_utils/devicebuffer_ocl.h: * \param[in]     deviceStream         GPU stream to perform asynchronous copy in.
src/gromacs/gpu_utils/devicebuffer_ocl.h:                        GpuApiCallBehavior       transferKind,
src/gromacs/gpu_utils/devicebuffer_ocl.h:        case GpuApiCallBehavior::Async:
src/gromacs/gpu_utils/devicebuffer_ocl.h:                    gmx::formatString("Asynchronous H2D copy failed (OpenCL error %d: %s)",
src/gromacs/gpu_utils/devicebuffer_ocl.h:        case GpuApiCallBehavior::Sync:
src/gromacs/gpu_utils/devicebuffer_ocl.h:                    gmx::formatString("Synchronous H2D copy failed (OpenCL error %d: %s)",
src/gromacs/gpu_utils/devicebuffer_ocl.h: * \param[in]     deviceStream         GPU stream to perform asynchronous copy in.
src/gromacs/gpu_utils/devicebuffer_ocl.h:                          GpuApiCallBehavior       transferKind,
src/gromacs/gpu_utils/devicebuffer_ocl.h:        case GpuApiCallBehavior::Async:
src/gromacs/gpu_utils/devicebuffer_ocl.h:                    gmx::formatString("Asynchronous D2H copy failed (OpenCL error %d: %s)",
src/gromacs/gpu_utils/devicebuffer_ocl.h:        case GpuApiCallBehavior::Sync:
src/gromacs/gpu_utils/devicebuffer_ocl.h:                    gmx::formatString("Synchronous D2H copy failed (OpenCL error %d: %s)",
src/gromacs/gpu_utils/devicebuffer_ocl.h:                              GpuApiCallBehavior /* transferKind */,
src/gromacs/gpu_utils/devicebuffer_ocl.h:    // OpenCL-TODO
src/gromacs/gpu_utils/devicebuffer_ocl.h:    gmx_fatal(FARGS, "D2D copy stub was called. Not yet implemented in OpenCL.");
src/gromacs/gpu_utils/devicebuffer_ocl.h: * \param[in]     deviceStream    GPU stream.
src/gromacs/gpu_utils/devicebuffer_ocl.h:    /* Apple OpenCL breaks if clEnqueueFillBuffer does not return an event.
src/gromacs/gpu_utils/devicebuffer_ocl.h:                       gmx::formatString("Couldn't clear the device buffer (OpenCL error %d: %s)",
src/gromacs/gpu_utils/devicebuffer_ocl.h:                       gmx::formatString("Constant memory allocation failed (OpenCL error %d: %s)",
src/gromacs/gpu_utils/devicebuffer_ocl.h:/*! \brief Release the OpenCL device buffer.
src/gromacs/gpu_utils/gputraits.h:#ifndef GMX_GPU_UTILS_GPUTRAITS_H
src/gromacs/gpu_utils/gputraits.h:#define GMX_GPU_UTILS_GPUTRAITS_H
src/gromacs/gpu_utils/gputraits.h: *  \brief Declares the GPU type traits for non-GPU builds.
src/gromacs/gpu_utils/gputraits.h: * \ingroup module_gpu_utils
src/gromacs/gpu_utils/gputraits.h:#if GMX_GPU_CUDA
src/gromacs/gpu_utils/gputraits.h:#    include "gromacs/gpu_utils/gputraits.cuh"
src/gromacs/gpu_utils/gputraits.h:#elif GMX_GPU_OPENCL
src/gromacs/gpu_utils/gputraits.h:#    include "gromacs/gpu_utils/gputraits_ocl.h"
src/gromacs/gpu_utils/gputraits.h:#elif GMX_GPU_SYCL
src/gromacs/gpu_utils/gputraits.h:#    include "gromacs/gpu_utils/gputraits_sycl.h"
src/gromacs/gpu_utils/gputraits.h:#elif GMX_GPU_HIP
src/gromacs/gpu_utils/gputraits.h:#    include "gromacs/gpu_utils/gputraits_hip.h"
src/gromacs/gpu_utils/gputraits.h://! \brief Single GPU call timing event
src/gromacs/gpu_utils/gputraits.h:#endif // GMX_GPU
src/gromacs/gpu_utils/gputraits.h:#endif // GMX_GPU_UTILS_GPUTRAITS_H
src/gromacs/gpu_utils/clfftinitializer.cpp:#if GMX_GPU_OPENCL && GMX_GPU_FFT_CLFFT
src/gromacs/gpu_utils/clfftinitializer.cpp:#if GMX_GPU_OPENCL && GMX_GPU_FFT_CLFFT
src/gromacs/gpu_utils/clfftinitializer.cpp:#if GMX_GPU_OPENCL && GMX_GPU_FFT_CLFFT
src/gromacs/gpu_utils/clfftinitializer.cpp:#if GMX_GPU_OPENCL && GMX_GPU_FFT_CLFFT
src/gromacs/gpu_utils/device_stream.cpp: * \ingroup module_gpu_utils
src/gromacs/gpu_utils/vectype_ops_hip_base_math.h:#ifndef GMX_GPU_UTILS_VECTYPE_OPS_HIP_BASE_MATH_H
src/gromacs/gpu_utils/vectype_ops_hip_base_math.h:#define GMX_GPU_UTILS_VECTYPE_OPS_HIP_BASE_MATH_H
src/gromacs/gpu_utils/vectype_ops_hip_base_math.h:#endif /* GMX_GPU_UTILS_VECTYPE_OPS_HIP_BASE_MATH_H */
src/gromacs/gpu_utils/device_event_sycl.h: *  \brief Implements a GpuEventSynchronizer class for SYCL.
src/gromacs/gpu_utils/device_event_sycl.h: *  which directly enqueues a CUDA-like barrier event.
src/gromacs/gpu_utils/device_event_sycl.h:#ifndef GMX_GPU_UTILS_DEVICE_EVENT_SYCL_H
src/gromacs/gpu_utils/device_event_sycl.h:#define GMX_GPU_UTILS_DEVICE_EVENT_SYCL_H
src/gromacs/gpu_utils/device_event_sycl.h:#include "gromacs/gpu_utils/device_stream.h"
src/gromacs/gpu_utils/device_event_sycl.h:#include "gromacs/gpu_utils/gmxsycl.h"
src/gromacs/gpu_utils/device_event_sycl.h:        // This will not launch any GPU operation, but it will mark an event which is returned
src/gromacs/gpu_utils/device_event_sycl.h:        // This will not launch any GPU operation, but it will mark an event which is returned
src/gromacs/gpu_utils/device_event_sycl.h:#endif // GMX_GPU_UTILS_DEVICE_EVENT_SYCL_H
src/gromacs/gpu_utils/gpuregiontimer_hip.h: *  \brief Implements the GPU region timer for HIP.
src/gromacs/gpu_utils/gpuregiontimer_hip.h:#ifndef GMX_GPU_UTILS_GPUREGIONTIMER_HIP_H
src/gromacs/gpu_utils/gpuregiontimer_hip.h:#define GMX_GPU_UTILS_GPUREGIONTIMER_HIP_H
src/gromacs/gpu_utils/gpuregiontimer_hip.h:#include "gromacs/gpu_utils/gputraits_hip.h"
src/gromacs/gpu_utils/gpuregiontimer_hip.h:#include "gromacs/gpu_utils/hiputils.h"
src/gromacs/gpu_utils/gpuregiontimer_hip.h:#include "gpuregiontimer.h"
src/gromacs/gpu_utils/gpuregiontimer_hip.h:// Disabling Doxygen to avoid it having GpuRegionTimerImpl from both OpenCL and SYCL
src/gromacs/gpu_utils/gpuregiontimer_hip.h: * This is a GPU region timing implementation for HIP.
src/gromacs/gpu_utils/gpuregiontimer_hip.h:class GpuRegionTimerImpl
src/gromacs/gpu_utils/gpuregiontimer_hip.h:    GpuRegionTimerImpl()
src/gromacs/gpu_utils/gpuregiontimer_hip.h:                              "GPU timing creation failure");
src/gromacs/gpu_utils/gpuregiontimer_hip.h:                              "GPU timing creation failure");
src/gromacs/gpu_utils/gpuregiontimer_hip.h:    ~GpuRegionTimerImpl()
src/gromacs/gpu_utils/gpuregiontimer_hip.h:        gmx::checkDeviceError(hipEventDestroy(eventStart_), "GPU timing destruction failure");
src/gromacs/gpu_utils/gpuregiontimer_hip.h:        gmx::checkDeviceError(hipEventDestroy(eventStop_), "GPU timing destruction failure");
src/gromacs/gpu_utils/gpuregiontimer_hip.h:    GpuRegionTimerImpl(const GpuRegionTimerImpl&) = delete;
src/gromacs/gpu_utils/gpuregiontimer_hip.h:    GpuRegionTimerImpl& operator=(GpuRegionTimerImpl&&) = delete;
src/gromacs/gpu_utils/gpuregiontimer_hip.h:    GpuRegionTimerImpl(GpuRegionTimerImpl&&) = delete;
src/gromacs/gpu_utils/gpuregiontimer_hip.h:                              "GPU timing recording failure");
src/gromacs/gpu_utils/gpuregiontimer_hip.h:                              "GPU timing recording failure");
src/gromacs/gpu_utils/gpuregiontimer_hip.h:                              "GPU timing update failure");
src/gromacs/gpu_utils/gpuregiontimer_hip.h:     * for passing into individual GPU API calls.
src/gromacs/gpu_utils/gpuregiontimer_hip.h:using GpuRegionTimer = GpuRegionTimerWrapper<GpuRegionTimerImpl>;
src/gromacs/gpu_utils/CMakeLists.txt:add_library(gpu_utils INTERFACE)
src/gromacs/gpu_utils/CMakeLists.txt:        gpu_utils.cpp
src/gromacs/gpu_utils/CMakeLists.txt:        gpueventsynchronizer_helpers.cpp
src/gromacs/gpu_utils/CMakeLists.txt:if(GMX_GPU_OPENCL)
src/gromacs/gpu_utils/CMakeLists.txt:        gpu_utils_impl.cpp
src/gromacs/gpu_utils/CMakeLists.txt:elseif(GMX_GPU_CUDA)
src/gromacs/gpu_utils/CMakeLists.txt:        gpu_utils.cu
src/gromacs/gpu_utils/CMakeLists.txt:    _gmx_add_files_to_property(CUDA_SOURCES
src/gromacs/gpu_utils/CMakeLists.txt:elseif(GMX_GPU_HIP)
src/gromacs/gpu_utils/CMakeLists.txt:        gpu_utils_hip.cpp
src/gromacs/gpu_utils/CMakeLists.txt:        gpu_utils_hip.cpp
src/gromacs/gpu_utils/CMakeLists.txt:elseif(GMX_GPU_SYCL)
src/gromacs/gpu_utils/CMakeLists.txt:        gpu_utils_impl.cpp
src/gromacs/gpu_utils/CMakeLists.txt:        gpu_utils_impl.cpp
src/gromacs/gpu_utils/CMakeLists.txt:target_link_libraries(gpu_utils PRIVATE
src/gromacs/gpu_utils/CMakeLists.txt:#target_include_directories(gpu_utils PUBLIC
src/gromacs/gpu_utils/CMakeLists.txt:#target_link_libraries(gpu_utils PUBLIC
src/gromacs/gpu_utils/CMakeLists.txt:target_link_libraries(gpu_utils INTERFACE
src/gromacs/gpu_utils/CMakeLists.txt:# TODO: when gpu_utils is an OBJECT target
src/gromacs/gpu_utils/CMakeLists.txt:#target_link_libraries(gpu_utils PUBLIC legacy_api)
src/gromacs/gpu_utils/CMakeLists.txt:#target_link_libraries(gpu_utils PRIVATE common)
src/gromacs/gpu_utils/CMakeLists.txt:#target_link_libraries(gpu_utils PRIVATE tng_io)
src/gromacs/gpu_utils/CMakeLists.txt:#target_link_libraries(gpu_utils PRIVATE legacy_modules)
src/gromacs/gpu_utils/gpuregiontimer.h: *  \brief Defines the GPU region timer implementation/wrapper classes.
src/gromacs/gpu_utils/gpuregiontimer.h: *  The implementations live in gpuregiontimer.cuh for CUDA and gpuregiontimer_ocl.h for OpenCL.
src/gromacs/gpu_utils/gpuregiontimer.h:#ifndef GMX_GPU_UTILS_GPUREGIONTIMER_H
src/gromacs/gpu_utils/gpuregiontimer.h:#define GMX_GPU_UTILS_GPUREGIONTIMER_H
src/gromacs/gpu_utils/gpuregiontimer.h://! Debug GPU timers in debug builds only
src/gromacs/gpu_utils/gpuregiontimer.h: * This is a GPU region timing wrapper class.
src/gromacs/gpu_utils/gpuregiontimer.h: * It allows for host-side tracking of the accumulated execution timespans in GPU code
src/gromacs/gpu_utils/gpuregiontimer.h: * Internally it uses GpuRegionTimerImpl for measuring regions.
src/gromacs/gpu_utils/gpuregiontimer.h:template<typename GpuRegionTimerImpl>
src/gromacs/gpu_utils/gpuregiontimer.h:class GpuRegionTimerWrapper
src/gromacs/gpu_utils/gpuregiontimer.h:    GpuRegionTimerImpl impl_;
src/gromacs/gpu_utils/gpuregiontimer.h:     * \param[in] deviceStream   The GPU command stream where the event being measured takes place.
src/gromacs/gpu_utils/gpuregiontimer.h:            std::string error = "GPU timer should be idle, but is "
src/gromacs/gpu_utils/gpuregiontimer.h:     * \param[in] deviceStream   The GPU command stream where the event being measured takes place.
src/gromacs/gpu_utils/gpuregiontimer.h:            std::string error = "GPU timer should be recording, but is "
src/gromacs/gpu_utils/gpuregiontimer.h:             * This can be reproduced in OpenCL build by running
src/gromacs/gpu_utils/gpuregiontimer.h:             * Similarly, the GpuRegionTimerImpl suffers from non-nullptr
src/gromacs/gpu_utils/gpuregiontimer.h:               std::string error = "GPU timer should be stopped, but is " + std::string((debugState_ == TimerState::Idle) ? "idle" : "recording") + ".";
src/gromacs/gpu_utils/gpuregiontimer.h:     * Gets a pointer to a new timing event for passing into individual GPU API calls
src/gromacs/gpu_utils/gpuregiontimer.h:     * within the region if they require it (e.g. on OpenCL).
src/gromacs/gpu_utils/gpuregiontimer.h:            std::string error = "GPU timer should be recording, but is "
src/gromacs/gpu_utils/devicebuffer.cuh:#ifndef GMX_GPU_UTILS_DEVICEBUFFER_CUH
src/gromacs/gpu_utils/devicebuffer.cuh:#define GMX_GPU_UTILS_DEVICEBUFFER_CUH
src/gromacs/gpu_utils/devicebuffer.cuh: *  \brief Implements the DeviceBuffer type and routines for CUDA.
src/gromacs/gpu_utils/devicebuffer.cuh:#include "gromacs/gpu_utils/cuda_arch_utils.cuh"
src/gromacs/gpu_utils/devicebuffer.cuh:#include "gromacs/gpu_utils/cudautils.cuh"
src/gromacs/gpu_utils/devicebuffer.cuh:#include "gromacs/gpu_utils/device_context.h"
src/gromacs/gpu_utils/devicebuffer.cuh:#include "gromacs/gpu_utils/device_stream.h"
src/gromacs/gpu_utils/devicebuffer.cuh:#include "gromacs/gpu_utils/devicebuffer_datatype.h"
src/gromacs/gpu_utils/devicebuffer.cuh:#include "gromacs/gpu_utils/gpu_utils.h" //only for GpuApiCallBehavior
src/gromacs/gpu_utils/devicebuffer.cuh:#include "gromacs/gpu_utils/gputraits.cuh"
src/gromacs/gpu_utils/devicebuffer.cuh: * \param[in]     deviceContext        The buffer's dummy device  context - not managed explicitly in CUDA RT.
src/gromacs/gpu_utils/devicebuffer.cuh:    cudaError_t stat = cudaMalloc(buffer, numValues * sizeof(ValueType));
src/gromacs/gpu_utils/devicebuffer.cuh:            stat == cudaSuccess,
src/gromacs/gpu_utils/devicebuffer.cuh: * Frees a device-side CUDA as well as NVSHMEM buffer.
src/gromacs/gpu_utils/devicebuffer.cuh:        // *buffer is a CUDA pointer.
src/gromacs/gpu_utils/devicebuffer.cuh:        cudaError_t stat = cudaFree(*buffer);
src/gromacs/gpu_utils/devicebuffer.cuh:                stat == cudaSuccess,
src/gromacs/gpu_utils/devicebuffer.cuh: * \param[in]     deviceStream         GPU stream to perform asynchronous copy in.
src/gromacs/gpu_utils/devicebuffer.cuh: *                                     Not used in CUDA implementation.
src/gromacs/gpu_utils/devicebuffer.cuh:                        GpuApiCallBehavior       transferKind,
src/gromacs/gpu_utils/devicebuffer.cuh:    cudaError_t  stat;
src/gromacs/gpu_utils/devicebuffer.cuh:        case GpuApiCallBehavior::Async:
src/gromacs/gpu_utils/devicebuffer.cuh:            GMX_ASSERT(isHostMemoryPinned(hostBuffer), "Source host buffer was not pinned for CUDA");
src/gromacs/gpu_utils/devicebuffer.cuh:            stat = cudaMemcpyAsync(*reinterpret_cast<ValueType**>(buffer) + startingOffset,
src/gromacs/gpu_utils/devicebuffer.cuh:                                   cudaMemcpyHostToDevice,
src/gromacs/gpu_utils/devicebuffer.cuh:                    stat == cudaSuccess,
src/gromacs/gpu_utils/devicebuffer.cuh:        case GpuApiCallBehavior::Sync:
src/gromacs/gpu_utils/devicebuffer.cuh:            stat = cudaMemcpy(*reinterpret_cast<ValueType**>(buffer) + startingOffset,
src/gromacs/gpu_utils/devicebuffer.cuh:                              cudaMemcpyHostToDevice);
src/gromacs/gpu_utils/devicebuffer.cuh:                    stat == cudaSuccess,
src/gromacs/gpu_utils/devicebuffer.cuh: * \param[in]     deviceStream         GPU stream to perform asynchronous copy in.
src/gromacs/gpu_utils/devicebuffer.cuh: *                                     Not used in CUDA implementation.
src/gromacs/gpu_utils/devicebuffer.cuh:                          GpuApiCallBehavior       transferKind,
src/gromacs/gpu_utils/devicebuffer.cuh:    cudaError_t  stat;
src/gromacs/gpu_utils/devicebuffer.cuh:        case GpuApiCallBehavior::Async:
src/gromacs/gpu_utils/devicebuffer.cuh:                       "Destination host buffer was not pinned for CUDA");
src/gromacs/gpu_utils/devicebuffer.cuh:            stat = cudaMemcpyAsync(hostBuffer,
src/gromacs/gpu_utils/devicebuffer.cuh:                                   cudaMemcpyDeviceToHost,
src/gromacs/gpu_utils/devicebuffer.cuh:                    stat == cudaSuccess,
src/gromacs/gpu_utils/devicebuffer.cuh:        case GpuApiCallBehavior::Sync:
src/gromacs/gpu_utils/devicebuffer.cuh:            stat = cudaMemcpy(hostBuffer,
src/gromacs/gpu_utils/devicebuffer.cuh:                              cudaMemcpyDeviceToHost);
src/gromacs/gpu_utils/devicebuffer.cuh:                    stat == cudaSuccess,
src/gromacs/gpu_utils/devicebuffer.cuh: * \param[in]     deviceStream             GPU stream to perform asynchronous copy in.
src/gromacs/gpu_utils/devicebuffer.cuh: * in. Not used in CUDA implementation.
src/gromacs/gpu_utils/devicebuffer.cuh:                              GpuApiCallBehavior       transferKind,
src/gromacs/gpu_utils/devicebuffer.cuh:    cudaError_t  stat;
src/gromacs/gpu_utils/devicebuffer.cuh:        case GpuApiCallBehavior::Async:
src/gromacs/gpu_utils/devicebuffer.cuh:            stat = cudaMemcpyAsync(*destinationDeviceBuffer,
src/gromacs/gpu_utils/devicebuffer.cuh:                                   cudaMemcpyDeviceToDevice,
src/gromacs/gpu_utils/devicebuffer.cuh:                    stat == cudaSuccess,
src/gromacs/gpu_utils/devicebuffer.cuh:        case GpuApiCallBehavior::Sync:
src/gromacs/gpu_utils/devicebuffer.cuh:            stat = cudaMemcpy(*destinationDeviceBuffer, *sourceDeviceBuffer, bytes, cudaMemcpyDeviceToDevice);
src/gromacs/gpu_utils/devicebuffer.cuh:                    stat == cudaSuccess,
src/gromacs/gpu_utils/devicebuffer.cuh: * \param[in]     deviceStream    GPU stream.
src/gromacs/gpu_utils/devicebuffer.cuh:    cudaError_t stat = cudaMemsetAsync(
src/gromacs/gpu_utils/devicebuffer.cuh:    GMX_RELEASE_ASSERT(stat == cudaSuccess,
src/gromacs/gpu_utils/devicebuffer.cuh:using DeviceTexture = cudaTextureObject_t;
src/gromacs/gpu_utils/devicebuffer.cuh:            deviceBuffer, hostBuffer, 0, numValues, deviceStream, GpuApiCallBehavior::Sync, nullptr);
src/gromacs/gpu_utils/devicebuffer.cuh:    if (!c_disableCudaTextures)
src/gromacs/gpu_utils/devicebuffer.cuh:        cudaResourceDesc rd;
src/gromacs/gpu_utils/devicebuffer.cuh:        cudaTextureDesc  td;
src/gromacs/gpu_utils/devicebuffer.cuh:        rd.resType                = cudaResourceTypeLinear;
src/gromacs/gpu_utils/devicebuffer.cuh:        rd.res.linear.desc        = cudaCreateChannelDesc<ValueType>();
src/gromacs/gpu_utils/devicebuffer.cuh:        td.readMode      = cudaReadModeElementType;
src/gromacs/gpu_utils/devicebuffer.cuh:        cudaError_t stat = cudaCreateTextureObject(deviceTexture, &rd, &td, nullptr);
src/gromacs/gpu_utils/devicebuffer.cuh:                stat == cudaSuccess,
src/gromacs/gpu_utils/devicebuffer.cuh:/*! \brief Unbind the texture and release the CUDA texture object.
src/gromacs/gpu_utils/devicebuffer.cuh:    if (!c_disableCudaTextures && deviceTexture && deviceBuffer)
src/gromacs/gpu_utils/devicebuffer.cuh:        cudaError_t stat = cudaDestroyTextureObject(*deviceTexture);
src/gromacs/gpu_utils/devicebuffer.cuh:                stat == cudaSuccess,
src/gromacs/gpu_utils/devicebuffer.cuh: * \param[in]     deviceContext        The buffer's dummy device  context - not managed explicitly in CUDA RT.
src/gromacs/gpu_utils/oclraii.h: * \brief Declare RAII helpers for OpenCL types, along with
src/gromacs/gpu_utils/oclraii.h:#ifndef GMX_GPU_UTILS_OCLRAII_H
src/gromacs/gpu_utils/oclraii.h:#define GMX_GPU_UTILS_OCLRAII_H
src/gromacs/gpu_utils/oclraii.h:#include "gromacs/gpu_utils/gmxopencl.h"
src/gromacs/gpu_utils/oclraii.h:/*! \libinternal \brief Stub for OpenCL type traits */
src/gromacs/gpu_utils/oclraii.h:struct OpenClTraits;
src/gromacs/gpu_utils/oclraii.h:/*! \libinternal \brief Implements common trait infrastructure for OpenCL types. */
src/gromacs/gpu_utils/oclraii.h:struct OpenClTraitsBase
src/gromacs/gpu_utils/oclraii.h:struct OpenClTraits<cl_context> : public OpenClTraitsBase<cl_context>
src/gromacs/gpu_utils/oclraii.h:struct OpenClTraits<cl_command_queue> : public OpenClTraitsBase<cl_command_queue>
src/gromacs/gpu_utils/oclraii.h:struct OpenClTraits<cl_program> : public OpenClTraitsBase<cl_program>
src/gromacs/gpu_utils/oclraii.h:struct OpenClTraits<cl_kernel> : public OpenClTraitsBase<cl_kernel>
src/gromacs/gpu_utils/oclraii.h:/*! \libinternal \brief Wrapper of OpenCL type \c cl_type to implement RAII.
src/gromacs/gpu_utils/oclraii.h: * by OpenClTraits.
src/gromacs/gpu_utils/oclraii.h: * need for that, and would require OpenCL API calls for deep copies
src/gromacs/gpu_utils/oclraii.h:    ~ClHandle() { OpenClTraits<cl_type>::releaser(handle_); }
src/gromacs/gpu_utils/gpueventsynchronizer_helpers.h: *  \brief Helper functions for a GpuEventSynchronizer class.
src/gromacs/gpu_utils/gpueventsynchronizer_helpers.h:#ifndef GMX_GPU_UTILS_GPUEVENTSYNCHRONIZER_HELPERS_H
src/gromacs/gpu_utils/gpueventsynchronizer_helpers.h:#define GMX_GPU_UTILS_GPUEVENTSYNCHRONIZER_HELPERS_H
src/gromacs/gpu_utils/gpueventsynchronizer_helpers.h:void disableGpuEventConsumptionCounting();
src/gromacs/gpu_utils/devicebuffer_datatype.h:#ifndef GMX_GPU_UTILS_DEVICEBUFFER_DATATYPE_H
src/gromacs/gpu_utils/devicebuffer_datatype.h:#define GMX_GPU_UTILS_DEVICEBUFFER_DATATYPE_H
src/gromacs/gpu_utils/devicebuffer_datatype.h:#if GMX_GPU_CUDA || GMX_GPU_HIP
src/gromacs/gpu_utils/devicebuffer_datatype.h:#elif GMX_GPU_OPENCL
src/gromacs/gpu_utils/devicebuffer_datatype.h:#    include "gromacs/gpu_utils/gputraits_ocl.h"
src/gromacs/gpu_utils/devicebuffer_datatype.h:#elif GMX_GPU_SYCL
src/gromacs/gpu_utils/devicebuffer_datatype.h:    //! Both explicit and implicit casts to void* are used in MPI+CUDA code, this stub is necessary for compilation.
src/gromacs/gpu_utils/devicebuffer_datatype.h:#endif // GMX_GPU_UTILS_DEVICEBUFFER_DATATYPE_H
src/gromacs/gpu_utils/hiputils.h:#ifndef GMX_GPU_UTILS_HIPUTILS_H
src/gromacs/gpu_utils/hiputils.h:#define GMX_GPU_UTILS_HIPUTILS_H
src/gromacs/gpu_utils/hiputils.h:#include "gromacs/gpu_utils/device_stream.h"
src/gromacs/gpu_utils/hiputils.h:#include "gromacs/gpu_utils/gputraits_hip.h"
src/gromacs/gpu_utils/hiputils.h:void prepareGpuKernelArgument(KernelPtr /*kernel*/,
src/gromacs/gpu_utils/hiputils.h:void prepareGpuKernelArgument(KernelPtr                          kernel,
src/gromacs/gpu_utils/hiputils.h:    prepareGpuKernelArgument(kernel, kernelArgsPtr, argIndex + 1, otherArgsPtrs...);
src/gromacs/gpu_utils/hiputils.h: * \returns A prepared parameter pack to be used with launchGpuKernel() as the last argument.
src/gromacs/gpu_utils/hiputils.h:std::array<void*, sizeof...(Args)> prepareGpuKernelArguments(KernelPtr kernel,
src/gromacs/gpu_utils/hiputils.h:    prepareGpuKernelArgument(kernel, &kernelArgs, 0, argsPtrs...);
src/gromacs/gpu_utils/hiputils.h: * \param[in] deviceStream    GPU stream to launch kernel in
src/gromacs/gpu_utils/hiputils.h: *                            prepareGpuKernelArguments()
src/gromacs/gpu_utils/hiputils.h:void launchGpuKernel(void (*kernel)(Args...),
src/gromacs/gpu_utils/hiputils.h:                       ("GPU kernel (" + std::string(kernelName)
src/gromacs/gpu_utils/gpuregiontimer_ocl.h: *  \brief Implements the GPU region timer for OpenCL.
src/gromacs/gpu_utils/gpuregiontimer_ocl.h:#ifndef GMX_GPU_UTILS_GPUREGIONTIMER_OCL_H
src/gromacs/gpu_utils/gpuregiontimer_ocl.h:#define GMX_GPU_UTILS_GPUREGIONTIMER_OCL_H
src/gromacs/gpu_utils/gpuregiontimer_ocl.h:#include "gromacs/gpu_utils/gputraits_ocl.h"
src/gromacs/gpu_utils/gpuregiontimer_ocl.h:#include "gromacs/gpu_utils/oclutils.h"
src/gromacs/gpu_utils/gpuregiontimer_ocl.h:#include "gpuregiontimer.h"
src/gromacs/gpu_utils/gpuregiontimer_ocl.h: * The OpenCL implementation of the GPU code region timing.
src/gromacs/gpu_utils/gpuregiontimer_ocl.h: * With OpenCL, one has to use cl_event handle for each API call that has to be timed, and
src/gromacs/gpu_utils/gpuregiontimer_ocl.h:class GpuRegionTimerImpl
src/gromacs/gpu_utils/gpuregiontimer_ocl.h:    GpuRegionTimerImpl()  = default;
src/gromacs/gpu_utils/gpuregiontimer_ocl.h:    ~GpuRegionTimerImpl() = default;
src/gromacs/gpu_utils/gpuregiontimer_ocl.h:    GpuRegionTimerImpl(const GpuRegionTimerImpl&) = delete;
src/gromacs/gpu_utils/gpuregiontimer_ocl.h:    GpuRegionTimerImpl& operator=(GpuRegionTimerImpl&&) = delete;
src/gromacs/gpu_utils/gpuregiontimer_ocl.h:    GpuRegionTimerImpl(GpuRegionTimerImpl&&) = delete;
src/gromacs/gpu_utils/gpuregiontimer_ocl.h:                           gmx::formatString("GPU timing update failure (OpenCL error %d: %s).",
src/gromacs/gpu_utils/gpuregiontimer_ocl.h:                           gmx::formatString("GPU timing update failure (OpenCL error %d: %s).",
src/gromacs/gpu_utils/gpuregiontimer_ocl.h:                GMX_ASSERT(CL_SUCCESS == cl_error, "OpenCL event release failure");
src/gromacs/gpu_utils/gpuregiontimer_ocl.h:     * for passing into individual GPU API calls
src/gromacs/gpu_utils/gpuregiontimer_ocl.h:     * within the region if the API requires it (e.g. on OpenCL).
src/gromacs/gpu_utils/gpuregiontimer_ocl.h:using GpuRegionTimer = GpuRegionTimerWrapper<GpuRegionTimerImpl>;
src/gromacs/gpu_utils/hip_kernel_utils.h:#ifndef GMX_GPU_UTILS_HIP_KERNEL_UTILS_H
src/gromacs/gpu_utils/hip_kernel_utils.h:#define GMX_GPU_UTILS_HIP_KERNEL_UTILS_H
src/gromacs/gpu_utils/hip_kernel_utils.h: *  \brief HIP device util functions (callable from GPU kernels).
src/gromacs/gpu_utils/hip_kernel_utils.h:    __attribute__((amdgpu_flat_work_group_size(WORK_GROUP_SIZE, WORK_GROUP_SIZE), \
src/gromacs/gpu_utils/hip_kernel_utils.h:                   amdgpu_waves_per_eu(WAVES_PER_EU, WAVES_PER_EU)))
src/gromacs/gpu_utils/hip_kernel_utils.h:    __attribute__((amdgpu_flat_work_group_size(WORK_GROUP_SIZE, WORK_GROUP_SIZE)))
src/gromacs/gpu_utils/hip_kernel_utils.h:#endif /* GMX_GPU_UTILS_HIP_KERNEL_UTILS_H */
src/gromacs/gpu_utils/device_stream_manager.h: * \brief This file declares a manager of GPU context and streams needed for
src/gromacs/gpu_utils/device_stream_manager.h: * running workloads on GPUs.
src/gromacs/gpu_utils/device_stream_manager.h: * \ingroup module_gpu_utils
src/gromacs/gpu_utils/device_stream_manager.h:#ifndef GMX_GPU_UTILS_GPUSTREAMMANAGER_H
src/gromacs/gpu_utils/device_stream_manager.h:#define GMX_GPU_UTILS_GPUSTREAMMANAGER_H
src/gromacs/gpu_utils/device_stream_manager.h: * for GPU work.
src/gromacs/gpu_utils/device_stream_manager.h: * it, objects of this class manage the lifetime of the GPU context
src/gromacs/gpu_utils/device_stream_manager.h: * If supported by the GPU API, the available runtime and the
src/gromacs/gpu_utils/device_stream_manager.h:    /*! \brief Returns a handle to the requested GPU stream.
src/gromacs/gpu_utils/device_stream_manager.h:    //! \brief Returns a handle to the GPU stream to compute bonded forces in.
src/gromacs/gpu_utils/device_stream_manager.h:    /*! \brief Return whether the requested GPU stream is valid for use.
src/gromacs/gpu_utils/device_stream_hip.cpp: * \ingroup module_gpu_utils
src/gromacs/gpu_utils/device_stream_hip.cpp:#include "gromacs/gpu_utils/hiputils.h"
src/gromacs/gpu_utils/gpuregiontimer_sycl.h: *  \brief Implements the GPU region timer for SYCL.
src/gromacs/gpu_utils/gpuregiontimer_sycl.h:#ifndef GMX_GPU_UTILS_GPUREGIONTIMER_SYCL_H
src/gromacs/gpu_utils/gpuregiontimer_sycl.h:#define GMX_GPU_UTILS_GPUREGIONTIMER_SYCL_H
src/gromacs/gpu_utils/gpuregiontimer_sycl.h:#include "gromacs/gpu_utils/devicebuffer_sycl.h"
src/gromacs/gpu_utils/gpuregiontimer_sycl.h:#include "gromacs/gpu_utils/gputraits.h"
src/gromacs/gpu_utils/gpuregiontimer_sycl.h:#include "gpuregiontimer.h"
src/gromacs/gpu_utils/gpuregiontimer_sycl.h:// Disabling Doxygen to avoid it having GpuRegionTimerImpl from both OpenCL and SYCL
src/gromacs/gpu_utils/gpuregiontimer_sycl.h: * The stub of SYCL implementation of the GPU code region timing.
src/gromacs/gpu_utils/gpuregiontimer_sycl.h:class GpuRegionTimerImpl
src/gromacs/gpu_utils/gpuregiontimer_sycl.h:    GpuRegionTimerImpl()  = default;
src/gromacs/gpu_utils/gpuregiontimer_sycl.h:    ~GpuRegionTimerImpl() = default;
src/gromacs/gpu_utils/gpuregiontimer_sycl.h:    GpuRegionTimerImpl(const GpuRegionTimerImpl&) = delete;
src/gromacs/gpu_utils/gpuregiontimer_sycl.h:    GpuRegionTimerImpl& operator=(GpuRegionTimerImpl&&) = delete;
src/gromacs/gpu_utils/gpuregiontimer_sycl.h:    GpuRegionTimerImpl(GpuRegionTimerImpl&&) = delete;
src/gromacs/gpu_utils/gpuregiontimer_sycl.h:     * for passing into individual GPU API calls
src/gromacs/gpu_utils/gpuregiontimer_sycl.h:     * within the region if the API requires it (e.g. on OpenCL).
src/gromacs/gpu_utils/gpuregiontimer_sycl.h:using GpuRegionTimer = GpuRegionTimerWrapper<GpuRegionTimerImpl>;
src/gromacs/gpu_utils/gmxsycl.h:#ifndef GMX_GPU_UTILS_GMXSYCL_H
src/gromacs/gpu_utils/gmxsycl.h:#define GMX_GPU_UTILS_GMXSYCL_H
src/gromacs/gpu_utils/gmxsycl.h: * Gives some nice performance optimizations, especially on AMD and NVIDIA devices.
src/gromacs/gpu_utils/device_stream_ocl.cpp: * \brief Implements the DeviceStream for OpenCL.
src/gromacs/gpu_utils/device_stream_ocl.cpp: * \ingroup module_gpu_utils
src/gromacs/gpu_utils/device_stream_ocl.cpp:#include "gromacs/gpu_utils/device_context.h"
src/gromacs/gpu_utils/device_stream_ocl.cpp:#include "gromacs/gpu_utils/device_stream.h"
src/gromacs/gpu_utils/device_stream_ocl.cpp:#include "gromacs/gpu_utils/gputraits_ocl.h"
src/gromacs/gpu_utils/device_stream_ocl.cpp:#include "gromacs/gpu_utils/oclutils.h"
src/gromacs/gpu_utils/device_stream_ocl.cpp:                "Failed to create OpenCL command queue on GPU %s (OpenCL error ID %d).",
src/gromacs/gpu_utils/device_stream_ocl.cpp:            std::fprintf(stderr, "Failed to release OpenCL stream (OpenCL error ID %d).\n", clError);
src/gromacs/gpu_utils/device_stream_ocl.cpp:            gmx::formatString("Error caught during clFinish (OpenCL error ID %d).", clError).c_str());
src/gromacs/gpu_utils/sycl_kernel_utils.h:#ifndef GMX_GPU_UTILS_SYCL_KERNEL_UTILS_H
src/gromacs/gpu_utils/sycl_kernel_utils.h:#define GMX_GPU_UTILS_SYCL_KERNEL_UTILS_H
src/gromacs/gpu_utils/sycl_kernel_utils.h://! \brief Full warp active thread mask used in CUDA warp-level primitives.
src/gromacs/gpu_utils/sycl_kernel_utils.h:static constexpr unsigned int c_cudaFullWarpMask = 0xffffffff;
src/gromacs/gpu_utils/sycl_kernel_utils.h: * Technically, asserts should work just fine with hipSYCL, since they are supported by CUDA since sm_20.
src/gromacs/gpu_utils/sycl_kernel_utils.h: * But with some settings (Clang 14, hipSYCL 0.9.2, RelWithAssert), CUDA build fails at link time:
src/gromacs/gpu_utils/sycl_kernel_utils.h: * Also disable asserts on AMD GPUs, since they cause `hipErrorIllegalState` (oneAPI 2024.1-2024.2, ROCm 5.4-6.1) */
src/gromacs/gpu_utils/sycl_kernel_utils.h:    // Skip compiling for CPU. Makes compiling this file ~10% faster for oneAPI/CUDA or
src/gromacs/gpu_utils/sycl_kernel_utils.h:    // hipSYCL/CUDA. For DPC++, any non-CPU targets must be explicitly allowed in the #if below.
src/gromacs/gpu_utils/sycl_kernel_utils.h:#if GMX_SYCL_ACPP && GMX_ACPP_HAVE_CUDA_TARGET
src/gromacs/gpu_utils/sycl_kernel_utils.h:     * e.g. Clang 14, hipSYCL 0.9.2, CUDA 11.5, Debug:
src/gromacs/gpu_utils/sycl_kernel_utils.h:     * As a demonstration of correctness, we don't use atomic loads in CUDA and it's doing fine.
src/gromacs/gpu_utils/sycl_kernel_utils.h: * Equivalent with CUDA's \c syncwarp(c_cudaFullWarpMask).
src/gromacs/gpu_utils/sycl_kernel_utils.h:#endif /* GMX_GPU_UTILS_SYCL_KERNEL_UTILS_H */
src/gromacs/gpu_utils/ocl_compiler.h: *  \brief Declare infrastructure for OpenCL JIT compilation
src/gromacs/gpu_utils/ocl_compiler.h:#ifndef GMX_GPU_UTILS_OCL_COMPILER_H
src/gromacs/gpu_utils/ocl_compiler.h:#define GMX_GPU_UTILS_OCL_COMPILER_H
src/gromacs/gpu_utils/ocl_compiler.h:#include "gromacs/gpu_utils/oclutils.h"
src/gromacs/gpu_utils/ocl_compiler.h: *  This is platform implementation dependent and seems to only work on the Nvidia and AMD
src/gromacs/gpu_utils/ocl_compiler.h: * platforms! Nvidia reports 32, AMD for GPU 64. Intel seems to report 16, but that is not correct,
src/gromacs/gpu_utils/ocl_compiler.h: *  \param  context   Current OpenCL context
src/gromacs/gpu_utils/ocl_compiler.h: *  \param  deviceId OpenCL device with the context
src/gromacs/gpu_utils/ocl_compiler.h: * \throws InternalError if an OpenCL error was encountered
src/gromacs/gpu_utils/ocl_compiler.h: *  \param  kernel   THe OpenCL kernel object
src/gromacs/gpu_utils/ocl_compiler.h: *  \param  deviceId OpenCL device for which the kernel warp size is queried
src/gromacs/gpu_utils/ocl_compiler.h: * \throws InternalError if an OpenCL error was encountered
src/gromacs/gpu_utils/ocl_compiler.h: * \param[in]  context               OpenCL context on the device to compile for
src/gromacs/gpu_utils/ocl_compiler.h: * \param[in]  deviceId              OpenCL device id of the device to compile for
src/gromacs/gpu_utils/ocl_compiler.h: * \returns The compiled OpenCL program
src/gromacs/gpu_utils/ocl_compiler.h: *         InternalError   if an OpenCL API error prevents returning a valid compiled program. */
src/gromacs/gpu_utils/gpuregiontimer.cuh: *  \brief Implements the GPU region timer for CUDA.
src/gromacs/gpu_utils/gpuregiontimer.cuh:#ifndef GMX_GPU_UTILS_GPUREGIONTIMER_CUH
src/gromacs/gpu_utils/gpuregiontimer.cuh:#define GMX_GPU_UTILS_GPUREGIONTIMER_CUH
src/gromacs/gpu_utils/gpuregiontimer.cuh:#include "gromacs/gpu_utils/cudautils.cuh"
src/gromacs/gpu_utils/gpuregiontimer.cuh:#include "gromacs/gpu_utils/gputraits.cuh"
src/gromacs/gpu_utils/gpuregiontimer.cuh:#include "gpuregiontimer.h"
src/gromacs/gpu_utils/gpuregiontimer.cuh: * This is a GPU region timing implementation for CUDA.
src/gromacs/gpu_utils/gpuregiontimer.cuh:class GpuRegionTimerImpl
src/gromacs/gpu_utils/gpuregiontimer.cuh:    cudaEvent_t eventStart_, eventStop_;
src/gromacs/gpu_utils/gpuregiontimer.cuh:    GpuRegionTimerImpl()
src/gromacs/gpu_utils/gpuregiontimer.cuh:        const int eventFlags = cudaEventDefault;
src/gromacs/gpu_utils/gpuregiontimer.cuh:        CU_RET_ERR(cudaEventCreate(&eventStart_, eventFlags), "GPU timing creation failure");
src/gromacs/gpu_utils/gpuregiontimer.cuh:        CU_RET_ERR(cudaEventCreate(&eventStop_, eventFlags), "GPU timing creation failure");
src/gromacs/gpu_utils/gpuregiontimer.cuh:    ~GpuRegionTimerImpl()
src/gromacs/gpu_utils/gpuregiontimer.cuh:        CU_RET_ERR(cudaEventDestroy(eventStart_), "GPU timing destruction failure");
src/gromacs/gpu_utils/gpuregiontimer.cuh:        CU_RET_ERR(cudaEventDestroy(eventStop_), "GPU timing destruction failure");
src/gromacs/gpu_utils/gpuregiontimer.cuh:    GpuRegionTimerImpl(const GpuRegionTimerImpl&) = delete;
src/gromacs/gpu_utils/gpuregiontimer.cuh:    GpuRegionTimerImpl& operator=(GpuRegionTimerImpl&&) = delete;
src/gromacs/gpu_utils/gpuregiontimer.cuh:    GpuRegionTimerImpl(GpuRegionTimerImpl&&) = delete;
src/gromacs/gpu_utils/gpuregiontimer.cuh:        CU_RET_ERR(cudaEventRecord(eventStart_, deviceStream.stream()),
src/gromacs/gpu_utils/gpuregiontimer.cuh:                   "GPU timing recording failure");
src/gromacs/gpu_utils/gpuregiontimer.cuh:        CU_RET_ERR(cudaEventRecord(eventStop_, deviceStream.stream()),
src/gromacs/gpu_utils/gpuregiontimer.cuh:                   "GPU timing recording failure");
src/gromacs/gpu_utils/gpuregiontimer.cuh:        CU_RET_ERR(cudaEventElapsedTime(&milliseconds, eventStart_, eventStop_),
src/gromacs/gpu_utils/gpuregiontimer.cuh:                   "GPU timing update failure");
src/gromacs/gpu_utils/gpuregiontimer.cuh:     * for passing into individual GPU API calls.
src/gromacs/gpu_utils/gpuregiontimer.cuh:     * This is just a dummy in CUDA.
src/gromacs/gpu_utils/gpuregiontimer.cuh:using GpuRegionTimer = GpuRegionTimerWrapper<GpuRegionTimerImpl>;
src/gromacs/gpu_utils/gpu_utils.h: *  \brief Declare functions for detection and initialization for GPU devices.
src/gromacs/gpu_utils/gpu_utils.h:#ifndef GMX_GPU_UTILS_GPU_UTILS_H
src/gromacs/gpu_utils/gpu_utils.h:#define GMX_GPU_UTILS_GPU_UTILS_H
src/gromacs/gpu_utils/gpu_utils.h:enum class GpuApiCallBehavior : int
src/gromacs/gpu_utils/gpu_utils.h://! String corresponding to GPU API call behavior
src/gromacs/gpu_utils/gpu_utils.h:const char* enumValueToString(GpuApiCallBehavior enumValue);
src/gromacs/gpu_utils/gpu_utils.h://! Types of actions associated to waiting or checking the completion of GPU tasks
src/gromacs/gpu_utils/gpu_utils.h:enum class GpuTaskCompletion
src/gromacs/gpu_utils/gpu_utils.h:/*! \brief Starts the GPU profiler if mdrun is being profiled.
src/gromacs/gpu_utils/gpu_utils.h: *  rest of the run (or until stopGpuProfiler is called).
src/gromacs/gpu_utils/gpu_utils.h: *  Note that this is implemented only for the CUDA API.
src/gromacs/gpu_utils/gpu_utils.h:void startGpuProfiler();
src/gromacs/gpu_utils/gpu_utils.h:/*! \brief Resets the GPU profiler if mdrun is being profiled.
src/gromacs/gpu_utils/gpu_utils.h: * Note that this is implemented only for the CUDA API.
src/gromacs/gpu_utils/gpu_utils.h:void resetGpuProfiler();
src/gromacs/gpu_utils/gpu_utils.h:/*! \brief Stops the CUDA profiler if mdrun is being profiled.
src/gromacs/gpu_utils/gpu_utils.h: *  Note that this is implemented only for the CUDA API.
src/gromacs/gpu_utils/gpu_utils.h:void stopGpuProfiler();
src/gromacs/gpu_utils/gpu_utils.h://! Tells whether the host buffer was pinned for non-blocking transfers. Only implemented for CUDA.
src/gromacs/gpu_utils/gpu_utils.h:/*! \brief Enable peer access between GPUs where supported
src/gromacs/gpu_utils/gpu_utils.h: * \param[in] gpuIdsToUse   List of GPU IDs in use
src/gromacs/gpu_utils/gpu_utils.h:void setupGpuDevicePeerAccess(gmx::ArrayRef<const int> gpuIdsToUse, const gmx::MDLogger& mdlog);
src/gromacs/gpu_utils/gpu_utils.h:/*! \brief Check the platform-defaults and environment variable to decide whether GPU timings
src/gromacs/gpu_utils/gpu_utils.h: * Currently, timings are enabled for OpenCL, but disabled for CUDA and SYCL. This can be overridden
src/gromacs/gpu_utils/gpu_utils.h: * by \c GMX_ENABLE_GPU_TIMING and \c GMX_DISABLE_GPU_TIMING environment variables.
src/gromacs/gpu_utils/gpu_utils.h:bool decideGpuTimingsUsage();
src/gromacs/gpu_utils/device_stream.cu: * \brief Implements the DeviceStream for CUDA.
src/gromacs/gpu_utils/device_stream.cu: * \ingroup module_gpu_utils
src/gromacs/gpu_utils/device_stream.cu:#include "gromacs/gpu_utils/cudautils.cuh"
src/gromacs/gpu_utils/device_stream.cu:    cudaError_t stat;
src/gromacs/gpu_utils/device_stream.cu:        stat = cudaStreamCreate(&stream_);
src/gromacs/gpu_utils/device_stream.cu:        gmx::checkDeviceError(stat, "Could not create CUDA stream.");
src/gromacs/gpu_utils/device_stream.cu:        stat = cudaDeviceGetStreamPriorityRange(nullptr, &highestPriority);
src/gromacs/gpu_utils/device_stream.cu:        gmx::checkDeviceError(stat, "Could not query CUDA stream priority range.");
src/gromacs/gpu_utils/device_stream.cu:        stat = cudaStreamCreateWithPriority(&stream_, cudaStreamDefault, highestPriority);
src/gromacs/gpu_utils/device_stream.cu:        gmx::checkDeviceError(stat, "Could not create CUDA stream with high priority.");
src/gromacs/gpu_utils/device_stream.cu:        cudaError_t stat = cudaStreamDestroy(stream_);
src/gromacs/gpu_utils/device_stream.cu:        if (stat != cudaSuccess)
src/gromacs/gpu_utils/device_stream.cu:                         "Failed to release CUDA stream. %s\n",
src/gromacs/gpu_utils/device_stream.cu:cudaStream_t DeviceStream::stream() const
src/gromacs/gpu_utils/device_stream.cu:    cudaError_t stat = cudaStreamSynchronize(stream_);
src/gromacs/gpu_utils/device_stream.cu:    GMX_RELEASE_ASSERT(stat == cudaSuccess,
src/gromacs/gpu_utils/device_stream.cu:                       ("cudaStreamSynchronize failed. " + gmx::getDeviceErrorString(stat)).c_str());
src/gromacs/gpu_utils/vectype_ops_hip.h:#ifndef GMX_GPU_UTILS_VECTYPE_OPS_HIP_H
src/gromacs/gpu_utils/vectype_ops_hip.h:#define GMX_GPU_UTILS_VECTYPE_OPS_HIP_H
src/gromacs/gpu_utils/vectype_ops_hip.h:#include "gromacs/gpu_utils/vectype_ops_hip_base_math.h"
src/gromacs/gpu_utils/vectype_ops_hip.h:#include "vectype_ops_cuda_hip_shared_declarations.h"
src/gromacs/gpu_utils/vectype_ops_hip.h:#include "vectype_ops_cuda_hip_shared_trig_math.h"
src/gromacs/gpu_utils/gputraits_hip.h:#ifndef GMX_GPU_UTILS_GPUTRAITS_HIP_H
src/gromacs/gpu_utils/gputraits_hip.h:#define GMX_GPU_UTILS_GPUTRAITS_HIP_H
src/gromacs/gpu_utils/gputraits_hip.h: * \ingroup module_gpu_utils
src/gromacs/gpu_utils/gputraits_hip.h://! \brief Single GPU call timing event - meaningless in HIP
src/gromacs/gpu_utils/gputraits_hip.h: * GPU kernels scheduling description. This is same in OpenCL/HIP.
src/gromacs/gpu_utils/gputraits_hip.h: * Provides reasonable defaults, one typically only needs to set the GPU stream
src/gromacs/gpu_utils/pmalloc.h:#ifndef GMX_GPU_UTILS_PMALLOC_H
src/gromacs/gpu_utils/pmalloc.h:#define GMX_GPU_UTILS_PMALLOC_H
src/gromacs/gpu_utils/devicebuffer_sycl.h:#ifndef GMX_GPU_UTILS_DEVICEBUFFER_SYCL_H
src/gromacs/gpu_utils/devicebuffer_sycl.h:#define GMX_GPU_UTILS_DEVICEBUFFER_SYCL_H
src/gromacs/gpu_utils/devicebuffer_sycl.h:#include "gromacs/gpu_utils/device_context.h"
src/gromacs/gpu_utils/devicebuffer_sycl.h:#include "gromacs/gpu_utils/device_stream.h"
src/gromacs/gpu_utils/devicebuffer_sycl.h:#include "gromacs/gpu_utils/devicebuffer_datatype.h"
src/gromacs/gpu_utils/devicebuffer_sycl.h:#include "gromacs/gpu_utils/gmxsycl.h"
src/gromacs/gpu_utils/devicebuffer_sycl.h:#include "gromacs/gpu_utils/gpu_utils.h" //only for GpuApiCallBehavior
src/gromacs/gpu_utils/devicebuffer_sycl.h:#include "gromacs/gpu_utils/gputraits_sycl.h"
src/gromacs/gpu_utils/devicebuffer_sycl.h: * A hacky way to make SYCL implementation of DeviceBuffer compatible with details of CUDA and
src/gromacs/gpu_utils/devicebuffer_sycl.h: * OpenCL implementations.
src/gromacs/gpu_utils/devicebuffer_sycl.h: * Unlike in CUDA and OpenCL, synchronous call does not guarantee that all previously
src/gromacs/gpu_utils/devicebuffer_sycl.h: * \param[in]     deviceStream         GPU stream to perform asynchronous copy in.
src/gromacs/gpu_utils/devicebuffer_sycl.h:                        GpuApiCallBehavior       transferKind,
src/gromacs/gpu_utils/devicebuffer_sycl.h:    if (transferKind == GpuApiCallBehavior::Async)
src/gromacs/gpu_utils/devicebuffer_sycl.h:    if (transferKind == GpuApiCallBehavior::Sync)
src/gromacs/gpu_utils/devicebuffer_sycl.h: * Unlike in CUDA and OpenCL, synchronous call does not guarantee that all previously
src/gromacs/gpu_utils/devicebuffer_sycl.h: * \param[in]     deviceStream         GPU stream to perform asynchronous copy in.
src/gromacs/gpu_utils/devicebuffer_sycl.h:                          GpuApiCallBehavior       transferKind,
src/gromacs/gpu_utils/devicebuffer_sycl.h:    if (transferKind == GpuApiCallBehavior::Async)
src/gromacs/gpu_utils/devicebuffer_sycl.h:    if (transferKind == GpuApiCallBehavior::Sync)
src/gromacs/gpu_utils/devicebuffer_sycl.h:                              GpuApiCallBehavior       transferKind,
src/gromacs/gpu_utils/devicebuffer_sycl.h:    if (transferKind == GpuApiCallBehavior::Sync)
src/gromacs/gpu_utils/devicebuffer_sycl.h: * \param[in]     deviceStream    GPU stream.
src/gromacs/gpu_utils/devicebuffer_sycl.h: * Like OpenCL, does not really do anything with textures, simply creates a buffer
src/gromacs/gpu_utils/devicebuffer_sycl.h:            deviceBuffer, hostBuffer, 0, numValues, deviceStream, GpuApiCallBehavior::Sync, nullptr);
src/gromacs/gpu_utils/devicebuffer_sycl.h:#endif // GMX_GPU_UTILS_DEVICEBUFFER_SYCL_H
src/gromacs/gpu_utils/ocl_caching.cpp: *  \brief Define infrastructure for OpenCL JIT compilation for Gromacs
src/gromacs/gpu_utils/ocl_caching.cpp:    // Note that the OpenCL API is defined in terms of bytes, and we
src/gromacs/gpu_utils/ocl_caching.cpp:        GMX_THROW(InternalError(formatString("Could not get OpenCL device name, error was %s",
src/gromacs/gpu_utils/ocl_caching.cpp:        GMX_THROW(InternalError("Could not create OpenCL program from the cache file " + filename
src/gromacs/gpu_utils/ocl_caching.cpp:        GMX_THROW(InternalError("Could not get OpenCL program binary size, error was "
src/gromacs/gpu_utils/ocl_caching.cpp:        GMX_THROW(InternalError("Could not get OpenCL program binary, error was "
src/gromacs/gpu_utils/vectype_ops.clh:gmx_opencl_inline float norm_f3(float3 a)
src/gromacs/gpu_utils/vectype_ops.clh:gmx_opencl_inline float norm_ref_f3(float3 a)
src/gromacs/gpu_utils/vectype_ops.clh:gmx_opencl_inline float norm2(float3 a)
src/gromacs/gpu_utils/vectype_ops.clh:gmx_opencl_inline float norm2_ref(float3 a)
src/gromacs/gpu_utils/vectype_ops.clh:gmx_opencl_inline float dist3_f3(float3 a, float3 b)
src/gromacs/gpu_utils/vectype_ops.clh:gmx_opencl_inline float dist3_ref_f3(float3 a, float3 b)
src/gromacs/gpu_utils/vectype_ops.clh:gmx_opencl_inline float norm_f4(float4 a)
src/gromacs/gpu_utils/vectype_ops.clh:gmx_opencl_inline float norm_ref_f4(float4 a)
src/gromacs/gpu_utils/vectype_ops.clh:gmx_opencl_inline float dist3_f4(float4 a, float4 b)
src/gromacs/gpu_utils/vectype_ops.clh:gmx_opencl_inline float dist3_ref_f4(float4 a, float4 b)
src/gromacs/gpu_utils/cuda_kernel_utils.cuh:#ifndef GMX_GPU_UTILS_CUDA_KERNEL_UTILS_CUH
src/gromacs/gpu_utils/cuda_kernel_utils.cuh:#define GMX_GPU_UTILS_CUDA_KERNEL_UTILS_CUH
src/gromacs/gpu_utils/cuda_kernel_utils.cuh: *  \brief CUDA device util functions (callable from GPU kernels).
src/gromacs/gpu_utils/cuda_kernel_utils.cuh:#include "gromacs/gpu_utils/cuda_arch_utils.cuh"
src/gromacs/gpu_utils/cuda_kernel_utils.cuh:static __forceinline__ __device__ T fetchFromTexture(const cudaTextureObject_t texObj, int index)
src/gromacs/gpu_utils/cuda_kernel_utils.cuh:    assert(!c_disableCudaTextures);
src/gromacs/gpu_utils/cuda_kernel_utils.cuh:                                                              const cudaTextureObject_t texObj,
src/gromacs/gpu_utils/cuda_kernel_utils.cuh:#if DISABLE_CUDA_TEXTURES
src/gromacs/gpu_utils/cuda_kernel_utils.cuh:#endif /* GMX_GPU_UTILS_CUDA_KERNEL_UTILS_CUH */
src/gromacs/gpu_utils/cudautils.cuh:#ifndef GMX_GPU_UTILS_CUDAUTILS_CUH
src/gromacs/gpu_utils/cudautils.cuh:#define GMX_GPU_UTILS_CUDAUTILS_CUH
src/gromacs/gpu_utils/cudautils.cuh:#include "gromacs/gpu_utils/device_stream.h"
src/gromacs/gpu_utils/cudautils.cuh:#include "gromacs/gpu_utils/gputraits.cuh"
src/gromacs/gpu_utils/cudautils.cuh: * \param[in]  deviceError  The error to assert cudaSuccess on.
src/gromacs/gpu_utils/cudautils.cuh: * \returns A description of the API error. Returns '(CUDA error #0 (cudaSuccess): no error)' in case deviceError is cudaSuccess.
src/gromacs/gpu_utils/cudautils.cuh:inline std::string getDeviceErrorString(const cudaError_t deviceError)
src/gromacs/gpu_utils/cudautils.cuh:    return formatString("CUDA error #%d (%s): %s.",
src/gromacs/gpu_utils/cudautils.cuh:                        cudaGetErrorName(deviceError),
src/gromacs/gpu_utils/cudautils.cuh:                        cudaGetErrorString(deviceError));
src/gromacs/gpu_utils/cudautils.cuh: * \param[in]  deviceError  The error to assert cudaSuccess on.
src/gromacs/gpu_utils/cudautils.cuh:inline void checkDeviceError(const cudaError_t deviceError, const std::string& errorMessage)
src/gromacs/gpu_utils/cudautils.cuh:    if (deviceError != cudaSuccess)
src/gromacs/gpu_utils/cudautils.cuh:    cudaError_t deviceError = cudaGetLastError();
src/gromacs/gpu_utils/cudautils.cuh:    if (deviceError == cudaSuccess)
src/gromacs/gpu_utils/cudautils.cuh:            errorMessage + " An unhandled error from a previous CUDA operation was detected. "
src/gromacs/gpu_utils/cudautils.cuh:    GMX_ASSERT(deviceError == cudaSuccess, fullErrorMessage.c_str());
src/gromacs/gpu_utils/cudautils.cuh:#define CHECK_CUDA_ERRORS
src/gromacs/gpu_utils/cudautils.cuh:#ifdef CHECK_CUDA_ERRORS
src/gromacs/gpu_utils/cudautils.cuh:/*! Check for CUDA error on the return status of a CUDA RT API call. */
src/gromacs/gpu_utils/cudautils.cuh:            if ((deviceError) != cudaSuccess)                                                       \
src/gromacs/gpu_utils/cudautils.cuh:#else /* CHECK_CUDA_ERRORS */
src/gromacs/gpu_utils/cudautils.cuh:#endif /* CHECK_CUDA_ERRORS */
src/gromacs/gpu_utils/cudautils.cuh:// GPU table object. There is also almost self-contained fetchFromParamLookupTable()
src/gromacs/gpu_utils/cudautils.cuh:// in cuda_kernel_utils.cuh. They could all live in a separate class/struct file.
src/gromacs/gpu_utils/cudautils.cuh: *  \param[in] deviceStream CUDA stream to check.
src/gromacs/gpu_utils/cudautils.cuh:    cudaError_t stat = cudaStreamQuery(deviceStream.stream());
src/gromacs/gpu_utils/cudautils.cuh:    if (stat == cudaErrorNotReady)
src/gromacs/gpu_utils/cudautils.cuh:    GMX_ASSERT(stat != cudaErrorInvalidResourceHandle,
src/gromacs/gpu_utils/cudautils.cuh:    // cudaSuccess and cudaErrorNotReady are the expected return values
src/gromacs/gpu_utils/cudautils.cuh:    CU_RET_ERR(stat, "Unexpected cudaStreamQuery failure. ");
src/gromacs/gpu_utils/cudautils.cuh:    GMX_ASSERT(stat == cudaSuccess,
src/gromacs/gpu_utils/cudautils.cuh:               ("Values other than cudaSuccess should have been explicitly handled. "
src/gromacs/gpu_utils/cudautils.cuh: * A function for setting up a single CUDA kernel argument.
src/gromacs/gpu_utils/cudautils.cuh:void prepareGpuKernelArgument(KernelPtr /*kernel*/,
src/gromacs/gpu_utils/cudautils.cuh: * Compile-time recursive function for setting up a single CUDA kernel argument.
src/gromacs/gpu_utils/cudautils.cuh:void prepareGpuKernelArgument(KernelPtr                          kernel,
src/gromacs/gpu_utils/cudautils.cuh:    prepareGpuKernelArgument(kernel, kernelArgsPtr, argIndex + 1, otherArgsPtrs...);
src/gromacs/gpu_utils/cudautils.cuh: * A wrapper function for setting up all the CUDA kernel arguments.
src/gromacs/gpu_utils/cudautils.cuh: * \returns A prepared parameter pack to be used with launchGpuKernel() as the last argument.
src/gromacs/gpu_utils/cudautils.cuh:std::array<void*, sizeof...(Args)> prepareGpuKernelArguments(KernelPtr kernel,
src/gromacs/gpu_utils/cudautils.cuh:    prepareGpuKernelArgument(kernel, &kernelArgs, 0, argsPtrs...);
src/gromacs/gpu_utils/cudautils.cuh:/*! \brief Launches the CUDA kernel and handles the errors.
src/gromacs/gpu_utils/cudautils.cuh: * \param[in] deviceStream    GPU stream to launch kernel in
src/gromacs/gpu_utils/cudautils.cuh: *                            prepareGpuKernelArguments()
src/gromacs/gpu_utils/cudautils.cuh:void launchGpuKernel(void (*kernel)(Args...),
src/gromacs/gpu_utils/cudautils.cuh:    auto stat = cudaLaunchKernel(reinterpret_cast<void*>(kernel),
src/gromacs/gpu_utils/cudautils.cuh:    GMX_RELEASE_ASSERT(stat == cudaSuccess,
src/gromacs/gpu_utils/cudautils.cuh:                       ("GPU kernel (" + std::string(kernelName)
src/gromacs/gpu_utils/gpu_macros.h:#ifndef GMX_GPU_UTILS_MACROS_H
src/gromacs/gpu_utils/gpu_macros.h:#define GMX_GPU_UTILS_MACROS_H
src/gromacs/gpu_utils/gpu_macros.h:   that non-GPU Gromacs can run with no overhead without conditionality
src/gromacs/gpu_utils/gpu_macros.h:   everywhere a GPU function is called. */
src/gromacs/gpu_utils/gpu_macros.h:#    define GPU_FUNC_QUALIFIER REAL_FUNC_QUALIFIER
src/gromacs/gpu_utils/gpu_macros.h:#    define GPU_FUNC_ARGUMENT REAL_FUNC_ARGUMENT
src/gromacs/gpu_utils/gpu_macros.h:#    define GPU_FUNC_TERM REAL_FUNC_TERM
src/gromacs/gpu_utils/gpu_macros.h:#    define GPU_FUNC_TERM_WITH_RETURN(arg) REAL_FUNC_TERM_WITH_RETURN(arg)
src/gromacs/gpu_utils/gpu_macros.h:#    define CUDA_HIP_FUNC_QUALIFIER REAL_FUNC_QUALIFIER
src/gromacs/gpu_utils/gpu_macros.h:#    define CUDA_HIP_FUNC_ARGUMENT REAL_FUNC_ARGUMENT
src/gromacs/gpu_utils/gpu_macros.h:#    define CUDA_HIP_FUNC_TERM REAL_FUNC_TERM
src/gromacs/gpu_utils/gpu_macros.h:#    define CUDA_HIP_FUNC_TERM_WITH_RETURN(arg) REAL_FUNC_TERM_WITH_RETURN(arg)
src/gromacs/gpu_utils/gpu_macros.h:#    define OPENCL_FUNC_QUALIFIER REAL_FUNC_QUALIFIER
src/gromacs/gpu_utils/gpu_macros.h:#    define OPENCL_FUNC_ARGUMENT REAL_FUNC_ARGUMENT
src/gromacs/gpu_utils/gpu_macros.h:#    define OPENCL_FUNC_TERM REAL_FUNC_TERM
src/gromacs/gpu_utils/gpu_macros.h:#    define OPENCL_FUNC_TERM_WITH_RETURN(arg) REAL_FUNC_TERM_WITH_RETURN(arg)
src/gromacs/gpu_utils/gpu_macros.h:/* GPU support is enabled, so these functions will have real code defined somewhere */
src/gromacs/gpu_utils/gpu_macros.h:#    if GMX_GPU
src/gromacs/gpu_utils/gpu_macros.h:#        define GPU_FUNC_QUALIFIER REAL_FUNC_QUALIFIER
src/gromacs/gpu_utils/gpu_macros.h:#        define GPU_FUNC_ARGUMENT REAL_FUNC_ARGUMENT
src/gromacs/gpu_utils/gpu_macros.h:#        define GPU_FUNC_TERM REAL_FUNC_TERM
src/gromacs/gpu_utils/gpu_macros.h:#        define GPU_FUNC_TERM_WITH_RETURN(arg) REAL_FUNC_TERM_WITH_RETURN(arg)
src/gromacs/gpu_utils/gpu_macros.h:#        define GPU_FUNC_QUALIFIER NULL_FUNC_QUALIFIER
src/gromacs/gpu_utils/gpu_macros.h:#        define GPU_FUNC_ARGUMENT NULL_FUNC_ARGUMENT
src/gromacs/gpu_utils/gpu_macros.h:#        define GPU_FUNC_TERM NULL_FUNC_TERM
src/gromacs/gpu_utils/gpu_macros.h:#        define GPU_FUNC_TERM_WITH_RETURN(arg) NULL_FUNC_TERM_WITH_RETURN(arg)
src/gromacs/gpu_utils/gpu_macros.h:#    if GMX_GPU_OPENCL
src/gromacs/gpu_utils/gpu_macros.h:#        define OPENCL_FUNC_QUALIFIER REAL_FUNC_QUALIFIER
src/gromacs/gpu_utils/gpu_macros.h:#        define OPENCL_FUNC_ARGUMENT REAL_FUNC_ARGUMENT
src/gromacs/gpu_utils/gpu_macros.h:#        define OPENCL_FUNC_TERM REAL_FUNC_TERM
src/gromacs/gpu_utils/gpu_macros.h:#        define OPENCL_FUNC_TERM_WITH_RETURN(arg) REAL_FUNC_TERM_WITH_RETURN(arg)
src/gromacs/gpu_utils/gpu_macros.h:#        define OPENCL_FUNC_QUALIFIER NULL_FUNC_QUALIFIER
src/gromacs/gpu_utils/gpu_macros.h:#        define OPENCL_FUNC_ARGUMENT NULL_FUNC_ARGUMENT
src/gromacs/gpu_utils/gpu_macros.h:#        define OPENCL_FUNC_TERM NULL_FUNC_TERM
src/gromacs/gpu_utils/gpu_macros.h:#        define OPENCL_FUNC_TERM_WITH_RETURN(arg) NULL_FUNC_TERM_WITH_RETURN(arg)
src/gromacs/gpu_utils/gpu_macros.h:#    if GMX_GPU_CUDA || GMX_GPU_HIP
src/gromacs/gpu_utils/gpu_macros.h:#        define CUDA_HIP_FUNC_QUALIFIER REAL_FUNC_QUALIFIER
src/gromacs/gpu_utils/gpu_macros.h:#        define CUDA_HIP_FUNC_ARGUMENT REAL_FUNC_ARGUMENT
src/gromacs/gpu_utils/gpu_macros.h:#        define CUDA_HIP_FUNC_TERM REAL_FUNC_TERM
src/gromacs/gpu_utils/gpu_macros.h:#        define CUDA_HIP_FUNC_TERM_WITH_RETURN(arg) REAL_FUNC_TERM_WITH_RETURN(arg)
src/gromacs/gpu_utils/gpu_macros.h:#        define CUDA_HIP_FUNC_QUALIFIER NULL_FUNC_QUALIFIER
src/gromacs/gpu_utils/gpu_macros.h:#        define CUDA_HIP_FUNC_ARGUMENT NULL_FUNC_ARGUMENT
src/gromacs/gpu_utils/gpu_macros.h:#        define CUDA_HIP_FUNC_TERM NULL_FUNC_TERM
src/gromacs/gpu_utils/gpu_macros.h:#        define CUDA_HIP_FUNC_TERM_WITH_RETURN(arg) NULL_FUNC_TERM_WITH_RETURN(arg)
src/gromacs/gpu_utils/gpu_macros.h:#    if GMX_GPU_SYCL
src/gromacs/gpu_utils/gpu_macros.h:#endif // GMX_GPU_UTILS_MACROS_H
src/gromacs/gpu_utils/hostallocator.h: * to CUDA GPUs, but other possibilities exist in future.
src/gromacs/gpu_utils/hostallocator.h:#ifndef GMX_GPU_UTILS_HOSTALLOCATOR_H
src/gromacs/gpu_utils/hostallocator.h:#define GMX_GPU_UTILS_HOSTALLOCATOR_H
src/gromacs/gpu_utils/hostallocator.h: * For an efficient non-blocking transfer (e.g. to a GPU), the memory
src/gromacs/gpu_utils/hostallocator.h: * containers whose memory may be used for e.g. GPU transfers. The
src/gromacs/gpu_utils/hostallocator.h: * allocations of memory that may be needed for e.g. GPU transfers.
src/gromacs/gpu_utils/hostallocator.h: * that use it. If the GROMACS build is configured with CUDA support,
src/gromacs/gpu_utils/hostallocator.h: * does not support CUDA, then the memory will be allocated with
src/gromacs/gpu_utils/hostallocator.h: * AlignedAllocator. The pin() and unpin() methods work with the CUDA
src/gromacs/gpu_utils/hostallocator.h: * that the allocation will be used to transfer to a GPU.
src/gromacs/gpu_utils/hostallocator.h:     * e.g. GPU transfers.
src/gromacs/gpu_utils/hostallocator.h: * whether a task will run on a GPU, or not). */
src/gromacs/gpu_utils/device_event_ocl.h: *  \brief Implements a DeviceEvent class for OpenCL.
src/gromacs/gpu_utils/device_event_ocl.h:#ifndef GMX_GPU_UTILS_DEVICE_EVENT_OCL_H
src/gromacs/gpu_utils/device_event_ocl.h:#define GMX_GPU_UTILS_DEVICE_EVENT_OCL_H
src/gromacs/gpu_utils/device_event_ocl.h:#include "gromacs/gpu_utils/gputraits_ocl.h"
src/gromacs/gpu_utils/device_event_ocl.h:#include "gromacs/gpu_utils/oclutils.h"
src/gromacs/gpu_utils/device_event_ocl.h:            GMX_THROW(gmx::InternalError("Failed to enqueue the GPU synchronization event: "
src/gromacs/gpu_utils/device_event_ocl.h:            GMX_THROW(gmx::InternalError("Failed to synchronize on the GPU event: "
src/gromacs/gpu_utils/device_event_ocl.h:            GMX_THROW(gmx::InternalError("Failed to enqueue device barrier for the GPU event: "
src/gromacs/gpu_utils/device_event_ocl.h:                GMX_THROW(gmx::InternalError("Failed to release the GPU event: "
src/gromacs/gpu_utils/gpu_utils_impl.cpp:#include "gpu_utils.h"
src/gromacs/gpu_utils/gpu_utils_impl.cpp:void startGpuProfiler() {}
src/gromacs/gpu_utils/gpu_utils_impl.cpp:void stopGpuProfiler() {}
src/gromacs/gpu_utils/gpu_utils_impl.cpp:void resetGpuProfiler() {}
src/gromacs/gpu_utils/gpu_utils_impl.cpp:void setupGpuDevicePeerAccess(gmx::ArrayRef<const int> /* gpuIdsToUse */, const gmx::MDLogger& /* mdlog */)
src/gromacs/gpu_utils/device_event_hip.h:#ifndef GMX_GPU_UTILS_DEVICE_EVENT_HIP_H
src/gromacs/gpu_utils/device_event_hip.h:#define GMX_GPU_UTILS_DEVICE_EVENT_HIP_H
src/gromacs/gpu_utils/device_event_hip.h:#include "gromacs/gpu_utils/device_stream.h"
src/gromacs/gpu_utils/device_event_hip.h:#include "gromacs/gpu_utils/hiputils.h"
src/gromacs/gpu_utils/syclutils.h:#ifndef GMX_GPU_UTILS_SYCLUTILS_H
src/gromacs/gpu_utils/syclutils.h:#define GMX_GPU_UTILS_SYCLUTILS_H
src/gromacs/gpu_utils/syclutils.h:#include "gromacs/gpu_utils/gputraits.h"
src/gromacs/gpu_utils/syclutils.h:void inline prepareGpuKernelArgument(ISyclKernelFunctor* /*kernel*/, size_t /*argIndex*/) {}
src/gromacs/gpu_utils/syclutils.h:void prepareGpuKernelArgument(ISyclKernelFunctor* kernel,
src/gromacs/gpu_utils/syclutils.h:    prepareGpuKernelArgument(kernel, argIndex + 1, otherArgsPtrs...);
src/gromacs/gpu_utils/syclutils.h: * \returns A dummy value to be used with launchGpuKernel() as the last argument.
src/gromacs/gpu_utils/syclutils.h:void* prepareGpuKernelArguments(void* kernel, const KernelLaunchConfig& /*config*/, const Args*... argsPtrs)
src/gromacs/gpu_utils/syclutils.h:    prepareGpuKernelArgument(kernelFunctor, 0, argsPtrs...);
src/gromacs/gpu_utils/syclutils.h: * \param[in] deviceStream    GPU stream to launch kernel in
src/gromacs/gpu_utils/syclutils.h: * \param[in] timingEvent     Timing event, fetched from GpuRegionTimer. Unused.
src/gromacs/gpu_utils/syclutils.h:inline void launchGpuKernel(void*                     kernel,
src/gromacs/gpu_utils/gmxopencl.h: * Wraps the complexity of including OpenCL in Gromacs.
src/gromacs/gpu_utils/gmxopencl.h: * Because OpenCL 2.0 is not officially supported widely, \Gromacs
src/gromacs/gpu_utils/gmxopencl.h:#ifndef GMX_GPU_UTILS_GMXOPENCL_H
src/gromacs/gpu_utils/gmxopencl.h:#define GMX_GPU_UTILS_GMXOPENCL_H
src/gromacs/gpu_utils/gmxopencl.h:/*! \brief Declare to OpenCL SDKs that we intend to use OpenCL API
src/gromacs/gpu_utils/gmxopencl.h:#define CL_USE_DEPRECATED_OPENCL_1_2_APIS
src/gromacs/gpu_utils/gmxopencl.h:#define CL_USE_DEPRECATED_OPENCL_2_0_APIS
src/gromacs/gpu_utils/gmxopencl.h:#    include <OpenCL/opencl.h>
src/gromacs/gpu_utils/gmxopencl.h:#    include <CL/opencl.h>
src/gromacs/gpu_utils/hostallocator.cpp: * suitable for e.g. GPU transfers on CUDA.
src/gromacs/gpu_utils/hostallocator.cpp:#include "gromacs/gpu_utils/gpu_utils.h"
src/gromacs/gpu_utils/hostallocator.cpp:#include "gromacs/gpu_utils/pmalloc.h"
src/gromacs/gpu_utils/device_event.h: *  defers valid GPU declarations to headers valid only in such
src/gromacs/gpu_utils/device_event.h:#ifndef GMX_GPU_UTILS_DEVICE_EVENT_H
src/gromacs/gpu_utils/device_event.h:#define GMX_GPU_UTILS_DEVICE_EVENT_H
src/gromacs/gpu_utils/device_event.h:#if !GMX_GPU || defined(DOXYGEN)
src/gromacs/gpu_utils/device_event.h:        GMX_THROW(gmx::NotImplementedError("Not implemented for non-GPU build"));
src/gromacs/gpu_utils/device_event.h:        GMX_THROW(gmx::NotImplementedError("Not implemented for non-GPU build"));
src/gromacs/gpu_utils/device_event.h:        GMX_THROW(gmx::NotImplementedError("Not implemented for non-GPU build"));
src/gromacs/gpu_utils/device_event.h:        GMX_THROW(gmx::NotImplementedError("Not implemented for non-GPU build"));
src/gromacs/gpu_utils/device_event.h:        GMX_THROW(gmx::NotImplementedError("Not implemented for non-GPU build"));
src/gromacs/gpu_utils/device_event.h:    //! Reset the event (not needed in CUDA)
src/gromacs/gpu_utils/device_event.h:        GMX_THROW(gmx::NotImplementedError("Not implemented for non-GPU build"));
src/gromacs/gpu_utils/device_event.h:#elif GMX_GPU_CUDA
src/gromacs/gpu_utils/device_event.h:#elif GMX_GPU_HIP
src/gromacs/gpu_utils/device_event.h:#elif GMX_GPU_OPENCL
src/gromacs/gpu_utils/device_event.h:#elif GMX_GPU_SYCL
src/gromacs/gpu_utils/device_event.h:#endif // GMX_GPU_UTILS_DEVICE_EVENT_H
src/gromacs/gpu_utils/device_context_sycl.cpp: * \ingroup module_gpu_utils
src/gromacs/gpu_utils/device_context_sycl.cpp:#include "gromacs/gpu_utils/device_context.h"
src/gromacs/gpu_utils/device_context_sycl.cpp:#include "gromacs/gpu_utils/gmxsycl.h"
src/gromacs/gpu_utils/device_context_sycl.cpp:#include "gromacs/gpu_utils/pmalloc.h"
src/gromacs/InstallLibInfo.cmake:    if(CMAKE_CUDA_COMPILER)
src/gromacs/InstallLibInfo.cmake:        set(_gmx_cuda_config
src/gromacs/InstallLibInfo.cmake:"SET(CMAKE_CUDA_COMPILER \"${CMAKE_CUDA_COMPILER}\" CACHE FILEPATH \"Hint for enable_language(CUDA).\")")
src/gromacs/InstallLibInfo.cmake:    unset(_gmx_cuda_config)
src/gromacs/nbnxm/kerneldispatch.cpp:#include "kernels_reference/kernel_gpu_ref.h"
src/gromacs/nbnxm/kerneldispatch.cpp:#include "gromacs/gpu_utils/hostallocator.h"
src/gromacs/nbnxm/kerneldispatch.cpp:#include "gromacs/nbnxm/gpu_data_mgmt.h"
src/gromacs/nbnxm/kerneldispatch.cpp:#include "nbnxm_gpu.h"
src/gromacs/nbnxm/kerneldispatch.cpp:    const bool usingGpuKernels = nbv.useGpu();
src/gromacs/nbnxm/kerneldispatch.cpp:    else if ((!usingGpuKernels && nbv.kernelSetup().ewaldExclusionType == EwaldExclusionType::Analytical)
src/gromacs/nbnxm/kerneldispatch.cpp:             || (usingGpuKernels && gpu_is_kernel_ewald_analytical(nbv.gpuNbv())))
src/gromacs/nbnxm/kerneldispatch.cpp:        case NbnxmKernelType::Gpu8x8x8: gpu_launch_kernel(gpuNbv_, stepWork, iLocality); break;
src/gromacs/nbnxm/kerneldispatch.cpp:            nbnxn_kernel_gpu_ref(pairlistSet.gpuList(),
src/gromacs/nbnxm/pairlistset.h: * holds a list of CPU- or GPU-type pairlist objects, one for each thread,
src/gromacs/nbnxm/pairlistset.h:                            int                      minimumIlistCountForGpuBalancing,
src/gromacs/nbnxm/pairlistset.h:    //! Returns a pointer to the GPU pairlist, nullptr when not present
src/gromacs/nbnxm/pairlistset.h:    const NbnxnPairlistGpu* gpuList() const
src/gromacs/nbnxm/pairlistset.h:        if (!gpuLists_.empty())
src/gromacs/nbnxm/pairlistset.h:            return gpuLists_.data();
src/gromacs/nbnxm/pairlistset.h:    //! List of pairlists in GPU layout
src/gromacs/nbnxm/pairlistset.h:    std::vector<NbnxnPairlistGpu> gpuLists_;
src/gromacs/nbnxm/pairlistset.h:    //! Tells whether the lists is of CPU type, otherwise GPU type
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_body_f_prune.cpp:template void launchNbnxmKernelHelper<8, true, false>(NbnxmGpu*                 nb,
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_body_f_prune.cpp:template void launchNbnxmKernelHelper<32, true, false>(NbnxmGpu*                 nb,
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_body_f_prune.cpp:template void launchNbnxmKernelHelper<64, true, false>(NbnxmGpu*                 nb,
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_utils.h:#include "gromacs/gpu_utils/sycl_kernel_utils.h"
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_utils.h:#include "gromacs/nbnxm/gpu_types_common.h"
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_utils.h:// i-cluster interaction mask for a super-cluster with all c_nbnxnGpuNumClusterPerSupercluster=8 bits set.
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_utils.h:    return ((1U << sc_gpuClusterPerSuperCluster(layoutType)) - 1U);
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_utils.h: *  Note that some ROCm versions' compilers can figure out that the nbnxm
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_utils.h:#if defined(__SYCL_DEVICE_ONLY__) && defined(__AMDGCN__) && GMX_GPU_NB_DISABLE_CLUSTER_PAIR_SPLIT
src/gromacs/nbnxm/sycl/nbnxm_sycl.cpp:#include "gromacs/nbnxm/gpu_common.h"
src/gromacs/nbnxm/sycl/nbnxm_sycl.cpp:static void launchSciSortOnGpu(GpuPairlist* plist, const int maxWorkGroupSize, const DeviceStream& deviceStream);
src/gromacs/nbnxm/sycl/nbnxm_sycl.cpp:void gpu_launch_kernel_pruneonly(NbnxmGpu* nb, const InteractionLocality iloc, const int numParts)
src/gromacs/nbnxm/sycl/nbnxm_sycl.cpp:    if (plist->haveFreshList && nbnxmSortListsOnGpu())
src/gromacs/nbnxm/sycl/nbnxm_sycl.cpp:        launchSciSortOnGpu(plist, deviceInfo.maxWorkGroupSize, *nb->deviceStreams[iloc]);
src/gromacs/nbnxm/sycl/nbnxm_sycl.cpp:void gpu_launch_kernel(NbnxmGpu* nb, const gmx::StepWorkload& stepWork, const InteractionLocality iloc)
src/gromacs/nbnxm/sycl/nbnxm_sycl.cpp:    const NBParamGpu* nbp   = nb->nbparam;
src/gromacs/nbnxm/sycl/nbnxm_sycl.cpp:        gpu_launch_kernel_pruneonly(nb, iloc, 1);
src/gromacs/nbnxm/sycl/nbnxm_sycl.cpp:    if (doPrune && nbnxmSortListsOnGpu())
src/gromacs/nbnxm/sycl/nbnxm_sycl.cpp:        launchSciSortOnGpu(plist, deviceInfo.maxWorkGroupSize, *nb->deviceStreams[iloc]);
src/gromacs/nbnxm/sycl/nbnxm_sycl.cpp: * NVIDIA's CUB uses fancier approach ("Single-pass Parallel Prefix Scan with Decoupled Look-back"),
src/gromacs/nbnxm/sycl/nbnxm_sycl.cpp:static void launchPrefixSumKernel(sycl::queue& q, GpuPairlistSorting* sorting)
src/gromacs/nbnxm/sycl/nbnxm_sycl.cpp:static void launchBucketSortKernel(sycl::queue& q, GpuPairlist* plist)
src/gromacs/nbnxm/sycl/nbnxm_sycl.cpp:static void launchSciSortOnGpu(GpuPairlist* plist, const int maxWorkGroupSize, const DeviceStream& deviceStream)
src/gromacs/nbnxm/sycl/nbnxm_sycl.cpp:     * MI250X ~1.7 times faster. But some Intel iGPUs only handle 256. */
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_body_fv_noprune.cpp:template void launchNbnxmKernelHelper<8, false, true>(NbnxmGpu*                 nb,
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_body_fv_noprune.cpp:template void launchNbnxmKernelHelper<32, false, true>(NbnxmGpu*                 nb,
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_body_fv_noprune.cpp:template void launchNbnxmKernelHelper<64, false, true>(NbnxmGpu*                 nb,
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel.cpp:             * The optimal one depends on the hardware, but we cannot choose c_nbnxnGpuClusterSize
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel.cpp:            case DeviceVendor::Intel: return sc_gpuParallelExecutionWidth(layoutType);
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel.cpp:                GMX_RELEASE_ASSERT(false, "Flexible sub-groups only supported for Intel GPUs");
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel.cpp:void launchNbnxmKernelHelper(NbnxmGpu* nb, const gmx::StepWorkload& stepWork, const InteractionLocality iloc);
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel.cpp:extern template void launchNbnxmKernelHelper<8, false, false>(NbnxmGpu* nb,const gmx::StepWorkload&  stepWork, const InteractionLocality iloc);
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel.cpp:extern template void launchNbnxmKernelHelper<8, false, true>(NbnxmGpu* nb, const gmx::StepWorkload&  stepWork, const InteractionLocality iloc);
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel.cpp:extern template void launchNbnxmKernelHelper<8, true, false>(NbnxmGpu* nb, const gmx::StepWorkload&  stepWork, const InteractionLocality iloc);
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel.cpp:extern template void launchNbnxmKernelHelper<8, true, true>(NbnxmGpu* nb, const gmx::StepWorkload&  stepWork, const InteractionLocality iloc);
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel.cpp:extern template void launchNbnxmKernelHelper<32, false, false>(NbnxmGpu* nb, const gmx::StepWorkload&  stepWork, const InteractionLocality iloc);
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel.cpp:extern template void launchNbnxmKernelHelper<32, false, true>(NbnxmGpu* nb, const gmx::StepWorkload&  stepWork, const InteractionLocality iloc);
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel.cpp:extern template void launchNbnxmKernelHelper<32, true, false>(NbnxmGpu* nb, const gmx::StepWorkload&  stepWork, const InteractionLocality iloc);
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel.cpp:extern template void launchNbnxmKernelHelper<32, true, true>(NbnxmGpu* nb, const gmx::StepWorkload&  stepWork, const InteractionLocality iloc);
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel.cpp:extern template void launchNbnxmKernelHelper<64, false, false>(NbnxmGpu* nb, const gmx::StepWorkload&  stepWork, const InteractionLocality iloc);
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel.cpp:extern template void launchNbnxmKernelHelper<64, false, true>(NbnxmGpu* nb, const gmx::StepWorkload&  stepWork, const InteractionLocality iloc);
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel.cpp:extern template void launchNbnxmKernelHelper<64, true, false>(NbnxmGpu* nb, const gmx::StepWorkload&  stepWork, const InteractionLocality iloc);
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel.cpp:extern template void launchNbnxmKernelHelper<64, true, true>(NbnxmGpu* nb, const gmx::StepWorkload&  stepWork, const InteractionLocality iloc);
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel.cpp:void launchNbnxmKernel(NbnxmGpu* nb, const gmx::StepWorkload& stepWork, const InteractionLocality iloc, bool doPrune)
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel.cpp:void launchNbnxmKernel(NbnxmGpu* nb, const gmx::StepWorkload& stepWork, const InteractionLocality iloc, bool doPrune)
src/gromacs/nbnxm/sycl/nbnxm_gpu_buffer_ops_internal_sycl.cpp:#include "gromacs/gpu_utils/devicebuffer.h"
src/gromacs/nbnxm/sycl/nbnxm_gpu_buffer_ops_internal_sycl.cpp:#include "gromacs/gpu_utils/gmxsycl.h"
src/gromacs/nbnxm/sycl/nbnxm_gpu_buffer_ops_internal_sycl.cpp:#include "gromacs/nbnxm/nbnxm_gpu_buffer_ops_internal.h"
src/gromacs/nbnxm/sycl/nbnxm_gpu_buffer_ops_internal_sycl.cpp:                                     NbnxmGpu*            nb,
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_body.h:#include "gromacs/gpu_utils/device_utils_hip_sycl.h"
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_body.h:#include "gromacs/gpu_utils/devicebuffer.h"
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_body.h:#include "gromacs/gpu_utils/gmxsycl.h"
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_body.h: * Intel GPUs without native floating-point operations emulate them via CAS-loop,
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_body.h:    return (sc_gpuClusterSize(layoutType) == 4);
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_body.h: * Note: This causes massive amount of spills with the tabulated kernel on gfx803 using ROCm 5.3.
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_body.h:    constexpr int c_clSize = sc_gpuClusterSize(sc_layoutType);
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_body.h:    constexpr int c_clSize = sc_gpuClusterSize(sc_layoutType);
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_body.h:    constexpr int        c_clSize         = sc_gpuClusterSize(sc_layoutType);
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_body.h:    constexpr int        c_clSize           = sc_gpuClusterSize(sc_layoutType);
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_body.h:    constexpr int        c_superClusterSize = sc_gpuClusterPerSuperCluster(sc_layoutType);
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_body.h:                /* Intel Xe (Gen12LP) and earlier GPUs implement floating-point atomics via
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_body.h:                 * Such optimization might be slightly beneficial for NVIDIA and AMD as well,
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_body.h: * One step (e.g, Intel iGPU, c_clSize == 4, subGroupSize == 8): handled in a separate
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_body.h: * Two steps (e.g., NVIDIA, c_clSize == 8, subGroupSize == 32): after two shuffle reduction steps,
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_body.h:    constexpr int         c_superClusterSize = sc_gpuClusterPerSuperCluster(sc_layoutType);
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_body.h:    constexpr int         c_clSize           = sc_gpuClusterSize(sc_layoutType);
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_body.h:/*! \brief \c reduceForceIAndFShiftShuffles specialization for single-step reduction (e.g., Intel iGPUs).
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_body.h:    constexpr int         c_superClusterSize = sc_gpuClusterPerSuperCluster(sc_layoutType);
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_body.h:    constexpr int         c_clSize           = sc_gpuClusterSize(sc_layoutType);
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_body.h: * Reduce c_nbnxnGpuNumClusterPerSupercluster i-force components stored in \p fCiBuf[]
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_body.h:    static_assert(gmx::isPowerOfTwo(sc_gpuClusterPerSuperCluster(sc_layoutType)));
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_body.h:    constexpr int c_clSize = sc_gpuClusterSize(sc_layoutType);
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_body.h:    constexpr int          c_clSize               = sc_gpuClusterSize(sc_layoutType);
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_body.h:    constexpr int          c_splitClSize          = sc_gpuSplitJClusterSize(sc_layoutType);
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_body.h:    constexpr int          c_superClusterSize     = sc_gpuClusterPerSuperCluster(sc_layoutType);
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_body.h:    constexpr int          c_nbnxnGpuJgroupSize   = sc_gpuJgroupSize(sc_layoutType);
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_body.h:     * But using nullptr as local buffers here triggers a bug with DPC++/OpenCL
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_body.h:        if constexpr (doPruneNBL && nbnxmSortListsOnGpu())
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_body.h:        if constexpr (doPruneNBL && nbnxmSortListsOnGpu())
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_body.h:            } // (nbSci.shift == gmx::c_centralShiftIndex && a_plistCJPacked[cijPackedBegin].cj[0] == sci * c_nbnxnGpuNumClusterPerSupercluster)
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_body.h:            // Unrolling has been verified to improve performance on AMD and Nvidia
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_body.h:                    c_nbnxnGpuJgroupSize; // Unrolling has been verified to improve performance on AMD
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_body.h:#elif defined(__SYCL_CUDA_ARCH__) && __SYCL_CUDA_ARCH__ >= 800
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_body.h:            // Unrolling parameters follow CUDA implementation for Ampere and later.
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_body.h:                                && (props.vdwCombLB || __SYCL_CUDA_ARCH__ == 800)))
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_body.h:            for (int jm = 0; jm < c_nbnxnGpuJgroupSize; jm++)
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_body.h:#if defined(__SYCL_CUDA_ARCH__)
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_body.h:                        /* The use of * was benchmarked in 2024 for DPC++ 2024.1 CUDA and
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_body.h:                         * found to be faster than the use of &&, just like for CUDA.
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_body.h:                } // for (int i = 0; i < c_nbnxnGpuNumClusterPerSupercluster; i++)
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_body.h:            } // for (int jm = 0; jm < c_nbnxnGpuJgroupSize; jm++)
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_body.h:                if constexpr (nbnxmSortListsOnGpu())
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_body.h:        if constexpr (doPruneNBL && nbnxmSortListsOnGpu())
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_body.h:    constexpr int           c_clSize  = sc_gpuClusterSize(sc_layoutType);
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_body.h:void launchNbnxmKernelHelper(NbnxmGpu* nb, const gmx::StepWorkload& stepWork, const InteractionLocality iloc)
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_body.h:    NBAtomDataGpu*      adat         = nb->atdat;
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_body.h:    NBParamGpu*         nbp          = nb->nbparam;
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_body.h:            (doPruneNBL || !nbnxmSortListsOnGpu()) ? plist->sci.get_pointer()
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel.h:struct NbnxmGpu;
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel.h:#define SYCL_NBNXM_SUPPORTS_SUBGROUP_SIZE_8 (GMX_GPU_NB_CLUSTER_SIZE == 4)
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel.h:#define SYCL_NBNXM_SUPPORTS_SUBGROUP_SIZE_32 (GMX_GPU_NB_CLUSTER_SIZE == 8)
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel.h:    (GMX_GPU_NB_CLUSTER_SIZE == 8 && !(GMX_SYCL_ACPP && !GMX_ACPP_HAVE_HIP_TARGET))
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel.h:void launchNbnxmKernel(NbnxmGpu* nb, const StepWorkload& stepWork, InteractionLocality iloc, bool doPrune);
src/gromacs/nbnxm/sycl/CMakeLists.txt:if (GMX_GPU_SYCL)
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_body_fv_prune.cpp:template void launchNbnxmKernelHelper<8, true, true>(NbnxmGpu*                 nb,
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_body_fv_prune.cpp:template void launchNbnxmKernelHelper<32, true, true>(NbnxmGpu*                 nb,
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_body_fv_prune.cpp:template void launchNbnxmKernelHelper<64, true, true>(NbnxmGpu*                 nb,
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_body_f_noprune.cpp:template void launchNbnxmKernelHelper<8, false, false>(NbnxmGpu*                 nb,
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_body_f_noprune.cpp:template void launchNbnxmKernelHelper<32, false, false>(NbnxmGpu*                 nb,
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_body_f_noprune.cpp:template void launchNbnxmKernelHelper<64, false, false>(NbnxmGpu*                 nb,
src/gromacs/nbnxm/sycl/nbnxm_sycl_data_mgmt.cpp:#include "gromacs/nbnxm/gpu_data_mgmt.h"
src/gromacs/nbnxm/sycl/nbnxm_sycl_data_mgmt.cpp:#include "gromacs/nbnxm/nbnxm_gpu.h"
src/gromacs/nbnxm/sycl/nbnxm_sycl_data_mgmt.cpp:#include "gromacs/nbnxm/nbnxm_gpu_data_mgmt.h"
src/gromacs/nbnxm/sycl/nbnxm_sycl_data_mgmt.cpp:void gpu_init_platform_specific(NbnxmGpu* /* nb */)
src/gromacs/nbnxm/sycl/nbnxm_sycl_data_mgmt.cpp:void gpu_free_platform_specific(NbnxmGpu* /* nb */)
src/gromacs/nbnxm/sycl/nbnxm_sycl_data_mgmt.cpp:int gpu_min_ci_balanced(NbnxmGpu* nb)
src/gromacs/nbnxm/sycl/nbnxm_sycl_data_mgmt.cpp:    // SYCL-TODO: Logic and magic values taken from OpenCL
src/gromacs/nbnxm/sycl/nbnxm_sycl_types.h:#include "gromacs/gpu_utils/devicebuffer.h"
src/gromacs/nbnxm/sycl/nbnxm_sycl_types.h:#include "gromacs/gpu_utils/devicebuffer_sycl.h"
src/gromacs/nbnxm/sycl/nbnxm_sycl_types.h:#include "gromacs/gpu_utils/gmxsycl.h"
src/gromacs/nbnxm/sycl/nbnxm_sycl_types.h:#include "gromacs/gpu_utils/gpueventsynchronizer.h"
src/gromacs/nbnxm/sycl/nbnxm_sycl_types.h:#include "gromacs/gpu_utils/gputraits.h"
src/gromacs/nbnxm/sycl/nbnxm_sycl_types.h:#include "gromacs/nbnxm/gpu_types_common.h"
src/gromacs/nbnxm/sycl/nbnxm_sycl_types.h:#include "gromacs/timing/gpu_timing.h"
src/gromacs/nbnxm/sycl/nbnxm_sycl_types.h:class GpuEventSynchronizer;
src/gromacs/nbnxm/sycl/nbnxm_sycl_types.h:struct NbnxmGpu
src/gromacs/nbnxm/sycl/nbnxm_sycl_types.h:    /*! \brief GPU device context.
src/gromacs/nbnxm/sycl/nbnxm_sycl_types.h:     * \todo Make it constant reference, once NbnxmGpu is a proper class.
src/gromacs/nbnxm/sycl/nbnxm_sycl_types.h:    /*! \brief true if doing both local/non-local NB work on GPU */
src/gromacs/nbnxm/sycl/nbnxm_sycl_types.h:    NBAtomDataGpu* atdat = nullptr;
src/gromacs/nbnxm/sycl/nbnxm_sycl_types.h:    // Data for GPU-side coordinate conversion between integrator and NBNXM
src/gromacs/nbnxm/sycl/nbnxm_sycl_types.h:    NBParamGpu* nbparam = nullptr;
src/gromacs/nbnxm/sycl/nbnxm_sycl_types.h:    EnumerationArray<InteractionLocality, std::unique_ptr<GpuPairlist>> plist = { { nullptr } };
src/gromacs/nbnxm/sycl/nbnxm_sycl_types.h:    /*! \brief local and non-local GPU streams */
src/gromacs/nbnxm/sycl/nbnxm_sycl_types.h:    GpuTimers* timers = nullptr;
src/gromacs/nbnxm/sycl/nbnxm_sycl_types.h:    gmx_wallclock_gpu_nbnxn_t* timings = nullptr;
src/gromacs/nbnxm/sycl/nbnxm_sycl_types.h:    GpuEventSynchronizer nonlocal_done;
src/gromacs/nbnxm/sycl/nbnxm_sycl_types.h:    GpuEventSynchronizer misc_ops_and_local_H2D_done;
src/gromacs/nbnxm/sycl/nbnxm_sycl_types.h:     * This includes local/nonlocal GPU work, either bonded or
src/gromacs/nbnxm/sycl/nbnxm_sycl_types.h:     * local/nonlocal, if there is bonded GPU work, both flags
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_pruneonly.h:struct NbnxmGpu;
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_pruneonly.h:void launchNbnxmKernelPruneOnly(NbnxmGpu*                 nb,
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_pruneonly.cpp:#include "gromacs/gpu_utils/devicebuffer.h"
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_pruneonly.cpp:#include "gromacs/gpu_utils/gmxsycl.h"
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_pruneonly.cpp:    constexpr int          c_clSize               = sc_gpuClusterSize(layoutType);
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_pruneonly.cpp:    constexpr int          c_superClusterSize     = sc_gpuClusterPerSuperCluster(layoutType);
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_pruneonly.cpp:    constexpr int          c_clusterPairSplit     = sc_gpuClusterPairSplit(layoutType);
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_pruneonly.cpp:        if constexpr (haveFreshList && nbnxmSortListsOnGpu())
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_pruneonly.cpp:    constexpr int warpSize = sc_gpuParallelExecutionWidth(layoutType);
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_pruneonly.cpp:    /* Somewhat weird behavior inherited from OpenCL.
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_pruneonly.cpp:     * With clSize == 4, we use sub_group size of 16 (not enforced in OpenCL implementation, but chosen
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_pruneonly.cpp:        if constexpr (haveFreshList && nbnxmSortListsOnGpu())
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_pruneonly.cpp:                for (int jm = 0; jm < sc_gpuJgroupSize(layoutType); jm++)
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_pruneonly.cpp:                        } // (int i = 0; i < c_nbnxnGpuNumClusterPerSupercluster; i++)
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_pruneonly.cpp:                    } // (imaskCheck & (superClInteractionMask << (jm * c_nbnxnGpuNumClusterPerSupercluster)))
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_pruneonly.cpp:                } // for (int jm = 0; jm < c_nbnxnGpuJgroupSize; jm++)
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_pruneonly.cpp:                    gm_plistIMask[jPacked * sc_gpuClusterPairSplit(layoutType) + widx] = imaskFull;
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_pruneonly.cpp:                    if constexpr (nbnxmSortListsOnGpu())
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_pruneonly.cpp:        if constexpr (haveFreshList && nbnxmSortListsOnGpu())
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_pruneonly.cpp:    constexpr int c_clSize = sc_gpuClusterSize(layoutType);
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_pruneonly.cpp:void launchNbnxmKernelPruneOnly(NbnxmGpu* nb, const InteractionLocality iloc, const int numParts, const int numSciInPartMax)
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_pruneonly.cpp:    NBAtomDataGpu*      adat          = nb->atdat;
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_pruneonly.cpp:    NBParamGpu*         nbp           = nb->nbparam;
src/gromacs/nbnxm/sycl/nbnxm_sycl_kernel_pruneonly.cpp:            (haveFreshList || !nbnxmSortListsOnGpu()) ? plist->sci.get_pointer()
src/gromacs/nbnxm/pairlist.h:#include "gromacs/gpu_utils/hostallocator.h"
src/gromacs/nbnxm/pairlist.h://! Currently hard coded default GPU pairlist layout
src/gromacs/nbnxm/pairlist.h:struct NbnxmPairlistGpuWork;
src/gromacs/nbnxm/pairlist.h://! Whether we want to use GPU for neighbour list sorting
src/gromacs/nbnxm/pairlist.h:constexpr bool nbnxmSortListsOnGpu()
src/gromacs/nbnxm/pairlist.h:    return (GMX_GPU_CUDA || GMX_GPU_SYCL || GMX_GPU_HIP);
src/gromacs/nbnxm/pairlist.h:    int cj[sc_gpuJgroupSize(sc_layoutType)];
src/gromacs/nbnxm/pairlist.h:    nbnxn_im_ei_t imei[sc_gpuClusterPairSplit(sc_layoutType)];
src/gromacs/nbnxm/pairlist.h:        return list_[index / sc_gpuJgroupSize(sc_layoutType)].cj[index & (sc_gpuJgroupSize(sc_layoutType) - 1)];
src/gromacs/nbnxm/pairlist.h:        return list_[index / sc_gpuJgroupSize(sc_layoutType)].imei[0].imask;
src/gromacs/nbnxm/pairlist.h://! Struct for storing the atom-pair interaction bits for a cluster pair in a GPU pairlist
src/gromacs/nbnxm/pairlist.h:    unsigned int pair[sc_gpuExclSize(sc_layoutType)];
src/gromacs/nbnxm/pairlist.h:/* Cluster pairlist type, with extra hierarchies, for on the GPU
src/gromacs/nbnxm/pairlist.h:struct NbnxnPairlistGpu
src/gromacs/nbnxm/pairlist.h:     * \param[in] pinningPolicy  Sets the pinning policy for all buffers used on the GPU
src/gromacs/nbnxm/pairlist.h:    NbnxnPairlistGpu(PinningPolicy pinningPolicy);
src/gromacs/nbnxm/pairlist.h:    std::unique_ptr<NbnxmPairlistGpuWork> work;
src/gromacs/nbnxm/pairlistparams.h:    { 4, 4, 4, sc_gpuClusterSize(PairlistType::Hierarchical8x8x8), 1 }
src/gromacs/nbnxm/pairlistparams.h:    { 2, 4, 8, sc_gpuClusterSize(PairlistType::Hierarchical8x8x8), 1 }
src/gromacs/nbnxm/pairlistparams.h://! True if given pairlist type is used on GPU, false if on CPU.
src/gromacs/nbnxm/pairlistparams.h:static constexpr gmx::EnumerationArray<PairlistType, bool> sc_isGpuPairListType = {
src/gromacs/nbnxm/pairlistparams.h:                   std::optional<PairlistType> gpuPairlistType,
src/gromacs/nbnxm/nbnxm_gpu.h: *  \brief Declare interface for GPU execution for NBNXN module
src/gromacs/nbnxm/nbnxm_gpu.h:#ifndef GMX_NBNXM_NBNXM_GPU_H
src/gromacs/nbnxm/nbnxm_gpu.h:#define GMX_NBNXM_NBNXM_GPU_H
src/gromacs/nbnxm/nbnxm_gpu.h:#include "gromacs/gpu_utils/gpu_macros.h"
src/gromacs/nbnxm/nbnxm_gpu.h:enum class GpuTaskCompletion;
src/gromacs/nbnxm/nbnxm_gpu.h:class ListedForcesGpu;
src/gromacs/nbnxm/nbnxm_gpu.h: * neither non-local nonbonded interactions nor bonded GPU work.
src/gromacs/nbnxm/nbnxm_gpu.h: * \param [in]    nb        GPU nonbonded data.
src/gromacs/nbnxm/nbnxm_gpu.h:GPU_FUNC_QUALIFIER
src/gromacs/nbnxm/nbnxm_gpu.h:void gpu_copy_xq_to_gpu(NbnxmGpu gmx_unused*                      nb,
src/gromacs/nbnxm/nbnxm_gpu.h:                        AtomLocality gmx_unused                   aloc) GPU_FUNC_TERM;
src/gromacs/nbnxm/nbnxm_gpu.h:GPU_FUNC_QUALIFIER
src/gromacs/nbnxm/nbnxm_gpu.h:void gpu_launch_kernel(NbnxmGpu gmx_unused*           nb,
src/gromacs/nbnxm/nbnxm_gpu.h:                       InteractionLocality gmx_unused iloc) GPU_FUNC_TERM;
src/gromacs/nbnxm/nbnxm_gpu.h: *  cudaStreamWaitEvent calls, 3-5 us/call) which we might be able to live with.
src/gromacs/nbnxm/nbnxm_gpu.h: *  be use-cases where more overlap may help (e.g. multiple ranks per GPU,
src/gromacs/nbnxm/nbnxm_gpu.h: *  share a GPU. Ranks that complete their nonbondeds sooner can schedule pruning earlier
src/gromacs/nbnxm/nbnxm_gpu.h: * \param [inout] nb        GPU nonbonded data.
src/gromacs/nbnxm/nbnxm_gpu.h:GPU_FUNC_QUALIFIER
src/gromacs/nbnxm/nbnxm_gpu.h:void gpu_launch_kernel_pruneonly(NbnxmGpu gmx_unused*           nb,
src/gromacs/nbnxm/nbnxm_gpu.h:                                 int gmx_unused                 numParts) GPU_FUNC_TERM;
src/gromacs/nbnxm/nbnxm_gpu.h: * Launch asynchronously the download of short-range forces from the GPU
src/gromacs/nbnxm/nbnxm_gpu.h:GPU_FUNC_QUALIFIER
src/gromacs/nbnxm/nbnxm_gpu.h:void gpu_launch_cpyback(NbnxmGpu gmx_unused*           nb,
src/gromacs/nbnxm/nbnxm_gpu.h:                        AtomLocality gmx_unused        aloc) GPU_FUNC_TERM;
src/gromacs/nbnxm/nbnxm_gpu.h:/*! \brief Attempts to complete nonbonded GPU task.
src/gromacs/nbnxm/nbnxm_gpu.h: *  This function attempts to complete the nonbonded task (both GPU and CPU auxiliary work).
src/gromacs/nbnxm/nbnxm_gpu.h: *  (achieved by passing GpuTaskCompletion::Check) or blocking wait until the results
src/gromacs/nbnxm/nbnxm_gpu.h: *  are ready (when GpuTaskCompletion::Wait is passed).
src/gromacs/nbnxm/nbnxm_gpu.h: *  As the "Check" mode the function will return immediately if the GPU stream
src/gromacs/nbnxm/nbnxm_gpu.h: *  of work on the CPU with GPU execution.
src/gromacs/nbnxm/nbnxm_gpu.h: *  - All nonbonded GPU tasks: both compute and device transfer(s)
src/gromacs/nbnxm/nbnxm_gpu.h: * In GpuTaskCompletion::Check mode this function does the timing and keeps correct count
src/gromacs/nbnxm/nbnxm_gpu.h: * for the nonbonded task (incrementing only once per task), in the GpuTaskCompletion::Wait mode
src/gromacs/nbnxm/nbnxm_gpu.h: *  force buffer (instead of that being passed only to nbnxn_gpu_launch_cpyback()) and by returning
src/gromacs/nbnxm/nbnxm_gpu.h: * \param[in]  nb             The nonbonded data GPU structure
src/gromacs/nbnxm/nbnxm_gpu.h:GPU_FUNC_QUALIFIER
src/gromacs/nbnxm/nbnxm_gpu.h:bool gpu_try_finish_task(NbnxmGpu gmx_unused*           nb,
src/gromacs/nbnxm/nbnxm_gpu.h:                         GpuTaskCompletion gmx_unused completionKind) GPU_FUNC_TERM_WITH_RETURN(false);
src/gromacs/nbnxm/nbnxm_gpu.h:/*! \brief  Completes the nonbonded GPU task blocking until GPU tasks and data
src/gromacs/nbnxm/nbnxm_gpu.h: * \param[in] nb The nonbonded data GPU structure
src/gromacs/nbnxm/nbnxm_gpu.h:GPU_FUNC_QUALIFIER
src/gromacs/nbnxm/nbnxm_gpu.h:float gpu_wait_finish_task(NbnxmGpu gmx_unused*           nb,
src/gromacs/nbnxm/nbnxm_gpu.h:                           gmx_wallcycle gmx_unused*      wcycle) GPU_FUNC_TERM_WITH_RETURN(0.0);
src/gromacs/nbnxm/nbnxm_gpu.h:/*! \brief Initialization for X buffer operations on GPU.
src/gromacs/nbnxm/nbnxm_gpu.h:GPU_FUNC_QUALIFIER
src/gromacs/nbnxm/nbnxm_gpu.h:void nbnxn_gpu_init_x_to_nbat_x(const GridSet gmx_unused& gridSet, NbnxmGpu gmx_unused* gpu_nbv) GPU_FUNC_TERM;
src/gromacs/nbnxm/nbnxm_gpu.h:/*! \brief X buffer operations on GPU: performs conversion from rvec to nb format.
src/gromacs/nbnxm/nbnxm_gpu.h: * \param[in,out] gpu_nbv          The nonbonded data GPU structure.
src/gromacs/nbnxm/nbnxm_gpu.h:GPU_FUNC_QUALIFIER
src/gromacs/nbnxm/nbnxm_gpu.h:void nbnxn_gpu_x_to_nbat_x(const Grid gmx_unused&           grid,
src/gromacs/nbnxm/nbnxm_gpu.h:                           NbnxmGpu gmx_unused*             gpu_nbv,
src/gromacs/nbnxm/nbnxm_gpu.h:                           GpuEventSynchronizer gmx_unused* xReadyOnDevice,
src/gromacs/nbnxm/nbnxm_gpu.h:                           bool gmx_unused mustInsertNonLocalDependency) GPU_FUNC_TERM;
src/gromacs/nbnxm/nbnxm_gpu.h: *  argument and inserts in the GPU stream a wait on the event on the nonlocal.
src/gromacs/nbnxm/nbnxm_gpu.h: * \param[in] nb                   The nonbonded data GPU structure
src/gromacs/nbnxm/nbnxm_gpu.h:GPU_FUNC_QUALIFIER
src/gromacs/nbnxm/nbnxm_gpu.h:void nbnxnInsertNonlocalGpuDependency(NbnxmGpu gmx_unused* nb,
src/gromacs/nbnxm/nbnxm_gpu.h:                                      InteractionLocality gmx_unused interactionLocality) GPU_FUNC_TERM;
src/gromacs/nbnxm/nbnxm_gpu.h: * As nonbondeds and bondeds share input/output buffers and GPU queues,
src/gromacs/nbnxm/nbnxm_gpu.h: * \param[inout]  nb               Pointer to the nonbonded GPU data structure
src/gromacs/nbnxm/nbnxm_gpu.h: * \param[in]     listedForcesGpu  Pointer to the GPU bonded data structure
src/gromacs/nbnxm/nbnxm_gpu.h:GPU_FUNC_QUALIFIER
src/gromacs/nbnxm/nbnxm_gpu.h:void setupGpuShortRangeWorkLow(NbnxmGpu gmx_unused*              nb,
src/gromacs/nbnxm/nbnxm_gpu.h:                               const ListedForcesGpu gmx_unused* listedForcesGpu,
src/gromacs/nbnxm/nbnxm_gpu.h:                               InteractionLocality gmx_unused    iLocality) GPU_FUNC_TERM;
src/gromacs/nbnxm/nbnxm_gpu.h:/*! \brief Returns true if there is GPU short-range work for the given interaction locality.
src/gromacs/nbnxm/nbnxm_gpu.h: * and therefore if there are GPU offloaded bonded interactions, this function will return
src/gromacs/nbnxm/nbnxm_gpu.h: * \param[inout]  nb                   Pointer to the nonbonded GPU data structure
src/gromacs/nbnxm/nbnxm_gpu.h:GPU_FUNC_QUALIFIER
src/gromacs/nbnxm/nbnxm_gpu.h:bool haveGpuShortRangeWork(const NbnxmGpu gmx_unused* nb, InteractionLocality gmx_unused interactionLocality)
src/gromacs/nbnxm/nbnxm_gpu.h:        GPU_FUNC_TERM_WITH_RETURN(false);
src/gromacs/nbnxm/gpu_data_mgmt.h: *  \brief Declare interface for GPU data transfer for NBNXN module
src/gromacs/nbnxm/gpu_data_mgmt.h:#ifndef GMX_NBNXN_GPU_DATA_MGMT_H
src/gromacs/nbnxm/gpu_data_mgmt.h:#define GMX_NBNXN_GPU_DATA_MGMT_H
src/gromacs/nbnxm/gpu_data_mgmt.h:#include "gromacs/gpu_utils/gpu_macros.h"
src/gromacs/nbnxm/gpu_data_mgmt.h:struct gmx_wallclock_gpu_nbnxn_t;
src/gromacs/nbnxm/gpu_data_mgmt.h:struct NbnxmGpu;
src/gromacs/nbnxm/gpu_data_mgmt.h:struct NBAtomDataGpu;
src/gromacs/nbnxm/gpu_data_mgmt.h:struct NbnxnPairlistGpu;
src/gromacs/nbnxm/gpu_data_mgmt.h:class GpuPairlist;
src/gromacs/nbnxm/gpu_data_mgmt.h:/** Initializes the data structures related to GPU nonbonded calculations. */
src/gromacs/nbnxm/gpu_data_mgmt.h:GPU_FUNC_QUALIFIER
src/gromacs/nbnxm/gpu_data_mgmt.h:NbnxmGpu* gpu_init(const DeviceStreamManager gmx_unused& deviceStreamManager,
src/gromacs/nbnxm/gpu_data_mgmt.h:                   /* true if both local and non-local are done on GPU */
src/gromacs/nbnxm/gpu_data_mgmt.h:                   bool gmx_unused bLocalAndNonlocal) GPU_FUNC_TERM_WITH_RETURN(nullptr);
src/gromacs/nbnxm/gpu_data_mgmt.h:/** Initializes pair-list data for GPU, called at every pair search step. */
src/gromacs/nbnxm/gpu_data_mgmt.h:GPU_FUNC_QUALIFIER
src/gromacs/nbnxm/gpu_data_mgmt.h:void gpu_init_pairlist(NbnxmGpu gmx_unused*                      nb,
src/gromacs/nbnxm/gpu_data_mgmt.h:                       const struct NbnxnPairlistGpu gmx_unused* h_nblist,
src/gromacs/nbnxm/gpu_data_mgmt.h:                       InteractionLocality gmx_unused            iloc) GPU_FUNC_TERM;
src/gromacs/nbnxm/gpu_data_mgmt.h:/** Initializes atom-data on the GPU, called at every pair search step. */
src/gromacs/nbnxm/gpu_data_mgmt.h:GPU_FUNC_QUALIFIER
src/gromacs/nbnxm/gpu_data_mgmt.h:void gpu_init_atomdata(NbnxmGpu gmx_unused* nb, const nbnxn_atomdata_t gmx_unused* nbat) GPU_FUNC_TERM;
src/gromacs/nbnxm/gpu_data_mgmt.h:/*! \brief Re-generate the GPU Ewald force table, resets rlist, and update the
src/gromacs/nbnxm/gpu_data_mgmt.h:GPU_FUNC_QUALIFIER
src/gromacs/nbnxm/gpu_data_mgmt.h:void gpu_pme_loadbal_update_param(struct nonbonded_verlet_t gmx_unused* nbv,
src/gromacs/nbnxm/gpu_data_mgmt.h:                                  const interaction_const_t gmx_unused& ic) GPU_FUNC_TERM;
src/gromacs/nbnxm/gpu_data_mgmt.h:/** Uploads shift vector to the GPU if the box is dynamic (otherwise just returns). */
src/gromacs/nbnxm/gpu_data_mgmt.h:GPU_FUNC_QUALIFIER
src/gromacs/nbnxm/gpu_data_mgmt.h:void gpu_upload_shiftvec(NbnxmGpu gmx_unused* nb, const nbnxn_atomdata_t gmx_unused* nbatom) GPU_FUNC_TERM;
src/gromacs/nbnxm/gpu_data_mgmt.h:/** Clears GPU outputs: nonbonded force, shift force and energy. */
src/gromacs/nbnxm/gpu_data_mgmt.h:GPU_FUNC_QUALIFIER
src/gromacs/nbnxm/gpu_data_mgmt.h:void gpu_clear_outputs(NbnxmGpu gmx_unused* nb, bool gmx_unused computeVirial) GPU_FUNC_TERM;
src/gromacs/nbnxm/gpu_data_mgmt.h:/** Frees all GPU resources used for the nonbonded calculations. */
src/gromacs/nbnxm/gpu_data_mgmt.h:GPU_FUNC_QUALIFIER
src/gromacs/nbnxm/gpu_data_mgmt.h:void gpu_free(NbnxmGpu gmx_unused* nb) GPU_FUNC_TERM;
src/gromacs/nbnxm/gpu_data_mgmt.h:/** Returns the GPU timings structure or NULL if GPU is not used or timing is off. */
src/gromacs/nbnxm/gpu_data_mgmt.h:GPU_FUNC_QUALIFIER
src/gromacs/nbnxm/gpu_data_mgmt.h:struct gmx_wallclock_gpu_nbnxn_t* gpu_get_timings(NbnxmGpu gmx_unused* nb)
src/gromacs/nbnxm/gpu_data_mgmt.h:        GPU_FUNC_TERM_WITH_RETURN(nullptr);
src/gromacs/nbnxm/gpu_data_mgmt.h:/** Resets nonbonded GPU timings. */
src/gromacs/nbnxm/gpu_data_mgmt.h:GPU_FUNC_QUALIFIER
src/gromacs/nbnxm/gpu_data_mgmt.h:void gpu_reset_timings(struct nonbonded_verlet_t gmx_unused* nbv) GPU_FUNC_TERM;
src/gromacs/nbnxm/gpu_data_mgmt.h: *  with GPU non-bonded kernels. */
src/gromacs/nbnxm/gpu_data_mgmt.h:GPU_FUNC_QUALIFIER
src/gromacs/nbnxm/gpu_data_mgmt.h:int gpu_min_ci_balanced(NbnxmGpu gmx_unused* nb) GPU_FUNC_TERM_WITH_RETURN(-1);
src/gromacs/nbnxm/gpu_data_mgmt.h:/** Returns if analytical Ewald GPU kernels are used. */
src/gromacs/nbnxm/gpu_data_mgmt.h:GPU_FUNC_QUALIFIER
src/gromacs/nbnxm/gpu_data_mgmt.h:bool gpu_is_kernel_ewald_analytical(const NbnxmGpu gmx_unused* nb) GPU_FUNC_TERM_WITH_RETURN(FALSE);
src/gromacs/nbnxm/gpu_data_mgmt.h:/** Returns an opaque pointer to the GPU NBNXM atom data.
src/gromacs/nbnxm/gpu_data_mgmt.h:GPU_FUNC_QUALIFIER
src/gromacs/nbnxm/gpu_data_mgmt.h:NBAtomDataGpu* gpuGetNBAtomData(NbnxmGpu gmx_unused* nb) GPU_FUNC_TERM_WITH_RETURN(nullptr);
src/gromacs/nbnxm/gpu_data_mgmt.h:GPU_FUNC_QUALIFIER
src/gromacs/nbnxm/gpu_data_mgmt.h:DeviceBuffer<RVec> gpu_get_f(NbnxmGpu gmx_unused* nb) GPU_FUNC_TERM_WITH_RETURN(DeviceBuffer<RVec>{});
src/gromacs/nbnxm/gpu_data_mgmt.h:/*! \brief Calculates working memory required for exclusive sum, used in neighbour list sorting on GPU.
src/gromacs/nbnxm/gpu_data_mgmt.h: * This is only used for CUDA/HIP, where the actual size is calculate based on the list.
src/gromacs/nbnxm/gpu_data_mgmt.h:CUDA_HIP_FUNC_QUALIFIER
src/gromacs/nbnxm/gpu_data_mgmt.h:size_t getExclusiveScanWorkingArraySize(GpuPairlist*        CUDA_HIP_FUNC_ARGUMENT(plist),
src/gromacs/nbnxm/gpu_data_mgmt.h:                                        const DeviceStream& CUDA_HIP_FUNC_ARGUMENT(deviceStream))
src/gromacs/nbnxm/gpu_data_mgmt.h:        CUDA_HIP_FUNC_TERM_WITH_RETURN(0);
src/gromacs/nbnxm/gpu_data_mgmt.h:CUDA_HIP_FUNC_QUALIFIER
src/gromacs/nbnxm/gpu_data_mgmt.h:void performExclusiveScan(size_t       CUDA_HIP_FUNC_ARGUMENT(temporaryBufferSize),
src/gromacs/nbnxm/gpu_data_mgmt.h:                          char*        CUDA_HIP_FUNC_ARGUMENT(temporaryBuffer),
src/gromacs/nbnxm/gpu_data_mgmt.h:                          GpuPairlist* CUDA_HIP_FUNC_ARGUMENT(plist),
src/gromacs/nbnxm/gpu_data_mgmt.h:                          const DeviceStream& CUDA_HIP_FUNC_ARGUMENT(deviceStream)) CUDA_HIP_FUNC_TERM;
src/gromacs/nbnxm/grid.h:#include "gromacs/gpu_utils/hostallocator.h"
src/gromacs/nbnxm/grid.h: * With a GPU geometry, each cell contains up to 8 clusters. The geometry is
src/gromacs/nbnxm/grid.h:        //! Is this grid simple (CPU) or hierarchical (GPU)
src/gromacs/nbnxm/grid.h:    //! Spatially sort the atoms within the given column range, for GPU geometry
src/gromacs/nbnxm/grid.h:    void sortColumnsGpuGeometry(GridSetData*            gridSetData,
src/gromacs/nbnxm/nbnxm.h: * implemented for a wide range of CPU and GPU architectures.
src/gromacs/nbnxm/nbnxm.h: * architectures as well as in CUDA and OpenCL for GPU architectures.
src/gromacs/nbnxm/nbnxm.h: * for GPU pair-list setup interaction kernel.
src/gromacs/nbnxm/nbnxm.h: * On a GPU, this dynamic pruning is performed in a rolling fashion, pruning
src/gromacs/nbnxm/nbnxm.h: * search, CPU kernels, GPU glue code + kernels.
src/gromacs/nbnxm/nbnxm.h:#include "gromacs/gpu_utils/devicebuffer_datatype.h"
src/gromacs/nbnxm/nbnxm.h:class GpuEventSynchronizer;
src/gromacs/nbnxm/nbnxm.h:struct NbnxmGpu;
src/gromacs/nbnxm/nbnxm.h:class ListedForcesGpu;
src/gromacs/nbnxm/nbnxm.h:     * \param[in] gpu_nbv       The GPU non-bonded setup, ownership is transferred, can be nullptr
src/gromacs/nbnxm/nbnxm.h:                       NbnxmGpu*                         gpu_nbv,
src/gromacs/nbnxm/nbnxm.h:     * \param[in] gpu_nbv       The GPU non-bonded setup, ownership is transferred, can be nullptr
src/gromacs/nbnxm/nbnxm.h:                       NbnxmGpu*                         gpu_nbv);
src/gromacs/nbnxm/nbnxm.h:    //! Returns whether a GPU is use for the non-bonded calculations
src/gromacs/nbnxm/nbnxm.h:    bool useGpu() const { return isGpuKernelType(kernelSetup_.kernelType); }
src/gromacs/nbnxm/nbnxm.h:    //! Returns whether a GPU is emulated for the non-bonded calculations
src/gromacs/nbnxm/nbnxm.h:    bool emulateGpu() const { return kernelSetup_.kernelType == NbnxmKernelType::Cpu8x8x8_PlainC; }
src/gromacs/nbnxm/nbnxm.h:    bool pairlistIsSimple() const { return !useGpu() && !emulateGpu(); }
src/gromacs/nbnxm/nbnxm.h:    /*!\brief Convert the coordinates to NBNXM format on the GPU for the given locality
src/gromacs/nbnxm/nbnxm.h:     * The API function for the transformation of the coordinates from one layout to another in the GPU memory.
src/gromacs/nbnxm/nbnxm.h:     * \param[in] d_x             GPU coordinates buffer in plain rvec format to be transformed.
src/gromacs/nbnxm/nbnxm.h:    void convertCoordinatesGpu(AtomLocality locality, DeviceBuffer<RVec> d_x, GpuEventSynchronizer* xReadyOnDevice);
src/gromacs/nbnxm/nbnxm.h:    //! Init for GPU version of setup coordinates in Nbnxm
src/gromacs/nbnxm/nbnxm.h:    void atomdata_init_copy_x_to_nbat_x_gpu() const;
src/gromacs/nbnxm/nbnxm.h:    //! Returns whether step is a dynamic list pruning step, for GPU lists
src/gromacs/nbnxm/nbnxm.h:    bool isDynamicPruningStepGpu(int64_t step) const;
src/gromacs/nbnxm/nbnxm.h:    //! Dispatches the dynamic pruning kernel for GPU lists
src/gromacs/nbnxm/nbnxm.h:    void dispatchPruneKernelGpu(int64_t step);
src/gromacs/nbnxm/nbnxm.h:    //! \brief Executes the non-bonded kernel of the GPU or launches it on the GPU
src/gromacs/nbnxm/nbnxm.h:    void setupGpuShortRangeWork(const ListedForcesGpu* listedForcesGpu, InteractionLocality iLocality) const;
src/gromacs/nbnxm/nbnxm.h:    //! Returns a pointer to the NbnxmGpu object, can return nullptr
src/gromacs/nbnxm/nbnxm.h:    const NbnxmGpu* gpuNbv() const { return gpuNbv_; }
src/gromacs/nbnxm/nbnxm.h:    //! Returns a pointer to the NbnxmGpu object, can return nullptr
src/gromacs/nbnxm/nbnxm.h:    NbnxmGpu* gpuNbv() { return gpuNbv_; }
src/gromacs/nbnxm/nbnxm.h:    //! GPU Nbnxm data, only used with a physical GPU (TODO: use unique_ptr)
src/gromacs/nbnxm/nbnxm.h:    NbnxmGpu* gpuNbv_;
src/gromacs/nbnxm/nbnxm.h:                                                   bool                       useGpuForNonbonded,
src/gromacs/nbnxm/nbnxm.h:/*! \brief Check if GROMACS has been built with GPU support.
src/gromacs/nbnxm/nbnxm.h:bool buildSupportsNonbondedOnGpu(std::string* error);
src/gromacs/nbnxm/pairsearch.h:     * \param[in] pinningPolicy            Sets the pinning policy for all buffers used on the GPU
src/gromacs/nbnxm/gpu_common.h: * \brief Common functions for the different NBNXN GPU implementations.
src/gromacs/nbnxm/gpu_common.h:#ifndef GMX_NBNXM_GPU_COMMON_H
src/gromacs/nbnxm/gpu_common.h:#define GMX_NBNXM_GPU_COMMON_H
src/gromacs/nbnxm/gpu_common.h:#if GMX_GPU_CUDA
src/gromacs/nbnxm/gpu_common.h:#    include "cuda/nbnxm_cuda_types.h"
src/gromacs/nbnxm/gpu_common.h:#if GMX_GPU_OPENCL
src/gromacs/nbnxm/gpu_common.h:#    include "opencl/nbnxm_ocl_types.h"
src/gromacs/nbnxm/gpu_common.h:#if GMX_GPU_SYCL
src/gromacs/nbnxm/gpu_common.h:#    include "gromacs/gpu_utils/syclutils.h"
src/gromacs/nbnxm/gpu_common.h:#if GMX_GPU_HIP
src/gromacs/nbnxm/gpu_common.h:#include "gromacs/gpu_utils/gpu_utils.h"
src/gromacs/nbnxm/gpu_common.h:#include "gromacs/timing/gpu_timing.h"
src/gromacs/nbnxm/gpu_common.h:#include "gpu_common_utils.h"
src/gromacs/nbnxm/gpu_common.h:#include "nbnxm_gpu.h"
src/gromacs/nbnxm/gpu_common.h:class ListedForcesGpu;
src/gromacs/nbnxm/gpu_common.h: * Note that the resetting of GpuTimers::didPrune and GpuTimers::didRollingPrune
src/gromacs/nbnxm/gpu_common.h: * \param[in] timers   structs with GPU timer objects
src/gromacs/nbnxm/gpu_common.h: * \param[inout] timings  GPU task timing data
src/gromacs/nbnxm/gpu_common.h:static void countPruneKernelTime(GpuTimers*                 timers,
src/gromacs/nbnxm/gpu_common.h:                                 gmx_wallclock_gpu_nbnxn_t* timings,
src/gromacs/nbnxm/gpu_common.h:    GpuTimers::Interaction& iTimers = timers->interaction[iloc];
src/gromacs/nbnxm/gpu_common.h: * Shift forces and electrostatic/LJ energies copied from the GPU into
src/gromacs/nbnxm/gpu_common.h:static inline void gpu_reduce_staged_outputs(const NBStagingData&      nbst,
src/gromacs/nbnxm/gpu_common.h: * NOTE: if timing with multiple GPUs (streams) becomes possible, the
src/gromacs/nbnxm/gpu_common.h: * \tparam     GpuPairlist       Pair list type
src/gromacs/nbnxm/gpu_common.h: * \param[out] timings           Pointer to the NB GPU timings data
src/gromacs/nbnxm/gpu_common.h: * \param[in]  timers            Pointer to GPU timers data
src/gromacs/nbnxm/gpu_common.h:template<typename GpuPairlist>
src/gromacs/nbnxm/gpu_common.h:static inline void gpu_accumulate_timings(gmx_wallclock_gpu_nbnxn_t* timings,
src/gromacs/nbnxm/gpu_common.h:                                          GpuTimers*                 timers,
src/gromacs/nbnxm/gpu_common.h:                                          const GpuPairlist*         plist,
src/gromacs/nbnxm/gpu_common.h:/*! \brief Attempts to complete nonbonded GPU task.
src/gromacs/nbnxm/gpu_common.h: * See documentation in nbnxm_gpu.h for details.
src/gromacs/nbnxm/gpu_common.h: * cuda_runtime.h if needed for any remaining CUDA-specific
src/gromacs/nbnxm/gpu_common.h:bool gpu_try_finish_task(NbnxmGpu*           nb,
src/gromacs/nbnxm/gpu_common.h:                         GpuTaskCompletion   completionKind)
src/gromacs/nbnxm/gpu_common.h:    GMX_ASSERT(nb, "Need a valid nbnxn_gpu object");
src/gromacs/nbnxm/gpu_common.h:    // (Note that useGpuFBufferOps and computeVirial are mutually exclusive
src/gromacs/nbnxm/gpu_common.h:            (!stepWork.useGpuFBufferOps
src/gromacs/nbnxm/gpu_common.h:    //  This is consistent with nbnxn_gpu_launch_kernel but it also considers possible
src/gromacs/nbnxm/gpu_common.h:    //  bonded GPU work.
src/gromacs/nbnxm/gpu_common.h:    if ((iLocality == InteractionLocality::Local) || haveGpuShortRangeWork(nb, iLocality))
src/gromacs/nbnxm/gpu_common.h:        // Query the state of the GPU stream and return early if we're not done
src/gromacs/nbnxm/gpu_common.h:        if (completionKind == GpuTaskCompletion::Check)
src/gromacs/nbnxm/gpu_common.h:        // with a future OpenCL implementation, but with CUDA timing is anyway disabled
src/gromacs/nbnxm/gpu_common.h:        gpu_accumulate_timings(
src/gromacs/nbnxm/gpu_common.h:            gpu_reduce_staged_outputs(nb->nbst,
src/gromacs/nbnxm/gpu_common.h: * \param[in] nb The nonbonded data GPU structure
src/gromacs/nbnxm/gpu_common.h: * \return            The number of cycles the gpu wait took
src/gromacs/nbnxm/gpu_common.h:float gpu_wait_finish_task(NbnxmGpu*           nb,
src/gromacs/nbnxm/gpu_common.h:                                ? WallCycleCounter::WaitGpuNbL
src/gromacs/nbnxm/gpu_common.h:                                : WallCycleCounter::WaitGpuNbNL;
src/gromacs/nbnxm/gpu_common.h:    gpu_try_finish_task(nb, stepWork, aloc, e_lj, e_el, shiftForces, GpuTaskCompletion::Wait);
src/gromacs/nbnxm/pairlist_tuning.h: * \param[in]     useOrEmulateGpuForNonbondeds  Tells if we are using a GPU for non-bondeds
src/gromacs/nbnxm/pairlist_tuning.h:                     bool              useOrEmulateGpuForNonbondeds,
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.h: *  \brief Declare common functions for NBNXM GPU data management.
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.h:#ifndef GMX_NBNXM_NBNXM_GPU_DATA_MGMT_H
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.h:#define GMX_NBNXM_NBNXM_GPU_DATA_MGMT_H
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.h:/*! \brief Initializes the NBNXM GPU data structures. */
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.h:void gpu_init_platform_specific(NbnxmGpu* nb);
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.h:/*! \brief Releases the NBNXM GPU data structures. */
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.h:void gpu_free_platform_specific(NbnxmGpu* nb);
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.h:#endif // GMX_NBNXM_NBNXM_GPU_DATA_MGMT_H
src/gromacs/nbnxm/nbnxm_gpu_buffer_ops_internal.h:#include "gromacs/gpu_utils/devicebuffer_datatype.h"
src/gromacs/nbnxm/nbnxm_gpu_buffer_ops_internal.h:#include "gromacs/gpu_utils/gputraits.h"
src/gromacs/nbnxm/nbnxm_gpu_buffer_ops_internal.h:struct NbnxmGpu;
src/gromacs/nbnxm/nbnxm_gpu_buffer_ops_internal.h:                                     NbnxmGpu*            nb,
src/gromacs/nbnxm/gpu_common_utils.h: * \brief Implements common util routines for different NBNXN GPU implementations
src/gromacs/nbnxm/gpu_common_utils.h:#ifndef GMX_NBNXM_GPU_COMMON_UTILS_H
src/gromacs/nbnxm/gpu_common_utils.h:#define GMX_NBNXM_GPU_COMMON_UTILS_H
src/gromacs/nbnxm/gpu_common_utils.h:#include "gromacs/listed_forces/listed_forces_gpu.h"
src/gromacs/nbnxm/gpu_common_utils.h:#include "gromacs/nbnxm/gpu_types_common.h"
src/gromacs/nbnxm/gpu_common_utils.h:/*! \brief An early return condition for empty NB GPU workloads
src/gromacs/nbnxm/gpu_common_utils.h:static inline bool canSkipNonbondedWork(const NbnxmGpu& nb, InteractionLocality iloc)
src/gromacs/nbnxm/gpu_common_utils.h:static inline Range<int> getGpuAtomRange(const NBAtomDataGpu* atomData, const AtomLocality atomLocality)
src/gromacs/nbnxm/tests/kernel_test.cpp:#include "gromacs/gpu_utils/hostallocator.h"
src/gromacs/nbnxm/tests/kernel_test.cpp:    //! Whether to use a GPU, currently GPUs are not supported
src/gromacs/nbnxm/tests/kernel_test.cpp:    bool useGpu = false;
src/gromacs/nbnxm/tests/kernel_test.cpp:            (options.useGpu ? PinningPolicy::PinnedIfSupported : PinningPolicy::CannotBePinned);
src/gromacs/nbnxm/tests/CMakeLists.txt:gmx_add_unit_test(NbnxmGpuTests nbnxm-gpu-test
src/gromacs/nbnxm/tests/CMakeLists.txt:    GPU_CPP_SOURCE_FILES
src/gromacs/nbnxm/tests/CMakeLists.txt:target_link_libraries(nbnxm-gpu-test PRIVATE nbnxm gpu_utils timing)
src/gromacs/nbnxm/tests/pairlist.cpp: * Tests for gpu pairlist datastructure memory handling.
src/gromacs/nbnxm/tests/pairlist.cpp:#if GMX_GPU
src/gromacs/nbnxm/tests/pairlist.cpp:#    include "gromacs/gpu_utils/device_context.h"
src/gromacs/nbnxm/tests/pairlist.cpp:#    include "gromacs/gpu_utils/device_stream.h"
src/gromacs/nbnxm/tests/pairlist.cpp:#    include "gromacs/gpu_utils/devicebuffer.h"
src/gromacs/nbnxm/tests/pairlist.cpp:#    include "gromacs/gpu_utils/gpu_utils.h"
src/gromacs/nbnxm/tests/pairlist.cpp:#    include "gromacs/gpu_utils/hostallocator.h"
src/gromacs/nbnxm/tests/pairlist.cpp:#    include "gromacs/nbnxm/gpu_types_common.h"
src/gromacs/nbnxm/tests/pairlist.cpp:void allocateDeviceBuffers(GpuPairlist* pairlist, int allocationSize, const DeviceContext& deviceContext)
src/gromacs/nbnxm/tests/pairlist.cpp:        for (int index = 0; index < sc_gpuClusterPairSplit(sc_layoutType); index++)
src/gromacs/nbnxm/tests/pairlist.cpp:        for (int index = 0; index < sc_gpuJgroupSize(sc_layoutType); index++)
src/gromacs/nbnxm/tests/pairlist.cpp:        for (int index = 0; index < sc_gpuExclSize(sc_layoutType); index++)
src/gromacs/nbnxm/tests/pairlist.cpp:void transferHostToDevice(GpuPairlist*        pairlist,
src/gromacs/nbnxm/tests/pairlist.cpp:            &pairlist->sci, hostbuffers.h_sci.data(), 0, dataSize, deviceStream, GpuApiCallBehavior::Sync, nullptr);
src/gromacs/nbnxm/tests/pairlist.cpp:                       GpuApiCallBehavior::Sync,
src/gromacs/nbnxm/tests/pairlist.cpp:            &pairlist->excl, hostbuffers.h_excl.data(), 0, dataSize, deviceStream, GpuApiCallBehavior::Sync, nullptr);
src/gromacs/nbnxm/tests/pairlist.cpp:                       GpuApiCallBehavior::Sync,
src/gromacs/nbnxm/tests/pairlist.cpp:void transferDeviceToHost(GpuPairlist* pairlist, HostBuffers* hostbuffers, int dataSize, const DeviceStream& deviceStream)
src/gromacs/nbnxm/tests/pairlist.cpp:            hostbuffers->h_sci.data(), &pairlist->sci, 0, dataSize, deviceStream, GpuApiCallBehavior::Sync, nullptr);
src/gromacs/nbnxm/tests/pairlist.cpp:                         GpuApiCallBehavior::Sync,
src/gromacs/nbnxm/tests/pairlist.cpp:            hostbuffers->h_excl.data(), &pairlist->excl, 0, dataSize, deviceStream, GpuApiCallBehavior::Sync, nullptr);
src/gromacs/nbnxm/tests/pairlist.cpp:                         GpuApiCallBehavior::Sync,
src/gromacs/nbnxm/tests/pairlist.cpp:                          GpuPairlist*         pairlist,
src/gromacs/nbnxm/tests/pairlist.cpp:void checkPairlistConsistency(const GpuPairlist& pairlist, int allocationSize)
src/gromacs/nbnxm/tests/pairlist.cpp:void checkValuesFromPairlist(GpuPairlist*        pairlist,
src/gromacs/nbnxm/tests/pairlist.cpp:TEST(GpuPairlistTest, PairlistInitWorks)
src/gromacs/nbnxm/tests/pairlist.cpp:        GpuPairlist   pairlist{};
src/gromacs/nbnxm/tests/exclusions.cpp:#include "gromacs/gpu_utils/hostallocator.h"
src/gromacs/nbnxm/atomdata.cpp:#include "nbnxm_gpu.h"
src/gromacs/nbnxm/atomdata.cpp:        GMX_RELEASE_ASSERT(numEnergyGroups == 1, "GPU kernels do not support energy groups");
src/gromacs/nbnxm/atomdata.cpp:        const int  nsubc = (grid.geometry().isSimple_) ? 1 : sc_gpuNumClusterPerCell(layoutType);
src/gromacs/nbnxm/atomdata.cpp:/* Copies (and reorders) the coordinates to nbnxn_atomdata_t on the GPU*/
src/gromacs/nbnxm/atomdata.cpp:void nbnxn_atomdata_x_to_nbat_x_gpu(const GridSet&        gridSet,
src/gromacs/nbnxm/atomdata.cpp:                                    NbnxmGpu*             gpu_nbv,
src/gromacs/nbnxm/atomdata.cpp:                                    GpuEventSynchronizer* xReadyOnDevice)
src/gromacs/nbnxm/atomdata.cpp:        nbnxn_gpu_x_to_nbat_x(gridSet.grids()[g],
src/gromacs/nbnxm/atomdata.cpp:                              gpu_nbv,
src/gromacs/nbnxm/hip/nbnxm_gpu_buffer_ops_internal_hip.cpp: *  \author Alan Gray <alang@nvidia.com>
src/gromacs/nbnxm/hip/nbnxm_gpu_buffer_ops_internal_hip.cpp: *  \author Jon Vincent <jvincent@nvidia.com>
src/gromacs/nbnxm/hip/nbnxm_gpu_buffer_ops_internal_hip.cpp:#include "gromacs/gpu_utils/typecasts_cuda_hip.h"
src/gromacs/nbnxm/hip/nbnxm_gpu_buffer_ops_internal_hip.cpp:#include "gromacs/gpu_utils/vectype_ops_hip.h"
src/gromacs/nbnxm/hip/nbnxm_gpu_buffer_ops_internal_hip.cpp:#include "gromacs/nbnxm/nbnxm_gpu_buffer_ops_internal.h"
src/gromacs/nbnxm/hip/nbnxm_gpu_buffer_ops_internal_hip.cpp:                                     NbnxmGpu*            nb,
src/gromacs/nbnxm/hip/nbnxm_gpu_buffer_ops_internal_hip.cpp:    const auto kernelArgs    = prepareGpuKernelArguments(
src/gromacs/nbnxm/hip/nbnxm_gpu_buffer_ops_internal_hip.cpp:    launchGpuKernel(kernelFn, config, deviceStream, nullptr, "XbufferOps", kernelArgs);
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_body.h:#include "gromacs/gpu_utils/device_utils_hip_sycl.h"
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_body.h:#include "gromacs/gpu_utils/devicebuffer.h"
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_body.h:#include "gromacs/gpu_utils/gpu_kernel_utils.h"
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_body.h:#include "gromacs/gpu_utils/typecasts_cuda_hip.h"
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_body.h:#include "gromacs/nbnxm/gpu_types_common.h"
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_body.h:    constexpr bool isWave64 = sc_gpuParallelExecutionWidth(pairlistType) == 64;
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_body.h:    // TODO: current ROCm (latest 6.0.2) compiler does not do this transformation. Remove when this is no longer the case.
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_body.h: * Note: This causes massive amount of spills with the tabulated kernel on gfx803 using ROCm 5.3.
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_body.h:    static_assert(isPowerOfTwo(sc_gpuClusterPerSuperCluster(pairlistType)));
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_body.h:    static_assert(sc_gpuClusterSize(pairlistType) == 8);
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_body.h:    static_assert(isPowerOfTwo(sc_gpuClusterPerSuperCluster(pairlistType)));
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_body.h:    static_assert(sc_gpuClusterSize(pairlistType) == 8);
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_body.h:    constexpr int c_clSize = sc_gpuClusterSize(pairlistType);
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_body.h:    static_assert(isPowerOfTwo(sc_gpuClusterPerSuperCluster(pairlistType)));
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_body.h:    constexpr int c_clSize                 = sc_gpuClusterSize(pairlistType);
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_body.h:    constexpr int c_parallelExecutionWidth = sc_gpuParallelExecutionWidth(pairlistType);
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_body.h:    constexpr int c_clusterPerSuperCluster = sc_gpuClusterPerSuperCluster(pairlistType);
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_body.h:    constexpr int c_clSize                 = sc_gpuClusterSize(pairlistType);
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_body.h:    if constexpr (gmx::sc_gpuParallelExecutionWidth(pairlistType) == 64)
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_body.h:    static_assert(isPowerOfTwo(sc_gpuClusterPerSuperCluster(pairlistType)));
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_body.h:    constexpr int c_parallelExecutionWidth = sc_gpuParallelExecutionWidth(pairlistType);
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_body.h:    if constexpr (sc_gpuParallelExecutionWidth(pairlistType) == 64)
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_body.h:        static void nbnxmKernel(NBAtomDataGpu atdat, NBParamGpu nbparam, GpuPairlist plist, bool doCalcShift)
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_body.h:        constexpr int c_clusterPerSuperCluster = sc_gpuClusterPerSuperCluster(pairlistType);
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_body.h:        constexpr int c_gpuJGroupSize          = sc_gpuJgroupSize(pairlistType);
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_body.h:        constexpr int c_clSize                 = sc_gpuClusterSize(pairlistType);
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_body.h:        constexpr int c_parallelExecutionWidth = sc_gpuParallelExecutionWidth(pairlistType);
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_body.h:        /*! i-cluster interaction mask for a super-cluster with all c_nbnxnGpuNumClusterPerSupercluster=8 bits set */
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_body.h:            } // (nbSci.shift == c_centralShiftIndex && a_plistCJPacked[cijPackedBegin].cj[0] == sci * c_nbnxnGpuNumClusterPerSupercluster)
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_body.h:#pragma unroll c_gpuJGroupSize
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_body.h:            for (int jm = 0; jm < c_gpuJGroupSize; jm++)
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_body.h:                                if constexpr (sc_gpuParallelExecutionWidth(pairlistType) == 64)
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_body.h:            } // for (int jm = 0; jm < c_gpuJGroupSize; jm++)
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_body.h:    constexpr int      c_clSize      = sc_gpuClusterSize(pairlistType);
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_body.h:    const auto kernelArgs = prepareGpuKernelArguments(kernel, config, args...);
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_body.h:    launchGpuKernel(kernel, config, deviceStream, nullptr, kernelName.c_str(), kernelArgs);
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_body.h:void launchNbnxmKernelHelper(NbnxmGpu* nb, const StepWorkload& stepWork, const InteractionLocality iloc)
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_body.h:    NBAtomDataGpu*           adat         = nb->atdat;
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_body.h:    NBParamGpu*              nbp          = nb->nbparam;
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_utils.h:#include "gromacs/gpu_utils/hip_kernel_utils.h"
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_utils.h:#include "gromacs/gpu_utils/typecasts_cuda_hip.h"
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_utils.h:#include "gromacs/gpu_utils/vectype_ops_hip.h"
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_utils.h:__device__ constexpr int c_subWarp = sc_gpuParallelExecutionWidth(pairlistType);
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_utils.h:__device__ constexpr int c_clSizeLog2 = StaticLog2<sc_gpuClusterSize(pairlistType)>::value;
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_utils.h:__device__ constexpr int c_clSizeSq = sc_gpuClusterSize(pairlistType) * sc_gpuClusterSize(pairlistType);
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_utils.h:    constexpr int offset = sc_gpuClusterPerSuperCluster(pairlistType) * sc_gpuClusterSize(pairlistType);
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_utils.h:            numThreadZ * sc_gpuClusterPerSuperCluster(pairlistType) * sc_gpuClusterSize(pairlistType);
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_utils.h:        constexpr int pruneKernelOffset = numThreadZ * gmx::sc_gpuClusterPairSplit(pairlistType)
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_utils.h:                                          * gmx::sc_gpuJgroupSize(pairlistType);
src/gromacs/nbnxm/hip/nbnxm_hip_kernel.h:struct NbnxmGpu;
src/gromacs/nbnxm/hip/nbnxm_hip_kernel.h:void launchNbnxmKernel(NbnxmGpu* nb, const StepWorkload& stepWork, InteractionLocality iloc, bool doPrune);
src/gromacs/nbnxm/hip/nbnxm_hip_kernel.cpp:#include "gromacs/gpu_utils/device_context.h"
src/gromacs/nbnxm/hip/nbnxm_hip_kernel.cpp:void launchNbnxmKernelHelper(NbnxmGpu* nb, const StepWorkload& stepWork, InteractionLocality iloc);
src/gromacs/nbnxm/hip/nbnxm_hip_kernel.cpp:extern template void launchNbnxmKernelHelper<false, false, false>(NbnxmGpu* nb, const StepWorkload&  stepWork, const InteractionLocality iloc);
src/gromacs/nbnxm/hip/nbnxm_hip_kernel.cpp:extern template void launchNbnxmKernelHelper<false, false, true>(NbnxmGpu* nb, const StepWorkload&  stepWork, const InteractionLocality iloc);
src/gromacs/nbnxm/hip/nbnxm_hip_kernel.cpp:extern template void launchNbnxmKernelHelper<false, true, true>(NbnxmGpu* nb, const StepWorkload&  stepWork, const InteractionLocality iloc);
src/gromacs/nbnxm/hip/nbnxm_hip_kernel.cpp:extern template void launchNbnxmKernelHelper<false, true, false>(NbnxmGpu* nb, const StepWorkload&  stepWork, const InteractionLocality iloc);
src/gromacs/nbnxm/hip/nbnxm_hip_kernel.cpp:extern template void launchNbnxmKernelHelper<true, false, false>(NbnxmGpu* nb, const StepWorkload&  stepWork, const InteractionLocality iloc);
src/gromacs/nbnxm/hip/nbnxm_hip_kernel.cpp:extern template void launchNbnxmKernelHelper<true, false, true>(NbnxmGpu* nb, const StepWorkload&  stepWork, const InteractionLocality iloc);
src/gromacs/nbnxm/hip/nbnxm_hip_kernel.cpp:extern template void launchNbnxmKernelHelper<true, true, true>(NbnxmGpu* nb, const StepWorkload&  stepWork, const InteractionLocality iloc);
src/gromacs/nbnxm/hip/nbnxm_hip_kernel.cpp:extern template void launchNbnxmKernelHelper<true, true, false>(NbnxmGpu* nb, const StepWorkload&  stepWork, const InteractionLocality iloc);
src/gromacs/nbnxm/hip/nbnxm_hip_kernel.cpp:void launchNbnxmKernel(NbnxmGpu* nb, const StepWorkload& stepWork, const InteractionLocality iloc, bool doPrune)
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_body_fv_noprune.cpp:template void launchNbnxmKernelHelper<true, false, true>(NbnxmGpu*                 nb,
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_body_fv_noprune.cpp:template void launchNbnxmKernelHelper<false, false, true>(NbnxmGpu*                 nb,
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_pruneonly.h:struct NbnxmGpu;
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_pruneonly.h:void launchNbnxmKernelPruneOnly(NbnxmGpu*                 nb,
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_sci_sort.cpp:#include "gromacs/nbnxm/gpu_data_mgmt.h"
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_sci_sort.cpp:#include "gromacs/nbnxm/gpu_types_common.h"
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_sci_sort.cpp:__launch_bounds__(threadsPerBlock) __global__ void nbnxmKernelBucketSciSort(GpuPairlist plist)
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_sci_sort.cpp:void launchNbnxmKernelHelperSciSort(const DeviceStream& deviceStream, GpuPairlist* plist)
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_sci_sort.cpp:    const auto kernelSciSortArgs   = prepareGpuKernelArguments(kernelSciSort, configSortSci, plist);
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_sci_sort.cpp:    launchGpuKernel(kernelSciSort, configSortSci, deviceStream, nullptr, "nbnxn_kernel_sci_sort", kernelSciSortArgs);
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_sci_sort.cpp:void launchNbnxmKernelSciSort(NbnxmGpu* nb, InteractionLocality iloc)
src/gromacs/nbnxm/hip/CMakeLists.txt:if(GMX_GPU_HIP)
src/gromacs/nbnxm/hip/CMakeLists.txt:         nbnxm_gpu_buffer_ops_internal_hip.cpp 
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_pruneonly.cpp:#include "gromacs/gpu_utils/device_context.h"
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_pruneonly.cpp:#include "gromacs/gpu_utils/device_utils_hip_sycl.h"
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_pruneonly.cpp:#include "gromacs/gpu_utils/devicebuffer_hip.h"
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_pruneonly.cpp:#include "gromacs/gpu_utils/hiputils.h"
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_pruneonly.cpp:#include "gromacs/nbnxm/gpu_types_common.h"
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_pruneonly.cpp:        static void nbnxmKernelPruneOnly(NBAtomDataGpu atdat, NBParamGpu nbparam, GpuPairlist plist, int numParts)
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_pruneonly.cpp:        constexpr int c_clSize                 = sc_gpuClusterSize(pairlistType);
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_pruneonly.cpp:        constexpr int c_clusterPerSuperCluster = sc_gpuClusterPerSuperCluster(pairlistType);
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_pruneonly.cpp:        constexpr int c_gpuJGroupSize          = sc_gpuJgroupSize(pairlistType);
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_pruneonly.cpp:        constexpr int c_parallelExecutionWidth = sc_gpuParallelExecutionWidth(pairlistType);
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_pruneonly.cpp:                imaskFull = gm_plistIMask[jPacked * sc_gpuClusterPairSplit(pairlistType) + widx];
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_pruneonly.cpp:                for (int jm = 0; jm < c_gpuJGroupSize; jm++)
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_pruneonly.cpp:                } // for (int jm = 0; jm < c_gpuJGroupSize; jm++)
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_pruneonly.cpp:                    gm_plistIMask[jPacked * gmx::sc_gpuClusterPairSplit(pairlistType) + widx] = imaskFull;
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_pruneonly.cpp:    constexpr bool isWave64 = sc_gpuParallelExecutionWidth(pairlistType) == 64;
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_pruneonly.cpp:    constexpr int c_clSize = sc_gpuClusterSize(pairlistType);
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_pruneonly.cpp:    const auto kernelArgs = prepareGpuKernelArguments(kernel, config, args...);
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_pruneonly.cpp:    launchGpuKernel(kernel, config, deviceStream, nullptr, kernelName.c_str(), kernelArgs);
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_pruneonly.cpp:void launchNbnxmKernelPruneOnly(NbnxmGpu* nb, const InteractionLocality iloc, const int* numParts, const int numSciInPart)
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_pruneonly.cpp:    NBAtomDataGpu*           adat          = nb->atdat;
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_pruneonly.cpp:    NBParamGpu*              nbp           = nb->nbparam;
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_body_f_prune.cpp:template void launchNbnxmKernelHelper<true, true, false>(NbnxmGpu*                 nb,
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_body_f_prune.cpp:template void launchNbnxmKernelHelper<false, true, false>(NbnxmGpu*                 nb,
src/gromacs/nbnxm/hip/nbnxm_hip.cpp:#include "gromacs/nbnxm/gpu_common.h"
src/gromacs/nbnxm/hip/nbnxm_hip.cpp:void gpu_launch_kernel_pruneonly(NbnxmGpu* nb, const InteractionLocality iloc, const int numParts)
src/gromacs/nbnxm/hip/nbnxm_hip.cpp:void gpu_launch_kernel(NbnxmGpu* nb, const StepWorkload& stepWork, const InteractionLocality iloc)
src/gromacs/nbnxm/hip/nbnxm_hip.cpp:    const NBParamGpu* nbp   = nb->nbparam;
src/gromacs/nbnxm/hip/nbnxm_hip.cpp:        gpu_launch_kernel_pruneonly(nb, iloc, 1);
src/gromacs/nbnxm/hip/nbnxm_hip_types.h:#include "gromacs/gpu_utils/gpueventsynchronizer.h"
src/gromacs/nbnxm/hip/nbnxm_hip_types.h:#include "gromacs/nbnxm/gpu_types_common.h"
src/gromacs/nbnxm/hip/nbnxm_hip_types.h:#include "gromacs/timing/gpu_timing.h"
src/gromacs/nbnxm/hip/nbnxm_hip_types.h:struct NbnxmGpu
src/gromacs/nbnxm/hip/nbnxm_hip_types.h:    /*! \brief GPU device context.
src/gromacs/nbnxm/hip/nbnxm_hip_types.h:     * \todo Make it constant reference, once NbnxmGpu is a proper class.
src/gromacs/nbnxm/hip/nbnxm_hip_types.h:    /*! \brief true if doing both local/non-local NB work on GPU */
src/gromacs/nbnxm/hip/nbnxm_hip_types.h:    NBAtomDataGpu* atdat = nullptr;
src/gromacs/nbnxm/hip/nbnxm_hip_types.h:    NBParamGpu* nbparam = nullptr;
src/gromacs/nbnxm/hip/nbnxm_hip_types.h:    EnumerationArray<InteractionLocality, std::unique_ptr<GpuPairlist>> plist;
src/gromacs/nbnxm/hip/nbnxm_hip_types.h:    /*! \brief local and non-local GPU streams */
src/gromacs/nbnxm/hip/nbnxm_hip_types.h:    GpuEventSynchronizer nonlocal_done;
src/gromacs/nbnxm/hip/nbnxm_hip_types.h:    GpuEventSynchronizer misc_ops_and_local_H2D_done;
src/gromacs/nbnxm/hip/nbnxm_hip_types.h:     * This includes local/nonlocal GPU work, either bonded or
src/gromacs/nbnxm/hip/nbnxm_hip_types.h:     * local/nonlocal, if there is bonded GPU work, both flags
src/gromacs/nbnxm/hip/nbnxm_hip_types.h:    GpuTimers* timers = nullptr;
src/gromacs/nbnxm/hip/nbnxm_hip_types.h:    gmx_wallclock_gpu_nbnxn_t* timings = nullptr;
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_body_fv_prune.cpp:template void launchNbnxmKernelHelper<true, true, true>(NbnxmGpu*                 nb,
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_body_fv_prune.cpp:template void launchNbnxmKernelHelper<false, true, true>(NbnxmGpu*                 nb,
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_sci_sort.h:struct NbnxmGpu;
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_sci_sort.h:void launchNbnxmKernelSciSort(NbnxmGpu* nb, InteractionLocality iloc);
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_body_f_noprune.cpp:template void launchNbnxmKernelHelper<true, false, false>(NbnxmGpu*                 nb,
src/gromacs/nbnxm/hip/nbnxm_hip_kernel_body_f_noprune.cpp:template void launchNbnxmKernelHelper<false, false, false>(NbnxmGpu*                 nb,
src/gromacs/nbnxm/hip/nbnxm_hip_data_mgmt.cpp: *  \brief Define HIP implementation for GPU data transfer for NBNXM module
src/gromacs/nbnxm/hip/nbnxm_hip_data_mgmt.cpp:// use unsupported dpp instructions. Tracked here, but not fixed even if the ticket says so: https://github.com/ROCm/rocPRIM/issues/452
src/gromacs/nbnxm/hip/nbnxm_hip_data_mgmt.cpp:// TODO We would like to move this down, but the way NbnxmGpu
src/gromacs/nbnxm/hip/nbnxm_hip_data_mgmt.cpp://      is currently declared means this has to be before gpu_types.h
src/gromacs/nbnxm/hip/nbnxm_hip_data_mgmt.cpp:#include "gromacs/gpu_utils/device_context.h"
src/gromacs/nbnxm/hip/nbnxm_hip_data_mgmt.cpp:#include "gromacs/gpu_utils/pmalloc.h"
src/gromacs/nbnxm/hip/nbnxm_hip_data_mgmt.cpp:#include "gromacs/nbnxm/gpu_data_mgmt.h"
src/gromacs/nbnxm/hip/nbnxm_hip_data_mgmt.cpp:#include "gromacs/nbnxm/nbnxm_gpu_data_mgmt.h"
src/gromacs/nbnxm/hip/nbnxm_hip_data_mgmt.cpp:static const unsigned int gpu_min_ci_balanced_factor = 44;
src/gromacs/nbnxm/hip/nbnxm_hip_data_mgmt.cpp:void gpu_init_platform_specific(NbnxmGpu* /* nb */)
src/gromacs/nbnxm/hip/nbnxm_hip_data_mgmt.cpp:void gpu_free_platform_specific(NbnxmGpu* /* nb */)
src/gromacs/nbnxm/hip/nbnxm_hip_data_mgmt.cpp:int gpu_min_ci_balanced(NbnxmGpu* nb)
src/gromacs/nbnxm/hip/nbnxm_hip_data_mgmt.cpp:    return nb != nullptr ? gpu_min_ci_balanced_factor * nb->deviceContext_->deviceInfo().prop.multiProcessorCount
src/gromacs/nbnxm/hip/nbnxm_hip_data_mgmt.cpp:                         gmx::GpuPairlist*   plist,
src/gromacs/nbnxm/hip/nbnxm_hip_data_mgmt.cpp:size_t getExclusiveScanWorkingArraySize(GpuPairlist* plist, const DeviceStream& deviceStream)
src/gromacs/nbnxm/hip/nbnxm_hip_data_mgmt.cpp:                          GpuPairlist*        plist,
src/gromacs/nbnxm/opencl/nbnxm_ocl_types.h:#ifndef GMX_NBNXM_NBNXM_OPENCL_TYPES_H
src/gromacs/nbnxm/opencl/nbnxm_ocl_types.h:#define GMX_NBNXM_NBNXM_OPENCL_TYPES_H
src/gromacs/nbnxm/opencl/nbnxm_ocl_types.h:#include "gromacs/gpu_utils/devicebuffer.h"
src/gromacs/nbnxm/opencl/nbnxm_ocl_types.h:#include "gromacs/gpu_utils/gmxopencl.h"
src/gromacs/nbnxm/opencl/nbnxm_ocl_types.h:#include "gromacs/gpu_utils/gpueventsynchronizer.h"
src/gromacs/nbnxm/opencl/nbnxm_ocl_types.h:#include "gromacs/gpu_utils/gputraits_ocl.h"
src/gromacs/nbnxm/opencl/nbnxm_ocl_types.h:#include "gromacs/gpu_utils/oclutils.h"
src/gromacs/nbnxm/opencl/nbnxm_ocl_types.h:#include "gromacs/nbnxm/gpu_types_common.h"
src/gromacs/nbnxm/opencl/nbnxm_ocl_types.h:#include "gromacs/nbnxm/nbnxm_gpu.h"
src/gromacs/nbnxm/opencl/nbnxm_ocl_types.h:struct gmx_wallclock_gpu_nbnxn_t;
src/gromacs/nbnxm/opencl/nbnxm_ocl_types.h: * \brief Data structure shared between the OpenCL device code and OpenCL host code
src/gromacs/nbnxm/opencl/nbnxm_ocl_types.h: * Must not contain OpenCL objects (buffers)
src/gromacs/nbnxm/opencl/nbnxm_ocl_types.h: * \brief Main data structure for OpenCL nonbonded force calculations.
src/gromacs/nbnxm/opencl/nbnxm_ocl_types.h:struct NbnxmGpu
src/gromacs/nbnxm/opencl/nbnxm_ocl_types.h:    /* \brief OpenCL device context
src/gromacs/nbnxm/opencl/nbnxm_ocl_types.h:     * \todo Make it constant reference, once NbnxmGpu is a proper class.
src/gromacs/nbnxm/opencl/nbnxm_ocl_types.h:    //! OpenCL runtime data (context, kernels)
src/gromacs/nbnxm/opencl/nbnxm_ocl_types.h:    //! true if doing both local/non-local NB work on GPU
src/gromacs/nbnxm/opencl/nbnxm_ocl_types.h:    NBAtomDataGpu* atdat = nullptr;
src/gromacs/nbnxm/opencl/nbnxm_ocl_types.h:    NBParamGpu* nbparam = nullptr;
src/gromacs/nbnxm/opencl/nbnxm_ocl_types.h:    gmx::EnumerationArray<InteractionLocality, std::unique_ptr<GpuPairlist>> plist = { nullptr };
src/gromacs/nbnxm/opencl/nbnxm_ocl_types.h:    // Data for GPU-side coordinate conversion between integrator and NBNXM
src/gromacs/nbnxm/opencl/nbnxm_ocl_types.h:    //! local and non-local GPU queues
src/gromacs/nbnxm/opencl/nbnxm_ocl_types.h:    GpuEventSynchronizer nonlocal_done;
src/gromacs/nbnxm/opencl/nbnxm_ocl_types.h:    GpuEventSynchronizer misc_ops_and_local_H2D_done;
src/gromacs/nbnxm/opencl/nbnxm_ocl_types.h:    //! True if there has been local/nonlocal GPU work, either bonded or nonbonded, scheduled
src/gromacs/nbnxm/opencl/nbnxm_ocl_types.h:    //  local/nonlocal, if there is bonded GPU work, both flags will be true.
src/gromacs/nbnxm/opencl/nbnxm_ocl_types.h:    //! OpenCL event-based timers.
src/gromacs/nbnxm/opencl/nbnxm_ocl_types.h:    GpuTimers* timers = nullptr;
src/gromacs/nbnxm/opencl/nbnxm_ocl_types.h:    gmx_wallclock_gpu_nbnxn_t* timings = nullptr;
src/gromacs/nbnxm/opencl/nbnxm_ocl_types.h:#endif /* NBNXN_OPENCL_TYPES_H */
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel.clh: *  \brief OpenCL non-bonded kernel.
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel.clh: *  OpenCL 1.2 support is expected.
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel.clh:/* Currently we enable CJ prefetch for AMD/NVIDIA and disable it for the "nowarp" kernel
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel.clh:   NB_KERNEL_FUNC_NAME differs from the CUDA equivalent as it is not a variadic macro due to OpenCL
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel.clh:NB_KERNEL_FUNC_NAME(nbnxn_kernel, _VF_prune_opencl)
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel.clh:NB_KERNEL_FUNC_NAME(nbnxn_kernel, _F_prune_opencl)
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel.clh:NB_KERNEL_FUNC_NAME(nbnxn_kernel, _VF_opencl)
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel.clh:NB_KERNEL_FUNC_NAME(nbnxn_kernel, _F_opencl)
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel.clh:    /*! i-cluster interaction mask for a super-cluster with all c_nbnxnGpuNumClusterPerSupercluster=8 bits set */
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel.clh:    const unsigned superClInteractionMask = ((1U << c_nbnxnGpuNumClusterPerSupercluster) - 1U);
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel.clh:#define LOCAL_OFFSET (xqib + c_nbnxnGpuNumClusterPerSupercluster * CL_SIZE)
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel.clh:#    define LOCAL_OFFSET (cjs + 2 * c_nbnxnGpuJgroupSize)
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel.clh:#        define LOCAL_OFFSET (atib + c_nbnxnGpuNumClusterPerSupercluster * CL_SIZE)
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel.clh:#        define LOCAL_OFFSET (ljcpib + c_nbnxnGpuNumClusterPerSupercluster * CL_SIZE)
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel.clh:    /* Local buffer used to implement __any warp vote function from CUDA.
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel.clh:    const bool c_loadUsingAllXYThreads = (CL_SIZE == c_nbnxnGpuNumClusterPerSupercluster);
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel.clh:    if (c_loadUsingAllXYThreads || tidxj < c_nbnxnGpuNumClusterPerSupercluster)
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel.clh:        for (int i = 0; i < c_nbnxnGpuNumClusterPerSupercluster; i += CL_SIZE)
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel.clh:            const int ci = sci * c_nbnxnGpuNumClusterPerSupercluster + tidxj + i;
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel.clh:    /* Initialise warp vote. (8x8 block) 2 warps for nvidia */
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel.clh:    fvec fci_buf[c_nbnxnGpuNumClusterPerSupercluster]; /* i force buffer */
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel.clh:    for (int ci_offset = 0; ci_offset < c_nbnxnGpuNumClusterPerSupercluster; ci_offset++)
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel.clh:        && pl_cjPacked[cjPackedBegin].cj[0] == sci * c_nbnxnGpuNumClusterPerSupercluster)
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel.clh:        for (int i = 0; i < c_nbnxnGpuNumClusterPerSupercluster; i++)
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel.clh:            E_lj += nbfp[atom_types[(sci * c_nbnxnGpuNumClusterPerSupercluster + i) * CL_SIZE + tidxi] * (ntypes + 1)]
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel.clh:             * TODO: check loop unrolling with NVIDIA OpenCL
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel.clh:#if !defined PRUNE_NBL && !defined _NVIDIA_SOURCE_
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel.clh:#    pragma unroll c_nbnxnGpuJgroupSize
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel.clh:            for (int jm = 0; jm < c_nbnxnGpuJgroupSize; jm++)
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel.clh:                if (imask & (superClInteractionMask << (jm * c_nbnxnGpuNumClusterPerSupercluster)))
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel.clh:                    unsigned int mask_ji = (1U << (jm * c_nbnxnGpuNumClusterPerSupercluster));
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel.clh:#    pragma unroll c_nbnxnGpuNumClusterPerSupercluster
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel.clh:                    for (int i = 0; i < c_nbnxnGpuNumClusterPerSupercluster; i++)
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel.clh:                            const int gmx_unused ci = sci * c_nbnxnGpuNumClusterPerSupercluster + i; /* i cluster index */
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp: *  \brief Define OpenCL implementation of nbnxm_gpu.h
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp: *  - Consider extracting common sections of the OpenCL and CUDA nbnxn logic, e.g:
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp: *    - in nbnxn_gpu_launch_kernel_pruneonly() the pre- and post-kernel launch logic
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:#include "gromacs/gpu_utils/device_context.h"
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:#include "gromacs/gpu_utils/gputraits_ocl.h"
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:#include "gromacs/gpu_utils/oclutils.h"
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:#include "gromacs/nbnxm/gpu_common.h"
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:#include "gromacs/nbnxm/gpu_common_utils.h"
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:#include "gromacs/nbnxm/gpu_data_mgmt.h"
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:#include "gromacs/nbnxm/nbnxm_gpu.h"
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:#include "gromacs/timing/gpu_timing.h"
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:    { "nbnxn_kernel_ElecCut_VdwLJ_F_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecCut_VdwLJCombGeom_F_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecCut_VdwLJCombLB_F_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecCut_VdwLJFsw_F_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecCut_VdwLJPsw_F_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecCut_VdwLJEwCombGeom_F_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecCut_VdwLJEwCombLB_F_opencl" },
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:    { "nbnxn_kernel_ElecRF_VdwLJ_F_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecRF_VdwLJCombGeom_F_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecRF_VdwLJCombLB_F_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecRF_VdwLJFsw_F_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecRF_VdwLJPsw_F_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecRF_VdwLJEwCombGeom_F_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecRF_VdwLJEwCombLB_F_opencl" },
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:    { "nbnxn_kernel_ElecEwQSTab_VdwLJ_F_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwQSTab_VdwLJCombGeom_F_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwQSTab_VdwLJCombLB_F_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwQSTab_VdwLJFsw_F_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwQSTab_VdwLJPsw_F_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwQSTab_VdwLJEwCombGeom_F_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwQSTab_VdwLJEwCombLB_F_opencl" },
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:    { "nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJ_F_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJCombGeom_F_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJCombLB_F_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJFsw_F_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJPsw_F_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJEwCombGeom_F_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJEwCombLB_F_opencl" },
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:    { "nbnxn_kernel_ElecEw_VdwLJ_F_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEw_VdwLJCombGeom_F_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEw_VdwLJCombLB_F_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEw_VdwLJFsw_F_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEw_VdwLJPsw_F_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEw_VdwLJEwCombGeom_F_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEw_VdwLJEwCombLB_F_opencl" },
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:    { "nbnxn_kernel_ElecEwTwinCut_VdwLJ_F_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwTwinCut_VdwLJCombGeom_F_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwTwinCut_VdwLJCombLB_F_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwTwinCut_VdwLJFsw_F_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwTwinCut_VdwLJPsw_F_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwTwinCut_VdwLJEwCombGeom_F_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwTwinCut_VdwLJEwCombLB_F_opencl" }
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:    { "nbnxn_kernel_ElecCut_VdwLJ_VF_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecCut_VdwLJCombGeom_VF_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecCut_VdwLJCombLB_VF_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecCut_VdwLJFsw_VF_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecCut_VdwLJPsw_VF_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecCut_VdwLJEwCombGeom_VF_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecCut_VdwLJEwCombLB_VF_opencl" },
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:    { "nbnxn_kernel_ElecRF_VdwLJ_VF_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecRF_VdwLJCombGeom_VF_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecRF_VdwLJCombLB_VF_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecRF_VdwLJFsw_VF_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecRF_VdwLJPsw_VF_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecRF_VdwLJEwCombGeom_VF_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecRF_VdwLJEwCombLB_VF_opencl" },
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:    { "nbnxn_kernel_ElecEwQSTab_VdwLJ_VF_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwQSTab_VdwLJCombGeom_VF_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwQSTab_VdwLJCombLB_VF_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwQSTab_VdwLJFsw_VF_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwQSTab_VdwLJPsw_VF_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwQSTab_VdwLJEwCombGeom_VF_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwQSTab_VdwLJEwCombLB_VF_opencl" },
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:    { "nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJ_VF_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJCombGeom_VF_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJCombLB_VF_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJFsw_VF_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJPsw_VF_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJEwCombGeom_VF_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJEwCombLB_VF_opencl" },
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:    { "nbnxn_kernel_ElecEw_VdwLJ_VF_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEw_VdwLJCombGeom_VF_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEw_VdwLJCombLB_VF_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEw_VdwLJFsw_VF_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEw_VdwLJPsw_VF_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEw_VdwLJEwCombGeom_VF_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEw_VdwLJEwCombLB_VF_opencl" },
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:    { "nbnxn_kernel_ElecEwTwinCut_VdwLJ_VF_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwTwinCut_VdwLJCombGeom_VF_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwTwinCut_VdwLJCombLB_VF_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwTwinCut_VdwLJFsw_VF_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwTwinCut_VdwLJPsw_VF_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwTwinCut_VdwLJEwCombGeom_VF_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwTwinCut_VdwLJEwCombLB_VF_opencl" }
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:    { "nbnxn_kernel_ElecCut_VdwLJ_F_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecCut_VdwLJCombGeom_F_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecCut_VdwLJCombLB_F_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecCut_VdwLJFsw_F_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecCut_VdwLJPsw_F_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecCut_VdwLJEwCombGeom_F_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecCut_VdwLJEwCombLB_F_prune_opencl" },
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:    { "nbnxn_kernel_ElecRF_VdwLJ_F_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecRF_VdwLJCombGeom_F_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecRF_VdwLJCombLB_F_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecRF_VdwLJFsw_F_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecRF_VdwLJPsw_F_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecRF_VdwLJEwCombGeom_F_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecRF_VdwLJEwCombLB_F_prune_opencl" },
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:    { "nbnxn_kernel_ElecEwQSTab_VdwLJ_F_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwQSTab_VdwLJCombGeom_F_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwQSTab_VdwLJCombLB_F_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwQSTab_VdwLJFsw_F_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwQSTab_VdwLJPsw_F_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwQSTab_VdwLJEwCombGeom_F_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwQSTab_VdwLJEwCombLB_F_prune_opencl" },
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:    { "nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJ_F_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJCombGeom_F_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJCombLB_F_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJFsw_F_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJPsw_F_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJEwCombGeom_F_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJEwCombLB_F_prune_opencl" },
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:    { "nbnxn_kernel_ElecEw_VdwLJ_F_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEw_VdwLJCombGeom_F_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEw_VdwLJCombLB_F_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEw_VdwLJFsw_F_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEw_VdwLJPsw_F_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEw_VdwLJEwCombGeom_F_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEw_VdwLJEwCombLB_F_prune_opencl" },
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:    { "nbnxn_kernel_ElecEwTwinCut_VdwLJ_F_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwTwinCut_VdwLJCombGeom_F_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwTwinCut_VdwLJCombLB_F_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwTwinCut_VdwLJFsw_F_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwTwinCut_VdwLJPsw_F_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwTwinCut_VdwLJEwCombGeom_F_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwTwinCut_VdwLJEwCombLB_F_prune_opencl" }
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:    { "nbnxn_kernel_ElecCut_VdwLJ_VF_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecCut_VdwLJCombGeom_VF_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecCut_VdwLJCombLB_VF_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecCut_VdwLJFsw_VF_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecCut_VdwLJPsw_VF_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecCut_VdwLJEwCombGeom_VF_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecCut_VdwLJEwCombLB_VF_prune_opencl" },
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:    { "nbnxn_kernel_ElecRF_VdwLJ_VF_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecRF_VdwLJCombGeom_VF_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecRF_VdwLJCombLB_VF_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecRF_VdwLJFsw_VF_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecRF_VdwLJPsw_VF_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecRF_VdwLJEwCombGeom_VF_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecRF_VdwLJEwCombLB_VF_prune_opencl" },
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:    { "nbnxn_kernel_ElecEwQSTab_VdwLJ_VF_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwQSTab_VdwLJCombGeom_VF_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwQSTab_VdwLJCombLB_VF_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwQSTab_VdwLJFsw_VF_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwQSTab_VdwLJPsw_VF_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwQSTab_VdwLJEwCombGeom_VF_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwQSTab_VdwLJEwCombLB_VF_prune_opencl" },
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:    { "nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJ_VF_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJCombGeom_VF_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJCombLB_VF_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJFsw_VF_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJPsw_VF_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJEwCombGeom_VF_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJEwCombLB_VF_prune_opencl" },
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:    { "nbnxn_kernel_ElecEw_VdwLJ_VF_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEw_VdwLJCombGeom_VF_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEw_VdwLJCombLB_VF_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEw_VdwLJFsw_VF_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEw_VdwLJPsw_VF_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEw_VdwLJEwCombGeom_VF_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEw_VdwLJEwCombLB_VF_prune_opencl" },
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:    { "nbnxn_kernel_ElecEwTwinCut_VdwLJ_VF_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwTwinCut_VdwLJCombGeom_VF_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwTwinCut_VdwLJCombLB_VF_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwTwinCut_VdwLJFsw_VF_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwTwinCut_VdwLJPsw_VF_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwTwinCut_VdwLJEwCombGeom_VF_prune_opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:      "nbnxn_kernel_ElecEwTwinCut_VdwLJEwCombLB_VF_prune_opencl" }
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp: *  OpenCL kernel objects are cached in nb. If the requested kernel is not
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:select_nbnxn_kernel(NbnxmGpu* nb, enum ElecType elecType, enum VdwType vdwType, bool bDoEne, bool bDoPrune)
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:               "The electrostatics type requested is not implemented in the OpenCL kernels.");
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:               "The VdW type requested is not implemented in the OpenCL kernels.");
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:    const int c_clSize           = sc_gpuClusterSize(sc_layoutType);
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:    const int c_superClusterSize = sc_gpuClusterPerSuperCluster(sc_layoutType);
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:    shmem += 2 * sc_gpuJgroupSize(sc_layoutType) * sizeof(int); /* cjs  */
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:/*! \brief Initializes data structures that are going to be sent to the OpenCL device.
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp: *  - OpenCL restrictions (pointers are not accepted inside data structures)
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp: *  - some host side fields are not needed for the OpenCL kernels.
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:static void fillin_ocl_structures(NBParamGpu* nbp, cl_nbparam_params_t* nbparams_params)
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:/*! \brief Launch GPU kernel
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:void gpu_launch_kernel(NbnxmGpu* nb, const gmx::StepWorkload& stepWork, const InteractionLocality iloc)
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:    NBAtomDataGpu*      adat         = nb->atdat;
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:    NBParamGpu*         nbp          = nb->nbparam;
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:    GpuTimers*          timers       = nb->timers;
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:    constexpr int c_clSize           = sc_gpuClusterSize(sc_layoutType);
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:    constexpr int c_superClusterSize = sc_gpuClusterPerSuperCluster(sc_layoutType);
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:        gpu_launch_kernel_pruneonly(nb, iloc, 1);
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:        /* Don't launch an empty local kernel (is not allowed with OpenCL).
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:                "Non-bonded GPU launch configuration:\n\tLocal work size: %zux%zux%zu\n\t"
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:    // The OpenCL kernel takes int as second to last argument because bool is
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:        const auto kernelArgs = prepareGpuKernelArguments(kernel,
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:        launchGpuKernel(kernel, config, deviceStream, timingEvent, kernelName, kernelArgs);
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:        const auto kernelArgs = prepareGpuKernelArguments(kernel,
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:        launchGpuKernel(kernel, config, deviceStream, timingEvent, kernelName, kernelArgs);
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp: *  Note that for the sake of simplicity we use the CUDA terminology "shared memory"
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp: *  for OpenCL local memory.
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:    const int c_clSize           = sc_gpuClusterSize(sc_layoutType);
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:    const int c_superClusterSize = sc_gpuClusterPerSuperCluster(sc_layoutType);
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:    const int c_clusterPairSplit = sc_gpuClusterPairSplit(sc_layoutType);
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:    shmem += num_threads_z * c_clusterPairSplit * sc_gpuJgroupSize(sc_layoutType) * sizeof(int);
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:void gpu_launch_kernel_pruneonly(NbnxmGpu* nb, const InteractionLocality iloc, const int numParts)
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:    NBAtomDataGpu*      adat               = nb->atdat;
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:    NBParamGpu*         nbp                = nb->nbparam;
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:    GpuTimers*          timers             = nb->timers;
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:    constexpr int       c_clSize           = sc_gpuClusterSize(sc_layoutType);
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:    constexpr int       c_superClusterSize = sc_gpuClusterPerSuperCluster(sc_layoutType);
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:    GpuRegionTimer* timer = nullptr;
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:                "Pruning GPU kernel launch configuration:\n\tLocal work size: %zux%zux%zu\n\t"
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:    const auto     kernelArgs   = prepareGpuKernelArguments(pruneKernel,
src/gromacs/nbnxm/opencl/nbnxm_ocl.cpp:    launchGpuKernel(pruneKernel, config, deviceStream, timingEvent, kernelName, kernelArgs);
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_pruneonly.clh: *  \brief OpenCL pruning kernel.
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_pruneonly.clh: *  OpenCL 1.2 support is expected; tested on AMD GCN and NVIDIA CC >3.0.
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_pruneonly.clh:nbnxn_kernel_prune_opencl
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_pruneonly.clh:nbnxn_kernel_prune_rolling_opencl
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_pruneonly.clh:    const int c_nbnxnGpuClusterpairSplit = 2;
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_pruneonly.clh:    /*! i-cluster interaction mask for a super-cluster with all c_nbnxnGpuNumClusterPerSupercluster=8 bits set */
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_pruneonly.clh:    const unsigned superClInteractionMask = ((1U << c_nbnxnGpuNumClusterPerSupercluster) - 1U);
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_pruneonly.clh:#define LOCAL_OFFSET (xib + c_nbnxnGpuNumClusterPerSupercluster * c_clSize)
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_pruneonly.clh:    cjs = (((__local int*)(LOCAL_OFFSET)) + tidxz * c_nbnxnGpuClusterpairSplit * c_nbnxnGpuJgroupSize);
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_pruneonly.clh:        (((__local int*)(xib + c_nbnxnGpuNumClusterPerSupercluster * c_clSize)) \
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_pruneonly.clh:         + (NTHREAD_Z * c_nbnxnGpuClusterpairSplit * c_nbnxnGpuJgroupSize))
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_pruneonly.clh:    /* Local buffer used to implement __any warp vote function from CUDA.
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_pruneonly.clh:    if (tidx % (c_clSize * c_clSize / c_nbnxnGpuClusterpairSplit) == 0)
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_pruneonly.clh:    const bool c_loadUsingAllXYThreads = (c_clSize == c_nbnxnGpuNumClusterPerSupercluster);
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_pruneonly.clh:    if (tidxz == 0 && (c_loadUsingAllXYThreads || tidxj < c_nbnxnGpuNumClusterPerSupercluster))
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_pruneonly.clh:        for (int i = 0; i < c_nbnxnGpuNumClusterPerSupercluster; i += CL_SIZE)
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_pruneonly.clh:            const int ci = sci * c_nbnxnGpuNumClusterPerSupercluster + tidxj + i;
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_pruneonly.clh:            imaskFull = prePrunedImask[jPacked * c_nbnxnGpuClusterpairSplit + widx];
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_pruneonly.clh:#pragma unroll c_nbnxnGpuJgroupSize
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_pruneonly.clh:            for (int jm = 0; jm < c_nbnxnGpuJgroupSize; jm++)
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_pruneonly.clh:                if (imaskCheck & (superClInteractionMask << (jm * c_nbnxnGpuNumClusterPerSupercluster)))
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_pruneonly.clh:                    unsigned int mask_ji = (1U << (jm * c_nbnxnGpuNumClusterPerSupercluster));
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_pruneonly.clh:#pragma unroll c_nbnxnGpuNumClusterPerSupercluster
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_pruneonly.clh:                    for (int i = 0; i < c_nbnxnGpuNumClusterPerSupercluster; i++)
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_pruneonly.clh:                prePrunedImask[jPacked * c_nbnxnGpuClusterpairSplit + widx] = imaskFull;
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_utils.clh: *  Utility constant and function declaration for the OpenCL non-bonded kernels.
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_utils.clh:#include "gromacs/gpu_utils/device_utils.clh"
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_utils.clh:#include "gromacs/gpu_utils/vectype_ops.clh"
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_utils.clh:#define CL_SIZE (c_nbnxnGpuClusterSize)
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_utils.clh:#define WARP_SIZE (CL_SIZE * CL_SIZE / 2) // Currently only c_nbnxnGpuClusterpairSplit=2 supported
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_utils.clh:#if defined _NVIDIA_SOURCE_ || defined _AMD_SOURCE_
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_utils.clh:/* Currently we enable CJ prefetch for AMD/NVIDIA and disable it for other vendors
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_utils.clh:#if defined cl_intel_subgroups || defined cl_khr_subgroups || defined __opencl_c_subgroups \
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_utils.clh:        || (defined __OPENCL_VERSION__ && __OPENCL_VERSION__ >= 210 && __OPENCL_VERSION__ < 300)
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_utils.clh:#ifndef NBNXN_OPENCL_KERNEL_UTILS_CLH
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_utils.clh:#    define NBNXN_OPENCL_KERNEL_UTILS_CLH
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_utils.clh:// Data structures shared between OpenCL device code and OpenCL host code
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_utils.clh:// Data structure shared between the OpenCL device code and OpenCL host code
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_utils.clh:// Must not contain OpenCL objects (buffers)
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_utils.clh:/*! i-cluster interaction mask for a super-cluster with all c_nbnxnGpuNumClusterPerSupercluster bits set */
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_utils.clh:__constant const unsigned supercl_interaction_mask = ((1U << c_nbnxnGpuNumClusterPerSupercluster) - 1U);
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_utils.clh:gmx_opencl_inline void preloadCjPackedGeneric(__local int*        sm_cjPreload,
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_utils.clh:#    if defined _AMD_SOURCE_ // TODO: fix by setting c_nbnxnGpuClusterpairSplit properly
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_utils.clh:    if (tidxj == 0 & tidxi < c_nbnxnGpuJgroupSize)
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_utils.clh:    const int c_nbnxnGpuClusterpairSplit = 2;
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_utils.clh:    const int c_splitClSize              = c_clSize / c_nbnxnGpuClusterpairSplit;
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_utils.clh:    if ((tidxj == 0 | tidxj == c_splitClSize) & (tidxi < c_nbnxnGpuJgroupSize))
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_utils.clh:        sm_cjPreload[tidxi + tidxj * c_nbnxnGpuJgroupSize / c_splitClSize] = gm_cj[tidxi];
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_utils.clh:gmx_opencl_inline int preloadCjPackedSubgroup(const __global int* gm_cj)
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_utils.clh: * - For NVIDIA and Apple once per warp (on 2x4 threads * NTHREAD_Z)
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_utils.clh:gmx_opencl_inline void preloadCjPacked(CjType gmx_unused*             cjs,
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_utils.clh:gmx_opencl_inline int loadCjPreload(__local int* sm_cjPreload, int jm, int gmx_unused tidxi, int gmx_unused tidxj)
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_utils.clh:    int warpLoadOffset = 0; // TODO: fix by setting c_nbnxnGpuClusterpairSplit properly
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_utils.clh:    const int c_nbnxnGpuClusterpairSplit = 2;
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_utils.clh:    const int c_splitClSize              = c_clSize / c_nbnxnGpuClusterpairSplit;
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_utils.clh:    int       warpLoadOffset = (tidxj & c_splitClSize) * c_nbnxnGpuJgroupSize / c_splitClSize;
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_utils.clh:gmx_opencl_inline int
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_utils.clh:gmx_opencl_inline float2 convert_sigma_epsilon_to_c6_c12(const float sigma, const float epsilon)
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_utils.clh:gmx_opencl_inline void calculate_force_switch_F(const cl_nbparam_params_t* nbparam,
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_utils.clh:gmx_opencl_inline void calculate_force_switch_F_E(const cl_nbparam_params_t* nbparam,
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_utils.clh:gmx_opencl_inline void calculate_potential_switch_F(const cl_nbparam_params_t* nbparam,
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_utils.clh:gmx_opencl_inline void calculate_potential_switch_F_E(const cl_nbparam_params_t* nbparam,
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_utils.clh:gmx_opencl_inline float calculate_lj_ewald_c6grid(__constant const float2* nbfp_comb, int typei, int typej)
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_utils.clh:gmx_opencl_inline void calculate_lj_ewald_comb_geom_F(__constant const float2* nbfp_comb,
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_utils.clh:gmx_opencl_inline void calculate_lj_ewald_comb_geom_F_E(__constant const float2*   nbfp_comb,
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_utils.clh:gmx_opencl_inline void calculate_lj_ewald_comb_LB_F_E(__constant const float2*   nbfp_comb,
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_utils.clh:gmx_opencl_inline float interpolate_coulomb_force_r(__constant const float* coulomb_tab, float r, float scale)
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_utils.clh:gmx_opencl_inline float pmecorrF(float z2)
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_utils.clh:gmx_opencl_inline void
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_utils.clh:     * for CL_SIZE>4. See CUDA code for required code */
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_utils.clh:gmx_opencl_inline void
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_utils.clh:gmx_opencl_inline void reduce_force_j(__local float gmx_unused* f_buf,
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_utils.clh:gmx_opencl_inline void reduce_force_i_and_shift_shfl(__private fvec  fci_buf[],
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_utils.clh:    for (int ci_offset = 0; ci_offset < c_nbnxnGpuNumClusterPerSupercluster; ci_offset++)
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_utils.clh:        int    aidx = (sci * c_nbnxnGpuNumClusterPerSupercluster + ci_offset) * CL_SIZE + tidxi;
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_utils.clh:gmx_opencl_inline void reduce_force_i_and_shift_pow2(volatile __local float* f_buf,
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_utils.clh:    for (int ci_offset = 0; ci_offset < c_nbnxnGpuNumClusterPerSupercluster; ci_offset++)
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_utils.clh:        int aidx = (sci * c_nbnxnGpuNumClusterPerSupercluster + ci_offset) * CL_SIZE + tidxi;
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_utils.clh:         * TODO: Test on Nvidia for performance difference between having the barrier here or after the atomicAdd
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_utils.clh:gmx_opencl_inline void reduce_force_i_and_shift(__local float gmx_unused* f_buf,
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_utils.clh:gmx_opencl_inline void reduce_energy_shfl(float                    E_lj,
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_utils.clh:     * (by supporting c_nbnxnGpuClusterpairSplit=1). */
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_utils.clh:gmx_opencl_inline void reduce_energy_pow2(volatile __local float*  buf,
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_utils.clh:gmx_opencl_inline void reduce_energy(volatile __local float gmx_unused* buf,
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_utils.clh:gmx_opencl_inline bool gmx_sub_group_any_localmem(volatile __local int* warp_any, int widx, bool pred)
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_utils.clh:gmx_opencl_inline bool gmx_sub_group_any(volatile __local int gmx_unused* warp_any, int gmx_unused widx, bool pred)
src/gromacs/nbnxm/opencl/nbnxm_ocl_kernel_utils.clh:#endif /* NBNXN_OPENCL_KERNEL_UTILS_CLH */
src/gromacs/nbnxm/opencl/nbnxm_ocl_jit_support.cpp: *  \brief Defines functions that support JIT compilation (e.g. for OpenCL)
src/gromacs/nbnxm/opencl/nbnxm_ocl_jit_support.cpp:#include "gromacs/gpu_utils/gpu_utils.h"
src/gromacs/nbnxm/opencl/nbnxm_ocl_jit_support.cpp:#include "gromacs/gpu_utils/ocl_compiler.h"
src/gromacs/nbnxm/opencl/nbnxm_ocl_jit_support.cpp:#include "gromacs/nbnxm/gpu_jit_support.h"
src/gromacs/nbnxm/opencl/nbnxm_ocl_jit_support.cpp:#include "gromacs/nbnxm/nbnxm_gpu.h"
src/gromacs/nbnxm/opencl/nbnxm_ocl_jit_support.cpp:/*! \brief Compiles nbnxn kernels for OpenCL GPU given by \p mygpu
src/gromacs/nbnxm/opencl/nbnxm_ocl_jit_support.cpp: * With OpenCL, a call to this function must not precede nbnxn_gpu_init() (which also calls it).
src/gromacs/nbnxm/opencl/nbnxm_ocl_jit_support.cpp: * OpenCL kernels are compiled.
src/gromacs/nbnxm/opencl/nbnxm_ocl_jit_support.cpp: * \param[inout] nb  Manages OpenCL non-bonded calculations; compiled kernels returned in deviceInfo members
src/gromacs/nbnxm/opencl/nbnxm_ocl_jit_support.cpp:void nbnxn_gpu_compile_kernels(NbnxmGpu* nb)
src/gromacs/nbnxm/opencl/nbnxm_ocl_jit_support.cpp:         * in include files outside the opencl as macros, to avoid
src/gromacs/nbnxm/opencl/nbnxm_ocl_jit_support.cpp:                " -Dc_nbnxnGpuClusterSize=%d"
src/gromacs/nbnxm/opencl/nbnxm_ocl_jit_support.cpp:                " -Dc_nbnxnGpuNumClusterPerSupercluster=%d"
src/gromacs/nbnxm/opencl/nbnxm_ocl_jit_support.cpp:                " -Dc_nbnxnGpuJgroupSize=%d"
src/gromacs/nbnxm/opencl/nbnxm_ocl_jit_support.cpp:                sc_gpuClusterSize(sc_layoutType),
src/gromacs/nbnxm/opencl/nbnxm_ocl_jit_support.cpp:                sc_gpuClusterPerSuperCluster(sc_layoutType),
src/gromacs/nbnxm/opencl/nbnxm_ocl_jit_support.cpp:                sc_gpuJgroupSize(sc_layoutType),
src/gromacs/nbnxm/opencl/nbnxm_ocl_jit_support.cpp:                                               "gromacs/nbnxm/opencl",
src/gromacs/nbnxm/opencl/nbnxm_ocl_jit_support.cpp:                    gmx::formatString("Failed to compile/load nbnxm kernels for GPU #%d %s\n",
src/gromacs/nbnxm/opencl/CMakeLists.txt:if(GMX_GPU_OPENCL)
src/gromacs/nbnxm/opencl/CMakeLists.txt:    file(GLOB OPENCL_NB_SOURCES *.cpp)
src/gromacs/nbnxm/opencl/CMakeLists.txt:    set(NBNXM_SOURCES ${NBNXM_SOURCES} ${OPENCL_NB_SOURCES} PARENT_SCOPE)
src/gromacs/nbnxm/opencl/CMakeLists.txt:   # TODO: remove readability-isolate-declaration when the nbnxm OpenCL kernels get modernized
src/gromacs/nbnxm/opencl/CMakeLists.txt:        foreach(VENDOR AMD NVIDIA INTEL)
src/gromacs/nbnxm/opencl/CMakeLists.txt:                -Dc_nbnxnGpuClusterSize=${CLUSTER_SIZE}
src/gromacs/nbnxm/opencl/CMakeLists.txt:                -Dc_nbnxnGpuNumClusterPerSupercluster=8
src/gromacs/nbnxm/opencl/CMakeLists.txt:                -Dc_nbnxnGpuJgroupSize=4
src/gromacs/nbnxm/opencl/nbnxm_gpu_buffer_ops_internal_ocl.cpp: *  \brief Define OpenCL implementation for transforming position coordinates from rvec to nbnxm layout.
src/gromacs/nbnxm/opencl/nbnxm_gpu_buffer_ops_internal_ocl.cpp:#include "gromacs/gpu_utils/device_stream.h"
src/gromacs/nbnxm/opencl/nbnxm_gpu_buffer_ops_internal_ocl.cpp:#include "gromacs/gpu_utils/devicebuffer_datatype.h"
src/gromacs/nbnxm/opencl/nbnxm_gpu_buffer_ops_internal_ocl.cpp:#include "gromacs/nbnxm/nbnxm_gpu.h"
src/gromacs/nbnxm/opencl/nbnxm_gpu_buffer_ops_internal_ocl.cpp:#include "gromacs/nbnxm/nbnxm_gpu_buffer_ops_internal.h"
src/gromacs/nbnxm/opencl/nbnxm_gpu_buffer_ops_internal_ocl.cpp:                                     NbnxmGpu* /* nb */,
src/gromacs/nbnxm/opencl/nbnxm_gpu_buffer_ops_internal_ocl.cpp:    GMX_RELEASE_ASSERT(false, "NBNXM buffer ops are not supported with OpenCL");
src/gromacs/nbnxm/opencl/nbnxm_ocl_consts.h: * Declares constants for OpenCL code
src/gromacs/nbnxm/opencl/nbnxm_ocl_data_mgmt.cpp: *  \brief Define OpenCL implementation of nbnxm_gpu_data_mgmt.h
src/gromacs/nbnxm/opencl/nbnxm_ocl_data_mgmt.cpp:#include "gromacs/nbnxm/gpu_data_mgmt.h"
src/gromacs/nbnxm/opencl/nbnxm_ocl_data_mgmt.cpp:#include "gromacs/nbnxm/gpu_jit_support.h"
src/gromacs/nbnxm/opencl/nbnxm_ocl_data_mgmt.cpp:#include "gromacs/nbnxm/nbnxm_gpu.h"
src/gromacs/nbnxm/opencl/nbnxm_ocl_data_mgmt.cpp:#include "gromacs/nbnxm/nbnxm_gpu_data_mgmt.h"
src/gromacs/nbnxm/opencl/nbnxm_ocl_data_mgmt.cpp:#include "gromacs/timing/gpu_timing.h"
src/gromacs/nbnxm/opencl/nbnxm_ocl_data_mgmt.cpp: * (and about 5% faster than 40, which is the default for CUDA
src/gromacs/nbnxm/opencl/nbnxm_ocl_data_mgmt.cpp:static unsigned int gpu_min_ci_balanced_factor = 50;
src/gromacs/nbnxm/opencl/nbnxm_ocl_data_mgmt.cpp:/*! \brief Initializes the OpenCL kernel pointers of the nbnxn_ocl_ptr_t input data structure. */
src/gromacs/nbnxm/opencl/nbnxm_ocl_data_mgmt.cpp:static cl_kernel nbnxn_gpu_create_kernel(NbnxmGpu* nb, const char* kernel_name)
src/gromacs/nbnxm/opencl/nbnxm_ocl_data_mgmt.cpp:                  "Failed to create kernel '%s' for GPU #%s: OpenCL error %d",
src/gromacs/nbnxm/opencl/nbnxm_ocl_data_mgmt.cpp:/*! \brief Initializes the OpenCL kernel pointers of the nbnxn_ocl_ptr_t input data structure. */
src/gromacs/nbnxm/opencl/nbnxm_ocl_data_mgmt.cpp:static void nbnxn_gpu_init_kernels(NbnxmGpu* nb)
src/gromacs/nbnxm/opencl/nbnxm_ocl_data_mgmt.cpp:    nb->kernel_pruneonly[epruneFirst] = nbnxn_gpu_create_kernel(nb, "nbnxn_kernel_prune_opencl");
src/gromacs/nbnxm/opencl/nbnxm_ocl_data_mgmt.cpp:            nbnxn_gpu_create_kernel(nb, "nbnxn_kernel_prune_rolling_opencl");
src/gromacs/nbnxm/opencl/nbnxm_ocl_data_mgmt.cpp:void gpu_init_platform_specific(NbnxmGpu* nb)
src/gromacs/nbnxm/opencl/nbnxm_ocl_data_mgmt.cpp:    /* set device info, just point it to the right GPU among the detected ones */
src/gromacs/nbnxm/opencl/nbnxm_ocl_data_mgmt.cpp:     * TODO: decide about NVIDIA
src/gromacs/nbnxm/opencl/nbnxm_ocl_data_mgmt.cpp:    /* NOTE: in CUDA we pick L1 cache configuration for the nbnxn kernels here,
src/gromacs/nbnxm/opencl/nbnxm_ocl_data_mgmt.cpp:     * but sadly this is not supported in OpenCL (yet?). Consider adding it if
src/gromacs/nbnxm/opencl/nbnxm_ocl_data_mgmt.cpp:    nbnxn_gpu_compile_kernels(nb);
src/gromacs/nbnxm/opencl/nbnxm_ocl_data_mgmt.cpp:    nbnxn_gpu_init_kernels(nb);
src/gromacs/nbnxm/opencl/nbnxm_ocl_data_mgmt.cpp:/*! \brief Releases an OpenCL kernel pointer */
src/gromacs/nbnxm/opencl/nbnxm_ocl_data_mgmt.cpp:/*! \brief Releases a list of OpenCL kernel pointers */
src/gromacs/nbnxm/opencl/nbnxm_ocl_data_mgmt.cpp:/*! \brief Free the OpenCL program.
src/gromacs/nbnxm/opencl/nbnxm_ocl_data_mgmt.cpp: *  The function releases the OpenCL program assuciated with the
src/gromacs/nbnxm/opencl/nbnxm_ocl_data_mgmt.cpp: *  \param program [in]  OpenCL program to release.
src/gromacs/nbnxm/opencl/nbnxm_ocl_data_mgmt.cpp:static void freeGpuProgram(cl_program program)
src/gromacs/nbnxm/opencl/nbnxm_ocl_data_mgmt.cpp:void gpu_free_platform_specific(NbnxmGpu* nb)
src/gromacs/nbnxm/opencl/nbnxm_ocl_data_mgmt.cpp:    freeGpuProgram(nb->dev_rundata->program);
src/gromacs/nbnxm/opencl/nbnxm_ocl_data_mgmt.cpp:int gpu_min_ci_balanced(NbnxmGpu* nb)
src/gromacs/nbnxm/opencl/nbnxm_ocl_data_mgmt.cpp:        return gpu_min_ci_balanced_factor * nb->deviceContext_->deviceInfo().compute_units
src/gromacs/nbnxm/pairlistwork.h: * \brief Declares working data structures for the CPU and GPU pairlists
src/gromacs/nbnxm/pairlistwork.h:#include "gromacs/gpu_utils/hostallocator.h"
src/gromacs/nbnxm/pairlistwork.h:struct NbnxmPairlistGpuWork
src/gromacs/nbnxm/pairlistwork.h:    NbnxmPairlistGpuWork(PairlistType layoutType);
src/gromacs/nbnxm/nbnxm.cpp:#include "nbnxm_gpu.h"
src/gromacs/nbnxm/nbnxm.cpp:bool nonbonded_verlet_t::isDynamicPruningStepGpu(int64_t step) const
src/gromacs/nbnxm/nbnxm.cpp:    return pairlistSets_->isDynamicPruningStepGpu(step);
src/gromacs/nbnxm/nbnxm.cpp:void nonbonded_verlet_t::convertCoordinatesGpu(const AtomLocality    locality,
src/gromacs/nbnxm/nbnxm.cpp:                                               GpuEventSynchronizer* xReadyOnDevice)
src/gromacs/nbnxm/nbnxm.cpp:    wallcycle_start(wcycle_, WallCycleCounter::LaunchGpuPp);
src/gromacs/nbnxm/nbnxm.cpp:    wallcycle_sub_start(wcycle_, WallCycleSubCounter::LaunchGpuNBXBufOps);
src/gromacs/nbnxm/nbnxm.cpp:    nbnxn_atomdata_x_to_nbat_x_gpu(pairSearch_->gridSet(), locality, gpuNbv_, d_x, xReadyOnDevice);
src/gromacs/nbnxm/nbnxm.cpp:    wallcycle_sub_stop(wcycle_, WallCycleSubCounter::LaunchGpuNBXBufOps);
src/gromacs/nbnxm/nbnxm.cpp:    wallcycle_stop(wcycle_, WallCycleCounter::LaunchGpuPp);
src/gromacs/nbnxm/nbnxm.cpp:    /* Skip the reduction if there was no short-range GPU work to do
src/gromacs/nbnxm/nbnxm.cpp:    if (!pairlistIsSimple() && !haveGpuShortRangeWork(gpuNbv_, atomToInteractionLocality(locality)))
src/gromacs/nbnxm/nbnxm.cpp:void nonbonded_verlet_t::setupGpuShortRangeWork(const ListedForcesGpu*    listedForcesGpu,
src/gromacs/nbnxm/nbnxm.cpp:    if (useGpu() && !emulateGpu())
src/gromacs/nbnxm/nbnxm.cpp:        setupGpuShortRangeWorkLow(gpuNbv_, listedForcesGpu, iLocality);
src/gromacs/nbnxm/nbnxm.cpp:void nonbonded_verlet_t::atomdata_init_copy_x_to_nbat_x_gpu() const
src/gromacs/nbnxm/nbnxm.cpp:    nbnxn_gpu_init_x_to_nbat_x(pairSearch_->gridSet(), gpuNbv_);
src/gromacs/nbnxm/nbnxm.cpp:bool buildSupportsNonbondedOnGpu(std::string* error)
src/gromacs/nbnxm/nbnxm.cpp:    errorReasons.startContext("Nonbonded interactions on GPUs are not supported in:");
src/gromacs/nbnxm/nbnxm.cpp:    errorReasons.appendIf(!GMX_GPU, "Non-GPU build of GROMACS.");
src/gromacs/nbnxm/nbnxm_kernel_utils.h:#ifndef GMX_GPU_UTILS_NBNXM_KERNEL_UTILS_H
src/gromacs/nbnxm/nbnxm_kernel_utils.h:#define GMX_GPU_UTILS_NBNXM_KERNEL_UTILS_H
src/gromacs/nbnxm/nbnxm_kernel_utils.h: *  NBNXM GPU kernel utility methods
src/gromacs/nbnxm/nbnxm_kernel_utils.h: *  \ingroup module_gpu_utils
src/gromacs/nbnxm/nbnxm_kernel_utils.h:#include "gromacs/gpu_utils/gpu_kernel_utils.h"
src/gromacs/nbnxm/nbnxm_kernel_utils.h:#include "gromacs/nbnxm/gpu_types_common.h"
src/gromacs/nbnxm/nbnxm_kernel_utils.h:    const float rSwitch = gmxGpuFDim(r, rVdwSwitch); // max(r - rVdwSwitch, 0)
src/gromacs/nbnxm/nbnxm_kernel_utils.h:    const float expmcr2   = gmxGpuExp(-cr2);
src/gromacs/nbnxm/nbnxm_kernel_utils.h:#if (defined(__SYCL_DEVICE_ONLY__) && defined(__AMDGCN__)) || GMX_GPU_HIP
src/gromacs/nbnxm/nbnxm_kernel_utils.h:    // TODO: up to ROCm v5.3 compiler does not do this transformation. Remove when this is no longer the case.
src/gromacs/nbnxm/nbnxm_geometry.cpp:real nbnxmPairlistVolumeRadiusIncrease(const bool useGpu, const real atomDensity)
src/gromacs/nbnxm/nbnxm_geometry.cpp:    if (useGpu)
src/gromacs/nbnxm/nbnxm_geometry.cpp:        iClusterSize = sc_gpuClusterSize(PairlistType::Hierarchical8x8x8);
src/gromacs/nbnxm/nbnxm_geometry.cpp:        jClusterSize = sc_gpuSplitJClusterSize(PairlistType::Hierarchical8x8x8);
src/gromacs/nbnxm/nbnxm_gpu_buffer_ops.cpp: *  Common code for GPU buffer operations, namely the coordinate layout conversion
src/gromacs/nbnxm/nbnxm_gpu_buffer_ops.cpp:#include "gromacs/gpu_utils/device_stream.h"
src/gromacs/nbnxm/nbnxm_gpu_buffer_ops.cpp:#include "gromacs/gpu_utils/gpueventsynchronizer.h"
src/gromacs/nbnxm/nbnxm_gpu_buffer_ops.cpp:#include "gromacs/nbnxm/nbnxm_gpu.h"
src/gromacs/nbnxm/nbnxm_gpu_buffer_ops.cpp:#include "gromacs/nbnxm/nbnxm_gpu_buffer_ops_internal.h"
src/gromacs/nbnxm/nbnxm_gpu_buffer_ops.cpp:#if GMX_GPU_CUDA
src/gromacs/nbnxm/nbnxm_gpu_buffer_ops.cpp:#    include "gromacs/nbnxm/cuda/nbnxm_cuda_types.h"
src/gromacs/nbnxm/nbnxm_gpu_buffer_ops.cpp:#elif GMX_GPU_OPENCL
src/gromacs/nbnxm/nbnxm_gpu_buffer_ops.cpp:#    include "gromacs/nbnxm/opencl/nbnxm_ocl_types.h"
src/gromacs/nbnxm/nbnxm_gpu_buffer_ops.cpp:#elif GMX_GPU_SYCL
src/gromacs/nbnxm/nbnxm_gpu_buffer_ops.cpp:#elif GMX_GPU_HIP
src/gromacs/nbnxm/nbnxm_gpu_buffer_ops.cpp:void nbnxn_gpu_x_to_nbat_x(const Grid&           grid,
src/gromacs/nbnxm/nbnxm_gpu_buffer_ops.cpp:                           NbnxmGpu*             nb,
src/gromacs/nbnxm/nbnxm_gpu_buffer_ops.cpp:                           GpuEventSynchronizer* xReadyOnDevice,
src/gromacs/nbnxm/nbnxm_gpu_buffer_ops.cpp:    GMX_ASSERT(bool(GMX_GPU_CUDA) || bool(GMX_GPU_SYCL) || bool(GMX_GPU_HIP),
src/gromacs/nbnxm/nbnxm_gpu_buffer_ops.cpp:               "NBNXM X buffer operations only supported in CUDA, SYCL and HIP");
src/gromacs/nbnxm/nbnxm_gpu_buffer_ops.cpp:    GMX_ASSERT(nb, "Need a valid nbnxn_gpu object");
src/gromacs/nbnxm/nbnxm_gpu_buffer_ops.cpp:        nbnxnInsertNonlocalGpuDependency(nb, interactionLoc);
src/gromacs/nbnxm/CMakeLists.txt:    kernels_reference/kernel_gpu_ref.cpp
src/gromacs/nbnxm/CMakeLists.txt:if(GMX_GPU_CUDA)
src/gromacs/nbnxm/CMakeLists.txt:    add_subdirectory(cuda)
src/gromacs/nbnxm/CMakeLists.txt:    gmx_add_libgromacs_sources(nbnxm_gpu_data_mgmt.cpp nbnxm_gpu_buffer_ops.cpp)
src/gromacs/nbnxm/CMakeLists.txt:    _gmx_add_files_to_property(CUDA_SOURCES
src/gromacs/nbnxm/CMakeLists.txt:        nbnxm_gpu_data_mgmt.cpp
src/gromacs/nbnxm/CMakeLists.txt:        nbnxm_gpu_buffer_ops.cpp
src/gromacs/nbnxm/CMakeLists.txt:if(GMX_GPU_OPENCL)
src/gromacs/nbnxm/CMakeLists.txt:    add_subdirectory(opencl)
src/gromacs/nbnxm/CMakeLists.txt:    set(NBNXM_OPENCL_KERNELS ${NBNXM_OPENCL_KERNELS} PARENT_SCOPE)
src/gromacs/nbnxm/CMakeLists.txt:    gmx_add_libgromacs_sources(nbnxm_gpu_data_mgmt.cpp nbnxm_gpu_buffer_ops.cpp)
src/gromacs/nbnxm/CMakeLists.txt:if(GMX_GPU_SYCL)
src/gromacs/nbnxm/CMakeLists.txt:    gmx_add_libgromacs_sources(nbnxm_gpu_data_mgmt.cpp nbnxm_gpu_buffer_ops.cpp)
src/gromacs/nbnxm/CMakeLists.txt:    _gmx_add_files_to_property(SYCL_SOURCES nbnxm_gpu_data_mgmt.cpp nbnxm_gpu_buffer_ops.cpp)
src/gromacs/nbnxm/CMakeLists.txt:if(GMX_GPU_HIP)
src/gromacs/nbnxm/CMakeLists.txt:    gmx_add_libgromacs_sources(nbnxm_gpu_data_mgmt.cpp nbnxm_gpu_buffer_ops.cpp)
src/gromacs/nbnxm/CMakeLists.txt:    _gmx_add_files_to_property(HIP_SOURCES nbnxm_gpu_data_mgmt.cpp nbnxm_gpu_buffer_ops.cpp)
src/gromacs/nbnxm/CMakeLists.txt:        gpu_utils
src/gromacs/nbnxm/pairlist_tuning.cpp: * increasing nstlist a lot. In parallel the MPI and CPU-GPU communication
src/gromacs/nbnxm/pairlist_tuning.cpp: * With GPUs we perform the dynamic pruning in a rolling fashion and this
src/gromacs/nbnxm/pairlist_tuning.cpp:// GPU: pair-search is a factor 1.5-3 slower than the non-bonded kernel.
src/gromacs/nbnxm/pairlist_tuning.cpp://! Target pair-list size increase ratio for GPU
src/gromacs/nbnxm/pairlist_tuning.cpp:static const float c_nbnxnListSizeFactorGPU = 1.4;
src/gromacs/nbnxm/pairlist_tuning.cpp:                     bool              useOrEmulateGpuForNonbondeds,
src/gromacs/nbnxm/pairlist_tuning.cpp:    const char* nstl_gpu =
src/gromacs/nbnxm/pairlist_tuning.cpp:            "\nFor optimal performance with a GPU nstlist (now %d) should be larger.\nThe "
src/gromacs/nbnxm/pairlist_tuning.cpp:            "optimum depends on your CPU and GPU resources.\nYou might want to try several "
src/gromacs/nbnxm/pairlist_tuning.cpp:        /* With a GPU and fixed nstlist suggest tuning nstlist */
src/gromacs/nbnxm/pairlist_tuning.cpp:        if (fp != nullptr && useOrEmulateGpuForNonbondeds && ir->nstlist < nstlist_try[0] * mtsFactor
src/gromacs/nbnxm/pairlist_tuning.cpp:            fprintf(fp, nstl_gpu, ir->nstlist);
src/gromacs/nbnxm/pairlist_tuning.cpp:    if (ir->verletbuf_tol == 0 && useOrEmulateGpuForNonbondeds)
src/gromacs/nbnxm/pairlist_tuning.cpp:                  "You are using an old tpr file with a GPU, please generate a new tpr file with "
src/gromacs/nbnxm/pairlist_tuning.cpp:    const float listfac_ok       = useOrEmulateGpuForNonbondeds ? c_nbnxnListSizeFactorGPU
src/gromacs/nbnxm/pairlist_tuning.cpp:            (useOrEmulateGpuForNonbondeds ? ListSetupType::Gpu : ListSetupType::CpuSimdWhenSupported);
src/gromacs/nbnxm/pairlist_tuning.cpp:    real rlist_inc = nbnxmPairlistVolumeRadiusIncrease(useOrEmulateGpuForNonbondeds, effectiveAtomDensity);
src/gromacs/nbnxm/pairlist_tuning.cpp:            // nstlist tuning happens before GPU DD is initialized so we can't check
src/gromacs/nbnxm/pairlist_tuning.cpp:            // whether the new cutoff would conflict with direct GPU communication.
src/gromacs/nbnxm/pairlist_tuning.cpp:            const bool checkGpuDdLimitation = false;
src/gromacs/nbnxm/pairlist_tuning.cpp:            bDD = change_dd_cutoff(cr, box, ArrayRef<const RVec>(), rlist_new, checkGpuDdLimitation);
src/gromacs/nbnxm/pairlist_tuning.cpp:/*! \brief The interval in steps at which we perform dynamic, rolling pruning on a GPU.
src/gromacs/nbnxm/pairlist_tuning.cpp: * a reasonable compromise that reduces GPU kernel launch overheads and
src/gromacs/nbnxm/pairlist_tuning.cpp: * also avoids inefficiency on large GPUs when pruning small lists.
src/gromacs/nbnxm/pairlist_tuning.cpp: * to be 2, which is indirectly asserted when the GPU pruning is dispatched
src/gromacs/nbnxm/pairlist_tuning.cpp:static constexpr int c_nbnxnGpuRollingListPruningInterval = 2;
src/gromacs/nbnxm/pairlist_tuning.cpp:/*! \brief The minimum nstlist for dynamic pair list pruning om GPUs.
src/gromacs/nbnxm/pairlist_tuning.cpp: * This value should be a multiple of \p c_nbnxnGpuRollingListPruningInterval
src/gromacs/nbnxm/pairlist_tuning.cpp:static constexpr int c_nbnxnGpuDynamicListPruningMinLifetime = 4;
src/gromacs/nbnxm/pairlist_tuning.cpp:    const bool                useGpuList;
src/gromacs/nbnxm/pairlist_tuning.cpp:    const int listLifetime = nstlist - (params.useGpuList ? 0 : params.mtsFactor);
src/gromacs/nbnxm/pairlist_tuning.cpp: * \param[in]     useGpuList  Tells if we are using a GPU type pairlist
src/gromacs/nbnxm/pairlist_tuning.cpp:                                                const bool                 useGpuList,
src/gromacs/nbnxm/pairlist_tuning.cpp:            { mtop, effectiveAtomDensity, inputrec, pressureTolerance, listSetup, useGpuList, mtsFactor });
src/gromacs/nbnxm/pairlist_tuning.cpp:        /* Dynamic pruning on the GPU is performed on the list for
src/gromacs/nbnxm/pairlist_tuning.cpp:        /* On the GPU we apply the dynamic pruning in a rolling fashion
src/gromacs/nbnxm/pairlist_tuning.cpp:         * every c_nbnxnGpuRollingListPruningInterval steps,
src/gromacs/nbnxm/pairlist_tuning.cpp:        tunedNstlistPrune += (useGpuList ? c_nbnxnGpuRollingListPruningInterval : 1) * mtsFactor;
src/gromacs/nbnxm/pairlist_tuning.cpp:     * When pruning on GPU this doesn't matter much as we prune parts every (second) step.
src/gromacs/nbnxm/pairlist_tuning.cpp:    if (!useGpuList)
src/gromacs/nbnxm/pairlist_tuning.cpp:    real rlistInc = nbnxmPairlistVolumeRadiusIncrease(useGpuList, effectiveAtomDensity);
src/gromacs/nbnxm/pairlist_tuning.cpp:     * With the GPU there will probably be more overhead than gain
src/gromacs/nbnxm/pairlist_tuning.cpp:    const bool useGpuList = sc_isGpuPairListType[listParams->pairlistType];
src/gromacs/nbnxm/pairlist_tuning.cpp:            static_assert(c_nbnxnGpuDynamicListPruningMinLifetime % c_nbnxnGpuRollingListPruningInterval == 0,
src/gromacs/nbnxm/pairlist_tuning.cpp:                          "c_nbnxnGpuDynamicListPruningMinLifetime sets the starting value for "
src/gromacs/nbnxm/pairlist_tuning.cpp:            listParams->nstlistPrune = (useGpuList ? c_nbnxnGpuDynamicListPruningMinLifetime
src/gromacs/nbnxm/pairlist_tuning.cpp:                inputrec, mtop, effectiveAtomDensity, useGpuList, ls, userSetNstlistPrune, interactionConst, listParams);
src/gromacs/nbnxm/pairlist_tuning.cpp:        if (listParams->useDynamicPruning && useGpuList)
src/gromacs/nbnxm/pairlist_tuning.cpp:            GMX_RELEASE_ASSERT(listParams->nstlistPrune >= c_nbnxnGpuRollingListPruningInterval,
src/gromacs/nbnxm/pairlist_tuning.cpp:                               ("With dynamic list pruning on GPUs pruning frequency must be at "
src/gromacs/nbnxm/pairlist_tuning.cpp:                                + std::to_string(c_nbnxnGpuRollingListPruningInterval) + ").")
src/gromacs/nbnxm/pairlist_tuning.cpp:                    listParams->nstlistPrune / c_nbnxnGpuRollingListPruningInterval;
src/gromacs/nbnxm/pairlist_tuning.cpp:            int listLifeTime = listParams->nstlistPrune - (useGpuList ? 0 : 1);
src/gromacs/nbnxm/pairlist_tuning.cpp:        const bool useGpuList = sc_isGpuPairListType[listParams.pairlistType];
src/gromacs/nbnxm/pairlist_tuning.cpp:                                                   useGpuList,
src/gromacs/nbnxm/kernel_common.cpp:#include "gromacs/gpu_utils/hostallocator.h"
src/gromacs/nbnxm/gpu_jit_support.h: *  \brief Declares functions that support JIT compilation (e.g. for OpenCL)
src/gromacs/nbnxm/gpu_jit_support.h:#ifndef GMX_NBNXM_GPU_JIT_SUPPORT_H
src/gromacs/nbnxm/gpu_jit_support.h:#define GMX_NBNXM_GPU_JIT_SUPPORT_H
src/gromacs/nbnxm/gpu_jit_support.h:struct NbnxmGpu;
src/gromacs/nbnxm/gpu_jit_support.h:void nbnxn_gpu_compile_kernels(NbnxmGpu gmx_unused* nb);
src/gromacs/nbnxm/prunekerneldispatch.cpp:#include "nbnxm_gpu.h"
src/gromacs/nbnxm/prunekerneldispatch.cpp:void nonbonded_verlet_t::dispatchPruneKernelGpu(int64_t step)
src/gromacs/nbnxm/prunekerneldispatch.cpp:    wallcycle_start_nocount(wcycle_, WallCycleCounter::LaunchGpuPp);
src/gromacs/nbnxm/prunekerneldispatch.cpp:    wallcycle_sub_start_nocount(wcycle_, WallCycleSubCounter::LaunchGpuNonBonded);
src/gromacs/nbnxm/prunekerneldispatch.cpp:    gpu_launch_kernel_pruneonly(gpuNbv_,
src/gromacs/nbnxm/prunekerneldispatch.cpp:    wallcycle_sub_stop(wcycle_, WallCycleSubCounter::LaunchGpuNonBonded);
src/gromacs/nbnxm/prunekerneldispatch.cpp:    wallcycle_stop(wcycle_, WallCycleCounter::LaunchGpuPp);
src/gromacs/nbnxm/atomdata.h:#include "gromacs/gpu_utils/devicebuffer_datatype.h"
src/gromacs/nbnxm/atomdata.h:#include "gromacs/gpu_utils/hostallocator.h"
src/gromacs/nbnxm/atomdata.h:class GpuEventSynchronizer;
src/gromacs/nbnxm/atomdata.h:struct NbnxmGpu;
src/gromacs/nbnxm/atomdata.h:     * \param[in] pinningPolicy    Sets the pinning policy for all buffers used on the GPU
src/gromacs/nbnxm/atomdata.h:         * \param[in] pinningPolicy  Sets the pinning policy for all data that might be transfered to a GPU
src/gromacs/nbnxm/atomdata.h:     *                               to a GPU
src/gromacs/nbnxm/atomdata.h:/*! \brief Transform coordinates to xbat layout on GPU
src/gromacs/nbnxm/atomdata.h: * Creates a GPU copy of the coordinates buffer using short-range ordering.
src/gromacs/nbnxm/atomdata.h: * As input, uses coordinates in plain rvec format in GPU memory.
src/gromacs/nbnxm/atomdata.h: * \param[in,out] gpu_nbv    The NBNXM GPU data structure.
src/gromacs/nbnxm/atomdata.h:void nbnxn_atomdata_x_to_nbat_x_gpu(const GridSet&        gridSet,
src/gromacs/nbnxm/atomdata.h:                                    NbnxmGpu*             gpu_nbv,
src/gromacs/nbnxm/atomdata.h:                                    GpuEventSynchronizer* xReadyOnDevice);
src/gromacs/nbnxm/gridset.cpp:#include "gromacs/gpu_utils/hostallocator.h"
src/gromacs/nbnxm/pairlistparams.cpp:                               const std::optional<PairlistType> gpuPairlistType,
src/gromacs/nbnxm/pairlistparams.cpp:        GMX_RELEASE_ASSERT(gpuPairlistType.has_value(),
src/gromacs/nbnxm/pairlistparams.cpp:                           "Need to have a valid GPU pairlist type at this point");
src/gromacs/nbnxm/pairlistparams.cpp:        pairlistType = gpuPairlistType.value();
src/gromacs/nbnxm/pairlistsets.h:                 int                   minimumIlistCountForGpuBalancing);
src/gromacs/nbnxm/pairlistsets.h:    //! Returns whether step is a dynamic list pruning step, for GPU lists
src/gromacs/nbnxm/pairlistsets.h:    bool isDynamicPruningStepGpu(int64_t step) const
src/gromacs/nbnxm/pairlistsets.h:    //! Pair list balancing parameter for use with GPU
src/gromacs/nbnxm/pairlistsets.h:    int minimumIlistCountForGpuBalancing_;
src/gromacs/nbnxm/clusterdistancekerneltype.h:    Gpu,           //!< For GPU list, can be either plain-C or SIMD
src/gromacs/nbnxm/clusterdistancekerneltype.h:        return ClusterDistanceKernelType::Gpu;
src/gromacs/nbnxm/pairlist.cpp:#include "gromacs/nbnxm/gpu_data_mgmt.h"
src/gromacs/nbnxm/pairlist.cpp:// Whether we use SIMD to compute the distance between a pair of clusters for the GPU pair list
src/gromacs/nbnxm/pairlist.cpp:static constexpr bool c_useSimdGpuClusterPairDistance(const PairlistType layoutType)
src/gromacs/nbnxm/pairlist.cpp:    return (sc_gpuClusterSize(layoutType) >= GMX_SIMD_REAL_WIDTH
src/gromacs/nbnxm/pairlist.cpp:            || (GMX_SIMD4_HAVE_REAL && sc_gpuClusterSize(layoutType) >= 4));
src/gromacs/nbnxm/pairlist.cpp:static inline bool gmx_unused clusterpairInRangePlainC(const NbnxmPairlistGpuWork& work,
src/gromacs/nbnxm/pairlist.cpp:    for (int i = 0; i < sc_gpuClusterSize(layoutType); i++)
src/gromacs/nbnxm/pairlist.cpp:        int i0 = (si * sc_gpuClusterSize(layoutType) + i) * DIM;
src/gromacs/nbnxm/pairlist.cpp:        for (int j = 0; j < sc_gpuClusterSize(layoutType); j++)
src/gromacs/nbnxm/pairlist.cpp:            int j0 = (csj * sc_gpuClusterSize(layoutType) + j) * jCoordStride;
src/gromacs/nbnxm/pairlist.cpp:static inline bool clusterpairInRangeSimd(const NbnxmPairlistGpuWork& work,
src/gromacs/nbnxm/pairlist.cpp:    constexpr int gpuClusterSize = sc_gpuClusterSize(layoutType);
src/gromacs/nbnxm/pairlist.cpp:    static_assert(gpuClusterSize >= simdWidth);
src/gromacs/nbnxm/pairlist.cpp:    constexpr int nR = gpuClusterSize / simdWidth;
src/gromacs/nbnxm/pairlist.cpp:    static_assert(nR * simdWidth == gpuClusterSize,
src/gromacs/nbnxm/pairlist.cpp:                  "The GPU cluster size should be a multiple of the SIMD width");
src/gromacs/nbnxm/pairlist.cpp:    constexpr int iDimStride = gpuClusterSize * DIM;
src/gromacs/nbnxm/pairlist.cpp:        ixV[i] = simdLoad<T>(x_i + si * iDimStride + XX * gpuClusterSize + i * simdWidth);
src/gromacs/nbnxm/pairlist.cpp:        iyV[i] = simdLoad<T>(x_i + si * iDimStride + YY * gpuClusterSize + i * simdWidth);
src/gromacs/nbnxm/pairlist.cpp:        izV[i] = simdLoad<T>(x_i + si * iDimStride + ZZ * gpuClusterSize + i * simdWidth);
src/gromacs/nbnxm/pairlist.cpp:    int j0 = csj * gpuClusterSize;
src/gromacs/nbnxm/pairlist.cpp:    int j1 = j0 + gpuClusterSize - 1;
src/gromacs/nbnxm/pairlist.cpp:// Returns whether any atom between a GPU cluster pair is within range
src/gromacs/nbnxm/pairlist.cpp:static inline bool clusterpairInRange(const NbnxmPairlistGpuWork& work,
src/gromacs/nbnxm/pairlist.cpp:    if constexpr (!c_useSimdGpuClusterPairDistance(layoutType))
src/gromacs/nbnxm/pairlist.cpp:        if constexpr (sc_gpuClusterSize(layoutType) >= GMX_SIMD_REAL_WIDTH)
src/gromacs/nbnxm/pairlist.cpp:NbnxnPairlistGpu::NbnxnPairlistGpu(PinningPolicy pinningPolicy) :
src/gromacs/nbnxm/pairlist.cpp:    na_ci(sc_gpuClusterSize(sc_layoutType)),
src/gromacs/nbnxm/pairlist.cpp:    na_cj(sc_gpuClusterSize(sc_layoutType)),
src/gromacs/nbnxm/pairlist.cpp:    na_sc(sc_gpuNumClusterPerCell(sc_layoutType) * sc_gpuClusterSize(sc_layoutType)),
src/gromacs/nbnxm/pairlist.cpp:    work(std::make_unique<NbnxmPairlistGpuWork>(sc_layoutType))
src/gromacs/nbnxm/pairlist.cpp:    static_assert(sc_gpuClusterPerSuperCluster(sc_layoutType) == sc_gpuNumClusterPerCell(sc_layoutType),
src/gromacs/nbnxm/pairlist.cpp:    static_assert(interactionMaskSize >= sc_gpuJgroupSize(sc_layoutType)
src/gromacs/nbnxm/pairlist.cpp:                                                 * sc_gpuNumClusterPerCell(sc_layoutType),
src/gromacs/nbnxm/pairlist.cpp:    static_assert(exclusionMaskSize >= sc_gpuJgroupSize(sc_layoutType) * sc_gpuNumClusterPerCell(sc_layoutType),
src/gromacs/nbnxm/pairlist.cpp:                  "The GPU exclusion mask does not contain a sufficient number of bits");
src/gromacs/nbnxm/pairlist.cpp:static std::vector<NbnxnPairlistGpu> createGpuPairlists(int numLists)
src/gromacs/nbnxm/pairlist.cpp:    auto lists = std::vector<NbnxnPairlistGpu>();
src/gromacs/nbnxm/pairlist.cpp:    /* Only list 0 is used on the GPU, use normal allocation for i>0 */
src/gromacs/nbnxm/pairlist.cpp:    lists.emplace_back(NbnxnPairlistGpu{ PinningPolicy::PinnedIfSupported });
src/gromacs/nbnxm/pairlist.cpp:        lists.emplace_back(NbnxnPairlistGpu{ PinningPolicy::CannotBePinned });
src/gromacs/nbnxm/pairlist.cpp:    combineLists_(sc_isGpuPairListType[pairlistParams.pairlistType]), // Currently GPU lists are always combined
src/gromacs/nbnxm/pairlist.cpp:    isCpuType_(!sc_isGpuPairListType[pairlistParams.pairlistType])
src/gromacs/nbnxm/pairlist.cpp:        gpuLists_ = createGpuPairlists(numLists);
src/gromacs/nbnxm/pairlist.cpp:static void print_nblist_statistics(FILE* fp, const NbnxnPairlistGpu& nbl, const GridSet& gridSet, const real rl)
src/gromacs/nbnxm/pairlist.cpp:    constexpr int gpuNumClusterPerCell        = sc_gpuNumClusterPerCell(layoutType);
src/gromacs/nbnxm/pairlist.cpp:    int           c[gpuNumClusterPerCell + 1] = { 0 };
src/gromacs/nbnxm/pairlist.cpp:            for (int j = 0; j < sc_gpuJgroupSize(layoutType); j++)
src/gromacs/nbnxm/pairlist.cpp:                for (int si = 0; si < gpuNumClusterPerCell; si++)
src/gromacs/nbnxm/pairlist.cpp:                    if (nbl.cjPacked.list_[jPacked].imei[0].imask & (1U << (j * gpuNumClusterPerCell + si)))
src/gromacs/nbnxm/pairlist.cpp:        for (int b = 0; b <= gpuNumClusterPerCell; b++)
src/gromacs/nbnxm/pairlist.cpp:                    100.0 * c[b] / (nbl.cjPacked.size() * sc_gpuJgroupSize(layoutType)));
src/gromacs/nbnxm/pairlist.cpp:static nbnxn_excl_t& get_exclusion_mask(NbnxnPairlistGpu* nbl, int cjPacked, int warp)
src/gromacs/nbnxm/pairlist.cpp:static void setSelfAndNewtonExclusionsGpu(NbnxnPairlistGpu* nbl,
src/gromacs/nbnxm/pairlist.cpp:    constexpr int numJatomsPerPart = sc_gpuSplitJClusterSize(layoutType);
src/gromacs/nbnxm/pairlist.cpp:    for (int part = 0; part < sc_gpuClusterPairSplit(layoutType); part++)
src/gromacs/nbnxm/pairlist.cpp:            for (int i = jOffset + jIndexInPart; i < sc_gpuClusterSize(layoutType); i++)
src/gromacs/nbnxm/pairlist.cpp:                excl.pair[jIndexInPart * sc_gpuClusterSize(layoutType) + i] &=
src/gromacs/nbnxm/pairlist.cpp:                        ~(1U << (jOffsetInGroup * sc_gpuNumClusterPerCell(layoutType) + iClusterInCell));
src/gromacs/nbnxm/pairlist.cpp:                                       NbnxnPairlistGpu* nbl,
src/gromacs/nbnxm/pairlist.cpp:    NbnxmPairlistGpuWork& work = *nbl->work;
src/gromacs/nbnxm/pairlist.cpp:    GMX_ASSERT(sc_gpuClusterSize(layoutType) == iGrid.geometry().numAtomsICluster_,
src/gromacs/nbnxm/pairlist.cpp:    GMX_ASSERT(sc_gpuClusterSize(layoutType) == jGrid.geometry().numAtomsICluster_,
src/gromacs/nbnxm/pairlist.cpp:     * and do atom pair distance based pruning on the GPU.
src/gromacs/nbnxm/pairlist.cpp:        const int cjPacked_ind = work.cj_ind / sc_gpuJgroupSize(layoutType);
src/gromacs/nbnxm/pairlist.cpp:        const int cj_offset    = work.cj_ind - cjPacked_ind * sc_gpuJgroupSize(layoutType);
src/gromacs/nbnxm/pairlist.cpp:        const int cj           = scj * sc_gpuNumClusterPerCell(layoutType) + subc;
src/gromacs/nbnxm/pairlist.cpp:        const int cj_gl = jGrid.cellOffset() * sc_gpuNumClusterPerCell(layoutType) + cj;
src/gromacs/nbnxm/pairlist.cpp:        *numDistanceChecks += sc_gpuClusterSize(layoutType) * 2;
src/gromacs/nbnxm/pairlist.cpp:        for (int ci = 0; ci < sc_gpuNumClusterPerCell(layoutType); ci++)
src/gromacs/nbnxm/pairlist.cpp:            *numDistanceChecks += sc_gpuClusterSize(layoutType) * sc_gpuClusterSize(layoutType);
src/gromacs/nbnxm/pairlist.cpp:                imask |= (1U << (cj_offset * sc_gpuNumClusterPerCell(layoutType) + ci));
src/gromacs/nbnxm/pairlist.cpp:            imask &= ~(1U << (cj_offset * sc_gpuNumClusterPerCell(layoutType) + ci_last));
src/gromacs/nbnxm/pairlist.cpp:                setSelfAndNewtonExclusionsGpu<layoutType>(nbl, cjPacked_ind, cj_offset, subc);
src/gromacs/nbnxm/pairlist.cpp:            for (int w = 0; w < sc_gpuClusterPairSplit(layoutType); w++)
src/gromacs/nbnxm/pairlist.cpp:            nbl->sci.back().cjPackedEnd = divideRoundUp(nbl->work->cj_ind, sc_gpuJgroupSize(layoutType));
src/gromacs/nbnxm/pairlist.cpp:    return cj & (sc_gpuJgroupSize(layoutType) - 1);
src/gromacs/nbnxm/pairlist.cpp:    return cj / sc_gpuJgroupSize(layoutType);
src/gromacs/nbnxm/pairlist.cpp:    return a & (sc_gpuSplitJClusterSize(layoutType) - 1);
src/gromacs/nbnxm/pairlist.cpp:                          NbnxnPairlistGpu*       nbl,
src/gromacs/nbnxm/pairlist.cpp:     * cjPacked entries, each with max c_nbnxnGpuJgroupSize cj's, each
src/gromacs/nbnxm/pairlist.cpp:     * On the GPU we don't support energy groups (yet).
src/gromacs/nbnxm/pairlist.cpp:    for (int c = 0; c < sc_gpuNumClusterPerCell(layoutType); c++)
src/gromacs/nbnxm/pairlist.cpp:        const int c_abs = sci * sc_gpuNumClusterPerCell(layoutType) + c;
src/gromacs/nbnxm/pairlist.cpp:                const int nrjMax = numJClusterGroups * sc_gpuJgroupSize(layoutType) * nbl->na_cj;
src/gromacs/nbnxm/pairlist.cpp:                // With GPUs, energy groups are not supported
src/gromacs/nbnxm/pairlist.cpp:                        c_abs - iGrid.cellOffset() * sc_gpuNumClusterPerCell(layoutType), i);
src/gromacs/nbnxm/pairlist.cpp:                    for (int gcj = 0; gcj < sc_gpuJgroupSize(layoutType); gcj++)
src/gromacs/nbnxm/pairlist.cpp:                             & (1U << (gcj * sc_gpuNumClusterPerCell(layoutType) + c)))
src/gromacs/nbnxm/pairlist.cpp:                                        - jGrid.cellOffset() * sc_gpuNumClusterPerCell(layoutType);
src/gromacs/nbnxm/pairlist.cpp:                                const int ind_j = (jGrid.cellOffset() * sc_gpuNumClusterPerCell(layoutType)
src/gromacs/nbnxm/pairlist.cpp:                                    const int jHalf = j / sc_gpuSplitJClusterSize(layoutType);
src/gromacs/nbnxm/pairlist.cpp:                                            (1U << (gcj * sc_gpuNumClusterPerCell(layoutType) + c));
src/gromacs/nbnxm/pairlist.cpp:                                    /* The unpruned GPU list has more than 2/3
src/gromacs/nbnxm/pairlist.cpp:                                     * relative to the fast GPU kernels.
src/gromacs/nbnxm/pairlist.cpp:                             * take an order of magnitude more time, the GPU
src/gromacs/nbnxm/pairlist.cpp:/* Set all atom-pair exclusions for the last i-super-cluster entry in the GPU list
src/gromacs/nbnxm/pairlist.cpp:                                   NbnxnPairlistGpu*       nbl,
src/gromacs/nbnxm/pairlist.cpp:            currentIEntry.cjPackedBegin * sc_gpuJgroupSize(layoutType), nbl->work->cj_ind, nbl->cjPacked);
src/gromacs/nbnxm/pairlist.cpp:    constexpr int c_clusterSize      = sc_gpuClusterSize(layoutType);
src/gromacs/nbnxm/pairlist.cpp:    constexpr int c_superClusterSize = sc_gpuClusterPerSuperCluster(layoutType) * c_clusterSize;
src/gromacs/nbnxm/pairlist.cpp:    GMX_ASSERT(nbl->na_ci == c_clusterSize, "na_ci should match the GPU cluster size");
src/gromacs/nbnxm/pairlist.cpp:                                (1U << (cj_mod_cjPacked<layoutType>(index) * sc_gpuNumClusterPerCell(layoutType)
src/gromacs/nbnxm/pairlist.cpp:                            /* Determine which j-half (CUDA warp) we are in */
src/gromacs/nbnxm/pairlist.cpp:                            const int jHalf = innerJ / sc_gpuSplitJClusterSize(layoutType);
src/gromacs/nbnxm/pairlist.cpp:static void addNewIEntry(NbnxnPairlistGpu* nbl, int sci, int shift, int gmx_unused flags)
src/gromacs/nbnxm/pairlist.cpp:/* Split sci entry for load balancing on the GPU.
src/gromacs/nbnxm/pairlist.cpp: * Splitting ensures we have enough lists to fully utilize the whole GPU.
src/gromacs/nbnxm/pairlist.cpp:split_sci_entry(NbnxnPairlistGpu* nbl, int nsp_target_av, bool progBal, float nsp_tot_est, int thread, int nthread)
src/gromacs/nbnxm/pairlist.cpp:        && jPackedLen * sc_gpuNumClusterPerCell(layoutType) * sc_gpuJgroupSize(layoutType) > nsp_max)
src/gromacs/nbnxm/pairlist.cpp:            for (int p = 0; p < sc_gpuNumClusterPerCell(layoutType) * sc_gpuJgroupSize(layoutType); p++)
src/gromacs/nbnxm/pairlist.cpp:static void closeIEntry(NbnxnPairlistGpu* nbl, int nsp_max_av, bool progBal, float nsp_tot_est, int thread, int nthread)
src/gromacs/nbnxm/pairlist.cpp:        int numPackedJClusters = divideRoundUp(nbl->work->cj_ind, sc_gpuJgroupSize(layoutType));
src/gromacs/nbnxm/pairlist.cpp:        nbl->work->cj_ind      = numPackedJClusters * sc_gpuJgroupSize(layoutType);
src/gromacs/nbnxm/pairlist.cpp:/* Syncs the working array before adding another grid pair to the GPU list */
src/gromacs/nbnxm/pairlist.cpp:/* Syncs the working array before adding another grid pair to the GPU list */
src/gromacs/nbnxm/pairlist.cpp:static void sync_work(NbnxnPairlistGpu* nbl)
src/gromacs/nbnxm/pairlist.cpp:    nbl->work->cj_ind = nbl->cjPacked.size() * sc_gpuJgroupSize(layoutType);
src/gromacs/nbnxm/pairlist.cpp:/* Clears an NbnxnPairlistGpu data structure */
src/gromacs/nbnxm/pairlist.cpp:static void clear_pairlist(NbnxnPairlistGpu* nbl)
src/gromacs/nbnxm/pairlist.cpp:    const int     cellBBStride = packedBoundingBoxesIndex(sc_gpuNumClusterPerCell(layoutType));
src/gromacs/nbnxm/pairlist.cpp:    for (int i = 0; i < sc_gpuNumClusterPerCell(layoutType); i++)
src/gromacs/nbnxm/pairlist.cpp:        set_icell_bb_simple(bb, ci * sc_gpuNumClusterPerCell(layoutType) + i, shift, &bb_ci[i]);
src/gromacs/nbnxm/pairlist.cpp:gmx_unused static void set_icell_bb(const Grid& iGrid, int ci, const RVec& shift, NbnxmPairlistGpuWork* work)
src/gromacs/nbnxm/pairlist.cpp:                        NbnxmPairlistGpuWork* work)
src/gromacs/nbnxm/pairlist.cpp:    if constexpr (!c_useSimdGpuClusterPairDistance(layoutType))
src/gromacs/nbnxm/pairlist.cpp:        int ia = ci * sc_gpuNumClusterPerCell(layoutType) * sc_gpuClusterSize(layoutType);
src/gromacs/nbnxm/pairlist.cpp:        for (int i = 0; i < sc_gpuNumClusterPerCell(layoutType) * sc_gpuClusterSize(layoutType); i++)
src/gromacs/nbnxm/pairlist.cpp:        for (int si = 0; si < sc_gpuNumClusterPerCell(layoutType); si++)
src/gromacs/nbnxm/pairlist.cpp:            const int inputOffset = (ci * sc_gpuNumClusterPerCell(layoutType) + si)
src/gromacs/nbnxm/pairlist.cpp:                                    * sc_gpuClusterSize(layoutType) * stride;
src/gromacs/nbnxm/pairlist.cpp:            const int outputOffset = si * sc_gpuClusterSize(layoutType) * DIM;
src/gromacs/nbnxm/pairlist.cpp:            for (int i = 0; i < sc_gpuClusterSize(layoutType); i++)
src/gromacs/nbnxm/pairlist.cpp:                    x_ci[outputOffset + d * sc_gpuClusterSize(layoutType) + i] =
src/gromacs/nbnxm/pairlist.cpp:        return std::min(dims.cellSize[XX] / sc_gpuNumClusterPerCellX(layoutType),
src/gromacs/nbnxm/pairlist.cpp:                        dims.cellSize[YY] / sc_gpuNumClusterPerCellY(layoutType));
src/gromacs/nbnxm/pairlist.cpp:    ls[XX] = dims.cellSize[XX] / sc_gpuNumClusterPerCellX(layoutType);
src/gromacs/nbnxm/pairlist.cpp:    ls[YY] = dims.cellSize[YY] / sc_gpuNumClusterPerCellY(layoutType);
src/gromacs/nbnxm/pairlist.cpp:static void print_nblist_sci_cj(FILE* fp, const NbnxnPairlistGpu& nbl)
src/gromacs/nbnxm/pairlist.cpp:            for (int j = 0; j < sc_gpuJgroupSize(layoutType); j++)
src/gromacs/nbnxm/pairlist.cpp:                for (int si = 0; si < sc_gpuNumClusterPerCell(layoutType); si++)
src/gromacs/nbnxm/pairlist.cpp:                        & (1U << (j * sc_gpuNumClusterPerCell(layoutType) + si)))
src/gromacs/nbnxm/pairlist.cpp:static void combine_nblists(ArrayRef<const NbnxnPairlistGpu> nbls, NbnxnPairlistGpu* nblc)
src/gromacs/nbnxm/pairlist.cpp:                for (int splitIdx = 0; splitIdx < sc_gpuClusterPairSplit(layoutType); splitIdx++)
src/gromacs/nbnxm/pairlist.cpp:     * this is because the GPU is much faster than the cpu.
src/gromacs/nbnxm/pairlist.cpp:        bbx /= sc_gpuNumClusterPerCellX(layoutType);
src/gromacs/nbnxm/pairlist.cpp:        bby /= sc_gpuNumClusterPerCellY(layoutType);
src/gromacs/nbnxm/pairlist.cpp:         * with GPUs better mixing of "upper" cells that have more empty
src/gromacs/nbnxm/pairlist.cpp:         * Without GPUs the regime of so few atoms per thread is less
src/gromacs/nbnxm/pairlist.cpp:static void makeClusterListWrapper(NbnxnPairlistGpu*                    nbl,
src/gromacs/nbnxm/pairlist.cpp:static int getNumSimpleJClustersInList(const gmx_unused NbnxnPairlistGpu& nbl)
src/gromacs/nbnxm/pairlist.cpp:static void incrementNumSimpleJClustersInList(NbnxnPairlistGpu gmx_unused* nbl, int gmx_unused ncj_old_j)
src/gromacs/nbnxm/pairlist.cpp:static void checkListSizeConsistency(const NbnxnPairlistGpu gmx_unused& nbl, bool gmx_unused haveFreeEnergy)
src/gromacs/nbnxm/pairlist.cpp:static void setBufferFlags(const NbnxnPairlistGpu gmx_unused& nbl,
src/gromacs/nbnxm/pairlist.cpp: * This avoids load imbalance on the GPU, as large lists will be
src/gromacs/nbnxm/pairlist.cpp:static void sort_sci(NbnxnPairlistGpu* nbl)
src/gromacs/nbnxm/pairlist.cpp:    /* For CUDA version, sorting is done on the GPU */
src/gromacs/nbnxm/pairlist.cpp:    if (nbnxmSortListsOnGpu())
src/gromacs/nbnxm/pairlist.cpp:    NbnxmPairlistGpuWork& work = *nbl->work;
src/gromacs/nbnxm/pairlist.cpp:                                     const int                minimumIlistCountForGpuBalancing,
src/gromacs/nbnxm/pairlist.cpp:    const int numLists = (isCpuType_ ? cpuLists_.size() : gpuLists_.size());
src/gromacs/nbnxm/pairlist.cpp:    if (!isCpuType_ && minimumIlistCountForGpuBalancing > 0)
src/gromacs/nbnxm/pairlist.cpp:                gridSet, locality, rlist, minimumIlistCountForGpuBalancing, &nsubpair_target, &nsubpair_tot_est);
src/gromacs/nbnxm/pairlist.cpp:            clear_pairlist(&gpuLists_[th]);
src/gromacs/nbnxm/pairlist.cpp:            /* With GPU: generate progressively smaller lists for
src/gromacs/nbnxm/pairlist.cpp:                        GMX_ASSERT(!isCpuType_, "Can only combine GPU lists");
src/gromacs/nbnxm/pairlist.cpp:                        clear_pairlist(&gpuLists_[th]);
src/gromacs/nbnxm/pairlist.cpp:                                                 &gpuLists_[th],
src/gromacs/nbnxm/pairlist.cpp:                    const NbnxnPairlistGpu& nbl = gpuLists_[th];
src/gromacs/nbnxm/pairlist.cpp:                                       : square(gpuLists_[0].na_ci);
src/gromacs/nbnxm/pairlist.cpp:                GMX_ASSERT(!isCpuType_, "Can only combine GPU lists");
src/gromacs/nbnxm/pairlist.cpp:                combine_nblists<sc_layoutType>(constArrayRefFromArray(&gpuLists_[1], numLists - 1),
src/gromacs/nbnxm/pairlist.cpp:                                               &gpuLists_[0]);
src/gromacs/nbnxm/pairlist.cpp:        if (combineLists_ || gpuLists_.size() == 1)
src/gromacs/nbnxm/pairlist.cpp:            sort_sci(&gpuLists_[0]);
src/gromacs/nbnxm/pairlist.cpp:                    sort_sci(&gpuLists_[th]);
src/gromacs/nbnxm/pairlist.cpp:     * or combined (GPU), so we should dump the final result to debug.
src/gromacs/nbnxm/pairlist.cpp:        else if (!isCpuType_ && gpuLists_.size() > 1)
src/gromacs/nbnxm/pairlist.cpp:            print_nblist_statistics<sc_layoutType>(debug, gpuLists_[0], gridSet, rlist);
src/gromacs/nbnxm/pairlist.cpp:                print_nblist_sci_cj<sc_layoutType>(debug, gpuLists_[0]);
src/gromacs/nbnxm/pairlist.cpp:                                              minimumIlistCountForGpuBalancing_,
src/gromacs/nbnxm/pairlist.cpp:    if (useGpu())
src/gromacs/nbnxm/pairlist.cpp:        /* Launch the transfer of the pairlist to the GPU.
src/gromacs/nbnxm/pairlist.cpp:        gpu_init_pairlist(gpuNbv_, pairlistSets().pairlistSet(iLocality).gpuList(), iLocality);
src/gromacs/nbnxm/gridsetdata.h:#include "gromacs/gpu_utils/hostallocator.h"
src/gromacs/nbnxm/nbnxm_setup.cpp: * \brief Common functions for the different NBNXN GPU implementations.
src/gromacs/nbnxm/nbnxm_setup.cpp:#include "gromacs/gpu_utils/hostallocator.h"
src/gromacs/nbnxm/nbnxm_setup.cpp:#include "gromacs/nbnxm/gpu_data_mgmt.h"
src/gromacs/nbnxm/nbnxm_setup.cpp:struct NbnxmGpu;
src/gromacs/nbnxm/nbnxm_setup.cpp:    Gpu,
src/gromacs/nbnxm/nbnxm_setup.cpp:    EmulateGpu
src/gromacs/nbnxm/nbnxm_setup.cpp:        case NbnxmKernelType::Gpu8x8x8: return "GPU";
src/gromacs/nbnxm/nbnxm_setup.cpp:        case NbnxmKernelType::Cpu8x8x8_PlainC: return "plain-C-GPU-layout";
src/gromacs/nbnxm/nbnxm_setup.cpp:                                          const PairlistType       gpuPairlistType,
src/gromacs/nbnxm/nbnxm_setup.cpp:    if (nonbondedResource == NonbondedResource::EmulateGpu)
src/gromacs/nbnxm/nbnxm_setup.cpp:        kernelSetup.ewaldExclusionType = EwaldExclusionType::DecidedByGpuModule;
src/gromacs/nbnxm/nbnxm_setup.cpp:        GMX_LOG(mdlog.warning).asParagraph().appendText("Emulating a GPU run on the CPU (slow)");
src/gromacs/nbnxm/nbnxm_setup.cpp:    else if (nonbondedResource == NonbondedResource::Gpu)
src/gromacs/nbnxm/nbnxm_setup.cpp:        kernelSetup.kernelType         = NbnxmKernelType::Gpu8x8x8;
src/gromacs/nbnxm/nbnxm_setup.cpp:        kernelSetup.ewaldExclusionType = EwaldExclusionType::DecidedByGpuModule;
src/gromacs/nbnxm/nbnxm_setup.cpp:    if (nonbondedResource == NonbondedResource::Gpu || nonbondedResource == NonbondedResource::EmulateGpu)
src/gromacs/nbnxm/nbnxm_setup.cpp:        const std::string gpuPairlistSplitMessage = sc_gpuIsSplitPairList(gpuPairlistType)
src/gromacs/nbnxm/nbnxm_setup.cpp:                                                            ? "with split GPU pairlist"
src/gromacs/nbnxm/nbnxm_setup.cpp:                                                            : "with unified GPU pairlist";
src/gromacs/nbnxm/nbnxm_setup.cpp:                .appendTextFormatted("NBNxM GPU setup: super-cluster %dx%dx%d / cluster %d, %s",
src/gromacs/nbnxm/nbnxm_setup.cpp:                                     sc_gpuNumClusterPerCellX(gpuPairlistType),
src/gromacs/nbnxm/nbnxm_setup.cpp:                                     sc_gpuNumClusterPerCellY(gpuPairlistType),
src/gromacs/nbnxm/nbnxm_setup.cpp:                                     sc_gpuNumClusterPerCellZ(gpuPairlistType),
src/gromacs/nbnxm/nbnxm_setup.cpp:                                     sc_gpuClusterSize(gpuPairlistType),
src/gromacs/nbnxm/nbnxm_setup.cpp:                                     gpuPairlistSplitMessage.c_str());
src/gromacs/nbnxm/nbnxm_setup.cpp:                           const int             minimumIlistCountForGpuBalancing) :
src/gromacs/nbnxm/nbnxm_setup.cpp:    params_(pairlistParams), minimumIlistCountForGpuBalancing_(minimumIlistCountForGpuBalancing)
src/gromacs/nbnxm/nbnxm_setup.cpp:/*! \brief Gets and returns the minimum i-list count for balacing based on the GPU used or env.var. when set */
src/gromacs/nbnxm/nbnxm_setup.cpp:static int getMinimumIlistCountForGpuBalancing(NbnxmGpu* nbnxmGpu)
src/gromacs/nbnxm/nbnxm_setup.cpp:        int minimumIlistCount = gpu_min_ci_balanced(nbnxmGpu);
src/gromacs/nbnxm/nbnxm_setup.cpp:                    "Neighbor-list balancing parameter: %d (auto-adjusted to the number of GPU "
src/gromacs/nbnxm/nbnxm_setup.cpp:                                                   const bool           useGpuForNonbonded,
src/gromacs/nbnxm/nbnxm_setup.cpp:    const bool emulateGpu = (getenv("GMX_EMULATE_GPU") != nullptr);
src/gromacs/nbnxm/nbnxm_setup.cpp:    GMX_RELEASE_ASSERT(!(emulateGpu && useGpuForNonbonded),
src/gromacs/nbnxm/nbnxm_setup.cpp:                       "When GPU emulation is active, there cannot be a GPU assignment");
src/gromacs/nbnxm/nbnxm_setup.cpp:    if (useGpuForNonbonded)
src/gromacs/nbnxm/nbnxm_setup.cpp:        nonbondedResource = NonbondedResource::Gpu;
src/gromacs/nbnxm/nbnxm_setup.cpp:    else if (emulateGpu)
src/gromacs/nbnxm/nbnxm_setup.cpp:        nonbondedResource = NonbondedResource::EmulateGpu;
src/gromacs/nbnxm/nbnxm_setup.cpp:    const auto gpuPairlistLayout = sc_layoutType;
src/gromacs/nbnxm/nbnxm_setup.cpp:            mdlog, forcerec.use_simd_kernels, hardwareInfo, gpuPairlistLayout, nonbondedResource, inputrec);
src/gromacs/nbnxm/nbnxm_setup.cpp:            kernelSetup.kernelType, gpuPairlistLayout, bFEP_NonBonded, inputrec.rlist, haveMultipleDomains);
src/gromacs/nbnxm/nbnxm_setup.cpp:    auto pinPolicy = (useGpuForNonbonded ? gmx::PinningPolicy::PinnedIfSupported
src/gromacs/nbnxm/nbnxm_setup.cpp:            (useGpuForNonbonded || emulateGpu) ? 1 : gmx_omp_nthreads_get(ModuleMultiThread::Nonbonded));
src/gromacs/nbnxm/nbnxm_setup.cpp:    NbnxmGpu* gpu_nbv                          = nullptr;
src/gromacs/nbnxm/nbnxm_setup.cpp:    int       minimumIlistCountForGpuBalancing = 0;
src/gromacs/nbnxm/nbnxm_setup.cpp:    if (useGpuForNonbonded)
src/gromacs/nbnxm/nbnxm_setup.cpp:        /* init the NxN GPU data; the last argument tells whether we'll have
src/gromacs/nbnxm/nbnxm_setup.cpp:         * both local and non-local NB calculation on GPU */
src/gromacs/nbnxm/nbnxm_setup.cpp:                "Device stream manager should be initialized in order to use GPU for non-bonded.");
src/gromacs/nbnxm/nbnxm_setup.cpp:        gpu_nbv = gpu_init(
src/gromacs/nbnxm/nbnxm_setup.cpp:        minimumIlistCountForGpuBalancing = getMinimumIlistCountForGpuBalancing(gpu_nbv);
src/gromacs/nbnxm/nbnxm_setup.cpp:            pairlistParams, haveMultipleDomains, minimumIlistCountForGpuBalancing);
src/gromacs/nbnxm/nbnxm_setup.cpp:                                                gpu_nbv,
src/gromacs/nbnxm/nbnxm_setup.cpp:                                       NbnxmGpu*                         gpu_nbv_ptr,
src/gromacs/nbnxm/nbnxm_setup.cpp:    gpuNbv_(gpu_nbv_ptr)
src/gromacs/nbnxm/nbnxm_setup.cpp:                                       NbnxmGpu*                         gpu_nbv_ptr) :
src/gromacs/nbnxm/nbnxm_setup.cpp:    gpuNbv_(gpu_nbv_ptr)
src/gromacs/nbnxm/nbnxm_setup.cpp:    gpu_free(gpuNbv_);
src/gromacs/nbnxm/benchmark/bench_setup.h:    //! Whether to use a GPU, currently GPUs are not supported
src/gromacs/nbnxm/benchmark/bench_setup.h:    bool useGpu = false;
src/gromacs/nbnxm/benchmark/bench_setup.cpp:#include "gromacs/gpu_utils/hostallocator.h"
src/gromacs/nbnxm/benchmark/bench_setup.cpp:            (options.useGpu ? PinningPolicy::PinnedIfSupported : PinningPolicy::CannotBePinned);
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp: *  \brief Define common implementation of nbnxm_gpu_data_mgmt.h
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:#if GMX_GPU_CUDA
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:#    include "cuda/nbnxm_cuda_types.h"
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:#if GMX_GPU_OPENCL
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:#    include "opencl/nbnxm_ocl_types.h"
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:#if GMX_GPU_HIP
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:#if GMX_GPU_SYCL
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:#include "gromacs/gpu_utils/device_stream_manager.h"
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:#include "gromacs/gpu_utils/devicebuffer.h"
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:#include "gromacs/gpu_utils/gputraits.h"
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:#include "gromacs/gpu_utils/hostallocator.h"
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:#include "gromacs/nbnxm/gpu_common_utils.h"
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:#include "gromacs/nbnxm/gpu_data_mgmt.h"
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:#include "gromacs/nbnxm/gpu_types_common.h"
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:#include "gromacs/timing/gpu_timing.h"
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:#include "nbnxm_gpu.h"
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:#include "nbnxm_gpu_data_mgmt.h"
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:                                                  NBParamGpu*                  nbp,
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:     *  - NVIDIA CC 7.0 and 8.0,
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:     *  - all AMD GPUs (although tested for gfx906 and 908 only).
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:     * Note 1: this function does not handle OpenCL.
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:     * Note 2: for SYCL, the heuristics are taken from CUDA/HIP ports, and were only partially
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:#if GMX_GPU_CUDA
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:#elif GMX_GPU_HIP
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:#elif GMX_GPU_SYCL
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:        case DeviceVendor::Nvidia:
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:static inline ElecType nbnxn_gpu_pick_ewald_kernel_type(const interaction_const_t& ic,
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:    const bool forceAnalyticalEwald = (getenv("GMX_GPU_NB_ANA_EWALD") != nullptr);
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:    const bool forceTabulatedEwald  = (getenv("GMX_GPU_NB_TAB_EWALD") != nullptr);
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:    const bool forceTwinCutoffEwald = (getenv("GMX_GPU_NB_EWALD_TWINCUT") != nullptr);
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:                "Both analytical and tabulated Ewald GPU non-bonded kernels "
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:            fprintf(debug, "Using analytical Ewald GPU kernels\n");
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:            fprintf(debug, "Using tabulated Ewald GPU kernels\n");
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:static inline void set_cutoff_parameters(NBParamGpu*                nbp,
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:GpuPairlistSorting::GpuPairlistSorting() {}
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:GpuPairlistSorting::~GpuPairlistSorting()
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:    if (nbnxmSortListsOnGpu())
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:GpuPairlist::GpuPairlist() {}
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:GpuPairlist::~GpuPairlist()
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:static inline void init_timings(gmx_wallclock_gpu_nbnxn_t* t)
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:static inline void initAtomdataFirst(NBAtomDataGpu*       atomdata,
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:static inline VdwType nbnxmGpuPickVdwKernelType(const interaction_const_t& ic,
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:                                "the GPU accelerated kernels!",
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:                                     "implemented in the GPU accelerated kernels!",
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:                "The requested VdW type %s is not implemented in the GPU accelerated kernels!",
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:static inline ElecType nbnxmGpuPickElectrostaticsKernelType(const interaction_const_t& ic,
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:        return nbnxn_gpu_pick_ewald_kernel_type(ic, deviceInfo);
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:                             "the GPU accelerated kernels!",
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:static inline void initNbparam(NBParamGpu*                     nbp,
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:    nbp->vdwType  = nbnxmGpuPickVdwKernelType(ic, nbatParams.ljCombinationRule);
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:    nbp->elecType = nbnxmGpuPickElectrostaticsKernelType(ic, deviceContext.deviceInfo());
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:using GpuPairlistByLocality = EnumerationArray<InteractionLocality, std::unique_ptr<GpuPairlist>>;
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:static GpuPairlistByLocality initializeGpuLists(bool localAndNonLocal)
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:    GpuPairlistByLocality list;
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:    list[InteractionLocality::Local] = std::make_unique<GpuPairlist>();
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:        list[InteractionLocality::NonLocal] = std::make_unique<GpuPairlist>();
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:NbnxmGpu* gpu_init(const DeviceStreamManager& deviceStreamManager,
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:    auto* nb           = new NbnxmGpu();
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:    nb->atdat          = new NBAtomDataGpu;
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:    nb->nbparam        = new NBParamGpu;
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:    nb->plist = initializeGpuLists(bLocalAndNonlocal);
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:    nb->timers = new GpuTimers();
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:    nb->bDoTime = decideGpuTimingsUsage();
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:    /* local/non-local GPU streams */
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:                       "Local non-bonded stream should be initialized to use GPU for non-bonded.");
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:                           "Non-local non-bonded stream should be initialized to use GPU for "
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:    gpu_init_platform_specific(nb);
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:        fprintf(debug, "Initialized NBNXM GPU data structures.\n");
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:void gpu_pme_loadbal_update_param(nonbonded_verlet_t* nbv, const interaction_const_t& ic)
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:    if (!nbv || !nbv->useGpu())
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:    NbnxmGpu*   nb  = nbv->gpuNbv();
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:    NBParamGpu* nbp = nb->nbparam;
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:    nbp->elecType = nbnxn_gpu_pick_ewald_kernel_type(ic, nb->deviceContext_->deviceInfo());
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:void gpu_upload_shiftvec(NbnxmGpu* nb, const nbnxn_atomdata_t* nbatom)
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:    NBAtomDataGpu*      adat        = nb->atdat;
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:                           GpuApiCallBehavior::Async,
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:void gpu_init_pairlist(NbnxmGpu* nb, const NbnxnPairlistGpu* h_plist, const InteractionLocality iloc)
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:    GpuTimers::Interaction& iTimers = nb->timers->interaction[iloc];
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:    // TODO most of this function is same in CUDA and OpenCL, move into the header
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:                       GpuApiCallBehavior::Async,
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:    /* gpu sorting is only implemented for cuda */
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:    if (nbnxmSortListsOnGpu())
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:                           GpuApiCallBehavior::Async,
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:                       GpuApiCallBehavior::Async,
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:                           h_plist->cjPacked.size() * sc_gpuClusterPairSplit(sc_layoutType),
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:                       GpuApiCallBehavior::Async,
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:    // per block, to allow asynchronous incrementation on the GPU without CPU involvement.
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:void gpu_init_atomdata(NbnxmGpu* nb, const nbnxn_atomdata_t* nbat)
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:    GpuTimers*           timers        = bDoTime ? nb->timers : nullptr;
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:    NBAtomDataGpu*       atdat         = nb->atdat;
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:    /* need to clear GPU f output if realloc happened */
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:                           GpuApiCallBehavior::Async,
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:                           GpuApiCallBehavior::Async,
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:void gpu_clear_outputs(NbnxmGpu* nb, bool computeVirial)
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:    NBAtomDataGpu*      adat        = nb->atdat;
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:gmx_wallclock_gpu_nbnxn_t* gpu_get_timings(NbnxmGpu* nb)
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:void gpu_reset_timings(nonbonded_verlet_t* nbv)
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:    if (nbv->gpuNbv() && nbv->gpuNbv()->bDoTime)
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:        init_timings(nbv->gpuNbv()->timings);
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:bool gpu_is_kernel_ewald_analytical(const NbnxmGpu* nb)
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:void setupGpuShortRangeWorkLow(NbnxmGpu*                 nb,
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:                               const ListedForcesGpu*    listedForcesGpu,
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:    GMX_ASSERT(nb, "Need a valid nbnxn_gpu object");
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:                               || (listedForcesGpu != nullptr && listedForcesGpu->haveInteractions()));
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:bool haveGpuShortRangeWork(const NbnxmGpu* nb, const InteractionLocality interactionLocality)
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:    GMX_ASSERT(nb, "Need a valid nbnxn_gpu object");
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp: * Launch asynchronously the download of nonbonded forces from the GPU
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:void gpu_launch_cpyback(NbnxmGpu*                nb,
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:    GMX_ASSERT(nb, "Need a valid nbnxn_gpu object");
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:    NBAtomDataGpu*      adat         = nb->atdat;
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:    GpuTimers*          timers       = nb->timers;
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:    if ((iloc == InteractionLocality::NonLocal) && !haveGpuShortRangeWork(nb, iloc))
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:           overhead. Curiously, for NVIDIA OpenCL with an empty-domain
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:           the API calls, but this has not been tested on AMD OpenCL,
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:    auto atomsRange = getGpuAtomRange(adat, atomLocality);
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:    if (!stepWork.useGpuFBufferOps)
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:                GpuApiCallBehavior::Async,
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:                                 GpuApiCallBehavior::Async,
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:                                 GpuApiCallBehavior::Async,
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:                                 GpuApiCallBehavior::Async,
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:void nbnxnInsertNonlocalGpuDependency(NbnxmGpu* nb, const InteractionLocality interactionLocality)
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:void gpu_copy_xq_to_gpu(NbnxmGpu* nb, const nbnxn_atomdata_t* nbatom, const AtomLocality atomLocality)
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:    GMX_ASSERT(nb, "Need a valid nbnxn_gpu object");
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:    NBAtomDataGpu*      adat         = nb->atdat;
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:    GpuTimers*          timers       = nb->timers;
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:       work in nbnxn_gpu_launch_kernel().
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:    if ((iloc == InteractionLocality::NonLocal) && !haveGpuShortRangeWork(nb, iloc))
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:    const auto atomsRange = getGpuAtomRange(adat, atomLocality);
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:                       GpuApiCallBehavior::Async,
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:    nbnxnInsertNonlocalGpuDependency(nb, iloc);
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:/* Initialization for X buffer operations on GPU. */
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:void nbnxn_gpu_init_x_to_nbat_x(const GridSet& gridSet, NbnxmGpu* gpu_nbv)
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:    const DeviceStream& localStream   = *gpu_nbv->deviceStreams[InteractionLocality::Local];
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:    const bool          bDoTime       = gpu_nbv->bDoTime;
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:    reallocateDeviceBuffer(&gpu_nbv->cxy_na,
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:                           &gpu_nbv->ncxy_na,
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:                           &gpu_nbv->ncxy_na_alloc,
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:                           *gpu_nbv->deviceContext_);
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:    reallocateDeviceBuffer(&gpu_nbv->cxy_ind,
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:                           &gpu_nbv->ncxy_ind,
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:                           &gpu_nbv->ncxy_ind_alloc,
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:                           *gpu_nbv->deviceContext_);
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:        auto* timerH2D = bDoTime ? &gpu_nbv->timers->xf[AtomLocality::Local].nb_h2d : nullptr;
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:        reallocateDeviceBuffer(&gpu_nbv->atomIndices,
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:                               &gpu_nbv->atomIndicesSize,
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:                               &gpu_nbv->atomIndicesSize_alloc,
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:                               *gpu_nbv->deviceContext_);
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:            copyToDeviceBuffer(&gpu_nbv->atomIndices,
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:                               GpuApiCallBehavior::Async,
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:            copyToDeviceBuffer(&gpu_nbv->cxy_na,
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:                               GpuApiCallBehavior::Async,
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:            copyToDeviceBuffer(&gpu_nbv->cxy_ind,
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:                               GpuApiCallBehavior::Async,
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:    nbnxnInsertNonlocalGpuDependency(gpu_nbv, InteractionLocality::Local);
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:    nbnxnInsertNonlocalGpuDependency(gpu_nbv, InteractionLocality::NonLocal);
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:void gpu_free(NbnxmGpu* nb)
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:     * In practice, that's not needed for CUDA since cudaFree synchronizes internally.
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:    gpu_free_platform_specific(nb);
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:    NBAtomDataGpu* atdat   = nb->atdat;
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:    NBParamGpu*    nbparam = nb->nbparam;
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:        fprintf(debug, "Cleaned up NBNXM GPU data structures.\n");
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:NBAtomDataGpu* gpuGetNBAtomData(NbnxmGpu* nb)
src/gromacs/nbnxm/nbnxm_gpu_data_mgmt.cpp:DeviceBuffer<RVec> gpu_get_f(NbnxmGpu* nb)
src/gromacs/nbnxm/gpu_types_common.h: * \brief Implements common internal types for different NBNXN GPU implementations
src/gromacs/nbnxm/gpu_types_common.h:#ifndef GMX_MDLIB_NBNXN_GPU_COMMON_TYPES_H
src/gromacs/nbnxm/gpu_types_common.h:#define GMX_MDLIB_NBNXN_GPU_COMMON_TYPES_H
src/gromacs/nbnxm/gpu_types_common.h:#include "gromacs/gpu_utils/hostallocator.h"
src/gromacs/nbnxm/gpu_types_common.h:#if GMX_GPU_OPENCL
src/gromacs/nbnxm/gpu_types_common.h:#    include "gromacs/gpu_utils/gpuregiontimer_ocl.h"
src/gromacs/nbnxm/gpu_types_common.h:#if GMX_GPU_CUDA
src/gromacs/nbnxm/gpu_types_common.h:#    include "gromacs/gpu_utils/gpuregiontimer.cuh"
src/gromacs/nbnxm/gpu_types_common.h:#if GMX_GPU_SYCL
src/gromacs/nbnxm/gpu_types_common.h:#    include "gromacs/gpu_utils/gpuregiontimer_sycl.h"
src/gromacs/nbnxm/gpu_types_common.h:#if GMX_GPU_HIP
src/gromacs/nbnxm/gpu_types_common.h:#    include "gromacs/gpu_utils/gpuregiontimer_hip.h"
src/gromacs/nbnxm/gpu_types_common.h:/*! \brief Number of separate bins used during sorting of plist on gpu
src/gromacs/nbnxm/gpu_types_common.h:/*! \brief Number of threads per block used by the gpu sorting kernel
src/gromacs/nbnxm/gpu_types_common.h:static constexpr int c_clusterSize = sc_gpuClusterSize(sc_layoutType);
src/gromacs/nbnxm/gpu_types_common.h:static constexpr int c_clusterSplitSize = sc_gpuClusterPairSplit(sc_layoutType);
src/gromacs/nbnxm/gpu_types_common.h:static constexpr int c_superClusterSize = sc_gpuClusterPerSuperCluster(sc_layoutType);
src/gromacs/nbnxm/gpu_types_common.h:static constexpr int c_jGroupSize = sc_gpuJgroupSize(sc_layoutType);
src/gromacs/nbnxm/gpu_types_common.h:static const int c_splitClSize = sc_gpuSplitJClusterSize(sc_layoutType);
src/gromacs/nbnxm/gpu_types_common.h:static constexpr int c_exclSize = sc_gpuExclSize(sc_layoutType);
src/gromacs/nbnxm/gpu_types_common.h:// i-cluster interaction mask for a super-cluster with all c_nbnxnGpuNumClusterPerSupercluster=8 bits set.
src/gromacs/nbnxm/gpu_types_common.h: * \brief Staging area for temporary data downloaded from the GPU.
src/gromacs/nbnxm/gpu_types_common.h: * But it allows prefetching of the data from GPU, and brings GPU backends closer together.
src/gromacs/nbnxm/gpu_types_common.h:struct NBAtomDataGpu
src/gromacs/nbnxm/gpu_types_common.h: * \brief Parameters required for the GPU nonbonded calculations.
src/gromacs/nbnxm/gpu_types_common.h:struct NBParamGpu
src/gromacs/nbnxm/gpu_types_common.h: * \brief GPU region timers used for timing GPU kernels and H2D/D2H transfers.
src/gromacs/nbnxm/gpu_types_common.h:struct GpuTimers
src/gromacs/nbnxm/gpu_types_common.h:        GpuRegionTimer nb_h2d;
src/gromacs/nbnxm/gpu_types_common.h:        GpuRegionTimer nb_d2h;
src/gromacs/nbnxm/gpu_types_common.h:        GpuRegionTimer pl_h2d;
src/gromacs/nbnxm/gpu_types_common.h:        GpuRegionTimer nb_k;
src/gromacs/nbnxm/gpu_types_common.h:        GpuRegionTimer prune_k;
src/gromacs/nbnxm/gpu_types_common.h:        GpuRegionTimer rollingPrune_k;
src/gromacs/nbnxm/gpu_types_common.h:    GpuRegionTimer atdat;
src/gromacs/nbnxm/gpu_types_common.h:    EnumerationArray<InteractionLocality, GpuTimers::Interaction> interaction;
src/gromacs/nbnxm/gpu_types_common.h: * \brief Sorted pair list on GPU and data required for performing the sorting */
src/gromacs/nbnxm/gpu_types_common.h:class GpuPairlistSorting
src/gromacs/nbnxm/gpu_types_common.h:    GpuPairlistSorting();
src/gromacs/nbnxm/gpu_types_common.h:    ~GpuPairlistSorting();
src/gromacs/nbnxm/gpu_types_common.h:    GpuPairlistSorting(const GpuPairlistSorting&) = delete;
src/gromacs/nbnxm/gpu_types_common.h:    GpuPairlistSorting(GpuPairlistSorting&&) = delete;
src/gromacs/nbnxm/gpu_types_common.h:    GpuPairlistSorting& operator=(const GpuPairlistSorting&) = delete;
src/gromacs/nbnxm/gpu_types_common.h:    GpuPairlistSorting& operator=(GpuPairlistSorting&&) = delete;
src/gromacs/nbnxm/gpu_types_common.h: * \brief GPU pair list structure */
src/gromacs/nbnxm/gpu_types_common.h:class GpuPairlist
src/gromacs/nbnxm/gpu_types_common.h:    GpuPairlist();
src/gromacs/nbnxm/gpu_types_common.h:    ~GpuPairlist();
src/gromacs/nbnxm/gpu_types_common.h:    GpuPairlist(const GpuPairlist&) = delete;
src/gromacs/nbnxm/gpu_types_common.h:    GpuPairlist(GpuPairlist&&) = delete;
src/gromacs/nbnxm/gpu_types_common.h:    GpuPairlist& operator=(const GpuPairlist&) = delete;
src/gromacs/nbnxm/gpu_types_common.h:    GpuPairlist& operator=(GpuPairlist&&) = delete;
src/gromacs/nbnxm/gpu_types_common.h:    GpuPairlistSorting sorting;
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_pruneonly.cu:#include "nbnxm_cuda_kernel_pruneonly.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_pruneonly.cu:#include "gromacs/nbnxm/gpu_types_common.h"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_pruneonly.cu:nbnxn_kernel_prune_cuda<false>(const NBAtomDataGpu, const NBParamGpu, const GpuPairlist, int);
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_pruneonly.cu:nbnxn_kernel_prune_cuda<true>(const NBAtomDataGpu, const NBParamGpu, const GpuPairlist, int);
src/gromacs/nbnxm/cuda/.clang-tidy:# This happens often in the GPU code and should generally be harmless
src/gromacs/nbnxm/cuda/nbnxm_gpu_buffer_ops_internal.cu: *  \brief Define CUDA kernel (and its wrapper) for transforming position coordinates from rvec to nbnxm layout.
src/gromacs/nbnxm/cuda/nbnxm_gpu_buffer_ops_internal.cu: *  \author Alan Gray <alang@nvidia.com>
src/gromacs/nbnxm/cuda/nbnxm_gpu_buffer_ops_internal.cu: *  \author Jon Vincent <jvincent@nvidia.com>
src/gromacs/nbnxm/cuda/nbnxm_gpu_buffer_ops_internal.cu:#include "gromacs/nbnxm/nbnxm_gpu_buffer_ops_internal.h"
src/gromacs/nbnxm/cuda/nbnxm_gpu_buffer_ops_internal.cu:#include "gromacs/gpu_utils/typecasts_cuda_hip.h"
src/gromacs/nbnxm/cuda/nbnxm_gpu_buffer_ops_internal.cu:#include "gromacs/gpu_utils/vectype_ops_cuda.h"
src/gromacs/nbnxm/cuda/nbnxm_gpu_buffer_ops_internal.cu:#include "gromacs/nbnxm/cuda/nbnxm_cuda_types.h"
src/gromacs/nbnxm/cuda/nbnxm_gpu_buffer_ops_internal.cu:/*! \brief CUDA kernel for transforming position coordinates from rvec to nbnxm layout.
src/gromacs/nbnxm/cuda/nbnxm_gpu_buffer_ops_internal.cu:static __global__ void nbnxn_gpu_x_to_nbat_x_kernel(int numColumns,
src/gromacs/nbnxm/cuda/nbnxm_gpu_buffer_ops_internal.cu:    // Map cell-level parallelism to y component of CUDA block index.
src/gromacs/nbnxm/cuda/nbnxm_gpu_buffer_ops_internal.cu://! Number of CUDA threads in a block
src/gromacs/nbnxm/cuda/nbnxm_gpu_buffer_ops_internal.cu:                                     NbnxmGpu*            nb,
src/gromacs/nbnxm/cuda/nbnxm_gpu_buffer_ops_internal.cu:    auto       kernelFn      = nbnxn_gpu_x_to_nbat_x_kernel;
src/gromacs/nbnxm/cuda/nbnxm_gpu_buffer_ops_internal.cu:    const auto kernelArgs    = prepareGpuKernelArguments(
src/gromacs/nbnxm/cuda/nbnxm_gpu_buffer_ops_internal.cu:    launchGpuKernel(kernelFn, config, deviceStream, nullptr, "XbufferOps", kernelArgs);
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu: *  \brief Define CUDA implementation of nbnxn_gpu.h
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:#include "gromacs/nbnxm/gpu_types_common.h"
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:#include "gromacs/nbnxm/nbnxm_gpu.h"
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:#include "gromacs/gpu_utils/devicebuffer.h"
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:#include "gromacs/gpu_utils/gpu_utils.h"
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:#include "gromacs/gpu_utils/gpueventsynchronizer.h"
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:#include "gromacs/gpu_utils/typecasts_cuda_hip.h"
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:#include "gromacs/gpu_utils/vectype_ops_cuda.h"
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:#include "gromacs/nbnxm/gpu_common.h"
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:#include "gromacs/nbnxm/gpu_common_utils.h"
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:#include "gromacs/nbnxm/gpu_data_mgmt.h"
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:#include "gromacs/nbnxm/nbnxm_gpu_data_mgmt.h"
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:#include "gromacs/timing/gpu_timing.h"
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:#include "nbnxm_cuda.h"
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:#include "nbnxm_cuda_kernel_utils.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:#include "nbnxm_cuda_types.h"
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:#include "nbnxm_cuda_kernels.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:#include "nbnxm_cuda_kernels.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:#include "nbnxm_cuda_kernels.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:#include "nbnxm_cuda_kernels.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:#include "nbnxm_cuda_kernel_pruneonly.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:#if GMX_CUDA_NB_SINGLE_COMPILATION_UNIT
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:#    include "nbnxm_cuda_kernel_F_noprune.cu"
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:#    include "nbnxm_cuda_kernel_F_prune.cu"
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:#    include "nbnxm_cuda_kernel_VF_noprune.cu"
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:#    include "nbnxm_cuda_kernel_VF_prune.cu"
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:#    include "nbnxm_cuda_kernel_pruneonly.cu"
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:#endif /* GMX_CUDA_NB_SINGLE_COMPILATION_UNIT */
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:#include "nbnxm_cuda_kernel_sci_sort.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:typedef void (*nbnxn_cu_kfunc_ptr_t)(const NBAtomDataGpu, const NBParamGpu, const GpuPairlist, bool);
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:/*! Returns the number of blocks to be used for the nonbonded GPU kernel. */
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:    /* CUDA does not accept grid dimension of 0 (which can happen e.g. with an
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu: *  defined in nbnxn_cuda_types.h.
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:    { nbnxn_kernel_ElecCut_VdwLJ_F_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecCut_VdwLJCombGeom_F_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecCut_VdwLJCombLB_F_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecCut_VdwLJFsw_F_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecCut_VdwLJPsw_F_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecCut_VdwLJEwCombGeom_F_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecCut_VdwLJEwCombLB_F_cuda },
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:    { nbnxn_kernel_ElecRF_VdwLJ_F_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecRF_VdwLJCombGeom_F_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecRF_VdwLJCombLB_F_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecRF_VdwLJFsw_F_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecRF_VdwLJPsw_F_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecRF_VdwLJEwCombGeom_F_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecRF_VdwLJEwCombLB_F_cuda },
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:    { nbnxn_kernel_ElecEwQSTab_VdwLJ_F_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwQSTab_VdwLJCombGeom_F_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwQSTab_VdwLJCombLB_F_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwQSTab_VdwLJFsw_F_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwQSTab_VdwLJPsw_F_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwQSTab_VdwLJEwCombGeom_F_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwQSTab_VdwLJEwCombLB_F_cuda },
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:    { nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJ_F_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJCombGeom_F_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJCombLB_F_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJFsw_F_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJPsw_F_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJEwCombGeom_F_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJEwCombLB_F_cuda },
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:    { nbnxn_kernel_ElecEw_VdwLJ_F_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEw_VdwLJCombGeom_F_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEw_VdwLJCombLB_F_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEw_VdwLJFsw_F_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEw_VdwLJPsw_F_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEw_VdwLJEwCombGeom_F_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEw_VdwLJEwCombLB_F_cuda },
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:    { nbnxn_kernel_ElecEwTwinCut_VdwLJ_F_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwTwinCut_VdwLJCombGeom_F_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwTwinCut_VdwLJCombLB_F_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwTwinCut_VdwLJFsw_F_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwTwinCut_VdwLJPsw_F_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwTwinCut_VdwLJEwCombGeom_F_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwTwinCut_VdwLJEwCombLB_F_cuda }
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:    { nbnxn_kernel_ElecCut_VdwLJ_VF_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecCut_VdwLJCombGeom_VF_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecCut_VdwLJCombLB_VF_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecCut_VdwLJFsw_VF_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecCut_VdwLJPsw_VF_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecCut_VdwLJEwCombGeom_VF_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecCut_VdwLJEwCombLB_VF_cuda },
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:    { nbnxn_kernel_ElecRF_VdwLJ_VF_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecRF_VdwLJCombGeom_VF_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecRF_VdwLJCombLB_VF_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecRF_VdwLJFsw_VF_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecRF_VdwLJPsw_VF_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecRF_VdwLJEwCombGeom_VF_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecRF_VdwLJEwCombLB_VF_cuda },
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:    { nbnxn_kernel_ElecEwQSTab_VdwLJ_VF_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwQSTab_VdwLJCombGeom_VF_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwQSTab_VdwLJCombLB_VF_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwQSTab_VdwLJFsw_VF_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwQSTab_VdwLJPsw_VF_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwQSTab_VdwLJEwCombGeom_VF_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwQSTab_VdwLJEwCombLB_VF_cuda },
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:    { nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJ_VF_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJCombGeom_VF_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJCombLB_VF_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJFsw_VF_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJPsw_VF_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJEwCombGeom_VF_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJEwCombLB_VF_cuda },
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:    { nbnxn_kernel_ElecEw_VdwLJ_VF_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEw_VdwLJCombGeom_VF_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEw_VdwLJCombLB_VF_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEw_VdwLJFsw_VF_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEw_VdwLJPsw_VF_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEw_VdwLJEwCombGeom_VF_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEw_VdwLJEwCombLB_VF_cuda },
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:    { nbnxn_kernel_ElecEwTwinCut_VdwLJ_VF_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwTwinCut_VdwLJCombGeom_VF_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwTwinCut_VdwLJCombLB_VF_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwTwinCut_VdwLJFsw_VF_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwTwinCut_VdwLJPsw_VF_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwTwinCut_VdwLJEwCombGeom_VF_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwTwinCut_VdwLJEwCombLB_VF_cuda }
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:    { nbnxn_kernel_ElecCut_VdwLJ_F_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecCut_VdwLJCombGeom_F_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecCut_VdwLJCombLB_F_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecCut_VdwLJFsw_F_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecCut_VdwLJPsw_F_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecCut_VdwLJEwCombGeom_F_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecCut_VdwLJEwCombLB_F_prune_cuda },
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:    { nbnxn_kernel_ElecRF_VdwLJ_F_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecRF_VdwLJCombGeom_F_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecRF_VdwLJCombLB_F_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecRF_VdwLJFsw_F_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecRF_VdwLJPsw_F_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecRF_VdwLJEwCombGeom_F_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecRF_VdwLJEwCombLB_F_prune_cuda },
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:    { nbnxn_kernel_ElecEwQSTab_VdwLJ_F_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwQSTab_VdwLJCombGeom_F_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwQSTab_VdwLJCombLB_F_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwQSTab_VdwLJFsw_F_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwQSTab_VdwLJPsw_F_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwQSTab_VdwLJEwCombGeom_F_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwQSTab_VdwLJEwCombLB_F_prune_cuda },
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:    { nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJ_F_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJCombGeom_F_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJCombLB_F_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJFsw_F_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJPsw_F_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJEwCombGeom_F_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJEwCombLB_F_prune_cuda },
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:    { nbnxn_kernel_ElecEw_VdwLJ_F_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEw_VdwLJCombGeom_F_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEw_VdwLJCombLB_F_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEw_VdwLJFsw_F_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEw_VdwLJPsw_F_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEw_VdwLJEwCombGeom_F_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEw_VdwLJEwCombLB_F_prune_cuda },
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:    { nbnxn_kernel_ElecEwTwinCut_VdwLJ_F_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwTwinCut_VdwLJCombGeom_F_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwTwinCut_VdwLJCombLB_F_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwTwinCut_VdwLJFsw_F_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwTwinCut_VdwLJPsw_F_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwTwinCut_VdwLJEwCombGeom_F_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwTwinCut_VdwLJEwCombLB_F_prune_cuda }
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:    { nbnxn_kernel_ElecCut_VdwLJ_VF_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecCut_VdwLJCombGeom_VF_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecCut_VdwLJCombLB_VF_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecCut_VdwLJFsw_VF_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecCut_VdwLJPsw_VF_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecCut_VdwLJEwCombGeom_VF_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecCut_VdwLJEwCombLB_VF_prune_cuda },
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:    { nbnxn_kernel_ElecRF_VdwLJ_VF_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecRF_VdwLJCombGeom_VF_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecRF_VdwLJCombLB_VF_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecRF_VdwLJFsw_VF_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecRF_VdwLJPsw_VF_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecRF_VdwLJEwCombGeom_VF_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecRF_VdwLJEwCombLB_VF_prune_cuda },
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:    { nbnxn_kernel_ElecEwQSTab_VdwLJ_VF_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwQSTab_VdwLJCombGeom_VF_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwQSTab_VdwLJCombLB_VF_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwQSTab_VdwLJFsw_VF_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwQSTab_VdwLJPsw_VF_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwQSTab_VdwLJEwCombGeom_VF_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwQSTab_VdwLJEwCombLB_VF_prune_cuda },
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:    { nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJ_VF_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJCombGeom_VF_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJCombLB_VF_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJFsw_VF_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJPsw_VF_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJEwCombGeom_VF_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwQSTabTwinCut_VdwLJEwCombLB_VF_prune_cuda },
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:    { nbnxn_kernel_ElecEw_VdwLJ_VF_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEw_VdwLJCombGeom_VF_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEw_VdwLJCombLB_VF_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEw_VdwLJFsw_VF_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEw_VdwLJPsw_VF_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEw_VdwLJEwCombGeom_VF_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEw_VdwLJEwCombLB_VF_prune_cuda },
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:    { nbnxn_kernel_ElecEwTwinCut_VdwLJ_VF_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwTwinCut_VdwLJCombGeom_VF_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwTwinCut_VdwLJCombLB_VF_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwTwinCut_VdwLJFsw_VF_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwTwinCut_VdwLJPsw_VF_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwTwinCut_VdwLJEwCombGeom_VF_prune_cuda,
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:      nbnxn_kernel_ElecEwTwinCut_VdwLJEwCombLB_VF_prune_cuda }
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:               "The electrostatics type requested is not implemented in the CUDA kernels.");
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:               "The VdW type requested is not implemented in the CUDA kernels.");
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:               "The CUDA kernels require the "
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:               "cluster_size_i*cluster_size_j/nbnxn_gpu_clusterpair_split to match the warp size "
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:                                                const NBParamGpu*                   nbp)
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:static inline void gpuLaunchKernelSciSort(GpuPairlist* plist, const DeviceStream& deviceStream)
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:    const auto kernelSciSortArgs = prepareGpuKernelArguments(kernelSciSort, configSortSci, plist);
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:    launchGpuKernel(kernelSciSort, configSortSci, deviceStream, nullptr, "nbnxn_kernel_sci_sort", kernelSciSortArgs);
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:void gpu_launch_kernel(NbnxmGpu* nb, const gmx::StepWorkload& stepWork, const InteractionLocality iloc)
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:    NBAtomDataGpu*      adat         = nb->atdat;
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:    NBParamGpu*         nbp          = nb->nbparam;
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:    GpuTimers*          timers       = nb->timers;
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:        gpu_launch_kernel_pruneonly(nb, iloc, 1);
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:        /* Don't launch an empty local kernel (not allowed with CUDA) */
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:                "Non-bonded GPU launch configuration:\n\tThread block: %zux%zux%zu\n\t"
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:            prepareGpuKernelArguments(kernel, config, adat, nbp, plist, &stepWork.computeVirial);
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:    launchGpuKernel(kernel, config, deviceStream, timingEvent, "k_calc_nb", kernelArgs);
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:        gpuLaunchKernelSciSort(plist, deviceStream);
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:        cudaStreamQuery(deviceStream.stream());
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:/*! Calculates the amount of shared memory required by the CUDA kernel in use. */
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:void gpu_launch_kernel_pruneonly(NbnxmGpu* nb, const InteractionLocality iloc, const int numParts)
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:    NBAtomDataGpu*      adat         = nb->atdat;
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:    NBParamGpu*         nbp          = nb->nbparam;
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:    GpuTimers*          timers       = nb->timers;
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:     * Also note that this CUDA implementation (parts tracking on device) differs from the
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:    /* Don't launch the kernel if there is no work to do (not allowed with CUDA) */
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:    GpuRegionTimer* timer = nullptr;
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:                "Pruning GPU kernel launch configuration:\n\tThread block: %zux%zux%zu\n\t"
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:            plist->haveFreshList ? nbnxn_kernel_prune_cuda<true> : nbnxn_kernel_prune_cuda<false>;
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:    const auto kernelArgs = prepareGpuKernelArguments(kernel, config, adat, nbp, plist, &numParts);
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:    launchGpuKernel(kernel, config, deviceStream, timingEvent, kernelName, kernelArgs);
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:        gpuLaunchKernelSciSort(plist, deviceStream);
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:        cudaStreamQuery(deviceStream.stream());
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:void cuda_set_cacheconfig()
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:    cudaError_t stat;
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:            cudaFuncSetCacheConfig(nb_kfunc_ener_prune_ptr[i][j], cudaFuncCachePreferEqual);
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:            cudaFuncSetCacheConfig(nb_kfunc_ener_noprune_ptr[i][j], cudaFuncCachePreferEqual);
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:            cudaFuncSetCacheConfig(nb_kfunc_noener_prune_ptr[i][j], cudaFuncCachePreferEqual);
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:            stat = cudaFuncSetCacheConfig(nb_kfunc_noener_noprune_ptr[i][j], cudaFuncCachePreferEqual);
src/gromacs/nbnxm/cuda/nbnxm_cuda.cu:            CU_RET_ERR(stat, "cudaFuncSetCacheConfig failed");
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_F_noprune.cu:#include "gromacs/gpu_utils/cudautils.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_F_noprune.cu:#include "nbnxm_cuda_kernel_utils.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_F_noprune.cu:#include "nbnxm_cuda_types.h"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_F_noprune.cu:#include "nbnxm_cuda_kernels.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_F_noprune.cu:#include "nbnxm_cuda_kernels.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_sci_sort.cuh: *  CUDA bucket sci sort kernel.
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_sci_sort.cuh: *  \author Zhengru Wang <zhengruw@nvidia.com>
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_sci_sort.cuh: *  \author Ania Brown <anbrown@nvidia.com>
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_sci_sort.cuh:/*! \brief CUDA bucket sci sort kernel.
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_sci_sort.cuh:#ifndef NBNXM_CUDA_KERNEL_SCI_SORT_CUH
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_sci_sort.cuh:#define NBNXM_CUDA_KERNEL_SCI_SORT_CUH
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_sci_sort.cuh:        void nbnxnKernelBucketSciSort(GpuPairlist plist)
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_sci_sort.cuh:#endif /* NBNXM_CUDA_KERNEL_SCI_SORT_CUH */
src/gromacs/nbnxm/cuda/nbnxm_cuda_types.h: *  Data types used internally in the nbnxn_cuda module.
src/gromacs/nbnxm/cuda/nbnxm_cuda_types.h:#ifndef NBNXM_CUDA_TYPES_H
src/gromacs/nbnxm/cuda/nbnxm_cuda_types.h:#define NBNXM_CUDA_TYPES_H
src/gromacs/nbnxm/cuda/nbnxm_cuda_types.h:#include "gromacs/gpu_utils/cuda_arch_utils.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_types.h:#include "gromacs/gpu_utils/cudautils.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_types.h:#include "gromacs/gpu_utils/devicebuffer.h"
src/gromacs/nbnxm/cuda/nbnxm_cuda_types.h:#include "gromacs/gpu_utils/devicebuffer_datatype.h"
src/gromacs/nbnxm/cuda/nbnxm_cuda_types.h:#include "gromacs/gpu_utils/gpueventsynchronizer.h"
src/gromacs/nbnxm/cuda/nbnxm_cuda_types.h:#include "gromacs/gpu_utils/gputraits.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_types.h:#include "gromacs/nbnxm/gpu_types_common.h"
src/gromacs/nbnxm/cuda/nbnxm_cuda_types.h:#include "gromacs/timing/gpu_timing.h"
src/gromacs/nbnxm/cuda/nbnxm_cuda_types.h: * \brief Main data structure for CUDA nonbonded force calculations.
src/gromacs/nbnxm/cuda/nbnxm_cuda_types.h:struct NbnxmGpu
src/gromacs/nbnxm/cuda/nbnxm_cuda_types.h:    /*! \brief GPU device context.
src/gromacs/nbnxm/cuda/nbnxm_cuda_types.h:     * \todo Make it constant reference, once NbnxmGpu is a proper class.
src/gromacs/nbnxm/cuda/nbnxm_cuda_types.h:    /*! \brief true if doing both local/non-local NB work on GPU */
src/gromacs/nbnxm/cuda/nbnxm_cuda_types.h:    NBAtomDataGpu* atdat = nullptr;
src/gromacs/nbnxm/cuda/nbnxm_cuda_types.h:    NBParamGpu* nbparam = nullptr;
src/gromacs/nbnxm/cuda/nbnxm_cuda_types.h:    EnumerationArray<InteractionLocality, std::unique_ptr<GpuPairlist>> plist = { { nullptr } };
src/gromacs/nbnxm/cuda/nbnxm_cuda_types.h:    /*! \brief local and non-local GPU streams */
src/gromacs/nbnxm/cuda/nbnxm_cuda_types.h:    GpuEventSynchronizer nonlocal_done;
src/gromacs/nbnxm/cuda/nbnxm_cuda_types.h:    GpuEventSynchronizer misc_ops_and_local_H2D_done;
src/gromacs/nbnxm/cuda/nbnxm_cuda_types.h:     * This includes local/nonlocal GPU work, either bonded or
src/gromacs/nbnxm/cuda/nbnxm_cuda_types.h:     * local/nonlocal, if there is bonded GPU work, both flags
src/gromacs/nbnxm/cuda/nbnxm_cuda_types.h:    /* NOTE: With current CUDA versions (<=5.0) timing doesn't work with multiple
src/gromacs/nbnxm/cuda/nbnxm_cuda_types.h:     * concurrent streams, so we won't time if both l/nl work is done on GPUs.
src/gromacs/nbnxm/cuda/nbnxm_cuda_types.h:     * setting bDoTime needs to be change if this CUDA "feature" gets fixed. */
src/gromacs/nbnxm/cuda/nbnxm_cuda_types.h:    /*! \brief CUDA event-based timers. */
src/gromacs/nbnxm/cuda/nbnxm_cuda_types.h:    GpuTimers* timers = nullptr;
src/gromacs/nbnxm/cuda/nbnxm_cuda_types.h:    gmx_wallclock_gpu_nbnxn_t* timings = nullptr;
src/gromacs/nbnxm/cuda/nbnxm_cuda_types.h:#endif /* NBNXN_CUDA_TYPES_H */
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernels.cuh:#include "gromacs/gpu_utils/cuda_arch_utils.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernels.cuh:#include "nbnxm_cuda_kernel.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernels.cuh:#include "nbnxm_cuda_kernel.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernels.cuh:#include "nbnxm_cuda_kernel.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernels.cuh:#include "nbnxm_cuda_kernel.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernels.cuh:#include "nbnxm_cuda_kernel.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernels.cuh:#include "nbnxm_cuda_kernel.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernels.cuh:#include "nbnxm_cuda_kernel.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernels.cuh:#include "nbnxm_cuda_kernel.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernels.cuh:#include "nbnxm_cuda_kernel.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernels.cuh:#include "nbnxm_cuda_kernel.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernels.cuh:#include "nbnxm_cuda_kernel.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernels.cuh:#include "nbnxm_cuda_kernel.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernels.cuh:#include "nbnxm_cuda_kernel.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernels.cuh:#include "nbnxm_cuda_kernel.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernels.cuh:#include "nbnxm_cuda_kernel.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernels.cuh:#include "nbnxm_cuda_kernel.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernels.cuh:#include "nbnxm_cuda_kernel.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernels.cuh:#include "nbnxm_cuda_kernel.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernels.cuh:#include "nbnxm_cuda_kernel.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernels.cuh:#include "nbnxm_cuda_kernel.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernels.cuh:#include "nbnxm_cuda_kernel.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernels.cuh:#include "nbnxm_cuda_kernel.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernels.cuh:#include "nbnxm_cuda_kernel.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernels.cuh:#include "nbnxm_cuda_kernel.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernels.cuh:#include "nbnxm_cuda_kernel.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernels.cuh:#include "nbnxm_cuda_kernel.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernels.cuh:#include "nbnxm_cuda_kernel.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernels.cuh:#include "nbnxm_cuda_kernel.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernels.cuh:#include "nbnxm_cuda_kernel.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernels.cuh:#include "nbnxm_cuda_kernel.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernels.cuh:#include "nbnxm_cuda_kernel.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernels.cuh:#include "nbnxm_cuda_kernel.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernels.cuh:#include "nbnxm_cuda_kernel.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernels.cuh:#include "nbnxm_cuda_kernel.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernels.cuh:#include "nbnxm_cuda_kernel.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernels.cuh:#include "nbnxm_cuda_kernel.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernels.cuh:#include "nbnxm_cuda_kernel.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernels.cuh:#include "nbnxm_cuda_kernel.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernels.cuh:#include "nbnxm_cuda_kernel.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernels.cuh:#include "nbnxm_cuda_kernel.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernels.cuh:#include "nbnxm_cuda_kernel.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernels.cuh:#include "nbnxm_cuda_kernel.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_VF_prune.cu:#include "gromacs/gpu_utils/cudautils.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_VF_prune.cu:#include "nbnxm_cuda_kernel_utils.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_VF_prune.cu:#include "nbnxm_cuda_types.h"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_VF_prune.cu:#include "nbnxm_cuda_kernels.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_VF_prune.cu:#include "nbnxm_cuda_kernels.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_F_prune.cu:#include "gromacs/gpu_utils/cudautils.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_F_prune.cu:#include "nbnxm_cuda_kernel_utils.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_F_prune.cu:#include "nbnxm_cuda_types.h"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_F_prune.cu:#include "nbnxm_cuda_kernels.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_F_prune.cu:#include "nbnxm_cuda_kernels.cuh"
src/gromacs/nbnxm/cuda/CMakeLists.txt:if(GMX_GPU_CUDA)
src/gromacs/nbnxm/cuda/CMakeLists.txt:    if(NOT GMX_CUDA_NB_SINGLE_COMPILATION_UNIT)
src/gromacs/nbnxm/cuda/CMakeLists.txt:        set(NBNXM_CUDA_KERNEL_SOURCES
src/gromacs/nbnxm/cuda/CMakeLists.txt:                nbnxm_cuda_kernel_F_noprune.cu
src/gromacs/nbnxm/cuda/CMakeLists.txt:                nbnxm_cuda_kernel_F_prune.cu
src/gromacs/nbnxm/cuda/CMakeLists.txt:                nbnxm_cuda_kernel_VF_noprune.cu
src/gromacs/nbnxm/cuda/CMakeLists.txt:                nbnxm_cuda_kernel_VF_prune.cu
src/gromacs/nbnxm/cuda/CMakeLists.txt:                nbnxm_cuda_kernel_pruneonly.cu)
src/gromacs/nbnxm/cuda/CMakeLists.txt:    file(GLOB NBNXM_CUDA_SOURCES
src/gromacs/nbnxm/cuda/CMakeLists.txt:         nbnxm_cuda.cu
src/gromacs/nbnxm/cuda/CMakeLists.txt:         nbnxm_cuda_data_mgmt.cu
src/gromacs/nbnxm/cuda/CMakeLists.txt:         nbnxm_cuda_jit_support.cu
src/gromacs/nbnxm/cuda/CMakeLists.txt:         nbnxm_gpu_buffer_ops_internal.cu
src/gromacs/nbnxm/cuda/CMakeLists.txt:         ${NBNXM_CUDA_KERNEL_SOURCES})
src/gromacs/nbnxm/cuda/CMakeLists.txt:    set(NBNXM_SOURCES ${NBNXM_SOURCES} ${NBNXM_CUDA_SOURCES} PARENT_SCOPE)
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel.cuh: *  CUDA non-bonded kernel used through preprocessor-based code generation
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel.cuh: *  of multiple kernel flavors, see nbnxn_cuda_kernels.cuh.
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel.cuh:#include "gromacs/gpu_utils/cuda_arch_utils.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel.cuh:#include "gromacs/gpu_utils/cuda_kernel_utils.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel.cuh:#include "gromacs/gpu_utils/typecasts_cuda_hip.h"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel.cuh:/* Note that floating-point constants in CUDA code should be suffixed
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel.cuh:    - shmem     = see nbnxn_cuda.cu:calc_shmem_required_nonbonded()
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel.cuh: * NOTEs on Volta / CUDA 9 extensions:
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel.cuh:        __global__ void NB_KERNEL_FUNC_NAME(nbnxn_kernel, _VF_prune_cuda)
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel.cuh:        __global__ void NB_KERNEL_FUNC_NAME(nbnxn_kernel, _F_prune_cuda)
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel.cuh:        __global__ void NB_KERNEL_FUNC_NAME(nbnxn_kernel, _VF_cuda)
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel.cuh:        __global__ void NB_KERNEL_FUNC_NAME(nbnxn_kernel, _F_cuda)
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel.cuh:                (NBAtomDataGpu atdat, NBParamGpu nbparam, GpuPairlist plist, bool bCalcFshift)
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel.cuh:    // Full or partial unroll on Ampere (and later) GPUs is beneficial given the increased L1
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel.cuh:    // instruction cache. Tested with CUDA 11-12.
src/gromacs/nbnxm/cuda/nbnxm_cuda.h: * Declares nbnxn cuda cache and texture helper functions
src/gromacs/nbnxm/cuda/nbnxm_cuda.h:#ifndef GMX_NBNXN_CUDA_NBNXN_CUDA_H
src/gromacs/nbnxm/cuda/nbnxm_cuda.h:#define GMX_NBNXN_CUDA_NBNXN_CUDA_H
src/gromacs/nbnxm/cuda/nbnxm_cuda.h:void cuda_set_cacheconfig();
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_pruneonly.cuh: *  CUDA non-bonded prune-only kernel.
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_pruneonly.cuh:#include "gromacs/gpu_utils/cuda_arch_utils.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_pruneonly.cuh:#include "gromacs/gpu_utils/typecasts_cuda_hip.h"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_pruneonly.cuh:#include "gromacs/nbnxm/gpu_types_common.h"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_pruneonly.cuh:#include "nbnxm_cuda_kernel_utils.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_pruneonly.cuh:#include "nbnxm_cuda_types.h"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_pruneonly.cuh:/* Note that floating-point constants in CUDA code should be suffixed
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_pruneonly.cuh: * additional factors including GPU arch, #SM's, we'll accept performance tradeoffs
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_pruneonly.cuh:#define MIN_BLOCKS_PER_MP (GMX_CUDA_MAX_THREADS_PER_MP / THREADS_PER_BLOCK)
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_pruneonly.cuh: *   - shmem     = see nbnxn_cuda.cu:calc_shmem_required_prune()
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_pruneonly.cuh:        void nbnxn_kernel_prune_cuda(NBAtomDataGpu atdat, NBParamGpu nbparam, GpuPairlist plist, int numParts)
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_pruneonly.cuh:nbnxn_kernel_prune_cuda<true>(const NBAtomDataGpu, const NBParamGpu, const GpuPairlist, int);
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_pruneonly.cuh:nbnxn_kernel_prune_cuda<false>(const NBAtomDataGpu, const NBParamGpu, const GpuPairlist, int);
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_VF_noprune.cu:#include "gromacs/gpu_utils/cudautils.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_VF_noprune.cu:#include "nbnxm_cuda_kernel_utils.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_VF_noprune.cu:#include "nbnxm_cuda_types.h"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_VF_noprune.cu:#include "nbnxm_cuda_kernels.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_VF_noprune.cu:#include "nbnxm_cuda_kernels.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_utils.cuh: *  Utility constant and function declaration for the CUDA non-bonded kernels.
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_utils.cuh: *  kernels are included (has to be preceded by nbnxn_cuda_types.h).
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_utils.cuh:/* Note that floating-point constants in CUDA code should be suffixed
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_utils.cuh:#include "gromacs/gpu_utils/cuda_arch_utils.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_utils.cuh:#include "gromacs/gpu_utils/cuda_kernel_utils.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_utils.cuh:#include "gromacs/gpu_utils/vectype_ops_cuda.h"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_utils.cuh:#include "nbnxm_cuda_types.h"
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_utils.cuh:#ifndef NBNXM_CUDA_KERNEL_UTILS_CUH
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_utils.cuh:#    define NBNXM_CUDA_KERNEL_UTILS_CUH
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_utils.cuh:calculate_force_switch_F(const NBParamGpu nbparam, float c6, float c12, float inv_r, float r2, float* F_invr)
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_utils.cuh:static __forceinline__ __device__ void calculate_force_switch_F_E(const NBParamGpu nbparam,
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_utils.cuh:calculate_potential_switch_F(const NBParamGpu& nbparam, float inv_r, float r2, float* F_invr, float* E_lj)
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_utils.cuh:calculate_potential_switch_F_E(const NBParamGpu nbparam, float inv_r, float r2, float* F_invr, float* E_lj)
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_utils.cuh:static __forceinline__ __device__ float calculate_lj_ewald_c6grid(const NBParamGpu nbparam, int typei, int typej)
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_utils.cuh:#    if DISABLE_CUDA_TEXTURES
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_utils.cuh:#    endif /* DISABLE_CUDA_TEXTURES */
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_utils.cuh:static __forceinline__ __device__ void calculate_lj_ewald_comb_geom_F(const NBParamGpu nbparam,
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_utils.cuh:static __forceinline__ __device__ void calculate_lj_ewald_comb_geom_F_E(const NBParamGpu nbparam,
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_utils.cuh:static __forceinline__ __device__ float2 fetch_nbfp_comb_c6_c12(const NBParamGpu nbparam, int type)
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_utils.cuh:#    if DISABLE_CUDA_TEXTURES
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_utils.cuh:#    endif /* DISABLE_CUDA_TEXTURES */
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_utils.cuh:static __forceinline__ __device__ void calculate_lj_ewald_comb_LB_F_E(const NBParamGpu nbparam,
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_utils.cuh:static __forceinline__ __device__ float2 fetch_coulomb_force_r(const NBParamGpu nbparam, int index)
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_utils.cuh:#    if DISABLE_CUDA_TEXTURES
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_utils.cuh:#    endif // DISABLE_CUDA_TEXTURES
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_utils.cuh:static __forceinline__ __device__ float interpolate_coulomb_force_r(const NBParamGpu nbparam, float r)
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_utils.cuh:static __forceinline__ __device__ void fetch_nbfp_c6_c12(float& c6, float& c12, const NBParamGpu nbparam, int baseIndex)
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_utils.cuh:#    if DISABLE_CUDA_TEXTURES
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_utils.cuh:#    endif // DISABLE_CUDA_TEXTURES
src/gromacs/nbnxm/cuda/nbnxm_cuda_kernel_utils.cuh:#endif /* NBNXN_CUDA_KERNEL_UTILS_CUH */
src/gromacs/nbnxm/cuda/nbnxm_cuda_data_mgmt.cu: *  \brief Define CUDA implementation of nbnxn_gpu_data_mgmt.h
src/gromacs/nbnxm/cuda/nbnxm_cuda_data_mgmt.cu:// TODO We would like to move this down, but the way NbnxmGpu
src/gromacs/nbnxm/cuda/nbnxm_cuda_data_mgmt.cu://      is currently declared means this has to be before gpu_types.h
src/gromacs/nbnxm/cuda/nbnxm_cuda_data_mgmt.cu:#include "nbnxm_cuda_types.h"
src/gromacs/nbnxm/cuda/nbnxm_cuda_data_mgmt.cu:#include "gromacs/gpu_utils/cudautils.cuh"
src/gromacs/nbnxm/cuda/nbnxm_cuda_data_mgmt.cu:#include "gromacs/gpu_utils/device_context.h"
src/gromacs/nbnxm/cuda/nbnxm_cuda_data_mgmt.cu:#include "gromacs/gpu_utils/gpu_utils.h"
src/gromacs/nbnxm/cuda/nbnxm_cuda_data_mgmt.cu:#include "gromacs/gpu_utils/gpueventsynchronizer.h"
src/gromacs/nbnxm/cuda/nbnxm_cuda_data_mgmt.cu:#include "gromacs/nbnxm/gpu_data_mgmt.h"
src/gromacs/nbnxm/cuda/nbnxm_cuda_data_mgmt.cu:#include "gromacs/nbnxm/gpu_types_common.h"
src/gromacs/nbnxm/cuda/nbnxm_cuda_data_mgmt.cu:#include "gromacs/nbnxm/nbnxm_gpu.h"
src/gromacs/nbnxm/cuda/nbnxm_cuda_data_mgmt.cu:#include "gromacs/nbnxm/nbnxm_gpu_data_mgmt.h"
src/gromacs/nbnxm/cuda/nbnxm_cuda_data_mgmt.cu:#include "gromacs/timing/gpu_timing.h"
src/gromacs/nbnxm/cuda/nbnxm_cuda_data_mgmt.cu:#include "nbnxm_cuda.h"
src/gromacs/nbnxm/cuda/nbnxm_cuda_data_mgmt.cu:/* Required to stop gcc emitting multiple definition warnings as cuda_fp16.h, which is included by
src/gromacs/nbnxm/cuda/nbnxm_cuda_data_mgmt.cu: * device_atomic_functions.h used by nbnxm_cuda_types.h. Seen in cuda 10 and 11 with gcc-11. */
src/gromacs/nbnxm/cuda/nbnxm_cuda_data_mgmt.cu:static const unsigned int gpu_min_ci_balanced_factor = 44;
src/gromacs/nbnxm/cuda/nbnxm_cuda_data_mgmt.cu:static const unsigned int gpu_min_ci_balanced_factor = 61;
src/gromacs/nbnxm/cuda/nbnxm_cuda_data_mgmt.cu:void gpu_init_platform_specific(NbnxmGpu* /* nb */)
src/gromacs/nbnxm/cuda/nbnxm_cuda_data_mgmt.cu:    /* set the kernel type for the current GPU */
src/gromacs/nbnxm/cuda/nbnxm_cuda_data_mgmt.cu:    cuda_set_cacheconfig();
src/gromacs/nbnxm/cuda/nbnxm_cuda_data_mgmt.cu:void gpu_free_platform_specific(NbnxmGpu* /* nb */)
src/gromacs/nbnxm/cuda/nbnxm_cuda_data_mgmt.cu:    // Nothing specific in CUDA
src/gromacs/nbnxm/cuda/nbnxm_cuda_data_mgmt.cu:int gpu_min_ci_balanced(NbnxmGpu* nb)
src/gromacs/nbnxm/cuda/nbnxm_cuda_data_mgmt.cu:    return nb != nullptr ? gpu_min_ci_balanced_factor * nb->deviceContext_->deviceInfo().prop.multiProcessorCount
src/gromacs/nbnxm/cuda/nbnxm_cuda_data_mgmt.cu:size_t cudaCubWrapper(size_t              temporaryBufferSize,
src/gromacs/nbnxm/cuda/nbnxm_cuda_data_mgmt.cu:                      GpuPairlist*        d_plist,
src/gromacs/nbnxm/cuda/nbnxm_cuda_data_mgmt.cu:size_t getExclusiveScanWorkingArraySize(GpuPairlist* plist, const DeviceStream& deviceStream)
src/gromacs/nbnxm/cuda/nbnxm_cuda_data_mgmt.cu:    return cudaCubWrapper(0, nullptr, plist, deviceStream);
src/gromacs/nbnxm/cuda/nbnxm_cuda_data_mgmt.cu:                          GpuPairlist*        plist,
src/gromacs/nbnxm/cuda/nbnxm_cuda_data_mgmt.cu:    std::ignore = cudaCubWrapper(temporaryBufferSize, temporaryBuffer, plist, deviceStream);
src/gromacs/nbnxm/nbnxm_geometry.h:        // all other cases are GPU kernels and are handled in the gpu specific method.
src/gromacs/nbnxm/nbnxm_geometry.h:        default: return sc_gpuClusterSize(PairlistType::Count);
src/gromacs/nbnxm/nbnxm_geometry.h:        // all other cases are GPU kernels and are handled in the gpu specific method.
src/gromacs/nbnxm/nbnxm_geometry.h:        default: return sc_gpuSplitJClusterSize(PairlistType::Count);
src/gromacs/nbnxm/nbnxm_geometry.h:real nbnxmPairlistVolumeRadiusIncrease(bool useGpu, real atomDensity);
src/gromacs/nbnxm/grid.cpp:    numAtomsPerCell_((isSimple_ ? 1 : sc_gpuNumClusterPerCell(pairlistType)) * numAtomsICluster_),
src/gromacs/nbnxm/grid.cpp:        return { tlen * sc_gpuNumClusterPerCellX(geometry.pairlistType_),
src/gromacs/nbnxm/grid.cpp:                 tlen * sc_gpuNumClusterPerCellY(geometry.pairlistType_) };
src/gromacs/nbnxm/grid.cpp:        pbb_.resize(packedBoundingBoxesIndex(maxNumCells * sc_gpuNumClusterPerCell(geometry_.pairlistType_)));
src/gromacs/nbnxm/grid.cpp:        bb_.resize(maxNumCells * sc_gpuNumClusterPerCell(geometry_.pairlistType_));
src/gromacs/nbnxm/grid.cpp:            int cs_w = (c * sc_gpuNumClusterPerCell(layoutType) + s) / c_packedBoundingBoxesDimSize;
src/gromacs/nbnxm/grid.cpp:            const int                         index = c * sc_gpuNumClusterPerCell(layoutType) + s;
src/gromacs/nbnxm/grid.cpp:                                               * sc_gpuNumClusterPerCellZ(layoutType))
src/gromacs/nbnxm/grid.cpp:            dims.cellSize[XX] / sc_gpuNumClusterPerCellX(layoutType),
src/gromacs/nbnxm/grid.cpp:            dims.cellSize[YY] / sc_gpuNumClusterPerCellY(layoutType),
src/gromacs/nbnxm/grid.cpp:            ba[XX] * sc_gpuNumClusterPerCellX(layoutType) * dims.invCellSize[XX],
src/gromacs/nbnxm/grid.cpp:            ba[YY] * sc_gpuNumClusterPerCellY(layoutType) * dims.invCellSize[YY],
src/gromacs/nbnxm/grid.cpp:                   - cellOffset_ * (geometry_.isSimple_ ? 1 : sc_gpuNumClusterPerCell(layoutType));
src/gromacs/nbnxm/grid.cpp:void Grid::sortColumnsGpuGeometry(GridSetData*            gridSetData,
src/gromacs/nbnxm/grid.cpp:    const int subdiv_y = sc_gpuNumClusterPerCellX(layoutType) * subdiv_x;
src/gromacs/nbnxm/grid.cpp:    const int subdiv_z = sc_gpuNumClusterPerCellY(layoutType) * subdiv_y;
src/gromacs/nbnxm/grid.cpp:        for (int sub_z = 0; sub_z < numCellsInColumn * sc_gpuNumClusterPerCellZ(layoutType); sub_z++)
src/gromacs/nbnxm/grid.cpp:            if (sub_z % sc_gpuNumClusterPerCellZ(layoutType) == 0)
src/gromacs/nbnxm/grid.cpp:                cz             = sub_z / sc_gpuNumClusterPerCellZ(layoutType);
src/gromacs/nbnxm/grid.cpp:                numClusters_[cell] = std::min(sc_gpuNumClusterPerCell(layoutType),
src/gromacs/nbnxm/grid.cpp:            if (sc_gpuNumClusterPerCellY(layoutType) > 1)
src/gromacs/nbnxm/grid.cpp:            for (int sub_y = 0; sub_y < sc_gpuNumClusterPerCellY(layoutType); sub_y++)
src/gromacs/nbnxm/grid.cpp:                if (sc_gpuNumClusterPerCellX(layoutType) > 1)
src/gromacs/nbnxm/grid.cpp:                               ((cz * sc_gpuNumClusterPerCellY(layoutType) + sub_y) & 1) != 0,
src/gromacs/nbnxm/grid.cpp:                for (int sub_x = 0; sub_x < sc_gpuNumClusterPerCellX(layoutType); sub_x++)
src/gromacs/nbnxm/grid.cpp:                sortColumnsGpuGeometry(
src/gromacs/nbnxm/nbnxm_enums.h:/*! \brief Nbnxm electrostatic GPU kernel flavors.
src/gromacs/nbnxm/nbnxm_enums.h: *  Types of electrostatics implementations available in the GPU non-bonded
src/gromacs/nbnxm/nbnxm_enums.h: *  nbnxn_cuda.cu by the nb_*_kfunc_ptr function pointer table
src/gromacs/nbnxm/nbnxm_enums.h:/*! \brief Nbnxm VdW GPU kernel flavors.
src/gromacs/nbnxm/nbnxm_enums.h: * The enumerates values correspond to the LJ implementations in the GPU non-bonded
src/gromacs/nbnxm/nbnxm_enums.h: * nbnxn_cuda_ocl.cpp/.cu by the nb_*_kfunc_ptr function pointer table
src/gromacs/nbnxm/nbnxm_enums.h:/*! \brief Nonbonded NxN kernel types: plain C, CPU SIMD, GPU, GPU emulation */
src/gromacs/nbnxm/nbnxm_enums.h:    Gpu8x8x8,         //<! GPU kernels, will be specialized further later
src/gromacs/nbnxm/nbnxm_enums.h:    Cpu8x8x8_PlainC,  //<! Reference plain C kernel for GPU pairlist layout
src/gromacs/nbnxm/nbnxm_enums.h:static constexpr bool isGpuKernelType(const NbnxmKernelType kernelType)
src/gromacs/nbnxm/nbnxm_enums.h:    return kernelType == NbnxmKernelType::Gpu8x8x8;
src/gromacs/nbnxm/nbnxm_enums.h:    DecidedByGpuModule
src/gromacs/nbnxm/nbnxm_enums.h:static constexpr bool isGpuSpecificPairlist(const PairlistType pairlistType)
src/gromacs/nbnxm/nbnxm_enums.h://! The i- and j-cluster size for GPU lists, 8 atoms for CUDA, set at configure time for OpenCL and SYCL
src/gromacs/nbnxm/nbnxm_enums.h:#if GMX_GPU_OPENCL || GMX_GPU_SYCL
src/gromacs/nbnxm/nbnxm_enums.h:constexpr int c_nbnxnGpuClusterSize = GMX_GPU_NB_CLUSTER_SIZE;
src/gromacs/nbnxm/nbnxm_enums.h:constexpr int c_nbnxnGpuClusterSize = 8;
src/gromacs/nbnxm/nbnxm_enums.h:/*! \brief The number of clusters along a direction in a pair-search grid cell for GPU lists
src/gromacs/nbnxm/nbnxm_enums.h:constexpr int c_gpuNumClusterPerCellZ = GMX_GPU_NB_NUM_CLUSTER_PER_CELL_Z;
src/gromacs/nbnxm/nbnxm_enums.h:constexpr int c_gpuNumClusterPerCellY = GMX_GPU_NB_NUM_CLUSTER_PER_CELL_Y;
src/gromacs/nbnxm/nbnxm_enums.h:constexpr int c_gpuNumClusterPerCellX = GMX_GPU_NB_NUM_CLUSTER_PER_CELL_X;
src/gromacs/nbnxm/nbnxm_enums.h:/*! \brief The number of sub-parts used for data storage for a GPU cluster pair
src/gromacs/nbnxm/nbnxm_enums.h: * In CUDA the number of threads in a warp is 32 and we have cluster pairs
src/gromacs/nbnxm/nbnxm_enums.h:#if GMX_GPU_NB_DISABLE_CLUSTER_PAIR_SPLIT
src/gromacs/nbnxm/nbnxm_enums.h:static constexpr int c_nbnxnGpuClusterpairSplit = 1;
src/gromacs/nbnxm/nbnxm_enums.h:static constexpr int c_nbnxnGpuClusterpairSplit = 2;
src/gromacs/nbnxm/nbnxm_enums.h://! The NBNxM GPU i-cluster size in atoms for the given NBNxM GPU kernel layout
src/gromacs/nbnxm/nbnxm_enums.h:static constexpr int sc_gpuClusterSize(const PairlistType pairlistType)
src/gromacs/nbnxm/nbnxm_enums.h:        default: return detail::c_nbnxnGpuClusterSize;
src/gromacs/nbnxm/nbnxm_enums.h:static constexpr int sc_gpuNumClusterPerCellX(const PairlistType pairlistType)
src/gromacs/nbnxm/nbnxm_enums.h:        default: return detail::c_gpuNumClusterPerCellX;
src/gromacs/nbnxm/nbnxm_enums.h:static constexpr int sc_gpuNumClusterPerCellY(const PairlistType pairlistType)
src/gromacs/nbnxm/nbnxm_enums.h:        default: return detail::c_gpuNumClusterPerCellY;
src/gromacs/nbnxm/nbnxm_enums.h:static constexpr int sc_gpuNumClusterPerCellZ(const PairlistType pairlistType)
src/gromacs/nbnxm/nbnxm_enums.h:        default: return detail::c_gpuNumClusterPerCellZ;
src/gromacs/nbnxm/nbnxm_enums.h://! The NBNxM GPU super cluster size according to the kernel layout.
src/gromacs/nbnxm/nbnxm_enums.h:static constexpr int sc_gpuClusterPerSuperCluster(const PairlistType pairlistType)
src/gromacs/nbnxm/nbnxm_enums.h:    return sc_gpuNumClusterPerCellX(pairlistType) * sc_gpuNumClusterPerCellY(pairlistType)
src/gromacs/nbnxm/nbnxm_enums.h:           * sc_gpuNumClusterPerCellZ(pairlistType);
src/gromacs/nbnxm/nbnxm_enums.h://! The NBNxM GPU super cluster size according to the kernel layout.
src/gromacs/nbnxm/nbnxm_enums.h:static constexpr int sc_gpuNumClusterPerCell(const PairlistType pairlistType)
src/gromacs/nbnxm/nbnxm_enums.h:    return sc_gpuNumClusterPerCellZ(pairlistType) * sc_gpuNumClusterPerCellY(pairlistType)
src/gromacs/nbnxm/nbnxm_enums.h:           * sc_gpuNumClusterPerCellX(pairlistType);
src/gromacs/nbnxm/nbnxm_enums.h:/*! \brief The number of sub-parts used for data storage for a GPU cluster pair
src/gromacs/nbnxm/nbnxm_enums.h: * In CUDA the number of threads in a warp is 32 and we have cluster pairs
src/gromacs/nbnxm/nbnxm_enums.h:static constexpr int sc_gpuClusterPairSplit(const PairlistType pairlistType)
src/gromacs/nbnxm/nbnxm_enums.h:        default: return detail::c_nbnxnGpuClusterpairSplit;
src/gromacs/nbnxm/nbnxm_enums.h:static constexpr bool sc_gpuIsSplitPairList(const PairlistType pairlistType)
src/gromacs/nbnxm/nbnxm_enums.h:    return sc_gpuClusterPairSplit(pairlistType) != 1;
src/gromacs/nbnxm/nbnxm_enums.h:/*! \brief The size of the J clusters on the GPU, after taking pair splitting into account.
src/gromacs/nbnxm/nbnxm_enums.h:static constexpr int sc_gpuSplitJClusterSize(const PairlistType pairlistType)
src/gromacs/nbnxm/nbnxm_enums.h:    return sc_gpuClusterSize(pairlistType) / sc_gpuClusterPairSplit(pairlistType);
src/gromacs/nbnxm/nbnxm_enums.h:/*! \brief With GPU kernels we group cluster pairs in 4 to optimize memory usage
src/gromacs/nbnxm/nbnxm_enums.h:constexpr int sc_gpuJgroupSize(const PairlistType pairlistType)
src/gromacs/nbnxm/nbnxm_enums.h:    return 32 / sc_gpuClusterPerSuperCluster(pairlistType);
src/gromacs/nbnxm/nbnxm_enums.h:static constexpr int sc_gpuParallelExecutionWidth(const PairlistType pairlistType)
src/gromacs/nbnxm/nbnxm_enums.h:    return sc_gpuClusterSize(pairlistType) * sc_gpuSplitJClusterSize(pairlistType);
src/gromacs/nbnxm/nbnxm_enums.h://! The fixed size of the exclusion mask array for a half GPU cluster pair
src/gromacs/nbnxm/nbnxm_enums.h:static constexpr int sc_gpuExclSize(const PairlistType pairlistType)
src/gromacs/nbnxm/nbnxm_enums.h:    return sc_gpuParallelExecutionWidth(pairlistType);
src/gromacs/nbnxm/kernels_reference/kernel_gpu_ref.cpp:#include "kernel_gpu_ref.h"
src/gromacs/nbnxm/kernels_reference/kernel_gpu_ref.cpp:static constexpr int c_clSize = sc_gpuClusterSize(refPairlistLayoutType);
src/gromacs/nbnxm/kernels_reference/kernel_gpu_ref.cpp:void nbnxn_kernel_gpu_ref(const NbnxnPairlistGpu*    nbl,
src/gromacs/nbnxm/kernels_reference/kernel_gpu_ref.cpp:    const nbnxn_excl_t* excl[sc_gpuClusterPairSplit(refPairlistLayoutType)];
src/gromacs/nbnxm/kernels_reference/kernel_gpu_ref.cpp:                  "The neighborlist cluster size in the GPU reference kernel is %d, expected it to "
src/gromacs/nbnxm/kernels_reference/kernel_gpu_ref.cpp:                       == sci * sc_gpuClusterPerSuperCluster(refPairlistLayoutType))
src/gromacs/nbnxm/kernels_reference/kernel_gpu_ref.cpp:            for (int im = 0; im < sc_gpuClusterPerSuperCluster(refPairlistLayoutType); im++)
src/gromacs/nbnxm/kernels_reference/kernel_gpu_ref.cpp:                const int ci = sci * sc_gpuClusterPerSuperCluster(refPairlistLayoutType) + im;
src/gromacs/nbnxm/kernels_reference/kernel_gpu_ref.cpp:            for (int splitIdx = 0; splitIdx < sc_gpuClusterPairSplit(refPairlistLayoutType); splitIdx++)
src/gromacs/nbnxm/kernels_reference/kernel_gpu_ref.cpp:            for (int jm = 0; jm < sc_gpuJgroupSize(refPairlistLayoutType); jm++)
src/gromacs/nbnxm/kernels_reference/kernel_gpu_ref.cpp:                for (int im = 0; im < sc_gpuClusterPerSuperCluster(refPairlistLayoutType); im++)
src/gromacs/nbnxm/kernels_reference/kernel_gpu_ref.cpp:                         >> (jm * sc_gpuClusterPerSuperCluster(refPairlistLayoutType) + im))
src/gromacs/nbnxm/kernels_reference/kernel_gpu_ref.cpp:                        const int ci = sci * sc_gpuClusterPerSuperCluster(refPairlistLayoutType) + im;
src/gromacs/nbnxm/kernels_reference/kernel_gpu_ref.cpp:                                        sc_gpuSplitJClusterSize(refPairlistLayoutType);
src/gromacs/nbnxm/kernels_reference/kernel_gpu_ref.cpp:                                         >> (jm * sc_gpuClusterPerSuperCluster(refPairlistLayoutType) + im))
src/gromacs/nbnxm/kernels_reference/kernel_gpu_ref.cpp:                             * In CUDA one work-unit is 2 warps.
src/gromacs/nbnxm/kernels_reference/kernel_gpu_ref.cpp:                            if ((ic + 1) % (c_clSize / sc_gpuClusterPairSplit(refPairlistLayoutType)) == 0)
src/gromacs/nbnxm/kernels_reference/kernel_gpu_ref.h: * Declares GPU reference kernel
src/gromacs/nbnxm/kernels_reference/kernel_gpu_ref.h:#ifndef GMX_NBNXM_KERNELS_REFERENCE_KERNEL_GPU_REF_H
src/gromacs/nbnxm/kernels_reference/kernel_gpu_ref.h:#define GMX_NBNXM_KERNELS_REFERENCE_KERNEL_GPU_REF_H
src/gromacs/nbnxm/kernels_reference/kernel_gpu_ref.h:struct NbnxnPairlistGpu;
src/gromacs/nbnxm/kernels_reference/kernel_gpu_ref.h://! Reference (slow) kernel for nb n vs n GPU type pair lists
src/gromacs/nbnxm/kernels_reference/kernel_gpu_ref.h:void nbnxn_kernel_gpu_ref(const NbnxnPairlistGpu*    nbl,
src/gromacs/nbnxm/pairlistwork.cpp: * Implements constructors for NbnxnPairlistGpuWork
src/gromacs/nbnxm/pairlistwork.cpp:NbnxmPairlistGpuWork::ISuperClusterData::ISuperClusterData(const PairlistType layoutType) :
src/gromacs/nbnxm/pairlistwork.cpp:    bb(sc_gpuNumClusterPerCell(layoutType)),
src/gromacs/nbnxm/pairlistwork.cpp:    bbPacked(sc_gpuNumClusterPerCell(layoutType) / c_packedBoundingBoxesDimSize * c_packedBoundingBoxesSize),
src/gromacs/nbnxm/pairlistwork.cpp:    x(sc_gpuNumClusterPerCell(layoutType) * sc_gpuClusterSize(layoutType) * DIM),
src/gromacs/nbnxm/pairlistwork.cpp:    xSimd(sc_gpuNumClusterPerCell(layoutType) * sc_gpuClusterSize(layoutType) * DIM)
src/gromacs/nbnxm/pairlistwork.cpp:NbnxmPairlistGpuWork::NbnxmPairlistGpuWork(const PairlistType layoutType) :
src/gromacs/nbnxm/pairlistwork.cpp:    distanceBuffer(sc_gpuNumClusterPerCell(layoutType)),
src/gromacs/modularsimulator/forceelement.cpp:#include "gromacs/listed_forces/listed_forces_gpu.h"
src/gromacs/modularsimulator/forceelement.cpp:                                runScheduleWork->simulationWork.useGpuPme)),
src/gromacs/modularsimulator/forceelement.cpp:        if (fr_->listedForcesGpu)
src/gromacs/modularsimulator/forceelement.cpp:            fr_->listedForcesGpu->updateHaveInteractions(localTopology_->idef);
src/gromacs/modularsimulator/modularsimulator.h:                                  bool                             useGpuForUpdate);
src/gromacs/modularsimulator/simulatoralgorithm.cpp:            legacySimulatorData->fr_->nbv->useGpu(),
src/gromacs/modularsimulator/modularsimulator.cpp:                                         bool                             useGpuForUpdate)
src/gromacs/modularsimulator/modularsimulator.cpp:                    !useGpuForUpdate,
src/gromacs/modularsimulator/modularsimulator.cpp:                    "Integration on the GPU is not supported by the modular simulator.");
src/gromacs/modularsimulator/firstorderpressurecoupling.cpp:    // Coordinates are always scaled except for GPU update (not implemented currently)
src/gromacs/modularsimulator/statepropagatordata.h:#include "gromacs/gpu_utils/hostallocator.h"
src/gromacs/modularsimulator/statepropagatordata.h:                        bool               useGPU,
src/gromacs/modularsimulator/statepropagatordata.cpp:                                         bool               useGPU,
src/gromacs/modularsimulator/statepropagatordata.cpp:    if (useGPU)
src/gromacs/modularsimulator/pmeloadbalancehelper.cpp:            && inputrec->cutoff_scheme != CutoffScheme::Group && !simWorkload.useGpuPmeDecomposition);
src/gromacs/modularsimulator/pmeloadbalancehelper.cpp:            &pme_loadbal_, cr_, mdlog_, *inputrec_, box, *fr_->ic, *fr_->nbv, fr_->pmedata, fr_->nbv->useGpu());
src/gromacs/modularsimulator/pmeloadbalancehelper.cpp:    // PME grid + cut-off optimization with GPUs or PME nodes
src/gromacs/modularsimulator/pmeloadbalancehelper.cpp:    // simulationWork.useGpuPmePpCommunication as is done in main MD loop.
src/gromacs/modularsimulator/pmeloadbalancehelper.cpp:    pme_loadbal_done(pme_loadbal_, fplog_, mdlog_, fr_->nbv->useGpu());
src/gromacs/compat/utility.h: * \todo Remove when CUDA 11 is a requirement.
src/gromacs/taskassignment/findallgputasks.h: * \brief Declares routine for collecting all GPU tasks found on ranks of a node.
src/gromacs/taskassignment/findallgputasks.h:#ifndef GMX_TASKASSIGNMENT_FINDALLGPUTASKS_H
src/gromacs/taskassignment/findallgputasks.h:#define GMX_TASKASSIGNMENT_FINDALLGPUTASKS_H
src/gromacs/taskassignment/findallgputasks.h:enum class GpuTask;
src/gromacs/taskassignment/findallgputasks.h://! Container of compute tasks suitable to run on a GPU e.g. on each rank of a node.
src/gromacs/taskassignment/findallgputasks.h:using GpuTasksOnRanks = std::vector<std::vector<GpuTask>>;
src/gromacs/taskassignment/findallgputasks.h: * that are eligible for GPU execution.
src/gromacs/taskassignment/findallgputasks.h: * \param[in]  haveGpusOnThisPhysicalNode Whether there are any GPUs on this physical node.
src/gromacs/taskassignment/findallgputasks.h: * \param[in]  useGpuForNonbonded         Whether GPUs will be used for nonbonded interactions.
src/gromacs/taskassignment/findallgputasks.h: * \param[in]  useGpuForPme               Whether GPUs will be used for PME interactions.
src/gromacs/taskassignment/findallgputasks.h:std::vector<GpuTask> findGpuTasksOnThisRank(bool       haveGpusOnThisPhysicalNode,
src/gromacs/taskassignment/findallgputasks.h:                                            bool       useGpuForNonbonded,
src/gromacs/taskassignment/findallgputasks.h:                                            bool       useGpuForPme,
src/gromacs/taskassignment/findallgputasks.h: * that are eligible for GPU execution.
src/gromacs/taskassignment/findallgputasks.h:GpuTasksOnRanks findAllGpuTasksOnThisNode(ArrayRef<const GpuTask>         gpuTasksOnThisRank,
src/gromacs/taskassignment/reportgpuusage.cpp: * \brief Defines routine for reporting GPU usage.
src/gromacs/taskassignment/reportgpuusage.cpp:#include "reportgpuusage.h"
src/gromacs/taskassignment/reportgpuusage.cpp:#include "gromacs/gpu_utils/gpu_utils.h"
src/gromacs/taskassignment/reportgpuusage.cpp:/*! \brief Count and return the number of unique GPUs (per node) selected.
src/gromacs/taskassignment/reportgpuusage.cpp: * As sharing GPUs among multiple ranks is possible, the number of
src/gromacs/taskassignment/reportgpuusage.cpp: * GPUs used (per node) can be different from the number of GPU IDs
src/gromacs/taskassignment/reportgpuusage.cpp:size_t countUniqueGpuIdsUsed(ArrayRef<const GpuTaskAssignment> gpuTaskAssignmentOnRanksOfThisNode)
src/gromacs/taskassignment/reportgpuusage.cpp:    for (const auto& assignmentsOnRank : gpuTaskAssignmentOnRanksOfThisNode)
src/gromacs/taskassignment/reportgpuusage.cpp:void reportGpuUsage(const MDLogger&                   mdlog,
src/gromacs/taskassignment/reportgpuusage.cpp:                    ArrayRef<const GpuTaskAssignment> gpuTaskAssignmentOnRanksOfThisNode,
src/gromacs/taskassignment/reportgpuusage.cpp:                    size_t                            numGpuTasksOnThisNode,
src/gromacs/taskassignment/reportgpuusage.cpp:    size_t numGpusInUse = countUniqueGpuIdsUsed(gpuTaskAssignmentOnRanksOfThisNode);
src/gromacs/taskassignment/reportgpuusage.cpp:    if (numGpusInUse == 0)
src/gromacs/taskassignment/reportgpuusage.cpp:        std::string gpuIdsString;
src/gromacs/taskassignment/reportgpuusage.cpp:        for (const auto& assignmentsOnRank : gpuTaskAssignmentOnRanksOfThisNode)
src/gromacs/taskassignment/reportgpuusage.cpp:                gpuIdsString += currentSeparator;
src/gromacs/taskassignment/reportgpuusage.cpp:                gpuIdsString += "none";
src/gromacs/taskassignment/reportgpuusage.cpp:                    const char* rankType = (assignmentOnRank.task_ == GpuTask::Nonbonded ? "PP" : "PME");
src/gromacs/taskassignment/reportgpuusage.cpp:                    gpuIdsString += currentSeparator;
src/gromacs/taskassignment/reportgpuusage.cpp:                    gpuIdsString += formatString("%s:%d", rankType, assignmentOnRank.deviceId_);
src/gromacs/taskassignment/reportgpuusage.cpp:        bool bPluralGpus = numGpusInUse > 1;
src/gromacs/taskassignment/reportgpuusage.cpp:                "%zu GPU%s selected for this run.\n"
src/gromacs/taskassignment/reportgpuusage.cpp:                "Mapping of GPU IDs to the %zu GPU task%s in the %zu rank%s on this node:\n  %s\n",
src/gromacs/taskassignment/reportgpuusage.cpp:                numGpusInUse,
src/gromacs/taskassignment/reportgpuusage.cpp:                bPluralGpus ? "s" : "",
src/gromacs/taskassignment/reportgpuusage.cpp:                numGpuTasksOnThisNode,
src/gromacs/taskassignment/reportgpuusage.cpp:                (numGpuTasksOnThisNode > 1) ? "s" : "",
src/gromacs/taskassignment/reportgpuusage.cpp:                gpuIdsString.c_str());
src/gromacs/taskassignment/reportgpuusage.cpp:        // Because there is a GPU in use, there must be a PP task on a GPU.
src/gromacs/taskassignment/reportgpuusage.cpp:                "PP tasks will do (non-perturbed) short-ranged%s interactions on the GPU\n",
src/gromacs/taskassignment/reportgpuusage.cpp:                simulationWork.useGpuBonded ? " and most bonded" : "");
src/gromacs/taskassignment/reportgpuusage.cpp:                                    simulationWork.useGpuUpdate ? "GPU" : "CPU");
src/gromacs/taskassignment/reportgpuusage.cpp:            output += gmx::formatString("PME tasks will do only spread and gather on the GPU\n");
src/gromacs/taskassignment/reportgpuusage.cpp:        else if (pmeRunMode == PmeRunMode::GPU)
src/gromacs/taskassignment/reportgpuusage.cpp:            output += gmx::formatString("PME tasks will do all aspects on the GPU\n");
src/gromacs/taskassignment/reportgpuusage.cpp:        if (simulationWork.useGpuDirectCommunication)
src/gromacs/taskassignment/reportgpuusage.cpp:                    gmx::formatString("GPU direct communication will be used between MPI ranks.\n");
src/gromacs/taskassignment/reportgpuusage.cpp:        if (simulationWork.useMdGpuGraph)
src/gromacs/taskassignment/reportgpuusage.cpp:                    "CUDA Graphs will be used, provided there are no CPU force computations.\n");
src/gromacs/taskassignment/tests/usergpuids.cpp: * Tests for NonbondedOnGpuFromUser
src/gromacs/taskassignment/tests/usergpuids.cpp:#include "gromacs/taskassignment/usergpuids.h"
src/gromacs/taskassignment/tests/usergpuids.cpp:            EXPECT_EQ("0", makeGpuIdString(assignment, 1));
src/gromacs/taskassignment/tests/usergpuids.cpp:            EXPECT_EQ("0,1", makeGpuIdString(assignment, 2));
src/gromacs/taskassignment/tests/usergpuids.cpp:            EXPECT_EQ("0,0,1", makeGpuIdString(assignment, 3));
src/gromacs/taskassignment/tests/usergpuids.cpp:            auto gpuidList = parseUserGpuIdString(s);
src/gromacs/taskassignment/tests/usergpuids.cpp:            EXPECT_THAT(gpuidList, ElementsAreArray(assignment));
src/gromacs/taskassignment/tests/usergpuids.cpp:        EXPECT_EQ("1", makeGpuIdString(assignment, 1));
src/gromacs/taskassignment/tests/usergpuids.cpp:        EXPECT_EQ("1,1", makeGpuIdString(assignment, 2));
src/gromacs/taskassignment/tests/usergpuids.cpp:        EXPECT_EQ("1,1,1", makeGpuIdString(assignment, 3));
src/gromacs/taskassignment/tests/usergpuids.cpp:        EXPECT_EQ("11", makeGpuIdString(assignment, 1));
src/gromacs/taskassignment/tests/usergpuids.cpp:        EXPECT_EQ("11,11", makeGpuIdString(assignment, 2));
src/gromacs/taskassignment/tests/usergpuids.cpp:        EXPECT_EQ("11,11,11", makeGpuIdString(assignment, 3));
src/gromacs/taskassignment/tests/usergpuids.cpp:        auto gpuidList = parseUserGpuIdString(s);
src/gromacs/taskassignment/tests/usergpuids.cpp:        EXPECT_THAT(gpuidList, ElementsAreArray(assignment));
src/gromacs/taskassignment/tests/usergpuids.cpp:            EXPECT_EQ("11", makeGpuIdString(assignment, 1));
src/gromacs/taskassignment/tests/usergpuids.cpp:            EXPECT_EQ("11,12", makeGpuIdString(assignment, 2));
src/gromacs/taskassignment/tests/usergpuids.cpp:            EXPECT_EQ("11,11,12", makeGpuIdString(assignment, 3));
src/gromacs/taskassignment/tests/usergpuids.cpp:    EXPECT_EQ("", makeGpuIdString(assignment, 0));
src/gromacs/taskassignment/tests/usergpuids.cpp:TEST(GpuIdAndAssignmentStringHandlingTest, InvalidInputsThrow)
src/gromacs/taskassignment/tests/usergpuids.cpp:            EXPECT_THROW(parseUserGpuIdString(s), InvalidInputError) << "for string " << s;
src/gromacs/taskassignment/tests/usergpuids.cpp:        // strings invalid only in user GPU ID strings
src/gromacs/taskassignment/tests/usergpuids.cpp:        const char* gpuidStrings[] = { "00", "0,0" };
src/gromacs/taskassignment/tests/usergpuids.cpp:        for (const auto& s : gpuidStrings)
src/gromacs/taskassignment/tests/usergpuids.cpp:            EXPECT_THROW(parseUserGpuIdString(s), InvalidInputError) << "for string " << s;
src/gromacs/taskassignment/tests/CMakeLists.txt:        usergpuids.cpp
src/gromacs/taskassignment/usergpuids.cpp: * \brief Defines routines for handling user-specified GPU IDs.
src/gromacs/taskassignment/usergpuids.cpp:#include "gromacs/taskassignment/usergpuids.h"
src/gromacs/taskassignment/usergpuids.cpp:/*! \brief Parse a GPU ID specifier string into a container.
src/gromacs/taskassignment/usergpuids.cpp: * \param[in]   gpuIdString  String like "013" or "0,1,3" typically
src/gromacs/taskassignment/usergpuids.cpp: * \returns  A vector of numeric IDs extracted from \c gpuIdString.
src/gromacs/taskassignment/usergpuids.cpp:static std::vector<int> parseGpuDeviceIdentifierList(const std::string& gpuIdString)
src/gromacs/taskassignment/usergpuids.cpp:    auto             foundCommaDelimiters = gpuIdString.find(',') != std::string::npos;
src/gromacs/taskassignment/usergpuids.cpp:        for (const auto& c : gpuIdString)
src/gromacs/taskassignment/usergpuids.cpp:                        formatString("Invalid character in GPU ID string: \"%c\"\n", c)));
src/gromacs/taskassignment/usergpuids.cpp:        if (gpuIdString[0] == ',')
src/gromacs/taskassignment/usergpuids.cpp:            GMX_THROW(InvalidInputError("Invalid use of leading comma in GPU ID string"));
src/gromacs/taskassignment/usergpuids.cpp:        std::istringstream ss(gpuIdString);
src/gromacs/taskassignment/usergpuids.cpp:        digits.reserve(gpuIdString.length());
src/gromacs/taskassignment/usergpuids.cpp:        token.reserve(gpuIdString.length());
src/gromacs/taskassignment/usergpuids.cpp:                GMX_THROW(InvalidInputError("Invalid use of comma in GPU ID string"));
src/gromacs/taskassignment/usergpuids.cpp:std::vector<int> parseUserGpuIdString(const std::string& gpuIdString)
src/gromacs/taskassignment/usergpuids.cpp:    // An optional comma is used to separate GPU IDs assigned to the
src/gromacs/taskassignment/usergpuids.cpp:    // more than ten GPUs.
src/gromacs/taskassignment/usergpuids.cpp:    auto digits = parseGpuDeviceIdentifierList(gpuIdString);
src/gromacs/taskassignment/usergpuids.cpp:                        InvalidInputError(formatString("The string of available GPU device IDs "
src/gromacs/taskassignment/usergpuids.cpp:                                                       gpuIdString.c_str())));
src/gromacs/taskassignment/usergpuids.cpp:    std::vector<int> devicesSelectedByUser = parseUserGpuIdString(devicesSelectedByUserString);
src/gromacs/taskassignment/usergpuids.cpp:        auto message = "You requested mdrun to use GPU devices with IDs " + devicesSelectedByUserString
src/gromacs/taskassignment/usergpuids.cpp:std::vector<int> parseUserTaskAssignmentString(const std::string& gpuIdString)
src/gromacs/taskassignment/usergpuids.cpp:    return parseGpuDeviceIdentifierList(gpuIdString);
src/gromacs/taskassignment/usergpuids.cpp:std::vector<int> makeGpuIds(ArrayRef<const int> compatibleGpus, size_t numGpuTasks)
src/gromacs/taskassignment/usergpuids.cpp:    std::vector<int> gpuIdsToUse;
src/gromacs/taskassignment/usergpuids.cpp:    gpuIdsToUse.reserve(numGpuTasks);
src/gromacs/taskassignment/usergpuids.cpp:    auto currentGpuId = compatibleGpus.begin();
src/gromacs/taskassignment/usergpuids.cpp:    for (size_t i = 0; i != numGpuTasks; ++i)
src/gromacs/taskassignment/usergpuids.cpp:        GMX_ASSERT(!compatibleGpus.empty(),
src/gromacs/taskassignment/usergpuids.cpp:                   "Must have compatible GPUs from which to build a list of GPU IDs to use");
src/gromacs/taskassignment/usergpuids.cpp:        gpuIdsToUse.push_back(*currentGpuId);
src/gromacs/taskassignment/usergpuids.cpp:        ++currentGpuId;
src/gromacs/taskassignment/usergpuids.cpp:        if (currentGpuId == compatibleGpus.end())
src/gromacs/taskassignment/usergpuids.cpp:            currentGpuId = compatibleGpus.begin();
src/gromacs/taskassignment/usergpuids.cpp:    std::sort(gpuIdsToUse.begin(), gpuIdsToUse.end());
src/gromacs/taskassignment/usergpuids.cpp:    return gpuIdsToUse;
src/gromacs/taskassignment/usergpuids.cpp:std::string makeGpuIdString(const std::vector<int>& gpuIds, int totalNumberOfTasks)
src/gromacs/taskassignment/usergpuids.cpp:    auto resultGpuIds = makeGpuIds(gpuIds, totalNumberOfTasks);
src/gromacs/taskassignment/usergpuids.cpp:    return formatAndJoin(resultGpuIds, ",", StringFormatter("%d"));
src/gromacs/taskassignment/usergpuids.cpp:void checkUserGpuIds(const ArrayRef<const std::unique_ptr<DeviceInformation>> deviceInfoList,
src/gromacs/taskassignment/usergpuids.cpp:                     const ArrayRef<const int>                                compatibleGpus,
src/gromacs/taskassignment/usergpuids.cpp:                     const ArrayRef<const int>                                gpuIds)
src/gromacs/taskassignment/usergpuids.cpp:    bool        foundIncompatibleGpuIds = false;
src/gromacs/taskassignment/usergpuids.cpp:            "Some of the requested GPUs do not exist, behave strangely, or are not compatible:\n";
src/gromacs/taskassignment/usergpuids.cpp:    for (const auto& gpuId : gpuIds)
src/gromacs/taskassignment/usergpuids.cpp:        if (std::find(compatibleGpus.begin(), compatibleGpus.end(), gpuId) == compatibleGpus.end())
src/gromacs/taskassignment/usergpuids.cpp:            foundIncompatibleGpuIds = true;
src/gromacs/taskassignment/usergpuids.cpp:            message += gmx::formatString("    GPU #%d: %s\n",
src/gromacs/taskassignment/usergpuids.cpp:                                         gpuId,
src/gromacs/taskassignment/usergpuids.cpp:                                         getDeviceCompatibilityDescription(deviceInfoList, gpuId).c_str());
src/gromacs/taskassignment/usergpuids.cpp:    if (foundIncompatibleGpuIds)
src/gromacs/taskassignment/taskassignment.cpp: * Note that the GPU ID assignment could potentially match many
src/gromacs/taskassignment/taskassignment.cpp: * only for particular tasks (e.g. PME-only ranks). Which GPU ID
src/gromacs/taskassignment/taskassignment.cpp:#include "gromacs/taskassignment/usergpuids.h"
src/gromacs/taskassignment/taskassignment.cpp:#include "findallgputasks.h"
src/gromacs/taskassignment/taskassignment.cpp:#include "reportgpuusage.h"
src/gromacs/taskassignment/taskassignment.cpp:/*! \brief Build the GPU task assignment for the ranks of this node.
src/gromacs/taskassignment/taskassignment.cpp: * \param[in]   gpuTasksOnRanksOfThisNode  For each rank on this node, the set of tasks
src/gromacs/taskassignment/taskassignment.cpp: *                                         that are eligible to run on GPUs.
src/gromacs/taskassignment/taskassignment.cpp: * \param[in]   gpuIds                     The GPU IDs for the tasks on this node, supplied
src/gromacs/taskassignment/taskassignment.cpp: *              describes the GPU tasks and the assigned device ID.
src/gromacs/taskassignment/taskassignment.cpp: * \throws InvalidInputError  when the user GPU assignment requests multiple devices on a rank
src/gromacs/taskassignment/taskassignment.cpp:std::vector<GpuTaskAssignment> buildTaskAssignment(const GpuTasksOnRanks& gpuTasksOnRanksOfThisNode,
src/gromacs/taskassignment/taskassignment.cpp:                                                   ArrayRef<const int>    gpuIds)
src/gromacs/taskassignment/taskassignment.cpp:    std::vector<GpuTaskAssignment> gpuTaskAssignmentOnRanksOfThisNode(gpuTasksOnRanksOfThisNode.size());
src/gromacs/taskassignment/taskassignment.cpp:    // been any GPU tasks identified, then gpuIds can be empty.
src/gromacs/taskassignment/taskassignment.cpp:    auto currentGpuId            = gpuIds.begin();
src/gromacs/taskassignment/taskassignment.cpp:    auto gpuTaskAssignmentOnRank = gpuTaskAssignmentOnRanksOfThisNode.begin();
src/gromacs/taskassignment/taskassignment.cpp:    for (const auto& gpuTasksOnRank : gpuTasksOnRanksOfThisNode)
src/gromacs/taskassignment/taskassignment.cpp:        if (gpuTasksOnRank.empty())
src/gromacs/taskassignment/taskassignment.cpp:            ++gpuTaskAssignmentOnRank;
src/gromacs/taskassignment/taskassignment.cpp:        gpuTaskAssignmentOnRank->reserve(gpuTasksOnRank.size());
src/gromacs/taskassignment/taskassignment.cpp:        // Keep a copy of the first GPU ID for this rank so we can
src/gromacs/taskassignment/taskassignment.cpp:        // check that it is the only GPU ID used on this rank.
src/gromacs/taskassignment/taskassignment.cpp:        const int gpuIdOnRank = *currentGpuId;
src/gromacs/taskassignment/taskassignment.cpp:        for (const auto& gpuTaskType : gpuTasksOnRank)
src/gromacs/taskassignment/taskassignment.cpp:            GMX_RELEASE_ASSERT(currentGpuId != gpuIds.end(), "Indexing out of range for GPU tasks");
src/gromacs/taskassignment/taskassignment.cpp:            gpuTaskAssignmentOnRank->push_back({ gpuTaskType, gpuIdOnRank });
src/gromacs/taskassignment/taskassignment.cpp:            if (*currentGpuId != gpuIdOnRank)
src/gromacs/taskassignment/taskassignment.cpp:                        "The GPU task assignment requested mdrun to use more than one GPU device "
src/gromacs/taskassignment/taskassignment.cpp:                        "on a rank, which is not supported. Request only one GPU device per rank.";
src/gromacs/taskassignment/taskassignment.cpp:            ++currentGpuId;
src/gromacs/taskassignment/taskassignment.cpp:        GMX_RELEASE_ASSERT(gpuTaskAssignmentOnRank->size() == gpuTasksOnRank.size(),
src/gromacs/taskassignment/taskassignment.cpp:                           "Mismatch in number of GPU tasks on a rank with the number of elements "
src/gromacs/taskassignment/taskassignment.cpp:        ++gpuTaskAssignmentOnRank;
src/gromacs/taskassignment/taskassignment.cpp:    return gpuTaskAssignmentOnRanksOfThisNode;
src/gromacs/taskassignment/taskassignment.cpp:/*! \brief Return whether a GPU device is shared between any ranks.
src/gromacs/taskassignment/taskassignment.cpp: * Sharing GPUs among multiple ranks is possible via either user or
src/gromacs/taskassignment/taskassignment.cpp:bool isAnyGpuSharedBetweenRanks(ArrayRef<const GpuTaskAssignment> gpuTaskAssignments)
src/gromacs/taskassignment/taskassignment.cpp:    // any tasks on them share GPU device IDs.
src/gromacs/taskassignment/taskassignment.cpp:    for (size_t i = 0; i < gpuTaskAssignments.size(); ++i)
src/gromacs/taskassignment/taskassignment.cpp:        for (const auto& taskOnRankI : gpuTaskAssignments[i])
src/gromacs/taskassignment/taskassignment.cpp:            for (size_t j = i + 1; j < gpuTaskAssignments.size(); ++j)
src/gromacs/taskassignment/taskassignment.cpp:                for (const auto& taskOnRankJ : gpuTaskAssignments[j])
src/gromacs/taskassignment/taskassignment.cpp:void GpuTaskAssignments::logPerformanceHints(const MDLogger& mdlog, size_t numAvailableDevicesOnThisNode)
src/gromacs/taskassignment/taskassignment.cpp:    if (numAvailableDevicesOnThisNode > numGpuTasksOnThisNode_)
src/gromacs/taskassignment/taskassignment.cpp:                        "NOTE: You assigned the GPU tasks on a node such that some GPUs "
src/gromacs/taskassignment/taskassignment.cpp:    if (isAnyGpuSharedBetweenRanks(assignmentForAllRanksOnThisNode_))
src/gromacs/taskassignment/taskassignment.cpp:                        "NOTE: You assigned the same GPU ID(s) to multiple ranks, which is a good "
src/gromacs/taskassignment/taskassignment.cpp://! Counts all the GPU tasks on this node.
src/gromacs/taskassignment/taskassignment.cpp:size_t countGpuTasksOnThisNode(const GpuTasksOnRanks& gpuTasksOnRanksOfThisNode)
src/gromacs/taskassignment/taskassignment.cpp:    size_t numGpuTasksOnThisNode = 0;
src/gromacs/taskassignment/taskassignment.cpp:    for (const auto& gpuTasksOnRank : gpuTasksOnRanksOfThisNode)
src/gromacs/taskassignment/taskassignment.cpp:        numGpuTasksOnThisNode += gpuTasksOnRank.size();
src/gromacs/taskassignment/taskassignment.cpp:    return numGpuTasksOnThisNode;
src/gromacs/taskassignment/taskassignment.cpp:GpuTaskAssignmentsBuilder::GpuTaskAssignmentsBuilder() = default;
src/gromacs/taskassignment/taskassignment.cpp:GpuTaskAssignments GpuTaskAssignmentsBuilder::build(const gmx::ArrayRef<const int> availableDevices,
src/gromacs/taskassignment/taskassignment.cpp:                                                    const gmx::ArrayRef<const int> userGpuTaskAssignment,
src/gromacs/taskassignment/taskassignment.cpp:                                                    const bool       useGpuForNonbonded,
src/gromacs/taskassignment/taskassignment.cpp:                                                    const bool       useGpuForPme,
src/gromacs/taskassignment/taskassignment.cpp:    std::vector<GpuTask> gpuTasksOnThisRank = findGpuTasksOnThisRank(!availableDevices.empty(),
src/gromacs/taskassignment/taskassignment.cpp:                                                                     useGpuForNonbonded,
src/gromacs/taskassignment/taskassignment.cpp:                                                                     useGpuForPme,
src/gromacs/taskassignment/taskassignment.cpp:     * be executed on a GPU, on each rank. */
src/gromacs/taskassignment/taskassignment.cpp:    auto gpuTasksOnRanksOfThisNode = findAllGpuTasksOnThisNode(gpuTasksOnThisRank, physicalNodeComm);
src/gromacs/taskassignment/taskassignment.cpp:    size_t numGpuTasksOnThisNode = countGpuTasksOnThisNode(gpuTasksOnRanksOfThisNode);
src/gromacs/taskassignment/taskassignment.cpp:    std::vector<GpuTaskAssignment> taskAssignmentOnRanksOfThisNode;
src/gromacs/taskassignment/taskassignment.cpp:        // Use the GPU IDs from the user if they supplied
src/gromacs/taskassignment/taskassignment.cpp:        // them. Otherwise, choose from the compatible GPUs.
src/gromacs/taskassignment/taskassignment.cpp:        // GPU ID assignment strings, if provided, cover all the ranks
src/gromacs/taskassignment/taskassignment.cpp:        // heterogeneous, then the GMX_GPU_ID environment variable
src/gromacs/taskassignment/taskassignment.cpp:        // must be set by a user who also wishes to direct GPU ID
src/gromacs/taskassignment/taskassignment.cpp:        // can assume it has a GPU ID assignment appropriate for the
src/gromacs/taskassignment/taskassignment.cpp:        // Valid GPU ID assignments are `an ordered set of digits that
src/gromacs/taskassignment/taskassignment.cpp:        // identify GPU device IDs (e.g. as understood by the GPU
src/gromacs/taskassignment/taskassignment.cpp:        // with CUDA_VISIBLE_DEVICES) that will be used for the
src/gromacs/taskassignment/taskassignment.cpp:        // GPU-suitable tasks on all of the ranks of that node.
src/gromacs/taskassignment/taskassignment.cpp:        std::vector<int> generatedGpuIds;
src/gromacs/taskassignment/taskassignment.cpp:        if (userGpuTaskAssignment.empty())
src/gromacs/taskassignment/taskassignment.cpp:            ArrayRef<const int> compatibleGpusToUse = availableDevices;
src/gromacs/taskassignment/taskassignment.cpp:            // offloaded we assign both tasks to the same GPU
src/gromacs/taskassignment/taskassignment.cpp:            // regardless of how many GPUs are detected. Similarly,
src/gromacs/taskassignment/taskassignment.cpp:            // tasks on each rank to the same GPU even when more than
src/gromacs/taskassignment/taskassignment.cpp:            // N GPUs are detected.
src/gromacs/taskassignment/taskassignment.cpp:            if (numRanksOnThisNode < compatibleGpusToUse.size())
src/gromacs/taskassignment/taskassignment.cpp:                compatibleGpusToUse = compatibleGpusToUse.subArray(0, numRanksOnThisNode);
src/gromacs/taskassignment/taskassignment.cpp:            // When doing automated assignment of GPU tasks to GPU
src/gromacs/taskassignment/taskassignment.cpp:            // IDs, even if we have more than one kind of GPU task, we
src/gromacs/taskassignment/taskassignment.cpp:            generatedGpuIds = makeGpuIds(compatibleGpusToUse, numGpuTasksOnThisNode);
src/gromacs/taskassignment/taskassignment.cpp:            if ((numGpuTasksOnThisNode > availableDevices.size())
src/gromacs/taskassignment/taskassignment.cpp:                && (numGpuTasksOnThisNode % availableDevices.size() != 0))
src/gromacs/taskassignment/taskassignment.cpp:                        "There were %zu GPU tasks found on node %s, but %zu GPUs were "
src/gromacs/taskassignment/taskassignment.cpp:                        "available. If the GPUs are equivalent, then it is usually best "
src/gromacs/taskassignment/taskassignment.cpp:                        "to have a number of tasks that is a multiple of the number of GPUs. "
src/gromacs/taskassignment/taskassignment.cpp:                        "You should reconsider your GPU task assignment, "
src/gromacs/taskassignment/taskassignment.cpp:                        numGpuTasksOnThisNode,
src/gromacs/taskassignment/taskassignment.cpp:            deviceIdAssignment = generatedGpuIds;
src/gromacs/taskassignment/taskassignment.cpp:            if (numGpuTasksOnThisNode != userGpuTaskAssignment.size())
src/gromacs/taskassignment/taskassignment.cpp:                        "There were %zu GPU tasks assigned on node %s, but %zu GPU tasks were "
src/gromacs/taskassignment/taskassignment.cpp:                        "identified, and these must match. Reconsider your GPU task assignment, "
src/gromacs/taskassignment/taskassignment.cpp:                        userGpuTaskAssignment.size(),
src/gromacs/taskassignment/taskassignment.cpp:                        numGpuTasksOnThisNode)));
src/gromacs/taskassignment/taskassignment.cpp:            // Did the user choose compatible GPUs?
src/gromacs/taskassignment/taskassignment.cpp:            checkUserGpuIds(hardwareInfo.deviceInfoList, availableDevices, userGpuTaskAssignment);
src/gromacs/taskassignment/taskassignment.cpp:            deviceIdAssignment = gmx::copyOf(userGpuTaskAssignment);
src/gromacs/taskassignment/taskassignment.cpp:                buildTaskAssignment(gpuTasksOnRanksOfThisNode, deviceIdAssignment);
src/gromacs/taskassignment/taskassignment.cpp:    // TODO There is no check that mdrun -nb gpu or -pme gpu or
src/gromacs/taskassignment/taskassignment.cpp:    // -gpu_id is actually being implemented such that nonbonded tasks
src/gromacs/taskassignment/taskassignment.cpp:    // are being run on compatible GPUs, on all applicable ranks. That
src/gromacs/taskassignment/taskassignment.cpp:    GpuTaskAssignments gpuTaskAssignments(hardwareInfo);
src/gromacs/taskassignment/taskassignment.cpp:    gpuTaskAssignments.assignmentForAllRanksOnThisNode_ = taskAssignmentOnRanksOfThisNode;
src/gromacs/taskassignment/taskassignment.cpp:    gpuTaskAssignments.indexOfThisRank_                 = physicalNodeComm.rank_;
src/gromacs/taskassignment/taskassignment.cpp:    gpuTaskAssignments.numGpuTasksOnThisNode_           = numGpuTasksOnThisNode;
src/gromacs/taskassignment/taskassignment.cpp:    gpuTaskAssignments.numRanksOnThisNode_              = numRanksOnThisNode;
src/gromacs/taskassignment/taskassignment.cpp:    gpuTaskAssignments.deviceIdsAssigned_               = deviceIdAssignment;
src/gromacs/taskassignment/taskassignment.cpp:    std::sort(gpuTaskAssignments.deviceIdsAssigned_.begin(), gpuTaskAssignments.deviceIdsAssigned_.end());
src/gromacs/taskassignment/taskassignment.cpp:    gpuTaskAssignments.deviceIdsAssigned_.erase(unique(gpuTaskAssignments.deviceIdsAssigned_.begin(),
src/gromacs/taskassignment/taskassignment.cpp:                                                       gpuTaskAssignments.deviceIdsAssigned_.end()),
src/gromacs/taskassignment/taskassignment.cpp:                                                gpuTaskAssignments.deviceIdsAssigned_.end());
src/gromacs/taskassignment/taskassignment.cpp:    return gpuTaskAssignments;
src/gromacs/taskassignment/taskassignment.cpp:GpuTaskAssignments::GpuTaskAssignments(const gmx_hw_info_t& hardwareInfo) :
src/gromacs/taskassignment/taskassignment.cpp:void GpuTaskAssignments::reportGpuUsage(const MDLogger&           mdlog,
src/gromacs/taskassignment/taskassignment.cpp:    gmx::reportGpuUsage(mdlog,
src/gromacs/taskassignment/taskassignment.cpp:                        numGpuTasksOnThisNode_,
src/gromacs/taskassignment/taskassignment.cpp: * \param[in] mapping  Current GPU task mapping.
src/gromacs/taskassignment/taskassignment.cpp:template<GpuTask TaskType>
src/gromacs/taskassignment/taskassignment.cpp:static bool hasTaskType(const GpuTaskMapping& mapping)
src/gromacs/taskassignment/taskassignment.cpp:/*! \brief Function for whether the \c mapping has the GPU PME or Nonbonded task.
src/gromacs/taskassignment/taskassignment.cpp: * \param[in] mapping  Current GPU task mapping.
src/gromacs/taskassignment/taskassignment.cpp: * \returns If PME on Nonbonded GPU task was assigned to this mapping.
src/gromacs/taskassignment/taskassignment.cpp:static bool hasPmeOrNonbondedTask(const GpuTaskMapping& mapping)
src/gromacs/taskassignment/taskassignment.cpp:    return hasTaskType<GpuTask::Pme>(mapping) || hasTaskType<GpuTask::Nonbonded>(mapping);
src/gromacs/taskassignment/taskassignment.cpp:DeviceInformation* GpuTaskAssignments::initDevice() const
src/gromacs/taskassignment/taskassignment.cpp:    const GpuTaskAssignment& gpuTaskAssignment = assignmentForAllRanksOnThisNode_[indexOfThisRank_];
src/gromacs/taskassignment/taskassignment.cpp:    auto gpuTaskMapping =
src/gromacs/taskassignment/taskassignment.cpp:            std::find_if(gpuTaskAssignment.begin(), gpuTaskAssignment.end(), hasPmeOrNonbondedTask);
src/gromacs/taskassignment/taskassignment.cpp:    if (gpuTaskMapping != gpuTaskAssignment.end())
src/gromacs/taskassignment/taskassignment.cpp:        return hardwareInfo_.deviceInfoList[gpuTaskMapping->deviceId_].get();
src/gromacs/taskassignment/taskassignment.cpp:bool GpuTaskAssignments::thisRankHasPmeGpuTask() const
src/gromacs/taskassignment/taskassignment.cpp:    const GpuTaskAssignment& gpuTaskAssignment = assignmentForAllRanksOnThisNode_[indexOfThisRank_];
src/gromacs/taskassignment/taskassignment.cpp:    auto pmeGpuTaskMapping = std::find_if(
src/gromacs/taskassignment/taskassignment.cpp:            gpuTaskAssignment.begin(), gpuTaskAssignment.end(), hasTaskType<GpuTask::Pme>);
src/gromacs/taskassignment/taskassignment.cpp:    const bool thisRankHasPmeGpuTask = (pmeGpuTaskMapping != gpuTaskAssignment.end());
src/gromacs/taskassignment/taskassignment.cpp:    return thisRankHasPmeGpuTask;
src/gromacs/taskassignment/taskassignment.cpp:bool GpuTaskAssignments::thisRankHasAnyGpuTask() const
src/gromacs/taskassignment/taskassignment.cpp:    const GpuTaskAssignment& gpuTaskAssignment = assignmentForAllRanksOnThisNode_[indexOfThisRank_];
src/gromacs/taskassignment/taskassignment.cpp:    const bool thisRankHasAnyGpuTask = !gpuTaskAssignment.empty();
src/gromacs/taskassignment/taskassignment.cpp:    return thisRankHasAnyGpuTask;
src/gromacs/taskassignment/include/gromacs/taskassignment/decidesimulationworkload.h: * \brief Declares routine for deciding simulation workload based on GPU tasks.
src/gromacs/taskassignment/include/gromacs/taskassignment/decidesimulationworkload.h: * task on GPUs.
src/gromacs/taskassignment/include/gromacs/taskassignment/decidesimulationworkload.h: * \param[in] useGpuForNonbonded Whether we have short-range nonbonded interactions
src/gromacs/taskassignment/include/gromacs/taskassignment/decidesimulationworkload.h: *                               calculations on GPU(s).
src/gromacs/taskassignment/include/gromacs/taskassignment/decidesimulationworkload.h: * \param[in] useGpuForBonded    Whether bonded interactions are calculated on GPU(s).
src/gromacs/taskassignment/include/gromacs/taskassignment/decidesimulationworkload.h: * \param[in] useGpuForUpdate    Whether coordinate update and constraint solving is performed on
src/gromacs/taskassignment/include/gromacs/taskassignment/decidesimulationworkload.h: *                               GPU(s).
src/gromacs/taskassignment/include/gromacs/taskassignment/decidesimulationworkload.h: * \param[in] useGpuDirectHalo   Whether halo exchange is performed directly between GPUs.
src/gromacs/taskassignment/include/gromacs/taskassignment/decidesimulationworkload.h: * \param[in] canUseDirectGpuComm Whether direct GPU communication can be used in this run.
src/gromacs/taskassignment/include/gromacs/taskassignment/decidesimulationworkload.h: * \param[in] useGpuPmeDecomposition GPU based PME decomposition used.
src/gromacs/taskassignment/include/gromacs/taskassignment/decidesimulationworkload.h:                                            bool       useGpuForNonbonded,
src/gromacs/taskassignment/include/gromacs/taskassignment/decidesimulationworkload.h:                                            bool       useGpuForBonded,
src/gromacs/taskassignment/include/gromacs/taskassignment/decidesimulationworkload.h:                                            bool       useGpuForUpdate,
src/gromacs/taskassignment/include/gromacs/taskassignment/decidesimulationworkload.h:                                            bool       useGpuDirectHalo,
src/gromacs/taskassignment/include/gromacs/taskassignment/decidesimulationworkload.h:                                            bool       canUseDirectGpuComm,
src/gromacs/taskassignment/include/gromacs/taskassignment/decidesimulationworkload.h:                                            bool       useGpuPmeDecomposition);
src/gromacs/taskassignment/include/gromacs/taskassignment/decidesimulationworkload.h: * Also note that fr->listedForcesGpu->updateHaveInteractions() should be called before
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \brief Declares functionality for deciding whether tasks will run on GPUs.
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:#ifndef GMX_TASKASSIGNMENT_DECIDEGPUUSAGE_H
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:#define GMX_TASKASSIGNMENT_DECIDEGPUUSAGE_H
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:    Gpu
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h://! Help pass GPU-emulation parameters with type safety.
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:enum class EmulateGpuNonbonded : bool
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:    //! Do not emulate GPUs.
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:    //! Do emulate GPUs.
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:    bool enableGpuBufferOps = false;
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:    //! True if the GPU-aware MPI can be used for GPU direct communication feature
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:    bool canUseGpuAwareMpi = false;
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:    //! True if GPU PME-decomposition is enabled
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:    bool enableGpuPmeDecomposition = false;
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:    //! True if CUDA Graphs are enabled
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:    bool enableCudaGraphs = false;
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:    //! True if NVSHMEM can be used for GPU communication
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * nonbonded tasks on GPUs.
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * The number of GPU tasks and devices influences both the choice of
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \param[in] userGpuTaskAssignment        The user-specified assignment of GPU tasks to device IDs.
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \param[in] emulateGpuNonbonded          Whether we will emulate GPU calculation of nonbonded
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \param[in] buildSupportsNonbondedOnGpu  Whether GROMACS was built with GPU support.
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \param[in] nonbondedOnGpuIsUseful       Whether computing nonbonded interactions on a GPU is
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \returns    Whether the simulation will run nonbonded tasks on GPUs.
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:bool decideWhetherToUseGpusForNonbondedWithThreadMpi(TaskTarget              nonbondedTarget,
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:                                                     const std::vector<int>& userGpuTaskAssignment,
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:                                                     EmulateGpuNonbonded     emulateGpuNonbonded,
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:                                                     bool buildSupportsNonbondedOnGpu,
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:                                                     bool nonbondedOnGpuIsUseful,
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * PME tasks on GPUs.
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * The number of GPU tasks and devices influences both the choice of
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \param[in]  useGpuForNonbonded        Whether GPUs will be used for nonbonded interactions.
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \param[in]  numDevicesToUse           The number of compatible GPUs that the user permitted us to use.
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \param[in]  userGpuTaskAssignment     The user-specified assignment of GPU tasks to device IDs.
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \returns    Whether the simulation will run PME tasks on GPUs.
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:bool decideWhetherToUseGpusForPmeWithThreadMpi(bool                    useGpuForNonbonded,
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:                                               const std::vector<int>& userGpuTaskAssignment,
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * tasks on GPUs.
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * is known. But we need to know if nonbonded will run on GPUs for
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * user requires GPUs for the tasks of that duty, then it will be an
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * decideWhetherToUseGpusForNonbondedWithThreadMpi() and
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * decideWhetherToUseGpusForPmeWithThreadMpi() to help determine
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \param[in]  userGpuTaskAssignment       The user-specified assignment of GPU tasks to device IDs.
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \param[in]  emulateGpuNonbonded         Whether we will emulate GPU calculation of nonbonded interactions.
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \param[in]  buildSupportsNonbondedOnGpu Whether GROMACS was build with GPU support.
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \param[in]  nonbondedOnGpuIsUseful      Whether computing nonbonded interactions on a GPU is useful for this calculation.
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \param[in]  gpusWereDetected            Whether compatible GPUs were detected on any node.
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \returns    Whether the simulation will run nonbonded and PME tasks, respectively, on GPUs.
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:bool decideWhetherToUseGpusForNonbonded(TaskTarget              nonbondedTarget,
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:                                        const std::vector<int>& userGpuTaskAssignment,
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:                                        EmulateGpuNonbonded     emulateGpuNonbonded,
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:                                        bool                    buildSupportsNonbondedOnGpu,
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:                                        bool                    nonbondedOnGpuIsUseful,
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:                                        bool                    gpusWereDetected);
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * different types on GPUs.
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * is known. But we need to know if nonbonded will run on GPUs for
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * user requires GPUs for the tasks of that duty, then it will be an
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * decideWhetherToUseGpusForNonbondedWithThreadMpi() and
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * decideWhetherToUseGpusForPmeWithThreadMpi() to help determine
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \param[in]  useGpuForNonbonded        Whether GPUs will be used for nonbonded interactions.
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \param[in]  userGpuTaskAssignment     The user-specified assignment of GPU tasks to device IDs.
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \param[in]  gpusWereDetected          Whether compatible GPUs were detected on any node.
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \returns    Whether the simulation will run nonbonded and PME tasks, respectively, on GPUs.
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:bool decideWhetherToUseGpusForPme(bool                    useGpuForNonbonded,
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:                                  const std::vector<int>& userGpuTaskAssignment,
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:                                  bool                    gpusWereDetected);
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * Given the PME task assignment in \p useGpuForPme and the user-provided
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \note Aborts the run upon incompatible values of \p useGpuForPme and \p pmeFftTarget.
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \param[in]  useGpuForPme              PME task assignment, true if PME task is mapped to the GPU.
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:PmeRunMode determinePmeRunMode(bool useGpuForPme, const TaskTarget& pmeFftTarget, const t_inputrec& inputrec);
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:/*! \brief Decide whether the simulation will try to run bonded tasks on GPUs.
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \param[in]  useGpuForNonbonded        Whether GPUs will be used for nonbonded interactions.
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \param[in]  useGpuForPme              Whether GPUs will be used for PME interactions.
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \param[in]  gpusWereDetected          Whether compatible GPUs were detected on any node.
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \returns    Whether the simulation will run bondeded tasks on GPUs.
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:bool decideWhetherToUseGpusForBonded(bool              useGpuForNonbonded,
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:                                     bool              useGpuForPme,
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:                                     bool              gpusWereDetected);
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:/*! \brief Decide whether to use GPU for update.
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \param[in]  pmeRunMode                   PME running mode: CPU, GPU or mixed.
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \param[in]  useGpuForNonbonded           Whether GPUs will be used for nonbonded interactions.
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \param[in]  updateTarget                 User choice for running simulation on GPU.
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \param[in]  gpusWereDetected             Whether compatible GPUs were detected on any node.
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \returns    Whether complete simulation can be run on GPU.
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:bool decideWhetherToUseGpuForUpdate(bool                 isDomainDecomposition,
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:                                    bool                 useGpuForNonbonded,
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:                                    bool                 gpusWereDetected,
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:/*! \brief Decide whether direct GPU communication can be used.
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * Takes into account the build type which determines feature support as well as GPU
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * development feature flags, determines whether this run can use direct GPU communication.
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \param[in]  devFlags                     GPU development / experimental feature flags.
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \returns    Whether the MPI-parallel runs can use direct GPU communication.
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:bool decideWhetherDirectGpuCommunicationCanBeUsed(const DevelopmentFeatureFlags& devFlags,
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:/*! \brief Decide whether to use GPU for halo exchange.
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \param[in]  useGpuForNonbonded           Whether GPUs will be used for nonbonded interactions.
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \param[in]  canUseDirectGpuComm          Whether direct GPU communication can be used.
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \returns    Whether halo exchange can be run on GPU.
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:bool decideWhetherToUseGpuForHalo(bool                 havePPDomainDecomposition,
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:                                  bool                 useGpuForNonbonded,
src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:                                  bool                 canUseDirectGpuComm,
src/gromacs/taskassignment/include/gromacs/taskassignment/resourcedivision.h: * with the hardware, except that ntmpi could be larger than number of GPUs.
src/gromacs/taskassignment/include/gromacs/taskassignment/resourcedivision.h:                     bool                 nonbondedOnGpu,
src/gromacs/taskassignment/include/gromacs/taskassignment/resourcedivision.h:                     bool                 pmeOnGpu,
src/gromacs/taskassignment/include/gromacs/taskassignment/resourcedivision.h:                                        bool                 willUsePhysicalGpu,
src/gromacs/taskassignment/include/gromacs/taskassignment/usergpuids.h: * \brief Declares routines for handling user-specified GPU IDs.
src/gromacs/taskassignment/include/gromacs/taskassignment/usergpuids.h:#ifndef GMX_TASKASSIGNMENT_USERGPUIDS_H
src/gromacs/taskassignment/include/gromacs/taskassignment/usergpuids.h:#define GMX_TASKASSIGNMENT_USERGPUIDS_H
src/gromacs/taskassignment/include/gromacs/taskassignment/usergpuids.h:/*! \brief Parse a GPU ID specifier string into a container describing device IDs exposed to the run.
src/gromacs/taskassignment/include/gromacs/taskassignment/usergpuids.h: * \param[in]   gpuIdString  String like "013" or "0,1,3" typically
src/gromacs/taskassignment/include/gromacs/taskassignment/usergpuids.h: *                           supplied by the user to mdrun -gpu_id.
src/gromacs/taskassignment/include/gromacs/taskassignment/usergpuids.h: * \returns  A vector of unique GPU IDs.
src/gromacs/taskassignment/include/gromacs/taskassignment/usergpuids.h:std::vector<int> parseUserGpuIdString(const std::string& gpuIdString);
src/gromacs/taskassignment/include/gromacs/taskassignment/usergpuids.h:/*! \brief Implement GPU ID selection by returning the available GPU
src/gromacs/taskassignment/include/gromacs/taskassignment/usergpuids.h: * all compatible GPUs on this physical node. Otherwise, check the
src/gromacs/taskassignment/include/gromacs/taskassignment/usergpuids.h: * user specified compatible GPUs and return their IDs.
src/gromacs/taskassignment/include/gromacs/taskassignment/usergpuids.h: * \param[in]  deviceInfoList               Information on the GPUs on this physical node.
src/gromacs/taskassignment/include/gromacs/taskassignment/usergpuids.h: *                                          supplied by the user to mdrun -gpu_id.
src/gromacs/taskassignment/include/gromacs/taskassignment/usergpuids.h: * \returns  A vector of unique compatible GPU IDs on this physical node.
src/gromacs/taskassignment/include/gromacs/taskassignment/usergpuids.h:/*! \brief Parse a GPU ID specifier string into a container describing device ID to task mapping.
src/gromacs/taskassignment/include/gromacs/taskassignment/usergpuids.h: * \param[in]   gpuIdString  String like "0011" or "0,0,1,1" typically
src/gromacs/taskassignment/include/gromacs/taskassignment/usergpuids.h: *                           supplied by the user to mdrun -gputasks.
src/gromacs/taskassignment/include/gromacs/taskassignment/usergpuids.h: * \returns  A vector of GPU IDs.
src/gromacs/taskassignment/include/gromacs/taskassignment/usergpuids.h:std::vector<int> parseUserTaskAssignmentString(const std::string& gpuIdString);
src/gromacs/taskassignment/include/gromacs/taskassignment/usergpuids.h:/*! \brief Make a vector containing \c numGpuTasks IDs of the IDs found in \c compatibleGpus.
src/gromacs/taskassignment/include/gromacs/taskassignment/usergpuids.h: * matches that of the number of GPU tasks required. If more tasks are
src/gromacs/taskassignment/include/gromacs/taskassignment/usergpuids.h: * required than compatible GPUs, then some GPU IDs will appear more
src/gromacs/taskassignment/include/gromacs/taskassignment/usergpuids.h:std::vector<int> makeGpuIds(ArrayRef<const int> compatibleGpus, size_t numGpuTasks);
src/gromacs/taskassignment/include/gromacs/taskassignment/usergpuids.h:/*! \brief Convert a container of GPU deviced IDs to a string that
src/gromacs/taskassignment/include/gromacs/taskassignment/usergpuids.h: * can be used by gmx tune_pme as input to mdrun -gputasks.
src/gromacs/taskassignment/include/gromacs/taskassignment/usergpuids.h: * Produce a valid input for mdrun -gputasks that refers to the device
src/gromacs/taskassignment/include/gromacs/taskassignment/usergpuids.h: * IDs in \c gpuIds but produces a mapping for \c
src/gromacs/taskassignment/include/gromacs/taskassignment/usergpuids.h: * currently support filling mdrun -gputasks.
src/gromacs/taskassignment/include/gromacs/taskassignment/usergpuids.h: * \param[in]   gpuIds              Container of device IDs
src/gromacs/taskassignment/include/gromacs/taskassignment/usergpuids.h: * \returns  A string that is suitable to pass to mdrun -gputasks.
src/gromacs/taskassignment/include/gromacs/taskassignment/usergpuids.h:std::string makeGpuIdString(const std::vector<int>& gpuIds, int totalNumberOfTasks);
src/gromacs/taskassignment/include/gromacs/taskassignment/usergpuids.h:/*! \brief Check that all user-selected GPUs are compatible.
src/gromacs/taskassignment/include/gromacs/taskassignment/usergpuids.h: * Given the \c gpuIds and \c hardwareInfo, throw if
src/gromacs/taskassignment/include/gromacs/taskassignment/usergpuids.h: * any selected GPUs is not compatible.
src/gromacs/taskassignment/include/gromacs/taskassignment/usergpuids.h: * GPUs detected on the main rank are reported, because of the
src/gromacs/taskassignment/include/gromacs/taskassignment/usergpuids.h: * \todo Note that the selected GPUs can be different on each rank,
src/gromacs/taskassignment/include/gromacs/taskassignment/usergpuids.h: * and the IDs of compatible GPUs can be different on each node, so
src/gromacs/taskassignment/include/gromacs/taskassignment/usergpuids.h: * \param[in]   deviceInfoList  Information on the GPUs on this physical node.
src/gromacs/taskassignment/include/gromacs/taskassignment/usergpuids.h: * \param[in]   compatibleGpus  Vector of GPUs that are compatible
src/gromacs/taskassignment/include/gromacs/taskassignment/usergpuids.h: * \param[in]   gpuIds          The GPU IDs selected by the user.
src/gromacs/taskassignment/include/gromacs/taskassignment/usergpuids.h: *          InconsistentInputError  If the assigned GPUs are not valid
src/gromacs/taskassignment/include/gromacs/taskassignment/usergpuids.h:void checkUserGpuIds(ArrayRef<const std::unique_ptr<DeviceInformation>> deviceInfoList,
src/gromacs/taskassignment/include/gromacs/taskassignment/usergpuids.h:                     ArrayRef<const int>                                compatibleGpus,
src/gromacs/taskassignment/include/gromacs/taskassignment/usergpuids.h:                     ArrayRef<const int>                                gpuIds);
src/gromacs/taskassignment/include/gromacs/taskassignment/taskassignment.h:/*! \brief Types of compute tasks that can be run on a GPU.
src/gromacs/taskassignment/include/gromacs/taskassignment/taskassignment.h:enum class GpuTask : int
src/gromacs/taskassignment/include/gromacs/taskassignment/taskassignment.h: * \brief Specifies the GPU deviceID_ available for task_ to use. */
src/gromacs/taskassignment/include/gromacs/taskassignment/taskassignment.h:struct GpuTaskMapping
src/gromacs/taskassignment/include/gromacs/taskassignment/taskassignment.h:    //! The type of this GPU task.
src/gromacs/taskassignment/include/gromacs/taskassignment/taskassignment.h:    GpuTask task_;
src/gromacs/taskassignment/include/gromacs/taskassignment/taskassignment.h:    //! Device ID on this node to which this GPU task is mapped.
src/gromacs/taskassignment/include/gromacs/taskassignment/taskassignment.h://! Container of GPU tasks on a rank, specifying the task type and GPU device ID, e.g. potentially ready for consumption by the modules on that rank.
src/gromacs/taskassignment/include/gromacs/taskassignment/taskassignment.h:using GpuTaskAssignment = std::vector<GpuTaskMapping>;
src/gromacs/taskassignment/include/gromacs/taskassignment/taskassignment.h:class GpuTaskAssignments;
src/gromacs/taskassignment/include/gromacs/taskassignment/taskassignment.h: * \brief Builder for the GpuTaskAssignments for all ranks on this
src/gromacs/taskassignment/include/gromacs/taskassignment/taskassignment.h: * reporting, and build the GpuTaskAssignments object used to
src/gromacs/taskassignment/include/gromacs/taskassignment/taskassignment.h: * configure the modules that might run tasks on GPUs.
src/gromacs/taskassignment/include/gromacs/taskassignment/taskassignment.h: * from which we construct gpuTasksOnThisRank.
src/gromacs/taskassignment/include/gromacs/taskassignment/taskassignment.h: * GPU, if present and compatible.  This has to be coordinated
src/gromacs/taskassignment/include/gromacs/taskassignment/taskassignment.h:class GpuTaskAssignmentsBuilder
src/gromacs/taskassignment/include/gromacs/taskassignment/taskassignment.h:    GpuTaskAssignmentsBuilder();
src/gromacs/taskassignment/include/gromacs/taskassignment/taskassignment.h:    /*! \brief Builds a GpuTaskAssignments
src/gromacs/taskassignment/include/gromacs/taskassignment/taskassignment.h:     * to assign the GPUs on each physical node to the tasks on
src/gromacs/taskassignment/include/gromacs/taskassignment/taskassignment.h:     * when a/the useful GPU task assignment is not possible.
src/gromacs/taskassignment/include/gromacs/taskassignment/taskassignment.h:     * \param[in]  userGpuTaskAssignment  The user-specified assignment of GPU tasks to device IDs.
src/gromacs/taskassignment/include/gromacs/taskassignment/taskassignment.h:     * \param[in]  useGpuForNonbonded     Whether GPUs will be used for nonbonded interactions.
src/gromacs/taskassignment/include/gromacs/taskassignment/taskassignment.h:     * \param[in]  useGpuForPme           Whether GPUs will be used for PME interactions.
src/gromacs/taskassignment/include/gromacs/taskassignment/taskassignment.h:    static GpuTaskAssignments build(ArrayRef<const int>             availableDevices,
src/gromacs/taskassignment/include/gromacs/taskassignment/taskassignment.h:                                    ArrayRef<const int>             userGpuTaskAssignment,
src/gromacs/taskassignment/include/gromacs/taskassignment/taskassignment.h:                                    bool                            useGpuForNonbonded,
src/gromacs/taskassignment/include/gromacs/taskassignment/taskassignment.h:                                    bool                            useGpuForPme,
src/gromacs/taskassignment/include/gromacs/taskassignment/taskassignment.h: * \brief Contains the GPU task assignment for all ranks on this
src/gromacs/taskassignment/include/gromacs/taskassignment/taskassignment.h: * run tasks on GPUs.
src/gromacs/taskassignment/include/gromacs/taskassignment/taskassignment.h: * This assignment is made by a GpuTaskAssignmentsBuilder object. */
src/gromacs/taskassignment/include/gromacs/taskassignment/taskassignment.h:class GpuTaskAssignments
src/gromacs/taskassignment/include/gromacs/taskassignment/taskassignment.h:    GpuTaskAssignments(GpuTaskAssignments&& source) noexcept = default;
src/gromacs/taskassignment/include/gromacs/taskassignment/taskassignment.h:    friend class GpuTaskAssignmentsBuilder;
src/gromacs/taskassignment/include/gromacs/taskassignment/taskassignment.h:    GpuTaskAssignments(const gmx_hw_info_t& hardwareInfo);
src/gromacs/taskassignment/include/gromacs/taskassignment/taskassignment.h:    //! The GPU task assignment for all ranks on this node
src/gromacs/taskassignment/include/gromacs/taskassignment/taskassignment.h:    std::vector<GpuTaskAssignment> assignmentForAllRanksOnThisNode_;
src/gromacs/taskassignment/include/gromacs/taskassignment/taskassignment.h:    //! Number of GPU tasks on this node.
src/gromacs/taskassignment/include/gromacs/taskassignment/taskassignment.h:    size_t numGpuTasksOnThisNode_ = 0;
src/gromacs/taskassignment/include/gromacs/taskassignment/taskassignment.h:    /*! \brief Log a report on how GPUs are being used on
src/gromacs/taskassignment/include/gromacs/taskassignment/taskassignment.h:    void reportGpuUsage(const MDLogger&           mdlog,
src/gromacs/taskassignment/include/gromacs/taskassignment/taskassignment.h:    /*! \brief Return handle to the initialized GPU to use in this rank.
src/gromacs/taskassignment/include/gromacs/taskassignment/taskassignment.h:     * \returns Device information on the selected device. Returns nullptr if no GPU task
src/gromacs/taskassignment/include/gromacs/taskassignment/taskassignment.h:    //! Return whether this rank has a PME task running on a GPU
src/gromacs/taskassignment/include/gromacs/taskassignment/taskassignment.h:    bool thisRankHasPmeGpuTask() const;
src/gromacs/taskassignment/include/gromacs/taskassignment/taskassignment.h:    //! Return whether this rank has any task running on a GPU
src/gromacs/taskassignment/include/gromacs/taskassignment/taskassignment.h:    bool thisRankHasAnyGpuTask() const;
src/gromacs/taskassignment/decidesimulationworkload.cpp:#include "gromacs/listed_forces/listed_forces_gpu.h"
src/gromacs/taskassignment/decidesimulationworkload.cpp:#include "gromacs/taskassignment/decidegpuusage.h"
src/gromacs/taskassignment/decidesimulationworkload.cpp:                                            bool       useGpuForNonbonded,
src/gromacs/taskassignment/decidesimulationworkload.cpp:                                            bool       useGpuForBonded,
src/gromacs/taskassignment/decidesimulationworkload.cpp:                                            bool       useGpuForUpdate,
src/gromacs/taskassignment/decidesimulationworkload.cpp:                                            bool       useGpuDirectHalo,
src/gromacs/taskassignment/decidesimulationworkload.cpp:                                            bool       canUseDirectGpuComm,
src/gromacs/taskassignment/decidesimulationworkload.cpp:                                            bool       useGpuPmeDecomposition)
src/gromacs/taskassignment/decidesimulationworkload.cpp:    simulationWorkload.useCpuNonbonded = !useGpuForNonbonded;
src/gromacs/taskassignment/decidesimulationworkload.cpp:    simulationWorkload.useGpuNonbonded = useGpuForNonbonded;
src/gromacs/taskassignment/decidesimulationworkload.cpp:    simulationWorkload.useGpuPme = (pmeRunMode == PmeRunMode::GPU || pmeRunMode == PmeRunMode::Mixed);
src/gromacs/taskassignment/decidesimulationworkload.cpp:    simulationWorkload.useGpuPmeFft                    = (pmeRunMode == PmeRunMode::GPU);
src/gromacs/taskassignment/decidesimulationworkload.cpp:    simulationWorkload.useGpuBonded                    = useGpuForBonded;
src/gromacs/taskassignment/decidesimulationworkload.cpp:    simulationWorkload.useGpuUpdate                    = useGpuForUpdate;
src/gromacs/taskassignment/decidesimulationworkload.cpp:    simulationWorkload.useCpuHaloExchange = havePpDomainDecomposition && !useGpuDirectHalo;
src/gromacs/taskassignment/decidesimulationworkload.cpp:    simulationWorkload.useGpuHaloExchange = useGpuDirectHalo;
src/gromacs/taskassignment/decidesimulationworkload.cpp:    simulationWorkload.useGpuPmePpCommunication =
src/gromacs/taskassignment/decidesimulationworkload.cpp:            haveSeparatePmeRank && canUseDirectGpuComm
src/gromacs/taskassignment/decidesimulationworkload.cpp:            && (pmeRunMode == PmeRunMode::GPU || pmeRunMode == PmeRunMode::Mixed);
src/gromacs/taskassignment/decidesimulationworkload.cpp:            haveSeparatePmeRank && !simulationWorkload.useGpuPmePpCommunication;
src/gromacs/taskassignment/decidesimulationworkload.cpp:    GMX_RELEASE_ASSERT(!(simulationWorkload.useGpuPmePpCommunication
src/gromacs/taskassignment/decidesimulationworkload.cpp:                       "Cannot do PME-PP communication on both CPU and GPU");
src/gromacs/taskassignment/decidesimulationworkload.cpp:    simulationWorkload.useGpuDirectCommunication =
src/gromacs/taskassignment/decidesimulationworkload.cpp:            simulationWorkload.useGpuHaloExchange || simulationWorkload.useGpuPmePpCommunication;
src/gromacs/taskassignment/decidesimulationworkload.cpp:    simulationWorkload.useGpuPmeDecomposition       = useGpuPmeDecomposition;
src/gromacs/taskassignment/decidesimulationworkload.cpp:    const bool featuresRequireGpuBufferOps = useGpuForUpdate || simulationWorkload.useGpuDirectCommunication;
src/gromacs/taskassignment/decidesimulationworkload.cpp:    simulationWorkload.useGpuXBufferOpsWhenAllowed =
src/gromacs/taskassignment/decidesimulationworkload.cpp:            (devFlags.enableGpuBufferOps || featuresRequireGpuBufferOps) && !inputrec.useMts;
src/gromacs/taskassignment/decidesimulationworkload.cpp:    simulationWorkload.useGpuFBufferOpsWhenAllowed =
src/gromacs/taskassignment/decidesimulationworkload.cpp:            (devFlags.enableGpuBufferOps || featuresRequireGpuBufferOps) && !inputrec.useMts;
src/gromacs/taskassignment/decidesimulationworkload.cpp:    if (simulationWorkload.useGpuXBufferOpsWhenAllowed || simulationWorkload.useGpuFBufferOpsWhenAllowed)
src/gromacs/taskassignment/decidesimulationworkload.cpp:        GMX_ASSERT(simulationWorkload.useGpuNonbonded,
src/gromacs/taskassignment/decidesimulationworkload.cpp:    constexpr bool haveSyclWithGraphIncompatibleGpuFftLibrary =
src/gromacs/taskassignment/decidesimulationworkload.cpp:            GMX_GPU_SYCL && !(GMX_GPU_FFT_BBFFT || GMX_GPU_FFT_MKL || GMX_GPU_FFT_ONEMKL);
src/gromacs/taskassignment/decidesimulationworkload.cpp:    simulationWorkload.useMdGpuGraph =
src/gromacs/taskassignment/decidesimulationworkload.cpp:            devFlags.enableCudaGraphs && useGpuForUpdate
src/gromacs/taskassignment/decidesimulationworkload.cpp:            && (simulationWorkload.haveSeparatePmeRank ? simulationWorkload.useGpuPmePpCommunication : true)
src/gromacs/taskassignment/decidesimulationworkload.cpp:            && (havePpDomainDecomposition ? simulationWorkload.useGpuHaloExchange : true)
src/gromacs/taskassignment/decidesimulationworkload.cpp:            && !(haveSyclWithGraphIncompatibleGpuFftLibrary && simulationWorkload.useGpuPmeFft);
src/gromacs/taskassignment/decidesimulationworkload.cpp:    simulationWorkload.useNvshmem = devFlags.enableNvshmem && simulationWorkload.useGpuDirectCommunication;
src/gromacs/taskassignment/decidesimulationworkload.cpp:    domainWork.haveGpuBondedWork =
src/gromacs/taskassignment/decidesimulationworkload.cpp:            ((fr.listedForcesGpu != nullptr) && fr.listedForcesGpu->haveInteractions());
src/gromacs/taskassignment/decidesimulationworkload.cpp:    if (simulationWork.useGpuXBufferOpsWhenAllowed || simulationWork.useGpuFBufferOpsWhenAllowed)
src/gromacs/taskassignment/decidesimulationworkload.cpp:        GMX_ASSERT(simulationWork.useGpuNonbonded,
src/gromacs/taskassignment/decidesimulationworkload.cpp:    flags.useGpuXBufferOps = simulationWork.useGpuXBufferOpsWhenAllowed && !flags.doNeighborSearch;
src/gromacs/taskassignment/decidesimulationworkload.cpp:    flags.useGpuFBufferOps = simulationWork.useGpuFBufferOpsWhenAllowed && !flags.computeVirial;
src/gromacs/taskassignment/decidesimulationworkload.cpp:    flags.useGpuPmeFReduction =
src/gromacs/taskassignment/decidesimulationworkload.cpp:            flags.computeSlowForces && flags.useGpuFBufferOps
src/gromacs/taskassignment/decidesimulationworkload.cpp:            && (simulationWork.haveGpuPmeOnPpRank() || simulationWork.useGpuPmePpCommunication);
src/gromacs/taskassignment/decidesimulationworkload.cpp:    flags.useGpuXHalo              = simulationWork.useGpuHaloExchange && !flags.doNeighborSearch;
src/gromacs/taskassignment/decidesimulationworkload.cpp:    flags.useGpuFHalo              = simulationWork.useGpuHaloExchange && flags.useGpuFBufferOps;
src/gromacs/taskassignment/decidesimulationworkload.cpp:    flags.haveGpuPmeOnThisRank     = simulationWork.haveGpuPmeOnPpRank() && flags.computeSlowForces;
src/gromacs/taskassignment/decidesimulationworkload.cpp:             && !(flags.computeVirial || simulationWork.useGpuNonbonded || flags.haveGpuPmeOnThisRank));
src/gromacs/taskassignment/decidesimulationworkload.cpp:    // On NS steps, the buffer is cleared in stateGpu->reinit, no need to clear it twice.
src/gromacs/taskassignment/decidesimulationworkload.cpp:    flags.clearGpuFBufferEarly =
src/gromacs/taskassignment/decidesimulationworkload.cpp:            flags.useGpuFHalo && !domainWork.haveCpuLocalForceWork && !flags.doNeighborSearch;
src/gromacs/taskassignment/CMakeLists.txt:               decidegpuusage.cpp
src/gromacs/taskassignment/CMakeLists.txt:               findallgputasks.cpp
src/gromacs/taskassignment/CMakeLists.txt:               reportgpuusage.cpp
src/gromacs/taskassignment/CMakeLists.txt:               usergpuids.cpp)
src/gromacs/taskassignment/CMakeLists.txt:# special compilation to suit e.g. GPU or MPI dependencies.
src/gromacs/taskassignment/findallgputasks.cpp: * Defines routine for collecting all GPU tasks found on ranks of a node.
src/gromacs/taskassignment/findallgputasks.cpp:#include "findallgputasks.h"
src/gromacs/taskassignment/findallgputasks.cpp:#include "gromacs/taskassignment/decidegpuusage.h"
src/gromacs/taskassignment/findallgputasks.cpp:std::vector<GpuTask> findGpuTasksOnThisRank(const bool       haveGpusOnThisPhysicalNode,
src/gromacs/taskassignment/findallgputasks.cpp:                                            const bool       useGpuForNonbonded,
src/gromacs/taskassignment/findallgputasks.cpp:                                            const bool       useGpuForPme,
src/gromacs/taskassignment/findallgputasks.cpp:    std::vector<GpuTask> gpuTasksOnThisRank;
src/gromacs/taskassignment/findallgputasks.cpp:        if (useGpuForNonbonded)
src/gromacs/taskassignment/findallgputasks.cpp:            // Note that any bonded tasks on a GPU always accompany a
src/gromacs/taskassignment/findallgputasks.cpp:            if (haveGpusOnThisPhysicalNode)
src/gromacs/taskassignment/findallgputasks.cpp:                gpuTasksOnThisRank.push_back(GpuTask::Nonbonded);
src/gromacs/taskassignment/findallgputasks.cpp:            else if (nonbondedTarget == TaskTarget::Gpu)
src/gromacs/taskassignment/findallgputasks.cpp:                          "Cannot run short-ranged nonbonded interactions on a GPU because no GPU "
src/gromacs/taskassignment/findallgputasks.cpp:            else if (bondedTarget == TaskTarget::Gpu)
src/gromacs/taskassignment/findallgputasks.cpp:                          "Cannot run bonded interactions on a GPU because no GPU is detected.");
src/gromacs/taskassignment/findallgputasks.cpp:            else if (updateTarget == TaskTarget::Gpu)
src/gromacs/taskassignment/findallgputasks.cpp:                          "Cannot run coordinate update on a GPU because no GPU is detected.");
src/gromacs/taskassignment/findallgputasks.cpp:        if (useGpuForPme)
src/gromacs/taskassignment/findallgputasks.cpp:            if (haveGpusOnThisPhysicalNode)
src/gromacs/taskassignment/findallgputasks.cpp:                gpuTasksOnThisRank.push_back(GpuTask::Pme);
src/gromacs/taskassignment/findallgputasks.cpp:            else if (pmeTarget == TaskTarget::Gpu)
src/gromacs/taskassignment/findallgputasks.cpp:                gmx_fatal(FARGS, "Cannot run PME on a GPU because no GPU is detected.");
src/gromacs/taskassignment/findallgputasks.cpp:    return gpuTasksOnThisRank;
src/gromacs/taskassignment/findallgputasks.cpp://! Helper function to all-gather the vector of all GPU tasks on ranks of this node.
src/gromacs/taskassignment/findallgputasks.cpp:std::vector<GpuTask> allgatherv(ArrayRef<const GpuTask> input,
src/gromacs/taskassignment/findallgputasks.cpp:    std::vector<GpuTask> result;
src/gromacs/taskassignment/findallgputasks.cpp:        MPI_Gatherv(reinterpret_cast<std::underlying_type_t<GpuTask>*>(const_cast<GpuTask*>(input.data())),
src/gromacs/taskassignment/findallgputasks.cpp:                    reinterpret_cast<std::underlying_type_t<GpuTask>*>(result.data()),
src/gromacs/taskassignment/findallgputasks.cpp:        MPI_Bcast(reinterpret_cast<std::underlying_type_t<GpuTask>*>(result.data()),
src/gromacs/taskassignment/findallgputasks.cpp:        for (const auto& gpuTask : input)
src/gromacs/taskassignment/findallgputasks.cpp:            result.push_back(gpuTask);
src/gromacs/taskassignment/findallgputasks.cpp: * that are eligible for GPU execution.
src/gromacs/taskassignment/findallgputasks.cpp:GpuTasksOnRanks findAllGpuTasksOnThisNode(ArrayRef<const GpuTask>         gpuTasksOnThisRank,
src/gromacs/taskassignment/findallgputasks.cpp:    // Find out how many GPU tasks are on each rank on this node.
src/gromacs/taskassignment/findallgputasks.cpp:    auto numGpuTasksOnEachRankOfThisNode =
src/gromacs/taskassignment/findallgputasks.cpp:            allgather(gpuTasksOnThisRank.size(), numRanksOnThisNode, communicator);
src/gromacs/taskassignment/findallgputasks.cpp:     * GPU tasks on this node, in ascending order of rank. This
src/gromacs/taskassignment/findallgputasks.cpp:     * the GPU tasks on each rank of this node start and end within
src/gromacs/taskassignment/findallgputasks.cpp:            computeDisplacements(numGpuTasksOnEachRankOfThisNode, numRanksOnThisNode);
src/gromacs/taskassignment/findallgputasks.cpp:    auto gpuTasksOnThisNode = allgatherv(
src/gromacs/taskassignment/findallgputasks.cpp:            gpuTasksOnThisRank, numGpuTasksOnEachRankOfThisNode, displacementsForEachRank, communicator);
src/gromacs/taskassignment/findallgputasks.cpp:     * of GPU tasks into something that can be indexed like
src/gromacs/taskassignment/findallgputasks.cpp:     * gpuTasks[rankIndex][taskIndex]. */
src/gromacs/taskassignment/findallgputasks.cpp:    GpuTasksOnRanks gpuTasksOnRanksOfThisNode;
src/gromacs/taskassignment/findallgputasks.cpp:        gpuTasksOnRanksOfThisNode.emplace_back();
src/gromacs/taskassignment/findallgputasks.cpp:            gpuTasksOnRanksOfThisNode.back().push_back(gpuTasksOnThisNode[taskOnThisRankIndex]);
src/gromacs/taskassignment/findallgputasks.cpp:    return gpuTasksOnRanksOfThisNode;
src/gromacs/taskassignment/resourcedivision.cpp:/*! \brief The minimum number of atoms per thread-MPI thread when GPUs
src/gromacs/taskassignment/resourcedivision.cpp:/*! \brief The minimum number of atoms per GPU with thread-MPI
src/gromacs/taskassignment/resourcedivision.cpp:static constexpr int min_atoms_per_gpu = 900;
src/gromacs/taskassignment/resourcedivision.cpp: * With one GPU, using MPI only is almost never optimal, so we need to
src/gromacs/taskassignment/resourcedivision.cpp:constexpr int nthreads_omp_faster_gpu_fac = 2;
src/gromacs/taskassignment/resourcedivision.cpp: * With thread-mpi and multiple GPUs or one GPU and too many threads
src/gromacs/taskassignment/resourcedivision.cpp: * is divisible by the number of GPUs.
src/gromacs/taskassignment/resourcedivision.cpp:constexpr int nthreads_omp_mpi_ok_min_gpu = 2;
src/gromacs/taskassignment/resourcedivision.cpp:// Too many ranks per GPU can lead to large overhead so we cap the
src/gromacs/taskassignment/resourcedivision.cpp:constexpr int c_maxAutoTmpiRanksPerGpu = 4;
src/gromacs/taskassignment/resourcedivision.cpp:static int nthreads_omp_faster(const gmx::CpuInfo& cpuInfo, gmx_bool bUseGPU)
src/gromacs/taskassignment/resourcedivision.cpp:    if (bUseGPU)
src/gromacs/taskassignment/resourcedivision.cpp:        nth *= nthreads_omp_faster_gpu_fac;
src/gromacs/taskassignment/resourcedivision.cpp:gmx_unused static int nthreads_omp_efficient_max(int gmx_unused nrank, const gmx::CpuInfo& cpuInfo, gmx_bool bUseGPU)
src/gromacs/taskassignment/resourcedivision.cpp:        return nthreads_omp_faster(cpuInfo, bUseGPU);
src/gromacs/taskassignment/resourcedivision.cpp:                                                   int                  ngpu)
src/gromacs/taskassignment/resourcedivision.cpp:    if (ngpu > 0)
src/gromacs/taskassignment/resourcedivision.cpp:            /* In this case it is unclear if we should use 1 rank per GPU
src/gromacs/taskassignment/resourcedivision.cpp:                      "When using GPUs, setting the number of OpenMP threads without specifying "
src/gromacs/taskassignment/resourcedivision.cpp:        nrank = ngpu;
src/gromacs/taskassignment/resourcedivision.cpp:         * if we simply start as many ranks as GPUs. To avoid this, we start as few
src/gromacs/taskassignment/resourcedivision.cpp:         * tMPI ranks as necessary to avoid oversubscription and instead leave GPUs idle.
src/gromacs/taskassignment/resourcedivision.cpp:            /* #thread < #gpu is very unlikely, but if so: waste gpu(s) */
src/gromacs/taskassignment/resourcedivision.cpp:        else if (nthreads_tot > nthreads_omp_faster(cpuInfo, ngpu > 0)
src/gromacs/taskassignment/resourcedivision.cpp:                 || (ngpu > 1 && nthreads_tot / ngpu > nthreads_omp_mpi_target_max))
src/gromacs/taskassignment/resourcedivision.cpp:             * per rank. This will lead to GPU sharing by MPI ranks/threads.
src/gromacs/taskassignment/resourcedivision.cpp:            // divisible by the rank count or we would have >4 ranks per GPU (which is inefficient).
src/gromacs/taskassignment/resourcedivision.cpp:            // Don't go below nthreads_omp_mpi_ok_min_gpu OpenMP threads/rank.
src/gromacs/taskassignment/resourcedivision.cpp:                nrank = ngpu * nshare;
src/gromacs/taskassignment/resourcedivision.cpp:            } while ((nthreads_tot / nrank > nthreads_omp_mpi_target_max && nshare < c_maxAutoTmpiRanksPerGpu)
src/gromacs/taskassignment/resourcedivision.cpp:                     || (nthreads_tot / (ngpu * (nshare + 1)) >= nthreads_omp_mpi_ok_min_gpu
src/gromacs/taskassignment/resourcedivision.cpp:        if (nthreads_tot <= nthreads_omp_faster(cpuInfo, ngpu > 0))
src/gromacs/taskassignment/resourcedivision.cpp: * with the hardware, except that ntmpi could be larger than #GPU.
src/gromacs/taskassignment/resourcedivision.cpp:                     bool                 nonbondedOnGpu,
src/gromacs/taskassignment/resourcedivision.cpp:                     bool                 pmeOnGpu,
src/gromacs/taskassignment/resourcedivision.cpp:    int nthreads_hw, nthreads_tot_max, nrank, ngpu;
src/gromacs/taskassignment/resourcedivision.cpp:    if (pmeOnGpu)
src/gromacs/taskassignment/resourcedivision.cpp:                                   && pme_gpu_supports_build(nullptr)
src/gromacs/taskassignment/resourcedivision.cpp:                                   && pme_gpu_supports_input(*inputrec, nullptr),
src/gromacs/taskassignment/resourcedivision.cpp:                           "PME can't be on GPUs unless we are using PME");
src/gromacs/taskassignment/resourcedivision.cpp:        // PME on GPUs supports a single PME rank with PP running on the same or few other ranks.
src/gromacs/taskassignment/resourcedivision.cpp:        // For now, let's treat separate PME GPU rank as opt-in.
src/gromacs/taskassignment/resourcedivision.cpp:    /* nonbondedOnGpu might be false e.g. because this simulation
src/gromacs/taskassignment/resourcedivision.cpp:    ngpu = (nonbondedOnGpu ? numDevicesToUse : 0);
src/gromacs/taskassignment/resourcedivision.cpp:    nrank = get_tmpi_omp_thread_division(hwinfo, *hw_opt, nthreads_tot_max, ngpu);
src/gromacs/taskassignment/resourcedivision.cpp:        if (ngpu >= 1)
src/gromacs/taskassignment/resourcedivision.cpp:            min_atoms_per_mpi_rank = min_atoms_per_gpu;
src/gromacs/taskassignment/resourcedivision.cpp:        if (ngpu > 0 && (nrank_new % ngpu) != 0)
src/gromacs/taskassignment/resourcedivision.cpp:            /* If we use GPUs, the number of ranks must be divisible by the number of GPUs,
src/gromacs/taskassignment/resourcedivision.cpp:             * unless the GPUs are very different (and if they are, user should manually
src/gromacs/taskassignment/resourcedivision.cpp:             * Rounding down the number of ranks and limiting the total count per GPU.
src/gromacs/taskassignment/resourcedivision.cpp:            if (nrank_new > ngpu)
src/gromacs/taskassignment/resourcedivision.cpp:                nrank_new = std::min(nrank_new / ngpu, c_maxAutoTmpiRanksPerGpu) * ngpu;
src/gromacs/taskassignment/resourcedivision.cpp:                nrank_new = ngpu;
src/gromacs/taskassignment/resourcedivision.cpp:            nt_omp_max = nthreads_omp_efficient_max(nrank, cpuInfo, ngpu >= 1);
src/gromacs/taskassignment/resourcedivision.cpp:                                        bool                 willUsePhysicalGpu,
src/gromacs/taskassignment/resourcedivision.cpp:    bool anyRankIsUsingGpus = willUsePhysicalGpu;
src/gromacs/taskassignment/resourcedivision.cpp:        count[1] = int(willUsePhysicalGpu);
src/gromacs/taskassignment/resourcedivision.cpp:        anyRankIsUsingGpus = count_max[1] > 0;
src/gromacs/taskassignment/resourcedivision.cpp:    if (!anyRankIsUsingGpus)
src/gromacs/taskassignment/resourcedivision.cpp:        /* With GPUs we set the minimum number of OpenMP threads to 2 to catch
src/gromacs/taskassignment/resourcedivision.cpp:        nthreads_omp_mpi_ok_min = nthreads_omp_mpi_ok_min_gpu;
src/gromacs/taskassignment/resourcedivision.cpp:    if (cr && (cr->nnodes > 1) && !anyRankIsUsingGpus)
src/gromacs/taskassignment/resourcedivision.cpp:    GMX_UNUSED_VALUE(willUsePhysicalGpu);
src/gromacs/taskassignment/resourcedivision.cpp:            "hw_opt: nt %d ntmpi %d ntomp %d ntomp_pme %d gpu_id '%s' gputasks '%s'\n",
src/gromacs/taskassignment/resourcedivision.cpp:            hw_opt->userGpuTaskAssignment.c_str());
src/gromacs/taskassignment/resourcedivision.cpp:    /* With both non-bonded and PME on GPU, the work left on the CPU is often
src/gromacs/taskassignment/resourcedivision.cpp:     * we turn off SMT in that case. Note that PME on GPU implies that also
src/gromacs/taskassignment/resourcedivision.cpp:     * the non-bonded are computed on the GPU.
src/gromacs/taskassignment/resourcedivision.cpp:     * Also more cores per GPU usually means the CPU gets faster than the GPU.
src/gromacs/taskassignment/resourcedivision.cpp:    bool simRunsSingleRankNBAndPmeOnGpu = (cr->nnodes == 1 && pmeRunMode == PmeRunMode::GPU);
src/gromacs/taskassignment/resourcedivision.cpp:    if (canChooseNumOpenmpThreads && haveSmtSupport && simRunsSingleRankNBAndPmeOnGpu)
src/gromacs/taskassignment/reportgpuusage.h: * \brief Declares routine for reporting GPU usage.
src/gromacs/taskassignment/reportgpuusage.h:#ifndef GMX_TASKASSIGNMENT_REPORTGPUUSAGE_H
src/gromacs/taskassignment/reportgpuusage.h:#define GMX_TASKASSIGNMENT_REPORTGPUUSAGE_H
src/gromacs/taskassignment/reportgpuusage.h:struct GpuTaskMapping;
src/gromacs/taskassignment/reportgpuusage.h:using GpuTaskAssignment = std::vector<GpuTaskMapping>;
src/gromacs/taskassignment/reportgpuusage.h:/*! \brief Log a report on how GPUs are being used on
src/gromacs/taskassignment/reportgpuusage.h: * \param[in]  gpuTaskAssignmentOnRanksOfThisNode  The selected GPU IDs.
src/gromacs/taskassignment/reportgpuusage.h: * \param[in]  numGpuTasksOnThisNode               The number of GPU tasks on this node.
src/gromacs/taskassignment/reportgpuusage.h:void reportGpuUsage(const MDLogger&                   mdlog,
src/gromacs/taskassignment/reportgpuusage.h:                    ArrayRef<const GpuTaskAssignment> gpuTaskAssignmentOnRanksOfThisNode,
src/gromacs/taskassignment/reportgpuusage.h:                    size_t                            numGpuTasksOnThisNode,
src/gromacs/taskassignment/decidegpuusage.cpp: * \brief Defines functionality for deciding whether tasks will run on GPUs.
src/gromacs/taskassignment/decidegpuusage.cpp:#include "gromacs/taskassignment/decidegpuusage.h"
src/gromacs/taskassignment/decidegpuusage.cpp:#include "gromacs/listed_forces/listed_forces_gpu.h"
src/gromacs/taskassignment/decidegpuusage.cpp:#include "gromacs/mdlib/update_constrain_gpu.h"
src/gromacs/taskassignment/decidegpuusage.cpp:        "When you use mdrun -gputasks, %s must be set to non-default "
src/gromacs/taskassignment/decidegpuusage.cpp:#if GMX_GPU
src/gromacs/taskassignment/decidegpuusage.cpp:        " If you simply want to restrict which GPUs are used, then it is "
src/gromacs/taskassignment/decidegpuusage.cpp:        "better to use mdrun -gpu_id. Otherwise, setting the "
src/gromacs/taskassignment/decidegpuusage.cpp:#    if GMX_GPU_CUDA
src/gromacs/taskassignment/decidegpuusage.cpp:        "CUDA_VISIBLE_DEVICES"
src/gromacs/taskassignment/decidegpuusage.cpp:#    elif GMX_GPU_OPENCL
src/gromacs/taskassignment/decidegpuusage.cpp:        // OpenCL standard, but the only current relevant case for GROMACS
src/gromacs/taskassignment/decidegpuusage.cpp:        // is AMD OpenCL, which offers this variable.
src/gromacs/taskassignment/decidegpuusage.cpp:        "GPU_DEVICE_ORDINAL"
src/gromacs/taskassignment/decidegpuusage.cpp:#    elif GMX_GPU_SYCL && GMX_SYCL_DPCPP
src/gromacs/taskassignment/decidegpuusage.cpp:#    elif GMX_GPU_SYCL && GMX_SYCL_ACPP && GMX_ACPP_HAVE_HIP_TARGET || GMX_GPU_HIP
src/gromacs/taskassignment/decidegpuusage.cpp:        // https://rocm.docs.amd.com/en/latest/conceptual/gpu-isolation.html
src/gromacs/taskassignment/decidegpuusage.cpp:#    elif GMX_GPU_SYCL && GMX_SYCL_ACPP && GMX_ACPP_HAVE_CUDA_TARGET
src/gromacs/taskassignment/decidegpuusage.cpp:        "CUDA_VISIBLE_DEVIES"
src/gromacs/taskassignment/decidegpuusage.cpp:constexpr bool c_gpuBuildSyclWithoutGpuFft =
src/gromacs/taskassignment/decidegpuusage.cpp:        (GMX_GPU_SYCL != 0) && (GMX_GPU_FFT_MKL == 0) && (GMX_GPU_FFT_ROCFFT == 0)
src/gromacs/taskassignment/decidegpuusage.cpp:        && (GMX_GPU_FFT_VKFFT == 0) && (GMX_GPU_FFT_BBFFT == 0)
src/gromacs/taskassignment/decidegpuusage.cpp:        && (GMX_GPU_FFT_ONEMKL == 0); // NOLINT(misc-redundant-expression)
src/gromacs/taskassignment/decidegpuusage.cpp:bool decideWhetherToUseGpusForNonbondedWithThreadMpi(const TaskTarget        nonbondedTarget,
src/gromacs/taskassignment/decidegpuusage.cpp:                                                     const std::vector<int>& userGpuTaskAssignment,
src/gromacs/taskassignment/decidegpuusage.cpp:                                                     const EmulateGpuNonbonded emulateGpuNonbonded,
src/gromacs/taskassignment/decidegpuusage.cpp:                                                     const bool buildSupportsNonbondedOnGpu,
src/gromacs/taskassignment/decidegpuusage.cpp:                                                     const bool nonbondedOnGpuIsUseful,
src/gromacs/taskassignment/decidegpuusage.cpp:    // First, exclude all cases where we can't run NB on GPUs.
src/gromacs/taskassignment/decidegpuusage.cpp:    if (nonbondedTarget == TaskTarget::Cpu || emulateGpuNonbonded == EmulateGpuNonbonded::Yes
src/gromacs/taskassignment/decidegpuusage.cpp:        || !nonbondedOnGpuIsUseful || binaryReproducibilityRequested || !buildSupportsNonbondedOnGpu)
src/gromacs/taskassignment/decidegpuusage.cpp:        // If the user required NB on GPUs, we issue an error later.
src/gromacs/taskassignment/decidegpuusage.cpp:    // We now know that NB on GPUs makes sense, if we have any.
src/gromacs/taskassignment/decidegpuusage.cpp:    if (!userGpuTaskAssignment.empty())
src/gromacs/taskassignment/decidegpuusage.cpp:        // Specifying -gputasks requires specifying everything.
src/gromacs/taskassignment/decidegpuusage.cpp:    if (nonbondedTarget == TaskTarget::Gpu)
src/gromacs/taskassignment/decidegpuusage.cpp:    // Because this is thread-MPI, we already know about the GPUs that
src/gromacs/taskassignment/decidegpuusage.cpp:    // If we get here, then the user permitted or required GPUs.
src/gromacs/taskassignment/decidegpuusage.cpp:static bool decideWhetherToUseGpusForPmeFft(const TaskTarget pmeFftTarget)
src/gromacs/taskassignment/decidegpuusage.cpp:                           || (pmeFftTarget == TaskTarget::Auto && c_gpuBuildSyclWithoutGpuFft);
src/gromacs/taskassignment/decidegpuusage.cpp:static bool canUseGpusForPme(const bool        useGpuForNonbonded,
src/gromacs/taskassignment/decidegpuusage.cpp:    errorReasons.startContext("Cannot compute PME interactions on a GPU, because:");
src/gromacs/taskassignment/decidegpuusage.cpp:    errorReasons.appendIf(!useGpuForNonbonded, "Nonbonded interactions must also run on GPUs.");
src/gromacs/taskassignment/decidegpuusage.cpp:    errorReasons.appendIf(GMX_GPU_HIP, "PME with HIP not implemented yet");
src/gromacs/taskassignment/decidegpuusage.cpp:    errorReasons.appendIf(!pme_gpu_supports_build(&tempString), tempString);
src/gromacs/taskassignment/decidegpuusage.cpp:    errorReasons.appendIf(!pme_gpu_supports_input(inputrec, &tempString), tempString);
src/gromacs/taskassignment/decidegpuusage.cpp:    if (!decideWhetherToUseGpusForPmeFft(pmeFftTarget))
src/gromacs/taskassignment/decidegpuusage.cpp:        errorReasons.appendIf(!pme_gpu_mixed_mode_supports_input(inputrec, &tempString), tempString);
src/gromacs/taskassignment/decidegpuusage.cpp:        if (pmeTarget == TaskTarget::Gpu && errorMessage != nullptr)
src/gromacs/taskassignment/decidegpuusage.cpp:bool decideWhetherToUseGpusForPmeWithThreadMpi(const bool              useGpuForNonbonded,
src/gromacs/taskassignment/decidegpuusage.cpp:                                               const std::vector<int>& userGpuTaskAssignment,
src/gromacs/taskassignment/decidegpuusage.cpp:    // First, exclude all cases where we can't run PME on GPUs.
src/gromacs/taskassignment/decidegpuusage.cpp:    if (!canUseGpusForPme(useGpuForNonbonded, pmeTarget, pmeFftTarget, inputrec, nullptr))
src/gromacs/taskassignment/decidegpuusage.cpp:        // PME can't run on a GPU. If the user required that, we issue an error later.
src/gromacs/taskassignment/decidegpuusage.cpp:    // We now know that PME on GPUs might make sense, if we have any.
src/gromacs/taskassignment/decidegpuusage.cpp:    if (pmeTarget == TaskTarget::Gpu)
src/gromacs/taskassignment/decidegpuusage.cpp:                    "PME tasks were required to run on GPUs with multiple ranks "
src/gromacs/taskassignment/decidegpuusage.cpp:    if (!userGpuTaskAssignment.empty())
src/gromacs/taskassignment/decidegpuusage.cpp:        // Follow the user's choice of GPU task assignment, if we
src/gromacs/taskassignment/decidegpuusage.cpp:        // can. Checking that their IDs are for compatible GPUs comes
src/gromacs/taskassignment/decidegpuusage.cpp:        // Specifying -gputasks requires specifying everything.
src/gromacs/taskassignment/decidegpuusage.cpp:        // PME on GPUs is only supported in a single case
src/gromacs/taskassignment/decidegpuusage.cpp:        if (pmeTarget == TaskTarget::Gpu)
src/gromacs/taskassignment/decidegpuusage.cpp:                        "When you run mdrun -pme gpu -gputasks, you must supply a PME-enabled .tpr "
src/gromacs/taskassignment/decidegpuusage.cpp:    // Because this is thread-MPI, we already know about the GPUs that
src/gromacs/taskassignment/decidegpuusage.cpp:    if (pmeTarget == TaskTarget::Gpu)
src/gromacs/taskassignment/decidegpuusage.cpp:                    "PME tasks were required to run on GPUs, but that is not implemented with "
src/gromacs/taskassignment/decidegpuusage.cpp:        // PME can run well on a GPU shared with NB, and we permit
src/gromacs/taskassignment/decidegpuusage.cpp:        // We have a single separate PME rank, that can use a GPU
src/gromacs/taskassignment/decidegpuusage.cpp:        // run well on a GPU shared with NB, and we permit mdrun to
src/gromacs/taskassignment/decidegpuusage.cpp:        // default to it if there is only one GPU available.
src/gromacs/taskassignment/decidegpuusage.cpp:    // Not enough support for PME on GPUs for anything else
src/gromacs/taskassignment/decidegpuusage.cpp:bool decideWhetherToUseGpusForNonbonded(const TaskTarget          nonbondedTarget,
src/gromacs/taskassignment/decidegpuusage.cpp:                                        const std::vector<int>&   userGpuTaskAssignment,
src/gromacs/taskassignment/decidegpuusage.cpp:                                        const EmulateGpuNonbonded emulateGpuNonbonded,
src/gromacs/taskassignment/decidegpuusage.cpp:                                        const bool                buildSupportsNonbondedOnGpu,
src/gromacs/taskassignment/decidegpuusage.cpp:                                        const bool                nonbondedOnGpuIsUseful,
src/gromacs/taskassignment/decidegpuusage.cpp:                                        const bool                gpusWereDetected)
src/gromacs/taskassignment/decidegpuusage.cpp:        if (!userGpuTaskAssignment.empty())
src/gromacs/taskassignment/decidegpuusage.cpp:                    "A GPU task assignment was specified, but nonbonded interactions were "
src/gromacs/taskassignment/decidegpuusage.cpp:    if (!buildSupportsNonbondedOnGpu && nonbondedTarget == TaskTarget::Gpu)
src/gromacs/taskassignment/decidegpuusage.cpp:                "Nonbonded interactions on the GPU were requested with -nb gpu, "
src/gromacs/taskassignment/decidegpuusage.cpp:                "but the GROMACS binary has been built without GPU support. "
src/gromacs/taskassignment/decidegpuusage.cpp:                "Either run without selecting GPU options, or recompile GROMACS "
src/gromacs/taskassignment/decidegpuusage.cpp:                "with GPU support enabled"));
src/gromacs/taskassignment/decidegpuusage.cpp:    // TODO refactor all these TaskTarget::Gpu checks into one place?
src/gromacs/taskassignment/decidegpuusage.cpp:    if (emulateGpuNonbonded == EmulateGpuNonbonded::Yes)
src/gromacs/taskassignment/decidegpuusage.cpp:        if (nonbondedTarget == TaskTarget::Gpu)
src/gromacs/taskassignment/decidegpuusage.cpp:                    "Nonbonded interactions on the GPU were required, which is inconsistent "
src/gromacs/taskassignment/decidegpuusage.cpp:        if (!userGpuTaskAssignment.empty())
src/gromacs/taskassignment/decidegpuusage.cpp:                    InconsistentInputError("GPU ID usage was specified, as was GPU emulation. Make "
src/gromacs/taskassignment/decidegpuusage.cpp:    if (!nonbondedOnGpuIsUseful)
src/gromacs/taskassignment/decidegpuusage.cpp:        if (nonbondedTarget == TaskTarget::Gpu)
src/gromacs/taskassignment/decidegpuusage.cpp:                    "Nonbonded interactions on the GPU were required, but not supported for these "
src/gromacs/taskassignment/decidegpuusage.cpp:                    "simulation settings. Change your settings, or do not require using GPUs."));
src/gromacs/taskassignment/decidegpuusage.cpp:        if (nonbondedTarget == TaskTarget::Gpu)
src/gromacs/taskassignment/decidegpuusage.cpp:                    "Nonbonded interactions on the GPU and binary reprocibility were required. "
src/gromacs/taskassignment/decidegpuusage.cpp:    if (!userGpuTaskAssignment.empty())
src/gromacs/taskassignment/decidegpuusage.cpp:        // Specifying -gputasks requires specifying everything.
src/gromacs/taskassignment/decidegpuusage.cpp:    if (nonbondedTarget == TaskTarget::Gpu)
src/gromacs/taskassignment/decidegpuusage.cpp:        // We still don't know whether it is an error if no GPUs are found
src/gromacs/taskassignment/decidegpuusage.cpp:        // GPUs.
src/gromacs/taskassignment/decidegpuusage.cpp:    // If we get here, then the user permitted GPUs, which we should
src/gromacs/taskassignment/decidegpuusage.cpp:    return buildSupportsNonbondedOnGpu && gpusWereDetected;
src/gromacs/taskassignment/decidegpuusage.cpp:bool decideWhetherToUseGpusForPme(const bool              useGpuForNonbonded,
src/gromacs/taskassignment/decidegpuusage.cpp:                                  const std::vector<int>& userGpuTaskAssignment,
src/gromacs/taskassignment/decidegpuusage.cpp:                                  const bool              gpusWereDetected)
src/gromacs/taskassignment/decidegpuusage.cpp:    if (!canUseGpusForPme(useGpuForNonbonded, pmeTarget, pmeFftTarget, inputrec, &message))
src/gromacs/taskassignment/decidegpuusage.cpp:        if (!userGpuTaskAssignment.empty())
src/gromacs/taskassignment/decidegpuusage.cpp:                    "A GPU task assignment was specified, but PME interactions were "
src/gromacs/taskassignment/decidegpuusage.cpp:    if (pmeTarget == TaskTarget::Gpu)
src/gromacs/taskassignment/decidegpuusage.cpp:                    "PME tasks were required to run on GPUs with multiple ranks "
src/gromacs/taskassignment/decidegpuusage.cpp:    if (!userGpuTaskAssignment.empty())
src/gromacs/taskassignment/decidegpuusage.cpp:        // Specifying -gputasks requires specifying everything.
src/gromacs/taskassignment/decidegpuusage.cpp:    // We still don't know whether it is an error if no GPUs are found
src/gromacs/taskassignment/decidegpuusage.cpp:    // GPUs.
src/gromacs/taskassignment/decidegpuusage.cpp:    if (pmeTarget == TaskTarget::Gpu)
src/gromacs/taskassignment/decidegpuusage.cpp:    // If we get here, then the user permitted GPUs.
src/gromacs/taskassignment/decidegpuusage.cpp:        // PME can run well on a single GPU shared with NB when there
src/gromacs/taskassignment/decidegpuusage.cpp:        // detected GPUs.
src/gromacs/taskassignment/decidegpuusage.cpp:        return gpusWereDetected;
src/gromacs/taskassignment/decidegpuusage.cpp:        // We have a single separate PME rank, that can use a GPU
src/gromacs/taskassignment/decidegpuusage.cpp:        return gpusWereDetected;
src/gromacs/taskassignment/decidegpuusage.cpp:    // Not enough support for PME on GPUs for anything else
src/gromacs/taskassignment/decidegpuusage.cpp:PmeRunMode determinePmeRunMode(const bool useGpuForPme, const TaskTarget& pmeFftTarget, const t_inputrec& inputrec)
src/gromacs/taskassignment/decidegpuusage.cpp:    if (useGpuForPme)
src/gromacs/taskassignment/decidegpuusage.cpp:        if (c_gpuBuildSyclWithoutGpuFft && pmeFftTarget == TaskTarget::Gpu)
src/gromacs/taskassignment/decidegpuusage.cpp:                    "GROMACS is built without SYCL GPU FFT library. Please do not use -pmefft "
src/gromacs/taskassignment/decidegpuusage.cpp:                    "gpu."));
src/gromacs/taskassignment/decidegpuusage.cpp:        if (!decideWhetherToUseGpusForPmeFft(pmeFftTarget))
src/gromacs/taskassignment/decidegpuusage.cpp:            return PmeRunMode::GPU;
src/gromacs/taskassignment/decidegpuusage.cpp:        if (pmeFftTarget == TaskTarget::Gpu)
src/gromacs/taskassignment/decidegpuusage.cpp:                    "Assigning FFTs to GPU requires PME to be assigned to GPU as well. With PME "
src/gromacs/taskassignment/decidegpuusage.cpp:bool decideWhetherToUseGpusForBonded(bool              useGpuForNonbonded,
src/gromacs/taskassignment/decidegpuusage.cpp:                                     bool              useGpuForPme,
src/gromacs/taskassignment/decidegpuusage.cpp:                                     bool              gpusWereDetected)
src/gromacs/taskassignment/decidegpuusage.cpp:    if (!buildSupportsListedForcesGpu(&errorMessage))
src/gromacs/taskassignment/decidegpuusage.cpp:        if (bondedTarget == TaskTarget::Gpu)
src/gromacs/taskassignment/decidegpuusage.cpp:    if (!inputSupportsListedForcesGpu(inputrec, mtop, &errorMessage))
src/gromacs/taskassignment/decidegpuusage.cpp:        if (bondedTarget == TaskTarget::Gpu)
src/gromacs/taskassignment/decidegpuusage.cpp:    if (!useGpuForNonbonded)
src/gromacs/taskassignment/decidegpuusage.cpp:        if (bondedTarget == TaskTarget::Gpu)
src/gromacs/taskassignment/decidegpuusage.cpp:                    "Bonded interactions on the GPU were required, but this requires that "
src/gromacs/taskassignment/decidegpuusage.cpp:                    "short-ranged non-bonded interactions are also run on the GPU. Change "
src/gromacs/taskassignment/decidegpuusage.cpp:                    "your settings, or do not require using GPUs."));
src/gromacs/taskassignment/decidegpuusage.cpp:    if (bondedTarget == TaskTarget::Gpu)
src/gromacs/taskassignment/decidegpuusage.cpp:        // We still don't know whether it is an error if no GPUs are
src/gromacs/taskassignment/decidegpuusage.cpp:    // If we get here, then the user permitted GPUs, which we should
src/gromacs/taskassignment/decidegpuusage.cpp:    // choose separate PME ranks when nonBonded are assigned to the GPU.
src/gromacs/taskassignment/decidegpuusage.cpp:                                     || (usingPmeOrEwald(inputrec.coulombtype) && !useGpuForPme
src/gromacs/taskassignment/decidegpuusage.cpp:    return gpusWereDetected && usingOurCpuForPmeOrEwald;
src/gromacs/taskassignment/decidegpuusage.cpp:bool decideWhetherToUseGpuForUpdate(const bool           isDomainDecomposition,
src/gromacs/taskassignment/decidegpuusage.cpp:                                    const bool           useGpuForNonbonded,
src/gromacs/taskassignment/decidegpuusage.cpp:                                    const bool           gpusWereDetected,
src/gromacs/taskassignment/decidegpuusage.cpp:            "Update task can not run on the GPU, because the following "
src/gromacs/taskassignment/decidegpuusage.cpp:    // Flag to set if we do not want to log the error with `-update auto` (e.g., for non-GPU build)
src/gromacs/taskassignment/decidegpuusage.cpp:                              "With separate PME rank(s), PME must run on the GPU.");
src/gromacs/taskassignment/decidegpuusage.cpp:    // Using the GPU-version of update if:
src/gromacs/taskassignment/decidegpuusage.cpp:    // 1. PME is on the GPU (there should be a copy of coordinates on GPU for PME spread) or inactive, or
src/gromacs/taskassignment/decidegpuusage.cpp:    // 2. Non-bonded interactions are on the GPU.
src/gromacs/taskassignment/decidegpuusage.cpp:    if ((pmeRunMode == PmeRunMode::CPU || pmeRunMode == PmeRunMode::None) && !useGpuForNonbonded)
src/gromacs/taskassignment/decidegpuusage.cpp:                "Either PME or short-ranged non-bonded interaction tasks must run on the GPU.");
src/gromacs/taskassignment/decidegpuusage.cpp:    if (!gpusWereDetected)
src/gromacs/taskassignment/decidegpuusage.cpp:        errorReasons.append("Compatible GPUs must have been found.");
src/gromacs/taskassignment/decidegpuusage.cpp:    if (!(GMX_GPU_CUDA || GMX_GPU_SYCL))
src/gromacs/taskassignment/decidegpuusage.cpp:        errorReasons.append("Only CUDA and SYCL builds are supported.");
src/gromacs/taskassignment/decidegpuusage.cpp:    // does not support it, the actual CUDA LINCS code does support it
src/gromacs/taskassignment/decidegpuusage.cpp:            !UpdateConstrainGpu::isNumCoupledConstraintsSupported(mtop),
src/gromacs/taskassignment/decidegpuusage.cpp:            "The number of coupled constraints is higher than supported in the GPU LINCS "
src/gromacs/taskassignment/decidegpuusage.cpp:    errorReasons.appendIf((hasAnyConstraints && !UpdateConstrainGpu::areConstraintsSupported()),
src/gromacs/taskassignment/decidegpuusage.cpp:                          "Chosen GPU implementation does not support constraints.");
src/gromacs/taskassignment/decidegpuusage.cpp:                          // There is a known bug with frozen atoms and GPU update, see Issue #3920.
src/gromacs/taskassignment/decidegpuusage.cpp:        else if (updateTarget == TaskTarget::Gpu)
src/gromacs/taskassignment/decidegpuusage.cpp:    return (updateTarget == TaskTarget::Gpu
src/gromacs/taskassignment/decidegpuusage.cpp:bool decideWhetherDirectGpuCommunicationCanBeUsed(const DevelopmentFeatureFlags& devFlags,
src/gromacs/taskassignment/decidegpuusage.cpp:    const bool buildSupportsDirectGpuComm = (GMX_GPU_CUDA || GMX_GPU_SYCL) && GMX_MPI;
src/gromacs/taskassignment/decidegpuusage.cpp:    if (!buildSupportsDirectGpuComm)
src/gromacs/taskassignment/decidegpuusage.cpp:    // Direct GPU communication is presently turned off due to insufficient testing
src/gromacs/taskassignment/decidegpuusage.cpp:    const bool enableDirectGpuComm = (getenv("GMX_ENABLE_DIRECT_GPU_COMM") != nullptr)
src/gromacs/taskassignment/decidegpuusage.cpp:                                     || (getenv("GMX_GPU_DD_COMMS") != nullptr)
src/gromacs/taskassignment/decidegpuusage.cpp:                                     || (getenv("GMX_GPU_PME_PP_COMMS") != nullptr);
src/gromacs/taskassignment/decidegpuusage.cpp:    if (GMX_THREAD_MPI && GMX_GPU_SYCL && enableDirectGpuComm)
src/gromacs/taskassignment/decidegpuusage.cpp:                        "GMX_ENABLE_DIRECT_GPU_COMM environment variable detected, "
src/gromacs/taskassignment/decidegpuusage.cpp:    errorReasons.startContext("GPU direct communication can not be activated because:");
src/gromacs/taskassignment/decidegpuusage.cpp:    bool runAndGpuSupportDirectGpuComm = (runUsesCompatibleFeatures && enableDirectGpuComm);
src/gromacs/taskassignment/decidegpuusage.cpp:    bool canUseDirectGpuCommWithThreadMpi =
src/gromacs/taskassignment/decidegpuusage.cpp:            (runAndGpuSupportDirectGpuComm && GMX_THREAD_MPI && !GMX_GPU_SYCL);
src/gromacs/taskassignment/decidegpuusage.cpp:    // GPU-aware MPI case off by default, can be enabled with dev flag
src/gromacs/taskassignment/decidegpuusage.cpp:    // Note: GMX_DISABLE_DIRECT_GPU_COMM already taken into account in devFlags.enableDirectGpuCommWithMpi
src/gromacs/taskassignment/decidegpuusage.cpp:    bool canUseDirectGpuCommWithMpi = (runAndGpuSupportDirectGpuComm && GMX_LIB_MPI
src/gromacs/taskassignment/decidegpuusage.cpp:                                       && devFlags.canUseGpuAwareMpi && enableDirectGpuComm);
src/gromacs/taskassignment/decidegpuusage.cpp:    return canUseDirectGpuCommWithThreadMpi || canUseDirectGpuCommWithMpi;
src/gromacs/taskassignment/decidegpuusage.cpp:bool decideWhetherToUseGpuForHalo(bool                 havePPDomainDecomposition,
src/gromacs/taskassignment/decidegpuusage.cpp:                                  bool                 useGpuForNonbonded,
src/gromacs/taskassignment/decidegpuusage.cpp:                                  bool                 canUseDirectGpuComm,
src/gromacs/taskassignment/decidegpuusage.cpp:    if (!canUseDirectGpuComm || !havePPDomainDecomposition || !useGpuForNonbonded)
src/gromacs/taskassignment/decidegpuusage.cpp:    errorReasons.startContext("GPU halo exchange will not be activated because:");
src/gromacs/mdtypes/forcerec.h:class ListedForcesGpu;
src/gromacs/mdtypes/forcerec.h:class GpuForceReduction;
src/gromacs/mdtypes/forcerec.h:class MdGpuGraph;
src/gromacs/mdtypes/forcerec.h:class StatePropagatorDataGpu;
src/gromacs/mdtypes/forcerec.h:class PmePpCommGpu;
src/gromacs/mdtypes/forcerec.h:    // The listed forces calculation data for GPU
src/gromacs/mdtypes/forcerec.h:    std::unique_ptr<gmx::ListedForcesGpu> listedForcesGpu;
src/gromacs/mdtypes/forcerec.h:    // The stateGpu object is created in runner, forcerec just keeps the copy of the pointer.
src/gromacs/mdtypes/forcerec.h:    // TODO: This is not supposed to be here. StatePropagatorDataGpu should be a part of
src/gromacs/mdtypes/forcerec.h:    gmx::StatePropagatorDataGpu* stateGpu = nullptr;
src/gromacs/mdtypes/forcerec.h:    /* For PME-PP GPU communication */
src/gromacs/mdtypes/forcerec.h:    std::unique_ptr<gmx::PmePpCommGpu> pmePpCommGpu;
src/gromacs/mdtypes/forcerec.h:    /* For GPU force reduction (on both local and non-local atoms) */
src/gromacs/mdtypes/forcerec.h:    gmx::EnumerationArray<gmx::AtomLocality, std::unique_ptr<gmx::GpuForceReduction>> gpuForceReduction;
src/gromacs/mdtypes/forcerec.h:    gmx::EnumerationArray<MdGraphEvenOrOddStep, std::unique_ptr<gmx::MdGpuGraph>> mdGraph;
src/gromacs/mdtypes/forcebuffers.cpp:#include "gromacs/gpu_utils/hostallocator.h"
src/gromacs/mdtypes/tests/forcebuffers.cpp:#include "gromacs/gpu_utils/hostallocator.h"
src/gromacs/mdtypes/commrec.cpp:#include "gromacs/gpu_utils/nvshmem_utils.h"
src/gromacs/mdtypes/simulation_workload.h:    /*! \brief Whether coordinate buffer ops are done on the GPU this step
src/gromacs/mdtypes/simulation_workload.h:    bool useGpuXBufferOps = false;
src/gromacs/mdtypes/simulation_workload.h:    //! Whether force buffer ops are done on the GPU this step
src/gromacs/mdtypes/simulation_workload.h:    bool useGpuFBufferOps = false;
src/gromacs/mdtypes/simulation_workload.h:    //! Whether PME forces are reduced with other contributions on the GPU this step
src/gromacs/mdtypes/simulation_workload.h:    bool useGpuPmeFReduction = false; // TODO: add this flag to the internal PME GPU data structures too
src/gromacs/mdtypes/simulation_workload.h:    //! Whether GPU coordinates halo exchange is active this step
src/gromacs/mdtypes/simulation_workload.h:    bool useGpuXHalo = false;
src/gromacs/mdtypes/simulation_workload.h:    //! Whether GPU forces halo exchange is active this step
src/gromacs/mdtypes/simulation_workload.h:    bool useGpuFHalo = false;
src/gromacs/mdtypes/simulation_workload.h:    //! Whether GPU PME work is computed on the current rank this step (can be false on PP-only ranks or on fast steps with MTS)
src/gromacs/mdtypes/simulation_workload.h:    bool haveGpuPmeOnThisRank = false;
src/gromacs/mdtypes/simulation_workload.h:    bool clearGpuFBufferEarly = false;
src/gromacs/mdtypes/simulation_workload.h:    //! Whether the current nstlist step-range has bonded work to run on a GPU.
src/gromacs/mdtypes/simulation_workload.h:    bool haveGpuBondedWork = false;
src/gromacs/mdtypes/simulation_workload.h:    //! Whether there are currently any non-local forces to be computed on the CPU and, with GPU update and DD, later reduced on the GPU.
src/gromacs/mdtypes/simulation_workload.h:    //! Whether the CPU force buffer has contributions to local atoms that need to be reduced on the GPU (with DD).
src/gromacs/mdtypes/simulation_workload.h:    //! If we have calculation of short range nonbondeds on GPU
src/gromacs/mdtypes/simulation_workload.h:    bool useGpuNonbonded = false;
src/gromacs/mdtypes/simulation_workload.h:    //! If we have calculation of long range PME in GPU
src/gromacs/mdtypes/simulation_workload.h:    //! If we have calculation of long range PME in GPU
src/gromacs/mdtypes/simulation_workload.h:    bool useGpuPme = false;
src/gromacs/mdtypes/simulation_workload.h:    //! If PME FFT solving is done on GPU.
src/gromacs/mdtypes/simulation_workload.h:    bool useGpuPmeFft = false;
src/gromacs/mdtypes/simulation_workload.h:    //! If bonded interactions are calculated on GPU.
src/gromacs/mdtypes/simulation_workload.h:    bool useGpuBonded = false;
src/gromacs/mdtypes/simulation_workload.h:    //! If update and constraint solving is performed on GPU.
src/gromacs/mdtypes/simulation_workload.h:    bool useGpuUpdate = false;
src/gromacs/mdtypes/simulation_workload.h:    //! If X buffer operations are allowed on GPU (actual use depends on other activities that step).
src/gromacs/mdtypes/simulation_workload.h:    bool useGpuXBufferOpsWhenAllowed = false;
src/gromacs/mdtypes/simulation_workload.h:    //! If F buffer operations are allowed on GPU (actual use depends on other activities that step).
src/gromacs/mdtypes/simulation_workload.h:    bool useGpuFBufferOpsWhenAllowed = false;
src/gromacs/mdtypes/simulation_workload.h:    //! If domain decomposition halo exchange is performed on CPU (in CPU-only runs or with staged GPU communication).
src/gromacs/mdtypes/simulation_workload.h:    //! If domain decomposition halo exchange is performed on GPU.
src/gromacs/mdtypes/simulation_workload.h:    bool useGpuHaloExchange = false;
src/gromacs/mdtypes/simulation_workload.h:    //! If PP-PME communication is done purely on CPU (in CPU-only runs or with staged GPU communication).
src/gromacs/mdtypes/simulation_workload.h:    //! If direct PP-PME communication between GPU is used.
src/gromacs/mdtypes/simulation_workload.h:    bool useGpuPmePpCommunication = false;
src/gromacs/mdtypes/simulation_workload.h:    //! If direct GPU-GPU communication is enabled.
src/gromacs/mdtypes/simulation_workload.h:    bool useGpuDirectCommunication = false;
src/gromacs/mdtypes/simulation_workload.h:    //! If GPU PME decomposition is enabled.
src/gromacs/mdtypes/simulation_workload.h:    bool useGpuPmeDecomposition = false;
src/gromacs/mdtypes/simulation_workload.h:    //! Whether a GPU graph should be used to execute steps in the MD loop if run conditions allow.
src/gromacs/mdtypes/simulation_workload.h:    bool useMdGpuGraph = false;
src/gromacs/mdtypes/simulation_workload.h:    //! Whether to use NVSHMEM enabled GPU initiated communication.
src/gromacs/mdtypes/simulation_workload.h:    //! Whether PME GPU is active on this PP rank (note that currently only PP ranks use SimulationWorkload)
src/gromacs/mdtypes/simulation_workload.h:    haveGpuPmeOnPpRank() const
src/gromacs/mdtypes/simulation_workload.h:        return useGpuPme && !haveSeparatePmeRank;
src/gromacs/mdtypes/CMakeLists.txt:if(GMX_GPU)
src/gromacs/mdtypes/CMakeLists.txt:       state_propagator_data_gpu_impl_gpu.cpp
src/gromacs/mdtypes/CMakeLists.txt:   if(GMX_GPU_CUDA)
src/gromacs/mdtypes/CMakeLists.txt:       _gmx_add_files_to_property(CUDA_SOURCES
src/gromacs/mdtypes/CMakeLists.txt:           state_propagator_data_gpu_impl_gpu.cpp
src/gromacs/mdtypes/CMakeLists.txt:   if(GMX_GPU_SYCL)
src/gromacs/mdtypes/CMakeLists.txt:           state_propagator_data_gpu_impl_gpu.cpp
src/gromacs/mdtypes/CMakeLists.txt:   if(GMX_GPU_HIP)
src/gromacs/mdtypes/CMakeLists.txt:		   state_propagator_data_gpu_impl.cpp
src/gromacs/mdtypes/CMakeLists.txt:      state_propagator_data_gpu_impl.cpp
src/gromacs/mdtypes/CMakeLists.txt:                      gpu_utils
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:#include "gromacs/gpu_utils/devicebuffer_datatype.h"
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:#include "gromacs/mdtypes/state_propagator_data_gpu.h"
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:class GpuEventSynchronizer;
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:enum class GpuApiCallBehavior : int;
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:#if !GMX_GPU || GMX_GPU_HIP
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:class StatePropagatorDataGpu::Impl
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:StatePropagatorDataGpu::StatePropagatorDataGpu(const DeviceStreamManager& /* deviceStreamManager */,
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:                                               GpuApiCallBehavior /* transferKind    */,
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:StatePropagatorDataGpu::StatePropagatorDataGpu(const DeviceStream* /* pmeStream       */,
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:                                               GpuApiCallBehavior /* transferKind    */,
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:StatePropagatorDataGpu::StatePropagatorDataGpu(StatePropagatorDataGpu&& /* other */) noexcept = default;
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:StatePropagatorDataGpu& StatePropagatorDataGpu::operator=(StatePropagatorDataGpu&& /* other */) noexcept = default;
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:StatePropagatorDataGpu::~StatePropagatorDataGpu() = default;
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:void StatePropagatorDataGpu::reinit(int /* numAtomsLocal */,
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:               "A CPU stub method from GPU state propagator data was called instead of one from "
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:               "GPU implementation.");
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:std::tuple<int, int> StatePropagatorDataGpu::getAtomRangesFromAtomLocality(AtomLocality /* atomLocality */) const
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:               "A CPU stub method from GPU state propagator data was called instead of one from "
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:               "GPU implementation.");
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:DeviceBuffer<RVec> StatePropagatorDataGpu::getCoordinates()
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:               "A CPU stub method from GPU state propagator data was called instead of one from "
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:               "GPU implementation.");
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:GpuEventSynchronizer* StatePropagatorDataGpu::getCoordinatesReadyOnDeviceEvent(
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:        GpuEventSynchronizer* /* gpuCoordinateHaloLaunched */)
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:               "A CPU stub method from GPU state propagator data was called instead of one from "
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:               "GPU implementation.");
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:void StatePropagatorDataGpu::waitCoordinatesCopiedToDevice(AtomLocality /* atomLocality */)
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:               "A CPU stub method from GPU state propagator data was called instead of one from "
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:               "GPU implementation.");
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:void StatePropagatorDataGpu::consumeCoordinatesCopiedToDeviceEvent(AtomLocality /* atomLocality */)
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:               "A CPU stub method from GPU state propagator data was called instead of one from "
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:               "GPU implementation.");
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:void StatePropagatorDataGpu::resetCoordinatesCopiedToDeviceEvent(AtomLocality /* atomLocality */)
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:               "A CPU stub method from GPU state propagator data was called instead of one from "
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:               "GPU implementation.");
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:void StatePropagatorDataGpu::setXUpdatedOnDeviceEvent(GpuEventSynchronizer* /* xUpdatedOnDeviceEvent */)
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:               "A CPU stub method from GPU state propagator data was called instead of one from "
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:               "GPU implementation.");
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:void StatePropagatorDataGpu::setXUpdatedOnDeviceEventExpectedConsumptionCount(int /* expectedConsumptionCount */)
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:               "A CPU stub method from GPU state propagator data was called instead of one from "
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:               "GPU implementation.");
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:void StatePropagatorDataGpu::setFReadyOnDeviceEventExpectedConsumptionCount(AtomLocality /*atomLocality*/,
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:               "A CPU stub method from GPU state propagator data was called instead of one from "
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:               "GPU implementation.");
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:void StatePropagatorDataGpu::copyCoordinatesToGpu(const gmx::ArrayRef<const gmx::RVec> /* h_x */,
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:               "A CPU stub method from GPU state propagator data was called instead of one from "
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:               "GPU implementation.");
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:void StatePropagatorDataGpu::waitCoordinatesReadyOnHost(AtomLocality /* atomLocality */)
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:               "A CPU stub method from GPU state propagator data was called instead of one from "
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:               "GPU implementation.");
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:void StatePropagatorDataGpu::copyCoordinatesFromGpu(gmx::ArrayRef<gmx::RVec> /* h_x          */,
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:                                                    GpuEventSynchronizer* /*dependency */)
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:               "A CPU stub method from GPU state propagator data was called instead of one from "
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:               "GPU implementation.");
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:DeviceBuffer<RVec> StatePropagatorDataGpu::getVelocities()
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:               "A CPU stub method from GPU state propagator data was called instead of one from "
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:               "GPU implementation.");
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:void StatePropagatorDataGpu::copyVelocitiesToGpu(const gmx::ArrayRef<const gmx::RVec> /* h_v */,
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:               "A CPU stub method from GPU state propagator data was called instead of one from "
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:               "GPU implementation.");
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:void StatePropagatorDataGpu::copyVelocitiesFromGpu(gmx::ArrayRef<gmx::RVec> /* h_v          */,
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:               "A CPU stub method from GPU state propagator data was called instead of one from "
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:               "GPU implementation.");
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:void StatePropagatorDataGpu::waitVelocitiesReadyOnHost(AtomLocality /* atomLocality */)
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:               "A CPU stub method from GPU state propagator data was called instead of one from "
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:               "GPU implementation.");
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:DeviceBuffer<RVec> StatePropagatorDataGpu::getForces()
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:               "A CPU stub method from GPU state propagator data was called instead of one from "
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:               "GPU implementation.");
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:void StatePropagatorDataGpu::copyForcesToGpu(const gmx::ArrayRef<const gmx::RVec> /* h_f          */,
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:               "A CPU stub method from GPU state propagator data was called instead of one from "
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:               "GPU implementation.");
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:void StatePropagatorDataGpu::clearForcesOnGpu(AtomLocality /* atomLocality */,
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:                                              GpuEventSynchronizer* /* dependency */)
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:               "A CPU stub method from GPU state propagator data was called instead of one from "
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:               "GPU implementation.");
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:GpuEventSynchronizer* StatePropagatorDataGpu::getLocalForcesReadyOnDeviceEvent(StepWorkload /* stepWork */,
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:               "A CPU stub method from GPU state propagator data was called instead of one from "
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:               "GPU implementation.");
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:GpuEventSynchronizer* StatePropagatorDataGpu::fReducedOnDevice(AtomLocality /*atomLocality*/)
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:               "A CPU stub method from GPU state propagator data was called instead of one from "
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:               "GPU implementation.");
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:void StatePropagatorDataGpu::consumeForcesReducedOnDeviceEvent(AtomLocality /*atomLocality*/)
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:               "A CPU stub method from GPU state propagator data was called instead of one from "
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:               "GPU implementation.");
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:GpuEventSynchronizer* StatePropagatorDataGpu::fReadyOnDevice(AtomLocality /*atomLocality*/)
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:               "A CPU stub method from GPU state propagator data was called instead of one from "
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:               "GPU implementation.");
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:void StatePropagatorDataGpu::copyForcesFromGpu(gmx::ArrayRef<gmx::RVec> /* h_f          */,
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:               "A CPU stub method from GPU state propagator data was called instead of one from "
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:               "GPU implementation.");
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:void StatePropagatorDataGpu::waitForcesReadyOnHost(AtomLocality /* atomLocality */)
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:               "A CPU stub method from GPU state propagator data was called instead of one from "
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:               "GPU implementation.");
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:const DeviceStream* StatePropagatorDataGpu::getUpdateStream()
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:               "A CPU stub method from GPU state propagator data was called instead of one from "
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:               "GPU implementation.");
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:int StatePropagatorDataGpu::numAtomsLocal() const
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:               "A CPU stub method from GPU state propagator data was called instead of one from "
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:               "GPU implementation.");
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:int StatePropagatorDataGpu::numAtomsAll() const
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:               "A CPU stub method from GPU state propagator data was called instead of one from "
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:               "GPU implementation.");
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:void StatePropagatorDataGpu::waitCoordinatesUpdatedOnDevice()
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:               "A CPU stub method from GPU state propagator data was called instead of one from "
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:               "GPU implementation.");
src/gromacs/mdtypes/state_propagator_data_gpu_impl.cpp:#endif // !GMX_GPU || GMX_GPU_HIP
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h: * \brief Declaration of low-level functions and fields of GPU state propagator object.
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:#ifndef GMX_MDTYPES_STATE_PROPAGATOR_DATA_GPU_IMPL_H
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:#define GMX_MDTYPES_STATE_PROPAGATOR_DATA_GPU_IMPL_H
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:#include "gromacs/gpu_utils/devicebuffer.h"
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:#include "gromacs/gpu_utils/gpueventsynchronizer.h"
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:#include "gromacs/mdtypes/state_propagator_data_gpu.h"
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:class StatePropagatorDataGpu::Impl
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:     * GPU, all coordinates are copied to the GPU and hence, for this rank, the
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:     * PME work on the GPU, and if that rank also does PP work that is the only
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:     * In OpenCL, only pmeStream is used since it is the only stream created in
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:     * ops are offloaded. This feature is currently not available in OpenCL and
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:         GpuApiCallBehavior         transferKind,
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:         GpuApiCallBehavior   transferKind,
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:    /*! \brief Get the positions buffer on the GPU.
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:     *  \returns GPU positions buffer.
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:    /*! \brief Copy positions to the GPU memory.
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:    void copyCoordinatesToGpu(gmx::ArrayRef<const gmx::RVec> h_x,
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:     * synchronizer indicates the completion of GPU update-constraint kernels. Otherwise, on search
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:     *  \param[in] gpuCoordinateHaloLaunched Event recorded when GPU coordinate halo has been launched.
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:    GpuEventSynchronizer* getCoordinatesReadyOnDeviceEvent(AtomLocality              atomLocality,
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:                                                           GpuEventSynchronizer* gpuCoordinateHaloLaunched = nullptr);
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:    /*! \brief Setter for the event synchronizer for the update is done on th GPU
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:    void setXUpdatedOnDeviceEvent(GpuEventSynchronizer* xUpdatedOnDeviceEvent);
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:    /*! \brief Set the expected consumption count for the event associated with GPU update.
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:    /*! \brief Set the expected consumption count for the event associated with GPU forces computation.
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:    /*! \brief Copy positions from the GPU memory, with an optional explicit dependency.
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:    void copyCoordinatesFromGpu(gmx::ArrayRef<gmx::RVec> h_x,
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:                                GpuEventSynchronizer*    dependency = nullptr);
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:    /*! \brief Get the velocities buffer on the GPU.
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:     *  \returns GPU velocities buffer.
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:    /*! \brief Copy velocities to the GPU memory.
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:    void copyVelocitiesToGpu(gmx::ArrayRef<const gmx::RVec> h_v, AtomLocality atomLocality);
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:    /*! \brief Copy velocities from the GPU memory.
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:    void copyVelocitiesFromGpu(gmx::ArrayRef<gmx::RVec> h_v, AtomLocality atomLocality);
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:    /*! \brief Get the force buffer on the GPU.
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:     *  \returns GPU force buffer.
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:    /*! \brief Copy forces to the GPU memory.
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:    void copyForcesToGpu(gmx::ArrayRef<const gmx::RVec> h_f, AtomLocality atomLocality);
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:    /*! \brief Clear forces in the GPU memory.
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:    void clearForcesOnGpu(AtomLocality atomLocality, GpuEventSynchronizer* dependency);
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:     *  1. The forces are copied to the device (when GPU buffer ops are off)
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:     *  2. The forces are reduced on the device (GPU buffer ops are on)
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:    GpuEventSynchronizer* getLocalForcesReadyOnDeviceEvent(StepWorkload       stepWork,
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:    /*! \brief Getter for the event synchronizer for when forces are reduced on the GPU.
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:     *  \returns                     The event to mark when forces are reduced on the GPU.
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:    GpuEventSynchronizer* fReducedOnDevice(AtomLocality atomLocality);
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:    /*! \brief Getter for the event synchronizer for the forces are ready for GPU update.
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:     *  \returns                     The event to mark when forces are ready for GPU update.
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:    GpuEventSynchronizer* fReadyOnDevice(AtomLocality atomLocality);
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:    /*! \brief Copy forces from the GPU memory.
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:    void copyForcesFromGpu(gmx::ArrayRef<gmx::RVec> h_f, AtomLocality atomLocality);
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:    //! GPU PME stream.
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:    //! GPU NBNXM local stream.
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:    //! GPU NBNXM non-local stream.
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:    //! GPU Update-constraints stream.
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:    EnumerationArray<AtomLocality, GpuEventSynchronizer> xReadyOnDevice_;
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:    GpuEventSynchronizer* xUpdatedOnDeviceEvent_ = nullptr;
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:    EnumerationArray<AtomLocality, GpuEventSynchronizer> xReadyOnHost_;
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:    EnumerationArray<AtomLocality, GpuEventSynchronizer> vReadyOnHost_;
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:    EnumerationArray<AtomLocality, GpuEventSynchronizer> fReadyOnDevice_;
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:    //! An array of events that indicate the forces were reduced on the GPU (one event for each atom locality)
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:    EnumerationArray<AtomLocality, GpuEventSynchronizer> fReducedOnDevice_;
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:    EnumerationArray<AtomLocality, GpuEventSynchronizer> fReadyOnHost_;
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:    GpuApiCallBehavior transferKind_ = GpuApiCallBehavior::Async;
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:     *  \param[in]  deviceStream   GPU stream to execute copy in.
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:     *  \param[in]  deviceStream   GPU stream to execute copy in.
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:     *  \param[in]  deviceStream   GPU stream to execute copy in.
src/gromacs/mdtypes/state_propagator_data_gpu_impl.h:#endif // GMX_MDTYPES_STATE_PROPAGATOR_DATA_GPU_IMPL_H
src/gromacs/mdtypes/state.h:#include "gromacs/gpu_utils/hostallocator.h"
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp: * \brief Definitions of interfaces for GPU state data propagator object.
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:#if GMX_GPU && !GMX_GPU_HIP
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:#    include "gromacs/gpu_utils/device_stream_manager.h"
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:#    include "gromacs/gpu_utils/devicebuffer.h"
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:#    include "gromacs/gpu_utils/gpueventsynchronizer.h"
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:#    include "gromacs/gpu_utils/nvshmem_utils.h"
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:#    include "gromacs/mdtypes/state_propagator_data_gpu.h"
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:#    include "state_propagator_data_gpu_impl.h"
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:StatePropagatorDataGpu::Impl::Impl(const DeviceStreamManager& deviceStreamManager,
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:                                   GpuApiCallBehavior         transferKind,
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:            GMX_GPU,
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:            "GPU state propagator data object should only be constructed on the GPU code-paths.");
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:StatePropagatorDataGpu::Impl::Impl(const DeviceStream*  pmeStream,
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:                                   GpuApiCallBehavior   transferKind,
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:            GMX_GPU,
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:            "GPU state propagator data object should only be constructed on the GPU code-paths.");
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:    GMX_ASSERT(pmeStream->isValid(), "GPU PME stream should be valid.");
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:StatePropagatorDataGpu::Impl::~Impl()
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:void StatePropagatorDataGpu::Impl::reinit(int numAtomsLocal, int numAtomsAll, const t_commrec& cr, int peerRank)
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:    wallcycle_start_nocount(wcycle_, WallCycleCounter::LaunchGpuPp);
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:    // the force accumulation stage before syncing with the local stream. Only done in CUDA and
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:    // SYCL, since the force buffer ops are not implemented in OpenCL.
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:    static constexpr bool sc_haveGpuFBufferOps = ((GMX_GPU_CUDA != 0) || (GMX_GPU_SYCL != 0));
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:    if (sc_haveGpuFBufferOps)
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:        // We need to synchronize to avoid a data race with copyForcesToGpu(Local).
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:    wallcycle_stop(wcycle_, WallCycleCounter::LaunchGpuPp);
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:std::tuple<int, int> StatePropagatorDataGpu::Impl::getAtomRangesFromAtomLocality(AtomLocality atomLocality) const
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:                               "Wrong range of atoms requested in GPU state data manager. Should "
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:               "The first element to copy has negative index. Probably, the GPU propagator state "
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:               "Number of atoms to copy is negative. Probably, the GPU propagator state was not "
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:void StatePropagatorDataGpu::Impl::copyToDevice(DeviceBuffer<RVec>                   d_data,
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:void StatePropagatorDataGpu::Impl::copyFromDevice(gmx::ArrayRef<gmx::RVec> h_data,
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:void StatePropagatorDataGpu::Impl::clearOnDevice(DeviceBuffer<RVec>  d_data,
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:DeviceBuffer<RVec> StatePropagatorDataGpu::Impl::getCoordinates()
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:void StatePropagatorDataGpu::Impl::copyCoordinatesToGpu(const gmx::ArrayRef<const gmx::RVec> h_x,
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:    wallcycle_start_nocount(wcycle_, WallCycleCounter::LaunchGpuPp);
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:    wallcycle_stop(wcycle_, WallCycleCounter::LaunchGpuPp);
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:GpuEventSynchronizer* StatePropagatorDataGpu::Impl::getCoordinatesReadyOnDeviceEvent(
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:        GpuEventSynchronizer*     gpuCoordinateHaloLaunched)
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:    // the copy event. Non-local coordinates are provided by the GPU halo exchange (if active), otherwise by H2D copy.
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:    if (atomLocality == AtomLocality::NonLocal && stepWork.useGpuXHalo)
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:        GMX_ASSERT(gpuCoordinateHaloLaunched != nullptr,
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:                   "GPU halo exchange is active but its completion event is null.");
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:        return gpuCoordinateHaloLaunched;
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:    if (atomLocality == AtomLocality::Local && simulationWork.useGpuUpdate && !stepWork.doNeighborSearch)
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:            /* On search steps, we do not consume the result of the GPU update
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:void StatePropagatorDataGpu::Impl::waitCoordinatesCopiedToDevice(AtomLocality atomLocality)
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:    wallcycle_start(wcycle_, WallCycleCounter::WaitGpuStatePropagatorData);
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:    wallcycle_stop(wcycle_, WallCycleCounter::WaitGpuStatePropagatorData);
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:void StatePropagatorDataGpu::Impl::consumeCoordinatesCopiedToDeviceEvent(AtomLocality atomLocality)
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:void StatePropagatorDataGpu::Impl::resetCoordinatesCopiedToDeviceEvent(AtomLocality atomLocality)
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:void StatePropagatorDataGpu::Impl::setXUpdatedOnDeviceEvent(GpuEventSynchronizer* xUpdatedOnDeviceEvent)
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:void StatePropagatorDataGpu::Impl::setXUpdatedOnDeviceEventExpectedConsumptionCount(int expectedConsumptionCount)
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:void StatePropagatorDataGpu::Impl::setFReadyOnDeviceEventExpectedConsumptionCount(AtomLocality atomLocality,
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:void StatePropagatorDataGpu::Impl::copyCoordinatesFromGpu(gmx::ArrayRef<gmx::RVec> h_x,
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:                                                          GpuEventSynchronizer*    dependency)
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:    wallcycle_start_nocount(wcycle_, WallCycleCounter::LaunchGpuPp);
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:    // Note: unlike copyCoordinatesToGpu this is not used in OpenCL, and the conditional is not needed.
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:    wallcycle_stop(wcycle_, WallCycleCounter::LaunchGpuPp);
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:void StatePropagatorDataGpu::Impl::waitCoordinatesReadyOnHost(AtomLocality atomLocality)
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:    wallcycle_start(wcycle_, WallCycleCounter::WaitGpuStatePropagatorData);
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:    wallcycle_stop(wcycle_, WallCycleCounter::WaitGpuStatePropagatorData);
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:DeviceBuffer<RVec> StatePropagatorDataGpu::Impl::getVelocities()
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:void StatePropagatorDataGpu::Impl::copyVelocitiesToGpu(const gmx::ArrayRef<const gmx::RVec> h_v,
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:    wallcycle_start_nocount(wcycle_, WallCycleCounter::LaunchGpuPp);
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:    wallcycle_stop(wcycle_, WallCycleCounter::LaunchGpuPp);
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:void StatePropagatorDataGpu::Impl::copyVelocitiesFromGpu(gmx::ArrayRef<gmx::RVec> h_v, AtomLocality atomLocality)
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:    wallcycle_start_nocount(wcycle_, WallCycleCounter::LaunchGpuPp);
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:    wallcycle_stop(wcycle_, WallCycleCounter::LaunchGpuPp);
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:void StatePropagatorDataGpu::Impl::waitVelocitiesReadyOnHost(AtomLocality atomLocality)
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:    wallcycle_start(wcycle_, WallCycleCounter::WaitGpuStatePropagatorData);
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:    wallcycle_stop(wcycle_, WallCycleCounter::WaitGpuStatePropagatorData);
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:DeviceBuffer<RVec> StatePropagatorDataGpu::Impl::getForces()
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:// Copy CPU forces to GPU using stream internal to this module to allow overlap
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:// with GPU force calculations.
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:void StatePropagatorDataGpu::Impl::copyForcesToGpu(const gmx::ArrayRef<const gmx::RVec> h_f,
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:    wallcycle_start_nocount(wcycle_, WallCycleCounter::LaunchGpuPp);
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:    wallcycle_stop(wcycle_, WallCycleCounter::LaunchGpuPp);
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:void StatePropagatorDataGpu::Impl::clearForcesOnGpu(AtomLocality atomLocality, GpuEventSynchronizer* dependency)
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:    wallcycle_start_nocount(wcycle_, WallCycleCounter::LaunchGpuPp);
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:    wallcycle_stop(wcycle_, WallCycleCounter::LaunchGpuPp);
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:GpuEventSynchronizer* StatePropagatorDataGpu::Impl::getLocalForcesReadyOnDeviceEvent(StepWorkload stepWork,
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:    if (stepWork.useGpuFBufferOps && !simulationWork.useCpuPmePpCommunication)
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:GpuEventSynchronizer* StatePropagatorDataGpu::Impl::fReducedOnDevice(AtomLocality atomLocality)
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:void StatePropagatorDataGpu::Impl::consumeForcesReducedOnDeviceEvent(AtomLocality atomLocality)
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:GpuEventSynchronizer* StatePropagatorDataGpu::Impl::fReadyOnDevice(AtomLocality atomLocality)
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:void StatePropagatorDataGpu::Impl::copyForcesFromGpu(gmx::ArrayRef<gmx::RVec> h_f, AtomLocality atomLocality)
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:    wallcycle_start_nocount(wcycle_, WallCycleCounter::LaunchGpuPp);
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:    wallcycle_stop(wcycle_, WallCycleCounter::LaunchGpuPp);
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:void StatePropagatorDataGpu::Impl::waitForcesReadyOnHost(AtomLocality atomLocality)
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:    wallcycle_start(wcycle_, WallCycleCounter::WaitGpuStatePropagatorData);
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:    wallcycle_stop(wcycle_, WallCycleCounter::WaitGpuStatePropagatorData);
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:const DeviceStream* StatePropagatorDataGpu::Impl::getUpdateStream()
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:int StatePropagatorDataGpu::Impl::numAtomsLocal() const
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:int StatePropagatorDataGpu::Impl::numAtomsAll() const
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:StatePropagatorDataGpu::StatePropagatorDataGpu(const DeviceStreamManager& deviceStreamManager,
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:                                               GpuApiCallBehavior         transferKind,
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:StatePropagatorDataGpu::StatePropagatorDataGpu(const DeviceStream*  pmeStream,
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:                                               GpuApiCallBehavior   transferKind,
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:StatePropagatorDataGpu::StatePropagatorDataGpu(StatePropagatorDataGpu&& /* other */) noexcept = default;
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:StatePropagatorDataGpu& StatePropagatorDataGpu::operator=(StatePropagatorDataGpu&& /* other */) noexcept = default;
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:StatePropagatorDataGpu::~StatePropagatorDataGpu() = default;
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:void StatePropagatorDataGpu::reinit(int numAtomsLocal, int numAtomsAll, const t_commrec& cr, int peerRank)
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:std::tuple<int, int> StatePropagatorDataGpu::getAtomRangesFromAtomLocality(AtomLocality atomLocality) const
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:DeviceBuffer<RVec> StatePropagatorDataGpu::getCoordinates()
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:void StatePropagatorDataGpu::copyCoordinatesToGpu(const gmx::ArrayRef<const gmx::RVec> h_x,
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:    return impl_->copyCoordinatesToGpu(h_x, atomLocality, expectedConsumptionCount);
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:GpuEventSynchronizer*
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:StatePropagatorDataGpu::getCoordinatesReadyOnDeviceEvent(AtomLocality              atomLocality,
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:                                                         GpuEventSynchronizer* gpuCoordinateHaloLaunched)
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:            atomLocality, simulationWork, stepWork, gpuCoordinateHaloLaunched);
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:void StatePropagatorDataGpu::waitCoordinatesCopiedToDevice(AtomLocality atomLocality)
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:void StatePropagatorDataGpu::consumeCoordinatesCopiedToDeviceEvent(AtomLocality atomLocality)
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:void StatePropagatorDataGpu::resetCoordinatesCopiedToDeviceEvent(AtomLocality atomLocality)
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:void StatePropagatorDataGpu::setXUpdatedOnDeviceEvent(GpuEventSynchronizer* xUpdatedOnDeviceEvent)
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:void StatePropagatorDataGpu::setXUpdatedOnDeviceEventExpectedConsumptionCount(int expectedConsumptionCount)
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:void StatePropagatorDataGpu::setFReadyOnDeviceEventExpectedConsumptionCount(AtomLocality atomLocality,
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:void StatePropagatorDataGpu::copyCoordinatesFromGpu(gmx::ArrayRef<RVec>   h_x,
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:                                                    GpuEventSynchronizer* dependency)
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:    return impl_->copyCoordinatesFromGpu(h_x, atomLocality, dependency);
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:void StatePropagatorDataGpu::waitCoordinatesReadyOnHost(AtomLocality atomLocality)
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:DeviceBuffer<RVec> StatePropagatorDataGpu::getVelocities()
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:void StatePropagatorDataGpu::copyVelocitiesToGpu(const gmx::ArrayRef<const gmx::RVec> h_v,
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:    return impl_->copyVelocitiesToGpu(h_v, atomLocality);
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:void StatePropagatorDataGpu::copyVelocitiesFromGpu(gmx::ArrayRef<RVec> h_v, AtomLocality atomLocality)
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:    return impl_->copyVelocitiesFromGpu(h_v, atomLocality);
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:void StatePropagatorDataGpu::waitVelocitiesReadyOnHost(AtomLocality atomLocality)
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:DeviceBuffer<RVec> StatePropagatorDataGpu::getForces()
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:void StatePropagatorDataGpu::copyForcesToGpu(const gmx::ArrayRef<const gmx::RVec> h_f, AtomLocality atomLocality)
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:    return impl_->copyForcesToGpu(h_f, atomLocality);
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:void StatePropagatorDataGpu::clearForcesOnGpu(AtomLocality atomLocality, GpuEventSynchronizer* dependency)
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:    return impl_->clearForcesOnGpu(atomLocality, dependency);
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:GpuEventSynchronizer* StatePropagatorDataGpu::getLocalForcesReadyOnDeviceEvent(StepWorkload stepWork,
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:GpuEventSynchronizer* StatePropagatorDataGpu::fReducedOnDevice(AtomLocality atomLocality)
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:void StatePropagatorDataGpu::consumeForcesReducedOnDeviceEvent(AtomLocality atomLocality)
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:GpuEventSynchronizer* StatePropagatorDataGpu::fReadyOnDevice(AtomLocality atomLocality)
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:void StatePropagatorDataGpu::copyForcesFromGpu(gmx::ArrayRef<RVec> h_f, AtomLocality atomLocality)
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:    return impl_->copyForcesFromGpu(h_f, atomLocality);
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:void StatePropagatorDataGpu::waitForcesReadyOnHost(AtomLocality atomLocality)
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:const DeviceStream* StatePropagatorDataGpu::getUpdateStream()
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:int StatePropagatorDataGpu::numAtomsLocal() const
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:int StatePropagatorDataGpu::numAtomsAll() const
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:void StatePropagatorDataGpu::waitCoordinatesUpdatedOnDevice()
src/gromacs/mdtypes/state_propagator_data_gpu_impl_gpu.cpp:#endif // GMX_GPU
src/gromacs/mdtypes/state_propagator_data_gpu.h: * \brief Declaration of interfaces for GPU state data propagator object.
src/gromacs/mdtypes/state_propagator_data_gpu.h: * all particles in the system on the GPU.
src/gromacs/mdtypes/state_propagator_data_gpu.h: * and signal buffers involved in PP GPU Halo Exchange. It also serves
src/gromacs/mdtypes/state_propagator_data_gpu.h: * involved in the PP GPU Halo Exchange for the sake of keeping
src/gromacs/mdtypes/state_propagator_data_gpu.h:#ifndef GMX_MDTYPES_STATE_PROPAGATOR_DATA_GPU_H
src/gromacs/mdtypes/state_propagator_data_gpu.h:#define GMX_MDTYPES_STATE_PROPAGATOR_DATA_GPU_H
src/gromacs/mdtypes/state_propagator_data_gpu.h:#include "gromacs/gpu_utils/devicebuffer_datatype.h"
src/gromacs/mdtypes/state_propagator_data_gpu.h:#include "gromacs/gpu_utils/gpu_utils.h"
src/gromacs/mdtypes/state_propagator_data_gpu.h:class GpuEventSynchronizer;
src/gromacs/mdtypes/state_propagator_data_gpu.h:/*!\brief If StatePropagatorDataGpu object is needed.
src/gromacs/mdtypes/state_propagator_data_gpu.h: * \return Whether the StatePropagatorDataGpu object is needed/was created for this run.
src/gromacs/mdtypes/state_propagator_data_gpu.h:inline bool needStateGpu(SimulationWorkload simulationWorkload)
src/gromacs/mdtypes/state_propagator_data_gpu.h:    return (simulationWorkload.haveGpuPmeOnPpRank()) || simulationWorkload.useGpuXBufferOpsWhenAllowed
src/gromacs/mdtypes/state_propagator_data_gpu.h:           || simulationWorkload.useGpuFBufferOpsWhenAllowed
src/gromacs/mdtypes/state_propagator_data_gpu.h:           || simulationWorkload.useGpuHaloExchange || simulationWorkload.useGpuUpdate;
src/gromacs/mdtypes/state_propagator_data_gpu.h:class StatePropagatorDataGpu
src/gromacs/mdtypes/state_propagator_data_gpu.h:     * GPU, all coordinates are copied to the GPU and hence, for this rank, the
src/gromacs/mdtypes/state_propagator_data_gpu.h:     * PME work on the GPU, and if that rank also does PP work that is the only
src/gromacs/mdtypes/state_propagator_data_gpu.h:     * In OpenCL, only pmeStream is used since it is the only stream created in
src/gromacs/mdtypes/state_propagator_data_gpu.h:     * ops are offloaded. This feature is currently not available in OpenCL and
src/gromacs/mdtypes/state_propagator_data_gpu.h:    StatePropagatorDataGpu(const DeviceStreamManager& deviceStreamManager,
src/gromacs/mdtypes/state_propagator_data_gpu.h:                           GpuApiCallBehavior         transferKind,
src/gromacs/mdtypes/state_propagator_data_gpu.h:     *  \param[in] deviceContext   Device context, nullptr allowed for non-OpenCL builds.
src/gromacs/mdtypes/state_propagator_data_gpu.h:    StatePropagatorDataGpu(const DeviceStream*  pmeStream,
src/gromacs/mdtypes/state_propagator_data_gpu.h:                           GpuApiCallBehavior   transferKind,
src/gromacs/mdtypes/state_propagator_data_gpu.h:    StatePropagatorDataGpu(StatePropagatorDataGpu&& other) noexcept;
src/gromacs/mdtypes/state_propagator_data_gpu.h:    StatePropagatorDataGpu& operator=(StatePropagatorDataGpu&& other) noexcept;
src/gromacs/mdtypes/state_propagator_data_gpu.h:    ~StatePropagatorDataGpu();
src/gromacs/mdtypes/state_propagator_data_gpu.h:    /*! \brief Get the positions buffer on the GPU.
src/gromacs/mdtypes/state_propagator_data_gpu.h:     *  \returns GPU positions buffer.
src/gromacs/mdtypes/state_propagator_data_gpu.h:    /*! \brief Copy positions to the GPU memory.
src/gromacs/mdtypes/state_propagator_data_gpu.h:     * the associated \ref GpuEventSynchronizer event more than once (or never).
src/gromacs/mdtypes/state_propagator_data_gpu.h:    void copyCoordinatesToGpu(gmx::ArrayRef<const gmx::RVec> h_x,
src/gromacs/mdtypes/state_propagator_data_gpu.h:     * synchronizer indicates the completion of GPU update-constraint kernels. Otherwise, on search
src/gromacs/mdtypes/state_propagator_data_gpu.h:     *  \param[in] gpuCoordinateHaloLaunched Event recorded when GPU coordinate halo has been launched.
src/gromacs/mdtypes/state_propagator_data_gpu.h:    GpuEventSynchronizer* getCoordinatesReadyOnDeviceEvent(AtomLocality              atomLocality,
src/gromacs/mdtypes/state_propagator_data_gpu.h:                                                           GpuEventSynchronizer* gpuCoordinateHaloLaunched = nullptr);
src/gromacs/mdtypes/state_propagator_data_gpu.h:    /*! \brief Setter for the event synchronizer for the update is done on the GPU
src/gromacs/mdtypes/state_propagator_data_gpu.h:    void setXUpdatedOnDeviceEvent(GpuEventSynchronizer* xUpdatedOnDeviceEvent);
src/gromacs/mdtypes/state_propagator_data_gpu.h:    /*! \brief Set the expected consumption count for the event associated with GPU update.
src/gromacs/mdtypes/state_propagator_data_gpu.h:    /*! \brief Set the expected consumption count for the event associated with GPU forces computation.
src/gromacs/mdtypes/state_propagator_data_gpu.h:     * This is generally 1, but with GPU halo exchange, the completion of force calculation are used
src/gromacs/mdtypes/state_propagator_data_gpu.h:    /*! \brief Copy positions from the GPU memory, with an optional explicit dependency.
src/gromacs/mdtypes/state_propagator_data_gpu.h:    void copyCoordinatesFromGpu(gmx::ArrayRef<gmx::RVec> h_x,
src/gromacs/mdtypes/state_propagator_data_gpu.h:                                GpuEventSynchronizer*    dependency = nullptr);
src/gromacs/mdtypes/state_propagator_data_gpu.h:    /*! \brief Get the velocities buffer on the GPU.
src/gromacs/mdtypes/state_propagator_data_gpu.h:     *  \returns GPU velocities buffer.
src/gromacs/mdtypes/state_propagator_data_gpu.h:    /*! \brief Copy velocities to the GPU memory.
src/gromacs/mdtypes/state_propagator_data_gpu.h:    void copyVelocitiesToGpu(gmx::ArrayRef<const gmx::RVec> h_v, AtomLocality atomLocality);
src/gromacs/mdtypes/state_propagator_data_gpu.h:    /*! \brief Copy velocities from the GPU memory.
src/gromacs/mdtypes/state_propagator_data_gpu.h:    void copyVelocitiesFromGpu(gmx::ArrayRef<gmx::RVec> h_v, AtomLocality atomLocality);
src/gromacs/mdtypes/state_propagator_data_gpu.h:    /*! \brief Get the force buffer on the GPU.
src/gromacs/mdtypes/state_propagator_data_gpu.h:     *  \returns GPU force buffer.
src/gromacs/mdtypes/state_propagator_data_gpu.h:    /*! \brief Copy forces to the GPU memory.
src/gromacs/mdtypes/state_propagator_data_gpu.h:    void copyForcesToGpu(gmx::ArrayRef<const gmx::RVec> h_f, AtomLocality atomLocality);
src/gromacs/mdtypes/state_propagator_data_gpu.h:    /*! \brief Clear forces in the GPU memory.
src/gromacs/mdtypes/state_propagator_data_gpu.h:    void clearForcesOnGpu(AtomLocality atomLocality, GpuEventSynchronizer* dependency);
src/gromacs/mdtypes/state_propagator_data_gpu.h:     *  1. The forces are copied to the device (when GPU buffer ops are off)
src/gromacs/mdtypes/state_propagator_data_gpu.h:     *  2. The forces are reduced on the device (GPU buffer ops are on)
src/gromacs/mdtypes/state_propagator_data_gpu.h:    GpuEventSynchronizer* getLocalForcesReadyOnDeviceEvent(StepWorkload       stepWork,
src/gromacs/mdtypes/state_propagator_data_gpu.h:    /*! \brief Getter for the event synchronizer for the forces are reduced on the GPU.
src/gromacs/mdtypes/state_propagator_data_gpu.h:     *  \returns                     The event to mark when forces are reduced on the GPU.
src/gromacs/mdtypes/state_propagator_data_gpu.h:    GpuEventSynchronizer* fReducedOnDevice(AtomLocality atomLocality);
src/gromacs/mdtypes/state_propagator_data_gpu.h:    /*! \brief Consume the event for when the forces are reduced on the GPU.
src/gromacs/mdtypes/state_propagator_data_gpu.h:    /*! \brief Getter for the event synchronizer for the forces are ready on the GPU.
src/gromacs/mdtypes/state_propagator_data_gpu.h:     *  \returns                     The event to mark when forces are ready on the GPU.
src/gromacs/mdtypes/state_propagator_data_gpu.h:    GpuEventSynchronizer* fReadyOnDevice(AtomLocality atomLocality);
src/gromacs/mdtypes/state_propagator_data_gpu.h:    /*! \brief Copy forces from the GPU memory.
src/gromacs/mdtypes/state_propagator_data_gpu.h:    void copyForcesFromGpu(gmx::ArrayRef<gmx::RVec> h_f, AtomLocality atomLocality);
src/gromacs/mdtypes/state_propagator_data_gpu.h:    GMX_DISALLOW_COPY_AND_ASSIGN(StatePropagatorDataGpu);
src/gromacs/mdtypes/state_propagator_data_gpu.h:#endif // GMX_MDTYPES_STATE_PROPAGATOR_DATA_GPU_H
src/gromacs/mdtypes/forcebuffers.h:#include "gromacs/gpu_utils/hostallocator.h"
src/gromacs/mdtypes/forcebuffers.h: * The force buffer (not forceMtsCombined) can be pinned for efficient transfer to/from GPUs.
src/gromacs/pbcutil/include/gromacs/pbcutil/pbc_aiuc_sycl.h:#include "gromacs/gpu_utils/gputraits_sycl.h"
src/gromacs/pbcutil/include/gromacs/pbcutil/pbc_aiuc_sycl.h: * \todo This routine uses CUDA float4 types for input coordinates and
src/gromacs/pbcutil/include/gromacs/pbcutil/pbc_aiuc_sycl.h: *       versions. This routine is used in GPU listed forces module.
src/gromacs/pbcutil/include/gromacs/pbcutil/pbc_aiuc_sycl.h:        // TODO: Use intrinsics with CUDA/HIP or sycl::native?
src/gromacs/pbcutil/include/gromacs/pbcutil/pbc_aiuc_sycl.h: *       periodic box, but essentially does the same thing as SIMD and GPU
src/gromacs/pbcutil/include/gromacs/pbcutil/pbc_aiuc.h: * \todo CPU, GPU and SIMD routines essentially do the same operations on
src/gromacs/pbcutil/include/gromacs/pbcutil/pbc_aiuc.h: *       periodic box, but essentially does the same thing as SIMD and GPU
src/gromacs/pbcutil/include/gromacs/pbcutil/pbc_aiuc_cuda.cuh: * \brief Basic routines to handle periodic boundary conditions with CUDA.
src/gromacs/pbcutil/include/gromacs/pbcutil/pbc_aiuc_cuda.cuh: * This file contains GPU implementation of the PBC-aware vector evaluation.
src/gromacs/pbcutil/include/gromacs/pbcutil/pbc_aiuc_cuda.cuh: * \todo CPU, GPU and SIMD routines essentially do the same operations on
src/gromacs/pbcutil/include/gromacs/pbcutil/pbc_aiuc_cuda.cuh:#ifndef GMX_PBCUTIL_PBC_AIUC_CUDA_CUH
src/gromacs/pbcutil/include/gromacs/pbcutil/pbc_aiuc_cuda.cuh:#define GMX_PBCUTIL_PBC_AIUC_CUDA_CUH
src/gromacs/pbcutil/include/gromacs/pbcutil/pbc_aiuc_cuda.cuh:#include "gromacs/gpu_utils/vectype_ops_cuda.h"
src/gromacs/pbcutil/include/gromacs/pbcutil/pbc_aiuc_cuda.cuh: * \todo This routine uses CUDA float4 types for input coordinates and
src/gromacs/pbcutil/include/gromacs/pbcutil/pbc_aiuc_cuda.cuh: *       versions. This routine is used in GPU listed forces module.
src/gromacs/pbcutil/include/gromacs/pbcutil/pbc_aiuc_cuda.cuh: * \todo This routine uses CUDA float3 types for both input and returns
src/gromacs/pbcutil/include/gromacs/pbcutil/pbc_aiuc_cuda.cuh: *       used in GPU-based constraints.
src/gromacs/pbcutil/include/gromacs/pbcutil/pbc_aiuc_cuda.cuh:#endif // GMX_PBCUTIL_PBC_AIUC_CUDA_CUH
src/gromacs/mdspan/tests/extents.cpp: * \author Carter Edwards <hedwards@nvidia.com>
src/gromacs/mdspan/tests/mdspan.cpp: * \author Carter Edwards <hedwards@nvidia.com>
src/gromacs/mdspan/mdspan.h: * \author Carter Edwards <hedwards@nvidia.com>
src/gromacs/mdspan/extents.h: * \author Carter Edwards <hedwards@nvidia.com>
src/gromacs/timing/wallcycle.cpp:#include "gromacs/timing/gpu_timing.h"
src/gromacs/timing/wallcycle.cpp:/* PME GPU timing events' names - correspond to the enum in the gpu_timing.h */
src/gromacs/timing/wallcycle.cpp: * The GPU timing is reported only for rank 0, so we want to preserve
src/gromacs/timing/wallcycle.cpp: * wcc_all are unused by the GPU reporting, but it is not satisfactory
src/gromacs/timing/wallcycle.cpp:            GMX_ASSERT(wcc[WallCycleCounter::PmeGpuMesh].c == 0,
src/gromacs/timing/wallcycle.cpp:                       "PME mesh GPU ticks should be 0 when PME mesh is running on CPU");
src/gromacs/timing/wallcycle.cpp:        if (wcc[WallCycleCounter::PmeGpuMesh].n > 0)
src/gromacs/timing/wallcycle.cpp:                       "PME mesh CPU ticks should be 0 when PME mesh is running on GPU");
src/gromacs/timing/wallcycle.cpp:            GMX_ASSERT(wcc[WallCycleCounter::Run].c >= wcc[WallCycleCounter::PmeGpuMesh].c,
src/gromacs/timing/wallcycle.cpp:                    wcc[WallCycleCounter::Run].c - wcc[WallCycleCounter::PmeGpuMesh].c;
src/gromacs/timing/wallcycle.cpp:static void print_gputimes(FILE* fplog, const char* name, int n, double t, double tot_t)
src/gromacs/timing/wallcycle.cpp:                     const gmx_wallclock_gpu_nbnxn_t* gpu_nbnxn_t,
src/gromacs/timing/wallcycle.cpp:                     const gmx_wallclock_gpu_pme_t*   gpu_pme_t)
src/gromacs/timing/wallcycle.cpp:    double      tot, tot_for_pp, tot_for_rest, tot_cpu_overlap, gpu_cpu_ratio;
src/gromacs/timing/wallcycle.cpp:    if (wc->wcc[WallCycleCounter::PmeMesh].n > 0 || wc->wcc[WallCycleCounter::PmeGpuMesh].n > 0)
src/gromacs/timing/wallcycle.cpp:        // TODO: figure out and record PME GPU counters (what to do with the waiting ones?)
src/gromacs/timing/wallcycle.cpp:    /* print GPU timing summary */
src/gromacs/timing/wallcycle.cpp:    double tot_gpu = 0.0;
src/gromacs/timing/wallcycle.cpp:    if (gpu_pme_t)
src/gromacs/timing/wallcycle.cpp:        for (auto key : keysOf(gpu_pme_t->timing))
src/gromacs/timing/wallcycle.cpp:            tot_gpu += gpu_pme_t->timing[key].t;
src/gromacs/timing/wallcycle.cpp:    if (gpu_nbnxn_t)
src/gromacs/timing/wallcycle.cpp:        tot_gpu += gpu_nbnxn_t->pl_h2d_t + gpu_nbnxn_t->nb_h2d_t + gpu_nbnxn_t->nb_d2h_t;
src/gromacs/timing/wallcycle.cpp:                tot_gpu += gpu_nbnxn_t->ktime[i][j].t;
src/gromacs/timing/wallcycle.cpp:        tot_gpu += gpu_nbnxn_t->pruneTime.t;
src/gromacs/timing/wallcycle.cpp:        fprintf(fplog, "\n GPU timings\n%s\n", hline);
src/gromacs/timing/wallcycle.cpp:        print_gputimes(fplog, "Pair list H2D", gpu_nbnxn_t->pl_h2d_c, gpu_nbnxn_t->pl_h2d_t, tot_gpu);
src/gromacs/timing/wallcycle.cpp:        print_gputimes(fplog, "X / q H2D", gpu_nbnxn_t->nb_c, gpu_nbnxn_t->nb_h2d_t, tot_gpu);
src/gromacs/timing/wallcycle.cpp:                if (gpu_nbnxn_t->ktime[i][j].c)
src/gromacs/timing/wallcycle.cpp:                    print_gputimes(fplog,
src/gromacs/timing/wallcycle.cpp:                                   gpu_nbnxn_t->ktime[i][j].c,
src/gromacs/timing/wallcycle.cpp:                                   gpu_nbnxn_t->ktime[i][j].t,
src/gromacs/timing/wallcycle.cpp:                                   tot_gpu);
src/gromacs/timing/wallcycle.cpp:        if (gpu_pme_t)
src/gromacs/timing/wallcycle.cpp:            for (auto key : keysOf(gpu_pme_t->timing))
src/gromacs/timing/wallcycle.cpp:                if (gpu_pme_t->timing[key].c)
src/gromacs/timing/wallcycle.cpp:                    print_gputimes(fplog,
src/gromacs/timing/wallcycle.cpp:                                   gpu_pme_t->timing[key].c,
src/gromacs/timing/wallcycle.cpp:                                   gpu_pme_t->timing[key].t,
src/gromacs/timing/wallcycle.cpp:                                   tot_gpu);
src/gromacs/timing/wallcycle.cpp:        if (gpu_nbnxn_t->pruneTime.c)
src/gromacs/timing/wallcycle.cpp:            print_gputimes(fplog, "Pruning kernel", gpu_nbnxn_t->pruneTime.c, gpu_nbnxn_t->pruneTime.t, tot_gpu);
src/gromacs/timing/wallcycle.cpp:        print_gputimes(fplog, "F D2H", gpu_nbnxn_t->nb_c, gpu_nbnxn_t->nb_d2h_t, tot_gpu);
src/gromacs/timing/wallcycle.cpp:        print_gputimes(fplog, "Total ", gpu_nbnxn_t->nb_c, tot_gpu, tot_gpu);
src/gromacs/timing/wallcycle.cpp:        if (gpu_nbnxn_t->dynamicPruneTime.c)
src/gromacs/timing/wallcycle.cpp:             * and avoid adding it to tot_gpu as this is not in the force
src/gromacs/timing/wallcycle.cpp:            print_gputimes(fplog,
src/gromacs/timing/wallcycle.cpp:                           gpu_nbnxn_t->dynamicPruneTime.c,
src/gromacs/timing/wallcycle.cpp:                           gpu_nbnxn_t->dynamicPruneTime.t,
src/gromacs/timing/wallcycle.cpp:                           tot_gpu);
src/gromacs/timing/wallcycle.cpp:        gpu_cpu_ratio = tot_gpu / tot_cpu_overlap;
src/gromacs/timing/wallcycle.cpp:        if (gpu_nbnxn_t->nb_c > 0 && wc->wcc[WallCycleCounter::Force].n > 0)
src/gromacs/timing/wallcycle.cpp:                    "\nAverage per-step force GPU/CPU evaluation time ratio: %.3f ms/%.3f ms = "
src/gromacs/timing/wallcycle.cpp:                    tot_gpu / gpu_nbnxn_t->nb_c,
src/gromacs/timing/wallcycle.cpp:                    gpu_cpu_ratio);
src/gromacs/timing/wallcycle.cpp:        /* only print notes related to CPU-GPU load balance with PME */
src/gromacs/timing/wallcycle.cpp:             * CPU-GPU load balancing is possible */
src/gromacs/timing/wallcycle.cpp:            if (gpu_cpu_ratio < 0.8 || gpu_cpu_ratio > 1.25)
src/gromacs/timing/wallcycle.cpp:                if (gpu_cpu_ratio < 0.8)
src/gromacs/timing/wallcycle.cpp:                                        "NOTE: The CPU has >25% more load than the GPU. This "
src/gromacs/timing/wallcycle.cpp:                                        "      GPU resources. Maybe the domain decomposition "
src/gromacs/timing/wallcycle.cpp:                                        "NOTE: The CPU has >25% more load than the GPU. This "
src/gromacs/timing/wallcycle.cpp:                                        "      GPU resources.");
src/gromacs/timing/wallcycle.cpp:                if (gpu_cpu_ratio > 1.25)
src/gromacs/timing/wallcycle.cpp:                                    "NOTE: The GPU has >25% more load than the CPU. This imbalance "
src/gromacs/timing/include/gromacs/timing/wallcycle.h:    LaunchGpuPp,
src/gromacs/timing/include/gromacs/timing/wallcycle.h:    PmeGpuMesh, /* PmeGpuMesh is used for GPU code and similar to PmeMesh on CPU. It includes WaitGpuPmePPRecvX cycles too. */
src/gromacs/timing/include/gromacs/timing/wallcycle.h:    WaitGpuPmeGridD2hCopy, /* Time for PME grid D2H transfer. Used in mixed mode. */
src/gromacs/timing/include/gromacs/timing/wallcycle.h:    WaitGpuPmeGather,
src/gromacs/timing/include/gromacs/timing/wallcycle.h:    PmeGpuFReduction,
src/gromacs/timing/include/gromacs/timing/wallcycle.h:    LaunchGpuPme, /* Time for launching PME specific GPU operations*/
src/gromacs/timing/include/gromacs/timing/wallcycle.h:    WaitGpuPmePPRecvX, /* Time for waiting on receiving PP X on PME rank. Used only when GPU direct comm is active.*/
src/gromacs/timing/include/gromacs/timing/wallcycle.h:    WaitGpuPmeSpread, /* Time taken to finish PME spread on GPU. Used only when PME halo-exchange is active with PME decomposition*/
src/gromacs/timing/include/gromacs/timing/wallcycle.h:    WaitGpuFftToPmeGrid, /* Time taken to convert to PME grid after FFTs are complete. Used only when PME halo-exchange is active with PME decomposition*/
src/gromacs/timing/include/gromacs/timing/wallcycle.h:    PmeWaitComm, /* PmeWaitComm = Run - PmeMesh. Without GPU direct comm, this includes time spent in waiting for coord and force comm.
src/gromacs/timing/include/gromacs/timing/wallcycle.h:                 With GPU direct comm, waiting for coord comm is part of PME mesh and is measured with WaitGpuPmePPRecvX sub-counter*/
src/gromacs/timing/include/gromacs/timing/wallcycle.h:    WaitGpuBonded,
src/gromacs/timing/include/gromacs/timing/wallcycle.h:    WaitGpuNbNL,
src/gromacs/timing/include/gromacs/timing/wallcycle.h:    WaitGpuNbL,
src/gromacs/timing/include/gromacs/timing/wallcycle.h:    WaitGpuStatePropagatorData,
src/gromacs/timing/include/gromacs/timing/wallcycle.h:    GpuSetConstr,
src/gromacs/timing/include/gromacs/timing/wallcycle.h:    MdGpuGraph,
src/gromacs/timing/include/gromacs/timing/wallcycle.h:    DDGpu,
src/gromacs/timing/include/gromacs/timing/wallcycle.h:    GpuBondedListUpdate,
src/gromacs/timing/include/gromacs/timing/wallcycle.h:    LaunchGpuNonBonded,
src/gromacs/timing/include/gromacs/timing/wallcycle.h:    LaunchGpuBonded,
src/gromacs/timing/include/gromacs/timing/wallcycle.h:    LaunchGpuNBXBufOps,
src/gromacs/timing/include/gromacs/timing/wallcycle.h:    LaunchGpuNBFBufOps,
src/gromacs/timing/include/gromacs/timing/wallcycle.h:    LaunchGpuMoveX,
src/gromacs/timing/include/gromacs/timing/wallcycle.h:    LaunchGpuMoveF,
src/gromacs/timing/include/gromacs/timing/wallcycle.h:    LaunchGpuUpdateConstrain,
src/gromacs/timing/include/gromacs/timing/wallcycle.h:    LaunchGpuPmeFft, /* Time for launching FFT operations on GPU*/
src/gromacs/timing/include/gromacs/timing/wallcycle.h:    MdGpuGraphWaitBeforeCapture,
src/gromacs/timing/include/gromacs/timing/wallcycle.h:    MdGpuGraphCapture,
src/gromacs/timing/include/gromacs/timing/wallcycle.h:    MdGpuGraphInstantiateOrUpdate,
src/gromacs/timing/include/gromacs/timing/wallcycle.h:    MdGpuGraphWaitBeforeLaunch,
src/gromacs/timing/include/gromacs/timing/wallcycle.h:    MdGpuGraphLaunch,
src/gromacs/timing/include/gromacs/timing/wallcycle.h:    GpuSetLincs,
src/gromacs/timing/include/gromacs/timing/wallcycle.h:    GpuSetSettle,
src/gromacs/timing/include/gromacs/timing/wallcycle.h:        "Launch PP GPU ops.",
src/gromacs/timing/include/gromacs/timing/wallcycle.h:        "PME GPU mesh",
src/gromacs/timing/include/gromacs/timing/wallcycle.h:        "Wait PME GPU D2H",
src/gromacs/timing/include/gromacs/timing/wallcycle.h:        "Wait PME GPU gather",
src/gromacs/timing/include/gromacs/timing/wallcycle.h:        "Reduce GPU PME F",
src/gromacs/timing/include/gromacs/timing/wallcycle.h:        "Launch PME GPU ops.",
src/gromacs/timing/include/gromacs/timing/wallcycle.h:        "Wait PME GPU spread",
src/gromacs/timing/include/gromacs/timing/wallcycle.h:        "Wait GPU FFT to PME",
src/gromacs/timing/include/gromacs/timing/wallcycle.h:        "Wait Bonded GPU",
src/gromacs/timing/include/gromacs/timing/wallcycle.h:        "Wait GPU NB nonloc.",
src/gromacs/timing/include/gromacs/timing/wallcycle.h:        "Wait GPU NB local",
src/gromacs/timing/include/gromacs/timing/wallcycle.h:        "Wait GPU state copy",
src/gromacs/timing/include/gromacs/timing/wallcycle.h:        "GPU constr. setup",
src/gromacs/timing/include/gromacs/timing/wallcycle.h:        "DD GPU ops.",
src/gromacs/timing/include/gromacs/timing/wallcycle.h:        "GPU Bonded list update",
src/gromacs/timing/include/gromacs/timing/wallcycle.h:        "Launch GPU NB tasks",
src/gromacs/timing/include/gromacs/timing/wallcycle.h:        "Launch GPU Bonded",
src/gromacs/timing/include/gromacs/timing/wallcycle.h:        "Launch GPU NB X ops.",
src/gromacs/timing/include/gromacs/timing/wallcycle.h:        "Launch GPU NB F ops.",
src/gromacs/timing/include/gromacs/timing/wallcycle.h:        "Launch GPU Comm. X",
src/gromacs/timing/include/gromacs/timing/wallcycle.h:        "Launch GPU Comm. F",
src/gromacs/timing/include/gromacs/timing/wallcycle.h:        "Launch GPU update",
src/gromacs/timing/include/gromacs/timing/wallcycle.h:        "Launch PME GPU FFT",
src/gromacs/timing/include/gromacs/timing/wallcycle.h:        "GPU LINCS setup",
src/gromacs/timing/include/gromacs/timing/wallcycle.h:        "GPU settle setup",
src/gromacs/timing/include/gromacs/timing/instrumentation.h: * - NVIDIA NVTX
src/gromacs/timing/include/gromacs/timing/wallcyclereporting.h:struct gmx_wallclock_gpu_nbnxn_t;
src/gromacs/timing/include/gromacs/timing/wallcyclereporting.h:struct gmx_wallclock_gpu_pme_t;
src/gromacs/timing/include/gromacs/timing/wallcyclereporting.h:                     const gmx_wallclock_gpu_nbnxn_t* gpu_nbnxn_t,
src/gromacs/timing/include/gromacs/timing/wallcyclereporting.h:                     const gmx_wallclock_gpu_pme_t*   gpu_pme_t);
src/gromacs/timing/include/gromacs/timing/gpu_timing.h: *  \brief Declares data types for GPU timing
src/gromacs/timing/include/gromacs/timing/gpu_timing.h:#ifndef GMX_TIMING_GPU_TIMING_H
src/gromacs/timing/include/gromacs/timing/gpu_timing.h:#define GMX_TIMING_GPU_TIMING_H
src/gromacs/timing/include/gromacs/timing/gpu_timing.h:/*! \internal \brief GPU kernel time and call count. */
src/gromacs/timing/include/gromacs/timing/gpu_timing.h: * PME GPU stages timing events indices, corresponding to the string in PMEStageNames in wallcycle.cpp.
src/gromacs/timing/include/gromacs/timing/gpu_timing.h:/*! \internal \brief GPU timings for PME. */
src/gromacs/timing/include/gromacs/timing/gpu_timing.h:struct gmx_wallclock_gpu_pme_t
src/gromacs/timing/include/gromacs/timing/gpu_timing.h:    /* A separate PME structure to avoid refactoring the NB code for gmx_wallclock_gpu_t later
src/gromacs/timing/include/gromacs/timing/gpu_timing.h:     * TODO: devise a better GPU timing data structuring.
src/gromacs/timing/include/gromacs/timing/gpu_timing.h:    /*! \brief Array of PME GPU timing data. */
src/gromacs/timing/include/gromacs/timing/gpu_timing.h:/*! \internal \brief GPU NB timings for kernels and H2d/D2H transfers. */
src/gromacs/timing/include/gromacs/timing/gpu_timing.h:struct gmx_wallclock_gpu_nbnxn_t
src/gromacs/timing/include/gromacs/timing/gpu_timing.h:    int    nb_c;     /**< total call count of the nonbonded gpu operations */
src/gromacs/timing/CMakeLists.txt:    find_package(CUDAToolkit ${GMX_CUDA_MINIMUM_REQUIRED_VERSION} REQUIRED)
src/gromacs/timing/CMakeLists.txt:    target_link_libraries(timing INTERFACE CUDA::nvtx3)
src/gromacs/timing/CMakeLists.txt:    # As of CUDA 11.8, there are a lot of old-style casts in nvToolsExt.h
src/gromacs/timing/CMakeLists.txt:        HINTS ENV ROCM_PATH ENV ROCM_HOME "${ACPP_ROCM_PATH}" "${HIP_PACKAGE_PREFIX_DIR}"
src/gromacs/timing/CMakeLists.txt:        HINTS ENV ROCM_PATH ENV ROCM_HOME "${ACPP_ROCM_PATH}" "${HIP_PACKAGE_PREFIX_DIR}"
src/gromacs/applied_forces/nnpot/torchmodel.h:#include <torch/cuda.h>
src/gromacs/applied_forces/nnpot/torchmodel.cpp:    // important to set requires_grad to true again after moving to GPU
src/gromacs/applied_forces/nnpot/torchmodel.cpp:    // check if environment variable GMX_NN_DEVICE is set (should be something like cuda:0 or cpu)
src/gromacs/applied_forces/nnpot/torchmodel.cpp:        else if (dev.find("cuda") != std::string::npos)
src/gromacs/applied_forces/nnpot/torchmodel.cpp:            if (!torch::cuda::is_available())
src/gromacs/applied_forces/nnpot/torchmodel.cpp:                        InternalError("Environment variable GMX_NN_DEVICE was set to cuda, but no "
src/gromacs/applied_forces/nnpot/torchmodel.cpp:                                      "CUDA device is available."));
src/gromacs/applied_forces/nnpot/torchmodel.cpp:            device_ = std::make_shared<torch::Device>(torch::kCUDA);
src/gromacs/applied_forces/nnpot/torchmodel.cpp:        // todo: default to gpu if available
src/CMakeLists.txt:if(GMX_GPU_CUDA)
src/CMakeLists.txt:    get_cuda_compiler_info(CUDA_COMPILER_INFO CUDA_DEVICE_COMPILER_FLAGS CUDA_HOST_COMPILER_FLAGS)
src/CMakeLists.txt:if(GMX_GPU_HIP)
src/testutils/TestMacros.cmake:            if(GMX_GPU_CUDA)
src/testutils/TestMacros.cmake:                # CUDA headers target C, so use old-style casts that clang
src/testutils/TestMacros.cmake:# if we ever support another GPU compilation approach.
src/testutils/TestMacros.cmake:#   GPU_CPP_SOURCE_FILES  file1.cpp file2.cpp ...
src/testutils/TestMacros.cmake:#     compiled in the way that suits GMX_GPU.
src/testutils/TestMacros.cmake:#   CUDA_CU_SOURCE_FILES      file1.cu  file2.cu  ...
src/testutils/TestMacros.cmake:#     All the normal CUDA .cu source files
src/testutils/TestMacros.cmake:#   OPENCL_CPP_SOURCE_FILES   file1.cpp file2.cpp ...
src/testutils/TestMacros.cmake:#     All the other C++ .cpp source files needed only with OpenCL
src/testutils/TestMacros.cmake:#   NON_GPU_CPP_SOURCE_FILES  file1.cpp file2.cpp ...
src/testutils/TestMacros.cmake:#     All the other C++ .cpp source files needed only with neither OpenCL nor CUDA nor SYCL
src/testutils/TestMacros.cmake:            CUDA_CU_SOURCE_FILES
src/testutils/TestMacros.cmake:            GPU_CPP_SOURCE_FILES
src/testutils/TestMacros.cmake:            OPENCL_CPP_SOURCE_FILES
src/testutils/TestMacros.cmake:            NON_GPU_CPP_SOURCE_FILES
src/testutils/TestMacros.cmake:        if (GMX_GPU_CUDA)
src/testutils/TestMacros.cmake:                ${ARG_CUDA_CU_SOURCE_FILES}
src/testutils/TestMacros.cmake:                ${ARG_GPU_CPP_SOURCE_FILES})
src/testutils/TestMacros.cmake:            if (GMX_CLANG_CUDA)
src/testutils/TestMacros.cmake:                set_target_properties(${EXENAME} PROPERTIES CUDA_ARCHITECTURES "${_CUDA_CLANG_GENCODE_FLAGS}")
src/testutils/TestMacros.cmake:                target_compile_options(${EXENAME} PRIVATE "$<$<COMPILE_LANGUAGE:CUDA>:${GMX_CUDA_CLANG_FLAGS}>")
src/testutils/TestMacros.cmake:                set_target_properties(${EXENAME} PROPERTIES CUDA_ARCHITECTURES "${GMX_CUDA_NVCC_GENCODE_FLAGS}")
src/testutils/TestMacros.cmake:                target_compile_options(${EXENAME} PRIVATE "$<$<COMPILE_LANGUAGE:CUDA>:${GMX_CUDA_NVCC_FLAGS}>")
src/testutils/TestMacros.cmake:            set_source_files_properties(${ARG_GPU_CPP_SOURCE_FILES} PROPERTIES LANGUAGE CUDA)
src/testutils/TestMacros.cmake:        elseif (GMX_GPU_HIP)
src/testutils/TestMacros.cmake:            set_source_files_properties(${ARG_GPU_CPP_SOURCE_FILES} PROPERTIES LANGUAGE HIP)
src/testutils/TestMacros.cmake:                    ${ARG_GPU_CPP_SOURCE_FILES})
src/testutils/TestMacros.cmake:            set_target_properties(${EXENAME} PROPERTIES CUDA_SEPARABLE_COMPILATION ON)
src/testutils/TestMacros.cmake:            set_target_properties(${EXENAME} PROPERTIES CUDA_RESOLVE_DEVICE_SYMBOLS ON)
src/testutils/TestMacros.cmake:        if (GMX_GPU_CUDA)
src/testutils/TestMacros.cmake:            target_link_libraries(${EXENAME} PRIVATE CUDA::cudart_static)
src/testutils/TestMacros.cmake:        elseif (GMX_GPU_HIP)
src/testutils/TestMacros.cmake:            if(ARG_HIP_CPP_SOURCE_FILES OR ARG_GPU_CPP_SOURCE_FILES)
src/testutils/TestMacros.cmake:        elseif (GMX_GPU_OPENCL)
src/testutils/TestMacros.cmake:            target_sources(${EXENAME} PRIVATE ${ARG_OPENCL_CPP_SOURCE_FILES} ${ARG_GPU_CPP_SOURCE_FILES})
src/testutils/TestMacros.cmake:            if(ARG_OPENCL_CPP_SOURCE_FILES OR ARG_GPU_CPP_SOURCE_FILES)
src/testutils/TestMacros.cmake:                target_link_libraries(${EXENAME} PRIVATE ${OpenCL_LIBRARIES})
src/testutils/TestMacros.cmake:        elseif (GMX_GPU_SYCL)
src/testutils/TestMacros.cmake:            target_sources(${EXENAME} PRIVATE ${ARG_SYCL_CPP_SOURCE_FILES} ${ARG_GPU_CPP_SOURCE_FILES})
src/testutils/TestMacros.cmake:            if(ARG_SYCL_CPP_SOURCE_FILES OR ARG_GPU_CPP_SOURCE_FILES)
src/testutils/TestMacros.cmake:                    SOURCES ${ARG_SYCL_CPP_SOURCE_FILES} ${ARG_GPU_CPP_SOURCE_FILES}
src/testutils/TestMacros.cmake:            target_sources(${EXENAME} PRIVATE ${ARG_NON_GPU_CPP_SOURCE_FILES} ${ARG_GPU_CPP_SOURCE_FILES})
src/testutils/TestMacros.cmake:            if(GMX_GPU_CUDA)
src/testutils/TestMacros.cmake:                # CUDA headers target C, so use old-style casts that clang
src/testutils/TestMacros.cmake:#   QUICK_GPU_TEST        marks tests that use GPUs and are fast (< 10 seconds on a desktop GPU in Release build);
src/testutils/TestMacros.cmake:#                         currently this label is used to select tests for CUDA Compute Sanitizer runs.
src/testutils/TestMacros.cmake:#   SLOW_GPU_TEST         marks all other tests that should run on a GPU, used to make sure GPU CI only runs GPU
src/testutils/TestMacros.cmake:        set(_options INTEGRATION_TEST SLOW_TEST IGNORE_LEAKS QUICK_GPU_TEST SLOW_GPU_TEST)
src/testutils/TestMacros.cmake:            # Both OpenCL (from JIT) and ThreadSanitizer (from how it
src/testutils/TestMacros.cmake:            if (GMX_GPU_OPENCL OR GMX_GPU_SYCL)
src/testutils/TestMacros.cmake:        if (ARG_QUICK_GPU_TEST)
src/testutils/TestMacros.cmake:            list(APPEND _labels QuickGpuTest)
src/testutils/TestMacros.cmake:        if (ARG_SLOW_GPU_TEST)
src/testutils/TestMacros.cmake:            list(APPEND _labels SlowGpuTest)
src/testutils/TestMacros.cmake:        # All unit tests should be quick, so mark them as QUICK_GPU_TEST if they use GPU
src/testutils/TestMacros.cmake:        set(_test_labels "QUICK_GPU_TEST")
src/testutils/TestMacros.cmake:            # All unit tests should be quick, so mark them as QUICK_GPU_TEST if they use GPU
src/testutils/TestMacros.cmake:            set(_test_labels "QUICK_GPU_TEST")
src/testutils/include/testutils/test_device.h: * Describes test environment class which performs GPU device enumeration for unit tests.
src/testutils/include/testutils/test_device.h:    //! Creates the device context and stream for tests on the GPU
src/testutils/include/testutils/naming.h:#if !defined(__CUDACC_VER_MAJOR__) || (__CUDACC_VER_MAJOR__ > 12) \
src/testutils/include/testutils/naming.h:        || ((__CUDACC_VER_MAJOR__ == 12) && (__CUDACC_VER_MINOR__ > 3))
src/testutils/include/testutils/naming.h:#if !defined(__CUDACC_VER_MAJOR__) || (__CUDACC_VER_MAJOR__ > 11) \
src/testutils/include/testutils/naming.h:        || ((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ > 4))
src/testutils/simulationdatabase.cpp:     * - uses cutoffs that fit inside boxes even after GPU mdrun scales rlist
src/testutils/CMakeLists.txt:if (GMX_GPU_CUDA)
src/testutils/CMakeLists.txt:        target_link_libraries(testutils PRIVATE CUDA::cufft)
src/testutils/CMakeLists.txt:elseif (GMX_GPU_HIP)
src/testutils/CMakeLists.txt:    if (GMX_GPU_FFT_VKFFT)
src/testutils/CMakeLists.txt:    elseif(GMX_GPU_FFT_ROCFFT)
src/testutils/CMakeLists.txt:if (GMX_GPU_SYCL)
src/testutils/test_device.cpp:#include "gromacs/gpu_utils/device_context.h"
src/testutils/test_device.cpp:#include "gromacs/gpu_utils/device_stream.h"
src/testutils/test_device.cpp:#include "gromacs/gpu_utils/gpu_utils.h"
src/testutils/test_hardware_environment.cpp:#include "gromacs/gpu_utils/gpu_utils.h"
src/testutils/test_hardware_environment.cpp:    // Constructing contexts for all compatible GPUs - will be empty on non-GPU builds
src/config.h.cmakein:/* Use cuFFT library for FFT on GPUs */
src/config.h.cmakein:#cmakedefine01 GMX_GPU_FFT_CUFFT
src/config.h.cmakein:/* Use clFFT library for FFT on GPUs */
src/config.h.cmakein:#cmakedefine01 GMX_GPU_FFT_CLFFT
src/config.h.cmakein:/* Use VkFFT library for FFT on GPUs */
src/config.h.cmakein:#cmakedefine01 GMX_GPU_FFT_VKFFT
src/config.h.cmakein:/* Use MKL library for FFT on GPUs */
src/config.h.cmakein:#cmakedefine01 GMX_GPU_FFT_MKL
src/config.h.cmakein:/* Use oneMKL interface library for FFT on GPUs */
src/config.h.cmakein:#cmakedefine01 GMX_GPU_FFT_ONEMKL
src/config.h.cmakein:/* Use rocFFT library for FFT on GPUs */
src/config.h.cmakein:#cmakedefine01 GMX_GPU_FFT_ROCFFT
src/config.h.cmakein:/* Use hipFFT library for FFT on GPUs */
src/config.h.cmakein:#cmakedefine01 GMX_GPU_FFT_HIPFFT
src/config.h.cmakein:/* Use double-batched FFT library (BBFFT) for FFT on GPUs */
src/config.h.cmakein:#cmakedefine01 GMX_GPU_FFT_BBFFT
src/config.h.cmakein:/* Enable NVIDIA NVTX instrumentation */
src/config.h.cmakein:/* Use NVSHMEM for GPU data communication*/
src/config.h.cmakein:/* Define if any type of GPU acceleration is compiled */
src/config.h.cmakein:#cmakedefine01 GMX_GPU
src/config.h.cmakein:/* Define if CUDA GPU acceleration is compiled */
src/config.h.cmakein:#cmakedefine01 GMX_GPU_CUDA
src/config.h.cmakein:/* Define if CUDA Graphs or SYCL Graph support is available */
src/config.h.cmakein:#cmakedefine01 GMX_HAVE_GPU_GRAPH_SUPPORT
src/config.h.cmakein:/* Define if OpenCL GPU acceleration is compiled */
src/config.h.cmakein:#cmakedefine01 GMX_GPU_OPENCL
src/config.h.cmakein:/* Define if SYCL GPU acceleration is compiled */
src/config.h.cmakein:#cmakedefine01 GMX_GPU_SYCL
src/config.h.cmakein:// Define if AdaptiveCpp has CUDA target(s)
src/config.h.cmakein:#cmakedefine01 GMX_ACPP_HAVE_CUDA_TARGET
src/config.h.cmakein:/* Use a single compilation unit when compiling the CUDA (non-bonded) kernels.  */
src/config.h.cmakein:#cmakedefine01 GMX_CUDA_NB_SINGLE_COMPILATION_UNIT
src/config.h.cmakein:/* Define if HIP GPU acceleration is compiled */
src/config.h.cmakein:#cmakedefine01 GMX_GPU_HIP
src/config.h.cmakein:#cmakedefine01 GMX_GPU_HIP_UNIFIED_MEMORY
src/config.h.cmakein:/* Whether MPI can report its GPU-awareness  */
src/config.h.cmakein:#cmakedefine01 MPI_SUPPORTS_CUDA_AWARE_DETECTION
src/config.h.cmakein:#cmakedefine01 MPI_SUPPORTS_ROCM_AWARE_DETECTION
src/config.h.cmakein:// Cluster size used by GPU nonbonded kernel.
src/config.h.cmakein:#define GMX_GPU_NB_CLUSTER_SIZE @GMX_GPU_NB_CLUSTER_SIZE@
src/config.h.cmakein:// Number of clusters along X in a pair-search grid cell for GPU lists.
src/config.h.cmakein:#define GMX_GPU_NB_NUM_CLUSTER_PER_CELL_X @GMX_GPU_NB_NUM_CLUSTER_PER_CELL_X@
src/config.h.cmakein:// Number of clusters along Y in a pair-search grid cell for GPU lists.
src/config.h.cmakein:#define GMX_GPU_NB_NUM_CLUSTER_PER_CELL_Y @GMX_GPU_NB_NUM_CLUSTER_PER_CELL_Y@
src/config.h.cmakein:// Number of clusters along Z in a pair-search grid cell for GPU lists.
src/config.h.cmakein:#define GMX_GPU_NB_NUM_CLUSTER_PER_CELL_Z @GMX_GPU_NB_NUM_CLUSTER_PER_CELL_Z@
src/config.h.cmakein:#cmakedefine01 GMX_GPU_NB_DISABLE_CLUSTER_PAIR_SPLIT
src/config.h.cmakein:/* Define relative path to OpenCL kernels */
src/config.h.cmakein:/* Minimum required OpenCL version support (both API and device) - split into integer components for convenience */
src/config.h.cmakein:#define REQUIRED_OPENCL_MIN_VERSION_MAJOR @REQUIRED_OPENCL_MIN_VERSION_MAJOR@
src/config.h.cmakein:#define REQUIRED_OPENCL_MIN_VERSION_MINOR @REQUIRED_OPENCL_MIN_VERSION_MINOR@
src/config.h.cmakein:// Mac OS 13.x has a bug where dispatch.h generates an error for OpenCL builds if

```
