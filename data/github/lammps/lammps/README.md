# https://github.com/lammps/lammps

```console
python/lammps/mliap/pytorch.py:    def forward(self, elems, descriptors, beta, energy,use_gpu_data=False):
python/lammps/mliap/pytorch.py:        if (use_gpu_data and (device is None) and (str(beta.device).find('CUDA') == 1)):
python/lammps/mliap/pytorch.py:            device = 'cuda' #Override device as it wasn't defined in the model
python/lammps/mliap/pytorch.py:            if (use_gpu_data):
python/lammps/mliap/pytorch.py:        if (use_gpu_data):
python/lammps/core.py:    self.lib.lammps_get_gpu_device_info.argtypes = [c_char_p, c_int]
python/lammps/core.py:    for p in ['GPU', 'KOKKOS', 'INTEL', 'OPENMP']:
python/lammps/core.py:      for s in ['cuda', 'hip', 'phi', 'pthreads', 'opencl', 'openmp', 'serial']:
python/lammps/core.py:  def has_gpu_device(self):
python/lammps/core.py:    """ Availability of GPU package compatible device
python/lammps/core.py:    This is a wrapper around the :cpp:func:`lammps_has_gpu_device`
python/lammps/core.py:    :return: True if a GPU package compatible device is present, otherwise False
python/lammps/core.py:    return self.lib.lammps_has_gpu_device() != 0
python/lammps/core.py:  def get_gpu_device_info(self):
python/lammps/core.py:    usable by the GPU package.
python/lammps/core.py:    This is a wrapper around the :cpp:func:`lammps_get_gpu_device_info`
python/lammps/core.py:    :return: GPU device info string
python/lammps/core.py:    self.lib.lammps_get_gpu_device_info(sb,8192)
fortran/lammps.f90:    PROCEDURE, NOPASS :: has_gpu_device => lmp_has_gpu_device
fortran/lammps.f90:    PROCEDURE, NOPASS :: get_gpu_device_info => lmp_get_gpu_device_info
fortran/lammps.f90:    FUNCTION lammps_has_gpu_device() BIND(C)
fortran/lammps.f90:      INTEGER(c_int) :: lammps_has_gpu_device
fortran/lammps.f90:    END FUNCTION lammps_has_gpu_device
fortran/lammps.f90:    SUBROUTINE lammps_get_gpu_device_info(buffer, buf_size) BIND(C)
fortran/lammps.f90:    END SUBROUTINE lammps_get_gpu_device_info
fortran/lammps.f90:  ! equivalent function to lammps_has_gpu_device
fortran/lammps.f90:  LOGICAL FUNCTION lmp_has_gpu_device()
fortran/lammps.f90:    lmp_has_gpu_device = (lammps_has_gpu_device() /= 0_c_int)
fortran/lammps.f90:  END FUNCTION lmp_has_gpu_device
fortran/lammps.f90:  ! equivalent subroutine to lammps_get_gpu_device_info
fortran/lammps.f90:  SUBROUTINE lmp_get_gpu_device_info(buffer)
fortran/lammps.f90:    CALL lammps_get_gpu_device_info(Cptr, buf_size)
fortran/lammps.f90:  END SUBROUTINE lmp_get_gpu_device_info
doc/graphviz/lammps-classes.dot:    Lg [shape=box label="PairLJCutGPU"]
doc/lammps.1:For example "-pk gpu 2" is the same as "package gpu 2" in the input
doc/lammps.1:fall back to the former. The most useful suffixes are  "gpu",
doc/utils/sphinx-config/false_positives.txt:cuda
doc/utils/sphinx-config/false_positives.txt:Cuda
doc/utils/sphinx-config/false_positives.txt:CUDA
doc/utils/sphinx-config/false_positives.txt:gpu
doc/utils/sphinx-config/false_positives.txt:gpuID
doc/utils/sphinx-config/false_positives.txt:gpus
doc/utils/sphinx-config/false_positives.txt:libgpu
doc/utils/sphinx-config/false_positives.txt:Ngpu
doc/utils/sphinx-config/false_positives.txt:ngpus
doc/utils/sphinx-config/false_positives.txt:nvidia
doc/utils/sphinx-config/false_positives.txt:opencl
doc/utils/fixup_headers.py:                if ext not in ('omp', 'intel', 'kk', 'gpu', 'opt'):
doc/utils/check-styles.py:gpu = re.compile("(.+)/gpu\\s*$")
doc/utils/check-styles.py:                if gpu.match(line): needs = True
doc/utils/check-styles.py:                if gpu.match(line): needs = True
doc/utils/check-styles.py:    if styles[name]['gpu']:
doc/utils/check-styles.py:                info = { 'kokkos':  0, 'gpu':     0, 'intel':   0, \
doc/utils/check-styles.py:                suffix = gpu.match(style)
doc/utils/check-styles.py:                    info['gpu'] = 1
doc/src/pair_style.rst:LAMMPS distribution for faster performance on CPUs, GPUs, and KNLs.
doc/src/Speed_kokkos.rst:to run efficiently on different kinds of hardware, such as GPUs, Intel
doc/src/Speed_kokkos.rst:different back end languages such as CUDA, OpenMP, or Pthreads.  The
doc/src/Speed_kokkos.rst:(threading for many-core CPUs and Intel Phi), CUDA (for NVIDIA GPUs) and
doc/src/Speed_kokkos.rst:HIP (for AMD GPUs).  Additional modes (e.g. OpenMP target, Intel data
doc/src/Speed_kokkos.rst:center GPUs) are under development.  You choose the mode at build time
doc/src/Speed_kokkos.rst:.. admonition:: NVIDIA CUDA support
doc/src/Speed_kokkos.rst:   To build with Kokkos support for NVIDIA GPUs, the NVIDIA CUDA toolkit
doc/src/Speed_kokkos.rst:   the discussion for the :doc:`GPU package <Speed_gpu>` for details of
doc/src/Speed_kokkos.rst:.. admonition:: AMD ROCm (HIP) support
doc/src/Speed_kokkos.rst:   To build with Kokkos support for AMD GPUs, the AMD ROCm toolkit
doc/src/Speed_kokkos.rst:.. admonition:: CUDA and MPI library compatibility
doc/src/Speed_kokkos.rst:   Kokkos with CUDA currently implicitly assumes that the MPI library is
doc/src/Speed_kokkos.rst:   GPU-aware.  This is not always the case, especially when using
doc/src/Speed_kokkos.rst:   not a problem when using only a single GPU with a single MPI
doc/src/Speed_kokkos.rst:   faults without GPU-aware MPI support. These can be avoided by adding
doc/src/Speed_kokkos.rst:   the flags :doc:`-pk kokkos gpu/aware off <Run_options>` to the
doc/src/Speed_kokkos.rst:   gpu/aware off <package>` in the input file.
doc/src/Speed_kokkos.rst:.. admonition:: Intel Data Center GPU support
doc/src/Speed_kokkos.rst:   Support for Kokkos with Intel Data Center GPU accelerators (formerly
doc/src/Speed_kokkos.rst:a single thread, no duplication or atomic operations are used. For CUDA
doc/src/Speed_kokkos.rst:Running on GPUs
doc/src/Speed_kokkos.rst:number of GPUs per node. Typically the ``-np`` setting of the ``mpirun`` command
doc/src/Speed_kokkos.rst:physical GPUs on the node. You can assign multiple MPI tasks to the same
doc/src/Speed_kokkos.rst:GPU with the KOKKOS package, but this is usually only faster if some
doc/src/Speed_kokkos.rst:speedup (see the KOKKOS :doc:`package <package>` command). Using CUDA MPS
doc/src/Speed_kokkos.rst:Using a GPU-aware MPI library is highly recommended. GPU-aware MPI use can be
doc/src/Speed_kokkos.rst:avoided by using :doc:`-pk kokkos gpu/aware off <package>`. As above for
doc/src/Speed_kokkos.rst:multicore CPUs (and no GPU), if N is the number of physical cores/node,
doc/src/Speed_kokkos.rst:Here are examples of how to use the KOKKOS package for GPUs, assuming
doc/src/Speed_kokkos.rst:one or more nodes, each with two GPUs:
doc/src/Speed_kokkos.rst:   # 1 node,   2 MPI tasks/node, 2 GPUs/node
doc/src/Speed_kokkos.rst:   mpirun -np 2 lmp_kokkos_cuda_openmpi -k on g 2 -sf kk -in in.lj
doc/src/Speed_kokkos.rst:   # 16 nodes, 2 MPI tasks/node, 2 GPUs/node (32 GPUs total)
doc/src/Speed_kokkos.rst:   mpirun -np 32 -ppn 2 lmp_kokkos_cuda_openmpi -k on g 2 -sf kk -in in.lj
doc/src/Speed_kokkos.rst:   running on GPUs is to use "full" neighbor lists and set the Newton
doc/src/Speed_kokkos.rst:   threaded communication. When running on Maxwell or Kepler GPUs, this
doc/src/Speed_kokkos.rst:   will typically be best. For Pascal GPUs and beyond, using "half"
doc/src/Speed_kokkos.rst:   GPUs. Use the ``-pk kokkos`` :doc:`command-line switch <Run_options>`
doc/src/Speed_kokkos.rst:   mpirun -np 2 lmp_kokkos_cuda_openmpi -k on g 2 -sf kk \
doc/src/Speed_kokkos.rst:   The default binsize for :doc:`atom sorting <atom_modify>` on GPUs
doc/src/Speed_kokkos.rst:   default GPU neighbor binsize). When running simple pair-wise
doc/src/Speed_kokkos.rst:   potentials like Lennard Jones on GPUs, using a 2x larger binsize for
doc/src/Speed_kokkos.rst:   atom sorting (equal to the default GPU neighbor binsize) and a more
doc/src/Speed_kokkos.rst:   When running on GPUs with many MPI ranks (tens of thousands and
doc/src/Speed_kokkos.rst:   on the GPU can slow down significantly or run out of GPU memory and
doc/src/Speed_kokkos.rst:   When using a GPU, you will achieve the best performance if your
doc/src/Speed_kokkos.rst:   Kokkos-enabled. This allows data to stay on the GPU for multiple
doc/src/Speed_kokkos.rst:   kspace, etc., you must set the environment variable ``CUDA_LAUNCH_BLOCKING=1``.
doc/src/Speed_kokkos.rst:**Using OpenMP threading and CUDA together:**
doc/src/Speed_kokkos.rst:With the KOKKOS package, both OpenMP multi-threading and GPUs can be
doc/src/Speed_kokkos.rst:"Cuda" and "OpenMP", as is the case for ``/src/MAKE/OPTIONS/Makefile.kokkos_cuda_mpi``.
doc/src/Speed_kokkos.rst:   KOKKOS_DEVICES=Cuda,OpenMP
doc/src/Speed_kokkos.rst:in the ``kokkos-cuda.cmake`` CMake preset file.
doc/src/Speed_kokkos.rst:   cmake -DKokkos_ENABLE_CUDA=yes -DKokkos_ENABLE_OPENMP=yes ../cmake
doc/src/Speed_kokkos.rst:The suffix "/kk" is equivalent to "/kk/device", and for Kokkos CUDA,
doc/src/Speed_kokkos.rst:using the ``-sf kk`` in the command line gives the default CUDA version
doc/src/Speed_kokkos.rst:as ``t Nt`` and the number of GPUs as ``g Ng``
doc/src/Speed_kokkos.rst:For example, the command to run with 1 GPU and 8 OpenMP threads is then:
doc/src/Speed_kokkos.rst:   mpiexec -np 1 lmp_kokkos_cuda_openmpi -in in.lj -k on g 1 t 8 -sf kk
doc/src/Speed_kokkos.rst:input script, then only that specific style will run on the GPU while
doc/src/Speed_kokkos.rst:execution of the CPU and GPU styles will NOT overlap, except for a
doc/src/Speed_kokkos.rst:GPU. First compile with ``--default-stream per-thread`` added to ``CCFLAGS``
doc/src/Speed_kokkos.rst:in the Kokkos CUDA Makefile.  Then explicitly use the "/kk/host"
doc/src/Speed_kokkos.rst:sure the environment variable ``CUDA_LAUNCH_BLOCKING`` is not set to "1"
doc/src/Speed_kokkos.rst:so CPU/GPU overlap can occur.
doc/src/Speed_kokkos.rst:* When running large number of atoms per GPU, KOKKOS is typically faster
doc/src/Speed_kokkos.rst:  than the GPU package when compiled for double precision.  The benefit
doc/src/Speed_kokkos.rst:  of using single or mixed precision with the GPU package depends
doc/src/Speed_kokkos.rst:  MPI rank per GPU. When trying to use multiple MPI ranks per GPU it is
doc/src/Speed_kokkos.rst:  mandatory to enable `CUDA Multi-Process Service (MPS)
doc/src/Speed_kokkos.rst:  <https://docs.nvidia.com/deploy/mps/index.html>`_ to get good
doc/src/fix_wall_flow.rst:**(Pavlov1)** Pavlov, Kolotinskii, Stegailov, "GPU-Based Molecular Dynamics of Turbulent Liquid Flows with OpenMM", Proceedings of PPAM-2022, LNCS (Springer), vol. 13826, pp. 346-358 (2023)
doc/src/fix_wall_flow.rst:**(Pavlov2)** Pavlov, Galigerov, Kolotinskii, Nikolskiy, Stegailov, "GPU-based Molecular Dynamics of Fluid Flows: Reaching for Turbulence", Int. J. High Perf. Comp. Appl., (2024)
doc/src/variable.rst:* *package*\ : features = *gpu* or *intel* or *kokkos* or *omp*
doc/src/variable.rst:Example 1: Disable use of suffix for PPPM when using GPU package
doc/src/variable.rst:the GPU), but do use the suffix otherwise (e.g. with OPENMP).
doc/src/variable.rst:   if $(is_active(package,gpu)) then "suffix off"
doc/src/fix_nve_asphere.rst:.. index:: fix nve/asphere/gpu
doc/src/fix_nve_asphere.rst:Accelerator Variants: *nve/asphere/gpu*, *nve/asphere/intel*
doc/src/Howto_cmake.rst:     - select which FFT library to use in Kokkos-enabled styles: ``FFTW3``, ``MKL``, ``HIPFFT``, ``CUFFT``, ``MKL_GPU``, ``KISS`` (default)
doc/src/pair_sw.rst:.. index:: pair_style sw/gpu
doc/src/pair_sw.rst:Accelerator Variants: *sw/gpu*, *sw/intel*, *sw/kk*, *sw/omp*
doc/src/pair_fep_soft.rst:.. index:: pair_style lj/cut/coul/cut/soft/gpu
doc/src/pair_fep_soft.rst:.. index:: pair_style lj/cut/coul/long/soft/gpu
doc/src/pair_fep_soft.rst:Accelerator Variants: *lj/cut/coul/cut/soft/gpu*, *lj/cut/coul/cut/soft/omp*
doc/src/pair_fep_soft.rst:Accelerator Variants: *lj/cut/coul/long/soft/gpu*, *lj/cut/coul/long/soft/omp*
doc/src/accel_styles.rst:Styles with a *gpu*, *intel*, *kk*, *omp*, or *opt* suffix are
doc/src/accel_styles.rst:These accelerated styles are part of the GPU, INTEL, KOKKOS,
doc/src/pair_hybrid.rst:   pair style multiple times: GPU-enabled pair styles in the GPU package.
doc/src/pair_hybrid.rst:   This is because the GPU package currently assumes that only one
doc/src/pair_hybrid.rst:  The individual accelerated sub-styles are part of the GPU, KOKKOS,
doc/src/pair_hybrid.rst:When using pair styles from the GPU package they must not be listed
doc/src/Packages_details.rst:   * :ref:`GPU <PKG-GPU>`
doc/src/Packages_details.rst:.. _PKG-GPU:
doc/src/Packages_details.rst:GPU package
doc/src/Packages_details.rst:solver optimized for GPUs.  All such styles have a "gpu" as a suffix
doc/src/Packages_details.rst:in their style name. The GPU code can be compiled with either CUDA or
doc/src/Packages_details.rst:OpenCL, however the OpenCL variants are no longer actively maintained
doc/src/Packages_details.rst:and only the CUDA versions are regularly tested.  The
doc/src/Packages_details.rst::doc:`Speed_gpu` page gives details of what hardware and GPU
doc/src/Packages_details.rst:gpu`` or ``-suffix gpu`` :doc:`command-line switches <Run_options>`.  See
doc/src/Packages_details.rst:also the :ref:`KOKKOS <PKG-KOKKOS>` package, which has GPU-enabled styles.
doc/src/Packages_details.rst:This package has :ref:`specific installation instructions <gpu>` on the
doc/src/Packages_details.rst:* ``src/GPU``: filenames -> commands
doc/src/Packages_details.rst:* ``src/GPU/README``
doc/src/Packages_details.rst:* ``lib/gpu/README``
doc/src/Packages_details.rst:* :doc:`GPU package <Speed_gpu>`
doc/src/Packages_details.rst:* :doc:`Section 2.6 -sf gpu <Run_options>`
doc/src/Packages_details.rst:* :doc:`Section 2.6 -pk gpu <Run_options>`
doc/src/Packages_details.rst:* :doc:`package gpu <package>`
doc/src/Packages_details.rst:them to OpenMP or CUDA code so that they run efficiently on multicore
doc/src/Packages_details.rst:CPUs, KNLs, or GPUs.  All the styles have a "kk" as a suffix in their
doc/src/Packages_details.rst:Also see the :ref:`GPU <PKG-GPU>`, :ref:`OPT <PKG-OPT>`, :ref:`INTEL
doc/src/Packages_details.rst:optimized for CPUs, KNLs, and GPUs.
doc/src/pair_lj96.rst:.. index:: pair_style lj96/cut/gpu
doc/src/pair_lj96.rst:Accelerator Variants: *lj96/cut/gpu*, *lj96/cut/omp*
doc/src/bond_style.rst:LAMMPS distribution for faster performance on CPUs, GPUs, and KNLs.
doc/src/angle_style.rst:LAMMPS distribution for faster performance on CPUs, GPUs, and KNLs.
doc/src/pair_gayberne.rst:.. index:: pair_style gayberne/gpu
doc/src/pair_gayberne.rst:Accelerator Variants: *gayberne/gpu*, *gayberne/intel*, *gayberne/omp*
doc/src/pair_yukawa_colloid.rst:.. index:: pair_style yukawa/colloid/gpu
doc/src/pair_yukawa_colloid.rst:Accelerator Variants: *yukawa/colloid/gpu*, *yukawa/colloid/kk*, *yukawa/colloid/omp*
doc/src/Commands_bond.rst:parenthesis: g = GPU, i = INTEL, k = KOKKOS, o = OPENMP, t =
doc/src/Commands_bond.rst:parenthesis: g = GPU, i = INTEL, k = KOKKOS, o = OPENMP, t =
doc/src/Commands_bond.rst:parenthesis: g = GPU, i = INTEL, k = KOKKOS, o = OPENMP, t =
doc/src/Commands_bond.rst:parenthesis: g = GPU, i = INTEL, k = KOKKOS, o = OPENMP, t =
doc/src/pair_dpd.rst:.. index:: pair_style dpd/gpu
doc/src/pair_dpd.rst:.. index:: pair_style dpd/tstat/gpu
doc/src/pair_dpd.rst:Accelerator Variants: *dpd/gpu*, *dpd/intel*, *dpd/kk*, *dpd/omp*
doc/src/pair_dpd.rst:Accelerator Variants: *dpd/tstat/gpu*, *dpd/tstat/kk*, *dpd/tstat/omp*
doc/src/pair_dpd.rst:Styles with a *gpu* suffix are implemented based on
doc/src/kspace_style.rst:.. index:: kspace_style pppm/gpu
doc/src/kspace_style.rst:* style = *none* or *ewald* or *ewald/dipole* or *ewald/dipole/spin* or *ewald/disp* or *ewald/disp/dipole* or *ewald/omp* or *ewald/electrode* or *pppm* or *pppm/cg* or *pppm/disp* or *pppm/tip4p* or *pppm/stagger* or *pppm/disp/tip4p* or *pppm/gpu* or *pppm/intel* or *pppm/disp/intel* or *pppm/kk* or *pppm/omp* or *pppm/cg/omp* or *pppm/disp/tip4p/omp* or *pppm/tip4p/omp* or *pppm/dielectic* or *pppm/disp/dielectric* or *pppm/electrode* or *pppm/electrode/intel* or *msm* or *msm/cg* or *msm/omp* or *msm/cg/omp* or *msm/dielectric* or *scafacos*
doc/src/kspace_style.rst:       *pppm/gpu* value = accuracy
doc/src/kspace_style.rst:   GPUs.  The use of the -DFFT_SINGLE flag is discussed on the :doc:`Build settings <Build_settings>` doc page. MSM does not currently support
doc/src/kspace_style.rst:  For the GPU package, the *pppm/gpu* style performs charge assignment
doc/src/kspace_style.rst:  and force interpolation calculations on the GPU.  These processes
doc/src/kspace_style.rst:  calculated on the CPU.  If *pppm/gpu* is used with a GPU-enabled
doc/src/kspace_style.rst:  concurrently on the GPU while other calculations for non-bonded and
doc/src/kspace_style.rst:  themselves, on the GPU or (optionally) threaded on the CPU when
doc/src/fix_nve.rst:.. index:: fix nve/gpu
doc/src/fix_nve.rst:Accelerator Variants: *nve/gpu*, *nve/intel*, *nve/kk*, *nve/omp*
doc/src/Errors_messages.rst:   system. For NVIDIA GPUs, see the nvidia-smi command to change this
doc/src/Errors_messages.rst:*Cannot (yet) do analytic differentiation with pppm/gpu*
doc/src/Errors_messages.rst:*Cannot currently use pppm/gpu with fix balance.*
doc/src/Errors_messages.rst:*Cannot use neigh_modify exclude with GPU neighbor builds*
doc/src/Errors_messages.rst:   This is a current limitation of the GPU implementation
doc/src/Errors_messages.rst:*Cannot use newton pair with beck/gpu pair style*
doc/src/Errors_messages.rst:*Cannot use newton pair with born/coul/long/gpu pair style*
doc/src/Errors_messages.rst:*Cannot use newton pair with born/coul/wolf/gpu pair style*
doc/src/Errors_messages.rst:*Cannot use newton pair with born/gpu pair style*
doc/src/Errors_messages.rst:*Cannot use newton pair with buck/coul/cut/gpu pair style*
doc/src/Errors_messages.rst:*Cannot use newton pair with buck/coul/long/gpu pair style*
doc/src/Errors_messages.rst:*Cannot use newton pair with buck/gpu pair style*
doc/src/Errors_messages.rst:*Cannot use newton pair with colloid/gpu pair style*
doc/src/Errors_messages.rst:*Cannot use newton pair with coul/cut/gpu pair style*
doc/src/Errors_messages.rst:*Cannot use newton pair with coul/debye/gpu pair style*
doc/src/Errors_messages.rst:*Cannot use newton pair with coul/dsf/gpu pair style*
doc/src/Errors_messages.rst:*Cannot use newton pair with coul/long/gpu pair style*
doc/src/Errors_messages.rst:*Cannot use newton pair with dipole/cut/gpu pair style*
doc/src/Errors_messages.rst:*Cannot use newton pair with dipole/sf/gpu pair style*
doc/src/Errors_messages.rst:*Cannot use newton pair with dpd/gpu pair style*
doc/src/Errors_messages.rst:*Cannot use newton pair with dpd/tstat/gpu pair style*
doc/src/Errors_messages.rst:*Cannot use newton pair with eam/alloy/gpu pair style*
doc/src/Errors_messages.rst:*Cannot use newton pair with eam/fs/gpu pair style*
doc/src/Errors_messages.rst:*Cannot use newton pair with eam/gpu pair style*
doc/src/Errors_messages.rst:*Cannot use newton pair with gauss/gpu pair style*
doc/src/Errors_messages.rst:*Cannot use newton pair with gayberne/gpu pair style*
doc/src/Errors_messages.rst:*Cannot use newton pair with lj/charmm/coul/long/gpu pair style*
doc/src/Errors_messages.rst:*Cannot use newton pair with lj/class2/coul/long/gpu pair style*
doc/src/Errors_messages.rst:*Cannot use newton pair with lj/class2/gpu pair style*
doc/src/Errors_messages.rst:*Cannot use newton pair with lj/cubic/gpu pair style*
doc/src/Errors_messages.rst:*Cannot use newton pair with lj/cut/coul/cut/gpu pair style*
doc/src/Errors_messages.rst:*Cannot use newton pair with lj/cut/coul/debye/gpu pair style*
doc/src/Errors_messages.rst:*Cannot use newton pair with lj/cut/coul/dsf/gpu pair style*
doc/src/Errors_messages.rst:*Cannot use newton pair with lj/cut/coul/long/gpu pair style*
doc/src/Errors_messages.rst:*Cannot use newton pair with lj/cut/coul/msm/gpu pair style*
doc/src/Errors_messages.rst:*Cannot use newton pair with lj/cut/gpu pair style*
doc/src/Errors_messages.rst:*Cannot use newton pair with lj/expand/gpu pair style*
doc/src/Errors_messages.rst:*Cannot use newton pair with lj/gromacs/gpu pair style*
doc/src/Errors_messages.rst:*Cannot use newton pair with lj/spica/coul/long/gpu pair style*
doc/src/Errors_messages.rst:*Cannot use newton pair with lj/spica/gpu pair style*
doc/src/Errors_messages.rst:*Cannot use newton pair with lj96/cut/gpu pair style*
doc/src/Errors_messages.rst:*Cannot use newton pair with mie/cut/gpu pair style*
doc/src/Errors_messages.rst:*Cannot use newton pair with morse/gpu pair style*
doc/src/Errors_messages.rst:*Cannot use newton pair with resquared/gpu pair style*
doc/src/Errors_messages.rst:*Cannot use newton pair with soft/gpu pair style*
doc/src/Errors_messages.rst:*Cannot use newton pair with table/gpu pair style*
doc/src/Errors_messages.rst:*Cannot use newton pair with yukawa/colloid/gpu pair style*
doc/src/Errors_messages.rst:*Cannot use newton pair with yukawa/gpu pair style*
doc/src/Errors_messages.rst:*Cannot use newton pair with zbl/gpu pair style*
doc/src/Errors_messages.rst:*Cannot use order greater than 8 with pppm/gpu.*
doc/src/Errors_messages.rst:*Cannot use package gpu neigh yes with triclinic box*
doc/src/Errors_messages.rst:   Could not initialize at least one of the devices specified for the gpu
doc/src/Errors_messages.rst:*GPU library not compiled for this accelerator*
doc/src/Errors_messages.rst:*GPU package does not (yet) work with atom_style template*
doc/src/Errors_messages.rst:*GPU particle split must be set to 1 for this pair style.*
doc/src/Errors_messages.rst:*GPUs are requested but Kokkos has not been compiled for CUDA*
doc/src/Errors_messages.rst:   Re-compile Kokkos with CUDA support to use GPUs.
doc/src/Errors_messages.rst:   There is insufficient memory on one of the devices specified for the gpu
doc/src/Errors_messages.rst:*Invalid custom OpenCL parameter string.*
doc/src/Errors_messages.rst:   GPU.
doc/src/Errors_messages.rst:   For 3-body potentials on the GPU, the threads_per_atom setting cannot be
doc/src/Errors_messages.rst:   greater than 4 for NVIDIA GPUs.
doc/src/Errors_messages.rst:*Kokkos has been compiled for CUDA but no GPUs are requested*
doc/src/Errors_messages.rst:   One or more GPUs must be used when Kokkos is compiled for CUDA.
doc/src/Errors_messages.rst:*Must use 'kspace_modify pressure/scalar no' with GPU MSM Pair styles*
doc/src/Errors_messages.rst:   The kspace scalar pressure option is not (yet) compatible with GPU MSM Pair styles.
doc/src/Errors_messages.rst:*Must use Kokkos half/thread or full neighbor list with threads or GPUs*
doc/src/Errors_messages.rst:*Package gpu command without GPU package installed*
doc/src/Errors_messages.rst:   The GPU package must be installed via "make yes-gpu" before LAMMPS is
doc/src/Errors_messages.rst:*Pair dipole/cut/gpu requires atom attributes q, mu, torque*
doc/src/Errors_messages.rst:*Pair dipole/sf/gpu requires atom attributes q, mu, torque*
doc/src/Errors_messages.rst:*Pair gayberne/gpu requires atom style ellipsoid*
doc/src/Errors_messages.rst:*Pair gayberne/gpu requires atoms with same type have same shape*
doc/src/Errors_messages.rst:*Pair resquared/gpu requires atom style ellipsoid*
doc/src/Errors_messages.rst:*Pair resquared/gpu requires atoms with same type have same shape*
doc/src/Errors_messages.rst:*Pair style born/coul/long/gpu requires atom attribute q*
doc/src/Errors_messages.rst:*Pair style buck/coul/long/gpu requires atom attribute q*
doc/src/Errors_messages.rst:*Pair style coul/cut/gpu requires atom attribute q*
doc/src/Errors_messages.rst:*Pair style coul/debye/gpu requires atom attribute q*
doc/src/Errors_messages.rst:*Pair style coul/dsf/gpu requires atom attribute q*
doc/src/Errors_messages.rst:*Pair style coul/long/gpu requires atom attribute q*
doc/src/Errors_messages.rst:*Pair style lj/charmm/coul/long/gpu requires atom attribute q*
doc/src/Errors_messages.rst:*Pair style lj/class2/coul/long/gpu requires atom attribute q*
doc/src/Errors_messages.rst:*Pair style lj/cut/coul/cut/gpu requires atom attribute q*
doc/src/Errors_messages.rst:*Pair style lj/cut/coul/debye/gpu requires atom attribute q*
doc/src/Errors_messages.rst:*Pair style lj/cut/coul/dsf/gpu requires atom attribute q*
doc/src/Errors_messages.rst:*Pair style lj/cut/coul/long/gpu requires atom attribute q*
doc/src/Errors_messages.rst:*Pair style lj/spica/coul/long/gpu requires atom attribute q*
doc/src/Errors_messages.rst:*Pair style sw/gpu requires atom IDs*
doc/src/Errors_messages.rst:*Pair style sw/gpu requires newton pair off*
doc/src/Errors_messages.rst:*Pair style vashishta/gpu requires atom IDs*
doc/src/Errors_messages.rst:*Pair style vashishta/gpu requires newton pair off*
doc/src/Errors_messages.rst:*Pair style tersoff/gpu requires atom IDs*
doc/src/Errors_messages.rst:   This is a requirement to use the tersoff/gpu potential.
doc/src/Errors_messages.rst:*Pair style tersoff/gpu requires newton pair off*
doc/src/Errors_messages.rst:*Pair yukawa/colloid/gpu requires atom style sphere*
doc/src/Errors_messages.rst:*The package gpu command is required for gpu styles*
doc/src/Errors_messages.rst:   There was a problem initializing an accelerator for the gpu package
doc/src/Errors_messages.rst:*Unknown error in GPU library*
doc/src/Errors_messages.rst:*Using suffix gpu without GPU package installed*
doc/src/pair_lj_cubic.rst:.. index:: pair_style lj/cubic/gpu
doc/src/pair_lj_cubic.rst:Accelerator Variants: *lj/cubic/gpu*, *lj/cubic/omp*
doc/src/pair_born.rst:.. index:: pair_style born/gpu
doc/src/pair_born.rst:.. index:: pair_style born/coul/long/gpu
doc/src/pair_born.rst:.. index:: pair_style born/coul/wolf/gpu
doc/src/pair_born.rst:Accelerator Variants: *born/omp*, *born/gpu*
doc/src/pair_born.rst:Accelerator Variants: *born/coul/long/gpu*, *born/coul/long/omp*
doc/src/pair_born.rst:Accelerator Variants: *born/coul/wolf/gpu*, *born/coul/wolf/omp*
doc/src/pair_spica.rst:.. index:: pair_style lj/spica/gpu
doc/src/pair_spica.rst:.. index:: pair_style lj/spica/coul/long/gpu
doc/src/pair_spica.rst:Accelerator Variants: *lj/spica/gpu*, *lj/spica/kk*, *lj/spica/omp*
doc/src/pair_spica.rst:Accelerator Variants: *lj/spica/coul/long/gpu*, *lj/spica/coul/long/omp*, *lj/spica/coul/long/kk*
doc/src/Developer_parallel.rst:proportional to the system size).  Additional parallelization using GPUs
doc/src/pair_lj_cut_tip4p.rst:.. index:: pair_style lj/cut/tip4p/long/gpu
doc/src/pair_lj_cut_tip4p.rst:Accelerator Variants: *lj/cut/tip4p/long/gpu*, *lj/cut/tip4p/long/omp*, *lj/cut/tip4p/long/opt*
doc/src/pair_charmm.rst:.. index:: pair_style lj/charmm/coul/charmm/gpu
doc/src/pair_charmm.rst:.. index:: pair_style lj/charmm/coul/long/gpu
doc/src/pair_charmm.rst:Accelerator Variants: *lj/charmm/coul/charmm/gpu*, *lj/charmm/coul/charmm/intel*, *lj/charmm/coul/charmm/kk*, *lj/charmm/coul/charmm/omp*
doc/src/pair_charmm.rst:Accelerator Variants: *lj/charmm/coul/long/gpu*, *lj/charmm/coul/long/intel*, *lj/charmm/coul/long/kk*, *lj/charmm/coul/long/opt*, *lj/charmm/coul/long/omp*
doc/src/Install_mac.rst:additional requirements not yet met: GPU, KOKKOS, MSCG, POEMS,
doc/src/Build_basics.rst:         # Building with PGI/Nvidia Compilers:
doc/src/Build_basics.rst:            Makefile.gpu                   # GPU package
doc/src/Build_basics.rst:            Makefile.kokkos_cuda_mpi       # KOKKOS package for GPUs
doc/src/compute_tally.rst:GPU, INTEL, KOKKOS, or OPENMP packages. They will either create an error
doc/src/pair_dipole.rst:.. index:: pair_style lj/cut/dipole/cut/gpu
doc/src/pair_dipole.rst:.. index:: pair_style lj/sf/dipole/sf/gpu
doc/src/pair_dipole.rst:.. index:: pair_style lj/cut/dipole/long/gpu
doc/src/pair_dipole.rst:Accelerator Variants: *lj/cut/dipole/cut/gpu*, *lj/cut/dipole/cut/kk*, *lj/cut/dipole/cut/omp*
doc/src/pair_dipole.rst:Accelerator Variants: *lj/sf/dipole/sf/gpu*, *lj/sf/dipole/sf/omp*
doc/src/pair_dipole.rst:Accelerator Variants: *lj/cut/dipole/long/gpu*
doc/src/fix.rst:LAMMPS distribution for faster performance on CPUs, GPUs, and KNLs.
doc/src/pair_python.rst:   GPU or multi-thread acceleration through the GPU, KOKKOS, or OPENMP
doc/src/pair_modify.rst:   GPU or the INTEL package and attempting to use it will cause
doc/src/pair_modify.rst:You cannot use *special* with pair styles from the GPU or
doc/src/Build_make.rst:   make kokkos_cuda_mpi # build with the KOKKOS package for GPUs
doc/src/pair_sph_lj.rst:.. index:: pair_style sph/lj/gpu
doc/src/pair_sph_lj.rst:Accelerator Variants: *sph/lj/gpu*
doc/src/pair_amoeba.rst:.. index:: pair_style amoeba/gpu
doc/src/pair_amoeba.rst:.. index:: pair_style hippo/gpu
doc/src/pair_amoeba.rst:Accelerator Variants: *amoeba/gpu*
doc/src/pair_amoeba.rst:Accelerator Variants: *hippo/gpu*
doc/src/pair_amoeba.rst:Accelerator support via the GPU package is available.
doc/src/pair_amoeba.rst:  Using the GPU accelerated pair styles 'amoeba/gpu' or 'hippo/gpu'
doc/src/pair_amoeba.rst:  when compiling the GPU package for OpenCL has a few known issues
doc/src/pair_amoeba.rst:  when running on integrated GPUs and the calculation may crash.
doc/src/pair_amoeba.rst:  The GPU accelerated pair styles are also not (yet) compatible
doc/src/Intro_overview.rst:support `OpenMP multi-threading <omp_>`_, vectorization, and GPU
doc/src/Intro_overview.rst:border their subdomain.  Multi-threading parallelization and GPU
doc/src/dihedral_style.rst:LAMMPS distribution for faster performance on CPUs, GPUs, and KNLs.
doc/src/Speed.rst:for certain kinds of hardware, including multicore CPUs, GPUs, and
doc/src/pair_reaxff.rst:is derived from the *reaxff* style.  The Kokkos version can run on GPUs
doc/src/pair_reaxff.rst:number of atoms/GPU on AMD hardware). It is also enabled by default
doc/src/pair_reaxff.rst:on GPU.
doc/src/Developer_write_pair.rst:(:ref:`GPU <PKG-GPU>`, :ref:`INTEL <PKG-INTEL>`, :ref:`KOKKOS
doc/src/Speed_gpu.rst:GPU package
doc/src/Speed_gpu.rst:The GPU package was developed by Mike Brown while at SNL and ORNL (now
doc/src/Speed_gpu.rst:Northwestern).  Support for AMD GPUs via HIP was added by Vsevolod Nikolskiy
doc/src/Speed_gpu.rst:The GPU package provides GPU versions of many pair styles and for
doc/src/Speed_gpu.rst:* It is designed to exploit common GPU hardware configurations where one
doc/src/Speed_gpu.rst:  or more GPUs are coupled to many cores of one or more multicore CPUs,
doc/src/Speed_gpu.rst:  between the CPU(s) and GPU every timestep.
doc/src/Speed_gpu.rst:* Neighbor lists can be built on the CPU or on the GPU
doc/src/Speed_gpu.rst:  run on the GPU.  The FFT portion, which requires MPI communication
doc/src/Speed_gpu.rst:  can be performed concurrently on the GPU and CPU(s), respectively.
doc/src/Speed_gpu.rst:* It allows for GPU computations to be performed in single or double
doc/src/Speed_gpu.rst:* LAMMPS-specific code is in the GPU package.  It makes calls to a
doc/src/Speed_gpu.rst:  generic GPU library in the lib/gpu directory.  This library provides
doc/src/Speed_gpu.rst:  either Nvidia support, AMD support, or more general OpenCL support
doc/src/Speed_gpu.rst:  (for Nvidia GPUs, AMD GPUs, Intel GPUs, and multicore CPUs).
doc/src/Speed_gpu.rst:To compile and use this package in CUDA mode, you currently need
doc/src/Speed_gpu.rst:to have an NVIDIA GPU and install the corresponding NVIDIA CUDA
doc/src/Speed_gpu.rst:* Check if you have an NVIDIA GPU: ``cat /proc/driver/nvidia/gpus/\*/information``
doc/src/Speed_gpu.rst:* Go to https://developer.nvidia.com/cuda-downloads
doc/src/Speed_gpu.rst:* Run ``lammps/lib/gpu/nvc_get_devices`` (after building the GPU library, see below) to
doc/src/Speed_gpu.rst:To compile and use this package in OpenCL mode, you currently need
doc/src/Speed_gpu.rst:to have the OpenCL headers and the (vendor neutral) OpenCL library installed.
doc/src/Speed_gpu.rst:In OpenCL mode, the acceleration depends on having an `OpenCL Installable Client Driver (ICD) <https://www.khronos.org/news/permalink/opencl-installable-client-driver-icd-loader>`_
doc/src/Speed_gpu.rst:(GPUs, CPUs, Accelerators) installed at the same time. OpenCL refers to those
doc/src/Speed_gpu.rst:as 'platforms'.  The GPU library will try to auto-select the best suitable platform,
doc/src/Speed_gpu.rst:command. run ``lammps/lib/gpu/ocl_get_devices`` to get a list of available
doc/src/Speed_gpu.rst:To compile and use this package for Intel GPUs, OpenCL or the Intel oneAPI
doc/src/Speed_gpu.rst:If you do not have a discrete GPU card installed, this package can still provide
doc/src/Speed_gpu.rst:significant speedups on some CPUs that include integrated GPUs. Additionally, for
doc/src/Speed_gpu.rst:many macs, OpenCL is already included with the OS and Makefiles are available
doc/src/Speed_gpu.rst:in the ``lib/gpu`` directory.
doc/src/Speed_gpu.rst:To compile and use this package in HIP mode, you have to have the AMD ROCm
doc/src/Speed_gpu.rst:software installed. Versions of ROCm older than 3.5 are currently deprecated
doc/src/Speed_gpu.rst:**Building LAMMPS with the GPU package:**
doc/src/Speed_gpu.rst:See the :ref:`Build extras <gpu>` page for
doc/src/Speed_gpu.rst:**Run with the GPU package from the command line:**
doc/src/Speed_gpu.rst:When using the GPU package, you cannot assign more than one GPU to a
doc/src/Speed_gpu.rst:single MPI task.  However multiple MPI tasks can share the same GPU,
doc/src/Speed_gpu.rst:# of CPU cores.  Assignment of multiple MPI tasks to a GPU will happen
doc/src/Speed_gpu.rst:GPUs/mode.  E.g. with 8 MPI tasks/node and 2 GPUs, each GPU will be
doc/src/Speed_gpu.rst:The GPU package also has limited support for OpenMP for both
doc/src/Speed_gpu.rst:This requires that the GPU library and LAMMPS are built with flags to
doc/src/Speed_gpu.rst:are also available in the GPU package. These run completely on the CPUs
doc/src/Speed_gpu.rst:Use the ``-sf gpu`` :doc:`command-line switch <Run_options>`, which will
doc/src/Speed_gpu.rst:automatically append "gpu" to styles that support it.  Use the ``-pk
doc/src/Speed_gpu.rst:gpu Ng`` :doc:`command-line switch <Run_options>` to set ``Ng`` = # of
doc/src/Speed_gpu.rst:GPUs/node to use. If ``Ng`` is 0, the number is selected automatically as
doc/src/Speed_gpu.rst:the number of matching GPUs that have the highest number of compute
doc/src/Speed_gpu.rst:   # 1 MPI task uses 1 GPU
doc/src/Speed_gpu.rst:   lmp_machine -sf gpu -pk gpu 1 -in in.script
doc/src/Speed_gpu.rst:   # 12 MPI tasks share 2 GPUs on a single 16-core (or whatever) node
doc/src/Speed_gpu.rst:   mpirun -np 12 lmp_machine -sf gpu -pk gpu 2 -in in.script
doc/src/Speed_gpu.rst:   mpirun -np 48 -ppn 12 lmp_machine -sf gpu -pk gpu 2 -in in.script
doc/src/Speed_gpu.rst:Note that if the ``-sf gpu`` switch is used, it also issues a default
doc/src/Speed_gpu.rst::doc:`package gpu 0 <package>` command, which will result in
doc/src/Speed_gpu.rst:automatic selection of the number of GPUs to use.
doc/src/Speed_gpu.rst:GPUs/node to use and additional options.  Its syntax is the same as
doc/src/Speed_gpu.rst:the ``package gpu`` command.  See the :doc:`package <package>`
doc/src/Speed_gpu.rst:Note that the default for the :doc:`package gpu <package>` command is to
doc/src/Speed_gpu.rst:GPU package pair styles.
doc/src/Speed_gpu.rst:**Or run with the GPU package by editing an input script:**
doc/src/Speed_gpu.rst:tasks/node, and use of multiple MPI tasks/GPU is the same.
doc/src/Speed_gpu.rst:Use the :doc:`suffix gpu <suffix>` command, or you can explicitly add an
doc/src/Speed_gpu.rst:"gpu" suffix to individual styles in your input script, e.g.
doc/src/Speed_gpu.rst:   pair_style lj/cut/gpu 2.5
doc/src/Speed_gpu.rst:You must also use the :doc:`package gpu <package>` command to enable the
doc/src/Speed_gpu.rst:GPU package, unless the ``-sf gpu`` or ``-pk gpu`` :doc:`command-line switches <Run_options>` were used.  It specifies the number of
doc/src/Speed_gpu.rst:GPUs/node to use, as well as other options.
doc/src/Speed_gpu.rst:The performance of a GPU versus a multicore CPU is a function of your
doc/src/Speed_gpu.rst:hardware, which pair style is used, the number of atoms/GPU, and the
doc/src/Speed_gpu.rst:precision used on the GPU (double, single, mixed). Using the GPU package
doc/src/Speed_gpu.rst:in OpenCL mode on CPUs (which uses vectorization and multithreading) is
doc/src/Speed_gpu.rst:LAMMPS website for performance of the GPU package on various
doc/src/Speed_gpu.rst:You should also experiment with how many MPI tasks per GPU to use to
doc/src/Speed_gpu.rst:Likewise, you should experiment with the precision setting for the GPU
doc/src/Speed_gpu.rst:cases using fewer MPI tasks and multiple OpenMP threads with the GPU
doc/src/Speed_gpu.rst:is higher for these styles with the GPU package in order to allow
doc/src/Speed_gpu.rst:* Using multiple MPI tasks per GPU will often give the best performance,
doc/src/Speed_gpu.rst:  as allowed my most multicore CPU/GPU configurations.
doc/src/Speed_gpu.rst:  GPU, even if you do not use all the cores on the compute node.
doc/src/Speed_gpu.rst:* The :doc:`package gpu <package>` command has several options for tuning
doc/src/Speed_gpu.rst:  performance.  Neighbor lists can be built on the GPU or CPU.  Force
doc/src/Speed_gpu.rst:  GPUs.  GPU-specific settings can be made which can be optimized
doc/src/Speed_gpu.rst:* As described by the :doc:`package gpu <package>` command, GPU
doc/src/Speed_gpu.rst:  computations and the time required to complete the GPU pair style
doc/src/Speed_gpu.rst:  computations. Any time spent for GPU-enabled pair styles for
doc/src/Speed_gpu.rst:* Since only part of the pppm kspace style is GPU accelerated, it
doc/src/Speed_gpu.rst:  may be faster to only use GPU acceleration for Pair styles with
doc/src/Speed_gpu.rst:  on the CPU and non-bonded interactions on the GPU can be balanced
doc/src/Speed_gpu.rst:* When the *mode* setting for the package gpu command is force/neigh,
doc/src/Speed_gpu.rst:  the time for neighbor list calculations on the GPU will be added into
doc/src/Speed_gpu.rst:  times required for various tasks on the GPU (data copy, neighbor
doc/src/Speed_gpu.rst:  timings represent total time spent on the GPU for each routine,
doc/src/Speed_gpu.rst:* The output section "GPU Time Info (average)" reports "Max Mem / Proc".
doc/src/Speed_gpu.rst:  This is the maximum memory used at one time on the GPU for data
doc/src/Speed_bench.rst:* GPU and Xeon Phi performance on same and related problems
doc/src/Speed_bench.rst:for running the same (or similar) problems using OpenMP or GPU or Xeon
doc/src/Speed_bench.rst:core.  For nodes with accelerator options or hardware (OpenMP, GPU,
doc/src/pair_class2.rst:.. index:: pair_style lj/class2/gpu
doc/src/pair_class2.rst:.. index:: pair_style lj/class2/coul/long/gpu
doc/src/pair_class2.rst:Accelerator Variants: *lj/class2/gpu*, *lj/class2/kk*, *lj/class2/omp*
doc/src/pair_class2.rst:Accelerator Variants: *lj/class2/coul/long/gpu*, *lj/class2/coul/long/kk*, *lj/class2/coul/long/omp*
doc/src/Bibliography.rst:   Pavlov, Galigerov, Kolotinskii, Nikolskiy, Stegailov, "GPU-based Molecular Dynamics of Fluid Flows: Reaching for Turbulence", Int. J. High Perf. Comp. Appl., (2024)
doc/src/pair_mie.rst:.. index:: pair_style mie/cut/gpu
doc/src/pair_mie.rst:Accelerator Variants: *mie/cut/gpu*
doc/src/Build_package.rst:   * :ref:`GPU <gpu>`
doc/src/Build_package.rst:   # GPU package and configure it for using CUDA. You can run.
doc/src/Build_package.rst:         -D PKG_GPU=on -D GPU_API=cuda ../cmake
doc/src/pair_soft.rst:.. index:: pair_style soft/gpu
doc/src/pair_soft.rst:Accelerator Variants: *soft/gpu*, *soft/kk*, *soft/omp*
doc/src/pair_gauss.rst:.. index:: pair_style gauss/gpu
doc/src/pair_gauss.rst:Accelerator Variants: *gauss/gpu*, *gauss/omp*
doc/src/Commands_compute.rst:additional letters in parenthesis: g = GPU, i = INTEL, k =
doc/src/pair_morse.rst:.. index:: pair_style morse/gpu
doc/src/pair_morse.rst:Accelerator Variants: *morse/gpu*, *morse/omp*, *morse/opt*, *morse/kk*
doc/src/Commands_kspace.rst:parenthesis: g = GPU, i = INTEL, k = KOKKOS, o = OPENMP, t =
doc/src/Python_install.rst:   (e.g. ``liblammps_mpi.so`` or ``liblammps_gpu.so``) into the
doc/src/kspace_modify.rst:or with suffix kspace/pair styles of MSM, like OMP or GPU.
doc/src/Developer_unittest.rst:Tests for the GPU package are time consuming and thus are only run
doc/src/Developer_unittest.rst:*after* a merge, or when a special label, ``gpu_unit_tests`` is added
doc/src/Developer_unittest.rst:tests when using KOKKOS with GPU acceleration is currently not supported.
doc/src/Developer_unittest.rst:   [ RUN      ] PairStyle.gpu
doc/src/Developer_unittest.rst:   /home/builder/workspace/dev/pull_requests/ubuntu_gpu/unit_tests/cmake_gpu_opencl_mixed_smallbig_clang_static/unittest/force-styles/test_main.cpp:63: Failure
doc/src/Developer_unittest.rst:   /home/builder/workspace/dev/pull_requests/ubuntu_gpu/unit_tests/cmake_gpu_opencl_mixed_smallbig_clang_static/unittest/force-styles/test_main.cpp:56: EXPECT_FORCES: init_forces (newton off)
doc/src/Developer_unittest.rst:   /home/builder/workspace/dev/pull_requests/ubuntu_gpu/unit_tests/cmake_gpu_opencl_mixed_smallbig_clang_static/unittest/force-styles/test_main.cpp:64: Failure
doc/src/pair_zbl.rst:.. index:: pair_style zbl/gpu
doc/src/pair_zbl.rst:Accelerator Variants: *zbl/gpu*, *zbl/kk*, *zbl/omp*
doc/src/pair_resquared.rst:.. index:: pair_style resquared/gpu
doc/src/pair_resquared.rst:Accelerator Variants: *resquared/gpu*, *resquared/omp*
doc/src/pair_cs.rst:.. index:: pair_style born/coul/long/cs/gpu
doc/src/pair_cs.rst:.. index:: pair_style born/coul/wolf/cs/gpu
doc/src/pair_cs.rst:.. index:: pair_style coul/long/cs/gpu
doc/src/pair_cs.rst:Accelerator Variants: *born/coul/long/cs/gpu*
doc/src/pair_cs.rst:Accelerator Variants: *born/coul/wolf/cs/gpu*
doc/src/pair_cs.rst:Accelerator Variants: *coul/long/cs/gpu*
doc/src/Install_windows.rst:GPU package is compiled for OpenCL with mixed precision kernels.
doc/src/pair_snap.rst:using the pair style *snap* with the KOKKOS package on GPUs and are
doc/src/pair_snap.rst:calculation will be broken up into two passes (running on a single GPU).
doc/src/pair_snap.rst:saturate the GPU threads otherwise.  However, the extra parallelism also
doc/src/pair_snap.rst:already large enough to saturate the GPU threads.  Extra parallelism
doc/src/pair_snap.rst:will be performed if the *chunksize* (or total number of atoms per GPU)
doc/src/pair_ufm.rst:.. index:: pair_style ufm/gpu
doc/src/pair_ufm.rst:Accelerator Variants: *ufm/gpu*, *ufm/omp*, *ufm/opt*
doc/src/pair_lj_smooth.rst:.. index:: pair_style lj/smooth/gpu
doc/src/pair_lj_smooth.rst:Accelerator Variants: *lj/smooth/gpu*, *lj/smooth/omp*
doc/src/Library_config.rst:- :cpp:func:`lammps_has_gpu_device`
doc/src/Library_config.rst:- :cpp:func:`lammps_gpu_device_info`
doc/src/Library_config.rst:.. doxygenfunction:: lammps_has_gpu_device
doc/src/Library_config.rst:.. doxygenfunction:: lammps_get_gpu_device_info
doc/src/pair_yukawa.rst:.. index:: pair_style yukawa/gpu
doc/src/pair_yukawa.rst:Accelerator Variants: *yukawa/gpu*, *yukawa/omp*, *yukawa/kk*
doc/src/Speed_packages.rst:to be present on your system, e.g. GPUs or Intel Xeon Phi
doc/src/Speed_packages.rst:| :doc:`GPU Package <Speed_gpu>`          | for GPUs via CUDA, OpenCL, or ROCm HIP                |
doc/src/Speed_packages.rst:| :doc:`KOKKOS Package <Speed_kokkos>`    | for NVIDIA GPUs, Intel Xeon Phi, and OpenMP threading |
doc/src/Speed_packages.rst:   Speed_gpu
doc/src/Speed_packages.rst:| GPUs            | :doc:`GPU <Speed_gpu>`, :doc:`KOKKOS <Speed_kokkos>` packages                                                               |
doc/src/Speed_packages.rst:* :doc:`pair_style lj/cut/gpu <pair_lj>`
doc/src/Speed_packages.rst:| build the accelerator library                             | only for GPU package                        |
doc/src/Speed_packages.rst:| use accelerated styles in your input via ``-sf``          | ``lmp_machine -in in.script -sf gpu``       |
doc/src/Speed_packages.rst:   Phi option for the INTEL package.  Or the OpenMP, CUDA, HIP, SYCL,
doc/src/Speed_packages.rst:   or Phi option for the KOKKOS package.  Or the OpenCL, HIP, or CUDA
doc/src/Speed_packages.rst:   option for the GPU package.
doc/src/Speed_packages.rst:* the INTEL Phi or Kokkos Phi option, and the GPU package
doc/src/Speed_packages.rst:* Styles with a "gpu" suffix are part of the GPU package and can be run
doc/src/Speed_packages.rst:  on Intel, NVIDIA, or AMD GPUs.  The speed-up on a GPU depends on a
doc/src/Speed_packages.rst:  run using OpenMP on multicore CPUs, on an NVIDIA or AMD GPU, or on an
doc/src/pair_lj_cut_coul.rst:.. index:: pair_style lj/cut/coul/cut/gpu
doc/src/pair_lj_cut_coul.rst:.. index:: pair_style lj/cut/coul/debye/gpu
doc/src/pair_lj_cut_coul.rst:.. index:: pair_style lj/cut/coul/dsf/gpu
doc/src/pair_lj_cut_coul.rst:.. index:: pair_style lj/cut/coul/long/gpu
doc/src/pair_lj_cut_coul.rst:.. index:: pair_style lj/cut/coul/msm/gpu
doc/src/pair_lj_cut_coul.rst:Accelerator Variants: *lj/cut/coul/cut/gpu*, *lj/cut/coul/cut/kk*, *lj/cut/coul/cut/omp*
doc/src/pair_lj_cut_coul.rst:Accelerator Variants: *lj/cut/coul/debye/gpu*, *lj/cut/coul/debye/kk*, *lj/cut/coul/debye/omp*
doc/src/pair_lj_cut_coul.rst:Accelerator Variants: *lj/cut/coul/dsf/gpu*, *lj/cut/coul/dsf/kk*, *lj/cut/coul/dsf/omp*
doc/src/pair_lj_cut_coul.rst:Accelerator Variants: *lj/cut/coul/long/gpu*, *lj/cut/coul/long/kk*, *lj/cut/coul/long/intel*, *lj/cut/coul/long/opt*, *lj/cut/coul/long/omp*
doc/src/pair_lj_cut_coul.rst:Accelerator Variants: *lj/cut/coul/msm/gpu*, *lj/cut/coul/msm/omp*
doc/src/atom_modify.rst:   When running simple pair-wise potentials like Lennard Jones on GPUs
doc/src/Developer_par_openmp.rst:or GPUs.
doc/src/Developer_par_openmp.rst:why option 2 for example is used in the GPU package because a GPU is a
doc/src/Commands_fix.rst:parenthesis: g = GPU, i = INTEL, k = KOKKOS, o = OPENMP, t =
doc/src/pair_tersoff_zbl.rst:.. index:: pair_style tersoff/zbl/gpu
doc/src/pair_tersoff_zbl.rst:Accelerator Variants: *tersoff/zbl/gpu*, *tersoff/zbl/kk*, *tersoff/zbl/omp*
doc/src/pair_tersoff_zbl.rst:The *shift* keyword is currently not supported for the *tersoff/gpu* and
doc/src/Intro_portability.rst:Windows.  Also, the Nvidia HPC SDK (formerly PGI compilers) will compile
doc/src/pair_sph_heatconduction.rst:.. index:: pair_style sph/heatconduction/gpu
doc/src/pair_sph_heatconduction.rst:Accelerator Variants: *sph/heatconduction/gpu*
doc/src/Build_windows.rst:compatible with this cross-compilation build is provided.  The GPU
doc/src/Build_windows.rst:package can only be compiled with OpenCL support.  To compile with MPI
doc/src/pair_buck.rst:.. index:: pair_style buck/gpu
doc/src/pair_buck.rst:.. index:: pair_style buck/coul/cut/gpu
doc/src/pair_buck.rst:.. index:: pair_style buck/coul/long/gpu
doc/src/pair_buck.rst:Accelerator Variants: *buck/gpu*, *buck/intel*, *buck/kk*, *buck/omp*
doc/src/pair_buck.rst:Accelerator Variants: *buck/coul/cut/gpu*, *buck/coul/cut/intel*, *buck/coul/cut/kk*, *buck/coul/cut/omp*
doc/src/pair_buck.rst:Accelerator Variants: *buck/coul/long/gpu*, *buck/coul/long/intel*, *buck/coul/long/kk*, *buck/coul/long/omp*
doc/src/pair_tersoff_mod.rst:.. index:: pair_style tersoff/mod/gpu
doc/src/pair_tersoff_mod.rst:Accelerator Variants: *tersoff/mod/gpu*, *tersoff/mod/kk*, *tersoff/mod/omp*
doc/src/pair_tersoff_mod.rst:The *shift* keyword is not supported by the *tersoff/gpu*,
doc/src/compute.rst:LAMMPS distribution for faster performance on CPUs, GPUs, and KNLs.
doc/src/Build_settings.rst:                                   # or HIPFFT or MKL_GPU, default is KISS
doc/src/Build_settings.rst:         Kokkos back end - for example, when using the CUDA back end of Kokkos,
doc/src/Build_settings.rst:                                       # HIPFFT, or MKL_GPU
doc/src/Build_settings.rst:         # MKL_GPU either precision
doc/src/Build_settings.rst:The NVIDIA Performance Libraries (NVPL) FFT library is optimized for NVIDIA
doc/src/Build_settings.rst:Grace Armv9.0 architecture. You can download it from https://docs.nvidia.com/nvpl/
doc/src/Build_settings.rst:The cuFFT and hipFFT FFT libraries are packaged with NVIDIA's CUDA and
doc/src/Build_settings.rst:GPU-resident (i.e., HIP or CUDA). Similarly, GPU offload of FFTs on
doc/src/Build_settings.rst:Intel GPUs with oneMKL currently requires the Kokkos acceleration
doc/src/Build_settings.rst:Also note that the GPU package requires its lib/gpu library to be
doc/src/Build_settings.rst:in whichever ``lib/gpu/Makefile`` is used must be the same as above.
doc/src/pair_beck.rst:.. index:: pair_style beck/gpu
doc/src/pair_beck.rst:Accelerator Variants: *beck/gpu*, *beck/omp*
doc/src/pair_pace.rst:*pace* with the KOKKOS package on GPUs and is ignored otherwise.  This
doc/src/pair_pace.rst:two passes (running on a single GPU).
doc/src/pair_pace.rst:When using the pair style *pace/extrapolation* with the KOKKOS package on GPUs
doc/src/Speed_compare.rst:run on GPUs, optimize for vector units on CPUs and run on Intel
doc/src/Speed_compare.rst:* Both, the GPU and the KOKKOS package allows you to assign multiple
doc/src/Speed_compare.rst:  MPI ranks (= CPU cores) to the same GPU. For the GPU package, this
doc/src/Speed_compare.rst:  can lead to a speedup through better utilization of the GPU (by
doc/src/Speed_compare.rst:  computation of the non-GPU accelerated parts of LAMMPS through MPI
doc/src/Speed_compare.rst:  data on the GPU.
doc/src/Speed_compare.rst:* The GPU package moves per-atom data (coordinates, forces, and
doc/src/Speed_compare.rst:  (optionally) neighbor list data, if not computed on the GPU) between
doc/src/Speed_compare.rst:  the CPU and GPU at every timestep.  The KOKKOS/CUDA package only does
doc/src/Speed_compare.rst:  a fix or compute that is non-GPU-ized). Hence, if you can formulate
doc/src/Speed_compare.rst:  your input script to only use GPU-ized fixes and computes, and avoid
doc/src/Speed_compare.rst:  then the data transfer cost of the KOKKOS/CUDA package can be very low,
doc/src/Speed_compare.rst:  causing it to run faster than the GPU package.
doc/src/Speed_compare.rst:* The GPU package is often faster than the KOKKOS/CUDA package, when the
doc/src/Speed_compare.rst:  number of atoms per GPU is on the smaller side.  The crossover point,
doc/src/Speed_compare.rst:  in terms of atoms/GPU at which the KOKKOS/CUDA package becomes faster
doc/src/Speed_compare.rst:  atoms per GPU.  When performing double precision calculations the
doc/src/Speed_compare.rst:* Both KOKKOS and GPU package compute bonded interactions (bonds, angles,
doc/src/Speed_compare.rst:  etc) on the CPU.  If the GPU package is running with several MPI processes
doc/src/Speed_compare.rst:  assigned to one GPU, the cost of computing the bonded interactions is
doc/src/Speed_compare.rst:  spread across more CPUs and hence the GPU package can run faster in these
doc/src/Speed_compare.rst:* When using LAMMPS with multiple MPI ranks assigned to the same GPU, its
doc/src/Speed_compare.rst:  the CPUs and the GPU. This can differ significantly based on the
doc/src/Speed_compare.rst:  number of available bus slots, or if GPUs are housed in an external
doc/src/Speed_compare.rst:* To achieve significant acceleration through GPUs, both KOKKOS and GPU
doc/src/Speed_compare.rst:  package require capable GPUs with fast on-device memory and efficient
doc/src/Speed_compare.rst:  (desktop) GPUs. Using lower performance GPUs (e.g. on laptops) may
doc/src/Speed_compare.rst:* For the GPU package, specifically when running in parallel with MPI,
doc/src/Speed_compare.rst:  if it often more efficient to exclude the PPPM kspace style from GPU
doc/src/Speed_compare.rst:  acceleration and instead run it - concurrently with a GPU accelerated
doc/src/Speed_compare.rst:**Differences between the GPU and KOKKOS packages:**
doc/src/Speed_compare.rst:* The GPU package accelerates only pair force, neighbor list, and (parts
doc/src/Speed_compare.rst:  calculation on the GPU, but can transparently support non-accelerated
doc/src/Speed_compare.rst:  host and GPU).
doc/src/Speed_compare.rst:* The GPU package requires neighbor lists to be built on the CPU when using
doc/src/Speed_compare.rst:* The GPU package can be compiled for CUDA or OpenCL and thus supports
doc/src/Speed_compare.rst:  both, NVIDIA and AMD GPUs well. On NVIDIA hardware, using CUDA is typically
doc/src/Speed_compare.rst:  resulting in equal or better performance over OpenCL.
doc/src/Speed_compare.rst:* OpenCL in the GPU package does theoretically also support Intel CPUs or
doc/src/info.rst:settings of included accelerator support for the GPU, KOKKOS, INTEL,
doc/src/Speed_measure.rst:dominating, you may want to look at GPU or OMP versions of the pair
doc/src/Packages_list.rst:   * - :ref:`GPU <PKG-GPU>`
doc/src/Packages_list.rst:     - GPU-enabled styles
doc/src/Packages_list.rst:     - :doc:`Section gpu <Speed_gpu>`
doc/src/fix_nh.rst:.. index:: fix nvt/gpu
doc/src/fix_nh.rst:.. index:: fix npt/gpu
doc/src/fix_nh.rst:Accelerator Variants: *nvt/gpu*, *nvt/intel*, *nvt/kk*, *nvt/omp*
doc/src/fix_nh.rst:Accelerator Variants: *npt/gpu*, *npt/intel*, *npt/kk*, *npt/omp*
doc/src/pair_coul_slater.rst:.. index:: pair_style coul/slater/long/gpu
doc/src/pair_coul_slater.rst:Accelerator Variants: *coul/slater/long/gpu*
doc/src/Run_options.rst:the GPU or OPENMP packages, for testing or benchmarking purposes.
doc/src/Run_options.rst:* g or gpus
doc/src/Run_options.rst:This option is only relevant if you built LAMMPS with CUDA=yes, you
doc/src/Run_options.rst:have more than one GPU per node, and if you are running with only one
doc/src/Run_options.rst:MPI task per node.  The Nd setting is the ID of the GPU on the node to
doc/src/Run_options.rst:run on.  By default Nd = 0.  If you have multiple GPUs per node, they
doc/src/Run_options.rst:MPI task per node, and assign each job to run on a different GPU.
doc/src/Run_options.rst:   gpus Ng Ns
doc/src/Run_options.rst:This option is only relevant if you built LAMMPS with CUDA=yes, you
doc/src/Run_options.rst:have more than one GPU per node, and you are running with multiple MPI
doc/src/Run_options.rst:tasks per node (up to one per GPU).  The Ng setting is how many GPUs
doc/src/Run_options.rst:GPU to skip when assigning MPI tasks to GPUs.  This may be useful if
doc/src/Run_options.rst:your desktop system reserves one GPU to drive the screen and the rest
doc/src/Run_options.rst:assign a unique GPU ID to the MPI task.
doc/src/Run_options.rst:LAMMPS is compiled with CUDA=yes.
doc/src/Run_options.rst:script.  For example ``-package gpu 2`` or ``-pk gpu 2`` is the same as
doc/src/Run_options.rst::doc:`package gpu 2 <package>` in the input script.  The possible styles
doc/src/Run_options.rst:be *gpu*, *intel*, *kk*, *omp*, *opt*, or *hybrid*\ .  These
doc/src/Run_options.rst:in :doc:`Accelerate performance <Speed>`.  The "gpu" style corresponds to the
doc/src/Run_options.rst:GPU package, the "intel" style to the INTEL package, the "kk"
doc/src/Run_options.rst:As an example, all of the packages provide a :doc:`pair_style lj/cut <pair_lj>` variant, with style names lj/cut/gpu,
doc/src/Run_options.rst:lj/cut/gpu.  If the -suffix switch is used the specified suffix
doc/src/Run_options.rst:(gpu,intel,kk,omp,opt) is automatically appended whenever your input
doc/src/Run_options.rst:For the GPU package, using this command-line switch also invokes the
doc/src/Run_options.rst:default GPU settings, as if the command "package gpu 1" were used at
doc/src/Run_options.rst:the ``-package gpu`` command-line switch or the :doc:`package gpu <package>` command in your script.
doc/src/pair_tersoff.rst:.. index:: pair_style tersoff/gpu
doc/src/pair_tersoff.rst:Accelerator Variants: *tersoff/gpu*, *tersoff/intel*, *tersoff/kk*, *tersoff/omp*
doc/src/pair_tersoff.rst:The *shift* keyword is not supported by the *tersoff/gpu*,
doc/src/pair_gromacs.rst:.. index:: pair_style lj/gromacs/gpu
doc/src/pair_gromacs.rst:Accelerator Variants: *lj/gromacs/gpu*, *lj/gromacs/kk*, *lj/gromacs/omp*
doc/src/pair_vashishta.rst:.. index:: pair_style vashishta/gpu
doc/src/pair_vashishta.rst:Accelerator Variants: *vashishta/gpu*, *vashishta/omp*, *vashishta/kk*
doc/src/pair_lj_expand.rst:.. index:: pair_style lj/expand/gpu
doc/src/pair_lj_expand.rst:.. index:: pair_style lj/expand/coul/long/gpu
doc/src/pair_lj_expand.rst:Accelerator Variants: *lj/expand/gpu*, *lj/expand/kk*, *lj/expand/omp*
doc/src/pair_lj_expand.rst:Accelerator Variants: *lj/expand/coul/long/gpu*, *lj/expand/coul/long/kk*
doc/src/pair_table.rst:.. index:: pair_style table/gpu
doc/src/pair_table.rst:Accelerator Variants: *table/gpu*, *table/kk*, *table/omp*
doc/src/Tools.rst:- Support for GPU, INTEL, KOKKOS/OpenMP, OPENMAP, and OPT and accelerator packages
doc/src/Tools.rst:the LAMMPS-GUI in parallel with MPI, but OpenMP multi-threading and GPU
doc/src/package.rst:* style = *gpu* or *intel* or *kokkos* or *omp*
doc/src/package.rst:       *gpu* args = Ngpu keyword value ...
doc/src/package.rst:         Ngpu = # of GPUs per node
doc/src/package.rst:         keywords = *neigh* or *newton* or *pair/only* or *binsize* or *split* or *gpuID* or *tpa* or *blocksize* or *omp* or *platform* or *device_type* or *ocl_args*
doc/src/package.rst:             *yes* = neighbor list build on GPU (default)
doc/src/package.rst:             *off* = apply "gpu" suffix to all available styles in the GPU package (default)
doc/src/package.rst:             *on* = apply "gpu" suffix only pair styles
doc/src/package.rst:             fraction = fraction of atoms assigned to GPU (default = 1.0)
doc/src/package.rst:             Nlanes = # of GPU vector lanes (CUDA threads) used per atom
doc/src/package.rst:             id = For OpenCL, platform ID for the GPU or accelerator
doc/src/package.rst:           *gpuID* values = id
doc/src/package.rst:             id = ID of first GPU to be used on each node
doc/src/package.rst:           *device_type* value = *intelgpu* or *nvidiagpu* or *amdgpu* or *applegpu* or *generic* or *custom*,val1,val2,...
doc/src/package.rst:             val1,val2,... = custom OpenCL accelerator configuration parameters (see below for details)
doc/src/package.rst:             args = List of additional OpenCL compiler arguments delimited by colons
doc/src/package.rst:         keywords = *neigh* or *neigh/qeq* or *neigh/thread* or *neigh/transpose* or *newton* or *binsize* or *comm* or *comm/exchange* or *comm/forward* or *comm/pair/forward* or *comm/fix/forward* or *comm/reverse* or *comm/pair/reverse* or *sort* or *atom/map* or *gpu/aware* or *pair/only*
doc/src/package.rst:             *off* = use same memory layout for GPU neigh list build as pair style
doc/src/package.rst:             *on* = use transposed memory layout for GPU neigh list build
doc/src/package.rst:             *device* = perform pack/unpack on device (e.g. on GPU)
doc/src/package.rst:             *device* = perform pack/unpack on device (e.g. on GPU)
doc/src/package.rst:             *device* = perform atom sorting on device (e.g. on GPU)
doc/src/package.rst:             *device* = build atom map on device (e.g. on GPU)
doc/src/package.rst:           *gpu/aware* = *off* or *on*
doc/src/package.rst:             *off* = do not use GPU-aware MPI
doc/src/package.rst:             *on* = use GPU-aware MPI (default)
doc/src/package.rst:             *off* = use device acceleration (e.g. GPU) for all available styles in the KOKKOS package (default)
doc/src/package.rst:   package gpu 0
doc/src/package.rst:   package gpu 1 split 0.75
doc/src/package.rst:   package gpu 2 split -1.0
doc/src/package.rst:   package gpu 0 omp 2 device_type intelgpu
doc/src/package.rst:packages use settings from this command: GPU, INTEL, KOKKOS, and
doc/src/package.rst:For the GPU, INTEL, and OPENMP packages, if a "-sf gpu" or "-sf
doc/src/package.rst:script, then those switches also invoke a "package gpu", "package
doc/src/package.rst:The *gpu* style invokes settings associated with the use of the GPU
doc/src/package.rst:The *Ngpu* argument sets the number of GPUs per node. If *Ngpu* is 0
doc/src/package.rst:and no other keywords are specified, GPU or accelerator devices are
doc/src/package.rst:accelerator devices and GPUs are chosen if available. The device with
doc/src/package.rst:the additional devices will be unused. The auto-selection of GPUs/
doc/src/package.rst:a non-zero value for *Ngpu* and / or using the *gpuID*, *platform*,
doc/src/package.rst:tasks (per node) than GPUs, multiple MPI tasks will share each GPU.
doc/src/package.rst:neighbor list building is performed on the GPU.  If *neigh* is *no*,
doc/src/package.rst:neighbor list building is performed on the CPU.  GPU neighbor list
doc/src/package.rst:building currently cannot be used with a triclinic box.  GPU neighbor
doc/src/package.rst:lists are not compatible with commands that are not GPU-enabled.  When
doc/src/package.rst:a non-GPU enabled command requires a neighbor list, it will also be
doc/src/package.rst:the GPU package pair styles require this setting.  This means more
doc/src/package.rst:The *pair/only* keyword can change how any "gpu" suffix is applied.
doc/src/package.rst:neighbor list builds performed on the GPU, if *neigh* = *yes* is set.
doc/src/package.rst:automatically using heuristics in the GPU package.
doc/src/package.rst:between CPU and GPU cores in GPU-enabled pair styles. If 0 < *split* <
doc/src/package.rst:1.0, a fixed fraction of particles is offloaded to the GPU while force
doc/src/package.rst:If *split* < 0.0, the optimal fraction (based on CPU and GPU timings)
doc/src/package.rst:the CPU and GPU is performed.  If *split* = 1.0, all force
doc/src/package.rst:calculations for GPU accelerated pair styles are performed on the GPU.
doc/src/package.rst:CPU while the GPU is performing force calculations for the GPU-enabled
doc/src/package.rst:pair style.  If all CPU force computations complete before the GPU
doc/src/package.rst:completes, LAMMPS will block until the GPU has finished before
doc/src/package.rst:As an example, if you have two GPUs per node and 8 CPU cores per node,
doc/src/package.rst:force calculation across CPU and GPU cores, you could specify
doc/src/package.rst:   mpirun -np 32 -sf gpu -in in.script    # launch command
doc/src/package.rst:   package gpu 2 split -1                 # input script command
doc/src/package.rst:In this case, all CPU cores and GPU devices on the nodes would be
doc/src/package.rst:utilized.  Each GPU device would be shared by 4 CPU cores. The CPU
doc/src/package.rst:particles at the same time the GPUs performed force calculation for
doc/src/package.rst:The *gpuID* keyword is used to specify the first ID for the GPU or
doc/src/package.rst:1 and *Ngpu* is 3, GPUs 1-3 will be used. Device IDs should be
doc/src/package.rst:as provided in the lib/gpu directory. When using OpenCL with
doc/src/package.rst:The *tpa* keyword sets the number of GPU vector lanes per atom used to
doc/src/package.rst:large cutoffs or with a small number of particles per GPU, increasing
doc/src/package.rst:for the GPU / accelerator. In the case it exceeds the SIMD width, it
doc/src/package.rst:per thread block. This number should be a multiple of 32 (for GPUs)
doc/src/package.rst:and its maximum depends on the specific GPU hardware. Typical choices
doc/src/package.rst:individual GPU cores, but reduces the total number of thread blocks,
doc/src/package.rst:The meaning of *Nthreads* is exactly the same for the GPU, INTEL,
doc/src/package.rst:and GPU packages.
doc/src/package.rst:The *platform* keyword is only used with OpenCL to specify the ID for
doc/src/package.rst:an OpenCL platform. See the output from ocl_get_devices in the lib/gpu
doc/src/package.rst:default (id=-1) the platform is auto-selected to find the GPU with the
doc/src/package.rst:most compute cores. When *Ngpu* or other keywords are specified, the
doc/src/package.rst:auto-selection is appropriately restricted. For example, if *Ngpu* is
doc/src/package.rst:restrictions can be enforced by the *gpuID* and *device_type* keywords.
doc/src/package.rst:The *device_type* keyword can be used for OpenCL to specify the type of
doc/src/package.rst:GPU to use or specify a custom configuration for an accelerator. In most
doc/src/package.rst:keyword. The *applegpu* type is not specific to a particular GPU vendor,
doc/src/package.rst:but is separate due to the more restrictive Apple OpenCL implementation.
doc/src/package.rst:(NVIDIA) or OpenCL extensions (Intel) should be used for horizontal
doc/src/package.rst:vector operations. FAST_MATH in {0,1} indicates that OpenCL fast math
doc/src/package.rst:For OpenCL, the routines are compiled at runtime for the specified GPU
doc/src/package.rst:The meaning of *Nthreads* is exactly the same for the GPU, INTEL,
doc/src/package.rst:and GPU packages.
doc/src/package.rst:CPUs (i.e. the Kokkos CUDA back end is not enabled).
doc/src/package.rst:running on GPUs. This performs twice as much computation as the *half*
doc/src/package.rst:that reason, *full* is the default setting for GPUs. However, when
doc/src/package.rst:enough parallelism to keep a GPU busy. When this keyword is set to *on*,
doc/src/package.rst:more GPUs and there are 16k atoms or less owned by an MPI rank. Not all
doc/src/package.rst:GPUs as used for the pair style. When this keyword is set to *on* it
doc/src/package.rst:list on GPUs. This can be faster in some cases (e.g. ReaxFF HNS
doc/src/package.rst:command allows. The default for GPUs is *off* because this will almost
doc/src/package.rst:distance. This is fine when neighbor lists are built on the CPU. For GPU
doc/src/package.rst:because the GPU is faster at performing pairwise interactions, then this
doc/src/package.rst:use the device, typically a GPU, to perform the packing/unpacking
doc/src/package.rst:identically. When using GPUs, the *device* value is the default since it
doc/src/package.rst:on the GPU for many timesteps without being moved between the host and
doc/src/package.rst:GPU, if you use the *device* value. If your script uses styles (e.g.
doc/src/package.rst:pack/unpack communicated data. When running small systems on a GPU,
doc/src/package.rst:since it reduces the number of CUDA kernel launches.
doc/src/package.rst:The *gpu/aware* keyword chooses whether GPU-aware MPI will be used. When
doc/src/package.rst:this keyword is set to *on*, buffers in GPU memory are passed directly
doc/src/package.rst:the data to the host CPU. However GPU-aware MPI is not supported on all
doc/src/package.rst:value of *off*\ . If LAMMPS can safely detect that GPU-aware MPI is not
doc/src/package.rst:the *gpu/aware* keyword is automatically set to *off* by default. When
doc/src/package.rst:the *gpu/aware* keyword is set to *off* while any of the *comm*
doc/src/package.rst:running on GPUs or if using only one MPI rank. GPU-aware MPI is available
doc/src/package.rst:"MV2_USE_CUDA" environment variable is set to "1", CrayMPI, and IBM
doc/src/package.rst:Spectrum MPI when the "-gpu" flag is used.
doc/src/package.rst:   If you build LAMMPS with the GPU, INTEL, and / or OPENMP
doc/src/package.rst:   for a hybrid suffix for gpu and omp. Note that KOKKOS also supports
doc/src/package.rst:The *gpu* style of this command can only be invoked if LAMMPS was built
doc/src/package.rst:with the GPU package.  See the :doc:`Build package <Build_package>` doc
doc/src/package.rst:For the GPU package, the default parameters and settings are:
doc/src/package.rst:   Ngpu = 0, neigh = yes, newton = off, binsize = 0.0, split = 1.0, gpuID = 0 to Ngpu-1, tpa = 1, omp = 0, platform=-1.
doc/src/package.rst:These settings are made automatically if the "-sf gpu"
doc/src/package.rst:you must invoke the package gpu command in your input script or via the
doc/src/package.rst:"-pk gpu" :doc:`command-line switch <Run_options>`.
doc/src/package.rst:For the KOKKOS package when using GPUs, the option defaults are:
doc/src/package.rst:   neigh = full, neigh/qeq = full, newton = off, binsize = 2x LAMMPS default value, comm = device, sort = device, atom/map = device, neigh/transpose = off, gpu/aware = on
doc/src/package.rst:For GPUs, option neigh/thread = on when there are 16k atoms or less on
doc/src/package.rst:GPU-aware MPI is not available, the default value of gpu/aware becomes
doc/src/pair_sph_taitwater.rst:.. index:: pair_style sph/taitwater/gpu
doc/src/pair_sph_taitwater.rst:Accelerator Variants: *sph/taitwater/gpu*
doc/src/Errors_warnings.rst:*Increasing communication cutoff for GPU style*
doc/src/Errors_warnings.rst:   the communication cutoff requirements for this pair style when run on the GPU.
doc/src/Errors_warnings.rst:*Using package gpu without any pair style defined*
doc/src/improper_style.rst:LAMMPS distribution for faster performance on CPUs, GPUs, and KNLs.
doc/src/lepton_expression.rst:additional speed or GPU acceleration (via GPU or KOKKOS) is required,
doc/src/Intro_features.rst:* particle decomposition inside spatial decomposition for OpenMP and GPU parallelism
doc/src/Intro_features.rst:* GPU (CUDA, OpenCL, HIP, SYCL), Intel Xeon Phi, and OpenMP support for many code features
doc/src/Commands_pair.rst:parenthesis: g = GPU, i = INTEL, k = KOKKOS, o = OPENMP, t =
doc/src/pair_mesodpd.rst:.. index:: pair_style edpd/gpu
doc/src/pair_mesodpd.rst:.. index:: pair_style mdpd/gpu
doc/src/pair_mesodpd.rst:Accelerator Variants: *edpd/gpu*
doc/src/pair_mesodpd.rst:Accelerator Variants: *mdpd/gpu*
doc/src/pair_eam.rst:.. index:: pair_style eam/gpu
doc/src/pair_eam.rst:.. index:: pair_style eam/alloy/gpu
doc/src/pair_eam.rst:.. index:: pair_style eam/fs/gpu
doc/src/pair_eam.rst:Accelerator Variants: *eam/gpu*, *eam/intel*, *eam/kk*, *eam/omp*, *eam/opt*
doc/src/pair_eam.rst:Accelerator Variants: *eam/alloy/gpu*, *eam/alloy/intel*, *eam/alloy/kk*, *eam/alloy/omp*, *eam/alloy/opt*
doc/src/pair_eam.rst:Accelerator Variants: *eam/fs/gpu*, *eam/fs/intel*, *eam/fs/kk*, *eam/fs/omp*, *eam/fs/opt*
doc/src/suffix.rst:* style = *off* or *on* or *gpu* or *intel* or *kk* or *omp* or *opt* or *hybrid*
doc/src/suffix.rst:   suffix gpu
doc/src/suffix.rst:The specified style can be *gpu*, *intel*, *kk*, *omp*, *opt* or
doc/src/suffix.rst:The "gpu" style corresponds to the GPU package, the "intel" style to
doc/src/suffix.rst:* GPU = a handful of pair styles and the PPPM kspace_style, optimized to
doc/src/suffix.rst:  run on one or more GPUs or multicore CPU/GPU nodes
doc/src/suffix.rst:  using the Kokkos library on various kinds of hardware, including GPUs
doc/src/suffix.rst:  via CUDA and many-core chips via OpenMP or threading.
doc/src/suffix.rst:lj/cut/gpu, lj/cut/intel, or lj/cut/kk.  A variant styles
doc/src/suffix.rst:lj/cut/gpu. If the suffix command is used with the appropriate style,
doc/src/suffix.rst:(opt,omp,gpu,intel,kk) is automatically appended whenever your
doc/src/pair_colloid.rst:.. index:: pair_style colloid/gpu
doc/src/pair_colloid.rst:Accelerator Variants: *colloid/gpu*, *colloid/omp*
doc/src/Developer_org.rst:code on request.  Each subdirectory, like ``lib/poems`` or ``lib/gpu``,
doc/src/Developer_org.rst:as Fortran or CUDA. These libraries included in the LAMMPS build, if the
doc/src/Intro_authors.rst:     - soft matter, GPU package, DIELECTRIC package, regression testing
doc/src/Intro_authors.rst:* Mike Brown (Intel), GPU and INTEL packages
doc/src/Intro_authors.rst:* Trung Ngyuen (U Chicago), GPU, RIGID, BODY, and DIELECTRIC packages
doc/src/Intro_authors.rst:* Christian Trott (Sandia), CUDA and KOKKOS packages
doc/src/Fortran.rst:   :f has_gpu_device: :f:func:`has_gpu_device`
doc/src/Fortran.rst:   :ftype has_gpu_device: function
doc/src/Fortran.rst:   :f get_gpu_device_info: :f:subr:`get_gpu_device_info`
doc/src/Fortran.rst:   :ftype get_gpu_device_info: subroutine
doc/src/Fortran.rst:   Supported packages names are "GPU", "KOKKOS", "INTEL", and "OPENMP".
doc/src/Fortran.rst:   Supported categories are "api" with possible settings "cuda", "hip", "phi",
doc/src/Fortran.rst:   "pthreads", "opencl", "openmp", and "serial"; and "precision" with
doc/src/Fortran.rst:.. f:function:: has_gpu_device()
doc/src/Fortran.rst:   Checks for the presence of a viable GPU package device.
doc/src/Fortran.rst:   This function calls :cpp:func:`lammps_has_gpu_device`, which checks at
doc/src/Fortran.rst:   :doc:`GPU package <Speed_gpu>`.
doc/src/Fortran.rst:   :f:subr:`get_gpu_device_info` subroutine.
doc/src/Fortran.rst:   :to: :cpp:func:`lammps_has_gpu_device`
doc/src/Fortran.rst:.. f:subroutine:: get_gpu_device_info(buffer)
doc/src/Fortran.rst:   Get GPU package device information.
doc/src/Fortran.rst:   Calls :cpp:func:`lammps_get_gpu_device_info` to retrieve detailed
doc/src/Fortran.rst:   :doc:`GPU package <Speed_gpu>`. It will fill *buffer* with a string that is
doc/src/Fortran.rst:   ``hip_get_device`` tools that are compiled alongside LAMMPS if the GPU
doc/src/Fortran.rst:   :to: :cpp:func:`lammps_get_gpu_device_info`
doc/src/Build_extras.rst:   * :ref:`GPU <gpu>`
doc/src/Build_extras.rst:.. _gpu:
doc/src/Build_extras.rst:GPU package
doc/src/Build_extras.rst:which GPU hardware to build for. The GPU package currently supports
doc/src/Build_extras.rst:three different types of back ends: OpenCL, CUDA and HIP.
doc/src/Build_extras.rst:   -D GPU_API=value             # value = opencl (default) or cuda or hip
doc/src/Build_extras.rst:   -D GPU_PREC=value            # precision setting
doc/src/Build_extras.rst:   -D GPU_ARCH=value            # primary GPU hardware choice for GPU_API=cuda
doc/src/Build_extras.rst:   -D GPU_DEBUG=value           # enable debug code in the GPU package library,
doc/src/Build_extras.rst:                                # GPU_API=HIP
doc/src/Build_extras.rst:   -D HIP_ARCH=value            # primary GPU hardware choice for GPU_API=hip
doc/src/Build_extras.rst:   -D HIP_USE_DEVICE_SORT=value # enables GPU sorting
doc/src/Build_extras.rst:   -D CUDPP_OPT=value           # use GPU binning with CUDA (should be off for modern GPUs)
doc/src/Build_extras.rst:                                # enables CUDA Performance Primitives, must be "no" for
doc/src/Build_extras.rst:                                # CUDA_MPS_SUPPORT=yes
doc/src/Build_extras.rst:   -D CUDA_MPS_SUPPORT=value    # enables some tweaks required to run with active
doc/src/Build_extras.rst:                                # nvidia-cuda-mps daemon
doc/src/Build_extras.rst:   -D CUDA_BUILD_MULTIARCH=value  # enables building CUDA kernels for all supported GPU
doc/src/Build_extras.rst:   -D USE_STATIC_OPENCL_LOADER=value  # downloads/includes OpenCL ICD loader library,
doc/src/Build_extras.rst:                                      # no local OpenCL headers/libs needed
doc/src/Build_extras.rst:``GPU_ARCH`` settings for different GPU hardware is as follows:
doc/src/Build_extras.rst:* ``sm_30`` for Kepler (supported since CUDA 5 and until CUDA 10.x)
doc/src/Build_extras.rst:* ``sm_35`` or ``sm_37`` for Kepler (supported since CUDA 5 and until CUDA 11.x)
doc/src/Build_extras.rst:* ``sm_50`` or ``sm_52`` for Maxwell (supported since CUDA 6)
doc/src/Build_extras.rst:* ``sm_60`` or ``sm_61`` for Pascal (supported since CUDA 8)
doc/src/Build_extras.rst:* ``sm_70`` for Volta (supported since CUDA 9)
doc/src/Build_extras.rst:* ``sm_75`` for Turing (supported since CUDA 10)
doc/src/Build_extras.rst:* ``sm_80`` or sm_86 for Ampere (supported since CUDA 11, sm_86 since CUDA 11.1)
doc/src/Build_extras.rst:* ``sm_89`` for Lovelace (supported since CUDA 11.8)
doc/src/Build_extras.rst:* ``sm_90`` for Hopper (supported since CUDA 12.0)
doc/src/Build_extras.rst:at `Wikipedia's CUDA article <https://en.wikipedia.org/wiki/CUDA#GPUs_supported>`_
doc/src/Build_extras.rst:CMake can detect which version of the CUDA toolkit is used and thus will
doc/src/Build_extras.rst:try to include support for **all** major GPU architectures supported by
doc/src/Build_extras.rst:this toolkit.  Thus the ``GPU_ARCH`` setting is merely an optimization, to
doc/src/Build_extras.rst:have code for the preferred GPU architecture directly included rather
doc/src/Build_extras.rst:than having to wait for the JIT compiler of the CUDA driver to translate
doc/src/Build_extras.rst:setting ``CUDA_ENABLE_MULTIARCH`` to ``no``.
doc/src/Build_extras.rst:When compiling for CUDA or HIP with CUDA, version 8.0 or later of the
doc/src/Build_extras.rst:CUDA toolkit is required and a GPU architecture of Kepler or later,
doc/src/Build_extras.rst:which must *also* be supported by the CUDA toolkit in use **and** the
doc/src/Build_extras.rst:CUDA driver in use.  When compiling for OpenCL, OpenCL version 1.2 or
doc/src/Build_extras.rst:later is required and the GPU must be supported by the GPU driver and
doc/src/Build_extras.rst:OpenCL runtime bundled with the driver.
doc/src/Build_extras.rst:When building with CMake, you **must NOT** build the GPU library in
doc/src/Build_extras.rst:``lib/gpu`` using the traditional build procedure. CMake will detect
doc/src/Build_extras.rst:If you are compiling for OpenCL, the default setting is to download,
doc/src/Build_extras.rst:build, and link with a static OpenCL ICD loader library and standard
doc/src/Build_extras.rst:OpenCL headers.  This way no local OpenCL development headers or library
doc/src/Build_extras.rst:needs to be present and only OpenCL compatible drivers need to be
doc/src/Build_extras.rst:installed to use OpenCL.  If this is not desired, you can set
doc/src/Build_extras.rst:``USE_STATIC_OPENCL_LOADER`` to ``no``.
doc/src/Build_extras.rst:The GPU library has some multi-thread support using OpenMP.  If LAMMPS
doc/src/Build_extras.rst:``HCC_AMDGPU_TARGET`` (for ROCm <= 4.0) or ``CUDA_PATH`` are
doc/src/Build_extras.rst:run HIP code on Intel GPUs via the OpenCL or Level Zero back ends. To use
doc/src/Build_extras.rst:the use of HIP for Intel GPUs is experimental. You should only use this
doc/src/Build_extras.rst:   # AMDGPU target (ROCm <= 4.0)
doc/src/Build_extras.rst:   export HCC_AMDGPU_TARGET=gfx906
doc/src/Build_extras.rst:   cmake -D PKG_GPU=on -D GPU_API=HIP -D HIP_ARCH=gfx906 -D CMAKE_CXX_COMPILER=hipcc ..
doc/src/Build_extras.rst:   # AMDGPU target (ROCm >= 4.1)
doc/src/Build_extras.rst:   cmake -D PKG_GPU=on -D GPU_API=HIP -D HIP_ARCH=gfx906 -D CMAKE_CXX_COMPILER=hipcc ..
doc/src/Build_extras.rst:   # CUDA target (not recommended, use GPU_ARCH=cuda)
doc/src/Build_extras.rst:   export CUDA_PATH=/usr/local/cuda
doc/src/Build_extras.rst:   cmake -D PKG_GPU=on -D GPU_API=HIP -D HIP_ARCH=sm_70 ..
doc/src/Build_extras.rst:   # SPIR-V target (Intel GPUs)
doc/src/Build_extras.rst:   cmake -D PKG_GPU=on -D GPU_API=HIP ..
doc/src/Build_extras.rst:Before building LAMMPS, you must build the GPU library in ``lib/gpu``\ .
doc/src/Build_extras.rst:``lib/gpu/README``.  Note that the GPU library uses MPI calls, so you must
doc/src/Build_extras.rst:using a command like these, which simply invokes the ``lib/gpu/Install.py``
doc/src/Build_extras.rst:  make lib-gpu
doc/src/Build_extras.rst:  # build GPU library with default Makefile.linux
doc/src/Build_extras.rst:  make lib-gpu args="-b"
doc/src/Build_extras.rst:  make lib-gpu args="-m xk7 -p single -o xk7.single"
doc/src/Build_extras.rst:  # build GPU library with mixed precision and P100 using other settings in Makefile.mpi
doc/src/Build_extras.rst:  make lib-gpu args="-m mpi -a sm_60 -p mixed -b"
doc/src/Build_extras.rst:Note that this procedure starts with a Makefile.machine in lib/gpu, as
doc/src/Build_extras.rst:* ``CUDA_HOME`` = where NVIDIA CUDA software is installed on your system
doc/src/Build_extras.rst:* ``CUDA_ARCH`` = ``sm_XX``, what GPU hardware you have, same as CMake ``GPU_ARCH`` above
doc/src/Build_extras.rst:* ``CUDA_PRECISION`` = precision (double, mixed, single)
doc/src/Build_extras.rst:The file ``Makefile.cuda`` is set up to include support for multiple
doc/src/Build_extras.rst:GPU architectures as supported by the CUDA toolkit in use. This is done
doc/src/Build_extras.rst:thus support all GPU architectures supported by your CUDA compiler.
doc/src/Build_extras.rst:To enable GPU binning via CUDA performance primitives set the Makefile variable
doc/src/Build_extras.rst:most modern GPUs.
doc/src/Build_extras.rst:To support the CUDA multiprocessor server you can set the define
doc/src/Build_extras.rst:``-DCUDA_MPS_SUPPORT``.  Please note that in this case you must **not** use
doc/src/Build_extras.rst:the CUDA performance primitives and thus set the variable ``CUDPP_OPT``
doc/src/Build_extras.rst:The GPU library has some multi-thread support using OpenMP.  You need to add
doc/src/Build_extras.rst:``lib/gpu/libgpu.a``\ , ``lib/gpu/nvc_get_devices``\ , and
doc/src/Build_extras.rst:``lib/gpu/Makefile.lammps``\ .  The latter has settings that enable LAMMPS
doc/src/Build_extras.rst:to link with CUDA libraries.  If the settings in ``Makefile.lammps`` for
doc/src/Build_extras.rst:``lib/gpu/Makefile.lammps`` may need to be edited.
doc/src/Build_extras.rst:   If you re-build the GPU library in ``lib/gpu``, you should always
doc/src/Build_extras.rst:   uninstall the GPU package in ``lammps/src``, then re-install it and
doc/src/Build_extras.rst:   re-build LAMMPS.  This is because the compilation of files in the GPU
doc/src/Build_extras.rst:   package uses the library settings from the ``lib/gpu/Makefile.machine``
doc/src/Build_extras.rst:   used to build the GPU library.
doc/src/Build_extras.rst:(e.g. a GPU).  The default setting is to have no host parallelization
doc/src/Build_extras.rst:   If you run Kokkos on a different GPU architecture than what LAMMPS
doc/src/Build_extras.rst:   while the just-in-time compiler is recompiling all GPU kernels for
doc/src/Build_extras.rst:   the new hardware.  This is, however, only supported for GPUs of the
doc/src/Build_extras.rst:      - **HOST or GPU**
doc/src/Build_extras.rst:      - GPU
doc/src/Build_extras.rst:      - NVIDIA Kepler generation CC 3.0 GPU
doc/src/Build_extras.rst:      - GPU
doc/src/Build_extras.rst:      - NVIDIA Kepler generation CC 3.2 GPU
doc/src/Build_extras.rst:      - GPU
doc/src/Build_extras.rst:      - NVIDIA Kepler generation CC 3.5 GPU
doc/src/Build_extras.rst:      - GPU
doc/src/Build_extras.rst:      - NVIDIA Kepler generation CC 3.7 GPU
doc/src/Build_extras.rst:      - GPU
doc/src/Build_extras.rst:      - NVIDIA Maxwell generation CC 5.0 GPU
doc/src/Build_extras.rst:      - GPU
doc/src/Build_extras.rst:      - NVIDIA Maxwell generation CC 5.2 GPU
doc/src/Build_extras.rst:      - GPU
doc/src/Build_extras.rst:      - NVIDIA Maxwell generation CC 5.3 GPU
doc/src/Build_extras.rst:      - GPU
doc/src/Build_extras.rst:      - NVIDIA Pascal generation CC 6.0 GPU
doc/src/Build_extras.rst:      - GPU
doc/src/Build_extras.rst:      - NVIDIA Pascal generation CC 6.1 GPU
doc/src/Build_extras.rst:      - GPU
doc/src/Build_extras.rst:      - NVIDIA Volta generation CC 7.0 GPU
doc/src/Build_extras.rst:      - GPU
doc/src/Build_extras.rst:      - NVIDIA Volta generation CC 7.2 GPU
doc/src/Build_extras.rst:      - GPU
doc/src/Build_extras.rst:      - NVIDIA Turing generation CC 7.5 GPU
doc/src/Build_extras.rst:      - GPU
doc/src/Build_extras.rst:      - NVIDIA Ampere generation CC 8.0 GPU
doc/src/Build_extras.rst:      - GPU
doc/src/Build_extras.rst:      - NVIDIA Ampere generation CC 8.6 GPU
doc/src/Build_extras.rst:      - GPU
doc/src/Build_extras.rst:      - NVIDIA Ada Lovelace generation CC 8.9 GPU
doc/src/Build_extras.rst:      - GPU
doc/src/Build_extras.rst:      - NVIDIA Hopper generation CC 9.0 GPU
doc/src/Build_extras.rst:      - GPU
doc/src/Build_extras.rst:      - AMD GPU MI50/MI60
doc/src/Build_extras.rst:      - GPU
doc/src/Build_extras.rst:      - AMD GPU MI100
doc/src/Build_extras.rst:      - GPU
doc/src/Build_extras.rst:      - AMD GPU MI200
doc/src/Build_extras.rst:      - GPU
doc/src/Build_extras.rst:      - AMD GPU MI300
doc/src/Build_extras.rst:      - GPU
doc/src/Build_extras.rst:      - AMD GPU V620/W6800
doc/src/Build_extras.rst:      - GPU
doc/src/Build_extras.rst:      - AMD GPU RX7900XTX
doc/src/Build_extras.rst:      - GPU
doc/src/Build_extras.rst:      - GPU
doc/src/Build_extras.rst:      - SPIR64-based devices, e.g. Intel GPUs, using JIT
doc/src/Build_extras.rst:      - GPU
doc/src/Build_extras.rst:      - Intel Iris XeMAX GPU
doc/src/Build_extras.rst:      - GPU
doc/src/Build_extras.rst:      - Intel GPU Gen9
doc/src/Build_extras.rst:      - GPU
doc/src/Build_extras.rst:      - Intel GPU Gen11
doc/src/Build_extras.rst:      - GPU
doc/src/Build_extras.rst:      - Intel GPU Gen12LP
doc/src/Build_extras.rst:      - GPU
doc/src/Build_extras.rst:      - Intel GPU Xe-HP
doc/src/Build_extras.rst:      - GPU
doc/src/Build_extras.rst:      - Intel GPU Ponte Vecchio
doc/src/Build_extras.rst:      For NVIDIA GPUs using CUDA, set these variables:
doc/src/Build_extras.rst:         -D Kokkos_ARCH_GPUARCH=yes    # GPUARCH = GPU from list above
doc/src/Build_extras.rst:         -D Kokkos_ENABLE_CUDA=yes
doc/src/Build_extras.rst:      This will also enable executing FFTs on the GPU, either via the
doc/src/Build_extras.rst:      library bundled with the CUDA toolkit, depending on whether CMake
doc/src/Build_extras.rst:      For AMD or NVIDIA GPUs using HIP, set these variables:
doc/src/Build_extras.rst:         -D Kokkos_ARCH_GPUARCH=yes    # GPUARCH = GPU from list above
doc/src/Build_extras.rst:      This will enable FFTs on the GPU, either by the internal KISSFFT library
doc/src/Build_extras.rst:      platform-appropriate vendor library: rocFFT on AMD GPUs or cuFFT on
doc/src/Build_extras.rst:      NVIDIA GPUs.
doc/src/Build_extras.rst:      For Intel GPUs using SYCL, set these variables:
doc/src/Build_extras.rst:         -D Kokkos_ARCH_GPUARCH=yes    # GPUARCH = GPU from list above
doc/src/Build_extras.rst:         -D FFT_KOKKOS=MKL_GPU
doc/src/Build_extras.rst:      This will enable FFTs on the GPU using the oneMKL library.
doc/src/Build_extras.rst:      ``kokkos-openmp.cmake``, ``kokkos-cuda.cmake``,
doc/src/Build_extras.rst:      ``kokkos-hip.cmake``, ``kokkos-sycl-nvidia.cmake``, and
doc/src/Build_extras.rst:      package and enable some hardware choices.  For GPU support those
doc/src/Build_extras.rst:      to compile with CUDA device parallelization with some common
doc/src/Build_extras.rst:         mkdir build-kokkos-cuda
doc/src/Build_extras.rst:         cd build-kokkos-cuda
doc/src/Build_extras.rst:               -C ../cmake/presets/kokkos-cuda.cmake ../cmake
doc/src/Build_extras.rst:      For NVIDIA GPUs using CUDA:
doc/src/Build_extras.rst:         KOKKOS_DEVICES = Cuda
doc/src/Build_extras.rst:         KOKKOS_ARCH = HOSTARCH,GPUARCH  # HOSTARCH = HOST from list above that is
doc/src/Build_extras.rst:                                         #            hosting the GPU
doc/src/Build_extras.rst:                                         # GPUARCH = GPU from list above
doc/src/Build_extras.rst:         KOKKOS_CUDA_OPTIONS = "enable_lambda"
doc/src/Build_extras.rst:      For GPUs, you also need the following lines in your
doc/src/Build_extras.rst:      ``nvcc`` for compiling CUDA files and a C++ compiler for
doc/src/Build_extras.rst:      non-Kokkos, non-CUDA files.
doc/src/Build_extras.rst:      For AMD or NVIDIA GPUs using HIP:
doc/src/Build_extras.rst:         KOKKOS_ARCH = HOSTARCH,GPUARCH  # HOSTARCH = HOST from list above that is
doc/src/Build_extras.rst:                                         #            hosting the GPU
doc/src/Build_extras.rst:                                         # GPUARCH = GPU from list above
doc/src/Build_extras.rst:      For Intel GPUs using SYCL:
doc/src/Build_extras.rst:         KOKKOS_ARCH = HOSTARCH,GPUARCH  # HOSTARCH = HOST from list above that is
doc/src/Build_extras.rst:                                         #            hosting the GPU
doc/src/Build_extras.rst:                                         # GPUARCH = GPU from list above
doc/src/Build_extras.rst:         FFT_INC = -DFFT_KOKKOS_MKL_GPU  # enable use of oneMKL for Intel GPUs (optional)
doc/src/Build_extras.rst:The CMake option ``-DKokkos_ENABLE_CUDA_UVM=on`` or the makefile
doc/src/Build_extras.rst:setting ``KOKKOS_CUDA_OPTIONS=enable_lambda,force_uvm`` enables the
doc/src/Build_extras.rst:use of CUDA "Unified Virtual Memory" (UVM) in Kokkos.  UVM allows to
doc/src/Build_extras.rst:GPU (with some performance penalty) and thus enables running larger
doc/src/Build_extras.rst:problems that would otherwise not fit into the RAM on the GPU.
doc/src/Build_extras.rst:with the *enable_lambda* option when using GPUs.  The CMake configuration
doc/src/pair_lj.rst:.. index:: pair_style lj/cut/gpu
doc/src/pair_lj.rst:Accelerator Variants: *lj/cut/gpu*, *lj/cut/intel*, *lj/cut/kk*, *lj/cut/opt*, *lj/cut/omp*
doc/src/pair_dpd_coul_slater_long.rst:.. index:: pair_style dpd/coul/slater/long/gpu
doc/src/pair_dpd_coul_slater_long.rst:Accelerator Variants: *dpd/coul/slater/long/gpu*
doc/src/pair_dpd_coul_slater_long.rst:   efficient (especially on GPUs) than using :doc:`pair_style
doc/src/pair_dpd_coul_slater_long.rst:   particularly true for the GPU package version of the pair style since
doc/src/pair_dpd_coul_slater_long.rst:   this version is compatible with computing neighbor lists on the GPU
doc/src/Commands_removed.rst:performance, supports OpenMP multi-threading via OPENMP, and GPU and
doc/src/Commands_removed.rst:USER-CUDA package
doc/src/Commands_removed.rst:The USER-CUDA package had been removed, since it had been unmaintained
doc/src/Commands_removed.rst:performance characteristics on NVIDIA GPUs. Both, the KOKKOS
doc/src/Commands_removed.rst:and the :ref:`GPU package <PKG-GPU>` are maintained
doc/src/Commands_removed.rst:and allow running LAMMPS with GPU acceleration.
doc/src/pair_coul.rst:.. index:: pair_style coul/cut/gpu
doc/src/pair_coul.rst:.. index:: pair_style coul/debye/gpu
doc/src/pair_coul.rst:.. index:: pair_style coul/dsf/gpu
doc/src/pair_coul.rst:.. index:: pair_style coul/long/gpu
doc/src/pair_coul.rst:Accelerator Variants: *coul/cut/gpu*, *coul/cut/kk*, *coul/cut/omp*
doc/src/pair_coul.rst:Accelerator Variants: *coul/debye/gpu*, *coul/debye/kk*, *coul/debye/omp*
doc/src/pair_coul.rst:Accelerator Variants: *coul/dsf/gpu*, *coul/dsf/kk*, *coul/dsf/omp*
doc/src/pair_coul.rst:Accelerator Variants: *coul/long/omp*, *coul/long/kk*, *coul/long/gpu*
doc/src/Examples.rst:| accelerate  | run with various acceleration options (OpenMP, GPU, Phi)         |
tools/kate/lammps.xml:    <item>freeze/cuda</item>
tools/kate/lammps.xml:    <item>addforce/cuda</item>
tools/kate/lammps.xml:    <item>aveforce/cuda</item>
tools/kate/lammps.xml:    <item>enforce2d/cuda</item>
tools/kate/lammps.xml:    <item>gravity/cuda</item>
tools/kate/lammps.xml:    <item>npt/cuda</item>
tools/kate/lammps.xml:    <item>nve/cuda</item>
tools/kate/lammps.xml:    <item>nvt/cuda</item>
tools/kate/lammps.xml:    <item>setforce/cuda</item>
tools/kate/lammps.xml:    <item>shake/cuda</item>
tools/kate/lammps.xml:    <item>temp/berendsen/cuda</item>
tools/kate/lammps.xml:    <item>temp/rescale/cuda</item>
tools/kate/lammps.xml:    <item>temp/rescale/limit/cuda</item>
tools/kate/lammps.xml:    <item>viscous/cuda</item>
tools/kate/lammps.xml:    <item>pe/cuda</item>
tools/kate/lammps.xml:    <item>pressure/cuda</item>
tools/kate/lammps.xml:    <item>temp/cuda</item>
tools/kate/lammps.xml:    <item>temp/partial/cuda</item>
tools/kate/lammps.xml:    <item>born/coul/long/cuda</item>
tools/kate/lammps.xml:    <item>born/coul/long/gpu</item>
tools/kate/lammps.xml:    <item>born/coul/wolf/gpu</item>
tools/kate/lammps.xml:    <item>born/gpu</item>
tools/kate/lammps.xml:    <item>buck/coul/cut/cuda</item>
tools/kate/lammps.xml:    <item>buck/coul/cut/gpu</item>
tools/kate/lammps.xml:    <item>buck/coul/long/cuda</item>
tools/kate/lammps.xml:    <item>buck/coul/long/gpu</item>
tools/kate/lammps.xml:    <item>buck/cuda</item>
tools/kate/lammps.xml:    <item>buck/gpu</item>
tools/kate/lammps.xml:    <item>colloid/gpu</item>
tools/kate/lammps.xml:    <item>coul/dsf/gpu</item>
tools/kate/lammps.xml:    <item>coul/long/gpu</item>
tools/kate/lammps.xml:    <item>dipole/cut/gpu</item>
tools/kate/lammps.xml:    <item>dipole/sf/gpu</item>
tools/kate/lammps.xml:    <item>eam/alloy/cuda</item>
tools/kate/lammps.xml:    <item>eam/alloy/gpu</item>
tools/kate/lammps.xml:    <item>eam/cuda</item>
tools/kate/lammps.xml:    <item>eam/fs/cuda</item>
tools/kate/lammps.xml:    <item>eam/fs/gpu</item>
tools/kate/lammps.xml:    <item>eam/gpu</item>
tools/kate/lammps.xml:    <item>gauss/gpu</item>
tools/kate/lammps.xml:    <item>gayberne/gpu</item>
tools/kate/lammps.xml:    <item>gran/hooke/cuda</item>
tools/kate/lammps.xml:    <item>lj/charmm/coul/charmm/cuda</item>
tools/kate/lammps.xml:    <item>lj/charmm/coul/charmm/implicit/cuda</item>
tools/kate/lammps.xml:    <item>lj/charmm/coul/long/cuda</item>
tools/kate/lammps.xml:    <item>lj/charmm/coul/long/gpu</item>
tools/kate/lammps.xml:    <item>lj/class2/coul/cut/cuda</item>
tools/kate/lammps.xml:    <item>lj/class2/coul/long/cuda</item>
tools/kate/lammps.xml:    <item>lj/class2/coul/long/gpu</item>
tools/kate/lammps.xml:    <item>lj/class2/cuda</item>
tools/kate/lammps.xml:    <item>lj/class2/gpu</item>
tools/kate/lammps.xml:    <item>lj/cut/coul/cut/cuda</item>
tools/kate/lammps.xml:    <item>lj/cut/coul/cut/gpu</item>
tools/kate/lammps.xml:    <item>lj/cut/coul/debye/cuda</item>
tools/kate/lammps.xml:    <item>lj/cut/coul/debye/gpu</item>
tools/kate/lammps.xml:    <item>lj/cut/coul/dsf/gpu</item>
tools/kate/lammps.xml:    <item>lj/cut/coul/long/cuda</item>
tools/kate/lammps.xml:    <item>lj/cut/coul/long/gpu</item>
tools/kate/lammps.xml:    <item>lj/cut/cuda</item>
tools/kate/lammps.xml:    <item>lj/cut/experimental/cuda</item>
tools/kate/lammps.xml:    <item>lj/cut/gpu</item>
tools/kate/lammps.xml:    <item>lj/expand/cuda</item>
tools/kate/lammps.xml:    <item>lj/expand/gpu</item>
tools/kate/lammps.xml:    <item>lj/gromacs/coul/gromacs/cuda</item>
tools/kate/lammps.xml:    <item>lj/gromacs/cuda</item>
tools/kate/lammps.xml:    <item>lj/spica/gpu</item>
tools/kate/lammps.xml:    <item>lj/spica/coul/long/gpu</item>
tools/kate/lammps.xml:    <item>lj/sdk/gpu</item>
tools/kate/lammps.xml:    <item>lj/sdk/coul/long/gpu</item>
tools/kate/lammps.xml:    <item>lj/smooth/cuda</item>
tools/kate/lammps.xml:    <item>lj96/cut/cuda</item>
tools/kate/lammps.xml:    <item>lj96/cut/gpu</item>
tools/kate/lammps.xml:    <item>morse/cuda</item>
tools/kate/lammps.xml:    <item>morse/gpu</item>
tools/kate/lammps.xml:    <item>resquared/gpu</item>
tools/kate/lammps.xml:    <item>sw/cuda</item>
tools/kate/lammps.xml:    <item>table/gpu</item>
tools/kate/lammps.xml:    <item>tersoff/cuda</item>
tools/kate/lammps.xml:    <item>yukawa/gpu</item>
tools/kate/lammps.xml:    <item>yukawa/colloid/gpu</item>
tools/kate/lammps.xml:    <item>pppm/cuda</item>
tools/kate/lammps.xml:    <item>pppm/gpu</item>
tools/singularity/ubuntu20.04_nvidia.def:From: nvidia/cuda:11.6.2-devel-ubuntu20.04
tools/singularity/ubuntu20.04_nvidia.def:        ocl-icd-libopencl1 \
tools/singularity/ubuntu20.04_nvidia.def:        ocl-icd-opencl-dev \
tools/singularity/ubuntu20.04_nvidia.def:    # NVIDIA OpenCL
tools/singularity/ubuntu20.04_nvidia.def:    mkdir -p /etc/OpenCL/vendors
tools/singularity/ubuntu20.04_nvidia.def:    echo "libnvidia-opencl.so.1" > /etc/OpenCL/vendors/nvidia.icd
tools/singularity/ubuntu20.04_nvidia.def:PS1="[ubuntu20.04/nvidia:\u@\h] \W> "
tools/singularity/ubuntu20.04_gpu.def:    export PATH=/usr/lib/ccache:/usr/local/cuda-12.0/bin:${PATH}:/opt/rocm/bin:/opt/rocm/profiler/bin:/opt/rocm/opencl/bin/x86_64
tools/singularity/ubuntu20.04_gpu.def:    export CUDADIR=/usr/local/cuda-12.0
tools/singularity/ubuntu20.04_gpu.def:    export CUDA_PATH=/usr/local/cuda-12.0
tools/singularity/ubuntu20.04_gpu.def:    export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-12.0/lib64:/opt/rocm/lib:/opt/rocm-5.4.3/llvm/lib
tools/singularity/ubuntu20.04_gpu.def:    export LIBRARY_PATH=/usr/local/cuda-12.0/lib64/stubs
tools/singularity/ubuntu20.04_gpu.def:    # ROCm 5.4.3
tools/singularity/ubuntu20.04_gpu.def:    wget https://repo.radeon.com/amdgpu-install/5.4.3/ubuntu/focal/amdgpu-install_5.4.50403-1_all.deb
tools/singularity/ubuntu20.04_gpu.def:    apt-get install -y ./amdgpu-install_5.4.50403-1_all.deb
tools/singularity/ubuntu20.04_gpu.def:    amdgpu-install --usecase=rocm --no-dkms -y
tools/singularity/ubuntu20.04_gpu.def:    # ROCm hipCUB and hipFFT
tools/singularity/ubuntu20.04_gpu.def:        ocl-icd-libopencl1 \
tools/singularity/ubuntu20.04_gpu.def:        ocl-icd-opencl-dev \
tools/singularity/ubuntu20.04_gpu.def:    # CUDA
tools/singularity/ubuntu20.04_gpu.def:    wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-keyring_1.0-1_all.deb
tools/singularity/ubuntu20.04_gpu.def:    dpkg -i cuda-keyring_1.0-1_all.deb
tools/singularity/ubuntu20.04_gpu.def:    rm cuda-keyring_1.0-1_all.deb
tools/singularity/ubuntu20.04_gpu.def:    export CUDA_PKG_VERSION=12.0
tools/singularity/ubuntu20.04_gpu.def:    export CUDA_DRV_VERSION=525
tools/singularity/ubuntu20.04_gpu.def:        cuda-libraries-${CUDA_PKG_VERSION} \
tools/singularity/ubuntu20.04_gpu.def:        cuda-command-line-tools-${CUDA_PKG_VERSION} \
tools/singularity/ubuntu20.04_gpu.def:        cuda-libraries-dev-${CUDA_PKG_VERSION} \
tools/singularity/ubuntu20.04_gpu.def:        cuda-minimal-build-${CUDA_PKG_VERSION} \
tools/singularity/ubuntu20.04_gpu.def:        cuda-compat-$CUDA_PKG_VERSION \
tools/singularity/ubuntu20.04_gpu.def:        libcublas-${CUDA_PKG_VERSION} \
tools/singularity/ubuntu20.04_gpu.def:        libcublas-dev-${CUDA_PKG_VERSION} \
tools/singularity/ubuntu20.04_gpu.def:        libnvidia-compute-${CUDA_DRV_VERSION}
tools/singularity/ubuntu20.04_gpu.def:    ln -s /usr/local/cuda-${CUDA_PKG_VERSION}/lib64/stubs/libcuda.so /usr/local/cuda-${CUDA_PKG_VERSION}/lib64/stubs/libcuda.so.1
tools/singularity/ubuntu20.04_gpu.def:    # NVIDIA OpenCL
tools/singularity/ubuntu20.04_gpu.def:    test ! -d /etc/OpenCL/vendors && mkdir -p /etc/OpenCL/vendors && \
tools/singularity/ubuntu20.04_gpu.def:         echo "libnvidia-opencl.so.1" > /etc/OpenCL/vendors/nvidia.icd
tools/singularity/ubuntu20.04_gpu.def:PS1="[ubuntu20.04/gpu:\u@\h] \W> "
tools/singularity/ubuntu18.04_gpu.def:    export PATH=/usr/lib/ccache:/usr/local/cuda-12.0/bin:${PATH}:/opt/rocm/bin:/opt/rocm/profiler/bin:/opt/rocm/opencl/bin/x86_64
tools/singularity/ubuntu18.04_gpu.def:    export CUDADIR=/usr/local/cuda-12.0
tools/singularity/ubuntu18.04_gpu.def:    export CUDA_PATH=/usr/local/cuda-12.0
tools/singularity/ubuntu18.04_gpu.def:    export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-12.0/lib64:/opt/rocm/lib:/opt/rocm-5.4.3/llvm/lib
tools/singularity/ubuntu18.04_gpu.def:    export LIBRARY_PATH=/usr/local/cuda-12.0/lib64/stubs
tools/singularity/ubuntu18.04_gpu.def:    # ROCm 5.4.3
tools/singularity/ubuntu18.04_gpu.def:    wget https://repo.radeon.com/amdgpu-install/5.4.3/ubuntu/focal/amdgpu-install_5.4.50403-1_all.deb
tools/singularity/ubuntu18.04_gpu.def:    apt-get install -y ./amdgpu-install_5.4.50403-1_all.deb
tools/singularity/ubuntu18.04_gpu.def:    amdgpu-install --usecase=rocm --no-dkms -y
tools/singularity/ubuntu18.04_gpu.def:    # ROCm hipCUB
tools/singularity/ubuntu18.04_gpu.def:        ocl-icd-libopencl1 \
tools/singularity/ubuntu18.04_gpu.def:        ocl-icd-opencl-dev \
tools/singularity/ubuntu18.04_gpu.def:    # CUDA
tools/singularity/ubuntu18.04_gpu.def:    wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-keyring_1.0-1_all.deb
tools/singularity/ubuntu18.04_gpu.def:    dpkg -i cuda-keyring_1.0-1_all.deb
tools/singularity/ubuntu18.04_gpu.def:    rm cuda-keyring_1.0-1_all.deb
tools/singularity/ubuntu18.04_gpu.def:    export CUDA_PKG_VERSION=12.0
tools/singularity/ubuntu18.04_gpu.def:    export CUDA_DRV_VERSION=525
tools/singularity/ubuntu18.04_gpu.def:        cuda-libraries-${CUDA_PKG_VERSION} \
tools/singularity/ubuntu18.04_gpu.def:        cuda-command-line-tools-${CUDA_PKG_VERSION} \
tools/singularity/ubuntu18.04_gpu.def:        cuda-libraries-dev-${CUDA_PKG_VERSION} \
tools/singularity/ubuntu18.04_gpu.def:        cuda-minimal-build-${CUDA_PKG_VERSION} \
tools/singularity/ubuntu18.04_gpu.def:        cuda-compat-$CUDA_PKG_VERSION \
tools/singularity/ubuntu18.04_gpu.def:        libcublas-${CUDA_PKG_VERSION} \
tools/singularity/ubuntu18.04_gpu.def:        libcublas-dev-${CUDA_PKG_VERSION} \
tools/singularity/ubuntu18.04_gpu.def:        libnvidia-compute-${CUDA_DRV_VERSION}
tools/singularity/ubuntu18.04_gpu.def:    ln -s /usr/local/cuda-${CUDA_PKG_VERSION}/lib64/stubs/libcuda.so /usr/local/cuda-${CUDA_PKG_VERSION}/lib64/stubs/libcuda.so.1
tools/singularity/ubuntu18.04_gpu.def:    # NVIDIA OpenCL
tools/singularity/ubuntu18.04_gpu.def:    test ! -d /etc/OpenCL/vendors && mkdir -p /etc/OpenCL/vendors && \
tools/singularity/ubuntu18.04_gpu.def:         echo "libnvidia-opencl.so.1" > /etc/OpenCL/vendors/nvidia.icd
tools/singularity/ubuntu18.04_gpu.def:PS1="[ubuntu18.04/gpu:\u@\h] \W> "
tools/singularity/ubuntu18.04_intel_opencl.def:        ocl-icd-libopencl1 \
tools/singularity/ubuntu18.04_intel_opencl.def:        ocl-icd-opencl-dev \
tools/singularity/ubuntu18.04_intel_opencl.def:    # Intel OpenCL
tools/singularity/ubuntu18.04_intel_opencl.def:    add-apt-repository ppa:intel-opencl/intel-opencl
tools/singularity/ubuntu18.04_intel_opencl.def:    apt-get install -y intel-opencl-icd
tools/singularity/ubuntu18.04_amd_rocm.def:    export PATH=/usr/lib/ccache:${PATH}:/opt/rocm/bin:/opt/rocm/profiler/bin:/opt/rocm/opencl/bin/x86_64
tools/singularity/ubuntu18.04_amd_rocm.def:    export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/rocm/lib:/opt/rocm-5.1.3/llvm/lib
tools/singularity/ubuntu18.04_amd_rocm.def:    # ROCm 5.1.3
tools/singularity/ubuntu18.04_amd_rocm.def:    wget https://repo.radeon.com/amdgpu-install/22.10.3/ubuntu/bionic/amdgpu-install_22.10.3.50103-1_all.deb
tools/singularity/ubuntu18.04_amd_rocm.def:    apt-get install -y ./amdgpu-install_22.10.3.50103-1_all.deb
tools/singularity/ubuntu18.04_amd_rocm.def:    amdgpu-install --usecase=rocm --no-dkms -y
tools/singularity/ubuntu18.04_amd_rocm.def:    # ROCm hipCUB
tools/singularity/ubuntu18.04_amd_rocm.def:PS1="[ubuntu18.04/rocm:\u@\h] \W> "
tools/singularity/ubuntu20.04_amd_rocm.def:    export PATH=/usr/lib/ccache:${PATH}:/opt/rocm/bin:/opt/rocm/profiler/bin:/opt/rocm/opencl/bin/x86_64
tools/singularity/ubuntu20.04_amd_rocm.def:    export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/rocm/lib:/opt/rocm-5.4.3/llvm/lib
tools/singularity/ubuntu20.04_amd_rocm.def:    # ROCm 5.4.3
tools/singularity/ubuntu20.04_amd_rocm.def:    wget https://repo.radeon.com/amdgpu-install/5.4.3/ubuntu/focal/amdgpu-install_5.4.50403-1_all.deb
tools/singularity/ubuntu20.04_amd_rocm.def:    apt-get install -y ./amdgpu-install_5.4.50403-1_all.deb
tools/singularity/ubuntu20.04_amd_rocm.def:    amdgpu-install --usecase=rocm --no-dkms -y
tools/singularity/ubuntu20.04_amd_rocm.def:    # ROCm hipCUB and hipFFT
tools/singularity/ubuntu20.04_amd_rocm.def:PS1="[ubuntu20.04/rocm:\u@\h] \W> "
tools/singularity/ubuntu18.04_nvidia.def:From: nvidia/cuda:11.6.2-devel-ubuntu18.04
tools/singularity/ubuntu18.04_nvidia.def:        ocl-icd-libopencl1 \
tools/singularity/ubuntu18.04_nvidia.def:        ocl-icd-opencl-dev \
tools/singularity/ubuntu18.04_nvidia.def:    # NVIDIA OpenCL
tools/singularity/ubuntu18.04_nvidia.def:    mkdir -p /etc/OpenCL/vendors
tools/singularity/ubuntu18.04_nvidia.def:    echo "libnvidia-opencl.so.1" > /etc/OpenCL/vendors/nvidia.icd
tools/singularity/ubuntu18.04_nvidia.def:PS1="[ubuntu18.04/nvidia:\u@\h] \W> "
tools/singularity/ubuntu20.04_intel_opencl.def:    # Intel OpenCL
tools/singularity/ubuntu20.04_intel_opencl.def:    add-apt-repository ppa:intel-opencl/intel-opencl
tools/singularity/ubuntu20.04_intel_opencl.def:    apt-get install -y intel-opencl-icd
tools/coding_standard/whitespace.py:    - lib/gpu/geryon/file_to_cstr.sh
tools/offline/scripts/init_http_cache.sh:OPENCL_LOADER_URL="${LAMMPS_THIRDPARTY_URL}/opencl-loader-2021.09.18.tar.gz"
tools/offline/scripts/init_http_cache.sh:    OPENCL_LOADER_URL
tools/lammps-gui/lammpswrapper.cpp:bool LammpsWrapper::has_gpu_device() const
tools/lammps-gui/lammpswrapper.cpp:    return ((liblammpsplugin_t *)plugin_handle)->has_gpu_device() != 0;
tools/lammps-gui/lammpswrapper.cpp:    return lammps_has_gpu_device() != 0;
tools/lammps-gui/preferences.h:    enum { None, Opt, OpenMP, Intel, Kokkos, Gpu };
tools/lammps-gui/lammpswrapper.h:    bool has_gpu_device() const;
tools/lammps-gui/help_index.table:fix_nh.html fix npt/gpu
tools/lammps-gui/help_index.table:fix_nh.html fix nvt/gpu
tools/lammps-gui/help_index.table:fix_nve_asphere.html fix nve/asphere/gpu
tools/lammps-gui/help_index.table:fix_nve.html fix nve/gpu
tools/lammps-gui/help_index.table:kspace_style.html kspace_style pppm/gpu
tools/lammps-gui/help_index.table:pair_amoeba.html pair_style amoeba/gpu
tools/lammps-gui/help_index.table:pair_amoeba.html pair_style hippo/gpu
tools/lammps-gui/help_index.table:pair_beck.html pair_style beck/gpu
tools/lammps-gui/help_index.table:pair_born.html pair_style born/coul/long/gpu
tools/lammps-gui/help_index.table:pair_born.html pair_style born/coul/wolf/gpu
tools/lammps-gui/help_index.table:pair_born.html pair_style born/gpu
tools/lammps-gui/help_index.table:pair_buck.html pair_style buck/coul/cut/gpu
tools/lammps-gui/help_index.table:pair_buck.html pair_style buck/coul/long/gpu
tools/lammps-gui/help_index.table:pair_buck.html pair_style buck/gpu
tools/lammps-gui/help_index.table:pair_charmm.html pair_style lj/charmm/coul/charmm/gpu
tools/lammps-gui/help_index.table:pair_charmm.html pair_style lj/charmm/coul/long/gpu
tools/lammps-gui/help_index.table:pair_class2.html pair_style lj/class2/coul/long/gpu
tools/lammps-gui/help_index.table:pair_class2.html pair_style lj/class2/gpu
tools/lammps-gui/help_index.table:pair_colloid.html pair_style colloid/gpu
tools/lammps-gui/help_index.table:pair_coul.html pair_style coul/cut/gpu
tools/lammps-gui/help_index.table:pair_coul.html pair_style coul/debye/gpu
tools/lammps-gui/help_index.table:pair_coul.html pair_style coul/dsf/gpu
tools/lammps-gui/help_index.table:pair_coul.html pair_style coul/long/gpu
tools/lammps-gui/help_index.table:pair_coul_slater.html pair_style coul/slater/long/gpu
tools/lammps-gui/help_index.table:pair_cs.html pair_style born/coul/long/cs/gpu
tools/lammps-gui/help_index.table:pair_cs.html pair_style born/coul/wolf/cs/gpu
tools/lammps-gui/help_index.table:pair_cs.html pair_style coul/long/cs/gpu
tools/lammps-gui/help_index.table:pair_dipole.html pair_style lj/cut/dipole/cut/gpu
tools/lammps-gui/help_index.table:pair_dipole.html pair_style lj/cut/dipole/long/gpu
tools/lammps-gui/help_index.table:pair_dipole.html pair_style lj/sf/dipole/sf/gpu
tools/lammps-gui/help_index.table:pair_dpd_coul_slater_long.html pair_style dpd/coul/slater/long/gpu
tools/lammps-gui/help_index.table:pair_dpd.html pair_style dpd/gpu
tools/lammps-gui/help_index.table:pair_dpd.html pair_style dpd/tstat/gpu
tools/lammps-gui/help_index.table:pair_eam.html pair_style eam/alloy/gpu
tools/lammps-gui/help_index.table:pair_eam.html pair_style eam/fs/gpu
tools/lammps-gui/help_index.table:pair_eam.html pair_style eam/gpu
tools/lammps-gui/help_index.table:pair_fep_soft.html pair_style lj/cut/coul/cut/soft/gpu
tools/lammps-gui/help_index.table:pair_fep_soft.html pair_style lj/cut/coul/long/soft/gpu
tools/lammps-gui/help_index.table:pair_gauss.html pair_style gauss/gpu
tools/lammps-gui/help_index.table:pair_gayberne.html pair_style gayberne/gpu
tools/lammps-gui/help_index.table:pair_gromacs.html pair_style lj/gromacs/gpu
tools/lammps-gui/help_index.table:pair_lj96.html pair_style lj96/cut/gpu
tools/lammps-gui/help_index.table:pair_lj_cubic.html pair_style lj/cubic/gpu
tools/lammps-gui/help_index.table:pair_lj_cut_coul.html pair_style lj/cut/coul/cut/gpu
tools/lammps-gui/help_index.table:pair_lj_cut_coul.html pair_style lj/cut/coul/debye/gpu
tools/lammps-gui/help_index.table:pair_lj_cut_coul.html pair_style lj/cut/coul/dsf/gpu
tools/lammps-gui/help_index.table:pair_lj_cut_coul.html pair_style lj/cut/coul/long/gpu
tools/lammps-gui/help_index.table:pair_lj_cut_coul.html pair_style lj/cut/coul/msm/gpu
tools/lammps-gui/help_index.table:pair_lj_cut_tip4p.html pair_style lj/cut/tip4p/long/gpu
tools/lammps-gui/help_index.table:pair_lj_expand.html pair_style lj/expand/coul/long/gpu
tools/lammps-gui/help_index.table:pair_lj_expand.html pair_style lj/expand/gpu
tools/lammps-gui/help_index.table:pair_lj.html pair_style lj/cut/gpu
tools/lammps-gui/help_index.table:pair_lj_smooth.html pair_style lj/smooth/gpu
tools/lammps-gui/help_index.table:pair_mesodpd.html pair_style edpd/gpu
tools/lammps-gui/help_index.table:pair_mesodpd.html pair_style mdpd/gpu
tools/lammps-gui/help_index.table:pair_mie.html pair_style mie/cut/gpu
tools/lammps-gui/help_index.table:pair_morse.html pair_style morse/gpu
tools/lammps-gui/help_index.table:pair_resquared.html pair_style resquared/gpu
tools/lammps-gui/help_index.table:pair_soft.html pair_style soft/gpu
tools/lammps-gui/help_index.table:pair_sph_heatconduction.html pair_style sph/heatconduction/gpu
tools/lammps-gui/help_index.table:pair_sph_lj.html pair_style sph/lj/gpu
tools/lammps-gui/help_index.table:pair_sph_taitwater.html pair_style sph/taitwater/gpu
tools/lammps-gui/help_index.table:pair_spica.html pair_style lj/spica/coul/long/gpu
tools/lammps-gui/help_index.table:pair_spica.html pair_style lj/spica/gpu
tools/lammps-gui/help_index.table:pair_sw.html pair_style sw/gpu
tools/lammps-gui/help_index.table:pair_table.html pair_style table/gpu
tools/lammps-gui/help_index.table:pair_tersoff_mod.html pair_style tersoff/mod/gpu
tools/lammps-gui/help_index.table:pair_tersoff.html pair_style tersoff/gpu
tools/lammps-gui/help_index.table:pair_tersoff_zbl.html pair_style tersoff/zbl/gpu
tools/lammps-gui/help_index.table:pair_ufm.html pair_style ufm/gpu
tools/lammps-gui/help_index.table:pair_vashishta.html pair_style vashishta/gpu
tools/lammps-gui/help_index.table:pair_yukawa_colloid.html pair_style yukawa/colloid/gpu
tools/lammps-gui/help_index.table:pair_yukawa.html pair_style yukawa/gpu
tools/lammps-gui/help_index.table:pair_zbl.html pair_style zbl/gpu
tools/lammps-gui/preferences.cpp:            if (allButton->objectName() == "gpu")
tools/lammps-gui/preferences.cpp:                settings->setValue("accelerator", QString::number(AcceleratorTab::Gpu));
tools/lammps-gui/preferences.cpp:    auto *gpu         = new QRadioButton("&GPU");
tools/lammps-gui/preferences.cpp:    buttonLayout->addWidget(gpu);
tools/lammps-gui/preferences.cpp:            !(lammps->config_accelerator("KOKKOS", "api", "cuda") ||
tools/lammps-gui/preferences.cpp:    gpu->setEnabled(lammps->config_has_package("GPU") && lammps->has_gpu_device());
tools/lammps-gui/preferences.cpp:    gpu->setObjectName("gpu");
tools/lammps-gui/preferences.cpp:        case AcceleratorTab::Gpu:
tools/lammps-gui/preferences.cpp:            if (gpu->isEnabled()) gpu->setChecked(true);
tools/lammps-gui/lammpsgui.cpp:    } else if (accel == AcceleratorTab::Gpu) {
tools/lammps-gui/lammpsgui.cpp:        if (!lammps.config_has_package("GPU") || !lammps.has_gpu_device())
tools/lammps-gui/lammpsgui.cpp:            if (style.endsWith("/gpu") || style.endsWith("/intel") || style.endsWith("/kk") || \
tools/lammps-gui/lammpsgui.cpp:    } else if (accel == AcceleratorTab::Gpu) {
tools/lammps-gui/lammpsgui.cpp:        lammps_args.push_back(mystrdup("gpu"));
tools/lammps-gui/lammpsgui.cpp:        lammps_args.push_back(mystrdup("gpu"));
tools/lammps-gui/org.lammps.lammps-gui.yml:      - -D PKG_GPU=yes
tools/lammps-gui/org.lammps.lammps-gui.yml:      - -D GPU_API=opencl
tools/valgrind/OpenMP.supp:   OpenMP_cuda_init_part1
tools/valgrind/OpenMP.supp:   obj:*/lib*/libcuda.so.*
tools/swig/lammps.i:extern int    lammps_has_gpu_device();
tools/swig/lammps.i:extern void   lammps_get_gpu_device_info(char *buffer, int buf_size);
tools/swig/lammps.i:extern int    lammps_has_gpu_device();
tools/swig/lammps.i:extern void   lammps_get_gpu_device_info(char *buffer, int buf_size);
tools/regression-tests/reference.yaml:in.bucky-plus-cnt-gpu: { folder: examples/PACKAGES/imd, status: "skipped", walltime: -2 }
tools/regression-tests/reference.yaml:in.deca-ala_imd-gpu: { folder: examples/PACKAGES/imd, status: "skipped", walltime: -2 }
tools/regression-tests/reference.yaml:in.melt_imd-gpu: { folder: examples/PACKAGES/imd, status: "skipped", walltime: -2 }
tools/regression-tests/run_tests.py:    - testing accelerator packages (GPU, INTEL, KOKKOS, OPENMP) need separate config files, "args: -sf omp -pk omp 4"
tools/regression-tests/config_gpu.yaml:  args: "-cite none -sf gpu -pk gpu 0"
tools/regression-tests/get_quick_list.py:    gpu = re.compile("(.+)/gpu$")
tools/regression-tests/get_quick_list.py:                        suffix = gpu.match(style)
tools/regression-tests/README:    + testing with accelerator packages (GPU, INTEL, KOKKOS, OPENMP) need separate config files, e.g., args: "-sf omp -pk omp 4"
tools/regression-tests/README:the runs are often `mpirun -np 2 lmp -in in.melt -k on g 2` (with the CUDA backend) or  `mpirun -np 2 lmp -in in.melt -k on t 2` (with the OpenMP backend).
unittest/python/python-commands.py:        self.assertEqual(self.lmp.extract_setting("kokkos_ngpus"), 0)
unittest/python/python-capabilities.py:        if self.cmake_cache['PKG_GPU']:
unittest/python/python-capabilities.py:            if self.cmake_cache['GPU_API'].lower() == 'opencl':
unittest/python/python-capabilities.py:                 self.assertIn('opencl',settings['GPU']['api'])
unittest/python/python-capabilities.py:            if self.cmake_cache['GPU_API'].lower() == 'cuda':
unittest/python/python-capabilities.py:                 self.assertIn('cuda',settings['GPU']['api'])
unittest/python/python-capabilities.py:            if self.cmake_cache['GPU_API'].lower() == 'hip':
unittest/python/python-capabilities.py:                 self.assertIn('hip',settings['GPU']['api'])
unittest/python/python-capabilities.py:            if self.cmake_cache['GPU_PREC'].lower() == 'double':
unittest/python/python-capabilities.py:                 self.assertIn('double',settings['GPU']['precision'])
unittest/python/python-capabilities.py:            if self.cmake_cache['GPU_PREC'].lower() == 'mixed':
unittest/python/python-capabilities.py:                 self.assertIn('mixed',settings['GPU']['precision'])
unittest/python/python-capabilities.py:            if self.cmake_cache['GPU_PREC'].lower() == 'single':
unittest/python/python-capabilities.py:                 self.assertIn('single',settings['GPU']['precision'])
unittest/python/python-capabilities.py:    def test_gpu_device(self):
unittest/python/python-capabilities.py:        info = self.lmp.get_gpu_device_info()
unittest/python/python-capabilities.py:        if self.lmp.has_gpu_device:
unittest/c-library/test_library_open.cpp:    // when GPU support is enabled in KOKKOS, it *must* be used
unittest/c-library/test_library_open.cpp:        lammps_config_accelerator("KOKKOS", "api", "cuda") ||
unittest/c-library/test_library_properties.cpp:    EXPECT_EQ(lammps_extract_setting(lmp, "kokkos_ngpus"), 0);
unittest/c-library/test_library_properties.cpp:    bool has_gpu     = false;
unittest/c-library/test_library_properties.cpp:    // when GPU support is enabled in KOKKOS, it *must* be used
unittest/c-library/test_library_properties.cpp:        lammps_config_accelerator("KOKKOS", "api", "cuda") ||
unittest/c-library/test_library_properties.cpp:        has_gpu = true;
unittest/c-library/test_library_properties.cpp:    if (has_gpu)
unittest/c-library/test_library_properties.cpp:        EXPECT_EQ(lammps_extract_setting(lmp, "kokkos_ngpus"), 1);
unittest/c-library/test_library_properties.cpp:        EXPECT_EQ(lammps_extract_setting(lmp, "kokkos_ngpus"), 0);
unittest/cplusplus/test_lammps_class.cpp:        // when GPU support is enabled in KOKKOS, it *must* be used
unittest/cplusplus/test_lammps_class.cpp:            Info::has_accelerator_feature("KOKKOS", "api", "cuda") ||
unittest/fortran/test_fortran_configuration.f90:FUNCTION f_lammps_has_gpu() BIND(C)
unittest/fortran/test_fortran_configuration.f90:  INTEGER(c_int) :: f_lammps_has_gpu
unittest/fortran/test_fortran_configuration.f90:  IF (lmp%has_gpu_device()) THEN
unittest/fortran/test_fortran_configuration.f90:    f_lammps_has_gpu = 1_c_int
unittest/fortran/test_fortran_configuration.f90:    f_lammps_has_gpu = 0_c_int
unittest/fortran/test_fortran_configuration.f90:END FUNCTION f_lammps_has_gpu
unittest/fortran/test_fortran_configuration.f90:FUNCTION f_lammps_get_gpu_info(buf_size) RESULT(info) BIND(C)
unittest/fortran/test_fortran_configuration.f90:  CALL lmp%get_gpu_device_info(string)
unittest/fortran/test_fortran_configuration.f90:END FUNCTION f_lammps_get_gpu_info
unittest/fortran/wrap_configuration.cpp:int f_lammps_has_gpu();
unittest/fortran/wrap_configuration.cpp:char *f_lammps_get_gpu_info(size_t);
unittest/fortran/wrap_configuration.cpp:        "EXTRA-PAIR", "FEP", "GPU", "GRANULAR", "H5MD", "INTEL", "INTEL/TEST", "INTERLAYER", "KIM",
unittest/fortran/wrap_configuration.cpp:    const std::vector<std::string> package           = {"GPU", "KOKKOS", "INTEL", "OPENMP"};
unittest/fortran/wrap_configuration.cpp:    const std::vector<std::string> setting_api       = {"cuda",   "hip",    "phi",   "pthreads",
unittest/fortran/wrap_configuration.cpp:                                                        "opencl", "openmp", "serial"};
unittest/fortran/wrap_configuration.cpp:TEST_F(LAMMPS_configuration, has_gpu)
unittest/fortran/wrap_configuration.cpp:    EXPECT_EQ(Info::has_gpu_device(), f_lammps_has_gpu());
unittest/fortran/wrap_configuration.cpp:TEST_F(LAMMPS_configuration, get_gpu_info)
unittest/fortran/wrap_configuration.cpp:    if (!Info::has_gpu_device()) GTEST_SKIP();
unittest/fortran/wrap_configuration.cpp:    auto cpp_info  = Info::get_gpu_device_info();
unittest/fortran/wrap_configuration.cpp:    char *f_string = f_lammps_get_gpu_info(cpp_info.size());
unittest/fortran/wrap_configuration.cpp:        f_string = f_lammps_get_gpu_info(80);
unittest/force-styles/test_angle_style.cpp:    // if KOKKOS has GPU support enabled, it *must* be used. We cannot test OpenMP only.
unittest/force-styles/test_angle_style.cpp:    if (Info::has_accelerator_feature("KOKKOS", "api", "cuda") ||
unittest/force-styles/tests/manybody-pair-meam_2nn.yaml:  if "$(is_active(package,gpu)) > 0.0" then "variable newton_pair index off" else "variable newton_pair index on"
unittest/force-styles/tests/mol-pair-lj_charmmfsw_coul_table.yaml:skip_tests: gpu
unittest/force-styles/tests/mol-pair-coul_table_cs.yaml:skip_tests: gpu
unittest/force-styles/tests/manybody-pair-lcbop.yaml:  if "$(is_active(package,gpu)) > 0.0" then "variable newton_pair index off" else "variable newton_pair index on"
unittest/force-styles/tests/mol-pair-dpd.yaml:skip_tests: gpu intel kokkos_omp single
unittest/force-styles/tests/mol-pair-dpd_coul_slater_long.yaml:skip_tests: gpu intel kokkos_omp single
unittest/force-styles/tests/manybody-pair-gw.yaml:  if "$(is_active(package,gpu)) > 0.0" then "variable newton_pair index off" else "variable newton_pair index on"
unittest/force-styles/tests/kspace-pppm_tri.yaml:skip_tests: gpu
unittest/force-styles/tests/manybody-pair-vashishta_table.yaml:  if "$(is_active(package,gpu)) > 0.0" then "variable newton_pair index off" else "variable newton_pair index on"
unittest/force-styles/tests/mol-pair-hybrid_multiple.yaml:skip_tests: gpu
unittest/force-styles/tests/manybody-pair-tersoff_mod_shift.yaml:skip_tests: gpu intel kokkos_omp
unittest/force-styles/tests/mol-pair-buck_coul_table.yaml:skip_tests: gpu
unittest/force-styles/tests/kspace-scafacos_ewald.yaml:skip_tests: extract gpu intel omp opt single
unittest/force-styles/tests/kspace-scafacos_fmm_tuned.yaml:skip_tests: extract gpu intel omp opt single
unittest/force-styles/tests/manybody-pair-nb3b_harmonic.yaml:  if "$(is_active(package,gpu)) > 0.0" then "variable newton_pair index off" else "variable newton_pair index on"
unittest/force-styles/tests/manybody-pair-airebo_00.yaml:  if "$(is_active(package,gpu)) > 0.0" then "variable newton_pair index off" else "variable newton_pair index on"
unittest/force-styles/tests/mol-pair-lj_cut_tip4p_long_soft.yaml:  if "$(is_active(package,gpu)) > 0.0" then "variable newton_pair index off" else "variable newton_pair index on"
unittest/force-styles/tests/manybody-pair-rebo.yaml:  if "$(is_active(package,gpu)) > 0.0" then "variable newton_pair index off" else "variable newton_pair index on"
unittest/force-styles/tests/manybody-pair-bop_save.yaml:  if "$(is_active(package,gpu)) > 0.0" then "variable newton_pair index off" else "variable newton_pair index on"
unittest/force-styles/tests/manybody-pair-bop.yaml:  if "$(is_active(package,gpu)) > 0.0" then "variable newton_pair index off" else "variable newton_pair index on"
unittest/force-styles/tests/kspace-pppm_conp_charge.yaml:skip_tests: gpu kokkos_omp omp
unittest/force-styles/tests/manybody-pair-tersoff_mod_c.yaml:  if "$(is_active(package,gpu)) > 0.0" then "variable newton_pair index off" else "variable newton_pair index on"
unittest/force-styles/tests/manybody-pair-tersoff.yaml:  if "$(is_active(package,gpu)) > 0.0" then "variable newton_pair index off" else "variable newton_pair index on"
unittest/force-styles/tests/manybody-pair-sw-mod-maxdelcs.yaml:  if "$(is_active(package,gpu)) > 0.0" then "variable newton_pair index off" else "variable newton_pair index on"
unittest/force-styles/tests/mol-pair-lj_cut_coul_msm_dielectric.yaml:skip_tests: gpu single
unittest/force-styles/tests/mol-pair-lj_cut_coul_long_soft.yaml:skip_tests: gpu
unittest/force-styles/tests/atomic-pair-meam_sc.yaml:  if "$(is_active(package,gpu)) > 0.0" then "variable newton_pair index off" else "variable newton_pair index on"
unittest/force-styles/tests/manybody-pair-tersoff_mod.yaml:  if "$(is_active(package,gpu)) > 0.0" then "variable newton_pair index off" else "variable newton_pair index on"
unittest/force-styles/tests/manybody-pair-mliap_snap_chem.yaml:  if "$(is_active(package,gpu)) > 0.0" then "variable newton_pair index off" else "variable newton_pair index on"
unittest/force-styles/tests/kspace-ewald_dipole.yaml:skip_tests: extract gpu single
unittest/force-styles/tests/manybody-pair-rann.yaml:  if "$(is_active(package,gpu)) > 0.0" then "variable newton_pair index off" else "variable newton_pair index on"
unittest/force-styles/tests/mol-pair-lj_class2_coul_table.yaml:skip_tests: gpu
unittest/force-styles/tests/mol-pair-dpd_ext.yaml:skip_tests: gpu intel kokkos_omp single
unittest/force-styles/tests/mol-pair-lj_cut_tip4p_cut.yaml:  if "$(is_active(package,gpu)) > 0.0" then "variable newton_pair index off" else "variable newton_pair index on"
unittest/force-styles/tests/mol-pair-tip4p_long_soft.yaml:  if "$(is_active(package,gpu)) > 0.0" then "variable newton_pair index off" else "variable newton_pair index on"
unittest/force-styles/tests/mol-pair-lj_long_tip4p_long.yaml:  if "$(is_active(package,gpu)) > 0.0" then "variable newton_pair index off" else "variable newton_pair index on"
unittest/force-styles/tests/atomic-pair-buck_coul_cut_qeq_point.yaml:skip_tests: gpu intel single
unittest/force-styles/tests/atomic-pair-atm.yaml:  if "$(is_active(package,gpu)) > 0.0" then "variable newton_pair index off" else "variable newton_pair index on"
unittest/force-styles/tests/manybody-pair-tersoff_table.yaml:  if "$(is_active(package,gpu)) > 0.0" then "variable newton_pair index off" else "variable newton_pair index on"
unittest/force-styles/tests/manybody-pair-tersoff_zbl_shift.yaml:skip_tests: gpu intel kokkos_omp
unittest/force-styles/tests/manybody-pair-sw_twobody.yaml:  if "$(is_active(package,gpu)) > 0.0" then "variable newton_pair index off" else "variable newton_pair index on"
unittest/force-styles/tests/mol-pair-lj_cut_coul_table.yaml:skip_tests: gpu
unittest/force-styles/tests/kspace-ewald_disp_dipole.yaml:skip_tests: extract gpu single
unittest/force-styles/tests/manybody-pair-sw-multi.yaml:  if "$(is_active(package,gpu)) > 0.0" then "variable newton_pair index off" else "variable newton_pair index on"
unittest/force-styles/tests/manybody-pair-polymorphic_tersoff.yaml:  if "$(is_active(package,gpu)) > 0.0" then "variable newton_pair index off" else "variable newton_pair index on"
unittest/force-styles/tests/mol-pair-lj_expand_coul_table.yaml:skip_tests: gpu
unittest/force-styles/tests/mol-pair-born_coul_table_cs.yaml:skip_tests: gpu
unittest/force-styles/tests/atomic-pair-buck_coul_cut_qeq_shielded.yaml:skip_tests: gpu intel single
unittest/force-styles/tests/mol-pair-lj_spica_coul_msm_table.yaml:skip_tests: gpu
unittest/force-styles/tests/manybody-pair-tersoff_shift.yaml:skip_tests: gpu intel kokkos_omp
unittest/force-styles/tests/kspace-pppm_nozforce.yaml:skip_tests: gpu
unittest/force-styles/tests/kspace-scafacos_fmm.yaml:skip_tests: extract gpu intel omp opt single
unittest/force-styles/tests/manybody-pair-comb3.yaml:  if "$(is_active(package,gpu)) > 0.0" then "variable newton_pair index off" else "variable newton_pair index on"
unittest/force-styles/tests/kspace-pppm_ad.yaml:skip_tests: gpu kokkos_omp
unittest/force-styles/tests/kspace-scafacos_direct.yaml:skip_tests: extract gpu intel omp opt single
unittest/force-styles/tests/kspace-ewald_disp.yaml:skip_tests: extract gpu single
unittest/force-styles/tests/manybody-pair-gw_zbl.yaml:  if "$(is_active(package,gpu)) > 0.0" then "variable newton_pair index off" else "variable newton_pair index on"
unittest/force-styles/tests/mol-pair-coul_streitz_long.yaml:  if "$(is_active(package,gpu)) > 0.0" then "variable newton_pair index off" else "variable newton_pair index on"
unittest/force-styles/tests/mol-pair-e3b.yaml:  if "$(is_active(package,gpu)) > 0.0" then "variable newton_pair index off" else "variable newton_pair index on"
unittest/force-styles/tests/atomic-pair-meam_spline.yaml:  if "$(is_active(package,gpu)) > 0.0" then "variable newton_pair index off" else "variable newton_pair index on"
unittest/force-styles/tests/mol-pair-lj_cut_coul_msm.yaml:skip_tests: gpu
unittest/force-styles/tests/atomic-pair-meam_sw_spline.yaml:  if "$(is_active(package,gpu)) > 0.0" then "variable newton_pair index off" else "variable newton_pair index on"
unittest/force-styles/tests/manybody-pair-meam.yaml:  if "$(is_active(package,gpu)) > 0.0" then "variable newton_pair index off" else "variable newton_pair index on"
unittest/force-styles/tests/mol-pair-lj_cut_coul_cut_soft.yaml:skip_tests: gpu
unittest/force-styles/tests/mol-pair-buck_coul_msm_table.yaml:skip_tests: gpu
unittest/force-styles/tests/manybody-pair-snap.yaml:  if "$(is_active(package,gpu)) > 0.0" then "variable newton_pair index off" else "variable newton_pair index on"
unittest/force-styles/tests/manybody-pair-airebo_m.yaml:  if "$(is_active(package,gpu)) > 0.0" then "variable newton_pair index off" else "variable newton_pair index on"
unittest/force-styles/tests/manybody-pair-sw_twothree.yaml:skip_tests: gpu
unittest/force-styles/tests/manybody-pair-extep.yaml:  if "$(is_active(package,gpu)) > 0.0" then "variable newton_pair index off" else "variable newton_pair index on"
unittest/force-styles/tests/manybody-pair-vashishta.yaml:  if "$(is_active(package,gpu)) > 0.0" then "variable newton_pair index off" else "variable newton_pair index on"
unittest/force-styles/tests/manybody-pair-mliap_snap_linear.yaml:  if "$(is_active(package,gpu)) > 0.0" then "variable newton_pair index off" else "variable newton_pair index on"
unittest/force-styles/tests/mol-pair-lj_sf_dipole_sf.yaml:skip_tests: gpu single
unittest/force-styles/tests/manybody-pair-sw.yaml:  if "$(is_active(package,gpu)) > 0.0" then "variable newton_pair index off" else "variable newton_pair index on"
unittest/force-styles/tests/atomic-pair-meam_ms.yaml:  if "$(is_active(package,gpu)) > 0.0" then "variable newton_pair index off" else "variable newton_pair index on"
unittest/force-styles/tests/mol-pair-coul_msm_table.yaml:skip_tests: gpu
unittest/force-styles/tests/mol-pair-tip4p_table.yaml:  if "$(is_active(package,gpu)) > 0.0" then "variable newton_pair index off" else "variable newton_pair index on"
unittest/force-styles/tests/mol-pair-lj_table_coul_table.yaml:skip_tests: gpu
unittest/force-styles/tests/mol-pair-lj_charmm_coul_msm_table.yaml:skip_tests: gpu
unittest/force-styles/tests/manybody-pair-airebo_m00.yaml:  if "$(is_active(package,gpu)) > 0.0" then "variable newton_pair index off" else "variable newton_pair index on"
unittest/force-styles/tests/manybody-pair-snap_chem.yaml:  if "$(is_active(package,gpu)) > 0.0" then "variable newton_pair index off" else "variable newton_pair index on"
unittest/force-styles/tests/manybody-pair-quip-sw.yaml:  if "$(is_active(package,gpu)) > 0.0" then "variable newton_pair index off" else "variable newton_pair index on"
unittest/force-styles/tests/manybody-pair-sw-mod-multi.yaml:  if "$(is_active(package,gpu)) > 0.0" then "variable newton_pair index off" else "variable newton_pair index on"
unittest/force-styles/tests/manybody-pair-polymorphic_sw.yaml:  if "$(is_active(package,gpu)) > 0.0" then "variable newton_pair index off" else "variable newton_pair index on"
unittest/force-styles/tests/kspace-scafacos_p2nfft.yaml:skip_tests: extract gpu intel omp opt single
unittest/force-styles/tests/mol-pair-lj_cut_tip4p_table.yaml:skip_tests: gpu
unittest/force-styles/tests/mol-pair-lj_cut_tip4p_table.yaml:  if "$(is_active(package,gpu)) > 0.0" then "variable newton_pair index off" else "variable newton_pair index on"
unittest/force-styles/tests/mol-pair-tip4p_long.yaml:  if "$(is_active(package,gpu)) > 0.0" then "variable newton_pair index off" else "variable newton_pair index on"
unittest/force-styles/tests/manybody-pair-tersoff_zbl.yaml:  if "$(is_active(package,gpu)) > 0.0" then "variable newton_pair index off" else "variable newton_pair index on"
unittest/force-styles/tests/manybody-pair_edip_multi.yaml:  if "$(is_active(package,gpu)) > 0.0" then "variable newton_pair index off" else "variable newton_pair index on"
unittest/force-styles/tests/mol-pair-tip4p_cut.yaml:  if "$(is_active(package,gpu)) > 0.0" then "variable newton_pair index off" else "variable newton_pair index on"
unittest/force-styles/tests/manybody-pair-rebomos.yaml:  if "$(is_active(package,gpu)) > 0.0" then "variable newton_pair index off" else "variable newton_pair index on"
unittest/force-styles/tests/mol-pair-lj_charmm_coul_table.yaml:skip_tests: gpu
unittest/force-styles/tests/mol-pair-coul_ctip.yaml:  if "$(is_active(package,gpu)) > 0.0" then "variable newton_pair index off" else "variable newton_pair index on"
unittest/force-styles/tests/mol-pair-lj_cut_dipole_long.yaml:skip_tests: gpu
unittest/force-styles/tests/atomic-pair-meam.yaml:  if "$(is_active(package,gpu)) > 0.0" then "variable newton_pair index off" else "variable newton_pair index on"
unittest/force-styles/tests/mol-pair-lj_cut_coul_msm_table.yaml:skip_tests: gpu
unittest/force-styles/tests/mol-pair-nm_cut_coul_table.yaml:skip_tests: gpu
unittest/force-styles/tests/kspace-ewald_conp_charge.yaml:skip_tests: gpu kokkos_omp omp
unittest/force-styles/tests/manybody-pair-airebo.yaml:  if "$(is_active(package,gpu)) > 0.0" then "variable newton_pair index off" else "variable newton_pair index on"
unittest/force-styles/tests/mol-pair-buck_table_coul_table.yaml:skip_tests: gpu
unittest/force-styles/tests/kspace-pppm_tilted.yaml:skip_tests: gpu
unittest/force-styles/tests/kspace-pppm_dipole.yaml:skip_tests: extract gpu single
unittest/force-styles/tests/atomic-pair-meam_2nn.yaml:  if "$(is_active(package,gpu)) > 0.0" then "variable newton_pair index off" else "variable newton_pair index on"
unittest/force-styles/tests/atomic-pair-polymorphic_eam.yaml:  if "$(is_active(package,gpu)) > 0.0" then "variable newton_pair index off" else "variable newton_pair index on"
unittest/force-styles/tests/mol-pair-hybrid-scaled.yaml:skip_tests: gpu intel kokkos_omp omp
unittest/force-styles/tests/mol-pair-lj_table_tip4p_table.yaml:  if "$(is_active(package,gpu)) > 0.0" then "variable newton_pair index off" else "variable newton_pair index on"
unittest/force-styles/tests/mol-pair-lj_cut_tip4p_long.yaml:  if "$(is_active(package,gpu)) > 0.0" then "variable newton_pair index off" else "variable newton_pair index on"
unittest/force-styles/tests/mol-pair-coul_streitz_wolf.yaml:  if "$(is_active(package,gpu)) > 0.0" then "variable newton_pair index off" else "variable newton_pair index on"
unittest/force-styles/tests/mol-pair-dpd_tstat.yaml:skip_tests: gpu intel kokkos_omp single
unittest/force-styles/tests/mol-pair-coul_table.yaml:skip_tests: gpu
unittest/force-styles/tests/mol-pair-lj_spica_coul_table.yaml:skip_tests: gpu
unittest/force-styles/tests/mol-pair-dpd_ext_tstat.yaml:skip_tests: gpu intel kokkos_omp single
unittest/force-styles/tests/manybody-pair-comb.yaml:  if "$(is_active(package,gpu)) > 0.0" then "variable newton_pair index off" else "variable newton_pair index on"
unittest/force-styles/tests/atomic-pair-edip.yaml:  if "$(is_active(package,gpu)) > 0.0" then "variable newton_pair index off" else "variable newton_pair index on"
unittest/force-styles/test_pair_style.cpp:    // if KOKKOS has GPU support enabled, it *must* be used. We cannot test OpenMP only.
unittest/force-styles/test_pair_style.cpp:    if (Info::has_accelerator_feature("KOKKOS", "api", "cuda") ||
unittest/force-styles/test_pair_style.cpp:TEST(PairStyle, gpu)
unittest/force-styles/test_pair_style.cpp:    if (!LAMMPS::is_installed_pkg("GPU")) GTEST_SKIP();
unittest/force-styles/test_pair_style.cpp:    if (!Info::has_gpu_device()) GTEST_SKIP();
unittest/force-styles/test_pair_style.cpp:    // when testing PPPM styles with GPUs and GPU support is compiled with single precision
unittest/force-styles/test_pair_style.cpp:        (Info::has_accelerator_feature("GPU", "precision", "single")) &&
unittest/force-styles/test_pair_style.cpp:                                 "screen",    "-nocite", "-sf",  "gpu"};
unittest/force-styles/test_pair_style.cpp:                                 "gpu",       "-pk",  "gpu",  "0",     "neigh",  "no"};
unittest/force-styles/test_pair_style.cpp:    // cannot use GPU neighbor list with hybrid pair style (yet)
unittest/force-styles/test_pair_style.cpp:        std::cerr << "One or more prerequisite styles with /gpu suffix\n"
unittest/force-styles/test_pair_style.cpp:    // relax error for GPU package depending on precision setting
unittest/force-styles/test_pair_style.cpp:    if (Info::has_accelerator_feature("GPU", "precision", "double"))
unittest/force-styles/test_pair_style.cpp:    else if (Info::has_accelerator_feature("GPU", "precision", "mixed"))
unittest/force-styles/test_pair_style.cpp:        Info::has_accelerator_feature("GPU", "precision", "double"))
unittest/force-styles/test_improper_style.cpp:    // if KOKKOS has GPU support enabled, it *must* be used. We cannot test OpenMP only.
unittest/force-styles/test_improper_style.cpp:    if (Info::has_accelerator_feature("KOKKOS", "api", "cuda") ||
unittest/force-styles/test_fix_timestep.cpp:    // if KOKKOS has GPU support enabled, it *must* be used. We cannot test OpenMP only.
unittest/force-styles/test_fix_timestep.cpp:    if (Info::has_accelerator_feature("KOKKOS", "api", "cuda") ||
unittest/force-styles/check_tests.py:gpu = re.compile("(.+)/gpu$")
unittest/force-styles/check_tests.py:        styles[name]['gpu'] += info['gpu']
unittest/force-styles/check_tests.py:                if style in ['lj/sdk', 'lj/sdk/coul/long', 'lj/sdk/coul/msm', 'sdk', 'lj/sdk/gpu',
unittest/force-styles/check_tests.py:                             'lj/sdk/coul/long/gpu', 'lj/sdk/omp', 'lj/sdk/coul/long/omp', 'sdk/omp',
unittest/force-styles/check_tests.py:                info = { 'kokkos':  0, 'gpu':     0, 'intel':   0, \
unittest/force-styles/check_tests.py:                suffix = gpu.match(style)
unittest/force-styles/check_tests.py:                    info['gpu'] = 1
unittest/force-styles/test_bond_style.cpp:    // if KOKKOS has GPU support enabled, it *must* be used. We cannot test OpenMP only.
unittest/force-styles/test_bond_style.cpp:    if (Info::has_accelerator_feature("KOKKOS", "api", "cuda") ||
unittest/force-styles/test_dihedral_style.cpp:    // if KOKKOS has GPU support enabled, it *must* be used. We cannot test OpenMP only.
unittest/force-styles/test_dihedral_style.cpp:    if (Info::has_accelerator_feature("KOKKOS", "api", "cuda") ||
lib/kokkos/Makefile.targets:ifeq ($(KOKKOS_INTERNAL_USE_CUDA), 1)
lib/kokkos/Makefile.targets:Kokkos_Cuda_Instance.o: $(KOKKOS_CPP_DEPENDS) $(KOKKOS_PATH)/core/src/Cuda/Kokkos_Cuda_Instance.cpp
lib/kokkos/Makefile.targets:	$(CXX) $(KOKKOS_CPPFLAGS) $(KOKKOS_CXXFLAGS) $(CXXFLAGS) -c $(KOKKOS_PATH)/core/src/Cuda/Kokkos_Cuda_Instance.cpp
lib/kokkos/Makefile.targets:Kokkos_CudaSpace.o: $(KOKKOS_CPP_DEPENDS) $(KOKKOS_PATH)/core/src/Cuda/Kokkos_CudaSpace.cpp
lib/kokkos/Makefile.targets:	$(CXX) $(KOKKOS_CPPFLAGS) $(KOKKOS_CXXFLAGS) $(CXXFLAGS) -c $(KOKKOS_PATH)/core/src/Cuda/Kokkos_CudaSpace.cpp
lib/kokkos/Makefile.targets:Kokkos_Cuda_Task.o: $(KOKKOS_CPP_DEPENDS) $(KOKKOS_PATH)/core/src/Cuda/Kokkos_Cuda_Task.cpp
lib/kokkos/Makefile.targets:	$(CXX) $(KOKKOS_CPPFLAGS) $(KOKKOS_CXXFLAGS) $(CXXFLAGS) -c $(KOKKOS_PATH)/core/src/Cuda/Kokkos_Cuda_Task.cpp
lib/kokkos/Makefile.targets:Lock_Array_CUDA.o: $(KOKKOS_CPP_DEPENDS) $(KOKKOS_PATH)/tpls/desul/src/Lock_Array_CUDA.cpp
lib/kokkos/Makefile.targets:	$(CXX) $(KOKKOS_CPPFLAGS) $(KOKKOS_CXXFLAGS) $(CXXFLAGS) -c $(KOKKOS_PATH)/tpls/desul/src/Lock_Array_CUDA.cpp
lib/kokkos/Makefile.targets:ifeq ($(KOKKOS_INTERNAL_USE_OPENACC), 1)
lib/kokkos/Makefile.targets:Kokkos_OpenACC.o: $(KOKKOS_CPP_DEPENDS) $(KOKKOS_PATH)/core/src/OpenACC/Kokkos_OpenACC.cpp
lib/kokkos/Makefile.targets:	$(CXX) $(KOKKOS_CPPFLAGS) $(KOKKOS_CXXFLAGS) $(CXXFLAGS) -c $(KOKKOS_PATH)/core/src/OpenACC/Kokkos_OpenACC.cpp
lib/kokkos/Makefile.targets:Kokkos_OpenACCSpace.o: $(KOKKOS_CPP_DEPENDS) $(KOKKOS_PATH)/core/src/OpenACC/Kokkos_OpenACCSpace.cpp
lib/kokkos/Makefile.targets:	$(CXX) $(KOKKOS_CPPFLAGS) $(KOKKOS_CXXFLAGS) $(CXXFLAGS) -c $(KOKKOS_PATH)/core/src/OpenACC/Kokkos_OpenACCSpace.cpp
lib/kokkos/Makefile.targets:Kokkos_OpenACC_Instance.o: $(KOKKOS_CPP_DEPENDS) $(KOKKOS_PATH)/core/src/OpenACC/Kokkos_OpenACC_Instance.cpp
lib/kokkos/Makefile.targets:	$(CXX) $(KOKKOS_CPPFLAGS) $(KOKKOS_CXXFLAGS) $(CXXFLAGS) -c $(KOKKOS_PATH)/core/src/OpenACC/Kokkos_OpenACC_Instance.cpp
lib/kokkos/Makefile.targets:Kokkos_OpenACC_SharedAllocationRecord.o: $(KOKKOS_CPP_DEPENDS) $(KOKKOS_PATH)/core/src/OpenACC/Kokkos_OpenACC_SharedAllocationRecord.cpp
lib/kokkos/Makefile.targets:	$(CXX) $(KOKKOS_CPPFLAGS) $(KOKKOS_CXXFLAGS) $(CXXFLAGS) -c $(KOKKOS_PATH)/core/src/OpenACC/Kokkos_OpenACC_SharedAllocationRecord.cpp
lib/kokkos/core/perf_test/test_sharedSpace.cpp:#if defined KOKKOS_ENABLE_CUDA
lib/kokkos/core/perf_test/test_sharedSpace.cpp:  return Kokkos::Cuda{}.cuda_device_prop().totalGlobalMem;
lib/kokkos/core/perf_test/test_sharedSpace.cpp:    "The sharedMemory test is only defined for Kokkos::Cuda, Kokkos::HIP, and Kokkos::SYCL"
lib/kokkos/core/perf_test/test_sharedSpace.cpp:  // WARMUP GPU
lib/kokkos/core/perf_test/test_sharedSpace.cpp:      numWarmupRepetitions);  // warming up gpu
lib/kokkos/core/perf_test/test_sharedSpace.cpp:    // WARMUP GPU
lib/kokkos/core/perf_test/test_sharedSpace.cpp:        numWarmupRepetitions);  // warming up gpu without touching the
lib/kokkos/core/perf_test/PerfTest_ViewResize_Raw.cpp:#if defined(KOKKOS_ENABLE_CUDA_LAMBDA) || !defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/core/perf_test/PerfTest_ViewResize_8.cpp:// FIXME_SYCL Avoid running out of resources on the CUDA GPU used in the CI
lib/kokkos/core/perf_test/PerfTest_ViewCopy_Raw.cpp:#if defined(KOKKOS_ENABLE_CUDA_LAMBDA) || !defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/core/perf_test/PerfTestGramSchmidt.cpp:// FIXME_SYCL SYCL+Cuda reports "an illegal memory access was encountered"
lib/kokkos/core/perf_test/PerfTestGramSchmidt.cpp:#if defined(KOKKOS_ENABLE_SYCL) && defined(KOKKOS_IMPL_ARCH_NVIDIA_GPU)
lib/kokkos/core/perf_test/PerfTest_ViewFill_Raw.cpp:#if defined(KOKKOS_ENABLE_CUDA_LAMBDA) || !defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/core/perf_test/Makefile:ifneq (,$(findstring Cuda,$(KOKKOS_DEVICES)))
lib/kokkos/core/perf_test/Makefile:  KOKKOS_CUDA_OPTIONS=enable_lambda
lib/kokkos/core/perf_test/CMakeLists.txt:# FIXME_OPENMPTARGET - the NVIDIA HPC compiler nvc++ in the OpenMPTarget backend does not pass the perf_tests.
lib/kokkos/core/perf_test/CMakeLists.txt:# FIXME_OPENACC - temporarily disabled due to unimplemented features
lib/kokkos/core/perf_test/CMakeLists.txt:IF ((KOKKOS_ENABLE_OPENMPTARGET OR KOKKOS_ENABLE_OPENACC) AND KOKKOS_CXX_COMPILER_ID STREQUAL NVHPC)
lib/kokkos/core/perf_test/CMakeLists.txt:IF (KOKKOS_ENABLE_OPENACC AND KOKKOS_CXX_COMPILER_ID STREQUAL Clang)
lib/kokkos/core/perf_test/CMakeLists.txt:  IF(KOKKOS_ENABLE_CUDA OR KOKKOS_ENABLE_HIP OR KOKKOS_ENABLE_SYCL)
lib/kokkos/core/perf_test/CMakeLists.txt:IF(NOT KOKKOS_ENABLE_CUDA OR KOKKOS_ENABLE_CUDA_LAMBDA)
lib/kokkos/core/perf_test/PerfTest_ExecSpacePartitioning.cpp:#ifdef KOKKOS_ENABLE_CUDA
lib/kokkos/core/perf_test/PerfTest_ExecSpacePartitioning.cpp:bool is_overlapping<Kokkos::Cuda>(const Kokkos::Cuda&) {
lib/kokkos/core/perf_test/PerfTest_ExecSpacePartitioning.cpp:  auto local_rank_str = std::getenv("CUDA_LAUNCH_BLOCKING");
lib/kokkos/core/perf_test/PerfTest_ExecSpacePartitioning.cpp:#if defined(KOKKOS_ENABLE_SYCL) && defined(KOKKOS_ARCH_INTEL_GPU)
lib/kokkos/core/perf_test/PerfTestDriver.hpp:#if !defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/core/perf_test/PerfTestDriver.hpp:#if defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/core/perf_test/PerfTestDriver.hpp:          // Note: Product of tile sizes must be < 1024 for Cuda
lib/kokkos/core/perf_test/PerfTestDriver.hpp:            printf("  Exceeded Cuda tile limits; onto next range set\n\n");
lib/kokkos/core/perf_test/PerfTestDriver.hpp:#if !defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/core/perf_test/PerfTestDriver.hpp:    // only for non-Cuda, else tile too long seconds_2 = collapse<2>-style
lib/kokkos/core/perf_test/PerfTestDriver.hpp:#if !defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/core/unit_test/TestView_64bit.hpp:  // We are running out of device memory on Intel GPUs
lib/kokkos/core/unit_test/TestView_64bit.hpp:// We are running out of device memory on Intel GPUs
lib/kokkos/core/unit_test/TestGraph.hpp:#if !defined(KOKKOS_ENABLE_CUDA) && \
lib/kokkos/core/unit_test/TestGraph.hpp:    !defined(KOKKOS_ENABLE_HIP)  // FIXME_CUDA FIXME_HIP
lib/kokkos/core/unit_test/TestGraph.hpp:// These fences are only necessary because of the weirdness of how CUDA
lib/kokkos/core/unit_test/TestGraph.hpp:#if defined(KOKKOS_ENABLE_CUDA) && defined(KOKKOS_ENABLE_CUDA_UVM) && \
lib/kokkos/core/unit_test/TestGraph.hpp:// These fences are only necessary because of the weirdness of how CUDA
lib/kokkos/core/unit_test/TestGraph.hpp:#if defined(KOKKOS_ENABLE_CUDA) && defined(KOKKOS_ENABLE_CUDA_UVM) && \
lib/kokkos/core/unit_test/TestGraph.hpp:  if constexpr (std::is_same_v<TEST_EXECSPACE, Kokkos::Cuda>) Kokkos::fence();
lib/kokkos/core/unit_test/TestUniqueToken.hpp:    // FIXME_SYCL wrong result on NVIDIA GPUs but correct on host and Intel GPUs
lib/kokkos/core/unit_test/TestUniqueToken.hpp:      // FIXME_SYCL The number of workgroups on CUDA devices can not be larger
lib/kokkos/core/unit_test/TestExecSpacePartitioning.hpp:#ifdef KOKKOS_ENABLE_CUDA
lib/kokkos/core/unit_test/TestExecSpacePartitioning.hpp:  if constexpr (std::is_same_v<ExecSpace, Kokkos::Cuda>) {
lib/kokkos/core/unit_test/TestExecSpacePartitioning.hpp:    ASSERT_NE(exec1.cuda_stream(), exec2.cuda_stream());
lib/kokkos/core/unit_test/TestAtomicOperations.hpp:#if defined(KOKKOS_ENABLE_OPENACC) && defined(KOKKOS_COMPILER_NVHPC)
lib/kokkos/core/unit_test/TestAtomicOperations.hpp:    // FIXME_NVHPC: atomic-fetch-shift operation fails due to NVHPC OpenACC
lib/kokkos/core/unit_test/TestAtomicOperations.hpp:    // compiler bugs, which are reported to NVIDIA.
lib/kokkos/core/unit_test/TestAtomicOperations.hpp:#if defined(KOKKOS_ENABLE_OPENACC) && defined(KOKKOS_COMPILER_NVHPC)
lib/kokkos/core/unit_test/TestCompilerMacros.cpp:#if defined(KOKKOS_ENABLE_CUDA) && !defined(KOKKOS_ENABLE_CUDA_LAMBDA)
lib/kokkos/core/unit_test/TestViewCtorPropEmbeddedDim.hpp:  // Cuda 7.0 has issues with using a lambda in parallel_for to initialize the
lib/kokkos/core/unit_test/sycl/TestSYCL_InterOp_StreamsMultiGPU.cpp:#include <TestMultiGPU.hpp>
lib/kokkos/core/unit_test/sycl/TestSYCL_InterOp_StreamsMultiGPU.cpp:  std::vector<sycl::device> gpu_devices =
lib/kokkos/core/unit_test/sycl/TestSYCL_InterOp_StreamsMultiGPU.cpp:      sycl::device::get_devices(sycl::info::device_type::gpu);
lib/kokkos/core/unit_test/sycl/TestSYCL_InterOp_StreamsMultiGPU.cpp:      sycl::queue{gpu_devices.front(), sycl::property::queue::in_order()});
lib/kokkos/core/unit_test/sycl/TestSYCL_InterOp_StreamsMultiGPU.cpp:      sycl::queue{gpu_devices.back(), sycl::property::queue::in_order()});
lib/kokkos/core/unit_test/sycl/TestSYCL_InterOp_StreamsMultiGPU.cpp:TEST(sycl_multi_gpu, managed_views) {
lib/kokkos/core/unit_test/sycl/TestSYCL_InterOp_StreamsMultiGPU.cpp:TEST(sycl_multi_gpu, unmanaged_views) {
lib/kokkos/core/unit_test/sycl/TestSYCL_InterOp_StreamsMultiGPU.cpp:TEST(sycl_multi_gpu, scratch_space) {
lib/kokkos/core/unit_test/sycl/TestSYCL_InterOp_Streams.cpp:  // This kernel corresponds to "offset_streams" in the HIP and CUDA tests.
lib/kokkos/core/unit_test/TestOccupancyControlTrait.hpp:// FIXME_MSVC_WITH_CUDA
lib/kokkos/core/unit_test/TestOccupancyControlTrait.hpp:// This test doesn't compile with CUDA on Windows
lib/kokkos/core/unit_test/TestOccupancyControlTrait.hpp:#if !(defined(_WIN32) && defined(KOKKOS_ENABLE_CUDA))
lib/kokkos/core/unit_test/TestReducers.hpp:#ifdef KOKKOS_ENABLE_OPENACC
lib/kokkos/core/unit_test/TestReducers.hpp:    if constexpr (std::is_same_v<ExecSpace, Kokkos::Experimental::OpenACC> &&
lib/kokkos/core/unit_test/TestReducers.hpp:      return;  // FIXME_OPENACC
lib/kokkos/core/unit_test/TestReducers.hpp:#if !defined(KOKKOS_ENABLE_OPENACC)
lib/kokkos/core/unit_test/TestReducers.hpp:    // FIXME_OPENACC - OpenACC (V3.3) does not support custom reductions.
lib/kokkos/core/unit_test/TestReducers.hpp:#if !defined(KOKKOS_ENABLE_OPENACC)
lib/kokkos/core/unit_test/TestReducers.hpp:    // FIXME_OPENACC - OpenACC (V3.3) does not support custom reductions.
lib/kokkos/core/unit_test/TestReducers.hpp:// FIXME_OPENACC - OpenACC (V3.3) does not support custom reductions.
lib/kokkos/core/unit_test/TestReducers.hpp:#if !defined(KOKKOS_ENABLE_OPENACC)
lib/kokkos/core/unit_test/TestReducers.hpp:#if !defined(KOKKOS_ENABLE_OPENACC)
lib/kokkos/core/unit_test/TestReducers.hpp:    // FIXME_OPENACC - OpenACC (V3.3) does not support custom reductions.
lib/kokkos/core/unit_test/TestReducers.hpp:#if defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/core/unit_test/TestReducers.hpp:    if (!std::is_same_v<ExecSpace, Kokkos::Cuda>)
lib/kokkos/core/unit_test/TestReducers.hpp:#if !defined(KOKKOS_ENABLE_OPENACC)
lib/kokkos/core/unit_test/TestReducers.hpp:    // FIXME_OPENACC - OpenACC (V3.3) does not support custom reductions.
lib/kokkos/core/unit_test/TestReducers.hpp:#if defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/core/unit_test/TestReducers.hpp:    if (!std::is_same_v<ExecSpace, Kokkos::Cuda>)
lib/kokkos/core/unit_test/TestReducers.hpp:// FIXME_OPENACC - OpenACC (V3.3) does not support custom reductions.
lib/kokkos/core/unit_test/TestReducers.hpp:#if !defined(KOKKOS_ENABLE_OPENACC)
lib/kokkos/core/unit_test/TestPrintf.hpp:#if !(defined(KOKKOS_ENABLE_OPENMPTARGET) && defined(KOKKOS_ARCH_INTEL_GPU))
lib/kokkos/core/unit_test/TestJoinBackwardCompatibility.hpp:#ifndef KOKKOS_ENABLE_OPENACC  // FIXME_OPENACC - temporarily disabled due to
lib/kokkos/core/unit_test/TestSpaceAwareAccessorAccessViolation.hpp:#if defined(KOKKOS_ENABLE_OPENACC)  // FIXME_OPENACC
lib/kokkos/core/unit_test/TestSpaceAwareAccessorAccessViolation.hpp:  if (std::is_same<ExecutionSpace, Kokkos::Experimental::OpenACC>::value) {
lib/kokkos/core/unit_test/TestSpaceAwareAccessorAccessViolation.hpp:    GTEST_SKIP() << "skipping because OpenACC backend is currently not "
lib/kokkos/core/unit_test/TestAbort.hpp:#elif defined(KOKKOS_ENABLE_OPENACC)  // FIXME_OPENACC
lib/kokkos/core/unit_test/TestAbort.hpp:  if (std::is_same<ExecutionSpace, Kokkos::Experimental::OpenACC>::value) {
lib/kokkos/core/unit_test/TestViewMemoryAccessViolation.hpp:#if defined(KOKKOS_ENABLE_OPENACC)  // FIXME_OPENACC
lib/kokkos/core/unit_test/TestViewMemoryAccessViolation.hpp:  if (std::is_same<ExecutionSpace, Kokkos::Experimental::OpenACC>::value) {
lib/kokkos/core/unit_test/TestViewMemoryAccessViolation.hpp:    GTEST_SKIP() << "skipping because OpenACC backend is currently not "
lib/kokkos/core/unit_test/TestTeamScratch.hpp:  // initialization of the maximum number of scratch pad indices in the Cuda
lib/kokkos/core/unit_test/TestAtomicOperations_shared.hpp:// FIXME_SYCL This doesn't work yet for SYCL+CUDA
lib/kokkos/core/unit_test/TestAtomicOperations_shared.hpp:#if !defined(KOKKOS_ENABLE_SYCL) || defined(KOKKOS_ARCH_INTEL_GPU)
lib/kokkos/core/unit_test/TestTeamPolicyCTAD.cpp:  // Workaround for nvc++ (CUDA-11.7-NVHPC) ignoring [[maybe_unused]] on
lib/kokkos/core/unit_test/TestTeamPolicyCTAD.cpp:  // Workaround for HIP-ROCm-5.2 warning about was declared but never referenced
lib/kokkos/core/unit_test/incremental/Test12a_ThreadScratch.hpp:    // FIXME_SYCL This deadlocks in the subgroup_barrier when running on CUDA
lib/kokkos/core/unit_test/incremental/Test12a_ThreadScratch.hpp:#ifdef KOKKOS_ENABLE_OPENACC  // FIXME_OPENACC
lib/kokkos/core/unit_test/incremental/Test12a_ThreadScratch.hpp:                  "OpenACC backend";
lib/kokkos/core/unit_test/incremental/Test14_MDRangeReduce.hpp:// FIXME_OPENACC: scalar reduction variable on the device is not yet supported.
lib/kokkos/core/unit_test/incremental/Test14_MDRangeReduce.hpp:#if !defined(KOKKOS_ENABLE_OPENACC)
lib/kokkos/core/unit_test/incremental/Test14_MDRangeReduce.hpp:// FIXME_OPENACC: scalar reduction variable on the device is not yet supported.
lib/kokkos/core/unit_test/incremental/Test14_MDRangeReduce.hpp:#if !defined(KOKKOS_ENABLE_OPENACC)
lib/kokkos/core/unit_test/incremental/Test14_MDRangeReduce.hpp:// FIXME_OPENACC: custom reductions are not yet supported in the
lib/kokkos/core/unit_test/incremental/Test14_MDRangeReduce.hpp:// OpenACC backend.
lib/kokkos/core/unit_test/incremental/Test14_MDRangeReduce.hpp:#if !defined(KOKKOS_ENABLE_OPENACC)
lib/kokkos/core/unit_test/incremental/Test14_MDRangeReduce.hpp:// FIXME_OPENACC: custom reductions are not yet supported in the
lib/kokkos/core/unit_test/incremental/Test14_MDRangeReduce.hpp:// OpenACC backend.
lib/kokkos/core/unit_test/incremental/Test14_MDRangeReduce.hpp:#if !defined(KOKKOS_ENABLE_OPENACC)
lib/kokkos/core/unit_test/incremental/Test12b_TeamScratch.hpp:      // when running on CUDA devices.
lib/kokkos/core/unit_test/incremental/Test12b_TeamScratch.hpp:#ifdef KOKKOS_ENABLE_OPENACC  // FIXME_OPENACC
lib/kokkos/core/unit_test/incremental/Test12b_TeamScratch.hpp:                  "OpenACC backend";
lib/kokkos/core/unit_test/TestMathematicalConstants.hpp:    defined(KOKKOS_ENABLE_OPENACC)
lib/kokkos/core/unit_test/TestMathematicalConstants.hpp:#if defined(KOKKOS_ENABLE_CUDA) || defined(KOKKOS_ENABLE_HIP) ||          \
lib/kokkos/core/unit_test/TestMathematicalConstants.hpp:    defined(KOKKOS_ENABLE_OPENACC)
lib/kokkos/core/unit_test/TestRangePolicyRequire.hpp:#if !defined(KOKKOS_ENABLE_CUDA) && !defined(KOKKOS_ENABLE_HIP) && \
lib/kokkos/core/unit_test/TestMDRangeReduce.hpp:#elif defined(KOKKOS_ENABLE_CUDA) && !defined(KOKKOS_ENABLE_CUDA_LAMBDA)
lib/kokkos/core/unit_test/TestMDRangeReduce.hpp:  GTEST_SKIP() << "Skipped ENABLE_CUDA_LAMBDA";
lib/kokkos/core/unit_test/TestConcepts.hpp:  FIXME_OPENACC
lib/kokkos/core/unit_test/TestConcepts.hpp:  OpenACCTeamMember is missing the following method:
lib/kokkos/core/unit_test/TestConcepts.hpp:#if !defined(KOKKOS_ENABLE_OPENMPTARGET) && !defined(KOKKOS_ENABLE_OPENACC)
lib/kokkos/core/unit_test/TestReductions_DeviceView.hpp:// FIXME_SYCL The number of workgroups on CUDA devices can not be larger than
lib/kokkos/core/unit_test/TestMDSpan.hpp:#if !defined(KOKKOS_ENABLE_OPENACC)
lib/kokkos/core/unit_test/TestMDSpan.hpp:#if !defined(KOKKOS_ENABLE_OPENACC)
lib/kokkos/core/unit_test/TestMDSpan.hpp:#if !defined(KOKKOS_ENABLE_OPENACC)
lib/kokkos/core/unit_test/configuration/test-code/test_config_device_list.bash:if [ ! -z "$CUDA_ROOT" ]
lib/kokkos/core/unit_test/configuration/test-code/test_config_device_list.bash:  AccDevices=(${AccDevices[@]} Cuda)
lib/kokkos/core/unit_test/configuration/test-code/Makefile:KOKKOS_CUDA_OPTIONS=enable_lambda
lib/kokkos/core/unit_test/configuration/test-code/Makefile:	rm -f *.o *.cuda *.host
lib/kokkos/core/unit_test/configuration/test-code/test_config_options_list.bash:CudaOptions=(lambda relocatable_device_code uvm constexpr)
lib/kokkos/core/unit_test/configuration/test-code/test_config_options_list.bash:  CudaOptions=(uvm)
lib/kokkos/core/unit_test/configuration/test-code/test_config_options_list.bash:  if [ ! -z $CudaOptions ]; then 
lib/kokkos/core/unit_test/configuration/test-code/test_config_options_list.bash:    for cuda_option in "${CudaOptions[@]}"
lib/kokkos/core/unit_test/configuration/test-code/test_config_options_list.bash:      cuda_option_up=`echo $cuda_option | tr a-z A-Z`
lib/kokkos/core/unit_test/configuration/test-code/test_config_options_list.bash:      CMAKE_CUDA_OPTION="-DKokkos_ENABLE_CUDA_${cuda_option_up}=ON"
lib/kokkos/core/unit_test/configuration/test-code/test_config_options_list.bash:      cuda_option=${cuda_option/lambda/enable_lambda}
lib/kokkos/core/unit_test/configuration/test-code/test_config_options_list.bash:      cuda_option=${cuda_option/constexpr/enable_constexpr}
lib/kokkos/core/unit_test/configuration/test-code/test_config_options_list.bash:      cuda_option=${cuda_option/relocatable_device_code/rdc}
lib/kokkos/core/unit_test/configuration/test-code/test_config_options_list.bash:      cuda_option=${cuda_option/uvm/force_uvm}
lib/kokkos/core/unit_test/configuration/test-code/test_config_options_list.bash:      ${SRC_DIR}/test_config_run.bash "$MakeDevices" "$CMakeDevices" "$MakeArch" "$CMakeArch" "KOKKOS_OPTIONS=$option KOKKOS_CUDA_OPTIONS=$cuda_option" "$CMAKE_OPTION $CMAKE_CUDA_OPTION"
lib/kokkos/core/unit_test/TestRange.hpp:#if !defined(KOKKOS_ENABLE_CUDA) && !defined(KOKKOS_ENABLE_HIP) && \
lib/kokkos/core/unit_test/TestTeamMDRange.hpp:// If KOKKOS_ENABLE_CUDA_LAMBDA is off, extended lambdas used in parallel_for
lib/kokkos/core/unit_test/TestTeamMDRange.hpp:#if !defined(KOKKOS_ENABLE_CUDA) || defined(KOKKOS_ENABLE_CUDA_LAMBDA)
lib/kokkos/core/unit_test/TestTeamMDRange.hpp:// FIXME_SYCL sycl::group_barrier doesn't work correctly for non-Intel GPUs
lib/kokkos/core/unit_test/TestTeamMDRange.hpp:#if defined(KOKKOS_ENABLE_SYCL) && !defined(KOKKOS_ARCH_INTEL_GPU)
lib/kokkos/core/unit_test/TestTeamMDRange.hpp:// FIXME_SYCL sycl::group_barrier doesn't work correctly for non-Intel GPUs
lib/kokkos/core/unit_test/TestTeamMDRange.hpp:#if defined(KOKKOS_ENABLE_SYCL) && !defined(KOKKOS_ARCH_INTEL_GPU)
lib/kokkos/core/unit_test/hip/TestHIP_InterOp_Streams.cpp:// The difference with the CUDA tests are: raw HIP vs raw CUDA and no launch
lib/kokkos/core/unit_test/hip/TestHIP_ScanUnit.cpp:    __attribute__((amdgpu_flat_work_group_size(1, 1024))) {
lib/kokkos/core/unit_test/Makefile:vpath %.cpp ${KOKKOS_PATH}/core/unit_test/cuda
lib/kokkos/core/unit_test/Makefile:ifneq (,$(findstring Cuda,$(KOKKOS_DEVICES)))
lib/kokkos/core/unit_test/Makefile:GPU_SPACE_TESTS = SharedAlloc ViewAPI_a ViewAPI_b ViewAPI_c ViewAPI_d ViewAPI_e ViewCopy_a ViewCopy_b ViewCopy_c ViewMapping_a ViewMapping_b ViewMapping_subview
lib/kokkos/core/unit_test/Makefile:KOKKOS_SUBVIEW_DEVICELIST := $(filter-out Cuda, $(KOKKOS_DEVICELIST))
lib/kokkos/core/unit_test/Makefile:ifeq ($(KOKKOS_INTERNAL_USE_CUDA), 1)
lib/kokkos/core/unit_test/Makefile:      $(if $(filter TestCuda_$(test).cpp, $(shell ls TestCuda_$(test).cpp 2>/dev/null)),,\
lib/kokkos/core/unit_test/Makefile:        $(shell echo "$(H)include <TestCudaUVM_Category.hpp>" > TestCuda_$(test).cpp); \
lib/kokkos/core/unit_test/Makefile:        $(shell echo "$(H)include <Test"$(test)".hpp>" >> TestCuda_$(test).cpp); \
lib/kokkos/core/unit_test/Makefile:    GPU_SPACES = CudaHostPinned CudaUVM
lib/kokkos/core/unit_test/Makefile:    tmp := $(foreach space, $(GPU_SPACES), \
lib/kokkos/core/unit_test/Makefile:      tmp2 := $(foreach test, $(GPU_SPACE_TESTS), \
lib/kokkos/core/unit_test/Makefile:    OBJ_CUDA = UnitTestMainInit.o gtest-all.o
lib/kokkos/core/unit_test/Makefile:    OBJ_CUDA += TestCuda_Init.o
lib/kokkos/core/unit_test/Makefile:    OBJ_CUDA += TestCuda_SharedAlloc.o TestCudaUVM_SharedAlloc.o TestCudaHostPinned_SharedAlloc.o
lib/kokkos/core/unit_test/Makefile:    OBJ_CUDA += TestCuda_RangePolicy.o TestCuda_RangePolicyRequire.o
lib/kokkos/core/unit_test/Makefile:    OBJ_CUDA += TestCuda_ViewAPI_a.o TestCuda_ViewAPI_b.o TestCuda_ViewAPI_c.o TestCuda_ViewAPI_d.o TestCuda_ViewAPI_e.o TestCuda_ViewCopy_a.o TestCuda_ViewCopy_b.o TestCuda_ViewCopy_c.o
lib/kokkos/core/unit_test/Makefile:    OBJ_CUDA += TestCuda_DeepCopyAlignment.o
lib/kokkos/core/unit_test/Makefile:    OBJ_CUDA += TestCuda_ViewMapping_a.o TestCuda_ViewMapping_b.o TestCuda_ViewMapping_subview.o TestCuda_ViewResize.o TestCuda_ViewLayoutStrideAssignment.o
lib/kokkos/core/unit_test/Makefile:    OBJ_CUDA += TestCudaUVM_ViewAPI_a.o TestCudaUVM_ViewAPI_b.o TestCudaUVM_ViewAPI_c.o TestCudaUVM_ViewAPI_d.o TestCudaUVM_ViewAPI_e.o
lib/kokkos/core/unit_test/Makefile:    OBJ_CUDA += TestCudaUVM_ViewCopy_a.o TestCudaUVM_ViewCopy_b.o TestCudaUVM_ViewCopy_c.o
lib/kokkos/core/unit_test/Makefile:    OBJ_CUDA += TestCudaUVM_ViewMapping_a.o TestCudaUVM_ViewMapping_b.o TestCudaUVM_ViewMapping_subview.o
lib/kokkos/core/unit_test/Makefile:    OBJ_CUDA += TestCudaHostPinned_ViewAPI_a.o TestCudaHostPinned_ViewAPI_b.o TestCudaHostPinned_ViewAPI_c.o TestCudaHostPinned_ViewAPI_d.o TestCudaHostPinned_ViewAPI_e.o
lib/kokkos/core/unit_test/Makefile:    OBJ_CUDA += TestCudaHostPinned_ViewCopy_a.o TestCudaHostPinned_ViewCopy_b.o TestCudaHostPinned_ViewCopy_c.o
lib/kokkos/core/unit_test/Makefile:    OBJ_CUDA += TestCudaHostPinned_ViewMapping_a.o TestCudaHostPinned_ViewMapping_b.o TestCudaHostPinned_ViewMapping_subview.o
lib/kokkos/core/unit_test/Makefile:    OBJ_CUDA += TestCuda_View_64bit.o
lib/kokkos/core/unit_test/Makefile:    OBJ_CUDA += TestCuda_ViewOfClass.o
lib/kokkos/core/unit_test/Makefile:    OBJ_CUDA += TestCuda_SubView_a.o TestCuda_SubView_b.o
lib/kokkos/core/unit_test/Makefile:    OBJ_CUDA += TestCuda_SubView_c01.o TestCuda_SubView_c02.o TestCuda_SubView_c03.o
lib/kokkos/core/unit_test/Makefile:    OBJ_CUDA += TestCuda_SubView_c04.o TestCuda_SubView_c05.o TestCuda_SubView_c06.o
lib/kokkos/core/unit_test/Makefile:    OBJ_CUDA += TestCuda_SubView_c07.o TestCuda_SubView_c08.o TestCuda_SubView_c09.o
lib/kokkos/core/unit_test/Makefile:    OBJ_CUDA += TestCuda_SubView_c10.o TestCuda_SubView_c11.o TestCuda_SubView_c12.o
lib/kokkos/core/unit_test/Makefile:    OBJ_CUDA += TestCuda_SubView_c13.o
lib/kokkos/core/unit_test/Makefile:    OBJ_CUDA += TestCuda_Reductions.o TestCuda_ParallelScanRangePolicy.o
lib/kokkos/core/unit_test/Makefile:    OBJ_CUDA += TestCuda_Reductions_DeviceView.o
lib/kokkos/core/unit_test/Makefile:    OBJ_CUDA += TestCuda_Reducers_a.o TestCuda_Reducers_b.o TestCuda_Reducers_c.o TestCuda_Reducers_d.o TestCuda_Reducers_e.o
lib/kokkos/core/unit_test/Makefile:    OBJ_CUDA += TestCuda_Complex.o
lib/kokkos/core/unit_test/Makefile:    OBJ_CUDA += TestCuda_AtomicOperations_int.o TestCuda_AtomicOperations_unsignedint.o TestCuda_AtomicOperations_longint.o
lib/kokkos/core/unit_test/Makefile:    OBJ_CUDA += TestCuda_AtomicOperations_unsignedlongint.o TestCuda_AtomicOperations_longlongint.o TestCuda_AtomicOperations_double.o TestCuda_AtomicOperations_float.o
lib/kokkos/core/unit_test/Makefile:    OBJ_CUDA += TestCuda_AtomicOperations_complexfloat.o TestCuda_AtomicOperations_complexdouble.o
lib/kokkos/core/unit_test/Makefile:    OBJ_CUDA += TestCuda_AtomicViews.o TestCuda_Atomics.o
lib/kokkos/core/unit_test/Makefile:    OBJ_CUDA += TestCuda_TeamBasic.o TestCuda_TeamScratch.o
lib/kokkos/core/unit_test/Makefile:    OBJ_CUDA += TestCuda_TeamReductionScan.o TestCuda_TeamTeamSize.o
lib/kokkos/core/unit_test/Makefile:    OBJ_CUDA += TestCuda_TeamVectorRange.o
lib/kokkos/core/unit_test/Makefile:    OBJ_CUDA += TestCuda_Other.o
lib/kokkos/core/unit_test/Makefile:    OBJ_CUDA += TestCuda_MDRange_a.o TestCuda_MDRange_b.o TestCuda_MDRange_c.o TestCuda_MDRange_d.o TestCuda_MDRange_e.o
lib/kokkos/core/unit_test/Makefile:    OBJ_CUDA += TestCuda_Crs.o
lib/kokkos/core/unit_test/Makefile:    OBJ_CUDA += TestCuda_Task.o TestCuda_WorkGraph.o
lib/kokkos/core/unit_test/Makefile:    OBJ_CUDA += TestCuda_Spaces.o
lib/kokkos/core/unit_test/Makefile:    OBJ_CUDA += TestCuda_UniqueToken.o
lib/kokkos/core/unit_test/Makefile:    OBJ_CUDA += TestCuda_LocalDeepCopy.o
lib/kokkos/core/unit_test/Makefile:    OBJ_CUDA += TestCuda_DebugSerialExecution.o
lib/kokkos/core/unit_test/Makefile:    OBJ_CUDA += TestCuda_DebugPinUVMSpace.o
lib/kokkos/core/unit_test/Makefile:    OBJ_CUDA += TestCuda_TeamScratchStreams.o
lib/kokkos/core/unit_test/Makefile:    TARGETS += KokkosCore_UnitTest_Cuda
lib/kokkos/core/unit_test/Makefile:    TARGETS += KokkosCore_UnitTest_CudaInterOpInit
lib/kokkos/core/unit_test/Makefile:    TARGETS += KokkosCore_UnitTest_CudaInterOpStreams
lib/kokkos/core/unit_test/Makefile:    TEST_TARGETS += test-cuda
lib/kokkos/core/unit_test/Makefile:	GPU_SPACES = HIPHostPinned
lib/kokkos/core/unit_test/Makefile:	tmp := $(foreach space, $(GPU_SPACES), \
lib/kokkos/core/unit_test/Makefile:	  tmp2 := $(foreach test, $(GPU_SPACE_TESTS), \
lib/kokkos/core/unit_test/Makefile:KokkosCore_UnitTest_Cuda: $(OBJ_CUDA) $(KOKKOS_LINK_DEPENDS)
lib/kokkos/core/unit_test/Makefile:	$(LINK) $(EXTRA_PATH) $(OBJ_CUDA) $(KOKKOS_LIBS) $(LIB) $(KOKKOS_LDFLAGS) $(LDFLAGS) -o KokkosCore_UnitTest_Cuda
lib/kokkos/core/unit_test/Makefile:KokkosCore_UnitTest_CudaInterOpInit: UnitTestMain.o gtest-all.o TestCuda_InterOp_Init.o $(KOKKOS_LINK_DEPENDS)
lib/kokkos/core/unit_test/Makefile:	$(LINK) $(EXTRA_PATH) UnitTestMain.o gtest-all.o TestCuda_InterOp_Init.o $(KOKKOS_LIBS) $(LIB) $(KOKKOS_LDFLAGS) $(LDFLAGS) -o KokkosCore_UnitTest_CudaInterOpInit
lib/kokkos/core/unit_test/Makefile:KokkosCore_UnitTest_CudaInterOpStreams: UnitTestMain.o gtest-all.o TestCuda_InterOp_Streams.o $(KOKKOS_LINK_DEPENDS)
lib/kokkos/core/unit_test/Makefile:	$(LINK) $(EXTRA_PATH) UnitTestMain.o gtest-all.o TestCuda_InterOp_Streams.o $(KOKKOS_LIBS) $(LIB) $(KOKKOS_LDFLAGS) $(LDFLAGS) -o KokkosCore_UnitTest_CudaInterOpStreams
lib/kokkos/core/unit_test/Makefile:test-cuda: KokkosCore_UnitTest_Cuda
lib/kokkos/core/unit_test/Makefile:	./KokkosCore_UnitTest_Cuda
lib/kokkos/core/unit_test/Makefile:	./KokkosCore_UnitTest_CudaInterOpInit
lib/kokkos/core/unit_test/Makefile:	./KokkosCore_UnitTest_CudaInterOpStreams
lib/kokkos/core/unit_test/Makefile:	rm -f *.o $(TARGETS) TestCuda*.cpp TestThreads*.cpp TestOpenMP*.cpp TestSerial*.cpp TestHIP*.cpp \
lib/kokkos/core/unit_test/openmp/TestOpenMP_InterOp.cpp:// Cuda.
lib/kokkos/core/unit_test/TestViewOutOfBoundsAccess.hpp:#if defined(KOKKOS_ENABLE_OPENACC)  // FIXME_OPENACC
lib/kokkos/core/unit_test/TestViewOutOfBoundsAccess.hpp:  if (std::is_same<ExecutionSpace, Kokkos::Experimental::OpenACC>::value) {
lib/kokkos/core/unit_test/TestViewOutOfBoundsAccess.hpp:    GTEST_SKIP() << "skipping because OpenACC backend is currently not "
lib/kokkos/core/unit_test/TestTeamScan.hpp:      // then add in another epsilon each iteration. For example, with CUDA
lib/kokkos/core/unit_test/TestTeamScan.hpp:#if !defined(KOKKOS_ENABLE_OPENACC)
lib/kokkos/core/unit_test/TestTeamScan.hpp:      // then add in another epsilon each iteration. For example, with CUDA
lib/kokkos/core/unit_test/TestViewBadAlloc.hpp:        << "ROCm 5.3 segfaults when trying to allocate too much memory";
lib/kokkos/core/unit_test/TestViewBadAlloc.hpp:#if defined(KOKKOS_ENABLE_OPENACC)  // FIXME_OPENACC
lib/kokkos/core/unit_test/TestViewBadAlloc.hpp:  if (std::is_same_v<ExecutionSpace, Kokkos::Experimental::OpenACC>) {
lib/kokkos/core/unit_test/tools/TestAllCalls.cpp:    // the test to run on platforms where CUDA lambda launch isn't supported.
lib/kokkos/core/unit_test/tools/TestEventCorrectness.hpp:class Cuda;
lib/kokkos/core/unit_test/tools/TestEventCorrectness.hpp:#ifdef KOKKOS_ENABLE_CUDA
lib/kokkos/core/unit_test/tools/TestEventCorrectness.hpp: * CUDA ONLY: test that creating instances from streams leads to events
lib/kokkos/core/unit_test/tools/TestEventCorrectness.hpp:      cudaStream_t s1, s2;
lib/kokkos/core/unit_test/tools/TestEventCorrectness.hpp:      cudaStreamCreate(&s1);
lib/kokkos/core/unit_test/tools/TestEventCorrectness.hpp:      cudaStreamCreate(&s2);
lib/kokkos/core/unit_test/tools/TestEventCorrectness.hpp:      Kokkos::Cuda default_space;
lib/kokkos/core/unit_test/tools/TestEventCorrectness.hpp:      Kokkos::Cuda space_s1(s1);
lib/kokkos/core/unit_test/tools/TestEventCorrectness.hpp:      Kokkos::Cuda space_s2(s2);
lib/kokkos/core/unit_test/tools/TestEventCorrectness.hpp:#ifndef KOKKOS_ENABLE_OPENACC
lib/kokkos/core/unit_test/tools/TestEventCorrectness.hpp:// FIXME_OPENACC: not supported reducer type
lib/kokkos/core/unit_test/tools/TestEventCorrectness.hpp:#ifndef KOKKOS_ENABLE_OPENACC
lib/kokkos/core/unit_test/tools/TestEventCorrectness.hpp:// FIXME_OPENACC: parallel_scan not implemented yet
lib/kokkos/core/unit_test/tools/TestWithoutInitializing.cpp:#ifdef KOKKOS_ENABLE_CUDA
lib/kokkos/core/unit_test/tools/TestWithoutInitializing.cpp:                   Kokkos::CudaUVMSpace>::value)
lib/kokkos/core/unit_test/tools/TestWithoutInitializing.cpp:        << "skipping since the CudaUVMSpace requires additional fences";
lib/kokkos/core/unit_test/TestSharedAlloc.hpp:#elif (TEST_CATEGORY_NUMBER == 5)  // cuda
lib/kokkos/core/unit_test/TestSharedAlloc.hpp:  test_shared_alloc<Kokkos::CudaSpace, Kokkos::DefaultHostExecutionSpace>();
lib/kokkos/core/unit_test/TestSharedAlloc.hpp:#elif (TEST_CATEGORY_NUMBER == 8)  // openacc
lib/kokkos/core/unit_test/TestSharedAlloc.hpp:  test_shared_alloc<Kokkos::Experimental::OpenACCSpace,
lib/kokkos/core/unit_test/TestAtomics.hpp:  // FIXME_OPENACC: atomic operations on composite types are not supported.
lib/kokkos/core/unit_test/TestAtomics.hpp:#if !defined(KOKKOS_ENABLE_OPENMPTARGET) && !defined(KOKKOS_ENABLE_OPENACC)
lib/kokkos/core/unit_test/TestReduce.hpp:#ifndef KOKKOS_ENABLE_OPENACC
lib/kokkos/core/unit_test/TestReduce.hpp:    // FIXME_OPENACC - OpenACC (V3.3) does not support custom reductions.
lib/kokkos/core/unit_test/TestReduce.hpp:// FIXME_OPENMPTARGET : The feature works with LLVM/13 on NVIDIA
lib/kokkos/core/unit_test/TestReduce.hpp:// FIXME_OPENACC: Not yet implemented.
lib/kokkos/core/unit_test/TestReduce.hpp:#ifndef KOKKOS_ENABLE_OPENACC
lib/kokkos/core/unit_test/TestTeamVector.hpp:#if !defined(KOKKOS_ENABLE_OPENACC)
lib/kokkos/core/unit_test/TestTeamVector.hpp:#if !defined(KOKKOS_ENABLE_OPENACC)
lib/kokkos/core/unit_test/TestTeamVector.hpp:#if (!defined(KOKKOS_ENABLE_CUDA)) || defined(KOKKOS_ENABLE_CUDA_LAMBDA)
lib/kokkos/core/unit_test/TestTeamVector.hpp:#else  // #if ( ! defined( KOKKOS_ENABLE_CUDA ) ) || defined(
lib/kokkos/core/unit_test/TestTeamVector.hpp:       // KOKKOS_ENABLE_CUDA_LAMBDA )
lib/kokkos/core/unit_test/TestTeamVector.hpp:// with a team size of 32 on GPUs, 16 is the max possible (at least on a K80
lib/kokkos/core/unit_test/TestTeamVector.hpp:// GPU) See https://github.com/kokkos/kokkos/issues/1513
lib/kokkos/core/unit_test/TestTeamVector.hpp:// For Intel GPUs, the requested workgroup size is just too large here.
lib/kokkos/core/unit_test/TestTeamVector.hpp:#if defined(KOKKOS_ENABLE_DEBUG) && defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/core/unit_test/TestTeamVector.hpp:  if (!std::is_same<TEST_EXECSPACE, Kokkos::Cuda>::value)
lib/kokkos/core/unit_test/UnitTest_DeviceAndThreads.cpp:#if defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/core/unit_test/UnitTest_DeviceAndThreads.cpp:  KOKKOS_IMPL_CUDA_SAFE_CALL(cudaGetDeviceCount(&num_devices));
lib/kokkos/core/unit_test/UnitTest_DeviceAndThreads.cpp:#elif defined(KOKKOS_ENABLE_OPENACC)
lib/kokkos/core/unit_test/UnitTest_DeviceAndThreads.cpp:  num_devices = sycl::device::get_devices(sycl::info::device_type::gpu).size();
lib/kokkos/core/unit_test/UnitTest_DeviceAndThreads.cpp:#if defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/core/unit_test/UnitTest_DeviceAndThreads.cpp:  KOKKOS_IMPL_CUDA_SAFE_CALL(cudaGetDevice(&device_id));
lib/kokkos/core/unit_test/UnitTest_DeviceAndThreads.cpp:#elif defined(KOKKOS_ENABLE_OPENACC)
lib/kokkos/core/unit_test/TestMDRange_g.hpp:// first one works successfully. There was a problem with this in the Cuda
lib/kokkos/core/unit_test/category_files/TestOpenACC_Category.hpp:#define TEST_CATEGORY openacc
lib/kokkos/core/unit_test/category_files/TestOpenACC_Category.hpp:#define TEST_CATEGORY_DEATH openacc_DeathTest
lib/kokkos/core/unit_test/category_files/TestOpenACC_Category.hpp:#define TEST_EXECSPACE Kokkos::Experimental::OpenACC
lib/kokkos/core/unit_test/category_files/TestOpenACC_Category.hpp:#define TEST_CATEGORY_FIXTURE(name) openacc_##name
lib/kokkos/core/unit_test/category_files/TestCudaUVM_Category.hpp:#ifndef KOKKOS_TEST_CUDAUVM_HPP
lib/kokkos/core/unit_test/category_files/TestCudaUVM_Category.hpp:#define KOKKOS_TEST_CUDAUVM_HPP
lib/kokkos/core/unit_test/category_files/TestCudaUVM_Category.hpp:#define TEST_CATEGORY cuda_uvm
lib/kokkos/core/unit_test/category_files/TestCudaUVM_Category.hpp:#define TEST_CATEGORY_DEATH cuda_uvm_DeathTest
lib/kokkos/core/unit_test/category_files/TestCudaUVM_Category.hpp:#define TEST_EXECSPACE Kokkos::CudaUVMSpace
lib/kokkos/core/unit_test/category_files/TestCudaHostPinned_Category.hpp:#ifndef KOKKOS_TEST_CUDAHOSTPINNED_HPP
lib/kokkos/core/unit_test/category_files/TestCudaHostPinned_Category.hpp:#define KOKKOS_TEST_CUDAHOSTPINNED_HPP
lib/kokkos/core/unit_test/category_files/TestCudaHostPinned_Category.hpp:#define TEST_CATEGORY cuda_hostpinned
lib/kokkos/core/unit_test/category_files/TestCudaHostPinned_Category.hpp:#define TEST_CATEGORY_DEATH cuda_hostpinned_DeathTest
lib/kokkos/core/unit_test/category_files/TestCudaHostPinned_Category.hpp:// Kokkos::Device<Kokkos::Cuda,Kokkos::CudaHostPinnedSpace>
lib/kokkos/core/unit_test/category_files/TestCudaHostPinned_Category.hpp:#define TEST_EXECSPACE Kokkos::CudaHostPinnedSpace
lib/kokkos/core/unit_test/category_files/TestCuda_Category.hpp:#ifndef KOKKOS_TEST_CUDA_HPP
lib/kokkos/core/unit_test/category_files/TestCuda_Category.hpp:#define KOKKOS_TEST_CUDA_HPP
lib/kokkos/core/unit_test/category_files/TestCuda_Category.hpp:#define TEST_CATEGORY cuda
lib/kokkos/core/unit_test/category_files/TestCuda_Category.hpp:#define TEST_CATEGORY_DEATH cuda_DeathTest
lib/kokkos/core/unit_test/category_files/TestCuda_Category.hpp:#define TEST_EXECSPACE Kokkos::Cuda
lib/kokkos/core/unit_test/category_files/TestCuda_Category.hpp:#define TEST_CATEGORY_FIXTURE(name) cuda_##name
lib/kokkos/core/unit_test/CMakeLists.txt:  IF((NOT (Kokkos_ENABLE_CUDA AND WIN32)) AND (NOT ("${KOKKOS_CXX_COMPILER_ID}" STREQUAL "Fujitsu")))
lib/kokkos/core/unit_test/CMakeLists.txt:SET(KOKKOS_CUDA_FEATURE_LEVEL 999)
lib/kokkos/core/unit_test/CMakeLists.txt:SET(KOKKOS_CUDA_NAME Cuda)
lib/kokkos/core/unit_test/CMakeLists.txt:# FIXME_OPENMPTARGET - The NVIDIA HPC compiler nvc++ only compiles the first 8 incremental tests for the OpenMPTarget backend.
lib/kokkos/core/unit_test/CMakeLists.txt:# FIXME_OPENACC - The Clang compiler only compiles the first 9 incremental tests for the OpenACC backend.
lib/kokkos/core/unit_test/CMakeLists.txt:  SET(KOKKOS_OPENACC_FEATURE_LEVEL 9)
lib/kokkos/core/unit_test/CMakeLists.txt:  SET(KOKKOS_OPENACC_FEATURE_LEVEL 17)
lib/kokkos/core/unit_test/CMakeLists.txt:SET(KOKKOS_OPENACC_NAME Experimental::OpenACC)
lib/kokkos/core/unit_test/CMakeLists.txt:foreach(Tag Threads;Serial;OpenMP;Cuda;HPX;OpenMPTarget;OpenACC;HIP;SYCL)
lib/kokkos/core/unit_test/CMakeLists.txt:    if (Tag STREQUAL "Cuda")
lib/kokkos/core/unit_test/CMakeLists.txt:      set(TagHostAccessible CudaUVM)
lib/kokkos/core/unit_test/CMakeLists.txt:foreach(PairDeviceSpace HIP-HostPinned;HIP-Managed;Cuda-HostPinned;Cuda-UVM;SYCL-HostUSM;SYCL-SharedUSM)
lib/kokkos/core/unit_test/CMakeLists.txt:if(Kokkos_ENABLE_OPENACC)
lib/kokkos/core/unit_test/CMakeLists.txt:  list(REMOVE_ITEM OpenACC_SOURCES
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_AtomicOperations_complexdouble.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_AtomicOperations_complexfloat.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_Crs.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_JoinBackwardCompatibility.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_LocalDeepCopy.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_Other.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_TeamCombinedReducers.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_TeamMDRange.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_TeamReductionScan.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_TeamScan.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_TeamVectorRange.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_ViewAPI_e.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_ViewMapping_subview.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_ViewOfClass.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_WorkGraph.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:  SET(KOKKOS_AMDGPU_ARCH TRUE)
lib/kokkos/core/unit_test/CMakeLists.txt:IF(KOKKOS_ENABLE_OPENMPTARGET AND KOKKOS_CXX_COMPILER_ID STREQUAL Clang AND KOKKOS_AMDGPU_ARCH)
lib/kokkos/core/unit_test/CMakeLists.txt:# when compiling for Intel's Xe-HP GPUs.
lib/kokkos/core/unit_test/CMakeLists.txt:# FIXME_OPENMPTARGET - Comment non-passing tests with the NVIDIA HPC compiler nvc++
lib/kokkos/core/unit_test/CMakeLists.txt:# FIXME_OPENACC - Comment non-passing tests with the NVIDIA HPC compiler nvc++
lib/kokkos/core/unit_test/CMakeLists.txt:IF(KOKKOS_ENABLE_OPENACC AND KOKKOS_CXX_COMPILER_ID STREQUAL NVHPC)
lib/kokkos/core/unit_test/CMakeLists.txt:  list(REMOVE_ITEM OpenACC_SOURCES
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_AtomicOperations_shared.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_BlockSizeDeduction.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_DeepCopyAlignment.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_HostSharedPtr.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_HostSharedPtrAccessOnDevice.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_MathematicalFunctions1.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_MathematicalFunctions2.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_MathematicalFunctions3.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_MDRange_c.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_MDRange_f.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_NumericTraits.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_RangePolicy.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_RangePolicyRequire.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_Reducers_a.cpp #fails if NVHPC V22.5 or lower.
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_Reducers_d.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_Reductions.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_Reductions_DeviceView.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_TeamBasic.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_TeamScratch.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_TeamTeamSize.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_UniqueToken.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_ViewResize.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:# FIXME_OPENACC - Comment non-passing tests with the Clang compiler
lib/kokkos/core/unit_test/CMakeLists.txt:IF(KOKKOS_ENABLE_OPENACC AND KOKKOS_CXX_COMPILER_ID STREQUAL Clang)
lib/kokkos/core/unit_test/CMakeLists.txt:  list(REMOVE_ITEM OpenACC_SOURCES
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_AtomicOperations_double.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_AtomicOperations_float.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_AtomicOperations_int.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_AtomicOperations_longint.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_AtomicOperations_longlongint.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_AtomicOperations_shared.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_AtomicOperations_unsignedint.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_AtomicOperations_unsignedlongint.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_Atomics.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_AtomicViews.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_BlockSizeDeduction.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_DeepCopyAlignment.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_HostSharedPtrAccessOnDevice.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_MathematicalFunctions1.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_MathematicalFunctions2.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_MDRange_c.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_MDRange_f.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_NumericTraits.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_RangePolicy.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_RangePolicyRequire.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_Reducers_a.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_Reducers_d.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_Reductions.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_Reductions_DeviceView.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_TeamBasic.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_TeamScratch.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_TeamTeamSize.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_UniqueToken.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_ViewMapping_b.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_ViewResize.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    # This test is not removed above for OpenACC+NVHPC but all its TEST
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_ViewCtorDimMatch.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    # These tests are not removed above for OpenACC+NVHPC.
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_Abort.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_Complex.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_ExecutionSpace.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_ExecSpacePartitioning.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_Init.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_MathematicalConstants.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_MathematicalSpecialFunctions.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_MinMaxClamp.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_ViewLayoutStrideAssignment.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_ViewMapping_a.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_ViewMemoryAccessViolation.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_WithoutInitializing.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_ViewAPI_d.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:  # When tested on a systme with AMD MI60 GPU and ROCm V5.4.0, these cause
lib/kokkos/core/unit_test/CMakeLists.txt:  IF(KOKKOS_AMDGPU_ARCH)
lib/kokkos/core/unit_test/CMakeLists.txt:    list(REMOVE_ITEM OpenACC_SOURCES
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_BitManipulationBuiltins.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_MathematicalFunctions3.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_ParallelScanRangePolicy.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_SubView_c04.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_SubView_c05.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_SubView_c06.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_SubView_c07.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_SubView_c08.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_SubView_c09.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_SubView_c10.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_SubView_c11.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_SubView_c12.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_ViewAPI_b.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    ${CMAKE_CURRENT_BINARY_DIR}/openacc/TestOpenACC_ViewAPI_c.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:if(Kokkos_ENABLE_OPENACC)
lib/kokkos/core/unit_test/CMakeLists.txt:    CoreUnitTest_OpenACC
lib/kokkos/core/unit_test/CMakeLists.txt:    ${OpenACC_SOURCES}
lib/kokkos/core/unit_test/CMakeLists.txt:if(Kokkos_ENABLE_CUDA)
lib/kokkos/core/unit_test/CMakeLists.txt:    CoreUnitTest_Cuda1
lib/kokkos/core/unit_test/CMakeLists.txt:      ${Cuda_SOURCES1}
lib/kokkos/core/unit_test/CMakeLists.txt:      cuda/TestCuda_ReducerViewSizeLimit.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    CoreUnitTest_Cuda2
lib/kokkos/core/unit_test/CMakeLists.txt:      ${Cuda_SOURCES2}
lib/kokkos/core/unit_test/CMakeLists.txt:    CoreUnitTest_Cuda3
lib/kokkos/core/unit_test/CMakeLists.txt:      cuda/TestCuda_Task.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:      cuda/TestCuda_TeamScratchStreams.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:      ${Cuda_SOURCES3}
lib/kokkos/core/unit_test/CMakeLists.txt:      cuda/TestCuda_Spaces.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:      ${Cuda_SOURCES_SHAREDSPACE}
lib/kokkos/core/unit_test/CMakeLists.txt:    CoreUnitTest_CudaTimingBased
lib/kokkos/core/unit_test/CMakeLists.txt:      cuda/TestCuda_DebugSerialExecution.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:      cuda/TestCuda_DebugPinUVMSpace.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    CoreUnitTest_CudaInterOpInit
lib/kokkos/core/unit_test/CMakeLists.txt:      cuda/TestCuda_InterOp_Init.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    CoreUnitTest_CudaInterOpStreams
lib/kokkos/core/unit_test/CMakeLists.txt:      cuda/TestCuda_InterOp_Streams.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:    CoreUnitTest_CudaInterOpStreamsMultiGPU
lib/kokkos/core/unit_test/CMakeLists.txt:      cuda/TestCuda_InterOp_StreamsMultiGPU.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:      CoreUnitTest_SYCLInterOpStreamsMultiGPU
lib/kokkos/core/unit_test/CMakeLists.txt:      sycl/TestSYCL_InterOp_StreamsMultiGPU.cpp
lib/kokkos/core/unit_test/CMakeLists.txt:# FIXME_OPENMPTARGET and FIXME_OPENACC do not provide a MemorySpace that can be accessed from all ExecSpaces
lib/kokkos/core/unit_test/CMakeLists.txt:if (KOKKOS_ENABLE_OPENACC OR KOKKOS_ENABLE_OPENMPTARGET OR KOKKOS_ENABLE_SYCL)
lib/kokkos/core/unit_test/CMakeLists.txt:# FIXME_OPENMPTARGET and FIXME_OPENACC do not provide a HostPinnedMemorySpace that can be accessed from all ExecSpaces
lib/kokkos/core/unit_test/CMakeLists.txt:if (KOKKOS_ENABLE_OPENACC OR KOKKOS_ENABLE_OPENMPTARGET)
lib/kokkos/core/unit_test/CMakeLists.txt:# FIXME_OPENMPTARGET, FIXME_OPENACC - Comment non-passing tests with the NVIDIA HPC compiler nvc++
lib/kokkos/core/unit_test/CMakeLists.txt:if ((KOKKOS_ENABLE_OPENMPTARGET OR KOKKOS_ENABLE_OPENACC) AND KOKKOS_CXX_COMPILER_ID STREQUAL NVHPC)
lib/kokkos/core/unit_test/CMakeLists.txt:# FIXME_OPENACC - Comment non-passing tests with the Clang compiler
lib/kokkos/core/unit_test/CMakeLists.txt:if (KOKKOS_ENABLE_OPENACC AND KOKKOS_CXX_COMPILER_ID STREQUAL Clang)
lib/kokkos/core/unit_test/CMakeLists.txt:  # when compiling for Intel's Xe-HP GPUs.
lib/kokkos/core/unit_test/CMakeLists.txt:    if((NOT (Kokkos_ENABLE_CUDA AND WIN32)) AND (NOT ("${KOKKOS_CXX_COMPILER_ID}" STREQUAL "Fujitsu")))
lib/kokkos/core/unit_test/standalone/UnitTestMainInit.cpp:#ifdef KOKKOS_ENABLE_CUDA
lib/kokkos/core/unit_test/standalone/UnitTestMainInit.cpp:#include <TestCuda_Category.hpp>
lib/kokkos/core/unit_test/standalone/UnitTestMainInit.cpp:#ifdef KOKKOS_ENABLE_OPENACC
lib/kokkos/core/unit_test/standalone/UnitTestMainInit.cpp:#include <TestOpenACC_Category.hpp>
lib/kokkos/core/unit_test/standalone/Makefile:KOKKOS_CUDA_OPTIONS=enable_lambda
lib/kokkos/core/unit_test/standalone/Makefile:ifneq (,$(findstring Cuda,$(KOKKOS_DEVICES)))
lib/kokkos/core/unit_test/standalone/Makefile:EXE = test.cuda
lib/kokkos/core/unit_test/standalone/Makefile:	rm -f *.o *.cuda *.host
lib/kokkos/core/unit_test/TestViewAPI_e.hpp:#if defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/core/unit_test/TestViewAPI_e.hpp:  std::conditional<std::is_same<TEST_EXECSPACE, Kokkos::Cuda>::value, \
lib/kokkos/core/unit_test/TestViewAPI_e.hpp:                   Kokkos::CudaHostPinnedSpace, TEST_EXECSPACE>::type
lib/kokkos/core/unit_test/TestViewAPI.hpp:    // For CUDA the constant random access View does not return
lib/kokkos/core/unit_test/TestViewAPI.hpp:#if defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/core/unit_test/TestViewAPI.hpp:    if (!std::is_same<typename device::execution_space, Kokkos::Cuda>::value)
lib/kokkos/core/unit_test/TestMathematicalFunctions.hpp:#if defined(KOKKOS_ENABLE_CUDA) || defined(KOKKOS_ENABLE_HIP) ||          \
lib/kokkos/core/unit_test/TestMathematicalFunctions.hpp:    defined(KOKKOS_ENABLE_OPENACC)
lib/kokkos/core/unit_test/TestMathematicalFunctions.hpp:#if !(defined(KOKKOS_ENABLE_CUDA) && defined(KOKKOS_COMPILER_MSVC))
lib/kokkos/core/unit_test/TestMathematicalFunctions.hpp:#if !(defined(KOKKOS_ENABLE_CUDA) && defined(KOKKOS_COMPILER_MSVC))
lib/kokkos/core/unit_test/TestMathematicalFunctions.hpp:#if !(defined(KOKKOS_ENABLE_CUDA) && defined(KOKKOS_COMPILER_MSVC))
lib/kokkos/core/unit_test/TestTeamCombinedReducers.hpp:// KOKKOS_ENABLE_CUDA_LAMBDA is off
lib/kokkos/core/unit_test/TestTeamCombinedReducers.hpp:#if !defined(KOKKOS_ENABLE_CUDA) || defined(KOKKOS_ENABLE_CUDA_LAMBDA)
lib/kokkos/core/unit_test/TestTeamCombinedReducers.hpp:#elif defined(KOKKOS_ENABLE_OPENACC)
lib/kokkos/core/unit_test/TestTeamCombinedReducers.hpp:  if constexpr (std::is_same_v<TEST_EXECSPACE, Kokkos::Experimental::OpenACC>)
lib/kokkos/core/unit_test/TestTeamCombinedReducers.hpp:#elif defined(KOKKOS_ENABLE_OPENACC)
lib/kokkos/core/unit_test/TestTeamCombinedReducers.hpp:  if constexpr (std::is_same_v<TEST_EXECSPACE, Kokkos::Experimental::OpenACC>)
lib/kokkos/core/unit_test/TestTeamCombinedReducers.hpp:#ifdef KOKKOS_ENABLE_OPENACC  // FIXME_OPENACC
lib/kokkos/core/unit_test/TestTeamCombinedReducers.hpp:  if constexpr (std::is_same_v<TEST_EXECSPACE, Kokkos::Experimental::OpenACC>)
lib/kokkos/core/unit_test/TestWithoutInitializing.hpp:#ifdef KOKKOS_ENABLE_CUDA
lib/kokkos/core/unit_test/TestWithoutInitializing.hpp:                   Kokkos::CudaUVMSpace>::value)
lib/kokkos/core/unit_test/TestWithoutInitializing.hpp:    GTEST_SKIP() << "skipping since CudaUVMSpace requires additional fences";
lib/kokkos/core/unit_test/TestWithoutInitializing.hpp:// FIXME_OPENACC: The OpenACC backend doesn't implement ZeroMemset
lib/kokkos/core/unit_test/TestWithoutInitializing.hpp:#ifdef KOKKOS_ENABLE_OPENACC
lib/kokkos/core/unit_test/TestWithoutInitializing.hpp:  if (std::is_same<TEST_EXECSPACE, Kokkos::Experimental::OpenACC>::value)
lib/kokkos/core/unit_test/TestWithoutInitializing.hpp:    GTEST_SKIP() << "skipping since the OpenACC backend doesn't implement "
lib/kokkos/core/unit_test/TestWithoutInitializing.hpp:#ifdef KOKKOS_ENABLE_CUDA
lib/kokkos/core/unit_test/TestWithoutInitializing.hpp:  if (std::is_same<TEST_EXECSPACE::memory_space, Kokkos::CudaUVMSpace>::value)
lib/kokkos/core/unit_test/TestWithoutInitializing.hpp:        << "skipping since the CudaUVMSpace requires additiional fences";
lib/kokkos/core/unit_test/TestTeamBasic.hpp:    // FIXME_CUDA
lib/kokkos/core/unit_test/TestTeamBasic.hpp:#ifdef KOKKOS_ENABLE_CUDA
lib/kokkos/core/unit_test/TestTeamBasic.hpp:    if (!std::is_same<TEST_EXECSPACE, Kokkos::Cuda>::value)
lib/kokkos/core/unit_test/TestTeamBasic.hpp:    // FIXME_CUDA
lib/kokkos/core/unit_test/TestTeamBasic.hpp:#ifdef KOKKOS_ENABLE_CUDA
lib/kokkos/core/unit_test/TestTeamBasic.hpp:    if (!std::is_same<TEST_EXECSPACE, Kokkos::Cuda>::value)
lib/kokkos/core/unit_test/TestSharedSpace.cpp:#if defined(KOKKOS_ARCH_AMD_GPU) && defined(KOKKOS_ENABLE_HIP)
lib/kokkos/core/unit_test/TestSharedSpace.cpp:#if defined(KOKKOS_ENABLE_SYCL) && !defined(KOKKOS_ARCH_INTEL_GPU)
lib/kokkos/core/unit_test/TestSharedSpace.cpp:      << "skipping because clock_tic is only defined for sycl+intel gpu";
lib/kokkos/core/unit_test/TestExecSpaceThreadSafety.hpp:#ifdef KOKKOS_ENABLE_OPENACC  // FIXME_OPENACC
lib/kokkos/core/unit_test/TestExecSpaceThreadSafety.hpp:  if (std::is_same_v<TEST_EXECSPACE, Kokkos::Experimental::OpenACC>)
lib/kokkos/core/unit_test/TestExecSpaceThreadSafety.hpp:    GTEST_SKIP() << "skipping since test is known to fail with OpenACC";
lib/kokkos/core/unit_test/TestExecSpaceThreadSafety.hpp:#if defined(KOKKOS_ENABLE_SYCL) && defined(KOKKOS_IMPL_ARCH_NVIDIA_GPU)
lib/kokkos/core/unit_test/TestExecSpaceThreadSafety.hpp:    GTEST_SKIP() << "skipping since test is know to fail with SYCL+Cuda";
lib/kokkos/core/unit_test/TestExecSpaceThreadSafety.hpp:#ifdef KOKKOS_ENABLE_OPENACC  // FIXME_OPENACC
lib/kokkos/core/unit_test/TestExecSpaceThreadSafety.hpp:  if (std::is_same_v<TEST_EXECSPACE, Kokkos::Experimental::OpenACC>)
lib/kokkos/core/unit_test/TestExecSpaceThreadSafety.hpp:    GTEST_SKIP() << "skipping since test is known to fail with OpenACC";
lib/kokkos/core/unit_test/TestViewMapping_a.hpp:#if !defined(KOKKOS_ENABLE_CUDA) || !defined(KOKKOS_ENABLE_CUDA_LAMBDA)
lib/kokkos/core/unit_test/TestViewMapping_a.hpp:    // Cannot launch host lambda when CUDA lambda is enabled.
lib/kokkos/core/unit_test/TestViewMapping_a.hpp:#endif  // #if !defined( KOKKOS_ENABLE_CUDA_LAMBDA )
lib/kokkos/core/unit_test/TestBitManipulationBuiltins.hpp:#if defined(KOKKOS_ENABLE_OPENACC) && defined(KOKKOS_COMPILER_NVHPC)
lib/kokkos/core/unit_test/TestBitManipulationBuiltins.hpp:                                Kokkos::Experimental::OpenACC>) {
lib/kokkos/core/unit_test/TestBitManipulationBuiltins.hpp:#if defined(KOKKOS_ENABLE_OPENACC) && defined(KOKKOS_COMPILER_NVHPC)
lib/kokkos/core/unit_test/TestBitManipulationBuiltins.hpp:#if defined(KOKKOS_ENABLE_OPENACC) && defined(KOKKOS_COMPILER_NVHPC)
lib/kokkos/core/unit_test/TestBitManipulationBuiltins.hpp:                                Kokkos::Experimental::OpenACC>) {
lib/kokkos/core/unit_test/TestBitManipulationBuiltins.hpp:#if defined(KOKKOS_ENABLE_OPENACC) && defined(KOKKOS_COMPILER_NVHPC)
lib/kokkos/core/unit_test/TestBitManipulationBuiltins.hpp:#if defined(KOKKOS_ENABLE_OPENACC) && defined(KOKKOS_COMPILER_NVHPC)
lib/kokkos/core/unit_test/TestBitManipulationBuiltins.hpp:                                Kokkos::Experimental::OpenACC>) {
lib/kokkos/core/unit_test/TestBitManipulationBuiltins.hpp:    !defined(KOKKOS_ARCH_INTEL_GPU)  // FIXME_SYCL returns wrong result
lib/kokkos/core/unit_test/TestBitManipulationBuiltins.hpp:#if defined(KOKKOS_ENABLE_OPENACC) && defined(KOKKOS_COMPILER_NVHPC)
lib/kokkos/core/unit_test/TestBitManipulationBuiltins.hpp:#if defined(KOKKOS_ENABLE_OPENACC) && defined(KOKKOS_COMPILER_NVHPC)
lib/kokkos/core/unit_test/TestBitManipulationBuiltins.hpp:                                Kokkos::Experimental::OpenACC>) {
lib/kokkos/core/unit_test/TestBitManipulationBuiltins.hpp:    !defined(KOKKOS_ARCH_INTEL_GPU)  // FIXME_SYCL returns wrong result
lib/kokkos/core/unit_test/TestBitManipulationBuiltins.hpp:#if defined(KOKKOS_ENABLE_OPENACC) && defined(KOKKOS_COMPILER_NVHPC)
lib/kokkos/core/unit_test/TestBitManipulationBuiltins.hpp:#if defined(KOKKOS_ENABLE_OPENACC) && defined(KOKKOS_COMPILER_NVHPC)
lib/kokkos/core/unit_test/TestBitManipulationBuiltins.hpp:                                Kokkos::Experimental::OpenACC>) {
lib/kokkos/core/unit_test/TestBitManipulationBuiltins.hpp:#if defined(KOKKOS_ENABLE_OPENACC) && defined(KOKKOS_COMPILER_NVHPC)
lib/kokkos/core/unit_test/TestBitManipulationBuiltins.hpp:#if defined(KOKKOS_ENABLE_OPENACC) && defined(KOKKOS_COMPILER_NVHPC)
lib/kokkos/core/unit_test/TestBitManipulationBuiltins.hpp:                                Kokkos::Experimental::OpenACC>) {
lib/kokkos/core/unit_test/TestBitManipulationBuiltins.hpp:#if defined(KOKKOS_ENABLE_OPENACC) && defined(KOKKOS_COMPILER_NVHPC)
lib/kokkos/core/unit_test/TestBitManipulationBuiltins.hpp:#if defined(KOKKOS_ENABLE_OPENACC) && defined(KOKKOS_COMPILER_NVHPC)
lib/kokkos/core/unit_test/TestBitManipulationBuiltins.hpp:                                Kokkos::Experimental::OpenACC>) {
lib/kokkos/core/unit_test/TestBitManipulationBuiltins.hpp:#if defined(KOKKOS_ENABLE_OPENACC) && defined(KOKKOS_COMPILER_NVHPC)
lib/kokkos/core/unit_test/TestBitManipulationBuiltins.hpp:#if defined(KOKKOS_ENABLE_OPENACC) && defined(KOKKOS_COMPILER_NVHPC)
lib/kokkos/core/unit_test/TestBitManipulationBuiltins.hpp:                                Kokkos::Experimental::OpenACC>) {
lib/kokkos/core/unit_test/TestBitManipulationBuiltins.hpp:#if defined(KOKKOS_ENABLE_OPENACC) && defined(KOKKOS_COMPILER_NVHPC)
lib/kokkos/core/unit_test/TestBitManipulationBuiltins.hpp:#if defined(KOKKOS_ENABLE_OPENACC) && defined(KOKKOS_COMPILER_NVHPC)
lib/kokkos/core/unit_test/TestBitManipulationBuiltins.hpp:                                Kokkos::Experimental::OpenACC>) {
lib/kokkos/core/unit_test/TestBitManipulationBuiltins.hpp:#if defined(KOKKOS_ENABLE_OPENACC) && defined(KOKKOS_COMPILER_NVHPC)
lib/kokkos/core/unit_test/TestBitManipulationBuiltins.hpp:#if defined(KOKKOS_ENABLE_OPENACC) && defined(KOKKOS_COMPILER_NVHPC)
lib/kokkos/core/unit_test/TestBitManipulationBuiltins.hpp:                                Kokkos::Experimental::OpenACC>) {
lib/kokkos/core/unit_test/TestBitManipulationBuiltins.hpp:#if defined(KOKKOS_ENABLE_OPENACC) && defined(KOKKOS_COMPILER_NVHPC)
lib/kokkos/core/unit_test/TestBitManipulationBuiltins.hpp:#if defined(KOKKOS_ENABLE_OPENACC) && defined(KOKKOS_COMPILER_NVHPC)
lib/kokkos/core/unit_test/TestBitManipulationBuiltins.hpp:                                Kokkos::Experimental::OpenACC>) {
lib/kokkos/core/unit_test/TestBitManipulationBuiltins.hpp:#if defined(KOKKOS_ENABLE_OPENACC) && defined(KOKKOS_COMPILER_NVHPC)
lib/kokkos/core/unit_test/TestBitManipulationBuiltins.hpp:#if defined(KOKKOS_ENABLE_OPENACC) && defined(KOKKOS_COMPILER_NVHPC)
lib/kokkos/core/unit_test/TestBitManipulationBuiltins.hpp:                                Kokkos::Experimental::OpenACC>) {
lib/kokkos/core/unit_test/TestBitManipulationBuiltins.hpp:#if defined(KOKKOS_ENABLE_OPENACC) && defined(KOKKOS_COMPILER_NVHPC)
lib/kokkos/core/unit_test/TestBitManipulationBuiltins.hpp:// CUDA doesn't provide memcmp
lib/kokkos/core/unit_test/TestNumericTraits.hpp:#ifdef __CUDA_ARCH__
lib/kokkos/core/unit_test/TestNumericTraits.hpp:#if !defined(KOKKOS_ENABLE_CUDA) || \
lib/kokkos/core/unit_test/TestNumericTraits.hpp:    defined(KOKKOS_ENABLE_OPENACC)
lib/kokkos/core/unit_test/TestNumericTraits.hpp:#if (defined(KOKKOS_COMPILER_NVCC) && defined(KOKKOS_ENABLE_CUDA)) || \
lib/kokkos/core/unit_test/TestNumericTraits.hpp:#if defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/core/unit_test/TestNumericTraits.hpp:    Kokkos::Cuda,
lib/kokkos/core/unit_test/TestNumericTraits.hpp:    // https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#constexpr-variables
lib/kokkos/core/unit_test/TestNumericTraits.hpp:  // FIXME_OPENMPTARGET long double on Intel GPUs
lib/kokkos/core/unit_test/TestNumericTraits.hpp:#if (!defined(KOKKOS_ENABLE_OPENMPTARGET) || !defined(KOKKOS_ARCH_INTEL_GPU))
lib/kokkos/core/unit_test/TestNumericTraits.hpp:  // FIXME_OPENMPTARGET long double on Intel GPUs
lib/kokkos/core/unit_test/TestNumericTraits.hpp:#if (!defined(KOKKOS_ENABLE_OPENMPTARGET) || !defined(KOKKOS_ARCH_INTEL_GPU))
lib/kokkos/core/unit_test/TestNumericTraits.hpp:  // FIXME_OPENMPTARGET long double on Intel GPUs
lib/kokkos/core/unit_test/TestNumericTraits.hpp:#if (!defined(KOKKOS_ENABLE_OPENMPTARGET) || !defined(KOKKOS_ARCH_INTEL_GPU))
lib/kokkos/core/unit_test/TestNumericTraits.hpp:  // FIXME_OPENMPTARGET long double on Intel GPUs
lib/kokkos/core/unit_test/TestNumericTraits.hpp:#if (!defined(KOKKOS_ENABLE_OPENMPTARGET) || !defined(KOKKOS_ARCH_INTEL_GPU))
lib/kokkos/core/unit_test/TestNumericTraits.hpp:  // FIXME_OPENMPTARGET long double on Intel GPUs
lib/kokkos/core/unit_test/TestNumericTraits.hpp:#if (!defined(KOKKOS_ENABLE_OPENMPTARGET) || !defined(KOKKOS_ARCH_INTEL_GPU))
lib/kokkos/core/unit_test/TestNumericTraits.hpp:  // FIXME_OPENMPTARGET long double on Intel GPUs
lib/kokkos/core/unit_test/TestNumericTraits.hpp:#if (!defined(KOKKOS_ENABLE_OPENMPTARGET) || !defined(KOKKOS_ARCH_INTEL_GPU))
lib/kokkos/core/unit_test/TestNumericTraits.hpp:  // FIXME_OPENMPTARGET long double on Intel GPUs
lib/kokkos/core/unit_test/TestNumericTraits.hpp:#if (!defined(KOKKOS_ENABLE_OPENMPTARGET) || !defined(KOKKOS_ARCH_INTEL_GPU))
lib/kokkos/core/unit_test/TestNumericTraits.hpp:  // FIXME_OPENMPTARGET long double on Intel GPUs
lib/kokkos/core/unit_test/TestNumericTraits.hpp:#if (!defined(KOKKOS_ENABLE_OPENMPTARGET) || !defined(KOKKOS_ARCH_INTEL_GPU))
lib/kokkos/core/unit_test/TestNumericTraits.hpp:  // FIXME_OPENMPTARGET long double on Intel GPUs
lib/kokkos/core/unit_test/TestNumericTraits.hpp:#if (!defined(KOKKOS_ENABLE_OPENMPTARGET) || !defined(KOKKOS_ARCH_INTEL_GPU))
lib/kokkos/core/unit_test/TestNumericTraits.hpp:  // FIXME_OPENMPTARGET long double on Intel GPUs
lib/kokkos/core/unit_test/TestNumericTraits.hpp:#if (!defined(KOKKOS_ENABLE_OPENMPTARGET) || !defined(KOKKOS_ARCH_INTEL_GPU))
lib/kokkos/core/unit_test/TestNumericTraits.hpp:  // FIXME_OPENMPTARGET long double on Intel GPUs
lib/kokkos/core/unit_test/TestNumericTraits.hpp:#if (!defined(KOKKOS_ENABLE_OPENMPTARGET) || !defined(KOKKOS_ARCH_INTEL_GPU))
lib/kokkos/core/unit_test/TestNumericTraits.hpp:  // FIXME_OPENMPTARGET long double on Intel GPUs
lib/kokkos/core/unit_test/TestNumericTraits.hpp:#if (!defined(KOKKOS_ENABLE_OPENMPTARGET) || !defined(KOKKOS_ARCH_INTEL_GPU))
lib/kokkos/core/unit_test/TestNumericTraits.hpp:  // FIXME_OPENMPTARGET long double on Intel GPUs
lib/kokkos/core/unit_test/TestNumericTraits.hpp:#if (!defined(KOKKOS_ENABLE_OPENMPTARGET) || !defined(KOKKOS_ARCH_INTEL_GPU))
lib/kokkos/core/unit_test/TestNumericTraits.hpp:// FIXME_OPENMPTARGET - The static_assert causes issues on Intel GPUs with the
lib/kokkos/core/unit_test/TestNumericTraits.hpp:// There is the same bug with CUDA 11.6
lib/kokkos/core/unit_test/TestNumericTraits.hpp:// FIXME_NVHPC FIXME_CUDA FIXME_NVCC
lib/kokkos/core/unit_test/TestNumericTraits.hpp:#if !defined(KOKKOS_COMPILER_NVHPC) && (CUDA_VERSION < 11060) && \
lib/kokkos/core/unit_test/TestNumericTraits.hpp:    !(defined(KOKKOS_COMPILER_NVCC) && !defined(KOKKOS_ENABLE_CUDA))
lib/kokkos/core/unit_test/TestNumericTraits.hpp:#ifdef __CUDA_ARCH__
lib/kokkos/core/unit_test/TestExecutionSpace.hpp:#ifdef KOKKOS_ENABLE_CUDA
lib/kokkos/core/unit_test/TestExecutionSpace.hpp:  static_assert(std::is_convertible_v<cudaStream_t, Kokkos::Cuda>);
lib/kokkos/core/unit_test/TestExecutionSpace.hpp:#ifdef KOKKOS_ENABLE_CUDA
lib/kokkos/core/unit_test/TestExecutionSpace.hpp:  static_assert(!std::is_convertible_v<cudaStream_t, Kokkos::Cuda>);
lib/kokkos/core/unit_test/TestExecutionSpace.hpp:#ifdef KOKKOS_ENABLE_OPENACC
lib/kokkos/core/unit_test/TestExecutionSpace.hpp:  static_assert(!std::is_convertible_v<int, Kokkos::Experimental::OpenACC>);
lib/kokkos/core/unit_test/TestDeviceAndThreads.py:        # by default use the first GPU available for execution
lib/kokkos/core/unit_test/TestStringManipulation.cpp:     ((__CUDACC_VER_MAJOR__ >= 11) && (__CUDACC_VER_MINOR__ >= 3))) && \
lib/kokkos/core/unit_test/TestMathematicalSpecialFunctions.hpp:// FIXME_SYCL Failing for Intel GPUs
lib/kokkos/core/unit_test/TestMathematicalSpecialFunctions.hpp:#if !(defined(KOKKOS_ENABLE_SYCL) && defined(KOKKOS_ARCH_INTEL_GPU))
lib/kokkos/core/unit_test/TestMathematicalSpecialFunctions.hpp:    // FIXME_HIP Disable the test when using ROCm 5.5 and 5.6 due to a known
lib/kokkos/core/unit_test/TestMathematicalSpecialFunctions.hpp:    // FIXME_SYCL Failing for Intel GPUs, 19 is the first failing test case
lib/kokkos/core/unit_test/TestMathematicalSpecialFunctions.hpp:#if defined(KOKKOS_ENABLE_SYCL) && defined(KOKKOS_ARCH_INTEL_GPU)
lib/kokkos/core/unit_test/TestMathematicalSpecialFunctions.hpp:    // FIXME_SYCL Failing for Intel GPUs, 8 is the first failing test case
lib/kokkos/core/unit_test/TestMathematicalSpecialFunctions.hpp:#if defined(KOKKOS_ENABLE_SYCL) && defined(KOKKOS_ARCH_INTEL_GPU)
lib/kokkos/core/unit_test/TestMathematicalSpecialFunctions.hpp:    // FIXME_HIP Disable the test when using ROCm 5.5 and 5.6 due to a known
lib/kokkos/core/unit_test/TestMathematicalSpecialFunctions.hpp:// FIXME_SYCL Failing for Intel GPUs, 17 is the first failing test case
lib/kokkos/core/unit_test/TestMathematicalSpecialFunctions.hpp:#if defined(KOKKOS_ENABLE_SYCL) && defined(KOKKOS_ARCH_INTEL_GPU)
lib/kokkos/core/unit_test/TestMathematicalSpecialFunctions.hpp:    // FIXME_SYCL Failing for Intel GPUs, 2 is the first failing test case
lib/kokkos/core/unit_test/TestMathematicalSpecialFunctions.hpp:#if defined(KOKKOS_ENABLE_SYCL) && defined(KOKKOS_ARCH_INTEL_GPU)
lib/kokkos/core/unit_test/TestMathematicalSpecialFunctions.hpp:    // FIXME_HIP Disable the test when using ROCm 5.5 and 5.6 due to a known
lib/kokkos/core/unit_test/TestMathematicalSpecialFunctions.hpp:// FIXME_SYCL Failing for Intel GPUs, 16 is the first failing test case
lib/kokkos/core/unit_test/TestMathematicalSpecialFunctions.hpp:#if defined(KOKKOS_ENABLE_SYCL) && defined(KOKKOS_ARCH_INTEL_GPU)
lib/kokkos/core/unit_test/TestMathematicalSpecialFunctions.hpp:    // FIXME_SYCL Failing for Intel GPUs, 1 is the first failing test case
lib/kokkos/core/unit_test/TestMathematicalSpecialFunctions.hpp:#if defined(KOKKOS_ENABLE_SYCL) && defined(KOKKOS_ARCH_INTEL_GPU)
lib/kokkos/core/unit_test/TestMathematicalSpecialFunctions.hpp:#if defined(KOKKOS_ENABLE_OPENMPTARGET) && defined(KOKKOS_ARCH_INTEL_GPU)
lib/kokkos/core/unit_test/TestMathematicalSpecialFunctions.hpp:                    "Intel GPUs";  // FIXME_OPENMPTARGET
lib/kokkos/core/unit_test/TestMathematicalSpecialFunctions.hpp:#if defined(KOKKOS_ENABLE_OPENMPTARGET) && defined(KOKKOS_ARCH_INTEL_GPU)
lib/kokkos/core/unit_test/TestMathematicalSpecialFunctions.hpp:                    "Intel GPUs";  // FIXME_OPENMPTARGET
lib/kokkos/core/unit_test/TestMathematicalSpecialFunctions.hpp:#if defined(KOKKOS_ENABLE_OPENMPTARGET) && defined(KOKKOS_ARCH_INTEL_GPU)
lib/kokkos/core/unit_test/TestMathematicalSpecialFunctions.hpp:                    "Intel GPUs";  // FIXME_OPENMPTARGET
lib/kokkos/core/unit_test/TestMathematicalSpecialFunctions.hpp:#if defined(KOKKOS_ENABLE_OPENMPTARGET) && defined(KOKKOS_ARCH_INTEL_GPU)
lib/kokkos/core/unit_test/TestMathematicalSpecialFunctions.hpp:                    "Intel GPUs";  // FIXME_OPENMPTARGET
lib/kokkos/core/unit_test/TestMathematicalSpecialFunctions.hpp:#if defined(KOKKOS_ENABLE_OPENMPTARGET) && defined(KOKKOS_ARCH_INTEL_GPU)
lib/kokkos/core/unit_test/TestMathematicalSpecialFunctions.hpp:                    "Intel GPUs";  // FIXME_OPENMPTARGET
lib/kokkos/core/unit_test/TestMathematicalSpecialFunctions.hpp:#if defined(KOKKOS_ENABLE_OPENMPTARGET) && defined(KOKKOS_ARCH_INTEL_GPU)
lib/kokkos/core/unit_test/TestMathematicalSpecialFunctions.hpp:                    "Intel GPUs";  // FIXME_OPENMPTARGET
lib/kokkos/core/unit_test/cuda/TestCuda_InterOp_Init.cpp:#include <TestCuda_Category.hpp>
lib/kokkos/core/unit_test/cuda/TestCuda_InterOp_Init.cpp:// Cuda.
lib/kokkos/core/unit_test/cuda/TestCuda_InterOp_Init.cpp:TEST(cuda, raw_cuda_interop) {
lib/kokkos/core/unit_test/cuda/TestCuda_InterOp_Init.cpp:  KOKKOS_IMPL_CUDA_SAFE_CALL(cudaMalloc(&p, sizeof(int) * 100));
lib/kokkos/core/unit_test/cuda/TestCuda_InterOp_Init.cpp:  KOKKOS_IMPL_CUDA_SAFE_CALL(cudaDeviceSynchronize());
lib/kokkos/core/unit_test/cuda/TestCuda_InterOp_Init.cpp:  cudaMemcpy(h_p.data(), p, sizeof(int) * 100, cudaMemcpyDefault);
lib/kokkos/core/unit_test/cuda/TestCuda_InterOp_Init.cpp:  KOKKOS_IMPL_CUDA_SAFE_CALL(cudaDeviceSynchronize());
lib/kokkos/core/unit_test/cuda/TestCuda_InterOp_Init.cpp:  KOKKOS_IMPL_CUDA_SAFE_CALL(cudaFree(p));
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:#include <TestCuda_Category.hpp>
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:__global__ void test_cuda_spaces_int_value(int *ptr) {
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:TEST(cuda, space_access) {
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:                                      Kokkos::CudaHostPinnedSpace>::assignable);
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:                                       Kokkos::CudaSpace>::assignable);
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:#ifndef KOKKOS_ENABLE_IMPL_CUDA_UNIFIED_MEMORY
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:                                       Kokkos::CudaSpace>::accessible);
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:                                                Kokkos::CudaSpace>::accessible);
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:                                       Kokkos::CudaUVMSpace>::assignable);
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:                                      Kokkos::CudaUVMSpace>::accessible);
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:  static_assert(Kokkos::Impl::MemorySpaceAccess<Kokkos::CudaSpace,
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:                                                Kokkos::CudaSpace>::assignable);
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:      Kokkos::Impl::MemorySpaceAccess<Kokkos::CudaSpace,
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:                                      Kokkos::CudaUVMSpace>::assignable);
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:                Kokkos::CudaSpace, Kokkos::CudaHostPinnedSpace>::assignable);
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:      Kokkos::Impl::MemorySpaceAccess<Kokkos::CudaSpace,
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:                                      Kokkos::CudaHostPinnedSpace>::accessible);
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:      !Kokkos::Impl::MemorySpaceAccess<Kokkos::CudaSpace,
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:      !Kokkos::Impl::MemorySpaceAccess<Kokkos::CudaSpace,
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:      Kokkos::Impl::MemorySpaceAccess<Kokkos::CudaUVMSpace,
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:                                      Kokkos::CudaUVMSpace>::assignable);
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:      !Kokkos::Impl::MemorySpaceAccess<Kokkos::CudaUVMSpace,
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:                                       Kokkos::CudaSpace>::assignable);
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:  static_assert(Kokkos::Impl::MemorySpaceAccess<Kokkos::CudaUVMSpace,
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:                                                Kokkos::CudaSpace>::accessible);
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:      !Kokkos::Impl::MemorySpaceAccess<Kokkos::CudaUVMSpace,
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:      !Kokkos::Impl::MemorySpaceAccess<Kokkos::CudaUVMSpace,
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:                Kokkos::CudaUVMSpace, Kokkos::CudaHostPinnedSpace>::assignable);
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:      Kokkos::Impl::MemorySpaceAccess<Kokkos::CudaUVMSpace,
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:                                      Kokkos::CudaHostPinnedSpace>::accessible);
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:      Kokkos::Impl::MemorySpaceAccess<Kokkos::CudaHostPinnedSpace,
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:                                      Kokkos::CudaHostPinnedSpace>::assignable);
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:      !Kokkos::Impl::MemorySpaceAccess<Kokkos::CudaHostPinnedSpace,
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:  static_assert(Kokkos::Impl::MemorySpaceAccess<Kokkos::CudaHostPinnedSpace,
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:      !Kokkos::Impl::MemorySpaceAccess<Kokkos::CudaHostPinnedSpace,
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:                                       Kokkos::CudaSpace>::assignable);
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:      !Kokkos::Impl::MemorySpaceAccess<Kokkos::CudaHostPinnedSpace,
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:                                       Kokkos::CudaSpace>::accessible);
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:      !Kokkos::Impl::MemorySpaceAccess<Kokkos::CudaHostPinnedSpace,
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:                                       Kokkos::CudaUVMSpace>::assignable);
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:      Kokkos::Impl::MemorySpaceAccess<Kokkos::CudaHostPinnedSpace,
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:                                      Kokkos::CudaUVMSpace>::accessible);
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:      !Kokkos::SpaceAccessibility<Kokkos::Cuda, Kokkos::HostSpace>::accessible);
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:      Kokkos::SpaceAccessibility<Kokkos::Cuda, Kokkos::CudaSpace>::accessible);
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:  static_assert(Kokkos::SpaceAccessibility<Kokkos::Cuda,
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:                                           Kokkos::CudaUVMSpace>::accessible);
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:      Kokkos::SpaceAccessibility<Kokkos::Cuda,
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:                                 Kokkos::CudaHostPinnedSpace>::accessible);
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:#ifndef KOKKOS_ENABLE_IMPL_CUDA_UNIFIED_MEMORY
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:                                            Kokkos::CudaSpace>::accessible);
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:                                           Kokkos::CudaSpace>::accessible);
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:                                           Kokkos::CudaUVMSpace>::accessible);
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:                                 Kokkos::CudaHostPinnedSpace>::accessible);
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:#ifndef KOKKOS_ENABLE_IMPL_CUDA_UNIFIED_MEMORY
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:  static_assert(std::is_same<Kokkos::Impl::HostMirror<Kokkos::CudaSpace>::Space,
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:  static_assert(std::is_same<Kokkos::Impl::HostMirror<Kokkos::CudaSpace>::Space,
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:                                            Kokkos::CudaSpace>>::value);
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:      std::is_same<Kokkos::Impl::HostMirror<Kokkos::CudaUVMSpace>::Space,
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:                                  Kokkos::CudaUVMSpace>>::value);
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:      std::is_same<Kokkos::Impl::HostMirror<Kokkos::CudaHostPinnedSpace>::Space,
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:                   Kokkos::CudaHostPinnedSpace>::value);
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:                                            Kokkos::CudaUVMSpace>,
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:                                            Kokkos::CudaUVMSpace>>::value);
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:      Kokkos::SpaceAccessibility<Kokkos::Impl::HostMirror<Kokkos::Cuda>::Space,
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:                Kokkos::Impl::HostMirror<Kokkos::CudaSpace>::Space,
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:                Kokkos::Impl::HostMirror<Kokkos::CudaUVMSpace>::Space,
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:                Kokkos::Impl::HostMirror<Kokkos::CudaHostPinnedSpace>::Space,
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:#ifdef KOKKOS_ENABLE_CUDA_UVM
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:  using uvm_view = Kokkos::View<double *, Kokkos::CudaUVMSpace>;
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:TEST(cuda, uvm) {
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:      Kokkos::kokkos_malloc<Kokkos::CudaUVMSpace>("uvm_ptr", sizeof(int)));
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:  test_cuda_spaces_int_value<<<1, 1>>>(uvm_ptr);
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:  Kokkos::kokkos_free<Kokkos::CudaUVMSpace>(uvm_ptr);
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:struct TestViewCudaAccessible {
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:  TestViewCudaAccessible() : m_base("base", N) {}
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:    TestViewCudaAccessible self;
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:TEST(cuda, impl_view_accessible) {
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:  TestViewCudaAccessible<Kokkos::CudaSpace, Kokkos::Cuda>::run();
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:  TestViewCudaAccessible<Kokkos::CudaUVMSpace, Kokkos::Cuda>::run();
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:  TestViewCudaAccessible<Kokkos::CudaUVMSpace,
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:  TestViewCudaAccessible<Kokkos::CudaHostPinnedSpace, Kokkos::Cuda>::run();
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:  TestViewCudaAccessible<Kokkos::CudaHostPinnedSpace,
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:struct TestViewCudaTexture {
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:  TestViewCudaTexture() : m_base("base", N), m_tex(m_base) {}
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:    TestViewCudaTexture self;
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:    Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::Cuda, TagInit>(0, N),
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:    Kokkos::parallel_reduce(Kokkos::RangePolicy<Kokkos::Cuda, TagTest>(0, N),
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:TEST(cuda, impl_view_texture) {
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:  TestViewCudaTexture<Kokkos::CudaSpace>::run();
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:  TestViewCudaTexture<Kokkos::CudaUVMSpace>::run();
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:// couldn't create a random-access subview of a view of const T in Kokkos::Cuda
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:  // MSVC+CUDA errors on CTAD here
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:// creating a RandomAccess subview of a view of const T in Kokkos::Cuda
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:TEST(cuda, view_subview_const_randomaccess) {
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:  issue_5594::test_view_subview_const_randomaccess<Kokkos::Cuda,
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:                                                   Kokkos::CudaSpace>();
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:  issue_5594::test_view_subview_const_randomaccess<Kokkos::Cuda,
lib/kokkos/core/unit_test/cuda/TestCuda_Spaces.cpp:                                                   Kokkos::CudaUVMSpace>();
lib/kokkos/core/unit_test/cuda/TestCuda_ReducerViewSizeLimit.cpp:#include <TestCuda_Category.hpp>
lib/kokkos/core/unit_test/cuda/TestCuda_ReducerViewSizeLimit.cpp:using MemSpace  = Kokkos::CudaSpace;
lib/kokkos/core/unit_test/cuda/TestCuda_ReducerViewSizeLimit.cpp:  const unsigned initBlockSize        = Kokkos::Impl::CudaTraits::WarpSize * 8;
lib/kokkos/core/unit_test/cuda/TestCuda_ReducerViewSizeLimit.cpp:TEST(cuda, reduceRangePolicyViewSizeLimit) {
lib/kokkos/core/unit_test/cuda/TestCuda_ReducerViewSizeLimit.cpp:TEST(cuda, reduceMDRangePolicyViewSizeLimit) {
lib/kokkos/core/unit_test/cuda/TestCuda_InterOp_StreamsMultiGPU.cpp:#include <TestCuda_Category.hpp>
lib/kokkos/core/unit_test/cuda/TestCuda_InterOp_StreamsMultiGPU.cpp:#include <TestMultiGPU.hpp>
lib/kokkos/core/unit_test/cuda/TestCuda_InterOp_StreamsMultiGPU.cpp:  std::array<cudaStream_t, 2> streams;
lib/kokkos/core/unit_test/cuda/TestCuda_InterOp_StreamsMultiGPU.cpp:    KOKKOS_IMPL_CUDA_SAFE_CALL(cudaGetDeviceCount(&n_devices));
lib/kokkos/core/unit_test/cuda/TestCuda_InterOp_StreamsMultiGPU.cpp:      KOKKOS_IMPL_CUDA_SAFE_CALL(cudaSetDevice(devices[i]));
lib/kokkos/core/unit_test/cuda/TestCuda_InterOp_StreamsMultiGPU.cpp:      KOKKOS_IMPL_CUDA_SAFE_CALL(cudaStreamCreate(&streams[i]));
lib/kokkos/core/unit_test/cuda/TestCuda_InterOp_StreamsMultiGPU.cpp:      KOKKOS_IMPL_CUDA_SAFE_CALL(cudaSetDevice(devices[i]));
lib/kokkos/core/unit_test/cuda/TestCuda_InterOp_StreamsMultiGPU.cpp:      KOKKOS_IMPL_CUDA_SAFE_CALL(cudaStreamDestroy(streams[i]));
lib/kokkos/core/unit_test/cuda/TestCuda_InterOp_StreamsMultiGPU.cpp:    ASSERT_EQ(exec0.cuda_device(), streams_and_devices.devices[0]);
lib/kokkos/core/unit_test/cuda/TestCuda_InterOp_StreamsMultiGPU.cpp:    ASSERT_EQ(exec1.cuda_device(), streams_and_devices.devices[1]);
lib/kokkos/core/unit_test/cuda/TestCuda_InterOp_StreamsMultiGPU.cpp:TEST(cuda_multi_gpu, managed_views) {
lib/kokkos/core/unit_test/cuda/TestCuda_InterOp_StreamsMultiGPU.cpp:TEST(cuda_multi_gpu, unmanaged_views) {
lib/kokkos/core/unit_test/cuda/TestCuda_InterOp_StreamsMultiGPU.cpp:    KOKKOS_IMPL_CUDA_SAFE_CALL(cudaSetDevice(execs[0].cuda_device()));
lib/kokkos/core/unit_test/cuda/TestCuda_InterOp_StreamsMultiGPU.cpp:    KOKKOS_IMPL_CUDA_SAFE_CALL(
lib/kokkos/core/unit_test/cuda/TestCuda_InterOp_StreamsMultiGPU.cpp:        cudaMalloc(reinterpret_cast<void **>(&p0), sizeof(int) * 100));
lib/kokkos/core/unit_test/cuda/TestCuda_InterOp_StreamsMultiGPU.cpp:    KOKKOS_IMPL_CUDA_SAFE_CALL(cudaSetDevice(execs[1].cuda_device()));
lib/kokkos/core/unit_test/cuda/TestCuda_InterOp_StreamsMultiGPU.cpp:    KOKKOS_IMPL_CUDA_SAFE_CALL(
lib/kokkos/core/unit_test/cuda/TestCuda_InterOp_StreamsMultiGPU.cpp:        cudaMalloc(reinterpret_cast<void **>(&p), sizeof(int) * 100));
lib/kokkos/core/unit_test/cuda/TestCuda_InterOp_StreamsMultiGPU.cpp:    KOKKOS_IMPL_CUDA_SAFE_CALL(cudaFree(p0));
lib/kokkos/core/unit_test/cuda/TestCuda_InterOp_StreamsMultiGPU.cpp:    KOKKOS_IMPL_CUDA_SAFE_CALL(cudaFree(p));
lib/kokkos/core/unit_test/cuda/TestCuda_InterOp_StreamsMultiGPU.cpp:TEST(cuda_multi_gpu, scratch_space) {
lib/kokkos/core/unit_test/cuda/TestCuda_Task.cpp:#include <TestCuda_Category.hpp>
lib/kokkos/core/unit_test/cuda/TestCuda_TeamScratchStreams.cpp:#include <TestCuda_Category.hpp>
lib/kokkos/core/unit_test/cuda/TestCuda_TeamScratchStreams.cpp:struct CudaStreamScratchTestFunctor {
lib/kokkos/core/unit_test/cuda/TestCuda_TeamScratchStreams.cpp:  using team_t    = Kokkos::TeamPolicy<Kokkos::Cuda>::member_type;
lib/kokkos/core/unit_test/cuda/TestCuda_TeamScratchStreams.cpp:  using scratch_t = Kokkos::View<int64_t*, Kokkos::Cuda::scratch_memory_space>;
lib/kokkos/core/unit_test/cuda/TestCuda_TeamScratchStreams.cpp:  Kokkos::View<int64_t, Kokkos::CudaSpace, Kokkos::MemoryTraits<Kokkos::Atomic>>
lib/kokkos/core/unit_test/cuda/TestCuda_TeamScratchStreams.cpp:  CudaStreamScratchTestFunctor(
lib/kokkos/core/unit_test/cuda/TestCuda_TeamScratchStreams.cpp:      Kokkos::View<int64_t, Kokkos::CudaSpace> counter_, int N_, int M_)
lib/kokkos/core/unit_test/cuda/TestCuda_TeamScratchStreams.cpp:void cuda_stream_scratch_test_one(
lib/kokkos/core/unit_test/cuda/TestCuda_TeamScratchStreams.cpp:    int N, int T, int M_base, Kokkos::View<int64_t, Kokkos::CudaSpace> counter,
lib/kokkos/core/unit_test/cuda/TestCuda_TeamScratchStreams.cpp:    Kokkos::Cuda cuda, int tid) {
lib/kokkos/core/unit_test/cuda/TestCuda_TeamScratchStreams.cpp:  Kokkos::TeamPolicy<Kokkos::Cuda> p(cuda, T, 64);
lib/kokkos/core/unit_test/cuda/TestCuda_TeamScratchStreams.cpp:  using scratch_t = Kokkos::View<int64_t*, Kokkos::Cuda::scratch_memory_space>;
lib/kokkos/core/unit_test/cuda/TestCuda_TeamScratchStreams.cpp:                         CudaStreamScratchTestFunctor(counter, N, M));
lib/kokkos/core/unit_test/cuda/TestCuda_TeamScratchStreams.cpp:void cuda_stream_scratch_test(
lib/kokkos/core/unit_test/cuda/TestCuda_TeamScratchStreams.cpp:    Kokkos::View<int64_t, Kokkos::CudaSpace> counter) {
lib/kokkos/core/unit_test/cuda/TestCuda_TeamScratchStreams.cpp:  cudaStream_t stream[4];
lib/kokkos/core/unit_test/cuda/TestCuda_TeamScratchStreams.cpp:  Kokkos::Cuda cuda[4];
lib/kokkos/core/unit_test/cuda/TestCuda_TeamScratchStreams.cpp:    cudaStreamCreate(&stream[i]);
lib/kokkos/core/unit_test/cuda/TestCuda_TeamScratchStreams.cpp:    cuda[i] = Kokkos::Cuda(stream[i]);
lib/kokkos/core/unit_test/cuda/TestCuda_TeamScratchStreams.cpp:      cuda_stream_scratch_test_one(N, T, M_base, counter, cuda[tid], tid);
lib/kokkos/core/unit_test/cuda/TestCuda_TeamScratchStreams.cpp:    cuda_stream_scratch_test_one(N, T, M_base, counter, cuda[tid], tid);
lib/kokkos/core/unit_test/cuda/TestCuda_TeamScratchStreams.cpp:    cuda_stream_scratch_test_one(N, T, M_base, counter, cuda[tid], tid);
lib/kokkos/core/unit_test/cuda/TestCuda_TeamScratchStreams.cpp:    cuda[i] = Kokkos::Cuda();
lib/kokkos/core/unit_test/cuda/TestCuda_TeamScratchStreams.cpp:    cudaStreamDestroy(stream[i]);
lib/kokkos/core/unit_test/cuda/TestCuda_TeamScratchStreams.cpp:TEST(cuda, team_scratch_1_streams) {
lib/kokkos/core/unit_test/cuda/TestCuda_TeamScratchStreams.cpp:  Kokkos::View<int64_t, Kokkos::CudaSpace> counter("C");
lib/kokkos/core/unit_test/cuda/TestCuda_TeamScratchStreams.cpp:  Impl::cuda_stream_scratch_test(N, T, M_base, counter);
lib/kokkos/core/unit_test/cuda/TestCuda_DebugSerialExecution.cpp:#include <TestCuda_Category.hpp>
lib/kokkos/core/unit_test/cuda/TestCuda_DebugSerialExecution.cpp:    Kokkos::parallel_for("CudaDebugSerialExecution::par_for", a.extent(0),
lib/kokkos/core/unit_test/cuda/TestCuda_DebugSerialExecution.cpp:    Kokkos::parallel_reduce("CudaDebugSerialExecution::par_red", a.extent(0),
lib/kokkos/core/unit_test/cuda/TestCuda_DebugSerialExecution.cpp:    Kokkos::parallel_scan("CudaDebugSerialExecution::par_scan", a.extent(0),
lib/kokkos/core/unit_test/cuda/TestCuda_DebugSerialExecution.cpp:TEST(cuda, debug_serial_execution) {
lib/kokkos/core/unit_test/cuda/TestCuda_DebugSerialExecution.cpp:#ifdef KOKKOS_IMPL_DEBUG_CUDA_SERIAL_EXECUTION
lib/kokkos/core/unit_test/cuda/TestCuda_DebugSerialExecution.cpp:    kokkos_impl_cuda_set_serial_execution(true);
lib/kokkos/core/unit_test/cuda/TestCuda_DebugSerialExecution.cpp:#ifdef KOKKOS_IMPL_DEBUG_CUDA_SERIAL_EXECUTION
lib/kokkos/core/unit_test/cuda/TestCuda_DebugSerialExecution.cpp:    kokkos_impl_cuda_set_serial_execution(false);
lib/kokkos/core/unit_test/cuda/TestCuda_DebugSerialExecution.cpp:#ifdef KOKKOS_IMPL_DEBUG_CUDA_SERIAL_EXECUTION
lib/kokkos/core/unit_test/cuda/TestCuda_DebugSerialExecution.cpp:#ifdef KOKKOS_IMPL_DEBUG_CUDA_SERIAL_EXECUTION
lib/kokkos/core/unit_test/cuda/TestCuda_DebugSerialExecution.cpp:    kokkos_impl_cuda_set_serial_execution(true);
lib/kokkos/core/unit_test/cuda/TestCuda_DebugSerialExecution.cpp:#ifdef KOKKOS_IMPL_DEBUG_CUDA_SERIAL_EXECUTION
lib/kokkos/core/unit_test/cuda/TestCuda_DebugSerialExecution.cpp:    kokkos_impl_cuda_set_serial_execution(false);
lib/kokkos/core/unit_test/cuda/TestCuda_DebugSerialExecution.cpp:#ifdef KOKKOS_IMPL_DEBUG_CUDA_SERIAL_EXECUTION
lib/kokkos/core/unit_test/cuda/TestCuda_DebugSerialExecution.cpp:#ifdef KOKKOS_IMPL_DEBUG_CUDA_SERIAL_EXECUTION
lib/kokkos/core/unit_test/cuda/TestCuda_DebugSerialExecution.cpp:    kokkos_impl_cuda_set_serial_execution(true);
lib/kokkos/core/unit_test/cuda/TestCuda_DebugSerialExecution.cpp:#ifdef KOKKOS_IMPL_DEBUG_CUDA_SERIAL_EXECUTION
lib/kokkos/core/unit_test/cuda/TestCuda_DebugSerialExecution.cpp:    kokkos_impl_cuda_set_serial_execution(false);
lib/kokkos/core/unit_test/cuda/TestCuda_DebugSerialExecution.cpp:#ifdef KOKKOS_IMPL_DEBUG_CUDA_SERIAL_EXECUTION
lib/kokkos/core/unit_test/cuda/TestCuda_InterOp_Streams.cpp:#include <TestCuda_Category.hpp>
lib/kokkos/core/unit_test/cuda/TestCuda_InterOp_Streams.cpp:// Test Interoperability with Cuda Streams
lib/kokkos/core/unit_test/cuda/TestCuda_InterOp_Streams.cpp:TEST(cuda, raw_cuda_streams) {
lib/kokkos/core/unit_test/cuda/TestCuda_InterOp_Streams.cpp:  cudaStream_t stream;
lib/kokkos/core/unit_test/cuda/TestCuda_InterOp_Streams.cpp:  cudaStreamCreate(&stream);
lib/kokkos/core/unit_test/cuda/TestCuda_InterOp_Streams.cpp:  cudaMalloc(&p, sizeof(int) * 100);
lib/kokkos/core/unit_test/cuda/TestCuda_InterOp_Streams.cpp:    Kokkos::parallel_for("Test::cuda::raw_cuda_stream::Range",
lib/kokkos/core/unit_test/cuda/TestCuda_InterOp_Streams.cpp:        "Test::cuda::raw_cuda_stream::RangeReduce",
lib/kokkos/core/unit_test/cuda/TestCuda_InterOp_Streams.cpp:    Kokkos::parallel_for("Test::cuda::raw_cuda_stream::MDRange",
lib/kokkos/core/unit_test/cuda/TestCuda_InterOp_Streams.cpp:        "Test::cuda::raw_cuda_stream::MDRangeReduce",
lib/kokkos/core/unit_test/cuda/TestCuda_InterOp_Streams.cpp:    Kokkos::parallel_for("Test::cuda::raw_cuda_stream::Team",
lib/kokkos/core/unit_test/cuda/TestCuda_InterOp_Streams.cpp:        "Test::cuda::raw_cuda_stream::Team",
lib/kokkos/core/unit_test/cuda/TestCuda_InterOp_Streams.cpp:  KOKKOS_IMPL_CUDA_SAFE_CALL(cudaDeviceSynchronize());
lib/kokkos/core/unit_test/cuda/TestCuda_InterOp_Streams.cpp:  cudaStreamDestroy(stream);
lib/kokkos/core/unit_test/cuda/TestCuda_InterOp_Streams.cpp:  cudaMemcpy(h_p, p, sizeof(int) * 100, cudaMemcpyDefault);
lib/kokkos/core/unit_test/cuda/TestCuda_InterOp_Streams.cpp:  KOKKOS_IMPL_CUDA_SAFE_CALL(cudaDeviceSynchronize());
lib/kokkos/core/unit_test/cuda/TestCuda_DebugPinUVMSpace.cpp:#include <TestCuda_Category.hpp>
lib/kokkos/core/unit_test/cuda/TestCuda_DebugPinUVMSpace.cpp:TEST(cuda, debug_pin_um_to_host) {
lib/kokkos/core/unit_test/cuda/TestCuda_DebugPinUVMSpace.cpp:  double time_cuda_space;
lib/kokkos/core/unit_test/cuda/TestCuda_DebugPinUVMSpace.cpp:  double time_cuda_host_pinned_space;
lib/kokkos/core/unit_test/cuda/TestCuda_DebugPinUVMSpace.cpp:  double time_cuda_uvm_space_not_pinned_1;
lib/kokkos/core/unit_test/cuda/TestCuda_DebugPinUVMSpace.cpp:  double time_cuda_uvm_space_pinned;
lib/kokkos/core/unit_test/cuda/TestCuda_DebugPinUVMSpace.cpp:  double time_cuda_uvm_space_not_pinned_2;
lib/kokkos/core/unit_test/cuda/TestCuda_DebugPinUVMSpace.cpp:    CopyFunctor<Kokkos::View<int*, Kokkos::CudaSpace>> f(N);
lib/kokkos/core/unit_test/cuda/TestCuda_DebugPinUVMSpace.cpp:    time_cuda_space = f.time_copy(R);
lib/kokkos/core/unit_test/cuda/TestCuda_DebugPinUVMSpace.cpp:    CopyFunctor<Kokkos::View<int*, Kokkos::CudaHostPinnedSpace>> f(N);
lib/kokkos/core/unit_test/cuda/TestCuda_DebugPinUVMSpace.cpp:    time_cuda_host_pinned_space = f.time_copy(R);
lib/kokkos/core/unit_test/cuda/TestCuda_DebugPinUVMSpace.cpp:    CopyFunctor<Kokkos::View<int*, Kokkos::CudaUVMSpace>> f(N);
lib/kokkos/core/unit_test/cuda/TestCuda_DebugPinUVMSpace.cpp:    time_cuda_uvm_space_not_pinned_1 = f.time_copy(R);
lib/kokkos/core/unit_test/cuda/TestCuda_DebugPinUVMSpace.cpp:#ifdef KOKKOS_IMPL_DEBUG_CUDA_PIN_UVM_TO_HOST
lib/kokkos/core/unit_test/cuda/TestCuda_DebugPinUVMSpace.cpp:    kokkos_impl_cuda_set_pin_uvm_to_host(true);
lib/kokkos/core/unit_test/cuda/TestCuda_DebugPinUVMSpace.cpp:    CopyFunctor<Kokkos::View<int*, Kokkos::CudaUVMSpace>> f(N);
lib/kokkos/core/unit_test/cuda/TestCuda_DebugPinUVMSpace.cpp:    time_cuda_uvm_space_pinned = f.time_copy(R);
lib/kokkos/core/unit_test/cuda/TestCuda_DebugPinUVMSpace.cpp:#ifdef KOKKOS_IMPL_DEBUG_CUDA_PIN_UVM_TO_HOST
lib/kokkos/core/unit_test/cuda/TestCuda_DebugPinUVMSpace.cpp:    kokkos_impl_cuda_set_pin_uvm_to_host(false);
lib/kokkos/core/unit_test/cuda/TestCuda_DebugPinUVMSpace.cpp:    CopyFunctor<Kokkos::View<int*, Kokkos::CudaUVMSpace>> f(N);
lib/kokkos/core/unit_test/cuda/TestCuda_DebugPinUVMSpace.cpp:    time_cuda_uvm_space_not_pinned_2 = f.time_copy(R);
lib/kokkos/core/unit_test/cuda/TestCuda_DebugPinUVMSpace.cpp:  bool uvm_approx_cuda_1 =
lib/kokkos/core/unit_test/cuda/TestCuda_DebugPinUVMSpace.cpp:      time_cuda_uvm_space_not_pinned_1 < time_cuda_space * 2.0;
lib/kokkos/core/unit_test/cuda/TestCuda_DebugPinUVMSpace.cpp:  bool uvm_approx_cuda_2 =
lib/kokkos/core/unit_test/cuda/TestCuda_DebugPinUVMSpace.cpp:      time_cuda_uvm_space_not_pinned_2 < time_cuda_space * 2.0;
lib/kokkos/core/unit_test/cuda/TestCuda_DebugPinUVMSpace.cpp:  bool pinned_slower_cuda = time_cuda_host_pinned_space > time_cuda_space * 2.0;
lib/kokkos/core/unit_test/cuda/TestCuda_DebugPinUVMSpace.cpp:  bool uvm_pinned_slower_cuda =
lib/kokkos/core/unit_test/cuda/TestCuda_DebugPinUVMSpace.cpp:      time_cuda_uvm_space_pinned > time_cuda_space * 2.0;
lib/kokkos/core/unit_test/cuda/TestCuda_DebugPinUVMSpace.cpp:  bool passed = uvm_approx_cuda_1 && uvm_approx_cuda_2 && pinned_slower_cuda &&
lib/kokkos/core/unit_test/cuda/TestCuda_DebugPinUVMSpace.cpp:#ifdef KOKKOS_IMPL_DEBUG_CUDA_PIN_UVM_TO_HOST
lib/kokkos/core/unit_test/cuda/TestCuda_DebugPinUVMSpace.cpp:                uvm_pinned_slower_cuda;
lib/kokkos/core/unit_test/cuda/TestCuda_DebugPinUVMSpace.cpp:                !uvm_pinned_slower_cuda;
lib/kokkos/core/unit_test/cuda/TestCuda_DebugPinUVMSpace.cpp:        "Time CudaSpace: %lf CudaUVMSpace_1: %lf CudaUVMSpace_2: %lf "
lib/kokkos/core/unit_test/cuda/TestCuda_DebugPinUVMSpace.cpp:        "CudaPinnedHostSpace: %lf CudaUVMSpace_Pinned: %lf\n",
lib/kokkos/core/unit_test/cuda/TestCuda_DebugPinUVMSpace.cpp:        time_cuda_space, time_cuda_uvm_space_not_pinned_1,
lib/kokkos/core/unit_test/cuda/TestCuda_DebugPinUVMSpace.cpp:        time_cuda_uvm_space_not_pinned_2, time_cuda_host_pinned_space,
lib/kokkos/core/unit_test/cuda/TestCuda_DebugPinUVMSpace.cpp:        time_cuda_uvm_space_pinned);
lib/kokkos/core/unit_test/TestCommonPolicyConstructors.hpp:// Disabling since EBO was not working with VS 16.11.3 and CUDA 11.4.2
lib/kokkos/core/unit_test/TestCommonPolicyConstructors.hpp:#if !(defined(_WIN32) && defined(KOKKOS_ENABLE_CUDA))
lib/kokkos/core/unit_test/TestComplex.hpp:#ifdef __CUDA_ARCH__
lib/kokkos/core/unit_test/TestComplex.hpp:#ifdef __CUDA_ARCH__
lib/kokkos/core/unit_test/TestTeam.hpp:#ifdef KOKKOS_ENABLE_CUDA
lib/kokkos/core/unit_test/TestTeam.hpp:    if (std::is_same<ExecSpace, Kokkos::Cuda>::value) team_size = 128;
lib/kokkos/core/unit_test/TestTeam.hpp:    // Choose a non-recommened (non-power of two for GPUs) team size
lib/kokkos/core/unit_test/TestMDRange.hpp:      // Launchbounds to ensure the tile fits into a CUDA block under register
lib/kokkos/core/unit_test/TestMDRange.hpp:      // Launchbounds to ensure the tile fits into a CUDA block under register
lib/kokkos/core/unit_test/TestMDRange.hpp:      // Launchbounds to ensure the tile fits into a CUDA block under register
lib/kokkos/core/unit_test/TestMDRange.hpp:      // Launchbounds to ensure the tile fits into a CUDA block under register
lib/kokkos/core/unit_test/TestMDRange.hpp:      // Launchbounds to ensure the tile fits into a CUDA block under register
lib/kokkos/core/unit_test/TestMDRange.hpp:      // Launchbounds to ensure the tile fits into a CUDA block under register
lib/kokkos/core/unit_test/TestMDRange.hpp:      // Launchbounds to ensure the tile fits into a CUDA block under register
lib/kokkos/core/unit_test/TestMDRange.hpp:      // Launchbounds to ensure the tile fits into a CUDA block under register
lib/kokkos/core/unit_test/TestMDRange.hpp:      // Launchbounds to ensure the tile fits into a CUDA block under register
lib/kokkos/core/unit_test/TestMDRange.hpp:      // Launchbounds to ensure the tile fits into a CUDA block under register
lib/kokkos/core/unit_test/TestMDRange.hpp:      // tile dims 3,3,3,3,3,3 more than cuda can handle with debugging
lib/kokkos/core/unit_test/TestMDRange.hpp:      // Launchbounds to ensure the tile fits into a CUDA block under register
lib/kokkos/core/unit_test/TestMDRange.hpp:      // Launchbounds to ensure the tile fits into a CUDA block under register
lib/kokkos/core/unit_test/TestMDRange.hpp:      // Launchbounds to ensure the tile fits into a CUDA block under register
lib/kokkos/core/unit_test/TestMDRange.hpp:      // Launchbounds to ensure the tile fits into a CUDA block under register
lib/kokkos/core/unit_test/TestMDRange.hpp:      // Launchbounds to ensure the tile fits into a CUDA block under register
lib/kokkos/core/unit_test/TestMDRange.hpp:      // Launchbounds to ensure the tile fits into a CUDA block under register
lib/kokkos/core/unit_test/TestMDRange.hpp:      // Launchbounds to ensure the tile fits into a CUDA block under register
lib/kokkos/core/unit_test/TestHostSharedPtrAccessOnDevice.hpp:  bool is_gpu =
lib/kokkos/core/unit_test/TestHostSharedPtrAccessOnDevice.hpp:    //  if on GPU: should not increase use_count, fp_d will not be tracked
lib/kokkos/core/unit_test/TestHostSharedPtrAccessOnDevice.hpp:    if (is_gpu)
lib/kokkos/core/unit_test/TestHostSharedPtrAccessOnDevice.hpp:    //   if default device is GPU: fp_h was untracked
lib/kokkos/core/unit_test/TestHostSharedPtrAccessOnDevice.hpp:    //  If on GPU: should not increase use_count of f1 and fp_d will not be
lib/kokkos/core/unit_test/TestHostSharedPtrAccessOnDevice.hpp:    if (is_gpu)
lib/kokkos/core/unit_test/TestHostSharedPtrAccessOnDevice.hpp:    //   if  if_gpu will not change use_count
lib/kokkos/core/unit_test/TestHostSharedPtrAccessOnDevice.hpp:    //   if !is_gpu will decrease use_count of f1
lib/kokkos/core/unit_test/TestHostSharedPtrAccessOnDevice.hpp:#ifdef KOKKOS_ENABLE_CUDA
lib/kokkos/core/unit_test/TestHostSharedPtrAccessOnDevice.hpp:  if (std::is_same<TEST_EXECSPACE, Kokkos::Cuda>::value)
lib/kokkos/core/unit_test/TestHostSharedPtrAccessOnDevice.hpp:    host_shared_ptr_test_reference_counting<Kokkos::CudaUVMSpace,
lib/kokkos/core/unit_test/TestHostSharedPtrAccessOnDevice.hpp:                                            Kokkos::CudaUVMSpace>();
lib/kokkos/core/unit_test/TestViewMapping_subview.hpp:#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ >= 1000
lib/kokkos/core/unit_test/TestViewMapping_subview.hpp:#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ >= 1000
lib/kokkos/core/unit_test/default/TestDefaultDeviceType_d.cpp:#if !defined(KOKKOS_ENABLE_CUDA) || defined(__CUDACC__)
lib/kokkos/core/unit_test/default/TestDefaultDeviceType_b1.cpp:#if !defined(KOKKOS_ENABLE_CUDA) || defined(__CUDACC__)
lib/kokkos/core/unit_test/default/TestDefaultDeviceType_a3.cpp:#if !defined(KOKKOS_ENABLE_CUDA) || defined(__CUDACC__)
lib/kokkos/core/unit_test/default/TestDefaultDeviceType_a1.cpp:#if !defined(KOKKOS_ENABLE_CUDA) || defined(__CUDACC__)
lib/kokkos/core/unit_test/default/TestDefaultDeviceType_b2.cpp:#if !defined(KOKKOS_ENABLE_CUDA) || defined(__CUDACC__)
lib/kokkos/core/unit_test/default/TestDefaultDeviceType_b3.cpp:#if !defined(KOKKOS_ENABLE_CUDA) || defined(__CUDACC__)
lib/kokkos/core/unit_test/default/TestDefaultDeviceType_c1.cpp:#if !defined(KOKKOS_ENABLE_CUDA) || defined(__CUDACC__)
lib/kokkos/core/unit_test/default/TestDefaultDeviceType_c2.cpp:#if !defined(KOKKOS_ENABLE_CUDA) || defined(__CUDACC__)
lib/kokkos/core/unit_test/default/TestDefaultDeviceType_c3.cpp:#if !defined(KOKKOS_ENABLE_CUDA) || defined(__CUDACC__)
lib/kokkos/core/unit_test/default/TestDefaultDeviceType.cpp:#if !defined(KOKKOS_ENABLE_CUDA) || defined(__CUDACC__)
lib/kokkos/core/unit_test/default/TestDefaultDeviceType_a2.cpp:#if !defined(KOKKOS_ENABLE_CUDA) || defined(__CUDACC__)
lib/kokkos/core/unit_test/TestCTestDevice.cpp:#include <impl/Kokkos_DeviceManagement.hpp>  // get_ctest_gpu
lib/kokkos/core/unit_test/TestCTestDevice.cpp:  setenv("CTEST_KOKKOS_DEVICE_TYPE", "gpus", 1);
lib/kokkos/core/unit_test/TestCTestDevice.cpp:  setenv("CTEST_RESOURCE_GROUP_3", "gpus", 1);
lib/kokkos/core/unit_test/TestCTestDevice.cpp:  unsetenv("CTEST_RESOURCE_GROUP_3_GPUS");
lib/kokkos/core/unit_test/TestCTestDevice.cpp:  setenv("CTEST_RESOURCE_GROUP_4", "gpus", 1);
lib/kokkos/core/unit_test/TestCTestDevice.cpp:  setenv("CTEST_RESOURCE_GROUP_4_GPUS", "id:2", 1);
lib/kokkos/core/unit_test/TestCTestDevice.cpp:  setenv("CTEST_RESOURCE_GROUP_5", "gpus", 1);
lib/kokkos/core/unit_test/TestCTestDevice.cpp:  setenv("CTEST_RESOURCE_GROUP_5_GPUS", "slots:1,id:2", 1);
lib/kokkos/core/unit_test/TestCTestDevice.cpp:  setenv("CTEST_RESOURCE_GROUP_6", "gpus", 1);
lib/kokkos/core/unit_test/TestCTestDevice.cpp:  setenv("CTEST_RESOURCE_GROUP_6_GPUS", "id:2,slots:1", 1);
lib/kokkos/core/unit_test/TestCTestDevice.cpp:  setenv("CTEST_RESOURCE_GROUP_7", "threads,gpus", 1);
lib/kokkos/core/unit_test/TestCTestDevice.cpp:  setenv("CTEST_RESOURCE_GROUP_7_GPUS", "id:3,slots:1", 1);
lib/kokkos/core/unit_test/TestCTestDevice.cpp:  setenv("CTEST_RESOURCE_GROUP_8", "gpus,threads", 1);
lib/kokkos/core/unit_test/TestCTestDevice.cpp:  setenv("CTEST_RESOURCE_GROUP_8_GPUS", "id:1,slots:1", 1);
lib/kokkos/core/unit_test/TestCTestDevice.cpp:  setenv("CTEST_RESOURCE_GROUP_9", "cores,gpus,threads", 1);
lib/kokkos/core/unit_test/TestCTestDevice.cpp:  setenv("CTEST_RESOURCE_GROUP_9_GPUS", "id:4,slots:1", 1);
lib/kokkos/core/unit_test/TestCTestDevice.cpp:  EXPECT_EQ(Kokkos::Impl::get_ctest_gpu(0), 0);
lib/kokkos/core/unit_test/TestCTestDevice.cpp:  EXPECT_EQ(Kokkos::Impl::get_ctest_gpu(0), 0);
lib/kokkos/core/unit_test/TestCTestDevice.cpp:      Kokkos::Impl::get_ctest_gpu(10), std::runtime_error,
lib/kokkos/core/unit_test/TestCTestDevice.cpp:      Kokkos::Impl::get_ctest_gpu(0), std::runtime_error,
lib/kokkos/core/unit_test/TestCTestDevice.cpp:      "Kokkos::Impl::get_ctest_gpu().");
lib/kokkos/core/unit_test/TestCTestDevice.cpp:      Kokkos::Impl::get_ctest_gpu(1), std::runtime_error,
lib/kokkos/core/unit_test/TestCTestDevice.cpp:      "Error: device type 'gpus' not included in CTEST_RESOURCE_GROUP_1. "
lib/kokkos/core/unit_test/TestCTestDevice.cpp:      "Raised by Kokkos::Impl::get_ctest_gpu().");
lib/kokkos/core/unit_test/TestCTestDevice.cpp:      Kokkos::Impl::get_ctest_gpu(2), std::runtime_error,
lib/kokkos/core/unit_test/TestCTestDevice.cpp:      "Error: device type 'gpus' not included in CTEST_RESOURCE_GROUP_2. "
lib/kokkos/core/unit_test/TestCTestDevice.cpp:      "Raised by Kokkos::Impl::get_ctest_gpu().");
lib/kokkos/core/unit_test/TestCTestDevice.cpp:      Kokkos::Impl::get_ctest_gpu(3), std::runtime_error,
lib/kokkos/core/unit_test/TestCTestDevice.cpp:      "Error: CTEST_RESOURCE_GROUP_3_GPUS is not specified. Raised by "
lib/kokkos/core/unit_test/TestCTestDevice.cpp:      "Kokkos::Impl::get_ctest_gpu().");
lib/kokkos/core/unit_test/TestCTestDevice.cpp:      Kokkos::Impl::get_ctest_gpu(4), std::runtime_error,
lib/kokkos/core/unit_test/TestCTestDevice.cpp:      "Error: invalid value of CTEST_RESOURCE_GROUP_4_GPUS: 'id:2'. Raised by "
lib/kokkos/core/unit_test/TestCTestDevice.cpp:      "Kokkos::Impl::get_ctest_gpu().");
lib/kokkos/core/unit_test/TestCTestDevice.cpp:      Kokkos::Impl::get_ctest_gpu(5), std::runtime_error,
lib/kokkos/core/unit_test/TestCTestDevice.cpp:      "Error: invalid value of CTEST_RESOURCE_GROUP_5_GPUS: 'slots:1,id:2'. "
lib/kokkos/core/unit_test/TestCTestDevice.cpp:      "Raised by Kokkos::Impl::get_ctest_gpu().");
lib/kokkos/core/unit_test/TestCTestDevice.cpp:  EXPECT_EQ(Kokkos::Impl::get_ctest_gpu(6), 2);
lib/kokkos/core/unit_test/TestCTestDevice.cpp:  EXPECT_EQ(Kokkos::Impl::get_ctest_gpu(7), 3);
lib/kokkos/core/unit_test/TestCTestDevice.cpp:  EXPECT_EQ(Kokkos::Impl::get_ctest_gpu(8), 1);
lib/kokkos/core/unit_test/TestCTestDevice.cpp:  EXPECT_EQ(Kokkos::Impl::get_ctest_gpu(9), 4);
lib/kokkos/core/unit_test/TestMDRangePolicyCTAD.cpp:  // Workaround for nvc++ (CUDA-11.7-NVHPC) ignoring [[maybe_unused]] on
lib/kokkos/core/unit_test/TestMDRangePolicyCTAD.cpp:  // Workaround for HIP-ROCm-5.2 "declared but never referenced"
lib/kokkos/core/unit_test/TestTaskScheduler.hpp:#if !defined(__HIP_DEVICE_COMPILE__) && !defined(__CUDA_ARCH__)
lib/kokkos/core/unit_test/TestTaskScheduler.hpp:#if !defined(__HIP_DEVICE_COMPILE__) && !defined(__CUDA_ARCH__)
lib/kokkos/core/src/Kokkos_ScratchSpace.hpp:      // in a CUDA build, so only print in debug mode.  The
lib/kokkos/core/src/decl/Kokkos_Declare_CUDA.hpp:#ifndef KOKKOS_DECLARE_CUDA_HPP
lib/kokkos/core/src/decl/Kokkos_Declare_CUDA.hpp:#define KOKKOS_DECLARE_CUDA_HPP
lib/kokkos/core/src/decl/Kokkos_Declare_CUDA.hpp:#if defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/core/src/decl/Kokkos_Declare_CUDA.hpp:#include <Cuda/Kokkos_Cuda.hpp>
lib/kokkos/core/src/decl/Kokkos_Declare_CUDA.hpp:#include <Cuda/Kokkos_Cuda_Half_Impl_Type.hpp>
lib/kokkos/core/src/decl/Kokkos_Declare_CUDA.hpp:#include <Cuda/Kokkos_Cuda_Half_Conversion.hpp>
lib/kokkos/core/src/decl/Kokkos_Declare_CUDA.hpp:#include <Cuda/Kokkos_Cuda_Parallel_MDRange.hpp>
lib/kokkos/core/src/decl/Kokkos_Declare_CUDA.hpp:#include <Cuda/Kokkos_Cuda_Parallel_Range.hpp>
lib/kokkos/core/src/decl/Kokkos_Declare_CUDA.hpp:#include <Cuda/Kokkos_Cuda_Parallel_Team.hpp>
lib/kokkos/core/src/decl/Kokkos_Declare_CUDA.hpp:#include <Cuda/Kokkos_Cuda_KernelLaunch.hpp>
lib/kokkos/core/src/decl/Kokkos_Declare_CUDA.hpp:#include <Cuda/Kokkos_Cuda_Instance.hpp>
lib/kokkos/core/src/decl/Kokkos_Declare_CUDA.hpp:#include <Cuda/Kokkos_Cuda_View.hpp>
lib/kokkos/core/src/decl/Kokkos_Declare_CUDA.hpp:#include <Cuda/Kokkos_Cuda_Team.hpp>
lib/kokkos/core/src/decl/Kokkos_Declare_CUDA.hpp:#include <Cuda/Kokkos_Cuda_Task.hpp>
lib/kokkos/core/src/decl/Kokkos_Declare_CUDA.hpp:#include <Cuda/Kokkos_Cuda_MDRangePolicy.hpp>
lib/kokkos/core/src/decl/Kokkos_Declare_CUDA.hpp:#include <Cuda/Kokkos_Cuda_UniqueToken.hpp>
lib/kokkos/core/src/decl/Kokkos_Declare_CUDA.hpp:#include <Cuda/Kokkos_Cuda_ZeroMemset.hpp>
lib/kokkos/core/src/decl/Kokkos_Declare_OPENACC.hpp:#ifndef KOKKOS_DECLARE_OPENACC_HPP
lib/kokkos/core/src/decl/Kokkos_Declare_OPENACC.hpp:#define KOKKOS_DECLARE_OPENACC_HPP
lib/kokkos/core/src/decl/Kokkos_Declare_OPENACC.hpp:#if defined(KOKKOS_ENABLE_OPENACC)
lib/kokkos/core/src/decl/Kokkos_Declare_OPENACC.hpp:#include <OpenACC/Kokkos_OpenACC.hpp>
lib/kokkos/core/src/decl/Kokkos_Declare_OPENACC.hpp:#include <OpenACC/Kokkos_OpenACCSpace.hpp>
lib/kokkos/core/src/decl/Kokkos_Declare_OPENACC.hpp:#include <OpenACC/Kokkos_OpenACC_DeepCopy.hpp>
lib/kokkos/core/src/decl/Kokkos_Declare_OPENACC.hpp:#include <OpenACC/Kokkos_OpenACC_SharedAllocationRecord.hpp>
lib/kokkos/core/src/decl/Kokkos_Declare_OPENACC.hpp:#include <OpenACC/Kokkos_OpenACC_ParallelFor_Range.hpp>
lib/kokkos/core/src/decl/Kokkos_Declare_OPENACC.hpp:#include <OpenACC/Kokkos_OpenACC_ParallelReduce_Range.hpp>
lib/kokkos/core/src/decl/Kokkos_Declare_OPENACC.hpp:#include <OpenACC/Kokkos_OpenACC_ParallelScan_Range.hpp>
lib/kokkos/core/src/decl/Kokkos_Declare_OPENACC.hpp:#include <OpenACC/Kokkos_OpenACC_MDRangePolicy.hpp>
lib/kokkos/core/src/decl/Kokkos_Declare_OPENACC.hpp:#include <OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp>
lib/kokkos/core/src/decl/Kokkos_Declare_OPENACC.hpp:#include <OpenACC/Kokkos_OpenACC_ParallelReduce_MDRange.hpp>
lib/kokkos/core/src/decl/Kokkos_Declare_OPENACC.hpp:#include <OpenACC/Kokkos_OpenACC_ParallelFor_Team.hpp>
lib/kokkos/core/src/decl/Kokkos_Declare_OPENACC.hpp:#include <OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp>
lib/kokkos/core/src/Kokkos_Atomic.hpp:/// directives and native CUDA intrinsics.
lib/kokkos/core/src/Kokkos_Atomic.hpp:///   - NVCC (for CUDA device code only)
lib/kokkos/core/src/Kokkos_Vectorization.hpp:#if defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/core/src/Kokkos_Vectorization.hpp:#include <Cuda/Kokkos_Cuda_Vectorization.hpp>
lib/kokkos/core/src/Kokkos_WorkGraphPolicy.hpp:#ifdef KOKKOS_ENABLE_CUDA
lib/kokkos/core/src/Kokkos_WorkGraphPolicy.hpp:#include "Cuda/Kokkos_Cuda_WorkGraphPolicy.hpp"
lib/kokkos/core/src/Kokkos_Macros.hpp: *  KOKKOS_ENABLE_CUDA                Kokkos::Cuda execution and memory spaces
lib/kokkos/core/src/Kokkos_Macros.hpp: *  KOKKOS_ENABLE_CUDA_UVM            Use CUDA UVM for Cuda memory space.
lib/kokkos/core/src/Kokkos_Macros.hpp:#include <impl/Kokkos_NvidiaGpuArchitectures.hpp>
lib/kokkos/core/src/Kokkos_Macros.hpp:    (defined(KOKKOS_ENABLE_THREADS) || defined(KOKKOS_ENABLE_CUDA) ||     \
lib/kokkos/core/src/Kokkos_Macros.hpp:     defined(KOKKOS_ENABLE_SYCL) || defined(KOKKOS_ENABLE_OPENACC))
lib/kokkos/core/src/Kokkos_Macros.hpp:// NVIDIA compiler is being used.
lib/kokkos/core/src/Kokkos_Macros.hpp:// NOTE: There is no __CUDACC_VER_PATCH__ officially, its __CUDACC_VER_BUILD__
lib/kokkos/core/src/Kokkos_Macros.hpp:  __CUDACC_VER_MAJOR__ * 100 + __CUDACC_VER_MINOR__ * 10
lib/kokkos/core/src/Kokkos_Macros.hpp://#if !defined( __CUDA_ARCH__ ) // Not compiling Cuda code to 'ptx'.
lib/kokkos/core/src/Kokkos_Macros.hpp:#if defined(__CUDA_ARCH__)
lib/kokkos/core/src/Kokkos_Macros.hpp:#if 1 < ((defined(KOKKOS_ENABLE_DEFAULT_DEVICE_TYPE_CUDA) ? 1 : 0) +         \
lib/kokkos/core/src/Kokkos_Macros.hpp:         (defined(KOKKOS_ENABLE_DEFAULT_DEVICE_TYPE_OPENACC) ? 1 : 0) +      \
lib/kokkos/core/src/Kokkos_Macros.hpp:// Priority: CUDA, HIP, SYCL, OPENACC, OPENMPTARGET, OPENMP, THREADS, HPX,
lib/kokkos/core/src/Kokkos_Macros.hpp:#if defined(KOKKOS_ENABLE_DEFAULT_DEVICE_TYPE_CUDA)
lib/kokkos/core/src/Kokkos_Macros.hpp:#elif defined(KOKKOS_ENABLE_DEFAULT_DEVICE_TYPE_OPENACC)
lib/kokkos/core/src/Kokkos_Macros.hpp:#elif defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/core/src/Kokkos_Macros.hpp:#define KOKKOS_ENABLE_DEFAULT_DEVICE_TYPE_CUDA
lib/kokkos/core/src/Kokkos_Macros.hpp:#elif defined(KOKKOS_ENABLE_OPENACC)
lib/kokkos/core/src/Kokkos_Macros.hpp:#define KOKKOS_ENABLE_DEFAULT_DEVICE_TYPE_OPENACC
lib/kokkos/core/src/Kokkos_Macros.hpp:#if defined(KOKKOS_ENABLE_CUDA) && defined(KOKKOS_COMPILER_NVHPC)
lib/kokkos/core/src/Kokkos_Macros.hpp:#ifdef KOKKOS_ENABLE_OPENACC
lib/kokkos/core/src/Kokkos_Macros.hpp:#include <openacc.h>
lib/kokkos/core/src/Kokkos_Macros.hpp:// FIXME_OPENACC acc_on_device is a non-constexpr function
lib/kokkos/core/src/Kokkos_Macros.hpp:#if (defined(KOKKOS_ENABLE_CUDA) && defined(__CUDA_ARCH__)) ||         \
lib/kokkos/core/src/Kokkos_Macros.hpp:// If compiling with CUDA, we must use relocatable device code to enable the
lib/kokkos/core/src/Kokkos_Macros.hpp:#if defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/core/src/Kokkos_Macros.hpp:#if defined(KOKKOS_ENABLE_CUDA_RELOCATABLE_DEVICE_CODE)
lib/kokkos/core/src/Kokkos_Macros.hpp:#if defined(KOKKOS_ENABLE_CUDA) && defined(KOKKOS_ENABLE_DEPRECATED_CODE_4)
lib/kokkos/core/src/Kokkos_Macros.hpp:#define KOKKOS_ENABLE_CUDA_LDG_INTRINSIC
lib/kokkos/core/src/Kokkos_Macros.hpp:// WORKAROUND for AMD aomp which apparently defines CUDA_ARCH when building for
lib/kokkos/core/src/Kokkos_Macros.hpp:// AMD GPUs with OpenMP Target ???
lib/kokkos/core/src/Kokkos_Macros.hpp:#if defined(__CUDA_ARCH__) && !defined(__CUDACC__) && \
lib/kokkos/core/src/Kokkos_Macros.hpp:    !defined(KOKKOS_ENABLE_HIP) && !defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/core/src/Kokkos_Macros.hpp:#undef __CUDA_ARCH__
lib/kokkos/core/src/Kokkos_Macros.hpp:#if (defined(KOKKOS_IMPL_WINDOWS_CUDA) || defined(KOKKOS_COMPILER_MSVC)) && \
lib/kokkos/core/src/fwd/Kokkos_Fwd_CUDA.hpp:#ifndef KOKKOS_CUDA_FWD_HPP_
lib/kokkos/core/src/fwd/Kokkos_Fwd_CUDA.hpp:#define KOKKOS_CUDA_FWD_HPP_
lib/kokkos/core/src/fwd/Kokkos_Fwd_CUDA.hpp:#if defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/core/src/fwd/Kokkos_Fwd_CUDA.hpp:class CudaSpace;            ///< Memory space on Cuda GPU
lib/kokkos/core/src/fwd/Kokkos_Fwd_CUDA.hpp:class CudaUVMSpace;         ///< Memory space on Cuda GPU with UVM
lib/kokkos/core/src/fwd/Kokkos_Fwd_CUDA.hpp:class CudaHostPinnedSpace;  ///< Memory space on Host accessible to Cuda GPU
lib/kokkos/core/src/fwd/Kokkos_Fwd_CUDA.hpp:class Cuda;                 ///< Execution space for Cuda GPU
lib/kokkos/core/src/fwd/Kokkos_Fwd_CUDA.hpp:void cuda_prefetch_pointer(const ExecSpace& /*space*/, const void* /*ptr*/,
lib/kokkos/core/src/fwd/Kokkos_Fwd_CUDA.hpp:void cuda_prefetch_pointer(const Cuda& space, const void* ptr, size_t bytes,
lib/kokkos/core/src/fwd/Kokkos_Fwd_OPENACC.hpp:#ifndef KOKKOS_OPENACC_FWD_HPP_
lib/kokkos/core/src/fwd/Kokkos_Fwd_OPENACC.hpp:#define KOKKOS_OPENACC_FWD_HPP_
lib/kokkos/core/src/fwd/Kokkos_Fwd_OPENACC.hpp:#if defined(KOKKOS_ENABLE_OPENACC)
lib/kokkos/core/src/fwd/Kokkos_Fwd_OPENACC.hpp:class OpenACC;  ///< OpenACC execution space.
lib/kokkos/core/src/fwd/Kokkos_Fwd_OPENACC.hpp:class OpenACCSpace;
lib/kokkos/core/src/fwd/Kokkos_Fwd_HIP.hpp:class HIPSpace;            ///< Memory space on HIP GPU
lib/kokkos/core/src/fwd/Kokkos_Fwd_HIP.hpp:class HIPHostPinnedSpace;  ///< Memory space on Host accessible to HIP GPU
lib/kokkos/core/src/fwd/Kokkos_Fwd_HIP.hpp:class HIPManagedSpace;     ///< Memory migratable between Host and HIP GPU
lib/kokkos/core/src/fwd/Kokkos_Fwd_HIP.hpp:class HIP;                 ///< Execution space for HIP GPU
lib/kokkos/core/src/Kokkos_Printf.hpp:// backends. The GPU backends always return 1 and NVHPC only compiles if we
lib/kokkos/core/src/Kokkos_Printf.hpp:#if defined(KOKKOS_ENABLE_OPENMPTARGET) && defined(KOKKOS_ARCH_INTEL_GPU)
lib/kokkos/core/src/Kokkos_Core_fwd.hpp:/// Kokkos::Cuda, Kokkos::Experimental::OpenMPTarget, Kokkos::OpenMP,
lib/kokkos/core/src/Kokkos_Core_fwd.hpp:#if defined(KOKKOS_ENABLE_DEFAULT_DEVICE_TYPE_CUDA)
lib/kokkos/core/src/Kokkos_Core_fwd.hpp:using DefaultExecutionSpace KOKKOS_IMPL_DEFAULT_EXEC_SPACE_ANNOTATION = Cuda;
lib/kokkos/core/src/Kokkos_Core_fwd.hpp:#elif defined(KOKKOS_ENABLE_DEFAULT_DEVICE_TYPE_OPENACC)
lib/kokkos/core/src/Kokkos_Core_fwd.hpp:    Experimental::OpenACC;
lib/kokkos/core/src/Kokkos_Core_fwd.hpp:    "At least one of the following execution spaces must be defined in order to use Kokkos: Kokkos::Cuda, Kokkos::HIP, Kokkos::Experimental::SYCL, Kokkos::Experimental::OpenMPTarget, Kokkos::Experimental::OpenACC, Kokkos::OpenMP, Kokkos::Threads, Kokkos::Experimental::HPX, or Kokkos::Serial."
lib/kokkos/core/src/Kokkos_Core_fwd.hpp:#if defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/core/src/Kokkos_Core_fwd.hpp:using SharedSpace = CudaUVMSpace;
lib/kokkos/core/src/Kokkos_Core_fwd.hpp:#elif !defined(KOKKOS_ENABLE_OPENACC) && !defined(KOKKOS_ENABLE_OPENMPTARGET)
lib/kokkos/core/src/Kokkos_Core_fwd.hpp:#if defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/core/src/Kokkos_Core_fwd.hpp:using SharedHostPinnedSpace = CudaHostPinnedSpace;
lib/kokkos/core/src/Kokkos_Core_fwd.hpp:#elif !defined(KOKKOS_ENABLE_OPENACC) && !defined(KOKKOS_ENABLE_OPENMPTARGET)
lib/kokkos/core/src/HPX/Kokkos_HPX.hpp:                20 * 1024 * 1024);    // Limit to keep compatibility with CUDA
lib/kokkos/core/src/Kokkos_Atomics_Desul_Wrapper.hpp:    !defined(__CUDA_ARCH__)
lib/kokkos/core/src/Kokkos_Concepts.hpp:/**\brief Specify Launch Bounds for CUDA execution.
lib/kokkos/core/src/Kokkos_Concepts.hpp:#if defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/core/src/Kokkos_Concepts.hpp:          || std::is_same<memory_space, Kokkos::CudaUVMSpace>::value ||
lib/kokkos/core/src/Kokkos_Concepts.hpp:          std::is_same<memory_space, Kokkos::CudaHostPinnedSpace>::value
lib/kokkos/core/src/Kokkos_Concepts.hpp:#if defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/core/src/Kokkos_Concepts.hpp:      std::is_same<execution_space, Kokkos::Cuda>::value ||
lib/kokkos/core/src/Kokkos_MemoryPool.hpp:#if defined(KOKKOS_ENABLE_SYCL) && !defined(KOKKOS_ARCH_INTEL_GPU)
lib/kokkos/core/src/Kokkos_MemoryPool.hpp:#ifdef __CUDA_ARCH__  // FIXME_CUDA
lib/kokkos/core/src/Kokkos_Abort.hpp:#ifdef KOKKOS_ENABLE_CUDA
lib/kokkos/core/src/Kokkos_Abort.hpp:#include <Cuda/Kokkos_Cuda_abort.hpp>
lib/kokkos/core/src/Kokkos_Abort.hpp:#if defined(KOKKOS_ENABLE_CUDA) && defined(__CUDA_ARCH__)
lib/kokkos/core/src/Kokkos_Abort.hpp:// cuda_abort aborts when building for other platforms than macOS
lib/kokkos/core/src/Kokkos_Abort.hpp:#elif !defined(KOKKOS_ENABLE_OPENMPTARGET) && !defined(KOKKOS_ENABLE_OPENACC)
lib/kokkos/core/src/Kokkos_Abort.hpp:    (defined(KOKKOS_ENABLE_CUDA) && defined(KOKKOS_ENABLE_DEBUG_BOUNDS_CHECK))
lib/kokkos/core/src/Kokkos_Abort.hpp:#if defined(KOKKOS_ENABLE_CUDA) || defined(KOKKOS_ENABLE_HIP) ||          \
lib/kokkos/core/src/Kokkos_Abort.hpp:    defined(KOKKOS_ENABLE_OPENACC)
lib/kokkos/core/src/Kokkos_Abort.hpp:#if defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/core/src/Kokkos_Abort.hpp:  ::Kokkos::Impl::cuda_abort(msg);
lib/kokkos/core/src/Kokkos_Abort.hpp:#elif defined(KOKKOS_ENABLE_OPENMPTARGET) || defined(KOKKOS_ENABLE_OPENACC)
lib/kokkos/core/src/Kokkos_Abort.hpp:  printf("%s", msg);  // FIXME_OPENMPTARGET FIXME_OPENACC
lib/kokkos/core/src/traits/Kokkos_Traits_fwd.hpp:// Without this the CUDA side does proper EBO while MSVC doesn't
lib/kokkos/core/src/traits/Kokkos_Traits_fwd.hpp:// leading to mismatched sizes of the driver objects (CudaParallel)
lib/kokkos/core/src/traits/Kokkos_Traits_fwd.hpp:#if defined(_WIN32) && defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/core/src/SYCL/Kokkos_SYCL_TeamPolicy.hpp:      // FIXME_SYCL Avoid requesting too many registers on NVIDIA GPUs.
lib/kokkos/core/src/SYCL/Kokkos_SYCL_TeamPolicy.hpp:#if defined(KOKKOS_IMPL_ARCH_NVIDIA_GPU)
lib/kokkos/core/src/SYCL/Kokkos_SYCL_TeamPolicy.hpp:      // FIXME_SYCL Avoid requesting too many registers on NVIDIA GPUs.
lib/kokkos/core/src/SYCL/Kokkos_SYCL_TeamPolicy.hpp:#if defined(KOKKOS_IMPL_ARCH_NVIDIA_GPU)
lib/kokkos/core/src/SYCL/Kokkos_SYCL_ParallelFor_MDRange.hpp:#include <impl/KokkosExp_IterateTileGPU.hpp>
lib/kokkos/core/src/SYCL/Kokkos_SYCL_WorkgroupReduction.hpp:#if defined(KOKKOS_ARCH_INTEL_GPU) || defined(KOKKOS_IMPL_ARCH_NVIDIA_GPU)
lib/kokkos/core/src/SYCL/Kokkos_SYCL_WorkgroupReduction.hpp:#if defined(KOKKOS_ARCH_INTEL_GPU) || defined(KOKKOS_IMPL_ARCH_NVIDIA_GPU)
lib/kokkos/core/src/SYCL/Kokkos_SYCL_ParallelScan_Range.hpp:#if defined(KOKKOS_ARCH_INTEL_GPU) || defined(KOKKOS_IMPL_ARCH_NVIDIA_GPU)
lib/kokkos/core/src/SYCL/Kokkos_SYCL_ParallelScan_Range.hpp:#if defined(KOKKOS_ARCH_INTEL_GPU) || defined(KOKKOS_IMPL_ARCH_NVIDIA_GPU)
lib/kokkos/core/src/SYCL/Kokkos_SYCL_Instance.hpp:  // compiling for the CUDA backend. Storing pointers instead works around this.
lib/kokkos/core/src/SYCL/Kokkos_SYCL_Instance.hpp:#if defined(SYCL_DEVICE_COPYABLE) && defined(KOKKOS_ARCH_INTEL_GPU)
lib/kokkos/core/src/SYCL/Kokkos_SYCL_Instance.hpp:#if defined(SYCL_DEVICE_COPYABLE) && defined(KOKKOS_ARCH_INTEL_GPU)
lib/kokkos/core/src/SYCL/Kokkos_SYCL_Instance.hpp:#if defined(SYCL_DEVICE_COPYABLE) && defined(KOKKOS_ARCH_INTEL_GPU)
lib/kokkos/core/src/SYCL/Kokkos_SYCL_Half_Impl_Type.hpp:#elif defined(SYCL_EXT_ONEAPI_BFLOAT16) && defined(KOKKOS_ARCH_INTEL_GPU)
lib/kokkos/core/src/SYCL/Kokkos_SYCL_Half_Impl_Type.hpp:// on Nvidia GPUs but SYCL_EXT_ONEAPI_BFLOAT16 is defined even for lower compute
lib/kokkos/core/src/SYCL/Kokkos_SYCL_ParallelReduce_Range.hpp:#ifndef KOKKOS_ARCH_INTEL_GPU
lib/kokkos/core/src/SYCL/Kokkos_SYCL_Team.hpp:#if defined(KOKKOS_ARCH_INTEL_GPU) || defined(KOKKOS_IMPL_ARCH_NVIDIA_GPU)
lib/kokkos/core/src/SYCL/Kokkos_SYCL_Team.hpp:// This is the same code as in CUDA and largely the same as in OpenMPTarget
lib/kokkos/core/src/SYCL/Kokkos_SYCL_Team.hpp:  // implementation leads to a deadlock only for SYCL+CUDA if not all threads in
lib/kokkos/core/src/SYCL/Kokkos_SYCL_Team.hpp:  // a subgroup see this barrier. For SYCL on Intel GPUs, the subgroup barrier
lib/kokkos/core/src/SYCL/Kokkos_SYCL_Instance.cpp:      "unsupported backend type! For this GPU architecture, only " #REQUIRED \
lib/kokkos/core/src/SYCL/Kokkos_SYCL_Instance.cpp:#if defined(KOKKOS_ARCH_INTEL_GPU)
lib/kokkos/core/src/SYCL/Kokkos_SYCL_Instance.cpp:#elif defined(KOKKOS_IMPL_ARCH_NVIDIA_GPU)
lib/kokkos/core/src/SYCL/Kokkos_SYCL_Instance.cpp:                                         sycl::backend::ext_oneapi_cuda);
lib/kokkos/core/src/SYCL/Kokkos_SYCL_Instance.cpp:#elif defined(KOKKOS_ARCH_AMD_GPU)
lib/kokkos/core/src/SYCL/Kokkos_SYCL_Instance.cpp:  // FIXME_SYCL this should give the correct value for NVIDIA GPUs
lib/kokkos/core/src/SYCL/Kokkos_SYCL.cpp:      case sycl::info::device_type::gpu: device_type = "gpu"; break;
lib/kokkos/core/src/SYCL/Kokkos_SYCL.cpp:      ::Kokkos::Impl::get_gpu(settings).value_or(visible_devices[0]);
lib/kokkos/core/src/SYCL/Kokkos_SYCL.cpp:            << "\nIs GPU: " << device.is_gpu()
lib/kokkos/core/src/SYCL/Kokkos_SYCL.cpp:#if defined(KOKKOS_ARCH_INTEL_GPU) || defined(KOKKOS_IMPL_ARCH_NVIDIA_GPU) || \
lib/kokkos/core/src/SYCL/Kokkos_SYCL.cpp:    defined(KOKKOS_ARCH_AMD_GPU)
lib/kokkos/core/src/SYCL/Kokkos_SYCL.cpp:      sycl::device::get_devices(sycl::info::device_type::gpu);
lib/kokkos/core/src/SYCL/Kokkos_SYCL.cpp:#if defined(KOKKOS_ARCH_INTEL_GPU)
lib/kokkos/core/src/SYCL/Kokkos_SYCL.cpp:#elif defined(KOKKOS_IMPL_ARCH_NVIDIA_GPU)
lib/kokkos/core/src/SYCL/Kokkos_SYCL.cpp:  sycl::backend backend = sycl::backend::ext_oneapi_cuda;
lib/kokkos/core/src/SYCL/Kokkos_SYCL.cpp:#elif defined(KOKKOS_ARCH_AMD_GPU)
lib/kokkos/core/src/SYCL/Kokkos_SYCL_ParallelReduce_MDRange.hpp:        // REMEMBER swap local x<->y to be conforming with Cuda/HIP
lib/kokkos/core/src/SYCL/Kokkos_SYCL_ParallelReduce_MDRange.hpp:              // SWAPPED here to be conforming with CUDA implementation
lib/kokkos/core/src/Kokkos_MathematicalFunctions.hpp:// isinf, isnan, and isinfinite do not work on Windows with CUDA with std::
lib/kokkos/core/src/Kokkos_MathematicalFunctions.hpp:#if defined(_WIN32) && defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/core/src/Kokkos_MathematicalFunctions.hpp:#if defined(KOKKOS_ENABLE_CUDA) || defined(KOKKOS_ENABLE_HIP) || \
lib/kokkos/core/src/Kokkos_MathematicalFunctions.hpp:// non-standard math functions provided by CUDA/HIP/SYCL
lib/kokkos/core/src/Kokkos_MathematicalFunctions.hpp:#if defined(KOKKOS_ENABLE_CUDA) || defined(KOKKOS_ENABLE_HIP)
lib/kokkos/core/src/Kokkos_MathematicalFunctions.hpp:#if defined(KOKKOS_ENABLE_CUDA) || defined(KOKKOS_ENABLE_HIP)
lib/kokkos/core/src/Kokkos_View.hpp: * different \c Space types.  For example, with the Cuda device,
lib/kokkos/core/src/Kokkos_View.hpp: * cache, whereas the non-GPU devices have no such hardware construct.
lib/kokkos/core/src/Kokkos_View.hpp: * doSomething (View<double*, Cuda> out,
lib/kokkos/core/src/Kokkos_View.hpp: *              View<const double*, Cuda> in)
lib/kokkos/core/src/Kokkos_View.hpp: *   View<const double*, Cuda, RandomAccess> in_rr = in;
lib/kokkos/core/src/Kokkos_View.hpp:// FIXME_OPENMPTARGET - The `declare target` is needed for the Intel GPUs with
lib/kokkos/core/src/Kokkos_Pair.hpp:/// \brief Replacement for std::pair that works on CUDA devices.
lib/kokkos/core/src/Kokkos_Pair.hpp:/// called on a CUDA device, such as an NVIDIA GPU.  This struct
lib/kokkos/core/src/Kokkos_Pair.hpp:/// CUDA device as well as on the host.
lib/kokkos/core/src/Kokkos_Pair.hpp:  /// CUDA device.  It is meant to be called on the host, if the user
lib/kokkos/core/src/Kokkos_Pair.hpp:  /// CUDA device.  It is meant to be called on the host, if the user
lib/kokkos/core/src/Kokkos_Pair.hpp:  /// CUDA device.  It is meant to be called on the host, if the user
lib/kokkos/core/src/Kokkos_Pair.hpp:  /// CUDA device.  It is meant to be called on the host, if the user
lib/kokkos/core/src/OpenMPTarget/Kokkos_OpenMPTarget_ParallelFor_Team.hpp:#if !defined(KOKKOS_IMPL_OPENMPTARGET_HIERARCHICAL_INTEL_GPU)
lib/kokkos/core/src/OpenMPTarget/Kokkos_OpenMPTarget_ParallelScan_Range.hpp:            // FIXME_OPENMPTARGET We seem to access memory illegaly on AMD GPUs
lib/kokkos/core/src/OpenMPTarget/Kokkos_OpenMPTarget_ParallelScan_Range.hpp:#if defined(KOKKOS_ARCH_AMD_GPU) && !defined(KOKKOS_ARCH_AMD_GFX1030) && \
lib/kokkos/core/src/OpenMPTarget/Kokkos_OpenMPTarget_ParallelScan_Team.hpp:#ifndef KOKKOS_ARCH_INTEL_GPU
lib/kokkos/core/src/OpenMPTarget/Kokkos_OpenMPTarget_ParallelScan_Team.hpp:// This is largely the same code as in HIP and CUDA except for the member name
lib/kokkos/core/src/OpenMPTarget/Kokkos_OpenMPTarget_ParallelScan_Team.hpp:  //   Note this thing is called .member in the CUDA specialization of
lib/kokkos/core/src/OpenMPTarget/Kokkos_OpenMPTarget_ParallelReduce_Team.hpp:#ifndef KOKKOS_ARCH_INTEL_GPU
lib/kokkos/core/src/OpenMPTarget/Kokkos_OpenMPTarget_Parallel_Common.hpp:    // based on NVIDIA-V100 and should be modifid to be based on the
lib/kokkos/core/src/OpenMPTarget/Kokkos_OpenMPTarget_Parallel_Common.hpp:#if !defined(KOKKOS_IMPL_OPENMPTARGET_HIERARCHICAL_INTEL_GPU)
lib/kokkos/core/src/OpenMPTarget/Kokkos_OpenMPTarget_Instance.cpp:#if defined(KOKKOS_IMPL_ARCH_NVIDIA_GPU)
lib/kokkos/core/src/OpenMPTarget/Kokkos_OpenMPTarget_Instance.cpp:#elif defined(KOKKOS_ARCH_INTEL_GPU)
lib/kokkos/core/src/OpenMPTarget/Kokkos_OpenMPTarget_Instance.cpp:#if defined(KOKKOS_IMPL_OPENMPTARGET_HIERARCHICAL_INTEL_GPU)
lib/kokkos/core/src/OpenMPTarget/Kokkos_OpenMPTarget_Instance.cpp:  os << "Defined KOKKOS_IMPL_OPENMPTARGET_HIERARCHICAL_INTEL_GPU: Workaround "
lib/kokkos/core/src/OpenMPTarget/Kokkos_OpenMPTarget_Instance.cpp:        "hierarchical parallelism for Intel GPUs.";
lib/kokkos/core/src/OpenMPTarget/Kokkos_OpenMPTarget_Instance.cpp:  // FIXME_OPENMPTARGET:  Only fix the number of teams for NVIDIA architectures
lib/kokkos/core/src/OpenMPTarget/Kokkos_OpenMPTarget_Instance.cpp:#if defined(KOKKOS_IMPL_ARCH_NVIDIA_GPU) && defined(KOKKOS_COMPILER_CLANG) && \
lib/kokkos/core/src/OpenMPTarget/Kokkos_OpenMPTarget_Instance.cpp:  using Kokkos::Impl::get_gpu;
lib/kokkos/core/src/OpenMPTarget/Kokkos_OpenMPTarget_Instance.cpp:  const int device_num = get_gpu(settings).value_or(visible_devices[0]);
lib/kokkos/core/src/OpenMPTarget/Kokkos_OpenMPTarget_Instance.cpp:// pointer Error - CUDA error: named symbol not found
lib/kokkos/core/src/OpenMPTarget/Kokkos_OpenMPTarget_Macros.hpp:#if defined(KOKKOS_ARCH_INTEL_GPU)
lib/kokkos/core/src/OpenMPTarget/Kokkos_OpenMPTarget_Macros.hpp:#define KOKKOS_IMPL_OPENMPTARGET_HIERARCHICAL_INTEL_GPU
lib/kokkos/core/src/OpenMPTarget/Kokkos_OpenMPTarget_Macros.hpp:// Define a macro for llvm compiler greater than version 17 and on NVIDIA and
lib/kokkos/core/src/OpenMPTarget/Kokkos_OpenMPTarget_Macros.hpp:// AMD GPUs. This would be useful in cases where non-OpenMP standard llvm
lib/kokkos/core/src/OpenMPTarget/Kokkos_OpenMPTarget_Macros.hpp:    (defined(KOKKOS_ARCH_AMD_GPU) || defined(KOKKOS_IMPL_ARCH_NVIDIA_GPU))
lib/kokkos/core/src/OpenMPTarget/Kokkos_OpenMPTarget.hpp:  //! Initialize, telling the CUDA run-time library which device to use.
lib/kokkos/core/src/OpenMPTarget/Kokkos_OpenMPTarget_Instance.hpp:  //! Initialize, telling the CUDA run-time library which device to use.
lib/kokkos/core/src/OpenMPTarget/Kokkos_OpenMPTarget_Parallel.hpp:  // FIXME_OPENMPTARGET : Currently this routine is a copy of the Cuda
lib/kokkos/core/src/OpenMPTarget/Kokkos_OpenMPTarget_Parallel.hpp:        level == 0 ? 1024 * 40 :  // 48kB is the max for CUDA, but we need some
lib/kokkos/core/src/OpenMPTarget/Kokkos_OpenMPTarget_Parallel.hpp:  // teams possible is calculated based on NVIDIA's Volta GPU. In
lib/kokkos/core/src/OpenMP/Kokkos_OpenMP_Instance.hpp:#if !defined(_OPENMP) && !defined(__CUDA_ARCH__) && \
lib/kokkos/core/src/OpenMP/Kokkos_OpenMP_Parallel_For.hpp:    // prevent bug in NVHPC 21.9/CUDA 11.4 (entering zero iterations loop)
lib/kokkos/core/src/OpenMP/Kokkos_OpenMP_Team.hpp:                20 * 1024 * 1024);    // Limit to keep compatibility with CUDA
lib/kokkos/core/src/OpenMP/Kokkos_OpenMP_Instance.cpp:    // Silence Cuda Warning
lib/kokkos/core/src/CMakeLists.txt:  IF(KOKKOS_ENABLE_CUDA)
lib/kokkos/core/src/CMakeLists.txt:    SET(DESUL_ATOMICS_ENABLE_CUDA ON)
lib/kokkos/core/src/CMakeLists.txt:  IF(KOKKOS_ENABLE_CUDA_RELOCATABLE_DEVICE_CODE)
lib/kokkos/core/src/CMakeLists.txt:    SET(DESUL_ATOMICS_ENABLE_CUDA_SEPARABLE_COMPILATION ON)
lib/kokkos/core/src/CMakeLists.txt:  IF(KOKKOS_ENABLE_OPENACC)
lib/kokkos/core/src/CMakeLists.txt:    SET(DESUL_ATOMICS_ENABLE_OPENACC ON)
lib/kokkos/core/src/CMakeLists.txt:IF (KOKKOS_ENABLE_CUDA)
lib/kokkos/core/src/CMakeLists.txt:  APPEND_GLOB(KOKKOS_CORE_SRCS ${CMAKE_CURRENT_SOURCE_DIR}/Cuda/*.cpp)
lib/kokkos/core/src/CMakeLists.txt:  APPEND_GLOB(KOKKOS_CORE_HEADERS ${CMAKE_CURRENT_SOURCE_DIR}/Cuda/*.hpp)
lib/kokkos/core/src/CMakeLists.txt:IF (KOKKOS_ENABLE_OPENACC)
lib/kokkos/core/src/CMakeLists.txt:  APPEND_GLOB(KOKKOS_CORE_SRCS ${CMAKE_CURRENT_SOURCE_DIR}/OpenACC/*.cpp)
lib/kokkos/core/src/CMakeLists.txt:  APPEND_GLOB(KOKKOS_CORE_HEADERS ${CMAKE_CURRENT_SOURCE_DIR}/OpenACC/*.hpp)
lib/kokkos/core/src/CMakeLists.txt:  IF (KOKKOS_ENABLE_CUDA)
lib/kokkos/core/src/CMakeLists.txt:    APPEND_GLOB(KOKKOS_CORE_SRCS ${CMAKE_CURRENT_SOURCE_DIR}/../../tpls/desul/src/Lock_Array_CUDA.cpp)
lib/kokkos/core/src/CMakeLists.txt:KOKKOS_LINK_TPL(kokkoscore PUBLIC CUDA)
lib/kokkos/core/src/CMakeLists.txt:  KOKKOS_LINK_TPL(kokkoscore PUBLIC ROCM)
lib/kokkos/core/src/impl/KokkosExp_IterateTileGPU.hpp:#ifndef KOKKOS_EXP_ITERATE_TILE_GPU_HPP
lib/kokkos/core/src/impl/KokkosExp_IterateTileGPU.hpp:#define KOKKOS_EXP_ITERATE_TILE_GPU_HPP
lib/kokkos/core/src/impl/KokkosExp_IterateTileGPU.hpp:struct EmulateCUDADim3 {
lib/kokkos/core/src/impl/KokkosExp_IterateTileGPU.hpp:      const EmulateCUDADim3<index_type> gridDim_,
lib/kokkos/core/src/impl/KokkosExp_IterateTileGPU.hpp:      const EmulateCUDADim3<index_type> blockIdx_,
lib/kokkos/core/src/impl/KokkosExp_IterateTileGPU.hpp:      const EmulateCUDADim3<index_type> threadIdx_)
lib/kokkos/core/src/impl/KokkosExp_IterateTileGPU.hpp:  const EmulateCUDADim3<index_type> gridDim;
lib/kokkos/core/src/impl/KokkosExp_IterateTileGPU.hpp:  const EmulateCUDADim3<index_type> blockIdx;
lib/kokkos/core/src/impl/KokkosExp_IterateTileGPU.hpp:  const EmulateCUDADim3<index_type> threadIdx;
lib/kokkos/core/src/impl/KokkosExp_IterateTileGPU.hpp:      const EmulateCUDADim3<index_type> gridDim_,
lib/kokkos/core/src/impl/KokkosExp_IterateTileGPU.hpp:      const EmulateCUDADim3<index_type> blockIdx_,
lib/kokkos/core/src/impl/KokkosExp_IterateTileGPU.hpp:      const EmulateCUDADim3<index_type> threadIdx_)
lib/kokkos/core/src/impl/KokkosExp_IterateTileGPU.hpp:  const EmulateCUDADim3<index_type> gridDim;
lib/kokkos/core/src/impl/KokkosExp_IterateTileGPU.hpp:  const EmulateCUDADim3<index_type> blockIdx;
lib/kokkos/core/src/impl/KokkosExp_IterateTileGPU.hpp:  const EmulateCUDADim3<index_type> threadIdx;
lib/kokkos/core/src/impl/KokkosExp_IterateTileGPU.hpp:      const EmulateCUDADim3<index_type> gridDim_,
lib/kokkos/core/src/impl/KokkosExp_IterateTileGPU.hpp:      const EmulateCUDADim3<index_type> blockIdx_,
lib/kokkos/core/src/impl/KokkosExp_IterateTileGPU.hpp:      const EmulateCUDADim3<index_type> threadIdx_)
lib/kokkos/core/src/impl/KokkosExp_IterateTileGPU.hpp:  const EmulateCUDADim3<index_type> gridDim;
lib/kokkos/core/src/impl/KokkosExp_IterateTileGPU.hpp:  const EmulateCUDADim3<index_type> blockIdx;
lib/kokkos/core/src/impl/KokkosExp_IterateTileGPU.hpp:  const EmulateCUDADim3<index_type> threadIdx;
lib/kokkos/core/src/impl/KokkosExp_IterateTileGPU.hpp:      const EmulateCUDADim3<index_type> gridDim_,
lib/kokkos/core/src/impl/KokkosExp_IterateTileGPU.hpp:      const EmulateCUDADim3<index_type> blockIdx_,
lib/kokkos/core/src/impl/KokkosExp_IterateTileGPU.hpp:      const EmulateCUDADim3<index_type> threadIdx_)
lib/kokkos/core/src/impl/KokkosExp_IterateTileGPU.hpp:  const EmulateCUDADim3<index_type> gridDim;
lib/kokkos/core/src/impl/KokkosExp_IterateTileGPU.hpp:  const EmulateCUDADim3<index_type> blockIdx;
lib/kokkos/core/src/impl/KokkosExp_IterateTileGPU.hpp:  const EmulateCUDADim3<index_type> threadIdx;
lib/kokkos/core/src/impl/KokkosExp_IterateTileGPU.hpp:      const EmulateCUDADim3<index_type> gridDim_,
lib/kokkos/core/src/impl/KokkosExp_IterateTileGPU.hpp:      const EmulateCUDADim3<index_type> blockIdx_,
lib/kokkos/core/src/impl/KokkosExp_IterateTileGPU.hpp:      const EmulateCUDADim3<index_type> threadIdx_)
lib/kokkos/core/src/impl/KokkosExp_IterateTileGPU.hpp:  const EmulateCUDADim3<index_type> gridDim;
lib/kokkos/core/src/impl/KokkosExp_IterateTileGPU.hpp:  const EmulateCUDADim3<index_type> blockIdx;
lib/kokkos/core/src/impl/KokkosExp_IterateTileGPU.hpp:  const EmulateCUDADim3<index_type> threadIdx;
lib/kokkos/core/src/impl/KokkosExp_IterateTileGPU.hpp:      const EmulateCUDADim3<index_type> gridDim_,
lib/kokkos/core/src/impl/KokkosExp_IterateTileGPU.hpp:      const EmulateCUDADim3<index_type> blockIdx_,
lib/kokkos/core/src/impl/KokkosExp_IterateTileGPU.hpp:      const EmulateCUDADim3<index_type> threadIdx_)
lib/kokkos/core/src/impl/KokkosExp_IterateTileGPU.hpp:  const EmulateCUDADim3<index_type> gridDim;
lib/kokkos/core/src/impl/KokkosExp_IterateTileGPU.hpp:  const EmulateCUDADim3<index_type> blockIdx;
lib/kokkos/core/src/impl/KokkosExp_IterateTileGPU.hpp:  const EmulateCUDADim3<index_type> threadIdx;
lib/kokkos/core/src/impl/Kokkos_BitOps.hpp:#if defined(KOKKOS_ENABLE_CUDA) || defined(KOKKOS_ENABLE_HIP)
lib/kokkos/core/src/impl/Kokkos_BitOps.hpp:#if defined(KOKKOS_ENABLE_CUDA) || defined(KOKKOS_ENABLE_HIP)
lib/kokkos/core/src/impl/Kokkos_BitOps.hpp:#if defined(KOKKOS_ENABLE_CUDA) || defined(KOKKOS_ENABLE_HIP)
lib/kokkos/core/src/impl/Kokkos_BitOps.hpp:#if defined(KOKKOS_ENABLE_CUDA) || defined(KOKKOS_ENABLE_HIP)
lib/kokkos/core/src/impl/Kokkos_DesulAtomicsConfig.hpp:#define DESUL_CUDA_ARCH_IS_PRE_PASCAL
lib/kokkos/core/src/impl/Kokkos_DesulAtomicsConfig.hpp:#define DESUL_CUDA_ARCH_IS_PRE_VOLTA
lib/kokkos/core/src/impl/Kokkos_Core.cpp:#if defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/core/src/impl/Kokkos_Core.cpp:  KOKKOS_IMPL_CUDA_SAFE_CALL(cudaGetDeviceCount(&count));
lib/kokkos/core/src/impl/Kokkos_Core.cpp:#elif defined(KOKKOS_ENABLE_OPENACC)
lib/kokkos/core/src/impl/Kokkos_Core.cpp:      Kokkos::Experimental::Impl::OpenACC_Traits::dev_type);
lib/kokkos/core/src/impl/Kokkos_Core.cpp:#if defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/core/src/impl/Kokkos_Core.cpp:  int device = Cuda().cuda_device();
lib/kokkos/core/src/impl/Kokkos_Core.cpp:#elif defined(KOKKOS_ENABLE_OPENACC)
lib/kokkos/core/src/impl/Kokkos_Core.cpp:  int device = Experimental::OpenACC().acc_device_number();
lib/kokkos/core/src/impl/Kokkos_Core.cpp:    return -1;  // no GPU backend enabled
lib/kokkos/core/src/impl/Kokkos_Core.cpp:int Kokkos::Impl::get_ctest_gpu(int local_rank) {
lib/kokkos/core/src/impl/Kokkos_Core.cpp:       << " by Kokkos::Impl::get_ctest_gpu().";
lib/kokkos/core/src/impl/Kokkos_Core.cpp:       << " by Kokkos::Impl::get_ctest_gpu().";
lib/kokkos/core/src/impl/Kokkos_Core.cpp:       << ". Raised by Kokkos::Impl::get_ctest_gpu().";
lib/kokkos/core/src/impl/Kokkos_Core.cpp:       << " is not specified. Raised by Kokkos::Impl::get_ctest_gpu().";
lib/kokkos/core/src/impl/Kokkos_Core.cpp:       << resource_str << "'. Raised by Kokkos::Impl::get_ctest_gpu().";
lib/kokkos/core/src/impl/Kokkos_Core.cpp:           << " Device id must be smaller than the number of GPUs available"
lib/kokkos/core/src/impl/Kokkos_Core.cpp:        "Error: no GPU available for execution.\n"
lib/kokkos/core/src/impl/Kokkos_Core.cpp:std::optional<int> Kokkos::Impl::get_gpu(
lib/kokkos/core/src/impl/Kokkos_Core.cpp:      ss << "Error: Requested GPU with invalid id '" << id << "'."
lib/kokkos/core/src/impl/Kokkos_Core.cpp:      ss << "Error: Requested GPU with id '" << id << "' but only "
lib/kokkos/core/src/impl/Kokkos_Core.cpp:         << num_devices << "GPU(s) available!"
lib/kokkos/core/src/impl/Kokkos_Core.cpp:                << " Falling back to the first GPU available for execution."
lib/kokkos/core/src/impl/Kokkos_Core.cpp:    return get_ctest_gpu(mpi_local_rank);
lib/kokkos/core/src/impl/Kokkos_Core.cpp:  declare_configuration_metadata("architecture", "GPU architecture",
lib/kokkos/core/src/impl/Kokkos_Core.cpp:  declare_configuration_metadata("architecture", "GPU architecture",
lib/kokkos/core/src/impl/Kokkos_Core.cpp:  declare_configuration_metadata("architecture", "GPU architecture",
lib/kokkos/core/src/impl/Kokkos_Core.cpp:  declare_configuration_metadata("architecture", "GPU architecture",
lib/kokkos/core/src/impl/Kokkos_Core.cpp:  declare_configuration_metadata("architecture", "GPU architecture",
lib/kokkos/core/src/impl/Kokkos_Core.cpp:  declare_configuration_metadata("architecture", "GPU architecture",
lib/kokkos/core/src/impl/Kokkos_Core.cpp:  declare_configuration_metadata("architecture", "GPU architecture",
lib/kokkos/core/src/impl/Kokkos_Core.cpp:  declare_configuration_metadata("architecture", "GPU architecture",
lib/kokkos/core/src/impl/Kokkos_Core.cpp:  declare_configuration_metadata("architecture", "GPU architecture",
lib/kokkos/core/src/impl/Kokkos_Core.cpp:  declare_configuration_metadata("architecture", "GPU architecture",
lib/kokkos/core/src/impl/Kokkos_Core.cpp:  declare_configuration_metadata("architecture", "GPU architecture",
lib/kokkos/core/src/impl/Kokkos_Core.cpp:  declare_configuration_metadata("architecture", "GPU architecture",
lib/kokkos/core/src/impl/Kokkos_Core.cpp:  declare_configuration_metadata("architecture", "GPU architecture",
lib/kokkos/core/src/impl/Kokkos_Core.cpp:  declare_configuration_metadata("architecture", "GPU architecture",
lib/kokkos/core/src/impl/Kokkos_Core.cpp:  declare_configuration_metadata("architecture", "GPU architecture",
lib/kokkos/core/src/impl/Kokkos_Core.cpp:  declare_configuration_metadata("architecture", "GPU architecture",
lib/kokkos/core/src/impl/Kokkos_Core.cpp:  declare_configuration_metadata("architecture", "GPU architecture", "VOLTA70");
lib/kokkos/core/src/impl/Kokkos_Core.cpp:  declare_configuration_metadata("architecture", "GPU architecture", "VOLTA72");
lib/kokkos/core/src/impl/Kokkos_Core.cpp:  declare_configuration_metadata("architecture", "GPU architecture",
lib/kokkos/core/src/impl/Kokkos_Core.cpp:  declare_configuration_metadata("architecture", "GPU architecture",
lib/kokkos/core/src/impl/Kokkos_Core.cpp:  declare_configuration_metadata("architecture", "GPU architecture",
lib/kokkos/core/src/impl/Kokkos_Core.cpp:  declare_configuration_metadata("architecture", "GPU architecture", "ADA89");
lib/kokkos/core/src/impl/Kokkos_Core.cpp:      declare_configuration_metadata("architecture", "GPU architecture",
lib/kokkos/core/src/impl/Kokkos_Core.cpp:  declare_configuration_metadata("architecture", "GPU architecture",
lib/kokkos/core/src/impl/Kokkos_Core.cpp:  declare_configuration_metadata("architecture", "GPU architecture",
lib/kokkos/core/src/impl/Kokkos_Core.cpp:  declare_configuration_metadata("architecture", "GPU architecture",
lib/kokkos/core/src/impl/Kokkos_Core.cpp:  declare_configuration_metadata("architecture", "GPU architecture",
lib/kokkos/core/src/impl/Kokkos_Core.cpp:  declare_configuration_metadata("architecture", "GPU architecture",
lib/kokkos/core/src/impl/Kokkos_Core.cpp:  declare_configuration_metadata("architecture", "GPU architecture",
lib/kokkos/core/src/impl/Kokkos_Core.cpp:  declare_configuration_metadata("architecture", "GPU architecture", "none");
lib/kokkos/core/src/impl/Kokkos_Profiling_Interface.hpp:  Cuda,
lib/kokkos/core/src/impl/Kokkos_Profiling_Interface.hpp:  OpenACC,
lib/kokkos/core/src/impl/Kokkos_Profiling_Interface.hpp:    case 2: return DeviceType::Cuda;
lib/kokkos/core/src/impl/Kokkos_Profiling_Interface.hpp:    case 8: return DeviceType::OpenACC;
lib/kokkos/core/src/impl/Kokkos_SharedAlloc_timpl.hpp:  SharedAllocationHeader const* const head_cuda =
lib/kokkos/core/src/impl/Kokkos_SharedAlloc_timpl.hpp:        exec_space, &head, head_cuda, sizeof(SharedAllocationHeader));
lib/kokkos/core/src/impl/Kokkos_SharedAlloc_timpl.hpp:  if (!alloc_ptr || record->m_alloc_ptr != head_cuda) {
lib/kokkos/core/src/impl/Kokkos_TaskQueueMultiple.hpp:#if !defined(__HIP_DEVICE_COMPILE__) && !defined(__CUDA_ARCH__)
lib/kokkos/core/src/impl/KokkosExp_Host_IterateTile.hpp:    defined(KOKKOS_ENABLE_PRAGMA_IVDEP) && !defined(__CUDA_ARCH__)
lib/kokkos/core/src/impl/KokkosExp_Host_IterateTile.hpp:  // rank w/ cuda+serial
lib/kokkos/core/src/impl/KokkosExp_Host_IterateTile.hpp:      : m_rp(rp)  // Cuda 7.0 does not like braces...
lib/kokkos/core/src/impl/KokkosExp_Host_IterateTile.hpp:    // Errors due to braces rather than parenthesis for init (with cuda 7.0)
lib/kokkos/core/src/impl/KokkosExp_Host_IterateTile.hpp:      : m_rp(rp)  // Cuda 7.0 does not like braces...
lib/kokkos/core/src/impl/Kokkos_ClockTic.hpp:// To use OpenCL(TM) built-in intrinsics inside kernels, we have to
lib/kokkos/core/src/impl/Kokkos_ClockTic.hpp:// https://github.com/intel/pti-gpu/blob/master/chapters/binary_instrumentation/OpenCLBuiltIn.md
lib/kokkos/core/src/impl/Kokkos_ClockTic.hpp:#if defined(KOKKOS_ENABLE_SYCL) && defined(KOKKOS_ARCH_INTEL_GPU) && \
lib/kokkos/core/src/impl/Kokkos_ClockTic.hpp:#if defined(KOKKOS_ENABLE_CUDA) || defined(KOKKOS_ENABLE_HIP)
lib/kokkos/core/src/impl/Kokkos_ClockTic.hpp:#elif defined(KOKKOS_ENABLE_SYCL) && defined(KOKKOS_ARCH_INTEL_GPU) && \
lib/kokkos/core/src/impl/Kokkos_ViewMapping.hpp:  // MSVC (16.5.5) + CUDA (10.2) did not generate the defaulted functions
lib/kokkos/core/src/impl/Kokkos_ViewMapping.hpp:#ifdef KOKKOS_IMPL_WINDOWS_CUDA
lib/kokkos/core/src/impl/Kokkos_ViewMapping.hpp:  // MSVC (16.5.5) + CUDA (10.2) did not generate the defaulted functions
lib/kokkos/core/src/impl/Kokkos_ViewMapping.hpp:#ifdef KOKKOS_IMPL_WINDOWS_CUDA
lib/kokkos/core/src/impl/Kokkos_ViewMapping.hpp:  // MSVC (16.5.5) + CUDA (10.2) did not generate the defaulted functions
lib/kokkos/core/src/impl/Kokkos_ViewMapping.hpp:#ifdef KOKKOS_IMPL_WINDOWS_CUDA
lib/kokkos/core/src/impl/Kokkos_ViewMapping.hpp:  // MSVC (16.5.5) + CUDA (10.2) did not generate the defaulted functions
lib/kokkos/core/src/impl/Kokkos_ViewMapping.hpp:#ifdef KOKKOS_IMPL_WINDOWS_CUDA
lib/kokkos/core/src/impl/Kokkos_ViewMapping.hpp:  // MSVC (16.5.5) + CUDA (10.2) did not generate the defaulted functions
lib/kokkos/core/src/impl/Kokkos_ViewMapping.hpp:#ifdef KOKKOS_IMPL_WINDOWS_CUDA
lib/kokkos/core/src/impl/Kokkos_ViewMapping.hpp: * memory) (ii)  Use special data handle type (e.g. add Cuda Texture Object)
lib/kokkos/core/src/impl/Kokkos_FixedBufferMemoryPool.hpp:  // pages (GPU)?
lib/kokkos/core/src/impl/Kokkos_DeviceManagement.hpp:std::optional<int> get_gpu(const Kokkos::InitializationSettings& settings);
lib/kokkos/core/src/impl/Kokkos_DeviceManagement.hpp:int get_ctest_gpu(int local_rank);
lib/kokkos/core/src/impl/Kokkos_Half_FloatingPointWrapper.hpp:// floating_pointer_wrapper operator paths should be used. For CUDA, let the
lib/kokkos/core/src/impl/Kokkos_Half_FloatingPointWrapper.hpp:#if defined(__CUDA_ARCH__) || defined(KOKKOS_ENABLE_SYCL)
lib/kokkos/core/src/impl/Kokkos_Half_FloatingPointWrapper.hpp:  // since Cuda supports half precision initialization via the below constructor
lib/kokkos/core/src/impl/Kokkos_Half_FloatingPointWrapper.hpp:// member function are not allowed" with VS 16.11.3 and CUDA 11.4.2
lib/kokkos/core/src/impl/Kokkos_Half_FloatingPointWrapper.hpp:#if defined(_WIN32) && defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/core/src/impl/Kokkos_Half_FloatingPointWrapper.hpp:    val = val + impl_type(1.0F);  // cuda has no operator++ for __nv_bfloat
lib/kokkos/core/src/impl/Kokkos_Half_FloatingPointWrapper.hpp:    val = val - impl_type(1.0F);  // cuda has no operator-- for __nv_bfloat
lib/kokkos/core/src/impl/Kokkos_Half_FloatingPointWrapper.hpp:    val = val + rhs.val;  // cuda has no operator+= for __nv_bfloat
lib/kokkos/core/src/impl/Kokkos_Half_FloatingPointWrapper.hpp:    val = val - rhs.val;  // cuda has no operator-= for __nv_bfloat
lib/kokkos/core/src/impl/Kokkos_Half_FloatingPointWrapper.hpp:    val = val * rhs.val;  // cuda has no operator*= for __nv_bfloat
lib/kokkos/core/src/impl/Kokkos_Half_FloatingPointWrapper.hpp:    val = val / rhs.val;  // cuda has no operator/= for __nv_bfloat
lib/kokkos/core/src/impl/Kokkos_TaskQueueCommon.hpp:    // thread to access it might be a Cuda thread from a different thread block.
lib/kokkos/core/src/impl/Kokkos_TaskBase.hpp:#ifdef __CUDA_ARCH__  // FIXME_CUDA
lib/kokkos/core/src/impl/Kokkos_SharedAlloc.hpp:#if defined(KOKKOS_ARCH_AMD_GPU)
lib/kokkos/core/src/impl/Kokkos_SharedAlloc.hpp:#if defined(KOKKOS_ARCH_AMD_GPU)
lib/kokkos/core/src/impl/Kokkos_TaskNode.hpp:  // Can't just do using base_t::base_t because of stupid stuff with clang cuda
lib/kokkos/core/src/impl/Kokkos_TaskNode.hpp:  // Can't just do using base_t::base_t because of stupid stuff with clang cuda
lib/kokkos/core/src/impl/Kokkos_TaskNode.hpp:  // Can't just do using base_t::base_t because of stupid stuff with clang cuda
lib/kokkos/core/src/impl/Kokkos_TaskNode.hpp:       // classes may not be ABI compatible with CUDA on Windows
lib/kokkos/core/src/impl/Kokkos_TaskNode.hpp:#ifdef __CUDA_ARCH__  // FIXME_CUDA
lib/kokkos/core/src/impl/Kokkos_NvidiaGpuArchitectures.hpp:#ifndef KOKKOS_CUDA_NVIDIA_GPU_ARCHITECTURES_HPP
lib/kokkos/core/src/impl/Kokkos_NvidiaGpuArchitectures.hpp:#define KOKKOS_CUDA_NVIDIA_GPU_ARCHITECTURES_HPP
lib/kokkos/core/src/impl/Kokkos_NvidiaGpuArchitectures.hpp:#define KOKKOS_IMPL_ARCH_NVIDIA_GPU 30
lib/kokkos/core/src/impl/Kokkos_NvidiaGpuArchitectures.hpp:#define KOKKOS_IMPL_ARCH_NVIDIA_GPU 32
lib/kokkos/core/src/impl/Kokkos_NvidiaGpuArchitectures.hpp:#define KOKKOS_IMPL_ARCH_NVIDIA_GPU 35
lib/kokkos/core/src/impl/Kokkos_NvidiaGpuArchitectures.hpp:#define KOKKOS_IMPL_ARCH_NVIDIA_GPU 37
lib/kokkos/core/src/impl/Kokkos_NvidiaGpuArchitectures.hpp:#define KOKKOS_IMPL_ARCH_NVIDIA_GPU 50
lib/kokkos/core/src/impl/Kokkos_NvidiaGpuArchitectures.hpp:#define KOKKOS_IMPL_ARCH_NVIDIA_GPU 52
lib/kokkos/core/src/impl/Kokkos_NvidiaGpuArchitectures.hpp:#define KOKKOS_IMPL_ARCH_NVIDIA_GPU 53
lib/kokkos/core/src/impl/Kokkos_NvidiaGpuArchitectures.hpp:#define KOKKOS_IMPL_ARCH_NVIDIA_GPU 60
lib/kokkos/core/src/impl/Kokkos_NvidiaGpuArchitectures.hpp:#define KOKKOS_IMPL_ARCH_NVIDIA_GPU 61
lib/kokkos/core/src/impl/Kokkos_NvidiaGpuArchitectures.hpp:#define KOKKOS_IMPL_ARCH_NVIDIA_GPU 70
lib/kokkos/core/src/impl/Kokkos_NvidiaGpuArchitectures.hpp:#define KOKKOS_IMPL_ARCH_NVIDIA_GPU 72
lib/kokkos/core/src/impl/Kokkos_NvidiaGpuArchitectures.hpp:#define KOKKOS_IMPL_ARCH_NVIDIA_GPU 75
lib/kokkos/core/src/impl/Kokkos_NvidiaGpuArchitectures.hpp:#define KOKKOS_IMPL_ARCH_NVIDIA_GPU 80
lib/kokkos/core/src/impl/Kokkos_NvidiaGpuArchitectures.hpp:#define KOKKOS_IMPL_ARCH_NVIDIA_GPU 86
lib/kokkos/core/src/impl/Kokkos_NvidiaGpuArchitectures.hpp:#define KOKKOS_IMPL_ARCH_NVIDIA_GPU 89
lib/kokkos/core/src/impl/Kokkos_NvidiaGpuArchitectures.hpp:#define KOKKOS_IMPL_ARCH_NVIDIA_GPU 90
lib/kokkos/core/src/impl/Kokkos_NvidiaGpuArchitectures.hpp:#elif defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/core/src/impl/Kokkos_NvidiaGpuArchitectures.hpp:// do not raise an error on other backends that may run on NVIDIA GPUs such as
lib/kokkos/core/src/impl/Kokkos_NvidiaGpuArchitectures.hpp:// OpenACC, OpenMPTarget, or SYCL
lib/kokkos/core/src/impl/Kokkos_NvidiaGpuArchitectures.hpp:#error NVIDIA GPU arch not recognized
lib/kokkos/core/src/impl/Kokkos_ViewCtor.hpp:// we can tell MSVC 16.5.5+CUDA 10.2 thinks that `ViewCtorProp` refers to the
lib/kokkos/core/src/impl/Kokkos_ViewCtor.hpp:  // directly, MSVC 16.5.5+CUDA 10.2 appears to think that `ViewCtorProp` refers
lib/kokkos/core/src/impl/Kokkos_ViewCtor.hpp:// end of non-void function" warnings from CUDA builds (issue #5470). Because
lib/kokkos/core/src/impl/Kokkos_ViewCtor.hpp:// cuda_abort(), an unreachable while(true); is placed as a fallback method
lib/kokkos/core/src/impl/Kokkos_ViewCtor.hpp:// end of non-void function" warnings from CUDA builds (issue #5470). Because
lib/kokkos/core/src/impl/Kokkos_ViewCtor.hpp:// cuda_abort(), an unreachable while(true); is placed as a fallback method
lib/kokkos/core/src/impl/Kokkos_ViewCtor.hpp:#ifdef __CUDA_ARCH__
lib/kokkos/core/src/Threads/Kokkos_Threads_Team.hpp:                20 * 1024 * 1024);    // Limit to keep compatibility with CUDA
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_MDRangePolicy.hpp:#ifndef KOKKOS_OPENACC_MDRANGE_POLICY_HPP_
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_MDRangePolicy.hpp:#define KOKKOS_OPENACC_MDRANGE_POLICY_HPP_
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_MDRangePolicy.hpp:struct Kokkos::default_outer_direction<Kokkos::Experimental::OpenACC> {
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_MDRangePolicy.hpp:struct Kokkos::default_inner_direction<Kokkos::Experimental::OpenACC> {
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_MDRangePolicy.hpp:struct ThreadAndVectorNestLevel<Rank, Kokkos::Experimental::OpenACC,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_MDRangePolicy.hpp:struct OpenACCCollapse {};
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_MDRangePolicy.hpp:struct OpenACCTile {};
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_MDRangePolicy.hpp:using OpenACCIterateLeft  = std::integral_constant<Iterate, Iterate::Left>;
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_MDRangePolicy.hpp:using OpenACCIterateRight = std::integral_constant<Iterate, Iterate::Right>;
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_MDRangePolicy.hpp:using OpenACCMDRangeBegin = decltype(MDRangePolicy<OpenACC, Rank<N>>::m_lower);
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_MDRangePolicy.hpp:using OpenACCMDRangeEnd = decltype(MDRangePolicy<OpenACC, Rank<N>>::m_upper);
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_MDRangePolicy.hpp:using OpenACCMDRangeTile = decltype(MDRangePolicy<OpenACC, Rank<N>>::m_tile);
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ScheduleType.hpp:#ifndef KOKKOS_OPENACC_SCHEDULE_TYPE_HPP
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ScheduleType.hpp:#define KOKKOS_OPENACC_SCHEDULE_TYPE_HPP
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ScheduleType.hpp:struct OpenACCSchedule {
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ScheduleType.hpp:using OpenACCScheduleType = typename OpenACCSchedule<Policy>::type;
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.cpp:#include <OpenACC/Kokkos_OpenACC.hpp>
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.cpp:#include <OpenACC/Kokkos_OpenACC_Instance.hpp>
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.cpp:#include <OpenACC/Kokkos_OpenACC_Traits.hpp>
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.cpp:Kokkos::Experimental::OpenACC::OpenACC()
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.cpp:          &Kokkos::Experimental::Impl::OpenACCInternal::singleton(),
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.cpp:          [](Impl::OpenACCInternal*) {}) {
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.cpp:  Impl::OpenACCInternal::singleton().verify_is_initialized(
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.cpp:      "OpenACC instance constructor");
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.cpp:Kokkos::Experimental::OpenACC::OpenACC(int async_arg)
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.cpp:    : m_space_instance(new Kokkos::Experimental::Impl::OpenACCInternal,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.cpp:                       [](Impl::OpenACCInternal* ptr) {
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.cpp:  Impl::OpenACCInternal::singleton().verify_is_initialized(
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.cpp:      "OpenACC instance constructor");
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.cpp:void Kokkos::Experimental::OpenACC::impl_initialize(
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.cpp:  if (Impl::OpenACC_Traits::may_fallback_to_host &&
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.cpp:      acc_get_num_devices(Impl::OpenACC_Traits::dev_type) == 0 &&
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.cpp:      std::cerr << "Warning: No GPU available for execution, falling back to"
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.cpp:    Impl::OpenACCInternal::m_acc_device_num =
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.cpp:    using Kokkos::Impl::get_gpu;
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.cpp:    int const dev_num = get_gpu(settings).value_or(visible_devices[0]);
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.cpp:    acc_set_device_num(dev_num, Impl::OpenACC_Traits::dev_type);
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.cpp:    Impl::OpenACCInternal::m_acc_device_num = dev_num;
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.cpp:  Impl::OpenACCInternal::singleton().initialize();
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.cpp:void Kokkos::Experimental::OpenACC::impl_finalize() {
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.cpp:  Impl::OpenACCInternal::singleton().finalize();
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.cpp:bool Kokkos::Experimental::OpenACC::impl_is_initialized() {
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.cpp:  return Impl::OpenACCInternal::singleton().is_initialized();
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.cpp:void Kokkos::Experimental::OpenACC::print_configuration(std::ostream& os,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.cpp:  os << "  KOKKOS_ENABLE_OPENACC: yes\n";
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.cpp:  os << "OpenACC Options:\n";
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.cpp:  os << "  KOKKOS_ENABLE_OPENACC_COLLAPSE_HIERARCHICAL_CONSTRUCTS: ";
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.cpp:#ifdef KOKKOS_ENABLE_OPENACC_COLLAPSE_HIERARCHICAL_CONSTRUCTS
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.cpp:void Kokkos::Experimental::OpenACC::fence(std::string const& name) const {
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.cpp:void Kokkos::Experimental::OpenACC::impl_static_fence(std::string const& name) {
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.cpp:      Kokkos::Experimental::OpenACC>(
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.cpp:uint32_t Kokkos::Experimental::OpenACC::impl_instance_id() const noexcept {
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.cpp:int Kokkos::Experimental::OpenACC::acc_async_queue() const {
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.cpp:int Kokkos::Experimental::OpenACC::acc_device_number() const {
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.cpp:  return Impl::OpenACCInternal::m_acc_device_num;
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.cpp:int g_openacc_space_factory_initialized =
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.cpp:    initialize_space_factory<Experimental::OpenACC>("170_OpenACC");
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelScan_Range.hpp:#ifndef KOKKOS_OPENACC_PARALLEL_SCAN_RANGE_HPP
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelScan_Range.hpp:#define KOKKOS_OPENACC_PARALLEL_SCAN_RANGE_HPP
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelScan_Range.hpp:#include <OpenACC/Kokkos_OpenACC.hpp>
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelScan_Range.hpp:#include <OpenACC/Kokkos_OpenACC_FunctorAdapter.hpp>
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelScan_Range.hpp:#include <OpenACC/Kokkos_OpenACC_Macros.hpp>
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelScan_Range.hpp:// OpenACC features: Clacc does not fully support private clauses for
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelScan_Range.hpp:// the gang-private arrays on GPU global memory using array expansion,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelScan_Range.hpp:class ParallelScanOpenACCBase {
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelScan_Range.hpp:  ParallelScanOpenACCBase(Functor const& arg_functor, Policy const& arg_policy,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelScan_Range.hpp:  void OpenACCParallelScanRangePolicy(const IndexType begin,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelScan_Range.hpp:            "with OpenACC parallel_scan()");
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelScan_Range.hpp:    Kokkos::View<ValueType*, Kokkos::Experimental::OpenACCSpace> chunk_values(
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelScan_Range.hpp:        "Kokkos::OpenACCParallelScan::chunk_values", n_chunks);
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelScan_Range.hpp:    Kokkos::View<ValueType*, Kokkos::Experimental::OpenACCSpace> offset_values(
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelScan_Range.hpp:        "Kokkos::OpenACCParallelScan::offset_values", n_chunks);
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelScan_Range.hpp:    Kokkos::View<ValueType, Kokkos::Experimental::OpenACCSpace> m_result_total(
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelScan_Range.hpp:        "Kokkos::OpenACCParallelScan::m_result_total");
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelScan_Range.hpp:      DeepCopy<HostSpace, Kokkos::Experimental::OpenACCSpace,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelScan_Range.hpp:               Kokkos::Experimental::OpenACC>(m_policy.space(), m_result_ptr,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelScan_Range.hpp:    OpenACCParallelScanRangePolicy(begin, end, chunk_size, async_arg);
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelScan_Range.hpp:                                 Kokkos::Experimental::OpenACC>
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelScan_Range.hpp:    : public ParallelScanOpenACCBase<Functor, void, Traits...> {
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelScan_Range.hpp:  using base_t    = ParallelScanOpenACCBase<Functor, void, Traits...>;
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelScan_Range.hpp:    base_t::OpenACCParallelScanRangePolicy(begin, end, chunk_size, async_arg);
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelScan_Range.hpp:    Kokkos::Experimental::OpenACC>
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelScan_Range.hpp:    : public ParallelScanOpenACCBase<FunctorType, ReturnType, Traits...> {
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelScan_Range.hpp:  using base_t    = ParallelScanOpenACCBase<FunctorType, ReturnType, Traits...>;
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelScan_Range.hpp:    base_t::OpenACCParallelScanRangePolicy(begin, end, chunk_size, async_arg);
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelScan_Range.hpp:               MemorySpaceAccess<Kokkos::Experimental::OpenACCSpace,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACCSpace.cpp:#include <OpenACC/Kokkos_OpenACC.hpp>
lib/kokkos/core/src/OpenACC/Kokkos_OpenACCSpace.cpp:#include <OpenACC/Kokkos_OpenACCSpace.hpp>
lib/kokkos/core/src/OpenACC/Kokkos_OpenACCSpace.cpp:#include <OpenACC/Kokkos_OpenACC_DeepCopy.hpp>
lib/kokkos/core/src/OpenACC/Kokkos_OpenACCSpace.cpp:#include <openacc.h>
lib/kokkos/core/src/OpenACC/Kokkos_OpenACCSpace.cpp:void *Kokkos::Experimental::OpenACCSpace::allocate(
lib/kokkos/core/src/OpenACC/Kokkos_OpenACCSpace.cpp:    const Kokkos::Experimental::OpenACC &exec_space,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACCSpace.cpp:void *Kokkos::Experimental::OpenACCSpace::allocate(
lib/kokkos/core/src/OpenACC/Kokkos_OpenACCSpace.cpp:void *Kokkos::Experimental::OpenACCSpace::allocate(
lib/kokkos/core/src/OpenACC/Kokkos_OpenACCSpace.cpp:    const Kokkos::Experimental::OpenACC &exec_space, const char *arg_label,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACCSpace.cpp:void *Kokkos::Experimental::OpenACCSpace::allocate(
lib/kokkos/core/src/OpenACC/Kokkos_OpenACCSpace.cpp:void *Kokkos::Experimental::OpenACCSpace::impl_allocate(
lib/kokkos/core/src/OpenACC/Kokkos_OpenACCSpace.cpp:    const Kokkos::Experimental::OpenACC &exec_space, const char *arg_label,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACCSpace.cpp:void *Kokkos::Experimental::OpenACCSpace::impl_allocate(
lib/kokkos/core/src/OpenACC/Kokkos_OpenACCSpace.cpp:void Kokkos::Experimental::OpenACCSpace::deallocate(
lib/kokkos/core/src/OpenACC/Kokkos_OpenACCSpace.cpp:void Kokkos::Experimental::OpenACCSpace::deallocate(
lib/kokkos/core/src/OpenACC/Kokkos_OpenACCSpace.cpp:void Kokkos::Experimental::OpenACCSpace::impl_deallocate(
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_SharedAllocationRecord.hpp:#ifndef KOKKOS_OPENACC_SHARED_ALLOCATION_RECORD_HPP
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_SharedAllocationRecord.hpp:#define KOKKOS_OPENACC_SHARED_ALLOCATION_RECORD_HPP
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_SharedAllocationRecord.hpp:#include <OpenACC/Kokkos_OpenACCSpace.hpp>
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_SharedAllocationRecord.hpp:    Kokkos::Experimental::OpenACCSpace);
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Instance.cpp:#include <OpenACC/Kokkos_OpenACC.hpp>
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Instance.cpp:#include <OpenACC/Kokkos_OpenACC_Instance.hpp>
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Instance.cpp:#include <openacc.h>
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Instance.cpp:int Kokkos::Experimental::Impl::OpenACCInternal::m_acc_device_num = -1;
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Instance.cpp:Kokkos::Experimental::Impl::OpenACCInternal&
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Instance.cpp:Kokkos::Experimental::Impl::OpenACCInternal::singleton() {
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Instance.cpp:  static OpenACCInternal self;
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Instance.cpp:bool Kokkos::Experimental::Impl::OpenACCInternal::verify_is_initialized(
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Instance.cpp:    Kokkos::abort((std::string("Kokkos::Experimental::OpenACC::") + label +
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Instance.cpp:void Kokkos::Experimental::Impl::OpenACCInternal::initialize(int async_arg) {
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Instance.cpp:    Kokkos::abort((std::string("Kokkos::Experimental::OpenACC::initialize()") +
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Instance.cpp:                   " unless being a special value defined in OpenACC\n")
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Instance.cpp:void Kokkos::Experimental::Impl::OpenACCInternal::finalize() {
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Instance.cpp:bool Kokkos::Experimental::Impl::OpenACCInternal::is_initialized() const {
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Instance.cpp:void Kokkos::Experimental::Impl::OpenACCInternal::print_configuration(
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Instance.cpp:  os << "Using OpenACC\n";  // FIXME_OPENACC
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Instance.cpp:void Kokkos::Experimental::Impl::OpenACCInternal::fence(
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Instance.cpp:      Kokkos::Experimental::OpenACC>(
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Instance.cpp:uint32_t Kokkos::Experimental::Impl::OpenACCInternal::instance_id() const
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Instance.cpp:  return Kokkos::Tools::Experimental::Impl::idForInstance<OpenACC>(
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_MDRange.hpp:#ifndef KOKKOS_OPENACC_PARALLEL_REDUCE_MDRANGE_HPP
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_MDRange.hpp:#define KOKKOS_OPENACC_PARALLEL_REDUCE_MDRANGE_HPP
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_MDRange.hpp:#include <OpenACC/Kokkos_OpenACC.hpp>
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_MDRange.hpp:#include <OpenACC/Kokkos_OpenACC_Macros.hpp>
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_MDRange.hpp:#include <OpenACC/Kokkos_OpenACC_FunctorAdapter.hpp>
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_MDRange.hpp:#include <OpenACC/Kokkos_OpenACC_MDRangePolicy.hpp>
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_MDRange.hpp:struct OpenACCParallelReduceMDRangeHelper {
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_MDRange.hpp:  OpenACCParallelReduceMDRangeHelper(Functor const&, Reducer const&,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_MDRange.hpp:                                   Kokkos::Experimental::OpenACC> {
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_MDRange.hpp:            MemorySpaceAccess<Kokkos::Experimental::OpenACCSpace,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_MDRange.hpp:    Kokkos::Experimental::Impl::OpenACCParallelReduceMDRangeHelper(
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_MDRange.hpp:    // OpenACC backend supports only built-in Reducer types; thus
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_MDRange.hpp:    // above OpenACC compute kernel can be executed asynchronously and val is a
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_MDRange.hpp:#define KOKKOS_IMPL_OPENACC_PARALLEL_REDUCE_DISPATCH_ITERATE(REDUCER,         \
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_MDRange.hpp:  void OpenACCParallelReduce##REDUCER(OpenACCIterateLeft, ValueType& aval,    \
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_MDRange.hpp:                                      OpenACCMDRangeBegin<2> const& begin,    \
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_MDRange.hpp:                                      OpenACCMDRangeEnd<2> const& end,        \
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_MDRange.hpp:  void OpenACCParallelReduce##REDUCER(OpenACCIterateRight, ValueType& aval,   \
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_MDRange.hpp:                                      OpenACCMDRangeBegin<2> const& begin,    \
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_MDRange.hpp:                                      OpenACCMDRangeEnd<2> const& end,        \
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_MDRange.hpp:  void OpenACCParallelReduce##REDUCER(OpenACCIterateLeft, ValueType& aval,    \
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_MDRange.hpp:                                      OpenACCMDRangeBegin<3> const& begin,    \
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_MDRange.hpp:                                      OpenACCMDRangeEnd<3> const& end,        \
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_MDRange.hpp:  void OpenACCParallelReduce##REDUCER(OpenACCIterateRight, ValueType& aval,   \
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_MDRange.hpp:                                      OpenACCMDRangeBegin<3> const& begin,    \
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_MDRange.hpp:                                      OpenACCMDRangeEnd<3> const& end,        \
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_MDRange.hpp:  void OpenACCParallelReduce##REDUCER(OpenACCIterateLeft, ValueType& aval,    \
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_MDRange.hpp:                                      OpenACCMDRangeBegin<4> const& begin,    \
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_MDRange.hpp:                                      OpenACCMDRangeEnd<4> const& end,        \
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_MDRange.hpp:  void OpenACCParallelReduce##REDUCER(OpenACCIterateRight, ValueType& aval,   \
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_MDRange.hpp:                                      OpenACCMDRangeBegin<4> const& begin,    \
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_MDRange.hpp:                                      OpenACCMDRangeEnd<4> const& end,        \
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_MDRange.hpp:  void OpenACCParallelReduce##REDUCER(OpenACCIterateLeft, ValueType& aval,    \
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_MDRange.hpp:                                      OpenACCMDRangeBegin<5> const& begin,    \
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_MDRange.hpp:                                      OpenACCMDRangeEnd<5> const& end,        \
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_MDRange.hpp:  void OpenACCParallelReduce##REDUCER(OpenACCIterateRight, ValueType& aval,   \
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_MDRange.hpp:                                      OpenACCMDRangeBegin<5> const& begin,    \
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_MDRange.hpp:                                      OpenACCMDRangeEnd<5> const& end,        \
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_MDRange.hpp:  void OpenACCParallelReduce##REDUCER(OpenACCIterateLeft, ValueType& aval,    \
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_MDRange.hpp:                                      OpenACCMDRangeBegin<6> const& begin,    \
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_MDRange.hpp:                                      OpenACCMDRangeEnd<6> const& end,        \
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_MDRange.hpp:  void OpenACCParallelReduce##REDUCER(OpenACCIterateRight, ValueType& aval,   \
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_MDRange.hpp:                                      OpenACCMDRangeBegin<6> const& begin,    \
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_MDRange.hpp:                                      OpenACCMDRangeEnd<6> const& end,        \
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_MDRange.hpp:#define KOKKOS_IMPL_OPENACC_PARALLEL_REDUCE_MDRANGE_HELPER(REDUCER, OPERATOR) \
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_MDRange.hpp:  KOKKOS_IMPL_OPENACC_PARALLEL_REDUCE_DISPATCH_ITERATE(REDUCER, OPERATOR)     \
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_MDRange.hpp:  struct Kokkos::Experimental::Impl::OpenACCParallelReduceMDRangeHelper<      \
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_MDRange.hpp:    OpenACCParallelReduceMDRangeHelper(Functor const& functor,                \
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_MDRange.hpp:      OpenACCParallelReduce##REDUCER(                                         \
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_MDRange.hpp:KOKKOS_IMPL_OPENACC_PARALLEL_REDUCE_MDRANGE_HELPER(Sum, +);
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_MDRange.hpp:KOKKOS_IMPL_OPENACC_PARALLEL_REDUCE_MDRANGE_HELPER(Prod, *);
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_MDRange.hpp:KOKKOS_IMPL_OPENACC_PARALLEL_REDUCE_MDRANGE_HELPER(Min, min);
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_MDRange.hpp:KOKKOS_IMPL_OPENACC_PARALLEL_REDUCE_MDRANGE_HELPER(Max, max);
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_MDRange.hpp:KOKKOS_IMPL_OPENACC_PARALLEL_REDUCE_MDRANGE_HELPER(LAnd, &&);
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_MDRange.hpp:KOKKOS_IMPL_OPENACC_PARALLEL_REDUCE_MDRANGE_HELPER(LOr, ||);
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_MDRange.hpp:KOKKOS_IMPL_OPENACC_PARALLEL_REDUCE_MDRANGE_HELPER(BAnd, &);
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_MDRange.hpp:KOKKOS_IMPL_OPENACC_PARALLEL_REDUCE_MDRANGE_HELPER(BOr, |);
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_MDRange.hpp:#undef KOKKOS_IMPL_OPENACC_PARALLEL_REDUCE_MDRANGE_HELPER
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_MDRange.hpp:#undef KOKKOS_IMPL_OPENACC_PARALLEL_REDUCE_DISPATCH_ITERATE
lib/kokkos/core/src/OpenACC/Kokkos_OpenACCSpace.hpp:#ifndef KOKKOS_OPENACC_SPACE_HPP
lib/kokkos/core/src/OpenACC/Kokkos_OpenACCSpace.hpp:#define KOKKOS_OPENACC_SPACE_HPP
lib/kokkos/core/src/OpenACC/Kokkos_OpenACCSpace.hpp:#include <openacc.h>
lib/kokkos/core/src/OpenACC/Kokkos_OpenACCSpace.hpp:class OpenACC;
lib/kokkos/core/src/OpenACC/Kokkos_OpenACCSpace.hpp:class OpenACCSpace {
lib/kokkos/core/src/OpenACC/Kokkos_OpenACCSpace.hpp:  using memory_space    = OpenACCSpace;
lib/kokkos/core/src/OpenACC/Kokkos_OpenACCSpace.hpp:  using execution_space = OpenACC;
lib/kokkos/core/src/OpenACC/Kokkos_OpenACCSpace.hpp:  OpenACCSpace() = default;
lib/kokkos/core/src/OpenACC/Kokkos_OpenACCSpace.hpp:  void* allocate(const Kokkos::Experimental::OpenACC& exec_space,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACCSpace.hpp:  void* allocate(const Kokkos::Experimental::OpenACC& exec_space,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACCSpace.hpp:  static constexpr char const* name() { return "OpenACCSpace"; }
lib/kokkos/core/src/OpenACC/Kokkos_OpenACCSpace.hpp:  void* impl_allocate(const Kokkos::Experimental::OpenACC& exec_space,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACCSpace.hpp:                                       Kokkos::Experimental::OpenACCSpace> {
lib/kokkos/core/src/OpenACC/Kokkos_OpenACCSpace.hpp:struct Kokkos::Impl::MemorySpaceAccess<Kokkos::Experimental::OpenACCSpace,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACCSpace.hpp:struct Kokkos::Impl::MemorySpaceAccess<Kokkos::Experimental::OpenACCSpace,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACCSpace.hpp:                                       Kokkos::Experimental::OpenACCSpace> {
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_FunctorAdapter.hpp:#ifndef KOKKOS_OPENACC_FUNCTOR_ADAPTER_HPP
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_FunctorAdapter.hpp:#define KOKKOS_OPENACC_FUNCTOR_ADAPTER_HPP
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_FunctorAdapter.hpp:#include <OpenACC/Kokkos_OpenACC_Macros.hpp>
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:#ifndef KOKKOS_OPENACC_PARALLEL_REDUCE_TEAM_HPP
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:#define KOKKOS_OPENACC_PARALLEL_REDUCE_TEAM_HPP
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:#include <OpenACC/Kokkos_OpenACC_Team.hpp>
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:#include <OpenACC/Kokkos_OpenACC_FunctorAdapter.hpp>
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:#include <OpenACC/Kokkos_OpenACC_Macros.hpp>
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:#ifdef KOKKOS_ENABLE_OPENACC_COLLAPSE_HIERARCHICAL_CONSTRUCTS
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:#define KOKKOS_IMPL_OPENACC_LOOP_CLAUSE \
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:#define KOKKOS_IMPL_OPENACC_LOOP_CLAUSE \
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:struct OpenACCParallelReduceTeamHelper {
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:  OpenACCParallelReduceTeamHelper(Functor const&, Reducer const&,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:                                   Kokkos::Experimental::OpenACC> {
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:      TeamPolicyInternal<Kokkos::Experimental::OpenACC, Properties...>;
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:    Kokkos::Experimental::Impl::OpenACCParallelReduceTeamHelper(
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:            FunctorType, Policy, KOKKOS_IMPL_OPENACC_LOOP_CLAUSE>(
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:    // OpenACC backend supports only built-in Reducer types; thus
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:    // above OpenACC compute kernel can be executed asynchronously and val is a
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:            MemorySpaceAccess<Kokkos::Experimental::OpenACCSpace,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:// FIXME_OPENACC: custom reduction is not implemented.
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:    const Impl::TeamThreadRangeBoundariesStruct<iType, Impl::OpenACCTeamMember>&
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:// FIXME_OPENACC: custom reduction is not implemented.
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:        iType, Impl::OpenACCTeamMember>& loop_boundaries,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:#ifdef KOKKOS_ENABLE_OPENACC_COLLAPSE_HIERARCHICAL_CONSTRUCTS
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:                    iType, Impl::OpenACCTeamMember>& loop_boundaries,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:                    iType, Impl::OpenACCTeamMember>& loop_boundaries,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:                    iType, Impl::OpenACCTeamMember>& loop_boundaries,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:                    iType, Impl::OpenACCTeamMember>& loop_boundaries,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:    const Impl::TeamVectorRangeBoundariesStruct<iType, Impl::OpenACCTeamMember>&
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:#else /* #ifdef KOKKOS_ENABLE_OPENACC_COLLAPSE_HIERARCHICAL_CONSTRUCTS */
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:// FIXME_OPENACC: below implementation conforms to the OpenACC standard, but
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:                    iType, Impl::OpenACCTeamMember>& loop_boundaries,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:#define KOKKOS_IMPL_OPENACC_PARALLEL_REDUCE_TEAM_THREAD(REDUCER, OPERATOR)   \
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:                          iType, Impl::OpenACCTeamMember>& loop_boundaries,  \
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:KOKKOS_IMPL_OPENACC_PARALLEL_REDUCE_TEAM_THREAD(Sum, +);
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:KOKKOS_IMPL_OPENACC_PARALLEL_REDUCE_TEAM_THREAD(Prod, *);
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:KOKKOS_IMPL_OPENACC_PARALLEL_REDUCE_TEAM_THREAD(Min, min);
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:KOKKOS_IMPL_OPENACC_PARALLEL_REDUCE_TEAM_THREAD(Max, max);
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:KOKKOS_IMPL_OPENACC_PARALLEL_REDUCE_TEAM_THREAD(LAnd, &&);
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:KOKKOS_IMPL_OPENACC_PARALLEL_REDUCE_TEAM_THREAD(LOr, ||);
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:KOKKOS_IMPL_OPENACC_PARALLEL_REDUCE_TEAM_THREAD(BAnd, &);
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:KOKKOS_IMPL_OPENACC_PARALLEL_REDUCE_TEAM_THREAD(BOr, |);
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:                    iType, Impl::OpenACCTeamMember>& loop_boundaries,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:#define KOKKOS_IMPL_OPENACC_PARALLEL_REDUCE_THREAD_VECTOR(REDUCER, OPERATOR) \
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:                          iType, Impl::OpenACCTeamMember>& loop_boundaries,  \
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:KOKKOS_IMPL_OPENACC_PARALLEL_REDUCE_THREAD_VECTOR(Sum, +);
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:KOKKOS_IMPL_OPENACC_PARALLEL_REDUCE_THREAD_VECTOR(Prod, *);
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:KOKKOS_IMPL_OPENACC_PARALLEL_REDUCE_THREAD_VECTOR(Min, min);
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:KOKKOS_IMPL_OPENACC_PARALLEL_REDUCE_THREAD_VECTOR(Max, max);
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:KOKKOS_IMPL_OPENACC_PARALLEL_REDUCE_THREAD_VECTOR(LAnd, &&);
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:KOKKOS_IMPL_OPENACC_PARALLEL_REDUCE_THREAD_VECTOR(LOr, ||);
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:KOKKOS_IMPL_OPENACC_PARALLEL_REDUCE_THREAD_VECTOR(BAnd, &);
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:KOKKOS_IMPL_OPENACC_PARALLEL_REDUCE_THREAD_VECTOR(BOr, |);
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:    const Impl::TeamVectorRangeBoundariesStruct<iType, Impl::OpenACCTeamMember>&
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:#undef KOKKOS_IMPL_OPENACC_PARALLEL_REDUCE_TEAM_THREAD
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:#undef KOKKOS_IMPL_OPENACC_PARALLEL_REDUCE_THREAD_VECTOR
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:#endif /* #ifdef KOKKOS_ENABLE_OPENACC_COLLAPSE_HIERARCHICAL_CONSTRUCTS */
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:#define KOKKOS_IMPL_OPENACC_PARALLEL_REDUCE_DISPATCH_SCHEDULE(REDUCER,     \
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:  void OpenACCParallelReduceTeam##REDUCER(Policy const policy,             \
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:#define KOKKOS_IMPL_OPENACC_PARALLEL_REDUCE_TEAM_HELPER(REDUCER, OPERATOR) \
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:  KOKKOS_IMPL_OPENACC_PARALLEL_REDUCE_DISPATCH_SCHEDULE(REDUCER, OPERATOR) \
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:  struct Kokkos::Experimental::Impl::OpenACCParallelReduceTeamHelper<      \
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:    OpenACCParallelReduceTeamHelper(Functor const& functor,                \
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:      OpenACCParallelReduceTeam##REDUCER(policy, val, functor, async_arg); \
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:KOKKOS_IMPL_OPENACC_PARALLEL_REDUCE_TEAM_HELPER(Sum, +);
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:KOKKOS_IMPL_OPENACC_PARALLEL_REDUCE_TEAM_HELPER(Prod, *);
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:KOKKOS_IMPL_OPENACC_PARALLEL_REDUCE_TEAM_HELPER(Min, min);
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:KOKKOS_IMPL_OPENACC_PARALLEL_REDUCE_TEAM_HELPER(Max, max);
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:KOKKOS_IMPL_OPENACC_PARALLEL_REDUCE_TEAM_HELPER(LAnd, &&);
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:KOKKOS_IMPL_OPENACC_PARALLEL_REDUCE_TEAM_HELPER(LOr, ||);
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:KOKKOS_IMPL_OPENACC_PARALLEL_REDUCE_TEAM_HELPER(BAnd, &);
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:KOKKOS_IMPL_OPENACC_PARALLEL_REDUCE_TEAM_HELPER(BOr, |);
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:#undef KOKKOS_IMPL_OPENACC_PARALLEL_REDUCE_TEAM_HELPER
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:#undef KOKKOS_IMPL_OPENACC_PARALLEL_REDUCE_DISPATCH_SCHEDULE
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Team.hpp:#endif /* #ifndef KOKKOS_OPENACC_PARALLEL_REDUCE_TEAM_HPP */
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Macros.hpp:#ifndef KOKKOS_OPENACC_MACROS_HPP
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Macros.hpp:#define KOKKOS_OPENACC_MACROS_HPP
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.hpp:#ifndef KOKKOS_OPENACC_HPP
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.hpp:#define KOKKOS_OPENACC_HPP
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.hpp:#include <OpenACC/Kokkos_OpenACCSpace.hpp>
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.hpp:#include <OpenACC/Kokkos_OpenACC_Traits.hpp>
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.hpp:#include <openacc.h>
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.hpp:// FIXME_OPENACC: Below macro is temporarily enabled to avoid issues on existing
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.hpp:// OpenACC compilers not supporting lambda with parallel loops.
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.hpp:#define KOKKOS_ENABLE_OPENACC_COLLAPSE_HIERARCHICAL_CONSTRUCTS
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.hpp:class OpenACCInternal;
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.hpp:class OpenACC {
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.hpp:  Kokkos::Impl::HostSharedPtr<Impl::OpenACCInternal> m_space_instance;
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.hpp:  friend bool operator==(OpenACC const& lhs, OpenACC const& rhs) {
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.hpp:  friend bool operator!=(OpenACC const& lhs, OpenACC const& rhs) {
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.hpp:  using execution_space = OpenACC;
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.hpp:  using memory_space    = OpenACCSpace;
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.hpp:  using scratch_memory_space = ScratchMemorySpace<OpenACC>;
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.hpp:  OpenACC();
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.hpp:  explicit OpenACC(int async_arg);
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.hpp:                 "Kokkos::OpenACC::fence(): Unnamed Instance Fence") const;
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.hpp:  static char const* name() { return "OpenACC"; }
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.hpp:  static int concurrency() { return 256000; }  // FIXME_OPENACC
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.hpp:  int concurrency() const { return 256000; }  // FIXME_OPENACC
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.hpp:  Impl::OpenACCInternal* impl_internal_space_instance() const {
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.hpp:    ::Kokkos::Experimental::OpenACC> {
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.hpp:      ::Kokkos::Profiling::Experimental::DeviceType::OpenACC;
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC.hpp:  static int device_id(const Kokkos::Experimental::OpenACC& accInstance) {
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Range.hpp:#ifndef KOKKOS_OPENACC_PARALLEL_REDUCE_RANGE_HPP
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Range.hpp:#define KOKKOS_OPENACC_PARALLEL_REDUCE_RANGE_HPP
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Range.hpp:#include <OpenACC/Kokkos_OpenACC.hpp>
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Range.hpp:#include <OpenACC/Kokkos_OpenACC_Macros.hpp>
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Range.hpp:#include <OpenACC/Kokkos_OpenACC_FunctorAdapter.hpp>
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Range.hpp:#include <OpenACC/Kokkos_OpenACC_ScheduleType.hpp>
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Range.hpp:struct OpenACCParallelReduceHelper {
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Range.hpp:  OpenACCParallelReduceHelper(Functor const&, Reducer const&, Policy const&) {
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Range.hpp:                                   Kokkos::Experimental::OpenACC> {
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Range.hpp:            MemorySpaceAccess<Kokkos::Experimental::OpenACCSpace,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Range.hpp:    Kokkos::Experimental::Impl::OpenACCParallelReduceHelper(
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Range.hpp:    // OpenACC backend supports only built-in Reducer types; thus
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Range.hpp:    // above OpenACC compute kernel can be executed asynchronously and val is a
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Range.hpp:#define KOKKOS_IMPL_OPENACC_PARALLEL_REDUCE_DISPATCH_SCHEDULE(REDUCER,    \
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Range.hpp:  void OpenACCParallelReduce##REDUCER(Schedule<Static>, int chunk_size,   \
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Range.hpp:    /* FIXME_OPENACC FIXME_NVHPC workaround compiler bug (incorrect scope \
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Range.hpp:  void OpenACCParallelReduce##REDUCER(Schedule<Dynamic>, int chunk_size,  \
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Range.hpp:    /* FIXME_OPENACC FIXME_NVHPC workaround compiler bug (incorrect scope \
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Range.hpp:#define KOKKOS_IMPL_OPENACC_PARALLEL_REDUCE_HELPER(REDUCER, OPERATOR)          \
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Range.hpp:  KOKKOS_IMPL_OPENACC_PARALLEL_REDUCE_DISPATCH_SCHEDULE(REDUCER, OPERATOR)     \
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Range.hpp:  struct Kokkos::Experimental::Impl::OpenACCParallelReduceHelper<              \
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Range.hpp:        Kokkos::Experimental::Impl::OpenACCScheduleType<Policy>;               \
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Range.hpp:    OpenACCParallelReduceHelper(Functor const& functor,                        \
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Range.hpp:      OpenACCParallelReduce##REDUCER(ScheduleType(), chunk_size, begin, end,   \
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Range.hpp:KOKKOS_IMPL_OPENACC_PARALLEL_REDUCE_HELPER(Sum, +);
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Range.hpp:KOKKOS_IMPL_OPENACC_PARALLEL_REDUCE_HELPER(Prod, *);
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Range.hpp:KOKKOS_IMPL_OPENACC_PARALLEL_REDUCE_HELPER(Min, min);
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Range.hpp:KOKKOS_IMPL_OPENACC_PARALLEL_REDUCE_HELPER(Max, max);
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Range.hpp:KOKKOS_IMPL_OPENACC_PARALLEL_REDUCE_HELPER(LAnd, &&);
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Range.hpp:KOKKOS_IMPL_OPENACC_PARALLEL_REDUCE_HELPER(LOr, ||);
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Range.hpp:KOKKOS_IMPL_OPENACC_PARALLEL_REDUCE_HELPER(BAnd, &);
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Range.hpp:KOKKOS_IMPL_OPENACC_PARALLEL_REDUCE_HELPER(BOr, |);
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Range.hpp:#undef KOKKOS_IMPL_OPENACC_PARALLEL_REDUCE_HELPER
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelReduce_Range.hpp:#undef KOKKOS_IMPL_OPENACC_PARALLEL_REDUCE_DISPATCH_SCHEDULE
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_SharedAllocationRecord.cpp:#include <OpenACC/Kokkos_OpenACC.hpp>
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_SharedAllocationRecord.cpp:#include <OpenACC/Kokkos_OpenACC_DeepCopy.hpp>
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_SharedAllocationRecord.cpp:#include <OpenACC/Kokkos_OpenACC_SharedAllocationRecord.hpp>
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_SharedAllocationRecord.cpp:    Kokkos::Experimental::OpenACCSpace);
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:#ifndef KOKKOS_OPENACC_PARALLEL_FOR_MDRANGE_HPP
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:#define KOKKOS_OPENACC_PARALLEL_FOR_MDRANGE_HPP
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:#include <OpenACC/Kokkos_OpenACC.hpp>
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:#include <OpenACC/Kokkos_OpenACC_FunctorAdapter.hpp>
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:#include <OpenACC/Kokkos_OpenACC_MDRangePolicy.hpp>
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:void OpenACCParallelForMDRangePolicy(OpenACCCollapse, OpenACCIterateLeft,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:                                     OpenACCMDRangeBegin<2> const& begin,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:                                     OpenACCMDRangeEnd<2> const& end,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:void OpenACCParallelForMDRangePolicy(OpenACCCollapse, OpenACCIterateRight,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:                                     OpenACCMDRangeBegin<2> const& begin,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:                                     OpenACCMDRangeEnd<2> const& end,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:void OpenACCParallelForMDRangePolicy(OpenACCTile, OpenACCIterateLeft,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:                                     OpenACCMDRangeBegin<2> const& begin,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:                                     OpenACCMDRangeEnd<2> const& end,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:                                     OpenACCMDRangeTile<2> const& tile,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:void OpenACCParallelForMDRangePolicy(OpenACCTile, OpenACCIterateRight,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:                                     OpenACCMDRangeBegin<2> const& begin,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:                                     OpenACCMDRangeEnd<2> const& end,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:                                     OpenACCMDRangeTile<2> const& tile,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:void OpenACCParallelForMDRangePolicy(OpenACCCollapse, OpenACCIterateLeft,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:                                     OpenACCMDRangeBegin<3> const& begin,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:                                     OpenACCMDRangeEnd<3> const& end,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:void OpenACCParallelForMDRangePolicy(OpenACCCollapse, OpenACCIterateRight,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:                                     OpenACCMDRangeBegin<3> const& begin,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:                                     OpenACCMDRangeEnd<3> const& end,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:void OpenACCParallelForMDRangePolicy(OpenACCTile, OpenACCIterateLeft,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:                                     OpenACCMDRangeBegin<3> const& begin,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:                                     OpenACCMDRangeEnd<3> const& end,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:                                     OpenACCMDRangeTile<3> const& tile,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:void OpenACCParallelForMDRangePolicy(OpenACCTile, OpenACCIterateRight,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:                                     OpenACCMDRangeBegin<3> const& begin,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:                                     OpenACCMDRangeEnd<3> const& end,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:                                     OpenACCMDRangeTile<3> const& tile,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:void OpenACCParallelForMDRangePolicy(OpenACCCollapse, OpenACCIterateLeft,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:                                     OpenACCMDRangeBegin<4> const& begin,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:                                     OpenACCMDRangeEnd<4> const& end,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:void OpenACCParallelForMDRangePolicy(OpenACCCollapse, OpenACCIterateRight,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:                                     OpenACCMDRangeBegin<4> const& begin,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:                                     OpenACCMDRangeEnd<4> const& end,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:void OpenACCParallelForMDRangePolicy(OpenACCTile, OpenACCIterateLeft,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:                                     OpenACCMDRangeBegin<4> const& begin,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:                                     OpenACCMDRangeEnd<4> const& end,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:                                     OpenACCMDRangeTile<4> const& tile,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:void OpenACCParallelForMDRangePolicy(OpenACCTile, OpenACCIterateRight,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:                                     OpenACCMDRangeBegin<4> const& begin,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:                                     OpenACCMDRangeEnd<4> const& end,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:                                     OpenACCMDRangeTile<4> const& tile,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:void OpenACCParallelForMDRangePolicy(OpenACCCollapse, OpenACCIterateLeft,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:                                     OpenACCMDRangeBegin<5> const& begin,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:                                     OpenACCMDRangeEnd<5> const& end,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:void OpenACCParallelForMDRangePolicy(OpenACCCollapse, OpenACCIterateRight,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:                                     OpenACCMDRangeBegin<5> const& begin,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:                                     OpenACCMDRangeEnd<5> const& end,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:void OpenACCParallelForMDRangePolicy(OpenACCTile, OpenACCIterateLeft,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:                                     OpenACCMDRangeBegin<5> const& begin,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:                                     OpenACCMDRangeEnd<5> const& end,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:                                     OpenACCMDRangeTile<5> const& tile,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:void OpenACCParallelForMDRangePolicy(OpenACCTile, OpenACCIterateRight,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:                                     OpenACCMDRangeBegin<5> const& begin,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:                                     OpenACCMDRangeEnd<5> const& end,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:                                     OpenACCMDRangeTile<5> const& tile,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:void OpenACCParallelForMDRangePolicy(OpenACCCollapse, OpenACCIterateLeft,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:                                     OpenACCMDRangeBegin<6> const& begin,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:                                     OpenACCMDRangeEnd<6> const& end,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:void OpenACCParallelForMDRangePolicy(OpenACCCollapse, OpenACCIterateRight,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:                                     OpenACCMDRangeBegin<6> const& begin,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:                                     OpenACCMDRangeEnd<6> const& end,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:void OpenACCParallelForMDRangePolicy(OpenACCTile, OpenACCIterateLeft,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:                                     OpenACCMDRangeBegin<6> const& begin,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:                                     OpenACCMDRangeEnd<6> const& end,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:                                     OpenACCMDRangeTile<6> const& tile,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:void OpenACCParallelForMDRangePolicy(OpenACCTile, OpenACCIterateRight,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:                                     OpenACCMDRangeBegin<6> const& begin,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:                                     OpenACCMDRangeEnd<6> const& end,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:                                     OpenACCMDRangeTile<6> const& tile,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:                                Kokkos::Experimental::OpenACC> {
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:#if 0  // FIXME_OPENACC: OpenACC requires tile size to be constant.
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:        Kokkos::Experimental::Impl::OpenACCParallelForMDRangePolicy(
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:            Kokkos::Experimental::Impl::OpenACCCollapse(),
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:    Kokkos::Experimental::Impl::OpenACCParallelForMDRangePolicy(
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:        Kokkos::Experimental::Impl::OpenACCTile(),
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:    Kokkos::Experimental::Impl::OpenACCParallelForMDRangePolicy(
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_MDRange.hpp:        Kokkos::Experimental::Impl::OpenACCCollapse(),
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Traits.hpp:#ifndef KOKKOS_OPENACC_TRAITS_HPP
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Traits.hpp:#define KOKKOS_OPENACC_TRAITS_HPP
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Traits.hpp:#include <openacc.h>
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Traits.hpp:struct OpenACC_Traits {
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Traits.hpp:#if defined(KOKKOS_IMPL_ARCH_NVIDIA_GPU)
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Traits.hpp:  static constexpr acc_device_t dev_type     = acc_device_nvidia;
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Traits.hpp:#elif defined(KOKKOS_ARCH_AMD_GPU)
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:#ifndef KOKKOS_OPENACC_TEAM_HPP
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:#define KOKKOS_OPENACC_TEAM_HPP
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:#include <openacc.h>
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:#include <OpenACC/Kokkos_OpenACC.hpp>
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:class OpenACCTeamMember {
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:  // FIXME_OPENACC: default-team-size macros are temporarily used for
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:  using execution_space      = Kokkos::Experimental::OpenACC;
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:  using team_handle          = OpenACCTeamMember;
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:  // FIXME_OPENACC: OpenACC does not provide any explicit barrier constructs
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:        "Kokkos::Experimental::OpenACC ERROR: OpenACC does not provide any "
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:  // FIXME_OPENACC: team_broadcast() is not implemented.
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:                  "OpenACC backend");
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:  // FIXME_OPENACC: team_reduce() is not implemented.
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:                  "OpenACC backend");
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:  // FIXME_OPENACC: team_scan() is not implemented.
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:        "Kokkos Error: team_scan() is not implemented for the OpenACC backend");
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:  // FIXME_OPENACC - 512(16*32) bytes at the begining of the scratch space
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:  OpenACCTeamMember(const int league_rank, const int league_size,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:                                              // OpenACC, Properties ...> & team
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:class TeamPolicyInternal<Kokkos::Experimental::OpenACC, Properties...>
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:  // FIXME_OPENACC: update team_size_max() APIs with realistic
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:  // FIXME_OPENACC: update team_size_recommended() APIs with realistic
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:      OpenACCTeamMember::DEFAULT_TEAM_SIZE_MAX;
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:      OpenACCTeamMember::DEFAULT_TEAM_SIZE_REC;
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:  Kokkos::Experimental::OpenACC space() const {
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:    return Kokkos::Experimental::OpenACC();
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:  using member_type = Impl::OpenACCTeamMember;
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:struct TeamThreadRangeBoundariesStruct<iType, OpenACCTeamMember> {
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:  const OpenACCTeamMember& team;
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:  TeamThreadRangeBoundariesStruct(const OpenACCTeamMember& thread_, iType count)
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:  TeamThreadRangeBoundariesStruct(const OpenACCTeamMember& thread_,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:struct ThreadVectorRangeBoundariesStruct<iType, OpenACCTeamMember> {
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:  const OpenACCTeamMember& team;
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:  ThreadVectorRangeBoundariesStruct(const OpenACCTeamMember& thread_,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:  ThreadVectorRangeBoundariesStruct(const OpenACCTeamMember& thread_,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:struct TeamVectorRangeBoundariesStruct<iType, OpenACCTeamMember> {
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:  const OpenACCTeamMember& team;
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:  TeamVectorRangeBoundariesStruct(const OpenACCTeamMember& thread_,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:  TeamVectorRangeBoundariesStruct(const OpenACCTeamMember& thread_,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:    Impl::TeamThreadRangeBoundariesStruct<iType, Impl::OpenACCTeamMember>
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:    TeamThreadRange(const Impl::OpenACCTeamMember& thread, const iType& count) {
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:  return Impl::TeamThreadRangeBoundariesStruct<iType, Impl::OpenACCTeamMember>(
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:    typename std::common_type<iType1, iType2>::type, Impl::OpenACCTeamMember>
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:TeamThreadRange(const Impl::OpenACCTeamMember& thread, const iType1& begin,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:  return Impl::TeamThreadRangeBoundariesStruct<iType, Impl::OpenACCTeamMember>(
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:    Impl::ThreadVectorRangeBoundariesStruct<iType, Impl::OpenACCTeamMember>
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:    ThreadVectorRange(const Impl::OpenACCTeamMember& thread,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:                                                 Impl::OpenACCTeamMember>(
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:    typename std::common_type<iType1, iType2>::type, Impl::OpenACCTeamMember>
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:ThreadVectorRange(const Impl::OpenACCTeamMember& thread,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:                                                 Impl::OpenACCTeamMember>(
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:    Impl::TeamVectorRangeBoundariesStruct<iType, Impl::OpenACCTeamMember>
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:    TeamVectorRange(const Impl::OpenACCTeamMember& thread, const iType& count) {
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:  return Impl::TeamVectorRangeBoundariesStruct<iType, Impl::OpenACCTeamMember>(
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:    typename std::common_type<iType1, iType2>::type, Impl::OpenACCTeamMember>
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:TeamVectorRange(const Impl::OpenACCTeamMember& thread, const iType1& arg_begin,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:  return Impl::TeamVectorRangeBoundariesStruct<iType, Impl::OpenACCTeamMember>(
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:Impl::ThreadSingleStruct<Impl::OpenACCTeamMember> PerTeam(
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:    const Impl::OpenACCTeamMember& thread) {
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:  return Impl::ThreadSingleStruct<Impl::OpenACCTeamMember>(thread);
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:Impl::VectorSingleStruct<Impl::OpenACCTeamMember> PerThread(
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:    const Impl::OpenACCTeamMember& thread) {
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:  return Impl::VectorSingleStruct<Impl::OpenACCTeamMember>(thread);
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:    const Impl::VectorSingleStruct<Impl::OpenACCTeamMember>&
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:    const Impl::ThreadSingleStruct<Impl::OpenACCTeamMember>& single_struct,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:    const Impl::VectorSingleStruct<Impl::OpenACCTeamMember>&
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:    const Impl::ThreadSingleStruct<Impl::OpenACCTeamMember>& single_struct,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Team.hpp:#endif /* #ifndef KOKKOS_OPENACC_TEAM_HPP */
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Instance.hpp:#ifndef KOKKOS_OPENACC_INSTANCE_HPP
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Instance.hpp:#define KOKKOS_OPENACC_INSTANCE_HPP
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Instance.hpp:#include <openacc.h>
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Instance.hpp:class OpenACCInternal {
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Instance.hpp:  OpenACCInternal(const OpenACCInternal&) = default;
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Instance.hpp:  OpenACCInternal& operator=(const OpenACCInternal&) = default;
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Instance.hpp:  OpenACCInternal() = default;
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_Instance.hpp:  static OpenACCInternal& singleton();
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_Range.hpp:#ifndef KOKKOS_OPENACC_PARALLEL_FOR_RANGE_HPP
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_Range.hpp:#define KOKKOS_OPENACC_PARALLEL_FOR_RANGE_HPP
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_Range.hpp:#include <OpenACC/Kokkos_OpenACC.hpp>
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_Range.hpp:#include <OpenACC/Kokkos_OpenACC_FunctorAdapter.hpp>
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_Range.hpp:#include <OpenACC/Kokkos_OpenACC_ScheduleType.hpp>
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_Range.hpp:void OpenACCParallelForRangePolicy(Schedule<Static>, int chunk_size,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_Range.hpp:  // FIXME_OPENACC FIXME_NVHPC workaround compiler bug (incorrect scope
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_Range.hpp:void OpenACCParallelForRangePolicy(Schedule<Dynamic>, int chunk_size,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_Range.hpp:  // FIXME_OPENACC FIXME_NVHPC workaround compiler bug (incorrect scope
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_Range.hpp:                                Kokkos::Experimental::OpenACC> {
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_Range.hpp:  using ScheduleType = Kokkos::Experimental::Impl::OpenACCScheduleType<Policy>;
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_Range.hpp:    Kokkos::Experimental::Impl::OpenACCParallelForRangePolicy(
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_Team.hpp:#ifndef KOKKOS_OPENACC_PARALLEL_FOR_TEAM_HPP
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_Team.hpp:#define KOKKOS_OPENACC_PARALLEL_FOR_TEAM_HPP
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_Team.hpp:#include <OpenACC/Kokkos_OpenACC_Team.hpp>
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_Team.hpp:#include <OpenACC/Kokkos_OpenACC_FunctorAdapter.hpp>
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_Team.hpp:#ifdef KOKKOS_ENABLE_OPENACC_COLLAPSE_HIERARCHICAL_CONSTRUCTS
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_Team.hpp:                                Kokkos::Experimental::OpenACC> {
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_Team.hpp:  using Policy = Kokkos::Impl::TeamPolicyInternal<Kokkos::Experimental::OpenACC,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_Team.hpp:    const Impl::TeamThreadRangeBoundariesStruct<iType, Impl::OpenACCTeamMember>&
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_Team.hpp:        iType, Impl::OpenACCTeamMember>& loop_boundaries,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_Team.hpp:    const Impl::TeamVectorRangeBoundariesStruct<iType, Impl::OpenACCTeamMember>&
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_Team.hpp:#else  // KOKKOS_ENABLE_OPENACC_COLLAPSE_HIERARCHICAL_CONSTRUCTS
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_Team.hpp:                                Kokkos::Experimental::OpenACC> {
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_Team.hpp:  using Policy = Kokkos::Impl::TeamPolicyInternal<Kokkos::Experimental::OpenACC,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_Team.hpp:    const Impl::TeamThreadRangeBoundariesStruct<iType, Impl::OpenACCTeamMember>&
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_Team.hpp:        iType, Impl::OpenACCTeamMember>& loop_boundaries,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_Team.hpp:    const Impl::TeamVectorRangeBoundariesStruct<iType, Impl::OpenACCTeamMember>&
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_Team.hpp:#endif /* #ifdef KOKKOS_ENABLE_OPENACC_COLLAPSE_HIERARCHICAL_CONSTRUCTS */
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_ParallelFor_Team.hpp:#endif /* #ifndef KOKKOS_OPENACC_PARALLEL_FOR_TEAM_HPP */
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_DeepCopy.hpp:#ifndef KOKKOS_OPENACC_DEEP_COPY_HPP
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_DeepCopy.hpp:#define KOKKOS_OPENACC_DEEP_COPY_HPP
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_DeepCopy.hpp:#include <OpenACC/Kokkos_OpenACC.hpp>
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_DeepCopy.hpp:#include <OpenACC/Kokkos_OpenACCSpace.hpp>
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_DeepCopy.hpp:#include <openacc.h>
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_DeepCopy.hpp:struct Kokkos::Impl::DeepCopy<Kokkos::Experimental::OpenACCSpace,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_DeepCopy.hpp:                              Kokkos::Experimental::OpenACCSpace,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_DeepCopy.hpp:                              Kokkos::Experimental::OpenACC> {
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_DeepCopy.hpp:    // clarified only in the latest OpenACC specification (V3.2), and thus the
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_DeepCopy.hpp:    // supports OpenACC V2.7.)
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_DeepCopy.hpp:  DeepCopy(const Kokkos::Experimental::OpenACC& exec, void* dst,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_DeepCopy.hpp:struct Kokkos::Impl::DeepCopy<Kokkos::Experimental::OpenACCSpace,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_DeepCopy.hpp:                              Kokkos::Experimental::OpenACCSpace,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_DeepCopy.hpp:        "Kokkos::Impl::DeepCopy<OpenACCSpace, OpenACCSpace, "
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_DeepCopy.hpp:struct Kokkos::Impl::DeepCopy<Kokkos::Experimental::OpenACCSpace,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_DeepCopy.hpp:                              Kokkos::Experimental::OpenACC> {
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_DeepCopy.hpp:  DeepCopy(const Kokkos::Experimental::OpenACC& exec, void* dst,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_DeepCopy.hpp:struct Kokkos::Impl::DeepCopy<Kokkos::Experimental::OpenACCSpace,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_DeepCopy.hpp:        "Kokkos::Impl::DeepCopy<OpenACCSpace, HostSpace, "
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_DeepCopy.hpp:                              Kokkos::Experimental::OpenACCSpace,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_DeepCopy.hpp:                              Kokkos::Experimental::OpenACC> {
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_DeepCopy.hpp:  DeepCopy(const Kokkos::Experimental::OpenACC& exec, void* dst,
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_DeepCopy.hpp:    Kokkos::HostSpace, Kokkos::Experimental::OpenACCSpace, ExecutionSpace> {
lib/kokkos/core/src/OpenACC/Kokkos_OpenACC_DeepCopy.hpp:        "Kokkos::Impl::DeepCopy<HostSpace, OpenACCSpace, "
lib/kokkos/core/src/Kokkos_TaskScheduler_fwd.hpp:#if defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/core/src/Kokkos_TaskScheduler_fwd.hpp:struct default_tasking_memory_space_for_execution_space<Kokkos::Cuda> {
lib/kokkos/core/src/Kokkos_TaskScheduler_fwd.hpp:  using type = Kokkos::CudaUVMSpace;
lib/kokkos/core/src/Kokkos_TaskScheduler.hpp:  // May be spawning a Cuda task, must use the specialization
lib/kokkos/core/src/Cuda/Kokkos_Cuda_BlockSize_Deduction.hpp:#ifndef KOKKOS_CUDA_INTERNAL_HPP
lib/kokkos/core/src/Cuda/Kokkos_Cuda_BlockSize_Deduction.hpp:#define KOKKOS_CUDA_INTERNAL_HPP
lib/kokkos/core/src/Cuda/Kokkos_Cuda_BlockSize_Deduction.hpp:#ifdef KOKKOS_ENABLE_CUDA
lib/kokkos/core/src/Cuda/Kokkos_Cuda_BlockSize_Deduction.hpp:#include <Cuda/Kokkos_Cuda_Error.hpp>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_BlockSize_Deduction.hpp:inline int cuda_warp_per_sm_allocation_granularity(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_BlockSize_Deduction.hpp:    cudaDeviceProp const& properties) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_BlockSize_Deduction.hpp:          "Unknown device in cuda warp per sm allocation granularity");
lib/kokkos/core/src/Cuda/Kokkos_Cuda_BlockSize_Deduction.hpp:inline int cuda_max_warps_per_sm_registers(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_BlockSize_Deduction.hpp:    cudaDeviceProp const& properties, cudaFuncAttributes const& attributes) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_BlockSize_Deduction.hpp:      cuda_warp_per_sm_allocation_granularity(properties);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_BlockSize_Deduction.hpp:inline int cuda_max_active_blocks_per_sm(cudaDeviceProp const& properties,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_BlockSize_Deduction.hpp:                                         cudaFuncAttributes const& attributes,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_BlockSize_Deduction.hpp:      cuda_max_warps_per_sm_registers(properties, attributes);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_BlockSize_Deduction.hpp:  // granularity defined in `cuda_warp_per_sm_allocation_granularity`.
lib/kokkos/core/src/Cuda/Kokkos_Cuda_BlockSize_Deduction.hpp:#if CUDA_VERSION >= 11000
lib/kokkos/core/src/Cuda/Kokkos_Cuda_BlockSize_Deduction.hpp:        throw_runtime_exception("Unknown device in cuda block size deduction");
lib/kokkos/core/src/Cuda/Kokkos_Cuda_BlockSize_Deduction.hpp:inline int cuda_deduce_block_size(bool early_termination,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_BlockSize_Deduction.hpp:                                  cudaDeviceProp const& properties,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_BlockSize_Deduction.hpp:                                  cudaFuncAttributes const& attributes,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_BlockSize_Deduction.hpp:    int blocks_per_sm = cuda_max_active_blocks_per_sm(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_BlockSize_Deduction.hpp:int cuda_get_max_block_size(const CudaInternal* cuda_instance,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_BlockSize_Deduction.hpp:                            const cudaFuncAttributes& attr,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_BlockSize_Deduction.hpp:  auto const& prop = cuda_instance->m_deviceProp;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_BlockSize_Deduction.hpp:  return cuda_deduce_block_size(true, prop, attr, block_size_to_dynamic_shmem,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_BlockSize_Deduction.hpp:int cuda_get_opt_block_size(const CudaInternal* cuda_instance,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_BlockSize_Deduction.hpp:                            const cudaFuncAttributes& attr,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_BlockSize_Deduction.hpp:  auto const& prop = cuda_instance->m_deviceProp;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_BlockSize_Deduction.hpp:  return cuda_deduce_block_size(false, prop, attr, block_size_to_dynamic_shmem,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_BlockSize_Deduction.hpp:// Thin version of cuda_get_opt_block_size for cases where there is no shared
lib/kokkos/core/src/Cuda/Kokkos_Cuda_BlockSize_Deduction.hpp:int cuda_get_opt_block_size_no_shmem(const cudaDeviceProp& prop,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_BlockSize_Deduction.hpp:                                     const cudaFuncAttributes& attr,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_BlockSize_Deduction.hpp:  return cuda_deduce_block_size(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_BlockSize_Deduction.hpp:#endif  // KOKKOS_ENABLE_CUDA
lib/kokkos/core/src/Cuda/Kokkos_Cuda_BlockSize_Deduction.hpp:#endif  /* #ifndef KOKKOS_CUDA_INTERNAL_HPP */
lib/kokkos/core/src/Cuda/Kokkos_Cuda_WorkGraphPolicy.hpp:#ifndef KOKKOS_CUDA_WORKGRAPHPOLICY_HPP
lib/kokkos/core/src/Cuda/Kokkos_Cuda_WorkGraphPolicy.hpp:#define KOKKOS_CUDA_WORKGRAPHPOLICY_HPP
lib/kokkos/core/src/Cuda/Kokkos_Cuda_WorkGraphPolicy.hpp:#include <Cuda/Kokkos_Cuda.hpp>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_WorkGraphPolicy.hpp:#include <Cuda/Kokkos_Cuda_KernelLaunch.hpp>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_WorkGraphPolicy.hpp:                  Kokkos::Cuda> {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_WorkGraphPolicy.hpp:  using Self   = ParallelFor<FunctorType, Policy, Kokkos::Cuda>;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_WorkGraphPolicy.hpp:        m_policy.space().cuda_device_prop().multiProcessorCount;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_WorkGraphPolicy.hpp:    const dim3 block(1, Kokkos::Impl::CudaTraits::WarpSize, warps_per_block);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_WorkGraphPolicy.hpp:    Kokkos::Impl::CudaParallelLaunch<Self>(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_WorkGraphPolicy.hpp:        *this, grid, block, shared, Cuda().impl_internal_space_instance());
lib/kokkos/core/src/Cuda/Kokkos_Cuda_WorkGraphPolicy.hpp:#endif /* #define KOKKOS_CUDA_WORKGRAPHPOLICY_HPP */
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ZeroMemset.hpp:#ifndef KOKKOS_CUDA_ZEROMEMSET_HPP
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ZeroMemset.hpp:#define KOKKOS_CUDA_ZEROMEMSET_HPP
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ZeroMemset.hpp:#include <Cuda/Kokkos_Cuda.hpp>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ZeroMemset.hpp:struct ZeroMemset<Kokkos::Cuda, View<T, P...>> {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ZeroMemset.hpp:  ZeroMemset(const Kokkos::Cuda& exec_space_instance,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ZeroMemset.hpp:    KOKKOS_IMPL_CUDA_SAFE_CALL(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ZeroMemset.hpp:             ->cuda_memset_async_wrapper(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ZeroMemset.hpp:#endif  // !defined(KOKKOS_CUDA_ZEROMEMSET_HPP)
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:#ifndef KOKKOS_CUDA_INSTANCE_HPP_
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:#define KOKKOS_CUDA_INSTANCE_HPP_
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:#include <Cuda/Kokkos_Cuda_Error.hpp>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:#include <cuda_runtime_api.h>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:#include "Kokkos_CudaSpace.hpp"
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:#ifdef KOKKOS_IMPL_DEBUG_CUDA_SERIAL_EXECUTION
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:extern "C" void kokkos_impl_cuda_set_serial_execution(bool);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:extern "C" bool kokkos_impl_cuda_use_serial_execution();
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:struct CudaTraits {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  static constexpr CudaSpace::size_type WarpSize = 32 /* 0x0020 */;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  static constexpr CudaSpace::size_type WarpIndexMask =
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  static constexpr CudaSpace::size_type WarpIndexShift =
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  static constexpr CudaSpace::size_type ConstantMemoryUsage =
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  static constexpr CudaSpace::size_type ConstantMemoryCache =
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  static constexpr CudaSpace::size_type KernelArgumentLimit =
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  static constexpr CudaSpace::size_type MaxHierarchicalParallelism =
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:CudaSpace::size_type* cuda_internal_scratch_flags(const Cuda&,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:CudaSpace::size_type* cuda_internal_scratch_space(const Cuda&,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:CudaSpace::size_type* cuda_internal_scratch_unified(const Cuda&,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:class CudaInternal {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  CudaInternal(const CudaInternal&);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  CudaInternal& operator=(const CudaInternal&);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:#ifdef KOKKOS_IMPL_DEBUG_CUDA_SERIAL_EXECUTION
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  static bool kokkos_impl_cuda_use_serial_execution_v;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  using size_type = Cuda::size_type;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  int m_cudaDev = -1;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  static int m_cudaArch;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  static cudaDeviceProp m_deviceProp;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  cudaStream_t m_stream;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  static std::set<int> cuda_devices;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  static std::map<int, cudaEvent_t> constantMemReusablePerDevice;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  static CudaInternal& singleton();
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  void initialize(cudaStream_t stream);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:#ifdef KOKKOS_IMPL_DEBUG_CUDA_SERIAL_EXECUTION
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  static bool cuda_use_serial_execution();
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  static void cuda_set_serial_execution(bool);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  ~CudaInternal();
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  CudaInternal()
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:            Kokkos::Tools::Experimental::Impl::idForInstance<Kokkos::Cuda>(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  // Using cudaAPI function/objects will be w.r.t. device 0 unless
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  // cudaSetDevice(device_id) is called with the correct device_id.
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  // CudaInternal::m_cudaDev set in Cuda::impl_initialize(). It is not
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  // sufficient to call cudaSetDevice(m_cudaDev) during cuda initialization
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  // given the default cuda env with device_id=0, causing errors when
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  // cudaAPI calls, as well as using cudaStream_t variables, must be proceeded
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  // by cudaSetDevice(device_id).
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  // This function sets device in cudaAPI to device requested at runtime (set in
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  // m_cudaDev).
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  void set_cuda_device() const {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    verify_is_initialized("set_cuda_device");
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    KOKKOS_IMPL_CUDA_SAFE_CALL(cudaSetDevice(m_cudaDev));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  template <bool setCudaDevice = true>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  cudaStream_t get_stream() const {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    if constexpr (setCudaDevice) set_cuda_device();
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  // The following are wrappers for cudaAPI functions (C and C++ routines) which
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  // set the correct device id directly before the cudaAPI call (unless
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  // explicitly disabled by providing setCudaDevice=false template).
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  // setCudaDevice=true should be used for all API calls which take a stream
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  // unless it is guarenteed to be from a cuda instance with the correct device
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  // set already (e.g., back-to-back cudaAPI calls in a single function). For
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  // cudaAPI functions that take a stream, an optional input stream is
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  // available. If no stream is given, the stream for the CudaInternal instance
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  // is used. All cudaAPI calls should be wrapped in these interface functions
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  cudaStream_t get_input_stream(cudaStream_t s) const {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  template <bool setCudaDevice = true>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  cudaError_t cuda_device_get_limit_wrapper(size_t* pValue,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:                                            cudaLimit limit) const {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    if constexpr (setCudaDevice) set_cuda_device();
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    return cudaDeviceGetLimit(pValue, limit);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  template <bool setCudaDevice = true>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  cudaError_t cuda_device_set_limit_wrapper(cudaLimit limit,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    if constexpr (setCudaDevice) set_cuda_device();
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    return cudaDeviceSetLimit(limit, value);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  template <bool setCudaDevice = true>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  cudaError_t cuda_event_create_wrapper(cudaEvent_t* event) const {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    if constexpr (setCudaDevice) set_cuda_device();
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    return cudaEventCreate(event);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  template <bool setCudaDevice = true>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  cudaError_t cuda_event_destroy_wrapper(cudaEvent_t event) const {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    if constexpr (setCudaDevice) set_cuda_device();
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    return cudaEventDestroy(event);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  template <bool setCudaDevice = true>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  cudaError_t cuda_event_record_wrapper(cudaEvent_t event,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:                                        cudaStream_t stream = nullptr) const {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    if constexpr (setCudaDevice) set_cuda_device();
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    return cudaEventRecord(event, get_input_stream(stream));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  template <bool setCudaDevice = true>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  cudaError_t cuda_event_synchronize_wrapper(cudaEvent_t event) const {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    if constexpr (setCudaDevice) set_cuda_device();
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    return cudaEventSynchronize(event);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  template <bool setCudaDevice = true>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  cudaError_t cuda_free_wrapper(void* devPtr) const {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    if constexpr (setCudaDevice) set_cuda_device();
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    return cudaFree(devPtr);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  template <bool setCudaDevice = true>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  cudaError_t cuda_free_host_wrapper(void* ptr) const {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    if constexpr (setCudaDevice) set_cuda_device();
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    return cudaFreeHost(ptr);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  template <bool setCudaDevice = true>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  cudaError_t cuda_graph_add_dependencies_wrapper(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:      cudaGraph_t graph, const cudaGraphNode_t* from, const cudaGraphNode_t* to,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    if constexpr (setCudaDevice) set_cuda_device();
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    return cudaGraphAddDependencies(graph, from, to, numDependencies);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  template <bool setCudaDevice = true>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  cudaError_t cuda_graph_add_empty_node_wrapper(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:      cudaGraphNode_t* pGraphNode, cudaGraph_t graph,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:      const cudaGraphNode_t* pDependencies, size_t numDependencies) const {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    if constexpr (setCudaDevice) set_cuda_device();
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    return cudaGraphAddEmptyNode(pGraphNode, graph, pDependencies,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  template <bool setCudaDevice = true>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  cudaError_t cuda_graph_add_kernel_node_wrapper(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:      cudaGraphNode_t* pGraphNode, cudaGraph_t graph,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:      const cudaGraphNode_t* pDependencies, size_t numDependencies,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:      const cudaKernelNodeParams* pNodeParams) const {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    if constexpr (setCudaDevice) set_cuda_device();
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    return cudaGraphAddKernelNode(pGraphNode, graph, pDependencies,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  template <bool setCudaDevice = true>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  cudaError_t cuda_graph_create_wrapper(cudaGraph_t* pGraph,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    if constexpr (setCudaDevice) set_cuda_device();
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    return cudaGraphCreate(pGraph, flags);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  template <bool setCudaDevice = true>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  cudaError_t cuda_graph_destroy_wrapper(cudaGraph_t graph) const {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    if constexpr (setCudaDevice) set_cuda_device();
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    return cudaGraphDestroy(graph);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  template <bool setCudaDevice = true>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  cudaError_t cuda_graph_exec_destroy_wrapper(cudaGraphExec_t graphExec) const {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    if constexpr (setCudaDevice) set_cuda_device();
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    return cudaGraphExecDestroy(graphExec);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  template <bool setCudaDevice = true>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  cudaError_t cuda_graph_launch_wrapper(cudaGraphExec_t graphExec,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:                                        cudaStream_t stream = nullptr) const {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    if constexpr (setCudaDevice) set_cuda_device();
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    return cudaGraphLaunch(graphExec, get_input_stream(stream));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  template <bool setCudaDevice = true>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  cudaError_t cuda_host_alloc_wrapper(void** pHost, size_t size,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    if constexpr (setCudaDevice) set_cuda_device();
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    return cudaHostAlloc(pHost, size, flags);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  template <bool setCudaDevice = true>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  cudaError_t cuda_malloc_wrapper(void** devPtr, size_t size) const {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    if constexpr (setCudaDevice) set_cuda_device();
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    return cudaMalloc(devPtr, size);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  template <bool setCudaDevice = true>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  cudaError_t cuda_malloc_host_wrapper(void** ptr, size_t size) const {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    if constexpr (setCudaDevice) set_cuda_device();
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    return cudaMallocHost(ptr, size);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  template <bool setCudaDevice = true>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  cudaError_t cuda_malloc_managed_wrapper(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:      unsigned int flags = cudaMemAttachGlobal) const {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    if constexpr (setCudaDevice) set_cuda_device();
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    return cudaMallocManaged(devPtr, size, flags);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  template <bool setCudaDevice = true>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  cudaError_t cuda_mem_advise_wrapper(const void* devPtr, size_t count,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:                                      cudaMemoryAdvise advice,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    if constexpr (setCudaDevice) set_cuda_device();
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    return cudaMemAdvise(devPtr, count, advice, device);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  template <bool setCudaDevice = true>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  cudaError_t cuda_mem_prefetch_async_wrapper(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:      cudaStream_t stream = nullptr) const {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    if constexpr (setCudaDevice) set_cuda_device();
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    return cudaMemPrefetchAsync(devPtr, count, dstDevice,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  template <bool setCudaDevice = true>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  cudaError_t cuda_memcpy_wrapper(void* dst, const void* src, size_t count,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:                                  cudaMemcpyKind kind) const {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    if constexpr (setCudaDevice) set_cuda_device();
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    return cudaMemcpy(dst, src, count, kind);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  template <bool setCudaDevice = true>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  cudaError_t cuda_memcpy_async_wrapper(void* dst, const void* src,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:                                        size_t count, cudaMemcpyKind kind,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:                                        cudaStream_t stream = nullptr) const {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    if constexpr (setCudaDevice) set_cuda_device();
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    return cudaMemcpyAsync(dst, src, count, kind, get_input_stream(stream));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  template <bool setCudaDevice = true>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  cudaError_t cuda_memcpy_to_symbol_async_wrapper(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:      cudaMemcpyKind kind, cudaStream_t stream = nullptr) const {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    if constexpr (setCudaDevice) set_cuda_device();
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    return cudaMemcpyToSymbolAsync(symbol, src, count, offset, kind,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  template <bool setCudaDevice = true>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  cudaError_t cuda_memset_wrapper(void* devPtr, int value, size_t count) const {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    if constexpr (setCudaDevice) set_cuda_device();
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    return cudaMemset(devPtr, value, count);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  template <bool setCudaDevice = true>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  cudaError_t cuda_memset_async_wrapper(void* devPtr, int value, size_t count,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:                                        cudaStream_t stream = nullptr) const {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    if constexpr (setCudaDevice) set_cuda_device();
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    return cudaMemsetAsync(devPtr, value, count, get_input_stream(stream));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  template <bool setCudaDevice = true>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  cudaError_t cuda_pointer_get_attributes_wrapper(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:      cudaPointerAttributes* attributes, const void* ptr) const {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    if constexpr (setCudaDevice) set_cuda_device();
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    return cudaPointerGetAttributes(attributes, ptr);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  template <bool setCudaDevice = true>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  cudaError_t cuda_stream_create_wrapper(cudaStream_t* pStream) const {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    if constexpr (setCudaDevice) set_cuda_device();
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    return cudaStreamCreate(pStream);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  template <bool setCudaDevice = true>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  cudaError_t cuda_stream_destroy_wrapper(cudaStream_t stream) const {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    if constexpr (setCudaDevice) set_cuda_device();
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    return cudaStreamDestroy(stream);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  template <bool setCudaDevice = true>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  cudaError_t cuda_stream_synchronize_wrapper(cudaStream_t stream) const {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    if constexpr (setCudaDevice) set_cuda_device();
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    return cudaStreamSynchronize(stream);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  template <typename T, bool setCudaDevice = true>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  cudaError_t cuda_func_get_attributes_wrapper(cudaFuncAttributes* attr,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    if constexpr (setCudaDevice) set_cuda_device();
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    return cudaFuncGetAttributes(attr, entry);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  template <typename T, bool setCudaDevice = true>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  cudaError_t cuda_func_set_attribute_wrapper(T* entry, cudaFuncAttribute attr,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    if constexpr (setCudaDevice) set_cuda_device();
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    return cudaFuncSetAttribute(entry, attr, value);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  template <bool setCudaDevice = true>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  cudaError_t cuda_graph_instantiate_wrapper(cudaGraphExec_t* pGraphExec,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:                                             cudaGraph_t graph,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:                                             cudaGraphNode_t* pErrorNode,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    if constexpr (setCudaDevice) set_cuda_device();
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:    return cudaGraphInstantiate(pGraphExec, graph, pErrorNode, pLogBuffer,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:void create_Cuda_instances(std::vector<Cuda>& instances);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:std::vector<Cuda> partition_space(const Cuda&, Args...) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  std::vector<Cuda> instances(sizeof...(Args));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  Kokkos::Impl::create_Cuda_instances(instances);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:std::vector<Cuda> partition_space(const Cuda&, std::vector<T> const& weights) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  std::vector<Cuda> instances(weights.size());
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.hpp:  Kokkos::Impl::create_Cuda_instances(instances);
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:#ifdef KOKKOS_ENABLE_CUDA
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:#include <Cuda/Kokkos_Cuda.hpp>
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:#include <Cuda/Kokkos_CudaSpace.hpp>
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:cudaStream_t Kokkos::Impl::cuda_get_deep_copy_stream() {
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:  static cudaStream_t s = nullptr;
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:    KOKKOS_IMPL_CUDA_SAFE_CALL(
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:        (CudaInternal::singleton().cuda_stream_create_wrapper(&s)));
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:const std::unique_ptr<Kokkos::Cuda> &Kokkos::Impl::cuda_get_deep_copy_space(
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:  static std::unique_ptr<Cuda> space = nullptr;
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:    space = std::make_unique<Cuda>(Kokkos::Impl::cuda_get_deep_copy_stream());
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:void DeepCopyCuda(void *dst, const void *src, size_t n) {
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:  KOKKOS_IMPL_CUDA_SAFE_CALL((CudaInternal::singleton().cuda_memcpy_wrapper(
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:      dst, src, n, cudaMemcpyDefault)));
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:void DeepCopyAsyncCuda(const Cuda &instance, void *dst, const void *src,
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:  KOKKOS_IMPL_CUDA_SAFE_CALL(
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:      (instance.impl_internal_space_instance()->cuda_memcpy_async_wrapper(
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:          dst, src, n, cudaMemcpyDefault)));
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:void DeepCopyAsyncCuda(void *dst, const void *src, size_t n) {
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:  cudaStream_t s = cuda_get_deep_copy_stream();
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:  KOKKOS_IMPL_CUDA_SAFE_CALL(
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:      (CudaInternal::singleton().cuda_memcpy_async_wrapper(
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:          dst, src, n, cudaMemcpyDefault, s)));
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:  Kokkos::Tools::Experimental::Impl::profile_fence_event<Kokkos::Cuda>(
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:      "Kokkos::Impl::DeepCopyAsyncCuda: Deep Copy Stream Sync",
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:      [&]() { KOKKOS_IMPL_CUDA_SAFE_CALL(cudaStreamSynchronize(s)); });
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:bool CudaUVMSpace::available() { return true; }
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:#ifdef KOKKOS_IMPL_DEBUG_CUDA_PIN_UVM_TO_HOST
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:// some CUDA issues.
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:bool CudaUVMSpace::kokkos_impl_cuda_pin_uvm_to_host_v = false;
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:bool CudaUVMSpace::cuda_pin_uvm_to_host() {
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:  return CudaUVMSpace::kokkos_impl_cuda_pin_uvm_to_host_v;
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:void CudaUVMSpace::cuda_set_pin_uvm_to_host(bool val) {
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:  CudaUVMSpace::kokkos_impl_cuda_pin_uvm_to_host_v = val;
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:#ifdef KOKKOS_IMPL_DEBUG_CUDA_PIN_UVM_TO_HOST
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:bool kokkos_impl_cuda_pin_uvm_to_host() {
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:  return Kokkos::CudaUVMSpace::cuda_pin_uvm_to_host();
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:void kokkos_impl_cuda_set_pin_uvm_to_host(bool val) {
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:  Kokkos::CudaUVMSpace::cuda_set_pin_uvm_to_host(val);
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:CudaSpace::CudaSpace()
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:    : m_device(Kokkos::Cuda().cuda_device()),
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:      m_stream(Kokkos::Cuda().cuda_stream()) {}
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:CudaSpace::CudaSpace(int device_id, cudaStream_t stream)
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:CudaUVMSpace::CudaUVMSpace()
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:    : m_device(Kokkos::Cuda().cuda_device()),
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:      m_stream(Kokkos::Cuda().cuda_stream()) {}
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:CudaUVMSpace::CudaUVMSpace(int device_id, cudaStream_t stream)
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:CudaHostPinnedSpace::CudaHostPinnedSpace()
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:    : m_device(Kokkos::Cuda().cuda_device()),
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:      m_stream(Kokkos::Cuda().cuda_stream()) {}
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:CudaHostPinnedSpace::CudaHostPinnedSpace(int device_id, cudaStream_t stream)
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:void *CudaSpace::allocate(const size_t arg_alloc_size) const {
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:void *CudaSpace::allocate(const Cuda &exec_space, const char *arg_label,
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:void *CudaSpace::allocate(const char *arg_label, const size_t arg_alloc_size,
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:                           [[maybe_unused]] const cudaStream_t stream,
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:  KOKKOS_IMPL_CUDA_SAFE_CALL(cudaSetDevice(device_id));
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:  cudaError_t error_code = cudaSuccess;
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:#ifndef CUDART_VERSION
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:#error CUDART_VERSION undefined!
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:#elif defined(KOKKOS_ENABLE_IMPL_CUDA_UNIFIED_MEMORY)
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:  // device, but that requires CUDA 12.2
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:  static_assert(CUDART_VERSION >= 12020,
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:                "CUDA runtime version >=12.2 required when "
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:                "Kokkos_ENABLE_IMPL_CUDA_UNIFIED_MEMORY is set. "
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:                "Please update your CUDA runtime version or "
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:                "-D Kokkos_ENABLE_IMPL_CUDA_UNIFIED_MEMORY=OFF");
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:  if (arg_alloc_size) {  // cudaMemAdvise_v2 does not work with nullptr
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:    error_code = cudaMallocManaged(&ptr, arg_alloc_size, cudaMemAttachGlobal);
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:    if (error_code == cudaSuccess) {
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:      // One would think cudaMemLocation{device_id,
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:      // cudaMemLocationTypeDevice} would work but it doesn't. I.e. the order of
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:      cudaMemLocation loc;
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:      loc.type = cudaMemLocationTypeDevice;
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:      KOKKOS_IMPL_CUDA_SAFE_CALL(cudaMemAdvise_v2(
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:          ptr, arg_alloc_size, cudaMemAdviseSetPreferredLocation, loc));
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:#elif (defined(KOKKOS_ENABLE_IMPL_CUDA_MALLOC_ASYNC) && CUDART_VERSION >= 11020)
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:    error_code = cudaMallocAsync(&ptr, arg_alloc_size, stream);
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:    if (error_code == cudaSuccess) {
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:        KOKKOS_IMPL_CUDA_SAFE_CALL(cudaStreamSynchronize(stream));
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:        Impl::cuda_device_synchronize(
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:            "Kokkos::Cuda: backend fence after async malloc");
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:    error_code = cudaMalloc(&ptr, arg_alloc_size);
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:  error_code = cudaMalloc(&ptr, arg_alloc_size);
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:  if (error_code != cudaSuccess) {  // TODO tag as unlikely branch
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:    cudaGetLastError();
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:void *CudaSpace::impl_allocate(
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:void *CudaSpace::impl_allocate(
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:    const Cuda &exec_space, const char *arg_label, const size_t arg_alloc_size,
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:      exec_space.cuda_device(), exec_space.cuda_stream(), arg_label,
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:void *CudaUVMSpace::allocate(const size_t arg_alloc_size) const {
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:void *CudaUVMSpace::allocate(const char *arg_label, const size_t arg_alloc_size,
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:void *CudaUVMSpace::impl_allocate(
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:  Cuda::impl_static_fence(
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:      "Kokkos::CudaUVMSpace::impl_allocate: Pre UVM Allocation");
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:    KOKKOS_IMPL_CUDA_SAFE_CALL(cudaSetDevice(m_device));
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:    cudaError_t error_code =
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:        cudaMallocManaged(&ptr, arg_alloc_size, cudaMemAttachGlobal);
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:    if (error_code != cudaSuccess) {  // TODO tag as unlikely branch
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:      cudaGetLastError();
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:#ifdef KOKKOS_IMPL_DEBUG_CUDA_PIN_UVM_TO_HOST
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:    if (Kokkos::CudaUVMSpace::cuda_pin_uvm_to_host())
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:      KOKKOS_IMPL_CUDA_SAFE_CALL(
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:          cudaMemAdvise(ptr, arg_alloc_size, cudaMemAdviseSetPreferredLocation,
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:                        cudaCpuDeviceId));
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:  Cuda::impl_static_fence(
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:      "Kokkos::CudaUVMSpace::impl_allocate: Post UVM Allocation");
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:void *CudaHostPinnedSpace::allocate(const size_t arg_alloc_size) const {
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:void *CudaHostPinnedSpace::allocate(const char *arg_label,
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:void *CudaHostPinnedSpace::impl_allocate(
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:  KOKKOS_IMPL_CUDA_SAFE_CALL(cudaSetDevice(m_device));
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:  cudaError_t error_code =
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:      cudaHostAlloc(&ptr, arg_alloc_size, cudaHostAllocDefault);
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:  if (error_code != cudaSuccess) {  // TODO tag as unlikely branch
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:    cudaGetLastError();
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:void CudaSpace::deallocate(void *const arg_alloc_ptr,
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:void CudaSpace::deallocate(const char *arg_label, void *const arg_alloc_ptr,
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:void CudaSpace::impl_deallocate(
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:#ifndef CUDART_VERSION
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:#error CUDART_VERSION undefined!
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:#elif defined(KOKKOS_ENABLE_IMPL_CUDA_UNIFIED_MEMORY)
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:  KOKKOS_IMPL_CUDA_SAFE_CALL(cudaSetDevice(m_device));
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:  KOKKOS_IMPL_CUDA_SAFE_CALL(cudaFree(arg_alloc_ptr));
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:#elif (defined(KOKKOS_ENABLE_IMPL_CUDA_MALLOC_ASYNC) && CUDART_VERSION >= 11020)
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:    Impl::cuda_device_synchronize(
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:        "Kokkos::Cuda: backend fence before async free");
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:    KOKKOS_IMPL_CUDA_SAFE_CALL(cudaSetDevice(m_device));
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:    KOKKOS_IMPL_CUDA_SAFE_CALL(cudaFreeAsync(arg_alloc_ptr, m_stream));
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:    Impl::cuda_device_synchronize(
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:        "Kokkos::Cuda: backend fence after async free");
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:    KOKKOS_IMPL_CUDA_SAFE_CALL(cudaSetDevice(m_device));
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:    KOKKOS_IMPL_CUDA_SAFE_CALL(cudaFree(arg_alloc_ptr));
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:  KOKKOS_IMPL_CUDA_SAFE_CALL(cudaSetDevice(m_device));
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:  KOKKOS_IMPL_CUDA_SAFE_CALL(cudaFree(arg_alloc_ptr));
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:void CudaUVMSpace::deallocate(void *const arg_alloc_ptr,
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:void CudaUVMSpace::deallocate(const char *arg_label, void *const arg_alloc_ptr,
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:void CudaUVMSpace::impl_deallocate(
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:  Cuda::impl_static_fence(
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:      "Kokkos::CudaUVMSpace::impl_deallocate: Pre UVM Deallocation");
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:    KOKKOS_IMPL_CUDA_SAFE_CALL(cudaSetDevice(m_device));
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:    KOKKOS_IMPL_CUDA_SAFE_CALL(cudaFree(arg_alloc_ptr));
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:  Cuda::impl_static_fence(
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:      "Kokkos::CudaUVMSpace::impl_deallocate: Post UVM Deallocation");
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:void CudaHostPinnedSpace::deallocate(void *const arg_alloc_ptr,
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:void CudaHostPinnedSpace::deallocate(const char *arg_label,
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:void CudaHostPinnedSpace::impl_deallocate(
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:  KOKKOS_IMPL_CUDA_SAFE_CALL(cudaSetDevice(m_device));
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:  KOKKOS_IMPL_CUDA_SAFE_CALL(cudaFreeHost(arg_alloc_ptr));
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:void cuda_prefetch_pointer(const Cuda &space, const void *ptr, size_t bytes,
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:  cudaPointerAttributes attr;
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:  KOKKOS_IMPL_CUDA_SAFE_CALL((
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:      space.impl_internal_space_instance()->cuda_pointer_get_attributes_wrapper(
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:  // cudaCpuDeviceId as the device if to_device is false
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:  bool is_managed = attr.type == cudaMemoryTypeManaged;
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:      space.cuda_device_prop().concurrentManagedAccess) {
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:    KOKKOS_IMPL_CUDA_SAFE_CALL(
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:        (space.impl_internal_space_instance()->cuda_mem_prefetch_async_wrapper(
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:            ptr, bytes, space.cuda_device())));
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:#if !defined(KOKKOS_ENABLE_IMPL_CUDA_UNIFIED_MEMORY)
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:    Kokkos::CudaSpace);
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:KOKKOS_IMPL_SHARED_ALLOCATION_RECORD_EXPLICIT_INSTANTIATION(Kokkos::CudaSpace);
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:    Kokkos::CudaUVMSpace);
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:    Kokkos::CudaHostPinnedSpace);
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:void KOKKOS_CORE_SRC_CUDA_CUDASPACE_PREVENT_LINK_ERROR() {}
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.cpp:#endif  // KOKKOS_ENABLE_CUDA
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_MDRange.hpp:#ifndef KOKKOS_CUDA_PARALLEL_MD_RANGE_HPP
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_MDRange.hpp:#define KOKKOS_CUDA_PARALLEL_MD_RANGE_HPP
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_MDRange.hpp:#if defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_MDRange.hpp:#include <Cuda/Kokkos_Cuda_KernelLaunch.hpp>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_MDRange.hpp:#include <Cuda/Kokkos_Cuda_ReduceScan.hpp>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_MDRange.hpp:#include <Cuda/Kokkos_Cuda_BlockSize_Deduction.hpp>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_MDRange.hpp:#include <impl/KokkosExp_IterateTileGPU.hpp>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_MDRange.hpp:  cudaFuncAttributes attr =
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_MDRange.hpp:      CudaParallelLaunch<ParallelType, LaunchBounds>::get_cuda_func_attributes(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_MDRange.hpp:          pol.space().cuda_device());
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_MDRange.hpp:  auto const& prop = pol.space().cuda_device_prop();
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_MDRange.hpp:      cuda_get_opt_block_size_no_shmem(prop, attr, LaunchBounds{});
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_MDRange.hpp:      Kokkos::Impl::cuda_max_warps_per_sm_registers(prop, attr);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_MDRange.hpp:      static_cast<int>(Kokkos::Impl::CudaTraits::MaxHierarchicalParallelism));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_MDRange.hpp:class ParallelFor<FunctorType, Kokkos::MDRangePolicy<Traits...>, Kokkos::Cuda> {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_MDRange.hpp:    const auto maxblocks = m_rp.space().cuda_device_prop().maxGridSize;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_MDRange.hpp:      CudaParallelLaunch<ParallelFor, LaunchBounds>(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_MDRange.hpp:      CudaParallelLaunch<ParallelFor, LaunchBounds>(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_MDRange.hpp:      CudaParallelLaunch<ParallelFor, LaunchBounds>(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_MDRange.hpp:      CudaParallelLaunch<ParallelFor, LaunchBounds>(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_MDRange.hpp:      CudaParallelLaunch<ParallelFor, LaunchBounds>(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_MDRange.hpp:      Kokkos::abort("Kokkos::MDRange Error: Exceeded rank bounds with Cuda\n");
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_MDRange.hpp:                     Kokkos::MDRangePolicy<Traits...>, Kokkos::Cuda> {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_MDRange.hpp:  using size_type      = Cuda::size_type;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_MDRange.hpp:  // smaller than int32_t (Kokkos::Cuda::size_type)
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_MDRange.hpp:              kokkos_impl_cuda_shared_memory<word_size_type>() +
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_MDRange.hpp:    if (cuda_single_inter_block_reduce_scan<false>(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_MDRange.hpp:            kokkos_impl_cuda_shared_memory<word_size_type>(), m_scratch_space,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_MDRange.hpp:          kokkos_impl_cuda_shared_memory<word_size_type>() +
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_MDRange.hpp:      if (CudaTraits::WarpSize < word_count.value) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_MDRange.hpp:    unsigned n = CudaTraits::WarpSize * 8;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_MDRange.hpp:        m_policy.space().cuda_device_prop().sharedMemPerBlock;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_MDRange.hpp:        cuda_single_inter_block_reduce_scan_shmem<false, WorkTag, value_type>(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_MDRange.hpp:                             Policy, Kokkos::Cuda>;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_MDRange.hpp:    cudaFuncAttributes attr = CudaParallelLaunch<closure_type, LaunchBounds>::
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_MDRange.hpp:        get_cuda_func_attributes(m_policy.space().cuda_device());
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_MDRange.hpp:             Kokkos::Impl::cuda_get_max_block_size<FunctorType, LaunchBounds>(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_MDRange.hpp:          cuda_single_inter_block_reduce_scan_shmem<false, WorkTag, value_type>(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_MDRange.hpp:          reinterpret_cast<word_size_type*>(cuda_internal_scratch_space(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_MDRange.hpp:          cuda_internal_scratch_flags(m_policy.space(), sizeof(size_type));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_MDRange.hpp:          reinterpret_cast<word_size_type*>(cuda_internal_scratch_unified(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_MDRange.hpp:              : cuda_single_inter_block_reduce_scan_shmem<false, WorkTag,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_MDRange.hpp:      CudaParallelLaunch<ParallelReduce, LaunchBounds>(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_MDRange.hpp:                "Kokkos::Impl::ParallelReduce<Cuda, MDRangePolicy>::execute: "
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_MDRange.hpp:            DeepCopy<HostSpace, CudaSpace, Cuda>(m_policy.space(), m_result_ptr,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_MDRange.hpp:            MemorySpaceAccess<Kokkos::CudaSpace,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Half_Conversion.hpp:#ifndef KOKKOS_CUDA_HALF_HPP_
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Half_Conversion.hpp:#define KOKKOS_CUDA_HALF_HPP_
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Half_Conversion.hpp:#ifdef KOKKOS_IMPL_CUDA_HALF_TYPE_DEFINED
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Half_Conversion.hpp:#if CUDA_VERSION >= 11000
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Half_Conversion.hpp:#include <cuda_bf16.h>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Half_Conversion.hpp:// CUDA before 11.1 only has the half <-> float conversions marked host device
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Half_Conversion.hpp:#if (CUDA_VERSION < 11010)
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Half_Conversion.hpp:  // double2half was only introduced in CUDA 11 too
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Half_Conversion.hpp:#else  // CUDA 11.1 versions follow
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Half_Conversion.hpp:// Go in this branch if CUDA version is >= 11.0.0 and less than 11.1.0 or if the
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Half_Conversion.hpp:#define KOKKOS_IMPL_NVIDIA_GPU_ARCH_SUPPORT_BHALF
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Half_Conversion.hpp:#if CUDA_VERSION >= 11000 && \
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Half_Conversion.hpp:    (CUDA_VERSION < 11010 || \
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Half_Conversion.hpp:     !defined(KOKKOS_IMPL_NVIDIA_GPU_ARCH_SUPPORT_BHALF))
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Half_Conversion.hpp:  // double2bfloat16 was only introduced in CUDA 11 too
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Half_Conversion.hpp:#endif  // CUDA_VERSION >= 11000 && CUDA_VERSION < 11010
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Half_Conversion.hpp:#if CUDA_VERSION >= 11010 && defined(KOKKOS_IMPL_NVIDIA_GPU_ARCH_SUPPORT_BHALF)
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Half_Conversion.hpp:#endif  // CUDA_VERSION >= 11010
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Half_Conversion.hpp:#undef KOKKOS_IMPL_NVIDIA_GPU_ARCH_SUPPORT_BHALF
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Half_Conversion.hpp:#if (CUDA_VERSION >= 11000)
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Half_Conversion.hpp:#endif  // CUDA_VERSION >= 11000
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Half_Conversion.hpp:// use float as the return type for sum and prod since cuda_fp16.h
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Half_Conversion.hpp:#endif  // KOKKOS_IMPL_CUDA_HALF_TYPE_DEFINED
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:#ifndef KOKKOS_CUDA_REDUCESCAN_HPP
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:#define KOKKOS_CUDA_REDUCESCAN_HPP
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:#if defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:#include <Cuda/Kokkos_Cuda_Vectorization.hpp>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:__device__ inline void cuda_intra_warp_reduction(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:__device__ inline void cuda_inter_warp_reduction(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:__device__ inline void cuda_intra_block_reduction(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:  cuda_intra_warp_reduction(value, reducer, max_active_thread);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:  cuda_inter_warp_reduction(value, reducer, max_active_thread);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:__device__ bool cuda_inter_block_reduction(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:    Cuda::size_type* const m_scratch_flags,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:  cuda_intra_block_reduction(value, reducer, max_active_thread);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:    Cuda::size_type count;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:struct CudaReductionsFunctor;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:struct CudaReductionsFunctor<FunctorType, false, true> {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:      const FunctorType& functor, const Cuda::size_type /*block_id*/,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:      const Cuda::size_type block_count, Cuda::size_type* const shared_data,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:      Cuda::size_type* const global_data, Cuda::size_type* const global_flags) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:struct CudaReductionsFunctor<FunctorType, false, false> {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:  template <class SizeType = Cuda::size_type>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:      const FunctorType& functor, const Cuda::size_type /*block_id*/,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:      const Cuda::size_type block_count, SizeType* const shared_data,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:      SizeType* const global_data, Cuda::size_type* const global_flags) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:// See section B.17 of Cuda C Programming Guide Version 3.2
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:__device__ void cuda_intra_block_reduce_scan(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:  const bool is_full_warp = (((threadIdx.y >> CudaTraits::WarpIndexShift) + 1)
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:                             << CudaTraits::WarpIndexShift) <= blockDim.y;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:                                        (CudaTraits::WarpSize - 1));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:      base_data + value_count * ((threadIdx.y >> CudaTraits::WarpIndexShift)
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:                                 << CudaTraits::WarpIndexShift);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:    // (CudaTraits::WarpSize - 1) away from the warp start adress. For the
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:        ((blockDim.y - 1) >> CudaTraits::WarpIndexShift) + 1;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:          threadIdx.y < (blockDim.y >> CudaTraits::WarpIndexShift);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:                             ? (threadIdx.y << CudaTraits::WarpIndexShift) +
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:                                   (CudaTraits::WarpSize - 1)
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:              : blockDim.y - (threadIdx.y << CudaTraits::WarpIndexShift);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:      const int rtid_inter = (threadIdx.y << CudaTraits::WarpIndexShift) +
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:                             (CudaTraits::WarpSize - 1) - index_shift;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:    if (mapped_idx >= CudaTraits::WarpSize &&
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:        (mapped_idx & (CudaTraits::WarpSize - 1)) != (CudaTraits::WarpSize - 1))
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:template <bool DoScan, class FunctorType, class SizeType = Cuda::size_type>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:__device__ bool cuda_single_inter_block_reduce_scan2(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:    const FunctorType& functor, const Cuda::size_type block_id,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:    const Cuda::size_type block_count, SizeType* const shared_data,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:    SizeType* const global_data, Cuda::size_type* const global_flags) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:        "Cuda::cuda_single_inter_block_reduce_scan requires power-of-two "
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:  cuda_intra_block_reduce_scan<false>(functor, pointer_type(shared_data));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:    cuda_intra_block_reduce_scan<DoScan>(functor, pointer_type(shared_data));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:template <bool DoScan, class FunctorType, class SizeType = Cuda::size_type>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:__device__ bool cuda_single_inter_block_reduce_scan(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:    const FunctorType& functor, const Cuda::size_type block_id,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:    const Cuda::size_type block_count, SizeType* const shared_data,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:    SizeType* const global_data, Cuda::size_type* const global_flags) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:    return Kokkos::Impl::CudaReductionsFunctor<
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:    return cuda_single_inter_block_reduce_scan2<DoScan>(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:cuda_single_inter_block_reduce_scan_shmem(const FunctorType& functor,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:                            RangePolicy<Cuda, ArgTag>, FunctorType, ValueType>;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:cuda_single_inter_block_reduce_scan_shmem(const FunctorType& functor,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:                            RangePolicy<Cuda, ArgTag>, FunctorType, ValueType>;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:  size_t minBlockSize = CudaTraits::WarpSize * 1;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:      cuda_single_inter_block_reduce_scan_shmem<false, WorkTag, ValueType>(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:  size_t maxShmemPerBlock = policy.space().cuda_device_prop().sharedMemPerBlock;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:        "Kokkos::Impl::ParallelReduce< Cuda > requested too much L0 scratch "
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:#endif /* #if defined(KOKKOS_ENABLE_CUDA) */
lib/kokkos/core/src/Cuda/Kokkos_Cuda_ReduceScan.hpp:#endif /* KOKKOS_CUDA_REDUCESCAN_HPP */
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:#ifndef KOKKOS_CUDAEXEC_HPP
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:#define KOKKOS_CUDAEXEC_HPP
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:#ifdef KOKKOS_ENABLE_CUDA
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:#include <Cuda/Kokkos_Cuda_abort.hpp>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:#include <Cuda/Kokkos_Cuda_Error.hpp>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:#include <Cuda/Kokkos_Cuda_Instance.hpp>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:#include <Cuda/Kokkos_Cuda_GraphNodeKernel.hpp>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:#include <Cuda/Kokkos_Cuda_BlockSize_Deduction.hpp>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:#ifdef KOKKOS_ENABLE_CUDA_RELOCATABLE_DEVICE_CODE
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:    kokkos_impl_cuda_constant_memory_buffer[];
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:__device__ __constant__ unsigned long kokkos_impl_cuda_constant_memory_buffer
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:    [Kokkos::Impl::CudaTraits::ConstantMemoryUsage / sizeof(unsigned long)];
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:inline __device__ T* kokkos_impl_cuda_shared_memory() {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:  extern __shared__ Kokkos::CudaSpace::size_type sh[];
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:// See section B.17 of Cuda C Programming Guide Version 3.2
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:__global__ static void cuda_parallel_launch_constant_memory() {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:      *((const DriverType*)kokkos_impl_cuda_constant_memory_buffer);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:    maxTperB, minBperSM) static void cuda_parallel_launch_constant_memory() {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:      *((const DriverType*)kokkos_impl_cuda_constant_memory_buffer);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:__global__ static void cuda_parallel_launch_local_memory(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:    minBperSM) static void cuda_parallel_launch_local_memory(const DriverType
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:__global__ static void cuda_parallel_launch_global_memory(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:    minBperSM) static void cuda_parallel_launch_global_memory(const DriverType*
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:inline void check_shmem_request(CudaInternal const* cuda_instance, int shmem) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:  int const maxShmemPerBlock = cuda_instance->m_deviceProp.sharedMemPerBlock;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:        "CudaParallelLaunch (or graph node creation) FAILED: shared memory "
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:const cudaFuncAttributes& get_cuda_kernel_func_attributes(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:    int cuda_device, const KernelFuncPtr& func) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:  // Only call cudaFuncGetAttributes once for each unique kernel
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:  static std::map<int, cudaFuncAttributes> func_attr;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:  if (func_attr.find(cuda_device) == func_attr.end()) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:    cudaFuncAttributes attr;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:    KOKKOS_IMPL_CUDA_SAFE_CALL(cudaSetDevice(cuda_device));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:    KOKKOS_IMPL_CUDA_SAFE_CALL(cudaFuncGetAttributes(&attr, func));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:    func_attr.emplace(cuda_device, attr);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:  return func_attr[cuda_device];
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:inline void configure_shmem_preference(const int cuda_device,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:                                       const cudaDeviceProp& device_props,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:      get_cuda_kernel_func_attributes<DriverType, LaunchBounds>(cuda_device,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:      cuda_max_warps_per_sm_registers(device_props, func_attr);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:  // granularity defined in `cuda_warp_per_sm_allocation_granularity`.
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:  // FIXME_CUDA_MULTIPLE_DEVICES
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:    KOKKOS_IMPL_CUDA_SAFE_CALL(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:        (CudaInternal::singleton().cuda_func_set_attribute_wrapper(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:            func, cudaFuncAttributePreferredSharedMemoryCarveout, carveout)));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:// <editor-fold desc="DeduceCudaLaunchMechanism"> {{{2
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:struct DeduceCudaLaunchMechanism {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:  static constexpr const Experimental::CudaLaunchMechanism
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:      (sizeof(DriverType) < CudaTraits::KernelArgumentLimit
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:           ? Experimental::CudaLaunchMechanism::LocalMemory
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:           : Experimental::CudaLaunchMechanism::Default) |
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:      (sizeof(DriverType) < CudaTraits::ConstantMemoryUsage
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:           ? Experimental::CudaLaunchMechanism::ConstantMemory
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:           : Experimental::CudaLaunchMechanism::Default) |
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:      Experimental::CudaLaunchMechanism::GlobalMemory;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:  static constexpr const Experimental::CudaLaunchMechanism
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:               ? Experimental::CudaLaunchMechanism::LocalMemory
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:               : Experimental::CudaLaunchMechanism::ConstantMemory) |
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:          Experimental::CudaLaunchMechanism::GlobalMemory;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:  static constexpr const Experimental::CudaLaunchMechanism
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:      (sizeof(DriverType) < CudaTraits::ConstantMemoryUseThreshold)
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:          ? Experimental::CudaLaunchMechanism::LocalMemory
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:          : ((sizeof(DriverType) < CudaTraits::ConstantMemoryUsage)
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:                 ? Experimental::CudaLaunchMechanism::ConstantMemory
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:                 : Experimental::CudaLaunchMechanism::GlobalMemory);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:  static constexpr const Experimental::CudaLaunchMechanism launch_mechanism =
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:          ? (sizeof(DriverType) < CudaTraits::KernelArgumentLimit
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:                 ? Experimental::CudaLaunchMechanism::LocalMemory
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:                 : Experimental::CudaLaunchMechanism::GlobalMemory)
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:                 ? (sizeof(DriverType) < CudaTraits::ConstantMemoryUsage
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:                        ? Experimental::CudaLaunchMechanism::ConstantMemory
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:                        : Experimental::CudaLaunchMechanism::GlobalMemory)
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:// </editor-fold> end DeduceCudaLaunchMechanism }}}2
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:// <editor-fold desc="CudaParallelLaunchKernelInvoker"> {{{1
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:          Experimental::CudaLaunchMechanism LaunchMechanism>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:struct CudaParallelLaunchKernelFunc;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:          Experimental::CudaLaunchMechanism LaunchMechanism>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:struct CudaParallelLaunchKernelInvoker;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:struct CudaParallelLaunchKernelFunc<
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:    Experimental::CudaLaunchMechanism::LocalMemory> {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:  static std::decay_t<decltype(cuda_parallel_launch_local_memory<
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:    return cuda_parallel_launch_local_memory<DriverType, MaxThreadsPerBlock,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:struct CudaParallelLaunchKernelFunc<
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:    Experimental::CudaLaunchMechanism::LocalMemory> {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:  static std::decay_t<decltype(cuda_parallel_launch_local_memory<DriverType>)>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:    return cuda_parallel_launch_local_memory<DriverType>;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:struct CudaParallelLaunchKernelInvoker<
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:    DriverType, LaunchBounds, Experimental::CudaLaunchMechanism::LocalMemory>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:    : CudaParallelLaunchKernelFunc<
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:          Experimental::CudaLaunchMechanism::LocalMemory> {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:  using base_t = CudaParallelLaunchKernelFunc<
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:      DriverType, LaunchBounds, Experimental::CudaLaunchMechanism::LocalMemory>;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:  static_assert(sizeof(DriverType) < CudaTraits::KernelArgumentLimit,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:                "Kokkos Error: Requested CudaLaunchLocalMemory with a Functor "
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:                            CudaInternal const* cuda_instance) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:                                  cuda_instance->get_stream()>>>(driver);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:      CudaInternal const* cuda_instance) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:    auto const& graph = Impl::get_cuda_graph_from_kernel(driver);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:    auto& graph_node = Impl::get_cuda_graph_node_from_kernel(driver);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:      Impl::check_shmem_request(cuda_instance, shmem);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:            cuda_instance->m_cudaDev, base_t::get_kernel_func(),
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:            cuda_instance->m_deviceProp, block_size, shmem, desired_occupancy);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:      cudaKernelNodeParams params = {};
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:      KOKKOS_IMPL_CUDA_SAFE_CALL(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:          (cuda_instance->cuda_graph_add_kernel_node_wrapper(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:      KOKKOS_IMPL_CUDA_SAFE_CALL(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:          (cuda_instance->cuda_graph_add_empty_node_wrapper(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:struct CudaParallelLaunchKernelFunc<
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:    Experimental::CudaLaunchMechanism::GlobalMemory> {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:    return cuda_parallel_launch_global_memory<DriverType, MaxThreadsPerBlock,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:struct CudaParallelLaunchKernelFunc<
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:    Experimental::CudaLaunchMechanism::GlobalMemory> {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:  static std::decay_t<decltype(cuda_parallel_launch_global_memory<DriverType>)>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:    return cuda_parallel_launch_global_memory<DriverType>;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:struct CudaParallelLaunchKernelInvoker<
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:    DriverType, LaunchBounds, Experimental::CudaLaunchMechanism::GlobalMemory>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:    : CudaParallelLaunchKernelFunc<
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:          Experimental::CudaLaunchMechanism::GlobalMemory> {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:  using base_t = CudaParallelLaunchKernelFunc<
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:      Experimental::CudaLaunchMechanism::GlobalMemory>;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:                            CudaInternal const* cuda_instance) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:        cuda_instance->scratch_functor(sizeof(DriverType)));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:    KOKKOS_IMPL_CUDA_SAFE_CALL((cuda_instance->cuda_memcpy_async_wrapper(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:        driver_ptr, &driver, sizeof(DriverType), cudaMemcpyDefault)));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:                                  cuda_instance->get_stream()>>>(driver_ptr);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:      CudaInternal const* cuda_instance) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:    auto const& graph = Impl::get_cuda_graph_from_kernel(driver);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:    auto& graph_node = Impl::get_cuda_graph_node_from_kernel(driver);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:      Impl::check_shmem_request(cuda_instance, shmem);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:            cuda_instance->m_cudaDev, base_t::get_kernel_func(),
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:            cuda_instance->m_deviceProp, block_size, shmem, desired_occupancy);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:      KOKKOS_IMPL_CUDA_SAFE_CALL((cuda_instance->cuda_memcpy_async_wrapper(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:          driver_ptr, &driver, sizeof(DriverType), cudaMemcpyDefault)));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:      cudaKernelNodeParams params = {};
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:      KOKKOS_IMPL_CUDA_SAFE_CALL(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:          (cuda_instance->cuda_graph_add_kernel_node_wrapper(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:      KOKKOS_IMPL_CUDA_SAFE_CALL(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:          (cuda_instance->cuda_graph_add_empty_node_wrapper(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:struct CudaParallelLaunchKernelFunc<
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:    Experimental::CudaLaunchMechanism::ConstantMemory> {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:  static std::decay_t<decltype(cuda_parallel_launch_constant_memory<
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:    return cuda_parallel_launch_constant_memory<DriverType, MaxThreadsPerBlock,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:struct CudaParallelLaunchKernelFunc<
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:    Experimental::CudaLaunchMechanism::ConstantMemory> {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:      decltype(cuda_parallel_launch_constant_memory<DriverType>)>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:    return cuda_parallel_launch_constant_memory<DriverType>;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:struct CudaParallelLaunchKernelInvoker<
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:    DriverType, LaunchBounds, Experimental::CudaLaunchMechanism::ConstantMemory>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:    : CudaParallelLaunchKernelFunc<
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:          Experimental::CudaLaunchMechanism::ConstantMemory> {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:  using base_t = CudaParallelLaunchKernelFunc<
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:      Experimental::CudaLaunchMechanism::ConstantMemory>;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:  static_assert(sizeof(DriverType) < CudaTraits::ConstantMemoryUsage,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:                "Kokkos Error: Requested CudaLaunchConstantMemory with a "
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:                            CudaInternal const* cuda_instance) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:    int cuda_device = cuda_instance->m_cudaDev;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:        CudaInternal::constantMemMutexPerDevice[cuda_device]);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:    KOKKOS_IMPL_CUDA_SAFE_CALL((cuda_instance->cuda_event_synchronize_wrapper(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:        CudaInternal::constantMemReusablePerDevice[cuda_device])));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:        cuda_instance->constantMemHostStagingPerDevice[cuda_device];
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:    KOKKOS_IMPL_CUDA_SAFE_CALL(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:        (cuda_instance->cuda_memcpy_to_symbol_async_wrapper(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:            kokkos_impl_cuda_constant_memory_buffer, staging,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:            sizeof(DriverType), 0, cudaMemcpyHostToDevice)));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:                                  cuda_instance->get_stream()>>>();
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:    KOKKOS_IMPL_CUDA_SAFE_CALL((cuda_instance->cuda_event_record_wrapper(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:        CudaInternal::constantMemReusablePerDevice[cuda_device])));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:      CudaInternal const* cuda_instance) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:    using global_launch_impl_t = CudaParallelLaunchKernelInvoker<
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:        Experimental::CudaLaunchMechanism::GlobalMemory>;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:        driver, grid, block, shmem, cuda_instance);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:// </editor-fold> end CudaParallelLaunchKernelInvoker }}}1
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:// <editor-fold desc="CudaParallelLaunchImpl"> {{{1
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:          Experimental::CudaLaunchMechanism LaunchMechanism>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:struct CudaParallelLaunchImpl;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:          Experimental::CudaLaunchMechanism LaunchMechanism>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:struct CudaParallelLaunchImpl<
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:    : CudaParallelLaunchKernelInvoker<
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:  using base_t = CudaParallelLaunchKernelInvoker<
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:                                   const CudaInternal* cuda_instance) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:      Impl::check_shmem_request(cuda_instance, shmem);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:            cuda_instance->m_cudaDev, base_t::get_kernel_func(),
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:            cuda_instance->m_deviceProp, block_size, shmem, desired_occupancy);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:      desul::ensure_cuda_lock_arrays_on_device();
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:      base_t::invoke_kernel(driver, grid, block, shmem, cuda_instance);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:      KOKKOS_IMPL_CUDA_SAFE_CALL(cudaGetLastError());
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:      cuda_instance->fence(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:  static cudaFuncAttributes get_cuda_func_attributes(int cuda_device) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:    return get_cuda_kernel_func_attributes<
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:        cuda_device, base_t::get_kernel_func());
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:// </editor-fold> end CudaParallelLaunchImpl }}}1
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:// <editor-fold desc="CudaParallelLaunch"> {{{1
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:          Experimental::CudaLaunchMechanism LaunchMechanism =
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:              DeduceCudaLaunchMechanism<DriverType>::launch_mechanism,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:struct CudaParallelLaunch;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:          Experimental::CudaLaunchMechanism LaunchMechanism>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:struct CudaParallelLaunch<DriverType, LaunchBounds, LaunchMechanism,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:    : CudaParallelLaunchImpl<DriverType, LaunchBounds, LaunchMechanism> {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:      CudaParallelLaunchImpl<DriverType, LaunchBounds, LaunchMechanism>;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:  CudaParallelLaunch(Args&&... args) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:          Experimental::CudaLaunchMechanism LaunchMechanism>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:struct CudaParallelLaunch<DriverType, LaunchBounds, LaunchMechanism,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:    : CudaParallelLaunchImpl<DriverType, LaunchBounds, LaunchMechanism> {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:      CudaParallelLaunchImpl<DriverType, LaunchBounds, LaunchMechanism>;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:  CudaParallelLaunch(Args&&... args) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:// </editor-fold> end CudaParallelLaunch }}}1
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:#endif /* defined( KOKKOS_ENABLE_CUDA ) */
lib/kokkos/core/src/Cuda/Kokkos_Cuda_KernelLaunch.hpp:#endif /* #ifndef KOKKOS_CUDAEXEC_HPP */
lib/kokkos/core/src/Cuda/Kokkos_Cuda_GraphNodeKernel.hpp:#ifndef KOKKOS_KOKKOS_CUDA_GRAPHNODEKERNEL_IMPL_HPP
lib/kokkos/core/src/Cuda/Kokkos_Cuda_GraphNodeKernel.hpp:#define KOKKOS_KOKKOS_CUDA_GRAPHNODEKERNEL_IMPL_HPP
lib/kokkos/core/src/Cuda/Kokkos_Cuda_GraphNodeKernel.hpp:#if defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/core/src/Cuda/Kokkos_Cuda_GraphNodeKernel.hpp:#include <Cuda/Kokkos_Cuda.hpp>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_GraphNodeKernel.hpp:class GraphNodeKernelImpl<Kokkos::Cuda, PolicyType, Functor, PatternTag,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_GraphNodeKernel.hpp:                                              Args..., Kokkos::Cuda>::type {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_GraphNodeKernel.hpp:                                                Args..., Kokkos::Cuda>::type;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_GraphNodeKernel.hpp:  using size_type = Kokkos::Cuda::size_type;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_GraphNodeKernel.hpp:  // that the cudaGraph_t one needs to be since it's a pointer under the
lib/kokkos/core/src/Cuda/Kokkos_Cuda_GraphNodeKernel.hpp:  Kokkos::ObservingRawPtr<const cudaGraph_t> m_graph_ptr    = nullptr;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_GraphNodeKernel.hpp:  Kokkos::ObservingRawPtr<cudaGraphNode_t> m_graph_node_ptr = nullptr;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_GraphNodeKernel.hpp:  // global kernel buffers in the Cuda instance are mutable...
lib/kokkos/core/src/Cuda/Kokkos_Cuda_GraphNodeKernel.hpp:  GraphNodeKernelImpl(std::string, Kokkos::Cuda const&, Functor arg_functor,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_GraphNodeKernel.hpp:  GraphNodeKernelImpl(Kokkos::Cuda const& ex, Functor arg_functor,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_GraphNodeKernel.hpp:      Kokkos::CudaSpace().deallocate(m_driver_storage, sizeof(base_t));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_GraphNodeKernel.hpp:  void set_cuda_graph_ptr(cudaGraph_t* arg_graph_ptr) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_GraphNodeKernel.hpp:  void set_cuda_graph_node_ptr(cudaGraphNode_t* arg_node_ptr) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_GraphNodeKernel.hpp:  cudaGraphNode_t* get_cuda_graph_node_ptr() const { return m_graph_node_ptr; }
lib/kokkos/core/src/Cuda/Kokkos_Cuda_GraphNodeKernel.hpp:  cudaGraph_t const* get_cuda_graph_ptr() const { return m_graph_ptr; }
lib/kokkos/core/src/Cuda/Kokkos_Cuda_GraphNodeKernel.hpp:    m_driver_storage = static_cast<base_t*>(Kokkos::CudaSpace().allocate(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_GraphNodeKernel.hpp:struct CudaGraphNodeAggregateKernel {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_GraphNodeKernel.hpp:  using graph_kernel = CudaGraphNodeAggregateKernel;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_GraphNodeKernel.hpp:          GraphNodeKernelImpl<Kokkos::Cuda, typename KernelType::Policy,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_GraphNodeKernel.hpp:          Kokkos::Cuda, typename KernelType::Policy,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_GraphNodeKernel.hpp:// <editor-fold desc="get_cuda_graph_*() helper functions"> {{{1
lib/kokkos/core/src/Cuda/Kokkos_Cuda_GraphNodeKernel.hpp:auto const& get_cuda_graph_from_kernel(KernelType const& kernel) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_GraphNodeKernel.hpp:  cudaGraph_t const* graph_ptr = kernel_as_graph_kernel.get_cuda_graph_ptr();
lib/kokkos/core/src/Cuda/Kokkos_Cuda_GraphNodeKernel.hpp:auto& get_cuda_graph_node_from_kernel(KernelType const& kernel) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_GraphNodeKernel.hpp:  auto* graph_node_ptr = kernel_as_graph_kernel.get_cuda_graph_node_ptr();
lib/kokkos/core/src/Cuda/Kokkos_Cuda_GraphNodeKernel.hpp:// </editor-fold> end get_cuda_graph_*() helper functions }}}1
lib/kokkos/core/src/Cuda/Kokkos_Cuda_GraphNodeKernel.hpp:#endif  // defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/core/src/Cuda/Kokkos_Cuda_GraphNodeKernel.hpp:#endif  // KOKKOS_KOKKOS_CUDA_GRAPHNODEKERNEL_IMPL_HPP
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Error.hpp:#ifndef KOKKOS_CUDA_ERROR_HPP
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Error.hpp:#define KOKKOS_CUDA_ERROR_HPP
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Error.hpp:#ifdef KOKKOS_ENABLE_CUDA
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Error.hpp:void cuda_device_synchronize(const std::string& name);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Error.hpp:void cuda_stream_synchronize(const cudaStream_t stream,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Error.hpp:[[noreturn]] void cuda_internal_error_throw(cudaError e, const char* name,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Error.hpp:             void cuda_internal_error_abort(cudaError e, const char* name,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Error.hpp:inline void cuda_internal_safe_call(cudaError e, const char* name,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Error.hpp:  // 2. Error codes for which, to continue using CUDA, the process must be
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Error.hpp:    case cudaSuccess: break;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Error.hpp:    case cudaErrorIllegalAddress:
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Error.hpp:    case cudaErrorAssert:
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Error.hpp:    case cudaErrorHardwareStackError:
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Error.hpp:    case cudaErrorIllegalInstruction:
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Error.hpp:    case cudaErrorMisalignedAddress:
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Error.hpp:    case cudaErrorInvalidAddressSpace:
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Error.hpp:    case cudaErrorInvalidPc:
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Error.hpp:    case cudaErrorLaunchFailure:
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Error.hpp:      cuda_internal_error_abort(e, name, file, line);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Error.hpp:    default: cuda_internal_error_throw(e, name, file, line); break;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Error.hpp:#define KOKKOS_IMPL_CUDA_SAFE_CALL(call) \
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Error.hpp:  Kokkos::Impl::cuda_internal_safe_call(call, #call, __FILE__, __LINE__)
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Error.hpp:#endif  // KOKKOS_ENABLE_CUDA
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Error.hpp:#endif  // KOKKOS_CUDA_ERROR_HPP
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Graph_Impl.hpp:#ifndef KOKKOS_KOKKOS_CUDA_GRAPH_IMPL_HPP
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Graph_Impl.hpp:#define KOKKOS_KOKKOS_CUDA_GRAPH_IMPL_HPP
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Graph_Impl.hpp:#if defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Graph_Impl.hpp:#include <Cuda/Kokkos_Cuda_GraphNode_Impl.hpp>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Graph_Impl.hpp:#include <Cuda/Kokkos_Cuda.hpp>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Graph_Impl.hpp:#include <Cuda/Kokkos_Cuda_Error.hpp>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Graph_Impl.hpp:#include <Cuda/Kokkos_Cuda_Instance.hpp>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Graph_Impl.hpp:struct GraphImpl<Kokkos::Cuda> {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Graph_Impl.hpp:  using execution_space = Kokkos::Cuda;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Graph_Impl.hpp:  cudaGraph_t m_graph          = nullptr;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Graph_Impl.hpp:  cudaGraphExec_t m_graph_exec = nullptr;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Graph_Impl.hpp:  using cuda_graph_flags_t = unsigned int;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Graph_Impl.hpp:  using node_details_t = GraphNodeBackendSpecificDetails<Kokkos::Cuda>;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Graph_Impl.hpp:    cudaGraphNode_t error_node      = nullptr;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Graph_Impl.hpp:    KOKKOS_IMPL_CUDA_SAFE_CALL(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Graph_Impl.hpp:             ->cuda_graph_instantiate_wrapper(&m_graph_exec, m_graph,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Graph_Impl.hpp:      GraphNodeImpl<Kokkos::Cuda, Kokkos::Experimental::TypeErasedTag,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Graph_Impl.hpp:  using aggregate_kernel_impl_t = CudaGraphNodeAggregateKernel;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Graph_Impl.hpp:      GraphNodeImpl<Kokkos::Cuda, aggregate_kernel_impl_t,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Graph_Impl.hpp:      KOKKOS_IMPL_CUDA_SAFE_CALL(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Graph_Impl.hpp:               ->cuda_graph_exec_destroy_wrapper(m_graph_exec)));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Graph_Impl.hpp:    KOKKOS_IMPL_CUDA_SAFE_CALL(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Graph_Impl.hpp:             ->cuda_graph_destroy_wrapper(m_graph)));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Graph_Impl.hpp:  explicit GraphImpl(Kokkos::Cuda arg_instance)
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Graph_Impl.hpp:    KOKKOS_IMPL_CUDA_SAFE_CALL(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Graph_Impl.hpp:             ->cuda_graph_create_wrapper(&m_graph, cuda_graph_flags_t{0})));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Graph_Impl.hpp:    KOKKOS_IMPL_CUDA_SAFE_CALL(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Graph_Impl.hpp:             ->cuda_graph_add_empty_node_wrapper(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Graph_Impl.hpp:    auto& cuda_node = static_cast<node_details_t*>(arg_node_ptr.get())->node;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Graph_Impl.hpp:    KOKKOS_EXPECTS(!bool(cuda_node));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Graph_Impl.hpp:    kernel.set_cuda_graph_ptr(&m_graph);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Graph_Impl.hpp:    kernel.set_cuda_graph_node_ptr(&cuda_node);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Graph_Impl.hpp:    KOKKOS_ENSURES(bool(cuda_node));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Graph_Impl.hpp:    // from CUDA 10.0 to CUDA 10.1
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Graph_Impl.hpp:    // cudaGraphAddDependencies(cudaGraph_t, cudaGraphNode_t*, cudaGraphNode_t*, size_t)
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Graph_Impl.hpp:    // cudaGraphAddDependencies(cudaGraph_t, const cudaGraphNode_t*, const cudaGraphNode_t*, size_t)
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Graph_Impl.hpp:    auto /*const*/& pred_cuda_node = pred_ptr->node_details_t::node;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Graph_Impl.hpp:    KOKKOS_EXPECTS(bool(pred_cuda_node))
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Graph_Impl.hpp:    auto /*const*/& cuda_node = arg_node_ptr->node_details_t::node;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Graph_Impl.hpp:    KOKKOS_EXPECTS(bool(cuda_node))
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Graph_Impl.hpp:    KOKKOS_IMPL_CUDA_SAFE_CALL(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Graph_Impl.hpp:             ->cuda_graph_add_dependencies_wrapper(m_graph, &pred_cuda_node,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Graph_Impl.hpp:                                                   &cuda_node, 1)));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Graph_Impl.hpp:    KOKKOS_IMPL_CUDA_SAFE_CALL(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Graph_Impl.hpp:             ->cuda_graph_launch_wrapper(m_graph_exec)));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Graph_Impl.hpp:    KOKKOS_IMPL_CUDA_SAFE_CALL(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Graph_Impl.hpp:             ->cuda_graph_add_empty_node_wrapper(&(rv->node_details_t::node),
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Graph_Impl.hpp:#endif  // defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Graph_Impl.hpp:#endif  // KOKKOS_KOKKOS_CUDA_GRAPH_IMPL_HPP
lib/kokkos/core/src/Cuda/Kokkos_Cuda_MDRangePolicy.hpp:#ifndef KOKKOS_CUDA_MDRANGEPOLICY_HPP_
lib/kokkos/core/src/Cuda/Kokkos_Cuda_MDRangePolicy.hpp:#define KOKKOS_CUDA_MDRANGEPOLICY_HPP_
lib/kokkos/core/src/Cuda/Kokkos_Cuda_MDRangePolicy.hpp:struct default_outer_direction<Kokkos::Cuda> {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_MDRangePolicy.hpp:struct default_inner_direction<Kokkos::Cuda> {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_MDRangePolicy.hpp:inline TileSizeProperties get_tile_size_properties<Kokkos::Cuda>(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_MDRangePolicy.hpp:    const Kokkos::Cuda& space) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_MDRangePolicy.hpp:struct ThreadAndVectorNestLevel<Rank, Cuda, ThreadAndVector>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Half_Impl_Type.hpp:#ifndef KOKKOS_CUDA_HALF_IMPL_TYPE_HPP_
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Half_Impl_Type.hpp:#define KOKKOS_CUDA_HALF_IMPL_TYPE_HPP_
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Half_Impl_Type.hpp:#ifdef KOKKOS_ENABLE_CUDA
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Half_Impl_Type.hpp:#include <cuda_fp16.h>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Half_Impl_Type.hpp:#if (CUDA_VERSION >= 11000)
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Half_Impl_Type.hpp:#include <cuda_bf16.h>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Half_Impl_Type.hpp:#endif  // CUDA_VERSION >= 11000
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Half_Impl_Type.hpp:#define KOKKOS_IMPL_CUDA_HALF_TYPE_DEFINED
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Half_Impl_Type.hpp:#if (CUDA_VERSION >= 11000)
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Half_Impl_Type.hpp:#endif  // CUDA_VERSION >= 11000
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Half_Impl_Type.hpp:#endif  // Disables for half_t on cuda:
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Half_Impl_Type.hpp:#endif  // KOKKOS_ENABLE_CUDA
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.cpp:#if defined(KOKKOS_ENABLE_CUDA) && defined(KOKKOS_ENABLE_TASKDAG)
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.cpp:    Kokkos::Cuda,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.cpp:    Impl::default_tasking_memory_space_for_execution_space_t<Kokkos::Cuda> >;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.cpp:    Kokkos::Cuda,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.cpp:    Impl::default_tasking_memory_space_for_execution_space_t<Kokkos::Cuda> >;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.cpp:void KOKKOS_CORE_SRC_CUDA_KOKKOS_CUDA_TASK_PREVENT_LINK_ERROR() {}
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.cpp:#endif /* #if defined( KOKKOS_ENABLE_CUDA ) && defined( KOKKOS_ENABLE_TASKDAG \
lib/kokkos/core/src/Cuda/Kokkos_Cuda_abort.hpp:#ifndef KOKKOS_CUDA_ABORT_HPP
lib/kokkos/core/src/Cuda/Kokkos_Cuda_abort.hpp:#define KOKKOS_CUDA_ABORT_HPP
lib/kokkos/core/src/Cuda/Kokkos_Cuda_abort.hpp:#if defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/core/src/Cuda/Kokkos_Cuda_abort.hpp:#include <cuda.h>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_abort.hpp:/*  Cuda runtime function, declared in <crt/device_runtime.h>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_abort.hpp:[[noreturn]] __device__ static void cuda_abort(const char *const message) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_abort.hpp:void KOKKOS_CORE_SRC_CUDA_ABORT_PREVENT_LINK_ERROR() {}
lib/kokkos/core/src/Cuda/Kokkos_Cuda_abort.hpp:#endif /* #if defined( KOKKOS_ENABLE_CUDA ) */
lib/kokkos/core/src/Cuda/Kokkos_Cuda_abort.hpp:#endif /* #ifndef KOKKOS_CUDA_ABORT_HPP */
lib/kokkos/core/src/Cuda/Kokkos_Cuda_GraphNode_Impl.hpp:#ifndef KOKKOS_KOKKOS_CUDA_GRAPHNODE_IMPL_HPP
lib/kokkos/core/src/Cuda/Kokkos_Cuda_GraphNode_Impl.hpp:#define KOKKOS_KOKKOS_CUDA_GRAPHNODE_IMPL_HPP
lib/kokkos/core/src/Cuda/Kokkos_Cuda_GraphNode_Impl.hpp:#if defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/core/src/Cuda/Kokkos_Cuda_GraphNode_Impl.hpp:#include <Cuda/Kokkos_Cuda.hpp>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_GraphNode_Impl.hpp:struct GraphNodeBackendSpecificDetails<Kokkos::Cuda> {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_GraphNode_Impl.hpp:  cudaGraphNode_t node = nullptr;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_GraphNode_Impl.hpp:struct GraphNodeBackendDetailsBeforeTypeErasure<Kokkos::Cuda, Kernel,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_GraphNode_Impl.hpp:      Kokkos::Cuda const&, Kernel&, PredecessorRef const&,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_GraphNode_Impl.hpp:      GraphNodeBackendSpecificDetails<Kokkos::Cuda>&) noexcept {}
lib/kokkos/core/src/Cuda/Kokkos_Cuda_GraphNode_Impl.hpp:      Kokkos::Cuda const&, _graph_node_is_root_ctor_tag,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_GraphNode_Impl.hpp:      GraphNodeBackendSpecificDetails<Kokkos::Cuda>&) noexcept {}
lib/kokkos/core/src/Cuda/Kokkos_Cuda_GraphNode_Impl.hpp:#include <Cuda/Kokkos_Cuda_GraphNodeKernel.hpp>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_GraphNode_Impl.hpp:#endif  // defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/core/src/Cuda/Kokkos_Cuda_GraphNode_Impl.hpp:#endif  // KOKKOS_KOKKOS_CUDA_GRAPHNODE_IMPL_HPP
lib/kokkos/core/src/Cuda/Kokkos_Cuda_UniqueToken.hpp:#ifndef KOKKOS_CUDA_UNIQUE_TOKEN_HPP
lib/kokkos/core/src/Cuda/Kokkos_Cuda_UniqueToken.hpp:#define KOKKOS_CUDA_UNIQUE_TOKEN_HPP
lib/kokkos/core/src/Cuda/Kokkos_Cuda_UniqueToken.hpp:#ifdef KOKKOS_ENABLE_CUDA
lib/kokkos/core/src/Cuda/Kokkos_Cuda_UniqueToken.hpp:#include <Cuda/Kokkos_CudaSpace.hpp>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_UniqueToken.hpp:Kokkos::View<uint32_t*, Kokkos::CudaSpace> cuda_global_unique_token_locks(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_UniqueToken.hpp:class UniqueToken<Cuda, UniqueTokenScope::Global> {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_UniqueToken.hpp:  Kokkos::View<uint32_t*, Kokkos::CudaSpace> m_locks;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_UniqueToken.hpp:  using execution_space = Cuda;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_UniqueToken.hpp:  explicit UniqueToken(execution_space const& = Cuda())
lib/kokkos/core/src/Cuda/Kokkos_Cuda_UniqueToken.hpp:      : m_locks(Kokkos::Impl::cuda_global_unique_token_locks()) {}
lib/kokkos/core/src/Cuda/Kokkos_Cuda_UniqueToken.hpp:    m_locks = Kokkos::View<uint32_t*, Kokkos::CudaSpace>(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_UniqueToken.hpp:    m_locks = Kokkos::View<uint32_t*, Kokkos::CudaSpace>(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_UniqueToken.hpp:class UniqueToken<Cuda, UniqueTokenScope::Instance>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_UniqueToken.hpp:    : public UniqueToken<Cuda, UniqueTokenScope::Global> {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_UniqueToken.hpp:      : UniqueToken<Cuda, UniqueTokenScope::Global>(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_UniqueToken.hpp:            Kokkos::Cuda().concurrency()) {}
lib/kokkos/core/src/Cuda/Kokkos_Cuda_UniqueToken.hpp:      : UniqueToken<Cuda, UniqueTokenScope::Global>(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_UniqueToken.hpp:            Kokkos::Cuda().concurrency(), arg) {}
lib/kokkos/core/src/Cuda/Kokkos_Cuda_UniqueToken.hpp:      : UniqueToken<Cuda, UniqueTokenScope::Global>(max_size) {}
lib/kokkos/core/src/Cuda/Kokkos_Cuda_UniqueToken.hpp:      : UniqueToken<Cuda, UniqueTokenScope::Global>(max_size, arg) {}
lib/kokkos/core/src/Cuda/Kokkos_Cuda_UniqueToken.hpp:#endif  // KOKKOS_ENABLE_CUDA
lib/kokkos/core/src/Cuda/Kokkos_Cuda_UniqueToken.hpp:#endif  // KOKKOS_CUDA_UNIQUE_TOKEN_HPP
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:#ifndef KOKKOS_CUDA_TEAM_HPP
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:#define KOKKOS_CUDA_TEAM_HPP
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:/* only compile this file if CUDA is enabled for Kokkos */
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:#if defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:#include <Cuda/Kokkos_Cuda_KernelLaunch.hpp>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:#include <Cuda/Kokkos_Cuda_ReduceScan.hpp>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:#include <Cuda/Kokkos_Cuda_BlockSize_Deduction.hpp>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:struct CudaJoinFunctor {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp: *  Cuda thread blocks for team closures are dimensioned as:
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:class CudaTeamMember {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:  using execution_space      = Kokkos::Cuda;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:  using team_handle          = CudaTeamMember;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:   *    blockDim.x <= CudaTraits::WarpSize
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:   *    ( 0 == CudaTraits::WarpSize % ( blockDim.x * blockDim.y )
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:             Impl::FunctorPatternInterface::REDUCE, TeamPolicy<Cuda>,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:         cuda_intra_block_reduction(value, wrapped_reducer, blockDim.y);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:        Impl::CudaJoinFunctor<Type> cuda_join_functor;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:            Impl::FunctorPatternInterface::SCAN, TeamPolicy<Cuda>,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:            Impl::CudaJoinFunctor<Type>, Type>::Reducer
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:            reducer(cuda_join_functor);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:        Impl::cuda_intra_block_reduce_scan<true>(reducer, base_data + 1);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:  CudaTeamMember(void* shared, const size_t shared_begin,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:struct TeamThreadRangeBoundariesStruct<iType, CudaTeamMember> {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:  const CudaTeamMember& member;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:  TeamThreadRangeBoundariesStruct(const CudaTeamMember& thread_, iType count)
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:  TeamThreadRangeBoundariesStruct(const CudaTeamMember& thread_, iType begin_,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:struct TeamVectorRangeBoundariesStruct<iType, CudaTeamMember> {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:  const CudaTeamMember& member;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:  TeamVectorRangeBoundariesStruct(const CudaTeamMember& thread_,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:  TeamVectorRangeBoundariesStruct(const CudaTeamMember& thread_,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:struct ThreadVectorRangeBoundariesStruct<iType, CudaTeamMember> {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:  ThreadVectorRangeBoundariesStruct(const CudaTeamMember, index_type count)
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:  ThreadVectorRangeBoundariesStruct(const CudaTeamMember, index_type arg_begin,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:    Impl::TeamThreadRangeBoundariesStruct<iType, Impl::CudaTeamMember>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:    TeamThreadRange(const Impl::CudaTeamMember& thread, iType count) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:  return Impl::TeamThreadRangeBoundariesStruct<iType, Impl::CudaTeamMember>(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:    std::common_type_t<iType1, iType2>, Impl::CudaTeamMember>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:TeamThreadRange(const Impl::CudaTeamMember& thread, iType1 begin, iType2 end) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:  return Impl::TeamThreadRangeBoundariesStruct<iType, Impl::CudaTeamMember>(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:    Impl::TeamVectorRangeBoundariesStruct<iType, Impl::CudaTeamMember>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:    TeamVectorRange(const Impl::CudaTeamMember& thread, const iType& count) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:  return Impl::TeamVectorRangeBoundariesStruct<iType, Impl::CudaTeamMember>(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:    std::common_type_t<iType1, iType2>, Impl::CudaTeamMember>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:TeamVectorRange(const Impl::CudaTeamMember& thread, const iType1& begin,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:  return Impl::TeamVectorRangeBoundariesStruct<iType, Impl::CudaTeamMember>(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:    Impl::ThreadVectorRangeBoundariesStruct<iType, Impl::CudaTeamMember>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:    ThreadVectorRange(const Impl::CudaTeamMember& thread, iType count) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:  return Impl::ThreadVectorRangeBoundariesStruct<iType, Impl::CudaTeamMember>(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:    std::common_type_t<iType1, iType2>, Impl::CudaTeamMember>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:ThreadVectorRange(const Impl::CudaTeamMember& thread, iType1 arg_begin,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:  return Impl::ThreadVectorRangeBoundariesStruct<iType, Impl::CudaTeamMember>(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:Impl::ThreadSingleStruct<Impl::CudaTeamMember> PerTeam(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:    const Impl::CudaTeamMember& thread) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:  return Impl::ThreadSingleStruct<Impl::CudaTeamMember>(thread);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:Impl::VectorSingleStruct<Impl::CudaTeamMember> PerThread(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:    const Impl::CudaTeamMember& thread) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:  return Impl::VectorSingleStruct<Impl::CudaTeamMember>(thread);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:    const Impl::TeamThreadRangeBoundariesStruct<iType, Impl::CudaTeamMember>&
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:                    iType, Impl::CudaTeamMember>& loop_boundaries,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:                    iType, Impl::CudaTeamMember>& loop_boundaries,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:    const Impl::TeamVectorRangeBoundariesStruct<iType, Impl::CudaTeamMember>&
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:                    iType, Impl::CudaTeamMember>& loop_boundaries,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:                    iType, Impl::CudaTeamMember>& loop_boundaries,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:    const Impl::ThreadVectorRangeBoundariesStruct<iType, Impl::CudaTeamMember>&
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:                    iType, Impl::CudaTeamMember> const& loop_boundaries,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:      Impl::CudaTeamMember::vector_reduce(reducer);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:                    iType, Impl::CudaTeamMember> const& loop_boundaries,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:       Impl::CudaTeamMember::vector_reduce(Kokkos::Sum<ValueType>(result));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:    const Impl::TeamThreadRangeBoundariesStruct<iType, Impl::CudaTeamMember>&
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:    const Impl::TeamThreadRangeBoundariesStruct<iType, Impl::CudaTeamMember>&
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:                  iType, Impl::CudaTeamMember>& loop_boundaries,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:      //   1 <= blockDim.x <= CudaTraits::WarpSize
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:        // where each CUDA thread is the root of a reduction tree
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:    const Impl::ThreadVectorRangeBoundariesStruct<iType, Impl::CudaTeamMember>&
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:    const Impl::ThreadVectorRangeBoundariesStruct<iType, Impl::CudaTeamMember>&
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:    const Impl::VectorSingleStruct<Impl::CudaTeamMember>&,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:    const Impl::ThreadSingleStruct<Impl::CudaTeamMember>&,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:    const Impl::VectorSingleStruct<Impl::CudaTeamMember>&,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:    const Impl::ThreadSingleStruct<Impl::CudaTeamMember>& single_struct,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:#endif /* defined(KOKKOS_ENABLE_CUDA) */
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Team.hpp:#endif /* #ifndef KOKKOS_CUDA_TEAM_HPP */
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:#ifndef KOKKOS_IMPL_CUDA_TASK_HPP
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:#define KOKKOS_IMPL_CUDA_TASK_HPP
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:#include <Cuda/Kokkos_Cuda_Error.hpp>  // KOKKOS_IMPL_CUDA_SAFE_CALL
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:#if defined(__CUDA_ARCH__)
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:#define KOKKOS_IMPL_CUDA_SYNCWARP_OR_RETURN(MSG)                           \
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:#define KOKKOS_IMPL_CUDA_SYNCWARP_OR_RETURN(MSG)
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:__global__ void set_cuda_task_base_apply_function_pointer(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:__global__ void cuda_task_queue_execute(Scheduler scheduler,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:class TaskQueueSpecialization<SimpleTaskScheduler<Kokkos::Cuda, QueueType>> {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:  using scheduler_type  = SimpleTaskScheduler<Kokkos::Cuda, QueueType>;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:  using execution_space = Kokkos::Cuda;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:  using memory_space    = Kokkos::CudaUVMSpace;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:  using member_type     = TaskExec<Kokkos::Cuda, scheduler_type>;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:    return space.cuda_device_prop().multiProcessorCount * warps_per_block;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:        for (int32_t i = warp_lane; i < e; i += CudaTraits::WarpSize) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:        // if(warp_lane < b % CudaTraits::WarpSize) b += CudaTraits::WarpSize;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:        // b -= b % CudaTraits::WarpSize;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:        for (int32_t i = b + warp_lane; i < e; i += CudaTraits::WarpSize) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:  // FIXME_CUDA_MULTIPLE_DEVICES
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:    const Kokkos::Cuda& exec  = scheduler.get_execution_space();
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:        exec.cuda_device_prop().multiProcessorCount;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:    const dim3 block(1, Kokkos::Impl::CudaTraits::WarpSize, warps_per_block);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:    const cudaStream_t stream = nullptr;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:                          Kokkos::Impl::CudaTraits::WarpSize));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:    Impl::cuda_device_synchronize(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:        "Cuda>::execute: Pre Task Execution");
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:    KOKKOS_IMPL_CUDA_SAFE_CALL(impl_instance->cuda_device_get_limit_wrapper(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:        &previous_stack_size, cudaLimitStackSize));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:      KOKKOS_IMPL_CUDA_SAFE_CALL(impl_instance->cuda_device_set_limit_wrapper(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:          cudaLimitStackSize, larger_stack_size));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:    cuda_task_queue_execute<<<grid, block, shared_total, stream>>>(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:    KOKKOS_IMPL_CUDA_SAFE_CALL(cudaGetLastError());
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:    Impl::cuda_device_synchronize(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:        "Cuda>::execute: Post Task Execution");
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:      KOKKOS_IMPL_CUDA_SAFE_CALL(impl_instance->cuda_device_set_limit_wrapper(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:          cudaLimitStackSize, previous_stack_size));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:    void* storage = cuda_internal_scratch_unified(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:        Kokkos::Cuda(), sizeof(function_type) + sizeof(destroy_type));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:    Impl::cuda_device_synchronize(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:        "Cuda>::execute: Pre Get Function Pointer for Tasks");
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:    set_cuda_task_base_apply_function_pointer<TaskType>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:    KOKKOS_IMPL_CUDA_SAFE_CALL(cudaGetLastError());
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:    Impl::cuda_device_synchronize(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:        "Cuda>::execute: Post Get Function Pointer for Tasks");
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:                   typename Scheduler::execution_space, Kokkos::Cuda>::value>> {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:  using execution_space = Kokkos::Cuda;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:  using memory_space    = Kokkos::CudaUVMSpace;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:  using member_type     = TaskExec<Kokkos::Cuda, Scheduler>;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:      // KOKKOS_IMPL_CUDA_SYNCWARP_OR_RETURN( "A" );
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:      KOKKOS_IMPL_CUDA_SYNCWARP_OR_RETURN("TaskQueue CUDA task_ptr");
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:        for (int32_t i = warp_lane; i < e; i += CudaTraits::WarpSize) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:        // KOKKOS_IMPL_CUDA_SYNCWARP_OR_RETURN( "B" );
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:        // KOKKOS_IMPL_CUDA_SYNCWARP_OR_RETURN( "C" );
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:        for (int32_t i = b + warp_lane; i < e; i += CudaTraits::WarpSize) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:        // KOKKOS_IMPL_CUDA_SYNCWARP_OR_RETURN( "D" );
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:  // FIXME_CUDA_MULTIPLE_DEVICES
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:    const Kokkos::Cuda exec   = Cuda();  // FIXME_CUDA_MULTIPLE_DEVICES
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:        // exec.cuda_device_prop().multiProcessorCount;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:    const dim3 block(1, Kokkos::Impl::CudaTraits::WarpSize, warps_per_block);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:    const cudaStream_t stream = 0;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:    Impl::cuda_device_synchronize(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:        "Kokkos::Cuda>::execute: Pre Execute Task");
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:    KOKKOS_IMPL_CUDA_SAFE_CALL(impl_instance->cuda_device_get_limit_wrapper(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:        &previous_stack_size, cudaLimitStackSize));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:      KOKKOS_IMPL_CUDA_SAFE_CALL(impl_instance->cuda_device_set_limit_wrapper(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:          cudaLimitStackSize, larger_stack_size));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:    cuda_task_queue_execute<<<grid, block, shared_total, stream>>>(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:    KOKKOS_IMPL_CUDA_SAFE_CALL(cudaGetLastError());
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:    Impl::cuda_device_synchronize(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:        "Kokkos::Cuda>::execute: Post Execute Task");
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:      KOKKOS_IMPL_CUDA_SAFE_CALL(impl_instance->cuda_device_set_limit_wrapper(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:          cudaLimitStackSize, previous_stack_size));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:    void* storage = cuda_internal_scratch_unified(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:        Kokkos::Cuda(), sizeof(function_type) + sizeof(destroy_type));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:    Impl::cuda_device_synchronize(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:        "Kokkos::Cuda>::get_function_pointer: Pre Get Function Pointer");
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:    set_cuda_task_base_apply_function_pointer<TaskType>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:    KOKKOS_IMPL_CUDA_SAFE_CALL(cudaGetLastError());
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:    Impl::cuda_device_synchronize(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:        "Kokkos::Cuda>::get_function_pointer: Post Get Function Pointer");
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:    Kokkos::Cuda,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:    default_tasking_memory_space_for_execution_space_t<Kokkos::Cuda>>;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:/**\brief  Impl::TaskExec<Cuda> is the TaskScheduler<Cuda>::member_type
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp: *         passed to tasks running in a Cuda space.
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp: *  Cuda thread blocks for tasking are dimensioned:
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp: *  Both single thread and thread team tasks are run by a full Cuda warp.
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:class TaskExec<Kokkos::Cuda, Scheduler> {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:  enum : int { WarpSize = Kokkos::Impl::CudaTraits::WarpSize };
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:      Kokkos::Cuda,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:      default_tasking_memory_space_for_execution_space_t<Kokkos::Cuda>>;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:#if defined(__CUDA_ARCH__)
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:                                       TaskExec<Kokkos::Cuda, Scheduler>> {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:  using member_type = TaskExec<Kokkos::Cuda, Scheduler>;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:#if defined(__CUDA_ARCH__)
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:                                         TaskExec<Kokkos::Cuda, Scheduler>> {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:  using member_type = TaskExec<Kokkos::Cuda, Scheduler>;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:#if defined(__CUDA_ARCH__)
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:// Impl::TeamThreadRangeBoundariesStruct< iType, Impl::TaskExec< Kokkos::Cuda >
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:// > TeamThreadRange( const Impl::TaskExec< Kokkos::Cuda > & thread, const iType
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp://  Kokkos::Cuda > >( thread, count );
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp://  , Impl::TaskExec< Kokkos::Cuda > >
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:// TeamThreadRange( const Impl::TaskExec< Kokkos::Cuda > & thread
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp://  Kokkos::Cuda > >(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:// Impl::ThreadVectorRangeBoundariesStruct<iType,Impl::TaskExec< Kokkos::Cuda >
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:// > ThreadVectorRange( const Impl::TaskExec< Kokkos::Cuda > & thread
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp://  Kokkos::Cuda > >(thread,count);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:// Impl::ThreadVectorRangeBoundariesStruct<iType,Impl::TaskExec< Kokkos::Cuda >
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:// > ThreadVectorRange( const Impl::TaskExec< Kokkos::Cuda > & thread
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp://  Kokkos::Cuda > >(thread,arg_begin,arg_end);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:// Impl::ThreadSingleStruct<Impl::TaskExec< Kokkos::Cuda > >
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:// PerTeam(const Impl::TaskExec< Kokkos::Cuda >& thread)
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp://   return Impl::ThreadSingleStruct<Impl::TaskExec< Kokkos::Cuda > >(thread);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:// Impl::VectorSingleStruct<Impl::TaskExec< Kokkos::Cuda > >
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:// PerThread(const Impl::TaskExec< Kokkos::Cuda >& thread)
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp://   return Impl::VectorSingleStruct<Impl::TaskExec< Kokkos::Cuda > >(thread);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:        iType, Impl::TaskExec<Kokkos::Cuda, Scheduler>>& loop_boundaries,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:        iType, Impl::TaskExec<Kokkos::Cuda, Scheduler>>& loop_boundaries,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:Kokkos::Cuda > >& loop_boundaries, const Lambda & lambda, const JoinType& join,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:threadIdx.x, Impl::CudaTraits::WarpSize );
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:        iType, Impl::TaskExec<Kokkos::Cuda, Scheduler>>& loop_boundaries,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:        initialized_result, threadIdx.x, Impl::CudaTraits::WarpSize);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:        iType, Impl::TaskExec<Kokkos::Cuda, Scheduler>>& loop_boundaries,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:        result, threadIdx.x, Impl::CudaTraits::WarpSize);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:Kokkos::Cuda > >& loop_boundaries, const Lambda & lambda, const JoinType& join,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:        iType, Impl::TaskExec<Kokkos::Cuda, Scheduler>>& loop_boundaries,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:        iType, Impl::TaskExec<Kokkos::Cuda, Scheduler>>& loop_boundaries,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:        iType, Impl::TaskExec<Kokkos::Cuda, Scheduler>>& loop_boundaries,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:      for (int offset = blockDim.x; offset < Impl::CudaTraits::WarpSize;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:        y = Kokkos::shfl_up(val, offset, Impl::CudaTraits::WarpSize);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:          val, threadIdx.x + Impl::CudaTraits::WarpSize - blockDim.x,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:          Impl::CudaTraits::WarpSize);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:      val = Kokkos::shfl_up(val, blockDim.x, Impl::CudaTraits::WarpSize);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:        iType, Impl::TaskExec<Kokkos::Cuda, Scheduler>>& loop_boundaries,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:    const Impl::VectorSingleStruct<Impl::TaskExec<Kokkos::Cuda, Scheduler>>&,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:#ifdef __CUDA_ARCH__
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:    const Impl::ThreadSingleStruct<Impl::TaskExec<Kokkos::Cuda, Scheduler>>&,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:#ifdef __CUDA_ARCH__
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:    const Impl::VectorSingleStruct<Impl::TaskExec<Kokkos::Cuda, Scheduler>>& s,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:#ifdef __CUDA_ARCH__
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:    const Impl::ThreadSingleStruct<Impl::TaskExec<Kokkos::Cuda, Scheduler>>&
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:#ifdef __CUDA_ARCH__
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:#undef KOKKOS_IMPL_CUDA_SYNCWARP_OR_RETURN
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Task.hpp:#endif /* #ifndef KOKKOS_IMPL_CUDA_TASK_HPP */
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Vectorization.hpp:#ifndef KOKKOS_CUDA_VECTORIZATION_HPP
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Vectorization.hpp:#define KOKKOS_CUDA_VECTORIZATION_HPP
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Vectorization.hpp:#ifdef KOKKOS_ENABLE_CUDA
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Vectorization.hpp:#define KOKKOS_IMPL_CUDA_MAX_SHFL_SIZEOF sizeof(long long)
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Vectorization.hpp:#define KOKKOS_IMPL_CUDA_MAX_SHFL_SIZEOF sizeof(int)
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Vectorization.hpp:    // TODO DSH shouldn't this be KOKKOS_IMPL_CUDA_MAX_SHFL_SIZEOF instead of
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Vectorization.hpp:#undef KOKKOS_IMPL_CUDA_MAX_SHFL_SIZEOF
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Vectorization.hpp:#endif  // defined( KOKKOS_ENABLE_CUDA )
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Vectorization.hpp:#endif  // !defined( KOKKOS_CUDA_VECTORIZATION_HPP )
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:#ifndef KOKKOS_CUDA_PARALLEL_RANGE_HPP
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:#define KOKKOS_CUDA_PARALLEL_RANGE_HPP
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:#if defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:#include <Cuda/Kokkos_Cuda_KernelLaunch.hpp>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:#include <Cuda/Kokkos_Cuda_ReduceScan.hpp>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:#include <Cuda/Kokkos_Cuda_BlockSize_Deduction.hpp>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:class ParallelFor<FunctorType, Kokkos::RangePolicy<Traits...>, Kokkos::Cuda> {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:    cudaFuncAttributes attr =
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:        CudaParallelLaunch<ParallelFor, LaunchBounds>::get_cuda_func_attributes(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:            m_policy.space().cuda_device());
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:        Kokkos::Impl::cuda_get_opt_block_size<FunctorType, LaunchBounds>(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:    const int maxGridSizeX = m_policy.space().cuda_device_prop().maxGridSize[0];
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:#ifdef KOKKOS_IMPL_DEBUG_CUDA_SERIAL_EXECUTION
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:    if (Kokkos::Impl::CudaInternal::cuda_use_serial_execution()) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:    CudaParallelLaunch<ParallelFor, LaunchBounds>(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:                     Kokkos::Cuda> {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:  // smaller than int32_t (Kokkos::Cuda::size_type)
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:      sizeof(value_type) < sizeof(Kokkos::Cuda::size_type),
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:      Kokkos::Cuda::size_type>;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:  // m_scratch_flags must be of type Cuda::size_type due to use of atomics
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:  // for tracking metadata in Kokkos_Cuda_ReduceScan.hpp
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:  Cuda::size_type* m_scratch_flags;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:  // FIXME_CUDA Shall we use the shfl based reduction or not (only use it for
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:              kokkos_impl_cuda_shared_memory<word_size_type>() +
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:      do_final_reduction = cuda_single_inter_block_reduce_scan<false>(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:          kokkos_impl_cuda_shared_memory<word_size_type>(), m_scratch_space,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:          kokkos_impl_cuda_shared_memory<word_size_type>() +
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:      if (CudaTraits::WarpSize < word_count.value) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:        // Inside cuda_single_inter_block_reduce_scan() and final() above,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:    unsigned n = CudaTraits::WarpSize * 8;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:        m_policy.space().cuda_device_prop().sharedMemPerBlock;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:        cuda_single_inter_block_reduce_scan_shmem<false, WorkTag, value_type>(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:                             Policy, Kokkos::Cuda>;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:    cudaFuncAttributes attr = CudaParallelLaunch<closure_type, LaunchBounds>::
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:        get_cuda_func_attributes(m_policy.space().cuda_device());
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:             Kokkos::Impl::cuda_get_max_block_size<FunctorType, LaunchBounds>(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:          cuda_single_inter_block_reduce_scan_shmem<false, WorkTag, value_type>(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:      m_scratch_space = (word_size_type*)cuda_internal_scratch_space(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:      // Intentionally do not downcast to word_size_type since we use Cuda
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:      // atomics in Kokkos_Cuda_ReduceScan.hpp
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:      m_scratch_flags = cuda_internal_scratch_flags(m_policy.space(),
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:                                                    sizeof(Cuda::size_type));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:          reinterpret_cast<word_size_type*>(cuda_internal_scratch_unified(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:              : cuda_single_inter_block_reduce_scan_shmem<false, WorkTag,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:#ifdef KOKKOS_IMPL_DEBUG_CUDA_SERIAL_EXECUTION
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:          || Kokkos::Impl::CudaInternal::cuda_use_serial_execution()
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:      CudaParallelLaunch<ParallelReduce, LaunchBounds>(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:                "Kokkos::Impl::ParallelReduce<Cuda, RangePolicy>::execute: "
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:            DeepCopy<HostSpace, CudaSpace, Cuda>(m_policy.space(), m_result_ptr,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:            MemorySpaceAccess<Kokkos::CudaSpace,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:class ParallelScan<FunctorType, Kokkos::RangePolicy<Traits...>, Kokkos::Cuda> {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:  using size_type      = Cuda::size_type;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:  // smaller than int32_t (Kokkos::Cuda::size_type)
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:#ifdef KOKKOS_IMPL_DEBUG_CUDA_SERIAL_EXECUTION
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:        kokkos_impl_cuda_shared_memory<word_size_type>() +
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:    cuda_single_inter_block_reduce_scan<true>(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:        kokkos_impl_cuda_shared_memory<word_size_type>(), m_scratch_space,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:        kokkos_impl_cuda_shared_memory<word_size_type>();
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:      if (CudaTraits::WarpSize < word_count.value) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:      cuda_intra_block_reduce_scan<true>(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:#ifdef KOKKOS_IMPL_DEBUG_CUDA_SERIAL_EXECUTION
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:#ifdef KOKKOS_IMPL_DEBUG_CUDA_SERIAL_EXECUTION
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:        m_policy.space().cuda_device_prop().sharedMemPerBlock;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:    unsigned n = CudaTraits::WarpSize * 4;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:               cuda_single_inter_block_reduce_scan_shmem<true, WorkTag,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:          reinterpret_cast<word_size_type*>(cuda_internal_scratch_space(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:          cuda_internal_scratch_flags(m_policy.space(), sizeof(size_type) * 1);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:#ifdef KOKKOS_IMPL_DEBUG_CUDA_SERIAL_EXECUTION
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:        CudaParallelLaunch<ParallelScan, LaunchBounds>(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:#ifdef KOKKOS_IMPL_DEBUG_CUDA_SERIAL_EXECUTION
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:      CudaParallelLaunch<ParallelScan, LaunchBounds>(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:#ifdef KOKKOS_IMPL_DEBUG_CUDA_SERIAL_EXECUTION
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:        m_run_serial(Kokkos::Impl::CudaInternal::cuda_use_serial_execution())
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:                            ReturnType, Kokkos::Cuda> {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:  using size_type      = Cuda::size_type;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:  // smaller than int32_t (Kokkos::Cuda::size_type)
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:#ifdef KOKKOS_IMPL_DEBUG_CUDA_SERIAL_EXECUTION
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:        kokkos_impl_cuda_shared_memory<word_size_type>() +
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:    cuda_single_inter_block_reduce_scan<true>(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:        kokkos_impl_cuda_shared_memory<word_size_type>(), m_scratch_space,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:        kokkos_impl_cuda_shared_memory<word_size_type>();
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:      if (CudaTraits::WarpSize < word_count.value) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:      cuda_intra_block_reduce_scan<true>(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:#ifdef KOKKOS_IMPL_DEBUG_CUDA_SERIAL_EXECUTION
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:#ifdef KOKKOS_IMPL_DEBUG_CUDA_SERIAL_EXECUTION
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:        m_policy.space().cuda_device_prop().sharedMemPerBlock;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:    unsigned n = CudaTraits::WarpSize * 4;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:               cuda_single_inter_block_reduce_scan_shmem<true, WorkTag,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:          reinterpret_cast<word_size_type*>(cuda_internal_scratch_space(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:          cuda_internal_scratch_flags(m_policy.space(), sizeof(size_type) * 1);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:#ifdef KOKKOS_IMPL_DEBUG_CUDA_SERIAL_EXECUTION
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:        CudaParallelLaunch<ParallelScanWithTotal, LaunchBounds>(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:      CudaParallelLaunch<ParallelScanWithTotal, LaunchBounds>(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:#ifdef KOKKOS_IMPL_DEBUG_CUDA_SERIAL_EXECUTION
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:        DeepCopy<HostSpace, CudaSpace, Cuda>(m_policy.space(), &m_returnvalue,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:          DeepCopy<HostSpace, CudaSpace, Cuda>(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:            MemorySpaceAccess<Kokkos::CudaSpace,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:#ifdef KOKKOS_IMPL_DEBUG_CUDA_SERIAL_EXECUTION
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Range.hpp:        m_run_serial(Kokkos::Impl::CudaInternal::cuda_use_serial_execution())
lib/kokkos/core/src/Cuda/Kokkos_Cuda_View.hpp:#ifndef KOKKOS_EXPERIMENTAL_CUDA_VIEW_HPP
lib/kokkos/core/src/Cuda/Kokkos_Cuda_View.hpp:#define KOKKOS_EXPERIMENTAL_CUDA_VIEW_HPP
lib/kokkos/core/src/Cuda/Kokkos_Cuda_View.hpp:#if defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/core/src/Cuda/Kokkos_Cuda_View.hpp:struct CudaLDGFetch {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_View.hpp:  CudaLDGFetch() = default;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_View.hpp:  explicit CudaLDGFetch(const ValueType* const arg_ptr) : m_ptr(arg_ptr) {}
lib/kokkos/core/src/Cuda/Kokkos_Cuda_View.hpp:  CudaLDGFetch(CudaLDGFetch const rhs, size_t offset)
lib/kokkos/core/src/Cuda/Kokkos_Cuda_View.hpp:/** \brief  Replace Default ViewDataHandle with CudaLDGFetch
lib/kokkos/core/src/Cuda/Kokkos_Cuda_View.hpp: * specialization if 'const' value type, CudaSpace and random access.
lib/kokkos/core/src/Cuda/Kokkos_Cuda_View.hpp:                // Is Cuda memory space
lib/kokkos/core/src/Cuda/Kokkos_Cuda_View.hpp:                              Kokkos::CudaSpace>::value ||
lib/kokkos/core/src/Cuda/Kokkos_Cuda_View.hpp:                              Kokkos::CudaUVMSpace>::value) &&
lib/kokkos/core/src/Cuda/Kokkos_Cuda_View.hpp:  using handle_type = Kokkos::Impl::CudaLDGFetch<value_type, alias_type>;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_View.hpp:#endif /* #if defined( KOKKOS_ENABLE_CUDA ) */
lib/kokkos/core/src/Cuda/Kokkos_Cuda_View.hpp:#endif /* #ifndef KOKKOS_CUDA_VIEW_HPP */
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:#ifndef KOKKOS_CUDASPACE_HPP
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:#define KOKKOS_CUDASPACE_HPP
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:#if defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:#include <Cuda/Kokkos_Cuda_abort.hpp>
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:#ifdef KOKKOS_IMPL_DEBUG_CUDA_PIN_UVM_TO_HOST
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:extern "C" bool kokkos_impl_cuda_pin_uvm_to_host();
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:extern "C" void kokkos_impl_cuda_set_pin_uvm_to_host(bool);
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:struct is_cuda_type_space : public std::false_type {};
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:/** \brief  Cuda on-device memory management */
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:class CudaSpace {
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  using memory_space    = CudaSpace;
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  using execution_space = Kokkos::Cuda;
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  CudaSpace();
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  CudaSpace(int device_id, cudaStream_t stream);
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  CudaSpace(CudaSpace&& rhs)      = default;
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  CudaSpace(const CudaSpace& rhs) = default;
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  CudaSpace& operator=(CudaSpace&& rhs) = default;
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  CudaSpace& operator=(const CudaSpace& rhs) = default;
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  ~CudaSpace()                               = default;
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  /**\brief  Allocate untracked memory in the cuda space */
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  void* allocate(const Cuda& exec_space, const size_t arg_alloc_size) const;
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  void* allocate(const Cuda& exec_space, const char* arg_label,
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:#if defined(KOKKOS_ENABLE_IMPL_CUDA_UNIFIED_MEMORY)
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  /**\brief  Deallocate untracked memory in the cuda space */
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  static CudaSpace impl_create(int device_id, cudaStream_t stream) {
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:    return CudaSpace(device_id, stream);
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  void* impl_allocate(const Cuda& exec_space, const char* arg_label,
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  cudaStream_t m_stream;
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  static constexpr const char* m_name = "Cuda";
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:struct Impl::is_cuda_type_space<CudaSpace> : public std::true_type {};
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:/** \brief  Cuda memory that is accessible to Host execution space
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp: *          through Cuda's unified virtual memory (UVM) runtime.
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:class CudaUVMSpace {
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  using memory_space    = CudaUVMSpace;
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  using execution_space = Cuda;
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  CudaUVMSpace();
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  CudaUVMSpace(int device_id, cudaStream_t stream);
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  CudaUVMSpace(CudaUVMSpace&& rhs)      = default;
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  CudaUVMSpace(const CudaUVMSpace& rhs) = default;
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  CudaUVMSpace& operator=(CudaUVMSpace&& rhs) = default;
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  CudaUVMSpace& operator=(const CudaUVMSpace& rhs) = default;
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  ~CudaUVMSpace()                                  = default;
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  /**\brief  Allocate untracked memory in the cuda space */
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  /**\brief  Deallocate untracked memory in the cuda space */
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:#ifdef KOKKOS_IMPL_DEBUG_CUDA_PIN_UVM_TO_HOST
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  static bool cuda_pin_uvm_to_host();
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  static void cuda_set_pin_uvm_to_host(bool val);
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  static CudaUVMSpace impl_create(int device_id, cudaStream_t stream) {
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:    return CudaUVMSpace(device_id, stream);
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  cudaStream_t m_stream;
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:#ifdef KOKKOS_IMPL_DEBUG_CUDA_PIN_UVM_TO_HOST
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  static bool kokkos_impl_cuda_pin_uvm_to_host_v;
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  static constexpr const char* m_name = "CudaUVM";
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:struct Impl::is_cuda_type_space<CudaUVMSpace> : public std::true_type {};
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:/** \brief  Host memory that is accessible to Cuda execution space
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp: *          through Cuda's host-pinned memory allocation.
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:class CudaHostPinnedSpace {
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  using memory_space    = CudaHostPinnedSpace;
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  CudaHostPinnedSpace();
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  CudaHostPinnedSpace(int device_id, cudaStream_t stream);
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  CudaHostPinnedSpace(CudaHostPinnedSpace&& rhs)      = default;
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  CudaHostPinnedSpace(const CudaHostPinnedSpace& rhs) = default;
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  CudaHostPinnedSpace& operator=(CudaHostPinnedSpace&& rhs) = default;
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  CudaHostPinnedSpace& operator=(const CudaHostPinnedSpace& rhs) = default;
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  ~CudaHostPinnedSpace()                                         = default;
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  static CudaHostPinnedSpace impl_create(int device_id, cudaStream_t stream) {
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:    return CudaHostPinnedSpace(device_id, stream);
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  cudaStream_t m_stream;
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  static constexpr const char* m_name = "CudaHostPinned";
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:struct Impl::is_cuda_type_space<CudaHostPinnedSpace> : public std::true_type {};
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:cudaStream_t cuda_get_deep_copy_stream();
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:const std::unique_ptr<Kokkos::Cuda>& cuda_get_deep_copy_space(
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:static_assert(Kokkos::Impl::MemorySpaceAccess<Kokkos::CudaSpace,
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:                                              Kokkos::CudaSpace>::assignable);
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:              Kokkos::CudaUVMSpace, Kokkos::CudaUVMSpace>::assignable);
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:    Kokkos::Impl::MemorySpaceAccess<Kokkos::CudaHostPinnedSpace,
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:                                    Kokkos::CudaHostPinnedSpace>::assignable);
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:struct MemorySpaceAccess<Kokkos::HostSpace, Kokkos::CudaSpace> {
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:#if !defined(KOKKOS_ENABLE_IMPL_CUDA_UNIFIED_MEMORY)
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:struct MemorySpaceAccess<Kokkos::HostSpace, Kokkos::CudaUVMSpace> {
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  // HostSpace::execution_space != CudaUVMSpace::execution_space
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:struct MemorySpaceAccess<Kokkos::HostSpace, Kokkos::CudaHostPinnedSpace> {
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  // HostSpace::execution_space == CudaHostPinnedSpace::execution_space
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:struct MemorySpaceAccess<Kokkos::CudaSpace, Kokkos::HostSpace> {
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:struct MemorySpaceAccess<Kokkos::CudaSpace, Kokkos::CudaUVMSpace> {
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  // CudaSpace::execution_space == CudaUVMSpace::execution_space
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:struct MemorySpaceAccess<Kokkos::CudaSpace, Kokkos::CudaHostPinnedSpace> {
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  // CudaSpace::execution_space != CudaHostPinnedSpace::execution_space
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  enum : bool { accessible = true };  // CudaSpace::execution_space
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:// CudaUVMSpace::execution_space == Cuda
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:// CudaUVMSpace accessible to both Cuda and Host
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:struct MemorySpaceAccess<Kokkos::CudaUVMSpace, Kokkos::HostSpace> {
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  enum : bool { accessible = false };  // Cuda cannot access HostSpace
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:struct MemorySpaceAccess<Kokkos::CudaUVMSpace, Kokkos::CudaSpace> {
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  // CudaUVMSpace::execution_space == CudaSpace::execution_space
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  // Can access CudaUVMSpace from Host but cannot access CudaSpace from Host
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  // CudaUVMSpace::execution_space can access CudaSpace
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:struct MemorySpaceAccess<Kokkos::CudaUVMSpace, Kokkos::CudaHostPinnedSpace> {
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  // CudaUVMSpace::execution_space != CudaHostPinnedSpace::execution_space
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  enum : bool { accessible = true };  // CudaUVMSpace::execution_space
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:// CudaHostPinnedSpace::execution_space == HostSpace::execution_space
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:// CudaHostPinnedSpace accessible to both Cuda and Host
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:struct MemorySpaceAccess<Kokkos::CudaHostPinnedSpace, Kokkos::HostSpace> {
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  enum : bool { assignable = false };  // Cannot access from Cuda
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  enum : bool { accessible = true };   // CudaHostPinnedSpace::execution_space
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:struct MemorySpaceAccess<Kokkos::CudaHostPinnedSpace, Kokkos::CudaSpace> {
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:struct MemorySpaceAccess<Kokkos::CudaHostPinnedSpace, Kokkos::CudaUVMSpace> {
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:void DeepCopyCuda(void* dst, const void* src, size_t n);
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:void DeepCopyAsyncCuda(const Cuda& instance, void* dst, const void* src,
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:void DeepCopyAsyncCuda(void* dst, const void* src, size_t n);
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:struct DeepCopy<MemSpace, HostSpace, Cuda,
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:                std::enable_if_t<is_cuda_type_space<MemSpace>::value>> {
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  DeepCopy(void* dst, const void* src, size_t n) { DeepCopyCuda(dst, src, n); }
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  DeepCopy(const Cuda& instance, void* dst, const void* src, size_t n) {
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:    DeepCopyAsyncCuda(instance, dst, src, n);
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:struct DeepCopy<HostSpace, MemSpace, Cuda,
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:                std::enable_if_t<is_cuda_type_space<MemSpace>::value>> {
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  DeepCopy(void* dst, const void* src, size_t n) { DeepCopyCuda(dst, src, n); }
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  DeepCopy(const Cuda& instance, void* dst, const void* src, size_t n) {
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:    DeepCopyAsyncCuda(instance, dst, src, n);
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:struct DeepCopy<MemSpace1, MemSpace2, Cuda,
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:                std::enable_if_t<is_cuda_type_space<MemSpace1>::value &&
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:                                 is_cuda_type_space<MemSpace2>::value>> {
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  DeepCopy(void* dst, const void* src, size_t n) { DeepCopyCuda(dst, src, n); }
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:  DeepCopy(const Cuda& instance, void* dst, const void* src, size_t n) {
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:    DeepCopyAsyncCuda(instance, dst, src, n);
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:                std::enable_if_t<is_cuda_type_space<MemSpace1>::value &&
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:                                 is_cuda_type_space<MemSpace2>::value &&
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:                                 !std::is_same<ExecutionSpace, Cuda>::value>> {
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:    DeepCopyCuda(dst, src, n);
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:    DeepCopyAsyncCuda(dst, src, n);
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:                std::enable_if_t<is_cuda_type_space<MemSpace>::value &&
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:                                 !std::is_same<ExecutionSpace, Cuda>::value>> {
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:    DeepCopyCuda(dst, src, n);
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:    DeepCopyAsyncCuda(dst, src, n);
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:                std::enable_if_t<is_cuda_type_space<MemSpace>::value &&
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:                                 !std::is_same<ExecutionSpace, Cuda>::value>> {
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:    DeepCopyCuda(dst, src, n);
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:    DeepCopyAsyncCuda(dst, src, n);
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:#if !defined(KOKKOS_ENABLE_IMPL_CUDA_UNIFIED_MEMORY)
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:    Kokkos::CudaSpace);
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:KOKKOS_IMPL_SHARED_ALLOCATION_SPECIALIZATION(Kokkos::CudaSpace);
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:KOKKOS_IMPL_SHARED_ALLOCATION_SPECIALIZATION(Kokkos::CudaUVMSpace);
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:KOKKOS_IMPL_SHARED_ALLOCATION_SPECIALIZATION(Kokkos::CudaHostPinnedSpace);
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:#endif /* #if defined( KOKKOS_ENABLE_CUDA ) */
lib/kokkos/core/src/Cuda/Kokkos_CudaSpace.hpp:#endif /* #define KOKKOS_CUDASPACE_HPP */
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:#ifndef KOKKOS_CUDA_PARALLEL_TEAM_HPP
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:#define KOKKOS_CUDA_PARALLEL_TEAM_HPP
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:#if defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:#include <Cuda/Kokkos_Cuda_KernelLaunch.hpp>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:#include <Cuda/Kokkos_Cuda_ReduceScan.hpp>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:#include <Cuda/Kokkos_Cuda_BlockSize_Deduction.hpp>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:#include <Cuda/Kokkos_Cuda_Team.hpp>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:#include <impl/KokkosExp_IterateTileGPU.hpp>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:class TeamPolicyInternal<Kokkos::Cuda, Properties...>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:  using execution_space = Kokkos::Cuda;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:    cudaFuncAttributes attr =
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:        CudaParallelLaunch<closure_type, typename traits::launch_bounds>::
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:            get_cuda_func_attributes(space().cuda_device());
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:        Kokkos::Impl::cuda_get_max_block_size<FunctorType,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:        TeamPolicy<Properties...>, Kokkos::Cuda>;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:                             TeamPolicy<Properties...>, Kokkos::Cuda>;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:    cudaFuncAttributes attr =
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:        CudaParallelLaunch<closure_type, typename traits::launch_bounds>::
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:            get_cuda_func_attributes(space().cuda_device());
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:        Kokkos::Impl::cuda_get_opt_block_size<FunctorType,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:        TeamPolicy<Properties...>, Kokkos::Cuda>;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:                             TeamPolicy<Properties...>, Kokkos::Cuda>;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:  inline static int vector_length_max() { return Impl::CudaTraits::WarpSize; }
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:    // Cuda Teams use (team_size + 2)*sizeof(double) shared memory for team
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:    size_t max_shmem = Cuda().cuda_device_prop().sharedMemPerBlock;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:        m_chunk_size(Impl::CudaTraits::WarpSize),
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:        m_chunk_size(Impl::CudaTraits::WarpSize),
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:    const int maxGridSizeX = m_space.cuda_device_prop().maxGridSize[0];
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:          "Requested too large league_size for TeamPolicy on Cuda execution "
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:        int(Impl::CudaTraits::MaxHierarchicalParallelism)) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:          std::string("Kokkos::TeamPolicy< Cuda > the team size is too large. "
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:  using member_type = Kokkos::Impl::CudaTeamMember;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:    cudaFuncAttributes attr =
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:        CudaParallelLaunch<closure_type, typename traits::launch_bounds>::
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:            get_cuda_func_attributes(space().cuda_device());
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:        Kokkos::Impl::cuda_get_max_block_size<FunctorType,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:        Kokkos::Impl::cuda_get_opt_block_size<FunctorType,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:__device__ inline int64_t cuda_get_scratch_index(Cuda::size_type league_size,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:__device__ inline void cuda_release_scratch_index(int32_t* scratch_locks,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:                  Kokkos::Cuda> {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:  using size_type    = Cuda::size_type;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:      threadid = cuda_get_scratch_index(m_league_size, m_scratch_locks,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:          kokkos_impl_cuda_shared_memory<void>(), m_shmem_begin, m_shmem_size,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:      cuda_release_scratch_index(m_scratch_locks, threadid);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:#ifdef KOKKOS_IMPL_DEBUG_CUDA_SERIAL_EXECUTION
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:    if (Kokkos::Impl::CudaInternal::cuda_use_serial_execution()) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:    CudaParallelLaunch<ParallelFor, LaunchBounds>(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:                  static_cast<std::int64_t>(Cuda().concurrency() /
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:        m_policy.space().cuda_device_prop().sharedMemPerBlock;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:          "Kokkos::Impl::ParallelFor< Cuda > insufficient shared memory"));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:          "Kokkos::Impl::ParallelFor< Cuda > requested too large team size."));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:                     Kokkos::TeamPolicy<Properties...>, Kokkos::Cuda> {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:  // smaller than int32_t (Kokkos::Cuda::size_type)
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:      sizeof(value_type) < sizeof(Kokkos::Cuda::size_type),
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:      Kokkos::Cuda::size_type>;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:  using size_type    = Cuda::size_type;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:  // m_scratch_flags must be of type Cuda::size_type due to use of atomics
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:  // for tracking metadata in Kokkos_Cuda_ReduceScan.hpp
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:  Cuda::size_type* m_scratch_flags;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:      threadid = cuda_get_scratch_index(m_league_size, m_scratch_locks,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:      cuda_release_scratch_index(m_scratch_locks, threadid);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:        kokkos_impl_cuda_shared_memory<word_size_type>() +
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:          Member(kokkos_impl_cuda_shared_memory<char>() + m_team_begin,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:      do_final_reduction = cuda_single_inter_block_reduce_scan<false>(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:          kokkos_impl_cuda_shared_memory<word_size_type>(), m_scratch_space,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:          kokkos_impl_cuda_shared_memory<word_size_type>() +
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:      if (CudaTraits::WarpSize < word_count.value) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:          Member(kokkos_impl_cuda_shared_memory<char>() + m_team_begin,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:    } else if (Impl::cuda_inter_block_reduction(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:          reinterpret_cast<word_size_type*>(cuda_internal_scratch_space(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:          cuda_internal_scratch_flags(m_policy.space(), sizeof(size_type));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:          reinterpret_cast<word_size_type*>(cuda_internal_scratch_unified(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:#ifdef KOKKOS_IMPL_DEBUG_CUDA_SERIAL_EXECUTION
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:          || Kokkos::Impl::CudaInternal::cuda_use_serial_execution()
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:      CudaParallelLaunch<ParallelReduce, LaunchBounds>(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:            "Kokkos::Impl::ParallelReduce<Cuda, TeamPolicy>::execute: Result "
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:            DeepCopy<HostSpace, CudaSpace, Cuda>(m_policy.space(), m_result_ptr,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:            MemorySpaceAccess<Kokkos::CudaSpace,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:            : cuda_single_inter_block_reduce_scan_shmem<false, WorkTag,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:                  static_cast<std::int64_t>(Cuda().concurrency() /
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:          "greater than 1 is not currently supported for CUDA for dynamic "
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:          "than 32 is not currently supported with CUDA for dynamic sized "
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:        m_policy.space().cuda_device_prop().sharedMemPerBlock;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:          std::string("Kokkos::Impl::ParallelReduce< Cuda > bad team size"));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:          std::string("Kokkos::Impl::ParallelReduce< Cuda > requested too much "
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:          std::string("Kokkos::Impl::ParallelReduce< Cuda > requested too "
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:#endif /* defined(KOKKOS_ENABLE_CUDA) */
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Parallel_Team.hpp:#endif /* #ifndef KOKKOS_CUDA_PARALLEL_HPP */
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:#ifdef KOKKOS_ENABLE_CUDA
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp://#include <Cuda/Kokkos_Cuda_Error.hpp>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp://#include <Cuda/Kokkos_Cuda_BlockSize_Deduction.hpp>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp://#include <Cuda/Kokkos_Cuda_Instance.hpp>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp://#include <Cuda/Kokkos_Cuda_UniqueToken.hpp>
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:#ifdef KOKKOS_IMPL_DEBUG_CUDA_SERIAL_EXECUTION
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:bool CudaInternal::kokkos_impl_cuda_use_serial_execution_v = false;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:void CudaInternal::cuda_set_serial_execution(bool val) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  CudaInternal::kokkos_impl_cuda_use_serial_execution_v = val;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:bool CudaInternal::cuda_use_serial_execution() {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  return CudaInternal::kokkos_impl_cuda_use_serial_execution_v;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:void kokkos_impl_cuda_set_serial_execution(bool val) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  Kokkos::Impl::CudaInternal::cuda_set_serial_execution(val);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:bool kokkos_impl_cuda_use_serial_execution() {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  return Kokkos::Impl::CudaInternal::cuda_use_serial_execution();
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:#ifdef KOKKOS_ENABLE_CUDA_RELOCATABLE_DEVICE_CODE
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:__device__ __constant__ unsigned long kokkos_impl_cuda_constant_memory_buffer
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:    [Kokkos::Impl::CudaTraits::ConstantMemoryUsage / sizeof(unsigned long)];
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:__global__ void query_cuda_kernel_arch(int *d_arch) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:#ifdef _NVHPC_CUDA
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:#if defined(__CUDA_ARCH__)
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  *d_arch = __CUDA_ARCH__;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:int cuda_kernel_arch(int device_id) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  KOKKOS_IMPL_CUDA_SAFE_CALL(cudaSetDevice(device_id));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  KOKKOS_IMPL_CUDA_SAFE_CALL(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:      cudaMalloc(reinterpret_cast<void **>(&d_arch), sizeof(int)));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  KOKKOS_IMPL_CUDA_SAFE_CALL(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:      cudaMemcpy(d_arch, &arch, sizeof(int), cudaMemcpyDefault));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  query_cuda_kernel_arch<<<1, 1>>>(d_arch);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  KOKKOS_IMPL_CUDA_SAFE_CALL(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:      cudaMemcpy(&arch, d_arch, sizeof(int), cudaMemcpyDefault));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  KOKKOS_IMPL_CUDA_SAFE_CALL(cudaFree(d_arch));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:    sizeof(Cuda::size_type[Impl::CudaTraits::WarpSize]);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:Kokkos::View<uint32_t *, Kokkos::CudaSpace> cuda_global_unique_token_locks(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  static Kokkos::View<uint32_t *, Kokkos::CudaSpace> locks =
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:      Kokkos::View<uint32_t *, Kokkos::CudaSpace>();
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:    locks = Kokkos::View<uint32_t *, Kokkos::CudaSpace>(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:        "Kokkos::UniqueToken<Cuda>::m_locks", Kokkos::Cuda().concurrency());
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  if (deallocate) locks = Kokkos::View<uint32_t *, Kokkos::CudaSpace>();
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:void cuda_device_synchronize(const std::string &name) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  Kokkos::Tools::Experimental::Impl::profile_fence_event<Kokkos::Cuda>(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:      // cudaDeviceSynchronize in device code
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:        for (int cuda_device : Kokkos::Impl::CudaInternal::cuda_devices) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:          KOKKOS_IMPL_CUDA_SAFE_CALL(cudaSetDevice(cuda_device));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:          KOKKOS_IMPL_CUDA_SAFE_CALL(cudaDeviceSynchronize());
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:void cuda_stream_synchronize(const cudaStream_t stream, const CudaInternal *ptr,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  Kokkos::Tools::Experimental::Impl::profile_fence_event<Kokkos::Cuda>(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:        KOKKOS_IMPL_CUDA_SAFE_CALL(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:            (ptr->cuda_stream_synchronize_wrapper(stream)));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:void cuda_internal_error_throw(cudaError e, const char *name, const char *file,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  out << name << " error( " << cudaGetErrorName(e)
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:      << "): " << cudaGetErrorString(e);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:void cuda_internal_error_abort(cudaError e, const char *name, const char *file,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  out << name << " error( " << cudaGetErrorName(e)
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:      << "): " << cudaGetErrorString(e);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:int Impl::CudaInternal::concurrency() {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:void CudaInternal::print_configuration(std::ostream &s) const {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:#if defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  s << "macro  KOKKOS_ENABLE_CUDA      : defined\n";
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:#if defined(CUDA_VERSION)
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  s << "macro  CUDA_VERSION          = " << CUDA_VERSION << " = version "
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:    << CUDA_VERSION / 1000 << "." << (CUDA_VERSION % 1000) / 10 << '\n';
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:    cudaDeviceProp prop;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:    KOKKOS_IMPL_CUDA_SAFE_CALL(cudaGetDeviceProperties(&prop, i));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:    s << "Kokkos::Cuda[ " << i << " ] " << prop.name << " capability "
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:    if (m_cudaDev == i) s << " : Selected";
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:CudaInternal::~CudaInternal() {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:    std::cerr << "Kokkos::Cuda ERROR: Failed to call Kokkos::Cuda::finalize()"
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:int CudaInternal::verify_is_initialized(const char *const label) const {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  if (m_cudaDev < 0) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:    Kokkos::abort((std::string("Kokkos::Cuda::") + label +
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  return 0 <= m_cudaDev;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:uint32_t CudaInternal::impl_get_instance_id() const { return m_instance_id; }
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:CudaInternal &CudaInternal::singleton() {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  static CudaInternal self;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:void CudaInternal::fence(const std::string &name) const {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  Impl::cuda_stream_synchronize(get_stream(), this, name);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:void CudaInternal::fence() const {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  fence("Kokkos::CudaInternal::fence(): Unnamed Instance Fence");
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:void CudaInternal::initialize(cudaStream_t stream) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:    Kokkos::abort("Calling Cuda::initialize after Cuda::finalize is illegal\n");
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  // Check that the device associated with the stream matches cuda_device
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  KOKKOS_IMPL_CUDA_SAFE_CALL(cudaError_t(cuStreamGetCtx(stream, &context)));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  KOKKOS_IMPL_CUDA_SAFE_CALL(cudaError_t(cuCtxPushCurrent(context)));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  KOKKOS_IMPL_CUDA_SAFE_CALL(cudaError_t(cuCtxGetDevice(&m_cudaDev)));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  KOKKOS_IMPL_CUDA_SAFE_CALL(cudaSetDevice(m_cudaDev));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  CudaInternal::cuda_devices.insert(m_cudaDev);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  if (!constantMemHostStagingPerDevice[m_cudaDev])
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:    KOKKOS_IMPL_CUDA_SAFE_CALL((cuda_malloc_host_wrapper(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:        reinterpret_cast<void **>(&constantMemHostStagingPerDevice[m_cudaDev]),
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:        CudaTraits::ConstantMemoryUsage)));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  if (!constantMemReusablePerDevice[m_cudaDev])
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:    KOKKOS_IMPL_CUDA_SAFE_CALL(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:        (cuda_event_create_wrapper(&constantMemReusablePerDevice[m_cudaDev])));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:        m_deviceProp.maxThreadsPerBlock / CudaTraits::WarpSize,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:        CudaTraits::WarpSize);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:        maxWarpCount * Impl::CudaTraits::WarpSize;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  KOKKOS_IMPL_CUDA_SAFE_CALL(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:      (cuda_malloc_wrapper(reinterpret_cast<void **>(&m_scratch_locks),
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  KOKKOS_IMPL_CUDA_SAFE_CALL((cuda_memset_wrapper(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:Cuda::size_type *CudaInternal::scratch_flags(const std::size_t size) const {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:    auto mem_space = Kokkos::CudaSpace::impl_create(m_cudaDev, m_stream);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:    KOKKOS_IMPL_CUDA_SAFE_CALL(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:        (cuda_memset_wrapper(m_scratchFlags, 0, alloc_size)));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:Cuda::size_type *CudaInternal::scratch_space(const std::size_t size) const {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:    auto mem_space = Kokkos::CudaSpace::impl_create(m_cudaDev, m_stream);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:Cuda::size_type *CudaInternal::scratch_unified(const std::size_t size) const {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:        Kokkos::CudaHostPinnedSpace::impl_create(m_cudaDev, m_stream);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:Cuda::size_type *CudaInternal::scratch_functor(const std::size_t size) const {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:    auto mem_space = Kokkos::CudaSpace::impl_create(m_cudaDev, m_stream);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:int CudaInternal::acquire_team_scratch_space() {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:void *CudaInternal::resize_team_scratch_space(int scratch_pool_id,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  auto mem_space = Kokkos::CudaSpace::impl_create(m_cudaDev, m_stream);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:        mem_space.allocate("Kokkos::CudaSpace::TeamScratchMemory",
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:        mem_space.allocate("Kokkos::CudaSpace::TeamScratchMemory", bytes);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:void CudaInternal::release_team_scratch_space(int scratch_pool_id) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:void CudaInternal::finalize() {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  auto cuda_mem_space = Kokkos::CudaSpace::impl_create(m_cudaDev, m_stream);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:        Kokkos::CudaHostPinnedSpace::impl_create(m_cudaDev, m_stream);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:    cuda_mem_space.deallocate(m_scratchFlags,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:    cuda_mem_space.deallocate(m_scratchSpace,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:      cuda_mem_space.deallocate(m_scratchFunctor, m_scratchFunctorSize);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:      cuda_mem_space.deallocate(m_team_scratch_ptr[i],
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  KOKKOS_IMPL_CUDA_SAFE_CALL((cuda_free_wrapper(m_scratch_locks)));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:Cuda::size_type *cuda_internal_scratch_space(const Cuda &instance,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:Cuda::size_type *cuda_internal_scratch_flags(const Cuda &instance,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:Cuda::size_type *cuda_internal_scratch_unified(const Cuda &instance,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:int Cuda::concurrency() {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:int Cuda::concurrency() const {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  return Impl::CudaInternal::concurrency();
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:int Cuda::impl_is_initialized() {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  return Impl::CudaInternal::singleton().is_initialized();
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:void Cuda::impl_initialize(InitializationSettings const &settings) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  const int cuda_device_id =
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:      Impl::get_gpu(settings).value_or(visible_devices[0]);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  cudaDeviceProp cudaProp;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  KOKKOS_IMPL_CUDA_SAFE_CALL(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:      cudaGetDeviceProperties(&cudaProp, cuda_device_id));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  Impl::CudaInternal::m_deviceProp = cudaProp;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  KOKKOS_IMPL_CUDA_SAFE_CALL(cudaSetDevice(cuda_device_id));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  KOKKOS_IMPL_CUDA_SAFE_CALL(cudaDeviceSynchronize());
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  Impl::CudaInternal::m_cudaArch = Impl::cuda_kernel_arch(cuda_device_id);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  if (Impl::CudaInternal::m_cudaArch == 0) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:        "Kokkos::Cuda::initialize ERROR: likely mismatch of architecture\n");
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  int compiled_major = Impl::CudaInternal::m_cudaArch / 100;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  int compiled_minor = (Impl::CudaInternal::m_cudaArch % 100) / 10;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  if ((compiled_major > cudaProp.major) ||
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:      ((compiled_major == cudaProp.major) &&
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:       (compiled_minor > cudaProp.minor))) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:    ss << "Kokkos::Cuda::initialize ERROR: running kernels compiled for "
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:       << " on device with compute capability " << cudaProp.major << "."
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:       << cudaProp.minor << " is not supported by CUDA!\n";
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:      (compiled_major != cudaProp.major || compiled_minor != cudaProp.minor)) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:    std::cerr << "Kokkos::Cuda::initialize WARNING: running kernels compiled "
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:              << " on device with compute capability " << cudaProp.major << "."
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:              << cudaProp.minor
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:#ifdef KOKKOS_ENABLE_CUDA_UVM
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:      getenv("CUDA_MANAGED_FORCE_DEVICE_ALLOC");
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  const char *env_visible_devices = getenv("CUDA_VISIBLE_DEVICES");
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:Kokkos::Cuda::initialize WARNING: Cuda is allocating into UVMSpace by default
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:                                  without setting CUDA_MANAGED_FORCE_DEVICE_ALLOC=1 or
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:                                  setting CUDA_VISIBLE_DEVICES.
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:                                  This could on multi GPU systems lead to severe performance"
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:#ifdef KOKKOS_ENABLE_IMPL_CUDA_UNIFIED_MEMORY
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  int cuda_result;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  cudaDeviceGetAttribute(&cuda_result, cudaDevAttrConcurrentManagedAccess,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:                         cuda_device_id);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  if (cuda_result == 0) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:        "Kokkos::Cuda::initialize ERROR: Unified memory is not available on "
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:        "-DKokkos_ENABLE_IMPL_CUDA_UNIFIED_MEMORY=OFF\n");
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  cudaStream_t singleton_stream;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  KOKKOS_IMPL_CUDA_SAFE_CALL(cudaSetDevice(cuda_device_id));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  KOKKOS_IMPL_CUDA_SAFE_CALL(cudaStreamCreate(&singleton_stream));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  Impl::CudaInternal::singleton().initialize(singleton_stream);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:void Cuda::impl_finalize() {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  (void)Impl::cuda_global_unique_token_locks(true);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  for (const auto cuda_device : Kokkos::Impl::CudaInternal::cuda_devices) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:    KOKKOS_IMPL_CUDA_SAFE_CALL(cudaSetDevice(cuda_device));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:    KOKKOS_IMPL_CUDA_SAFE_CALL(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:        cudaFreeHost(Kokkos::Impl::CudaInternal::constantMemHostStagingPerDevice
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:                         [cuda_device]));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:    KOKKOS_IMPL_CUDA_SAFE_CALL(cudaEventDestroy(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:        Kokkos::Impl::CudaInternal::constantMemReusablePerDevice[cuda_device]));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  auto &deep_copy_space = Impl::cuda_get_deep_copy_space(/*initialize*/ false);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  KOKKOS_IMPL_CUDA_SAFE_CALL(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:      cudaStreamDestroy(Impl::cuda_get_deep_copy_stream()));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  Impl::CudaInternal::singleton().finalize();
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  KOKKOS_IMPL_CUDA_SAFE_CALL(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:      cudaStreamDestroy(Impl::CudaInternal::singleton().m_stream));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:Cuda::Cuda()
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:    : m_space_instance(&Impl::CudaInternal::singleton(),
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:                       [](Impl::CudaInternal *) {}) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  Impl::CudaInternal::singleton().verify_is_initialized(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:      "Cuda instance constructor");
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:KOKKOS_DEPRECATED Cuda::Cuda(cudaStream_t stream, bool manage_stream)
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:    : Cuda(stream,
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:Cuda::Cuda(cudaStream_t stream, Impl::ManageStream manage_stream)
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:          new Impl::CudaInternal, [manage_stream](Impl::CudaInternal *ptr) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:              KOKKOS_IMPL_CUDA_SAFE_CALL(cudaStreamDestroy(ptr->m_stream));
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  Impl::CudaInternal::singleton().verify_is_initialized(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:      "Cuda instance constructor");
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:void Cuda::print_configuration(std::ostream &os, bool /*verbose*/) const {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  os << "  KOKKOS_ENABLE_CUDA: yes\n";
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  os << "Cuda Options:\n";
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  os << "  KOKKOS_ENABLE_CUDA_LAMBDA: ";
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:#ifdef KOKKOS_ENABLE_CUDA_LAMBDA
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  os << "  KOKKOS_ENABLE_CUDA_LDG_INTRINSIC: ";
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  os << "  KOKKOS_ENABLE_CUDA_RELOCATABLE_DEVICE_CODE: ";
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:#ifdef KOKKOS_ENABLE_CUDA_RELOCATABLE_DEVICE_CODE
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  os << "  KOKKOS_ENABLE_CUDA_UVM: ";
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:#ifdef KOKKOS_ENABLE_CUDA_UVM
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  os << "  KOKKOS_ENABLE_IMPL_CUDA_MALLOC_ASYNC: ";
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:#ifdef KOKKOS_ENABLE_IMPL_CUDA_MALLOC_ASYNC
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:#ifdef KOKKOS_ENABLE_IMPL_CUDA_UNIFIED_MEMORY
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  os << "  KOKKOS_ENABLE_IMPL_CUDA_UNIFIED_MEMORY: ";
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  os << "\nCuda Runtime Configuration:\n";
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:void Cuda::impl_static_fence(const std::string &name) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:  Kokkos::Impl::cuda_device_synchronize(name);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:void Cuda::fence(const std::string &name) const {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:const char *Cuda::name() { return "Cuda"; }
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:uint32_t Cuda::impl_instance_id() const noexcept {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:cudaStream_t Cuda::cuda_stream() const {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:int Cuda::cuda_device() const { return m_space_instance->m_cudaDev; }
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:const cudaDeviceProp &Cuda::cuda_device_prop() const {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:int g_cuda_space_factory_initialized =
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:    initialize_space_factory<Cuda>("150_Cuda");
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:int CudaInternal::m_cudaArch = -1;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:cudaDeviceProp CudaInternal::m_deviceProp;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:std::set<int> CudaInternal::cuda_devices = {};
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:std::map<int, unsigned long *> CudaInternal::constantMemHostStagingPerDevice =
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:std::map<int, cudaEvent_t> CudaInternal::constantMemReusablePerDevice = {};
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:std::map<int, std::mutex> CudaInternal::constantMemMutexPerDevice     = {};
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:void Kokkos::Impl::create_Cuda_instances(std::vector<Cuda> &instances) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:    cudaStream_t stream;
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:    KOKKOS_IMPL_CUDA_SAFE_CALL((
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:        instances[s].impl_internal_space_instance()->cuda_stream_create_wrapper(
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:    instances[s] = Cuda(stream, ManageStream::yes);
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:void KOKKOS_CORE_SRC_CUDA_IMPL_PREVENT_LINK_ERROR() {}
lib/kokkos/core/src/Cuda/Kokkos_Cuda_Instance.cpp:#endif  // KOKKOS_ENABLE_CUDA
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:#ifndef KOKKOS_CUDA_HPP
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:#define KOKKOS_CUDA_HPP
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:#if defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:#include <Cuda/Kokkos_CudaSpace.hpp>
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:#include <Cuda/Kokkos_Cuda_Error.hpp>  // CUDA_SAFE_CALL
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:class CudaInternal;
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:enum class CudaLaunchMechanism : unsigned {
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:constexpr inline CudaLaunchMechanism operator|(CudaLaunchMechanism p1,
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:                                               CudaLaunchMechanism p2) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:  return static_cast<CudaLaunchMechanism>(static_cast<unsigned>(p1) |
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:constexpr inline CudaLaunchMechanism operator&(CudaLaunchMechanism p1,
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:                                               CudaLaunchMechanism p2) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:  return static_cast<CudaLaunchMechanism>(static_cast<unsigned>(p1) &
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:template <CudaLaunchMechanism l>
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:struct CudaDispatchProperties {
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:  CudaLaunchMechanism launch_mechanism = l;
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:/// \class Cuda
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:/// \brief Kokkos Execution Space that uses CUDA to run on GPUs.
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:/// sequentially.  The Cuda execution space uses NVIDIA's CUDA programming
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:/// model to execute kernels in parallel on GPUs.
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:class Cuda {
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:  using execution_space = Cuda;
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:#if defined(KOKKOS_ENABLE_CUDA_UVM)
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:  using memory_space = CudaUVMSpace;
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:  using memory_space = CudaSpace;
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:  using scratch_memory_space = ScratchMemorySpace<Cuda>;
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:#if defined(__CUDA_ARCH__)
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:                 "Kokkos::Cuda::fence(): Unnamed Instance Fence") const;
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:  //! \name  Cuda space instances
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:  Cuda();
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:  explicit Cuda(cudaStream_t stream) : Cuda(stream, Impl::ManageStream::no) {}
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:      "Cuda execution space should be constructed explicitly.")
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:  Cuda(cudaStream_t stream)
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:      : Cuda(stream) {}
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:  Cuda(cudaStream_t stream, Impl::ManageStream manage_stream);
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:  KOKKOS_DEPRECATED Cuda(cudaStream_t stream, bool manage_stream);
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:  //! Initialize, telling the CUDA run-time library which device to use.
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:  /// \brief Cuda device architecture of the selected device.
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:  /// This matches the __CUDA_ARCH__ specification.
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:    const cudaDeviceProp cudaProp = Cuda().cuda_device_prop();
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:    return cudaProp.major * 100 + cudaProp.minor;
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:    KOKKOS_IMPL_CUDA_SAFE_CALL(cudaGetDeviceCount(&count));
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:   *          as defined by the __CUDA_ARCH__ specification.
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:    KOKKOS_IMPL_CUDA_SAFE_CALL(cudaGetDeviceCount(&count));
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:      cudaDeviceProp prop;
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:      KOKKOS_IMPL_CUDA_SAFE_CALL(cudaGetDeviceProperties(&prop, i));
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:  cudaStream_t cuda_stream() const;
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:  int cuda_device() const;
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:  const cudaDeviceProp& cuda_device_prop() const;
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:  inline Impl::CudaInternal* impl_internal_space_instance() const {
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:  friend bool operator==(Cuda const& lhs, Cuda const& rhs) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:  friend bool operator!=(Cuda const& lhs, Cuda const& rhs) {
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:  Kokkos::Impl::HostSharedPtr<Impl::CudaInternal> m_space_instance;
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:struct DeviceTypeTraits<Cuda> {
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:  static constexpr DeviceType id = DeviceType::Cuda;
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:  static int device_id(const Cuda& exec) { return exec.cuda_device(); }
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:struct MemorySpaceAccess<Kokkos::CudaSpace,
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:                         Kokkos::Cuda::scratch_memory_space> {
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:#if defined(KOKKOS_ENABLE_CUDA_UVM)
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:// then must assume that CudaUVMSpace
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:// can be a stand-in for CudaSpace.
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:// that defines CudaUVMSpace as its preferredmemory space.
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:struct MemorySpaceAccess<Kokkos::CudaUVMSpace,
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:                         Kokkos::Cuda::scratch_memory_space> {
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:#endif /* #if defined( KOKKOS_ENABLE_CUDA ) */
lib/kokkos/core/src/Cuda/Kokkos_Cuda.hpp:#endif /* #ifndef KOKKOS_CUDA_HPP */
lib/kokkos/core/src/Kokkos_Timer.hpp:// gcc 10.3.0 with CUDA doesn't support std::chrono,
lib/kokkos/core/src/Kokkos_ExecPolicy.hpp: *    LaunchBounds<unsigned,unsigned> Launch Bounds for CUDA compilation,
lib/kokkos/core/src/Kokkos_ExecPolicy.hpp:#ifdef KOKKOS_ENABLE_CUDA
lib/kokkos/core/src/Kokkos_ExecPolicy.hpp:                                  Kokkos::Cuda>
lib/kokkos/core/src/Kokkos_ExecPolicy.hpp:#ifdef KOKKOS_ENABLE_CUDA
lib/kokkos/core/src/Kokkos_ExecPolicy.hpp:                                  Kokkos::Cuda>
lib/kokkos/core/src/Kokkos_HostSpace.hpp:  // Example: Kokkos::OpenMP can access, Kokkos::Cuda cannot
lib/kokkos/core/src/Kokkos_HostSpace.hpp:  // Example:  Cannot access Kokkos::CudaSpace, can access Kokkos::CudaUVMSpace
lib/kokkos/core/src/Kokkos_Graph.hpp:#include <Cuda/Kokkos_Cuda_Graph_Impl.hpp>
lib/kokkos/core/src/Kokkos_Graph.hpp:// The implementation of hipGraph in ROCm 5.2 is bugged, so we cannot use it.
lib/kokkos/core/src/HIP/Kokkos_HIP_ParallelReduce_MDRange.hpp:#include <impl/KokkosExp_IterateTileGPU.hpp>
lib/kokkos/core/src/HIP/Kokkos_HIP_Team.hpp:// This is the same code as in CUDA and largely the same as in OpenMPTarget
lib/kokkos/core/src/HIP/Kokkos_HIP.cpp:      Impl::get_gpu(settings).value_or(visible_devices[0]);
lib/kokkos/core/src/HIP/Kokkos_HIP.cpp:  // https://github.com/ROCm-Developer-Tools/HIP/blob/a0b5dfd625d99af7e288629747b40dd057183173/vdi/hip_platform.cpp#L742
lib/kokkos/core/src/HIP/Kokkos_HIP_Instance.cpp:    std::string gpu_type = hipProp.integrated == 1 ? "APU" : "dGPU";
lib/kokkos/core/src/HIP/Kokkos_HIP_Instance.cpp:      << ", APU or dGPU: " << gpu_type
lib/kokkos/core/src/HIP/Kokkos_HIP_ParallelFor_MDRange.hpp:#include <impl/KokkosExp_IterateTileGPU.hpp>
lib/kokkos/core/src/HIP/Kokkos_HIP_KernelLaunch.hpp:// The hip_parallel_launch_*_memory code is identical to the cuda code
lib/kokkos/core/src/HIP/Kokkos_HIP_KernelLaunch.hpp:// The following code is identical to the cuda code
lib/kokkos/core/src/HIP/Kokkos_HIP_Space.cpp:                                 Please refer to the ROCm documentation on unified/managed memory.)warning"
lib/kokkos/core/src/HIP/Kokkos_HIP_Space.cpp:                              amdgpu.noretry=0 was set in the Linux kernel boot line.
lib/kokkos/core/src/HIP/Kokkos_HIP_BlockSize_Deduction.hpp:  // translate LB from CUDA to HIP
lib/kokkos/core/src/setup/Kokkos_Setup_SYCL.hpp:// GPUs and we run into correctness issues with out-of-order queues on NVIDIA
lib/kokkos/core/src/setup/Kokkos_Setup_SYCL.hpp:// GPUs.
lib/kokkos/core/src/setup/Kokkos_Setup_Cuda.hpp:#ifndef KOKKOS_CUDA_SETUP_HPP_
lib/kokkos/core/src/setup/Kokkos_Setup_Cuda.hpp:#define KOKKOS_CUDA_SETUP_HPP_
lib/kokkos/core/src/setup/Kokkos_Setup_Cuda.hpp:#if !defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/core/src/setup/Kokkos_Setup_Cuda.hpp:    "KOKKOS_ENABLE_CUDA was not defined, but Kokkos_Setup_Cuda.hpp was included anyway."
lib/kokkos/core/src/setup/Kokkos_Setup_Cuda.hpp:#if defined(KOKKOS_ENABLE_CUDA) && !defined(__CUDACC__)
lib/kokkos/core/src/setup/Kokkos_Setup_Cuda.hpp:    "KOKKOS_ENABLE_CUDA defined but the compiler is not defining the __CUDACC__ macro as expected"
lib/kokkos/core/src/setup/Kokkos_Setup_Cuda.hpp:#define __CUDACC__
lib/kokkos/core/src/setup/Kokkos_Setup_Cuda.hpp:#endif /* defined(KOKKOS_ENABLE_CUDA) && !defined(__CUDACC__) */
lib/kokkos/core/src/setup/Kokkos_Setup_Cuda.hpp:// Compiling with a CUDA compiler.
lib/kokkos/core/src/setup/Kokkos_Setup_Cuda.hpp://  Include <cuda.h> to pick up the CUDA_VERSION macro defined as:
lib/kokkos/core/src/setup/Kokkos_Setup_Cuda.hpp://    CUDA_VERSION = ( MAJOR_VERSION * 1000 ) + ( MINOR_VERSION * 10 )
lib/kokkos/core/src/setup/Kokkos_Setup_Cuda.hpp://  When generating device code the __CUDA_ARCH__ macro is defined as:
lib/kokkos/core/src/setup/Kokkos_Setup_Cuda.hpp://    __CUDA_ARCH__ = ( MAJOR_CAPABILITY * 100 ) + ( MINOR_CAPABILITY * 10 )
lib/kokkos/core/src/setup/Kokkos_Setup_Cuda.hpp:#include <cuda_runtime.h>
lib/kokkos/core/src/setup/Kokkos_Setup_Cuda.hpp:#include <cuda.h>
lib/kokkos/core/src/setup/Kokkos_Setup_Cuda.hpp:#define KOKKOS_IMPL_WINDOWS_CUDA
lib/kokkos/core/src/setup/Kokkos_Setup_Cuda.hpp:#if !defined(CUDA_VERSION)
lib/kokkos/core/src/setup/Kokkos_Setup_Cuda.hpp:#error "#include <cuda.h> did not define CUDA_VERSION."
lib/kokkos/core/src/setup/Kokkos_Setup_Cuda.hpp:#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 300)
lib/kokkos/core/src/setup/Kokkos_Setup_Cuda.hpp:// Compiling with CUDA compiler for device code.
lib/kokkos/core/src/setup/Kokkos_Setup_Cuda.hpp:#error "Cuda device capability >= 3.0 is required."
lib/kokkos/core/src/setup/Kokkos_Setup_Cuda.hpp:#endif /* KOKKOS_CUDA_SETUP_HPP_ */
lib/kokkos/core/src/Serial/Kokkos_Serial.hpp:/// extensions, and the Cuda device uses NVIDIA's CUDA programming
lib/kokkos/core/src/Kokkos_BitManipulation.hpp:#if defined(KOKKOS_ENABLE_CUDA) || defined(KOKKOS_ENABLE_HIP)
lib/kokkos/core/src/Kokkos_BitManipulation.hpp:#if defined(KOKKOS_ENABLE_CUDA) || defined(KOKKOS_ENABLE_HIP)
lib/kokkos/core/src/Kokkos_BitManipulation.hpp:#if defined(KOKKOS_ENABLE_CUDA) || defined(KOKKOS_ENABLE_HIP)
lib/kokkos/core/src/View/MDSpan/Kokkos_MDSpan_Header.hpp:// Only use standard library mdspan if we are not running Cuda or HIP.
lib/kokkos/core/src/View/MDSpan/Kokkos_MDSpan_Header.hpp:#if (__cpp_lib_mdspan >= 202207L) && !defined(KOKKOS_ENABLE_CUDA) && \
lib/kokkos/core/src/View/Kokkos_ViewAlloc.hpp: *  Primarily to work around an unresolved CUDA back-end bug
lib/kokkos/core/src/View/Kokkos_ViewAlloc.hpp: *  that would lose the destruction cuda device function when
lib/kokkos/core/src/View/Kokkos_ViewAlloc.hpp:#ifdef KOKKOS_ENABLE_CUDA
lib/kokkos/core/src/View/Kokkos_ViewAlloc.hpp:    if (std::is_same<ExecSpace, Kokkos::Cuda>::value) {
lib/kokkos/core/src/View/Kokkos_ViewAlloc.hpp:      Kokkos::Impl::cuda_prefetch_pointer(space, ptr, sizeof(ValueType) * n,
lib/kokkos/core/src/View/Kokkos_ViewAlloc.hpp:  // This is a workaround to avoid "cudaErrorInvalidDeviceFunction" error later
lib/kokkos/core/src/View/Kokkos_ViewAlloc.hpp:  // when the function is queried with cudaFuncGetAttributes
lib/kokkos/core/src/View/Kokkos_ViewAlloc.hpp:#if defined(KOKKOS_ENABLE_CUDA) || defined(KOKKOS_ENABLE_HIP) || \
lib/kokkos/core/src/View/Kokkos_ViewAlloc.hpp:#ifdef KOKKOS_ENABLE_CUDA
lib/kokkos/core/src/View/Kokkos_ViewAlloc.hpp:    if (std::is_same<ExecSpace, Kokkos::Cuda>::value) {
lib/kokkos/core/src/View/Kokkos_ViewAlloc.hpp:      Kokkos::Impl::cuda_prefetch_pointer(space, ptr, sizeof(ValueType) * n,
lib/kokkos/core/src/Kokkos_Complex.hpp:///   currently forbidden in CUDA device kernels.
lib/kokkos/core/src/Kokkos_Complex.hpp:  /// This constructor cannot be called in a CUDA device function,
lib/kokkos/core/src/Kokkos_Complex.hpp:  /// marked as CUDA device functions.
lib/kokkos/core/src/Kokkos_Complex.hpp:  /// This operator cannot be called in a CUDA device function,
lib/kokkos/core/src/Kokkos_Complex.hpp:  /// marked as CUDA device functions.
lib/kokkos/core/src/Kokkos_Complex.hpp:  /// This constructor cannot be called in a CUDA device function,
lib/kokkos/core/src/Kokkos_Complex.hpp:  /// marked as CUDA device functions.
lib/kokkos/core/src/Kokkos_Complex.hpp:/// This function cannot be called in a CUDA device function, because
lib/kokkos/core/src/Kokkos_Complex.hpp:/// CUDA device functions.
lib/kokkos/core/src/Kokkos_Complex.hpp:/// This function cannot be called in a CUDA device function,
lib/kokkos/core/src/Kokkos_Complex.hpp:/// marked as CUDA device functions.
lib/kokkos/simd/unit_tests/include/TestSIMD_GeneratorCtors.hpp:#if !(defined(KOKKOS_ENABLE_CUDA) && defined(KOKKOS_COMPILER_MSVC))
lib/kokkos/simd/unit_tests/CMakeLists.txt:IF((NOT (Kokkos_ENABLE_CUDA AND WIN32)))
lib/kokkos/simd/src/Kokkos_SIMD.hpp:#ifdef KOKKOS_ENABLE_CUDA
lib/kokkos/simd/src/Kokkos_SIMD.hpp:struct ForSpace<Kokkos::Cuda> {
lib/kokkos/simd/src/Kokkos_SIMD.hpp:#ifdef KOKKOS_ENABLE_OPENACC
lib/kokkos/simd/src/Kokkos_SIMD.hpp:struct ForSpace<Kokkos::Experimental::OpenACC> {
lib/kokkos/simd/src/Kokkos_SIMD_AVX2.hpp:// FIXME_HIP ROCm 5.6, 5.7, and 6.0 can't compile with the intrinsic used here.
lib/kokkos/simd/src/Kokkos_SIMD_AVX2.hpp:#define KOKKOS_IMPL_WORKAROUND_ROCM_AVX2_ISSUE
lib/kokkos/simd/src/Kokkos_SIMD_AVX2.hpp:      // FIXME_HIP ROCm 5.6, 5.7, and 6.0 can't compile with the intrinsic used
lib/kokkos/simd/src/Kokkos_SIMD_AVX2.hpp:#ifdef KOKKOS_IMPL_WORKAROUND_ROCM_AVX2_ISSUE
lib/kokkos/simd/src/Kokkos_SIMD_AVX2.hpp:    // FIXME_HIP ROCm 5.6, 5.7, and 6.0 can't compile with the intrinsic used
lib/kokkos/simd/src/Kokkos_SIMD_AVX2.hpp:#ifdef KOKKOS_IMPL_WORKAROUND_ROCM_AVX2_ISSUE
lib/kokkos/simd/src/Kokkos_SIMD_AVX2.hpp:    // FIXME_HIP ROCm 5.6 can't compile with the intrinsic used here.
lib/kokkos/simd/src/Kokkos_SIMD_AVX2.hpp:#ifdef KOKKOS_IMPL_WORKAROUND_ROCM_AVX2_ISSUE
lib/kokkos/simd/src/Kokkos_SIMD_AVX2.hpp:    // FIXME_HIP ROCm 5.6, 5.7, and 6.0 can't compile with the intrinsic used
lib/kokkos/simd/src/Kokkos_SIMD_AVX2.hpp:#ifdef KOKKOS_IMPL_WORKAROUND_ROCM_AVX2_ISSUE
lib/kokkos/simd/src/Kokkos_SIMD_AVX2.hpp:    // FIXME_HIP ROCm 5.6, 5.7, and 6.0 can't compile with the intrinsic used
lib/kokkos/simd/src/Kokkos_SIMD_AVX2.hpp:#ifdef KOKKOS_IMPL_WORKAROUND_ROCM_AVX2_ISSUE
lib/kokkos/simd/src/Kokkos_SIMD_AVX2.hpp:#ifdef KOKKOS_IMPL_WORKAROUND_ROCM_AVX2_ISSUE
lib/kokkos/simd/src/Kokkos_SIMD_AVX2.hpp:#ifdef KOKKOS_IMPL_WORKAROUND_ROCM_AVX2_ISSUE
lib/kokkos/simd/src/Kokkos_SIMD_AVX2.hpp:#ifdef KOKKOS_IMPL_WORKAROUND_ROCM_AVX2_ISSUE
lib/kokkos/simd/src/Kokkos_SIMD_AVX2.hpp:#ifdef KOKKOS_IMPL_WORKAROUND_ROCM_AVX2_ISSUE
lib/kokkos/simd/src/Kokkos_SIMD_AVX2.hpp:#ifdef KOKKOS_IMPL_WORKAROUND_ROCM_AVX2_ISSUE
lib/kokkos/simd/src/Kokkos_SIMD_AVX2.hpp:#ifdef KOKKOS_IMPL_WORKAROUND_ROCM_AVX2_ISSUE
lib/kokkos/simd/src/Kokkos_SIMD_AVX2.hpp:#ifdef KOKKOS_IMPL_WORKAROUND_ROCM_AVX2_ISSUE
lib/kokkos/simd/src/Kokkos_SIMD_AVX2.hpp:#ifdef KOKKOS_IMPL_WORKAROUND_ROCM_AVX2_ISSUE
lib/kokkos/simd/src/Kokkos_SIMD_AVX2.hpp:#ifdef KOKKOS_IMPL_WORKAROUND_ROCM_AVX2_ISSUE
lib/kokkos/simd/src/Kokkos_SIMD_AVX2.hpp:#ifdef KOKKOS_IMPL_WORKAROUND_ROCM_AVX2_ISSUE
lib/kokkos/simd/src/Kokkos_SIMD_AVX2.hpp:#ifdef KOKKOS_IMPL_WORKAROUND_ROCM_AVX2_ISSUE
lib/kokkos/simd/src/Kokkos_SIMD_AVX2.hpp:#ifdef KOKKOS_IMPL_WORKAROUND_ROCM_AVX2_ISSUE
lib/kokkos/simd/src/Kokkos_SIMD_AVX2.hpp:#undef KOKKOS_IMPL_WORKAROUND_ROCM_AVX2_ISSUE
lib/kokkos/CHANGELOG.md:#### CUDA:
lib/kokkos/CHANGELOG.md:#### CUDA:
lib/kokkos/CHANGELOG.md:* Use recommended/max team size functions in Cuda ParallelFor and Reduce constructors [\#6891](https://github.com/kokkos/kokkos/issues/6891)
lib/kokkos/CHANGELOG.md:* Improve compile-times when building with `Kokkos_ENABLE_DEBUG_BOUNDS_CHECK` in Cuda [\#7013](https://github.com/kokkos/kokkos/pull/7013)
lib/kokkos/CHANGELOG.md:* Enable user-specified compiler and linker flags for AMD GPUs [\#7127](https://github.com/kokkos/kokkos/pull/7127)
lib/kokkos/CHANGELOG.md:* Fix multi-GPU support [\#6887](https://github.com/kokkos/kokkos/pull/6887)
lib/kokkos/CHANGELOG.md:#### OpenACC:
lib/kokkos/CHANGELOG.md:* Update Intel GPU architectures in Makefile [\#6895](https://github.com/kokkos/kokkos/pull/6895)
lib/kokkos/CHANGELOG.md:* Fix use of OpenMP with Cuda or HIP as compile language [\#6972](https://github.com/kokkos/kokkos/pull/6972)
lib/kokkos/CHANGELOG.md:* Add nvidia Grace CPU architecture: `Kokkos_ARCH_ARMV9_GRACE` [\#7158](https://github.com/kokkos/kokkos/pull/7158)
lib/kokkos/CHANGELOG.md:* Remove support for NVHPC as CUDA device compiler [\#6987](https://github.com/kokkos/kokkos/pull/6987)
lib/kokkos/CHANGELOG.md:* Fix using CUDAToolkit for CMake 3.28.4 and higher [\#7062](https://github.com/kokkos/kokkos/pull/7062)
lib/kokkos/CHANGELOG.md:* Fix CUDA reduction overflow for `RangePolicy` [\#6578](https://github.com/kokkos/kokkos/pull/6578)
lib/kokkos/CHANGELOG.md:#### CUDA:
lib/kokkos/CHANGELOG.md:* Experimental multi-GPU support (from the same process) [\#6782](https://github.com/kokkos/kokkos/pull/6782)
lib/kokkos/CHANGELOG.md:* Link against CUDA libraries even with KOKKOS_ENABLE_COMPILE_AS_CMAKE_LANGUAGE [\#6701](https://github.com/kokkos/kokkos/pull/6701)
lib/kokkos/CHANGELOG.md:* Don't use the compiler launcher script if the CMake compile language is CUDA. [\#6704](https://github.com/kokkos/kokkos/pull/6704)
lib/kokkos/CHANGELOG.md: * Fix compilation when using amdclang (with ROCm >= 5.7) and RDC [\#6857](https://github.com/kokkos/kokkos/pull/6857)
lib/kokkos/CHANGELOG.md:  * Filter GPU devices for `ext_onapi_*` GPU devices [\#6758](https://github.com/kokkos/kokkos/pull/6784)
lib/kokkos/CHANGELOG.md:* Update linker flags for Intel GPUs update [\#6735](https://github.com/kokkos/kokkos/pull/6735)
lib/kokkos/CHANGELOG.md:* Improve handling of printf on Intel GPUs [\#6652](https://github.com/kokkos/kokkos/pull/6652)
lib/kokkos/CHANGELOG.md:#### OpenACC:
lib/kokkos/CHANGELOG.md:* Make the OpenACC backend asynchronous [\#6772](https://github.com/kokkos/kokkos/pull/6772)
lib/kokkos/CHANGELOG.md:* Improve performance of random number generation when using a normal distribution on GPUs [\#6556](https://github.com/kokkos/kokkos/pull/6556)
lib/kokkos/CHANGELOG.md:* Cuda: Fix configuring with CMake >= 3.28.4 - temporary fallback to internal CudaToolkit.cmake [\#6898](https://github.com/kokkos/kokkos/pull/6898)
lib/kokkos/CHANGELOG.md:* Deprecate {Cuda,HIP}::detect_device_count() and Cuda::[detect_]device_arch() [\#6710](https://github.com/kokkos/kokkos/pull/6710)
lib/kokkos/CHANGELOG.md:* Fix CUDA and SYCL small value type (16-bit) team reductions [\#5334](https://github.com/kokkos/kokkos/pull/5334)
lib/kokkos/CHANGELOG.md:* HIP,Cuda,OpenMPTarget: Fixup use provided execution space when copying host inaccessible reduction result [\#6777](https://github.com/kokkos/kokkos/pull/6777)
lib/kokkos/CHANGELOG.md:* Fix typo in `cuda_func_set_attribute[s]_wrapper` preventing proper setting of desired occupancy [\#6786](https://github.com/kokkos/kokkos/pull/6786)
lib/kokkos/CHANGELOG.md:* Fix a bug in Makefile.kokkos when using AMD GPU architectures as `AMD_GFXYYY` [\#6892](https://github.com/kokkos/kokkos/pull/6892)
lib/kokkos/CHANGELOG.md:#### CUDA:
lib/kokkos/CHANGELOG.md:- Add support for gfx940 (AMD Instinct MI300 GPU) [\#6671](https://github.com/kokkos/kokkos/pull/6671)
lib/kokkos/CHANGELOG.md:- Fix various issues for MSVC CUDA builds [\#6659](https://github.com/kokkos/kokkos/pull/6659)
lib/kokkos/CHANGELOG.md:#### CUDA:
lib/kokkos/CHANGELOG.md:- Fixed potential data race in Cuda `parallel_reduce` [\#6236](https://github.com/kokkos/kokkos/pull/6236)
lib/kokkos/CHANGELOG.md:- Use `cudaMallocAsync` by default [\#6402](https://github.com/kokkos/kokkos/pull/6402)
lib/kokkos/CHANGELOG.md:- New naming convention for AMD GPU: VEGA906, VEGA908, VEGA90A, NAVI1030 to AMD_GFX906, AMD_GFX908, AMD_GFX90A, AMD_GFX1030 [\#6266](https://github.com/kokkos/kokkos/pull/6266)
lib/kokkos/CHANGELOG.md:- Allow using the SYCL execution space on AMD GPUs [\#6321](https://github.com/kokkos/kokkos/pull/6321)
lib/kokkos/CHANGELOG.md:#### OpenACC:
lib/kokkos/CHANGELOG.md: - Fix initialization of scratch lock variables in the `Cuda` backend [\#6433](https://github.com/kokkos/kokkos/pull/6433)
lib/kokkos/CHANGELOG.md:#### CUDA:
lib/kokkos/CHANGELOG.md:* Remove ability to disable CMake option `Kokkos_ENABLE_CUDA_LAMBDA` and unconditionally enable CUDA extended lambda support. [\#5964](https://github.com/kokkos/kokkos/pull/5964)
lib/kokkos/CHANGELOG.md:* Drop unnecessary fences around the memory allocation when using `CudaUVMSpace` in views [\#6008](https://github.com/kokkos/kokkos/pull/6008)
lib/kokkos/CHANGELOG.md:* Export CMake `Kokkos_{CUDA,HIP}_ARCHITECTURES` variables [\#5919](https://github.com/kokkos/kokkos/pull/5919) [\#5925](https://github.com/kokkos/kokkos/pull/5925)
lib/kokkos/CHANGELOG.md:* Drop `KOKKOS_ENABLE_CUDA_ASM*` and `KOKKOS_ENABLE_*_ATOMICS` macros [\#5940](https://github.com/kokkos/kokkos/pull/5940)
lib/kokkos/CHANGELOG.md:* Deprecate `Kokkos_ENABLE_CUDA_LAMBDA` configuration option and force it to `ON` [\#5964](https://github.com/kokkos/kokkos/pull/5964)
lib/kokkos/CHANGELOG.md:* Cuda: Remove unused attach_texture_object [\#6129](https://github.com/kokkos/kokkos/pull/6129)
lib/kokkos/CHANGELOG.md:* Fix incorrect results of `parallel_reduce` of types smaller than `int` on CUDA and HIP: [\#5745](https://github.com/kokkos/kokkos/pull/5745)
lib/kokkos/CHANGELOG.md:#### CUDA:
lib/kokkos/CHANGELOG.md:- Add CUDA Ada architecture support [\#6022](https://github.com/kokkos/kokkos/pull/6022)
lib/kokkos/CHANGELOG.md:- Add support for AMDGPU target NAVI31 / RX 7900 XT(X): gfx1100 [\#6021](https://github.com/kokkos/kokkos/pull/6021)
lib/kokkos/CHANGELOG.md:#### CUDA:
lib/kokkos/CHANGELOG.md:- Allow CUDA PTX forward compatibility [\#3612](https://github.com/kokkos/kokkos/pull/3612) [\#5536](https://github.com/kokkos/kokkos/pull/5536) [\#5527](https://github.com/kokkos/kokkos/pull/5527)
lib/kokkos/CHANGELOG.md:- Add support for NVIDIA Hopper GPU architecture [\#5538](https://github.com/kokkos/kokkos/pull/5538)
lib/kokkos/CHANGELOG.md:- Don't rely on synchronization behavior of default stream in CUDA and HIP [\#5391](https://github.com/kokkos/kokkos/pull/5391)
lib/kokkos/CHANGELOG.md:- Improve CUDA cache config settings [\#5706](https://github.com/kokkos/kokkos/pull/5706)
lib/kokkos/CHANGELOG.md: - Don't rely on synchronization behavior of default stream in CUDA and HIP [\#5391](https://github.com/kokkos/kokkos/pull/5391)
lib/kokkos/CHANGELOG.md: - Add parameter to force using GlobalMemory launch mechanism. This can be used when encountering compiler bugs with ROCm 5.3 and 5.4  [\#5796](https://github.com/kokkos/kokkos/pull/5796)
lib/kokkos/CHANGELOG.md:- Refactor desul atomics to support compiling CUDA with NVC++ [\#5431](https://github.com/kokkos/kokkos/pull/5431) [\#5497](https://github.com/kokkos/kokkos/pull/5497) [\#5498](https://github.com/kokkos/kokkos/pull/5498)
lib/kokkos/CHANGELOG.md:- Do not add `-cuda` to the link line with NVHPC compiler when the CUDA backend is not actually enabled [\#5485](https://github.com/kokkos/kokkos/pull/5485)
lib/kokkos/CHANGELOG.md:- `Kokkos_ENABLE_CUDA_LAMBDA` now `ON` by default with NVCC [\#5580](https://github.com/kokkos/kokkos/pull/5580)
lib/kokkos/CHANGELOG.md:- Fix enabling of relocatable device code when using CUDA as CMake language [\#5564](https://github.com/kokkos/kokkos/pull/5564)
lib/kokkos/CHANGELOG.md:- Fix cmake configuration with CUDA 12 [\#5691](https://github.com/kokkos/kokkos/pull/5691)
lib/kokkos/CHANGELOG.md:- Remove `KOKKOS_COMPILER_CUDA_VERSION` [\#5430](https://github.com/kokkos/kokkos/pull/5430)
lib/kokkos/CHANGELOG.md:- `ENABLE_CUDA_UVM` is dropped in favor of using `SharedSpace` as `MemorySpace` explicitly [\#5608](https://github.com/kokkos/kokkos/pull/5608)
lib/kokkos/CHANGELOG.md:- Remove Kokkos_ENABLE_CUDA_LDG_INTRINSIC option [\#5623](https://github.com/kokkos/kokkos/pull/5623)
lib/kokkos/CHANGELOG.md:- Don't rely on synchronization behavior of default stream in CUDA and HIP - this potentially will break unintended implicit synchronization with other libraries such as MPI [\#5391](https://github.com/kokkos/kokkos/pull/5391)
lib/kokkos/CHANGELOG.md:- Deprecate `CudaUVMSpace::available()` which always returned `true` [\#5614](https://github.com/kokkos/kokkos/pull/5614)
lib/kokkos/CHANGELOG.md:- Fix incorrect offset in CUDA and HIP `parallel_scan` for < 4 byte types [\#5555](https://github.com/kokkos/kokkos/pull/5555) (3.7 patch release candidate)
lib/kokkos/CHANGELOG.md:- Fix max scratch size calculation for level 0 scratch in CUDA and HIP [\#5718](https://github.com/kokkos/kokkos/pull/5718)
lib/kokkos/CHANGELOG.md:#### CUDA
lib/kokkos/CHANGELOG.md:- Add Hopper support and update nvcc_wrapper to work with CUDA-12 [\#5693](https://github.com/kokkos/kokkos/pull/5693)
lib/kokkos/CHANGELOG.md:- Fix CUDA lock arrays for current Desul [\#5812](https://github.com/kokkos/kokkos/pull/5812)
lib/kokkos/CHANGELOG.md:- Do not add -cuda to the link line with NVHPC compiler when the CUDA backend is not actually enabled [\#5569](https://github.com/kokkos/kokkos/pull/5569)
lib/kokkos/CHANGELOG.md:- Export the flags in `KOKKOS_AMDGPU_OPTIONS` when using Trilinos [\#5571](https://github.com/kokkos/kokkos/pull/5571)
lib/kokkos/CHANGELOG.md:- Fix incorrect offset in CUDA and HIP parallel scan for < 4 byte types [\#5607](https://github.com/kokkos/kokkos/pull/5607)
lib/kokkos/CHANGELOG.md:- Fix initialization of Cuda lock arrays [\#5622](https://github.com/kokkos/kokkos/pull/5622)
lib/kokkos/CHANGELOG.md:- Add `HIPManagedSpace`, similar to `CudaUVMSpace` [\#5112](https://github.com/kokkos/kokkos/pull/5112)
lib/kokkos/CHANGELOG.md:- Add arch flags for Intel GPU Ponte Vecchio [\#4932](https://github.com/kokkos/kokkos/pull/4932)
lib/kokkos/CHANGELOG.md:- SYCL: require GPU if GPU architecture was set at configuration time (i.e. do not allow fallback to CPU device) [\#5264](https://github.com/kokkos/kokkos/pull/5264) [\#5222](https://github.com/kokkos/kokkos/pull/5222)
lib/kokkos/CHANGELOG.md:- Only show GPU architectures with enabled corresponding backend [\#5119](https://github.com/kokkos/kokkos/pull/5119)
lib/kokkos/CHANGELOG.md:- AOT flags for OpenMPTarget targeting Intel GPUs [\#4915](https://github.com/kokkos/kokkos/pull/4915)
lib/kokkos/CHANGELOG.md:- Replace amdgpu-target with offload-arch [\#4874](https://github.com/kokkos/kokkos/pull/4874)
lib/kokkos/CHANGELOG.md:- CUDA Reductions: Fix data races reported by Nvidia `compute-sanitizer` [\#4855](https://github.com/kokkos/kokkos/pull/4855)
lib/kokkos/CHANGELOG.md:- Fix CUDA+MSVC build issue [\#5261](https://github.com/kokkos/kokkos/pull/5261)
lib/kokkos/CHANGELOG.md:- Finalize `deep_copy_space` early avoiding printing to `std::cerr` for Cuda [\#5151](https://github.com/kokkos/kokkos/pull/5151)
lib/kokkos/CHANGELOG.md:- Fix building with NVCC as the CXX compiler while the CUDA backend is not enabled [\#5115](https://github.com/kokkos/kokkos/pull/5115)
lib/kokkos/CHANGELOG.md:- Fix bug with CUDA's team reduction for empty ranges [\#5079](https://github.com/kokkos/kokkos/pull/5079)
lib/kokkos/CHANGELOG.md:- Fixed `_CUDA_ARCH__` to `__CUDA_ARCH__` for CUDA LDG [\#4893](https://github.com/kokkos/kokkos/pull/4893)
lib/kokkos/CHANGELOG.md:- Fix `abort` with HIP backend for ROCm 5.0.2 and beyond [\#4873](https://github.com/kokkos/kokkos/pull/4873)
lib/kokkos/CHANGELOG.md:- Fixed `_CUDA_ARCH__` to `__CUDA_ARCH__` for CUDA LDG [\#4893](https://github.com/kokkos/kokkos/pull/4893)
lib/kokkos/CHANGELOG.md:#### CUDA:
lib/kokkos/CHANGELOG.md:- Require ROCm 4.5 [\#4689](https://github.com/kokkos/kokkos/pull/4689)
lib/kokkos/CHANGELOG.md:- Lock constant memory in Cuda/HIP kernel launch with a mutex (thread safety) [\#4525](https://github.com/kokkos/kokkos/pull/4525)
lib/kokkos/CHANGELOG.md:- Fix a bug in CUDA scratch memory pool (abnormally high memory consumption) [\#4673](https://github.com/kokkos/kokkos/pull/4673)
lib/kokkos/CHANGELOG.md:- SYCL fix to run when no GPU is detected [\#4623](https://github.com/kokkos/kokkos/pull/4623)
lib/kokkos/CHANGELOG.md:- Deprecate CUDA_SAFE_CALL and HIP_SAFE_CALL [\#4249](https://github.com/kokkos/kokkos/pull/4249)
lib/kokkos/CHANGELOG.md:#### CUDA:
lib/kokkos/CHANGELOG.md:- Cuda improve heuristic for blocksize [\#4271](https://github.com/kokkos/kokkos/pull/4271)
lib/kokkos/CHANGELOG.md:- Update support for cuda reductions to work with types < 4bytes [\#4156](https://github.com/kokkos/kokkos/pull/4156)
lib/kokkos/CHANGELOG.md:- Adding opt-in CudaMallocSync support when using CUDA version >= 11.2 [\#4026](https://github.com/kokkos/kokkos/pull/4026) [\#4233](https://github.com/kokkos/kokkos/pull/4233)
lib/kokkos/CHANGELOG.md:- Fix a potential race condition in the CUDA backend [\#3999](https://github.com/kokkos/kokkos/pull/3999)
lib/kokkos/CHANGELOG.md:- nvcc_wrapper: suppress duplicates of GPU architecture and RDC flags [\#3968](https://github.com/kokkos/kokkos/pull/3968)
lib/kokkos/CHANGELOG.md:- Add OpenMPTarget CI build on AMD GPUs [\#4055](https://github.com/kokkos/kokkos/pull/4055)
lib/kokkos/CHANGELOG.md:- Remove pre CUDA 9 KOKKOS_IMPL_CUDA_* macros [\#4138](https://github.com/kokkos/kokkos/pull/4138)
lib/kokkos/CHANGELOG.md:- Kokkos_Cuda.hpp: Fix shadow warning with cuda/11.0 [\#4252](https://github.com/kokkos/kokkos/pull/4252)
lib/kokkos/CHANGELOG.md:- nvcc_wrapper: fix errors in argument handling, suppress duplicates of GPU architecture and RDC flags [\#4006](https://github.com/kokkos/kokkos/pull/4006)
lib/kokkos/CHANGELOG.md:- SYCL choose a specific GPU [\#3918](https://github.com/kokkos/kokkos/pull/3918)
lib/kokkos/CHANGELOG.md:- Introduce HostSharedPtr to manage m_space_instance for Cuda/HIP/SYCL [\#3824](https://github.com/kokkos/kokkos/pull/3824)
lib/kokkos/CHANGELOG.md:- core/Cuda: Half_t updates for cgsolve [\#3746](https://github.com/kokkos/kokkos/pull/3746)
lib/kokkos/CHANGELOG.md:- kokkos_launch_compiler + CUDA auto-detect arch [\#3770](https://github.com/kokkos/kokkos/pull/3770)
lib/kokkos/CHANGELOG.md:- Add support for -Wno-deprecated-gpu-targets [\#3722](https://github.com/kokkos/kokkos/pull/3722)
lib/kokkos/CHANGELOG.md:- Add configuration to target CUDA compute capability 8.6 [\#3713](https://github.com/kokkos/kokkos/pull/3713)
lib/kokkos/CHANGELOG.md:- Try detecting ndevices in get_gpu [\#3921](https://github.com/kokkos/kokkos/pull/3921)
lib/kokkos/CHANGELOG.md:- Cleanup writing to output streams in Cuda [\#3859](https://github.com/kokkos/kokkos/pull/3859)
lib/kokkos/CHANGELOG.md:- Fixup cache CUDA fallback execution space instance used by DualView::sync [\#3856](https://github.com/kokkos/kokkos/pull/3856)
lib/kokkos/CHANGELOG.md:- Fix typo FOUND_CUDA_{DRIVVER -> DRIVER} [\#3852](https://github.com/kokkos/kokkos/pull/3852)
lib/kokkos/CHANGELOG.md:- Fix cuda cache config not being set correct [\#3712](https://github.com/kokkos/kokkos/pull/3712)
lib/kokkos/CHANGELOG.md:- Restrict MDRange Policy tests for Intel GPUs [\#3853](https://github.com/kokkos/kokkos/pull/3853)
lib/kokkos/CHANGELOG.md:- OpenMPTarget: Block unit tests that do not pass with the nvidia compiler [\#3839](https://github.com/kokkos/kokkos/pull/3839)
lib/kokkos/CHANGELOG.md:- Fix performance bug in CUDA backend, where the cuda Cache config was not set correct.
lib/kokkos/CHANGELOG.md:- Added Kokkos Graph API analogous to CUDA Graphs.
lib/kokkos/CHANGELOG.md:- Experimental feature: control cuda occupancy [\#3379](https://github.com/kokkos/kokkos/pull/3379)
lib/kokkos/CHANGELOG.md:- Kokkos Graph: add Cuda Graph implementation [\#3369](https://github.com/kokkos/kokkos/pull/3369)
lib/kokkos/CHANGELOG.md:- Fixes for ROCm 3.9 [\#3565](https://github.com/kokkos/kokkos/pull/3565)
lib/kokkos/CHANGELOG.md:- Fix windows build issues which crept in for the CUDA build [\#3532](https://github.com/kokkos/kokkos/pull/3532)
lib/kokkos/CHANGELOG.md:- Clang compilation of CUDA backend on Windows [\#3345](https://github.com/kokkos/kokkos/pull/3345)
lib/kokkos/CHANGELOG.md:- Remove ROCm backend [\#3148](https://github.com/kokkos/kokkos/pull/3148)
lib/kokkos/CHANGELOG.md:- Disallow KOKKOS_ENABLE_CUDA_RELOCATABLE_DEVICE_CODE in shared library builds [\#3332](https://github.com/kokkos/kokkos/pull/3332)
lib/kokkos/CHANGELOG.md:  - cuda/9.2 zero-length reduction fix [\#3580](https://github.com/kokkos/kokkos/pull/3580)
lib/kokkos/CHANGELOG.md:- Guard KOKKOS_ALL_COMPILE_OPTIONS if Cuda is not enabled [\#3387](https://github.com/kokkos/kokkos/pull/3387)
lib/kokkos/CHANGELOG.md:- Fix NVIDIA GPU arch macro with autodetection [\#3473](https://github.com/kokkos/kokkos/pull/3473)
lib/kokkos/CHANGELOG.md:- Separate Cuda timing-based tests into their own executable [\#3407](https://github.com/kokkos/kokkos/pull/3407)
lib/kokkos/CHANGELOG.md:- OpenMPTarget: Significant update to the new experimental backend.  Requires C++17, works on Intel GPUs, reference counting fixes. [\#3169](https://github.com/kokkos/kokkos/issues/3169)
lib/kokkos/CHANGELOG.md:- Windows Cuda support [\#3018](https://github.com/kokkos/kokkos/issues/3018)
lib/kokkos/CHANGELOG.md:- Pass `-Wext-lambda-captures-this` to NVCC when support for `__host__ __device__` lambda is enabled from CUDA 11 [\#3241](https://github.com/kokkos/kokkos/issues/3241)
lib/kokkos/CHANGELOG.md:- Cuda: Caching cudaFunctorAttributes and whether L1/Shmem prefer was set [\#3151](https://github.com/kokkos/kokkos/issues/3151)
lib/kokkos/CHANGELOG.md:- Cuda: Update CUDA occupancy calculation [\#3124](https://github.com/kokkos/kokkos/issues/3124)
lib/kokkos/CHANGELOG.md:- BuildSystem: Add CUDA Ampere configuration support [\#3122](https://github.com/kokkos/kokkos/issues/3122)
lib/kokkos/CHANGELOG.md:- Extract and use get\_gpu [\#3061](https://github.com/kokkos/kokkos/issues/3061) , [\#3048](https://github.com/kokkos/kokkos/issues/3048)
lib/kokkos/CHANGELOG.md:- Check error code from `cudaStreamSynchronize` in CUDA fences [\#3255](https://github.com/kokkos/kokkos/issues/3255)
lib/kokkos/CHANGELOG.md:- Fix memory leak with CUDA streams [\#3170](https://github.com/kokkos/kokkos/issues/3170)
lib/kokkos/CHANGELOG.md:- nvcc\_wrapper: send --cudart to nvcc instead of host compiler [\#3092](https://github.com/kokkos/kokkos/issues/3092)
lib/kokkos/CHANGELOG.md:- BuildSystem: Fixes for Cuda/11 and c++17 [\#3085](https://github.com/kokkos/kokkos/issues/3085)
lib/kokkos/CHANGELOG.md:- Conditionally define get\_gpu [\#3072](https://github.com/kokkos/kokkos/issues/3072)
lib/kokkos/CHANGELOG.md:- Fix Cuda minor arch check [\#3035](https://github.com/kokkos/kokkos/issues/3035)
lib/kokkos/CHANGELOG.md:- Remove KOKKOS\_INTERNAL\_ENABLE\_NON\_CUDA\_BACKEND [\#3147](https://github.com/kokkos/kokkos/issues/3147)
lib/kokkos/CHANGELOG.md:- Cuda 11 -\> allow C++17 [\#3083](https://github.com/kokkos/kokkos/issues/3083)
lib/kokkos/CHANGELOG.md:- Compiler issue: Cuda build with clang 10 has errors with the atomic unit tests [\#3237](https://github.com/kokkos/kokkos/issues/3237)
lib/kokkos/CHANGELOG.md:- Incompatibility of flags for C++ standard with PGI v20.4 on Power9/NVIDIA V100 system [\#3252](https://github.com/kokkos/kokkos/issues/3252)
lib/kokkos/CHANGELOG.md:- CMake fails with Nvidia compilers when the GPU architecture option is not supplied (Fix configure with OMPT and Cuda) [\#3207](https://github.com/kokkos/kokkos/issues/3207)
lib/kokkos/CHANGELOG.md:- Cuda: Memory leak when using CUDA stream [\#3167](https://github.com/kokkos/kokkos/issues/3167)
lib/kokkos/CHANGELOG.md:- Missing write fence for lock based atomics on CUDA [\#3038](https://github.com/kokkos/kokkos/issues/3038)
lib/kokkos/CHANGELOG.md:- CUDA compute capability version check problem [\#3026](https://github.com/kokkos/kokkos/issues/3026)
lib/kokkos/CHANGELOG.md:- Reductions of non-trivial types of size 4 fail in CUDA shfl operations [\#2990](https://github.com/kokkos/kokkos/issues/2990)
lib/kokkos/CHANGELOG.md:- complex\_double misalignment in reduce, clang+CUDA [\#2989](https://github.com/kokkos/kokkos/issues/2989)
lib/kokkos/CHANGELOG.md:- Fix complex_double misalignment in reduce, clang+CUDA [\#2989](https://github.com/kokkos/kokkos/issues/2989)
lib/kokkos/CHANGELOG.md:- Fix compilation fails when profiling disabled and CUDA enabled [\#3001](https://github.com/kokkos/kokkos/issues/3001)
lib/kokkos/CHANGELOG.md:- Fix cuda reduction of non-trivial scalars of size 4 [\#2990](https://github.com/kokkos/kokkos/issues/2990)
lib/kokkos/CHANGELOG.md:- clang/7+cuda/9 build -Werror-unused parameter error in nightly test [\#2884](https://github.com/kokkos/kokkos/issues/2884)
lib/kokkos/CHANGELOG.md:- clang/8+cuda/10.0 build error with c++17 [\#2809](https://github.com/kokkos/kokkos/issues/2809)
lib/kokkos/CHANGELOG.md:- Unnecessary\(?\) check for host execution space initialization from Cuda initialization [\#2652](https://github.com/kokkos/kokkos/issues/2652)
lib/kokkos/CHANGELOG.md:- Kokkos error reporting failures with CUDA GPUs in exclusive mode [\#2471](https://github.com/kokkos/kokkos/issues/2471)
lib/kokkos/CHANGELOG.md:- Warnings with Cuda 10.1 [\#2206](https://github.com/kokkos/kokkos/issues/2206)
lib/kokkos/CHANGELOG.md:- Check error code  from cudaOccupancyMaxActiveBlocksPerMultiprocessor [\#2172](https://github.com/kokkos/kokkos/issues/2172)
lib/kokkos/CHANGELOG.md:- team\_broadcast of bool failed on CUDA backend [\#1908](https://github.com/kokkos/kokkos/issues/1908)
lib/kokkos/CHANGELOG.md:- Misleading Kokkos::Cuda::initialize ERROR message when compiled for wrong GPU architecture [\#1944](https://github.com/kokkos/kokkos/issues/1944)
lib/kokkos/CHANGELOG.md:- DualView sync\_device with length zero creates cuda errors [\#2946](https://github.com/kokkos/kokkos/issues/2946)
lib/kokkos/CHANGELOG.md:- Cuda 9.1,10.1 debug builds failing due to -Werror=unused-parameter [\#2880](https://github.com/kokkos/kokkos/issues/2880)
lib/kokkos/CHANGELOG.md:- cuda.view\_64bit test hangs on Power8+Kepler37 system - develop and 2.9.00 branches [\#2771](https://github.com/kokkos/kokkos/issues/2771)
lib/kokkos/CHANGELOG.md:- BuildSystem: Drop support for CUDA 7 and CUDA 8 [\#2489](https://github.com/kokkos/kokkos/issues/2489)
lib/kokkos/CHANGELOG.md:- TeamPolicy: reducers with valuetypes without += broken on CUDA [\#2410](https://github.com/kokkos/kokkos/issues/2410)
lib/kokkos/CHANGELOG.md:- Capability: CUDA Streams [\#1723](https://github.com/kokkos/kokkos/issues/1723)
lib/kokkos/CHANGELOG.md:- Capability: CUDA Stream support for parallel\_reduce [\#2061](https://github.com/kokkos/kokkos/issues/2061)
lib/kokkos/CHANGELOG.md:- Cuda: TeamThreadRange loop count on device is passed by reference to host static constexpr [\#1733](https://github.com/kokkos/kokkos/issues/1733)
lib/kokkos/CHANGELOG.md:- Cuda: Build error with relocatable device code with CUDA 10.1 GCC 7.3 [\#2134](https://github.com/kokkos/kokkos/issues/2134)
lib/kokkos/CHANGELOG.md:- Cuda: cudaFuncSetCacheConfig is setting CachePreferShared too often [\#2066](https://github.com/kokkos/kokkos/issues/2066)
lib/kokkos/CHANGELOG.md:- Cuda: TeamPolicy doesn't throw then created with non-viable vector length and also doesn't backscale to viable one [\#2020](https://github.com/kokkos/kokkos/issues/2020)
lib/kokkos/CHANGELOG.md:- Cuda: cudaMemcpy error for large league sizes on V100 [\#1991](https://github.com/kokkos/kokkos/issues/1991)
lib/kokkos/CHANGELOG.md:- Cuda: illegal warp sync in parallel\_reduce by functor on Turing 75 [\#1958](https://github.com/kokkos/kokkos/issues/1958)
lib/kokkos/CHANGELOG.md:- Cuda: Fix Volta Issues 1 Non-deterministic behavior on Volta, runs fine on Pascal [\#1949](https://github.com/kokkos/kokkos/issues/1949)
lib/kokkos/CHANGELOG.md:- Cuda: Fix Volta Issues 2 CUDA Team Scan gives wrong values on Volta with -G compile flag [\#1942](https://github.com/kokkos/kokkos/issues/1942)
lib/kokkos/CHANGELOG.md:- Cuda: illegal warp sync in parallel\_reduce by functor on Turing 75 [\#1958](https://github.com/kokkos/kokkos/issues/1958)
lib/kokkos/CHANGELOG.md:- BuildSystem: Make core/src/Makefile for Cuda use needed nvcc\_wrapper [\#1296](https://github.com/kokkos/kokkos/issues/1296)
lib/kokkos/CHANGELOG.md:- ROCm: support team vector scan  [\#1645](https://github.com/kokkos/kokkos/issues/1645)
lib/kokkos/CHANGELOG.md:- ROCm:  Merge from rocm-hackathon2 [\#1636](https://github.com/kokkos/kokkos/issues/1636)
lib/kokkos/CHANGELOG.md:- ROCm:  Add ParallelScanWithTotal [\#1611](https://github.com/kokkos/kokkos/issues/1611)
lib/kokkos/CHANGELOG.md:- ROCm: Implement MDRange in ROCm [\#1314](https://github.com/kokkos/kokkos/issues/1314)
lib/kokkos/CHANGELOG.md:- ROCm: Implement Reducers for Nested Parallelism Levels [\#963](https://github.com/kokkos/kokkos/issues/963)
lib/kokkos/CHANGELOG.md:- ROCm: Add asynchronous deep copy [\#959](https://github.com/kokkos/kokkos/issues/959)
lib/kokkos/CHANGELOG.md:- Tests: cuda.scatterview unit test causes "Bus error" when force\_uvm and enable\_lambda are enabled [\#1852](https://github.com/kokkos/kokkos/issues/1852)
lib/kokkos/CHANGELOG.md:- Tests: cuda.cxx11 unit test fails when force\_uvm and enable\_lambda are enabled [\#1850](https://github.com/kokkos/kokkos/issues/1850)
lib/kokkos/CHANGELOG.md:- Tests: threads.reduce\_device\_view\_range\_policy failing with Cuda/8.0.44 and RDC [\#1836](https://github.com/kokkos/kokkos/issues/1836)
lib/kokkos/CHANGELOG.md:- Build: White cuda/9.2 + gcc/7.2 warnings triggering errors  [\#1833](https://github.com/kokkos/kokkos/issues/1833)
lib/kokkos/CHANGELOG.md:- Cuda: Apollo cuda.team\_broadcast test fail with clang-6.0 [\#1762](https://github.com/kokkos/kokkos/issues/1762)
lib/kokkos/CHANGELOG.md:- Cuda: Clang spurious test failure in impl\_view\_accessible [\#1753](https://github.com/kokkos/kokkos/issues/1753)
lib/kokkos/CHANGELOG.md:- Cuda: Kokkos::complex\<double\> atomic deadlocks with Clang 6 Cuda build with -O0 [\#1752](https://github.com/kokkos/kokkos/issues/1752)
lib/kokkos/CHANGELOG.md:- Cuda: LayoutStride Test fails for UVM as default memory space [\#1688](https://github.com/kokkos/kokkos/issues/1688)
lib/kokkos/CHANGELOG.md:- Cuda: Scan wrong values on Volta [\#1676](https://github.com/kokkos/kokkos/issues/1676)
lib/kokkos/CHANGELOG.md:- Cuda: Kokkos::deep\_copy error with CudaUVM and Kokkos::Serial spaces [\#1652](https://github.com/kokkos/kokkos/issues/1652)
lib/kokkos/CHANGELOG.md:- Cuda: cudaErrorInvalidConfiguration with debug build [\#1647](https://github.com/kokkos/kokkos/issues/1647)
lib/kokkos/CHANGELOG.md:- Cuda: parallel\_for with TeamPolicy::team\_size\_recommended with launch bounds not working -- reported by Daniel Holladay [\#1283](https://github.com/kokkos/kokkos/issues/1283)
lib/kokkos/CHANGELOG.md:- Cuda: Using KOKKOS\_CLASS\_LAMBDA in a class with Kokkos::Random\_XorShift64\_Pool member data [\#1696](https://github.com/kokkos/kokkos/issues/1696)
lib/kokkos/CHANGELOG.md:- clang 6.0 + cuda debug out-of-memory test failure [\#1521](https://github.com/kokkos/kokkos/issues/1521)
lib/kokkos/CHANGELOG.md:- Cuda UniqueToken interface not consistent with other backends [\#1505](https://github.com/kokkos/kokkos/issues/1505)
lib/kokkos/CHANGELOG.md:- Expose round-robin GPU assignment outside of initialize\(int, char\*\*\) [\#1318](https://github.com/kokkos/kokkos/issues/1318)
lib/kokkos/CHANGELOG.md:- ROCm: Add ROCmHostPinnedSpace [\#958](https://github.com/kokkos/kokkos/issues/958)
lib/kokkos/CHANGELOG.md:- CUDA 8 has 64bit \_\_shfl [\#361](https://github.com/kokkos/kokkos/issues/361)
lib/kokkos/CHANGELOG.md:- CUDA atomic\_fetch\_sub for doubles is hitting CAS instead of intrinsic [\#1624](https://github.com/kokkos/kokkos/issues/1624)
lib/kokkos/CHANGELOG.md:- CUDA Volta another warpsync bug [\#1520](https://github.com/kokkos/kokkos/issues/1520)
lib/kokkos/CHANGELOG.md:- triple\_nested\_parallelism fails with KOKKOS\_DEBUG and CUDA [\#1513](https://github.com/kokkos/kokkos/issues/1513)
lib/kokkos/CHANGELOG.md:- CMake example broken with CUDA backend [\#1468](https://github.com/kokkos/kokkos/issues/1468)
lib/kokkos/CHANGELOG.md:- Support NVIDIA Volta microarchitecture [\#1466](https://github.com/kokkos/kokkos/issues/1466)
lib/kokkos/CHANGELOG.md:- Cuda task team collectives and stack size [\#1353](https://github.com/kokkos/kokkos/issues/1353)
lib/kokkos/CHANGELOG.md:- cuda\_internal\_maximum\_warp\_count returns 8, but I believe it should return 16 for P100  [\#1269](https://github.com/kokkos/kokkos/issues/1269)
lib/kokkos/CHANGELOG.md:- Cuda: level 1 scratch memory bug \(reported by Stan Moore\) [\#1434](https://github.com/kokkos/kokkos/issues/1434)
lib/kokkos/CHANGELOG.md:- CudaUVMSpace::allocate/deallocate must fence [\#1302](https://github.com/kokkos/kokkos/issues/1302)
lib/kokkos/CHANGELOG.md:- ViewResize on CUDA fails in Debug because of too many resources requested [\#1299](https://github.com/kokkos/kokkos/issues/1299)
lib/kokkos/CHANGELOG.md:- Cuda 9 and intrepid2 calls from Panzer. [\#1183](https://github.com/kokkos/kokkos/issues/1183)
lib/kokkos/CHANGELOG.md:- CUDA9: Fix warning for unsupported long double [\#1189](https://github.com/kokkos/kokkos/issues/1189)
lib/kokkos/CHANGELOG.md:- CUDA9: fix warning on defaulted function marking [\#1188](https://github.com/kokkos/kokkos/issues/1188)
lib/kokkos/CHANGELOG.md:- CUDA9: fix warnings for deprecated warp level functions [\#1187](https://github.com/kokkos/kokkos/issues/1187)
lib/kokkos/CHANGELOG.md:- Add CUDA 9.0 nightly testing [\#1174](https://github.com/kokkos/kokkos/issues/1174)
lib/kokkos/CHANGELOG.md:- KOKKOS\_HAVE\_CUDA\_LAMBDA became KOKKOS\_CUDA\_USE\_LAMBDA [\#1274](https://github.com/kokkos/kokkos/issues/1274)
lib/kokkos/CHANGELOG.md:- CUDA9: compiler error with static assert template arguments [\#1190](https://github.com/kokkos/kokkos/issues/1190)
lib/kokkos/CHANGELOG.md:- Kokkos\_ENABLE\_Cuda\_Lambda should default ON [\#1101](https://github.com/kokkos/kokkos/issues/1101)
lib/kokkos/CHANGELOG.md:- Fix ROCm:  Performance tests not building [\#1038](https://github.com/kokkos/kokkos/issues/1038)
lib/kokkos/CHANGELOG.md:- Improve subview construction on Cuda backend [\#615](https://github.com/kokkos/kokkos/issues/615)
lib/kokkos/CHANGELOG.md:- Cuda launch bounds performance regression bug [\#1140](https://github.com/kokkos/kokkos/issues/1140)
lib/kokkos/CHANGELOG.md:- CUDA compile error [\#1128](https://github.com/kokkos/kokkos/issues/1128)
lib/kokkos/CHANGELOG.md:- subview construction on Cuda backend [\#615](https://github.com/kokkos/kokkos/issues/615)
lib/kokkos/CHANGELOG.md:- Fix MD iteration policy ignores lower bound on GPUs [\#1041](https://github.com/kokkos/kokkos/issues/1041)
lib/kokkos/CHANGELOG.md:- (Experimental) ROCm:  algorithms/unit\_tests test\_sort failing with segfault [\#1070](https://github.com/kokkos/kokkos/issues/1070)
lib/kokkos/CHANGELOG.md:- Added ROCm backend to support AMD GPUs
lib/kokkos/CHANGELOG.md:- Add view label to `View bounds error` message for CUDA backend [\#870](https://github.com/kokkos/kokkos/issues/870)
lib/kokkos/CHANGELOG.md:- Warnings about lock\_address\_cuda\_space [\#852](https://github.com/kokkos/kokkos/issues/852)
lib/kokkos/CHANGELOG.md:- Fix Compiler warnings when compiling core unit tests for Cuda [\#214](https://github.com/kokkos/kokkos/issues/214)
lib/kokkos/CHANGELOG.md:- CudaClang: Fix failing test with Clang 4.0 [\#941](https://github.com/kokkos/kokkos/issues/941)
lib/kokkos/CHANGELOG.md:- task\_depend test failing, CUDA 8.0 + Pascal + RDC [\#829](https://github.com/kokkos/kokkos/issues/829)
lib/kokkos/CHANGELOG.md:- Kokkos CUDA failes to compile with OMPI\_CXX and MPICH\_CXX wrappers [\#776](https://github.com/kokkos/kokkos/issues/776)
lib/kokkos/CHANGELOG.md:- CUDA stack overflow with TaskDAG test [\#758](https://github.com/kokkos/kokkos/issues/758)
lib/kokkos/CHANGELOG.md:- TeamVector test on Cuda [\#670](https://github.com/kokkos/kokkos/issues/670)
lib/kokkos/CHANGELOG.md:- Clang 4.0 Cuda Build broken again [\#560](https://github.com/kokkos/kokkos/issues/560)
lib/kokkos/CHANGELOG.md:- Abort when running on a NVIDIA CC5.0 or higher architecture with code compiled for CC \< 5.0 [\#813](https://github.com/kokkos/kokkos/issues/813)
lib/kokkos/CHANGELOG.md:- TeamScratch Level 1 on Cuda hangs [\#820](https://github.com/kokkos/kokkos/issues/820)
lib/kokkos/CHANGELOG.md:- CUDA stack overflow with TaskDAG test [\#758](https://github.com/kokkos/kokkos/issues/758)
lib/kokkos/CHANGELOG.md:- Cuda: architecture flag not added to link line [\#688](https://github.com/kokkos/kokkos/issues/688)
lib/kokkos/CHANGELOG.md:- Add CMake option to enable Cuda Lambda support [\#589](https://github.com/kokkos/kokkos/issues/589)
lib/kokkos/CHANGELOG.md:- Add CMake option to enable Cuda RDC support [\#588](https://github.com/kokkos/kokkos/issues/588)
lib/kokkos/CHANGELOG.md:- nvcc\_wrapper not installed with Kokkos built with CUDA through CMake [\#543](https://github.com/kokkos/kokkos/issues/543)
lib/kokkos/CHANGELOG.md:- Cuda UVM Allocation test broken with UVM as default space [\#586](https://github.com/kokkos/kokkos/issues/586)
lib/kokkos/CHANGELOG.md:- Makefile.kokkos adds expt-extended-lambda to cuda build with clang [\#490](https://github.com/kokkos/kokkos/issues/490)
lib/kokkos/CHANGELOG.md:- Add official Cuda 8.0 support [\#468](https://github.com/kokkos/kokkos/issues/468)
lib/kokkos/CHANGELOG.md:- Add Clang 4.0+ compilation of Cuda code [\#455](https://github.com/kokkos/kokkos/issues/455)
lib/kokkos/CHANGELOG.md:- CudaUVMSpace should track \# allocations, due to CUDA limit on \# UVM allocations [\#300](https://github.com/kokkos/kokkos/issues/300)
lib/kokkos/CHANGELOG.md:- Bug in TestCuda\_Other.cpp: most likely assembly inserted into Device code [\#515](https://github.com/kokkos/kokkos/issues/515)
lib/kokkos/CHANGELOG.md:- Cuda Compute Capability check of GPU is outdated [\#509](https://github.com/kokkos/kokkos/issues/509)
lib/kokkos/CHANGELOG.md:- Makefiles for test and examples have issues in Cuda when CXX is not explicitly specified [\#497](https://github.com/kokkos/kokkos/issues/497)
lib/kokkos/CHANGELOG.md:- const subview of const view with compile time dimensions on Cuda backend [\#310](https://github.com/kokkos/kokkos/issues/310)
lib/kokkos/CHANGELOG.md:- Kokkos \(in Trilinos\) Causes Internal Compiler Error on CUDA 8.0.21-EA on POWER8 [\#307](https://github.com/kokkos/kokkos/issues/307)
lib/kokkos/CHANGELOG.md:- TaskPolicy\<Cuda\> performance requires teams mapped to warps [\#218](https://github.com/kokkos/kokkos/issues/218)
lib/kokkos/CHANGELOG.md:- Failing Tests on NVIDIA Pascal GPUs [\#398](https://github.com/kokkos/kokkos/issues/398)
lib/kokkos/CHANGELOG.md:- Build warning \(signed / unsigned comparison\) in Cuda implementation [\#365](https://github.com/kokkos/kokkos/issues/365)
lib/kokkos/CHANGELOG.md:- wrong results for a parallel\_reduce with CUDA8 / Maxwell50 [\#352](https://github.com/kokkos/kokkos/issues/352)
lib/kokkos/CHANGELOG.md:- Unit tests with Cuda - Maxwell [\#196](https://github.com/kokkos/kokkos/issues/196)
lib/kokkos/CHANGELOG.md:- Less restrictive TeamPolicy reduction on Cuda [\#286](https://github.com/kokkos/kokkos/issues/286)
lib/kokkos/CHANGELOG.md:- Fence CudaUVMSpace allocations [\#230](https://github.com/kokkos/kokkos/issues/230)
lib/kokkos/CHANGELOG.md:- Develop TaskPolicy for CUDA [\#142](https://github.com/kokkos/kokkos/issues/142)
lib/kokkos/CHANGELOG.md:- Make tests pass with -expt-extended-lambda on CUDA [\#108](https://github.com/kokkos/kokkos/issues/108)
lib/kokkos/CHANGELOG.md:- UVM allocations in multi-GPU systems [\#50](https://github.com/kokkos/kokkos/issues/50)
lib/kokkos/CHANGELOG.md:- seg fault Kokkos::Impl::CudaInternal::print\_configuration [\#338](https://github.com/kokkos/kokkos/issues/338)
lib/kokkos/CHANGELOG.md:- MemoryPool fails to compile on non-cuda non-x86 [\#297](https://github.com/kokkos/kokkos/issues/297)
lib/kokkos/CHANGELOG.md:- Arbitrary-sized atomics on GPUs broken; loop forever [\#238](https://github.com/kokkos/kokkos/issues/238)
lib/kokkos/CHANGELOG.md:- fatal error: Cuda/Kokkos\_Cuda\_abort.hpp: No such file or directory [\#157](https://github.com/kokkos/kokkos/issues/157)
lib/kokkos/CHANGELOG.md:- parallel\_launch\_local\_memory and cuda 7.5 [\#125](https://github.com/kokkos/kokkos/issues/125)
lib/kokkos/CHANGELOG.md:- Resize can fail with Cuda due to asynchronous dispatch [\#119](https://github.com/kokkos/kokkos/issues/119)
lib/kokkos/CHANGELOG.md:- Throw with Cuda when using \(2D\) team\_policy parallel\_reduce with less than a warp size [\#76](https://github.com/kokkos/kokkos/issues/76)
lib/kokkos/CHANGELOG.md:- Reduce the number of threads per team for Cuda [\#63](https://github.com/kokkos/kokkos/issues/63)
lib/kokkos/CHANGELOG.md:- Named Kernels fail for reductions with CUDA [\#60](https://github.com/kokkos/kokkos/issues/60)
lib/kokkos/CHANGELOG.md:- develop branch broken with CUDA 8 and --expt-extended-lambda  [\#354](https://github.com/kokkos/kokkos/issues/354)
lib/kokkos/CHANGELOG.md:- Error building with Cuda when passing -DKOKKOS\_CUDA\_USE\_LAMBDA to generate\_makefile.bash [\#343](https://github.com/kokkos/kokkos/issues/343)
lib/kokkos/CHANGELOG.md:- build of Kokkos\_Sparse\_MV\_impl\_spmv\_Serial.cpp.o fails if you use nvcc and have cuda disabled [\#209](https://github.com/kokkos/kokkos/issues/209)
lib/kokkos/CHANGELOG.md:- Unit test failure on Hansen  KokkosCore\_UnitTest\_Cuda\_MPI\_1 [\#200](https://github.com/kokkos/kokkos/issues/200)
lib/kokkos/CHANGELOG.md:- Signed/unsigned  comparison warning in CUDA parallel [\#130](https://github.com/kokkos/kokkos/issues/130)
lib/kokkos/CHANGELOG.md:- Kokkos shared memory on Cuda uses a lot of registers [\#31](https://github.com/kokkos/kokkos/issues/31)
lib/kokkos/CHANGELOG.md:- Can not pass unit test `cuda.space` without a GT 720 [\#25](https://github.com/kokkos/kokkos/issues/25)
lib/kokkos/CHANGELOG.md:- Kokkos can not complete unit tests with CUDA UVM enabled [\#23](https://github.com/kokkos/kokkos/issues/23)
lib/kokkos/README.md:CUDA, HIP, SYCL, HPX, OpenMP and C++ threads as backend programming models with several other
lib/kokkos/algorithms/CMakeLists.txt:# FIXME_OPENACC: temporarily disabled due to unimplemented features
lib/kokkos/algorithms/CMakeLists.txt:IF(NOT ((KOKKOS_ENABLE_OPENMPTARGET AND KOKKOS_CXX_COMPILER_ID STREQUAL NVHPC) OR KOKKOS_ENABLE_OPENACC))
lib/kokkos/algorithms/unit_tests/TestSort.hpp:  exec.fence();  // Need this fence to prevent BusError with Cuda
lib/kokkos/algorithms/unit_tests/TestStdAlgorithmsTeamInclusiveScan.cpp:#if defined(KOKKOS_ENABLE_OPENMPTARGET) && defined(KOKKOS_ARCH_INTEL_GPU)
lib/kokkos/algorithms/unit_tests/TestStdAlgorithmsTeamInclusiveScan.cpp:  GTEST_SKIP() << "the test is known to fail with OpenMPTarget on Intel GPUs";
lib/kokkos/algorithms/unit_tests/TestStdAlgorithmsTeamRemoveCopyIf.cpp:#if defined(KOKKOS_ENABLE_OPENMPTARGET) && defined(KOKKOS_ARCH_INTEL_GPU)
lib/kokkos/algorithms/unit_tests/TestStdAlgorithmsTeamRemoveCopyIf.cpp:  GTEST_SKIP() << "the test is known to fail with OpenMPTarget on Intel GPUs";
lib/kokkos/algorithms/unit_tests/TestStdAlgorithmsMismatch.cpp:  // otherwise we get an error for CUDA NVCC DEBUG CI
lib/kokkos/algorithms/unit_tests/Makefile:ifneq (,$(findstring Cuda,$(KOKKOS_DEVICES)))
lib/kokkos/algorithms/unit_tests/Makefile:ifeq ($(KOKKOS_INTERNAL_USE_CUDA), 1)
lib/kokkos/algorithms/unit_tests/Makefile:	OBJ_CUDA = TestCuda.o TestStdAlgorithmsCommon.o UnitTestMain.o gtest-all.o
lib/kokkos/algorithms/unit_tests/Makefile:	TARGETS += KokkosAlgorithms_UnitTest_Cuda
lib/kokkos/algorithms/unit_tests/Makefile:	TEST_TARGETS += test-cuda
lib/kokkos/algorithms/unit_tests/Makefile:KokkosAlgorithms_UnitTest_Cuda: $(OBJ_CUDA) $(KOKKOS_LINK_DEPENDS)
lib/kokkos/algorithms/unit_tests/Makefile:	$(LINK) $(EXTRA_PATH) $(OBJ_CUDA) $(KOKKOS_LIBS) $(LIB) $(KOKKOS_LDFLAGS) $(LDFLAGS) -o KokkosAlgorithms_UnitTest_Cuda
lib/kokkos/algorithms/unit_tests/Makefile:test-cuda: KokkosAlgorithms_UnitTest_Cuda
lib/kokkos/algorithms/unit_tests/Makefile:	./KokkosAlgorithms_UnitTest_Cuda
lib/kokkos/algorithms/unit_tests/CMakeLists.txt:foreach(Tag Threads;Serial;OpenMP;Cuda;HPX;HIP;SYCL;OpenMPTarget)
lib/kokkos/algorithms/unit_tests/CMakeLists.txt:# when compiling for Intel's Xe-HP GPUs.
lib/kokkos/algorithms/unit_tests/CMakeLists.txt:# when compiling for Intel's Xe-HP GPUs.
lib/kokkos/algorithms/unit_tests/CMakeLists.txt:# when compiling for Intel's Xe-HP GPUs.
lib/kokkos/algorithms/unit_tests/TestStdAlgorithmsTeamRemoveCopy.cpp:#if defined(KOKKOS_ENABLE_OPENMPTARGET) && defined(KOKKOS_ARCH_INTEL_GPU)
lib/kokkos/algorithms/unit_tests/TestStdAlgorithmsTeamRemoveCopy.cpp:  GTEST_SKIP() << "the test is known to fail with OpenMPTarget on Intel GPUs";
lib/kokkos/algorithms/unit_tests/TestStdAlgorithmsModSeqOps.cpp:// cuda illegal instruction error appears for this one:
lib/kokkos/algorithms/unit_tests/TestStdAlgorithmsTeamExclusiveScan.cpp:#if defined(KOKKOS_ENABLE_OPENMPTARGET) && defined(KOKKOS_ARCH_INTEL_GPU)
lib/kokkos/algorithms/unit_tests/TestStdAlgorithmsTeamExclusiveScan.cpp:  GTEST_SKIP() << "the test is known to fail with OpenMPTarget on Intel GPUs";
lib/kokkos/algorithms/unit_tests/TestStdAlgorithmsTeamCopyIf.cpp:#if defined(KOKKOS_ENABLE_OPENMPTARGET) && defined(KOKKOS_ARCH_INTEL_GPU)
lib/kokkos/algorithms/unit_tests/TestStdAlgorithmsTeamCopyIf.cpp:  GTEST_SKIP() << "the test is known to fail with OpenMPTarget on Intel GPUs";
lib/kokkos/algorithms/unit_tests/TestRandom.hpp:#if defined(KOKKOS_ENABLE_SYCL) || defined(KOKKOS_ENABLE_CUDA) || \
lib/kokkos/algorithms/unit_tests/TestRandom.hpp:#if defined(KOKKOS_ENABLE_SYCL) || defined(KOKKOS_ENABLE_CUDA) || \
lib/kokkos/algorithms/unit_tests/TestRandom.hpp:#if defined(KOKKOS_ENABLE_SYCL) && defined(KOKKOS_IMPL_ARCH_NVIDIA_GPU)
lib/kokkos/algorithms/unit_tests/TestRandom.hpp:    GTEST_SKIP() << "Failing on NVIDIA GPUs";  // FIXME_SYCL
lib/kokkos/algorithms/unit_tests/TestStdAlgorithmsTeamUniqueCopy.cpp:#if defined(KOKKOS_ENABLE_OPENMPTARGET) && defined(KOKKOS_ARCH_INTEL_GPU)
lib/kokkos/algorithms/unit_tests/TestStdAlgorithmsTeamUniqueCopy.cpp:  GTEST_SKIP() << "the test is known to fail with OpenMPTarget on Intel GPUs";
lib/kokkos/algorithms/src/Kokkos_Random.hpp:      //will have its private generator. Note: on Cuda getting a state involves atomics,
lib/kokkos/algorithms/src/Kokkos_Random.hpp:#ifdef KOKKOS_ENABLE_CUDA
lib/kokkos/algorithms/src/Kokkos_Random.hpp:struct Random_XorShift1024_UseCArrayState<Kokkos::Cuda> : std::false_type {};
lib/kokkos/algorithms/src/Kokkos_Random.hpp:#if defined(KOKKOS_ENABLE_CUDA) || defined(KOKKOS_ENABLE_HIP)
lib/kokkos/algorithms/src/Kokkos_Random.hpp:#if defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/algorithms/src/Kokkos_Random.hpp:#define KOKKOS_IMPL_EXECUTION_SPACE_CUDA_OR_HIP Kokkos::Cuda
lib/kokkos/algorithms/src/Kokkos_Random.hpp:#define KOKKOS_IMPL_EXECUTION_SPACE_CUDA_OR_HIP Kokkos::HIP
lib/kokkos/algorithms/src/Kokkos_Random.hpp:    Kokkos::Device<KOKKOS_IMPL_EXECUTION_SPACE_CUDA_OR_HIP, MemorySpace>> {
lib/kokkos/algorithms/src/Kokkos_Random.hpp:      View<int**, Kokkos::Device<KOKKOS_IMPL_EXECUTION_SPACE_CUDA_OR_HIP,
lib/kokkos/algorithms/src/Kokkos_Random.hpp:#undef KOKKOS_IMPL_EXECUTION_SPACE_CUDA_OR_HIP
lib/kokkos/algorithms/src/sorting/impl/Kokkos_SortImpl.hpp:#if defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/algorithms/src/sorting/impl/Kokkos_SortImpl.hpp:// Also see https://github.com/NVIDIA/cub/pull/170.
lib/kokkos/algorithms/src/sorting/impl/Kokkos_SortImpl.hpp://    <snip>/thrust/system/cuda/detail/core/agent_launcher.h:557:11:
lib/kokkos/algorithms/src/sorting/impl/Kokkos_SortImpl.hpp:// The exact combination of versions for Clang and Thrust (or CUDA) for this
lib/kokkos/algorithms/src/sorting/impl/Kokkos_SortImpl.hpp:// (Clang 10.0.0 and Cuda 10.0) demonstrated failure.
lib/kokkos/algorithms/src/sorting/impl/Kokkos_SortImpl.hpp:#if defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/algorithms/src/sorting/impl/Kokkos_SortImpl.hpp:void sort_cudathrust(const Cuda& space,
lib/kokkos/algorithms/src/sorting/impl/Kokkos_SortImpl.hpp:  const auto exec = thrust::cuda::par.on(space.cuda_stream());
lib/kokkos/algorithms/src/sorting/impl/Kokkos_SortImpl.hpp:#if defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/algorithms/src/sorting/impl/Kokkos_SortImpl.hpp:    const Cuda& exec, const Kokkos::View<DataType, Properties...>& view) {
lib/kokkos/algorithms/src/sorting/impl/Kokkos_SortImpl.hpp:  sort_cudathrust(exec, view);
lib/kokkos/algorithms/src/sorting/impl/Kokkos_SortImpl.hpp:#if defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/algorithms/src/sorting/impl/Kokkos_SortImpl.hpp:    const Cuda& exec, const Kokkos::View<DataType, Properties...>& view,
lib/kokkos/algorithms/src/sorting/impl/Kokkos_SortImpl.hpp:  sort_cudathrust(exec, view, comparator);
lib/kokkos/algorithms/src/sorting/impl/Kokkos_SortByKeyImpl.hpp:#if defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/algorithms/src/sorting/impl/Kokkos_SortByKeyImpl.hpp:// Also see https://github.com/NVIDIA/cub/pull/170.
lib/kokkos/algorithms/src/sorting/impl/Kokkos_SortByKeyImpl.hpp://    <snip>/thrust/system/cuda/detail/core/agent_launcher.h:557:11:
lib/kokkos/algorithms/src/sorting/impl/Kokkos_SortByKeyImpl.hpp:// The exact combination of versions for Clang and Thrust (or CUDA) for this
lib/kokkos/algorithms/src/sorting/impl/Kokkos_SortByKeyImpl.hpp:// (Clang 10.0.0 and Cuda 10.0) demonstrated failure.
lib/kokkos/algorithms/src/sorting/impl/Kokkos_SortByKeyImpl.hpp:#if defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/algorithms/src/sorting/impl/Kokkos_SortByKeyImpl.hpp:inline constexpr bool sort_on_device_v<Kokkos::Cuda, Layout> = true;
lib/kokkos/algorithms/src/sorting/impl/Kokkos_SortByKeyImpl.hpp:void sort_by_key_cudathrust(
lib/kokkos/algorithms/src/sorting/impl/Kokkos_SortByKeyImpl.hpp:    const Kokkos::Cuda& exec,
lib/kokkos/algorithms/src/sorting/impl/Kokkos_SortByKeyImpl.hpp:  const auto policy = thrust::cuda::par.on(exec.cuda_stream());
lib/kokkos/algorithms/src/sorting/impl/Kokkos_SortByKeyImpl.hpp:#if defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/algorithms/src/sorting/impl/Kokkos_SortByKeyImpl.hpp:    const Kokkos::Cuda& exec,
lib/kokkos/algorithms/src/sorting/impl/Kokkos_SortByKeyImpl.hpp:  sort_by_key_cudathrust(exec, keys, values);
lib/kokkos/algorithms/src/sorting/impl/Kokkos_SortByKeyImpl.hpp:#if defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/algorithms/src/sorting/impl/Kokkos_SortByKeyImpl.hpp:    const Kokkos::Cuda& exec,
lib/kokkos/algorithms/src/sorting/impl/Kokkos_SortByKeyImpl.hpp:  sort_by_key_cudathrust(exec, keys, values, comparator);
lib/kokkos/algorithms/src/std_algorithms/impl/Kokkos_MustUseKokkosSingleInTeam.hpp:// FIXME_OPENACC
lib/kokkos/algorithms/src/std_algorithms/impl/Kokkos_MustUseKokkosSingleInTeam.hpp:#if defined(KOKKOS_ENABLE_OPENACC)
lib/kokkos/algorithms/src/std_algorithms/impl/Kokkos_MustUseKokkosSingleInTeam.hpp:    Kokkos::Experimental::OpenACC> : std::true_type {};
lib/kokkos/algorithms/src/std_algorithms/impl/Kokkos_InclusiveScan.hpp:  // #if defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/algorithms/src/std_algorithms/impl/Kokkos_InclusiveScan.hpp:  // #if defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/algorithms/src/std_algorithms/impl/Kokkos_InclusiveScan.hpp:  // #if defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/tpls/desul/include/desul/atomics/Common.hpp:// Device or socket scope (i.e. a CPU socket, a single GPU)
lib/kokkos/tpls/desul/include/desul/atomics/Common.hpp:// support on device Currently that is still considered experimental on CUDA and
lib/kokkos/tpls/desul/include/desul/atomics/Thread_Fence_OpenACC.hpp:#ifndef DESUL_ATOMICS_THREAD_FENCE_OPENACC_HPP_
lib/kokkos/tpls/desul/include/desul/atomics/Thread_Fence_OpenACC.hpp:#define DESUL_ATOMICS_THREAD_FENCE_OPENACC_HPP_
lib/kokkos/tpls/desul/include/desul/atomics/Thread_Fence_OpenACC.hpp:  // FIXME_OPENACC: The current OpenACC standard does not support explicit thread fence
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Based_Fetch_Op.hpp:#ifdef DESUL_HAVE_CUDA_ATOMICS
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Based_Fetch_Op.hpp:#include <desul/atomics/Lock_Based_Fetch_Op_CUDA.hpp>
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Based_Fetch_Op.hpp:#ifdef DESUL_HAVE_OPENACC_ATOMICS
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Based_Fetch_Op.hpp:#include <desul/atomics/Lock_Based_Fetch_Op_OpenACC.hpp>
lib/kokkos/tpls/desul/include/desul/atomics/Thread_Fence.hpp:#ifdef DESUL_HAVE_CUDA_ATOMICS
lib/kokkos/tpls/desul/include/desul/atomics/Thread_Fence.hpp:#include <desul/atomics/Thread_Fence_CUDA.hpp>
lib/kokkos/tpls/desul/include/desul/atomics/Thread_Fence.hpp:#ifdef DESUL_HAVE_OPENACC_ATOMICS
lib/kokkos/tpls/desul/include/desul/atomics/Thread_Fence.hpp:#include <desul/atomics/Thread_Fence_OpenACC.hpp>
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Array.hpp:#ifdef DESUL_HAVE_CUDA_ATOMICS
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Array.hpp:#include <desul/atomics/Lock_Array_CUDA.hpp>
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Array.hpp:#ifdef DESUL_HAVE_CUDA_ATOMICS
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Array.hpp:  init_lock_arrays_cuda();
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Array.hpp:#ifdef DESUL_HAVE_CUDA_ATOMICS
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Array.hpp:  finalize_lock_arrays_cuda();
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Array.hpp:#ifdef DESUL_HAVE_CUDA_ATOMICS
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Array.hpp:  ensure_cuda_lock_arrays_on_device();
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Array_HIP.hpp:/// \brief After this call, the g_host_cuda_lock_arrays variable has
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op.hpp:#ifdef DESUL_HAVE_CUDA_ATOMICS
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op.hpp:#include <desul/atomics/Fetch_Op_CUDA.hpp>
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op.hpp:#ifdef DESUL_HAVE_OPENACC_ATOMICS
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op.hpp:#include <desul/atomics/Fetch_Op_OpenACC.hpp>
lib/kokkos/tpls/desul/include/desul/atomics/Macros.hpp:#ifdef DESUL_ATOMICS_ENABLE_CUDA
lib/kokkos/tpls/desul/include/desul/atomics/Macros.hpp:#if (defined(__clang__) && defined(__CUDA__) && defined(__CLANG_RDC__)) || \
lib/kokkos/tpls/desul/include/desul/atomics/Macros.hpp:    defined(__CUDACC_RDC__)
lib/kokkos/tpls/desul/include/desul/atomics/Macros.hpp:#define DESUL_IMPL_CUDA_RDC
lib/kokkos/tpls/desul/include/desul/atomics/Macros.hpp:#if (defined(DESUL_ATOMICS_ENABLE_CUDA_SEPARABLE_COMPILATION) &&  \
lib/kokkos/tpls/desul/include/desul/atomics/Macros.hpp:     !defined(DESUL_IMPL_CUDA_RDC)) ||                            \
lib/kokkos/tpls/desul/include/desul/atomics/Macros.hpp:    (!defined(DESUL_ATOMICS_ENABLE_CUDA_SEPARABLE_COMPILATION) && \
lib/kokkos/tpls/desul/include/desul/atomics/Macros.hpp:     defined(DESUL_IMPL_CUDA_RDC))
lib/kokkos/tpls/desul/include/desul/atomics/Macros.hpp:#ifdef DESUL_IMPL_CUDA_RDC
lib/kokkos/tpls/desul/include/desul/atomics/Macros.hpp:#undef DESUL_IMPL_CUDA_RDC
lib/kokkos/tpls/desul/include/desul/atomics/Macros.hpp:#if defined(DESUL_ATOMICS_ENABLE_CUDA) && defined(__CUDACC__)
lib/kokkos/tpls/desul/include/desul/atomics/Macros.hpp:#define DESUL_HAVE_CUDA_ATOMICS
lib/kokkos/tpls/desul/include/desul/atomics/Macros.hpp:#if defined(DESUL_ATOMICS_ENABLE_OPENACC)
lib/kokkos/tpls/desul/include/desul/atomics/Macros.hpp:#define DESUL_HAVE_OPENACC_ATOMICS
lib/kokkos/tpls/desul/include/desul/atomics/Macros.hpp:#if defined(DESUL_HAVE_CUDA_ATOMICS) || defined(DESUL_HAVE_HIP_ATOMICS)
lib/kokkos/tpls/desul/include/desul/atomics/Macros.hpp:#if (defined(DESUL_ATOMICS_ENABLE_CUDA) && defined(__CUDACC__)) && defined(__NVCOMPILER)
lib/kokkos/tpls/desul/include/desul/atomics/Macros.hpp:#if defined(DESUL_HAVE_OPENACC_ATOMICS)
lib/kokkos/tpls/desul/include/desul/atomics/Macros.hpp:#include <openacc.h>
lib/kokkos/tpls/desul/include/desul/atomics/Macros.hpp:// FIXME_OPENACC We cannot determine in a constant expresion whether we are on host or
lib/kokkos/tpls/desul/include/desul/atomics/Macros.hpp:#if (defined(DESUL_ATOMICS_ENABLE_CUDA) && defined(__CUDA_ARCH__)) ||         \
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_OpenACC.hpp:#ifndef DESUL_ATOMICS_FETCH_OP_OPENACC_HPP_
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_OpenACC.hpp:#define DESUL_ATOMICS_FETCH_OP_OPENACC_HPP_
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_OpenACC.hpp:inline constexpr bool is_openacc_integral_type_v =
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_OpenACC.hpp:inline constexpr bool is_openacc_arithmetic_type_v = std::is_same_v<T, float> ||
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_OpenACC.hpp:#ifndef DESUL_CUDA_ARCH_IS_PRE_PASCAL
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_OpenACC.hpp:                                                     is_openacc_integral_type_v<T>;
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_OpenACC.hpp:inline constexpr bool is_openacc_integral_type_v = std::is_integral_v<T>;
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_OpenACC.hpp:inline constexpr bool is_openacc_arithmetic_type_v = std::is_arithmetic_v<T>;
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_OpenACC.hpp:std::enable_if_t<is_openacc_arithmetic_type_v<T>, T> device_atomic_fetch_add(
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_OpenACC.hpp:std::enable_if_t<is_openacc_arithmetic_type_v<T>, T> device_atomic_fetch_inc(
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_OpenACC.hpp:std::enable_if_t<is_openacc_arithmetic_type_v<T>, T> device_atomic_fetch_sub(
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_OpenACC.hpp:std::enable_if_t<is_openacc_arithmetic_type_v<T>, T> device_atomic_fetch_dec(
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_OpenACC.hpp:std::enable_if_t<is_openacc_arithmetic_type_v<T>, T> device_atomic_fetch_mul(
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_OpenACC.hpp:std::enable_if_t<is_openacc_arithmetic_type_v<T>, T> device_atomic_fetch_div(
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_OpenACC.hpp:std::enable_if_t<is_openacc_integral_type_v<T>, T> device_atomic_fetch_lshift(
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_OpenACC.hpp:std::enable_if_t<is_openacc_integral_type_v<T>, T> device_atomic_fetch_rshift(
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_OpenACC.hpp:std::enable_if_t<is_openacc_arithmetic_type_v<T>, T> device_atomic_fetch_max(
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_OpenACC.hpp:std::enable_if_t<is_openacc_arithmetic_type_v<T>, T> device_atomic_fetch_min(
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_OpenACC.hpp:std::enable_if_t<is_openacc_integral_type_v<T>, T> device_atomic_fetch_and(
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_OpenACC.hpp:std::enable_if_t<is_openacc_integral_type_v<T>, T> device_atomic_fetch_or(
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_OpenACC.hpp:std::enable_if_t<is_openacc_integral_type_v<T>, T> device_atomic_fetch_xor(
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_OpenACC.hpp:std::enable_if_t<is_openacc_arithmetic_type_v<T>, T> device_atomic_add_fetch(
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_OpenACC.hpp:std::enable_if_t<is_openacc_arithmetic_type_v<T>, T> device_atomic_inc_fetch(
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_OpenACC.hpp:std::enable_if_t<is_openacc_arithmetic_type_v<T>, T> device_atomic_sub_fetch(
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_OpenACC.hpp:std::enable_if_t<is_openacc_arithmetic_type_v<T>, T> device_atomic_dec_fetch(
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_OpenACC.hpp:std::enable_if_t<is_openacc_arithmetic_type_v<T>, T> device_atomic_mul_fetch(
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_OpenACC.hpp:std::enable_if_t<is_openacc_arithmetic_type_v<T>, T> device_atomic_div_fetch(
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_OpenACC.hpp:std::enable_if_t<is_openacc_integral_type_v<T>, T> device_atomic_lshift_fetch(
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_OpenACC.hpp:std::enable_if_t<is_openacc_integral_type_v<T>, T> device_atomic_rshift_fetch(
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_OpenACC.hpp:std::enable_if_t<is_openacc_arithmetic_type_v<T>, T> device_atomic_max_fetch(
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_OpenACC.hpp:std::enable_if_t<is_openacc_arithmetic_type_v<T>, T> device_atomic_min_fetch(
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_OpenACC.hpp:std::enable_if_t<is_openacc_integral_type_v<T>, T> device_atomic_and_fetch(
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_OpenACC.hpp:std::enable_if_t<is_openacc_integral_type_v<T>, T> device_atomic_or_fetch(
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_OpenACC.hpp:std::enable_if_t<is_openacc_integral_type_v<T>, T> device_atomic_xor_fetch(
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_OpenACC.hpp:std::enable_if_t<is_openacc_arithmetic_type_v<T>, void> device_atomic_store(
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_OpenACC.hpp:std::enable_if_t<is_openacc_arithmetic_type_v<T>, void> device_atomic_store(
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_OpenACC.hpp:        "operation in the OpenACC backend\n");
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_OpenACC.hpp:std::enable_if_t<is_openacc_arithmetic_type_v<T>, T> device_atomic_load(
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_OpenACC.hpp:std::enable_if_t<is_openacc_arithmetic_type_v<T>, T> device_atomic_load(
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_OpenACC.hpp:        "operation in the OpenACC backend\n");
lib/kokkos/tpls/desul/include/desul/atomics/Compare_Exchange_OpenACC.hpp:#ifndef DESUL_ATOMICS_COMPARE_EXCHANGE_OPENACC_HPP_
lib/kokkos/tpls/desul/include/desul/atomics/Compare_Exchange_OpenACC.hpp:#define DESUL_ATOMICS_COMPARE_EXCHANGE_OPENACC_HPP_
lib/kokkos/tpls/desul/include/desul/atomics/Compare_Exchange_OpenACC.hpp:#include <openacc.h>
lib/kokkos/tpls/desul/include/desul/atomics/Compare_Exchange_OpenACC.hpp:#include <desul/atomics/Thread_Fence_OpenACC.hpp>
lib/kokkos/tpls/desul/include/desul/atomics/Compare_Exchange_OpenACC.hpp:    // FIXME_OPENACC
lib/kokkos/tpls/desul/include/desul/atomics/Compare_Exchange_OpenACC.hpp:          "the OpenACC backend\n");
lib/kokkos/tpls/desul/include/desul/atomics/Compare_Exchange_OpenACC.hpp:    // while (!lock_address_openacc((void*)dest, scope)) {
lib/kokkos/tpls/desul/include/desul/atomics/Compare_Exchange_OpenACC.hpp:    // unlock_address_openacc((void*)dest, scope);
lib/kokkos/tpls/desul/include/desul/atomics/Compare_Exchange_OpenACC.hpp:#ifdef DESUL_CUDA_ARCH_IS_PRE_PASCAL
lib/kokkos/tpls/desul/include/desul/atomics/Compare_Exchange_OpenACC.hpp:    // FIXME_OPENACC
lib/kokkos/tpls/desul/include/desul/atomics/Compare_Exchange_OpenACC.hpp:          "operation in the OpenACC backend\n");
lib/kokkos/tpls/desul/include/desul/atomics/Compare_Exchange_OpenACC.hpp:    // while (!lock_address_openacc((void*)dest, scope)) {
lib/kokkos/tpls/desul/include/desul/atomics/Compare_Exchange_OpenACC.hpp:    // unlock_address_openacc((void*)dest, scope);
lib/kokkos/tpls/desul/include/desul/atomics/Compare_Exchange_OpenACC.hpp:    // FIXME_OPENACC
lib/kokkos/tpls/desul/include/desul/atomics/Compare_Exchange_OpenACC.hpp:        "the OpenACC backend\n");
lib/kokkos/tpls/desul/include/desul/atomics/Compare_Exchange_OpenACC.hpp:    // while (!lock_address_openacc((void*)dest, scope)) {
lib/kokkos/tpls/desul/include/desul/atomics/Compare_Exchange_OpenACC.hpp:    // unlock_address_openacc((void*)dest, scope);
lib/kokkos/tpls/desul/include/desul/atomics/Compare_Exchange_OpenACC.hpp:  // FIXME_OPENACC
lib/kokkos/tpls/desul/include/desul/atomics/Compare_Exchange_OpenACC.hpp:      "in the OpenACC backend\n");
lib/kokkos/tpls/desul/include/desul/atomics/Compare_Exchange_OpenACC.hpp:  // while (!lock_address_openacc((void*)dest, scope)) {
lib/kokkos/tpls/desul/include/desul/atomics/Compare_Exchange_OpenACC.hpp:  // unlock_address_openacc((void*)dest, scope);
lib/kokkos/tpls/desul/include/desul/atomics/Thread_Fence_CUDA.hpp:#ifndef DESUL_ATOMICS_THREAD_FENCE_CUDA_HPP_
lib/kokkos/tpls/desul/include/desul/atomics/Thread_Fence_CUDA.hpp:#define DESUL_ATOMICS_THREAD_FENCE_CUDA_HPP_
lib/kokkos/tpls/desul/include/desul/atomics/Thread_Fence_CUDA.hpp:#ifndef DESUL_CUDA_ARCH_IS_PRE_PASCAL
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Based_Fetch_Op_CUDA.hpp:#ifndef DESUL_ATOMICS_LOCK_BASED_FETCH_OP_CUDA_HPP_
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Based_Fetch_Op_CUDA.hpp:#define DESUL_ATOMICS_LOCK_BASED_FETCH_OP_CUDA_HPP_
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Based_Fetch_Op_CUDA.hpp:#include <desul/atomics/Lock_Array_CUDA.hpp>
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Based_Fetch_Op_CUDA.hpp:#include <desul/atomics/Thread_Fence_CUDA.hpp>
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Based_Fetch_Op_CUDA.hpp:      if (lock_address_cuda((void*)dest, scope)) {
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Based_Fetch_Op_CUDA.hpp:        unlock_address_cuda((void*)dest, scope);
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Based_Fetch_Op_CUDA.hpp:      if (lock_address_cuda((void*)dest, scope)) {
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Based_Fetch_Op_CUDA.hpp:        unlock_address_cuda((void*)dest, scope);
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_CUDA.hpp:#ifndef DESUL_ATOMICS_FETCH_OP_CUDA_HPP_
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_CUDA.hpp:#define DESUL_ATOMICS_FETCH_OP_CUDA_HPP_
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_CUDA.hpp:#ifndef DESUL_CUDA_ARCH_IS_PRE_VOLTA
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_CUDA.hpp:#define DESUL_HAVE_CUDA_ATOMICS_ASM
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_CUDA.hpp:#include <desul/atomics/cuda/CUDA_asm.hpp>
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_CUDA.hpp:#ifndef DESUL_CUDA_ARCH_IS_PRE_PASCAL
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_CUDA.hpp:#ifndef DESUL_CUDA_ARCH_IS_PRE_PASCAL
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_CUDA.hpp:#define DESUL_IMPL_CUDA_DEVICE_ATOMIC_FETCH_OP(FETCH_OP, TYPE)                         \
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_CUDA.hpp:#define DESUL_IMPL_CUDA_DEVICE_ATOMIC_FETCH_OP_INTEGRAL(FETCH_OP) \
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_CUDA.hpp:  DESUL_IMPL_CUDA_DEVICE_ATOMIC_FETCH_OP(FETCH_OP, int)           \
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_CUDA.hpp:  DESUL_IMPL_CUDA_DEVICE_ATOMIC_FETCH_OP(FETCH_OP, unsigned int)  \
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_CUDA.hpp:  DESUL_IMPL_CUDA_DEVICE_ATOMIC_FETCH_OP(FETCH_OP, unsigned long long)
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_CUDA.hpp:#ifdef DESUL_CUDA_ARCH_IS_PRE_PASCAL
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_CUDA.hpp:#define DESUL_IMPL_CUDA_DEVICE_ATOMIC_FETCH_OP_FLOATING_POINT(FETCH_OP) \
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_CUDA.hpp:  DESUL_IMPL_CUDA_DEVICE_ATOMIC_FETCH_OP(FETCH_OP, float)
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_CUDA.hpp:#define DESUL_IMPL_CUDA_DEVICE_ATOMIC_FETCH_OP_FLOATING_POINT(FETCH_OP) \
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_CUDA.hpp:  DESUL_IMPL_CUDA_DEVICE_ATOMIC_FETCH_OP(FETCH_OP, float)               \
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_CUDA.hpp:  DESUL_IMPL_CUDA_DEVICE_ATOMIC_FETCH_OP(FETCH_OP, double)
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_CUDA.hpp:DESUL_IMPL_CUDA_DEVICE_ATOMIC_FETCH_OP_INTEGRAL(fetch_min)
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_CUDA.hpp:DESUL_IMPL_CUDA_DEVICE_ATOMIC_FETCH_OP_INTEGRAL(fetch_max)
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_CUDA.hpp:DESUL_IMPL_CUDA_DEVICE_ATOMIC_FETCH_OP_INTEGRAL(fetch_and)
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_CUDA.hpp:DESUL_IMPL_CUDA_DEVICE_ATOMIC_FETCH_OP_INTEGRAL(fetch_or)
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_CUDA.hpp:DESUL_IMPL_CUDA_DEVICE_ATOMIC_FETCH_OP_INTEGRAL(fetch_xor)
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_CUDA.hpp:DESUL_IMPL_CUDA_DEVICE_ATOMIC_FETCH_OP_FLOATING_POINT(fetch_add)
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_CUDA.hpp:DESUL_IMPL_CUDA_DEVICE_ATOMIC_FETCH_OP_INTEGRAL(fetch_add)
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_CUDA.hpp:DESUL_IMPL_CUDA_DEVICE_ATOMIC_FETCH_OP_FLOATING_POINT(fetch_sub)
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_CUDA.hpp:DESUL_IMPL_CUDA_DEVICE_ATOMIC_FETCH_OP_INTEGRAL(fetch_sub)
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_CUDA.hpp:DESUL_IMPL_CUDA_DEVICE_ATOMIC_FETCH_OP_INTEGRAL(fetch_inc)
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_CUDA.hpp:DESUL_IMPL_CUDA_DEVICE_ATOMIC_FETCH_OP_INTEGRAL(fetch_dec)
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_CUDA.hpp:DESUL_IMPL_CUDA_DEVICE_ATOMIC_FETCH_OP(fetch_inc_mod, unsigned int)
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_CUDA.hpp:DESUL_IMPL_CUDA_DEVICE_ATOMIC_FETCH_OP(fetch_dec_mod, unsigned int)
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_CUDA.hpp:#undef DESUL_IMPL_CUDA_DEVICE_ATOMIC_FETCH_OP_FLOATING_POINT
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_CUDA.hpp:#undef DESUL_IMPL_CUDA_DEVICE_ATOMIC_FETCH_OP_INTEGRAL
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_CUDA.hpp:#undef DESUL_IMPL_CUDA_DEVICE_ATOMIC_FETCH_OP
lib/kokkos/tpls/desul/include/desul/atomics/Adapt_SYCL.hpp:// FIXME_SYCL generic_space isn't available yet for CUDA.
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Array_CUDA.hpp:#ifndef DESUL_ATOMICS_LOCK_ARRAY_CUDA_HPP_
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Array_CUDA.hpp:#define DESUL_ATOMICS_LOCK_ARRAY_CUDA_HPP_
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Array_CUDA.hpp:extern int32_t* CUDA_SPACE_ATOMIC_LOCKS_DEVICE_h;
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Array_CUDA.hpp:extern int32_t* CUDA_SPACE_ATOMIC_LOCKS_NODE_h;
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Array_CUDA.hpp:/// \brief After this call, the g_host_cuda_lock_arrays variable has
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Array_CUDA.hpp:void init_lock_arrays_cuda();
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Array_CUDA.hpp:/// \brief After this call, the g_host_cuda_lock_arrays variable has
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Array_CUDA.hpp:void finalize_lock_arrays_cuda();
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Array_CUDA.hpp:/// \brief This global variable in CUDA space is what kernels use
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Array_CUDA.hpp:/// whose definition will be in Kokkos_Cuda_Locks.cpp (and whose declaration
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Array_CUDA.hpp:/// This one instance will be initialized by initialize_host_cuda_lock_arrays
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Array_CUDA.hpp:/// Since the Kokkos_Cuda_Locks.cpp translation unit cannot initialize the
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Array_CUDA.hpp:/// instances in other translation units, we must update this CUDA global
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Array_CUDA.hpp:/// That is the purpose of the ensure_cuda_lock_arrays_on_device function.
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Array_CUDA.hpp:#ifdef DESUL_ATOMICS_ENABLE_CUDA_SEPARABLE_COMPILATION
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Array_CUDA.hpp:    __device__ __constant__ int32_t* CUDA_SPACE_ATOMIC_LOCKS_DEVICE;
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Array_CUDA.hpp:#ifdef DESUL_ATOMICS_ENABLE_CUDA_SEPARABLE_COMPILATION
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Array_CUDA.hpp:    __device__ __constant__ int32_t* CUDA_SPACE_ATOMIC_LOCKS_NODE;
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Array_CUDA.hpp:#define CUDA_SPACE_ATOMIC_MASK 0x1FFFF
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Array_CUDA.hpp:__device__ inline bool lock_address_cuda(void* ptr, desul::MemoryScopeDevice) {
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Array_CUDA.hpp:  offset = offset & CUDA_SPACE_ATOMIC_MASK;
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Array_CUDA.hpp:  return (0 == atomicExch(&desul::Impl::CUDA_SPACE_ATOMIC_LOCKS_DEVICE[offset], 1));
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Array_CUDA.hpp:__device__ inline bool lock_address_cuda(void* ptr, desul::MemoryScopeNode) {
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Array_CUDA.hpp:  offset = offset & CUDA_SPACE_ATOMIC_MASK;
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Array_CUDA.hpp:  return (0 == atomicExch(&desul::Impl::CUDA_SPACE_ATOMIC_LOCKS_NODE[offset], 1));
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Array_CUDA.hpp:__device__ inline void unlock_address_cuda(void* ptr, desul::MemoryScopeDevice) {
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Array_CUDA.hpp:  offset = offset & CUDA_SPACE_ATOMIC_MASK;
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Array_CUDA.hpp:  atomicExch(&desul::Impl::CUDA_SPACE_ATOMIC_LOCKS_DEVICE[offset], 0);
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Array_CUDA.hpp:__device__ inline void unlock_address_cuda(void* ptr, desul::MemoryScopeNode) {
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Array_CUDA.hpp:  offset = offset & CUDA_SPACE_ATOMIC_MASK;
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Array_CUDA.hpp:  atomicExch(&desul::Impl::CUDA_SPACE_ATOMIC_LOCKS_NODE[offset], 0);
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Array_CUDA.hpp:#ifdef DESUL_ATOMICS_ENABLE_CUDA_SEPARABLE_COMPILATION
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Array_CUDA.hpp:    copy_cuda_lock_arrays_to_device() {
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Array_CUDA.hpp:    cudaMemcpyToSymbol(CUDA_SPACE_ATOMIC_LOCKS_DEVICE,
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Array_CUDA.hpp:                       &CUDA_SPACE_ATOMIC_LOCKS_DEVICE_h,
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Array_CUDA.hpp:    cudaMemcpyToSymbol(CUDA_SPACE_ATOMIC_LOCKS_NODE,
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Array_CUDA.hpp:                       &CUDA_SPACE_ATOMIC_LOCKS_NODE_h,
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Array_CUDA.hpp:#ifdef DESUL_ATOMICS_ENABLE_CUDA_SEPARABLE_COMPILATION
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Array_CUDA.hpp:inline void ensure_cuda_lock_arrays_on_device() {}
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Array_CUDA.hpp:static inline void ensure_cuda_lock_arrays_on_device() {
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Array_CUDA.hpp:  Impl::copy_cuda_lock_arrays_to_device();
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Array_CUDA.hpp:#endif /* #ifndef DESUL_ATOMICS_LOCK_ARRAY_CUDA_HPP_ */
lib/kokkos/tpls/desul/include/desul/atomics/Compare_Exchange.hpp:#ifdef DESUL_HAVE_CUDA_ATOMICS
lib/kokkos/tpls/desul/include/desul/atomics/Compare_Exchange.hpp:#include <desul/atomics/Compare_Exchange_CUDA.hpp>
lib/kokkos/tpls/desul/include/desul/atomics/Compare_Exchange.hpp:#ifdef DESUL_HAVE_OPENACC_ATOMICS
lib/kokkos/tpls/desul/include/desul/atomics/Compare_Exchange.hpp:#include <desul/atomics/Compare_Exchange_OpenACC.hpp>
lib/kokkos/tpls/desul/include/desul/atomics/Fetch_Op_HIP.hpp:// atomic min/max gives the wrong results (tested with ROCm 6.0 on Frontier)
lib/kokkos/tpls/desul/include/desul/atomics/Compare_Exchange_CUDA.hpp:#ifndef DESUL_ATOMICS_COMPARE_EXCHANGE_CUDA_HPP_
lib/kokkos/tpls/desul/include/desul/atomics/Compare_Exchange_CUDA.hpp:#define DESUL_ATOMICS_COMPARE_EXCHANGE_CUDA_HPP_
lib/kokkos/tpls/desul/include/desul/atomics/Compare_Exchange_CUDA.hpp:#include <desul/atomics/Lock_Array_CUDA.hpp>
lib/kokkos/tpls/desul/include/desul/atomics/Compare_Exchange_CUDA.hpp:#include <desul/atomics/Thread_Fence_CUDA.hpp>
lib/kokkos/tpls/desul/include/desul/atomics/Compare_Exchange_CUDA.hpp:// Including CUDA ptx based exchange atomics
lib/kokkos/tpls/desul/include/desul/atomics/Compare_Exchange_CUDA.hpp:// We simply can say DESUL proper doesn't support clang CUDA build pre Volta,
lib/kokkos/tpls/desul/include/desul/atomics/Compare_Exchange_CUDA.hpp:// clang with pre Volta as CUDA compiler
lib/kokkos/tpls/desul/include/desul/atomics/Compare_Exchange_CUDA.hpp:#ifndef DESUL_CUDA_ARCH_IS_PRE_VOLTA
lib/kokkos/tpls/desul/include/desul/atomics/Compare_Exchange_CUDA.hpp:#include <desul/atomics/cuda/CUDA_asm_exchange.hpp>
lib/kokkos/tpls/desul/include/desul/atomics/Compare_Exchange_CUDA.hpp:      if (lock_address_cuda((void*)dest, scope)) {
lib/kokkos/tpls/desul/include/desul/atomics/Compare_Exchange_CUDA.hpp:        unlock_address_cuda((void*)dest, scope);
lib/kokkos/tpls/desul/include/desul/atomics/Compare_Exchange_CUDA.hpp:      if (lock_address_cuda((void*)dest, scope)) {
lib/kokkos/tpls/desul/include/desul/atomics/Compare_Exchange_CUDA.hpp:        unlock_address_cuda((void*)dest, scope);
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Based_Fetch_Op_OpenACC.hpp:#ifndef DESUL_ATOMICS_LOCK_BASED_FETCH_OP_OPENACC_HPP_
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Based_Fetch_Op_OpenACC.hpp:#define DESUL_ATOMICS_LOCK_BASED_FETCH_OP_OPENACC_HPP_
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Based_Fetch_Op_OpenACC.hpp:        "the OpenACC backend\n");
lib/kokkos/tpls/desul/include/desul/atomics/Lock_Based_Fetch_Op_OpenACC.hpp:        "the OpenACC backend\n");
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_memorder.inc:#define __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER MemoryOrderRelaxed
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_memorder.inc:#define __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM ".relaxed"
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_memorder.inc:#include "desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc"
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_memorder.inc:#include "desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc"
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_memorder.inc:#undef __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_memorder.inc:#undef __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_memorder.inc:#define __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER MemoryOrderRelease
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_memorder.inc:#define __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM ".release"
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_memorder.inc:#include "desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc"
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_memorder.inc:#include "desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc"
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_memorder.inc:#undef __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_memorder.inc:#undef __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_memorder.inc:#define __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER MemoryOrderAcquire
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_memorder.inc:#define __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM ".acquire"
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_memorder.inc:#include "desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc"
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_memorder.inc:#undef __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_memorder.inc:#undef __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_memorder.inc:#define __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER MemoryOrderAcqRel
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_memorder.inc:#define __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM ".acq_rel"
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_memorder.inc:#include "desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc"
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_memorder.inc:#undef __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_memorder.inc:#undef __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm.inc:#define __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE MemoryScopeDevice
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm.inc:#define __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM ".gpu"
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm.inc:#include "desul/atomics/cuda/cuda_cc7_asm_memorder.inc"
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm.inc:#undef __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm.inc:#undef __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm.inc:#define __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE MemoryScopeNode
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm.inc:#define __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM ".sys"
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm.inc:#include "desul/atomics/cuda/cuda_cc7_asm_memorder.inc"
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm.inc:#undef __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm.inc:#undef __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm.inc:#define __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE MemoryScopeCore
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm.inc:#define __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM ".cta"
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm.inc:#include "desul/atomics/cuda/cuda_cc7_asm_memorder.inc"
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm.inc:#undef __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm.inc:#undef __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc:#ifdef DESUL_IMPL_ATOMIC_CUDA_PTX_ISGLOBAL
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc:#include "cuda_cc7_asm_atomic_op.inc_isglobal"
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc:#ifdef DESUL_IMPL_ATOMIC_CUDA_PTX_PREDICATE
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc:#include "cuda_cc7_asm_atomic_op.inc_predicate"
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_predicate:#define __DESUL_IMPL_CUDA_ASM_ATOMIC_ADD(type,asm_type,reg_type) \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_predicate:inline __device__ void device_atomic_add(type* dest, type value, __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER, __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE) { \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_predicate:          "@p  red.add.global" __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM asm_type " [%0],%1;\n\t" \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_predicate:          "@!p red.add"        __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM asm_type " [%0],%1;\n\t" \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_predicate:#define __DESUL_IMPL_CUDA_ASM_ATOMIC_SUB(type,asm_type,reg_type) \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_predicate:inline __device__ void device_atomic_sub(type* dest, type value, __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER, __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE) { \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_predicate:          "@p  red.add.global" __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM asm_type " [%0],%1;\n\t" \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_predicate:          "@!p red.add"        __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM asm_type " [%0],%1;\n\t" \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_predicate:#define __DESUL_IMPL_CUDA_ASM_ATOMIC_MIN(type,asm_type,reg_type) \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_predicate:inline __device__ void device_atomic_min(type* dest, type value, __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER, __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE) { \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_predicate:          "@p  red.min.global" __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM asm_type " [%0],%1;\n\t" \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_predicate:          "@!p red.min"        __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM asm_type " [%0],%1;\n\t" \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_predicate:#define __DESUL_IMPL_CUDA_ASM_ATOMIC_MAX(type,asm_type,reg_type) \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_predicate:inline __device__ void device_atomic_max(type* dest, type value, __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER, __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE) { \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_predicate:          "@p  red.max.global" __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM asm_type " [%0],%1;\n\t" \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_predicate:          "@!p red.max"        __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM asm_type " [%0],%1;\n\t" \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_predicate:#define __DESUL_IMPL_CUDA_ASM_ATOMIC_INC(type,asm_type,reg_type) \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_predicate:inline __device__ void device_atomic_inc(type* dest, __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER, __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE) { \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_predicate:          "@p  red.inc.global" __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM asm_type " [%0],%1;\n\t" \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_predicate:          "@!p red.inc"        __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM asm_type " [%0],%1;\n\t" \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_predicate:#define __DESUL_IMPL_CUDA_ASM_ATOMIC_DEC(type,asm_type,reg_type) \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_predicate:inline __device__ void device_atomic_dec(type* dest, __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER, __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE) { \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_predicate:          "@p  red.dec.global" __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM asm_type " [%0],%1;\n\t" \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_predicate:          "@!p red.dec"        __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM asm_type " [%0],%1;\n\t" \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_predicate:__DESUL_IMPL_CUDA_ASM_ATOMIC_ADD(float,".f32","f")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_predicate:__DESUL_IMPL_CUDA_ASM_ATOMIC_SUB(float,".f32","f")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_predicate:__DESUL_IMPL_CUDA_ASM_ATOMIC_ADD(double,".f64","d")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_predicate:__DESUL_IMPL_CUDA_ASM_ATOMIC_SUB(double,".f64","d")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_predicate:__DESUL_IMPL_CUDA_ASM_ATOMIC_ADD(uint32_t,".u32","r")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_predicate:__DESUL_IMPL_CUDA_ASM_ATOMIC_SUB(uint32_t,".u32","r")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_predicate:__DESUL_IMPL_CUDA_ASM_ATOMIC_MIN(uint32_t,".u32","r")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_predicate:__DESUL_IMPL_CUDA_ASM_ATOMIC_MAX(uint32_t,".u32","r")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_predicate:__DESUL_IMPL_CUDA_ASM_ATOMIC_INC(uint32_t,".u32","r")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_predicate:__DESUL_IMPL_CUDA_ASM_ATOMIC_DEC(uint32_t,".u32","r")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_predicate:__DESUL_IMPL_CUDA_ASM_ATOMIC_ADD(uint64_t,".u64","l")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_predicate:__DESUL_IMPL_CUDA_ASM_ATOMIC_SUB(uint64_t,".u64","l")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_predicate:__DESUL_IMPL_CUDA_ASM_ATOMIC_MIN(uint64_t,".u64","l")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_predicate:__DESUL_IMPL_CUDA_ASM_ATOMIC_MAX(uint64_t,".u64","l")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_predicate://__DESUL_IMPL_CUDA_ASM_ATOMIC_INC(uint64_t,".u64","l")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_predicate://__DESUL_IMPL_CUDA_ASM_ATOMIC_DEC(uint64_t,".u64","l")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_predicate:__DESUL_IMPL_CUDA_ASM_ATOMIC_ADD(int32_t,".s32","r")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_predicate:__DESUL_IMPL_CUDA_ASM_ATOMIC_SUB(int32_t,".s32","r")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_predicate:__DESUL_IMPL_CUDA_ASM_ATOMIC_MIN(int32_t,".s32","r")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_predicate:__DESUL_IMPL_CUDA_ASM_ATOMIC_MAX(int32_t,".s32","r")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_predicate://__DESUL_IMPL_CUDA_ASM_ATOMIC_INC(int32_t,".s32","r")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_predicate://__DESUL_IMPL_CUDA_ASM_ATOMIC_DEC(int32_t,".s32","r")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_predicate:__DESUL_IMPL_CUDA_ASM_ATOMIC_ADD(int64_t,".u64","l")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_predicate:__DESUL_IMPL_CUDA_ASM_ATOMIC_SUB(int64_t,".u64","l")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_predicate:__DESUL_IMPL_CUDA_ASM_ATOMIC_MIN(int64_t,".s64","l")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_predicate:__DESUL_IMPL_CUDA_ASM_ATOMIC_MAX(int64_t,".s64","l")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_predicate://__DESUL_IMPL_CUDA_ASM_ATOMIC_INC(int64_t,".s64","l")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_predicate://__DESUL_IMPL_CUDA_ASM_ATOMIC_DEC(int64_t,".s64","l")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc:#ifdef DESUL_IMPL_ATOMIC_CUDA_PTX_ISGLOBAL
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc:#include "cuda_cc7_asm_atomic_fetch_op.inc_isglobal"
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc:#ifdef DESUL_IMPL_ATOMIC_CUDA_PTX_PREDICATE
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc:#include "cuda_cc7_asm_atomic_fetch_op.inc_predicate"
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:#define __DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_AND() \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:inline __device__ typename std::enable_if<sizeof(ctype)==4, ctype>::type device_atomic_fetch_and(ctype* dest, ctype value, __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER, __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE) { \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:          "@p  atom.and.global" __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM ".b32" " %0,[%1],%2;\n\t" \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:          "@!p atom.and"        __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM ".b32" " %0,[%1],%2;\n\t" \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:inline __device__ typename std::enable_if<sizeof(ctype)==8, ctype>::type device_atomic_fetch_and(ctype* dest, ctype value, __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER, __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE) { \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:          "@p  atom.and.global" __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM ".b64" " %0,[%1],%2;\n\t" \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:          "@!p atom.and"        __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM ".b64" " %0,[%1],%2;\n\t" \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:#define __DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_OR() \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:inline __device__ typename std::enable_if<sizeof(ctype)==4, ctype>::type device_atomic_fetch_or(ctype* dest, ctype value, __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER, __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE) { \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:          "@p  atom.or.global" __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM ".b32" " %0,[%1],%2;\n\t" \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:          "@!p atom.or"        __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM ".b32" " %0,[%1],%2;\n\t" \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:inline __device__ typename std::enable_if<sizeof(ctype)==8, ctype>::type device_atomic_fetch_or(ctype* dest, ctype value, __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER, __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE) { \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:          "@p  atom.or.global" __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM ".b64" " %0,[%1],%2;\n\t" \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:          "@!p atom.or"        __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM ".b64" " %0,[%1],%2;\n\t" \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:#define __DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_XOR() \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:inline __device__ typename std::enable_if<sizeof(ctype)==4, ctype>::type device_atomic_fetch_xor(ctype* dest, ctype value, __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER, __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE) { \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:          "@p  atom.xor.global" __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM ".b32" " %0,[%1],%2;\n\t" \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:          "@!p atom.xor"        __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM ".b32" " %0,[%1],%2;\n\t" \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:inline __device__ typename std::enable_if<sizeof(ctype)==8, ctype>::type device_atomic_fetch_xor(ctype* dest, ctype value, __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER, __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE) { \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:          "@p  atom.xor.global" __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM ".b64" " %0,[%1],%2;\n\t" \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:          "@!p atom.xor"        __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM ".b64" " %0,[%1],%2;\n\t" \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:#define __DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_ADD(ctype,asm_ctype,reg_ctype,reg_ret_ctype) \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:inline __device__ ctype device_atomic_fetch_add(ctype* dest, ctype value, __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER, __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE) { \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:          "@p  atom.add.global" __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM asm_ctype " %0,[%1],%2;\n\t" \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:          "@!p atom.add"        __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM asm_ctype " %0,[%1],%2;\n\t" \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:#define __DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_SUB(ctype,asm_ctype,reg_ctype,reg_ret_ctype) \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:inline __device__ ctype device_atomic_fetch_sub(ctype* dest, ctype value, __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER, __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE) { \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:          "@p  atom.add.global" __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM asm_ctype " %0,[%1],%2;\n\t" \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:          "@!p atom.add"        __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM asm_ctype " %0,[%1],%2;\n\t" \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:#define __DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_MIN(ctype,asm_ctype,reg_ctype,reg_ret_ctype) \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:inline __device__ ctype device_atomic_fetch_min(ctype* dest, ctype value, __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER, __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE) { \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:          "@p  atom.min.global" __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM asm_ctype " %0,[%1],%2;\n\t" \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:          "@!p atom.min"        __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM asm_ctype " %0,[%1],%2;\n\t" \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:#define __DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_MAX(ctype,asm_ctype,reg_ctype,reg_ret_ctype) \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:inline __device__ ctype device_atomic_fetch_max(ctype* dest, ctype value, __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER, __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE) { \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:          "@p  atom.max.global" __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM asm_ctype " %0,[%1],%2;\n\t" \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:          "@!p atom.max"        __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM asm_ctype " %0,[%1],%2;\n\t" \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:#define __DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_INC(ctype,asm_ctype,reg_ctype,reg_ret_ctype) \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:inline __device__ ctype device_atomic_fetch_inc(ctype* dest, __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER, __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE) { \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:          "@p  atom.inc.global" __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM asm_ctype " %0,[%1],%2;\n\t" \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:          "@!p atom.inc"       __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM asm_ctype " %0,[%1],%2;\n\t" \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:inline __device__ ctype device_atomic_fetch_inc_mod(ctype* dest, ctype limit, __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER, __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE) { \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:          "@p  atom.inc.global" __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM asm_ctype " %0,[%1],%2;\n\t" \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:          "@!p atom.inc"       __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM asm_ctype " %0,[%1],%2;\n\t" \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:#define __DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_DEC(ctype,asm_ctype,reg_ctype,reg_ret_ctype) \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:inline __device__ ctype device_atomic_fetch_dec(ctype* dest, __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER, __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE) { \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:          "@p  atom.dec.global" __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM asm_ctype " %0,[%1],%2;\n\t" \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:          "@!p atom.dec"        __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM asm_ctype " %0,[%1],%2;\n\t" \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:inline __device__ ctype device_atomic_fetch_dec_mod(ctype* dest, ctype limit, __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER, __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE) { \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:          "@p  atom.dec.global" __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM asm_ctype " %0,[%1],%2;\n\t" \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:          "@!p atom.dec"        __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM asm_ctype " %0,[%1],%2;\n\t" \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:#define __DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_BIN_OP() \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_AND() \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_OR() \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_XOR()
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_ADD(float,".f32","f","=f")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_SUB(float,".f32","f","=f")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_ADD(double,".f64","d","=d")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_SUB(double,".f64","d","=d")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_ADD(uint32_t,".u32","r","=r")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_SUB(uint32_t,".u32","r","=r")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_MIN(uint32_t,".u32","r","=r")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_MAX(uint32_t,".u32","r","=r")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_INC(uint32_t,".u32","r","=r")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_DEC(uint32_t,".u32","r","=r")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_ADD(uint64_t,".u64","l","=l")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_SUB(uint64_t,".u64","l","=l")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_MIN(uint64_t,".u64","l","=l")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_MAX(uint64_t,".u64","l","=l")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate://__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_INC(uint64_t,".u64","l","=l")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate://__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_DEC(uint64_t,".u64","l","=l")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_ADD(int32_t,".s32","r","=r")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_SUB(int32_t,".s32","r","=r")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_MIN(int32_t,".s32","r","=r")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_MAX(int32_t,".s32","r","=r")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate://__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_INC(int32_t,".s32","r","=r")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate://__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_DEC(int32_t,".s32","r","=r")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_ADD(int64_t,".u64","l","=l")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_SUB(int64_t,".u64","l","=l")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_MIN(int64_t,".s64","l","=l")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_MAX(int64_t,".s64","l","=l")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate://__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_INC(int64_t,".s64","l","=l")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate://__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_DEC(int64_t,".s64","l","=l")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_BIN_OP()
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:#undef __DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_ADD
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:#undef __DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_MIN
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:#undef __DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_MAX
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:#undef __DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_INC
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:#undef __DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_DEC
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_predicate:#undef __DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_AND
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_exchange_op.inc:#define __DESUL_IMPL_CUDA_ASM_ATOMIC_EXCHANGE() \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_exchange_op.inc:inline __device__ typename ::std::enable_if<sizeof(ctype)==4, ctype>::type device_atomic_exchange(ctype* dest, ctype value, __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER, __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE) { \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_exchange_op.inc:  asm volatile("atom.exch" __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM ".b32" " %0,[%1],%2;" : "=r"(asm_result) : "l"(dest),"r"(asm_value) : "memory"); \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_exchange_op.inc:inline __device__ typename ::std::enable_if<sizeof(ctype)==8, ctype>::type device_atomic_exchange(ctype* dest, ctype value, __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER, __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE) { \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_exchange_op.inc:  asm volatile("atom.exch" __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM ".b64" " %0,[%1],%2;" : "=l"(asm_result) : "l"(dest),"l"(asm_value) : "memory"); \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_exchange_op.inc:#define __DESUL_IMPL_CUDA_ASM_ATOMIC_COMPARE_EXCHANGE() \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_exchange_op.inc:inline __device__ typename ::std::enable_if<sizeof(ctype)==4, ctype>::type device_atomic_compare_exchange(ctype* dest, ctype compare, ctype value, __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER, __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE) { \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_exchange_op.inc:  asm volatile("atom.cas" __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM ".b32" " %0,[%1],%2,%3;" : "=r"(asm_result) : "l"(dest),"r"(asm_compare),"r"(asm_value) : "memory"); \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_exchange_op.inc:inline __device__ typename ::std::enable_if<sizeof(ctype)==8, ctype>::type device_atomic_compare_exchange(ctype* dest, ctype compare, ctype value, __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER, __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE) { \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_exchange_op.inc:  asm volatile("atom.cas" __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM ".b64" " %0,[%1],%2,%3;" : "=l"(asm_result) : "l"(dest),"l"(asm_compare),"l"(asm_value) : "memory"); \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_exchange_op.inc:__DESUL_IMPL_CUDA_ASM_ATOMIC_EXCHANGE()
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_exchange_op.inc:__DESUL_IMPL_CUDA_ASM_ATOMIC_COMPARE_EXCHANGE()
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_exchange_op.inc:#undef __DESUL_IMPL_CUDA_ASM_ATOMIC_EXCHANGE
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_exchange_op.inc:#undef __DESUL_IMPL_CUDA_ASM_ATOMIC_COMPARE_EXCHANGE
lib/kokkos/tpls/desul/include/desul/atomics/cuda/CUDA_asm.hpp:// The __isGlobal intrinsic was only introduced in CUDA 11.2
lib/kokkos/tpls/desul/include/desul/atomics/cuda/CUDA_asm.hpp:// this is a bug in NVHPC, not treating CUDA intrinsics correctly
lib/kokkos/tpls/desul/include/desul/atomics/cuda/CUDA_asm.hpp:#if !defined(DESUL_IMPL_ATOMIC_CUDA_PTX_PREDICATE) && \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/CUDA_asm.hpp:    !defined(DESUL_IMPL_ATOMIC_CUDA_PTX_ISGLOBAL)
lib/kokkos/tpls/desul/include/desul/atomics/cuda/CUDA_asm.hpp:#if ((__CUDACC_VER_MAJOR__ > 11) ||                                   \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/CUDA_asm.hpp:     ((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ > 1))) && \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/CUDA_asm.hpp:#define DESUL_IMPL_ATOMIC_CUDA_PTX_ISGLOBAL
lib/kokkos/tpls/desul/include/desul/atomics/cuda/CUDA_asm.hpp:#define DESUL_IMPL_ATOMIC_CUDA_PTX_PREDICATE
lib/kokkos/tpls/desul/include/desul/atomics/cuda/CUDA_asm.hpp:#include <desul/atomics/cuda/cuda_cc7_asm.inc>
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:#define __DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_AND() \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:inline __device__ typename std::enable_if<sizeof(ctype)==4, ctype>::type device_atomic_fetch_and(ctype* dest, ctype value, __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER, __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE) { \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:  asm volatile("atom.and.global" __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM ".b32" " %0,[%1],%2;" : "=r"(asm_result) : "l"(dest),"r"(asm_value) : "memory"); \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:  asm volatile("atom.and"        __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM ".b32" " %0,[%1],%2;" : "=r"(asm_result) : "l"(dest),"r"(asm_value) : "memory"); \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:inline __device__ typename std::enable_if<sizeof(ctype)==8, ctype>::type device_atomic_fetch_and(ctype* dest, ctype value, __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER, __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE) { \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:  asm volatile("atom.and.global" __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM ".b64" " %0,[%1],%2;" : "=l"(asm_result) : "l"(dest),"l"(asm_value) : "memory"); \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:  asm volatile("atom.and"        __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM ".b64" " %0,[%1],%2;" : "=l"(asm_result) : "l"(dest),"l"(asm_value) : "memory"); \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:#define __DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_OR() \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:inline __device__ typename std::enable_if<sizeof(ctype)==4, ctype>::type device_atomic_fetch_or(ctype* dest, ctype value, __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER, __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE) { \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:  asm volatile("atom.or.global" __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM ".b32" " %0,[%1],%2;" : "=r"(asm_result) : "l"(dest),"r"(asm_value) : "memory"); \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:  asm volatile("atom.or"        __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM ".b32" " %0,[%1],%2;" : "=r"(asm_result) : "l"(dest),"r"(asm_value) : "memory"); \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:inline __device__ typename std::enable_if<sizeof(ctype)==8, ctype>::type device_atomic_fetch_or(ctype* dest, ctype value, __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER, __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE) { \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:  asm volatile("atom.or.global" __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM ".b64" " %0,[%1],%2;" : "=l"(asm_result) : "l"(dest),"l"(asm_value) : "memory"); \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:  asm volatile("atom.or"        __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM ".b64" " %0,[%1],%2;" : "=l"(asm_result) : "l"(dest),"l"(asm_value) : "memory"); \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:#define __DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_XOR() \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:inline __device__ typename std::enable_if<sizeof(ctype)==4, ctype>::type device_atomic_fetch_xor(ctype* dest, ctype value, __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER, __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE) { \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:  asm volatile("atom.xor.global" __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM ".b32" " %0,[%1],%2;" : "=r"(asm_result) : "l"(dest),"r"(asm_value) : "memory"); \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:  asm volatile("atom.xor"        __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM ".b32" " %0,[%1],%2;" : "=r"(asm_result) : "l"(dest),"r"(asm_value) : "memory"); \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:inline __device__ typename std::enable_if<sizeof(ctype)==8, ctype>::type device_atomic_fetch_xor(ctype* dest, ctype value, __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER, __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE) { \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:  asm volatile("atom.xor.global" __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM ".b64" " %0,[%1],%2;" : "=l"(asm_result) : "l"(dest),"l"(asm_value) : "memory"); \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:  asm volatile("atom.xor"        __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM ".b64" " %0,[%1],%2;" : "=l"(asm_result) : "l"(dest),"l"(asm_value) : "memory"); \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:#define __DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_ADD(ctype,asm_ctype,reg_ctype,reg_ret_ctype) \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:inline __device__ ctype device_atomic_fetch_add(ctype* dest, ctype value, __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER, __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE) { \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:  asm volatile("atom.add.global" __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM asm_ctype " %0,[%1],%2;" : reg_ret_ctype(result) : "l"(dest),reg_ctype(value) : "memory"); \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:  asm volatile("atom.add" __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM asm_ctype " %0,[%1],%2;" : reg_ret_ctype(result) : "l"(dest),reg_ctype(value) : "memory"); \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:#define __DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_SUB(ctype,asm_ctype,reg_ctype,reg_ret_ctype) \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:inline __device__ ctype device_atomic_fetch_sub(ctype* dest, ctype value, __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER, __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE) { \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:  asm volatile("atom.add.global" __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM asm_ctype " %0,[%1],%2;" : reg_ret_ctype(result) : "l"(dest),reg_ctype(neg_value) : "memory"); \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:  asm volatile("atom.add" __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM asm_ctype " %0,[%1],%2;" : reg_ret_ctype(result) : "l"(dest),reg_ctype(neg_value) : "memory"); \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:#define __DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_MIN(ctype,asm_ctype,reg_ctype,reg_ret_ctype) \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:inline __device__ ctype device_atomic_fetch_min(ctype* dest, ctype value, __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER, __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE) { \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:  asm volatile("atom.min.global" __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM asm_ctype " %0,[%1],%2;" : reg_ret_ctype(result) : "l"(dest),reg_ctype(value) : "memory"); \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:  asm volatile("atom.min"        __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM asm_ctype " %0,[%1],%2;" : reg_ret_ctype(result) : "l"(dest),reg_ctype(value) : "memory"); \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:#define __DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_MAX(ctype,asm_ctype,reg_ctype,reg_ret_ctype) \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:inline __device__ ctype device_atomic_fetch_max(ctype* dest, ctype value, __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER, __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE) { \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:  asm volatile("atom.max.global" __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM asm_ctype " %0,[%1],%2;" : reg_ret_ctype(result) : "l"(dest),reg_ctype(value) : "memory"); \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:  asm volatile("atom.max"        __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM asm_ctype " %0,[%1],%2;" : reg_ret_ctype(result) : "l"(dest),reg_ctype(value) : "memory"); \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:#define __DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_INC(ctype,asm_ctype,reg_ctype,reg_ret_ctype) \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:inline __device__ ctype device_atomic_fetch_inc(ctype* dest, __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER, __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE) { \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:  asm volatile("atom.inc.global" __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM asm_ctype " %0,[%1],%2;" : reg_ret_ctype(result) : "l"(dest),reg_ctype(limit) : "memory"); \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:  asm volatile("atom.inc"        __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM asm_ctype " %0,[%1],%2;" : reg_ret_ctype(result) : "l"(dest),reg_ctype(limit) : "memory"); \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:inline __device__ ctype device_atomic_fetch_inc_mod(ctype* dest, ctype limit, __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER, __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE) { \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:  asm volatile("atom.inc.global" __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM asm_ctype " %0,[%1],%2;" : reg_ret_ctype(result) : "l"(dest),reg_ctype(limit) : "memory"); \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:  asm volatile("atom.inc"        __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM asm_ctype " %0,[%1],%2;" : reg_ret_ctype(result) : "l"(dest),reg_ctype(limit) : "memory"); \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:#define __DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_DEC(ctype,asm_ctype,reg_ctype,reg_ret_ctype) \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:inline __device__ ctype device_atomic_fetch_dec(ctype* dest, __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER, __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE) { \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:  asm volatile("atom.dec.global" __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM asm_ctype " %0,[%1],%2;" : reg_ret_ctype(result) : "l"(dest),reg_ctype(limit) : "memory"); \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:  asm volatile("atom.dec"        __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM asm_ctype " %0,[%1],%2;" : reg_ret_ctype(result) : "l"(dest),reg_ctype(limit) : "memory"); \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:inline __device__ ctype device_atomic_fetch_dec_mod(ctype* dest, ctype limit, __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER, __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE) { \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:  asm volatile("atom.dec.global" __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM asm_ctype " %0,[%1],%2;" : reg_ret_ctype(result) : "l"(dest),reg_ctype(limit) : "memory"); \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:  asm volatile("atom.dec"        __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM asm_ctype " %0,[%1],%2;" : reg_ret_ctype(result) : "l"(dest),reg_ctype(limit) : "memory"); \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:#define __DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_BIN_OP() \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_AND() \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_OR() \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_XOR()
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_ADD(float,".f32","f","=f")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_SUB(float,".f32","f","=f")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_ADD(double,".f64","d","=d")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_SUB(double,".f64","d","=d")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_ADD(uint32_t,".u32","r","=r")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_SUB(uint32_t,".u32","r","=r")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_MIN(uint32_t,".u32","r","=r")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_MAX(uint32_t,".u32","r","=r")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_INC(uint32_t,".u32","r","=r")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_DEC(uint32_t,".u32","r","=r")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_ADD(uint64_t,".u64","l","=l")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_SUB(uint64_t,".u64","l","=l")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_MIN(uint64_t,".u64","l","=l")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_MAX(uint64_t,".u64","l","=l")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal://__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_INC(uint64_t,".u64","l","=l")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal://__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_DEC(uint64_t,".u64","l","=l")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_ADD(int32_t,".s32","r","=r")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_SUB(int32_t,".s32","r","=r")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_MIN(int32_t,".s32","r","=r")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_MAX(int32_t,".s32","r","=r")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal://__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_INC(int32_t,".s32","r","=r")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal://__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_DEC(int32_t,".s32","r","=r")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_ADD(int64_t,".u64","l","=l")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_SUB(int64_t,".u64","l","=l")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_MIN(int64_t,".s64","l","=l")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_MAX(int64_t,".s64","l","=l")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal://__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_INC(int64_t,".s64","l","=l")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal://__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_DEC(int64_t,".s64","l","=l")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:__DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_BIN_OP()
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:#undef __DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_ADD
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:#undef __DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_MIN
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:#undef __DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_MAX
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:#undef __DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_INC
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:#undef __DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_DEC
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:#undef __DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_AND
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_fetch_op.inc_isglobal:#undef __DESUL_IMPL_CUDA_ASM_ATOMIC_FETCH_BIN_OP
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_exchange.inc:#define __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE MemoryScopeDevice
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_exchange.inc:#define __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM ".gpu"
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_exchange.inc:#include "desul/atomics/cuda/cuda_cc7_asm_exchange_memorder.inc"
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_exchange.inc:#undef __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_exchange.inc:#undef __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_exchange.inc:#define __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE MemoryScopeNode
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_exchange.inc:#define __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM ".sys"
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_exchange.inc:#include "desul/atomics/cuda/cuda_cc7_asm_exchange_memorder.inc"
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_exchange.inc:#undef __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_exchange.inc:#undef __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_exchange.inc:#define __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE MemoryScopeCore
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_exchange.inc:#define __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM ".cta"
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_exchange.inc:#include "desul/atomics/cuda/cuda_cc7_asm_exchange_memorder.inc"
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_exchange.inc:#undef __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_exchange.inc:#undef __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_exchange_memorder.inc:#define __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER MemoryOrderRelaxed
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_exchange_memorder.inc:#define __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM ".relaxed"
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_exchange_memorder.inc:#include "desul/atomics/cuda/cuda_cc7_asm_exchange_op.inc"
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_exchange_memorder.inc:#undef __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_exchange_memorder.inc:#undef __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_exchange_memorder.inc:#define __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER MemoryOrderRelease
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_exchange_memorder.inc:#define __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM ".release"
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_exchange_memorder.inc:#include "desul/atomics/cuda/cuda_cc7_asm_exchange_op.inc"
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_exchange_memorder.inc:#undef __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_exchange_memorder.inc:#undef __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_exchange_memorder.inc:#define __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER MemoryOrderAcquire
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_exchange_memorder.inc:#define __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM ".acquire"
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_exchange_memorder.inc:#include "desul/atomics/cuda/cuda_cc7_asm_exchange_op.inc"
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_exchange_memorder.inc:#undef __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_exchange_memorder.inc:#undef __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_exchange_memorder.inc:#define __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER MemoryOrderAcqRel
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_exchange_memorder.inc:#define __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM ".acq_rel"
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_exchange_memorder.inc:#include "desul/atomics/cuda/cuda_cc7_asm_exchange_op.inc"
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_exchange_memorder.inc:#undef __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_exchange_memorder.inc:#undef __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_isglobal:#define __DESUL_IMPL_CUDA_ASM_ATOMIC_ADD(type,asm_type,reg_type) \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_isglobal:inline __device__ void device_atomic_add(type* dest, type value, __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER, __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE) { \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_isglobal:  asm volatile("red.add.global" __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM asm_type " [%0],%1;" :: "l"(dest),reg_type(value) : "memory"); \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_isglobal:  asm volatile("red.add"        __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM asm_type " [%0],%1;" :: "l"(dest),reg_type(value) : "memory"); \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_isglobal:#define __DESUL_IMPL_CUDA_ASM_ATOMIC_SUB(type,asm_type,reg_type) \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_isglobal:inline __device__ void device_atomic_sub(type* dest, type value, __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER, __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE) { \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_isglobal:  asm volatile("red.add.global" __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM asm_type " [%0],%1;" :: "l"(dest),reg_type(neg_value) : "memory"); \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_isglobal:  asm volatile("red.add"        __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM asm_type " [%0],%1;" :: "l"(dest),reg_type(neg_value) : "memory"); \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_isglobal:#define __DESUL_IMPL_CUDA_ASM_ATOMIC_MIN(type,asm_type,reg_type) \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_isglobal:inline __device__ void device_atomic_min(type* dest, type value, __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER, __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE) { \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_isglobal:  asm volatile("red.min.global" __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM asm_type " [%0],%1;" :: "l"(dest),reg_type(value) : "memory"); \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_isglobal:  asm volatile("red.min"        __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM asm_type " [%0],%1;" :: "l"(dest),reg_type(value) : "memory"); \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_isglobal:#define __DESUL_IMPL_CUDA_ASM_ATOMIC_MAX(type,asm_type,reg_type) \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_isglobal:inline __device__ void device_atomic_max(type* dest, type value, __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER, __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE) { \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_isglobal:  asm volatile("red.max.global" __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM asm_type " [%0],%1;" :: "l"(dest),reg_type(value) : "memory"); \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_isglobal:  asm volatile("red.max"        __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM asm_type " [%0],%1;" :: "l"(dest),reg_type(value) : "memory"); \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_isglobal:#define __DESUL_IMPL_CUDA_ASM_ATOMIC_INC(type,asm_type,reg_type) \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_isglobal:inline __device__ void device_atomic_inc(type* dest, __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER, __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE) { \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_isglobal:  asm volatile("red.inc.global" __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM asm_type " [%0],%1;" :: "l"(dest),reg_type(limit) : "memory"); \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_isglobal:  asm volatile("red.inc"        __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM asm_type " [%0],%1;" :: "l"(dest),reg_type(limit) : "memory"); \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_isglobal:#define __DESUL_IMPL_CUDA_ASM_ATOMIC_DEC(type,asm_type,reg_type) \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_isglobal:inline __device__ void device_atomic_dec(type* dest, __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER, __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE) { \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_isglobal:  asm volatile("red.dec.global" __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM asm_type " [%0],%1;" :: "l"(dest),reg_type(limit) : "memory"); \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_isglobal:  asm volatile("red.dec"        __DESUL_IMPL_CUDA_ASM_MEMORY_ORDER_ASM __DESUL_IMPL_CUDA_ASM_MEMORY_SCOPE_ASM asm_type " [%0],%1;" :: "l"(dest),reg_type(limit) : "memory"); \
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_isglobal:__DESUL_IMPL_CUDA_ASM_ATOMIC_ADD(float,".f32","f")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_isglobal:__DESUL_IMPL_CUDA_ASM_ATOMIC_SUB(float,".f32","f")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_isglobal:__DESUL_IMPL_CUDA_ASM_ATOMIC_ADD(double,".f64","d")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_isglobal:__DESUL_IMPL_CUDA_ASM_ATOMIC_SUB(double,".f64","d")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_isglobal:__DESUL_IMPL_CUDA_ASM_ATOMIC_ADD(uint32_t,".u32","r")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_isglobal:__DESUL_IMPL_CUDA_ASM_ATOMIC_SUB(uint32_t,".u32","r")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_isglobal:__DESUL_IMPL_CUDA_ASM_ATOMIC_MIN(uint32_t,".u32","r")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_isglobal:__DESUL_IMPL_CUDA_ASM_ATOMIC_MAX(uint32_t,".u32","r")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_isglobal:__DESUL_IMPL_CUDA_ASM_ATOMIC_INC(uint32_t,".u32","r")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_isglobal:__DESUL_IMPL_CUDA_ASM_ATOMIC_DEC(uint32_t,".u32","r")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_isglobal:__DESUL_IMPL_CUDA_ASM_ATOMIC_ADD(uint64_t,".u64","l")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_isglobal:__DESUL_IMPL_CUDA_ASM_ATOMIC_SUB(uint64_t,".u64","l")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_isglobal:__DESUL_IMPL_CUDA_ASM_ATOMIC_MIN(uint64_t,".u64","l")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_isglobal:__DESUL_IMPL_CUDA_ASM_ATOMIC_MAX(uint64_t,".u64","l")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_isglobal://__DESUL_IMPL_CUDA_ASM_ATOMIC_INC(uint64_t,".u64","l")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_isglobal://__DESUL_IMPL_CUDA_ASM_ATOMIC_DEC(uint64_t,".u64","l")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_isglobal:__DESUL_IMPL_CUDA_ASM_ATOMIC_ADD(int32_t,".s32","r")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_isglobal:__DESUL_IMPL_CUDA_ASM_ATOMIC_SUB(int32_t,".s32","r")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_isglobal:__DESUL_IMPL_CUDA_ASM_ATOMIC_MIN(int32_t,".s32","r")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_isglobal:__DESUL_IMPL_CUDA_ASM_ATOMIC_MAX(int32_t,".s32","r")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_isglobal://__DESUL_IMPL_CUDA_ASM_ATOMIC_INC(int32_t,".s32","r")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_isglobal://__DESUL_IMPL_CUDA_ASM_ATOMIC_DEC(int32_t,".s32","r")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_isglobal:__DESUL_IMPL_CUDA_ASM_ATOMIC_ADD(int64_t,".u64","l")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_isglobal:__DESUL_IMPL_CUDA_ASM_ATOMIC_SUB(int64_t,".u64","l")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_isglobal:__DESUL_IMPL_CUDA_ASM_ATOMIC_MIN(int64_t,".s64","l")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_isglobal:__DESUL_IMPL_CUDA_ASM_ATOMIC_MAX(int64_t,".s64","l")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_isglobal://__DESUL_IMPL_CUDA_ASM_ATOMIC_INC(int64_t,".s64","l")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/cuda_cc7_asm_atomic_op.inc_isglobal://__DESUL_IMPL_CUDA_ASM_ATOMIC_DEC(int64_t,".s64","l")
lib/kokkos/tpls/desul/include/desul/atomics/cuda/CUDA_asm_exchange.hpp:#include <desul/atomics/cuda/cuda_cc7_asm_exchange.inc>
lib/kokkos/tpls/desul/Config.hpp.cmake.in:#cmakedefine DESUL_ATOMICS_ENABLE_CUDA
lib/kokkos/tpls/desul/Config.hpp.cmake.in:#cmakedefine DESUL_ATOMICS_ENABLE_CUDA_SEPARABLE_COMPILATION
lib/kokkos/tpls/desul/Config.hpp.cmake.in:#cmakedefine DESUL_ATOMICS_ENABLE_OPENACC
lib/kokkos/tpls/desul/src/Lock_Array_CUDA.cpp:#ifdef DESUL_ATOMICS_ENABLE_CUDA_SEPARABLE_COMPILATION
lib/kokkos/tpls/desul/src/Lock_Array_CUDA.cpp:__device__ __constant__ int32_t* CUDA_SPACE_ATOMIC_LOCKS_DEVICE = nullptr;
lib/kokkos/tpls/desul/src/Lock_Array_CUDA.cpp:__device__ __constant__ int32_t* CUDA_SPACE_ATOMIC_LOCKS_NODE = nullptr;
lib/kokkos/tpls/desul/src/Lock_Array_CUDA.cpp:__global__ void init_lock_arrays_cuda_kernel() {
lib/kokkos/tpls/desul/src/Lock_Array_CUDA.cpp:  if (i < CUDA_SPACE_ATOMIC_MASK + 1) {
lib/kokkos/tpls/desul/src/Lock_Array_CUDA.cpp:    Impl::CUDA_SPACE_ATOMIC_LOCKS_DEVICE[i] = 0;
lib/kokkos/tpls/desul/src/Lock_Array_CUDA.cpp:    Impl::CUDA_SPACE_ATOMIC_LOCKS_NODE[i] = 0;
lib/kokkos/tpls/desul/src/Lock_Array_CUDA.cpp:int32_t* CUDA_SPACE_ATOMIC_LOCKS_DEVICE_h = nullptr;
lib/kokkos/tpls/desul/src/Lock_Array_CUDA.cpp:int32_t* CUDA_SPACE_ATOMIC_LOCKS_NODE_h = nullptr;
lib/kokkos/tpls/desul/src/Lock_Array_CUDA.cpp:void check_error_and_throw_cuda(cudaError e, const std::string msg) {
lib/kokkos/tpls/desul/src/Lock_Array_CUDA.cpp:  if (e != cudaSuccess) {
lib/kokkos/tpls/desul/src/Lock_Array_CUDA.cpp:    out << "Desul::Error: " << msg << " error(" << cudaGetErrorName(e)
lib/kokkos/tpls/desul/src/Lock_Array_CUDA.cpp:        << "): " << cudaGetErrorString(e);
lib/kokkos/tpls/desul/src/Lock_Array_CUDA.cpp:void init_lock_arrays_cuda() {
lib/kokkos/tpls/desul/src/Lock_Array_CUDA.cpp:  if (CUDA_SPACE_ATOMIC_LOCKS_DEVICE_h != nullptr) return;
lib/kokkos/tpls/desul/src/Lock_Array_CUDA.cpp:  auto error_malloc1 = cudaMalloc(&CUDA_SPACE_ATOMIC_LOCKS_DEVICE_h,
lib/kokkos/tpls/desul/src/Lock_Array_CUDA.cpp:                                  sizeof(int32_t) * (CUDA_SPACE_ATOMIC_MASK + 1));
lib/kokkos/tpls/desul/src/Lock_Array_CUDA.cpp:  check_error_and_throw_cuda(error_malloc1,
lib/kokkos/tpls/desul/src/Lock_Array_CUDA.cpp:                             "init_lock_arrays_cuda: cudaMalloc device locks");
lib/kokkos/tpls/desul/src/Lock_Array_CUDA.cpp:  auto error_malloc2 = cudaMallocHost(&CUDA_SPACE_ATOMIC_LOCKS_NODE_h,
lib/kokkos/tpls/desul/src/Lock_Array_CUDA.cpp:                                      sizeof(int32_t) * (CUDA_SPACE_ATOMIC_MASK + 1));
lib/kokkos/tpls/desul/src/Lock_Array_CUDA.cpp:  check_error_and_throw_cuda(error_malloc2,
lib/kokkos/tpls/desul/src/Lock_Array_CUDA.cpp:                             "init_lock_arrays_cuda: cudaMalloc host locks");
lib/kokkos/tpls/desul/src/Lock_Array_CUDA.cpp:  auto error_sync1 = cudaDeviceSynchronize();
lib/kokkos/tpls/desul/src/Lock_Array_CUDA.cpp:  copy_cuda_lock_arrays_to_device();
lib/kokkos/tpls/desul/src/Lock_Array_CUDA.cpp:  check_error_and_throw_cuda(error_sync1, "init_lock_arrays_cuda: post mallocs");
lib/kokkos/tpls/desul/src/Lock_Array_CUDA.cpp:  init_lock_arrays_cuda_kernel<<<(CUDA_SPACE_ATOMIC_MASK + 1 + 255) / 256, 256>>>();
lib/kokkos/tpls/desul/src/Lock_Array_CUDA.cpp:  auto error_sync2 = cudaDeviceSynchronize();
lib/kokkos/tpls/desul/src/Lock_Array_CUDA.cpp:  check_error_and_throw_cuda(error_sync2, "init_lock_arrays_cuda: post init kernel");
lib/kokkos/tpls/desul/src/Lock_Array_CUDA.cpp:void finalize_lock_arrays_cuda() {
lib/kokkos/tpls/desul/src/Lock_Array_CUDA.cpp:  if (CUDA_SPACE_ATOMIC_LOCKS_DEVICE_h == nullptr) return;
lib/kokkos/tpls/desul/src/Lock_Array_CUDA.cpp:  cudaFree(CUDA_SPACE_ATOMIC_LOCKS_DEVICE_h);
lib/kokkos/tpls/desul/src/Lock_Array_CUDA.cpp:  cudaFreeHost(CUDA_SPACE_ATOMIC_LOCKS_NODE_h);
lib/kokkos/tpls/desul/src/Lock_Array_CUDA.cpp:  CUDA_SPACE_ATOMIC_LOCKS_DEVICE_h = nullptr;
lib/kokkos/tpls/desul/src/Lock_Array_CUDA.cpp:  CUDA_SPACE_ATOMIC_LOCKS_NODE_h = nullptr;
lib/kokkos/tpls/desul/src/Lock_Array_CUDA.cpp:#ifdef DESUL_ATOMICS_ENABLE_CUDA_SEPARABLE_COMPILATION
lib/kokkos/tpls/desul/src/Lock_Array_CUDA.cpp:  copy_cuda_lock_arrays_to_device();
lib/kokkos/tpls/desul/src/Lock_Array_CUDA.cpp:template void init_lock_arrays_cuda<int>();
lib/kokkos/tpls/desul/src/Lock_Array_CUDA.cpp:template void finalize_lock_arrays_cuda<int>();
lib/kokkos/tpls/mdspan/include/experimental/__p0009_bits/config.hpp:#ifndef _MDSPAN_HAS_CUDA
lib/kokkos/tpls/mdspan/include/experimental/__p0009_bits/config.hpp:#  if defined(__CUDACC__)
lib/kokkos/tpls/mdspan/include/experimental/__p0009_bits/config.hpp:#    define _MDSPAN_HAS_CUDA __CUDACC__
lib/kokkos/tpls/mdspan/include/experimental/__p0009_bits/config.hpp:#  if (!defined(__NVCC__) || (__CUDACC_VER_MAJOR__ * 100 + __CUDACC_VER_MINOR__ * 10 >= 1170)) && \
lib/kokkos/tpls/mdspan/include/experimental/__p0009_bits/macros.hpp:#if defined(_MDSPAN_HAS_CUDA) || defined(_MDSPAN_HAS_HIP) || defined(_MDSPAN_HAS_SYCL)
lib/kokkos/tpls/mdspan/include/experimental/__p0009_bits/macros.hpp:#  if defined(_MDSPAN_HAS_CUDA) || defined(_MDSPAN_HAS_HIP)
lib/kokkos/tpls/mdspan/include/experimental/__p0009_bits/macros.hpp:// In CUDA defaulted functions do not need host device markup
lib/kokkos/tpls/mdspan/include/experimental/__p0009_bits/macros.hpp:#if defined(_MDSPAN_HAS_CUDA) || defined(_MDSPAN_HAS_HIP)
lib/kokkos/tpls/mdspan/include/experimental/__p0009_bits/mdspan.hpp:  /* Might need this on NVIDIA?
lib/kokkos/tpls/mdspan/include/experimental/__p0009_bits/mdspan.hpp:    #if !defined(_MDSPAN_HAS_HIP) && !defined(_MDSPAN_HAS_CUDA)
lib/kokkos/tpls/mdspan/include/experimental/__p2630_bits/submdspan_mapping.hpp:// Depending on the CUDA and GCC version we need both the builtin
lib/kokkos/tpls/mdspan/include/experimental/__p2630_bits/submdspan_mapping.hpp:#ifdef __CUDA_ARCH__
lib/kokkos/tpls/mdspan/include/experimental/__p2630_bits/submdspan_mapping.hpp:#if defined(__NVCC__) && !defined(__CUDA_ARCH__) && defined(__GNUC__)
lib/kokkos/tpls/mdspan/include/experimental/__p2630_bits/submdspan_mapping.hpp:// the issue But Clang-CUDA also doesn't accept the use of deduction guide so
lib/kokkos/tpls/mdspan/include/experimental/__p2630_bits/submdspan_mapping.hpp:// disable it for CUDA altogether
lib/kokkos/tpls/mdspan/include/experimental/__p2630_bits/submdspan_mapping.hpp:#if defined(_MDSPAN_HAS_HIP) || defined(_MDSPAN_HAS_CUDA)
lib/kokkos/tpls/mdspan/include/experimental/__p2630_bits/submdspan_mapping.hpp:#if defined(__NVCC__) && !defined(__CUDA_ARCH__) && defined(__GNUC__)
lib/kokkos/tpls/mdspan/include/experimental/__p2630_bits/submdspan_mapping.hpp:      // Compilers: CUDA 11.2 with GCC 9.1
lib/kokkos/tpls/mdspan/include/experimental/__p2630_bits/submdspan_mapping.hpp:// the issue But Clang-CUDA also doesn't accept the use of deduction guide so
lib/kokkos/tpls/mdspan/include/experimental/__p2630_bits/submdspan_mapping.hpp:// disable it for CUDA alltogether
lib/kokkos/tpls/mdspan/include/experimental/__p2630_bits/submdspan_mapping.hpp:#if defined(_MDSPAN_HAS_HIP) || defined(_MDSPAN_HAS_CUDA)
lib/kokkos/tpls/mdspan/include/experimental/__p2630_bits/submdspan_mapping.hpp:#if defined(__NVCC__) && !defined(__CUDA_ARCH__) && defined(__GNUC__)
lib/kokkos/tpls/mdspan/include/experimental/__p2630_bits/submdspan_mapping.hpp:#if defined(__NVCC__) && !defined(__CUDA_ARCH__) && defined(__GNUC__)
lib/kokkos/tpls/mdspan/include/experimental/__p2630_bits/submdspan_mapping.hpp:// the issue But Clang-CUDA also doesn't accept the use of deduction guide so
lib/kokkos/tpls/mdspan/include/experimental/__p2630_bits/submdspan_mapping.hpp:// disable it for CUDA altogether
lib/kokkos/tpls/mdspan/include/experimental/__p2630_bits/submdspan_mapping.hpp:#if defined(_MDSPAN_HAS_HIP) || defined(_MDSPAN_HAS_CUDA)
lib/kokkos/tpls/mdspan/include/experimental/__p2630_bits/submdspan_mapping.hpp:#if defined(__NVCC__) && !defined(__CUDA_ARCH__) && defined(__GNUC__)
lib/kokkos/tpls/mdspan/include/experimental/__p2630_bits/submdspan_mapping.hpp:// the issue But Clang-CUDA also doesn't accept the use of deduction guide so
lib/kokkos/tpls/mdspan/include/experimental/__p2630_bits/submdspan_mapping.hpp:// disable it for CUDA alltogether
lib/kokkos/tpls/mdspan/include/experimental/__p2630_bits/submdspan_mapping.hpp:#if defined(_MDSPAN_HAS_HIP) || defined(_MDSPAN_HAS_CUDA)
lib/kokkos/tpls/mdspan/include/experimental/__p2630_bits/submdspan_mapping.hpp:#if defined(__NVCC__) && !defined(__CUDA_ARCH__) && defined(__GNUC__)
lib/kokkos/tpls/mdspan/include/experimental/__p2630_bits/submdspan_mapping.hpp:     (__CUDACC_VER_MAJOR__ * 100 + __CUDACC_VER_MINOR__ * 10) < 1120)
lib/kokkos/tpls/mdspan/include/experimental/__p2630_bits/submdspan_mapping.hpp:#ifdef __CUDA_ARCH__
lib/kokkos/tpls/mdspan/include/experimental/__p2630_bits/submdspan_extents.hpp:// Depending on the CUDA and GCC version we need both the builtin
lib/kokkos/tpls/mdspan/include/experimental/__p2630_bits/submdspan_extents.hpp:      #ifdef __CUDA_ARCH__
lib/kokkos/tpls/mdspan/include/experimental/__p2630_bits/submdspan_extents.hpp:#if defined(__NVCC__) && !defined(__CUDA_ARCH__) && defined(__GNUC__)
lib/kokkos/tpls/mdspan/include/experimental/__p2630_bits/submdspan_extents.hpp:  // Even with CUDA_ARCH protection this thing warns about calling host function
lib/kokkos/tpls/mdspan/include/experimental/__p2630_bits/submdspan_extents.hpp:      #ifdef __CUDA_ARCH__
lib/kokkos/CMakeLists.txt:  IF (Kokkos_ENABLE_COMPILE_AS_CMAKE_LANGUAGE AND Kokkos_ENABLE_CUDA)
lib/kokkos/CMakeLists.txt:    # but we still need a C++ compiler even if we build all our cpp files as CUDA only
lib/kokkos/CMakeLists.txt:    # This is just the rather odd way CMake does this, since CUDA doesn't imply C++ even
lib/kokkos/CMakeLists.txt:    # though it is a C++ extension ... (but I guess it didn't use to be back in CUDA 4 or 5
lib/kokkos/CMakeLists.txt:    SET(KOKKOS_COMPILE_LANGUAGE CUDA)
lib/kokkos/CMakeLists.txt:    MESSAGE(FATAL_ERROR "Kokkos did not configure correctly and failed to validate compiler. The most likely cause is CUDA linkage using nvcc_wrapper. Please ensure your CUDA environment is correctly configured.")
lib/kokkos/CMakeLists.txt:IF (Kokkos_ENABLE_CUDA)
lib/kokkos/CMakeLists.txt:  # If we are building CUDA, we have tricked CMake because we declare a CXX project
lib/kokkos/CMakeLists.txt:  # This breaks CUDA compilation (CUDA compiler can have a different default
lib/kokkos/CMakeLists.txt:GLOBAL_SET(KOKKOS_AMDGPU_OPTIONS)
lib/kokkos/CMakeLists.txt:GLOBAL_SET(KOKKOS_CUDA_OPTIONS)
lib/kokkos/CMakeLists.txt:GLOBAL_SET(KOKKOS_CUDAFE_OPTIONS)
lib/kokkos/CMakeLists.txt:    LIST(APPEND KOKKOS_COMPILE_OPTIONS ${KOKKOS_AMDGPU_OPTIONS})
lib/kokkos/CMakeLists.txt:  IF (KOKKOS_ENABLE_CUDA)
lib/kokkos/CMakeLists.txt:    LIST(APPEND KOKKOS_ALL_COMPILE_OPTIONS ${KOKKOS_CUDA_OPTIONS})
lib/kokkos/CMakeLists.txt:  IF (KOKKOS_ENABLE_CUDA)
lib/kokkos/CMakeLists.txt:    STRING(REPLACE ";" " " KOKKOSCORE_CUDA_OPTIONS    "${KOKKOS_CUDA_OPTIONS}")
lib/kokkos/CMakeLists.txt:    FOREACH(CUDAFE_FLAG ${KOKKOS_CUDAFE_OPTIONS})
lib/kokkos/CMakeLists.txt:      SET(KOKKOSCORE_CUDAFE_OPTIONS "${KOKKOSCORE_CUDAFE_OPTIONS} -Xcudafe ${CUDAFE_FLAG}")
lib/kokkos/CMakeLists.txt:      LIST(APPEND KOKKOS_ALL_COMPILE_OPTIONS -Xcudafe ${CUDAFE_FLAG})
lib/kokkos/CMakeLists.txt:# nvcc_wrapper is Kokkos' wrapper for NVIDIA's NVCC CUDA compiler.
lib/kokkos/benchmarks/bytes_and_flops/main.cpp:        "  S:   shared memory per team (used to control occupancy on GPUs)\n");
lib/kokkos/benchmarks/bytes_and_flops/main.cpp:    printf("Example Input GPU:\n");
lib/kokkos/benchmarks/bytes_and_flops/Makefile:KOKKOS_DEVICES=Cuda
lib/kokkos/benchmarks/bytes_and_flops/Makefile:KOKKOS_CUDA_OPTIONS=enable_lambda
lib/kokkos/benchmarks/bytes_and_flops/Makefile:ifneq (,$(findstring Cuda,$(KOKKOS_DEVICES)))
lib/kokkos/benchmarks/bytes_and_flops/Makefile:EXE = bytes_and_flops.cuda
lib/kokkos/benchmarks/bytes_and_flops/Makefile:	rm -f *.o *.cuda *.host
lib/kokkos/benchmarks/stream/Makefile:KOKKOS_DEVICES=Cuda
lib/kokkos/benchmarks/stream/Makefile:KOKKOS_CUDA_OPTIONS=enable_lambda
lib/kokkos/benchmarks/stream/Makefile:ifneq (,$(findstring Cuda,$(KOKKOS_DEVICES)))
lib/kokkos/benchmarks/stream/Makefile:EXE = stream.cuda
lib/kokkos/benchmarks/stream/Makefile:	rm -f *.o stream.cuda stream.exe
lib/kokkos/benchmarks/view_copy_constructor/Makefile:	rm -f *.o view_copy_constructor.cuda view_copy_constructor.exe
lib/kokkos/benchmarks/policy_performance/Makefile:KOKKOS_DEVICES=Cuda
lib/kokkos/benchmarks/policy_performance/Makefile:KOKKOS_CUDA_OPTIONS=enable_lambda
lib/kokkos/benchmarks/policy_performance/Makefile:ifneq (,$(findstring Cuda,$(KOKKOS_DEVICES)))
lib/kokkos/benchmarks/policy_performance/Makefile:EXE = policy_perf.cuda
lib/kokkos/benchmarks/policy_performance/Makefile:	rm -f *.o policy_perf.cuda policy_perf.exe
lib/kokkos/benchmarks/policy_performance/script_basic_testing.sh:SUFFIX=cuda
lib/kokkos/benchmarks/policy_performance/script_basic_testing.sh:echo "Cuda tests Static schedule"
lib/kokkos/benchmarks/policy_performance/script_basic_testing.sh:echo "Cuda tests Dynamic schedule"
lib/kokkos/benchmarks/policy_performance/script_sample_usage.sh:# Cuda tests
lib/kokkos/benchmarks/policy_performance/script_sample_usage.sh:SUFFIX=cuda
lib/kokkos/benchmarks/policy_performance/script_sample_usage.sh:echo "Cuda"
lib/kokkos/benchmarks/policy_performance/script_sample_usage.sh:fi #end cuda
lib/kokkos/benchmarks/policy_performance/policy_perf_test.hpp:        // This does not compile with pre Cuda 8.0 - see Github Issue #913 for explanation
lib/kokkos/benchmarks/policy_performance/policy_perf_test.hpp:      // result = v1( team_size*team_range - 1 ); // won't work with Cuda - need
lib/kokkos/benchmarks/gather/main.cpp:    printf("Example Input GPU:\n");
lib/kokkos/benchmarks/gather/Makefile:KOKKOS_DEVICES=Cuda
lib/kokkos/benchmarks/gather/Makefile:KOKKOS_CUDA_OPTIONS=enable_lambda
lib/kokkos/benchmarks/gather/Makefile:ifneq (,$(findstring Cuda,$(KOKKOS_DEVICES)))
lib/kokkos/benchmarks/gather/Makefile:EXE = gather.cuda
lib/kokkos/benchmarks/gather/Makefile:	rm -f *.o gather.cuda gather.exe
lib/kokkos/benchmarks/benchmark_suite/scripts/build_code.bash:    --with-cuda-options*)
lib/kokkos/benchmarks/benchmark_suite/scripts/build_code.bash:      KOKKOS_CUDA_OPTIONS="--with-cuda-options=${key#*=}"
lib/kokkos/benchmarks/benchmark_suite/scripts/run_tests.bash:USE_CUDA=`grep "_CUDA" KokkosCore_config.h | wc -l`
lib/kokkos/benchmarks/benchmark_suite/scripts/run_tests.bash:if [[ ${USE_CUDA} > 0 ]]; then
lib/kokkos/benchmarks/benchmark_suite/scripts/run_tests.bash:  BAF_EXE=bytes_and_flops.cuda
lib/kokkos/benchmarks/atomic/main.cpp:      printf("Example Input GPU:\n");
lib/kokkos/benchmarks/atomic/Makefile:KOKKOS_DEVICES=Cuda
lib/kokkos/benchmarks/atomic/Makefile:KOKKOS_CUDA_OPTIONS=enable_lambda
lib/kokkos/benchmarks/atomic/Makefile:ifneq (,$(findstring Cuda,$(KOKKOS_DEVICES)))
lib/kokkos/benchmarks/atomic/Makefile:EXE = atomic_perf.cuda
lib/kokkos/benchmarks/atomic/Makefile:	rm -f *.o atomic_perf.cuda atomic_perf.exe
lib/kokkos/Makefile.kokkos:# Options: Cuda,HIP,SYCL,OpenMPTarget,OpenMP,Threads,Serial
lib/kokkos/Makefile.kokkos:# NVIDIA:   Kepler,Kepler30,Kepler32,Kepler35,Kepler37,Maxwell,Maxwell50,Maxwell52,Maxwell53,Pascal60,Pascal61,Volta70,Volta72,Turing75,Ampere80,Ampere86,Ada89,Hopper90
lib/kokkos/Makefile.kokkos:# AMD-GPUS: AMD_GFX906,AMD_GFX908,AMD_GFX90A,AMD_GFX940,AMD_GFX942,AMD_GFX1030,AMD_GFX1100,AMD_GFX1103
lib/kokkos/Makefile.kokkos:# Intel-GPUs: Intel_Gen,Intel_Gen9,Intel_Gen11,Intel_Gen12LP,Intel_DG1,Intel_XeHP,Intel_PVC
lib/kokkos/Makefile.kokkos:KOKKOS_CUDA_OPTIONS ?= "disable_malloc_async"
lib/kokkos/Makefile.kokkos:KOKKOS_INTERNAL_CUDA_USE_LDG := $(call kokkos_has_string,$(KOKKOS_CUDA_OPTIONS),use_ldg)
lib/kokkos/Makefile.kokkos:KOKKOS_INTERNAL_CUDA_USE_UVM := $(call kokkos_has_string,$(KOKKOS_CUDA_OPTIONS),force_uvm)
lib/kokkos/Makefile.kokkos:KOKKOS_INTERNAL_CUDA_USE_RELOC := $(call kokkos_has_string,$(KOKKOS_CUDA_OPTIONS),rdc)
lib/kokkos/Makefile.kokkos:KOKKOS_INTERNAL_CUDA_USE_LAMBDA := $(call kokkos_has_string,$(KOKKOS_CUDA_OPTIONS),enable_lambda)
lib/kokkos/Makefile.kokkos:KOKKOS_INTERNAL_CUDA_USE_CONSTEXPR := $(call kokkos_has_string,$(KOKKOS_CUDA_OPTIONS),enable_constexpr)
lib/kokkos/Makefile.kokkos:KOKKOS_INTERNAL_CUDA_DISABLE_MALLOC_ASYNC := $(call kokkos_has_string,$(KOKKOS_CUDA_OPTIONS),disable_malloc_async)
lib/kokkos/Makefile.kokkos:KOKKOS_INTERNAL_USE_CUDA := $(call kokkos_has_string,$(KOKKOS_DEVICES),Cuda)
lib/kokkos/Makefile.kokkos:KOKKOS_INTERNAL_USE_OPENACC := $(call kokkos_has_string,$(KOKKOS_DEVICES),OpenACC)
lib/kokkos/Makefile.kokkos:ifeq ($(KOKKOS_INTERNAL_USE_CUDA), 1)
lib/kokkos/Makefile.kokkos:  KOKKOS_DEVICELIST += Cuda
lib/kokkos/Makefile.kokkos:ifeq ($(KOKKOS_INTERNAL_USE_OPENACC), 1)
lib/kokkos/Makefile.kokkos:  KOKKOS_DEVICELIST += OpenACC
lib/kokkos/Makefile.kokkos:ifeq ($(KOKKOS_INTERNAL_USE_CUDA), 1)
lib/kokkos/Makefile.kokkos:  ifeq ($(origin CUDA_PATH), undefined)
lib/kokkos/Makefile.kokkos:    CUDA_PATH = $(KOKKOS_INTERNAL_NVCC_PATH:/bin/nvcc=)
lib/kokkos/Makefile.kokkos:  ifeq ($(CUDA_PATH),)
lib/kokkos/Makefile.kokkos:    CUDA_PATH = $(KOKKOS_INTERNAL_NVCC_PATH:/bin/nvcc=)
lib/kokkos/Makefile.kokkos:  ifeq ($(KOKKOS_INTERNAL_USE_CUDA), 1)
lib/kokkos/Makefile.kokkos:      $(error Compiling Cuda code directly with Clang requires version 4.0.0 or higher)
lib/kokkos/Makefile.kokkos:    KOKKOS_INTERNAL_CUDA_USE_LAMBDA := 1
lib/kokkos/Makefile.kokkos:    KOKKOS_INTERNAL_OPENMPTARGET_FLAG := -mp=gpu 
lib/kokkos/Makefile.kokkos:ifeq ($(KOKKOS_INTERNAL_USE_OPENACC), 1)
lib/kokkos/Makefile.kokkos:  # Set OpenACC flags.
lib/kokkos/Makefile.kokkos:    KOKKOS_INTERNAL_OPENACC_FLAG := -acc
lib/kokkos/Makefile.kokkos:    $(error Makefile.kokkos: OpenACC is enabled but the compiler must be NVHPC (got version string $(KOKKOS_CXX_VERSION)))
lib/kokkos/Makefile.kokkos:# NVIDIA based.
lib/kokkos/Makefile.kokkos:KOKKOS_INTERNAL_USE_ARCH_NVIDIA := $(shell expr $(KOKKOS_INTERNAL_USE_ARCH_KEPLER30)  \
lib/kokkos/Makefile.kokkos:ifeq ($(KOKKOS_INTERNAL_USE_ARCH_NVIDIA), 0)
lib/kokkos/Makefile.kokkos:  KOKKOS_INTERNAL_USE_ARCH_NVIDIA := $(shell expr $(KOKKOS_INTERNAL_USE_ARCH_KEPLER35)  \
lib/kokkos/Makefile.kokkos:ifeq ($(KOKKOS_INTERNAL_USE_ARCH_NVIDIA), 1)
lib/kokkos/Makefile.kokkos:    CUDA_PATH ?= $(KOKKOS_INTERNAL_NVCC_PATH:/bin/nvcc=)
lib/kokkos/Makefile.kokkos:      KOKKOS_INTERNAL_OPENMPTARGET_FLAG := $(KOKKOS_INTERNAL_OPENMPTARGET_FLAG) --cuda-path=$(CUDA_PATH)
lib/kokkos/Makefile.kokkos:KOKKOS_INTERNAL_USE_ARCH_MULTIGPU := $(strip $(shell echo "$(KOKKOS_INTERNAL_USE_ARCH_NVIDIA)>1") | bc)
lib/kokkos/Makefile.kokkos:ifeq ($(KOKKOS_INTERNAL_USE_ARCH_MULTIGPU), 1)
lib/kokkos/Makefile.kokkos:  $(error Defined Multiple GPU architectures: KOKKOS_ARCH=$(KOKKOS_ARCH) )
lib/kokkos/Makefile.kokkos:ifeq ($(KOKKOS_INTERNAL_USE_CUDA), 1)
lib/kokkos/Makefile.kokkos:  tmp := $(call kokkos_append_header,"$H""define KOKKOS_ENABLE_CUDA")
lib/kokkos/Makefile.kokkos:ifeq ($(KOKKOS_INTERNAL_USE_OPENACC), 1)
lib/kokkos/Makefile.kokkos:  tmp := $(call kokkos_append_header,"$H""define KOKKOS_ENABLE_OPENACC")
lib/kokkos/Makefile.kokkos:tmp := $(call kokkos_append_header,"/* Cuda Settings */")
lib/kokkos/Makefile.kokkos:ifeq ($(KOKKOS_INTERNAL_USE_CUDA), 1)
lib/kokkos/Makefile.kokkos:  ifeq ($(KOKKOS_INTERNAL_CUDA_USE_LDG), 1)
lib/kokkos/Makefile.kokkos:    tmp := $(call kokkos_append_header,"$H""define KOKKOS_ENABLE_CUDA_LDG_INTRINSIC")
lib/kokkos/Makefile.kokkos:    tmp := $(call kokkos_append_header,"$H""define KOKKOS_ENABLE_CUDA_LDG_INTRINSIC")
lib/kokkos/Makefile.kokkos:  ifeq ($(KOKKOS_INTERNAL_CUDA_USE_UVM), 1)
lib/kokkos/Makefile.kokkos:    tmp := $(call kokkos_append_header,"$H""define KOKKOS_ENABLE_CUDA_UVM")
lib/kokkos/Makefile.kokkos:  ifeq ($(KOKKOS_INTERNAL_CUDA_USE_RELOC), 1)
lib/kokkos/Makefile.kokkos:    tmp := $(call kokkos_append_header,"$H""define KOKKOS_ENABLE_CUDA_RELOCATABLE_DEVICE_CODE")
lib/kokkos/Makefile.kokkos:      KOKKOS_CXXFLAGS += -fcuda-rdc
lib/kokkos/Makefile.kokkos:      KOKKOS_LDFLAGS += -fcuda-rdc
lib/kokkos/Makefile.kokkos:      # This diagnostic is just plain wrong in CUDA 9
lib/kokkos/Makefile.kokkos:      KOKKOS_CXXFLAGS += -Xcudafe --diag_suppress=esa_on_defaulted_function_ignored
lib/kokkos/Makefile.kokkos:    tmp := $(call kokkos_append_header,"$H""define KOKKOS_ENABLE_CUDA_LAMBDA")
lib/kokkos/Makefile.kokkos:    tmp := $(call kokkos_append_header,"$H""define KOKKOS_ENABLE_CUDA_LAMBDA")
lib/kokkos/Makefile.kokkos:  ifeq ($(KOKKOS_INTERNAL_CUDA_USE_CONSTEXPR), 1)
lib/kokkos/Makefile.kokkos:      tmp := $(call kokkos_append_header,"$H""define KOKKOS_ENABLE_CUDA_CONSTEXPR")
lib/kokkos/Makefile.kokkos:      tmp := $(call kokkos_append_header,"$H""define KOKKOS_ENABLE_CUDA_CONSTEXPR")
lib/kokkos/Makefile.kokkos:  ifeq ($(KOKKOS_INTERNAL_CUDA_DISABLE_MALLOC_ASYNC), 0)
lib/kokkos/Makefile.kokkos:    tmp := $(call kokkos_append_header,"$H""define KOKKOS_ENABLE_IMPL_CUDA_MALLOC_ASYNC")
lib/kokkos/Makefile.kokkos:    tmp := $(call kokkos_append_header,"/* $H""undef KOKKOS_ENABLE_IMPL_CUDA_MALLOC_ASYNC */")
lib/kokkos/Makefile.kokkos:# Figure out the architecture flag for Cuda.
lib/kokkos/Makefile.kokkos:ifeq ($(KOKKOS_INTERNAL_USE_CUDA), 1)
lib/kokkos/Makefile.kokkos:   KOKKOS_INTERNAL_USE_CUDA_ARCH=1
lib/kokkos/Makefile.kokkos:     KOKKOS_INTERNAL_USE_CUDA_ARCH=1
lib/kokkos/Makefile.kokkos:ifeq ($(KOKKOS_INTERNAL_USE_CUDA_ARCH), 1)
lib/kokkos/Makefile.kokkos:    KOKKOS_INTERNAL_CUDA_ARCH_FLAG=-arch
lib/kokkos/Makefile.kokkos:		KOKKOS_INTERNAL_CUDA_ARCH_FLAG=--cuda-gpu-arch
lib/kokkos/Makefile.kokkos:		KOKKOS_CXXFLAGS += -x cuda
lib/kokkos/Makefile.kokkos:    $(error Makefile.kokkos: CUDA is enabled but the compiler is neither NVCC nor Clang (got version string $(KOKKOS_CXX_VERSION)) )
lib/kokkos/Makefile.kokkos:  KOKKOS_INTERNAL_USE_CUDA_ARCH = 1
lib/kokkos/Makefile.kokkos:    KOKKOS_INTERNAL_CUDA_ARCH_FLAG=-fopenmp
lib/kokkos/Makefile.kokkos:      KOKKOS_INTERNAL_CUDA_ARCH_FLAG=-fopenmp --offload-arch
lib/kokkos/Makefile.kokkos:      KOKKOS_INTERNAL_CUDA_ARCH_FLAG := $(KOKKOS_INTERNAL_CUDA_ARCH_FLAG)=sm_30
lib/kokkos/Makefile.kokkos:      KOKKOS_INTERNAL_CUDA_ARCH_FLAG := $(KOKKOS_INTERNAL_CUDA_ARCH_FLAG)=sm_32
lib/kokkos/Makefile.kokkos:      KOKKOS_INTERNAL_CUDA_ARCH_FLAG := $(KOKKOS_INTERNAL_CUDA_ARCH_FLAG)=sm_35
lib/kokkos/Makefile.kokkos:      KOKKOS_INTERNAL_CUDA_ARCH_FLAG := $(KOKKOS_INTERNAL_CUDA_ARCH_FLAG)=sm_37
lib/kokkos/Makefile.kokkos:      KOKKOS_INTERNAL_CUDA_ARCH_FLAG := $(KOKKOS_INTERNAL_CUDA_ARCH_FLAG)=sm_50
lib/kokkos/Makefile.kokkos:      KOKKOS_INTERNAL_CUDA_ARCH_FLAG := $(KOKKOS_INTERNAL_CUDA_ARCH_FLAG)=sm_52
lib/kokkos/Makefile.kokkos:      KOKKOS_INTERNAL_CUDA_ARCH_FLAG := $(KOKKOS_INTERNAL_CUDA_ARCH_FLAG)=sm_53
lib/kokkos/Makefile.kokkos:      KOKKOS_INTERNAL_CUDA_ARCH_FLAG := $(KOKKOS_INTERNAL_CUDA_ARCH_FLAG)=sm_60
lib/kokkos/Makefile.kokkos:      KOKKOS_INTERNAL_CUDA_ARCH_FLAG := $(KOKKOS_INTERNAL_CUDA_ARCH_FLAG)=sm_61
lib/kokkos/Makefile.kokkos:      KOKKOS_INTERNAL_CUDA_ARCH_FLAG := $(KOKKOS_INTERNAL_CUDA_ARCH_FLAG)=sm_70
lib/kokkos/Makefile.kokkos:      KOKKOS_INTERNAL_CUDA_ARCH_FLAG := $(KOKKOS_INTERNAL_CUDA_ARCH_FLAG)=sm_72
lib/kokkos/Makefile.kokkos:      KOKKOS_INTERNAL_CUDA_ARCH_FLAG := $(KOKKOS_INTERNAL_CUDA_ARCH_FLAG)=sm_75
lib/kokkos/Makefile.kokkos:      KOKKOS_INTERNAL_CUDA_ARCH_FLAG := $(KOKKOS_INTERNAL_CUDA_ARCH_FLAG)=sm_80
lib/kokkos/Makefile.kokkos:      KOKKOS_INTERNAL_CUDA_ARCH_FLAG := $(KOKKOS_INTERNAL_CUDA_ARCH_FLAG)=sm_86
lib/kokkos/Makefile.kokkos:      KOKKOS_INTERNAL_CUDA_ARCH_FLAG := $(KOKKOS_INTERNAL_CUDA_ARCH_FLAG)=sm_89
lib/kokkos/Makefile.kokkos:      KOKKOS_INTERNAL_CUDA_ARCH_FLAG := $(KOKKOS_INTERNAL_CUDA_ARCH_FLAG)=sm_90
lib/kokkos/Makefile.kokkos:ifneq ($(KOKKOS_INTERNAL_USE_ARCH_NVIDIA), 0)
lib/kokkos/Makefile.kokkos:  KOKKOS_CXXFLAGS += $(KOKKOS_INTERNAL_CUDA_ARCH_FLAG)
lib/kokkos/Makefile.kokkos:    KOKKOS_LDFLAGS += $(KOKKOS_INTERNAL_CUDA_ARCH_FLAG)
lib/kokkos/Makefile.kokkos:      KOKKOS_LDFLAGS += $(KOKKOS_INTERNAL_CUDA_ARCH_FLAG)
lib/kokkos/Makefile.kokkos:# Figure out the architecture flag for ROCm.
lib/kokkos/Makefile.kokkos:  tmp := $(call kokkos_append_header,"$H""define KOKKOS_ARCH_AMD_GPU")
lib/kokkos/Makefile.kokkos:  tmp := $(call kokkos_append_header,"$H""define KOKKOS_ARCH_AMD_GPU")
lib/kokkos/Makefile.kokkos:  tmp := $(call kokkos_append_header,"$H""define KOKKOS_ARCH_AMD_GPU")
lib/kokkos/Makefile.kokkos:  tmp := $(call kokkos_append_header,"$H""define KOKKOS_ARCH_AMD_GPU")
lib/kokkos/Makefile.kokkos:  tmp := $(call kokkos_append_header,"$H""define KOKKOS_ARCH_AMD_GPU")
lib/kokkos/Makefile.kokkos:  tmp := $(call kokkos_append_header,"$H""define KOKKOS_ARCH_AMD_GPU")
lib/kokkos/Makefile.kokkos:  tmp := $(call kokkos_append_header,"$H""define KOKKOS_ARCH_AMD_GPU")
lib/kokkos/Makefile.kokkos:  tmp := $(call kokkos_append_header,"$H""define KOKKOS_ARCH_AMD_GPU")
lib/kokkos/Makefile.kokkos:    KOKKOS_CXXFLAGS+=-fgpu-rdc
lib/kokkos/Makefile.kokkos:    KOKKOS_LDFLAGS+=-fgpu-rdc
lib/kokkos/Makefile.kokkos:    KOKKOS_CXXFLAGS+=-fno-gpu-rdc
lib/kokkos/Makefile.kokkos:    KOKKOS_LDFLAGS+=-fno-gpu-rdc
lib/kokkos/Makefile.kokkos:  tmp := $(call kokkos_append_header,"$H""define KOKKOS_ARCH_INTEL_GPU")
lib/kokkos/Makefile.kokkos:  tmp := $(call kokkos_append_header,"$H""define KOKKOS_ARCH_INTEL_GPU")
lib/kokkos/Makefile.kokkos:  tmp := $(call kokkos_append_header,"$H""define KOKKOS_ARCH_INTEL_GPU")
lib/kokkos/Makefile.kokkos:  tmp := $(call kokkos_append_header,"$H""define KOKKOS_ARCH_INTEL_GPU")
lib/kokkos/Makefile.kokkos:  tmp := $(call kokkos_append_header,"$H""define KOKKOS_ARCH_INTEL_GPU")
lib/kokkos/Makefile.kokkos:  tmp := $(call kokkos_append_header,"$H""define KOKKOS_ARCH_INTEL_GPU")
lib/kokkos/Makefile.kokkos:  tmp := $(call kokkos_append_header,"$H""define KOKKOS_ARCH_INTEL_GPU")
lib/kokkos/Makefile.kokkos:  ifeq ($(KOKKOS_INTERNAL_USE_CUDA), 1)
lib/kokkos/Makefile.kokkos:    tmp := $(call kokkos_append_config_header,"$H""include <fwd/Kokkos_Fwd_CUDA.hpp>","KokkosCore_Config_FwdBackend.hpp")
lib/kokkos/Makefile.kokkos:    tmp := $(call kokkos_append_config_header,"$H""include <decl/Kokkos_Declare_CUDA.hpp>","KokkosCore_Config_DeclareBackend.hpp")
lib/kokkos/Makefile.kokkos:    tmp := $(call kokkos_append_config_header,"$H""include <setup/Kokkos_Setup_Cuda.hpp>","KokkosCore_Config_SetupBackend.hpp")
lib/kokkos/Makefile.kokkos:  ifeq ($(KOKKOS_INTERNAL_USE_OPENACC), 1)
lib/kokkos/Makefile.kokkos:    tmp := $(call kokkos_append_config_header,"$H""include <fwd/Kokkos_Fwd_OPENACC.hpp>","KokkosCore_Config_FwdBackend.hpp")
lib/kokkos/Makefile.kokkos:    tmp := $(call kokkos_append_config_header,"$H""include <decl/Kokkos_Declare_OPENACC.hpp>","KokkosCore_Config_DeclareBackend.hpp")
lib/kokkos/Makefile.kokkos:ifeq ($(KOKKOS_INTERNAL_USE_CUDA), 1)
lib/kokkos/Makefile.kokkos:  KOKKOS_SRC += $(wildcard $(KOKKOS_PATH)/core/src/Cuda/*.cpp)
lib/kokkos/Makefile.kokkos:  KOKKOS_SRC += $(KOKKOS_PATH)/tpls/desul/src/Lock_Array_CUDA.cpp
lib/kokkos/Makefile.kokkos:  KOKKOS_HEADERS += $(wildcard $(KOKKOS_PATH)/core/src/Cuda/*.hpp)
lib/kokkos/Makefile.kokkos:  ifneq ($(CUDA_PATH),)
lib/kokkos/Makefile.kokkos:    KOKKOS_CPPLAGS += -I$(CUDA_PATH)/include
lib/kokkos/Makefile.kokkos:    ifeq ($(call kokkos_path_exists,$(CUDA_PATH)/lib64), 1)
lib/kokkos/Makefile.kokkos:      KOKKOS_LIBDIRS += -L$(CUDA_PATH)/lib64
lib/kokkos/Makefile.kokkos:      KOKKOS_CXXLDFLAGS += -L$(CUDA_PATH)/lib64
lib/kokkos/Makefile.kokkos:      KOKKOS_TPL_LIBRARY_DIRS += $(CUDA_PATH)/lib64
lib/kokkos/Makefile.kokkos:    else ifeq ($(call kokkos_path_exists,$(CUDA_PATH)/lib), 1)
lib/kokkos/Makefile.kokkos:      KOKKOS_LIBDIRS += -L$(CUDA_PATH)/lib
lib/kokkos/Makefile.kokkos:      KOKKOS_CXXLDFLAGS += -L$(CUDA_PATH)/lib
lib/kokkos/Makefile.kokkos:      KOKKOS_TPL_LIBRARY_DIRS += $(CUDA_PATH)/lib
lib/kokkos/Makefile.kokkos:      $(error Can't find CUDA library directory: no lib64 or lib directory in $(CUDA_PATH))
lib/kokkos/Makefile.kokkos:    KOKKOS_TPL_INCLUDE_DIRS += $(CUDA_PATH)/include
lib/kokkos/Makefile.kokkos:      KOKKOS_CXXFLAGS += --cuda-path=$(CUDA_PATH)
lib/kokkos/Makefile.kokkos:  KOKKOS_LIBS += -lcudart -lcuda
lib/kokkos/Makefile.kokkos:  KOKKOS_TPL_LIBRARY_NAMES += cudart cuda
lib/kokkos/Makefile.kokkos:  ifeq ($(KOKKOS_INTERNAL_USE_CUDA), 1)
lib/kokkos/Makefile.kokkos:ifeq ($(KOKKOS_INTERNAL_USE_OPENACC), 1)
lib/kokkos/Makefile.kokkos:  KOKKOS_SRC += $(wildcard $(KOKKOS_PATH)/core/src/OpenACC/*.cpp)
lib/kokkos/Makefile.kokkos:  KOKKOS_HEADERS += $(wildcard $(KOKKOS_PATH)/core/src/OpenACC/*.hpp)
lib/kokkos/Makefile.kokkos:  KOKKOS_CXXFLAGS += $(KOKKOS_INTERNAL_OPENACC_FLAG)
lib/kokkos/Makefile.kokkos:  KOKKOS_LDFLAGS += $(KOKKOS_INTERNAL_OPENACC_FLAG)
lib/kokkos/Makefile.kokkos:  KOKKOS_LIBS += $(KOKKOS_INTERNAL_OPENACC_LIB)
lib/kokkos/Makefile.kokkos:ifeq ($(KOKKOS_INTERNAL_USE_CUDA), 1)
lib/kokkos/Makefile.kokkos:  tmp := $(call desul_append_header,"$H""define DESUL_ATOMICS_ENABLE_CUDA")
lib/kokkos/Makefile.kokkos:  tmp := $(call desul_append_header,"/* $H""undef DESUL_ATOMICS_ENABLE_CUDA */")
lib/kokkos/Makefile.kokkos:ifeq ($(KOKKOS_INTERNAL_CUDA_USE_RELOC), 1)
lib/kokkos/Makefile.kokkos:  tmp := $(call desul_append_header,"$H""define DESUL_ATOMICS_ENABLE_CUDA_SEPARABLE_COMPILATION")
lib/kokkos/Makefile.kokkos:  tmp := $(call desul_append_header,"/* $H""undef DESUL_ATOMICS_ENABLE_CUDA_SEPARABLE_COMPILATION */")
lib/kokkos/Makefile.kokkos:ifeq ($(KOKKOS_INTERNAL_USE_OPENACC), 1)
lib/kokkos/Makefile.kokkos:  tmp := $(call desul_append_header,"$H""define DESUL_ATOMICS_ENABLE_OPENACC")
lib/kokkos/Makefile.kokkos:  tmp := $(call desul_append_header,"/* $H""undef DESUL_ATOMICS_ENABLE_OPENACC */")
lib/kokkos/generate_makefile.bash:     if [ "${UC_DEVICE}" == "CUDA" ]; then
lib/kokkos/generate_makefile.bash:       WITH_CUDA_BACKEND=ON
lib/kokkos/generate_makefile.bash:  if [ "${WITH_CUDA_BACKEND}" == "ON" ] && [ "${WITH_HIP_BACKEND}" == "ON" ]; then
lib/kokkos/generate_makefile.bash:     echo "Invalid configuration - Cuda and Hip cannot be simultaneously enabled"
lib/kokkos/generate_makefile.bash:  if [ "${WITH_CUDA_BACKEND}" == "ON" ] && [ "${WITH_OMPT_BACKEND}" == "ON" ]; then
lib/kokkos/generate_makefile.bash:     echo "Invalid configuration - Cuda and OpenMPTarget cannot be simultaneously enabled"
lib/kokkos/generate_makefile.bash:get_kokkos_cuda_option_list() {
lib/kokkos/generate_makefile.bash:  echo parsing KOKKOS_CUDA_OPTIONS=$KOKKOS_CUDA_OPTIONS
lib/kokkos/generate_makefile.bash:  KOKKOS_CUDA_OPTION_CMD=
lib/kokkos/generate_makefile.bash:  PARSE_CUDA_LST=$(echo $KOKKOS_CUDA_OPTIONS | tr "," "\n")
lib/kokkos/generate_makefile.bash:  for CUDA_ in $PARSE_CUDA_LST
lib/kokkos/generate_makefile.bash:     CUDA_OPT_NAME=
lib/kokkos/generate_makefile.bash:     if [ "${CUDA_}" == "enable_lambda" ]; then
lib/kokkos/generate_makefile.bash:        CUDA_OPT_NAME=CUDA_LAMBDA
lib/kokkos/generate_makefile.bash:     elif  [ "${CUDA_}" == "rdc" ]; then
lib/kokkos/generate_makefile.bash:        CUDA_OPT_NAME=CUDA_RELOCATABLE_DEVICE_CODE
lib/kokkos/generate_makefile.bash:     elif  [ "${CUDA_}" == "force_uvm" ]; then
lib/kokkos/generate_makefile.bash:        CUDA_OPT_NAME=CUDA_UVM
lib/kokkos/generate_makefile.bash:        echo "${CUDA_} is not a valid cuda options..."
lib/kokkos/generate_makefile.bash:     if [ "${CUDA_OPT_NAME}" != "" ]; then
lib/kokkos/generate_makefile.bash:        KOKKOS_CUDA_OPTION_CMD="-DKokkos_ENABLE_${CUDA_OPT_NAME}=ON ${KOKKOS_CUDA_OPTION_CMD}"
lib/kokkos/generate_makefile.bash:      echo "--with-cuda[=/Path/To/Cuda]:          Enable Cuda and set path to Cuda Toolkit."
lib/kokkos/generate_makefile.bash:      echo "--with-hip[=/Path/To/Hip]:            Enable Hip and set path to ROCM Toolkit."
lib/kokkos/generate_makefile.bash:      echo "               [AMD: GPU]"
lib/kokkos/generate_makefile.bash:      echo "                 AMD_GFX906      = AMD GPU MI50/MI60 GFX906"
lib/kokkos/generate_makefile.bash:      echo "                 AMD_GFX908      = AMD GPU MI100 GFX908"
lib/kokkos/generate_makefile.bash:      echo "                 AMD_GFX90A      = AMD GPU MI200 GFX90A"
lib/kokkos/generate_makefile.bash:      echo "                 AMD_GFX940      = AMD GPU MI300 GFX940"
lib/kokkos/generate_makefile.bash:      echo "                 AMD_GFX942      = AMD GPU MI300 GFX942"
lib/kokkos/generate_makefile.bash:      echo "                 AMD_GFX1030     = AMD GPU V620/W6800 GFX1030"
lib/kokkos/generate_makefile.bash:      echo "                 AMD_GFX1100     = AMD GPU RX 7900 XT(X) GFX1100"
lib/kokkos/generate_makefile.bash:      echo "               [Intel: GPU]"
lib/kokkos/generate_makefile.bash:      echo "                 INTEL_GEN       = SPIR64-based devices, e.g. Intel GPUs, using JIT"
lib/kokkos/generate_makefile.bash:      echo "                 INTEL_DG1       = Intel Iris XeMAX GPU"
lib/kokkos/generate_makefile.bash:      echo "                 INTEL_GEN9      = Intel GPU Gen9"
lib/kokkos/generate_makefile.bash:      echo "                 INTEL_GEN11     = Intel GPU Gen11"
lib/kokkos/generate_makefile.bash:      echo "                 INTEL_GEN12LP   = Intel GPU Gen12LP"
lib/kokkos/generate_makefile.bash:      echo "                 INTEL_XEHP      = Intel GPU Xe-HP"
lib/kokkos/generate_makefile.bash:      echo "                 INTEL_PVC       = Intel GPU Ponte Vecchio"
lib/kokkos/generate_makefile.bash:      echo "               [NVIDIA]"
lib/kokkos/generate_makefile.bash:      echo "                 Kepler30        = NVIDIA Kepler generation CC 3.0"
lib/kokkos/generate_makefile.bash:      echo "                 Kepler32        = NVIDIA Kepler generation CC 3.2"
lib/kokkos/generate_makefile.bash:      echo "                 Kepler35        = NVIDIA Kepler generation CC 3.5"
lib/kokkos/generate_makefile.bash:      echo "                 Kepler37        = NVIDIA Kepler generation CC 3.7"
lib/kokkos/generate_makefile.bash:      echo "                 Maxwell50       = NVIDIA Maxwell generation CC 5.0"
lib/kokkos/generate_makefile.bash:      echo "                 Maxwell52       = NVIDIA Maxwell generation CC 5.2"
lib/kokkos/generate_makefile.bash:      echo "                 Maxwell53       = NVIDIA Maxwell generation CC 5.3"
lib/kokkos/generate_makefile.bash:      echo "                 Pascal60        = NVIDIA Pascal generation CC 6.0"
lib/kokkos/generate_makefile.bash:      echo "                 Pascal61        = NVIDIA Pascal generation CC 6.1"
lib/kokkos/generate_makefile.bash:      echo "                 Volta70         = NVIDIA Volta generation CC 7.0"
lib/kokkos/generate_makefile.bash:      echo "                 Volta72         = NVIDIA Volta generation CC 7.2"
lib/kokkos/generate_makefile.bash:      echo "                 Ampere80        = NVIDIA Ampere generation CC 8.0"
lib/kokkos/generate_makefile.bash:      echo "                 Ampere86        = NVIDIA Ampere generation CC 8.6"
lib/kokkos/generate_makefile.bash:      echo "--with-cuda-options=[OPT]:    Additional options to CUDA:"
lib/kokkos/generate_makefile.bash:# For tracking if Cuda and Hip devices are enabled simultaneously
lib/kokkos/generate_makefile.bash:WITH_CUDA_BACKEND=OFF
lib/kokkos/generate_makefile.bash:    --with-cuda)
lib/kokkos/generate_makefile.bash:      update_kokkos_devices Cuda
lib/kokkos/generate_makefile.bash:      CUDA_PATH_NVCC=$(command -v nvcc)
lib/kokkos/generate_makefile.bash:      CUDA_PATH=${CUDA_PATH_NVCC%/bin/nvcc}
lib/kokkos/generate_makefile.bash:    # Catch this before '--with-cuda*'
lib/kokkos/generate_makefile.bash:    --with-cuda-options*)
lib/kokkos/generate_makefile.bash:      KOKKOS_CUDA_OPTIONS="${key#*=}"
lib/kokkos/generate_makefile.bash:    --with-cuda*)
lib/kokkos/generate_makefile.bash:      update_kokkos_devices Cuda
lib/kokkos/generate_makefile.bash:      CUDA_PATH="${key#*=}"
lib/kokkos/generate_makefile.bash:get_kokkos_cuda_option_list
lib/kokkos/generate_makefile.bash:   if [ ! "${CUDA_PATH}" == "" ]; then
lib/kokkos/generate_makefile.bash:      KOKKOS_CXXFLAGS="${KOKKOS_CXXFLAGS} --cuda-path=${CUDA_PATH}"
lib/kokkos/generate_makefile.bash:echo cmake $COMPILER_CMD  -DCMAKE_CXX_FLAGS="${KOKKOS_CXXFLAGS}" -DCMAKE_EXE_LINKER_FLAGS="${KOKKOS_LDFLAGS}" -DCMAKE_INSTALL_PREFIX=${PREFIX} ${KOKKOS_DEVICE_CMD} ${KOKKOS_ARCH_CMD} -DKokkos_ENABLE_TESTS=${KOKKOS_DO_TESTS} -DKokkos_ENABLE_EXAMPLES=${KOKKOS_DO_EXAMPLES} ${KOKKOS_OPTION_CMD} ${KOKKOS_CUDA_OPTION_CMD} ${KOKKOS_HIP_OPTION_CMD} -DCMAKE_VERBOSE_MAKEFILE=ON -DCMAKE_CXX_EXTENSIONS=OFF ${STANDARD_CMD} ${KOKKOS_DEBUG_CMD} ${KOKKOS_BC_CMD} ${KOKKOS_HWLOC_CMD} ${KOKKOS_HWLOC_PATH_CMD} -DKokkos_ENABLE_DEPRECATION_WARNINGS=${KOKKOS_DEPRECATED_CODE_WARNINGS} -DKokkos_ENABLE_DEPRECATED_CODE_4=${KOKKOS_DEPRECATED_CODE} ${KOKKOS_PATH}
lib/kokkos/generate_makefile.bash:cmake $COMPILER_CMD  -DCMAKE_CXX_FLAGS="${KOKKOS_CXXFLAGS//\"}" -DCMAKE_EXE_LINKER_FLAGS="${KOKKOS_LDFLAGS//\"}" -DCMAKE_INSTALL_PREFIX=${PREFIX} ${KOKKOS_DEVICE_CMD} ${KOKKOS_ARCH_CMD} -DKokkos_ENABLE_TESTS=${KOKKOS_DO_TESTS} -DKokkos_ENABLE_EXAMPLES=${KOKKOS_DO_EXAMPLES} ${KOKKOS_OPTION_CMD} ${KOKKOS_CUDA_OPTION_CMD} ${KOKKOS_HIP_OPTION_CMD} -DCMAKE_VERBOSE_MAKEFILE=ON -DCMAKE_CXX_EXTENSIONS=OFF ${STANDARD_CMD} ${KOKKOS_DEBUG_CMD} ${KOKKOS_BC_CMD} ${KOKKOS_HWLOC_CMD} ${KOKKOS_HWLOC_PATH_CMD} ${PASSTHRU_CMAKE_FLAGS} -DKokkos_ENABLE_DEPRECATION_WARNINGS=${KOKKOS_DEPRECATED_CODE_WARNINGS} -DKokkos_ENABLE_DEPRECATED_CODE_4=${KOKKOS_DEPRECATED_CODE} ${KOKKOS_PATH}
lib/kokkos/cmake/kokkos_arch.cmake:DECLARE_AND_CHECK_HOST_ARCH(ARMV9_GRACE       "ARMv9 NVIDIA Grace CPU")
lib/kokkos/cmake/kokkos_arch.cmake:IF(Kokkos_ENABLE_CUDA OR Kokkos_ENABLE_OPENMPTARGET OR Kokkos_ENABLE_OPENACC OR Kokkos_ENABLE_SYCL)
lib/kokkos/cmake/kokkos_arch.cmake:  SET(KOKKOS_SHOW_CUDA_ARCHS ON)
lib/kokkos/cmake/kokkos_arch.cmake:KOKKOS_ARCH_OPTION(KEPLER30        GPU  "NVIDIA Kepler generation CC 3.0"  "KOKKOS_SHOW_CUDA_ARCHS")
lib/kokkos/cmake/kokkos_arch.cmake:KOKKOS_ARCH_OPTION(KEPLER32        GPU  "NVIDIA Kepler generation CC 3.2"  "KOKKOS_SHOW_CUDA_ARCHS")
lib/kokkos/cmake/kokkos_arch.cmake:KOKKOS_ARCH_OPTION(KEPLER35        GPU  "NVIDIA Kepler generation CC 3.5"  "KOKKOS_SHOW_CUDA_ARCHS")
lib/kokkos/cmake/kokkos_arch.cmake:KOKKOS_ARCH_OPTION(KEPLER37        GPU  "NVIDIA Kepler generation CC 3.7"  "KOKKOS_SHOW_CUDA_ARCHS")
lib/kokkos/cmake/kokkos_arch.cmake:KOKKOS_ARCH_OPTION(MAXWELL50       GPU  "NVIDIA Maxwell generation CC 5.0" "KOKKOS_SHOW_CUDA_ARCHS")
lib/kokkos/cmake/kokkos_arch.cmake:KOKKOS_ARCH_OPTION(MAXWELL52       GPU  "NVIDIA Maxwell generation CC 5.2" "KOKKOS_SHOW_CUDA_ARCHS")
lib/kokkos/cmake/kokkos_arch.cmake:KOKKOS_ARCH_OPTION(MAXWELL53       GPU  "NVIDIA Maxwell generation CC 5.3" "KOKKOS_SHOW_CUDA_ARCHS")
lib/kokkos/cmake/kokkos_arch.cmake:KOKKOS_ARCH_OPTION(PASCAL60        GPU  "NVIDIA Pascal generation CC 6.0"  "KOKKOS_SHOW_CUDA_ARCHS")
lib/kokkos/cmake/kokkos_arch.cmake:KOKKOS_ARCH_OPTION(PASCAL61        GPU  "NVIDIA Pascal generation CC 6.1"  "KOKKOS_SHOW_CUDA_ARCHS")
lib/kokkos/cmake/kokkos_arch.cmake:KOKKOS_ARCH_OPTION(VOLTA70         GPU  "NVIDIA Volta generation CC 7.0"   "KOKKOS_SHOW_CUDA_ARCHS")
lib/kokkos/cmake/kokkos_arch.cmake:KOKKOS_ARCH_OPTION(VOLTA72         GPU  "NVIDIA Volta generation CC 7.2"   "KOKKOS_SHOW_CUDA_ARCHS")
lib/kokkos/cmake/kokkos_arch.cmake:KOKKOS_ARCH_OPTION(TURING75        GPU  "NVIDIA Turing generation CC 7.5"  "KOKKOS_SHOW_CUDA_ARCHS")
lib/kokkos/cmake/kokkos_arch.cmake:KOKKOS_ARCH_OPTION(AMPERE80        GPU  "NVIDIA Ampere generation CC 8.0"  "KOKKOS_SHOW_CUDA_ARCHS")
lib/kokkos/cmake/kokkos_arch.cmake:KOKKOS_ARCH_OPTION(AMPERE86        GPU  "NVIDIA Ampere generation CC 8.6"  "KOKKOS_SHOW_CUDA_ARCHS")
lib/kokkos/cmake/kokkos_arch.cmake:KOKKOS_ARCH_OPTION(ADA89           GPU  "NVIDIA Ada generation CC 8.9"     "KOKKOS_SHOW_CUDA_ARCHS")
lib/kokkos/cmake/kokkos_arch.cmake:KOKKOS_ARCH_OPTION(HOPPER90        GPU  "NVIDIA Hopper generation CC 9.0"  "KOKKOS_SHOW_CUDA_ARCHS")
lib/kokkos/cmake/kokkos_arch.cmake:IF(Kokkos_ENABLE_HIP OR Kokkos_ENABLE_OPENMPTARGET OR Kokkos_ENABLE_OPENACC OR Kokkos_ENABLE_SYCL)
lib/kokkos/cmake/kokkos_arch.cmake:LIST(APPEND SUPPORTED_AMD_GPUS       MI300 MI300)
lib/kokkos/cmake/kokkos_arch.cmake:LIST(APPEND SUPPORTED_AMD_GPUS       MI200    MI200       MI100    MI100)
lib/kokkos/cmake/kokkos_arch.cmake:LIST(APPEND SUPPORTED_AMD_GPUS       MI50/60  MI50/60)
lib/kokkos/cmake/kokkos_arch.cmake:LIST(APPEND SUPPORTED_AMD_GPUS       PHOENIX      RX7900XTX    V620/W6800  V620/W6800)
lib/kokkos/cmake/kokkos_arch.cmake:  LIST(GET SUPPORTED_AMD_GPUS ${LIST_INDEX} GPU)
lib/kokkos/cmake/kokkos_arch.cmake:  KOKKOS_ARCH_OPTION(${ARCH}         GPU  "AMD GPU ${GPU} ${FLAG}"      "KOKKOS_SHOW_HIP_ARCHS")
lib/kokkos/cmake/kokkos_arch.cmake:KOKKOS_ARCH_OPTION(INTEL_GEN       GPU  "SPIR64-based devices, e.g. Intel GPUs, using JIT" "KOKKOS_SHOW_SYCL_ARCHS")
lib/kokkos/cmake/kokkos_arch.cmake:KOKKOS_ARCH_OPTION(INTEL_DG1       GPU  "Intel Iris XeMAX GPU"                             "KOKKOS_SHOW_SYCL_ARCHS")
lib/kokkos/cmake/kokkos_arch.cmake:KOKKOS_ARCH_OPTION(INTEL_GEN9      GPU  "Intel GPU Gen9"                                   "KOKKOS_SHOW_SYCL_ARCHS")
lib/kokkos/cmake/kokkos_arch.cmake:KOKKOS_ARCH_OPTION(INTEL_GEN11     GPU  "Intel GPU Gen11"                                  "KOKKOS_SHOW_SYCL_ARCHS")
lib/kokkos/cmake/kokkos_arch.cmake:KOKKOS_ARCH_OPTION(INTEL_GEN12LP   GPU  "Intel GPU Gen12LP"                                "KOKKOS_SHOW_SYCL_ARCHS")
lib/kokkos/cmake/kokkos_arch.cmake:KOKKOS_ARCH_OPTION(INTEL_XEHP      GPU  "Intel GPU Xe-HP"                                  "KOKKOS_SHOW_SYCL_ARCHS")
lib/kokkos/cmake/kokkos_arch.cmake:KOKKOS_ARCH_OPTION(INTEL_PVC       GPU  "Intel GPU Ponte Vecchio"                          "KOKKOS_SHOW_SYCL_ARCHS")
lib/kokkos/cmake/kokkos_arch.cmake:  IF(KOKKOS_ENABLE_OPENACC)
lib/kokkos/cmake/kokkos_arch.cmake:#------------------------------- KOKKOS_CUDA_OPTIONS ---------------------------
lib/kokkos/cmake/kokkos_arch.cmake:GLOBAL_SET(KOKKOS_CUDA_OPTIONS)
lib/kokkos/cmake/kokkos_arch.cmake:IF(KOKKOS_CXX_COMPILER_ID STREQUAL NVIDIA)
lib/kokkos/cmake/kokkos_arch.cmake:  GLOBAL_APPEND(KOKKOS_CUDA_OPTIONS "-extended-lambda")
lib/kokkos/cmake/kokkos_arch.cmake:  GLOBAL_APPEND(KOKKOS_CUDA_OPTIONS "-Wext-lambda-captures-this")
lib/kokkos/cmake/kokkos_arch.cmake:IF (KOKKOS_ENABLE_CUDA_CONSTEXPR)
lib/kokkos/cmake/kokkos_arch.cmake:  IF(KOKKOS_CXX_COMPILER_ID STREQUAL NVIDIA)
lib/kokkos/cmake/kokkos_arch.cmake:    GLOBAL_APPEND(KOKKOS_CUDA_OPTIONS "-expt-relaxed-constexpr")
lib/kokkos/cmake/kokkos_arch.cmake:  SET(CUDA_ARCH_FLAG "--cuda-gpu-arch")
lib/kokkos/cmake/kokkos_arch.cmake:  GLOBAL_APPEND(KOKKOS_CUDA_OPTIONS -x cuda)
lib/kokkos/cmake/kokkos_arch.cmake:  # Kokkos_CUDA_DIR has priority over CUDAToolkit_BIN_DIR
lib/kokkos/cmake/kokkos_arch.cmake:  IF (Kokkos_CUDA_DIR)
lib/kokkos/cmake/kokkos_arch.cmake:    GLOBAL_APPEND(KOKKOS_CUDA_OPTIONS --cuda-path=${Kokkos_CUDA_DIR})
lib/kokkos/cmake/kokkos_arch.cmake:  ELSEIF(CUDAToolkit_BIN_DIR)
lib/kokkos/cmake/kokkos_arch.cmake:    GLOBAL_APPEND(KOKKOS_CUDA_OPTIONS --cuda-path=${CUDAToolkit_BIN_DIR}/..)
lib/kokkos/cmake/kokkos_arch.cmake:ELSEIF(KOKKOS_CXX_COMPILER_ID STREQUAL NVIDIA)
lib/kokkos/cmake/kokkos_arch.cmake:  SET(CUDA_ARCH_FLAG "-arch")
lib/kokkos/cmake/kokkos_arch.cmake:IF (KOKKOS_CXX_COMPILER_ID STREQUAL NVIDIA)
lib/kokkos/cmake/kokkos_arch.cmake:    GLOBAL_APPEND(KOKKOS_CUDA_OPTIONS -lineinfo)
lib/kokkos/cmake/kokkos_arch.cmake:KOKKOS_OPTION(IMPL_AMDGPU_FLAGS "" STRING "Set compiler flags for AMD GPUs")
lib/kokkos/cmake/kokkos_arch.cmake:KOKKOS_OPTION(IMPL_AMDGPU_LINK "" STRING "Set linker flags for AMD GPUs")
lib/kokkos/cmake/kokkos_arch.cmake:MARK_AS_ADVANCED(Kokkos_IMPL_AMDGPU_FLAGS)
lib/kokkos/cmake/kokkos_arch.cmake:MARK_AS_ADVANCED(Kokkos_IMPL_AMDGPU_LINK)
lib/kokkos/cmake/kokkos_arch.cmake:GLOBAL_SET(KOKKOS_AMDGPU_OPTIONS)
lib/kokkos/cmake/kokkos_arch.cmake:  SET(AMDGPU_ARCH_FLAG "--offload-arch")
lib/kokkos/cmake/kokkos_arch.cmake:    GLOBAL_APPEND(KOKKOS_AMDGPU_OPTIONS -xhip)
lib/kokkos/cmake/kokkos_arch.cmake:    IF(DEFINED ENV{ROCM_PATH})
lib/kokkos/cmake/kokkos_arch.cmake:      GLOBAL_APPEND(KOKKOS_AMDGPU_OPTIONS --rocm-path=$ENV{ROCM_PATH})
lib/kokkos/cmake/kokkos_arch.cmake:IF(KOKKOS_ARCH_ARM_NEON AND KOKKOS_CXX_COMPILER_ID STREQUAL NVIDIA)
lib/kokkos/cmake/kokkos_arch.cmake:IF (NOT KOKKOS_COMPILE_LANGUAGE STREQUAL CUDA)
lib/kokkos/cmake/kokkos_arch.cmake:  IF (KOKKOS_ENABLE_CUDA_RELOCATABLE_DEVICE_CODE)
lib/kokkos/cmake/kokkos_arch.cmake:        Clang  -fcuda-rdc
lib/kokkos/cmake/kokkos_arch.cmake:        NVIDIA --relocatable-device-code=true
lib/kokkos/cmake/kokkos_arch.cmake:      DEFAULT -fgpu-rdc
lib/kokkos/cmake/kokkos_arch.cmake:    IF (NOT KOKKOS_CXX_COMPILER_ID STREQUAL HIPCC AND NOT KOKKOS_IMPL_AMDGPU_FLAGS)
lib/kokkos/cmake/kokkos_arch.cmake:      DEFAULT -fno-gpu-rdc
lib/kokkos/cmake/kokkos_arch.cmake:SET(CUDA_ARCH_ALREADY_SPECIFIED "")
lib/kokkos/cmake/kokkos_arch.cmake:FUNCTION(CHECK_CUDA_ARCH ARCH FLAG)
lib/kokkos/cmake/kokkos_arch.cmake:    IF(CUDA_ARCH_ALREADY_SPECIFIED)
lib/kokkos/cmake/kokkos_arch.cmake:      MESSAGE(FATAL_ERROR "Multiple GPU architectures given! Already have ${CUDA_ARCH_ALREADY_SPECIFIED}, but trying to add ${ARCH}. If you are re-running CMake, try clearing the cache and running again.")
lib/kokkos/cmake/kokkos_arch.cmake:    SET(CUDA_ARCH_ALREADY_SPECIFIED ${ARCH} PARENT_SCOPE)
lib/kokkos/cmake/kokkos_arch.cmake:    IF (NOT KOKKOS_ENABLE_CUDA AND NOT KOKKOS_ENABLE_OPENMPTARGET AND NOT KOKKOS_ENABLE_SYCL AND NOT KOKKOS_ENABLE_OPENACC)
lib/kokkos/cmake/kokkos_arch.cmake:      MESSAGE(WARNING "Given CUDA arch ${ARCH}, but Kokkos_ENABLE_CUDA, Kokkos_ENABLE_SYCL, Kokkos_ENABLE_OPENACC, and Kokkos_ENABLE_OPENMPTARGET are OFF. Option will be ignored.")
lib/kokkos/cmake/kokkos_arch.cmake:      IF(KOKKOS_ENABLE_CUDA)
lib/kokkos/cmake/kokkos_arch.cmake:        SET(KOKKOS_CUDA_ARCHITECTURES ${CMAKE_ARCH})
lib/kokkos/cmake/kokkos_arch.cmake:        SET(KOKKOS_CUDA_ARCHITECTURES ${CMAKE_ARCH} PARENT_SCOPE)
lib/kokkos/cmake/kokkos_arch.cmake:      SET(KOKKOS_CUDA_ARCH_FLAG ${FLAG} PARENT_SCOPE)
lib/kokkos/cmake/kokkos_arch.cmake:        SET(CMAKE_CUDA_ARCHITECTURES ${KOKKOS_CUDA_ARCHITECTURES} PARENT_SCOPE)
lib/kokkos/cmake/kokkos_arch.cmake:        GLOBAL_APPEND(KOKKOS_CUDA_OPTIONS "${CUDA_ARCH_FLAG}=${FLAG}")
lib/kokkos/cmake/kokkos_arch.cmake:        IF(KOKKOS_ENABLE_CUDA_RELOCATABLE_DEVICE_CODE OR KOKKOS_CXX_COMPILER_ID STREQUAL NVIDIA)
lib/kokkos/cmake/kokkos_arch.cmake:          GLOBAL_APPEND(KOKKOS_LINK_OPTIONS "${CUDA_ARCH_FLAG}=${FLAG}")
lib/kokkos/cmake/kokkos_arch.cmake:  LIST(APPEND KOKKOS_CUDA_ARCH_FLAGS ${FLAG})
lib/kokkos/cmake/kokkos_arch.cmake:  SET(KOKKOS_CUDA_ARCH_FLAGS ${KOKKOS_CUDA_ARCH_FLAGS} PARENT_SCOPE)
lib/kokkos/cmake/kokkos_arch.cmake:  LIST(APPEND KOKKOS_CUDA_ARCH_LIST ${ARCH})
lib/kokkos/cmake/kokkos_arch.cmake:  SET(KOKKOS_CUDA_ARCH_LIST ${KOKKOS_CUDA_ARCH_LIST} PARENT_SCOPE)
lib/kokkos/cmake/kokkos_arch.cmake:#These will define KOKKOS_CUDA_ARCH_FLAG
lib/kokkos/cmake/kokkos_arch.cmake:CHECK_CUDA_ARCH(KEPLER30  sm_30)
lib/kokkos/cmake/kokkos_arch.cmake:CHECK_CUDA_ARCH(KEPLER32  sm_32)
lib/kokkos/cmake/kokkos_arch.cmake:CHECK_CUDA_ARCH(KEPLER35  sm_35)
lib/kokkos/cmake/kokkos_arch.cmake:CHECK_CUDA_ARCH(KEPLER37  sm_37)
lib/kokkos/cmake/kokkos_arch.cmake:CHECK_CUDA_ARCH(MAXWELL50 sm_50)
lib/kokkos/cmake/kokkos_arch.cmake:CHECK_CUDA_ARCH(MAXWELL52 sm_52)
lib/kokkos/cmake/kokkos_arch.cmake:CHECK_CUDA_ARCH(MAXWELL53 sm_53)
lib/kokkos/cmake/kokkos_arch.cmake:CHECK_CUDA_ARCH(PASCAL60  sm_60)
lib/kokkos/cmake/kokkos_arch.cmake:CHECK_CUDA_ARCH(PASCAL61  sm_61)
lib/kokkos/cmake/kokkos_arch.cmake:CHECK_CUDA_ARCH(VOLTA70   sm_70)
lib/kokkos/cmake/kokkos_arch.cmake:CHECK_CUDA_ARCH(VOLTA72   sm_72)
lib/kokkos/cmake/kokkos_arch.cmake:CHECK_CUDA_ARCH(TURING75  sm_75)
lib/kokkos/cmake/kokkos_arch.cmake:CHECK_CUDA_ARCH(AMPERE80  sm_80)
lib/kokkos/cmake/kokkos_arch.cmake:CHECK_CUDA_ARCH(AMPERE86  sm_86)
lib/kokkos/cmake/kokkos_arch.cmake:CHECK_CUDA_ARCH(ADA89     sm_89)
lib/kokkos/cmake/kokkos_arch.cmake:CHECK_CUDA_ARCH(HOPPER90  sm_90)
lib/kokkos/cmake/kokkos_arch.cmake:SET(AMDGPU_ARCH_ALREADY_SPECIFIED "")
lib/kokkos/cmake/kokkos_arch.cmake:FUNCTION(CHECK_AMDGPU_ARCH ARCH FLAG)
lib/kokkos/cmake/kokkos_arch.cmake:    IF(AMDGPU_ARCH_ALREADY_SPECIFIED)
lib/kokkos/cmake/kokkos_arch.cmake:      MESSAGE(FATAL_ERROR "Multiple GPU architectures given! Already have ${AMDGPU_ARCH_ALREADY_SPECIFIED}, but trying to add ${ARCH}. If you are re-running CMake, try clearing the cache and running again.")
lib/kokkos/cmake/kokkos_arch.cmake:    SET(AMDGPU_ARCH_ALREADY_SPECIFIED ${ARCH} PARENT_SCOPE)
lib/kokkos/cmake/kokkos_arch.cmake:    IF (NOT KOKKOS_ENABLE_HIP AND NOT KOKKOS_ENABLE_OPENMPTARGET AND NOT KOKKOS_ENABLE_OPENACC AND NOT KOKKOS_ENABLE_SYCL)
lib/kokkos/cmake/kokkos_arch.cmake:      MESSAGE(WARNING "Given AMD GPU architecture ${ARCH}, but Kokkos_ENABLE_HIP, Kokkos_ENABLE_SYCL, Kokkos_ENABLE_OPENACC, and Kokkos_ENABLE_OPENMPTARGET are OFF. Option will be ignored.")
lib/kokkos/cmake/kokkos_arch.cmake:        IF(NOT KOKKOS_IMPL_AMDGPU_FLAGS)
lib/kokkos/cmake/kokkos_arch.cmake:          SET(KOKKOS_AMDGPU_ARCH_FLAG ${FLAG} PARENT_SCOPE)
lib/kokkos/cmake/kokkos_arch.cmake:          GLOBAL_APPEND(KOKKOS_AMDGPU_OPTIONS "${AMDGPU_ARCH_FLAG}=${FLAG}")
lib/kokkos/cmake/kokkos_arch.cmake:          GLOBAL_APPEND(KOKKOS_LINK_OPTIONS "${AMDGPU_ARCH_FLAG}=${FLAG}")
lib/kokkos/cmake/kokkos_arch.cmake:#These will define KOKKOS_AMDGPU_ARCH_FLAG
lib/kokkos/cmake/kokkos_arch.cmake:  CHECK_AMDGPU_ARCH(${ARCH} ${FLAG})
lib/kokkos/cmake/kokkos_arch.cmake:IF(KOKKOS_IMPL_AMDGPU_FLAGS)
lib/kokkos/cmake/kokkos_arch.cmake:  IF (NOT AMDGPU_ARCH_ALREADY_SPECIFIED)
lib/kokkos/cmake/kokkos_arch.cmake:    MESSAGE(FATAL_ERROR "When IMPL_AMDGPU_FLAGS is set the architecture autodectection is disabled. "
lib/kokkos/cmake/kokkos_arch.cmake:      "Please explicitly set the GPU architecture.")
lib/kokkos/cmake/kokkos_arch.cmake:  GLOBAL_APPEND(KOKKOS_AMDGPU_OPTIONS "${KOKKOS_IMPL_AMDGPU_FLAGS}")
lib/kokkos/cmake/kokkos_arch.cmake:  GLOBAL_APPEND(KOKKOS_LINK_OPTIONS "${KOKKOS_IMPL_AMDGPU_LINK}")
lib/kokkos/cmake/kokkos_arch.cmake:  CHECK_AMDGPU_ARCH(${ARCH} ${FLAG})
lib/kokkos/cmake/kokkos_arch.cmake:  IF(KOKKOS_ARCH_INTEL_GPU)
lib/kokkos/cmake/kokkos_arch.cmake:    MESSAGE(FATAL_ERROR "Specifying multiple Intel GPU architectures is not allowed!")
lib/kokkos/cmake/kokkos_arch.cmake:  SET(KOKKOS_ARCH_INTEL_GPU ON)
lib/kokkos/cmake/kokkos_arch.cmake:  SET(CLANG_CUDA_ARCH ${KOKKOS_CUDA_ARCH_FLAG})
lib/kokkos/cmake/kokkos_arch.cmake:  IF (CLANG_CUDA_ARCH)
lib/kokkos/cmake/kokkos_arch.cmake:      STRING(REPLACE "sm_" "cc" NVHPC_CUDA_ARCH ${CLANG_CUDA_ARCH})
lib/kokkos/cmake/kokkos_arch.cmake:        Clang -Xopenmp-target -march=${CLANG_CUDA_ARCH} -fopenmp-targets=nvptx64
lib/kokkos/cmake/kokkos_arch.cmake:        NVHPC -gpu=${NVHPC_CUDA_ARCH}
lib/kokkos/cmake/kokkos_arch.cmake:  SET(CLANG_AMDGPU_ARCH ${KOKKOS_AMDGPU_ARCH_FLAG})
lib/kokkos/cmake/kokkos_arch.cmake:  IF (CLANG_AMDGPU_ARCH)
lib/kokkos/cmake/kokkos_arch.cmake:      Clang -Xopenmp-target=amdgcn-amd-amdhsa -march=${CLANG_AMDGPU_ARCH} -fopenmp-targets=amdgcn-amd-amdhsa
lib/kokkos/cmake/kokkos_arch.cmake:IF (KOKKOS_ENABLE_OPENACC)
lib/kokkos/cmake/kokkos_arch.cmake:  IF(KOKKOS_CUDA_ARCH_FLAG)
lib/kokkos/cmake/kokkos_arch.cmake:    SET(CLANG_CUDA_ARCH ${KOKKOS_CUDA_ARCH_FLAG})
lib/kokkos/cmake/kokkos_arch.cmake:    STRING(REPLACE "sm_" "cc" NVHPC_CUDA_ARCH ${KOKKOS_CUDA_ARCH_FLAG})
lib/kokkos/cmake/kokkos_arch.cmake:      NVHPC -acc -gpu=${NVHPC_CUDA_ARCH}
lib/kokkos/cmake/kokkos_arch.cmake:      Clang -Xopenmp-target=nvptx64-nvidia-cuda -march=${CLANG_CUDA_ARCH}
lib/kokkos/cmake/kokkos_arch.cmake:            -fopenmp-targets=nvptx64-nvidia-cuda
lib/kokkos/cmake/kokkos_arch.cmake:  ELSEIF(KOKKOS_AMDGPU_ARCH_FLAG)
lib/kokkos/cmake/kokkos_arch.cmake:      Clang -Xopenmp-target=amdgcn-amd-amdhsa -march=${KOKKOS_AMDGPU_ARCH_FLAG}
lib/kokkos/cmake/kokkos_arch.cmake:  IF(CUDA_ARCH_ALREADY_SPECIFIED)
lib/kokkos/cmake/kokkos_arch.cmake:        DEFAULT -fsycl-targets=nvptx64-nvidia-cuda -Xsycl-target-backend=nvptx64-nvidia-cuda --cuda-gpu-arch=${KOKKOS_CUDA_ARCH_FLAG}
lib/kokkos/cmake/kokkos_arch.cmake:      MESSAGE(SEND_ERROR "Setting a CUDA architecture for SYCL is only allowed with Kokkos_ENABLE_UNSUPPORTED_ARCHS=ON!")
lib/kokkos/cmake/kokkos_arch.cmake:  ELSEIF(AMDGPU_ARCH_ALREADY_SPECIFIED)
lib/kokkos/cmake/kokkos_arch.cmake:        DEFAULT -fsycl-targets=amdgcn-amd-amdhsa -Xsycl-target-backend --offload-arch=${KOKKOS_AMDGPU_ARCH_FLAG}
lib/kokkos/cmake/kokkos_arch.cmake:      MESSAGE(SEND_ERROR "Setting a AMDGPU architecture for SYCL is only allowed with Kokkos_ENABLE_UNSUPPORTED_ARCHS=ON!")
lib/kokkos/cmake/kokkos_arch.cmake:IF(KOKKOS_ENABLE_CUDA AND NOT CUDA_ARCH_ALREADY_SPECIFIED)
lib/kokkos/cmake/kokkos_arch.cmake:  # Try to autodetect the CUDA Compute Capability by asking the device
lib/kokkos/cmake/kokkos_arch.cmake:  SET(_BINARY_TEST_DIR ${CMAKE_CURRENT_BINARY_DIR}/cmake/compile_tests/CUDAComputeCapabilityWorkdir)
lib/kokkos/cmake/kokkos_arch.cmake:    ${CMAKE_CURRENT_SOURCE_DIR}/cmake/compile_tests/cuda_compute_capability.cc
lib/kokkos/cmake/kokkos_arch.cmake:    RUN_OUTPUT_VARIABLE _CUDA_COMPUTE_CAPABILITY)
lib/kokkos/cmake/kokkos_arch.cmake:    # check to see if CUDA is not already enabled (may happen when Kokkos is subproject)
lib/kokkos/cmake/kokkos_arch.cmake:    # language has to be fully enabled, just checking for CMAKE_CUDA_COMPILER isn't enough
lib/kokkos/cmake/kokkos_arch.cmake:    IF(NOT "CUDA" IN_LIST _ENABLED_LANGUAGES)
lib/kokkos/cmake/kokkos_arch.cmake:      # make sure the user knows that we aren't using CUDA compiler for anything else
lib/kokkos/cmake/kokkos_arch.cmake:      MESSAGE(STATUS "CUDA auto-detection of architecture failed with ${CMAKE_CXX_COMPILER}. Enabling CUDA language ONLY to auto-detect architecture...")
lib/kokkos/cmake/kokkos_arch.cmake:      CHECK_LANGUAGE(CUDA)
lib/kokkos/cmake/kokkos_arch.cmake:      IF(CMAKE_CUDA_COMPILER)
lib/kokkos/cmake/kokkos_arch.cmake:        ENABLE_LANGUAGE(CUDA)
lib/kokkos/cmake/kokkos_arch.cmake:        MESSAGE(STATUS "CUDA language could not be enabled")
lib/kokkos/cmake/kokkos_arch.cmake:    # if CUDA was enabled, this will be defined
lib/kokkos/cmake/kokkos_arch.cmake:    IF(CMAKE_CUDA_COMPILER)
lib/kokkos/cmake/kokkos_arch.cmake:      # copy our test to .cu so cmake compiles as CUDA
lib/kokkos/cmake/kokkos_arch.cmake:        ${CMAKE_CURRENT_SOURCE_DIR}/cmake/compile_tests/cuda_compute_capability.cc
lib/kokkos/cmake/kokkos_arch.cmake:        ${CMAKE_CURRENT_BINARY_DIR}/compile_tests/cuda_compute_capability.cu
lib/kokkos/cmake/kokkos_arch.cmake:        ${CMAKE_CURRENT_BINARY_DIR}/compile_tests/cuda_compute_capability.cu
lib/kokkos/cmake/kokkos_arch.cmake:        RUN_OUTPUT_VARIABLE _CUDA_COMPUTE_CAPABILITY)
lib/kokkos/cmake/kokkos_arch.cmake:  LIST(FIND KOKKOS_CUDA_ARCH_FLAGS sm_${_CUDA_COMPUTE_CAPABILITY} FLAG_INDEX)
lib/kokkos/cmake/kokkos_arch.cmake:    MESSAGE(STATUS "Detected CUDA Compute Capability ${_CUDA_COMPUTE_CAPABILITY}")
lib/kokkos/cmake/kokkos_arch.cmake:    LIST(GET KOKKOS_CUDA_ARCH_LIST ${FLAG_INDEX} ARCHITECTURE)
lib/kokkos/cmake/kokkos_arch.cmake:    CHECK_CUDA_ARCH(${ARCHITECTURE} sm_${_CUDA_COMPUTE_CAPABILITY})
lib/kokkos/cmake/kokkos_arch.cmake:    MESSAGE(SEND_ERROR "CUDA enabled but no NVIDIA GPU architecture currently enabled and auto-detection failed. "
lib/kokkos/cmake/kokkos_arch.cmake:                       "Please give one -DKokkos_ARCH_{..}=ON' to enable an NVIDIA GPU architecture.\n"
lib/kokkos/cmake/kokkos_arch.cmake:                       "You can yourself try to compile ${CMAKE_CURRENT_SOURCE_DIR}/cmake/compile_tests/cuda_compute_capability.cc and run the executable. "
lib/kokkos/cmake/kokkos_arch.cmake:#HIP detection of gpu arch
lib/kokkos/cmake/kokkos_arch.cmake:IF(KOKKOS_ENABLE_HIP AND NOT AMDGPU_ARCH_ALREADY_SPECIFIED AND NOT KOKKOS_IMPL_AMDGPU_FLAGS)
lib/kokkos/cmake/kokkos_arch.cmake:  FIND_PROGRAM(ROCM_ENUMERATOR rocm_agent_enumerator)
lib/kokkos/cmake/kokkos_arch.cmake:  IF(NOT ROCM_ENUMERATOR)
lib/kokkos/cmake/kokkos_arch.cmake:    MESSAGE(FATAL_ERROR "Autodetection of AMD GPU architecture not possible as "
lib/kokkos/cmake/kokkos_arch.cmake:      "rocm_agent_enumerator could not be found. "
lib/kokkos/cmake/kokkos_arch.cmake:    EXECUTE_PROCESS(COMMAND ${ROCM_ENUMERATOR} OUTPUT_VARIABLE GPU_ARCHS)
lib/kokkos/cmake/kokkos_arch.cmake:    STRING(LENGTH "${GPU_ARCHS}" len_str)
lib/kokkos/cmake/kokkos_arch.cmake:      MESSAGE(SEND_ERROR "HIP enabled but no AMD GPU architecture could be automatically detected. "
lib/kokkos/cmake/kokkos_arch.cmake:                         "Please manually specify one AMD GPU architecture via -DKokkos_ARCH_{..}=ON'.")
lib/kokkos/cmake/kokkos_arch.cmake:    # check for known gpu archs, otherwise error out
lib/kokkos/cmake/kokkos_arch.cmake:        STRING(REGEX MATCH "(${FLAG})" DETECTED_GPU_ARCH ${GPU_ARCHS})
lib/kokkos/cmake/kokkos_arch.cmake:        IF("${DETECTED_GPU_ARCH}" STREQUAL "${FLAG}")
lib/kokkos/cmake/kokkos_arch.cmake:        MESSAGE(FATAL_ERROR "HIP enabled but no automatically detected AMD GPU architecture "
lib/kokkos/cmake/kokkos_arch.cmake:         "Please manually specify one AMD GPU architecture via -DKokkos_ARCH_{..}=ON'.")
lib/kokkos/cmake/kokkos_arch.cmake:    SET(KOKKOS_ARCH_AMD_GPU ON)
lib/kokkos/cmake/kokkos_arch.cmake:FOREACH (_BACKEND Cuda OpenMPTarget HIP SYCL OpenACC)
lib/kokkos/cmake/kokkos_arch.cmake:    IF (${_BACKEND} STREQUAL "Cuda")
lib/kokkos/cmake/kokkos_arch.cmake:       IF(KOKKOS_ENABLE_CUDA_UVM)
lib/kokkos/cmake/kokkos_arch.cmake:          MESSAGE(DEPRECATION "Setting Kokkos_ENABLE_CUDA_UVM is deprecated - use the portable Kokkos::SharedSpace as an explicit memory space in your code instead")
lib/kokkos/cmake/kokkos_arch.cmake:            MESSAGE(FATAL_ERROR "Kokkos_ENABLE_DEPRECATED_CODE_4 must be set to use Kokkos_ENABLE_CUDA_UVM")
lib/kokkos/cmake/kokkos_corner_cases.cmake:IF (KOKKOS_CXX_COMPILER_ID STREQUAL NVIDIA AND KOKKOS_ENABLE_CUDA_CONSTEXPR AND KOKKOS_CXX_COMPILER_VERSION VERSION_LESS 11.2)
lib/kokkos/cmake/kokkos_corner_cases.cmake:  MESSAGE(WARNING "You have requested -DKokkos_ENABLE_CUDA_CONSTEXPR=ON for NVCC ${KOKKOS_CXX_COMPILER_VERSION} which is known to trigger compiler bugs before NVCC version 11.2. See https://github.com/kokkos/kokkos/issues/3496")
lib/kokkos/cmake/compile_tests/cuda_compute_capability.cc:#include <cuda_runtime_api.h>
lib/kokkos/cmake/compile_tests/cuda_compute_capability.cc:  cudaDeviceProp device_properties;
lib/kokkos/cmake/compile_tests/cuda_compute_capability.cc:  const cudaError_t error = cudaGetDeviceProperties(&device_properties,
lib/kokkos/cmake/compile_tests/cuda_compute_capability.cc:  if (error != cudaSuccess) {
lib/kokkos/cmake/compile_tests/cuda_compute_capability.cc:    std::cout << "CUDA error: " << cudaGetErrorString(error) << '\n';
lib/kokkos/cmake/KokkosCore_config.h.in:#cmakedefine KOKKOS_ENABLE_OPENACC
lib/kokkos/cmake/KokkosCore_config.h.in:#cmakedefine KOKKOS_ENABLE_CUDA
lib/kokkos/cmake/KokkosCore_config.h.in:#cmakedefine KOKKOS_ENABLE_CUDA_RELOCATABLE_DEVICE_CODE
lib/kokkos/cmake/KokkosCore_config.h.in:#cmakedefine KOKKOS_ENABLE_CUDA_UVM
lib/kokkos/cmake/KokkosCore_config.h.in:#cmakedefine KOKKOS_ENABLE_CUDA_LAMBDA  // deprecated
lib/kokkos/cmake/KokkosCore_config.h.in:#cmakedefine KOKKOS_ENABLE_CUDA_CONSTEXPR
lib/kokkos/cmake/KokkosCore_config.h.in:#cmakedefine KOKKOS_ENABLE_IMPL_CUDA_MALLOC_ASYNC
lib/kokkos/cmake/KokkosCore_config.h.in:#cmakedefine KOKKOS_ENABLE_IMPL_CUDA_UNIFIED_MEMORY
lib/kokkos/cmake/KokkosCore_config.h.in:#cmakedefine KOKKOS_ARCH_INTEL_GPU
lib/kokkos/cmake/KokkosCore_config.h.in:#cmakedefine KOKKOS_ARCH_AMD_GPU
lib/kokkos/cmake/kokkos_enable_options.cmake:KOKKOS_ENABLE_OPTION(CUDA_RELOCATABLE_DEVICE_CODE  OFF "Whether to enable relocatable device code (RDC) for CUDA")
lib/kokkos/cmake/kokkos_enable_options.cmake:KOKKOS_ENABLE_OPTION(CUDA_UVM             OFF "Whether to use unified memory (UM) for CUDA by default")
lib/kokkos/cmake/kokkos_enable_options.cmake:KOKKOS_ENABLE_OPTION(CUDA_LDG_INTRINSIC   OFF "Whether to use CUDA LDG intrinsics")
lib/kokkos/cmake/kokkos_enable_options.cmake:# In contrast to other CUDA-dependent, options CUDA_LAMBDA is ON by default.
lib/kokkos/cmake/kokkos_enable_options.cmake:# That is problematic when CUDA is not enabled because this not only yields a
lib/kokkos/cmake/kokkos_enable_options.cmake:# bogus warning, but also exports the Kokkos_ENABLE_CUDA_LAMBDA variable and
lib/kokkos/cmake/kokkos_enable_options.cmake:IF (Trilinos_ENABLE_Kokkos AND TPL_ENABLE_CUDA)
lib/kokkos/cmake/kokkos_enable_options.cmake:   SET(CUDA_LAMBDA_DEFAULT ON)
lib/kokkos/cmake/kokkos_enable_options.cmake:ELSEIF (KOKKOS_ENABLE_CUDA)
lib/kokkos/cmake/kokkos_enable_options.cmake:   SET(CUDA_LAMBDA_DEFAULT ON)
lib/kokkos/cmake/kokkos_enable_options.cmake:   SET(CUDA_LAMBDA_DEFAULT OFF)
lib/kokkos/cmake/kokkos_enable_options.cmake:KOKKOS_ENABLE_OPTION(CUDA_LAMBDA ${CUDA_LAMBDA_DEFAULT} "Whether to allow lambda expressions on the device with NVCC **DEPRECATED**")
lib/kokkos/cmake/kokkos_enable_options.cmake:# May be used to disable our use of CudaMallocAsync.  It had caused issues in
lib/kokkos/cmake/kokkos_enable_options.cmake:KOKKOS_ENABLE_OPTION(IMPL_CUDA_MALLOC_ASYNC ON  "Whether to enable CudaMallocAsync (requires CUDA Toolkit 11.2)")
lib/kokkos/cmake/kokkos_enable_options.cmake:KOKKOS_ENABLE_OPTION(IMPL_NVHPC_AS_DEVICE_COMPILER OFF "Whether to allow nvc++ as Cuda device compiler")
lib/kokkos/cmake/kokkos_enable_options.cmake:KOKKOS_ENABLE_OPTION(IMPL_CUDA_UNIFIED_MEMORY OFF "Whether to leverage unified memory architectures for CUDA")
lib/kokkos/cmake/kokkos_enable_options.cmake:IF (KOKKOS_ENABLE_CUDA AND (KOKKOS_CXX_COMPILER_ID STREQUAL Clang))
lib/kokkos/cmake/kokkos_enable_options.cmake:  SET(CUDA_CONSTEXPR_DEFAULT ON)
lib/kokkos/cmake/kokkos_enable_options.cmake:  SET(CUDA_CONSTEXPR_DEFAULT OFF)
lib/kokkos/cmake/kokkos_enable_options.cmake:KOKKOS_ENABLE_OPTION(CUDA_CONSTEXPR ${CUDA_CONSTEXPR_DEFAULT} "Whether to activate experimental relaxed constexpr functions")
lib/kokkos/cmake/kokkos_enable_options.cmake:CHECK_DEVICE_SPECIFIC_OPTIONS(DEVICE CUDA OPTIONS CUDA_UVM CUDA_RELOCATABLE_DEVICE_CODE CUDA_LAMBDA CUDA_CONSTEXPR CUDA_LDG_INTRINSIC IMPL_CUDA_UNIFIED_MEMORY)
lib/kokkos/cmake/kokkos_enable_options.cmake:# Force consistency of KOKKOS_ENABLE_CUDA_RELOCATABLE_DEVICE_CODE
lib/kokkos/cmake/kokkos_enable_options.cmake:# and CMAKE_CUDA_SEPARABLE_COMPILATION when we are compiling
lib/kokkos/cmake/kokkos_enable_options.cmake:# using the CMake CUDA language support.
lib/kokkos/cmake/kokkos_enable_options.cmake:IF (KOKKOS_COMPILE_LANGUAGE STREQUAL CUDA)
lib/kokkos/cmake/kokkos_enable_options.cmake:  IF (KOKKOS_ENABLE_CUDA_RELOCATABLE_DEVICE_CODE)
lib/kokkos/cmake/kokkos_enable_options.cmake:    IF (NOT CMAKE_CUDA_SEPARABLE_COMPILATION)
lib/kokkos/cmake/kokkos_enable_options.cmake:      MESSAGE(STATUS "Setting CMAKE_CUDA_SEPARABLE_COMPILATION=ON since Kokkos_ENABLE_CUDA_RELOCATABLE_DEVICE_CODE is true. When compiling Kokkos with CMake language CUDA, please use CMAKE_CUDA_SEPARABLE_COMPILATION to control RDC support")
lib/kokkos/cmake/kokkos_enable_options.cmake:      SET(CMAKE_CUDA_SEPARABLE_COMPILATION ON)
lib/kokkos/cmake/kokkos_enable_options.cmake:    IF (CMAKE_CUDA_SEPARABLE_COMPILATION)
lib/kokkos/cmake/kokkos_enable_options.cmake:      SET(KOKKOS_ENABLE_CUDA_RELOCATABLE_DEVICE_CODE ON)
lib/kokkos/cmake/kokkos_enable_options.cmake:IF (KOKKOS_ENABLE_CUDA_RELOCATABLE_DEVICE_CODE AND KOKKOS_CXX_COMPILER_ID STREQUAL Clang)
lib/kokkos/cmake/kokkos_enable_options.cmake:IF (KOKKOS_ENABLE_CUDA_RELOCATABLE_DEVICE_CODE AND BUILD_SHARED_LIBS)
lib/kokkos/cmake/kokkos_enable_options.cmake:IF(Kokkos_ENABLE_CUDA_LDG_INTRINSIC)
lib/kokkos/cmake/kokkos_enable_options.cmake:    MESSAGE(DEPRECATION "Setting Kokkos_ENABLE_CUDA_LDG_INTRINSIC is deprecated. LDG intrinsics are always enabled.")
lib/kokkos/cmake/kokkos_enable_options.cmake:    MESSAGE(FATAL_ERROR "Kokkos_ENABLE_CUDA_LDG_INTRINSIC has been removed. LDG intrinsics are always enabled.")
lib/kokkos/cmake/kokkos_enable_options.cmake:IF(Kokkos_ENABLE_CUDA AND NOT Kokkos_ENABLE_CUDA_LAMBDA)
lib/kokkos/cmake/kokkos_enable_options.cmake:    MESSAGE(DEPRECATION "Setting Kokkos_ENABLE_CUDA_LAMBDA is deprecated. Lambda expressions in device code are always enabled. Forcing -DKokkos_ENABLE_CUDA_LAMBDA=ON")
lib/kokkos/cmake/kokkos_enable_options.cmake:    set(Kokkos_ENABLE_CUDA_LAMBDA ON CACHE BOOL "Kokkos turned Cuda lambda support ON!" FORCE)
lib/kokkos/cmake/kokkos_enable_options.cmake:    set(KOKKOS_ENABLE_CUDA_LAMBDA ON)
lib/kokkos/cmake/kokkos_enable_options.cmake:    MESSAGE(FATAL_ERROR "Kokkos_ENABLE_CUDA_LAMBDA has been removed. Lambda expressions in device code always enabled.")
lib/kokkos/cmake/kokkos_functions.cmake:  SET(COMPILERS NVIDIA NVHPC DEFAULT Cray Intel Clang AppleClang IntelLLVM GNU HIPCC Fujitsu MSVC)
lib/kokkos/cmake/kokkos_functions.cmake:# this function checks whether the current CXX compiler supports building CUDA
lib/kokkos/cmake/kokkos_functions.cmake:FUNCTION(kokkos_cxx_compiler_cuda_test _VAR)
lib/kokkos/cmake/kokkos_functions.cmake:    FILE(WRITE ${PROJECT_BINARY_DIR}/compile_tests/compiles_cuda.cpp
lib/kokkos/cmake/kokkos_functions.cmake:#include <cuda.h>
lib/kokkos/cmake/kokkos_functions.cmake:    auto ret = cudaMalloc(&data, blocks * grids * sizeof(double));
lib/kokkos/cmake/kokkos_functions.cmake:    if(ret != cudaSuccess)
lib/kokkos/cmake/kokkos_functions.cmake:    cudaDeviceSynchronize();
lib/kokkos/cmake/kokkos_functions.cmake:        SOURCES ${PROJECT_BINARY_DIR}/compile_tests/compiles_cuda.cpp)
lib/kokkos/cmake/kokkos_functions.cmake:    SET(${_VAR} ${_RET} CACHE STRING "CXX compiler supports building CUDA")
lib/kokkos/cmake/kokkos_functions.cmake:    # check whether the compiler already supports building CUDA
lib/kokkos/cmake/kokkos_functions.cmake:    KOKKOS_CXX_COMPILER_CUDA_TEST(Kokkos_CXX_COMPILER_COMPILES_CUDA)
lib/kokkos/cmake/kokkos_functions.cmake:    # if CUDA compile test has already been performed, just return
lib/kokkos/cmake/kokkos_functions.cmake:    IF(Kokkos_CXX_COMPILER_COMPILES_CUDA)
lib/kokkos/cmake/deps/CUDA.cmake:# Check for CUDA support
lib/kokkos/cmake/deps/CUDA.cmake:SET(_CUDA_FAILURE OFF)
lib/kokkos/cmake/deps/CUDA.cmake:# Have CMake find CUDA
lib/kokkos/cmake/deps/CUDA.cmake:IF(NOT _CUDA_FAILURE)
lib/kokkos/cmake/deps/CUDA.cmake:  FIND_PACKAGE(CUDA 3.2)
lib/kokkos/cmake/deps/CUDA.cmake:  IF (NOT CUDA_FOUND)
lib/kokkos/cmake/deps/CUDA.cmake:    SET(_CUDA_FAILURE ON)
lib/kokkos/cmake/deps/CUDA.cmake:IF(NOT _CUDA_FAILURE)
lib/kokkos/cmake/deps/CUDA.cmake:  macro(PACKAGE_ADD_CUDA_LIBRARY cuda_target)
lib/kokkos/cmake/deps/CUDA.cmake:    TRIBITS_ADD_LIBRARY(${cuda_target} ${ARGN} CUDALIBRARY)
lib/kokkos/cmake/deps/CUDA.cmake:  GLOBAL_SET(TPL_CUDA_LIBRARY_DIRS)
lib/kokkos/cmake/deps/CUDA.cmake:  GLOBAL_SET(TPL_CUDA_INCLUDE_DIRS ${CUDA_TOOLKIT_INCLUDE})
lib/kokkos/cmake/deps/CUDA.cmake:  GLOBAL_SET(TPL_CUDA_LIBRARIES ${CUDA_CUDART_LIBRARY} ${CUDA_cublas_LIBRARY} ${CUDA_cufft_LIBRARY})
lib/kokkos/cmake/deps/CUDA.cmake:  SET(TPL_ENABLE_CUDA OFF)
lib/kokkos/cmake/README.md:Include paths, C++ standard flags, architecture-specific optimizations, or OpenMP and CUDA flags are all examples of flags that Kokkos configures and adds to your project.
lib/kokkos/cmake/README.md:1. Additional paths not in the CMake default list or provided by the user that Kokkos decides to add. For example, Kokkos may query `nvcc` or `LD_LIBRARY_PATH` for where to find CUDA libraries.
lib/kokkos/cmake/kokkos_tribits.cmake:  IF (KOKKOS_ENABLE_CUDA)
lib/kokkos/cmake/kokkos_tribits.cmake:      PUBLIC $<$<COMPILE_LANGUAGE:${KOKKOS_COMPILE_LANGUAGE}>:${KOKKOS_CUDA_OPTIONS}>
lib/kokkos/cmake/kokkos_tribits.cmake:    SET(NODEDUP_CUDAFE_OPTIONS)
lib/kokkos/cmake/kokkos_tribits.cmake:    FOREACH(OPT ${KOKKOS_CUDAFE_OPTIONS})
lib/kokkos/cmake/kokkos_tribits.cmake:      LIST(APPEND NODEDUP_CUDAFE_OPTIONS -Xcudafe ${OPT})
lib/kokkos/cmake/kokkos_tribits.cmake:      PUBLIC $<$<COMPILE_LANGUAGE:${KOKKOS_COMPILE_LANGUAGE}>:${NODEDUP_CUDAFE_OPTIONS}>
lib/kokkos/cmake/kokkos_tribits.cmake:      PUBLIC $<$<COMPILE_LANGUAGE:${KOKKOS_COMPILE_LANGUAGE}>:${KOKKOS_AMDGPU_OPTIONS}>
lib/kokkos/cmake/KokkosConfig.cmake.in:        CHECK_CUDA_COMPILES)
lib/kokkos/cmake/KokkosConfig.cmake.in:ELSEIF(@Kokkos_ENABLE_CUDA@
lib/kokkos/cmake/KokkosConfig.cmake.in:    AND NOT @KOKKOS_COMPILE_LANGUAGE@ STREQUAL CUDA
lib/kokkos/cmake/KokkosConfig.cmake.in:    # if CUDA was enabled, the compilation language was not set to CUDA, and separable compilation was not
lib/kokkos/cmake/KokkosConfig.cmake.in:    # kokkos_launch_compiler will re-direct to the compiler used to compile CUDA code during installation.
lib/kokkos/cmake/kokkos_compiler_id.cmake:IF(Kokkos_ENABLE_CUDA)
lib/kokkos/cmake/kokkos_compiler_id.cmake:    kokkos_internal_have_compiler_nvcc(${CMAKE_CUDA_COMPILER})
lib/kokkos/cmake/kokkos_compiler_id.cmake:        MESSAGE(FATAL_ERROR "Cannot use CMAKE_CXX_COMPILER_LAUNCHER if the CMAKE_CXX_COMPILER is not able to compile CUDA code, i.e. nvcc_wrapper or clang++!")
lib/kokkos/cmake/kokkos_compiler_id.cmake:  SET(KOKKOS_CXX_COMPILER_ID NVIDIA CACHE STRING INTERNAL FORCE)
lib/kokkos/cmake/kokkos_compiler_id.cmake:  SET(KOKKOS_CLANG_CUDA_MINIMUM           10.0.0)
lib/kokkos/cmake/kokkos_compiler_id.cmake:  SET(KOKKOS_CLANG_CUDA_MINIMUM           14.0.0)
lib/kokkos/cmake/kokkos_compiler_id.cmake:SET(KOKKOS_MESSAGE_TEXT "${KOKKOS_MESSAGE_TEXT}\n    Clang(CUDA)         ${KOKKOS_CLANG_CUDA_MINIMUM}")
lib/kokkos/cmake/kokkos_compiler_id.cmake:IF(KOKKOS_CXX_COMPILER_ID STREQUAL Clang AND NOT Kokkos_ENABLE_CUDA)
lib/kokkos/cmake/kokkos_compiler_id.cmake:ELSEIF(KOKKOS_CXX_COMPILER_ID STREQUAL Clang AND Kokkos_ENABLE_CUDA)
lib/kokkos/cmake/kokkos_compiler_id.cmake:  IF(KOKKOS_CXX_COMPILER_VERSION VERSION_LESS ${KOKKOS_CLANG_CUDA_MINIMUM})
lib/kokkos/cmake/kokkos_compiler_id.cmake:ELSEIF(KOKKOS_CXX_COMPILER_ID STREQUAL NVIDIA)
lib/kokkos/cmake/KokkosConfigCommon.cmake.in:# backends, e.g. Kokkos_ENABLE_OPENMP, Kokkos_ENABLE_CUDA, Kokkos_ENABLE_HIP,
lib/kokkos/cmake/KokkosConfigCommon.cmake.in:# Kokkos_ENABLE_CUDA_RELOCATABLE_DEVICE_CODE, Kokkos_ENABLE_DEBUG, etc.
lib/kokkos/cmake/KokkosConfigCommon.cmake.in:IF(Kokkos_ENABLE_CUDA)
lib/kokkos/cmake/KokkosConfigCommon.cmake.in:  SET(Kokkos_CUDA_ARCHITECTURES @KOKKOS_CUDA_ARCHITECTURES@)
lib/kokkos/cmake/KokkosConfigCommon.cmake.in:IF (Kokkos_ENABLE_CUDA)
lib/kokkos/cmake/KokkosConfigCommon.cmake.in:  # If we are building CUDA, we have tricked CMake because we declare a CXX project
lib/kokkos/cmake/KokkosConfigCommon.cmake.in:  # This breaks CUDA compilation (CUDA compiler can have a different default
lib/kokkos/cmake/KokkosConfigCommon.cmake.in:#     [DEVICES <devices>...]   # Set of backends (e.g. "OpenMP" and/or "Cuda")
lib/kokkos/cmake/KokkosConfigCommon.cmake.in:# this is called only when Kokkos was installed with Kokkos_ENABLE_CUDA=ON
lib/kokkos/cmake/KokkosConfigCommon.cmake.in:# this function checks whether the current CXX compiler supports building CUDA
lib/kokkos/cmake/KokkosConfigCommon.cmake.in:FUNCTION(kokkos_cxx_compiler_cuda_test _VAR _COMPILER)
lib/kokkos/cmake/KokkosConfigCommon.cmake.in:    FILE(WRITE ${PROJECT_BINARY_DIR}/compile_tests/compiles_cuda.cu
lib/kokkos/cmake/KokkosConfigCommon.cmake.in:#include <cuda.h>
lib/kokkos/cmake/KokkosConfigCommon.cmake.in:    int ret = cudaMalloc(&data, blocks * grids * sizeof(double));
lib/kokkos/cmake/KokkosConfigCommon.cmake.in:    if(ret != cudaSuccess)
lib/kokkos/cmake/KokkosConfigCommon.cmake.in:    cudaDeviceSynchronize();
lib/kokkos/cmake/KokkosConfigCommon.cmake.in:    SET(_COMMANDS "${_COMPILER} ${ARGN} -c ${PROJECT_BINARY_DIR}/compile_tests/compiles_cuda.cu")
lib/kokkos/cmake/KokkosConfigCommon.cmake.in:    EXECUTE_PROCESS(COMMAND ${_COMPILER} ${ARGN} -c ${PROJECT_BINARY_DIR}/compile_tests/compiles_cuda.cu
lib/kokkos/cmake/KokkosConfigCommon.cmake.in:        SET(_COMMANDS "${_COMMAND}\n${_COMPILER} --cuda-gpu-arch=sm_35 ${ARGN} -c ${PROJECT_BINARY_DIR}/compile_tests/compiles_cuda.cu")
lib/kokkos/cmake/KokkosConfigCommon.cmake.in:        EXECUTE_PROCESS(COMMAND ${_COMPILER} --cuda-gpu-arch=sm_35 -c ${PROJECT_BINARY_DIR}/compile_tests/compiles_cuda.cu
lib/kokkos/cmake/KokkosConfigCommon.cmake.in:# set the values to ${Kokkos_CXX_COMPILER} unless Kokkos_ENABLE_CUDA=ON and
lib/kokkos/cmake/KokkosConfigCommon.cmake.in:# Kokkos_CXX_COMPILER_ID is NVIDIA, then it will set it to nvcc_wrapper
lib/kokkos/cmake/KokkosConfigCommon.cmake.in:# Use CHECK_CUDA_COMPILES to run a check when CUDA is enabled
lib/kokkos/cmake/KokkosConfigCommon.cmake.in:        "GLOBAL;PROJECT;CHECK_CUDA_COMPILES"
lib/kokkos/cmake/KokkosConfigCommon.cmake.in:    # if built w/o CUDA support, we want to basically make this a no-op
lib/kokkos/cmake/KokkosConfigCommon.cmake.in:    SET(_Kokkos_ENABLE_CUDA @Kokkos_ENABLE_CUDA@)
lib/kokkos/cmake/KokkosConfigCommon.cmake.in:        IF(_Kokkos_ENABLE_CUDA AND Kokkos_CXX_COMPILER_ID STREQUAL NVIDIA)
lib/kokkos/cmake/KokkosConfigCommon.cmake.in:    # try to ensure that compiling cuda code works!
lib/kokkos/cmake/KokkosConfigCommon.cmake.in:    IF(_Kokkos_ENABLE_CUDA AND COMP_CHECK_CUDA_COMPILES)
lib/kokkos/cmake/KokkosConfigCommon.cmake.in:        kokkos_cxx_compiler_cuda_test(_COMPILES_CUDA
lib/kokkos/cmake/KokkosConfigCommon.cmake.in:        IF(NOT _COMPILES_CUDA)
lib/kokkos/cmake/KokkosConfigCommon.cmake.in:            MESSAGE(FATAL_ERROR "kokkos_cxx_compiler_cuda_test failed! Test commands:\n${_COMPILES_CUDA_COMMANDS}")
lib/kokkos/cmake/kokkos_test_cxx_std.cmake:    IF (KOKKOS_CXX_COMPILER_ID STREQUAL NVIDIA AND (KOKKOS_CXX_HOST_COMPILER_ID STREQUAL GNU OR KOKKOS_CXX_HOST_COMPILER_ID STREQUAL Clang))
lib/kokkos/cmake/kokkos_test_cxx_std.cmake:  ELSEIF((KOKKOS_CXX_COMPILER_ID STREQUAL "NVIDIA") AND WIN32)
lib/kokkos/cmake/kokkos_test_cxx_std.cmake:# For compiling CUDA code using nvcc_wrapper, we will use the host compiler's
lib/kokkos/cmake/kokkos_test_cxx_std.cmake:# that we can only use host compilers for CUDA builds that use those flags.
lib/kokkos/cmake/kokkos_test_cxx_std.cmake:# It also means that extensions (gnu++17) can't be turned on for CUDA builds.
lib/kokkos/cmake/kokkos_test_cxx_std.cmake:IF(KOKKOS_CXX_COMPILER_ID STREQUAL NVIDIA)
lib/kokkos/cmake/kokkos_test_cxx_std.cmake:IF(KOKKOS_ENABLE_CUDA)
lib/kokkos/cmake/kokkos_test_cxx_std.cmake:  # ENFORCE that the compiler can compile CUDA code.
lib/kokkos/cmake/kokkos_test_cxx_std.cmake:      MESSAGE(FATAL_ERROR "Compiling CUDA code directly with Clang requires version 4.0.0 or higher.")
lib/kokkos/cmake/kokkos_test_cxx_std.cmake:      MESSAGE(FATAL_ERROR "Compiling CUDA code with clang doesn't support C++ extensions.  Set -DCMAKE_CXX_EXTENSIONS=OFF")
lib/kokkos/cmake/kokkos_test_cxx_std.cmake:  ELSEIF(NOT KOKKOS_CXX_COMPILER_ID STREQUAL NVIDIA)
lib/kokkos/cmake/kokkos_test_cxx_std.cmake:    MESSAGE(FATAL_ERROR "Invalid compiler for CUDA. The compiler must be nvcc_wrapper or Clang or use kokkos_launch_compiler, but compiler ID was ${KOKKOS_CXX_COMPILER_ID}")
lib/kokkos/cmake/kokkos_test_cxx_std.cmake:  ELSEIF((KOKKOS_CXX_COMPILER_ID STREQUAL "MSVC") OR ((KOKKOS_CXX_COMPILER_ID STREQUAL "NVIDIA") AND WIN32))
lib/kokkos/cmake/kokkos_tpls.cmake:KOKKOS_TPL_OPTION(CUDA    ${Kokkos_ENABLE_CUDA} TRIBITS CUDA)
lib/kokkos/cmake/kokkos_tpls.cmake:  SET(ROCM_DEFAULT ON)
lib/kokkos/cmake/kokkos_tpls.cmake:  SET(ROCM_DEFAULT OFF)
lib/kokkos/cmake/kokkos_tpls.cmake:KOKKOS_TPL_OPTION(ROCM    ${ROCM_DEFAULT})
lib/kokkos/cmake/kokkos_tpls.cmake:#Make sure we use our local FindKokkosCuda.cmake
lib/kokkos/cmake/kokkos_tpls.cmake:KOKKOS_IMPORT_TPL(CUDA INTERFACE)
lib/kokkos/cmake/kokkos_tpls.cmake:  KOKKOS_IMPORT_TPL(ROCM INTERFACE)
lib/kokkos/cmake/kokkos_tpls.cmake:    GLOBAL_APPEND(KOKKOS_AMDGPU_OPTIONS ${OpenMP_CXX_FLAGS})
lib/kokkos/cmake/kokkos_tpls.cmake:  IF(Kokkos_ENABLE_CUDA AND KOKKOS_COMPILE_LANGUAGE STREQUAL CUDA)
lib/kokkos/cmake/kokkos_tpls.cmake:    GLOBAL_APPEND(KOKKOS_CUDA_OPTIONS -Xcompiler ${OpenMP_CXX_FLAGS})
lib/kokkos/cmake/Dependencies.cmake:  LIB_OPTIONAL_TPLS Pthread CUDA HWLOC DLlib
lib/kokkos/cmake/Modules/FindTPLROCTHRUST.cmake:# ROCm 5.6 and earlier set AMDGPU_TARGETS and GPU_TARGETS to all the supported
lib/kokkos/cmake/Modules/FindTPLROCTHRUST.cmake:# architecture. Starting with ROCm 5.7 AMDGPU_TARGETS and GPU_TARGETS are empty.
lib/kokkos/cmake/Modules/FindTPLROCTHRUST.cmake:# behavior of ROCm 5.7 and later for earlier version of ROCm we set
lib/kokkos/cmake/Modules/FindTPLROCTHRUST.cmake:# AMDGPU_TARGETS and GPU_TARGETS to empty and set the values in the cache. If
lib/kokkos/cmake/Modules/FindTPLROCTHRUST.cmake:SET(AMDGPU_TARGETS "" CACHE STRING "AMD GPU targets to compile for")
lib/kokkos/cmake/Modules/FindTPLROCTHRUST.cmake:SET(GPU_TARGETS "" CACHE STRING "GPU targets to compile for")
lib/kokkos/cmake/Modules/FindTPLCUDA.cmake:IF (NOT CUDAToolkit_ROOT)
lib/kokkos/cmake/Modules/FindTPLCUDA.cmake:  IF (NOT CUDA_ROOT)
lib/kokkos/cmake/Modules/FindTPLCUDA.cmake:    SET(CUDA_ROOT $ENV{CUDA_ROOT})
lib/kokkos/cmake/Modules/FindTPLCUDA.cmake:  IF(CUDA_ROOT)
lib/kokkos/cmake/Modules/FindTPLCUDA.cmake:    SET(CUDAToolkit_ROOT ${CUDA_ROOT})
lib/kokkos/cmake/Modules/FindTPLCUDA.cmake:  find_package(CUDAToolkit REQUIRED)
lib/kokkos/cmake/Modules/FindTPLCUDA.cmake:  KOKKOS_CREATE_IMPORTED_TPL(CUDA INTERFACE
lib/kokkos/cmake/Modules/FindTPLCUDA.cmake:    LINK_LIBRARIES CUDA::cuda_driver CUDA::cudart
lib/kokkos/cmake/Modules/FindTPLCUDA.cmake:  KOKKOS_EXPORT_CMAKE_TPL(CUDAToolkit REQUIRED)
lib/kokkos/cmake/Modules/FindTPLCUDA.cmake:  include(${CMAKE_CURRENT_LIST_DIR}/CudaToolkit.cmake)
lib/kokkos/cmake/Modules/FindTPLCUDA.cmake:  IF (TARGET CUDA::cudart)
lib/kokkos/cmake/Modules/FindTPLCUDA.cmake:    SET(FOUND_CUDART TRUE)
lib/kokkos/cmake/Modules/FindTPLCUDA.cmake:    KOKKOS_EXPORT_IMPORTED_TPL(CUDA::cudart)
lib/kokkos/cmake/Modules/FindTPLCUDA.cmake:    SET(FOUND_CUDART FALSE)
lib/kokkos/cmake/Modules/FindTPLCUDA.cmake:  IF (TARGET CUDA::cuda_driver)
lib/kokkos/cmake/Modules/FindTPLCUDA.cmake:    SET(FOUND_CUDA_DRIVER TRUE)
lib/kokkos/cmake/Modules/FindTPLCUDA.cmake:    KOKKOS_EXPORT_IMPORTED_TPL(CUDA::cuda_driver)
lib/kokkos/cmake/Modules/FindTPLCUDA.cmake:    SET(FOUND_CUDA_DRIVER FALSE)
lib/kokkos/cmake/Modules/FindTPLCUDA.cmake:  FIND_PACKAGE_HANDLE_STANDARD_ARGS(TPLCUDA ${DEFAULT_MSG} FOUND_CUDART FOUND_CUDA_DRIVER)
lib/kokkos/cmake/Modules/FindTPLCUDA.cmake:  IF (FOUND_CUDA_DRIVER AND FOUND_CUDART)
lib/kokkos/cmake/Modules/FindTPLCUDA.cmake:    KOKKOS_CREATE_IMPORTED_TPL(CUDA INTERFACE
lib/kokkos/cmake/Modules/FindTPLCUDA.cmake:      LINK_LIBRARIES CUDA::cuda_driver CUDA::cudart
lib/kokkos/cmake/Modules/FindTPLROCM.cmake:FIND_LIBRARY(AMD_HIP_LIBRARY amdhip64 PATHS ENV ROCM_PATH PATH_SUFFIXES lib)
lib/kokkos/cmake/Modules/FindTPLROCM.cmake:FIND_LIBRARY(HSA_RUNTIME_LIBRARY hsa-runtime64 PATHS ENV ROCM_PATH PATH_SUFFIXES lib)
lib/kokkos/cmake/Modules/FindTPLROCM.cmake:# FIXME_HIP Starting with ROCm 5.5 it is not necessary to link againt clang_rt.
lib/kokkos/cmake/Modules/FindTPLROCM.cmake:# We keep the code as is for now because it is hard to find the version of ROCM
lib/kokkos/cmake/Modules/FindTPLROCM.cmake:# https://github.com/ROCm-Developer-Tools/hipamd/blob/d1e0ee98a0f3d79f7bf43295f82d0053a69ec742/hip-config.cmake.in#L241
lib/kokkos/cmake/Modules/FindTPLROCM.cmake:find_package_handle_standard_args(TPLROCM DEFAULT_MSG AMD_HIP_LIBRARY HSA_RUNTIME_LIBRARY CLANG_RT_LIBRARY)
lib/kokkos/cmake/Modules/FindTPLROCM.cmake:kokkos_create_imported_tpl(ROCM INTERFACE
lib/kokkos/cmake/Modules/CudaToolkit.cmake:FindCUDAToolkit
lib/kokkos/cmake/Modules/CudaToolkit.cmake:This script locates the NVIDIA CUDA toolkit and the associated libraries, but
lib/kokkos/cmake/Modules/CudaToolkit.cmake:does not require the ``CUDA`` language be enabled for a given project. This
lib/kokkos/cmake/Modules/CudaToolkit.cmake:module does not search for the NVIDIA CUDA Samples.
lib/kokkos/cmake/Modules/CudaToolkit.cmake:Finding the CUDA Toolkit requires finding the ``nvcc`` executable, which is
lib/kokkos/cmake/Modules/CudaToolkit.cmake:1. If the ``CUDA`` language has been enabled we will use the directory
lib/kokkos/cmake/Modules/CudaToolkit.cmake:2. If the ``CUDAToolkit_ROOT`` cmake configuration variable (e.g.,
lib/kokkos/cmake/Modules/CudaToolkit.cmake:   ``-DCUDAToolkit_ROOT=/some/path``) *or* environment variable is defined, it
lib/kokkos/cmake/Modules/CudaToolkit.cmake:   found underneath the directory specified by ``CUDAToolkit_ROOT``.  If
lib/kokkos/cmake/Modules/CudaToolkit.cmake:   ``CUDAToolkit_ROOT`` is specified, but no ``nvcc`` is found underneath, this
lib/kokkos/cmake/Modules/CudaToolkit.cmake:3. If the CUDA_PATH environment variable is defined, it will be searched.
lib/kokkos/cmake/Modules/CudaToolkit.cmake:   the desired path in the event that multiple CUDA Toolkits are installed.
lib/kokkos/cmake/Modules/CudaToolkit.cmake:5. On Unix systems, if the symbolic link ``/usr/local/cuda`` exists, this is
lib/kokkos/cmake/Modules/CudaToolkit.cmake:   candidate is found, this is used.  The default CUDA Toolkit install locations
lib/kokkos/cmake/Modules/CudaToolkit.cmake:   | macOS       | ``/Developer/NVIDIA/CUDA-X.Y``                              |
lib/kokkos/cmake/Modules/CudaToolkit.cmake:   | Other Unix  | ``/usr/local/cuda-X.Y``                                     |
lib/kokkos/cmake/Modules/CudaToolkit.cmake:   | Windows     | ``C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\vX.Y`` |
lib/kokkos/cmake/Modules/CudaToolkit.cmake:   Where ``X.Y`` would be a specific version of the CUDA Toolkit, such as
lib/kokkos/cmake/Modules/CudaToolkit.cmake:   ``/usr/local/cuda-9.0`` or
lib/kokkos/cmake/Modules/CudaToolkit.cmake:   ``C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:       When multiple CUDA Toolkits are installed in the default location of a
lib/kokkos/cmake/Modules/CudaToolkit.cmake:       system (e.g., both ``/usr/local/cuda-9.0`` and ``/usr/local/cuda-10.0``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:       exist but the ``/usr/local/cuda`` symbolic link does **not** exist), this
lib/kokkos/cmake/Modules/CudaToolkit.cmake:       the presence of multiple CUDA Toolkits being installed.  In this
lib/kokkos/cmake/Modules/CudaToolkit.cmake:       situation, users are encouraged to either (1) set ``CUDAToolkit_ROOT`` or
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    If specified, describes the version of the CUDA Toolkit to search for.
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    If specified, configuration will error if a suitable CUDA Toolkit is not
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    If specified, the search for a suitable CUDA Toolkit will not produce any
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    If specified, the CUDA Toolkit is considered found only if the exact
lib/kokkos/cmake/Modules/CudaToolkit.cmake:An :ref:`imported target <Imported targets>` named ``CUDA::toolkit`` is provided.
lib/kokkos/cmake/Modules/CudaToolkit.cmake:of the following libraries that are part of the CUDAToolkit:
lib/kokkos/cmake/Modules/CudaToolkit.cmake:- :ref:`CUDA Runtime Library<cuda_toolkit_rt_lib>`
lib/kokkos/cmake/Modules/CudaToolkit.cmake:- :ref:`CUDA Driver Library<cuda_toolkit_driver_lib>`
lib/kokkos/cmake/Modules/CudaToolkit.cmake:- :ref:`cuBLAS<cuda_toolkit_cuBLAS>`
lib/kokkos/cmake/Modules/CudaToolkit.cmake:- :ref:`cuFFT<cuda_toolkit_cuFFT>`
lib/kokkos/cmake/Modules/CudaToolkit.cmake:- :ref:`cuRAND<cuda_toolkit_cuRAND>`
lib/kokkos/cmake/Modules/CudaToolkit.cmake:- :ref:`cuSOLVER<cuda_toolkit_cuSOLVER>`
lib/kokkos/cmake/Modules/CudaToolkit.cmake:- :ref:`cuSPARSE<cuda_toolkit_cuSPARSE>`
lib/kokkos/cmake/Modules/CudaToolkit.cmake:- :ref:`cuPTI<cuda_toolkit_cupti>`
lib/kokkos/cmake/Modules/CudaToolkit.cmake:- :ref:`NPP<cuda_toolkit_NPP>`
lib/kokkos/cmake/Modules/CudaToolkit.cmake:- :ref:`nvBLAS<cuda_toolkit_nvBLAS>`
lib/kokkos/cmake/Modules/CudaToolkit.cmake:- :ref:`nvGRAPH<cuda_toolkit_nvGRAPH>`
lib/kokkos/cmake/Modules/CudaToolkit.cmake:- :ref:`nvJPEG<cuda_toolkit_nvJPEG>`
lib/kokkos/cmake/Modules/CudaToolkit.cmake:- :ref:`nvidia-ML<cuda_toolkit_nvML>`
lib/kokkos/cmake/Modules/CudaToolkit.cmake:- :ref:`nvRTC<cuda_toolkit_nvRTC>`
lib/kokkos/cmake/Modules/CudaToolkit.cmake:- :ref:`nvToolsExt<cuda_toolkit_nvToolsExt>`
lib/kokkos/cmake/Modules/CudaToolkit.cmake:- :ref:`OpenCL<cuda_toolkit_opencl>`
lib/kokkos/cmake/Modules/CudaToolkit.cmake:- :ref:`cuLIBOS<cuda_toolkit_cuLIBOS>`
lib/kokkos/cmake/Modules/CudaToolkit.cmake:.. _`cuda_toolkit_rt_lib`:
lib/kokkos/cmake/Modules/CudaToolkit.cmake:CUDA Runtime Library
lib/kokkos/cmake/Modules/CudaToolkit.cmake:The CUDA Runtime library (cudart) are what most applications will typically
lib/kokkos/cmake/Modules/CudaToolkit.cmake:need to link against to make any calls such as `cudaMalloc`, and `cudaFree`.
lib/kokkos/cmake/Modules/CudaToolkit.cmake:- ``CUDA::cudart``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:- ``CUDA::cudart_static``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:.. _`cuda_toolkit_driver_lib`:
lib/kokkos/cmake/Modules/CudaToolkit.cmake:CUDA Driver Library
lib/kokkos/cmake/Modules/CudaToolkit.cmake:The CUDA Driver library (cuda) are used by applications that use calls
lib/kokkos/cmake/Modules/CudaToolkit.cmake:- ``CUDA::cuda_driver``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:- ``CUDA::cuda_driver``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:.. _`cuda_toolkit_cuBLAS`:
lib/kokkos/cmake/Modules/CudaToolkit.cmake:The `cuBLAS <https://docs.nvidia.com/cuda/cublas/index.html>`_ library.
lib/kokkos/cmake/Modules/CudaToolkit.cmake:- ``CUDA::cublas``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:- ``CUDA::cublas_static``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:.. _`cuda_toolkit_cuFFT`:
lib/kokkos/cmake/Modules/CudaToolkit.cmake:The `cuFFT <https://docs.nvidia.com/cuda/cufft/index.html>`_ library.
lib/kokkos/cmake/Modules/CudaToolkit.cmake:- ``CUDA::cufft``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:- ``CUDA::cufftw``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:- ``CUDA::cufft_static``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:- ``CUDA::cufftw_static``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:The `cuRAND <https://docs.nvidia.com/cuda/curand/index.html>`_ library.
lib/kokkos/cmake/Modules/CudaToolkit.cmake:- ``CUDA::curand``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:- ``CUDA::curand_static``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:.. _`cuda_toolkit_cuSOLVER`:
lib/kokkos/cmake/Modules/CudaToolkit.cmake:The `cuSOLVER <https://docs.nvidia.com/cuda/cusolver/index.html>`_ library.
lib/kokkos/cmake/Modules/CudaToolkit.cmake:- ``CUDA::cusolver``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:- ``CUDA::cusolver_static``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:.. _`cuda_toolkit_cuSPARSE`:
lib/kokkos/cmake/Modules/CudaToolkit.cmake:The `cuSPARSE <https://docs.nvidia.com/cuda/cusparse/index.html>`_ library.
lib/kokkos/cmake/Modules/CudaToolkit.cmake:- ``CUDA::cusparse``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:- ``CUDA::cusparse_static``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:.. _`cuda_toolkit_cupti`:
lib/kokkos/cmake/Modules/CudaToolkit.cmake:The `NVIDIA CUDA Profiling Tools Interface <https://developer.nvidia.com/CUPTI>`_.
lib/kokkos/cmake/Modules/CudaToolkit.cmake:- ``CUDA::cupti``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:- ``CUDA::cupti_static``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:.. _`cuda_toolkit_NPP`:
lib/kokkos/cmake/Modules/CudaToolkit.cmake:The `NPP <https://docs.nvidia.com/cuda/npp/index.html>`_ libraries.
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  - ``CUDA::nppc``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  - ``CUDA::nppc_static``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  - ``CUDA::nppial``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  - ``CUDA::nppial_static``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  - ``CUDA::nppicc``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  - ``CUDA::nppicc_static``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  - ``CUDA::nppicom``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  - ``CUDA::nppicom_static``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  - ``CUDA::nppidei``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  - ``CUDA::nppidei_static``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  - ``CUDA::nppif``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  - ``CUDA::nppif_static``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  - ``CUDA::nppig``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  - ``CUDA::nppig_static``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  - ``CUDA::nppim``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  - ``CUDA::nppim_static``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  - ``CUDA::nppist``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  - ``CUDA::nppist_static``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  - ``CUDA::nppisu``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  - ``CUDA::nppisu_static``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  - ``CUDA::nppitc``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  - ``CUDA::nppitc_static``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  - ``CUDA::npps``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  - ``CUDA::npps_static``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:.. _`cuda_toolkit_nvBLAS`:
lib/kokkos/cmake/Modules/CudaToolkit.cmake:The `nvBLAS <https://docs.nvidia.com/cuda/nvblas/index.html>`_ libraries.
lib/kokkos/cmake/Modules/CudaToolkit.cmake:- ``CUDA::nvblas``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:.. _`cuda_toolkit_nvGRAPH`:
lib/kokkos/cmake/Modules/CudaToolkit.cmake:The `nvGRAPH <https://docs.nvidia.com/cuda/nvgraph/index.html>`_ library.
lib/kokkos/cmake/Modules/CudaToolkit.cmake:- ``CUDA::nvgraph``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:- ``CUDA::nvgraph_static``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:.. _`cuda_toolkit_nvJPEG`:
lib/kokkos/cmake/Modules/CudaToolkit.cmake:The `nvJPEG <https://docs.nvidia.com/cuda/nvjpeg/index.html>`_ library.
lib/kokkos/cmake/Modules/CudaToolkit.cmake:Introduced in CUDA 10.
lib/kokkos/cmake/Modules/CudaToolkit.cmake:- ``CUDA::nvjpeg``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:- ``CUDA::nvjpeg_static``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:.. _`cuda_toolkit_nvRTC`:
lib/kokkos/cmake/Modules/CudaToolkit.cmake:The `nvRTC <https://docs.nvidia.com/cuda/nvrtc/index.html>`_ (Runtime Compilation) library.
lib/kokkos/cmake/Modules/CudaToolkit.cmake:- ``CUDA::nvrtc``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:.. _`cuda_toolkit_nvml`:
lib/kokkos/cmake/Modules/CudaToolkit.cmake:nvidia-ML
lib/kokkos/cmake/Modules/CudaToolkit.cmake:The `NVIDIA Management Library <https://developer.nvidia.com/nvidia-management-library-nvml>`_.
lib/kokkos/cmake/Modules/CudaToolkit.cmake:- ``CUDA::nvml``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:.. _`cuda_toolkit_nvToolsExt`:
lib/kokkos/cmake/Modules/CudaToolkit.cmake:The `NVIDIA Tools Extension <https://docs.nvidia.com/gameworks/content/gameworkslibrary/nvtx/nvidia_tools_extension_library_nvtx.htm>`_.
lib/kokkos/cmake/Modules/CudaToolkit.cmake:- ``CUDA::nvToolsExt``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:.. _`cuda_toolkit_opencl`:
lib/kokkos/cmake/Modules/CudaToolkit.cmake:OpenCL
lib/kokkos/cmake/Modules/CudaToolkit.cmake:The `NVIDIA OpenCL Library <https://developer.nvidia.com/opencl>`_.
lib/kokkos/cmake/Modules/CudaToolkit.cmake:- ``CUDA::OpenCL``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:.. _`cuda_toolkit_cuLIBOS`:
lib/kokkos/cmake/Modules/CudaToolkit.cmake:static only.  The ``CUDA::cublas_static``, ``CUDA::cusparse_static``,
lib/kokkos/cmake/Modules/CudaToolkit.cmake:``CUDA::cufft_static``, ``CUDA::curand_static``, and (when implemented) NPP
lib/kokkos/cmake/Modules/CudaToolkit.cmake:- ``CUDA::culibos``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:.. _`cuda_toolkit_cuRAND`:
lib/kokkos/cmake/Modules/CudaToolkit.cmake:``CUDAToolkit_FOUND``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    A boolean specifying whether or not the CUDA Toolkit was found.
lib/kokkos/cmake/Modules/CudaToolkit.cmake:``CUDAToolkit_VERSION``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    The exact version of the CUDA Toolkit found (as reported by
lib/kokkos/cmake/Modules/CudaToolkit.cmake:``CUDAToolkit_VERSION_MAJOR``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    The major version of the CUDA Toolkit.
lib/kokkos/cmake/Modules/CudaToolkit.cmake:``CUDAToolkit_VERSION_MAJOR``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    The minor version of the CUDA Toolkit.
lib/kokkos/cmake/Modules/CudaToolkit.cmake:``CUDAToolkit_VERSION_PATCH``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    The patch version of the CUDA Toolkit.
lib/kokkos/cmake/Modules/CudaToolkit.cmake:``CUDAToolkit_BIN_DIR``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    The path to the CUDA Toolkit library directory that contains the CUDA
lib/kokkos/cmake/Modules/CudaToolkit.cmake:``CUDAToolkit_INCLUDE_DIRS``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    The path to the CUDA Toolkit ``include`` folder containing the header files
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    required to compile a project linking against CUDA.
lib/kokkos/cmake/Modules/CudaToolkit.cmake:``CUDAToolkit_LIBRARY_DIR``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    The path to the CUDA Toolkit library directory that contains the CUDA
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    Runtime library ``cudart``.
lib/kokkos/cmake/Modules/CudaToolkit.cmake:``CUDAToolkit_TARGET_DIR``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    The path to the CUDA Toolkit directory including the target architecture
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    ``CUDAToolkit_ROOT_DIR``.
lib/kokkos/cmake/Modules/CudaToolkit.cmake:``CUDAToolkit_NVCC_EXECUTABLE``
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    The path to the NVIDIA CUDA compiler ``nvcc``.  Note that this path may
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    :variable:`CMAKE_CUDA_COMPILER <CMAKE_<LANG>_COMPILER>`.  ``nvcc`` must be
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    found to determine the CUDA Toolkit version as well as determining other
lib/kokkos/cmake/Modules/CudaToolkit.cmake:# NOTE: much of this was simply extracted from FindCUDA.cmake.
lib/kokkos/cmake/Modules/CudaToolkit.cmake:#   James Bigler, NVIDIA Corp (nvidia.com - jbigler)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:#   Abe Stephens, SCI Institute -- http://www.sci.utah.edu/~abe/FindCuda.html
lib/kokkos/cmake/Modules/CudaToolkit.cmake:#   Copyright (c) 2008 - 2009 NVIDIA Corporation.  All rights reserved.
lib/kokkos/cmake/Modules/CudaToolkit.cmake:#   This code is licensed under the MIT License.  See the FindCUDA.cmake script
lib/kokkos/cmake/Modules/CudaToolkit.cmake:if(CMAKE_CUDA_COMPILER_LOADED AND NOT CUDAToolkit_BIN_DIR AND CMAKE_CUDA_COMPILER_ID STREQUAL "NVIDIA")
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  get_filename_component(cuda_dir "${CMAKE_CUDA_COMPILER}" DIRECTORY)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  set(CUDAToolkit_BIN_DIR "${cuda_dir}" CACHE PATH "")
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  mark_as_advanced(CUDAToolkit_BIN_DIR)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  unset(cuda_dir)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:if(CUDAToolkit_BIN_DIR)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  find_program(CUDAToolkit_NVCC_EXECUTABLE
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    PATHS ${CUDAToolkit_BIN_DIR}
lib/kokkos/cmake/Modules/CudaToolkit.cmake:# Search using CUDAToolkit_ROOT
lib/kokkos/cmake/Modules/CudaToolkit.cmake:find_program(CUDAToolkit_NVCC_EXECUTABLE
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  PATHS ENV CUDA_PATH
lib/kokkos/cmake/Modules/CudaToolkit.cmake:# If the user specified CUDAToolkit_ROOT but nvcc could not be found, this is an error.
lib/kokkos/cmake/Modules/CudaToolkit.cmake:if (NOT CUDAToolkit_NVCC_EXECUTABLE AND (DEFINED CUDAToolkit_ROOT OR DEFINED ENV{CUDAToolkit_ROOT}))
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  set(cuda_root_fail "${fail_base} CUDAToolkit_ROOT=${CUDAToolkit_ROOT}")
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  set(env_cuda_root_fail "${fail_base} environment variable CUDAToolkit_ROOT=$ENV{CUDAToolkit_ROOT}")
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  if (CUDAToolkit_FIND_REQUIRED)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    if (DEFINED CUDAToolkit_ROOT)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:      message(FATAL_ERROR ${cuda_root_fail})
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    elseif (DEFINED ENV{CUDAToolkit_ROOT})
lib/kokkos/cmake/Modules/CudaToolkit.cmake:      message(FATAL_ERROR ${env_cuda_root_fail})
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    if (NOT CUDAToolkit_FIND_QUIETLY)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:      if (DEFINED CUDAToolkit_ROOT)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:        message(STATUS ${cuda_root_fail})
lib/kokkos/cmake/Modules/CudaToolkit.cmake:      elseif (DEFINED ENV{CUDAToolkit_ROOT})
lib/kokkos/cmake/Modules/CudaToolkit.cmake:        message(STATUS ${env_cuda_root_fail})
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    set(CUDAToolkit_FOUND FALSE)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    unset(cuda_root_fail)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    unset(env_cuda_root_fail)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:# CUDAToolkit_ROOT cmake / env variable not specified, try platform defaults.
lib/kokkos/cmake/Modules/CudaToolkit.cmake:# - Linux: /usr/local/cuda-X.Y
lib/kokkos/cmake/Modules/CudaToolkit.cmake:# - macOS: /Developer/NVIDIA/CUDA-X.Y
lib/kokkos/cmake/Modules/CudaToolkit.cmake:# - Windows: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\vX.Y
lib/kokkos/cmake/Modules/CudaToolkit.cmake:# We will also search the default symlink location /usr/local/cuda first since
lib/kokkos/cmake/Modules/CudaToolkit.cmake:# if CUDAToolkit_ROOT is not specified, it is assumed that the symlinked
lib/kokkos/cmake/Modules/CudaToolkit.cmake:if (NOT CUDAToolkit_NVCC_EXECUTABLE)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:      set(platform_base "/usr/local/cuda-")
lib/kokkos/cmake/Modules/CudaToolkit.cmake:      set(platform_base "/Developer/NVIDIA/CUDA-")
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    set(platform_base "C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v")
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  # Build out a descending list of possible cuda installations, e.g.
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  # every possible version of CUDA installed, this wouldn't create any
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  # Force the global default /usr/local/cuda to the front on Unix.
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    list(INSERT search_paths 0 "/usr/local/cuda")
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  find_program(CUDAToolkit_NVCC_EXECUTABLE
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  if (NOT CUDAToolkit_NVCC_EXECUTABLE)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    if (CUDAToolkit_FIND_REQUIRED)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:      message(FATAL_ERROR "Could not find nvcc, please set CUDAToolkit_ROOT.")
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    elseif(NOT CUDAToolkit_FIND_QUIETLY)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:      message(STATUS "Could not find nvcc, please set CUDAToolkit_ROOT.")
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    set(CUDAToolkit_FOUND FALSE)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:if(NOT CUDAToolkit_BIN_DIR AND CUDAToolkit_NVCC_EXECUTABLE)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  get_filename_component(cuda_dir "${CUDAToolkit_NVCC_EXECUTABLE}" DIRECTORY)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  set(CUDAToolkit_BIN_DIR "${cuda_dir}" CACHE PATH "" FORCE)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  mark_as_advanced(CUDAToolkit_BIN_DIR)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  unset(cuda_dir)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:if(CUDAToolkit_NVCC_EXECUTABLE AND
lib/kokkos/cmake/Modules/CudaToolkit.cmake:   CUDAToolkit_NVCC_EXECUTABLE STREQUAL CMAKE_CUDA_COMPILER)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  # Need to set these based off the already computed CMAKE_CUDA_COMPILER_VERSION value
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  if(CMAKE_CUDA_COMPILER_VERSION MATCHES [=[([0-9]+)\.([0-9]+)\.([0-9]+)]=])
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    set(CUDAToolkit_VERSION_MAJOR "${CMAKE_MATCH_1}")
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    set(CUDAToolkit_VERSION_MINOR "${CMAKE_MATCH_2}")
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    set(CUDAToolkit_VERSION_PATCH "${CMAKE_MATCH_3}")
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    set(CUDAToolkit_VERSION "${CMAKE_CUDA_COMPILER_VERSION}")
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  execute_process (COMMAND ${CUDAToolkit_NVCC_EXECUTABLE} "--version" OUTPUT_VARIABLE NVCC_OUT)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    set(CUDAToolkit_VERSION_MAJOR "${CMAKE_MATCH_1}")
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    set(CUDAToolkit_VERSION_MINOR "${CMAKE_MATCH_2}")
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    set(CUDAToolkit_VERSION_PATCH "${CMAKE_MATCH_3}")
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    set(CUDAToolkit_VERSION  "${CMAKE_MATCH_1}.${CMAKE_MATCH_2}.${CMAKE_MATCH_3}")
lib/kokkos/cmake/Modules/CudaToolkit.cmake:get_filename_component(CUDAToolkit_ROOT_DIR ${CUDAToolkit_BIN_DIR} DIRECTORY ABSOLUTE)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    set (CUDAToolkit_TARGET_NAME "armv7-linux-androideabi")
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    set(CUDAToolkit_TARGET_NAME "armv7-linux-gnueabihf")
lib/kokkos/cmake/Modules/CudaToolkit.cmake:      set(CUDAToolkit_TARGET_NAME "aarch64-linux-androideabi")
lib/kokkos/cmake/Modules/CudaToolkit.cmake:      set(CUDAToolkit_TARGET_NAME "aarch64-linux")
lib/kokkos/cmake/Modules/CudaToolkit.cmake:      set(CUDAToolkit_TARGET_NAME "x86_64-linux")
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  if (EXISTS "${CUDAToolkit_ROOT_DIR}/targets/${CUDAToolkit_TARGET_NAME}")
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    set(CUDAToolkit_TARGET_DIR "${CUDAToolkit_ROOT_DIR}/targets/${CUDAToolkit_TARGET_NAME}")
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    # add known CUDA target root path to the set of directories we search for programs, libraries and headers
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    list(PREPEND CMAKE_FIND_ROOT_PATH "${CUDAToolkit_TARGET_DIR}")
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    # found all cuda libraries so that searches for our cross-compilation
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    # libraries work when another cuda sdk is in CMAKE_PREFIX_PATH or
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    set(_CUDAToolkit_Pop_ROOT_PATH True)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  set(CUDAToolkit_TARGET_DIR "${CUDAToolkit_ROOT_DIR}")
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  list(APPEND CMAKE_PREFIX_PATH ${CUDAToolkit_ROOT_DIR})
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  # found the cudart library.
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  set(_CUDAToolkit_Pop_Prefix True)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:find_path(CUDAToolkit_INCLUDE_DIR
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  NAMES cuda_runtime.h
lib/kokkos/cmake/Modules/CudaToolkit.cmake:# And find the CUDA Runtime Library libcudart
lib/kokkos/cmake/Modules/CudaToolkit.cmake:find_library(CUDA_CUDART
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  NAMES cudart
lib/kokkos/cmake/Modules/CudaToolkit.cmake:if (NOT CUDA_CUDART)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  find_library(CUDA_CUDART
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    NAMES cudart
lib/kokkos/cmake/Modules/CudaToolkit.cmake:if (NOT CUDA_CUDART AND NOT CUDAToolkit_FIND_QUIETLY)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  message(STATUS "Unable to find cudart library.")
lib/kokkos/cmake/Modules/CudaToolkit.cmake:unset(CUDAToolkit_ROOT_DIR)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:if(_CUDAToolkit_Pop_Prefix)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  unset(_CUDAToolkit_Pop_Prefix)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:find_package_handle_standard_args(CUDAToolkit
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    CUDAToolkit_INCLUDE_DIR
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    CUDA_CUDART
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    CUDAToolkit_NVCC_EXECUTABLE
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    CUDAToolkit_VERSION
lib/kokkos/cmake/Modules/CudaToolkit.cmake:mark_as_advanced(CUDA_CUDART
lib/kokkos/cmake/Modules/CudaToolkit.cmake:                 CUDAToolkit_INCLUDE_DIR
lib/kokkos/cmake/Modules/CudaToolkit.cmake:                 CUDAToolkit_NVCC_EXECUTABLE
lib/kokkos/cmake/Modules/CudaToolkit.cmake:if(CUDAToolkit_FOUND)
lib/kokkos/cmake/Modules/CudaToolkit.cmake: set(CUDAToolkit_INCLUDE_DIRS ${CUDAToolkit_INCLUDE_DIR})
lib/kokkos/cmake/Modules/CudaToolkit.cmake: get_filename_component(CUDAToolkit_LIBRARY_DIR ${CUDA_CUDART} DIRECTORY ABSOLUTE)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:if(CUDAToolkit_FOUND)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  function(_CUDAToolkit_find_and_add_import_lib lib_name)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    find_library(CUDA_${lib_name}_LIBRARY
lib/kokkos/cmake/Modules/CudaToolkit.cmake:      HINTS ${CUDAToolkit_LIBRARY_DIR}
lib/kokkos/cmake/Modules/CudaToolkit.cmake:            ENV CUDA_PATH
lib/kokkos/cmake/Modules/CudaToolkit.cmake:      PATH_SUFFIXES nvidia/current lib64 lib/x64 lib
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    if(NOT CUDA_${lib_name}_LIBRARY)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:      find_library(CUDA_${lib_name}_LIBRARY
lib/kokkos/cmake/Modules/CudaToolkit.cmake:        HINTS ${CUDAToolkit_LIBRARY_DIR}
lib/kokkos/cmake/Modules/CudaToolkit.cmake:              ENV CUDA_PATH
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    mark_as_advanced(CUDA_${lib_name}_LIBRARY)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    if (NOT TARGET CUDA::${lib_name} AND CUDA_${lib_name}_LIBRARY)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:      add_library(CUDA::${lib_name} IMPORTED INTERFACE)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:      target_include_directories(CUDA::${lib_name} SYSTEM INTERFACE "${CUDAToolkit_INCLUDE_DIRS}")
lib/kokkos/cmake/Modules/CudaToolkit.cmake:      target_link_libraries(CUDA::${lib_name} INTERFACE "${CUDA_${lib_name}_LIBRARY}")
lib/kokkos/cmake/Modules/CudaToolkit.cmake:        if(TARGET CUDA::${dep})
lib/kokkos/cmake/Modules/CudaToolkit.cmake:          target_link_libraries(CUDA::${lib_name} INTERFACE CUDA::${dep})
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  if(NOT TARGET CUDA::toolkit)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    add_library(CUDA::toolkit IMPORTED INTERFACE)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    target_include_directories(CUDA::toolkit SYSTEM INTERFACE "${CUDAToolkit_INCLUDE_DIRS}")
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    target_link_directories(CUDA::toolkit INTERFACE "${CUDAToolkit_LIBRARY_DIR}")
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  _CUDAToolkit_find_and_add_import_lib(cuda_driver ALT cuda)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  _CUDAToolkit_find_and_add_import_lib(cudart)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  _CUDAToolkit_find_and_add_import_lib(cudart_static)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  # setup dependencies that are required for cudart_static when building
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  # on linux. These are generally only required when using the CUDA toolkit
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  # when CUDA language is disabled
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  if(NOT TARGET CUDA::cudart_static_deps
lib/kokkos/cmake/Modules/CudaToolkit.cmake:     AND TARGET CUDA::cudart_static)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    add_library(CUDA::cudart_static_deps IMPORTED INTERFACE)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    target_link_libraries(CUDA::cudart_static INTERFACE CUDA::cudart_static_deps)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:      target_link_libraries(CUDA::cudart_static_deps INTERFACE Threads::Threads ${CMAKE_DL_LIBS})
lib/kokkos/cmake/Modules/CudaToolkit.cmake:      # On Linux, you must link against librt when using the static cuda runtime.
lib/kokkos/cmake/Modules/CudaToolkit.cmake:      find_library(CUDAToolkit_rt_LIBRARY rt)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:      mark_as_advanced(CUDAToolkit_rt_LIBRARY)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:      if(NOT CUDAToolkit_rt_LIBRARY)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:        message(WARNING "Could not find librt library, needed by CUDA::cudart_static")
lib/kokkos/cmake/Modules/CudaToolkit.cmake:        target_link_libraries(CUDA::cudart_static_deps INTERFACE ${CUDAToolkit_rt_LIBRARY})
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  _CUDAToolkit_find_and_add_import_lib(culibos) # it's a static library
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  foreach (cuda_lib cublas cufft curand cusparse nppc nvjpeg)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    _CUDAToolkit_find_and_add_import_lib(${cuda_lib})
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    _CUDAToolkit_find_and_add_import_lib(${cuda_lib}_static DEPS culibos)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  _CUDAToolkit_find_and_add_import_lib(cufftw DEPS cufft)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  _CUDAToolkit_find_and_add_import_lib(cufftw DEPS cufft_static)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  _CUDAToolkit_find_and_add_import_lib(cusolver DEPS cublas cusparse)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  _CUDAToolkit_find_and_add_import_lib(cusolver_static DEPS cublas_static cusparse_static culibos)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  _CUDAToolkit_find_and_add_import_lib(nvgraph DEPS curand cusolver)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  _CUDAToolkit_find_and_add_import_lib(nvgraph_static DEPS curand_static cusolver_static)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  foreach (cuda_lib nppial nppicc nppidei nppif nppig nppim nppist nppitc npps nppicom nppisu)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    _CUDAToolkit_find_and_add_import_lib(${cuda_lib} DEPS nppc)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    _CUDAToolkit_find_and_add_import_lib(${cuda_lib}_static DEPS nppc_static)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  _CUDAToolkit_find_and_add_import_lib(cupti
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  _CUDAToolkit_find_and_add_import_lib(cupti_static
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  _CUDAToolkit_find_and_add_import_lib(nvrtc DEPS cuda_driver)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  _CUDAToolkit_find_and_add_import_lib(nvml ALT nvidia-ml nvml)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    # nvtools can be installed outside the CUDA toolkit directory
lib/kokkos/cmake/Modules/CudaToolkit.cmake:    find_library(CUDA_nvToolsExt_LIBRARY
lib/kokkos/cmake/Modules/CudaToolkit.cmake:            ENV CUDA_PATH
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  _CUDAToolkit_find_and_add_import_lib(nvToolsExt ALT nvToolsExt64)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  _CUDAToolkit_find_and_add_import_lib(OpenCL)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:if(_CUDAToolkit_Pop_ROOT_PATH)
lib/kokkos/cmake/Modules/CudaToolkit.cmake:  unset(_CUDAToolkit_Pop_ROOT_PATH)
lib/kokkos/cmake/kokkos_enable_devices.cmake:# Without this we can't make e.g. CudaSpace accessible by HostSpace
lib/kokkos/cmake/kokkos_enable_devices.cmake:KOKKOS_DEVICE_OPTION(OPENACC OFF DEVICE "Whether to build the OpenACC backend")
lib/kokkos/cmake/kokkos_enable_devices.cmake:IF (KOKKOS_ENABLE_OPENACC)
lib/kokkos/cmake/kokkos_enable_devices.cmake:    Clang -fopenacc -fopenacc-fake-async-wait
lib/kokkos/cmake/kokkos_enable_devices.cmake:          -Wno-openacc-and-cxx -Wno-openmp-mapping -Wno-unknown-cuda-version
lib/kokkos/cmake/kokkos_enable_devices.cmake:    NVHPC      -mp=gpu
lib/kokkos/cmake/kokkos_enable_devices.cmake:IF(Trilinos_ENABLE_Kokkos AND TPL_ENABLE_CUDA)
lib/kokkos/cmake/kokkos_enable_devices.cmake:  SET(CUDA_DEFAULT ON)
lib/kokkos/cmake/kokkos_enable_devices.cmake:  SET(CUDA_DEFAULT OFF)
lib/kokkos/cmake/kokkos_enable_devices.cmake:KOKKOS_DEVICE_OPTION(CUDA ${CUDA_DEFAULT} DEVICE "Whether to build CUDA backend")
lib/kokkos/cmake/kokkos_enable_devices.cmake:IF (KOKKOS_ENABLE_CUDA)
lib/kokkos/cmake/kokkos_enable_devices.cmake:  GLOBAL_SET(KOKKOS_DONT_ALLOW_EXTENSIONS "CUDA enabled")
lib/kokkos/cmake/kokkos_enable_devices.cmake:## Cuda has extra setup requirements, turn on Kokkos_Setup_Cuda.hpp in macros
lib/kokkos/cmake/kokkos_enable_devices.cmake:  LIST(APPEND DEVICE_SETUP_LIST Cuda)
lib/kokkos/containers/performance_tests/TestCuda.cpp:  std::cout << "Cuda" << std::endl;
lib/kokkos/containers/performance_tests/TestCuda.cpp:  test_dynrankview_op_perf<Kokkos::Cuda>(40960);
lib/kokkos/containers/performance_tests/TestCuda.cpp:  std::cout << "Cuda" << std::endl;
lib/kokkos/containers/performance_tests/TestCuda.cpp:    test_global_to_local_ids<Kokkos::Cuda>(i);
lib/kokkos/containers/performance_tests/TestCuda.cpp:  Perf::run_performance_tests<Kokkos::Cuda, true>("cuda-near");
lib/kokkos/containers/performance_tests/TestCuda.cpp:  Perf::run_performance_tests<Kokkos::Cuda, false>("cuda-far");
lib/kokkos/containers/performance_tests/Makefile:ifneq (,$(findstring Cuda,$(KOKKOS_DEVICES)))
lib/kokkos/containers/performance_tests/Makefile:ifeq ($(KOKKOS_INTERNAL_USE_CUDA), 1)
lib/kokkos/containers/performance_tests/Makefile:	OBJ_CUDA = TestCuda.o TestMain.o gtest-all.o
lib/kokkos/containers/performance_tests/Makefile:	TARGETS += KokkosContainers_PerformanceTest_Cuda
lib/kokkos/containers/performance_tests/Makefile:	TEST_TARGETS += test-cuda
lib/kokkos/containers/performance_tests/Makefile:KokkosContainers_PerformanceTest_Cuda: $(OBJ_CUDA) $(KOKKOS_LINK_DEPENDS)
lib/kokkos/containers/performance_tests/Makefile:	$(LINK) $(KOKKOS_LDFLAGS) $(LDFLAGS) $(EXTRA_PATH) $(OBJ_CUDA) $(KOKKOS_LIBS) $(LIB) -o KokkosContainers_PerformanceTest_Cuda
lib/kokkos/containers/performance_tests/Makefile:test-cuda: KokkosContainers_PerformanceTest_Cuda
lib/kokkos/containers/performance_tests/Makefile:	./KokkosContainers_PerformanceTest_Cuda
lib/kokkos/containers/performance_tests/CMakeLists.txt:foreach(Tag Threads;OpenMP;Cuda;HPX;HIP)
lib/kokkos/containers/CMakeLists.txt:# FIXME_OPENACC: temporarily disabled due to unimplemented features
lib/kokkos/containers/CMakeLists.txt:IF(NOT KOKKOS_ENABLE_OPENACC)
lib/kokkos/containers/unit_tests/TestViewCtorPropEmbeddedDim.hpp:  // Cuda 7.0 has issues with using a lambda in parallel_for to initialize the
lib/kokkos/containers/unit_tests/Makefile:vpath %.cpp ${KOKKOS_PATH}/containers/unit_tests/cuda
lib/kokkos/containers/unit_tests/Makefile:ifneq (,$(findstring Cuda,$(KOKKOS_DEVICES)))
lib/kokkos/containers/unit_tests/Makefile:ifeq ($(KOKKOS_INTERNAL_USE_CUDA), 1)
lib/kokkos/containers/unit_tests/Makefile:	OBJ_CUDA = UnitTestMain.o gtest-all.o
lib/kokkos/containers/unit_tests/Makefile:	OBJ_CUDA += TestCuda_Bitset.o
lib/kokkos/containers/unit_tests/Makefile:	OBJ_CUDA += TestCuda_DualView.o
lib/kokkos/containers/unit_tests/Makefile:	OBJ_CUDA += TestCuda_DynamicView.o
lib/kokkos/containers/unit_tests/Makefile:	OBJ_CUDA += TestCuda_DynViewAPI_generic.o
lib/kokkos/containers/unit_tests/Makefile:	OBJ_CUDA += TestCuda_DynViewAPI_rank12345.o
lib/kokkos/containers/unit_tests/Makefile:	OBJ_CUDA += TestCuda_DynViewAPI_rank67.o
lib/kokkos/containers/unit_tests/Makefile:	OBJ_CUDA += TestCuda_ErrorReporter.o
lib/kokkos/containers/unit_tests/Makefile:	OBJ_CUDA += TestCuda_OffsetView.o
lib/kokkos/containers/unit_tests/Makefile:	OBJ_CUDA += TestCuda_ScatterView.o
lib/kokkos/containers/unit_tests/Makefile:	OBJ_CUDA += TestCuda_StaticCrsGraph.o
lib/kokkos/containers/unit_tests/Makefile:	OBJ_CUDA += TestCuda_UnorderedMap.o
lib/kokkos/containers/unit_tests/Makefile:	OBJ_CUDA += TestCuda_ViewCtorPropEmbeddedDim.o
lib/kokkos/containers/unit_tests/Makefile:	TARGETS += KokkosContainers_UnitTest_Cuda
lib/kokkos/containers/unit_tests/Makefile:	TEST_TARGETS += test-cuda
lib/kokkos/containers/unit_tests/Makefile:KokkosContainers_UnitTest_Cuda: $(OBJ_CUDA) $(KOKKOS_LINK_DEPENDS)
lib/kokkos/containers/unit_tests/Makefile:	$(LINK) $(EXTRA_PATH) $(OBJ_CUDA) $(KOKKOS_LIBS) $(LIB) $(KOKKOS_LDFLAGS) $(LDFLAGS) -o KokkosContainers_UnitTest_Cuda
lib/kokkos/containers/unit_tests/Makefile:test-cuda: KokkosContainers_UnitTest_Cuda
lib/kokkos/containers/unit_tests/Makefile:	./KokkosContainers_UnitTest_Cuda
lib/kokkos/containers/unit_tests/TestDynViewAPI.hpp:    // For CUDA the constant random access View does not return
lib/kokkos/containers/unit_tests/TestDynViewAPI.hpp:#if defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/containers/unit_tests/TestDynViewAPI.hpp:    if (!std::is_same<typename device::execution_space, Kokkos::Cuda>::value)
lib/kokkos/containers/unit_tests/TestDynViewAPI.hpp:#if (!defined(KOKKOS_ENABLE_CUDA) || defined(KOKKOS_ENABLE_CUDA_UVM)) && \
lib/kokkos/containers/unit_tests/TestOffsetView.hpp:#if defined(KOKKOS_ENABLE_CUDA_LAMBDA) || !defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/containers/unit_tests/TestOffsetView.hpp:#if defined(KOKKOS_ENABLE_CUDA_LAMBDA) || !defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/containers/unit_tests/TestOffsetView.hpp:#if defined(KOKKOS_ENABLE_CUDA_LAMBDA) || !defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/containers/unit_tests/TestOffsetView.hpp:#if defined(KOKKOS_ENABLE_CUDA_LAMBDA) || !defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/containers/unit_tests/TestOffsetView.hpp:#if defined(KOKKOS_ENABLE_CUDA_LAMBDA) || !defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/containers/unit_tests/TestOffsetView.hpp:#if defined(KOKKOS_ENABLE_CUDA_LAMBDA) || !defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/containers/unit_tests/TestOffsetView.hpp:#if defined(KOKKOS_ENABLE_CUDA_LAMBDA) || !defined(KOKKOS_ENABLE_CUDA)
lib/kokkos/containers/unit_tests/CMakeLists.txt:foreach(Tag Threads;Serial;OpenMP;HPX;Cuda;HIP;SYCL)
lib/kokkos/containers/unit_tests/CMakeLists.txt:    if(KOKKOS_ENABLE_CUDA AND WIN32)
lib/kokkos/containers/unit_tests/CMakeLists.txt:     LIST(REMOVE_ITEM UnitTestSources ${dir}/TestCuda_DynViewAPI_generic.cpp)
lib/kokkos/containers/unit_tests/CMakeLists.txt:    if(KOKKOS_ENABLE_CUDA AND KOKKOS_CXX_COMPILER_ID STREQUAL NVHPC)
lib/kokkos/containers/unit_tests/CMakeLists.txt:     LIST(REMOVE_ITEM UnitTestSources ${dir}/TestCuda_WithoutInitializing.cpp)
lib/kokkos/containers/unit_tests/TestDualView.hpp:#if defined(KOKKOS_ENABLE_CUDA) || defined(KOKKOS_ENABLE_SYCL) || \
lib/kokkos/containers/unit_tests/TestDualView.hpp:#ifdef KOKKOS_ENABLE_CUDA  // specific to CUDA
lib/kokkos/containers/unit_tests/TestDualView.hpp:struct UVMSpaceFor<Kokkos::Cuda> {
lib/kokkos/containers/unit_tests/TestDualView.hpp:  using type = Kokkos::CudaUVMSpace;
lib/kokkos/containers/unit_tests/TestUnorderedMap.hpp:#if !defined(KOKKOS_ENABLE_CUDA) || \
lib/kokkos/containers/unit_tests/TestUnorderedMap.hpp:    (defined(KOKKOS_ENABLE_CUDA) && defined(KOKKOS_ENABLE_CUDA_LAMBDA))
lib/kokkos/containers/unit_tests/TestWithoutInitializing.hpp:/// Some tests are skipped for @c CudaUVM memory space.
lib/kokkos/containers/unit_tests/TestWithoutInitializing.hpp:/// @todo To be revised according to the future of @c KOKKOS_ENABLE_CUDA_UVM.
lib/kokkos/containers/unit_tests/TestWithoutInitializing.hpp:#ifdef KOKKOS_ENABLE_CUDA
lib/kokkos/containers/unit_tests/TestWithoutInitializing.hpp:#define GTEST_SKIP_IF_CUDAUVM_MEMORY_SPACE                            \
lib/kokkos/containers/unit_tests/TestWithoutInitializing.hpp:                               Kokkos::CudaUVMSpace>)                 \
lib/kokkos/containers/unit_tests/TestWithoutInitializing.hpp:    GTEST_SKIP() << "skipping since CudaUVMSpace requires additional fences";
lib/kokkos/containers/unit_tests/TestWithoutInitializing.hpp:#define GTEST_SKIP_IF_CUDAUVM_MEMORY_SPACE
lib/kokkos/containers/unit_tests/TestWithoutInitializing.hpp:#if defined(KOKKOS_ENABLE_IMPL_CUDA_UNIFIED_MEMORY)
lib/kokkos/containers/unit_tests/TestWithoutInitializing.hpp:                               Kokkos::CudaSpace>)                       \
lib/kokkos/containers/unit_tests/TestWithoutInitializing.hpp:  GTEST_SKIP_IF_CUDAUVM_MEMORY_SPACE
lib/kokkos/containers/unit_tests/TestWithoutInitializing.hpp:  GTEST_SKIP_IF_CUDAUVM_MEMORY_SPACE
lib/kokkos/containers/unit_tests/TestWithoutInitializing.hpp:  GTEST_SKIP_IF_CUDAUVM_MEMORY_SPACE
lib/kokkos/containers/unit_tests/TestWithoutInitializing.hpp:  GTEST_SKIP_IF_CUDAUVM_MEMORY_SPACE
lib/kokkos/containers/unit_tests/TestWithoutInitializing.hpp:  GTEST_SKIP_IF_CUDAUVM_MEMORY_SPACE
lib/kokkos/containers/unit_tests/TestWithoutInitializing.hpp:  GTEST_SKIP_IF_CUDAUVM_MEMORY_SPACE
lib/kokkos/containers/unit_tests/TestErrorReporter.hpp:#if !defined(KOKKOS_ENABLE_CUDA) || defined(KOKKOS_ENABLE_CUDA_LAMBDA)
lib/kokkos/containers/unit_tests/TestErrorReporter.hpp:    (!defined(KOKKOS_ENABLE_CUDA) || defined(KOKKOS_ENABLE_CUDA_LAMBDA))
lib/kokkos/containers/unit_tests/TestScatterView.hpp:#ifdef KOKKOS_ENABLE_CUDA
lib/kokkos/containers/unit_tests/TestScatterView.hpp:// disable duplicated instantiation with CUDA until
lib/kokkos/containers/unit_tests/TestScatterView.hpp:struct TestDuplicatedScatterView<Kokkos::Cuda, ScatterType, NumberType> {
lib/kokkos/containers/unit_tests/TestScatterView.hpp:    Kokkos::Device<Kokkos::Cuda, Kokkos::CudaSpace>, ScatterType, NumberType> {
lib/kokkos/containers/unit_tests/TestScatterView.hpp:    Kokkos::Device<Kokkos::Cuda, Kokkos::CudaUVMSpace>, ScatterType,
lib/kokkos/containers/unit_tests/TestScatterView.hpp:#if defined(KOKKOS_ENABLE_CUDA) || defined(KOKKOS_ENABLE_HIP)
lib/kokkos/containers/unit_tests/TestScatterView.hpp:#ifdef KOKKOS_ENABLE_CUDA
lib/kokkos/containers/unit_tests/TestScatterView.hpp:  using device_execution_space = Kokkos::Cuda;
lib/kokkos/containers/unit_tests/TestScatterView.hpp:  using device_memory_space    = Kokkos::CudaSpace;
lib/kokkos/containers/unit_tests/TestScatterView.hpp:  using host_accessible_space  = Kokkos::CudaUVMSpace;
lib/kokkos/containers/src/Kokkos_Vector.hpp:#ifdef KOKKOS_ENABLE_CUDA_UVM
lib/kokkos/containers/src/Kokkos_DualView.hpp:#ifdef KOKKOS_ENABLE_CUDA
lib/kokkos/containers/src/Kokkos_DualView.hpp:inline const Kokkos::Cuda& get_cuda_space(const Kokkos::Cuda& in) { return in; }
lib/kokkos/containers/src/Kokkos_DualView.hpp:inline const Kokkos::Cuda& get_cuda_space() {
lib/kokkos/containers/src/Kokkos_DualView.hpp:  return *Kokkos::Impl::cuda_get_deep_copy_space();
lib/kokkos/containers/src/Kokkos_DualView.hpp:template <typename NonCudaExecSpace>
lib/kokkos/containers/src/Kokkos_DualView.hpp:inline const Kokkos::Cuda& get_cuda_space(const NonCudaExecSpace&) {
lib/kokkos/containers/src/Kokkos_DualView.hpp:  return get_cuda_space();
lib/kokkos/containers/src/Kokkos_DualView.hpp:#endif  // KOKKOS_ENABLE_CUDA
lib/kokkos/containers/src/Kokkos_DualView.hpp:  /// For example, suppose you create a DualView on Cuda, like this:
lib/kokkos/containers/src/Kokkos_DualView.hpp:  ///       Kokkos::DualView<float, Kokkos::LayoutRight, Kokkos::Cuda>;
lib/kokkos/containers/src/Kokkos_DualView.hpp:  /// If you want to get the CUDA device View, do this:
lib/kokkos/containers/src/Kokkos_DualView.hpp:  ///   typename dual_view_type::t_dev cudaView = DV.view<Kokkos::Cuda> ();
lib/kokkos/containers/src/Kokkos_DualView.hpp:#ifdef KOKKOS_ENABLE_CUDA
lib/kokkos/containers/src/Kokkos_DualView.hpp:                         Kokkos::CudaUVMSpace>::value) {
lib/kokkos/containers/src/Kokkos_DualView.hpp:            Kokkos::Impl::cuda_prefetch_pointer(
lib/kokkos/containers/src/Kokkos_DualView.hpp:                Impl::get_cuda_space(args...), d_view.data(),
lib/kokkos/containers/src/Kokkos_DualView.hpp:#ifdef KOKKOS_ENABLE_CUDA
lib/kokkos/containers/src/Kokkos_DualView.hpp:                         Kokkos::CudaUVMSpace>::value) {
lib/kokkos/containers/src/Kokkos_DualView.hpp:            Kokkos::Impl::cuda_prefetch_pointer(
lib/kokkos/containers/src/Kokkos_DualView.hpp:                Impl::get_cuda_space(args...), d_view.data(),
lib/kokkos/containers/src/Kokkos_DualView.hpp:#ifdef KOKKOS_ENABLE_CUDA
lib/kokkos/containers/src/Kokkos_DualView.hpp:                       Kokkos::CudaUVMSpace>::value) {
lib/kokkos/containers/src/Kokkos_DualView.hpp:          Kokkos::Impl::cuda_prefetch_pointer(
lib/kokkos/containers/src/Kokkos_DualView.hpp:              Impl::get_cuda_space(args...), d_view.data(),
lib/kokkos/containers/src/Kokkos_DualView.hpp:#ifdef KOKKOS_ENABLE_CUDA
lib/kokkos/containers/src/Kokkos_DualView.hpp:                       Kokkos::CudaUVMSpace>::value) {
lib/kokkos/containers/src/Kokkos_DualView.hpp:          Kokkos::Impl::cuda_prefetch_pointer(
lib/kokkos/containers/src/Kokkos_DualView.hpp:              Impl::get_cuda_space(args...), d_view.data(),
lib/kokkos/containers/src/Kokkos_ScatterView.hpp:#ifdef KOKKOS_ENABLE_CUDA
lib/kokkos/containers/src/Kokkos_ScatterView.hpp:struct DefaultDuplication<Kokkos::Cuda> {
lib/kokkos/containers/src/Kokkos_ScatterView.hpp:struct DefaultContribution<Kokkos::Cuda,
lib/kokkos/containers/src/Kokkos_ScatterView.hpp:struct DefaultContribution<Kokkos::Cuda,
lib/kokkos/containers/src/Kokkos_UnorderedMap.hpp:/// Xeon Phi ("MIC"), and NVIDIA GPUs.
lib/kokkos/containers/src/Kokkos_UnorderedMap.hpp:///   <tt>Device</tt> is \c Cuda, it may use texture fetches to access
lib/kokkos/containers/src/Kokkos_UnorderedMap.hpp:  /// 'const value_type' via Cuda texture fetch must return by value.
lib/kokkos/example/make_buildlink/Makefile:KOKKOS_CUDA_OPTIONS=enable_lambda
lib/kokkos/example/make_buildlink/Makefile:ifneq (,$(findstring Cuda,$(KOKKOS_DEVICES)))
lib/kokkos/example/make_buildlink/Makefile:EXE = make_buildlink.cuda
lib/kokkos/example/make_buildlink/Makefile:	rm -f *.o *.cuda *.host
lib/kokkos/example/tutorial/01_hello_world/Makefile:ifneq (,$(findstring Cuda,$(KOKKOS_DEVICES)))
lib/kokkos/example/tutorial/01_hello_world/Makefile:EXE = 01_hello_world.cuda
lib/kokkos/example/tutorial/01_hello_world/Makefile:KOKKOS_DEVICES = "Cuda,OpenMP"
lib/kokkos/example/tutorial/01_hello_world/Makefile:	rm -f *.o *.cuda *.host
lib/kokkos/example/tutorial/01_hello_world/hello_world.cpp:  // KOKKOS_INLINE_FUNCTION macro.  If building with CUDA, this macro
lib/kokkos/example/tutorial/01_hello_world/hello_world.cpp:  // will mark your method as suitable for running on the CUDA device
lib/kokkos/example/tutorial/01_hello_world/hello_world.cpp:  // (as well as on the host).  If not building with CUDA, the macro
lib/kokkos/example/tutorial/01_hello_world/hello_world.cpp:  // could be OpenMP, Threads, Cuda, Serial, or even some other
lib/kokkos/example/tutorial/02_simple_reduce/Makefile:ifneq (,$(findstring Cuda,$(KOKKOS_DEVICES)))
lib/kokkos/example/tutorial/02_simple_reduce/Makefile:EXE = 02_simple_reduce.cuda
lib/kokkos/example/tutorial/02_simple_reduce/Makefile:KOKKOS_DEVICES = "Cuda,OpenMP"
lib/kokkos/example/tutorial/02_simple_reduce/Makefile:	rm -f *.o *.cuda *.host
lib/kokkos/example/tutorial/04_simple_memoryspaces/simple_memoryspaces.cpp:// example, if view_type lives in CUDA device memory, host_view_type
lib/kokkos/example/tutorial/04_simple_memoryspaces/simple_memoryspaces.cpp:// the case for HostSpace, CudaUVMSpace and CudaHostPinnedSpace.
lib/kokkos/example/tutorial/04_simple_memoryspaces/Makefile:ifneq (,$(findstring Cuda,$(KOKKOS_DEVICES)))
lib/kokkos/example/tutorial/04_simple_memoryspaces/Makefile:EXE = 04_simple_memoryspaces.cuda
lib/kokkos/example/tutorial/04_simple_memoryspaces/Makefile:KOKKOS_DEVICES = "Cuda,OpenMP"
lib/kokkos/example/tutorial/04_simple_memoryspaces/Makefile:	rm -f *.o *.cuda *.host
lib/kokkos/example/tutorial/Hierarchical_Parallelism/01_thread_teams/Makefile:ifneq (,$(findstring Cuda,$(KOKKOS_DEVICES)))
lib/kokkos/example/tutorial/Hierarchical_Parallelism/01_thread_teams/Makefile:EXE = 01_thread_teams.cuda
lib/kokkos/example/tutorial/Hierarchical_Parallelism/01_thread_teams/Makefile:KOKKOS_DEVICES = "Cuda,OpenMP"
lib/kokkos/example/tutorial/Hierarchical_Parallelism/01_thread_teams/Makefile:	rm -f *.o *.cuda *.host
lib/kokkos/example/tutorial/Hierarchical_Parallelism/Makefile:  ifdef KOKKOS_CUDA_OPTIONS
lib/kokkos/example/tutorial/Hierarchical_Parallelism/Makefile:    KOKKOS_SETTINGS += "KOKKOS_CUDA_OPTIONS=${KOKKOS_CUDA_OPTIONS}"
lib/kokkos/example/tutorial/Hierarchical_Parallelism/03_vectorization/Makefile:ifneq (,$(findstring Cuda,$(KOKKOS_DEVICES)))
lib/kokkos/example/tutorial/Hierarchical_Parallelism/03_vectorization/Makefile:EXE = 03_vectorization.cuda
lib/kokkos/example/tutorial/Hierarchical_Parallelism/03_vectorization/Makefile:KOKKOS_DEVICES = "Cuda,OpenMP"
lib/kokkos/example/tutorial/Hierarchical_Parallelism/03_vectorization/Makefile:	rm -f *.o *.cuda *.host
lib/kokkos/example/tutorial/Hierarchical_Parallelism/02_nested_parallel_for/nested_parallel_for.cpp:    // GPU for example with a team_size() larger than 31 only the first 31
lib/kokkos/example/tutorial/Hierarchical_Parallelism/02_nested_parallel_for/Makefile:ifneq (,$(findstring Cuda,$(KOKKOS_DEVICES)))
lib/kokkos/example/tutorial/Hierarchical_Parallelism/02_nested_parallel_for/Makefile:EXE = 02_nested_parallel_for.cuda
lib/kokkos/example/tutorial/Hierarchical_Parallelism/02_nested_parallel_for/Makefile:KOKKOS_DEVICES = "Cuda,OpenMP"
lib/kokkos/example/tutorial/Hierarchical_Parallelism/02_nested_parallel_for/Makefile:	rm -f *.o *.cuda *.host
lib/kokkos/example/tutorial/Hierarchical_Parallelism/04_team_scan/Makefile:ifneq (,$(findstring Cuda,$(KOKKOS_DEVICES)))
lib/kokkos/example/tutorial/Hierarchical_Parallelism/04_team_scan/Makefile:EXE = 04_team_scan.cuda
lib/kokkos/example/tutorial/Hierarchical_Parallelism/04_team_scan/Makefile:KOKKOS_DEVICES = "Cuda,OpenMP"
lib/kokkos/example/tutorial/Hierarchical_Parallelism/04_team_scan/Makefile:	rm -f *.o *.cuda *.host
lib/kokkos/example/tutorial/Hierarchical_Parallelism/01_thread_teams_lambda/thread_teams_lambda.cpp:// with a backend which doesn't support it (i.e. Cuda 6.5/7.0).
lib/kokkos/example/tutorial/Hierarchical_Parallelism/01_thread_teams_lambda/Makefile:ifneq (,$(findstring Cuda,$(KOKKOS_DEVICES)))
lib/kokkos/example/tutorial/Hierarchical_Parallelism/01_thread_teams_lambda/Makefile:EXE = 01_thread_teams_lambda.cuda
lib/kokkos/example/tutorial/Hierarchical_Parallelism/01_thread_teams_lambda/Makefile:KOKKOS_DEVICES = "Cuda,OpenMP"
lib/kokkos/example/tutorial/Hierarchical_Parallelism/01_thread_teams_lambda/Makefile:KOKKOS_CUDA_OPTIONS += "enable_lambda"
lib/kokkos/example/tutorial/Hierarchical_Parallelism/01_thread_teams_lambda/Makefile:	rm -f *.o *.cuda *.host
lib/kokkos/example/tutorial/Makefile:  ifdef KOKKOS_CUDA_OPTIONS
lib/kokkos/example/tutorial/Makefile:    KOKKOS_SETTINGS += "KOKKOS_CUDA_OPTIONS=${KOKKOS_CUDA_OPTIONS}"
lib/kokkos/example/tutorial/02_simple_reduce_lambda/Makefile:ifneq (,$(findstring Cuda,$(KOKKOS_DEVICES)))
lib/kokkos/example/tutorial/02_simple_reduce_lambda/Makefile:EXE = 02_simple_reduce_lambda.cuda
lib/kokkos/example/tutorial/02_simple_reduce_lambda/Makefile:KOKKOS_DEVICES = "Cuda,OpenMP"
lib/kokkos/example/tutorial/02_simple_reduce_lambda/Makefile:KOKKOS_CUDA_OPTIONS += "enable_lambda"
lib/kokkos/example/tutorial/02_simple_reduce_lambda/Makefile:	rm -f *.o *.cuda *.host
lib/kokkos/example/tutorial/02_simple_reduce_lambda/simple_reduce_lambda.cpp:// It also handles any other syntax needed for CUDA.
lib/kokkos/example/tutorial/02_simple_reduce_lambda/simple_reduce_lambda.cpp:// with a backend which doesn't support it (i.e. Cuda 6.5/7.0).
lib/kokkos/example/tutorial/Algorithms/Makefile:  ifdef KOKKOS_CUDA_OPTIONS
lib/kokkos/example/tutorial/Algorithms/Makefile:    KOKKOS_SETTINGS += "KOKKOS_CUDA_OPTIONS=${KOKKOS_CUDA_OPTIONS}"
lib/kokkos/example/tutorial/Algorithms/01_random_numbers/Makefile:ifneq (,$(findstring Cuda,$(KOKKOS_DEVICES)))
lib/kokkos/example/tutorial/Algorithms/01_random_numbers/Makefile:EXE = 01_random_numbers.cuda
lib/kokkos/example/tutorial/Algorithms/01_random_numbers/Makefile:KOKKOS_DEVICES = "Cuda,OpenMP"
lib/kokkos/example/tutorial/Algorithms/01_random_numbers/Makefile:	rm -f *.o *.cuda *.host
lib/kokkos/example/tutorial/Algorithms/01_random_numbers/random_numbers.cpp:// the thread number, on CUDA about 128k states are generated (enough to give
lib/kokkos/example/tutorial/Algorithms/01_random_numbers/random_numbers.cpp:// threads. On GPUs (i.e. using the CUDA backend it is not deterministic because
lib/kokkos/example/tutorial/06_simple_mdrangepolicy/Makefile:ifneq (,$(findstring Cuda,$(KOKKOS_DEVICES)))
lib/kokkos/example/tutorial/06_simple_mdrangepolicy/Makefile:EXE = 06_simple_mdrangepolicy.cuda
lib/kokkos/example/tutorial/06_simple_mdrangepolicy/Makefile:KOKKOS_DEVICES = "Cuda,OpenMP"
lib/kokkos/example/tutorial/06_simple_mdrangepolicy/Makefile:	rm -f *.o *.cuda *.host
lib/kokkos/example/tutorial/06_simple_mdrangepolicy/simple_mdrangepolicy.cpp:  //   ExecutionSpace: Kokkos::Serial, Kokkos::OpenMP, Kokkos::Cuda, etc.
lib/kokkos/example/tutorial/launch_bounds/Makefile:ifneq (,$(findstring Cuda,$(KOKKOS_DEVICES)))
lib/kokkos/example/tutorial/launch_bounds/Makefile:EXE = launch_bounds.cuda
lib/kokkos/example/tutorial/launch_bounds/Makefile:KOKKOS_DEVICES = "Cuda,OpenMP"
lib/kokkos/example/tutorial/launch_bounds/Makefile:# WAR for "undefined memcpy" w/ Ubuntu + CUDA 7.5
lib/kokkos/example/tutorial/launch_bounds/Makefile:	echo $(KOKKOS_INTERNAL_USE_CUDA) $(CUDA_PATH)
lib/kokkos/example/tutorial/launch_bounds/Makefile:	rm -f *.o *.cuda
lib/kokkos/example/tutorial/05_simple_atomics/Makefile:ifneq (,$(findstring Cuda,$(KOKKOS_DEVICES)))
lib/kokkos/example/tutorial/05_simple_atomics/Makefile:EXE = 05_simple_atomics.cuda
lib/kokkos/example/tutorial/05_simple_atomics/Makefile:KOKKOS_DEVICES = "Cuda,OpenMP"
lib/kokkos/example/tutorial/05_simple_atomics/Makefile:	rm -f *.o *.cuda *.host
lib/kokkos/example/tutorial/03_simple_view_lambda/Makefile:ifneq (,$(findstring Cuda,$(KOKKOS_DEVICES)))
lib/kokkos/example/tutorial/03_simple_view_lambda/Makefile:EXE = 03_simple_view_lambda.cuda
lib/kokkos/example/tutorial/03_simple_view_lambda/Makefile:KOKKOS_DEVICES = "Cuda,OpenMP"
lib/kokkos/example/tutorial/03_simple_view_lambda/Makefile:KOKKOS_CUDA_OPTIONS += "enable_lambda"
lib/kokkos/example/tutorial/03_simple_view_lambda/Makefile:	rm -f *.o *.cuda *.host
lib/kokkos/example/tutorial/03_simple_view_lambda/simple_view_lambda.cpp:// with a backend which doesn't support it (i.e. Cuda 6.5/7.0).
lib/kokkos/example/tutorial/Advanced_Views/03_subviews/Makefile:ifneq (,$(findstring Cuda,$(KOKKOS_DEVICES)))
lib/kokkos/example/tutorial/Advanced_Views/03_subviews/Makefile:EXE = 03_subviews.cuda
lib/kokkos/example/tutorial/Advanced_Views/03_subviews/Makefile:KOKKOS_DEVICES = "Cuda,OpenMP"
lib/kokkos/example/tutorial/Advanced_Views/03_subviews/Makefile:	rm -f *.o *.cuda *.host
lib/kokkos/example/tutorial/Advanced_Views/05_NVIDIA_UVM/uvm_example.cpp:#ifdef KOKKOS_ENABLE_CUDA
lib/kokkos/example/tutorial/Advanced_Views/05_NVIDIA_UVM/uvm_example.cpp:using view_type = Kokkos::View<double*, Kokkos::CudaUVMSpace>;
lib/kokkos/example/tutorial/Advanced_Views/05_NVIDIA_UVM/uvm_example.cpp:using idx_type  = Kokkos::View<int**, Kokkos::CudaUVMSpace>;
lib/kokkos/example/tutorial/Advanced_Views/05_NVIDIA_UVM/uvm_example.cpp:    // When using UVM Cuda views can be accessed on the Host directly
lib/kokkos/example/tutorial/Advanced_Views/05_NVIDIA_UVM/Makefile:SRC = $(wildcard ${KOKKOS_SRC_PATH}/example/tutorial/Advanced_Views/05_NVIDIA_UVM/*.cpp)
lib/kokkos/example/tutorial/Advanced_Views/05_NVIDIA_UVM/Makefile:ifneq (,$(findstring Cuda,$(KOKKOS_DEVICES)))
lib/kokkos/example/tutorial/Advanced_Views/05_NVIDIA_UVM/Makefile:EXE = 05_NVIDIA_UVM.cuda
lib/kokkos/example/tutorial/Advanced_Views/05_NVIDIA_UVM/Makefile:KOKKOS_DEVICES = "Cuda,OpenMP"
lib/kokkos/example/tutorial/Advanced_Views/05_NVIDIA_UVM/Makefile:EXE = 05_NVIDIA_UVM.host
lib/kokkos/example/tutorial/Advanced_Views/05_NVIDIA_UVM/Makefile:	rm -f *.o *.cuda *.host
lib/kokkos/example/tutorial/Advanced_Views/05_NVIDIA_UVM/CMakeLists.txt:IF (Kokkos_ENABLE_CUDA_UVM)
lib/kokkos/example/tutorial/Advanced_Views/05_NVIDIA_UVM/CMakeLists.txt:  tutorial_advancedviews_05_nvidia_uvm
lib/kokkos/example/tutorial/Advanced_Views/Makefile:  ifdef KOKKOS_CUDA_OPTIONS
lib/kokkos/example/tutorial/Advanced_Views/Makefile:    KOKKOS_SETTINGS += "KOKKOS_CUDA_OPTIONS=${KOKKOS_CUDA_OPTIONS}"
lib/kokkos/example/tutorial/Advanced_Views/Makefile:	mkdir -p 05_NVIDIA_UVM
lib/kokkos/example/tutorial/Advanced_Views/Makefile:	cd ./05_NVIDIA_UVM; \
lib/kokkos/example/tutorial/Advanced_Views/Makefile:	$(MAKE) build -f ${KOKKOS_PATH}/example/tutorial/Advanced_Views/05_NVIDIA_UVM/Makefile ${KOKKOS_SETTINGS}
lib/kokkos/example/tutorial/Advanced_Views/Makefile:	cd ./05_NVIDIA_UVM; \
lib/kokkos/example/tutorial/Advanced_Views/Makefile:	cd ./05_NVIDIA_UVM; \
lib/kokkos/example/tutorial/Advanced_Views/Makefile:	$(MAKE) test -f ${KOKKOS_PATH}/example/tutorial/Advanced_Views/05_NVIDIA_UVM/Makefile ${KOKKOS_SETTINGS}
lib/kokkos/example/tutorial/Advanced_Views/Makefile:	cd ./05_NVIDIA_UVM; \
lib/kokkos/example/tutorial/Advanced_Views/Makefile:	cd ./05_NVIDIA_UVM; \
lib/kokkos/example/tutorial/Advanced_Views/Makefile:	$(MAKE) clean -f ${KOKKOS_PATH}/example/tutorial/Advanced_Views/05_NVIDIA_UVM/Makefile ${KOKKOS_SETTINGS}
lib/kokkos/example/tutorial/Advanced_Views/Makefile:	cd ./05_NVIDIA_UVM; \
lib/kokkos/example/tutorial/Advanced_Views/CMakeLists.txt:IF (Kokkos_ENABLE_CUDA_UVM)
lib/kokkos/example/tutorial/Advanced_Views/CMakeLists.txt:  KOKKOS_ADD_EXAMPLE_DIRECTORIES(05_NVIDIA_UVM)
lib/kokkos/example/tutorial/Advanced_Views/01_data_layouts/Makefile:ifneq (,$(findstring Cuda,$(KOKKOS_DEVICES)))
lib/kokkos/example/tutorial/Advanced_Views/01_data_layouts/Makefile:EXE = 01_data_layouts.cuda
lib/kokkos/example/tutorial/Advanced_Views/01_data_layouts/Makefile:KOKKOS_DEVICES = "Cuda,OpenMP"
lib/kokkos/example/tutorial/Advanced_Views/01_data_layouts/Makefile:	rm -f *.o *.cuda *.host
lib/kokkos/example/tutorial/Advanced_Views/01_data_layouts/data_layouts.cpp:// space.  For example, the default Cuda layout is LayoutLeft, and the
lib/kokkos/example/tutorial/Advanced_Views/01_data_layouts/data_layouts.cpp:    // On GPUs threads should do coalesced loads and stores. That means
lib/kokkos/example/tutorial/Advanced_Views/01_data_layouts/data_layouts.cpp:  // In order to get coalesced access on GPUs where i corresponds closely to
lib/kokkos/example/tutorial/Advanced_Views/01_data_layouts/data_layouts.cpp:    // fast on GPUs and slow on CPUs
lib/kokkos/example/tutorial/Advanced_Views/01_data_layouts/data_layouts.cpp:    // fast on CPUs and slow on GPUs
lib/kokkos/example/tutorial/Advanced_Views/07_Overlapping_DeepCopy/Makefile:ifneq (,$(findstring Cuda,$(KOKKOS_DEVICES)))
lib/kokkos/example/tutorial/Advanced_Views/07_Overlapping_DeepCopy/Makefile:EXE = 07_Overlapping_DeepCopy.cuda
lib/kokkos/example/tutorial/Advanced_Views/07_Overlapping_DeepCopy/Makefile:KOKKOS_DEVICES = "Cuda,OpenMP"
lib/kokkos/example/tutorial/Advanced_Views/07_Overlapping_DeepCopy/Makefile:	rm -f *.o *.cuda *.host
lib/kokkos/example/tutorial/Advanced_Views/07_Overlapping_DeepCopy/overlapping_deepcopy.cpp:  Kokkos::View<double*, Kokkos::LayoutLeft, Kokkos::CudaSpace> a;
lib/kokkos/example/tutorial/Advanced_Views/07_Overlapping_DeepCopy/overlapping_deepcopy.cpp:      const Kokkos::View<double*, Kokkos::LayoutLeft, Kokkos::CudaSpace>& d_a)
lib/kokkos/example/tutorial/Advanced_Views/07_Overlapping_DeepCopy/overlapping_deepcopy.cpp:  Kokkos::View<double*, Kokkos::LayoutLeft, Kokkos::CudaSpace> a;
lib/kokkos/example/tutorial/Advanced_Views/07_Overlapping_DeepCopy/overlapping_deepcopy.cpp:  Kokkos::View<double*, Kokkos::LayoutLeft, Kokkos::CudaSpace> b;
lib/kokkos/example/tutorial/Advanced_Views/07_Overlapping_DeepCopy/overlapping_deepcopy.cpp:      const Kokkos::View<double*, Kokkos::LayoutLeft, Kokkos::CudaSpace>& d_a,
lib/kokkos/example/tutorial/Advanced_Views/07_Overlapping_DeepCopy/overlapping_deepcopy.cpp:      const Kokkos::View<double*, Kokkos::LayoutLeft, Kokkos::CudaSpace>& d_b)
lib/kokkos/example/tutorial/Advanced_Views/07_Overlapping_DeepCopy/overlapping_deepcopy.cpp:  Kokkos::View<double*, Kokkos::LayoutLeft, Kokkos::CudaHostPinnedSpace> a;
lib/kokkos/example/tutorial/Advanced_Views/07_Overlapping_DeepCopy/overlapping_deepcopy.cpp:  Kokkos::View<double*, Kokkos::LayoutLeft, Kokkos::CudaHostPinnedSpace> b;
lib/kokkos/example/tutorial/Advanced_Views/07_Overlapping_DeepCopy/overlapping_deepcopy.cpp:                                  Kokkos::CudaHostPinnedSpace>& d_a,
lib/kokkos/example/tutorial/Advanced_Views/07_Overlapping_DeepCopy/overlapping_deepcopy.cpp:                                  Kokkos::CudaHostPinnedSpace>& d_b)
lib/kokkos/example/tutorial/Advanced_Views/07_Overlapping_DeepCopy/overlapping_deepcopy.cpp:  Kokkos::View<double*, Kokkos::LayoutLeft, Kokkos::CudaSpace> a;
lib/kokkos/example/tutorial/Advanced_Views/07_Overlapping_DeepCopy/overlapping_deepcopy.cpp:  Kokkos::View<double*, Kokkos::LayoutLeft, Kokkos::CudaSpace> b;
lib/kokkos/example/tutorial/Advanced_Views/07_Overlapping_DeepCopy/overlapping_deepcopy.cpp:      const Kokkos::View<double*, Kokkos::LayoutLeft, Kokkos::CudaSpace>& d_a,
lib/kokkos/example/tutorial/Advanced_Views/07_Overlapping_DeepCopy/overlapping_deepcopy.cpp:      const Kokkos::View<double*, Kokkos::LayoutLeft, Kokkos::CudaSpace>& d_b)
lib/kokkos/example/tutorial/Advanced_Views/07_Overlapping_DeepCopy/overlapping_deepcopy.cpp:  Kokkos::View<double*, Kokkos::LayoutLeft, Kokkos::CudaSpace> d_a("Device A",
lib/kokkos/example/tutorial/Advanced_Views/07_Overlapping_DeepCopy/overlapping_deepcopy.cpp:  Kokkos::View<double*, Kokkos::LayoutLeft, Kokkos::CudaSpace> d_b("Device B",
lib/kokkos/example/tutorial/Advanced_Views/07_Overlapping_DeepCopy/overlapping_deepcopy.cpp:  Kokkos::View<double*, Kokkos::LayoutLeft, Kokkos::CudaSpace> d_tmp(
lib/kokkos/example/tutorial/Advanced_Views/07_Overlapping_DeepCopy/overlapping_deepcopy.cpp:  Kokkos::View<double*, Kokkos::LayoutLeft, Kokkos::CudaHostPinnedSpace> h_a(
lib/kokkos/example/tutorial/Advanced_Views/07_Overlapping_DeepCopy/overlapping_deepcopy.cpp:  Kokkos::View<double*, Kokkos::LayoutLeft, Kokkos::CudaHostPinnedSpace> h_b(
lib/kokkos/example/tutorial/Advanced_Views/07_Overlapping_DeepCopy/overlapping_deepcopy.cpp:  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::Cuda>(0, size),
lib/kokkos/example/tutorial/Advanced_Views/07_Overlapping_DeepCopy/overlapping_deepcopy.cpp:  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::Cuda>(0, size),
lib/kokkos/example/tutorial/Advanced_Views/07_Overlapping_DeepCopy/overlapping_deepcopy.cpp:  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::Cuda>(0, size),
lib/kokkos/example/tutorial/Advanced_Views/07_Overlapping_DeepCopy/overlapping_deepcopy.cpp:  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::Cuda>(0, size),
lib/kokkos/example/tutorial/Advanced_Views/02_memory_traits/memory_traits.cpp:// In CUDA, RandomAccess allows accesses through the texture
lib/kokkos/example/tutorial/Advanced_Views/02_memory_traits/memory_traits.cpp:    // trait. On GPUs where can be a dramatic difference
lib/kokkos/example/tutorial/Advanced_Views/02_memory_traits/Makefile:ifneq (,$(findstring Cuda,$(KOKKOS_DEVICES)))
lib/kokkos/example/tutorial/Advanced_Views/02_memory_traits/Makefile:EXE = 02_memory_traits.cuda
lib/kokkos/example/tutorial/Advanced_Views/02_memory_traits/Makefile:KOKKOS_DEVICES = "Cuda,OpenMP"
lib/kokkos/example/tutorial/Advanced_Views/02_memory_traits/Makefile:	rm -f *.o *.cuda *.host
lib/kokkos/example/tutorial/Advanced_Views/06_AtomicViews/Makefile:ifneq (,$(findstring Cuda,$(KOKKOS_DEVICES)))
lib/kokkos/example/tutorial/Advanced_Views/06_AtomicViews/Makefile:EXE = 06_AtomicViews.cuda
lib/kokkos/example/tutorial/Advanced_Views/06_AtomicViews/Makefile:KOKKOS_DEVICES = "Cuda,OpenMP"
lib/kokkos/example/tutorial/Advanced_Views/06_AtomicViews/Makefile:	rm -f *.o *.cuda *.host
lib/kokkos/example/tutorial/Advanced_Views/04_dualviews/Makefile:ifneq (,$(findstring Cuda,$(KOKKOS_DEVICES)))
lib/kokkos/example/tutorial/Advanced_Views/04_dualviews/Makefile:EXE = 04_dualviews.cuda
lib/kokkos/example/tutorial/Advanced_Views/04_dualviews/Makefile:KOKKOS_DEVICES = "Cuda,OpenMP"
lib/kokkos/example/tutorial/Advanced_Views/04_dualviews/Makefile:	rm -f *.o *.cuda *.host
lib/kokkos/example/tutorial/Advanced_Views/04_dualviews/dual_view.cpp:// two different memory spaces.  Examples include CUDA device memory
lib/kokkos/example/tutorial/Advanced_Views/04_dualviews/dual_view.cpp:// have ported only some parts of you application to run in CUDA,
lib/kokkos/example/tutorial/Advanced_Views/04_dualviews/dual_view.cpp:// application that work best with CUDA, and the parts that work
lib/kokkos/example/tutorial/Advanced_Views/04_dualviews/dual_view.cpp:// when not running in a memory space like CUDA.  DualView's
lib/kokkos/example/tutorial/Advanced_Views/04_dualviews/dual_view.cpp:    // is CUDA, the data layout will not be optimal on host, so performance is
lib/kokkos/example/tutorial/01_hello_world_lambda/hello_world_lambda.cpp:  // KOKKOS_LAMBDA macro.  If CUDA is disabled, this just turns into
lib/kokkos/example/tutorial/01_hello_world_lambda/hello_world_lambda.cpp:  // value.  Do NOT capture them by reference!  If CUDA is enabled,
lib/kokkos/example/tutorial/01_hello_world_lambda/hello_world_lambda.cpp:  // work correctly with CUDA.  Compare to the KOKKOS_INLINE_FUNCTION
lib/kokkos/example/tutorial/01_hello_world_lambda/hello_world_lambda.cpp:  // macro, which has a special meaning if CUDA is enabled.
lib/kokkos/example/tutorial/01_hello_world_lambda/hello_world_lambda.cpp:  // with a backend which doesn't support it (i.e. Cuda 6.5/7.0).
lib/kokkos/example/tutorial/01_hello_world_lambda/Makefile:ifneq (,$(findstring Cuda,$(KOKKOS_DEVICES)))
lib/kokkos/example/tutorial/01_hello_world_lambda/Makefile:EXE = 01_hello_world_lambda.cuda
lib/kokkos/example/tutorial/01_hello_world_lambda/Makefile:KOKKOS_DEVICES = "Cuda,OpenMP"
lib/kokkos/example/tutorial/01_hello_world_lambda/Makefile:KOKKOS_CUDA_OPTIONS += "enable_lambda"
lib/kokkos/example/tutorial/01_hello_world_lambda/Makefile:	rm -f *.o *.cuda *.host
lib/kokkos/example/tutorial/README:KOKKOS_DEVICES=Cuda    make -j 16
lib/kokkos/example/tutorial/03_simple_view/Makefile:ifneq (,$(findstring Cuda,$(KOKKOS_DEVICES)))
lib/kokkos/example/tutorial/03_simple_view/Makefile:EXE = 03_simple_view.cuda
lib/kokkos/example/tutorial/03_simple_view/Makefile:KOKKOS_DEVICES = "Cuda,OpenMP"
lib/kokkos/example/tutorial/03_simple_view/Makefile:	rm -f *.o *.cuda *.host
lib/kokkos/example/query_device/Makefile:ifneq (,$(findstring Cuda,$(KOKKOS_DEVICES)))
lib/kokkos/example/query_device/Makefile:  EXE = $(addsuffix .cuda, $(shell basename $(SRC_DIR)))
lib/kokkos/example/query_device/Makefile:	rm -f *.a *.o *.cuda *.host
lib/kokkos/example/virtual_functions/Makefile:KOKKOS_DEVICES=Cuda
lib/kokkos/example/virtual_functions/Makefile:KOKKOS_CUDA_OPTIONS=enable_lambda,rdc
lib/kokkos/example/virtual_functions/Makefile:#KOKKOS_CUDA_OPTIONS=enable_lambda
lib/kokkos/example/virtual_functions/Makefile:ifneq (,$(findstring Cuda,$(KOKKOS_DEVICES)))
lib/kokkos/example/virtual_functions/Makefile:EXE = virtual.cuda
lib/kokkos/example/virtual_functions/Makefile:	rm -f *.o *.cuda *.host
lib/kokkos/example/build_cmake_installed_kk_as_language/CMakeLists.txt:project(Example CXX Fortran CUDA)
lib/kokkos/example/build_cmake_installed_kk_as_language/CMakeLists.txt:set_source_files_properties(cmake_example.cpp PROPERTIES LANGUAGE CUDA)
lib/kokkos/bin/nvcc_wrapper:# NVCC, if you are building legacy C or C++ code with CUDA enabled.
lib/kokkos/bin/nvcc_wrapper:# sm_70 is supported by every CUDA version from 9-12 and is thus
lib/kokkos/bin/nvcc_wrapper:if [ ! -z $CUDA_ROOT ]; then
lib/kokkos/bin/nvcc_wrapper:  nvcc_compiler="$CUDA_ROOT/bin/nvcc"
lib/kokkos/bin/nvcc_wrapper:# Cuda (NVCC) only arguments
lib/kokkos/bin/nvcc_wrapper:cuda_args=""
lib/kokkos/bin/nvcc_wrapper:# Enable workaround for CUDA 6.5 for pragma ident
lib/kokkos/bin/nvcc_wrapper:  #handle source files to be compiled as cuda files
lib/kokkos/bin/nvcc_wrapper:        cuda_args="$cuda_args $rdc_flag"
lib/kokkos/bin/nvcc_wrapper:        cuda_args="$cuda_args $rdc_flag"
lib/kokkos/bin/nvcc_wrapper:    cuda_args="$cuda_args $1"
lib/kokkos/bin/nvcc_wrapper:  --extended-lambda|--expt-extended-lambda|--expt-relaxed-constexpr|--Wno-deprecated-gpu-targets|-Wno-deprecated-gpu-targets|-allow-unsupported-compiler|--allow-unsupported-compiler|--disable-warnings)
lib/kokkos/bin/nvcc_wrapper:    cuda_args="$cuda_args $1"
lib/kokkos/bin/nvcc_wrapper:    cuda_args="$cuda_args $1"
lib/kokkos/bin/nvcc_wrapper:  -maxrregcount|--default-stream|-Xnvlink|--fmad|-cudart|--cudart|-include|-time|-Xptxas)
lib/kokkos/bin/nvcc_wrapper:    cuda_args="$cuda_args $1 $2"
lib/kokkos/bin/nvcc_wrapper:    cuda_args="$cuda_args $1 $2"
lib/kokkos/bin/nvcc_wrapper:    cuda_args="$cuda_args $1"
lib/kokkos/bin/nvcc_wrapper:      # check if the first kind is one of the allowed ones, then this must be an nvcc list so put all of them to the cuda compiler
lib/kokkos/bin/nvcc_wrapper:        cuda_args="$cuda_args $1 $2"
lib/kokkos/bin/nvcc_wrapper:    first_werror_cuda=1
lib/kokkos/bin/nvcc_wrapper:        if [ $first_werror_cuda -ne 0 ]; then
lib/kokkos/bin/nvcc_wrapper:          cuda_args="$cuda_args -Werror="
lib/kokkos/bin/nvcc_wrapper:          first_werror_cuda=0
lib/kokkos/bin/nvcc_wrapper:          cuda_args="$cuda_args,"
lib/kokkos/bin/nvcc_wrapper:        cuda_args="$cuda_args$kind"
lib/kokkos/bin/nvcc_wrapper:    cuda_main_version=$([[ $(${nvcc_compiler} --version) =~ V([0-9]+) ]] && echo ${BASH_REMATCH[1]})
lib/kokkos/bin/nvcc_wrapper:    if [ ${cuda_main_version} -lt 12 ]; then
lib/kokkos/bin/nvcc_wrapper:    cuda_main_version=$([[ $(${nvcc_compiler} --version) =~ V([0-9]+) ]] && echo ${BASH_REMATCH[1]})
lib/kokkos/bin/nvcc_wrapper:    if [ ${cuda_main_version} -lt 11 ]; then
lib/kokkos/bin/nvcc_wrapper:    cuda_args="$cuda_args $1 $2"
lib/kokkos/bin/nvcc_wrapper:        cuda_args="$cuda_args $arch_flag"
lib/kokkos/bin/nvcc_wrapper:        cuda_args="$cuda_args $arch_flag"
lib/kokkos/bin/nvcc_wrapper:    cuda_args="$cuda_args $1"
lib/kokkos/bin/nvcc_wrapper:    cuda_args="$cuda_args $1 $2"
lib/kokkos/bin/nvcc_wrapper:  #Handle -Xcudafe argument
lib/kokkos/bin/nvcc_wrapper:  -Xcudafe)
lib/kokkos/bin/nvcc_wrapper:    cuda_args="$cuda_args -Xcudafe $2"
lib/kokkos/bin/nvcc_wrapper:  cuda_args="$cuda_args -ccbin $host_compiler"
lib/kokkos/bin/nvcc_wrapper:  cuda_args="$cuda_args -arch=$default_arch"
lib/kokkos/bin/nvcc_wrapper:nvcc_command="$nvcc_compiler $cuda_args $shared_args $xlinker_args $shared_versioned_libraries"
lib/kokkos/bin/hpcbind:# Check if nvidia-smi exist
lib/kokkos/bin/hpcbind:declare -i HPCBIND_HAS_NVIDIA=0
lib/kokkos/bin/hpcbind:type nvidia-smi >/dev/null 2>&1
lib/kokkos/bin/hpcbind:HPCBIND_HAS_NVIDIA=$((! $?))
lib/kokkos/bin/hpcbind:# Check if rocm-smi exist
lib/kokkos/bin/hpcbind:type rocm-smi >/dev/null 2>&1
lib/kokkos/bin/hpcbind:# Get visible gpu
lib/kokkos/bin/hpcbind:declare -i NUM_GPUS=0
lib/kokkos/bin/hpcbind:HPCBIND_VISIBLE_GPUS=""
lib/kokkos/bin/hpcbind:if [[ ${HPCBIND_HAS_NVIDIA} -eq 1 ]]; then
lib/kokkos/bin/hpcbind:  nvidia-smi >/dev/null 2>&1
lib/kokkos/bin/hpcbind:  HPCBIND_HAS_NVIDIA=$((! $?))
lib/kokkos/bin/hpcbind:  if [[ ${HPCBIND_HAS_NVIDIA} -eq 1 ]]; then
lib/kokkos/bin/hpcbind:    NUM_GPUS=$(nvidia-smi -L | wc -l);
lib/kokkos/bin/hpcbind:    HPCBIND_HAS_NVIDIA=$((! $?))
lib/kokkos/bin/hpcbind:    if [[ ${HPCBIND_HAS_NVIDIA} -eq 1 ]]; then
lib/kokkos/bin/hpcbind:      GPU_LIST="$( seq 0 $((NUM_GPUS-1)) )"
lib/kokkos/bin/hpcbind:      HPCBIND_VISIBLE_GPUS=${CUDA_VISIBLE_DEVICES:-${GPU_LIST}}
lib/kokkos/bin/hpcbind:  # rocm-smi doesn't have an error code if there is no hardware
lib/kokkos/bin/hpcbind:  # check for /sys/module/amdgpu/initstate instead
lib/kokkos/bin/hpcbind:  stat /sys/module/amdgpu/initstate >/dev/null 2>&1
lib/kokkos/bin/hpcbind:    NUM_GPUS=$(rocm-smi -i --csv | sed '/^$/d' | tail -n +2 | wc -l);
lib/kokkos/bin/hpcbind:      GPU_LIST="$( seq 0 $((NUM_GPUS-1)) )"
lib/kokkos/bin/hpcbind:      HPCBIND_VISIBLE_GPUS=${ROCR_VISIBLE_DEVICES:-${GPU_LIST}}
lib/kokkos/bin/hpcbind:declare -i HPCBIND_ENABLE_GPU_MAPPING=$((NUM_GPUS > 0))
lib/kokkos/bin/hpcbind:  echo "  Set the process mask, OMP environment variables and CUDA/ROCm environment"
lib/kokkos/bin/hpcbind:  echo "  variables to sane values if possible. Uses hwloc and nvidia-smi/rocm-smi if"
lib/kokkos/bin/hpcbind:  echo "  --visible-gpus=<L>    Comma separated list of gpu ids"
lib/kokkos/bin/hpcbind:  echo "                        Default: CUDA_VISIBLE_DEVICES/ROCR_VISIBLE_DEVICES or all gpus in"
lib/kokkos/bin/hpcbind:  echo "  --ignore-queue        Ignore queue job id when choosing visible GPU and partition"
lib/kokkos/bin/hpcbind:  echo "  --no-gpu-mapping      Do not set CUDA_VISIBLE_DEVICES/ROCR_VISIBLE_DEVICES"
lib/kokkos/bin/hpcbind:  echo "  Skip GPU 0 when mapping visible devices"
lib/kokkos/bin/hpcbind:  echo "    ${cmd} --distribute=4 --distribute-partition=0 --visible-gpus=1,2 -v -- command ..."
lib/kokkos/bin/hpcbind:    --visible-gpus=*)
lib/kokkos/bin/hpcbind:      HPCBIND_VISIBLE_GPUS=$(echo "${i#*=}" | tr ',' ' ')
lib/kokkos/bin/hpcbind:    --no-gpu-mapping)
lib/kokkos/bin/hpcbind:      HPCBIND_ENABLE_GPU_MAPPING=0
lib/kokkos/bin/hpcbind:# Check that visible gpus are valid
lib/kokkos/bin/hpcbind:HPCBIND_VISIBLE_GPUS=(${HPCBIND_VISIBLE_GPUS})
lib/kokkos/bin/hpcbind:if [[ ${HPCBIND_ENABLE_GPU_MAPPING} -eq 1 ]]; then
lib/kokkos/bin/hpcbind:  for ((i=0; i < ${#HPCBIND_VISIBLE_GPUS[*]}; i++)); do
lib/kokkos/bin/hpcbind:    if [[ ${HPCBIND_VISIBLE_GPUS[$i]} -ge ${NUM_GPUS} ||
lib/kokkos/bin/hpcbind:      ${HPCBIND_VISIBLE_GPUS[$i]} -lt 0 ]]; then
lib/kokkos/bin/hpcbind:      echo "HPCBIND Invaild GPU ID ${HPCBIND_VISIBLE_GPUS[$i]} (setting to 0)" > >(tee -a ${HPCBIND_LOG})
lib/kokkos/bin/hpcbind:      HPCBIND_VISIBLE_GPUS[$i]=0;
lib/kokkos/bin/hpcbind:  NUM_GPUS=${#HPCBIND_VISIBLE_GPUS[@]}
lib/kokkos/bin/hpcbind:# Set CUDA environment variables
lib/kokkos/bin/hpcbind:if [[ ${HPCBIND_ENABLE_GPU_MAPPING} -eq 1 ]]; then
lib/kokkos/bin/hpcbind:  if [[ ${HPCBIND_HAS_NVIDIA} -eq 1 ]]; then
lib/kokkos/bin/hpcbind:      declare -i GPU_ID=$((HPCBIND_PARTITION % NUM_GPUS))
lib/kokkos/bin/hpcbind:      export CUDA_VISIBLE_DEVICES="${HPCBIND_VISIBLE_GPUS[${GPU_ID}]}"
lib/kokkos/bin/hpcbind:      declare -i GPU_ID=$((MY_TASK_ID % NUM_GPUS))
lib/kokkos/bin/hpcbind:      export CUDA_VISIBLE_DEVICES="${HPCBIND_VISIBLE_GPUS[${GPU_ID}]}"
lib/kokkos/bin/hpcbind:      declare -i GPU_ID=$((HPCBIND_PARTITION % NUM_GPUS))
lib/kokkos/bin/hpcbind:      export ROCR_VISIBLE_DEVICES="${HPCBIND_VISIBLE_GPUS[${GPU_ID}]}"
lib/kokkos/bin/hpcbind:      declare -i GPU_ID=$((MY_TASK_ID % NUM_GPUS))
lib/kokkos/bin/hpcbind:      export ROCR_VISIBLE_DEVICES="${HPCBIND_VISIBLE_GPUS[${GPU_ID}]}"
lib/kokkos/bin/hpcbind:export HPCBIND_HAS_NVIDIA=${HPCBIND_HAS_NVIDIA}
lib/kokkos/bin/hpcbind:if [[ ${HPCBIND_HAS_NVIDIA} -eq 1 ]]; then
lib/kokkos/bin/hpcbind:  export HPCBIND_NVIDIA_ENABLE_GPU_MAPPING=${HPCBIND_ENABLE_GPU_MAPPING}
lib/kokkos/bin/hpcbind:  export HPCBIND_NVIDIA_VISIBLE_GPUS=$(echo "${HPCBIND_VISIBLE_GPUS[*]}" | tr ' ' ',')
lib/kokkos/bin/hpcbind:  export HPCBIND_AMD_ENABLE_GPU_MAPPING=${HPCBIND_ENABLE_GPU_MAPPING}
lib/kokkos/bin/hpcbind:  export HPCBIND_AMD_VISIBLE_GPUS=$(echo "${HPCBIND_VISIBLE_GPUS[*]}" | tr ' ' ',')
lib/kokkos/bin/hpcbind:  echo "[CUDA]" >> ${HPCBIND_LOG}
lib/kokkos/bin/hpcbind:  echo "${TMP_ENV}" | grep -E "^CUDA_" >> ${HPCBIND_LOG}
lib/kokkos/bin/hpcbind:  echo "[ROCM]" >> ${HPCBIND_LOG}
lib/kokkos/bin/hpcbind:  echo "${TMP_ENV}" | grep -E "^ROCM_" >> ${HPCBIND_LOG}
lib/kokkos/bin/hpcbind:  echo "[CUDA]" > >(tee -a ${HPCBIND_LOG})
lib/kokkos/bin/hpcbind:  echo "${TMP_ENV}" | grep -E "^CUDA_" > >(tee -a ${HPCBIND_LOG})
lib/kokkos/bin/hpcbind:  echo "[ROCM]" > >(tee -a ${HPCBIND_LOG})
lib/kokkos/bin/hpcbind:  echo "${TMP_ENV}" | grep -E "^ROCM_" > >(tee -a ${HPCBIND_LOG})
lib/kokkos/gnu_generate_makefile.bash:    --with-cuda)
lib/kokkos/gnu_generate_makefile.bash:      KOKKOS_DEVICES="${KOKKOS_DEVICES},Cuda"
lib/kokkos/gnu_generate_makefile.bash:      CUDA_PATH_NVCC=$(command -v nvcc)
lib/kokkos/gnu_generate_makefile.bash:      CUDA_PATH=${CUDA_PATH_NVCC%/bin/nvcc}
lib/kokkos/gnu_generate_makefile.bash:    # Catch this before '--with-cuda*'
lib/kokkos/gnu_generate_makefile.bash:    --with-cuda-options*)
lib/kokkos/gnu_generate_makefile.bash:      KOKKOS_CUDA_OPT="${key#*=}"
lib/kokkos/gnu_generate_makefile.bash:    --with-cuda*)
lib/kokkos/gnu_generate_makefile.bash:      KOKKOS_DEVICES="${KOKKOS_DEVICES},Cuda"
lib/kokkos/gnu_generate_makefile.bash:      CUDA_PATH="${key#*=}"
lib/kokkos/gnu_generate_makefile.bash:      echo "--with-cuda[=/Path/To/Cuda]:          Enable Cuda and set path to Cuda Toolkit."
lib/kokkos/gnu_generate_makefile.bash:      echo "               [Intel: GPU]"
lib/kokkos/gnu_generate_makefile.bash:      echo "                 INTEL_GEN       = SPIR64-based devices, e.g. Intel GPUs, using JIT"
lib/kokkos/gnu_generate_makefile.bash:      echo "                 INTEL_DG1       = Intel Iris XeMAX GPU"
lib/kokkos/gnu_generate_makefile.bash:      echo "                 INTEL_GEN9      = Intel GPU Gen9"
lib/kokkos/gnu_generate_makefile.bash:      echo "                 INTEL_GEN11     = Intel GPU Gen11"
lib/kokkos/gnu_generate_makefile.bash:      echo "                 INTEL_GEN12LP   = Intel GPU Gen12LP"
lib/kokkos/gnu_generate_makefile.bash:      echo "                 INTEL_XEHP      = Intel GPU Xe-HP"
lib/kokkos/gnu_generate_makefile.bash:      echo "                 INTEL_PVC       = Intel GPU Ponte Vecchio"
lib/kokkos/gnu_generate_makefile.bash:      echo "               [NVIDIA]"
lib/kokkos/gnu_generate_makefile.bash:      echo "                 Kepler30        = NVIDIA Kepler generation CC 3.0"
lib/kokkos/gnu_generate_makefile.bash:      echo "                 Kepler32        = NVIDIA Kepler generation CC 3.2"
lib/kokkos/gnu_generate_makefile.bash:      echo "                 Kepler35        = NVIDIA Kepler generation CC 3.5"
lib/kokkos/gnu_generate_makefile.bash:      echo "                 Kepler37        = NVIDIA Kepler generation CC 3.7"
lib/kokkos/gnu_generate_makefile.bash:      echo "                 Maxwell50       = NVIDIA Maxwell generation CC 5.0"
lib/kokkos/gnu_generate_makefile.bash:      echo "                 Maxwell52       = NVIDIA Maxwell generation CC 5.2"
lib/kokkos/gnu_generate_makefile.bash:      echo "                 Maxwell53       = NVIDIA Maxwell generation CC 5.3"
lib/kokkos/gnu_generate_makefile.bash:      echo "                 Pascal60        = NVIDIA Pascal generation CC 6.0"
lib/kokkos/gnu_generate_makefile.bash:      echo "                 Pascal61        = NVIDIA Pascal generation CC 6.1"
lib/kokkos/gnu_generate_makefile.bash:      echo "                 Volta70         = NVIDIA Volta generation CC 7.0"
lib/kokkos/gnu_generate_makefile.bash:      echo "                 Volta72         = NVIDIA Volta generation CC 7.2"
lib/kokkos/gnu_generate_makefile.bash:      echo "--with-cuda-options=[OPT]:    Additional options to CUDA:"
lib/kokkos/gnu_generate_makefile.bash:   [ ${#COMPILER} -eq 0 ] && [[ ${KOKKOS_DEVICES} =~ .*Cuda.* ]]; then
lib/kokkos/gnu_generate_makefile.bash:if [ ${#CUDA_PATH} -gt 0 ]; then
lib/kokkos/gnu_generate_makefile.bash:  KOKKOS_SETTINGS="${KOKKOS_SETTINGS} CUDA_PATH=${CUDA_PATH}"
lib/kokkos/gnu_generate_makefile.bash:if [ ${#KOKKOS_CUDA_OPT} -gt 0 ]; then
lib/kokkos/gnu_generate_makefile.bash:  KOKKOS_SETTINGS="${KOKKOS_SETTINGS} KOKKOS_CUDA_OPTIONS=${KOKKOS_CUDA_OPT}"
lib/kokkos/Spack.md:By default, Spack doesn't 'see' anything on your system - including things like CMake and CUDA.
lib/kokkos/Spack.md:This can be limited by adding a `packages.yaml` to your `$HOME/.spack` folder that includes CMake (and CUDA, if applicable).  For example, your `packages.yaml` file could be:
lib/kokkos/Spack.md:  cuda:
lib/kokkos/Spack.md:    - prefix: /opt/local/ppc64le-pwr8-nvidia/cuda/10.1.243
lib/kokkos/Spack.md:      spec: cuda@10.1.243
lib/kokkos/Spack.md:      - cuda/10.1.243
lib/kokkos/Spack.md:      spec: cuda@10.1.243
lib/kokkos/Spack.md:> spack graph kokkos +cuda
lib/kokkos/Spack.md:o |  cuda
lib/kokkos/Spack.md:Without the existing CUDA and CMake being identified in `packages.yaml`, a (subset!) of the output would be:
lib/kokkos/Spack.md:  depends_on("kokkos+cuda", when="+cuda")
lib/kokkos/Spack.md:if your project needs CUDA-specific logic to configure and build.
lib/kokkos/Spack.md:> spack install superscience+cuda
lib/kokkos/Spack.md:This flows upstream to the Kokkos dependency, causing the `kokkos+cuda` variant to build.
lib/kokkos/Spack.md:  variants: +cuda +openmp +cuda_lambda +wrapper ^cuda@10.1 cuda_arch=70
lib/kokkos/Spack.md:which gives the "best" Kokkos configuration as CUDA+OpenMP optimized for a Volta 70 architecture using CUDA 10.1.
lib/kokkos/Spack.md:It also enables support for CUDA Lambdas.
lib/kokkos/Spack.md:Note here that we use the built-in `cuda_arch` variant of Spack to specify the archicture.
lib/kokkos/Spack.md:and CUDA architectures.
lib/kokkos/Spack.md:Spack does not currently provide an AMD GPU microarchitecture option.
lib/kokkos/Spack.md:If building for HIP or an AMD GPU, Kokkos provides an `amd_gpu_arch` similar to `cuda_arch`.
lib/kokkos/Spack.md:  variants: +hip amd_gpu_arch=vega900
lib/kokkos/Spack.md:For example, CUDA is not enabled by default (there is no easy logic to conditionally activate this for CUDA-enabled systems).
lib/kokkos/Spack.md:If you don't specify a CUDA build variant in a `packages.yaml` and you build your Kokkos-dependent project:
lib/kokkos/Spack.md:kokkos-kernels@3.0%gcc@8.3.0~blas build_type=RelWithDebInfo ~cblas~complex_double~complex_float~cublas~cuda cuda_arch=none ~cusparse~diy+double execspace_cuda=auto execspace_openmp=auto execspace_serial=auto execspace_threads=auto ~float~lapack~lapacke+layoutleft~layoutright memspace_cudaspace=auto memspace_cudauvmspace=auto +memspace_hostspace~mkl+offset_int+offset_size_t~openmp+ordinal_int~ordinal_int64_t~serial~superlu arch=linux-rhel7-skylake_avx512
lib/kokkos/Spack.md:        ^kokkos@3.0%gcc@8.3.0~aggressive_vectorization~amdavx~armv80~armv81~armv8_thunderx~armv8_tx2~bdw~bgq build_type=RelWithDebInfo ~carrizo~compiler_warnings+cuda cuda_arch=none +cuda_lambda~cuda_ldg_intrinsic~cuda_relocatable_device_code~cuda_uvm~debug~debug_bounds_check~debug_dualview_modify_check~deprecated_code~diy~epyc~examples~explicit_instantiation~fiji~gfx901~hpx~hpx_async_dispatch~hsw~hwloc~kaveri~kepler30~kepler32~kepler35~kepler37~knc~knl~maxwell50~maxwell52~maxwell53~memkind~numactl+openmp~pascal60~pascal61~power7~power8~power9+profiling~profiling_load_print~pthread~qthread~rocm~ryzen~serial~skx~snb std=14 ~tests~turing75~vega+volta70~volta72+wrapper~wsm arch=linux-rhel7-skylake_avx512
lib/kokkos/Spack.md:                ^cuda@10.1%gcc@8.3.0 arch=linux-rhel7-skylake_avx512
lib/kokkos/Spack.md:                                    ^openmpi@4.0.2%gcc@8.3.0~cuda+cxx_exceptions fabrics=none ~java~legacylaunchers~memchecker patches=073477a76bba780c67c36e959cd3ee6910743e2735c7e76850ffba6791d498e4 ~pmi schedulers=none ~sqlite3~thread_multiple+vt arch=linux-rhel7-skylake_avx512
lib/kokkos/Spack.md:kokkos@3.0%gcc@8.3.0~aggressive_vectorization~amdavx~armv80~armv81~armv8_thunderx~armv8_tx2~bdw~bgq build_type=RelWithDebInfo ~carrizo~compiler_warnings+cuda cuda_arch=none +cuda_lambda~cuda_ldg_intrinsic~cuda_relocatable_device_code~cuda_uvm~debug~debug_bounds_check~debug_dualview_modify_check~deprecated_code~diy~epyc~examples~explicit_instantiation~fiji~gfx901~hpx~hpx_async_dispatch~hsw~hwloc~kaveri~kepler30~kepler32~kepler35~kepler37~knc~knl~maxwell50~maxwell52~maxwell53~memkind~numactl+openmp~pascal60~pascal61~power7~power8~power9+profiling~profiling_load_print~pthread~qthread~rocm~ryzen~serial~skx~snb std=11 ~tests~turing75~vega+volta70~volta72+wrapper~wsm arch=linux-rhel7-skylake_avx512
lib/kokkos/Spack.md:> spack add kokkos +cuda +cuda_lambda +volta70
lib/kokkos/Spack.md:All packages within the environment will build against the CUDA-enabled Kokkos,
lib/kokkos/Spack.md:Kokkos is a C++ project, but often builds for the CUDA backend.
lib/kokkos/Spack.md:This is still valid for Kokkos. To use the special wrapper for CUDA builds, request a desired compiler and simply add the `+wrapper` variant.
lib/kokkos/Spack.md:> spack install kokkos +cuda +wrapper %gcc@7.2.0
lib/kokkos/Spack.md:This has the unfortunate consequence that Kokkos CUDA projects not using MPI will implicitly depend on MPI anyway.
lib/molfile/molfile_plugin.h:    * kernel-bypass direct I/O or using GPU-Direct Storage APIs,
lib/gpu/lal_lj_dsf_ext.cpp:int ljd_gpu_init(const int ntypes, double **cutsq, double **host_lj1,
lib/gpu/lal_lj_dsf_ext.cpp:                 const double cell_size, int &gpu_mode, FILE *screen,
lib/gpu/lal_lj_dsf_ext.cpp:  gpu_mode=LJDMF.device->gpu_mode();
lib/gpu/lal_lj_dsf_ext.cpp:  double gpu_split=LJDMF.device->particle_split();
lib/gpu/lal_lj_dsf_ext.cpp:  int first_gpu=LJDMF.device->first_device();
lib/gpu/lal_lj_dsf_ext.cpp:  int last_gpu=LJDMF.device->last_device();
lib/gpu/lal_lj_dsf_ext.cpp:  int gpu_rank=LJDMF.device->gpu_rank();
lib/gpu/lal_lj_dsf_ext.cpp:  int procs_per_gpu=LJDMF.device->procs_per_gpu();
lib/gpu/lal_lj_dsf_ext.cpp:  LJDMF.device->init_message(screen,"lj/cut/coul/dsf",first_gpu,last_gpu);
lib/gpu/lal_lj_dsf_ext.cpp:                       maxspecial, cell_size, gpu_split, screen, host_cut_ljsq,
lib/gpu/lal_lj_dsf_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_lj_dsf_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_lj_dsf_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_lj_dsf_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_lj_dsf_ext.cpp:                last_gpu,i);
lib/gpu/lal_lj_dsf_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_lj_dsf_ext.cpp:                         cell_size, gpu_split, screen, host_cut_ljsq,
lib/gpu/lal_lj_dsf_ext.cpp:    LJDMF.estimate_gpu_overhead();
lib/gpu/lal_lj_dsf_ext.cpp:void ljd_gpu_clear() {
lib/gpu/lal_lj_dsf_ext.cpp:int** ljd_gpu_compute_n(const int ago, const int inum_full,
lib/gpu/lal_lj_dsf_ext.cpp:void ljd_gpu_compute(const int ago, const int inum_full, const int nall,
lib/gpu/lal_lj_dsf_ext.cpp:double ljd_gpu_bytes() {
lib/gpu/lal_lj_coul_soft.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_lj_coul_soft.h:    * - -1 if fix gpu not found
lib/gpu/lal_lj_coul_soft.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_lj_coul_soft.h:           const double gpu_split, FILE *screen, double **host_cut_ljsq,
lib/gpu/lal_amoeba_ext.cpp:int amoeba_gpu_init(const int ntypes, const int max_amtype, const int max_amclass,
lib/gpu/lal_amoeba_ext.cpp:                    const double cell_size, int &gpu_mode, FILE *screen,
lib/gpu/lal_amoeba_ext.cpp:  gpu_mode=AMOEBAMF.device->gpu_mode();
lib/gpu/lal_amoeba_ext.cpp:  double gpu_split=AMOEBAMF.device->particle_split();
lib/gpu/lal_amoeba_ext.cpp:  int first_gpu=AMOEBAMF.device->first_device();
lib/gpu/lal_amoeba_ext.cpp:  int last_gpu=AMOEBAMF.device->last_device();
lib/gpu/lal_amoeba_ext.cpp:  int gpu_rank=AMOEBAMF.device->gpu_rank();
lib/gpu/lal_amoeba_ext.cpp:  int procs_per_gpu=AMOEBAMF.device->procs_per_gpu();
lib/gpu/lal_amoeba_ext.cpp:  AMOEBAMF.device->init_message(screen,"amoeba",first_gpu,last_gpu);
lib/gpu/lal_amoeba_ext.cpp:    fprintf(screen,"Initializing GPU and compiling on process 0...");
lib/gpu/lal_amoeba_ext.cpp:                          maxspecial, maxspecial15, cell_size, gpu_split,
lib/gpu/lal_amoeba_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_amoeba_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_amoeba_ext.cpp:        fprintf(screen,"Initializing GPU %d on core %d...",first_gpu,i);
lib/gpu/lal_amoeba_ext.cpp:        fprintf(screen,"Initializing GPUs %d-%d on core %d...",first_gpu,
lib/gpu/lal_amoeba_ext.cpp:                last_gpu,i);
lib/gpu/lal_amoeba_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_amoeba_ext.cpp:                            maxspecial, maxspecial15, cell_size, gpu_split,
lib/gpu/lal_amoeba_ext.cpp:    AMOEBAMF.device->gpu_barrier();
lib/gpu/lal_amoeba_ext.cpp:    AMOEBAMF.estimate_gpu_overhead();
lib/gpu/lal_amoeba_ext.cpp:void amoeba_gpu_clear() {
lib/gpu/lal_amoeba_ext.cpp:int** amoeba_gpu_precompute(const int ago, const int inum_full, const int nall,
lib/gpu/lal_amoeba_ext.cpp:void amoeba_gpu_compute_multipole_real(const int ago, const int inum_full,
lib/gpu/lal_amoeba_ext.cpp:void amoeba_gpu_compute_udirect2b(int *host_amtype, int *host_amgroup, double **host_rpole,
lib/gpu/lal_amoeba_ext.cpp:void amoeba_gpu_compute_umutual2b(int *host_amtype, int *host_amgroup, double **host_rpole,
lib/gpu/lal_amoeba_ext.cpp:void amoeba_gpu_update_fieldp(void **fieldp_ptr) {
lib/gpu/lal_amoeba_ext.cpp:void amoeba_gpu_compute_polar_real(int *host_amtype, int *host_amgroup, double **host_rpole,
lib/gpu/lal_amoeba_ext.cpp:void amoeba_gpu_precompute_kspace(const int inum_full, const int bsorder,
lib/gpu/lal_amoeba_ext.cpp:void amoeba_gpu_fphi_uind(double ****host_grid_brick, void **host_fdip_phi1,
lib/gpu/lal_amoeba_ext.cpp:void amoeba_gpu_fphi_mpole(double ***host_grid_brick, void **host_fphi, const double felec) {
lib/gpu/lal_amoeba_ext.cpp:double amoeba_gpu_bytes() {
lib/gpu/lal_pppm.cu:// Allow PPPM to compile without atomics for NVIDIA 1.0 cards, error
lib/gpu/lal_pppm.cu:// generated at runtime with use of pppm/gpu
lib/gpu/lal_pppm.cu:#if defined(NV_KERNEL) && (__CUDA_ARCH__ < 110)
lib/gpu/lal_pppm.cu:#pragma OPENCL EXTENSION cl_khr_global_int32_base_atomics: enable
lib/gpu/lal_pppm.cu:#pragma OPENCL EXTENSION cl_amd_fp64 : enable
lib/gpu/lal_pppm.cu:#pragma OPENCL EXTENSION cl_khr_fp64 : enable
lib/gpu/lal_edpd.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_edpd.h:    * - -1 if fix gpu not found
lib/gpu/lal_edpd.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_edpd.h:           const int maxspecial, const double cell_size, const double gpu_split,
lib/gpu/lal_gayberne.cu:      gpu_quat_to_mat_trans(q,i,a1);
lib/gpu/lal_gayberne.cu:      gpu_diag_times3(ishape,a1,t);
lib/gpu/lal_gayberne.cu:      gpu_transpose_times3(a1,t,g1);
lib/gpu/lal_gayberne.cu:      gpu_diag_times3(well[itype],a1,t);
lib/gpu/lal_gayberne.cu:      gpu_transpose_times3(a1,t,b1);
lib/gpu/lal_gayberne.cu:      numtyp ir = gpu_dot3(r12,r12);
lib/gpu/lal_gayberne.cu:      gpu_quat_to_mat_trans(q,j,a2);
lib/gpu/lal_gayberne.cu:              gpu_diag_times3(shape[jtype],a2,g12);
lib/gpu/lal_gayberne.cu:              gpu_transpose_times3(a2,g12,g2);
lib/gpu/lal_gayberne.cu:              gpu_plus3(g1,g2,g12);
lib/gpu/lal_gayberne.cu:            gpu_mldivide3(g12,r12,kappa,err_flag);
lib/gpu/lal_gayberne.cu:              sigma12 = gpu_dot3(r12,kappa);
lib/gpu/lal_gayberne.cu:              numtyp temp2 = gpu_dot3(kappa,r12);
lib/gpu/lal_gayberne.cu:              gpu_row_times3(kappa,g1,tempv2);
lib/gpu/lal_gayberne.cu:              gpu_cross3(tempv,tempv2,tUr);
lib/gpu/lal_gayberne.cu:          numtyp det_g12 = gpu_det3(g12);
lib/gpu/lal_gayberne.cu:        gpu_cross3(a1,tempv,tempv2);
lib/gpu/lal_gayberne.cu:        gpu_cross3(a1+3,tempv,tempv2);
lib/gpu/lal_gayberne.cu:        gpu_cross3(a1+6,tempv,tempv2);
lib/gpu/lal_gayberne.cu:          gpu_diag_times3(well[jtype],a2,b12);
lib/gpu/lal_gayberne.cu:          gpu_transpose_times3(a2,b12,b2);
lib/gpu/lal_gayberne.cu:          gpu_plus3(b1,b2,b12);
lib/gpu/lal_gayberne.cu:        gpu_mldivide3(b12,r12,iota,err_flag);
lib/gpu/lal_gayberne.cu:        chi = gpu_dot3(r12,iota);
lib/gpu/lal_gayberne.cu:        numtyp temp1 = gpu_dot3(iota,r12);
lib/gpu/lal_gayberne.cu:        gpu_row_times3(iota,b1,tempv);
lib/gpu/lal_gayberne.cu:        gpu_cross3(tempv,iota,tchi);
lib/gpu/lal_eam.cpp:#if defined(USE_OPENCL)
lib/gpu/lal_eam.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_eam.cpp:               const double gpu_split, FILE *_screen)
lib/gpu/lal_eam.cpp:  #ifdef USE_OPENCL
lib/gpu/lal_eam.cpp:                            gpu_split,_screen,eam,"k_eam",onetype);
lib/gpu/lal_eam.cpp:  // Initialize timers for selected GPU
lib/gpu/lal_eam.cpp:// Reneighbor on GPU and then compute per-atom fp
lib/gpu/lal_eam.cpp:  inum=this->hd_balancer.get_gpu_count(ago,inum_full);
lib/gpu/lal_eam.cpp:  // Build neighbor list on GPU if necessary
lib/gpu/lal_table.cpp:#if defined(USE_OPENCL)
lib/gpu/lal_table.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_table.cpp:                const double gpu_split, FILE *_screen,
lib/gpu/lal_table.cpp:                            gpu_split,_screen,table,"k_table");
lib/gpu/lal_coul_ext.cpp:int coul_gpu_init(const int ntypes, double **host_scale,
lib/gpu/lal_coul_ext.cpp:                  int &gpu_mode, FILE *screen, const double qqrd2e) {
lib/gpu/lal_coul_ext.cpp:  gpu_mode=COULMF.device->gpu_mode();
lib/gpu/lal_coul_ext.cpp:  double gpu_split=COULMF.device->particle_split();
lib/gpu/lal_coul_ext.cpp:  int first_gpu=COULMF.device->first_device();
lib/gpu/lal_coul_ext.cpp:  int last_gpu=COULMF.device->last_device();
lib/gpu/lal_coul_ext.cpp:  int gpu_rank=COULMF.device->gpu_rank();
lib/gpu/lal_coul_ext.cpp:  int procs_per_gpu=COULMF.device->procs_per_gpu();
lib/gpu/lal_coul_ext.cpp:  COULMF.device->init_message(screen,"coul/cut",first_gpu,last_gpu);
lib/gpu/lal_coul_ext.cpp:    fprintf(screen,"Initializing GPU and compiling on process 0...");
lib/gpu/lal_coul_ext.cpp:                       maxspecial, cell_size, gpu_split, screen, qqrd2e);
lib/gpu/lal_coul_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_coul_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_coul_ext.cpp:        fprintf(screen,"Initializing GPU %d on core %d...",first_gpu,i);
lib/gpu/lal_coul_ext.cpp:        fprintf(screen,"Initializing GPUs %d-%d on core %d...",first_gpu,
lib/gpu/lal_coul_ext.cpp:                last_gpu,i);
lib/gpu/lal_coul_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_coul_ext.cpp:                          maxspecial, cell_size, gpu_split, screen, qqrd2e);
lib/gpu/lal_coul_ext.cpp:    COULMF.estimate_gpu_overhead();
lib/gpu/lal_coul_ext.cpp:void coul_gpu_reinit(const int ntypes, double **host_scale) {
lib/gpu/lal_coul_ext.cpp:  int gpu_rank=COULMF.device->gpu_rank();
lib/gpu/lal_coul_ext.cpp:  int procs_per_gpu=COULMF.device->procs_per_gpu();
lib/gpu/lal_coul_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_coul_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_coul_ext.cpp:void coul_gpu_clear() {
lib/gpu/lal_coul_ext.cpp:int** coul_gpu_compute_n(const int ago, const int inum_full,
lib/gpu/lal_coul_ext.cpp:void coul_gpu_compute(const int ago, const int inum_full, const int nall,
lib/gpu/lal_coul_ext.cpp:double coul_gpu_bytes() {
lib/gpu/lal_lj_spica_long_ext.cpp:int spical_gpu_init(const int ntypes, double **cutsq, int **cg_type,
lib/gpu/lal_lj_spica_long_ext.cpp:                  const int maxspecial, const double cell_size, int &gpu_mode,
lib/gpu/lal_lj_spica_long_ext.cpp:  gpu_mode=CMMLMF.device->gpu_mode();
lib/gpu/lal_lj_spica_long_ext.cpp:  double gpu_split=CMMLMF.device->particle_split();
lib/gpu/lal_lj_spica_long_ext.cpp:  int first_gpu=CMMLMF.device->first_device();
lib/gpu/lal_lj_spica_long_ext.cpp:  int last_gpu=CMMLMF.device->last_device();
lib/gpu/lal_lj_spica_long_ext.cpp:  int gpu_rank=CMMLMF.device->gpu_rank();
lib/gpu/lal_lj_spica_long_ext.cpp:  int procs_per_gpu=CMMLMF.device->procs_per_gpu();
lib/gpu/lal_lj_spica_long_ext.cpp:  CMMLMF.device->init_message(screen,"lj/spica/coul/long",first_gpu,last_gpu);
lib/gpu/lal_lj_spica_long_ext.cpp:                        maxspecial, cell_size, gpu_split, screen, host_cut_ljsq,
lib/gpu/lal_lj_spica_long_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_lj_spica_long_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_lj_spica_long_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_lj_spica_long_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_lj_spica_long_ext.cpp:                last_gpu,i);
lib/gpu/lal_lj_spica_long_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_lj_spica_long_ext.cpp:                          maxspecial, cell_size, gpu_split, screen,
lib/gpu/lal_lj_spica_long_ext.cpp:    CMMLMF.estimate_gpu_overhead();
lib/gpu/lal_lj_spica_long_ext.cpp:void spical_gpu_clear() {
lib/gpu/lal_lj_spica_long_ext.cpp:int** spical_gpu_compute_n(const int ago, const int inum_full,
lib/gpu/lal_lj_spica_long_ext.cpp:void spical_gpu_compute(const int ago, const int inum_full, const int nall,
lib/gpu/lal_lj_spica_long_ext.cpp:double spical_gpu_bytes() {
lib/gpu/lal_buck_coul_ext.cpp:int buckc_gpu_init(const int ntypes, double **cutsq, double **host_rhoinv,
lib/gpu/lal_buck_coul_ext.cpp:                 const double cell_size, int &gpu_mode, FILE *screen,
lib/gpu/lal_buck_coul_ext.cpp:  gpu_mode=BUCKCMF.device->gpu_mode();
lib/gpu/lal_buck_coul_ext.cpp:  double gpu_split=BUCKCMF.device->particle_split();
lib/gpu/lal_buck_coul_ext.cpp:  int first_gpu=BUCKCMF.device->first_device();
lib/gpu/lal_buck_coul_ext.cpp:  int last_gpu=BUCKCMF.device->last_device();
lib/gpu/lal_buck_coul_ext.cpp:  int gpu_rank=BUCKCMF.device->gpu_rank();
lib/gpu/lal_buck_coul_ext.cpp:  int procs_per_gpu=BUCKCMF.device->procs_per_gpu();
lib/gpu/lal_buck_coul_ext.cpp:  BUCKCMF.device->init_message(screen,"buck",first_gpu,last_gpu);
lib/gpu/lal_buck_coul_ext.cpp:                       maxspecial, cell_size, gpu_split, screen,
lib/gpu/lal_buck_coul_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_buck_coul_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_buck_coul_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_buck_coul_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_buck_coul_ext.cpp:                last_gpu,i);
lib/gpu/lal_buck_coul_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_buck_coul_ext.cpp:                       maxspecial, cell_size, gpu_split, screen,
lib/gpu/lal_buck_coul_ext.cpp:    BUCKCMF.estimate_gpu_overhead();
lib/gpu/lal_buck_coul_ext.cpp:void buckc_gpu_clear() {
lib/gpu/lal_buck_coul_ext.cpp:int ** buckc_gpu_compute_n(const int ago, const int inum_full,
lib/gpu/lal_buck_coul_ext.cpp:void buckc_gpu_compute(const int ago, const int inum_full, const int nall,
lib/gpu/lal_buck_coul_ext.cpp:double buckc_gpu_bytes() {
lib/gpu/lal_base_dipole.cpp:                             const double gpu_split, FILE *_screen,
lib/gpu/lal_base_dipole.cpp:  int gpu_nbor=0;
lib/gpu/lal_base_dipole.cpp:  if (device->gpu_mode()==Device<numtyp,acctyp>::GPU_NEIGH)
lib/gpu/lal_base_dipole.cpp:    gpu_nbor=1;
lib/gpu/lal_base_dipole.cpp:  else if (device->gpu_mode()==Device<numtyp,acctyp>::GPU_HYB_NEIGH)
lib/gpu/lal_base_dipole.cpp:    gpu_nbor=2;
lib/gpu/lal_base_dipole.cpp:  int _gpu_host=0;
lib/gpu/lal_base_dipole.cpp:  int host_nlocal=hd_balancer.first_host_count(nlocal,gpu_split,gpu_nbor);
lib/gpu/lal_base_dipole.cpp:    _gpu_host=1;
lib/gpu/lal_base_dipole.cpp:  if (ucl_device!=device->gpu) _compiled=false;
lib/gpu/lal_base_dipole.cpp:  ucl_device=device->gpu;
lib/gpu/lal_base_dipole.cpp:  if (_threads_per_atom>1 && gpu_nbor==0) {
lib/gpu/lal_base_dipole.cpp:  success = device->init_nbor(nbor,nlocal,host_nlocal,nall,maxspecial,_gpu_host,
lib/gpu/lal_base_dipole.cpp:  hd_balancer.init(device,gpu_nbor,gpu_split);
lib/gpu/lal_base_dipole.cpp:  // Initialize timers for the selected GPU
lib/gpu/lal_base_dipole.cpp:  _max_an_bytes=ans->gpu_bytes()+nbor->gpu_bytes();
lib/gpu/lal_base_dipole.cpp:void BaseDipoleT::estimate_gpu_overhead() {
lib/gpu/lal_base_dipole.cpp:  device->estimate_gpu_overhead(1,_gpu_overhead,_driver_overhead);
lib/gpu/lal_base_dipole.cpp:  _gpu_overhead*=hd_balancer.timestep();
lib/gpu/lal_base_dipole.cpp:                       _gpu_overhead,_driver_overhead,_threads_per_atom,screen);
lib/gpu/lal_base_dipole.cpp:  double bytes=ans->gpu_bytes()+nbor->gpu_bytes();
lib/gpu/lal_base_dipole.cpp:  double bytes=ans->gpu_bytes()+nbor->gpu_bytes();
lib/gpu/lal_base_dipole.cpp:// Reneighbor on GPU if necessary and then compute forces, virials, energies
lib/gpu/lal_base_dipole.cpp:  int inum=hd_balancer.get_gpu_count(ago,inum_full);
lib/gpu/lal_base_dipole.cpp:  // Build neighbor list on GPU if necessary
lib/gpu/lal_base_dipole.cpp:  #if defined(USE_OPENCL) && (defined(CL_VERSION_2_1) || defined(CL_VERSION_3_0))
lib/gpu/Makefile.oneapi:EXTRAMAKE = Makefile.lammps.opencl
lib/gpu/Makefile.oneapi:OCL_LINK = -L$(ONEAPI_ROOT)/compiler/latest/linux/lib -lOpenCL
lib/gpu/Makefile.oneapi:OCL_TUNE = -DMPI_GERYON -DCUDA_PROXY -DGERYON_NUMA_FISSION -DUCL_NO_EXIT -DGERYON_NO_OCL_MARKERS
lib/gpu/Makefile.oneapi:include Opencl.makefile
lib/gpu/lal_neighbor_shared.cpp:#if defined(USE_OPENCL)
lib/gpu/lal_neighbor_shared.cpp:#include "neighbor_gpu_cl.h"
lib/gpu/lal_neighbor_shared.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_neighbor_shared.cpp:const char *neighbor_gpu=0;
lib/gpu/lal_neighbor_shared.cpp:#include "neighbor_gpu_cubin.h"
lib/gpu/lal_neighbor_shared.cpp:    if (_gpu_nbor>0) {
lib/gpu/lal_neighbor_shared.cpp:      if (_gpu_nbor==1) {
lib/gpu/lal_neighbor_shared.cpp:void NeighborShared::compile_kernels(UCL_Device &dev, const int gpu_nbor,
lib/gpu/lal_neighbor_shared.cpp:  _gpu_nbor=gpu_nbor;
lib/gpu/lal_neighbor_shared.cpp:  if (_gpu_nbor==0) {
lib/gpu/lal_neighbor_shared.cpp:    build_program->load_string(neighbor_gpu,flags.c_str(),nullptr,stderr);
lib/gpu/lal_neighbor_shared.cpp:    if (_gpu_nbor==1) {
lib/gpu/lal_sw.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_sw.h:    * - -1 if fix gpu not found
lib/gpu/lal_sw.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_sw.h:           const double gpu_split, FILE *screen, double **ncutsq,
lib/gpu/lal_lj_expand_ext.cpp:                            Inderaj Bains (NVIDIA)
lib/gpu/lal_lj_expand_ext.cpp:    email                : ibains@nvidia.com
lib/gpu/lal_lj_expand_ext.cpp:int lje_gpu_init(const int ntypes, double **cutsq, double **host_lj1,
lib/gpu/lal_lj_expand_ext.cpp:                 const int maxspecial, const double cell_size, int &gpu_mode,
lib/gpu/lal_lj_expand_ext.cpp:  gpu_mode=LJEMF.device->gpu_mode();
lib/gpu/lal_lj_expand_ext.cpp:  double gpu_split=LJEMF.device->particle_split();
lib/gpu/lal_lj_expand_ext.cpp:  int first_gpu=LJEMF.device->first_device();
lib/gpu/lal_lj_expand_ext.cpp:  int last_gpu=LJEMF.device->last_device();
lib/gpu/lal_lj_expand_ext.cpp:  int gpu_rank=LJEMF.device->gpu_rank();
lib/gpu/lal_lj_expand_ext.cpp:  int procs_per_gpu=LJEMF.device->procs_per_gpu();
lib/gpu/lal_lj_expand_ext.cpp:  LJEMF.device->init_message(screen,"lj/expand",first_gpu,last_gpu);
lib/gpu/lal_lj_expand_ext.cpp:                       maxspecial, cell_size, gpu_split, screen);
lib/gpu/lal_lj_expand_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_lj_expand_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_lj_expand_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_lj_expand_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_lj_expand_ext.cpp:                last_gpu,i);
lib/gpu/lal_lj_expand_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_lj_expand_ext.cpp:                         cell_size, gpu_split,screen);
lib/gpu/lal_lj_expand_ext.cpp:    LJEMF.estimate_gpu_overhead();
lib/gpu/lal_lj_expand_ext.cpp:void lje_gpu_reinit(const int ntypes, double **cutsq, double **host_lj1,
lib/gpu/lal_lj_expand_ext.cpp:  int gpu_rank=LJEMF.device->gpu_rank();
lib/gpu/lal_lj_expand_ext.cpp:  int procs_per_gpu=LJEMF.device->procs_per_gpu();
lib/gpu/lal_lj_expand_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_lj_expand_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_lj_expand_ext.cpp:void lje_gpu_clear() {
lib/gpu/lal_lj_expand_ext.cpp:int** lje_gpu_compute_n(const int ago, const int inum_full,
lib/gpu/lal_lj_expand_ext.cpp:void lje_gpu_compute(const int ago, const int inum_full, const int nall,
lib/gpu/lal_lj_expand_ext.cpp:double lje_gpu_bytes() {
lib/gpu/Makefile.lammps.standard:# settings for Nvidia CUDA and OpenCL builds
lib/gpu/Makefile.lammps.standard:ifeq ($(CUDA_HOME),)
lib/gpu/Makefile.lammps.standard:CUDA_HOME=/usr/local/cuda
lib/gpu/Makefile.lammps.standard:gpu_SYSINC =
lib/gpu/Makefile.lammps.standard:gpu_SYSLIB =  -lcudart -lcuda -lcufft
lib/gpu/Makefile.lammps.standard:gpu_SYSPATH = -L$(CUDA_HOME)/lib64 -L$(CUDA_HOME)/lib64/stubs
lib/gpu/lal_lj_coul_msm_ext.cpp:int ljcm_gpu_init(const int ntypes, double **cutsq, double **host_lj1,
lib/gpu/lal_lj_coul_msm_ext.cpp:                 const double cell_size, int &gpu_mode, FILE *screen,
lib/gpu/lal_lj_coul_msm_ext.cpp:  gpu_mode=LJCMLMF.device->gpu_mode();
lib/gpu/lal_lj_coul_msm_ext.cpp:  double gpu_split=LJCMLMF.device->particle_split();
lib/gpu/lal_lj_coul_msm_ext.cpp:  int first_gpu=LJCMLMF.device->first_device();
lib/gpu/lal_lj_coul_msm_ext.cpp:  int last_gpu=LJCMLMF.device->last_device();
lib/gpu/lal_lj_coul_msm_ext.cpp:  int gpu_rank=LJCMLMF.device->gpu_rank();
lib/gpu/lal_lj_coul_msm_ext.cpp:  int procs_per_gpu=LJCMLMF.device->procs_per_gpu();
lib/gpu/lal_lj_coul_msm_ext.cpp:  LJCMLMF.device->init_message(screen,"lj/cut/coul/msm",first_gpu,last_gpu);
lib/gpu/lal_lj_coul_msm_ext.cpp:                        cell_size, gpu_split, screen, host_cut_ljsq,
lib/gpu/lal_lj_coul_msm_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_lj_coul_msm_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_lj_coul_msm_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_lj_coul_msm_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_lj_coul_msm_ext.cpp:                last_gpu,i);
lib/gpu/lal_lj_coul_msm_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_lj_coul_msm_ext.cpp:                          cell_size, gpu_split, screen, host_cut_ljsq,
lib/gpu/lal_lj_coul_msm_ext.cpp:    LJCMLMF.estimate_gpu_overhead();
lib/gpu/lal_lj_coul_msm_ext.cpp:void ljcm_gpu_clear() {
lib/gpu/lal_lj_coul_msm_ext.cpp:int** ljcm_gpu_compute_n(const int ago, const int inum_full,
lib/gpu/lal_lj_coul_msm_ext.cpp:void ljcm_gpu_compute(const int ago, const int inum_full, const int nall,
lib/gpu/lal_lj_coul_msm_ext.cpp:double ljcm_gpu_bytes() {
lib/gpu/lal_buck_ext.cpp:int buck_gpu_init(const int ntypes, double **cutsq, double **host_rhoinv,
lib/gpu/lal_buck_ext.cpp:                 const double cell_size, int &gpu_mode, FILE *screen) {
lib/gpu/lal_buck_ext.cpp:  gpu_mode=BUCKMF.device->gpu_mode();
lib/gpu/lal_buck_ext.cpp:  double gpu_split=BUCKMF.device->particle_split();
lib/gpu/lal_buck_ext.cpp:  int first_gpu=BUCKMF.device->first_device();
lib/gpu/lal_buck_ext.cpp:  int last_gpu=BUCKMF.device->last_device();
lib/gpu/lal_buck_ext.cpp:  int gpu_rank=BUCKMF.device->gpu_rank();
lib/gpu/lal_buck_ext.cpp:  int procs_per_gpu=BUCKMF.device->procs_per_gpu();
lib/gpu/lal_buck_ext.cpp:  BUCKMF.device->init_message(screen,"buck",first_gpu,last_gpu);
lib/gpu/lal_buck_ext.cpp:                       maxspecial, cell_size, gpu_split, screen);
lib/gpu/lal_buck_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_buck_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_buck_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_buck_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_buck_ext.cpp:                last_gpu,i);
lib/gpu/lal_buck_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_buck_ext.cpp:                       maxspecial, cell_size, gpu_split, screen);
lib/gpu/lal_buck_ext.cpp:    BUCKMF.estimate_gpu_overhead();
lib/gpu/lal_buck_ext.cpp:void buck_gpu_reinit(const int ntypes, double **cutsq, double **host_rhoinv,
lib/gpu/lal_buck_ext.cpp:  int gpu_rank=BUCKMF.device->gpu_rank();
lib/gpu/lal_buck_ext.cpp:  int procs_per_gpu=BUCKMF.device->procs_per_gpu();
lib/gpu/lal_buck_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_buck_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_buck_ext.cpp:void buck_gpu_clear() {
lib/gpu/lal_buck_ext.cpp:int ** buck_gpu_compute_n(const int ago, const int inum_full,
lib/gpu/lal_buck_ext.cpp:void buck_gpu_compute(const int ago, const int inum_full, const int nall,
lib/gpu/lal_buck_ext.cpp:double buck_gpu_bytes() {
lib/gpu/lal_charmm_long_ext.cpp:int crml_gpu_init(const int ntypes, double cut_bothsq, double **host_lj1,
lib/gpu/lal_charmm_long_ext.cpp:                  const double cell_size, int &gpu_mode, FILE *screen,
lib/gpu/lal_charmm_long_ext.cpp:  gpu_mode=CRMLMF.device->gpu_mode();
lib/gpu/lal_charmm_long_ext.cpp:  double gpu_split=CRMLMF.device->particle_split();
lib/gpu/lal_charmm_long_ext.cpp:  int first_gpu=CRMLMF.device->first_device();
lib/gpu/lal_charmm_long_ext.cpp:  int last_gpu=CRMLMF.device->last_device();
lib/gpu/lal_charmm_long_ext.cpp:  int gpu_rank=CRMLMF.device->gpu_rank();
lib/gpu/lal_charmm_long_ext.cpp:  int procs_per_gpu=CRMLMF.device->procs_per_gpu();
lib/gpu/lal_charmm_long_ext.cpp:  CRMLMF.device->init_message(screen,"lj/charmm/coul/long",first_gpu,last_gpu);
lib/gpu/lal_charmm_long_ext.cpp:                gpu_split, screen, host_cut_ljsq, host_cut_coulsq,
lib/gpu/lal_charmm_long_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_charmm_long_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_charmm_long_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_charmm_long_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_charmm_long_ext.cpp:                last_gpu,i);
lib/gpu/lal_charmm_long_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_charmm_long_ext.cpp:                          maxspecial, cell_size, gpu_split, screen,
lib/gpu/lal_charmm_long_ext.cpp:    CRMLMF.estimate_gpu_overhead();
lib/gpu/lal_charmm_long_ext.cpp:void crml_gpu_clear() {
lib/gpu/lal_charmm_long_ext.cpp:int** crml_gpu_compute_n(const int ago, const int inum_full,
lib/gpu/lal_charmm_long_ext.cpp:void crml_gpu_compute(const int ago, const int inum_full,
lib/gpu/lal_charmm_long_ext.cpp:double crml_gpu_bytes() {
lib/gpu/lal_born_coul_wolf.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_born_coul_wolf.h:    * - -1 if fix gpu not found
lib/gpu/lal_born_coul_wolf.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_born_coul_wolf.h:           const double gpu_split, FILE *screen, double **host_cut_ljsq,
lib/gpu/lal_lj_tip4p_long.cpp:#if defined(USE_OPENCL)
lib/gpu/lal_lj_tip4p_long.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_lj_tip4p_long.cpp:    const double gpu_split, FILE *_screen,
lib/gpu/lal_lj_tip4p_long.cpp:  success=this->init_atomic(nlocal,nall,max_nbors,maxspecial,cell_size,gpu_split,
lib/gpu/lal_lj_tip4p_long.cpp:  this->device->gpu->sync();
lib/gpu/lal_lj_tip4p_long.cpp:// Reneighbor on GPU if necessary and then compute forces, virials, energies
lib/gpu/lal_lj_tip4p_long.cpp:  int inum=this->hd_balancer.get_gpu_count(ago,inum_full);
lib/gpu/lal_lj_tip4p_long.cpp:  // Build neighbor list on GPU if necessary
lib/gpu/lal_lj_coul_msm.cpp:#if defined(USE_OPENCL)
lib/gpu/lal_lj_coul_msm.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_lj_coul_msm.cpp:                     const double gpu_split, FILE *_screen,
lib/gpu/lal_lj_coul_msm.cpp:  success=this->init_atomic(nlocal,nall,max_nbors,maxspecial,cell_size,gpu_split,
lib/gpu/lal_base_amoeba.cpp:  #if 0 // !defined(USE_OPENCL) && !defined(USE_HIP)
lib/gpu/lal_base_amoeba.cpp:                             const double cell_size, const double gpu_split,
lib/gpu/lal_base_amoeba.cpp:  int gpu_nbor=0;
lib/gpu/lal_base_amoeba.cpp:  if (device->gpu_mode()==Device<numtyp,acctyp>::GPU_NEIGH)
lib/gpu/lal_base_amoeba.cpp:    gpu_nbor=1;
lib/gpu/lal_base_amoeba.cpp:  else if (device->gpu_mode()==Device<numtyp,acctyp>::GPU_HYB_NEIGH)
lib/gpu/lal_base_amoeba.cpp:    gpu_nbor=2;
lib/gpu/lal_base_amoeba.cpp:  int _gpu_host=0;
lib/gpu/lal_base_amoeba.cpp:  int host_nlocal=hd_balancer.first_host_count(nlocal,gpu_split,gpu_nbor);
lib/gpu/lal_base_amoeba.cpp:    _gpu_host=1;
lib/gpu/lal_base_amoeba.cpp:  if (ucl_device!=device->gpu) _compiled=false;
lib/gpu/lal_base_amoeba.cpp:  ucl_device=device->gpu;
lib/gpu/lal_base_amoeba.cpp:  if (_threads_per_atom>1 && gpu_nbor==0) {
lib/gpu/lal_base_amoeba.cpp:                              _gpu_host,max_nbors,cell_size,alloc_packed,
lib/gpu/lal_base_amoeba.cpp:  hd_balancer.init(device,gpu_nbor,gpu_split);
lib/gpu/lal_base_amoeba.cpp:  // Initialize timers for the selected GPU
lib/gpu/lal_base_amoeba.cpp:  _max_an_bytes=ans->gpu_bytes()+nbor->gpu_bytes();
lib/gpu/lal_base_amoeba.cpp:  #if 0 // !defined(USE_OPENCL) && !defined(USE_HIP)
lib/gpu/lal_base_amoeba.cpp:void BaseAmoebaT::estimate_gpu_overhead(const int add_kernels) {
lib/gpu/lal_base_amoeba.cpp:  device->estimate_gpu_overhead(1+add_kernels,_gpu_overhead,_driver_overhead);
lib/gpu/lal_base_amoeba.cpp:  _gpu_overhead*=hd_balancer.timestep();
lib/gpu/lal_base_amoeba.cpp:                       _gpu_overhead,_driver_overhead,_threads_per_atom,screen);
lib/gpu/lal_base_amoeba.cpp:  double bytes=ans->gpu_bytes()+nbor->gpu_bytes();
lib/gpu/lal_base_amoeba.cpp:  double bytes=ans->gpu_bytes()+nbor->gpu_bytes();
lib/gpu/lal_base_amoeba.cpp:  int inum=hd_balancer.get_gpu_count(ago,inum_full);
lib/gpu/lal_base_amoeba.cpp:  // Build neighbor list on GPU if necessary
lib/gpu/lal_base_amoeba.cpp://   this is the first part in a time step done on the GPU for AMOEBA for now
lib/gpu/lal_base_amoeba.cpp:// Reneighbor on GPU if necessary, and then compute the direct real space part
lib/gpu/lal_base_amoeba.cpp:// Reneighbor on GPU if necessary, and then compute the direct real space part
lib/gpu/lal_base_amoeba.cpp:  //       after umutual1 and self are done on the GPU
lib/gpu/lal_base_amoeba.cpp:// Reneighbor on GPU if necessary, and then compute polar real-space
lib/gpu/lal_base_amoeba.cpp:  #if 0 // !defined(USE_OPENCL) && !defined(USE_HIP)
lib/gpu/lal_base_amoeba.cpp:  #if defined(USE_OPENCL) && (defined(CL_VERSION_2_1) || defined(CL_VERSION_3_0))
lib/gpu/lal_dipole_lj_ext.cpp:int dpl_gpu_init(const int ntypes, double **cutsq, double **host_lj1,
lib/gpu/lal_dipole_lj_ext.cpp:                 const double cell_size, int &gpu_mode, FILE *screen,
lib/gpu/lal_dipole_lj_ext.cpp:  gpu_mode=DPLMF.device->gpu_mode();
lib/gpu/lal_dipole_lj_ext.cpp:  double gpu_split=DPLMF.device->particle_split();
lib/gpu/lal_dipole_lj_ext.cpp:  int first_gpu=DPLMF.device->first_device();
lib/gpu/lal_dipole_lj_ext.cpp:  int last_gpu=DPLMF.device->last_device();
lib/gpu/lal_dipole_lj_ext.cpp:  int gpu_rank=DPLMF.device->gpu_rank();
lib/gpu/lal_dipole_lj_ext.cpp:  int procs_per_gpu=DPLMF.device->procs_per_gpu();
lib/gpu/lal_dipole_lj_ext.cpp:  DPLMF.device->init_message(screen,"dipole/cut",first_gpu,last_gpu);
lib/gpu/lal_dipole_lj_ext.cpp:                       maxspecial, cell_size, gpu_split, screen, host_cut_ljsq,
lib/gpu/lal_dipole_lj_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_dipole_lj_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_dipole_lj_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_dipole_lj_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_dipole_lj_ext.cpp:                last_gpu,i);
lib/gpu/lal_dipole_lj_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_dipole_lj_ext.cpp:                         cell_size, gpu_split, screen, host_cut_ljsq,
lib/gpu/lal_dipole_lj_ext.cpp:    DPLMF.estimate_gpu_overhead();
lib/gpu/lal_dipole_lj_ext.cpp:void dpl_gpu_clear() {
lib/gpu/lal_dipole_lj_ext.cpp:int** dpl_gpu_compute_n(const int ago, const int inum_full,
lib/gpu/lal_dipole_lj_ext.cpp:void dpl_gpu_compute(const int ago, const int inum_full, const int nall,
lib/gpu/lal_dipole_lj_ext.cpp:double dpl_gpu_bytes() {
lib/gpu/lal_device.cpp:#if defined(USE_OPENCL)
lib/gpu/lal_device.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_device.cpp:                    _gpu_mode(GPU_FORCE), _first_device(0),
lib/gpu/lal_device.cpp:int DeviceT::init_device(MPI_Comm /*world*/, MPI_Comm replica, const int ngpu,
lib/gpu/lal_device.cpp:                         const int first_gpu_id, const int gpu_mode,
lib/gpu/lal_device.cpp:  int ndevices=ngpu;
lib/gpu/lal_device.cpp:  _first_device=first_gpu_id;
lib/gpu/lal_device.cpp:  _gpu_mode=gpu_mode;
lib/gpu/lal_device.cpp:  // support selecting OpenCL platform id with "package platform" keyword
lib/gpu/lal_device.cpp:  gpu=new UCL_Device();
lib/gpu/lal_device.cpp:  // ---------------------- OpenCL Compiler Args -------------------------
lib/gpu/lal_device.cpp:  // Setup OpenCL platform and parameters based on platform
lib/gpu/lal_device.cpp:  // Setup the OpenCL platform
lib/gpu/lal_device.cpp:  // device type. Give preference to platforms with GPUs.
lib/gpu/lal_device.cpp:  enum UCL_DEVICE_TYPE type=UCL_GPU;
lib/gpu/lal_device.cpp:  #ifndef USE_OPENCL
lib/gpu/lal_device.cpp:  pres=gpu->set_platform(0);
lib/gpu/lal_device.cpp:    pres=gpu->set_platform(_platform_id);
lib/gpu/lal_device.cpp:      if (ocl_vstring=="intelgpu")
lib/gpu/lal_device.cpp:      } else if (ocl_vstring=="nvidiagpu")
lib/gpu/lal_device.cpp:        vendor="nvidia";
lib/gpu/lal_device.cpp:      else if (ocl_vstring=="amdgpu")
lib/gpu/lal_device.cpp:      else if (ocl_vstring=="applegpu")
lib/gpu/lal_device.cpp:    pres=gpu->auto_set_platform(type,vendor,ndevices,_first_device);
lib/gpu/lal_device.cpp:  if (_first_device > -1 && _first_device >= gpu->num_devices())
lib/gpu/lal_device.cpp:  if (ndevices > gpu->num_devices())
lib/gpu/lal_device.cpp:  if (_first_device + ndevices > gpu->num_devices())
lib/gpu/lal_device.cpp:  if (gpu->num_devices()==0)
lib/gpu/lal_device.cpp:    unsigned best_cus = gpu->cus(0);
lib/gpu/lal_device.cpp:    bool type_match = (gpu->device_type(0) == type);
lib/gpu/lal_device.cpp:    for (int i = 1; i < gpu->num_devices(); i++) {
lib/gpu/lal_device.cpp:      if (type_match && (gpu->device_type(i) != type))
lib/gpu/lal_device.cpp:      if (!type_match && (gpu->device_type(i) == type)) {
lib/gpu/lal_device.cpp:        best_cus = gpu->cus(i);
lib/gpu/lal_device.cpp:      if (gpu->cus(i) > best_cus) {
lib/gpu/lal_device.cpp:        best_cus = gpu->cus(i);
lib/gpu/lal_device.cpp:    type = gpu->device_type(_first_device);
lib/gpu/lal_device.cpp:        if (_last_device + 1 == gpu->num_devices())
lib/gpu/lal_device.cpp:          if (gpu->device_type(_last_device+1)==type &&
lib/gpu/lal_device.cpp:              gpu->device_type(_first_device-1)!=type)
lib/gpu/lal_device.cpp:          else if (gpu->device_type(_last_device+1)!=type &&
lib/gpu/lal_device.cpp:                   gpu->device_type(_first_device-1)==type)
lib/gpu/lal_device.cpp:          else if (gpu->cus(_last_device+1) > gpu->cus(_first_device-1))
lib/gpu/lal_device.cpp:  // If ngpus not specified, expand range to include matching devices
lib/gpu/lal_device.cpp:    for (int i = _first_device; i < gpu->num_devices(); i++) {
lib/gpu/lal_device.cpp:      if (gpu->device_type(i)==gpu->device_type(_first_device) &&
lib/gpu/lal_device.cpp:          gpu->cus(i)==gpu->cus(_first_device))
lib/gpu/lal_device.cpp:  _procs_per_gpu=static_cast<int>(ceil(static_cast<double>(procs_per_node)/
lib/gpu/lal_device.cpp:  int my_gpu=node_rank/_procs_per_gpu+_first_device;
lib/gpu/lal_device.cpp:  // Time on the device only if 1 proc per gpu
lib/gpu/lal_device.cpp:  if (_procs_per_gpu>1)
lib/gpu/lal_device.cpp:    gpu->configure_profiling(false);
lib/gpu/lal_device.cpp:  MPI_Comm_split(node_comm,my_gpu,0,&_comm_gpu);
lib/gpu/lal_device.cpp:  MPI_Comm_rank(_comm_gpu,&_gpu_rank);
lib/gpu/lal_device.cpp:  #if !defined(CUDA_MPS_SUPPORT)
lib/gpu/lal_device.cpp:  if (_procs_per_gpu>1 && !gpu->sharing_supported(my_gpu))
lib/gpu/lal_device.cpp:  if (gpu->set(my_gpu)!=UCL_SUCCESS)
lib/gpu/lal_device.cpp:  #if !defined(USE_OPENCL) && !defined(USE_HIP)
lib/gpu/lal_device.cpp:  if (gpu->arch()<7.0) {
lib/gpu/lal_device.cpp:    gpu->push_command_queue();
lib/gpu/lal_device.cpp:    gpu->set_command_queue(1);
lib/gpu/lal_device.cpp:  // If OpenCL parameters not specified by user, try to auto detect
lib/gpu/lal_device.cpp:  #ifdef USE_OPENCL
lib/gpu/lal_device.cpp:    std::string pname = gpu->platform_name();
lib/gpu/lal_device.cpp:    if (pname.find("NVIDIA")!=std::string::npos)
lib/gpu/lal_device.cpp:      ocl_vstring="nvidiagpu";
lib/gpu/lal_device.cpp:      if (gpu->device_type()==UCL_GPU)
lib/gpu/lal_device.cpp:        ocl_vstring="intelgpu";
lib/gpu/lal_device.cpp:      else if (gpu->device_type()==UCL_CPU)
lib/gpu/lal_device.cpp:      if (gpu->device_type()==UCL_GPU)
lib/gpu/lal_device.cpp:        ocl_vstring="amdgpu";
lib/gpu/lal_device.cpp:      if (gpu->device_type()==UCL_GPU)
lib/gpu/lal_device.cpp:        ocl_vstring="applegpu";
lib/gpu/lal_device.cpp:  for (int i=0; i<_procs_per_gpu; i++) {
lib/gpu/lal_device.cpp:    if (_gpu_rank==i)
lib/gpu/lal_device.cpp:  if (!gpu->double_precision())
lib/gpu/lal_device.cpp:  #ifdef USE_OPENCL
lib/gpu/lal_device.cpp:    // workaround for double precision with Intel OpenCL
lib/gpu/lal_device.cpp:    UCL_Program ptest(*gpu);
lib/gpu/lal_device.cpp:    UCL_Program ptest(*gpu);
lib/gpu/lal_device.cpp:  if (gpu->has_subgroup_support())
lib/gpu/lal_device.cpp:    _ocl_compile_string+=" -DUSE_OPENCL_SUBGROUPS";
lib/gpu/lal_device.cpp:  if (!gpu->has_shuffle_support())
lib/gpu/lal_device.cpp:  if (sizeof(acctyp)==sizeof(double) && !gpu->double_precision())
lib/gpu/lal_device.cpp:  int gpu_nbor=0;
lib/gpu/lal_device.cpp:  if (_gpu_mode==Device<numtyp,acctyp>::GPU_NEIGH)
lib/gpu/lal_device.cpp:    gpu_nbor=1;
lib/gpu/lal_device.cpp:  else if (_gpu_mode==Device<numtyp,acctyp>::GPU_HYB_NEIGH)
lib/gpu/lal_device.cpp:    gpu_nbor=2;
lib/gpu/lal_device.cpp:  if (gpu_nbor==1) gpu_nbor=2;
lib/gpu/lal_device.cpp:  if (gpu_nbor==1) gpu_nbor=2;
lib/gpu/lal_device.cpp:    if (!atom.init(nall,charge,rot,*gpu,gpu_nbor,gpu_nbor>0 && maxspecial>0,vel,extra_fields))
lib/gpu/lal_device.cpp:    if (!atom.add_fields(charge,rot,gpu_nbor,gpu_nbor>0 && maxspecial,vel,extra_fields))
lib/gpu/lal_device.cpp:  if (!ans.init(ef_nlocal,charge,rot,*gpu))
lib/gpu/lal_device.cpp:  if (sizeof(acctyp)==sizeof(double) && !gpu->double_precision())
lib/gpu/lal_device.cpp:    if (!atom.init(nall,true,false,*gpu,false,false))
lib/gpu/lal_device.cpp:  if (!ans.init(nlocal,true,false,*gpu))
lib/gpu/lal_device.cpp:                       const int maxspecial, const int gpu_host,
lib/gpu/lal_device.cpp:  int gpu_nbor=0;
lib/gpu/lal_device.cpp:  if (_gpu_mode==Device<numtyp,acctyp>::GPU_NEIGH)
lib/gpu/lal_device.cpp:    gpu_nbor=1;
lib/gpu/lal_device.cpp:  else if (_gpu_mode==Device<numtyp,acctyp>::GPU_HYB_NEIGH)
lib/gpu/lal_device.cpp:    gpu_nbor=2;
lib/gpu/lal_device.cpp:  if (gpu_nbor==1)
lib/gpu/lal_device.cpp:    gpu_nbor=2;
lib/gpu/lal_device.cpp:  if (gpu_nbor==1)
lib/gpu/lal_device.cpp:    gpu_nbor=2;
lib/gpu/lal_device.cpp:                  *gpu,gpu_nbor,gpu_host,pre_cut,_block_cell_2d,
lib/gpu/lal_device.cpp:                     (PPPM<numtyp,acctyp,float,_lgpu_float4> *pppm) {
lib/gpu/lal_device.cpp:                     (PPPM<numtyp,acctyp,double,_lgpu_double4> *pppm) {
lib/gpu/lal_device.cpp:                           const int first_gpu, const int last_gpu) {
lib/gpu/lal_device.cpp:  #if defined(USE_OPENCL)
lib/gpu/lal_device.cpp:  #elif defined(USE_CUDART)
lib/gpu/lal_device.cpp:  std::string fs=toa(gpu->free_gigabytes())+"/";
lib/gpu/lal_device.cpp:    fprintf(screen,"-  with %d proc(s) per device.\n",_procs_per_gpu);
lib/gpu/lal_device.cpp:    #ifdef USE_OPENCL
lib/gpu/lal_device.cpp:    fprintf(screen,"-  with OpenCL Parameters for: %s (%d)\n",
lib/gpu/lal_device.cpp:    if (gpu->shared_memory(first_gpu))
lib/gpu/lal_device.cpp:    int last=last_gpu+1;
lib/gpu/lal_device.cpp:    if (last>gpu->num_devices())
lib/gpu/lal_device.cpp:      last=gpu->num_devices();
lib/gpu/lal_device.cpp:    if (gpu->num_platforms()>1) {
lib/gpu/lal_device.cpp:      std::string pname=gpu->platform_name();
lib/gpu/lal_device.cpp:    for (int i=first_gpu; i<last; i++) {
lib/gpu/lal_device.cpp:      if (i==first_gpu)
lib/gpu/lal_device.cpp:        sname=gpu->name(i)+", "+toa(gpu->cus(i))+" CUs, "+fs+
lib/gpu/lal_device.cpp:              toa(gpu->gigabytes(i))+" GB, "+toa(gpu->clock_rate(i))+" GHZ (";
lib/gpu/lal_device.cpp:        sname=gpu->name(i)+", "+toa(gpu->cus(i))+" CUs, "+
lib/gpu/lal_device.cpp:              toa(gpu->clock_rate(i))+" GHZ (";
lib/gpu/lal_device.cpp:void DeviceT::estimate_gpu_overhead(const int kernel_calls,
lib/gpu/lal_device.cpp:                                    double &gpu_overhead,
lib/gpu/lal_device.cpp:                                    double &gpu_driver_overhead) {
lib/gpu/lal_device.cpp:  UCL_Timer over_timer(*gpu);
lib/gpu/lal_device.cpp:    host_data_in[i].alloc(1,*gpu);
lib/gpu/lal_device.cpp:    dev_data_in[i].alloc(1,*gpu);
lib/gpu/lal_device.cpp:    timers_in[i].init(*gpu);
lib/gpu/lal_device.cpp:    host_data_out[i].alloc(1,*gpu);
lib/gpu/lal_device.cpp:    dev_data_out[i].alloc(1,*gpu);
lib/gpu/lal_device.cpp:    timers_out[i].init(*gpu);
lib/gpu/lal_device.cpp:    kernel_data[i].alloc(1,*gpu);
lib/gpu/lal_device.cpp:    timers_kernel[i].init(*gpu);
lib/gpu/lal_device.cpp:  gpu_overhead=0.0;
lib/gpu/lal_device.cpp:  gpu_driver_overhead=0.0;
lib/gpu/lal_device.cpp:  // The following estimation currently fails on Intel GPUs
lib/gpu/lal_device.cpp:  // that do not support double precision with OpenCL error code -5.
lib/gpu/lal_device.cpp:  if (!gpu->double_precision()) zloops = 0;
lib/gpu/lal_device.cpp:    gpu->sync();
lib/gpu/lal_device.cpp:    gpu_barrier();
lib/gpu/lal_device.cpp:    gpu->sync();
lib/gpu/lal_device.cpp:    gpu_barrier();
lib/gpu/lal_device.cpp:    MPI_Allreduce(&time,&mpi_time,1,MPI_DOUBLE,MPI_MAX,gpu_comm());
lib/gpu/lal_device.cpp:                  gpu_comm());
lib/gpu/lal_device.cpp:      gpu_overhead+=mpi_time;
lib/gpu/lal_device.cpp:      gpu_driver_overhead+=mpi_driver_time;
lib/gpu/lal_device.cpp:  gpu_overhead/=10.0;
lib/gpu/lal_device.cpp:  gpu_driver_overhead/=10.0;
lib/gpu/lal_device.cpp:  gpu->sync();
lib/gpu/lal_device.cpp:                           const double max_bytes, const double gpu_overhead,
lib/gpu/lal_device.cpp:  single[5]=gpu_overhead;
lib/gpu/lal_device.cpp:  double my_max_bytes=max_bytes+atom.max_gpu_bytes();
lib/gpu/lal_device.cpp:  #ifdef USE_OPENCL
lib/gpu/lal_device.cpp:  // Workaround for timing issue on Intel OpenCL
lib/gpu/lal_device.cpp:        if (nbor.gpu_nbor() > 0.0)
lib/gpu/lal_device.cpp:      if (nbor.gpu_nbor()==2)
lib/gpu/lal_device.cpp:      if (nbor.gpu_nbor()==2) {
lib/gpu/lal_device.cpp:      } else if (nbor.gpu_nbor()==1) {
lib/gpu/lal_device.cpp:      } else if (nbor.gpu_nbor()==0)
lib/gpu/lal_device.cpp:  double my_max_bytes=max_bytes+atom.max_gpu_bytes();
lib/gpu/lal_device.cpp:  #ifdef USE_OPENCL
lib/gpu/lal_device.cpp:  // Workaround for timing issue on Intel OpenCL
lib/gpu/lal_device.cpp:    delete gpu;
lib/gpu/lal_device.cpp:  dev_program=new UCL_Program(*gpu);
lib/gpu/lal_device.cpp:  UCL_Vector<int,int> gpu_lib_data(20,*gpu,UCL_NOT_PINNED);
lib/gpu/lal_device.cpp:  k_info.run(&gpu_lib_data);
lib/gpu/lal_device.cpp:  gpu_lib_data.update_host(false);
lib/gpu/lal_device.cpp:  _ptx_arch=static_cast<double>(gpu_lib_data[0])/100.0;
lib/gpu/lal_device.cpp:  #if !(defined(USE_OPENCL) || defined(USE_HIP))
lib/gpu/lal_device.cpp:  if (_ptx_arch>gpu->arch() || floor(_ptx_arch)<floor(gpu->arch()))
lib/gpu/lal_device.cpp:  _config_id=gpu_lib_data[1];
lib/gpu/lal_device.cpp:    _simd_size=std::max(gpu_lib_data[2],gpu->preferred_fp32_width());
lib/gpu/lal_device.cpp:    _simd_size=std::max(gpu_lib_data[2],gpu->preferred_fp64_width());
lib/gpu/lal_device.cpp:  _num_mem_threads=gpu_lib_data[3];
lib/gpu/lal_device.cpp:  _shuffle_avail=gpu_lib_data[4];
lib/gpu/lal_device.cpp:  _fast_math=gpu_lib_data[5];
lib/gpu/lal_device.cpp:    _threads_per_atom=gpu_lib_data[6];
lib/gpu/lal_device.cpp:    _threads_per_charge=gpu_lib_data[7];
lib/gpu/lal_device.cpp:    _threads_per_three=gpu_lib_data[8];
lib/gpu/lal_device.cpp:    _block_pair=gpu_lib_data[9];
lib/gpu/lal_device.cpp:    _block_bio_pair=gpu_lib_data[10];
lib/gpu/lal_device.cpp:    _block_ellipse=gpu_lib_data[11];
lib/gpu/lal_device.cpp:  _pppm_block=gpu_lib_data[12];
lib/gpu/lal_device.cpp:  _block_nbor_build=gpu_lib_data[13];
lib/gpu/lal_device.cpp:  _block_cell_2d=gpu_lib_data[14];
lib/gpu/lal_device.cpp:  _block_cell_id=gpu_lib_data[15];
lib/gpu/lal_device.cpp:  _max_shared_types=gpu_lib_data[16];
lib/gpu/lal_device.cpp:  _max_bio_shared_types=gpu_lib_data[17];
lib/gpu/lal_device.cpp:  _pppm_max_spline=gpu_lib_data[18];
lib/gpu/lal_device.cpp:  if (static_cast<size_t>(_block_pair) > gpu->group_size_dim(0) ||
lib/gpu/lal_device.cpp:      static_cast<size_t>(_block_bio_pair) > gpu->group_size_dim(0) ||
lib/gpu/lal_device.cpp:      static_cast<size_t>(_block_ellipse) > gpu->group_size_dim(0) ||
lib/gpu/lal_device.cpp:      static_cast<size_t>(_pppm_block) > gpu->group_size_dim(0) ||
lib/gpu/lal_device.cpp:      static_cast<size_t>(_block_nbor_build) > gpu->group_size_dim(0) ||
lib/gpu/lal_device.cpp:      static_cast<size_t>(_block_cell_2d) > gpu->group_size_dim(0) ||
lib/gpu/lal_device.cpp:      static_cast<size_t>(_block_cell_2d) > gpu->group_size_dim(1) ||
lib/gpu/lal_device.cpp:      static_cast<size_t>(_block_cell_id) > gpu->group_size_dim(0) ||
lib/gpu/lal_device.cpp:      static_cast<size_t>(_max_shared_types*_max_shared_types*sizeof(numtyp)*17 > gpu->slm_size()) ||
lib/gpu/lal_device.cpp:      static_cast<size_t>(_max_bio_shared_types*2*sizeof(numtyp) > gpu->slm_size()))
lib/gpu/lal_device.cpp:// check if a suitable GPU is present.
lib/gpu/lal_device.cpp:// for mixed and double precision GPU library compilation
lib/gpu/lal_device.cpp:// also the GPU needs to support double precision.
lib/gpu/lal_device.cpp:bool lmp_has_compatible_gpu_device()
lib/gpu/lal_device.cpp:  UCL_Device gpu;
lib/gpu/lal_device.cpp:  bool compatible_gpu = gpu.num_platforms() > 0;
lib/gpu/lal_device.cpp:  if (compatible_gpu && !gpu.double_precision(0))
lib/gpu/lal_device.cpp:    compatible_gpu = false;
lib/gpu/lal_device.cpp:  return compatible_gpu;
lib/gpu/lal_device.cpp:std::string lmp_gpu_device_info()
lib/gpu/lal_device.cpp:  UCL_Device gpu;
lib/gpu/lal_device.cpp:  if (gpu.num_platforms() > 0)
lib/gpu/lal_device.cpp:    gpu.print_all(out);
lib/gpu/lal_device.cpp:int lmp_init_device(MPI_Comm world, MPI_Comm replica, const int ngpu,
lib/gpu/lal_device.cpp:                    const int first_gpu_id, const int gpu_mode,
lib/gpu/lal_device.cpp:                    const double user_cell_size, char *opencl_config,
lib/gpu/lal_device.cpp:  return global_device.init_device(world,replica,ngpu,first_gpu_id,gpu_mode,
lib/gpu/lal_device.cpp:                                   opencl_config,ocl_platform,
lib/gpu/lal_device.cpp:double lmp_gpu_forces(double **f, double **tor, double *eatom, double **vatom,
lib/gpu/lal_device.cpp:  return global_device.fix_gpu(f,tor,eatom,vatom,virial,ecoul,error_flag);
lib/gpu/lal_device.cpp:double lmp_gpu_update_bin_size(const double subx, const double suby,
lib/gpu/lal_device.cpp:bool lmp_gpu_config(const std::string &category, const std::string &setting)
lib/gpu/lal_device.cpp:#if defined(USE_OPENCL)
lib/gpu/lal_device.cpp:    return setting == "opencl";
lib/gpu/lal_device.cpp:#elif defined(USE_CUDA) || defined(USE_CUDART)
lib/gpu/lal_device.cpp:    return setting == "cuda";
lib/gpu/lal_mie.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_mie.h:    * - -1 if fix gpu not found
lib/gpu/lal_mie.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_mie.h:           const double gpu_split, FILE *screen);
lib/gpu/lal_lj_spica.cpp:#if defined(USE_OPENCL)
lib/gpu/lal_lj_spica.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_lj_spica.cpp:                          const double gpu_split, FILE *_screen) {
lib/gpu/lal_lj_spica.cpp:  success=this->init_atomic(nlocal,nall,max_nbors,maxspecial,cell_size,gpu_split,
lib/gpu/lal_re_squared.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_re_squared.h:    * - -1 if fix gpu not found
lib/gpu/lal_re_squared.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_re_squared.h:           const double gpu_split, FILE *screen);
lib/gpu/lal_coul_debye.cpp:#ifdef USE_OPENCL
lib/gpu/lal_coul_debye.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_coul_debye.cpp:                     const double gpu_split, FILE *_screen,
lib/gpu/lal_coul_debye.cpp:  success=this->init_atomic(nlocal,nall,max_nbors,maxspecial,cell_size,gpu_split,
lib/gpu/lal_mie.cpp:#ifdef USE_OPENCL
lib/gpu/lal_mie.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_mie.cpp:               const double gpu_split, FILE *_screen) {
lib/gpu/lal_mie.cpp:  success=this->init_atomic(nlocal,nall,max_nbors,maxspecial,cell_size,gpu_split,
lib/gpu/Makefile.mac_opencl_mpi:#  Generic Mac Makefile for OpenCL - Single precision with FFT_SINGLE
lib/gpu/Makefile.mac_opencl_mpi:OCL_LINK = -framework OpenCL
lib/gpu/Makefile.mac_opencl_mpi:include Opencl.makefile
lib/gpu/lal_lj_expand_coul_long.cpp:#if defined(USE_OPENCL)
lib/gpu/lal_lj_expand_coul_long.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_lj_expand_coul_long.cpp:                           const double gpu_split, FILE *_screen,
lib/gpu/lal_lj_expand_coul_long.cpp:  success=this->init_atomic(nlocal,nall,max_nbors,maxspecial,cell_size,gpu_split,
lib/gpu/lal_tersoff.cu:                                  const int gpu_nbor) {
lib/gpu/lal_tersoff.cu:      if (gpu_nbor) nbor_k=j+nbor_pitch;
lib/gpu/lal_tersoff.cu:                                    const int gpu_nbor) {
lib/gpu/lal_tersoff.cu:      if (gpu_nbor) nbor_k=j+nbor_pitch;
lib/gpu/lal_mdpd_ext.cpp:int mdpd_gpu_init(const int ntypes, double **cutsq,
lib/gpu/lal_mdpd_ext.cpp:                  const double cell_size, int &gpu_mode, FILE *screen) {
lib/gpu/lal_mdpd_ext.cpp:  gpu_mode=MDPDMF.device->gpu_mode();
lib/gpu/lal_mdpd_ext.cpp:  double gpu_split=MDPDMF.device->particle_split();
lib/gpu/lal_mdpd_ext.cpp:  int first_gpu=MDPDMF.device->first_device();
lib/gpu/lal_mdpd_ext.cpp:  int last_gpu=MDPDMF.device->last_device();
lib/gpu/lal_mdpd_ext.cpp:  int gpu_rank=MDPDMF.device->gpu_rank();
lib/gpu/lal_mdpd_ext.cpp:  int procs_per_gpu=MDPDMF.device->procs_per_gpu();
lib/gpu/lal_mdpd_ext.cpp:  MDPDMF.device->init_message(screen,"mdpd",first_gpu,last_gpu);
lib/gpu/lal_mdpd_ext.cpp:                        maxspecial, cell_size, gpu_split, screen);
lib/gpu/lal_mdpd_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_mdpd_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_mdpd_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_mdpd_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_mdpd_ext.cpp:                last_gpu,i);
lib/gpu/lal_mdpd_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_mdpd_ext.cpp:                          maxspecial, cell_size, gpu_split, screen);
lib/gpu/lal_mdpd_ext.cpp:    MDPDMF.estimate_gpu_overhead();
lib/gpu/lal_mdpd_ext.cpp:void mdpd_gpu_clear() {
lib/gpu/lal_mdpd_ext.cpp:int ** mdpd_gpu_compute_n(const int ago, const int inum_full, const int nall,
lib/gpu/lal_mdpd_ext.cpp:void mdpd_gpu_compute(const int ago, const int inum_full, const int nall,
lib/gpu/lal_mdpd_ext.cpp:void mdpd_gpu_get_extra_data(double *host_rho) {
lib/gpu/lal_mdpd_ext.cpp:double mdpd_gpu_bytes() {
lib/gpu/lal_re_squared_ext.cpp:int re_gpu_init(const int ntypes, double **shape, double **well, double **cutsq,
lib/gpu/lal_re_squared_ext.cpp:                const double cell_size, int &gpu_mode, FILE *screen) {
lib/gpu/lal_re_squared_ext.cpp:  gpu_mode=REMF.device->gpu_mode();
lib/gpu/lal_re_squared_ext.cpp:  double gpu_split=REMF.device->particle_split();
lib/gpu/lal_re_squared_ext.cpp:  int first_gpu=REMF.device->first_device();
lib/gpu/lal_re_squared_ext.cpp:  int last_gpu=REMF.device->last_device();
lib/gpu/lal_re_squared_ext.cpp:  int gpu_rank=REMF.device->gpu_rank();
lib/gpu/lal_re_squared_ext.cpp:  int procs_per_gpu=REMF.device->procs_per_gpu();
lib/gpu/lal_re_squared_ext.cpp:  REMF.device->init_message(screen,"resquared",first_gpu,last_gpu);
lib/gpu/lal_re_squared_ext.cpp:                      gpu_split, screen);
lib/gpu/lal_re_squared_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_re_squared_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_re_squared_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_re_squared_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_re_squared_ext.cpp:                last_gpu,i);
lib/gpu/lal_re_squared_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_re_squared_ext.cpp:                        max_nbors, maxspecial, cell_size, gpu_split, screen);
lib/gpu/lal_re_squared_ext.cpp:    REMF.estimate_gpu_overhead();
lib/gpu/lal_re_squared_ext.cpp:void re_gpu_clear() {
lib/gpu/lal_re_squared_ext.cpp:int** re_gpu_compute_n(const int ago, const int inum_full, const int nall,
lib/gpu/lal_re_squared_ext.cpp:int * re_gpu_compute(const int ago, const int inum_full, const int nall,
lib/gpu/lal_re_squared_ext.cpp:double re_gpu_bytes() {
lib/gpu/lal_coul_dsf.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_coul_dsf.h:    * - -1 if fix gpu not found
lib/gpu/lal_coul_dsf.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_coul_dsf.h:           const double cell_size, const double gpu_split, FILE *screen,
lib/gpu/lal_lj_coul.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_lj_coul.h:    * - -1 if fix gpu not found
lib/gpu/lal_lj_coul.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_lj_coul.h:           const double gpu_split, FILE *screen, double **host_cut_ljsq,
lib/gpu/lal_beck_ext.cpp:int beck_gpu_init(const int ntypes, double **cutsq, double **aa,
lib/gpu/lal_beck_ext.cpp:                  const double cell_size, int &gpu_mode, FILE *screen) {
lib/gpu/lal_beck_ext.cpp:  gpu_mode=BLMF.device->gpu_mode();
lib/gpu/lal_beck_ext.cpp:  double gpu_split=BLMF.device->particle_split();
lib/gpu/lal_beck_ext.cpp:  int first_gpu=BLMF.device->first_device();
lib/gpu/lal_beck_ext.cpp:  int last_gpu=BLMF.device->last_device();
lib/gpu/lal_beck_ext.cpp:  int gpu_rank=BLMF.device->gpu_rank();
lib/gpu/lal_beck_ext.cpp:  int procs_per_gpu=BLMF.device->procs_per_gpu();
lib/gpu/lal_beck_ext.cpp:  BLMF.device->init_message(screen,"beck",first_gpu,last_gpu);
lib/gpu/lal_beck_ext.cpp:                      maxspecial, cell_size, gpu_split, screen);
lib/gpu/lal_beck_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_beck_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_beck_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_beck_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_beck_ext.cpp:                last_gpu,i);
lib/gpu/lal_beck_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_beck_ext.cpp:                        cell_size, gpu_split, screen);
lib/gpu/lal_beck_ext.cpp:    BLMF.estimate_gpu_overhead();
lib/gpu/lal_beck_ext.cpp:void beck_gpu_clear() {
lib/gpu/lal_beck_ext.cpp:int ** beck_gpu_compute_n(const int ago, const int inum_full,
lib/gpu/lal_beck_ext.cpp:void beck_gpu_compute(const int ago, const int inum_full, const int nall,
lib/gpu/lal_beck_ext.cpp:double beck_gpu_bytes() {
lib/gpu/lal_neighbor.h:                              Peng Wang (Nvidia)
lib/gpu/lal_neighbor.h:    email                : brownw@ornl.gov, penwang@nvidia.com
lib/gpu/lal_neighbor.h:#if !defined(USE_OPENCL) && !defined(USE_HIP)
lib/gpu/lal_neighbor.h:// Issue with incorrect results with CUDA >= 11.2 and pre-12.0
lib/gpu/lal_neighbor.h:#if (CUDA_VERSION > 11019) && (CUDA_VERSION < 12000)
lib/gpu/lal_neighbor.h:    * \param gpu_nbor 0 if neighboring will be performed on host
lib/gpu/lal_neighbor.h:    *        gpu_nbor 1 if neighboring will be performed on device
lib/gpu/lal_neighbor.h:    *        gpu_nbor 2 if binning on host and neighboring on device
lib/gpu/lal_neighbor.h:    * \param gpu_host 0 if host will not perform force calculations,
lib/gpu/lal_neighbor.h:    *                 1 if gpu_nbor is true, and host needs a half nbor list,
lib/gpu/lal_neighbor.h:    *                 2 if gpu_nbor is true, and host needs a full nbor list
lib/gpu/lal_neighbor.h:            const int gpu_nbor, const int gpu_host, const bool pre_cut,
lib/gpu/lal_neighbor.h:        if (_gpu_nbor==2) {
lib/gpu/lal_neighbor.h:  inline int gpu_nbor() const { return _gpu_nbor; }
lib/gpu/lal_neighbor.h:  inline double gpu_bytes() {
lib/gpu/lal_neighbor.h:    double res = _gpu_bytes + _c_bytes + _cell_bytes;
lib/gpu/lal_neighbor.h:    if (_gpu_nbor==0)
lib/gpu/lal_neighbor.h:  // ----------------- Data for GPU Neighbor Calculation ---------------
lib/gpu/lal_neighbor.h:  int _gpu_nbor, _max_atoms, _max_nbors, _max_host, _nbor_pitch, _maxspecial;
lib/gpu/lal_neighbor.h:  bool _gpu_host, _alloc_packed, _ilist_map, _auto_cell_size;
lib/gpu/lal_neighbor.h:  double _gpu_bytes, _c_bytes, _cell_bytes;
lib/gpu/lal_coul_long_ext.cpp:int cl_gpu_init(const int ntypes, double **host_scale,
lib/gpu/lal_coul_long_ext.cpp:                const int maxspecial, const double cell_size, int &gpu_mode,
lib/gpu/lal_coul_long_ext.cpp:  gpu_mode=CLMF.device->gpu_mode();
lib/gpu/lal_coul_long_ext.cpp:  double gpu_split=CLMF.device->particle_split();
lib/gpu/lal_coul_long_ext.cpp:  int first_gpu=CLMF.device->first_device();
lib/gpu/lal_coul_long_ext.cpp:  int last_gpu=CLMF.device->last_device();
lib/gpu/lal_coul_long_ext.cpp:  int gpu_rank=CLMF.device->gpu_rank();
lib/gpu/lal_coul_long_ext.cpp:  int procs_per_gpu=CLMF.device->procs_per_gpu();
lib/gpu/lal_coul_long_ext.cpp:  CLMF.device->init_message(screen,"coul/long",first_gpu,last_gpu);
lib/gpu/lal_coul_long_ext.cpp:                      cell_size, gpu_split, screen, host_cut_coulsq,
lib/gpu/lal_coul_long_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_coul_long_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_coul_long_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_coul_long_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_coul_long_ext.cpp:                last_gpu,i);
lib/gpu/lal_coul_long_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_coul_long_ext.cpp:                        cell_size, gpu_split, screen, host_cut_coulsq,
lib/gpu/lal_coul_long_ext.cpp:    CLMF.estimate_gpu_overhead();
lib/gpu/lal_coul_long_ext.cpp:void cl_gpu_reinit(const int ntypes, double **host_scale) {
lib/gpu/lal_coul_long_ext.cpp:  int gpu_rank=CLMF.device->gpu_rank();
lib/gpu/lal_coul_long_ext.cpp:  int procs_per_gpu=CLMF.device->procs_per_gpu();
lib/gpu/lal_coul_long_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_coul_long_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_coul_long_ext.cpp:void cl_gpu_clear() {
lib/gpu/lal_coul_long_ext.cpp:int** cl_gpu_compute_n(const int ago, const int inum_full,
lib/gpu/lal_coul_long_ext.cpp:void cl_gpu_compute(const int ago, const int inum_full, const int nall,
lib/gpu/lal_coul_long_ext.cpp:double cl_gpu_bytes() {
lib/gpu/lal_lj_expand.h:                            Inderaj Bains (NVIDIA)
lib/gpu/lal_lj_expand.h:    email                : ibains@nvidia.com
lib/gpu/lal_lj_expand.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_lj_expand.h:    * - -1 if fix gpu not found
lib/gpu/lal_lj_expand.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_lj_expand.h:           const double gpu_split, FILE *screen);
lib/gpu/lal_base_three.h:#if defined(USE_OPENCL)
lib/gpu/lal_base_three.h:#elif defined(USE_CUDART)
lib/gpu/lal_base_three.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_base_three.h:    * - -1 if fix gpu not found
lib/gpu/lal_base_three.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_base_three.h:                 const double gpu_split, FILE *screen,
lib/gpu/lal_base_three.h:  /// Estimate the overhead for GPU context changes and CPU driver
lib/gpu/lal_base_three.h:  void estimate_gpu_overhead(const int add_kernels=0);
lib/gpu/lal_base_three.h:  int _gpu_nbor, _onetype, _onetype3, _spq;
lib/gpu/lal_base_three.h:  double _gpu_overhead, _driver_overhead;
lib/gpu/lal_lj_ext.cpp:int ljl_gpu_init(const int ntypes, double **cutsq, double **host_lj1,
lib/gpu/lal_lj_ext.cpp:                 const double cell_size, int &gpu_mode, FILE *screen) {
lib/gpu/lal_lj_ext.cpp:  gpu_mode=LJLMF.device->gpu_mode();
lib/gpu/lal_lj_ext.cpp:  double gpu_split=LJLMF.device->particle_split();
lib/gpu/lal_lj_ext.cpp:  int first_gpu=LJLMF.device->first_device();
lib/gpu/lal_lj_ext.cpp:  int last_gpu=LJLMF.device->last_device();
lib/gpu/lal_lj_ext.cpp:  int gpu_rank=LJLMF.device->gpu_rank();
lib/gpu/lal_lj_ext.cpp:  int procs_per_gpu=LJLMF.device->procs_per_gpu();
lib/gpu/lal_lj_ext.cpp:  LJLMF.device->init_message(screen,"lj/cut",first_gpu,last_gpu);
lib/gpu/lal_lj_ext.cpp:                       maxspecial, cell_size, gpu_split, screen);
lib/gpu/lal_lj_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_lj_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_lj_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_lj_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_lj_ext.cpp:                last_gpu,i);
lib/gpu/lal_lj_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_lj_ext.cpp:                         cell_size, gpu_split, screen);
lib/gpu/lal_lj_ext.cpp:    LJLMF.estimate_gpu_overhead();
lib/gpu/lal_lj_ext.cpp:void ljl_gpu_reinit(const int ntypes, double **cutsq, double **host_lj1,
lib/gpu/lal_lj_ext.cpp:  int gpu_rank=LJLMF.device->gpu_rank();
lib/gpu/lal_lj_ext.cpp:  int procs_per_gpu=LJLMF.device->procs_per_gpu();
lib/gpu/lal_lj_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_lj_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_lj_ext.cpp:void ljl_gpu_clear() {
lib/gpu/lal_lj_ext.cpp:int ** ljl_gpu_compute_n(const int ago, const int inum_full,
lib/gpu/lal_lj_ext.cpp:void ljl_gpu_compute(const int ago, const int inum_full, const int nall,
lib/gpu/lal_lj_ext.cpp:double ljl_gpu_bytes() {
lib/gpu/lal_ufm.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_ufm.h:    * - -1 if fix gpu not found
lib/gpu/lal_ufm.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_ufm.h:           const double gpu_split, FILE *screen);
lib/gpu/lal_dipole_long_lj.cpp:#ifdef USE_OPENCL
lib/gpu/lal_dipole_long_lj.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_dipole_long_lj.cpp:                    const double gpu_split, FILE *_screen,
lib/gpu/lal_dipole_long_lj.cpp:  success=this->init_atomic(nlocal,nall,max_nbors,maxspecial,cell_size,gpu_split,
lib/gpu/lal_base_charge.cpp:                             const double cell_size, const double gpu_split,
lib/gpu/lal_base_charge.cpp:  int gpu_nbor=0;
lib/gpu/lal_base_charge.cpp:  if (device->gpu_mode()==Device<numtyp,acctyp>::GPU_NEIGH)
lib/gpu/lal_base_charge.cpp:    gpu_nbor=1;
lib/gpu/lal_base_charge.cpp:  else if (device->gpu_mode()==Device<numtyp,acctyp>::GPU_HYB_NEIGH)
lib/gpu/lal_base_charge.cpp:    gpu_nbor=2;
lib/gpu/lal_base_charge.cpp:  int _gpu_host=0;
lib/gpu/lal_base_charge.cpp:  int host_nlocal=hd_balancer.first_host_count(nlocal,gpu_split,gpu_nbor);
lib/gpu/lal_base_charge.cpp:    _gpu_host=1;
lib/gpu/lal_base_charge.cpp:  if (ucl_device!=device->gpu) _compiled=false;
lib/gpu/lal_base_charge.cpp:  ucl_device=device->gpu;
lib/gpu/lal_base_charge.cpp:  if (_threads_per_atom>1 && gpu_nbor==0) {
lib/gpu/lal_base_charge.cpp:  success = device->init_nbor(nbor,nlocal,host_nlocal,nall,maxspecial,_gpu_host,
lib/gpu/lal_base_charge.cpp:  hd_balancer.init(device,gpu_nbor,gpu_split);
lib/gpu/lal_base_charge.cpp:  // Initialize timers for the selected GPU
lib/gpu/lal_base_charge.cpp:  _max_an_bytes=ans->gpu_bytes()+nbor->gpu_bytes();
lib/gpu/lal_base_charge.cpp:void BaseChargeT::estimate_gpu_overhead(const int add_kernels) {
lib/gpu/lal_base_charge.cpp:  device->estimate_gpu_overhead(1+add_kernels,_gpu_overhead,_driver_overhead);
lib/gpu/lal_base_charge.cpp:  _gpu_overhead*=hd_balancer.timestep();
lib/gpu/lal_base_charge.cpp:                       _gpu_overhead,_driver_overhead,_threads_per_atom,screen);
lib/gpu/lal_base_charge.cpp:  double bytes=ans->gpu_bytes()+nbor->gpu_bytes();
lib/gpu/lal_base_charge.cpp:  double bytes=ans->gpu_bytes()+nbor->gpu_bytes();
lib/gpu/lal_base_charge.cpp:// Reneighbor on GPU if necessary and then compute forces, virials, energies
lib/gpu/lal_base_charge.cpp:  int inum=hd_balancer.get_gpu_count(ago,inum_full);
lib/gpu/lal_base_charge.cpp:  // Build neighbor list on GPU if necessary
lib/gpu/lal_base_charge.cpp:  #if defined(USE_OPENCL) && (defined(CL_VERSION_2_1) || defined(CL_VERSION_3_0))
lib/gpu/lal_charmm.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_charmm.h:    * - -1 if fix gpu not found
lib/gpu/lal_charmm.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_charmm.h:           const double gpu_split, FILE *screen, double host_cut_ljsq,
lib/gpu/lal_ellipsoid_extra.h:ucl_inline numtyp gpu_dot3(const numtyp *v1, const numtyp *v2)
lib/gpu/lal_ellipsoid_extra.h:ucl_inline void gpu_cross3(const numtyp *v1, const numtyp *v2, numtyp *ans)
lib/gpu/lal_ellipsoid_extra.h:ucl_inline numtyp gpu_det3(const numtyp m[9])
lib/gpu/lal_ellipsoid_extra.h:ucl_inline void gpu_diag_times3(const numtyp4 shape, const numtyp m[9],
lib/gpu/lal_ellipsoid_extra.h:ucl_inline void gpu_plus3(const numtyp m[9], const numtyp m2[9], numtyp ans[9])
lib/gpu/lal_ellipsoid_extra.h:ucl_inline void gpu_transpose_times3(const numtyp m[9], const numtyp m2[9],
lib/gpu/lal_ellipsoid_extra.h:ucl_inline void gpu_row_times3(const numtyp *v, const numtyp m[9], numtyp *ans)
lib/gpu/lal_ellipsoid_extra.h:ucl_inline void gpu_mldivide3(const numtyp m[9], const numtyp *v, numtyp *ans,
lib/gpu/lal_ellipsoid_extra.h:ucl_inline void gpu_quat_to_mat_trans(__global const numtyp4 *qif, const int qi,
lib/gpu/lal_ellipsoid_extra.h:ucl_inline void gpu_transpose_times_diag3(const numtyp m[9],
lib/gpu/lal_ellipsoid_extra.h:ucl_inline void gpu_times3(const numtyp m[9], const numtyp m2[9],
lib/gpu/lal_ellipsoid_extra.h:ucl_inline void gpu_rotation_generator_x(const numtyp m[9], numtyp ans[9])
lib/gpu/lal_ellipsoid_extra.h:ucl_inline void gpu_rotation_generator_y(const numtyp m[9], numtyp ans[9])
lib/gpu/lal_ellipsoid_extra.h:ucl_inline void gpu_rotation_generator_z(const numtyp m[9], numtyp ans[9])
lib/gpu/lal_ellipsoid_extra.h:ucl_inline void gpu_times_column3(const numtyp m[9], const numtyp v[3],
lib/gpu/Makefile.linux_multi:#  Generic Linux Makefile for CUDA complied for multiple compute capabilities
lib/gpu/Makefile.linux_multi:#     - Add your GPU to CUDA_CODE
lib/gpu/Makefile.linux_multi:ifeq ($(CUDA_HOME),)
lib/gpu/Makefile.linux_multi:CUDA_HOME = /usr/local/cuda
lib/gpu/Makefile.linux_multi:#CUDA_ARCH = -arch=sm_30
lib/gpu/Makefile.linux_multi:#CUDA_ARCH = -arch=sm_32
lib/gpu/Makefile.linux_multi:#CUDA_ARCH = -arch=sm_35
lib/gpu/Makefile.linux_multi:#CUDA_ARCH = -arch=sm_37
lib/gpu/Makefile.linux_multi:CUDA_ARCH = -arch=sm_50
lib/gpu/Makefile.linux_multi:#CUDA_ARCH = -arch=sm_52
lib/gpu/Makefile.linux_multi:#CUDA_ARCH = -arch=sm_60
lib/gpu/Makefile.linux_multi:#CUDA_ARCH = -arch=sm_61
lib/gpu/Makefile.linux_multi:#CUDA_ARCH = -arch=sm_70
lib/gpu/Makefile.linux_multi:#CUDA_ARCH = -arch=sm_75
lib/gpu/Makefile.linux_multi:#CUDA_ARCH = -arch=sm_80
lib/gpu/Makefile.linux_multi:#CUDA_ARCH = -arch=sm_86
lib/gpu/Makefile.linux_multi:#CUDA_ARCH = -arch=sm_89
lib/gpu/Makefile.linux_multi:#CUDA_ARCH = -arch=sm_90
lib/gpu/Makefile.linux_multi:CUDA_CODE = -gencode arch=compute_50,code=[sm_50,compute_50] -gencode arch=compute_52,code=[sm_52,compute_52] \
lib/gpu/Makefile.linux_multi:CUDA_ARCH += $(CUDA_CODE)
lib/gpu/Makefile.linux_multi:# precision for GPU calculations
lib/gpu/Makefile.linux_multi:CUDA_PRECISION = -D_SINGLE_DOUBLE
lib/gpu/Makefile.linux_multi:CUDA_INCLUDE = -I$(CUDA_HOME)/include
lib/gpu/Makefile.linux_multi:CUDA_LIB = -L$(CUDA_HOME)/lib64 -L$(CUDA_HOME)/lib64/stubs
lib/gpu/Makefile.linux_multi:CUDA_OPTS = -DUNIX -O3 --use_fast_math $(LMP_INC) -Xcompiler -fPIC -allow-unsupported-compiler
lib/gpu/Makefile.linux_multi:# GPU binning not recommended with modern GPUs
lib/gpu/Makefile.linux_multi:include Nvidia.makefile_multi
lib/gpu/lal_sph_heatconduction.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_sph_heatconduction.h:    * - -1 if fix gpu not found
lib/gpu/lal_sph_heatconduction.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_sph_heatconduction.h:           const double gpu_split, FILE *screen);
lib/gpu/lal_base_atomic.h:#if defined(USE_OPENCL)
lib/gpu/lal_base_atomic.h:#elif defined(USE_CUDART)
lib/gpu/lal_base_atomic.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_base_atomic.h:    * - -1 if fix gpu not found
lib/gpu/lal_base_atomic.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_base_atomic.h:                  const double gpu_split, FILE *screen,
lib/gpu/lal_base_atomic.h:  /// Estimate the overhead for GPU context changes and CPU driver
lib/gpu/lal_base_atomic.h:  void estimate_gpu_overhead(const int add_kernels=0);
lib/gpu/lal_base_atomic.h:  double _gpu_overhead, _driver_overhead;
lib/gpu/lal_tersoff.cpp:#if defined(USE_OPENCL)
lib/gpu/lal_tersoff.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_tersoff.cpp:                   const double cell_size, const double gpu_split, FILE *_screen,
lib/gpu/lal_tersoff.cpp:  #ifdef USE_OPENCL
lib/gpu/lal_tersoff.cpp:  success=this->init_three(nlocal,nall,max_nbors,0,cell_size,gpu_split,
lib/gpu/lal_tersoff.cpp:                          &this->_gpu_nbor);
lib/gpu/lal_tersoff.cpp:                          &this->_gpu_nbor);
lib/gpu/lal_re_squared.cpp:#if defined(USE_OPENCL)
lib/gpu/lal_re_squared.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_re_squared.cpp:                     const double gpu_split, FILE *_screen) {
lib/gpu/lal_re_squared.cpp:  success=this->init_base(nlocal,nall,max_nbors,maxspecial,cell_size,gpu_split,
lib/gpu/lal_re_squared.cpp:  // Copy shape, well, sigma, epsilon, and cutsq onto GPU
lib/gpu/lal_atom.cpp:                              _max_gpu_bytes(0) {
lib/gpu/lal_atom.cpp:  if (_gpu_nbor==1)
lib/gpu/lal_atom.cpp:  else if (_gpu_nbor==2)
lib/gpu/lal_atom.cpp:    #ifdef GPU_CAST
lib/gpu/lal_atom.cpp:  if (_gpu_nbor==1) {
lib/gpu/lal_atom.cpp:  if (_gpu_nbor==1) {
lib/gpu/lal_atom.cpp:  int gpu_bytes=0;
lib/gpu/lal_atom.cpp:  #ifdef GPU_CAST
lib/gpu/lal_atom.cpp:  gpu_bytes+=x_cast.device.row_bytes()+type_cast.device.row_bytes();
lib/gpu/lal_atom.cpp:    gpu_bytes+=q.device.row_bytes();
lib/gpu/lal_atom.cpp:    gpu_bytes+=quat.device.row_bytes();
lib/gpu/lal_atom.cpp:    gpu_bytes+=v.device.row_bytes();
lib/gpu/lal_atom.cpp:    gpu_bytes+=extra.device.row_bytes();
lib/gpu/lal_atom.cpp:  if (_gpu_nbor>0) {
lib/gpu/lal_atom.cpp:      gpu_bytes+=dev_tag.row_bytes();
lib/gpu/lal_atom.cpp:    if (_gpu_nbor==1) {
lib/gpu/lal_atom.cpp:      gpu_bytes+=dev_cell_id.row_bytes();
lib/gpu/lal_atom.cpp:    if (_gpu_nbor==2 && _host_view)
lib/gpu/lal_atom.cpp:    gpu_bytes+=dev_particle_id.row_bytes();
lib/gpu/lal_atom.cpp:  gpu_bytes+=x.device.row_bytes();
lib/gpu/lal_atom.cpp:  if (gpu_bytes>_max_gpu_bytes)
lib/gpu/lal_atom.cpp:    _max_gpu_bytes=gpu_bytes;
lib/gpu/lal_atom.cpp:                       const int gpu_nbor, const bool bonds, const bool vel,
lib/gpu/lal_atom.cpp:  int gpu_bytes=0;
lib/gpu/lal_atom.cpp:      gpu_bytes+=q.device.row_bytes();
lib/gpu/lal_atom.cpp:    gpu_bytes+=quat.device.row_bytes();
lib/gpu/lal_atom.cpp:      gpu_bytes+=v.device.row_bytes();
lib/gpu/lal_atom.cpp:      gpu_bytes+=extra.device.row_bytes();
lib/gpu/lal_atom.cpp:    if (_bonds && _gpu_nbor>0) {
lib/gpu/lal_atom.cpp:      gpu_bytes+=dev_tag.row_bytes();
lib/gpu/lal_atom.cpp:  if (gpu_nbor>0 && _gpu_nbor==0) {
lib/gpu/lal_atom.cpp:    _gpu_nbor=gpu_nbor;
lib/gpu/lal_atom.cpp:    if (_gpu_nbor==1) {
lib/gpu/lal_atom.cpp:    if (_gpu_nbor==1) {
lib/gpu/lal_atom.cpp:    gpu_bytes+=dev_particle_id.row_bytes();
lib/gpu/lal_atom.cpp:      gpu_bytes+=dev_tag.row_bytes();
lib/gpu/lal_atom.cpp:    if (_gpu_nbor==1) {
lib/gpu/lal_atom.cpp:      gpu_bytes+=dev_cell_id.row_bytes();
lib/gpu/lal_atom.cpp:                 UCL_Device &devi, const int gpu_nbor, const bool bonds, const bool vel,
lib/gpu/lal_atom.cpp:  _gpu_nbor=gpu_nbor;
lib/gpu/lal_atom.cpp:  #ifdef GPU_CAST
lib/gpu/lal_atom.cpp:  #ifdef GPU_CAST
lib/gpu/lal_atom.cpp:  if (_gpu_nbor==1) cudppDestroyPlan(sort_plan);
lib/gpu/lal_atom.cpp:  if (_gpu_nbor==1) {
lib/gpu/lal_atom.cpp:  if (_gpu_nbor==2) {
lib/gpu/lal_atom.cpp:  _max_gpu_bytes=0;
lib/gpu/lal_atom.cpp:  #ifdef GPU_CAST
lib/gpu/lal_atom.cpp:#ifdef GPU_CAST
lib/gpu/lal_atom.cpp:#if defined(USE_OPENCL)
lib/gpu/lal_atom.cpp:#elif defined(USE_CUDART)
lib/gpu/cudpp_mini/cudpp.cpp:// cuDPP -- CUDA Data Parallel Primitives library
lib/gpu/cudpp_mini/cudpp.cpp: * The CUDA public interface comprises the functions, structs, and enums
lib/gpu/cudpp_mini/cudpp.cpp: * GPU memory (d_in) and places the output in GPU memory
lib/gpu/cudpp_mini/cudpp.cpp: * @param[out] d_out output of scan, in GPU memory
lib/gpu/cudpp_mini/cudpp.cpp: * @param[in] d_in input to scan, in GPU memory
lib/gpu/cudpp_mini/cudpp.cpp: * GPU memory (d_idata) and places the output in GPU memory
lib/gpu/cudpp_mini/cudpp.cpp: * @param[out] d_out output of segmented scan, in GPU memory
lib/gpu/cudpp_mini/cudpp.cpp: * @param[in] d_idata input data to segmented scan, in GPU memory
lib/gpu/cudpp_mini/cudpp.cpp: * @param[in] d_iflags input flags to segmented scan, in GPU memory
lib/gpu/cudpp_mini/cudpp.cpp: * The easy way to do this is to use cudaMallocPitch() to allocate a
lib/gpu/cudpp_mini/cudpp.cpp: * returned by cudaMallocPitch to cudppPlan() via \a rowPitch.
lib/gpu/cudpp_mini/cudpp.cpp: * @param[out] d_out output of scan, in GPU memory
lib/gpu/cudpp_mini/cudpp.cpp: * @param[in] d_in input to scan, in GPU memory
lib/gpu/cudpp_mini/cudpp.cpp: * Takes as input an array of elements in GPU memory
lib/gpu/cudpp_mini/cudpp.cpp: * (\a d_in) and an equal-sized unsigned int array in GPU memory
lib/gpu/cudpp_mini/cudpp.cpp: * valid. The output is a packed array, in GPU memory, of only those
lib/gpu/cudpp_mini/cudpp.cpp: * Takes as input an array of keys in GPU memory
lib/gpu/cudpp_mini/cudpp.cpp: * @param[out] d_out output of rand, in GPU memory.  Should be an array of unsigned integers.
lib/gpu/cudpp_mini/cudpp.cpp:// c-file-style: "NVIDIA"
lib/gpu/cudpp_mini/cudpp_scan.h:// cuDPP -- CUDA Data Parallel Primitives library
lib/gpu/cudpp_mini/cta/scan_cta.cu://  cuDPP -- CUDA Data Parallel Primitives library
lib/gpu/cudpp_mini/cta/scan_cta.cu:  * The CUDPP CTA-Level API contains functions that run on the GPU 
lib/gpu/cudpp_mini/cta/scan_cta.cu:  * device.  These are CUDA \c __device__ functions that are called
lib/gpu/cudpp_mini/cta/scan_cta.cu:  * from within other CUDA device functions (typically 
lib/gpu/cudpp_mini/cta/scan_cta.cu://! @todo Parameterize this in case this perf detail changes on future GPUs.
lib/gpu/cudpp_mini/cta/scan_cta.cu:  * Previous versions of CUDPP used the Blelloch algorithm.  For current GPUs, 
lib/gpu/cudpp_mini/cta/scan_cta.cu:* CUDA __global__ or __device__ functions. This function scans 2 * CTA_SIZE elements.
lib/gpu/cudpp_mini/cta/radixsort_cta.cu:// CUDPP -- CUDA Data Parallel Primitives library
lib/gpu/cudpp_mini/kernel/scan_kernel.cu:// cuDPP -- CUDA Data Parallel Primitives library
lib/gpu/cudpp_mini/kernel/scan_kernel.cu:  * The CUDPP Kernel-Level API contains functions that run on the GPU 
lib/gpu/cudpp_mini/kernel/scan_kernel.cu:  * must be invoked from host (CPU) code.  They generally invoke GPU 
lib/gpu/cudpp_mini/kernel/vector_kernel.cu:// CUDPP -- CUDA Data Parallel Primitives library
lib/gpu/cudpp_mini/kernel/vector_kernel.cu: * @brief CUDA kernel methods for basic operations on vectors.  
lib/gpu/cudpp_mini/kernel/vector_kernel.cu: * CUDA kernel methods for basic operations on vectors.  
lib/gpu/cudpp_mini/kernel/vector_kernel.cu: * CUDA kernel methods for basic operations on vectors.  
lib/gpu/cudpp_mini/kernel/radixsort_kernel.cu:// CUDPP -- CUDA Data Parallel Primitives library
lib/gpu/cudpp_mini/kernel/radixsort_kernel.cu:* be resident in the GPU and no more, rather than launching as many threads as
lib/gpu/cudpp_mini/kernel/radixsort_kernel.cu:* GPUs than it is on compute version 1.2 GPUs.
lib/gpu/cudpp_mini/kernel/radixsort_kernel.cu:* @param[in]  keysIn Input of unsorted keys in GPU 
lib/gpu/cudpp_mini/kernel/radixsort_kernel.cu:* be resident in the GPU and no more, rather than launching as many threads as
lib/gpu/cudpp_mini/kernel/radixsort_kernel.cu:* GPUs than it is on compute version 1.2 GPUs.
lib/gpu/cudpp_mini/kernel/radixsort_kernel.cu:* offsets have been found. On compute version 1.1 and earlier GPUs, this code depends 
lib/gpu/cudpp_mini/kernel/radixsort_kernel.cu:* On compute version 1.1 GPUs ("manualCoalesce=true") this function ensures
lib/gpu/cudpp_mini/kernel/radixsort_kernel.cu:* GPUs coalescing rules have been relaxed, so this extra overhead hurts 
lib/gpu/cudpp_mini/kernel/radixsort_kernel.cu:* performance.  On these GPUs we set manualCoalesce=false and directly store
lib/gpu/cudpp_mini/kernel/radixsort_kernel.cu:* be resident in the GPU and no more, rather than launching as many threads as
lib/gpu/cudpp_mini/kernel/radixsort_kernel.cu:* GPUs than it is on compute version 1.2 GPUs.
lib/gpu/cudpp_mini/kernel/radixsort_kernel.cu:* @param[in] keys Input of unsorted keys in GPU 
lib/gpu/cudpp_mini/kernel/radixsort_kernel.cu:* be resident in the GPU and no more, rather than launching as many threads as
lib/gpu/cudpp_mini/kernel/radixsort_kernel.cu:* GPUs than it is on compute version 1.2 GPUs.
lib/gpu/cudpp_mini/kernel/radixsort_kernel.cu:* @param[out] keysOut Output of sorted keys GPU main memory
lib/gpu/cudpp_mini/kernel/radixsort_kernel.cu:* @param[in] keysIn Input of unsorted keys in GPU main memory
lib/gpu/cudpp_mini/kernel/radixsort_kernel.cu:* have been found. On compute version 1.1 and earlier GPUs, this code depends 
lib/gpu/cudpp_mini/kernel/radixsort_kernel.cu:* On compute version 1.1 GPUs ("manualCoalesce=true") this function ensures
lib/gpu/cudpp_mini/kernel/radixsort_kernel.cu:* GPUs coalescing rules have been relaxed, so this extra overhead hurts 
lib/gpu/cudpp_mini/kernel/radixsort_kernel.cu:* performance.  On these GPUs we set manualCoalesce=false and directly store
lib/gpu/cudpp_mini/kernel/radixsort_kernel.cu:* be resident in the GPU and no more, rather than launching as many threads as
lib/gpu/cudpp_mini/kernel/radixsort_kernel.cu:* GPUs than it is on compute version 1.2 GPUs.
lib/gpu/cudpp_mini/cudpp_maximal_launch.h:// cuDPP -- CUDA Data Parallel Primitives library
lib/gpu/cudpp_mini/cudpp_maximal_launch.h:#include "cuda_runtime.h"
lib/gpu/cudpp_mini/cudpp_maximal_launch.h:size_t maxBlocks(cudaFuncAttributes &attribs,
lib/gpu/cudpp_mini/cudpp_maximal_launch.h:                 cudaDeviceProp &devprop,
lib/gpu/cudpp_mini/cutil.h:* Copyright 1993-2006 NVIDIA Corporation.  All rights reserved.
lib/gpu/cudpp_mini/cutil.h:* This source code is subject to NVIDIA ownership rights under U.S. and
lib/gpu/cudpp_mini/cutil.h:* NVIDIA MAKES NO REPRESENTATION ABOUT THE SUITABILITY OF THIS SOURCE
lib/gpu/cudpp_mini/cutil.h:* IMPLIED WARRANTY OF ANY KIND.  NVIDIA DISCLAIMS ALL WARRANTIES WITH
lib/gpu/cudpp_mini/cutil.h:* IN NO EVENT SHALL NVIDIA BE LIABLE FOR ANY SPECIAL, INDIRECT, INCIDENTAL,
lib/gpu/cudpp_mini/cutil.h:/* CUda UTility Library */
lib/gpu/cudpp_mini/cutil.h:#include <cuda_runtime.h>
lib/gpu/cudpp_mini/cutil.h:    if( CUDA_SUCCESS != err) {                                               \
lib/gpu/cudpp_mini/cutil.h:        fprintf(stderr, "Cuda driver error %x in file '%s' in line %i.\n",   \
lib/gpu/cudpp_mini/cutil.h:    if( CUDA_SUCCESS != err) {                                               \
lib/gpu/cudpp_mini/cutil.h:        fprintf(stderr, "Cuda driver error %x in file '%s' in line %i.\n",   \
lib/gpu/cudpp_mini/cutil.h:#  define CUDA_SAFE_CALL_NO_SYNC( call) do {                                 \
lib/gpu/cudpp_mini/cutil.h:    cudaError err = call;                                                    \
lib/gpu/cudpp_mini/cutil.h:    if( cudaSuccess != err) {                                                \
lib/gpu/cudpp_mini/cutil.h:        fprintf(stderr, "Cuda error in file '%s' in line %i : %s.\n",        \
lib/gpu/cudpp_mini/cutil.h:                __FILE__, __LINE__, cudaGetErrorString( err) );              \
lib/gpu/cudpp_mini/cutil.h:#  define CUDA_SAFE_CALL( call) do {                                         \
lib/gpu/cudpp_mini/cutil.h:    CUDA_SAFE_CALL_NO_SYNC(call);                                            \
lib/gpu/cudpp_mini/cutil.h:    cudaError err = cudaThreadSynchronize();                                 \
lib/gpu/cudpp_mini/cutil.h:    if( cudaSuccess != err) {                                                \
lib/gpu/cudpp_mini/cutil.h:        fprintf(stderr, "Cuda error in file '%s' in line %i : %s.\n",        \
lib/gpu/cudpp_mini/cutil.h:                __FILE__, __LINE__, cudaGetErrorString( err) );              \
lib/gpu/cudpp_mini/cutil.h:    //! Check for CUDA error
lib/gpu/cudpp_mini/cutil.h:    cudaError_t err = cudaGetLastError();                                    \
lib/gpu/cudpp_mini/cutil.h:    if( cudaSuccess != err) {                                                \
lib/gpu/cudpp_mini/cutil.h:        fprintf(stderr, "Cuda error: %s in file '%s' in line %i : %s.\n",    \
lib/gpu/cudpp_mini/cutil.h:                errorMessage, __FILE__, __LINE__, cudaGetErrorString( err) );\
lib/gpu/cudpp_mini/cutil.h:    err = cudaThreadSynchronize();                                           \
lib/gpu/cudpp_mini/cutil.h:    if( cudaSuccess != err) {                                                \
lib/gpu/cudpp_mini/cutil.h:        fprintf(stderr, "Cuda error: %s in file '%s' in line %i : %s.\n",    \
lib/gpu/cudpp_mini/cutil.h:                errorMessage, __FILE__, __LINE__, cudaGetErrorString( err) );\
lib/gpu/cudpp_mini/cutil.h:#  define CUDA_SAFE_CALL_NO_SYNC( call) call
lib/gpu/cudpp_mini/cutil.h:#  define CUDA_SAFE_CALL( call) call
lib/gpu/cudpp_mini/cutil.h:    CUDA_SAFE_CALL_NO_SYNC(cudaGetDeviceCount(&deviceCount));                \
lib/gpu/cudpp_mini/cutil.h:        fprintf(stderr, "cutil error: no devices supporting CUDA.\n");       \
lib/gpu/cudpp_mini/cutil.h:    cudaDeviceProp deviceProp;                                               \
lib/gpu/cudpp_mini/cutil.h:    CUDA_SAFE_CALL_NO_SYNC(cudaGetDeviceProperties(&deviceProp, dev));       \
lib/gpu/cudpp_mini/cutil.h:        fprintf(stderr, "cutil error: device does not support CUDA.\n");     \
lib/gpu/cudpp_mini/cutil.h:    CUDA_SAFE_CALL(cudaSetDevice(dev));                                      \
lib/gpu/cudpp_mini/cutil.h:    if (CUDA_SUCCESS == err)                                                 \
lib/gpu/cudpp_mini/cutil.h:        fprintf(stderr, "cutil error: no devices supporting CUDA\n");        \
lib/gpu/cudpp_mini/cudpp_plan.h:// CUDPP -- CUDA Data Parallel Primitives library
lib/gpu/cudpp_mini/cudpp_plan.h:                                         //!            which is the last element of that row. Resides in GPU memory.
lib/gpu/cudpp_mini/cudpp_plan.h:                                    //!            which is the first element of that row. Resides in GPU memory.
lib/gpu/cudpp_mini/cudpp_plan.cpp:// cuDPP -- CUDA Data Parallel Primitives library
lib/gpu/cudpp_mini/scan_app.cu:// CUDPP -- CUDA Data Parallel Primitives library
lib/gpu/cudpp_mini/scan_app.cu:  * that run on the host CPU and invoke GPU routines in 
lib/gpu/cudpp_mini/scan_app.cu:  * invokes the CUDA kernels which perform the scan on individual blocks. 
lib/gpu/cudpp_mini/scan_app.cu:  * where each block is scanned by a single CUDA thread block.  At each recursive level of the
lib/gpu/cudpp_mini/scan_app.cu:  * block in this level.  See "Parallel Prefix Sum (Scan) in CUDA" for more information (see
lib/gpu/cudpp_mini/scan_app.cu:    // make sure there are no CUDA errors before we start
lib/gpu/cudpp_mini/scan_app.cu:  * of block scans, where each block is scanned by a single CUDA thread block.  
lib/gpu/cudpp_mini/scan_app.cu:            // Use cudaMallocPitch for multi-row block sums to ensure alignment
lib/gpu/cudpp_mini/scan_app.cu:                CUDA_SAFE_CALL( cudaMallocPitch((void**) &(plan->m_blockSums[level]), 
lib/gpu/cudpp_mini/scan_app.cu:                CUDA_SAFE_CALL(cudaMalloc((void**) &(plan->m_blockSums[level++]),  
lib/gpu/cudpp_mini/scan_app.cu:        cudaFree(plan->m_blockSums[i]);
lib/gpu/cudpp_mini/cudpp_maximal_launch.cpp:// cuDPP -- CUDA Data Parallel Primitives library
lib/gpu/cudpp_mini/cudpp_maximal_launch.cpp:size_t maxBlocks(cudaFuncAttributes &attribs,
lib/gpu/cudpp_mini/cudpp_maximal_launch.cpp:                 cudaDeviceProp &devprop,
lib/gpu/cudpp_mini/cudpp_maximal_launch.cpp:    // This is equivalent to the calculation done in the CUDA Occupancy Calculator spreadsheet
lib/gpu/cudpp_mini/cudpp_maximal_launch.cpp:    const unsigned int maxThreadsPerSM = (devprop.major < 2 && devprop.minor < 2) ? 768 : 1024;  // sm_12 GPUs increase threads/SM to 1024
lib/gpu/cudpp_mini/cudpp_maximal_launch.cpp:    cudaDeviceProp devprop;
lib/gpu/cudpp_mini/cudpp_maximal_launch.cpp:    cudaError_t err = cudaGetDevice(&deviceID);
lib/gpu/cudpp_mini/cudpp_maximal_launch.cpp:    if (err == cudaSuccess)
lib/gpu/cudpp_mini/cudpp_maximal_launch.cpp:        err = cudaGetDeviceProperties(&devprop, deviceID);
lib/gpu/cudpp_mini/cudpp_maximal_launch.cpp:        if (err != cudaSuccess)
lib/gpu/cudpp_mini/cudpp_maximal_launch.cpp:        cudaFuncAttributes attr;
lib/gpu/cudpp_mini/cudpp_maximal_launch.cpp:        err = cudaFuncGetAttributes(&attr, (const char*)kernel);
lib/gpu/cudpp_mini/cudpp_maximal_launch.cpp:        if (err != cudaSuccess)
lib/gpu/cudpp_mini/cudpp.h:// CUDPP -- CUDA Data Parallel Primitives library
lib/gpu/cudpp_mini/cudpp.h: * CUDPP is the CUDA Data Parallel Primitives Library. CUDPP is a
lib/gpu/cudpp_mini/cudpp.h: * fixes to support CUDA 3.0 and the new NVIDIA Fermi architecture,
lib/gpu/cudpp_mini/cudpp.h: * including GeForce 400 series and Tesla 20 series GPUs.  It also has
lib/gpu/cudpp_mini/cudpp.h: * - Windows XP (32-bit) (CUDA 2.2, 3.0)
lib/gpu/cudpp_mini/cudpp.h: * - Windows 7 (64-bit) (CUDA 3.0)
lib/gpu/cudpp_mini/cudpp.h: * - Redhat Enterprise Linux 5 (64-bit) (CUDA 3.0)
lib/gpu/cudpp_mini/cudpp.h: * - and Mac OS X 10.6 (Snow Leopard, 64-bit) (CUDA 3.0)
lib/gpu/cudpp_mini/cudpp.h: * Notes: CUDPP is not compatible with CUDA 2.1.  A compiler bug in 2.1
lib/gpu/cudpp_mini/cudpp.h: * no longer testing CUDA device emulation, because it is deprecated in
lib/gpu/cudpp_mini/cudpp.h: * CUDA 3.0 and will be removed from future CUDA versions.
lib/gpu/cudpp_mini/cudpp.h: * \section cuda CUDA
lib/gpu/cudpp_mini/cudpp.h: * <a href="http://developer.nvidia.com/cuda">CUDA C/C++</a>. It requires the
lib/gpu/cudpp_mini/cudpp.h: * CUDA Toolkit version 2.2 or later.  Please see the NVIDIA
lib/gpu/cudpp_mini/cudpp.h: * <a href="http://developer.nvidia.com/cuda">CUDA</a> homepage to download
lib/gpu/cudpp_mini/cudpp.h: * CUDA as well as the CUDA Programming Guide and CUDA SDK, which includes many
lib/gpu/cudpp_mini/cudpp.h: * CUDA code examples.  Some of the samples in the CUDA SDK (including
lib/gpu/cudpp_mini/cudpp.h: *   benchmarked, and compared against other implementations on GPUs and other
lib/gpu/cudpp_mini/cudpp.h: *   - CUDPP calls run on the GPU on GPU data. Thus they can be used
lib/gpu/cudpp_mini/cudpp.h: *     as standalone calls on the GPU (on GPU data initialized by the
lib/gpu/cudpp_mini/cudpp.h: *     calling application) and, more importantly, as GPU components in larger
lib/gpu/cudpp_mini/cudpp.h: *     CPU/GPU applications.
lib/gpu/cudpp_mini/cudpp.h: *        CPU (host) and the GPU by calling into the
lib/gpu/cudpp_mini/cudpp.h: *        that run entirely on the GPU across an entire grid of thread blocks.
lib/gpu/cudpp_mini/cudpp.h: *        entirely on the GPU within a single Cooperative Thread Array (CTA,
lib/gpu/cudpp_mini/cudpp.h: *        (CUDA \c __shared__) memory.
lib/gpu/cudpp_mini/cudpp.h: * In the future, if and when CUDA supports building device-level libraries, we
lib/gpu/cudpp_mini/cudpp.h: * - Mark Harris, Shubhabrata Sengupta, and John D. Owens. "Parallel Prefix Sum (Scan) with CUDA". In Hubert Nguyen, editor, <i>GPU Gems 3</i>, chapter 39, pages 851&ndash;876. Addison Wesley, August 2007. http://graphics.idav.ucdavis.edu/publications/print_pub?pub_id=916
lib/gpu/cudpp_mini/cudpp.h: * - Shubhabrata Sengupta, Mark Harris, Yao Zhang, and John D. Owens. "Scan Primitives for GPU Computing". In <i>Graphics Hardware 2007</i>, pages 97&ndash;106, August 2007. http://graphics.idav.ucdavis.edu/publications/print_pub?pub_id=915
lib/gpu/cudpp_mini/cudpp.h: * - Shubhabrata Sengupta, Mark Harris, and Michael Garland. "Efficient parallel scan algorithms for GPUs". NVIDIA Technical Report NVR-2008-003, December 2008. http://mgarland.org/papers.html#segscan-tr
lib/gpu/cudpp_mini/cudpp.h: * - Nadathur Satish, Mark Harris, and Michael Garland. "Designing Efficient Sorting Algorithms for Manycore GPUs". In <i>Proceedings of the 23rd IEEE International Parallel & Distributed Processing Symposium</i>, May 2009. http://mgarland.org/papers.html#gpusort
lib/gpu/cudpp_mini/cudpp.h: * - Stanley Tzeng, Li-Yi Wei. "Parallel White Noise Generation on a GPU via Cryptographic Hash". In <i>Proceedings of the 2008 Symposium on Interactive 3D Graphics and Games</i>, pages 79&ndash;87, February 2008. http://research.microsoft.com/apps/pubs/default.aspx?id=70502
lib/gpu/cudpp_mini/cudpp.h: * the GPU Gems paper describes (unsegmented) scan, multi-scan for
lib/gpu/cudpp_mini/cudpp.h: * summed-area tables, and stream compaction. The NVIDIA technical report
lib/gpu/cudpp_mini/cudpp.h: * - <a href="http://www.markmark.net">Mark Harris</a>, NVIDIA Corporation
lib/gpu/cudpp_mini/cudpp.h: * - Generous hardware donations from NVIDIA
lib/gpu/cudpp_mini/cudpp.h: * and NVIDIA Corporation.  The library, examples, and all source code are
lib/gpu/cudpp_mini/cudpp.h: * two are self-explanatory.  The second two are built to use CUDA device
lib/gpu/cudpp_mini/cudpp.h: * first build the CUDA Utility Library (libcutil) by typing "make; make dbg=1"
lib/gpu/cudpp_mini/cudpp.h:// c-file-style: "NVIDIA"
lib/gpu/cudpp_mini/cudpp_plan_manager.h:// cuDPP -- CUDA Data Parallel Primitives library
lib/gpu/cudpp_mini/cudpp_plan_manager.cpp:// cuDPP -- CUDA Data Parallel Primitives library
lib/gpu/cudpp_mini/license.txt:campus ("The Regents") and NVIDIA Corporation ("NVIDIA"). All rights reserved.
lib/gpu/cudpp_mini/license.txt:    * Neither the name of the The Regents, nor NVIDIA, nor the names of its 
lib/gpu/cudpp_mini/radixsort_app.cu:// CUDPP -- CUDA Data Parallel Primitives library
lib/gpu/cudpp_mini/radixsort_app.cu:    if (cudaSuccess == cudaGetDevice(&deviceID))
lib/gpu/cudpp_mini/radixsort_app.cu:        cudaDeviceProp devprop;
lib/gpu/cudpp_mini/radixsort_app.cu:        cudaGetDeviceProperties(&devprop, deviceID);
lib/gpu/cudpp_mini/radixsort_app.cu:            // as can fill the GPU, which loop over the "blocks" of work. For smaller 
lib/gpu/cudpp_mini/radixsort_app.cu:            // arrays it is better to use the typical CUDA approach of launching one CTA
lib/gpu/cudpp_mini/radixsort_app.cu:        CUDA_SAFE_CALL(cudaMalloc((void **)&plan->m_tempKeys, 
lib/gpu/cudpp_mini/radixsort_app.cu:            CUDA_SAFE_CALL(cudaMalloc((void **)&plan->m_tempValues, 
lib/gpu/cudpp_mini/radixsort_app.cu:        CUDA_SAFE_CALL(cudaMalloc((void **)&plan->m_counters, 
lib/gpu/cudpp_mini/radixsort_app.cu:        CUDA_SAFE_CALL(cudaMalloc((void **)&plan->m_countersSum,
lib/gpu/cudpp_mini/radixsort_app.cu:        CUDA_SAFE_CALL(cudaMalloc((void **)&plan->m_blockOffsets, 
lib/gpu/cudpp_mini/radixsort_app.cu:        CUDA_SAFE_CALL(cudaMalloc((void **)&plan->m_tempKeys,
lib/gpu/cudpp_mini/radixsort_app.cu:            CUDA_SAFE_CALL(cudaMalloc((void **)&plan->m_tempValues,
lib/gpu/cudpp_mini/radixsort_app.cu:        CUDA_SAFE_CALL(cudaMalloc((void **)&plan->m_counters,
lib/gpu/cudpp_mini/radixsort_app.cu:        CUDA_SAFE_CALL(cudaMalloc((void **)&plan->m_countersSum,
lib/gpu/cudpp_mini/radixsort_app.cu:        CUDA_SAFE_CALL(cudaMalloc((void **)&plan->m_blockOffsets,
lib/gpu/cudpp_mini/radixsort_app.cu:    CUDA_SAFE_CALL( cudaFree(plan->m_tempKeys));
lib/gpu/cudpp_mini/radixsort_app.cu:    CUDA_SAFE_CALL( cudaFree(plan->m_tempValues));
lib/gpu/cudpp_mini/radixsort_app.cu:    CUDA_SAFE_CALL( cudaFree(plan->m_counters));
lib/gpu/cudpp_mini/radixsort_app.cu:    CUDA_SAFE_CALL( cudaFree(plan->m_countersSum));
lib/gpu/cudpp_mini/radixsort_app.cu:    CUDA_SAFE_CALL( cudaFree(plan->m_blockOffsets));
lib/gpu/cudpp_mini/cudpp_radixsort.h:// cuDPP -- CUDA Data Parallel Primitives library
lib/gpu/cudpp_mini/sharedmem.h:// cuDPP -- CUDA Data Parallel Primitives library
lib/gpu/cudpp_mini/sharedmem.h: * Because dynamically sized shared memory arrays are declared "extern" in CUDA,
lib/gpu/cudpp_mini/sharedmem.h:// c-file-style: "NVIDIA"
lib/gpu/cudpp_mini/cudpp_globals.h:// cuDPP -- CUDA Data Parallel Primitives library
lib/gpu/cudpp_mini/cudpp_globals.h: * @brief Global declarations defining machine characteristics of GPU target
lib/gpu/cudpp_mini/cudpp_globals.h: * These are currently set for best performance on G8X GPUs.  The optimal
lib/gpu/cudpp_mini/cudpp_globals.h: * parameters may change on future GPUs. In the future, we hope to make
lib/gpu/cudpp_mini/cudpp_globals.h:// c-file-style: "NVIDIA"
lib/gpu/cudpp_mini/README:of the CUDA performance primitives library for
lib/gpu/cudpp_mini/README:use with the GPU package in LAMMPS.
lib/gpu/cudpp_mini/cudpp_util.h:// cuDPP -- CUDA Data Parallel Primitives library
lib/gpu/cudpp_mini/cudpp_util.h:#include <cuda.h>
lib/gpu/cudpp_mini/cudpp_util.h:#if (CUDA_VERSION >= 3000)
lib/gpu/cudpp_mini/cudpp_util.h:// c-file-style: "NVIDIA"
lib/gpu/lal_base_ellipsoid.cpp:#if defined(USE_OPENCL)
lib/gpu/lal_base_ellipsoid.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_base_ellipsoid.cpp:                              const double cell_size, const double gpu_split,
lib/gpu/lal_base_ellipsoid.cpp:  int gpu_nbor=0;
lib/gpu/lal_base_ellipsoid.cpp:  if (device->gpu_mode()==Device<numtyp,acctyp>::GPU_NEIGH)
lib/gpu/lal_base_ellipsoid.cpp:    gpu_nbor=1;
lib/gpu/lal_base_ellipsoid.cpp:  else if (device->gpu_mode()==Device<numtyp,acctyp>::GPU_HYB_NEIGH)
lib/gpu/lal_base_ellipsoid.cpp:    gpu_nbor=2;
lib/gpu/lal_base_ellipsoid.cpp:  int _gpu_host=0;
lib/gpu/lal_base_ellipsoid.cpp:  int host_nlocal=hd_balancer.first_host_count(nlocal,gpu_split,gpu_nbor);
lib/gpu/lal_base_ellipsoid.cpp:    _gpu_host=1;
lib/gpu/lal_base_ellipsoid.cpp:  if (ucl_device!=device->gpu) _compiled=false;
lib/gpu/lal_base_ellipsoid.cpp:  ucl_device=device->gpu;
lib/gpu/lal_base_ellipsoid.cpp:  success = device->init_nbor(nbor,nlocal,host_nlocal,nall,maxspecial,_gpu_host,
lib/gpu/lal_base_ellipsoid.cpp:  hd_balancer.init(device,gpu_nbor,gpu_split);
lib/gpu/lal_base_ellipsoid.cpp:  // Initialize timers for the selected GPU
lib/gpu/lal_base_ellipsoid.cpp:  if (_multiple_forms && gpu_nbor!=0)
lib/gpu/lal_base_ellipsoid.cpp:  _max_an_bytes=ans->gpu_bytes()+nbor->gpu_bytes();
lib/gpu/lal_base_ellipsoid.cpp:void BaseEllipsoidT::estimate_gpu_overhead() {
lib/gpu/lal_base_ellipsoid.cpp:  device->estimate_gpu_overhead(2,_gpu_overhead,_driver_overhead);
lib/gpu/lal_base_ellipsoid.cpp:  single[6]=_gpu_overhead;
lib/gpu/lal_base_ellipsoid.cpp:  _max_bytes+=atom->max_gpu_bytes();
lib/gpu/lal_base_ellipsoid.cpp:  #ifdef USE_OPENCL
lib/gpu/lal_base_ellipsoid.cpp:  // Workaround for timing issue on Intel OpenCL
lib/gpu/lal_base_ellipsoid.cpp:      if (device->procs_per_gpu()==1 && (times[3] > 0.0)) {
lib/gpu/lal_base_ellipsoid.cpp:        if (nbor->gpu_nbor() > 0.0)
lib/gpu/lal_base_ellipsoid.cpp:      if (nbor->gpu_nbor()==2)
lib/gpu/lal_base_ellipsoid.cpp:  double bytes=ans->gpu_bytes()+nbor->gpu_bytes();
lib/gpu/lal_base_ellipsoid.cpp:  double bytes=ans->gpu_bytes()+nbor->gpu_bytes();
lib/gpu/lal_base_ellipsoid.cpp:// Reneighbor on GPU if necessary and then compute forces, virials, energies
lib/gpu/lal_base_ellipsoid.cpp:  int inum=hd_balancer.get_gpu_count(ago,inum_full);
lib/gpu/lal_base_ellipsoid.cpp:  // Build neighbor list on GPU if necessary
lib/gpu/lal_base_ellipsoid.cpp:  #if defined(USE_OPENCL) && (defined(CL_VERSION_2_1) || defined(CL_VERSION_3_0))
lib/gpu/lal_dipole_lj.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_dipole_lj.h:    * - -1 if fix gpu not found
lib/gpu/lal_dipole_lj.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_dipole_lj.h:           const double gpu_split, FILE *screen, double **host_cut_ljsq,
lib/gpu/lal_coul_slater_long.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_coul_slater_long.h:    * - -1 if fix gpu not found
lib/gpu/lal_coul_slater_long.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_coul_slater_long.h:                 const double gpu_split, FILE *screen,
lib/gpu/lal_lj_coul_soft_ext.cpp:int ljcs_gpu_init(const int ntypes, double **cutsq, double **host_lj1,
lib/gpu/lal_lj_coul_soft_ext.cpp:                 const double cell_size, int &gpu_mode, FILE *screen,
lib/gpu/lal_lj_coul_soft_ext.cpp:  gpu_mode=LJCSMF.device->gpu_mode();
lib/gpu/lal_lj_coul_soft_ext.cpp:  double gpu_split=LJCSMF.device->particle_split();
lib/gpu/lal_lj_coul_soft_ext.cpp:  int first_gpu=LJCSMF.device->first_device();
lib/gpu/lal_lj_coul_soft_ext.cpp:  int last_gpu=LJCSMF.device->last_device();
lib/gpu/lal_lj_coul_soft_ext.cpp:  int gpu_rank=LJCSMF.device->gpu_rank();
lib/gpu/lal_lj_coul_soft_ext.cpp:  int procs_per_gpu=LJCSMF.device->procs_per_gpu();
lib/gpu/lal_lj_coul_soft_ext.cpp:  LJCSMF.device->init_message(screen,"lj/cut/coul/cut/soft",first_gpu,last_gpu);
lib/gpu/lal_lj_coul_soft_ext.cpp:                       maxspecial, cell_size, gpu_split, screen, host_cut_ljsq,
lib/gpu/lal_lj_coul_soft_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_lj_coul_soft_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_lj_coul_soft_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_lj_coul_soft_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_lj_coul_soft_ext.cpp:                last_gpu,i);
lib/gpu/lal_lj_coul_soft_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_lj_coul_soft_ext.cpp:                         cell_size, gpu_split, screen, host_cut_ljsq,
lib/gpu/lal_lj_coul_soft_ext.cpp:    LJCSMF.device->gpu_barrier();
lib/gpu/lal_lj_coul_soft_ext.cpp:    LJCSMF.estimate_gpu_overhead();
lib/gpu/lal_lj_coul_soft_ext.cpp:void ljcs_gpu_clear() {
lib/gpu/lal_lj_coul_soft_ext.cpp:int** ljcs_gpu_compute_n(const int ago, const int inum_full,
lib/gpu/lal_lj_coul_soft_ext.cpp:void ljcs_gpu_compute(const int ago, const int inum_full, const int nall,
lib/gpu/lal_lj_coul_soft_ext.cpp:double ljcs_gpu_bytes() {
lib/gpu/lal_lj96.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_lj96.h:    * - -1 if fix gpu not found
lib/gpu/lal_lj96.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_lj96.h:           const double gpu_split, FILE *screen);
lib/gpu/lal_coul_long.cpp:#if defined(USE_OPENCL)
lib/gpu/lal_coul_long.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_coul_long.cpp:extern Device<PRECISION,ACC_PRECISION> pair_gpu_device;
lib/gpu/lal_coul_long.cpp:                    const double gpu_split, FILE *_screen,
lib/gpu/lal_coul_long.cpp:                                              gpu_split,_screen,coul_long,"k_coul_long");
lib/gpu/lal_coul_long_cs_ext.cpp:int clcs_gpu_init(const int ntypes, double **host_scale,
lib/gpu/lal_coul_long_cs_ext.cpp:                const int maxspecial, const double cell_size, int &gpu_mode,
lib/gpu/lal_coul_long_cs_ext.cpp:  gpu_mode=CLCSMF.device->gpu_mode();
lib/gpu/lal_coul_long_cs_ext.cpp:  double gpu_split=CLCSMF.device->particle_split();
lib/gpu/lal_coul_long_cs_ext.cpp:  int first_gpu=CLCSMF.device->first_device();
lib/gpu/lal_coul_long_cs_ext.cpp:  int last_gpu=CLCSMF.device->last_device();
lib/gpu/lal_coul_long_cs_ext.cpp:  int gpu_rank=CLCSMF.device->gpu_rank();
lib/gpu/lal_coul_long_cs_ext.cpp:  int procs_per_gpu=CLCSMF.device->procs_per_gpu();
lib/gpu/lal_coul_long_cs_ext.cpp:  CLCSMF.device->init_message(screen,"coul/long/cs",first_gpu,last_gpu);
lib/gpu/lal_coul_long_cs_ext.cpp:                      cell_size, gpu_split, screen, host_cut_coulsq,
lib/gpu/lal_coul_long_cs_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_coul_long_cs_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_coul_long_cs_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_coul_long_cs_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_coul_long_cs_ext.cpp:                last_gpu,i);
lib/gpu/lal_coul_long_cs_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_coul_long_cs_ext.cpp:                        cell_size, gpu_split, screen, host_cut_coulsq,
lib/gpu/lal_coul_long_cs_ext.cpp:    CLCSMF.estimate_gpu_overhead();
lib/gpu/lal_coul_long_cs_ext.cpp:void clcs_gpu_reinit(const int ntypes, double **host_scale) {
lib/gpu/lal_coul_long_cs_ext.cpp:  int gpu_rank=CLCSMF.device->gpu_rank();
lib/gpu/lal_coul_long_cs_ext.cpp:  int procs_per_gpu=CLCSMF.device->procs_per_gpu();
lib/gpu/lal_coul_long_cs_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_coul_long_cs_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_coul_long_cs_ext.cpp:void clcs_gpu_clear() {
lib/gpu/lal_coul_long_cs_ext.cpp:int** clcs_gpu_compute_n(const int ago, const int inum_full,
lib/gpu/lal_coul_long_cs_ext.cpp:void clcs_gpu_compute(const int ago, const int inum_full, const int nall,
lib/gpu/lal_coul_long_cs_ext.cpp:double clcs_gpu_bytes() {
lib/gpu/lal_gayberne.cpp:#if defined(USE_OPENCL)
lib/gpu/lal_gayberne.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_gayberne.cpp:                         const double gpu_split, FILE *_screen) {
lib/gpu/lal_gayberne.cpp:  success=this->init_base(nlocal,nall,max_nbors,maxspecial,cell_size,gpu_split,
lib/gpu/lal_gayberne.cpp:  // Copy shape, well, sigma, epsilon, and cutsq onto GPU
lib/gpu/lal_ufm_ext.cpp:int ufml_gpu_init(const int ntypes, double **cutsq, double **host_uf1,
lib/gpu/lal_ufm_ext.cpp:                 int &gpu_mode, FILE *screen) {
lib/gpu/lal_ufm_ext.cpp:  gpu_mode=UFMLMF.device->gpu_mode();
lib/gpu/lal_ufm_ext.cpp:  double gpu_split=UFMLMF.device->particle_split();
lib/gpu/lal_ufm_ext.cpp:  int first_gpu=UFMLMF.device->first_device();
lib/gpu/lal_ufm_ext.cpp:  int last_gpu=UFMLMF.device->last_device();
lib/gpu/lal_ufm_ext.cpp:  int gpu_rank=UFMLMF.device->gpu_rank();
lib/gpu/lal_ufm_ext.cpp:  int procs_per_gpu=UFMLMF.device->procs_per_gpu();
lib/gpu/lal_ufm_ext.cpp:  UFMLMF.device->init_message(screen,"ufm",first_gpu,last_gpu);
lib/gpu/lal_ufm_ext.cpp:                        maxspecial, cell_size, gpu_split, screen);
lib/gpu/lal_ufm_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_ufm_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_ufm_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_ufm_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_ufm_ext.cpp:                last_gpu,i);
lib/gpu/lal_ufm_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_ufm_ext.cpp:                         cell_size, gpu_split, screen);
lib/gpu/lal_ufm_ext.cpp:    UFMLMF.estimate_gpu_overhead();
lib/gpu/lal_ufm_ext.cpp:void ufml_gpu_reinit(const int ntypes, double **cutsq, double **host_uf1,
lib/gpu/lal_ufm_ext.cpp:  int gpu_rank=UFMLMF.device->gpu_rank();
lib/gpu/lal_ufm_ext.cpp:  int procs_per_gpu=UFMLMF.device->procs_per_gpu();
lib/gpu/lal_ufm_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_ufm_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_ufm_ext.cpp:void ufml_gpu_clear() {
lib/gpu/lal_ufm_ext.cpp:int ** ufml_gpu_compute_n(const int ago, const int inum_full,
lib/gpu/lal_ufm_ext.cpp:void ufml_gpu_compute(const int ago, const int inum_full, const int nall,
lib/gpu/lal_ufm_ext.cpp:double ufml_gpu_bytes() {
lib/gpu/lal_gauss.cpp:#ifdef USE_OPENCL
lib/gpu/lal_gauss.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_gauss.cpp:                 const double gpu_split, FILE *_screen) {
lib/gpu/lal_gauss.cpp:  success=this->init_atomic(nlocal,nall,max_nbors,maxspecial,cell_size,gpu_split,
lib/gpu/lal_dpd_ext.cpp:int dpd_gpu_init(const int ntypes, double **cutsq, double **host_a0,
lib/gpu/lal_dpd_ext.cpp:                 const double cell_size, int &gpu_mode, FILE *screen) {
lib/gpu/lal_dpd_ext.cpp:  gpu_mode=DPDMF.device->gpu_mode();
lib/gpu/lal_dpd_ext.cpp:  double gpu_split=DPDMF.device->particle_split();
lib/gpu/lal_dpd_ext.cpp:  int first_gpu=DPDMF.device->first_device();
lib/gpu/lal_dpd_ext.cpp:  int last_gpu=DPDMF.device->last_device();
lib/gpu/lal_dpd_ext.cpp:  int gpu_rank=DPDMF.device->gpu_rank();
lib/gpu/lal_dpd_ext.cpp:  int procs_per_gpu=DPDMF.device->procs_per_gpu();
lib/gpu/lal_dpd_ext.cpp:  DPDMF.device->init_message(screen,"dpd",first_gpu,last_gpu);
lib/gpu/lal_dpd_ext.cpp:                       maxspecial, cell_size, gpu_split, screen);
lib/gpu/lal_dpd_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_dpd_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_dpd_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_dpd_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_dpd_ext.cpp:                last_gpu,i);
lib/gpu/lal_dpd_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_dpd_ext.cpp:                         maxspecial, cell_size, gpu_split, screen);
lib/gpu/lal_dpd_ext.cpp:    DPDMF.estimate_gpu_overhead();
lib/gpu/lal_dpd_ext.cpp:void dpd_gpu_clear() {
lib/gpu/lal_dpd_ext.cpp:int ** dpd_gpu_compute_n(const int ago, const int inum_full, const int nall,
lib/gpu/lal_dpd_ext.cpp:void dpd_gpu_compute(const int ago, const int inum_full, const int nall,
lib/gpu/lal_dpd_ext.cpp:void dpd_gpu_update_coeff(int ntypes, double **host_a0, double **host_gamma,
lib/gpu/lal_dpd_ext.cpp:double dpd_gpu_bytes() {
lib/gpu/lal_morse.cpp:#if defined(USE_OPENCL)
lib/gpu/lal_morse.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_morse.cpp:                          const double gpu_split, FILE *_screen) {
lib/gpu/lal_morse.cpp:  success=this->init_atomic(nlocal,nall,max_nbors,maxspecial,cell_size,gpu_split,
lib/gpu/Install.py:Install.py tool to build the GPU library
lib/gpu/Install.py:Syntax from src dir: make lib-gpu args="-m machine -h hdir -a arch -p precision -e esuffix -b -o osuffix"
lib/gpu/Install.py:copies an existing Makefile.machine in lib/gpu to Makefile.auto
lib/gpu/Install.py:  CUDA_HOME, CUDA_ARCH, CUDA_PRECISION, EXTRAMAKE
lib/gpu/Install.py:optionally uses Makefile.auto to build the GPU library -> libgpu.a
lib/gpu/Install.py:See lib/gpu/README and the LAMMPS manual for more information
lib/gpu/Install.py:make lib-gpu args="-b"      # build GPU lib with default Makefile.linux
lib/gpu/Install.py:make lib-gpu args="-m xk7 -p single -o xk7.single"      # create new Makefile.xk7.single, altered for single-precision
lib/gpu/Install.py:make lib-gpu args="-m mpi -a sm_35 -p single -o mpi.mixed -b" # create new Makefile.mpi.mixed, also build GPU lib with these settings
lib/gpu/Install.py:                    help="build the GPU library from scratch from a customized Makefile.auto")
lib/gpu/Install.py:                    help="set GPU architecture and instruction set (default: 'sm_50')")
lib/gpu/Install.py:                    help="set GPU kernel precision mode (default: mixed)")
lib/gpu/Install.py:parser.add_argument("-c", "--cuda",
lib/gpu/Install.py:                    help="set CUDA_HOME variable in Makefile.auto. Will be used if $CUDA_HOME environment variable is not set")
lib/gpu/Install.py:if args.cuda:
lib/gpu/Install.py:  hdir = args.cuda
lib/gpu/Install.py:# reset EXTRAMAKE, CUDA_HOME, CUDA_ARCH, CUDA_PRECISION if requested
lib/gpu/Install.py:  sys.exit("lib/gpu/Makefile.%s does not exist" % isuffix)
lib/gpu/Install.py:  if hflag and words[0] == "CUDA_HOME" and words[1] == '=':
lib/gpu/Install.py:  if words[0] == "CUDA_ARCH" and words[1] == '=':
lib/gpu/Install.py:  if words[0] == "CUDA_PRECISION" and words[1] == '=':
lib/gpu/Install.py:  print("Building libgpu.a ...")
lib/gpu/Install.py:  if os.path.exists("libgpu.a"):
lib/gpu/Install.py:    os.remove("libgpu.a")
lib/gpu/Install.py:  if not os.path.exists("libgpu.a"):
lib/gpu/Install.py:    sys.exit("Build of lib/gpu/libgpu.a was NOT successful")
lib/gpu/Install.py:    sys.exit("lib/gpu/Makefile.lammps was NOT created")
lib/gpu/lal_lj_coul_long.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_lj_coul_long.h:    * - -1 if fix gpu not found
lib/gpu/lal_lj_coul_long.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_lj_coul_long.h:           const double gpu_split, FILE *screen, double **host_cut_ljsq,
lib/gpu/lal_born_coul_long.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_born_coul_long.h:    * - -1 if fix gpu not found
lib/gpu/lal_born_coul_long.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_born_coul_long.h:           const double gpu_split, FILE *screen, double **host_cut_ljsq,
lib/gpu/lal_dipole_long_lj.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_dipole_long_lj.h:    * - -1 if fix gpu not found
lib/gpu/lal_dipole_long_lj.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_dipole_long_lj.h:           const double gpu_split, FILE *screen, double **host_cut_ljsq,
lib/gpu/lal_eam_ext.cpp:int eam_gpu_init(const int ntypes, double host_cutforcesq,
lib/gpu/lal_eam_ext.cpp:                 int &gpu_mode, FILE *screen, int &fp_size) {
lib/gpu/lal_eam_ext.cpp:  gpu_mode=EAMMF.device->gpu_mode();
lib/gpu/lal_eam_ext.cpp:  double gpu_split=EAMMF.device->particle_split();
lib/gpu/lal_eam_ext.cpp:  int first_gpu=EAMMF.device->first_device();
lib/gpu/lal_eam_ext.cpp:  int last_gpu=EAMMF.device->last_device();
lib/gpu/lal_eam_ext.cpp:  int gpu_rank=EAMMF.device->gpu_rank();
lib/gpu/lal_eam_ext.cpp:  int procs_per_gpu=EAMMF.device->procs_per_gpu();
lib/gpu/lal_eam_ext.cpp:  if (gpu_split != 1.0)
lib/gpu/lal_eam_ext.cpp:  EAMMF.device->init_message(screen,"eam",first_gpu,last_gpu);
lib/gpu/lal_eam_ext.cpp:                       gpu_split, screen);
lib/gpu/lal_eam_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_eam_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_eam_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_eam_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_eam_ext.cpp:                last_gpu,i);
lib/gpu/lal_eam_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_eam_ext.cpp:                         cell_size, gpu_split, screen);
lib/gpu/lal_eam_ext.cpp:    EAMMF.estimate_gpu_overhead(1);
lib/gpu/lal_eam_ext.cpp:void eam_gpu_clear() {
lib/gpu/lal_eam_ext.cpp:int ** eam_gpu_compute_n(const int ago, const int inum_full,
lib/gpu/lal_eam_ext.cpp:void eam_gpu_compute(const int ago, const int inum_full, const int nlocal,
lib/gpu/lal_eam_ext.cpp:void eam_gpu_compute_force(int *ilist, const bool eflag, const bool vflag,
lib/gpu/lal_eam_ext.cpp:double eam_gpu_bytes() {
lib/gpu/lal_tersoff_zbl_ext.cpp:int tersoff_zbl_gpu_init(const int ntypes, const int inum, const int nall,
lib/gpu/lal_tersoff_zbl_ext.cpp:                     const int max_nbors, const double cell_size, int &gpu_mode,
lib/gpu/lal_tersoff_zbl_ext.cpp:  gpu_mode=TSZMF.device->gpu_mode();
lib/gpu/lal_tersoff_zbl_ext.cpp:  double gpu_split=TSZMF.device->particle_split();
lib/gpu/lal_tersoff_zbl_ext.cpp:  int first_gpu=TSZMF.device->first_device();
lib/gpu/lal_tersoff_zbl_ext.cpp:  int last_gpu=TSZMF.device->last_device();
lib/gpu/lal_tersoff_zbl_ext.cpp:  int gpu_rank=TSZMF.device->gpu_rank();
lib/gpu/lal_tersoff_zbl_ext.cpp:  int procs_per_gpu=TSZMF.device->procs_per_gpu();
lib/gpu/lal_tersoff_zbl_ext.cpp:  if (gpu_split != 1.0)
lib/gpu/lal_tersoff_zbl_ext.cpp:  TSZMF.device->init_message(screen,"tersoff/zbl/gpu",first_gpu,last_gpu);
lib/gpu/lal_tersoff_zbl_ext.cpp:    init_ok=TSZMF.init(ntypes, inum, nall, max_nbors, cell_size, gpu_split, screen,
lib/gpu/lal_tersoff_zbl_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_tersoff_zbl_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_tersoff_zbl_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_tersoff_zbl_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_tersoff_zbl_ext.cpp:                last_gpu,i);
lib/gpu/lal_tersoff_zbl_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_tersoff_zbl_ext.cpp:      init_ok=TSZMF.init(ntypes, inum, nall, max_nbors, cell_size, gpu_split, screen,
lib/gpu/lal_tersoff_zbl_ext.cpp:    TSZMF.estimate_gpu_overhead(1);
lib/gpu/lal_tersoff_zbl_ext.cpp:void tersoff_zbl_gpu_clear() {
lib/gpu/lal_tersoff_zbl_ext.cpp:int ** tersoff_zbl_gpu_compute_n(const int ago, const int inum_full,
lib/gpu/lal_tersoff_zbl_ext.cpp:void tersoff_zbl_gpu_compute(const int ago, const int nlocal, const int nall,
lib/gpu/lal_tersoff_zbl_ext.cpp:double tersoff_zbl_gpu_bytes() {
lib/gpu/lal_amoeba.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_amoeba.h:    * - -1 if fix gpu not found
lib/gpu/lal_amoeba.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_amoeba.h:           const double gpu_split, FILE *_screen,
lib/gpu/lal_mie_ext.cpp:int mie_gpu_init(const int ntypes, double **cutsq, double **host_mie1,
lib/gpu/lal_mie_ext.cpp:                 const double cell_size, int &gpu_mode, FILE *screen) {
lib/gpu/lal_mie_ext.cpp:  gpu_mode=MLMF.device->gpu_mode();
lib/gpu/lal_mie_ext.cpp:  double gpu_split=MLMF.device->particle_split();
lib/gpu/lal_mie_ext.cpp:  int first_gpu=MLMF.device->first_device();
lib/gpu/lal_mie_ext.cpp:  int last_gpu=MLMF.device->last_device();
lib/gpu/lal_mie_ext.cpp:  int gpu_rank=MLMF.device->gpu_rank();
lib/gpu/lal_mie_ext.cpp:  int procs_per_gpu=MLMF.device->procs_per_gpu();
lib/gpu/lal_mie_ext.cpp:  MLMF.device->init_message(screen,"mie",first_gpu,last_gpu);
lib/gpu/lal_mie_ext.cpp:                      maxspecial, cell_size, gpu_split, screen);
lib/gpu/lal_mie_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_mie_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_mie_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_mie_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_mie_ext.cpp:                last_gpu,i);
lib/gpu/lal_mie_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_mie_ext.cpp:                        cell_size, gpu_split, screen);
lib/gpu/lal_mie_ext.cpp:    MLMF.estimate_gpu_overhead();
lib/gpu/lal_mie_ext.cpp:void mie_gpu_clear() {
lib/gpu/lal_mie_ext.cpp:int ** mie_gpu_compute_n(const int ago, const int inum_full,
lib/gpu/lal_mie_ext.cpp:void mie_gpu_compute(const int ago, const int inum_full, const int nall,
lib/gpu/lal_mie_ext.cpp:double mie_gpu_bytes() {
lib/gpu/lal_table_ext.cpp:int table_gpu_init(const int ntypes, double **cutsq, double ***table_coeffs,
lib/gpu/lal_table_ext.cpp:                 const double cell_size, int &gpu_mode, FILE *screen,
lib/gpu/lal_table_ext.cpp:  gpu_mode=TBMF.device->gpu_mode();
lib/gpu/lal_table_ext.cpp:  double gpu_split=TBMF.device->particle_split();
lib/gpu/lal_table_ext.cpp:  int first_gpu=TBMF.device->first_device();
lib/gpu/lal_table_ext.cpp:  int last_gpu=TBMF.device->last_device();
lib/gpu/lal_table_ext.cpp:  int gpu_rank=TBMF.device->gpu_rank();
lib/gpu/lal_table_ext.cpp:  int procs_per_gpu=TBMF.device->procs_per_gpu();
lib/gpu/lal_table_ext.cpp:  TBMF.device->init_message(screen,"table",first_gpu,last_gpu);
lib/gpu/lal_table_ext.cpp:                      gpu_split, screen, tabstyle, ntables, tablength);
lib/gpu/lal_table_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_table_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_table_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_table_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_table_ext.cpp:                last_gpu,i);
lib/gpu/lal_table_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_table_ext.cpp:                      gpu_split, screen, tabstyle, ntables, tablength);
lib/gpu/lal_table_ext.cpp:    TBMF.estimate_gpu_overhead();
lib/gpu/lal_table_ext.cpp:void table_gpu_clear() {
lib/gpu/lal_table_ext.cpp:int ** table_gpu_compute_n(const int ago, const int inum_full,
lib/gpu/lal_table_ext.cpp:void table_gpu_compute(const int ago, const int inum_full, const int nall,
lib/gpu/lal_table_ext.cpp:double table_gpu_bytes() {
lib/gpu/lal_lj_coul_long_soft_ext.cpp:int ljcls_gpu_init(const int ntypes, double **cutsq, double **host_lj1,
lib/gpu/lal_lj_coul_long_soft_ext.cpp:                  const double cell_size, int &gpu_mode, FILE *screen,
lib/gpu/lal_lj_coul_long_soft_ext.cpp:  gpu_mode=LJCLSMF.device->gpu_mode();
lib/gpu/lal_lj_coul_long_soft_ext.cpp:  double gpu_split=LJCLSMF.device->particle_split();
lib/gpu/lal_lj_coul_long_soft_ext.cpp:  int first_gpu=LJCLSMF.device->first_device();
lib/gpu/lal_lj_coul_long_soft_ext.cpp:  int last_gpu=LJCLSMF.device->last_device();
lib/gpu/lal_lj_coul_long_soft_ext.cpp:  int gpu_rank=LJCLSMF.device->gpu_rank();
lib/gpu/lal_lj_coul_long_soft_ext.cpp:  int procs_per_gpu=LJCLSMF.device->procs_per_gpu();
lib/gpu/lal_lj_coul_long_soft_ext.cpp:  LJCLSMF.device->init_message(screen,"lj/cut/coul/long/soft",first_gpu,last_gpu);
lib/gpu/lal_lj_coul_long_soft_ext.cpp:                        cell_size, gpu_split, screen, host_cut_ljsq,
lib/gpu/lal_lj_coul_long_soft_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_lj_coul_long_soft_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_lj_coul_long_soft_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_lj_coul_long_soft_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_lj_coul_long_soft_ext.cpp:                last_gpu,i);
lib/gpu/lal_lj_coul_long_soft_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_lj_coul_long_soft_ext.cpp:                          cell_size, gpu_split, screen, host_cut_ljsq,
lib/gpu/lal_lj_coul_long_soft_ext.cpp:    LJCLSMF.device->gpu_barrier();
lib/gpu/lal_lj_coul_long_soft_ext.cpp:    LJCLSMF.estimate_gpu_overhead();
lib/gpu/lal_lj_coul_long_soft_ext.cpp:void ljcls_gpu_reinit(const int ntypes, double **cutsq, double **host_lj1,
lib/gpu/lal_lj_coul_long_soft_ext.cpp:  int gpu_rank=LJCLSMF.device->gpu_rank();
lib/gpu/lal_lj_coul_long_soft_ext.cpp:  int procs_per_gpu=LJCLSMF.device->procs_per_gpu();
lib/gpu/lal_lj_coul_long_soft_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_lj_coul_long_soft_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_lj_coul_long_soft_ext.cpp:    LJCLSMF.device->gpu_barrier();
lib/gpu/lal_lj_coul_long_soft_ext.cpp:void ljcls_gpu_clear() {
lib/gpu/lal_lj_coul_long_soft_ext.cpp:int** ljcls_gpu_compute_n(const int ago, const int inum_full,
lib/gpu/lal_lj_coul_long_soft_ext.cpp:void ljcls_gpu_compute(const int ago, const int inum_full, const int nall,
lib/gpu/lal_lj_coul_long_soft_ext.cpp:double ljcls_gpu_bytes() {
lib/gpu/lal_coul.cpp:#if defined(USE_OPENCL)
lib/gpu/lal_coul.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_coul.cpp:                const double gpu_split, FILE *_screen,
lib/gpu/lal_coul.cpp:  success=this->init_atomic(nlocal,nall,max_nbors,maxspecial,cell_size,gpu_split,
lib/gpu/lal_soft_ext.cpp:int soft_gpu_init(const int ntypes, double **cutsq, double **host_prefactor,
lib/gpu/lal_soft_ext.cpp:                  const double cell_size, int &gpu_mode, FILE *screen) {
lib/gpu/lal_soft_ext.cpp:  gpu_mode=SLMF.device->gpu_mode();
lib/gpu/lal_soft_ext.cpp:  double gpu_split=SLMF.device->particle_split();
lib/gpu/lal_soft_ext.cpp:  int first_gpu=SLMF.device->first_device();
lib/gpu/lal_soft_ext.cpp:  int last_gpu=SLMF.device->last_device();
lib/gpu/lal_soft_ext.cpp:  int gpu_rank=SLMF.device->gpu_rank();
lib/gpu/lal_soft_ext.cpp:  int procs_per_gpu=SLMF.device->procs_per_gpu();
lib/gpu/lal_soft_ext.cpp:  SLMF.device->init_message(screen,"soft",first_gpu,last_gpu);
lib/gpu/lal_soft_ext.cpp:                      maxspecial, cell_size, gpu_split, screen);
lib/gpu/lal_soft_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_soft_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_soft_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_soft_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_soft_ext.cpp:                last_gpu,i);
lib/gpu/lal_soft_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_soft_ext.cpp:                        cell_size, gpu_split, screen);
lib/gpu/lal_soft_ext.cpp:    SLMF.estimate_gpu_overhead();
lib/gpu/lal_soft_ext.cpp:void soft_gpu_reinit(const int ntypes, double **cutsq, double **host_prefactor,
lib/gpu/lal_soft_ext.cpp:  int gpu_rank=SLMF.device->gpu_rank();
lib/gpu/lal_soft_ext.cpp:  int procs_per_gpu=SLMF.device->procs_per_gpu();
lib/gpu/lal_soft_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_soft_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_soft_ext.cpp:void soft_gpu_clear() {
lib/gpu/lal_soft_ext.cpp:int ** soft_gpu_compute_n(const int ago, const int inum_full,
lib/gpu/lal_soft_ext.cpp:void soft_gpu_compute(const int ago, const int inum_full, const int nall,
lib/gpu/lal_soft_ext.cpp:double soft_gpu_bytes() {
lib/gpu/lal_dipole_lj_sf.cpp:#ifdef USE_OPENCL
lib/gpu/lal_dipole_lj_sf.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_dipole_lj_sf.cpp:                      const double gpu_split, FILE *_screen,
lib/gpu/lal_dipole_lj_sf.cpp:  success=this->init_atomic(nlocal,nall,max_nbors,maxspecial,cell_size,gpu_split,
lib/gpu/lal_born.cpp:#ifdef USE_OPENCL
lib/gpu/lal_born.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_born.cpp:                const double gpu_split, FILE *_screen) {
lib/gpu/lal_born.cpp:  success=this->init_atomic(nlocal,nall,max_nbors,maxspecial,cell_size,gpu_split,
lib/gpu/lal_amoeba.cpp:#if defined(USE_OPENCL)
lib/gpu/lal_amoeba.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_amoeba.cpp:                  const double cell_size, const double gpu_split, FILE *_screen,
lib/gpu/lal_amoeba.cpp:                            cell_size,gpu_split,_screen,amoeba,
lib/gpu/lal_eam_alloy_ext.cpp:int eam_alloy_gpu_init(const int ntypes, double host_cutforcesq,
lib/gpu/lal_eam_alloy_ext.cpp:                 int &gpu_mode, FILE *screen, int &fp_size) {
lib/gpu/lal_eam_alloy_ext.cpp:  gpu_mode=EAMALMF.device->gpu_mode();
lib/gpu/lal_eam_alloy_ext.cpp:  double gpu_split=EAMALMF.device->particle_split();
lib/gpu/lal_eam_alloy_ext.cpp:  int first_gpu=EAMALMF.device->first_device();
lib/gpu/lal_eam_alloy_ext.cpp:  int last_gpu=EAMALMF.device->last_device();
lib/gpu/lal_eam_alloy_ext.cpp:  int gpu_rank=EAMALMF.device->gpu_rank();
lib/gpu/lal_eam_alloy_ext.cpp:  int procs_per_gpu=EAMALMF.device->procs_per_gpu();
lib/gpu/lal_eam_alloy_ext.cpp:  if (gpu_split != 1.0)
lib/gpu/lal_eam_alloy_ext.cpp:  EAMALMF.device->init_message(screen,"eam/alloy",first_gpu,last_gpu);
lib/gpu/lal_eam_alloy_ext.cpp:                       gpu_split, screen);
lib/gpu/lal_eam_alloy_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_eam_alloy_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_eam_alloy_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_eam_alloy_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_eam_alloy_ext.cpp:                last_gpu,i);
lib/gpu/lal_eam_alloy_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_eam_alloy_ext.cpp:                         cell_size, gpu_split, screen);
lib/gpu/lal_eam_alloy_ext.cpp:    EAMALMF.estimate_gpu_overhead();
lib/gpu/lal_eam_alloy_ext.cpp:void eam_alloy_gpu_clear() {
lib/gpu/lal_eam_alloy_ext.cpp:int ** eam_alloy_gpu_compute_n(const int ago, const int inum_full,
lib/gpu/lal_eam_alloy_ext.cpp:void eam_alloy_gpu_compute(const int ago, const int inum_full, const int nlocal,
lib/gpu/lal_eam_alloy_ext.cpp:void eam_alloy_gpu_compute_force(int *ilist, const bool eflag, const bool vflag,
lib/gpu/lal_eam_alloy_ext.cpp:double eam_alloy_gpu_bytes() {
lib/gpu/lal_eam.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_eam.h:    * - -1 if fix gpu not found
lib/gpu/lal_eam.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_eam.h:           const double gpu_split, FILE *_screen);
lib/gpu/lal_born_coul_wolf_cs.cpp:#ifdef USE_OPENCL
lib/gpu/lal_born_coul_wolf_cs.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_born_coul_wolf_cs.cpp:                        const double gpu_split, FILE *_screen,
lib/gpu/lal_born_coul_wolf_cs.cpp:  success=this->init_atomic(nlocal,nall,max_nbors,maxspecial,cell_size,gpu_split,
lib/gpu/lal_dipole_lj_sf_ext.cpp:int dplsf_gpu_init(const int ntypes, double **cutsq, double **host_lj1,
lib/gpu/lal_dipole_lj_sf_ext.cpp:                   const double cell_size, int &gpu_mode, FILE *screen,
lib/gpu/lal_dipole_lj_sf_ext.cpp:  gpu_mode=DPLSFMF.device->gpu_mode();
lib/gpu/lal_dipole_lj_sf_ext.cpp:  double gpu_split=DPLSFMF.device->particle_split();
lib/gpu/lal_dipole_lj_sf_ext.cpp:  int first_gpu=DPLSFMF.device->first_device();
lib/gpu/lal_dipole_lj_sf_ext.cpp:  int last_gpu=DPLSFMF.device->last_device();
lib/gpu/lal_dipole_lj_sf_ext.cpp:  int gpu_rank=DPLSFMF.device->gpu_rank();
lib/gpu/lal_dipole_lj_sf_ext.cpp:  int procs_per_gpu=DPLSFMF.device->procs_per_gpu();
lib/gpu/lal_dipole_lj_sf_ext.cpp:  DPLSFMF.device->init_message(screen,"dipole/sf",first_gpu,last_gpu);
lib/gpu/lal_dipole_lj_sf_ext.cpp:                         maxspecial, cell_size, gpu_split, screen, host_cut_ljsq,
lib/gpu/lal_dipole_lj_sf_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_dipole_lj_sf_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_dipole_lj_sf_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_dipole_lj_sf_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_dipole_lj_sf_ext.cpp:                last_gpu,i);
lib/gpu/lal_dipole_lj_sf_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_dipole_lj_sf_ext.cpp:                           cell_size, gpu_split, screen, host_cut_ljsq,
lib/gpu/lal_dipole_lj_sf_ext.cpp:    DPLSFMF.estimate_gpu_overhead();
lib/gpu/lal_dipole_lj_sf_ext.cpp:void dplsf_gpu_clear() {
lib/gpu/lal_dipole_lj_sf_ext.cpp:int** dplsf_gpu_compute_n(const int ago, const int inum_full,
lib/gpu/lal_dipole_lj_sf_ext.cpp:void dplsf_gpu_compute(const int ago, const int inum_full, const int nall,
lib/gpu/lal_dipole_lj_sf_ext.cpp:double dplsf_gpu_bytes() {
lib/gpu/lal_lj_gromacs_ext.cpp:int ljgrm_gpu_init(const int ntypes, double **cutsq, double **host_lj1,
lib/gpu/lal_lj_gromacs_ext.cpp:                   const double cell_size, int &gpu_mode, FILE *screen,
lib/gpu/lal_lj_gromacs_ext.cpp:  gpu_mode=LJGRMMF.device->gpu_mode();
lib/gpu/lal_lj_gromacs_ext.cpp:  double gpu_split=LJGRMMF.device->particle_split();
lib/gpu/lal_lj_gromacs_ext.cpp:  int first_gpu=LJGRMMF.device->first_device();
lib/gpu/lal_lj_gromacs_ext.cpp:  int last_gpu=LJGRMMF.device->last_device();
lib/gpu/lal_lj_gromacs_ext.cpp:  int gpu_rank=LJGRMMF.device->gpu_rank();
lib/gpu/lal_lj_gromacs_ext.cpp:  int procs_per_gpu=LJGRMMF.device->procs_per_gpu();
lib/gpu/lal_lj_gromacs_ext.cpp:  LJGRMMF.device->init_message(screen,"lj/gromacs",first_gpu,last_gpu);
lib/gpu/lal_lj_gromacs_ext.cpp:    fprintf(screen,"Initializing GPU and compiling on process 0...");
lib/gpu/lal_lj_gromacs_ext.cpp:                 gpu_split, screen, host_ljsw1, host_ljsw2, host_ljsw3,
lib/gpu/lal_lj_gromacs_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_lj_gromacs_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_lj_gromacs_ext.cpp:        fprintf(screen,"Initializing GPU %d on core %d...",first_gpu,i);
lib/gpu/lal_lj_gromacs_ext.cpp:        fprintf(screen,"Initializing GPUs %d-%d on core %d...",first_gpu,
lib/gpu/lal_lj_gromacs_ext.cpp:                last_gpu,i);
lib/gpu/lal_lj_gromacs_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_lj_gromacs_ext.cpp:                           gpu_split, screen, host_ljsw1, host_ljsw2, host_ljsw3,
lib/gpu/lal_lj_gromacs_ext.cpp:    LJGRMMF.estimate_gpu_overhead();
lib/gpu/lal_lj_gromacs_ext.cpp:void ljgrm_gpu_clear() {
lib/gpu/lal_lj_gromacs_ext.cpp:int ** ljgrm_gpu_compute_n(const int ago, const int inum_full,
lib/gpu/lal_lj_gromacs_ext.cpp:void ljgrm_gpu_compute(const int ago, const int inum_full, const int nall,
lib/gpu/lal_lj_gromacs_ext.cpp:double ljgrm_gpu_bytes() {
lib/gpu/Makefile.aurora:#  Generic Linux Makefile for OpenCL
lib/gpu/Makefile.aurora:EXTRAMAKE = Makefile.lammps.opencl
lib/gpu/Makefile.aurora:# OCL_TUNE = -DFERMI_OCL       # -- Uncomment for NVIDIA Fermi
lib/gpu/Makefile.aurora:# OCL_TUNE = -DKEPLER_OCL    # -- Uncomment for NVIDIA Kepler
lib/gpu/Makefile.aurora:OCL_CPP = mpicxx -cxx=icpx -DCUDA_PROXY $(DEFAULT_DEVICE) -xHost -O2 -ffp-model=fast -qoverride-limits -DMPI_GERYON -DGERYON_NUMA_FISSION -DUCL_NO_EXIT -DMPICH_IGNORE_CXX_SEEK $(LMP_INC) $(OCL_INC) -DGERYON_NO_PROF
lib/gpu/Makefile.aurora:OCL_LINK = -L/opt/intel/oneapi/compiler/latest/linux/lib/ -lOpenCL
lib/gpu/Makefile.aurora:include Opencl.makefile
lib/gpu/lal_dpd_tstat_ext.cpp:int dpd_tstat_gpu_init(const int ntypes, double **cutsq, double **host_a0,
lib/gpu/lal_dpd_tstat_ext.cpp:                       const double cell_size, int &gpu_mode, FILE *screen) {
lib/gpu/lal_dpd_tstat_ext.cpp:  gpu_mode=DPDTMF.device->gpu_mode();
lib/gpu/lal_dpd_tstat_ext.cpp:  double gpu_split=DPDTMF.device->particle_split();
lib/gpu/lal_dpd_tstat_ext.cpp:  int first_gpu=DPDTMF.device->first_device();
lib/gpu/lal_dpd_tstat_ext.cpp:  int last_gpu=DPDTMF.device->last_device();
lib/gpu/lal_dpd_tstat_ext.cpp:  int gpu_rank=DPDTMF.device->gpu_rank();
lib/gpu/lal_dpd_tstat_ext.cpp:  int procs_per_gpu=DPDTMF.device->procs_per_gpu();
lib/gpu/lal_dpd_tstat_ext.cpp:  DPDTMF.device->init_message(screen,"dpd/tstat",first_gpu,last_gpu);
lib/gpu/lal_dpd_tstat_ext.cpp:                       maxspecial, cell_size, gpu_split, screen);
lib/gpu/lal_dpd_tstat_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_dpd_tstat_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_dpd_tstat_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_dpd_tstat_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_dpd_tstat_ext.cpp:                last_gpu,i);
lib/gpu/lal_dpd_tstat_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_dpd_tstat_ext.cpp:                         maxspecial, cell_size, gpu_split, screen);
lib/gpu/lal_dpd_tstat_ext.cpp:    DPDTMF.estimate_gpu_overhead();
lib/gpu/lal_dpd_tstat_ext.cpp:void dpd_tstat_gpu_clear() {
lib/gpu/lal_dpd_tstat_ext.cpp:int ** dpd_tstat_gpu_compute_n(const int ago, const int inum_full, const int nall,
lib/gpu/lal_dpd_tstat_ext.cpp:void dpd_tstat_gpu_compute(const int ago, const int inum_full, const int nall,
lib/gpu/lal_dpd_tstat_ext.cpp:void dpd_tstat_gpu_update_coeff(int ntypes, double **host_a0, double **host_gamma,
lib/gpu/lal_dpd_tstat_ext.cpp:double dpd_tstat_gpu_bytes() {
lib/gpu/lal_soft.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_soft.h:    * - -1 if fix gpu not found
lib/gpu/lal_soft.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_soft.h:           const double gpu_split, FILE *screen);
lib/gpu/lal_lj_tip4p_long.cu:#ifdef USE_OPENCL
lib/gpu/lal_lj_tip4p_long.cu:#ifdef USE_OPENCL
lib/gpu/lal_lj_tip4p_long.cu:   GPU analogue of Atom::map inline method,
lib/gpu/lal_lj_tip4p_long.cu:   GPU version of Domain::closest_image(int, int) method.
lib/gpu/lal_lj.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_lj.h:    * - -1 if fix gpu not found
lib/gpu/lal_lj.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_lj.h:           const double gpu_split, FILE *screen);
lib/gpu/lal_atom.h:#ifndef PAIR_GPU_ATOM_H
lib/gpu/lal_atom.h:#define PAIR_GPU_ATOM_H
lib/gpu/lal_atom.h:#if defined(USE_OPENCL)
lib/gpu/lal_atom.h:using namespace ucl_opencl;
lib/gpu/lal_atom.h:#elif defined(USE_CUDART)
lib/gpu/lal_atom.h:using namespace ucl_cudart;
lib/gpu/lal_atom.h:using namespace ucl_cudadr;
lib/gpu/lal_atom.h:    * \param gpu_nbor 0 if neighboring will be performed on host
lib/gpu/lal_atom.h:    *        gpu_nbor 1 if neighboring will be performed on device
lib/gpu/lal_atom.h:    *        gpu_nbor 2 if binning on host and neighboring on device **/
lib/gpu/lal_atom.h:            UCL_Device &dev, const int gpu_nbor=0, const bool bonds=false,
lib/gpu/lal_atom.h:    * \param gpu_nbor 0 if neighboring will be performed on host
lib/gpu/lal_atom.h:    *        gpu_nbor 1 if neighboring will be performed on device
lib/gpu/lal_atom.h:    *        gpu_nbor 2 if binning on host and neighboring on device **/
lib/gpu/lal_atom.h:  bool add_fields(const bool charge, const bool rot, const int gpu_nbor,
lib/gpu/lal_atom.h:  /// Returns true if GPU is using charges
lib/gpu/lal_atom.h:  /// Returns true if GPU is using quaternions
lib/gpu/lal_atom.h:  /// Returns true if GPU is using velocities
lib/gpu/lal_atom.h:  /// Returns true if GPU is using extra fields
lib/gpu/lal_atom.h:  // -------------------------COPY TO GPU ----------------------------------
lib/gpu/lal_atom.h:      #ifdef GPU_CAST
lib/gpu/lal_atom.h:      #ifdef GPU_CAST
lib/gpu/lal_atom.h:  inline double max_gpu_bytes()
lib/gpu/lal_atom.h:    { double m=_max_gpu_bytes; _max_gpu_bytes=0.0; return m; }
lib/gpu/lal_atom.h:  #ifdef GPU_CAST
lib/gpu/lal_atom.h:  #ifdef GPU_CAST
lib/gpu/lal_atom.h:  int _max_atoms, _nall, _gpu_nbor;
lib/gpu/lal_atom.h:  double _max_gpu_bytes;
lib/gpu/lal_lj_class2_long.cpp:#if defined(USE_OPENCL)
lib/gpu/lal_lj_class2_long.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_lj_class2_long.cpp:                        const double gpu_split, FILE *_screen,
lib/gpu/lal_lj_class2_long.cpp:  success=this->init_atomic(nlocal,nall,max_nbors,maxspecial,cell_size,gpu_split,
lib/gpu/lal_lj_spica_ext.cpp:int spica_gpu_init(const int ntypes, double **cutsq, int **cg_types,
lib/gpu/lal_lj_spica_ext.cpp:                 const int maxspecial, const double cell_size, int &gpu_mode,
lib/gpu/lal_lj_spica_ext.cpp:  gpu_mode=CMMMF.device->gpu_mode();
lib/gpu/lal_lj_spica_ext.cpp:  double gpu_split=CMMMF.device->particle_split();
lib/gpu/lal_lj_spica_ext.cpp:  int first_gpu=CMMMF.device->first_device();
lib/gpu/lal_lj_spica_ext.cpp:  int last_gpu=CMMMF.device->last_device();
lib/gpu/lal_lj_spica_ext.cpp:  int gpu_rank=CMMMF.device->gpu_rank();
lib/gpu/lal_lj_spica_ext.cpp:  int procs_per_gpu=CMMMF.device->procs_per_gpu();
lib/gpu/lal_lj_spica_ext.cpp:  CMMMF.device->init_message(screen,"lj/spica",first_gpu,last_gpu);
lib/gpu/lal_lj_spica_ext.cpp:                       maxspecial, cell_size, gpu_split, screen);
lib/gpu/lal_lj_spica_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_lj_spica_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_lj_spica_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_lj_spica_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_lj_spica_ext.cpp:                last_gpu,i);
lib/gpu/lal_lj_spica_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_lj_spica_ext.cpp:                         maxspecial, cell_size, gpu_split, screen);
lib/gpu/lal_lj_spica_ext.cpp:    CMMMF.estimate_gpu_overhead();
lib/gpu/lal_lj_spica_ext.cpp:void spica_gpu_clear() {
lib/gpu/lal_lj_spica_ext.cpp:int** spica_gpu_compute_n(const int ago, const int inum_full,
lib/gpu/lal_lj_spica_ext.cpp:void spica_gpu_compute(const int ago, const int inum_full, const int nall,
lib/gpu/lal_lj_spica_ext.cpp:double spica_gpu_bytes() {
lib/gpu/lal_born_coul_long.cpp:#ifdef USE_OPENCL
lib/gpu/lal_born_coul_long.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_born_coul_long.cpp:                       const double gpu_split, FILE *_screen,
lib/gpu/lal_born_coul_long.cpp:  success=this->init_atomic(nlocal,nall,max_nbors,maxspecial,cell_size,gpu_split,
lib/gpu/lal_dipole_long_lj_ext.cpp:int dplj_gpu_init(const int ntypes, double **cutsq, double **host_lj1,
lib/gpu/lal_dipole_long_lj_ext.cpp:                 const double cell_size, int &gpu_mode, FILE *screen,
lib/gpu/lal_dipole_long_lj_ext.cpp:  gpu_mode=DPLJMF.device->gpu_mode();
lib/gpu/lal_dipole_long_lj_ext.cpp:  double gpu_split=DPLJMF.device->particle_split();
lib/gpu/lal_dipole_long_lj_ext.cpp:  int first_gpu=DPLJMF.device->first_device();
lib/gpu/lal_dipole_long_lj_ext.cpp:  int last_gpu=DPLJMF.device->last_device();
lib/gpu/lal_dipole_long_lj_ext.cpp:  int gpu_rank=DPLJMF.device->gpu_rank();
lib/gpu/lal_dipole_long_lj_ext.cpp:  int procs_per_gpu=DPLJMF.device->procs_per_gpu();
lib/gpu/lal_dipole_long_lj_ext.cpp:  DPLJMF.device->init_message(screen,"lj/cut/dipole/long",first_gpu,last_gpu);
lib/gpu/lal_dipole_long_lj_ext.cpp:                       maxspecial, cell_size, gpu_split, screen, host_cut_ljsq,
lib/gpu/lal_dipole_long_lj_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_dipole_long_lj_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_dipole_long_lj_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_dipole_long_lj_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_dipole_long_lj_ext.cpp:                last_gpu,i);
lib/gpu/lal_dipole_long_lj_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_dipole_long_lj_ext.cpp:                         cell_size, gpu_split, screen, host_cut_ljsq,
lib/gpu/lal_dipole_long_lj_ext.cpp:    DPLJMF.estimate_gpu_overhead();
lib/gpu/lal_dipole_long_lj_ext.cpp:void dplj_gpu_clear() {
lib/gpu/lal_dipole_long_lj_ext.cpp:int** dplj_gpu_compute_n(const int ago, const int inum_full,
lib/gpu/lal_dipole_long_lj_ext.cpp:void dplj_gpu_compute(const int ago, const int inum_full, const int nall,
lib/gpu/lal_dipole_long_lj_ext.cpp:double dplj_gpu_bytes() {
lib/gpu/lal_morse_ext.cpp:int mor_gpu_init(const int ntypes, double **cutsq,
lib/gpu/lal_morse_ext.cpp:                 const int maxspecial, const double cell_size, int &gpu_mode,
lib/gpu/lal_morse_ext.cpp:  gpu_mode=MORMF.device->gpu_mode();
lib/gpu/lal_morse_ext.cpp:  double gpu_split=MORMF.device->particle_split();
lib/gpu/lal_morse_ext.cpp:  int first_gpu=MORMF.device->first_device();
lib/gpu/lal_morse_ext.cpp:  int last_gpu=MORMF.device->last_device();
lib/gpu/lal_morse_ext.cpp:  int gpu_rank=MORMF.device->gpu_rank();
lib/gpu/lal_morse_ext.cpp:  int procs_per_gpu=MORMF.device->procs_per_gpu();
lib/gpu/lal_morse_ext.cpp:  MORMF.device->init_message(screen,"morse",first_gpu,last_gpu);
lib/gpu/lal_morse_ext.cpp:                       maxspecial, cell_size, gpu_split, screen);
lib/gpu/lal_morse_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_morse_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_morse_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_morse_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_morse_ext.cpp:                last_gpu,i);
lib/gpu/lal_morse_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_morse_ext.cpp:                         cell_size, gpu_split, screen);
lib/gpu/lal_morse_ext.cpp:    MORMF.estimate_gpu_overhead();
lib/gpu/lal_morse_ext.cpp:void mor_gpu_clear() {
lib/gpu/lal_morse_ext.cpp:int** mor_gpu_compute_n(const int ago, const int inum_full,
lib/gpu/lal_morse_ext.cpp:void mor_gpu_compute(const int ago, const int inum_full, const int nall,
lib/gpu/lal_morse_ext.cpp:double mor_gpu_bytes() {
lib/gpu/lal_dipole_lj.cpp:#ifdef USE_OPENCL
lib/gpu/lal_dipole_lj.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_dipole_lj.cpp:                    const double gpu_split, FILE *_screen,
lib/gpu/lal_dipole_lj.cpp:  success=this->init_atomic(nlocal,nall,max_nbors,maxspecial,cell_size,gpu_split,
lib/gpu/lal_lj_tip4p_long.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_lj_tip4p_long.h:    * - -1 if fix gpu not found
lib/gpu/lal_lj_tip4p_long.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_lj_tip4p_long.h:           const double gpu_split, FILE *screen,
lib/gpu/lal_sph_heatconduction.cpp:#if defined(USE_OPENCL)
lib/gpu/lal_sph_heatconduction.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_sph_heatconduction.cpp:                 const double gpu_split, FILE *_screen) {
lib/gpu/lal_sph_heatconduction.cpp:  #ifdef USE_OPENCL
lib/gpu/lal_sph_heatconduction.cpp:                            gpu_split,_screen,sph_heatconduction,"k_sph_heatconduction",
lib/gpu/lal_dpd_coul_slater_long.cpp:#if defined(USE_OPENCL)
lib/gpu/lal_dpd_coul_slater_long.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_dpd_coul_slater_long.cpp:               const double gpu_split, FILE *_screen, double *host_special_coul,
lib/gpu/lal_dpd_coul_slater_long.cpp:  #ifdef USE_OPENCL
lib/gpu/lal_dpd_coul_slater_long.cpp:                            gpu_split,_screen,dpd_coul_slater_long,"k_dpd_coul_slater_long",onetype, extra_fields, need_charges);
lib/gpu/lal_colloid.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_colloid.h:    * - -1 if fix gpu not found
lib/gpu/lal_colloid.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_colloid.h:           const double gpu_split, FILE *screen);
lib/gpu/lal_eam_fs_ext.cpp:int eam_fs_gpu_init(const int ntypes, double host_cutforcesq,
lib/gpu/lal_eam_fs_ext.cpp:                 int &gpu_mode, FILE *screen, int &fp_size) {
lib/gpu/lal_eam_fs_ext.cpp:  gpu_mode=EAMFSMF.device->gpu_mode();
lib/gpu/lal_eam_fs_ext.cpp:  double gpu_split=EAMFSMF.device->particle_split();
lib/gpu/lal_eam_fs_ext.cpp:  int first_gpu=EAMFSMF.device->first_device();
lib/gpu/lal_eam_fs_ext.cpp:  int last_gpu=EAMFSMF.device->last_device();
lib/gpu/lal_eam_fs_ext.cpp:  int gpu_rank=EAMFSMF.device->gpu_rank();
lib/gpu/lal_eam_fs_ext.cpp:  int procs_per_gpu=EAMFSMF.device->procs_per_gpu();
lib/gpu/lal_eam_fs_ext.cpp:  if (gpu_split != 1.0)
lib/gpu/lal_eam_fs_ext.cpp:  EAMFSMF.device->init_message(screen,"eam/fs",first_gpu,last_gpu);
lib/gpu/lal_eam_fs_ext.cpp:                       gpu_split, screen);
lib/gpu/lal_eam_fs_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_eam_fs_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_eam_fs_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_eam_fs_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_eam_fs_ext.cpp:                last_gpu,i);
lib/gpu/lal_eam_fs_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_eam_fs_ext.cpp:                         cell_size, gpu_split, screen);
lib/gpu/lal_eam_fs_ext.cpp:    EAMFSMF.estimate_gpu_overhead();
lib/gpu/lal_eam_fs_ext.cpp:void eam_fs_gpu_clear() {
lib/gpu/lal_eam_fs_ext.cpp:int ** eam_fs_gpu_compute_n(const int ago, const int inum_full,
lib/gpu/lal_eam_fs_ext.cpp:void eam_fs_gpu_compute(const int ago, const int inum_full, const int nlocal,
lib/gpu/lal_eam_fs_ext.cpp:void eam_fs_gpu_compute_force(int *ilist, const bool eflag, const bool vflag,
lib/gpu/lal_eam_fs_ext.cpp:double eam_fs_gpu_bytes() {
lib/gpu/lal_coul_debye_ext.cpp:int cdebye_gpu_init(const int ntypes, double **host_scale, double **cutsq,
lib/gpu/lal_coul_debye_ext.cpp:                    const double cell_size, int &gpu_mode, FILE *screen,
lib/gpu/lal_coul_debye_ext.cpp:  gpu_mode=CDEMF.device->gpu_mode();
lib/gpu/lal_coul_debye_ext.cpp:  double gpu_split=CDEMF.device->particle_split();
lib/gpu/lal_coul_debye_ext.cpp:  int first_gpu=CDEMF.device->first_device();
lib/gpu/lal_coul_debye_ext.cpp:  int last_gpu=CDEMF.device->last_device();
lib/gpu/lal_coul_debye_ext.cpp:  int gpu_rank=CDEMF.device->gpu_rank();
lib/gpu/lal_coul_debye_ext.cpp:  int procs_per_gpu=CDEMF.device->procs_per_gpu();
lib/gpu/lal_coul_debye_ext.cpp:  CDEMF.device->init_message(screen,"coul/debye",first_gpu,last_gpu);
lib/gpu/lal_coul_debye_ext.cpp:    fprintf(screen,"Initializing GPU and compiling on process 0...");
lib/gpu/lal_coul_debye_ext.cpp:                       maxspecial, cell_size, gpu_split, screen, qqrd2e, kappa);
lib/gpu/lal_coul_debye_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_coul_debye_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_coul_debye_ext.cpp:        fprintf(screen,"Initializing GPU %d on core %d...",first_gpu,i);
lib/gpu/lal_coul_debye_ext.cpp:        fprintf(screen,"Initializing GPUs %d-%d on core %d...",first_gpu,
lib/gpu/lal_coul_debye_ext.cpp:                last_gpu,i);
lib/gpu/lal_coul_debye_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_coul_debye_ext.cpp:                         maxspecial, cell_size, gpu_split, screen, qqrd2e, kappa);
lib/gpu/lal_coul_debye_ext.cpp:    CDEMF.estimate_gpu_overhead();
lib/gpu/lal_coul_debye_ext.cpp:void cdebye_gpu_reinit(const int ntypes, double **host_scale) {
lib/gpu/lal_coul_debye_ext.cpp:  int gpu_rank=CDEMF.device->gpu_rank();
lib/gpu/lal_coul_debye_ext.cpp:  int procs_per_gpu=CDEMF.device->procs_per_gpu();
lib/gpu/lal_coul_debye_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_coul_debye_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_coul_debye_ext.cpp:void cdebye_gpu_clear() {
lib/gpu/lal_coul_debye_ext.cpp:int** cdebye_gpu_compute_n(const int ago, const int inum_full,
lib/gpu/lal_coul_debye_ext.cpp:void cdebye_gpu_compute(const int ago, const int inum_full, const int nall,
lib/gpu/lal_coul_debye_ext.cpp:double cdebye_gpu_bytes() {
lib/gpu/lal_yukawa_colloid.cpp:#ifdef USE_OPENCL
lib/gpu/lal_yukawa_colloid.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_yukawa_colloid.cpp:                   const double gpu_split, FILE *_screen, const double kappa) {
lib/gpu/lal_yukawa_colloid.cpp:  success=this->init_atomic(nlocal,nall,max_nbors,maxspecial,cell_size,gpu_split,
lib/gpu/lal_yukawa_colloid.cpp:// Reneighbor on GPU and then compute per-atom densities
lib/gpu/lal_yukawa_colloid.cpp:  int inum=this->hd_balancer.get_gpu_count(ago,inum_full);
lib/gpu/lal_yukawa_colloid.cpp:  // Build neighbor list on GPU if necessary
lib/gpu/lal_balance.h:  inline void init(Device<numtyp, acctyp> *gpu, const int gpu_nbor,
lib/gpu/lal_balance.h:  inline int first_host_count(const int nlocal, const double gpu_split,
lib/gpu/lal_balance.h:                              const int gpu_nbor) const {
lib/gpu/lal_balance.h:    if (gpu_nbor>0 && gpu_split!=1.0) {
lib/gpu/lal_balance.h:      if (gpu_split>0)
lib/gpu/lal_balance.h:        host_nlocal=static_cast<int>(ceil((1.0-gpu_split)*nlocal));
lib/gpu/lal_balance.h:  inline int get_gpu_count(const int ago, const int inum_full);
lib/gpu/lal_balance.h:      _device->gpu->sync();
lib/gpu/lal_balance.h:      _device->gpu_barrier();
lib/gpu/lal_balance.h:      _device->gpu->sync();
lib/gpu/lal_balance.h:      _device->gpu_barrier();
lib/gpu/lal_balance.h:  /// Calls balance() and then get_gpu_count()
lib/gpu/lal_balance.h:    return get_gpu_count(ago,inum_full);
lib/gpu/lal_balance.h:  int _gpu_nbor;
lib/gpu/lal_balance.h:void BalanceT::init(Device<numtyp, acctyp> *gpu,
lib/gpu/lal_balance.h:                           const int gpu_nbor, const double split) {
lib/gpu/lal_balance.h:  _gpu_nbor=gpu_nbor;
lib/gpu/lal_balance.h:  _device=gpu;
lib/gpu/lal_balance.h:  _device_time.init(*gpu->gpu);
lib/gpu/lal_balance.h:int BalanceT::get_gpu_count(const int ago, const int inum_full) {
lib/gpu/lal_balance.h:    double gpu_time=_device_time.seconds();
lib/gpu/lal_balance.h:    double max_gpu_time;
lib/gpu/lal_balance.h:    MPI_Allreduce(&gpu_time,&max_gpu_time,1,MPI_DOUBLE,MPI_MAX,
lib/gpu/lal_balance.h:                  _device->gpu_comm());
lib/gpu/lal_balance.h:    int host_inum=static_cast<int>((max_gpu_time-cpu_other_time)/
lib/gpu/lal_balance.h:    if (_gpu_nbor==0) {
lib/gpu/lal_buck.cpp:#if defined(USE_OPENCL)
lib/gpu/lal_buck.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_buck.cpp:           const double gpu_split, FILE *_screen) {
lib/gpu/lal_buck.cpp:  success=this->init_atomic(nlocal,nall,max_nbors,maxspecial,cell_size,gpu_split,
lib/gpu/lal_zbl.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_zbl.h:    * - -1 if fix gpu not found
lib/gpu/lal_zbl.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_zbl.h:           const double gpu_split, FILE *screen);
lib/gpu/lal_pppm.h:#if defined(USE_OPENCL)
lib/gpu/lal_pppm.h:#elif defined(USE_CUDART)
lib/gpu/lal_pppm.h:    * - -1 if fix gpu not found
lib/gpu/lal_pppm.h:    * - -2 if GPU could not be found
lib/gpu/lal_pppm.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_vashishta.cu:#if (__CUDACC_VER_MAJOR__ >= 11)
lib/gpu/lal_vashishta.cu:                             const int t_per_atom, const int gpu_nbor) {
lib/gpu/lal_vashishta.cu:      if (gpu_nbor) nbor_k=j+nbor_pitch;
lib/gpu/lal_vashishta.cu:                             const int t_per_atom, const int gpu_nbor) {
lib/gpu/lal_vashishta.cu:      if (gpu_nbor) nbor_k=j+nbor_pitch;
lib/gpu/lal_lj_expand.cu://                            Inderaj Bains (NVIDIA)
lib/gpu/lal_lj_expand.cu://    email                : ibains@nvidia.com
lib/gpu/lal_vashishta.cpp:#if defined(USE_OPENCL)
lib/gpu/lal_vashishta.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_vashishta.cpp:           const double cell_size, const double gpu_split, FILE *_screen,
lib/gpu/lal_vashishta.cpp:  success=this->init_three(nlocal,nall,max_nbors,0,cell_size,gpu_split,
lib/gpu/lal_vashishta.cpp:                          &this->_gpu_nbor);
lib/gpu/lal_vashishta.cpp:                          &this->_gpu_nbor);
lib/gpu/Makefile.hip:#     - change HIP_ARCH for your GPU
lib/gpu/Makefile.hip:# precision for GPU calculations
lib/gpu/Makefile.hip:# requires linking with hipcc and hipCUB + (rocPRIM or CUB for AMD or Nvidia respectively)
lib/gpu/Makefile.hip:# newer version of ROCm (5.1+) require c++14 for rocprim
lib/gpu/Makefile.hip:HIP_GPU_OPTS += $(HIP_OPTS) -I./
lib/gpu/Makefile.hip:	HIP_GPU_CC  = $(HIP_PATH)/bin/hipcc -c
lib/gpu/Makefile.hip:	HIP_GPU_OPTS_S =
lib/gpu/Makefile.hip:	HIP_GPU_OPTS_E =
lib/gpu/Makefile.hip:	export HCC_AMDGPU_TARGET := $(HIP_ARCH)
lib/gpu/Makefile.hip:	HIP_GPU_CC  = $(HIP_PATH)/bin/hipcc --genco
lib/gpu/Makefile.hip:	HIP_GPU_OPTS_S = --offload-arch=$(HIP_ARCH)
lib/gpu/Makefile.hip:	HIP_GPU_OPTS_E =
lib/gpu/Makefile.hip:	HIP_LIBS_TARGET = export HCC_AMDGPU_TARGET := $(HIP_ARCH)
lib/gpu/Makefile.hip:	export HCC_AMDGPU_TARGET := $(HIP_ARCH)
lib/gpu/Makefile.hip:	HIP_GPU_CC  = $(HIP_PATH)/bin/hipcc --genco
lib/gpu/Makefile.hip:	HIP_GPU_OPTS_S = -t="$(HIP_ARCH)" -f=\"
lib/gpu/Makefile.hip:	HIP_GPU_OPTS_E = \"
lib/gpu/Makefile.hip:	HIP_LIBS_TARGET = export HCC_AMDGPU_TARGET := $(HIP_ARCH)
lib/gpu/Makefile.hip:	export HCC_AMDGPU_TARGET := $(HIP_ARCH)
lib/gpu/Makefile.hip:	HIP_GPU_CC  = $(HIP_PATH)/bin/hipcc --fatbin
lib/gpu/Makefile.hip:	HIP_GPU_OPTS += $(HIP_ARCH)
lib/gpu/Makefile.hip:	HIP_GPU_SORT_ARCH = $(HIP_ARCH)
lib/gpu/Makefile.hip:# hipcc is essential for device sort, because of hipcub is header only library and ROCm gpu code generation is deferred to the linking stage
lib/gpu/Makefile.hip:all: $(OBJ_DIR) $(CUHS) $(LIB_DIR)/libgpu.a $(BIN_DIR)/hip_get_devices
lib/gpu/Makefile.hip:# GPU kernels compilation
lib/gpu/Makefile.hip:	$(HIP_GPU_CC) $(HIP_GPU_OPTS_S) $(HIP_GPU_OPTS) -Dgrdtyp=float  -Dgrdtyp4=float4 $(HIP_GPU_OPTS_E)  -o $(OBJ_DIR)/pppm_f.cubin $(OBJ_DIR)/temp_pppm_f.cu$(HIP_KERNEL_SUFFIX)
lib/gpu/Makefile.hip:	$(HIP_GPU_CC) $(HIP_GPU_OPTS_S) $(HIP_GPU_OPTS) -Dgrdtyp=double -Dgrdtyp4=double4 $(HIP_GPU_OPTS_E)  -o $(OBJ_DIR)/pppm_d.cubin $(OBJ_DIR)/temp_pppm_d.cu$(HIP_KERNEL_SUFFIX)
lib/gpu/Makefile.hip:	$(HIP_GPU_CC) $(HIP_GPU_OPTS_S) $(HIP_GPU_OPTS) $(HIP_GPU_OPTS_E)  -o $(OBJ_DIR)/$*.cubin $(OBJ_DIR)/temp_$*.cu$(HIP_KERNEL_SUFFIX)
lib/gpu/Makefile.hip:	$(HIP_HOST_CC_CMD) -o $@ -c $< -I$(OBJ_DIR) $(HIP_GPU_SORT_ARCH)
lib/gpu/Makefile.hip:# libgpu building
lib/gpu/Makefile.hip:$(LIB_DIR)/libgpu.a: $(OBJS)
lib/gpu/Makefile.hip:	-rm -f $(BIN_DIR)/hip_get_devices $(LIB_DIR)/libgpu.a $(OBJS) $(OBJ_DIR)/temp_* $(CUHS)
lib/gpu/geryon/ucl_get_devices.cpp:  List properties of cuda devices
lib/gpu/geryon/ucl_get_devices.cpp:#ifdef UCL_OPENCL
lib/gpu/geryon/ucl_get_devices.cpp:using namespace ucl_opencl;
lib/gpu/geryon/ucl_get_devices.cpp:#ifdef UCL_CUDADR
lib/gpu/geryon/ucl_get_devices.cpp:using namespace ucl_cudadr;
lib/gpu/geryon/ucl_get_devices.cpp:#ifdef UCL_CUDART
lib/gpu/geryon/ucl_get_devices.cpp:using namespace ucl_cudart;
lib/gpu/geryon/hip_kernel.h:/// Class for dealing with CUDA Driver kernels
lib/gpu/geryon/ucl_basemat.h:// Only allow this file to be included by CUDA and OpenCL specific headers
lib/gpu/geryon/ucl_basemat.h:  * For CUDA, this is the default stream.
lib/gpu/geryon/ucl_basemat.h:  * that do not specify a queue. For OpenCL, this queue is also used in
lib/gpu/geryon/ucl_h_vec.h:// Only allow this file to be included by CUDA and OpenCL specific headers
lib/gpu/geryon/ucl_h_vec.h:    *   allocating container when using CUDA APIs
lib/gpu/geryon/ucl_h_vec.h:    *   allocating container when using CUDA APIs
lib/gpu/geryon/ucl_h_vec.h:    *   allocating container when using CUDA APIs
lib/gpu/geryon/ucl_h_vec.h:    *   allocating container when using CUDA APIs
lib/gpu/geryon/ucl_h_vec.h:    *   allocating container when using CUDA APIs
lib/gpu/geryon/ucl_h_vec.h:    *   allocating container when using CUDA APIs
lib/gpu/geryon/ucl_h_vec.h:    *   allocating container when using CUDA APIs
lib/gpu/geryon/ucl_h_vec.h:    *   allocating container when using CUDA APIs
lib/gpu/geryon/ucl_h_vec.h:    *   allocating container when using CUDA APIs
lib/gpu/geryon/ucl_h_vec.h:    *   allocating container when using CUDA APIs
lib/gpu/geryon/ucl_h_vec.h:    *   allocating container when using CUDA APIs
lib/gpu/geryon/ucl_h_vec.h:    *   allocating container when using CUDA APIs
lib/gpu/geryon/ucl_h_vec.h:    *   allocating container when using CUDA APIs
lib/gpu/geryon/ucl_h_vec.h:  /** \note Always 0 for host matrices and CUDA APIs **/
lib/gpu/geryon/ucl_h_vec.h:  /** \note Always 0 for host matrices and CUDA APIs **/
lib/gpu/geryon/ucl_h_vec.h:  /// For OpenCL, returns a reference to the cl_mem object
lib/gpu/geryon/ucl_h_vec.h:  /// For OpenCL, returns a reference to the cl_mem object
lib/gpu/geryon/hip_memory.h:} // namespace ucl_cudart
lib/gpu/geryon/ucl_vector.h:// Only allow this file to be included by CUDA and OpenCL specific headers
lib/gpu/geryon/nvd_texture.h:  Utilities for dealing with CUDA Driver textures
lib/gpu/geryon/nvd_texture.h:namespace ucl_cudadr {
lib/gpu/geryon/nvd_texture.h:    #if (CUDA_VERSION < 11000)
lib/gpu/geryon/nvd_texture.h:  #if (CUDA_VERSION < 11000)
lib/gpu/geryon/nvd_texture.h:    #if (CUDA_VERSION < 11000)
lib/gpu/geryon/ucl_h_mat.h:// Only allow this file to be included by CUDA and OpenCL specific headers
lib/gpu/geryon/ucl_h_mat.h:    *   allocating container when using CUDA APIs
lib/gpu/geryon/ucl_h_mat.h:    *   allocating container when using CUDA APIs
lib/gpu/geryon/ucl_h_mat.h:    *   allocating container when using CUDA APIs
lib/gpu/geryon/ucl_h_mat.h:    *   allocating container when using CUDA APIs
lib/gpu/geryon/ucl_h_mat.h:    *   will be used for view when using CUDA APIs
lib/gpu/geryon/ucl_h_mat.h:    *   allocating container when using CUDA APIs
lib/gpu/geryon/ucl_h_mat.h:    *   allocating container when using CUDA APIs
lib/gpu/geryon/ucl_h_mat.h:    *   allocating container when using CUDA APIs
lib/gpu/geryon/ucl_h_mat.h:    *   allocating container when using CUDA APIs
lib/gpu/geryon/ucl_h_mat.h:    *   allocating container when using CUDA APIs
lib/gpu/geryon/ucl_h_mat.h:    *   allocating container when using CUDA APIs
lib/gpu/geryon/ucl_h_mat.h:    *   allocating container when using CUDA APIs
lib/gpu/geryon/ucl_h_mat.h:    *   allocating container when using CUDA APIs
lib/gpu/geryon/ucl_h_mat.h:    *   allocating container when using CUDA APIs
lib/gpu/geryon/ucl_h_mat.h:  /** \note Always 0 for host matrices and CUDA APIs **/
lib/gpu/geryon/ucl_h_mat.h:  /** \note Always 0 for host matrices and CUDA APIs **/
lib/gpu/geryon/ucl_h_mat.h:  /// Returns an API specific device pointer (cl_mem& for OpenCL, void ** for CUDA)
lib/gpu/geryon/ucl_h_mat.h:  /// Returns an API specific device pointer (cl_mem& for OpenCL, void ** for CUDA)
lib/gpu/geryon/ucl_h_mat.h:  /// Returns an API specific device pointer (cl_mem& for OpenCL, void ** for CUDA)
lib/gpu/geryon/ucl_h_mat.h:  /// Returns an API specific device pointer (cl_mem& for OpenCL, void ** for CUDA)
lib/gpu/geryon/ocl_mat.h:  OpenCL Specific Vector/Matrix Containers, Memory Management, and I/O
lib/gpu/geryon/ocl_mat.h:/// Namespace for OpenCL routines
lib/gpu/geryon/ocl_mat.h:namespace ucl_opencl {
lib/gpu/geryon/ocl_mat.h:} // namespace ucl_opencl
lib/gpu/geryon/ucl_types.h:  UCL_GPU,            ///< Device is a GPU
lib/gpu/geryon/ucl_nv_kernel.h:  Preprocessor macros for OpenCL/CUDA compatibility
lib/gpu/geryon/ucl_nv_kernel.h:// Only allow this file to be included by CUDA and OpenCL specific headers
lib/gpu/geryon/nvd_device.h:  Utilities for dealing with cuda devices
lib/gpu/geryon/nvd_device.h:namespace ucl_cudadr {
lib/gpu/geryon/nvd_device.h:  CUDA_INT_TYPE totalGlobalMem;
lib/gpu/geryon/nvd_device.h:  /// Collect properties for every GPU on the node
lib/gpu/geryon/nvd_device.h:  /** \note You must set the active GPU with set() before using the device **/
lib/gpu/geryon/nvd_device.h:  /// Returns 1 (For compatibility with OpenCL)
lib/gpu/geryon/nvd_device.h:    { return "NVIDIA Corporation NVIDIA CUDA Driver"; }
lib/gpu/geryon/nvd_device.h:  /// Return the number of devices that support CUDA
lib/gpu/geryon/nvd_device.h:  /** No-op for CUDA and HIP **/
lib/gpu/geryon/nvd_device.h:  /// Set the CUDA device to the specified device number
lib/gpu/geryon/nvd_device.h:  /// Get the current CUDA device name
lib/gpu/geryon/nvd_device.h:  /// Get the CUDA device name
lib/gpu/geryon/nvd_device.h:  inline std::string device_type_name(const int i) { return "GPU"; }
lib/gpu/geryon/nvd_device.h:  /// Get current device type (UCL_CPU, UCL_GPU, UCL_ACCELERATOR, UCL_DEFAULT)
lib/gpu/geryon/nvd_device.h:  /// Get device type (UCL_CPU, UCL_GPU, UCL_ACCELERATOR, UCL_DEFAULT)
lib/gpu/geryon/nvd_device.h:  inline enum UCL_DEVICE_TYPE device_type(const int i) { return UCL_GPU; }
lib/gpu/geryon/nvd_device.h:    CUDA_INT_TYPE dfree, dtotal;
lib/gpu/geryon/nvd_device.h:  /// Return the GPGPU compute capability for current device
lib/gpu/geryon/nvd_device.h:  /// Return the GPGPU compute capability
lib/gpu/geryon/nvd_device.h:  inline int auto_set_platform(const enum UCL_DEVICE_TYPE type=UCL_GPU,
lib/gpu/geryon/nvd_device.h:#if CUDA_VERSION < 8000
lib/gpu/geryon/nvd_device.h:#error CUDA Toolkit version 8 or later required
lib/gpu/geryon/nvd_device.h:// Set the CUDA device to the specified device number
lib/gpu/geryon/nvd_device.h:  if (err!=CUDA_SUCCESS) {
lib/gpu/geryon/nvd_device.h:  out << "CUDA Driver Version:                           "
lib/gpu/geryon/nvd_device.h:    out << "There is no device supporting CUDA\n";
lib/gpu/geryon/hip_timer.h:/// Class for timing CUDA Driver events
lib/gpu/geryon/ocl_memory.h:  OpenCL Specific Memory Management and Vector/Matrix Containers
lib/gpu/geryon/ocl_memory.h:namespace ucl_opencl {
lib/gpu/geryon/ocl_memory.h:            << " Error compiling OpenCL Program...\n"
lib/gpu/geryon/ocl_memory.h:    "#pragma OPENCL EXTENSION cl_khr_fp64 : enable\n",
lib/gpu/geryon/ocl_memory.h:} // namespace ucl_cudart
lib/gpu/geryon/ocl_kernel.h:  Utilities for dealing with OpenCL kernels
lib/gpu/geryon/ocl_kernel.h:namespace ucl_opencl {
lib/gpu/geryon/ocl_kernel.h:          << " UCL Error: Error compiling OpenCL Program ("
lib/gpu/geryon/ocl_kernel.h:                  " UCL Error: Error compiling OpenCL Program (%d) ...\n",
lib/gpu/geryon/ocl_kernel.h:/// Class for dealing with OpenCL kernels
lib/gpu/geryon/ocl_kernel.h:        std::cerr << "TOO MANY ARGUMENTS TO OPENCL FUNCTION: "
lib/gpu/geryon/ocl_kernel.h:      std::cerr << "TOO MANY ARGUMENTS TO OPENCL FUNCTION: "
lib/gpu/geryon/ocl_texture.h:  Utilities for dealing with OpenCL textures
lib/gpu/geryon/ocl_texture.h:namespace ucl_opencl {
lib/gpu/geryon/nvd_memory.h:  CUDA Driver Specific Memory Management and Vector/Matrix Containers
lib/gpu/geryon/nvd_memory.h:namespace ucl_cudadr {
lib/gpu/geryon/nvd_memory.h:  CUresult err=CUDA_SUCCESS;
lib/gpu/geryon/nvd_memory.h:  if (err!=CUDA_SUCCESS || *(mat.host_ptr())==nullptr)
lib/gpu/geryon/nvd_memory.h:  CUresult err=CUDA_SUCCESS;
lib/gpu/geryon/nvd_memory.h:  if (err!=CUDA_SUCCESS || *(mat.host_ptr())==nullptr)
lib/gpu/geryon/nvd_memory.h:  CUresult err=CUDA_SUCCESS;
lib/gpu/geryon/nvd_memory.h:  if (err!=CUDA_SUCCESS || *(mat.host_ptr())==nullptr)
lib/gpu/geryon/nvd_memory.h:  if (err!=CUDA_SUCCESS)
lib/gpu/geryon/nvd_memory.h:  if (err!=CUDA_SUCCESS)
lib/gpu/geryon/nvd_memory.h:  CUDA_INT_TYPE upitch;
lib/gpu/geryon/nvd_memory.h:  if (err!=CUDA_SUCCESS)
lib/gpu/geryon/nvd_memory.h:  if (err!=CUDA_SUCCESS)
lib/gpu/geryon/nvd_memory.h:  if (err!=CUDA_SUCCESS)
lib/gpu/geryon/nvd_memory.h:  CUDA_INT_TYPE upitch;
lib/gpu/geryon/nvd_memory.h:  if (err!=CUDA_SUCCESS)
lib/gpu/geryon/nvd_memory.h:inline void _nvd_set_2D_loc(CUDA_MEMCPY2D &ins, const size_t dpitch,
lib/gpu/geryon/nvd_memory.h:    CUDA_MEMCPY2D ins;
lib/gpu/geryon/nvd_memory.h:    CUDA_MEMCPY2D ins;
lib/gpu/geryon/nvd_memory.h:    CUDA_MEMCPY2D ins;
lib/gpu/geryon/nvd_memory.h:    CUDA_MEMCPY2D ins;
lib/gpu/geryon/nvd_memory.h:    CUDA_MEMCPY2D ins;
lib/gpu/geryon/nvd_memory.h:    CUDA_MEMCPY2D ins;
lib/gpu/geryon/nvd_memory.h:    CUDA_MEMCPY2D ins;
lib/gpu/geryon/nvd_memory.h:    CUDA_MEMCPY2D ins;
lib/gpu/geryon/nvd_memory.h:    CUDA_MEMCPY2D ins;
lib/gpu/geryon/nvd_memory.h:    CUDA_MEMCPY2D ins;
lib/gpu/geryon/nvd_memory.h:    CUDA_MEMCPY2D ins;
lib/gpu/geryon/nvd_memory.h:    CUDA_MEMCPY2D ins;
lib/gpu/geryon/nvd_memory.h:    CUDA_MEMCPY2D ins;
lib/gpu/geryon/nvd_memory.h:    CUDA_MEMCPY2D ins;
lib/gpu/geryon/nvd_memory.h:    CUDA_MEMCPY2D ins;
lib/gpu/geryon/nvd_memory.h:    CUDA_MEMCPY2D ins;
lib/gpu/geryon/nvd_memory.h:      CUDA_MEMCPY2D ins;
lib/gpu/geryon/nvd_memory.h:      CUDA_MEMCPY2D ins;
lib/gpu/geryon/nvd_memory.h:} // namespace ucl_cudart
lib/gpu/geryon/ucl_d_vec.h:// Only allow this file to be included by CUDA and OpenCL specific headers
lib/gpu/geryon/ucl_d_vec.h:    *   allocating container when using CUDA APIs **/
lib/gpu/geryon/ucl_d_vec.h:    *   allocating container when using CUDA APIs
lib/gpu/geryon/ucl_d_vec.h:    *   allocating container when using CUDA APIs
lib/gpu/geryon/ucl_d_vec.h:    *   allocating container when using CUDA APIs
lib/gpu/geryon/ucl_d_vec.h:    *   allocating container when using CUDA APIs **/
lib/gpu/geryon/ucl_d_vec.h:    *   allocating container when using CUDA APIs
lib/gpu/geryon/ucl_d_vec.h:    *   allocating container when using CUDA APIs **/
lib/gpu/geryon/ucl_d_vec.h:    *   allocating container when using CUDA APIs **/
lib/gpu/geryon/ucl_d_vec.h:    *   allocating container when using CUDA APIs
lib/gpu/geryon/ucl_d_vec.h:    *   allocating container when using CUDA APIs
lib/gpu/geryon/ucl_d_vec.h:    *   allocating container when using CUDA APIs
lib/gpu/geryon/ucl_d_vec.h:    *   allocating container when using CUDA APIs **/
lib/gpu/geryon/ucl_d_vec.h:    *   allocating container when using CUDA APIs
lib/gpu/geryon/ucl_d_vec.h:    *   allocating container when using CUDA APIs **/
lib/gpu/geryon/ucl_d_vec.h:  /// For OpenCL, returns a (void *) device pointer to memory allocation
lib/gpu/geryon/ucl_d_vec.h:  /// For OpenCL, returns a (void *) device pointer to memory allocation
lib/gpu/geryon/ucl_d_vec.h:  /// For CUDA-RT, get device pointer to first element
lib/gpu/geryon/ucl_d_vec.h:  /// For CUDA-RT, get device pointer to first element
lib/gpu/geryon/ucl_d_vec.h:  /// For CUDA-RT, get device pointer to one past last element
lib/gpu/geryon/ucl_d_vec.h:  /// For CUDA-RT, get device pointer to one past last element
lib/gpu/geryon/ucl_d_vec.h:  /** - For OpenCL, returns a &cl_mem object
lib/gpu/geryon/ucl_d_vec.h:    * - For CUDA Driver, returns a &CUdeviceptr
lib/gpu/geryon/ucl_d_vec.h:    * - For CUDA-RT, returns void** **/
lib/gpu/geryon/ucl_d_vec.h:  /** - For OpenCL, returns a &cl_mem object
lib/gpu/geryon/ucl_d_vec.h:    * - For CUDA Driver, returns a &CUdeviceptr
lib/gpu/geryon/ucl_d_vec.h:    * - For CUDA-RT, returns void** **/
lib/gpu/geryon/ucl_d_vec.h:  /** - For OpenCL, returns a &cl_mem object
lib/gpu/geryon/ucl_d_vec.h:    * - For CUDA Driver, returns a &CUdeviceptr
lib/gpu/geryon/ucl_d_vec.h:    * - For CUDA-RT, returns numtyp** **/
lib/gpu/geryon/ucl_d_vec.h:  /** - For OpenCL, returns a &cl_mem object
lib/gpu/geryon/ucl_d_vec.h:    * - For CUDA Driver, returns a &CUdeviceptr
lib/gpu/geryon/ucl_d_vec.h:    * - For CUDA-RT, returns numtyp** **/
lib/gpu/geryon/ucl_d_vec.h:  /// For CUDA-RT, allocate row vector and bind texture
lib/gpu/geryon/ucl_d_vec.h:  /// For CUDA-RT, assign a texture to matrix
lib/gpu/geryon/ucl_d_vec.h:  /// For CUDA-RT, bind to texture
lib/gpu/geryon/ucl_d_vec.h:    cuda_gb_get_channel<numtyp>(_channel);
lib/gpu/geryon/ucl_d_vec.h:    (*_tex_ptr).addressMode[0] = cudaAddressModeClamp;
lib/gpu/geryon/ucl_d_vec.h:    (*_tex_ptr).addressMode[1] = cudaAddressModeClamp;
lib/gpu/geryon/ucl_d_vec.h:    (*_tex_ptr).filterMode = cudaFilterModePoint;
lib/gpu/geryon/ucl_d_vec.h:    CUDA_SAFE_CALL(cudaBindTexture(nullptr,_tex_ptr,_array,&_channel));
lib/gpu/geryon/ucl_d_vec.h:  /// For CUDA-RT, unbind texture
lib/gpu/geryon/ucl_d_vec.h:  inline void unbind() { CUDA_SAFE_CALL(cudaUnbindTexture(_tex_ptr)); }
lib/gpu/geryon/ucl_d_vec.h:  /** \note Always 0 for host matrices and CUDA APIs **/
lib/gpu/geryon/ucl_d_vec.h:  /** \note Always 0 for host matrices and CUDA APIs **/
lib/gpu/geryon/ucl_d_vec.h:  /** \note Always 0 for host matrices and CUDA APIs **/
lib/gpu/geryon/ucl_d_vec.h:  cudaChannelFormatDesc _channel;
lib/gpu/geryon/nvd_kernel.h:  Utilities for dealing with CUDA Driver kernels
lib/gpu/geryon/nvd_kernel.h:namespace ucl_cudadr {
lib/gpu/geryon/nvd_kernel.h:    if (err != CUDA_SUCCESS) {
lib/gpu/geryon/nvd_kernel.h:    } else if (err!=CUDA_SUCCESS) {
lib/gpu/geryon/nvd_kernel.h:    //else if (err!=CUDA_SUCCESS)
lib/gpu/geryon/nvd_kernel.h:/// Class for dealing with CUDA Driver kernels
lib/gpu/geryon/nvd_kernel.h:    if (err!=CUDA_SUCCESS) {
lib/gpu/geryon/ucl_d_mat.h:// Only allow this file to be included by CUDA and OpenCL specific headers
lib/gpu/geryon/ucl_d_mat.h:    *   allocating container when using CUDA APIs
lib/gpu/geryon/ucl_d_mat.h:    *   allocating container when using CUDA APIs **/
lib/gpu/geryon/ucl_d_mat.h:    *   allocating container when using CUDA APIs
lib/gpu/geryon/ucl_d_mat.h:    *   allocating container when using CUDA APIs
lib/gpu/geryon/ucl_d_mat.h:    *   allocating container when using CUDA APIs
lib/gpu/geryon/ucl_d_mat.h:    *   allocating container when using CUDA APIs **/
lib/gpu/geryon/ucl_d_mat.h:    *   allocating container when using CUDA APIs **/
lib/gpu/geryon/ucl_d_mat.h:    *   allocating container when using CUDA APIs
lib/gpu/geryon/ucl_d_mat.h:    *   allocating container when using CUDA APIs **/
lib/gpu/geryon/ucl_d_mat.h:    *   allocating container when using CUDA APIs
lib/gpu/geryon/ucl_d_mat.h:    *   allocating container when using CUDA APIs
lib/gpu/geryon/ucl_d_mat.h:    *   allocating container when using CUDA APIs
lib/gpu/geryon/ucl_d_mat.h:    *   allocating container when using CUDA APIs **/
lib/gpu/geryon/ucl_d_mat.h:    *   allocating container when using CUDA APIs **/
lib/gpu/geryon/ucl_d_mat.h:  /// For OpenCL, returns a (void *) device pointer to memory allocation
lib/gpu/geryon/ucl_d_mat.h:  /// For OpenCL, returns a (void *) device pointer to memory allocation
lib/gpu/geryon/ucl_d_mat.h:  /// For CUDA-RT, get device pointer to first element
lib/gpu/geryon/ucl_d_mat.h:  /// For CUDA-RT, get device pointer to first element
lib/gpu/geryon/ucl_d_mat.h:  /// For CUDA-RT, get device pointer to one past last element
lib/gpu/geryon/ucl_d_mat.h:  /// For CUDA-RT, get device pointer to one past last element
lib/gpu/geryon/ucl_d_mat.h:  /** - For OpenCL, returns a &cl_mem object
lib/gpu/geryon/ucl_d_mat.h:    * - For CUDA Driver, returns a &CUdeviceptr
lib/gpu/geryon/ucl_d_mat.h:    * - For CUDA-RT, returns void** **/
lib/gpu/geryon/ucl_d_mat.h:  /** - For OpenCL, returns a &cl_mem object
lib/gpu/geryon/ucl_d_mat.h:    * - For CUDA Driver, returns a &CUdeviceptr
lib/gpu/geryon/ucl_d_mat.h:    * - For CUDA-RT, returns void** **/
lib/gpu/geryon/ucl_d_mat.h:  /** - For OpenCL, returns a &cl_mem object
lib/gpu/geryon/ucl_d_mat.h:    * - For CUDA Driver, returns a &CUdeviceptr
lib/gpu/geryon/ucl_d_mat.h:    * - For CUDA-RT, returns numtyp** **/
lib/gpu/geryon/ucl_d_mat.h:  /** - For OpenCL, returns a &cl_mem object
lib/gpu/geryon/ucl_d_mat.h:    * - For CUDA Driver, returns a &CUdeviceptr
lib/gpu/geryon/ucl_d_mat.h:    * - For CUDA-RT, returns numtyp** **/
lib/gpu/geryon/ucl_d_mat.h:  /** \note Always 0 for host matrices and CUDA APIs **/
lib/gpu/geryon/ucl_d_mat.h:  /** \note Always 0 for host matrices and CUDA APIs **/
lib/gpu/geryon/ucl_d_mat.h:  /** \note Always 0 for host matrices and CUDA APIs **/
lib/gpu/geryon/hip_mat.h:/// Namespace for CUDA Driver routines
lib/gpu/geryon/hip_mat.h:} // namespace ucl_cudadr
lib/gpu/geryon/ocl_device.h:  Utilities for dealing with OpenCL devices
lib/gpu/geryon/ocl_device.h:#ifndef CL_TARGET_OPENCL_VERSION
lib/gpu/geryon/ocl_device.h:#define CL_TARGET_OPENCL_VERSION 300
lib/gpu/geryon/ocl_device.h:#include <OpenCL/cl.h>
lib/gpu/geryon/ocl_device.h:#include <OpenCL/cl_platform.h>
lib/gpu/geryon/ocl_device.h:namespace ucl_opencl {
lib/gpu/geryon/ocl_device.h:   /** \note You must set the active GPU with set() before using the device **/
lib/gpu/geryon/ocl_device.h:  /// Return the number of devices that support OpenCL
lib/gpu/geryon/ocl_device.h:  /** No-op for CUDA and HIP **/
lib/gpu/geryon/ocl_device.h:  /// Set the OpenCL device to the specified device number
lib/gpu/geryon/ocl_device.h:  /// Get the current OpenCL device name
lib/gpu/geryon/ocl_device.h:  /// Get the OpenCL device name
lib/gpu/geryon/ocl_device.h:  /// Get current device type (UCL_CPU, UCL_GPU, UCL_ACCELERATOR, UCL_DEFAULT)
lib/gpu/geryon/ocl_device.h:  /// Get device type (UCL_CPU, UCL_GPU, UCL_ACCELERATOR, UCL_DEFAULT)
lib/gpu/geryon/ocl_device.h:  /// Return the GPGPU revision number for current device
lib/gpu/geryon/ocl_device.h:  /// Return the GPGPU revision number
lib/gpu/geryon/ocl_device.h:  /// OpenCL version supported by the device
lib/gpu/geryon/ocl_device.h:  /// OpenCL version supported by the device
lib/gpu/geryon/ocl_device.h:  /// Return the OpenCL type for the device
lib/gpu/geryon/ocl_device.h:  inline int auto_set_platform(const enum UCL_DEVICE_TYPE type=UCL_GPU,
lib/gpu/geryon/ocl_device.h:  cl_platform_id _cl_platform; // OpenCL ID for current platform
lib/gpu/geryon/ocl_device.h:  cl_platform_id _cl_platforms[20]; // OpenCL IDs for all platforms
lib/gpu/geryon/ocl_device.h:  cl_device_id _cl_device;                // OpenCL ID for current device
lib/gpu/geryon/ocl_device.h:  std::vector<cl_device_id> _cl_devices;  // OpenCL IDs for all devices
lib/gpu/geryon/ocl_device.h:  CL_SAFE_CALL(clGetDeviceInfo(device_list,CL_DEVICE_OPENCL_C_VERSION,1024,
lib/gpu/geryon/ocl_device.h:  else if (_properties[i].device_type==CL_DEVICE_TYPE_GPU)
lib/gpu/geryon/ocl_device.h:    return "GPU";
lib/gpu/geryon/ocl_device.h:  else if (_properties[i].device_type==CL_DEVICE_TYPE_GPU)
lib/gpu/geryon/ocl_device.h:    return UCL_GPU;
lib/gpu/geryon/ocl_device.h:// Set the CUDA device to the specified device number
lib/gpu/geryon/ocl_device.h:      out << "There is no device supporting OpenCL\n";
lib/gpu/geryon/ocl_device.h:      out << "  Supported OpenCL Version:                      "
lib/gpu/geryon/ocl_device.h:} // namespace ucl_opencl
lib/gpu/geryon/hip_macros.h:#define CUDA_INT_TYPE size_t
lib/gpu/geryon/ocl_timer.h:  Class for timing OpenCL routines
lib/gpu/geryon/ocl_timer.h:namespace ucl_opencl {
lib/gpu/geryon/ocl_timer.h:/// Class for timing OpenCL events
lib/gpu/geryon/ucl_matrix.h:// Only allow this file to be included by CUDA and OpenCL specific headers
lib/gpu/geryon/hip_device.h:  CUDA_INT_TYPE totalGlobalMem;
lib/gpu/geryon/hip_device.h:  CUDA_INT_TYPE sharedMemPerBlock;
lib/gpu/geryon/hip_device.h:  CUDA_INT_TYPE totalConstantMemory;
lib/gpu/geryon/hip_device.h:  /// Collect properties for every GPU on the node
lib/gpu/geryon/hip_device.h:  /** \note You must set the active GPU with set() before using the device **/
lib/gpu/geryon/hip_device.h:  /// Returns 1 (For compatibility with OpenCL)
lib/gpu/geryon/hip_device.h:  /// Return the number of devices that support CUDA
lib/gpu/geryon/hip_device.h:  /** No-op for CUDA and HIP **/
lib/gpu/geryon/hip_device.h:  /// Set the CUDA device to the specified device number
lib/gpu/geryon/hip_device.h:  /// Get the current CUDA device name
lib/gpu/geryon/hip_device.h:  /// Get the CUDA device name
lib/gpu/geryon/hip_device.h:  inline std::string device_type_name(const int) { return "GPU"; }
lib/gpu/geryon/hip_device.h:  /// Get current device type (UCL_CPU, UCL_GPU, UCL_ACCELERATOR, UCL_DEFAULT)
lib/gpu/geryon/hip_device.h:  /// Get device type (UCL_CPU, UCL_GPU, UCL_ACCELERATOR, UCL_DEFAULT)
lib/gpu/geryon/hip_device.h:  inline enum UCL_DEVICE_TYPE device_type(const int) { return UCL_GPU; }
lib/gpu/geryon/hip_device.h:    CUDA_INT_TYPE dfree, dtotal;
lib/gpu/geryon/hip_device.h:  /// Return the GPGPU compute capability for current device
lib/gpu/geryon/hip_device.h:  /// Return the GPGPU compute capability
lib/gpu/geryon/hip_device.h:// Set the CUDA device to the specified device number
lib/gpu/geryon/ocl_macros.h:#ifndef CL_TARGET_OPENCL_VERSION
lib/gpu/geryon/ocl_macros.h:#define CL_TARGET_OPENCL_VERSION 300
lib/gpu/geryon/ocl_macros.h:#include <OpenCL/cl.h>
lib/gpu/geryon/ocl_macros.h:        fprintf(stderr, "OpenCL error in file '%s' in line %i : %d.\n",    \
lib/gpu/geryon/ocl_macros.h:        fprintf(stderr, "OpenCL error in file '%s' in line %i : %d.\n",    \
lib/gpu/geryon/README:Geryon is intended to be a simple library for managing the CUDA Runtime,
lib/gpu/geryon/README:CUDA Driver, and OpenCL APIs with a consistent interface:
lib/gpu/geryon/nvd_mat.h:  CUDA Driver Specific Vector/Matrix Containers, Memory Management, and I/O
lib/gpu/geryon/nvd_mat.h:/// Namespace for CUDA Driver routines
lib/gpu/geryon/nvd_mat.h:namespace ucl_cudadr {
lib/gpu/geryon/nvd_mat.h:} // namespace ucl_cudadr
lib/gpu/geryon/nvd_timer.h:  Class for timing CUDA Driver routines
lib/gpu/geryon/nvd_timer.h:namespace ucl_cudadr {
lib/gpu/geryon/nvd_timer.h:/// Class for timing CUDA Driver events
lib/gpu/geryon/nvd_macros.h:#include <cuda.h>
lib/gpu/geryon/nvd_macros.h:#define CUDA_INT_TYPE size_t
lib/gpu/geryon/nvd_macros.h:// Use the primary context for GPU access to enable compatibility with tools
lib/gpu/geryon/nvd_macros.h:// like OpenMPI accessing the GPU through the runtime interface.
lib/gpu/geryon/nvd_macros.h:    if( CUDA_SUCCESS != err) {                                               \
lib/gpu/geryon/nvd_macros.h:        fprintf(stderr, "Cuda driver error %d in call at file '%s' in line %i.\n",   \
lib/gpu/geryon/nvd_macros.h:    if( CUDA_SUCCESS != err) {                                               \
lib/gpu/geryon/nvd_macros.h:        fprintf(stderr, "Cuda driver error %d in file '%s' in line %i.\n",   \
lib/gpu/lal_sw.cpp:#if defined(USE_OPENCL)
lib/gpu/lal_sw.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_sw.cpp:              const double gpu_split, FILE *_screen, double **ncutsq,
lib/gpu/lal_sw.cpp:  #ifdef USE_OPENCL
lib/gpu/lal_sw.cpp:  success=this->init_three(nlocal,nall,max_nbors,0,cell_size,gpu_split,
lib/gpu/lal_sw.cpp:  // this->_nbor_data == nbor->dev_packed for gpu_nbor == 0 and tpa > 1
lib/gpu/lal_sw.cpp:  // this->_nbor_data == nbor->dev_nbor for gpu_nbor == 1 or tpa == 1
lib/gpu/lal_sw.cpp:                                &this->_gpu_nbor);
lib/gpu/lal_sw.cpp:                          &this->_threads_per_atom, &this->_gpu_nbor);
lib/gpu/lal_tersoff_zbl.cu:                                  const int t_per_atom, const int gpu_nbor) {
lib/gpu/lal_tersoff_zbl.cu:      if (gpu_nbor) nbor_k=j+nbor_pitch;
lib/gpu/lal_tersoff_zbl.cu:                                        const int t_per_atom, const int gpu_nbor) {
lib/gpu/lal_tersoff_zbl.cu:      if (gpu_nbor) nbor_k=j+nbor_pitch;
lib/gpu/lal_coul_debye.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_coul_debye.h:    * - -1 if fix gpu not found
lib/gpu/lal_coul_debye.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_coul_debye.h:           const double gpu_split, FILE *screen,
lib/gpu/lal_yukawa_colloid.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_yukawa_colloid.h:    * - -1 if fix gpu not found
lib/gpu/lal_yukawa_colloid.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_yukawa_colloid.h:           const double gpu_split, FILE *screen, const double kappa);
lib/gpu/lal_edpd.cpp:#if defined(USE_OPENCL)
lib/gpu/lal_edpd.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_edpd.cpp:                const double gpu_split, FILE *_screen) {
lib/gpu/lal_edpd.cpp:  #ifdef USE_OPENCL
lib/gpu/lal_edpd.cpp:                            gpu_split,_screen,edpd,"k_edpd",onetype,extra_fields);
lib/gpu/lal_coul_dsf_ext.cpp:int cdsf_gpu_init(const int ntypes, const int inum, const int nall,
lib/gpu/lal_coul_dsf_ext.cpp:                  const double cell_size, int &gpu_mode, FILE *screen,
lib/gpu/lal_coul_dsf_ext.cpp:  gpu_mode=CDMF.device->gpu_mode();
lib/gpu/lal_coul_dsf_ext.cpp:  double gpu_split=CDMF.device->particle_split();
lib/gpu/lal_coul_dsf_ext.cpp:  int first_gpu=CDMF.device->first_device();
lib/gpu/lal_coul_dsf_ext.cpp:  int last_gpu=CDMF.device->last_device();
lib/gpu/lal_coul_dsf_ext.cpp:  int gpu_rank=CDMF.device->gpu_rank();
lib/gpu/lal_coul_dsf_ext.cpp:  int procs_per_gpu=CDMF.device->procs_per_gpu();
lib/gpu/lal_coul_dsf_ext.cpp:  CDMF.device->init_message(screen,"coul/dsf",first_gpu,last_gpu);
lib/gpu/lal_coul_dsf_ext.cpp:                      gpu_split, screen, host_cut_coulsq, host_special_coul,
lib/gpu/lal_coul_dsf_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_coul_dsf_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_coul_dsf_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_coul_dsf_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_coul_dsf_ext.cpp:                last_gpu,i);
lib/gpu/lal_coul_dsf_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_coul_dsf_ext.cpp:                        gpu_split, screen, host_cut_coulsq, host_special_coul,
lib/gpu/lal_coul_dsf_ext.cpp:    CDMF.estimate_gpu_overhead();
lib/gpu/lal_coul_dsf_ext.cpp:void cdsf_gpu_clear() {
lib/gpu/lal_coul_dsf_ext.cpp:int** cdsf_gpu_compute_n(const int ago, const int inum_full,
lib/gpu/lal_coul_dsf_ext.cpp:void cdsf_gpu_compute(const int ago, const int inum_full, const int nall,
lib/gpu/lal_coul_dsf_ext.cpp:double cdsf_gpu_bytes() {
lib/gpu/Makefile.mac_opencl:#  Generic Mac Makefile for OpenCL - Single precision with FFT_SINGLE
lib/gpu/Makefile.mac_opencl:OCL_LINK = -framework OpenCL
lib/gpu/Makefile.mac_opencl:include Opencl.makefile
lib/gpu/lal_mdpd.cpp:#if defined(USE_OPENCL)
lib/gpu/lal_mdpd.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_mdpd.cpp:                const double gpu_split, FILE *_screen) {
lib/gpu/lal_mdpd.cpp:  #ifdef USE_OPENCL
lib/gpu/lal_mdpd.cpp:                            gpu_split,_screen,mdpd,"k_mdpd",onetype,extra_fields);
lib/gpu/lal_re_squared.cu:      gpu_quat_to_mat_trans(q,i,a1);
lib/gpu/lal_re_squared.cu:      gpu_transpose_times_diag3(a1,well[itype],aTe1);
lib/gpu/lal_re_squared.cu:      gpu_transpose_times_diag3(a1,ishape2,aTs);
lib/gpu/lal_re_squared.cu:      gpu_diag_times3(ishape2,a1,sa1);
lib/gpu/lal_re_squared.cu:      gpu_times3(aTs,a1,gamma1);
lib/gpu/lal_re_squared.cu:      gpu_rotation_generator_x(a1,lA1_0);
lib/gpu/lal_re_squared.cu:      gpu_rotation_generator_y(a1,lA1_1);
lib/gpu/lal_re_squared.cu:      gpu_rotation_generator_z(a1,lA1_2);
lib/gpu/lal_re_squared.cu:      gpu_times3(aTs,lA1_0,lAtwo1_0);
lib/gpu/lal_re_squared.cu:      gpu_transpose_times3(lA1_0,sa1,lAsa1_0);
lib/gpu/lal_re_squared.cu:      gpu_plus3(lAsa1_0,lAtwo1_0,lAsa1_0);
lib/gpu/lal_re_squared.cu:      gpu_times3(aTs,lA1_1,lAtwo1_1);
lib/gpu/lal_re_squared.cu:      gpu_transpose_times3(lA1_1,sa1,lAsa1_1);
lib/gpu/lal_re_squared.cu:      gpu_plus3(lAsa1_1,lAtwo1_1,lAsa1_1);
lib/gpu/lal_re_squared.cu:      gpu_times3(aTs,lA1_2,lAtwo1_2);
lib/gpu/lal_re_squared.cu:      gpu_transpose_times3(lA1_2,sa1,lAsa1_2);
lib/gpu/lal_re_squared.cu:      gpu_plus3(lAsa1_2,lAtwo1_2,lAsa1_2);
lib/gpu/lal_re_squared.cu:      rnorm = gpu_dot3(r,r);
lib/gpu/lal_re_squared.cu:        gpu_quat_to_mat_trans(q,j,a2);
lib/gpu/lal_re_squared.cu:        gpu_transpose_times_diag3(a2,jshape2,aTs);
lib/gpu/lal_re_squared.cu:        gpu_times3(aTs,a2,gamma2);
lib/gpu/lal_re_squared.cu:      gpu_plus3(gamma1,gamma2,temp);
lib/gpu/lal_re_squared.cu:      gpu_mldivide3(temp,rhat,s,err_flag);
lib/gpu/lal_re_squared.cu:      sigma12 = ucl_rsqrt((numtyp)0.5*gpu_dot3(s,rhat));
lib/gpu/lal_re_squared.cu:      gpu_times_column3(a1,rhat,z1);
lib/gpu/lal_re_squared.cu:      gpu_times_column3(a2,rhat,z2);
lib/gpu/lal_re_squared.cu:      sigma1 = ucl_sqrt(gpu_dot3(z1,v1));
lib/gpu/lal_re_squared.cu:      sigma2 = ucl_sqrt(gpu_dot3(z2,v2));
lib/gpu/lal_re_squared.cu:      dH=gpu_det3(H12);
lib/gpu/lal_re_squared.cu:      gpu_times3(aTe1,a1,temp);
lib/gpu/lal_re_squared.cu:      gpu_transpose_times_diag3(a2,well[jtype],aTe2);
lib/gpu/lal_re_squared.cu:      gpu_times3(aTe2,a2,temp2);
lib/gpu/lal_re_squared.cu:      gpu_plus3(temp,temp2,temp);
lib/gpu/lal_re_squared.cu:      gpu_mldivide3(temp,rhat,w,err_flag);
lib/gpu/lal_re_squared.cu:      chi = (numtyp)2.0*gpu_dot3(rhat,w);
lib/gpu/lal_re_squared.cu:        gpu_times_column3(a1,u,u1);
lib/gpu/lal_re_squared.cu:        gpu_times_column3(a2,u,u2);
lib/gpu/lal_re_squared.cu:        dsigma1=gpu_dot3(u1,vsigma1);
lib/gpu/lal_re_squared.cu:        dsigma2=gpu_dot3(u2,vsigma2);
lib/gpu/lal_re_squared.cu:        dchi = gpu_dot3(u,fourw);
lib/gpu/lal_re_squared.cu:        dh12 = rhat[i]+gpu_dot3(u,spr);
lib/gpu/lal_re_squared.cu:      gpu_row_times3(fourw,aTe1,fwae);
lib/gpu/lal_re_squared.cu:        gpu_times_column3(lA1_0,rhat,p);
lib/gpu/lal_re_squared.cu:        dsigma1 = gpu_dot3(p,vsigma1);
lib/gpu/lal_re_squared.cu:        gpu_times_column3(lA1_0,w,tempv);
lib/gpu/lal_re_squared.cu:        dchi = -gpu_dot3(fwae,tempv);
lib/gpu/lal_re_squared.cu:        gpu_times_column3(lAtwo1_0,spr,tempv);
lib/gpu/lal_re_squared.cu:        dh12 = -gpu_dot3(s,tempv);
lib/gpu/lal_re_squared.cu:        gpu_times_column3(lA1_1,rhat,p);
lib/gpu/lal_re_squared.cu:        dsigma1 = gpu_dot3(p,vsigma1);
lib/gpu/lal_re_squared.cu:        gpu_times_column3(lA1_1,w,tempv);
lib/gpu/lal_re_squared.cu:        dchi = -gpu_dot3(fwae,tempv);
lib/gpu/lal_re_squared.cu:        gpu_times_column3(lAtwo1_1,spr,tempv);
lib/gpu/lal_re_squared.cu:        dh12 = -gpu_dot3(s,tempv);
lib/gpu/lal_re_squared.cu:        gpu_times_column3(lA1_2,rhat,p);
lib/gpu/lal_re_squared.cu:        dsigma1 = gpu_dot3(p,vsigma1);
lib/gpu/lal_re_squared.cu:        gpu_times_column3(lA1_2,w,tempv);
lib/gpu/lal_re_squared.cu:        dchi = -gpu_dot3(fwae,tempv);
lib/gpu/lal_re_squared.cu:        gpu_times_column3(lAtwo1_2,spr,tempv);
lib/gpu/lal_re_squared.cu:        dh12 = -gpu_dot3(s,tempv);
lib/gpu/Makefile.linux:#  Generic Linux Makefile for CUDA
lib/gpu/Makefile.linux:#     - Change CUDA_ARCH for your GPU
lib/gpu/Makefile.linux:ifeq ($(CUDA_HOME),)
lib/gpu/Makefile.linux:CUDA_HOME = /usr/local/cuda
lib/gpu/Makefile.linux:#CUDA_ARCH = -arch=sm_13
lib/gpu/Makefile.linux:#CUDA_ARCH = -arch=sm_10 -DCUDA_PRE_THREE
lib/gpu/Makefile.linux:#CUDA_ARCH = -arch=sm_20
lib/gpu/Makefile.linux:#CUDA_ARCH = -arch=sm_21
lib/gpu/Makefile.linux:#CUDA_ARCH = -arch=sm_30
lib/gpu/Makefile.linux:#CUDA_ARCH = -arch=sm_32
lib/gpu/Makefile.linux:#CUDA_ARCH = -arch=sm_35
lib/gpu/Makefile.linux:#CUDA_ARCH = -arch=sm_37
lib/gpu/Makefile.linux:#CUDA_ARCH = -arch=sm_50
lib/gpu/Makefile.linux:#CUDA_ARCH = -arch=sm_52
lib/gpu/Makefile.linux:CUDA_ARCH = -arch=sm_60
lib/gpu/Makefile.linux:#CUDA_ARCH = -arch=sm_61
lib/gpu/Makefile.linux:#CUDA_ARCH = -arch=sm_70
lib/gpu/Makefile.linux:#CUDA_ARCH = -arch=sm_75
lib/gpu/Makefile.linux:#CUDA_ARCH = -arch=sm_80
lib/gpu/Makefile.linux:#CUDA_ARCH = -arch=sm_86
lib/gpu/Makefile.linux:#CUDA_ARCH = -arch=sm_89
lib/gpu/Makefile.linux:#CUDA_ARCH = -arch=sm_90
lib/gpu/Makefile.linux:# precision for GPU calculations
lib/gpu/Makefile.linux:CUDA_PRECISION = -D_SINGLE_DOUBLE
lib/gpu/Makefile.linux:CUDA_INCLUDE = -I$(CUDA_HOME)/include
lib/gpu/Makefile.linux:CUDA_LIB = -L$(CUDA_HOME)/lib64 -L$(CUDA_HOME)/lib64/stubs
lib/gpu/Makefile.linux:CUDA_OPTS = -DUNIX -O3 --use_fast_math $(LMP_INC) -Xcompiler -fPIC
lib/gpu/Makefile.linux:# GPU binning not recommended for most modern GPUs
lib/gpu/Makefile.linux:include Nvidia.makefile
lib/gpu/lal_gayberne_lj.cu:      numtyp ir = gpu_dot3(r12,r12);
lib/gpu/lal_gayberne_lj.cu:      gpu_quat_to_mat_trans(q,j,a2);
lib/gpu/lal_gayberne_lj.cu:            gpu_diag_times3(shape[jtype],a2,g12);
lib/gpu/lal_gayberne_lj.cu:            gpu_transpose_times3(a2,g12,g2);
lib/gpu/lal_gayberne_lj.cu:            gpu_mldivide3(g12,r12,kappa,err_flag);
lib/gpu/lal_gayberne_lj.cu:              sigma12 = gpu_dot3(r12hat,kappa);
lib/gpu/lal_gayberne_lj.cu:              numtyp temp2 = gpu_dot3(kappa,r12hat);
lib/gpu/lal_gayberne_lj.cu:          numtyp det_g12 = gpu_det3(g12);
lib/gpu/lal_gayberne_lj.cu:          gpu_diag_times3(well[jtype],a2,b12);
lib/gpu/lal_gayberne_lj.cu:          gpu_transpose_times3(a2,b12,b2);
lib/gpu/lal_gayberne_lj.cu:        gpu_mldivide3(b12,r12,iota,err_flag);
lib/gpu/lal_gayberne_lj.cu:        chi = gpu_dot3(r12hat,iota);
lib/gpu/lal_gayberne_lj.cu:        numtyp temp1 = gpu_dot3(iota,r12hat);
lib/gpu/lal_colloid.cpp:#ifdef USE_OPENCL
lib/gpu/lal_colloid.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_colloid.cpp:                   const double gpu_split, FILE *_screen) {
lib/gpu/lal_colloid.cpp:  success=this->init_atomic(nlocal,nall,max_nbors,maxspecial,cell_size,gpu_split,
lib/gpu/Makefile.cuda_mps:#  Generic Linux Makefile for CUDA with the Multi-Process Service (MPS)
lib/gpu/Makefile.cuda_mps:#     - change CUDA_ARCH for your GPU
lib/gpu/Makefile.cuda_mps:ifeq ($(CUDA_HOME),)
lib/gpu/Makefile.cuda_mps:CUDA_HOME = /usr/local/cuda
lib/gpu/Makefile.cuda_mps:# precision for GPU calculations
lib/gpu/Makefile.cuda_mps:CUDA_PRECISION = -D_SINGLE_DOUBLE
lib/gpu/Makefile.cuda_mps:CUDA_MPS  = -DCUDA_MPS_SUPPORT
lib/gpu/Makefile.cuda_mps:CUDA_ARCH = -gencode arch=compute_50,code=[sm_50,compute_50] -gencode arch=compute_52,code=[sm_52,compute_52] \
lib/gpu/Makefile.cuda_mps:CUDA_INCLUDE = -I$(CUDA_HOME)/include
lib/gpu/Makefile.cuda_mps:CUDA_LIB = -L$(CUDA_HOME)/lib64 -L$(CUDA_HOME)/lib64/stubs
lib/gpu/Makefile.cuda_mps:CUDA_OPTS = -DUNIX -O3 --use_fast_math $(LMP_INC) -Xcompiler -fPIC
lib/gpu/Makefile.cuda_mps:CUDA_LINK = $(CUDA_LIB) -lcudart
lib/gpu/Makefile.cuda_mps:CUDA  = $(NVCC) $(CUDA_INCLUDE) $(CUDA_OPTS) $(CUDA_ARCH) \
lib/gpu/Makefile.cuda_mps:             $(CUDA_PRECISION)
lib/gpu/Makefile.cuda_mps:BIN2C = $(CUDA_HOME)/bin/bin2c
lib/gpu/Makefile.cuda_mps:CUDR  = $(CUDR_CPP) $(CUDR_OPTS) $(CUDA_MPS) $(CUDA_PRECISION) $(CUDA_INCLUDE) \
lib/gpu/Makefile.cuda_mps:GPU_LIB = $(LIB_DIR)/libgpu.a
lib/gpu/Makefile.cuda_mps:all: $(OBJ_DIR) $(CUHS) $(GPU_LIB) $(EXECS)
lib/gpu/Makefile.cuda_mps:	$(CUDA) --fatbin -DNV_KERNEL -Dgrdtyp=float -Dgrdtyp4=float4 -o $@ lal_pppm.cu
lib/gpu/Makefile.cuda_mps:	$(CUDA) --fatbin -DNV_KERNEL -Dgrdtyp=double -Dgrdtyp4=double4 -o $@ lal_pppm.cu
lib/gpu/Makefile.cuda_mps:	$(CUDA) --fatbin -DNV_KERNEL -o $(OBJ_DIR)/$*.cubin $(OBJ_DIR)/lal_$*.cu
lib/gpu/Makefile.cuda_mps:	$(CUDA) -o $@ -c cudpp_mini/radixsort_app.cu
lib/gpu/Makefile.cuda_mps:	$(CUDA) -o $@ -c cudpp_mini/scan_app.cu
lib/gpu/Makefile.cuda_mps:# build libgpu.a
lib/gpu/Makefile.cuda_mps:$(GPU_LIB): $(OBJS) $(CUDPP)
lib/gpu/Makefile.cuda_mps:	$(AR) -crusv $(GPU_LIB) $(OBJS) $(CUDPP)
lib/gpu/Makefile.cuda_mps:	$(CUDR) -o $@ ./geryon/ucl_get_devices.cpp -DUCL_CUDADR $(CUDA_LIB) -lcuda
lib/gpu/Makefile.cuda_mps:	-rm -f $(EXECS) $(GPU_LIB) $(OBJS) $(CUDPP) $(CUHS) *.linkinfo
lib/gpu/Makefile.cuda_mps:	-rm -f $(EXECS) $(GPU_LIB) $(OBJS) $(CUHS) *.linkinfo
lib/gpu/lal_tersoff_zbl.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_tersoff_zbl.h:    * - -1 if fix gpu not found
lib/gpu/lal_tersoff_zbl.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_tersoff_zbl.h:           const double cell_size, const double gpu_split, FILE *screen,
lib/gpu/lal_tersoff_zbl.cpp:#if defined(USE_OPENCL)
lib/gpu/lal_tersoff_zbl.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_tersoff_zbl.cpp:                    const double gpu_split, FILE *_screen, int* host_map,
lib/gpu/lal_tersoff_zbl.cpp:  #ifdef USE_OPENCL
lib/gpu/lal_tersoff_zbl.cpp:  success=this->init_three(nlocal,nall,max_nbors,0,cell_size,gpu_split,
lib/gpu/lal_tersoff_zbl.cpp:                          &this->_gpu_nbor);
lib/gpu/lal_tersoff_zbl.cpp:                          &this->_gpu_nbor);
lib/gpu/lal_lj_class2_long.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_lj_class2_long.h:    * - -1 if fix gpu not found
lib/gpu/lal_lj_class2_long.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_lj_class2_long.h:           const double gpu_split, FILE *screen, double **host_cut_ljsq,
lib/gpu/lal_base_amoeba.h:#if defined(USE_OPENCL)
lib/gpu/lal_base_amoeba.h:#elif defined(USE_CUDART)
lib/gpu/lal_base_amoeba.h:#if !defined(USE_OPENCL) && !defined(USE_HIP)
lib/gpu/lal_base_amoeba.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_base_amoeba.h:    * - -1 if fix gpu not found
lib/gpu/lal_base_amoeba.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_base_amoeba.h:                  const double gpu_split, FILE *screen, const void *pair_program,
lib/gpu/lal_base_amoeba.h:  /// Estimate the overhead for GPU context changes and CPU driver
lib/gpu/lal_base_amoeba.h:  void estimate_gpu_overhead(const int add_kernels=0);
lib/gpu/lal_base_amoeba.h:  double _gpu_overhead, _driver_overhead;
lib/gpu/lal_base_amoeba.h:  #if !defined(USE_OPENCL) && !defined(USE_HIP)
lib/gpu/lal_beck.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_beck.h:    * - -1 if fix gpu not found
lib/gpu/lal_beck.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_beck.h:           const double gpu_split, FILE *screen);
lib/gpu/lal_lj_smooth.cpp:#if defined(USE_OPENCL)
lib/gpu/lal_lj_smooth.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_lj_smooth.cpp:                          const double gpu_split, FILE *_screen,
lib/gpu/lal_lj_smooth.cpp:  #ifdef USE_OPENCL
lib/gpu/lal_lj_smooth.cpp:  success=this->init_atomic(nlocal,nall,max_nbors,maxspecial,cell_size,gpu_split,
lib/gpu/lal_preprocessor.h://  For OpenCL, the configuration is a string (optionally controlled at
lib/gpu/lal_preprocessor.h://                   100-199 for NVIDIA GPUs with CUDA / HIP
lib/gpu/lal_preprocessor.h://                   200-299 for NVIDIA GPUs with OpenCL
lib/gpu/lal_preprocessor.h://                   300-399 for AMD GPUs with HIP
lib/gpu/lal_preprocessor.h://                   400-499 for AMD GPUs with OpenCL
lib/gpu/lal_preprocessor.h://                   500-599 for Intel GPUs with OpenCL
lib/gpu/lal_preprocessor.h://     Definition:   For CUDA this is the warp size.
lib/gpu/lal_preprocessor.h://                   For OpenCL < 2.1 this is the number of workitems
lib/gpu/lal_preprocessor.h://                   For OpenCL >= 2.1 this is the smallest expected subgroup
lib/gpu/lal_preprocessor.h://                   usage. 1 enables for CUDA, HIP, and OpenCL >= 2.1 on
lib/gpu/lal_preprocessor.h://                   NVIDIA and Intel devices.
lib/gpu/lal_preprocessor.h://                   native transcendentals for OpenCL (fused multiply-add
lib/gpu/lal_preprocessor.h://                   still enabled). For CUDA and HIP, this is controlled by
lib/gpu/lal_preprocessor.h://     Definition:   Default number of work items or CUDA threads assigned per
lib/gpu/lal_preprocessor.h://     Definition:   Default number of work items or CUDA threads assigned per
lib/gpu/lal_preprocessor.h://     Definition:   Default number of work items or CUDA threads assigned per
lib/gpu/lal_preprocessor.h://                           CUDA and HIP DEFINITIONS
lib/gpu/lal_preprocessor.h:#include "lal_pre_cuda_hip.h"
lib/gpu/lal_preprocessor.h://                         OPENCL DEVICE CONFIGURATAIONS
lib/gpu/lal_preprocessor.h:// See lal_pre_ocl_config.h for OpenCL device configurations
lib/gpu/lal_preprocessor.h:#define USE_OPENCL
lib/gpu/lal_preprocessor.h://                         OPENCL KERNEL MACROS
lib/gpu/lal_preprocessor.h:#if (__OPENCL_VERSION__ > 199)
lib/gpu/lal_preprocessor.h:#define NOUNROLL __attribute__((opencl_unroll_hint(1)))
lib/gpu/lal_preprocessor.h://                      OPENCL KERNEL MACROS - TEXTURES
lib/gpu/lal_preprocessor.h://                       OPENCL KERNEL MACROS - MATH
lib/gpu/lal_preprocessor.h:#pragma OPENCL EXTENSION cl_khr_fp64 : enable
lib/gpu/lal_preprocessor.h:#pragma OPENCL EXTENSION cl_khr_fp64 : enable
lib/gpu/lal_preprocessor.h:#pragma OPENCL EXTENSION cl_amd_fp64 : enable
lib/gpu/lal_preprocessor.h://                      OPENCL KERNEL MACROS - SHUFFLE
lib/gpu/lal_preprocessor.h:    #pragma OPENCL EXTENSION cl_intel_subgroups : enable
lib/gpu/lal_preprocessor.h://                      OPENCL KERNEL MACROS - SUBGROUPS
lib/gpu/lal_preprocessor.h:#ifdef USE_OPENCL_SUBGROUPS
lib/gpu/lal_preprocessor.h:    #pragma OPENCL EXTENSION cl_khr_subgroups : enable
lib/gpu/lal_preprocessor.h://                      OPENCL KERNEL MACROS - PREFETCH
lib/gpu/lal_preprocessor.h:struct _lgpu_float3 {
lib/gpu/lal_preprocessor.h:#define acctyp3 struct _lgpu_float3
lib/gpu/lal_preprocessor.h:struct _lgpu_double3 {
lib/gpu/lal_preprocessor.h:#define acctyp3 struct _lgpu_double3
lib/gpu/lal_preprocessor.h://                            END OPENCL DEFINITIONS
lib/gpu/lal_base_dpd.cpp:                          const double cell_size, const double gpu_split,
lib/gpu/lal_base_dpd.cpp:  int gpu_nbor=0;
lib/gpu/lal_base_dpd.cpp:  if (device->gpu_mode()==Device<numtyp,acctyp>::GPU_NEIGH)
lib/gpu/lal_base_dpd.cpp:    gpu_nbor=1;
lib/gpu/lal_base_dpd.cpp:  else if (device->gpu_mode()==Device<numtyp,acctyp>::GPU_HYB_NEIGH)
lib/gpu/lal_base_dpd.cpp:    gpu_nbor=2;
lib/gpu/lal_base_dpd.cpp:  int _gpu_host=0;
lib/gpu/lal_base_dpd.cpp:  int host_nlocal=hd_balancer.first_host_count(nlocal,gpu_split,gpu_nbor);
lib/gpu/lal_base_dpd.cpp:    _gpu_host=1;
lib/gpu/lal_base_dpd.cpp:  if (ucl_device!=device->gpu) _compiled=false;
lib/gpu/lal_base_dpd.cpp:  ucl_device=device->gpu;
lib/gpu/lal_base_dpd.cpp:  if (_threads_per_atom>1 && gpu_nbor==0) {
lib/gpu/lal_base_dpd.cpp:  success = device->init_nbor(nbor,nlocal,host_nlocal,nall,maxspecial,_gpu_host,
lib/gpu/lal_base_dpd.cpp:  hd_balancer.init(device,gpu_nbor,gpu_split);
lib/gpu/lal_base_dpd.cpp:  // Initialize timers for the selected GPU
lib/gpu/lal_base_dpd.cpp:  _max_an_bytes=ans->gpu_bytes()+nbor->gpu_bytes();
lib/gpu/lal_base_dpd.cpp:void BaseDPDT::estimate_gpu_overhead() {
lib/gpu/lal_base_dpd.cpp:  device->estimate_gpu_overhead(1,_gpu_overhead,_driver_overhead);
lib/gpu/lal_base_dpd.cpp:  _gpu_overhead*=hd_balancer.timestep();
lib/gpu/lal_base_dpd.cpp:                       _gpu_overhead,_driver_overhead,_threads_per_atom,screen);
lib/gpu/lal_base_dpd.cpp:  double bytes=ans->gpu_bytes()+nbor->gpu_bytes();
lib/gpu/lal_base_dpd.cpp:  double bytes=ans->gpu_bytes()+nbor->gpu_bytes();
lib/gpu/lal_base_dpd.cpp:// Reneighbor on GPU if necessary and then compute forces, virials, energies
lib/gpu/lal_base_dpd.cpp:  int inum=hd_balancer.get_gpu_count(ago,inum_full);
lib/gpu/lal_base_dpd.cpp:  // Build neighbor list on GPU if necessary
lib/gpu/lal_base_dpd.cpp:  #if defined(USE_OPENCL) && (defined(CL_VERSION_2_1) || defined(CL_VERSION_3_0))
lib/gpu/lal_lj_gromacs.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_lj_gromacs.h:    * - -1 if fix gpu not found
lib/gpu/lal_lj_gromacs.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_lj_gromacs.h:           const double gpu_split, FILE *screen,
lib/gpu/Makefile.oneapi_prof:EXTRAMAKE = Makefile.lammps.opencl
lib/gpu/Makefile.oneapi_prof:OCL_LINK = -L$(ONEAPI_ROOT)/compiler/latest/linux/lib -lOpenCL
lib/gpu/Makefile.oneapi_prof:OCL_TUNE = -DMPI_GERYON -DCUDA_PROXY -DGERYON_NUMA_FISSION -DUCL_NO_EXIT
lib/gpu/Makefile.oneapi_prof:include Opencl.makefile
lib/gpu/lal_yukawa_colloid_ext.cpp:int ykcolloid_gpu_init(const int ntypes, double **cutsq, double **host_a,
lib/gpu/lal_yukawa_colloid_ext.cpp:                       const double cell_size, int &gpu_mode, FILE *screen,
lib/gpu/lal_yukawa_colloid_ext.cpp:  gpu_mode=YKCOLLMF.device->gpu_mode();
lib/gpu/lal_yukawa_colloid_ext.cpp:  double gpu_split=YKCOLLMF.device->particle_split();
lib/gpu/lal_yukawa_colloid_ext.cpp:  int first_gpu=YKCOLLMF.device->first_device();
lib/gpu/lal_yukawa_colloid_ext.cpp:  int last_gpu=YKCOLLMF.device->last_device();
lib/gpu/lal_yukawa_colloid_ext.cpp:  int gpu_rank=YKCOLLMF.device->gpu_rank();
lib/gpu/lal_yukawa_colloid_ext.cpp:  int procs_per_gpu=YKCOLLMF.device->procs_per_gpu();
lib/gpu/lal_yukawa_colloid_ext.cpp:  YKCOLLMF.device->init_message(screen,"yukawa/colloid",first_gpu,last_gpu);
lib/gpu/lal_yukawa_colloid_ext.cpp:                          inum, nall, max_nbors, maxspecial, cell_size, gpu_split,
lib/gpu/lal_yukawa_colloid_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_yukawa_colloid_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_yukawa_colloid_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_yukawa_colloid_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_yukawa_colloid_ext.cpp:                last_gpu,i);
lib/gpu/lal_yukawa_colloid_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_yukawa_colloid_ext.cpp:                            inum, nall, max_nbors, maxspecial, cell_size, gpu_split,
lib/gpu/lal_yukawa_colloid_ext.cpp:    YKCOLLMF.estimate_gpu_overhead();
lib/gpu/lal_yukawa_colloid_ext.cpp:void ykcolloid_gpu_clear() {
lib/gpu/lal_yukawa_colloid_ext.cpp:int ** ykcolloid_gpu_compute_n(const int ago, const int inum_full,
lib/gpu/lal_yukawa_colloid_ext.cpp:void ykcolloid_gpu_compute(const int ago, const int inum_full,
lib/gpu/lal_yukawa_colloid_ext.cpp:double ykcolloid_gpu_bytes() {
lib/gpu/lal_vashishta.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_vashishta.h:    * - -1 if fix gpu not found
lib/gpu/lal_vashishta.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_vashishta.h:           const double cell_size, const double gpu_split, FILE *screen,
lib/gpu/lal_base_charge.h:#if defined(USE_OPENCL)
lib/gpu/lal_base_charge.h:#elif defined(USE_CUDART)
lib/gpu/lal_base_charge.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_base_charge.h:    * - -1 if fix gpu not found
lib/gpu/lal_base_charge.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_base_charge.h:                  const double gpu_split, FILE *screen,
lib/gpu/lal_base_charge.h:  /// Estimate the overhead for GPU context changes and CPU driver
lib/gpu/lal_base_charge.h:  void estimate_gpu_overhead(const int add_kernels=0);
lib/gpu/lal_base_charge.h:  double _gpu_overhead, _driver_overhead;
lib/gpu/lal_buck_coul_long.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_buck_coul_long.h:    * - -1 if fix gpu not found
lib/gpu/lal_buck_coul_long.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_buck_coul_long.h:           const double gpu_split, FILE *screen, double **host_cut_ljsq,
lib/gpu/lal_base_dpd.h:#ifdef USE_OPENCL
lib/gpu/lal_base_dpd.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_base_dpd.h:    * - -1 if fix gpu not found
lib/gpu/lal_base_dpd.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_base_dpd.h:                  const double gpu_split, FILE *screen,
lib/gpu/lal_base_dpd.h:  /// Estimate the overhead for GPU context changes and CPU driver
lib/gpu/lal_base_dpd.h:  void estimate_gpu_overhead();
lib/gpu/lal_base_dpd.h:  double _gpu_overhead, _driver_overhead;
lib/gpu/lal_pppm.cpp:#if defined(USE_OPENCL)
lib/gpu/lal_pppm.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_pppm.cpp:  if (ucl_device!=device->gpu) _compiled=false;
lib/gpu/lal_pppm.cpp:  ucl_device=device->gpu;
lib/gpu/lal_pppm.cpp:  // Initialize timers for the selected GPU
lib/gpu/lal_pppm.cpp:  _max_an_bytes=ans->gpu_bytes();
lib/gpu/lal_pppm.cpp:    double bytes=ans->gpu_bytes();
lib/gpu/lal_pppm.cpp:  #ifdef USE_OPENCL
lib/gpu/lal_pppm.cpp:  #ifdef USE_OPENCL
lib/gpu/lal_pppm.cpp:template class PPPM<PRECISION,ACC_PRECISION,float,_lgpu_float4>;
lib/gpu/lal_pppm.cpp:template class PPPM<PRECISION,ACC_PRECISION,double,_lgpu_double4>;
lib/gpu/lal_lj_smooth.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_lj_smooth.h:    * - -1 if fix gpu not found
lib/gpu/lal_lj_smooth.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_lj_smooth.h:           const double gpu_split, FILE *screen,
lib/gpu/lal_mdpd.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_mdpd.h:    * - -1 if fix gpu not found
lib/gpu/lal_mdpd.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_mdpd.h:           const int maxspecial, const double cell_size, const double gpu_split,
lib/gpu/lal_coul_long.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_coul_long.h:    * - -1 if fix gpu not found
lib/gpu/lal_coul_long.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_coul_long.h:                 const double gpu_split, FILE *screen,
lib/gpu/lal_zbl.cpp:#ifdef USE_OPENCL
lib/gpu/lal_zbl.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_zbl.cpp:               const double gpu_split, FILE *_screen) {
lib/gpu/lal_zbl.cpp:  success=this->init_atomic(nlocal,nall,max_nbors,maxspecial,cell_size,gpu_split,
lib/gpu/lal_buck_coul_long_ext.cpp:int buckcl_gpu_init(const int ntypes, double **cutsq, double **host_rhoinv,
lib/gpu/lal_buck_coul_long_ext.cpp:                  const double cell_size, int &gpu_mode, FILE *screen,
lib/gpu/lal_buck_coul_long_ext.cpp:  gpu_mode=BUCKCLMF.device->gpu_mode();
lib/gpu/lal_buck_coul_long_ext.cpp:  double gpu_split=BUCKCLMF.device->particle_split();
lib/gpu/lal_buck_coul_long_ext.cpp:  int first_gpu=BUCKCLMF.device->first_device();
lib/gpu/lal_buck_coul_long_ext.cpp:  int last_gpu=BUCKCLMF.device->last_device();
lib/gpu/lal_buck_coul_long_ext.cpp:  int gpu_rank=BUCKCLMF.device->gpu_rank();
lib/gpu/lal_buck_coul_long_ext.cpp:  int procs_per_gpu=BUCKCLMF.device->procs_per_gpu();
lib/gpu/lal_buck_coul_long_ext.cpp:  BUCKCLMF.device->init_message(screen,"buck/coul/long",first_gpu,last_gpu);
lib/gpu/lal_buck_coul_long_ext.cpp:                        maxspecial, cell_size, gpu_split, screen, host_cut_ljsq,
lib/gpu/lal_buck_coul_long_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_buck_coul_long_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_buck_coul_long_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_buck_coul_long_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_buck_coul_long_ext.cpp:                last_gpu,i);
lib/gpu/lal_buck_coul_long_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_buck_coul_long_ext.cpp:                        maxspecial, cell_size, gpu_split, screen, host_cut_ljsq,
lib/gpu/lal_buck_coul_long_ext.cpp:    BUCKCLMF.estimate_gpu_overhead();
lib/gpu/lal_buck_coul_long_ext.cpp:void buckcl_gpu_clear() {
lib/gpu/lal_buck_coul_long_ext.cpp:int** buckcl_gpu_compute_n(const int ago, const int inum_full,
lib/gpu/lal_buck_coul_long_ext.cpp:void buckcl_gpu_compute(const int ago, const int inum_full, const int nall,
lib/gpu/lal_buck_coul_long_ext.cpp:double buckcl_gpu_bytes() {
lib/gpu/lal_pre_cuda_hip.h://                               pre_cuda_hip.h
lib/gpu/lal_pre_cuda_hip.h://  Device-side preprocessor definitions for CUDA and HIP builds
lib/gpu/lal_pre_cuda_hip.h://                           CUDA and HIP DEFINITIONS
lib/gpu/lal_pre_cuda_hip.h:#if (__CUDACC_VER_MAJOR__ >= 9)
lib/gpu/lal_pre_cuda_hip.h:#if (__CUDACC_VER_MAJOR__ < 11)
lib/gpu/lal_pre_cuda_hip.h:#if (__CUDACC_VER_MAJOR__ < 9)
lib/gpu/lal_pre_cuda_hip.h:#define CUDA_PRE_NINE
lib/gpu/lal_pre_cuda_hip.h:#if defined(CUDA_PRE_NINE) || defined(__HIP_PLATFORM_HCC__) || defined(__HIP_PLATFORM_AMD__) || defined(__HIP_PLATFORM_SPIRV__)
lib/gpu/lal_pre_cuda_hip.h://                            END CUDA / HIP DEFINITIONS
lib/gpu/lal_dipole_lj_sf.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_dipole_lj_sf.h:    * - -1 if fix gpu not found
lib/gpu/lal_dipole_lj_sf.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_dipole_lj_sf.h:           const double gpu_split, FILE *screen, double **host_cut_ljsq,
lib/gpu/lal_buck_coul.cpp:#if defined(USE_OPENCL)
lib/gpu/lal_buck_coul.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_buck_coul.cpp:                   const double gpu_split, FILE *_screen, double **host_cut_ljsq,
lib/gpu/lal_buck_coul.cpp:  success=this->init_atomic(nlocal,nall,max_nbors,maxspecial,cell_size,gpu_split,
lib/gpu/lal_charmm_long.cpp:#if defined(USE_OPENCL)
lib/gpu/lal_charmm_long.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_charmm_long.cpp:                      const double cell_size, const double gpu_split, FILE *_screen,
lib/gpu/lal_charmm_long.cpp:  success=this->init_atomic(nlocal,nall,max_nbors,maxspecial,cell_size,gpu_split,
lib/gpu/lal_coul_long_cs.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_coul_long_cs.h:    * - -1 if fix gpu not found
lib/gpu/lal_coul_long_cs.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_coul_long_cs.h:           const double gpu_split, FILE *screen,
lib/gpu/lal_born_coul_long_cs.cpp:#ifdef USE_OPENCL
lib/gpu/lal_born_coul_long_cs.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_born_coul_long_cs.cpp:                       const double gpu_split, FILE *_screen,
lib/gpu/lal_born_coul_long_cs.cpp:  success=this->init_atomic(nlocal,nall,max_nbors,maxspecial,cell_size,gpu_split,
lib/gpu/lal_charmm_ext.cpp:int crm_gpu_init(const int ntypes, double cut_bothsq, double **host_lj1,
lib/gpu/lal_charmm_ext.cpp:                   const double cell_size, int &gpu_mode, FILE *screen,
lib/gpu/lal_charmm_ext.cpp:  gpu_mode=CRMMF.device->gpu_mode();
lib/gpu/lal_charmm_ext.cpp:  double gpu_split=CRMMF.device->particle_split();
lib/gpu/lal_charmm_ext.cpp:  int first_gpu=CRMMF.device->first_device();
lib/gpu/lal_charmm_ext.cpp:  int last_gpu=CRMMF.device->last_device();
lib/gpu/lal_charmm_ext.cpp:  int gpu_rank=CRMMF.device->gpu_rank();
lib/gpu/lal_charmm_ext.cpp:  int procs_per_gpu=CRMMF.device->procs_per_gpu();
lib/gpu/lal_charmm_ext.cpp:  CRMMF.device->init_message(screen,"lj/charmm/coul/charmm",first_gpu,
lib/gpu/lal_charmm_ext.cpp:                              last_gpu);
lib/gpu/lal_charmm_ext.cpp:                gpu_split, screen, host_cut_ljsq, host_cut_coulsq,
lib/gpu/lal_charmm_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_charmm_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_charmm_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_charmm_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_charmm_ext.cpp:                last_gpu,i);
lib/gpu/lal_charmm_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_charmm_ext.cpp:                          maxspecial, cell_size, gpu_split, screen,
lib/gpu/lal_charmm_ext.cpp:    CRMMF.estimate_gpu_overhead();
lib/gpu/lal_charmm_ext.cpp:void crm_gpu_clear() {
lib/gpu/lal_charmm_ext.cpp:int** crm_gpu_compute_n(const int ago, const int inum_full, const int nall,
lib/gpu/lal_charmm_ext.cpp:void crm_gpu_compute(const int ago, const int inum_full, const int nall,
lib/gpu/lal_charmm_ext.cpp:double crm_gpu_bytes() {
lib/gpu/lal_lj_coul_debye.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_lj_coul_debye.h:    * - -1 if fix gpu not found
lib/gpu/lal_lj_coul_debye.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_lj_coul_debye.h:           const double gpu_split, FILE *screen, double **host_cut_ljsq,
lib/gpu/lal_tersoff_ext.cpp:int tersoff_gpu_init(const int ntypes, const int inum, const int nall, const int max_nbors,
lib/gpu/lal_tersoff_ext.cpp:                     const double cell_size, int &gpu_mode, FILE *screen,
lib/gpu/lal_tersoff_ext.cpp:  gpu_mode=TSMF.device->gpu_mode();
lib/gpu/lal_tersoff_ext.cpp:  double gpu_split=TSMF.device->particle_split();
lib/gpu/lal_tersoff_ext.cpp:  int first_gpu=TSMF.device->first_device();
lib/gpu/lal_tersoff_ext.cpp:  int last_gpu=TSMF.device->last_device();
lib/gpu/lal_tersoff_ext.cpp:  int gpu_rank=TSMF.device->gpu_rank();
lib/gpu/lal_tersoff_ext.cpp:  int procs_per_gpu=TSMF.device->procs_per_gpu();
lib/gpu/lal_tersoff_ext.cpp:  if (gpu_split != 1.0)
lib/gpu/lal_tersoff_ext.cpp:  TSMF.device->init_message(screen,"tersoff/gpu",first_gpu,last_gpu);
lib/gpu/lal_tersoff_ext.cpp:    init_ok=TSMF.init(ntypes, inum, nall, max_nbors, cell_size, gpu_split, screen,
lib/gpu/lal_tersoff_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_tersoff_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_tersoff_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_tersoff_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_tersoff_ext.cpp:                last_gpu,i);
lib/gpu/lal_tersoff_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_tersoff_ext.cpp:      init_ok=TSMF.init(ntypes, inum, nall, max_nbors, cell_size, gpu_split, screen,
lib/gpu/lal_tersoff_ext.cpp:    TSMF.estimate_gpu_overhead(1);
lib/gpu/lal_tersoff_ext.cpp:void tersoff_gpu_clear() {
lib/gpu/lal_tersoff_ext.cpp:int ** tersoff_gpu_compute_n(const int ago, const int inum_full,
lib/gpu/lal_tersoff_ext.cpp:void tersoff_gpu_compute(const int ago, const int nlocal, const int nall,
lib/gpu/lal_tersoff_ext.cpp:double tersoff_gpu_bytes() {
lib/gpu/lal_charmm_long.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_charmm_long.h:    * - -1 if fix gpu not found
lib/gpu/lal_charmm_long.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_charmm_long.h:           const double gpu_split, FILE *screen, double host_cut_ljsq,
lib/gpu/lal_soft.cpp:#ifdef USE_OPENCL
lib/gpu/lal_soft.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_soft.cpp:                 const double gpu_split, FILE *_screen) {
lib/gpu/lal_soft.cpp:  success=this->init_atomic(nlocal,nall,max_nbors,maxspecial,cell_size,gpu_split,
lib/gpu/lal_re_squared_lj.cu:      gpu_quat_to_mat_trans(q,i,a);
lib/gpu/lal_re_squared_lj.cu:      gpu_transpose_times_diag3(a,well[itype],aTe);
lib/gpu/lal_re_squared_lj.cu:      gpu_rotation_generator_x(a,lA_0);
lib/gpu/lal_re_squared_lj.cu:      gpu_rotation_generator_y(a,lA_1);
lib/gpu/lal_re_squared_lj.cu:      gpu_rotation_generator_z(a,lA_2);
lib/gpu/lal_re_squared_lj.cu:      rnorm = gpu_dot3(r,r);
lib/gpu/lal_re_squared_lj.cu:      gpu_transpose_times_diag3(a,scorrect,aTs);
lib/gpu/lal_re_squared_lj.cu:      gpu_times3(aTs,a,gamma);
lib/gpu/lal_re_squared_lj.cu:      gpu_mldivide3(gamma,rhat,s,err_flag);
lib/gpu/lal_re_squared_lj.cu:      numtyp sigma12 = ucl_rsqrt((numtyp)0.5*gpu_dot3(s,rhat));
lib/gpu/lal_re_squared_lj.cu:      gpu_times3(aTe,a,temp);
lib/gpu/lal_re_squared_lj.cu:      gpu_mldivide3(temp,rhat,w,err_flag);
lib/gpu/lal_re_squared_lj.cu:      numtyp chi = (numtyp)2.0*gpu_dot3(rhat,w);
lib/gpu/lal_re_squared_lj.cu:        numtyp dchi = gpu_dot3(u,fourw);
lib/gpu/lal_re_squared_lj.cu:        numtyp dh12 = rhat[i]+gpu_dot3(u,spr);
lib/gpu/lal_re_squared_lj.cu:      gpu_row_times3(fourw,aTe,fwae);
lib/gpu/lal_re_squared_lj.cu:        gpu_times_column3(lA_0,rhat,p);
lib/gpu/lal_re_squared_lj.cu:        gpu_times_column3(lA_0,w,tempv);
lib/gpu/lal_re_squared_lj.cu:        numtyp dchi = -gpu_dot3(fwae,tempv);
lib/gpu/lal_re_squared_lj.cu:        gpu_times3(aTs,lA_0,lAtwo);
lib/gpu/lal_re_squared_lj.cu:        gpu_times_column3(lAtwo,spr,tempv);
lib/gpu/lal_re_squared_lj.cu:        numtyp dh12 = -gpu_dot3(s,tempv);
lib/gpu/lal_re_squared_lj.cu:        gpu_times_column3(lA_1,rhat,p);
lib/gpu/lal_re_squared_lj.cu:        gpu_times_column3(lA_1,w,tempv);
lib/gpu/lal_re_squared_lj.cu:        numtyp dchi = -gpu_dot3(fwae,tempv);
lib/gpu/lal_re_squared_lj.cu:        gpu_times3(aTs,lA_1,lAtwo);
lib/gpu/lal_re_squared_lj.cu:        gpu_times_column3(lAtwo,spr,tempv);
lib/gpu/lal_re_squared_lj.cu:        numtyp dh12 = -gpu_dot3(s,tempv);
lib/gpu/lal_re_squared_lj.cu:        gpu_times_column3(lA_2,rhat,p);
lib/gpu/lal_re_squared_lj.cu:        gpu_times_column3(lA_2,w,tempv);
lib/gpu/lal_re_squared_lj.cu:        numtyp dchi = -gpu_dot3(fwae,tempv);
lib/gpu/lal_re_squared_lj.cu:        gpu_times3(aTs,lA_2,lAtwo);
lib/gpu/lal_re_squared_lj.cu:        gpu_times_column3(lAtwo,spr,tempv);
lib/gpu/lal_re_squared_lj.cu:        numtyp dh12 = -gpu_dot3(s,tempv);
lib/gpu/lal_re_squared_lj.cu:      gpu_quat_to_mat_trans(q,i,a);
lib/gpu/lal_re_squared_lj.cu:      gpu_transpose_times_diag3(a,well[itype],aTe);
lib/gpu/lal_re_squared_lj.cu:      rnorm = gpu_dot3(r,r);
lib/gpu/lal_re_squared_lj.cu:      gpu_transpose_times_diag3(a,scorrect,aTs);
lib/gpu/lal_re_squared_lj.cu:      gpu_times3(aTs,a,gamma);
lib/gpu/lal_re_squared_lj.cu:      gpu_mldivide3(gamma,rhat,s,err_flag);
lib/gpu/lal_re_squared_lj.cu:      numtyp sigma12 = ucl_rsqrt((numtyp)0.5*gpu_dot3(s,rhat));
lib/gpu/lal_re_squared_lj.cu:      gpu_times3(aTe,a,temp);
lib/gpu/lal_re_squared_lj.cu:      gpu_mldivide3(temp,rhat,w,err_flag);
lib/gpu/lal_re_squared_lj.cu:      numtyp chi = (numtyp)2.0*gpu_dot3(rhat,w);
lib/gpu/lal_re_squared_lj.cu:        numtyp dchi = gpu_dot3(u,fourw);
lib/gpu/lal_re_squared_lj.cu:        numtyp dh12 = rhat[i]+gpu_dot3(u,spr);
lib/gpu/lal_lj_expand.cpp:                            Inderaj Bains (NVIDIA)
lib/gpu/lal_lj_expand.cpp:    email                : ibains@nvidia.com
lib/gpu/lal_lj_expand.cpp:#if defined(USE_OPENCL)
lib/gpu/lal_lj_expand.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_lj_expand.cpp:                          const double gpu_split, FILE *_screen) {
lib/gpu/lal_lj_expand.cpp:  success=this->init_atomic(nlocal,nall,max_nbors,maxspecial,cell_size,gpu_split,
lib/gpu/lal_lj_dsf.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_lj_dsf.h:    * - -1 if fix gpu not found
lib/gpu/lal_lj_dsf.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_lj_dsf.h:           const double gpu_split, FILE *screen, double **host_cut_ljsq,
lib/gpu/lal_born_coul_long_cs.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_born_coul_long_cs.h:    * - -1 if fix gpu not found
lib/gpu/lal_born_coul_long_cs.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_born_coul_long_cs.h:           const double gpu_split, FILE *screen, double **host_cut_ljsq,
lib/gpu/lal_gayberne_ext.cpp:int gb_gpu_init(const int ntypes, const double gamma,
lib/gpu/lal_gayberne_ext.cpp:                const int maxspecial, const double cell_size, int &gpu_mode,
lib/gpu/lal_gayberne_ext.cpp:  gpu_mode=GBMF.device->gpu_mode();
lib/gpu/lal_gayberne_ext.cpp:  double gpu_split=GBMF.device->particle_split();
lib/gpu/lal_gayberne_ext.cpp:  int first_gpu=GBMF.device->first_device();
lib/gpu/lal_gayberne_ext.cpp:  int last_gpu=GBMF.device->last_device();
lib/gpu/lal_gayberne_ext.cpp:  int gpu_rank=GBMF.device->gpu_rank();
lib/gpu/lal_gayberne_ext.cpp:  int procs_per_gpu=GBMF.device->procs_per_gpu();
lib/gpu/lal_gayberne_ext.cpp:  GBMF.device->init_message(screen,"gayberne",first_gpu,last_gpu);
lib/gpu/lal_gayberne_ext.cpp:                      inum, nall, max_nbors, maxspecial, cell_size, gpu_split,
lib/gpu/lal_gayberne_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_gayberne_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_gayberne_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_gayberne_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_gayberne_ext.cpp:                last_gpu,i);
lib/gpu/lal_gayberne_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_gayberne_ext.cpp:                        max_nbors, maxspecial, cell_size, gpu_split,  screen);
lib/gpu/lal_gayberne_ext.cpp:    GBMF.estimate_gpu_overhead();
lib/gpu/lal_gayberne_ext.cpp:void gb_gpu_clear() {
lib/gpu/lal_gayberne_ext.cpp:int** gb_gpu_compute_n(const int ago, const int inum_full, const int nall,
lib/gpu/lal_gayberne_ext.cpp:int * gb_gpu_compute(const int ago, const int inum_full, const int nall,
lib/gpu/lal_gayberne_ext.cpp:double gb_gpu_bytes() {
lib/gpu/lal_born.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_born.h:    * - -1 if fix gpu not found
lib/gpu/lal_born.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_born.h:           const double gpu_split, FILE *screen);
lib/gpu/lal_table.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_table.h:    * - -1 if fix gpu not found
lib/gpu/lal_table.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_table.h:           const double gpu_split, FILE *screen,
lib/gpu/lal_vashishta_ext.cpp:int vashishta_gpu_init(const int ntypes, const int inum, const int nall, const int max_nbors,
lib/gpu/lal_vashishta_ext.cpp:                const double cell_size, int &gpu_mode, FILE *screen,
lib/gpu/lal_vashishta_ext.cpp:  gpu_mode=VashishtaMF.device->gpu_mode();
lib/gpu/lal_vashishta_ext.cpp:  double gpu_split=VashishtaMF.device->particle_split();
lib/gpu/lal_vashishta_ext.cpp:  int first_gpu=VashishtaMF.device->first_device();
lib/gpu/lal_vashishta_ext.cpp:  int last_gpu=VashishtaMF.device->last_device();
lib/gpu/lal_vashishta_ext.cpp:  int gpu_rank=VashishtaMF.device->gpu_rank();
lib/gpu/lal_vashishta_ext.cpp:  int procs_per_gpu=VashishtaMF.device->procs_per_gpu();
lib/gpu/lal_vashishta_ext.cpp:  if (gpu_split != 1.0)
lib/gpu/lal_vashishta_ext.cpp:  VashishtaMF.device->init_message(screen,"vashishta/gpu",first_gpu,last_gpu);
lib/gpu/lal_vashishta_ext.cpp:    init_ok=VashishtaMF.init(ntypes, inum, nall, max_nbors, cell_size, gpu_split, screen,
lib/gpu/lal_vashishta_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_vashishta_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_vashishta_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_vashishta_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_vashishta_ext.cpp:                last_gpu,i);
lib/gpu/lal_vashishta_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_vashishta_ext.cpp:      init_ok=VashishtaMF.init(ntypes, inum, nall, max_nbors, cell_size, gpu_split, screen,
lib/gpu/lal_vashishta_ext.cpp:    VashishtaMF.estimate_gpu_overhead();
lib/gpu/lal_vashishta_ext.cpp:void vashishta_gpu_clear() {
lib/gpu/lal_vashishta_ext.cpp:int ** vashishta_gpu_compute_n(const int ago, const int inum_full,
lib/gpu/lal_vashishta_ext.cpp:void vashishta_gpu_compute(const int ago, const int nlocal, const int nall,
lib/gpu/lal_vashishta_ext.cpp:double vashishta_gpu_bytes() {
lib/gpu/lal_sph_taitwater.cpp:#if defined(USE_OPENCL)
lib/gpu/lal_sph_taitwater.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_sph_taitwater.cpp:                        const double gpu_split, FILE *_screen) {
lib/gpu/lal_sph_taitwater.cpp:  #ifdef USE_OPENCL
lib/gpu/lal_sph_taitwater.cpp:                            gpu_split,_screen,sph_taitwater,"k_sph_taitwater",
lib/gpu/lal_sph_lj.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_sph_lj.h:    * - -1 if fix gpu not found
lib/gpu/lal_sph_lj.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_sph_lj.h:           const int maxspecial, const double cell_size, const double gpu_split,
lib/gpu/lal_born_coul_long_ext.cpp:int borncl_gpu_init(const int ntypes, double **cutsq, double **host_rhoinv,
lib/gpu/lal_born_coul_long_ext.cpp:                    const int maxspecial, const double cell_size, int &gpu_mode,
lib/gpu/lal_born_coul_long_ext.cpp:  gpu_mode=BORNCLMF.device->gpu_mode();
lib/gpu/lal_born_coul_long_ext.cpp:  double gpu_split=BORNCLMF.device->particle_split();
lib/gpu/lal_born_coul_long_ext.cpp:  int first_gpu=BORNCLMF.device->first_device();
lib/gpu/lal_born_coul_long_ext.cpp:  int last_gpu=BORNCLMF.device->last_device();
lib/gpu/lal_born_coul_long_ext.cpp:  int gpu_rank=BORNCLMF.device->gpu_rank();
lib/gpu/lal_born_coul_long_ext.cpp:  int procs_per_gpu=BORNCLMF.device->procs_per_gpu();
lib/gpu/lal_born_coul_long_ext.cpp:  BORNCLMF.device->init_message(screen,"born/coul/long",first_gpu,last_gpu);
lib/gpu/lal_born_coul_long_ext.cpp:                          gpu_split, screen, host_cut_ljsq, host_cut_coulsq,
lib/gpu/lal_born_coul_long_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_born_coul_long_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_born_coul_long_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_born_coul_long_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_born_coul_long_ext.cpp:                last_gpu,i);
lib/gpu/lal_born_coul_long_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_born_coul_long_ext.cpp:                            gpu_split, screen, host_cut_ljsq, host_cut_coulsq,
lib/gpu/lal_born_coul_long_ext.cpp:    BORNCLMF.estimate_gpu_overhead();
lib/gpu/lal_born_coul_long_ext.cpp:void borncl_gpu_clear() {
lib/gpu/lal_born_coul_long_ext.cpp:int** borncl_gpu_compute_n(const int ago, const int inum_full,
lib/gpu/lal_born_coul_long_ext.cpp:void borncl_gpu_compute(const int ago, const int inum_full, const int nall,
lib/gpu/lal_born_coul_long_ext.cpp:double borncl_gpu_bytes() {
lib/gpu/lal_lj_coul_long_soft.cpp:#if defined(USE_OPENCL)
lib/gpu/lal_lj_coul_long_soft.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_lj_coul_long_soft.cpp:                           const double gpu_split, FILE *_screen,
lib/gpu/lal_lj_coul_long_soft.cpp:  success=this->init_atomic(nlocal,nall,max_nbors,maxspecial,cell_size,gpu_split,
lib/gpu/lal_sph_taitwater.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_sph_taitwater.h:    * - -1 if fix gpu not found
lib/gpu/lal_sph_taitwater.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_sph_taitwater.h:           const double gpu_split, FILE *screen);
lib/gpu/lal_lj96_ext.cpp:int lj96_gpu_init(const int ntypes, double **cutsq, double **host_lj1,
lib/gpu/lal_lj96_ext.cpp:                  const double cell_size, int &gpu_mode, FILE *screen) {
lib/gpu/lal_lj96_ext.cpp:  gpu_mode=LJ96MF.device->gpu_mode();
lib/gpu/lal_lj96_ext.cpp:  double gpu_split=LJ96MF.device->particle_split();
lib/gpu/lal_lj96_ext.cpp:  int first_gpu=LJ96MF.device->first_device();
lib/gpu/lal_lj96_ext.cpp:  int last_gpu=LJ96MF.device->last_device();
lib/gpu/lal_lj96_ext.cpp:  int gpu_rank=LJ96MF.device->gpu_rank();
lib/gpu/lal_lj96_ext.cpp:  int procs_per_gpu=LJ96MF.device->procs_per_gpu();
lib/gpu/lal_lj96_ext.cpp:  LJ96MF.device->init_message(screen,"lj96/cut",first_gpu,last_gpu);
lib/gpu/lal_lj96_ext.cpp:                        maxspecial, cell_size, gpu_split, screen);
lib/gpu/lal_lj96_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_lj96_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_lj96_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_lj96_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_lj96_ext.cpp:                last_gpu,i);
lib/gpu/lal_lj96_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_lj96_ext.cpp:                          cell_size, gpu_split, screen);
lib/gpu/lal_lj96_ext.cpp:    LJ96MF.estimate_gpu_overhead();
lib/gpu/lal_lj96_ext.cpp:void lj96_gpu_clear() {
lib/gpu/lal_lj96_ext.cpp:int** lj96_gpu_compute_n(const int ago, const int inum_full,
lib/gpu/lal_lj96_ext.cpp:void lj96_gpu_compute(const int ago, const int inum_full, const int nall,
lib/gpu/lal_lj96_ext.cpp:double lj96_gpu_bytes() {
lib/gpu/lal_tersoff_mod_ext.cpp:int tersoff_mod_gpu_init(const int ntypes, const int inum, const int nall,
lib/gpu/lal_tersoff_mod_ext.cpp:       const int max_nbors, const double cell_size, int &gpu_mode, FILE *screen,
lib/gpu/lal_tersoff_mod_ext.cpp:  gpu_mode=TSMMF.device->gpu_mode();
lib/gpu/lal_tersoff_mod_ext.cpp:  double gpu_split=TSMMF.device->particle_split();
lib/gpu/lal_tersoff_mod_ext.cpp:  int first_gpu=TSMMF.device->first_device();
lib/gpu/lal_tersoff_mod_ext.cpp:  int last_gpu=TSMMF.device->last_device();
lib/gpu/lal_tersoff_mod_ext.cpp:  int gpu_rank=TSMMF.device->gpu_rank();
lib/gpu/lal_tersoff_mod_ext.cpp:  int procs_per_gpu=TSMMF.device->procs_per_gpu();
lib/gpu/lal_tersoff_mod_ext.cpp:  if (gpu_split != 1.0)
lib/gpu/lal_tersoff_mod_ext.cpp:  TSMMF.device->init_message(screen,"tersoff/mod/gpu",first_gpu,last_gpu);
lib/gpu/lal_tersoff_mod_ext.cpp:    init_ok=TSMMF.init(ntypes, inum, nall, max_nbors, cell_size, gpu_split, screen,
lib/gpu/lal_tersoff_mod_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_tersoff_mod_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_tersoff_mod_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_tersoff_mod_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_tersoff_mod_ext.cpp:                last_gpu,i);
lib/gpu/lal_tersoff_mod_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_tersoff_mod_ext.cpp:      init_ok=TSMMF.init(ntypes, inum, nall, max_nbors, cell_size, gpu_split, screen,
lib/gpu/lal_tersoff_mod_ext.cpp:    TSMMF.estimate_gpu_overhead(1);
lib/gpu/lal_tersoff_mod_ext.cpp:void tersoff_mod_gpu_clear() {
lib/gpu/lal_tersoff_mod_ext.cpp:int ** tersoff_mod_gpu_compute_n(const int ago, const int inum_full,
lib/gpu/lal_tersoff_mod_ext.cpp:void tersoff_mod_gpu_compute(const int ago, const int nlocal, const int nall,
lib/gpu/lal_tersoff_mod_ext.cpp:double tersoff_mod_gpu_bytes() {
lib/gpu/Nvidia.makefile_multi:GPU_LIB = $(LIB_DIR)/libgpu.a
lib/gpu/Nvidia.makefile_multi:all: $(OBJ_DIR) $(CUHS) $(GPU_LIB) $(EXECS)
lib/gpu/Nvidia.makefile_multi:CUDA  = $(NVCC) $(CUDA_INCLUDE) $(CUDA_OPTS) $(CUDA_ARCH) \
lib/gpu/Nvidia.makefile_multi:             $(CUDA_PRECISION)
lib/gpu/Nvidia.makefile_multi:CUDR  = $(CUDR_CPP) $(CUDR_OPTS) $(CUDA_PRECISION) $(CUDA_INCLUDE) \
lib/gpu/Nvidia.makefile_multi:CUDA_LINK = $(CUDA_LIB) -lcudart
lib/gpu/Nvidia.makefile_multi:BIN2C = $(CUDA_HOME)/bin/bin2c
lib/gpu/Nvidia.makefile_multi:	$(CUDA) --fatbin -DNV_KERNEL -Dgrdtyp=float -Dgrdtyp4=float4 -o $@ lal_pppm.cu
lib/gpu/Nvidia.makefile_multi:	$(CUDA) --fatbin -DNV_KERNEL -Dgrdtyp=double -Dgrdtyp4=double4 -o $@ lal_pppm.cu
lib/gpu/Nvidia.makefile_multi:	$(CUDA) --fatbin -DNV_KERNEL -o $(OBJ_DIR)/$*.cubin $(OBJ_DIR)/lal_$*.cu
lib/gpu/Nvidia.makefile_multi:	$(CUDA) -o $@ -c cudpp_mini/radixsort_app.cu
lib/gpu/Nvidia.makefile_multi:	$(CUDA) -o $@ -c cudpp_mini/scan_app.cu
lib/gpu/Nvidia.makefile_multi:# build libgpu.a
lib/gpu/Nvidia.makefile_multi:$(GPU_LIB): $(OBJS) $(CUDPP)
lib/gpu/Nvidia.makefile_multi:	$(AR) -crusv $(GPU_LIB) $(OBJS) $(CUDPP)
lib/gpu/Nvidia.makefile_multi:	$(CUDR) -o $@ ./geryon/ucl_get_devices.cpp -DUCL_CUDADR $(CUDA_LIB) -lcuda
lib/gpu/Nvidia.makefile_multi:	-rm -f $(EXECS) $(GPU_LIB) $(OBJS) $(CUDPP) $(CUHS) *.linkinfo
lib/gpu/Nvidia.makefile_multi:	-rm -f $(EXECS) $(GPU_LIB) $(OBJS) $(CUHS) *.linkinfo
lib/gpu/lal_base_atomic.cpp:                             const double cell_size, const double gpu_split,
lib/gpu/lal_base_atomic.cpp:  int gpu_nbor=0;
lib/gpu/lal_base_atomic.cpp:  if (device->gpu_mode()==Device<numtyp,acctyp>::GPU_NEIGH)
lib/gpu/lal_base_atomic.cpp:    gpu_nbor=1;
lib/gpu/lal_base_atomic.cpp:  else if (device->gpu_mode()==Device<numtyp,acctyp>::GPU_HYB_NEIGH)
lib/gpu/lal_base_atomic.cpp:    gpu_nbor=2;
lib/gpu/lal_base_atomic.cpp:  int _gpu_host=0;
lib/gpu/lal_base_atomic.cpp:  int host_nlocal=hd_balancer.first_host_count(nlocal,gpu_split,gpu_nbor);
lib/gpu/lal_base_atomic.cpp:    _gpu_host=1;
lib/gpu/lal_base_atomic.cpp:  if (ucl_device!=device->gpu) _compiled=false;
lib/gpu/lal_base_atomic.cpp:  ucl_device=device->gpu;
lib/gpu/lal_base_atomic.cpp:  if (_threads_per_atom>1 && gpu_nbor==0) {
lib/gpu/lal_base_atomic.cpp:  success = device->init_nbor(nbor,nlocal,host_nlocal,nall,maxspecial,_gpu_host,
lib/gpu/lal_base_atomic.cpp:  hd_balancer.init(device,gpu_nbor,gpu_split);
lib/gpu/lal_base_atomic.cpp:  // Initialize timers for the selected GPU
lib/gpu/lal_base_atomic.cpp:  _max_an_bytes=ans->gpu_bytes()+nbor->gpu_bytes();
lib/gpu/lal_base_atomic.cpp:void BaseAtomicT::estimate_gpu_overhead(const int add_kernels) {
lib/gpu/lal_base_atomic.cpp:  device->estimate_gpu_overhead(1+add_kernels,_gpu_overhead,_driver_overhead);
lib/gpu/lal_base_atomic.cpp:  _gpu_overhead*=hd_balancer.timestep();
lib/gpu/lal_base_atomic.cpp:                       _gpu_overhead,_driver_overhead,_threads_per_atom,screen);
lib/gpu/lal_base_atomic.cpp:  double bytes=ans->gpu_bytes()+nbor->gpu_bytes();
lib/gpu/lal_base_atomic.cpp:  double bytes=ans->gpu_bytes()+nbor->gpu_bytes();
lib/gpu/lal_base_atomic.cpp:// Reneighbor on GPU if necessary and then compute forces, virials, energies
lib/gpu/lal_base_atomic.cpp:  int inum=hd_balancer.get_gpu_count(ago,inum_full);
lib/gpu/lal_base_atomic.cpp:  // Build neighbor list on GPU if necessary
lib/gpu/lal_base_atomic.cpp:  #if defined(USE_OPENCL) && (defined(CL_VERSION_2_1) || defined(CL_VERSION_3_0))
lib/gpu/lal_colloid_ext.cpp:int colloid_gpu_init(const int ntypes, double **cutsq, double **host_lj1,
lib/gpu/lal_colloid_ext.cpp:                     const double cell_size, int &gpu_mode, FILE *screen) {
lib/gpu/lal_colloid_ext.cpp:  gpu_mode=COLLMF.device->gpu_mode();
lib/gpu/lal_colloid_ext.cpp:  double gpu_split=COLLMF.device->particle_split();
lib/gpu/lal_colloid_ext.cpp:  int first_gpu=COLLMF.device->first_device();
lib/gpu/lal_colloid_ext.cpp:  int last_gpu=COLLMF.device->last_device();
lib/gpu/lal_colloid_ext.cpp:  int gpu_rank=COLLMF.device->gpu_rank();
lib/gpu/lal_colloid_ext.cpp:  int procs_per_gpu=COLLMF.device->procs_per_gpu();
lib/gpu/lal_colloid_ext.cpp:  COLLMF.device->init_message(screen,"colloid",first_gpu,last_gpu);
lib/gpu/lal_colloid_ext.cpp:                        maxspecial, cell_size, gpu_split, screen);
lib/gpu/lal_colloid_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_colloid_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_colloid_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_colloid_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_colloid_ext.cpp:                last_gpu,i);
lib/gpu/lal_colloid_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_colloid_ext.cpp:                          cell_size, gpu_split, screen);
lib/gpu/lal_colloid_ext.cpp:    COLLMF.estimate_gpu_overhead();
lib/gpu/lal_colloid_ext.cpp:void colloid_gpu_clear() {
lib/gpu/lal_colloid_ext.cpp:int ** colloid_gpu_compute_n(const int ago, const int inum_full,
lib/gpu/lal_colloid_ext.cpp:void colloid_gpu_compute(const int ago, const int inum_full, const int nall,
lib/gpu/lal_colloid_ext.cpp:double colloid_gpu_bytes() {
lib/gpu/Makefile.linux_opencl:#  Generic Linux Makefile for OpenCL - Mixed precision
lib/gpu/Makefile.linux_opencl:EXTRAMAKE = Makefile.lammps.opencl
lib/gpu/Makefile.linux_opencl:OCL_LINK = -lOpenCL
lib/gpu/Makefile.linux_opencl:include Opencl.makefile
lib/gpu/lal_lj_coul_long_soft.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_lj_coul_long_soft.h:    * - -1 if fix gpu not found
lib/gpu/lal_lj_coul_long_soft.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_lj_coul_long_soft.h:           const double gpu_split, FILE *screen, double **host_cut_ljsq,
lib/gpu/Nvidia.makefile:         lal_pre_cuda_hip.h
lib/gpu/Nvidia.makefile:GPU_LIB = $(LIB_DIR)/libgpu.a
lib/gpu/Nvidia.makefile:all: $(OBJ_DIR) $(CUHS) $(GPU_LIB) $(EXECS)
lib/gpu/Nvidia.makefile:CUDA  = $(NVCC) $(CUDA_INCLUDE) $(CUDA_OPTS) $(CUDA_ARCH) \
lib/gpu/Nvidia.makefile:             $(CUDA_PRECISION)
lib/gpu/Nvidia.makefile:CUDR  = $(CUDR_CPP) $(CUDR_OPTS) $(CUDA_PRECISION) $(CUDA_INCLUDE) \
lib/gpu/Nvidia.makefile:CUDA_LINK = $(CUDA_LIB) -lcudart
lib/gpu/Nvidia.makefile:BIN2C = $(CUDA_HOME)/bin/bin2c
lib/gpu/Nvidia.makefile:                         lal_pre_cuda_hip.h
lib/gpu/Nvidia.makefile:	$(CUDA) --fatbin -DNV_KERNEL -Dgrdtyp=float -Dgrdtyp4=float4 -o $@ lal_pppm.cu
lib/gpu/Nvidia.makefile:                         lal_pre_cuda_hip.h
lib/gpu/Nvidia.makefile:	$(CUDA) --fatbin -DNV_KERNEL -Dgrdtyp=double -Dgrdtyp4=double4 -o $@ lal_pppm.cu
lib/gpu/Nvidia.makefile:	$(CUDA) --fatbin -DNV_KERNEL -o $(OBJ_DIR)/$*.cubin $(OBJ_DIR)/lal_$*.cu
lib/gpu/Nvidia.makefile:	$(CUDA) -o $@ -c cudpp_mini/radixsort_app.cu -Icudpp_mini
lib/gpu/Nvidia.makefile:	$(CUDA) -o $@ -c cudpp_mini/scan_app.cu -Icudpp_mini
lib/gpu/Nvidia.makefile:# build libgpu.a
lib/gpu/Nvidia.makefile:$(GPU_LIB): $(OBJS) $(CUDPP)
lib/gpu/Nvidia.makefile:	$(AR) -crusv $(GPU_LIB) $(OBJS) $(CUDPP)
lib/gpu/Nvidia.makefile:	$(CUDR) -o $@ ./geryon/ucl_get_devices.cpp -DUCL_CUDADR $(CUDA_LIB) -lcuda
lib/gpu/Nvidia.makefile:	-rm -f $(EXECS) $(GPU_LIB) $(OBJS) $(CUDPP) $(CUHS) *.cubin *.linkinfo
lib/gpu/Nvidia.makefile:	-rm -f $(EXECS) $(GPU_LIB) $(OBJS) $(CUHS) *.linkinfo
lib/gpu/lal_tersoff_mod.cu:                                  const int t_per_atom, const int gpu_nbor) {
lib/gpu/lal_tersoff_mod.cu:      if (gpu_nbor) nbor_k=j+nbor_pitch;
lib/gpu/lal_tersoff_mod.cu:                                        const int t_per_atom, const int gpu_nbor) {
lib/gpu/lal_tersoff_mod.cu:      if (gpu_nbor) nbor_k=j+nbor_pitch;
lib/gpu/lal_eam.cu:#if (__CUDACC_VER_MAJOR__ >= 11)
lib/gpu/lal_hippo_ext.cpp:int hippo_gpu_init(const int ntypes, const int max_amtype, const int max_amclass,
lib/gpu/lal_hippo_ext.cpp:                    const double cell_size, int &gpu_mode, FILE *screen,
lib/gpu/lal_hippo_ext.cpp:  gpu_mode=HIPPOMF.device->gpu_mode();
lib/gpu/lal_hippo_ext.cpp:  double gpu_split=HIPPOMF.device->particle_split();
lib/gpu/lal_hippo_ext.cpp:  int first_gpu=HIPPOMF.device->first_device();
lib/gpu/lal_hippo_ext.cpp:  int last_gpu=HIPPOMF.device->last_device();
lib/gpu/lal_hippo_ext.cpp:  int gpu_rank=HIPPOMF.device->gpu_rank();
lib/gpu/lal_hippo_ext.cpp:  int procs_per_gpu=HIPPOMF.device->procs_per_gpu();
lib/gpu/lal_hippo_ext.cpp:  HIPPOMF.device->init_message(screen,"HIPPO",first_gpu,last_gpu);
lib/gpu/lal_hippo_ext.cpp:    fprintf(screen,"Initializing GPU and compiling on process 0...");
lib/gpu/lal_hippo_ext.cpp:                         maxspecial, maxspecial15, cell_size, gpu_split,
lib/gpu/lal_hippo_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_hippo_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_hippo_ext.cpp:        fprintf(screen,"Initializing GPU %d on core %d...",first_gpu,i);
lib/gpu/lal_hippo_ext.cpp:        fprintf(screen,"Initializing GPUs %d-%d on core %d...",first_gpu,
lib/gpu/lal_hippo_ext.cpp:                last_gpu,i);
lib/gpu/lal_hippo_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_hippo_ext.cpp:                           maxspecial, maxspecial15, cell_size, gpu_split,
lib/gpu/lal_hippo_ext.cpp:    HIPPOMF.device->gpu_barrier();
lib/gpu/lal_hippo_ext.cpp:    HIPPOMF.estimate_gpu_overhead();
lib/gpu/lal_hippo_ext.cpp:void hippo_gpu_clear() {
lib/gpu/lal_hippo_ext.cpp:int** hippo_gpu_precompute(const int ago, const int inum_full, const int nall,
lib/gpu/lal_hippo_ext.cpp:void hippo_gpu_compute_repulsion(const int ago, const int inum_full,
lib/gpu/lal_hippo_ext.cpp:void hippo_gpu_compute_dispersion_real(int *host_amtype, int *host_amgroup,
lib/gpu/lal_hippo_ext.cpp:void hippo_gpu_compute_multipole_real(const int ago, const int inum_full,
lib/gpu/lal_hippo_ext.cpp:void hippo_gpu_compute_udirect2b(int *host_amtype, int *host_amgroup, double **host_rpole,
lib/gpu/lal_hippo_ext.cpp:void hippo_gpu_compute_umutual2b(int *host_amtype, int *host_amgroup, double **host_rpole,
lib/gpu/lal_hippo_ext.cpp:void hippo_gpu_update_fieldp(void **fieldp_ptr) {
lib/gpu/lal_hippo_ext.cpp:void hippo_gpu_compute_polar_real(int *host_amtype, int *host_amgroup, double **host_rpole,
lib/gpu/lal_hippo_ext.cpp:void hippo_gpu_precompute_kspace(const int inum_full, const int bsorder,
lib/gpu/lal_hippo_ext.cpp:void hippo_gpu_fphi_uind(double ****host_grid_brick, void **host_fdip_phi1,
lib/gpu/lal_hippo_ext.cpp:double hippo_gpu_bytes() {
lib/gpu/lal_born_coul_wolf_cs_ext.cpp:int borncwcs_gpu_init(const int ntypes, double **cutsq, double **host_rhoinv,
lib/gpu/lal_born_coul_wolf_cs_ext.cpp:                    const double cell_size, int &gpu_mode, FILE *screen,
lib/gpu/lal_born_coul_wolf_cs_ext.cpp:  gpu_mode=BornCWCST.device->gpu_mode();
lib/gpu/lal_born_coul_wolf_cs_ext.cpp:  double gpu_split=BornCWCST.device->particle_split();
lib/gpu/lal_born_coul_wolf_cs_ext.cpp:  int first_gpu=BornCWCST.device->first_device();
lib/gpu/lal_born_coul_wolf_cs_ext.cpp:  int last_gpu=BornCWCST.device->last_device();
lib/gpu/lal_born_coul_wolf_cs_ext.cpp:  int gpu_rank=BornCWCST.device->gpu_rank();
lib/gpu/lal_born_coul_wolf_cs_ext.cpp:  int procs_per_gpu=BornCWCST.device->procs_per_gpu();
lib/gpu/lal_born_coul_wolf_cs_ext.cpp:  BornCWCST.device->init_message(screen,"born/coul/wolf/cs",first_gpu,last_gpu);
lib/gpu/lal_born_coul_wolf_cs_ext.cpp:                          maxspecial, cell_size, gpu_split, screen, host_cut_ljsq,
lib/gpu/lal_born_coul_wolf_cs_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_born_coul_wolf_cs_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_born_coul_wolf_cs_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_born_coul_wolf_cs_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_born_coul_wolf_cs_ext.cpp:                last_gpu,i);
lib/gpu/lal_born_coul_wolf_cs_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_born_coul_wolf_cs_ext.cpp:                            maxspecial, cell_size, gpu_split, screen, host_cut_ljsq,
lib/gpu/lal_born_coul_wolf_cs_ext.cpp:    BornCWCST.estimate_gpu_overhead();
lib/gpu/lal_born_coul_wolf_cs_ext.cpp:void borncwcs_gpu_clear() {
lib/gpu/lal_born_coul_wolf_cs_ext.cpp:int** borncwcs_gpu_compute_n(const int ago, const int inum_full,
lib/gpu/lal_born_coul_wolf_cs_ext.cpp:void borncwcs_gpu_compute(const int ago, const int inum_full, const int nall,
lib/gpu/lal_born_coul_wolf_cs_ext.cpp:double borncwcs_gpu_bytes() {
lib/gpu/lal_precision.h:#if defined(USE_CUDART)
lib/gpu/lal_precision.h:#include <cuda_runtime.h>
lib/gpu/lal_precision.h:struct _lgpu_int2 {
lib/gpu/lal_precision.h:#define int2 _lgpu_int2
lib/gpu/lal_precision.h:struct _lgpu_float2 {
lib/gpu/lal_precision.h:struct _lgpu_float3 {
lib/gpu/lal_precision.h:struct _lgpu_float4 {
lib/gpu/lal_precision.h:struct _lgpu_double2 {
lib/gpu/lal_precision.h:struct _lgpu_double3 {
lib/gpu/lal_precision.h:struct _lgpu_double4 {
lib/gpu/lal_precision.h:inline std::ostream & operator<<(std::ostream &out, const _lgpu_float2 &v) {
lib/gpu/lal_precision.h:inline std::ostream & operator<<(std::ostream &out, const _lgpu_float3 &v) {
lib/gpu/lal_precision.h:inline std::ostream & operator<<(std::ostream &out, const _lgpu_float4 &v) {
lib/gpu/lal_precision.h:inline std::ostream & operator<<(std::ostream &out, const _lgpu_double2 &v) {
lib/gpu/lal_precision.h:inline std::ostream & operator<<(std::ostream &out, const _lgpu_double3 &v) {
lib/gpu/lal_precision.h:inline std::ostream & operator<<(std::ostream &out, const _lgpu_double4 &v) {
lib/gpu/lal_precision.h:#define numtyp2 _lgpu_float2
lib/gpu/lal_precision.h:#define numtyp3 _lgpu_float3
lib/gpu/lal_precision.h:#define numtyp4 _lgpu_float4
lib/gpu/lal_precision.h:#define acctyp2 _lgpu_double2
lib/gpu/lal_precision.h:#define acctyp3 _lgpu_double3
lib/gpu/lal_precision.h:#define acctyp4 _lgpu_double4
lib/gpu/lal_precision.h:#define numtyp2 _lgpu_double2
lib/gpu/lal_precision.h:#define numtyp3 _lgpu_double3
lib/gpu/lal_precision.h:#define numtyp4 _lgpu_double4
lib/gpu/lal_precision.h:#define acctyp2 _lgpu_double2
lib/gpu/lal_precision.h:#define acctyp3 _lgpu_double3
lib/gpu/lal_precision.h:#define acctyp4 _lgpu_double4
lib/gpu/lal_precision.h:#define numtyp2 _lgpu_float2
lib/gpu/lal_precision.h:#define numtyp3 _lgpu_float3
lib/gpu/lal_precision.h:#define numtyp4 _lgpu_float4
lib/gpu/lal_precision.h:#define acctyp2 _lgpu_float2
lib/gpu/lal_precision.h:#define acctyp3 _lgpu_float3
lib/gpu/lal_precision.h:#define acctyp4 _lgpu_float4
lib/gpu/lal_lj_cubic.cpp:#if defined(USE_OPENCL)
lib/gpu/lal_lj_cubic.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_lj_cubic.cpp:                   const double gpu_split, FILE *_screen) {
lib/gpu/lal_lj_cubic.cpp:  success=this->init_atomic(nlocal,nall,max_nbors,maxspecial,cell_size,gpu_split,
lib/gpu/lal_lj_expand_coul_long.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_lj_expand_coul_long.h:    * - -1 if fix gpu not found
lib/gpu/lal_lj_expand_coul_long.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_lj_expand_coul_long.h:           const double gpu_split, FILE *screen, double **host_cut_ljsq,
lib/gpu/lal_lj_coul_long_ext.cpp:int ljcl_gpu_init(const int ntypes, double **cutsq, double **host_lj1,
lib/gpu/lal_lj_coul_long_ext.cpp:                  const double cell_size, int &gpu_mode, FILE *screen,
lib/gpu/lal_lj_coul_long_ext.cpp:  gpu_mode=LJCLMF.device->gpu_mode();
lib/gpu/lal_lj_coul_long_ext.cpp:  double gpu_split=LJCLMF.device->particle_split();
lib/gpu/lal_lj_coul_long_ext.cpp:  int first_gpu=LJCLMF.device->first_device();
lib/gpu/lal_lj_coul_long_ext.cpp:  int last_gpu=LJCLMF.device->last_device();
lib/gpu/lal_lj_coul_long_ext.cpp:  int gpu_rank=LJCLMF.device->gpu_rank();
lib/gpu/lal_lj_coul_long_ext.cpp:  int procs_per_gpu=LJCLMF.device->procs_per_gpu();
lib/gpu/lal_lj_coul_long_ext.cpp:  LJCLMF.device->init_message(screen,"lj/cut/coul/long",first_gpu,last_gpu);
lib/gpu/lal_lj_coul_long_ext.cpp:                        cell_size, gpu_split, screen, host_cut_ljsq,
lib/gpu/lal_lj_coul_long_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_lj_coul_long_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_lj_coul_long_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_lj_coul_long_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_lj_coul_long_ext.cpp:                last_gpu,i);
lib/gpu/lal_lj_coul_long_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_lj_coul_long_ext.cpp:                          cell_size, gpu_split, screen, host_cut_ljsq,
lib/gpu/lal_lj_coul_long_ext.cpp:    LJCLMF.estimate_gpu_overhead();
lib/gpu/lal_lj_coul_long_ext.cpp:void ljcl_gpu_reinit(const int ntypes, double **cutsq, double **host_lj1,
lib/gpu/lal_lj_coul_long_ext.cpp:  int gpu_rank=LJCLMF.device->gpu_rank();
lib/gpu/lal_lj_coul_long_ext.cpp:  int procs_per_gpu=LJCLMF.device->procs_per_gpu();
lib/gpu/lal_lj_coul_long_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_lj_coul_long_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_lj_coul_long_ext.cpp:void ljcl_gpu_clear() {
lib/gpu/lal_lj_coul_long_ext.cpp:int** ljcl_gpu_compute_n(const int ago, const int inum_full,
lib/gpu/lal_lj_coul_long_ext.cpp:void ljcl_gpu_compute(const int ago, const int inum_full, const int nall,
lib/gpu/lal_lj_coul_long_ext.cpp:double ljcl_gpu_bytes() {
lib/gpu/lal_lj_coul_long.cpp:#if defined(USE_OPENCL)
lib/gpu/lal_lj_coul_long.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_lj_coul_long.cpp:                           const double gpu_split, FILE *_screen,
lib/gpu/lal_lj_coul_long.cpp:  success=this->init_atomic(nlocal,nall,max_nbors,maxspecial,cell_size,gpu_split,
lib/gpu/lal_sph_lj.cpp:#if defined(USE_OPENCL)
lib/gpu/lal_sph_lj.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_sph_lj.cpp:                 const double gpu_split, FILE *_screen) {
lib/gpu/lal_sph_lj.cpp:  #ifdef USE_OPENCL
lib/gpu/lal_sph_lj.cpp:                            gpu_split,_screen,sph_lj,"k_sph_lj",onetype,extra_fields);
lib/gpu/lal_tersoff_mod.cpp:#if defined(USE_OPENCL)
lib/gpu/lal_tersoff_mod.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_tersoff_mod.cpp:                   const double cell_size, const double gpu_split, FILE *_screen,
lib/gpu/lal_tersoff_mod.cpp:  #ifdef USE_OPENCL
lib/gpu/lal_tersoff_mod.cpp:  success=this->init_three(nlocal,nall,max_nbors,0,cell_size,gpu_split,
lib/gpu/lal_tersoff_mod.cpp:                          &this->_gpu_nbor);
lib/gpu/lal_tersoff_mod.cpp:                          &this->_gpu_nbor);
lib/gpu/lal_edpd_ext.cpp:int edpd_gpu_init(const int ntypes, double **cutsq, double **host_a0,
lib/gpu/lal_edpd_ext.cpp:                  const double cell_size, int &gpu_mode, FILE *screen) {
lib/gpu/lal_edpd_ext.cpp:  gpu_mode=EDPDMF.device->gpu_mode();
lib/gpu/lal_edpd_ext.cpp:  double gpu_split=EDPDMF.device->particle_split();
lib/gpu/lal_edpd_ext.cpp:  int first_gpu=EDPDMF.device->first_device();
lib/gpu/lal_edpd_ext.cpp:  int last_gpu=EDPDMF.device->last_device();
lib/gpu/lal_edpd_ext.cpp:  int gpu_rank=EDPDMF.device->gpu_rank();
lib/gpu/lal_edpd_ext.cpp:  int procs_per_gpu=EDPDMF.device->procs_per_gpu();
lib/gpu/lal_edpd_ext.cpp:  EDPDMF.device->init_message(screen,"edpd",first_gpu,last_gpu);
lib/gpu/lal_edpd_ext.cpp:                        cell_size, gpu_split, screen);
lib/gpu/lal_edpd_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_edpd_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_edpd_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_edpd_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_edpd_ext.cpp:                last_gpu,i);
lib/gpu/lal_edpd_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_edpd_ext.cpp:                          cell_size, gpu_split, screen);
lib/gpu/lal_edpd_ext.cpp:    EDPDMF.estimate_gpu_overhead();
lib/gpu/lal_edpd_ext.cpp:void edpd_gpu_clear() {
lib/gpu/lal_edpd_ext.cpp:int ** edpd_gpu_compute_n(const int ago, const int inum_full, const int nall,
lib/gpu/lal_edpd_ext.cpp:void edpd_gpu_compute(const int ago, const int inum_full, const int nall,
lib/gpu/lal_edpd_ext.cpp:void edpd_gpu_get_extra_data(double *host_T, double *host_cv) {
lib/gpu/lal_edpd_ext.cpp:void edpd_gpu_update_flux(void **flux_ptr) {
lib/gpu/lal_edpd_ext.cpp:double edpd_gpu_bytes() {
lib/gpu/lal_yukawa_colloid.cu:#if (__CUDACC_VER_MAJOR__ >= 11)
lib/gpu/lal_lj_coul_msm.cu:#if (__CUDACC_VER_MAJOR__ >= 11)
lib/gpu/lal_lj_coul_msm.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_lj_coul_msm.h:    * - -1 if fix gpu not found
lib/gpu/lal_lj_coul_msm.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_lj_coul_msm.h:           const double gpu_split, FILE *screen, double **host_cut_ljsq,
lib/gpu/lal_base_sph.cpp:                          const double cell_size, const double gpu_split,
lib/gpu/lal_base_sph.cpp:  int gpu_nbor=0;
lib/gpu/lal_base_sph.cpp:  if (device->gpu_mode()==Device<numtyp,acctyp>::GPU_NEIGH)
lib/gpu/lal_base_sph.cpp:    gpu_nbor=1;
lib/gpu/lal_base_sph.cpp:  else if (device->gpu_mode()==Device<numtyp,acctyp>::GPU_HYB_NEIGH)
lib/gpu/lal_base_sph.cpp:    gpu_nbor=2;
lib/gpu/lal_base_sph.cpp:  int _gpu_host=0;
lib/gpu/lal_base_sph.cpp:  int host_nlocal=hd_balancer.first_host_count(nlocal,gpu_split,gpu_nbor);
lib/gpu/lal_base_sph.cpp:    _gpu_host=1;
lib/gpu/lal_base_sph.cpp:  if (ucl_device!=device->gpu) _compiled=false;
lib/gpu/lal_base_sph.cpp:  ucl_device=device->gpu;
lib/gpu/lal_base_sph.cpp:  if (_threads_per_atom>1 && gpu_nbor==0) {
lib/gpu/lal_base_sph.cpp:  success = device->init_nbor(nbor,nlocal,host_nlocal,nall,maxspecial,_gpu_host,
lib/gpu/lal_base_sph.cpp:  hd_balancer.init(device,gpu_nbor,gpu_split);
lib/gpu/lal_base_sph.cpp:  // Initialize timers for the selected GPU
lib/gpu/lal_base_sph.cpp:  _max_an_bytes=ans->gpu_bytes()+nbor->gpu_bytes();
lib/gpu/lal_base_sph.cpp:void BaseSPHT::estimate_gpu_overhead() {
lib/gpu/lal_base_sph.cpp:  device->estimate_gpu_overhead(1,_gpu_overhead,_driver_overhead);
lib/gpu/lal_base_sph.cpp:  _gpu_overhead*=hd_balancer.timestep();
lib/gpu/lal_base_sph.cpp:                       _gpu_overhead,_driver_overhead,_threads_per_atom,screen);
lib/gpu/lal_base_sph.cpp:  double bytes=ans->gpu_bytes()+nbor->gpu_bytes();
lib/gpu/lal_base_sph.cpp:  double bytes=ans->gpu_bytes()+nbor->gpu_bytes();
lib/gpu/lal_base_sph.cpp:// Reneighbor on GPU if necessary and then compute forces, virials, energies
lib/gpu/lal_base_sph.cpp:  int inum=hd_balancer.get_gpu_count(ago,inum_full);
lib/gpu/lal_base_sph.cpp:  // Build neighbor list on GPU if necessary
lib/gpu/lal_base_sph.cpp:  #if defined(USE_OPENCL) && (defined(CL_VERSION_2_1) || defined(CL_VERSION_3_0))
lib/gpu/lal_gauss_ext.cpp:int gauss_gpu_init(const int ntypes, double **cutsq, double **host_a,
lib/gpu/lal_gauss_ext.cpp:                   const double cell_size, int &gpu_mode, FILE *screen) {
lib/gpu/lal_gauss_ext.cpp:  gpu_mode=GLMF.device->gpu_mode();
lib/gpu/lal_gauss_ext.cpp:  double gpu_split=GLMF.device->particle_split();
lib/gpu/lal_gauss_ext.cpp:  int first_gpu=GLMF.device->first_device();
lib/gpu/lal_gauss_ext.cpp:  int last_gpu=GLMF.device->last_device();
lib/gpu/lal_gauss_ext.cpp:  int gpu_rank=GLMF.device->gpu_rank();
lib/gpu/lal_gauss_ext.cpp:  int procs_per_gpu=GLMF.device->procs_per_gpu();
lib/gpu/lal_gauss_ext.cpp:  GLMF.device->init_message(screen,"gauss",first_gpu,last_gpu);
lib/gpu/lal_gauss_ext.cpp:                       maxspecial, cell_size, gpu_split, screen);
lib/gpu/lal_gauss_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_gauss_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_gauss_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_gauss_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_gauss_ext.cpp:                last_gpu,i);
lib/gpu/lal_gauss_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_gauss_ext.cpp:                        cell_size, gpu_split, screen);
lib/gpu/lal_gauss_ext.cpp:    GLMF.estimate_gpu_overhead();
lib/gpu/lal_gauss_ext.cpp:void gauss_gpu_reinit(const int ntypes, double **cutsq, double **host_a,
lib/gpu/lal_gauss_ext.cpp:  int gpu_rank=GLMF.device->gpu_rank();
lib/gpu/lal_gauss_ext.cpp:  int procs_per_gpu=GLMF.device->procs_per_gpu();
lib/gpu/lal_gauss_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_gauss_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_gauss_ext.cpp:void gauss_gpu_clear() {
lib/gpu/lal_gauss_ext.cpp:int ** gauss_gpu_compute_n(const int ago, const int inum_full,
lib/gpu/lal_gauss_ext.cpp:void gauss_gpu_compute(const int ago, const int inum_full, const int nall,
lib/gpu/lal_gauss_ext.cpp:double gauss_gpu_bytes() {
lib/gpu/lal_neighbor.cpp:                              Peng Wang (Nvidia)
lib/gpu/lal_neighbor.cpp:    email                : brownw@ornl.gov, penwang@nvidia.com
lib/gpu/lal_neighbor.cpp:  if (_gpu_nbor==1)
lib/gpu/lal_neighbor.cpp:  else if (_gpu_nbor==2)
lib/gpu/lal_neighbor.cpp:                    const int maxspecial, UCL_Device &devi, const int gpu_nbor,
lib/gpu/lal_neighbor.cpp:                    const int gpu_host, const bool pre_cut,
lib/gpu/lal_neighbor.cpp:  _gpu_nbor=gpu_nbor;
lib/gpu/lal_neighbor.cpp:  if (gpu_host==0)
lib/gpu/lal_neighbor.cpp:    _gpu_host=false;
lib/gpu/lal_neighbor.cpp:  else if (gpu_host==1)
lib/gpu/lal_neighbor.cpp:    _gpu_host=true;
lib/gpu/lal_neighbor.cpp:  _alloc_packed = pre_cut || gpu_nbor==0;
lib/gpu/lal_neighbor.cpp:  // Initialize timers for the selected GPU
lib/gpu/lal_neighbor.cpp:  if (_gpu_nbor != 1)
lib/gpu/lal_neighbor.cpp:  if (gpu_nbor==0)
lib/gpu/lal_neighbor.cpp:  if (gpu_nbor==0)
lib/gpu/lal_neighbor.cpp:      _shared->compile_kernels(devi, gpu_nbor, compile_flags+
lib/gpu/lal_neighbor.cpp:      _shared->compile_kernels(devi,gpu_nbor,compile_flags);
lib/gpu/lal_neighbor.cpp:    if (_gpu_nbor) {
lib/gpu/lal_neighbor.cpp:      #if defined(USE_OPENCL) && (defined(CL_VERSION_2_1) || \
lib/gpu/lal_neighbor.cpp:          _shared->compile_kernels(devi, gpu_nbor, compile_flags+
lib/gpu/lal_neighbor.cpp:  if (!_use_packing || _gpu_nbor>0) {
lib/gpu/lal_neighbor.cpp:  if (_gpu_nbor != 2 || _max_host>0)
lib/gpu/lal_neighbor.cpp:      if (_gpu_nbor) {
lib/gpu/lal_neighbor.cpp:    // Some OpenCL implementations return errors for nullptr pointers as args
lib/gpu/lal_neighbor.cpp:    _gpu_bytes+=dev_nspecial.row_bytes()+dev_special.row_bytes()+
lib/gpu/lal_neighbor.cpp:  _gpu_bytes=0.0;
lib/gpu/lal_neighbor.cpp:    if (_gpu_nbor==2)
lib/gpu/lal_neighbor.cpp:  if (_gpu_nbor>0) {
lib/gpu/lal_neighbor.cpp:    if (_gpu_host)
lib/gpu/lal_neighbor.cpp:  if (_ilist_map && _gpu_nbor==0) {
lib/gpu/lal_neighbor.cpp:    _gpu_bytes=dev_nbor.row_bytes();
lib/gpu/lal_neighbor.cpp:      _gpu_bytes+=nbor_host.row_bytes();
lib/gpu/lal_neighbor.cpp:      _gpu_bytes+=dev_packed.row_bytes();
lib/gpu/lal_neighbor.cpp:    if (_gpu_nbor==2) {
lib/gpu/lal_neighbor.cpp:  if (_gpu_nbor==2) {
lib/gpu/lal_neighbor.cpp:  // If binning on GPU, do this now
lib/gpu/lal_neighbor.cpp:  if (_gpu_nbor==1) {
lib/gpu/lal_neighbor.cpp:  if (_gpu_nbor!=2 || inum<nt) {
lib/gpu/lal_neighbor.cpp:  if (_gpu_nbor!=2) {
lib/gpu/lal_sw.cu:                             const int t_per_atom, const int gpu_nbor) {
lib/gpu/lal_sw.cu:      if (gpu_nbor) nbor_k=j+nbor_pitch;
lib/gpu/lal_sw.cu:                             const int t_per_atom, const int gpu_nbor) {
lib/gpu/lal_sw.cu:      if (gpu_nbor) nbor_k=j+nbor_pitch;
lib/gpu/lal_lj_spica_long.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_lj_spica_long.h:    * - -1 if fix gpu not found
lib/gpu/lal_lj_spica_long.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_lj_spica_long.h:           const double gpu_split, FILE *screen, double **host_cut_ljsq,
lib/gpu/lal_charmm.cpp:#if defined(USE_OPENCL)
lib/gpu/lal_charmm.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_charmm.cpp:                   const double cell_size, const double gpu_split,
lib/gpu/lal_charmm.cpp:                            gpu_split,_screen,charmm,"k_charmm");
lib/gpu/lal_lj_coul.cpp:#if defined(USE_OPENCL)
lib/gpu/lal_lj_coul.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_lj_coul.cpp:                          const double gpu_split, FILE *_screen,
lib/gpu/lal_lj_coul.cpp:  success=this->init_atomic(nlocal,nall,max_nbors,maxspecial,cell_size,gpu_split,
lib/gpu/Makefile.serial:#  Generic Linux Makefile for CUDA without MPI libraries
lib/gpu/Makefile.serial:#     - Change CUDA_ARCH for your GPU
lib/gpu/Makefile.serial:ifeq ($(CUDA_HOME),)
lib/gpu/Makefile.serial:CUDA_HOME = /usr/local/cuda
lib/gpu/Makefile.serial:#CUDA_ARCH = -arch=sm_13
lib/gpu/Makefile.serial:#CUDA_ARCH = -arch=sm_10 -DCUDA_PRE_THREE
lib/gpu/Makefile.serial:#CUDA_ARCH = -arch=sm_20
lib/gpu/Makefile.serial:#CUDA_ARCH = -arch=sm_21
lib/gpu/Makefile.serial:#CUDA_ARCH = -arch=sm_30
lib/gpu/Makefile.serial:#CUDA_ARCH = -arch=sm_32
lib/gpu/Makefile.serial:#CUDA_ARCH = -arch=sm_35
lib/gpu/Makefile.serial:#CUDA_ARCH = -arch=sm_37
lib/gpu/Makefile.serial:#CUDA_ARCH = -arch=sm_50
lib/gpu/Makefile.serial:#CUDA_ARCH = -arch=sm_52
lib/gpu/Makefile.serial:CUDA_ARCH = -arch=sm_60
lib/gpu/Makefile.serial:#CUDA_ARCH = -arch=sm_61
lib/gpu/Makefile.serial:#CUDA_ARCH = -arch=sm_70
lib/gpu/Makefile.serial:#CUDA_ARCH = -arch=sm_75
lib/gpu/Makefile.serial:#CUDA_ARCH = -arch=sm_80
lib/gpu/Makefile.serial:#CUDA_ARCH = -arch=sm_86
lib/gpu/Makefile.serial:# precision for GPU calculations
lib/gpu/Makefile.serial:CUDA_PRECISION = -D_SINGLE_DOUBLE
lib/gpu/Makefile.serial:CUDA_INCLUDE = -I$(CUDA_HOME)/include
lib/gpu/Makefile.serial:CUDA_LIB = -L$(CUDA_HOME)/lib64 -L$(CUDA_HOME)/lib64/stubs -L../../src/STUBS -lmpi_stubs
lib/gpu/Makefile.serial:CUDA_OPTS = -DUNIX -O3 --use_fast_math $(LMP_INC) -Xcompiler -fPIC
lib/gpu/Makefile.serial:# GPU binning not recommended for most modern GPUs
lib/gpu/Makefile.serial:include Nvidia.makefile
lib/gpu/lal_lj_coul_debye_ext.cpp:int ljcd_gpu_init(const int ntypes, double **cutsq, double **host_lj1,
lib/gpu/lal_lj_coul_debye_ext.cpp:                  const double cell_size, int &gpu_mode, FILE *screen,
lib/gpu/lal_lj_coul_debye_ext.cpp:  gpu_mode=LJCDMF.device->gpu_mode();
lib/gpu/lal_lj_coul_debye_ext.cpp:  double gpu_split=LJCDMF.device->particle_split();
lib/gpu/lal_lj_coul_debye_ext.cpp:  int first_gpu=LJCDMF.device->first_device();
lib/gpu/lal_lj_coul_debye_ext.cpp:  int last_gpu=LJCDMF.device->last_device();
lib/gpu/lal_lj_coul_debye_ext.cpp:  int gpu_rank=LJCDMF.device->gpu_rank();
lib/gpu/lal_lj_coul_debye_ext.cpp:  int procs_per_gpu=LJCDMF.device->procs_per_gpu();
lib/gpu/lal_lj_coul_debye_ext.cpp:  LJCDMF.device->init_message(screen,"lj/cut/coul/debye",first_gpu,last_gpu);
lib/gpu/lal_lj_coul_debye_ext.cpp:                        maxspecial, cell_size, gpu_split, screen, host_cut_ljsq,
lib/gpu/lal_lj_coul_debye_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_lj_coul_debye_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_lj_coul_debye_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_lj_coul_debye_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_lj_coul_debye_ext.cpp:                last_gpu,i);
lib/gpu/lal_lj_coul_debye_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_lj_coul_debye_ext.cpp:                          cell_size, gpu_split, screen, host_cut_ljsq,
lib/gpu/lal_lj_coul_debye_ext.cpp:    LJCDMF.estimate_gpu_overhead();
lib/gpu/lal_lj_coul_debye_ext.cpp:void ljcd_gpu_clear() {
lib/gpu/lal_lj_coul_debye_ext.cpp:int** ljcd_gpu_compute_n(const int ago, const int inum_full,
lib/gpu/lal_lj_coul_debye_ext.cpp:void ljcd_gpu_compute(const int ago, const int inum_full, const int nall,
lib/gpu/lal_lj_coul_debye_ext.cpp:double ljcd_gpu_bytes() {
lib/gpu/lal_buck_coul.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_buck_coul.h:    * - -1 if fix gpu not found
lib/gpu/lal_buck_coul.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_buck_coul.h:           const double gpu_split, FILE *screen, double **host_cut_ljsq,
lib/gpu/lal_hippo.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_hippo.h:    * - -1 if fix gpu not found
lib/gpu/lal_hippo.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_hippo.h:           const double gpu_split, FILE *_screen,
lib/gpu/Makefile.cuda:#  Generic Linux Makefile for CUDA
lib/gpu/Makefile.cuda:#     - change CUDA_ARCH for your GPU
lib/gpu/Makefile.cuda:ifeq ($(CUDA_HOME),)
lib/gpu/Makefile.cuda:CUDA_HOME = /usr/local/cuda
lib/gpu/Makefile.cuda:# precision for GPU calculations
lib/gpu/Makefile.cuda:CUDA_PRECISION = -D_SINGLE_DOUBLE
lib/gpu/Makefile.cuda:# GPU binning not recommended for modern GPUs
lib/gpu/Makefile.cuda:CUDA_MPS  =
lib/gpu/Makefile.cuda:CUDA_ARCH = -gencode arch=compute_50,code=[sm_50,compute_50] -gencode arch=compute_52,code=[sm_52,compute_52] \
lib/gpu/Makefile.cuda:CUDA_INCLUDE = -I$(CUDA_HOME)/include
lib/gpu/Makefile.cuda:CUDA_LIB = -L$(CUDA_HOME)/lib64 -L$(CUDA_HOME)/lib64/stubs
lib/gpu/Makefile.cuda:CUDA_OPTS = -DUNIX -O3 --use_fast_math $(LMP_INC) -Xcompiler -fPIC
lib/gpu/Makefile.cuda:CUDA_LINK = $(CUDA_LIB) -lcudart
lib/gpu/Makefile.cuda:CUDA  = $(NVCC) $(CUDA_INCLUDE) $(CUDA_OPTS) $(CUDA_ARCH) \
lib/gpu/Makefile.cuda:             $(CUDA_PRECISION)
lib/gpu/Makefile.cuda:BIN2C = $(CUDA_HOME)/bin/bin2c
lib/gpu/Makefile.cuda:CUDR  = $(CUDR_CPP) $(CUDR_OPTS) $(CUDA_MPS) $(CUDA_PRECISION) $(CUDA_INCLUDE) \
lib/gpu/Makefile.cuda:GPU_LIB = $(LIB_DIR)/libgpu.a
lib/gpu/Makefile.cuda:all: $(OBJ_DIR) $(CUHS) $(GPU_LIB) $(EXECS)
lib/gpu/Makefile.cuda:	$(CUDA) --fatbin -DNV_KERNEL -Dgrdtyp=float -Dgrdtyp4=float4 -o $@ lal_pppm.cu
lib/gpu/Makefile.cuda:	$(CUDA) --fatbin -DNV_KERNEL -Dgrdtyp=double -Dgrdtyp4=double4 -o $@ lal_pppm.cu
lib/gpu/Makefile.cuda:	$(CUDA) --fatbin -DNV_KERNEL -o $(OBJ_DIR)/$*.cubin $(OBJ_DIR)/lal_$*.cu
lib/gpu/Makefile.cuda:	$(CUDA) -o $@ -c cudpp_mini/radixsort_app.cu
lib/gpu/Makefile.cuda:	$(CUDA) -o $@ -c cudpp_mini/scan_app.cu
lib/gpu/Makefile.cuda:# build libgpu.a
lib/gpu/Makefile.cuda:$(GPU_LIB): $(OBJS) $(CUDPP)
lib/gpu/Makefile.cuda:	$(AR) -crusv $(GPU_LIB) $(OBJS) $(CUDPP)
lib/gpu/Makefile.cuda:	$(CUDR) -o $@ ./geryon/ucl_get_devices.cpp -DUCL_CUDADR $(CUDA_LIB) -lcuda
lib/gpu/Makefile.cuda:	-rm -f $(EXECS) $(GPU_LIB) $(OBJS) $(CUDPP) $(CUHS) *.linkinfo
lib/gpu/Makefile.cuda:	-rm -f $(EXECS) $(GPU_LIB) $(OBJS) $(CUHS) *.linkinfo
lib/gpu/lal_device.cu:  #ifdef __CUDA_ARCH__
lib/gpu/lal_device.cu:  info[0]=__CUDA_ARCH__;
lib/gpu/lal_base_three.cpp:                           const double cell_size, const double gpu_split,
lib/gpu/lal_base_three.cpp:  int gpu_nbor=0;
lib/gpu/lal_base_three.cpp:  if (device->gpu_mode()==Device<numtyp,acctyp>::GPU_NEIGH)
lib/gpu/lal_base_three.cpp:    gpu_nbor=1;
lib/gpu/lal_base_three.cpp:  else if (device->gpu_mode()==Device<numtyp,acctyp>::GPU_HYB_NEIGH)
lib/gpu/lal_base_three.cpp:    gpu_nbor=2;
lib/gpu/lal_base_three.cpp:  _gpu_nbor=gpu_nbor;
lib/gpu/lal_base_three.cpp:  int _gpu_host=0;
lib/gpu/lal_base_three.cpp:  int host_nlocal=hd_balancer.first_host_count(nlocal,gpu_split,gpu_nbor);
lib/gpu/lal_base_three.cpp:    _gpu_host=1;
lib/gpu/lal_base_three.cpp:  if (ucl_device!=device->gpu) _compiled=false;
lib/gpu/lal_base_three.cpp:  ucl_device=device->gpu;
lib/gpu/lal_base_three.cpp:  if (!ans2->init(ans->max_inum(),false,false,*(device->gpu)))
lib/gpu/lal_base_three.cpp:                              _gpu_host,max_nbors,cell_size,true,1,true);
lib/gpu/lal_base_three.cpp:  hd_balancer.init(device,gpu_nbor,gpu_split);
lib/gpu/lal_base_three.cpp:  // Initialize timers for the selected GPU
lib/gpu/lal_base_three.cpp:  _max_an_bytes=ans->gpu_bytes()+nbor->gpu_bytes();
lib/gpu/lal_base_three.cpp:  _max_an_bytes+=ans2->gpu_bytes();
lib/gpu/lal_base_three.cpp:void BaseThreeT::estimate_gpu_overhead(const int add_kernels) {
lib/gpu/lal_base_three.cpp:  device->estimate_gpu_overhead(4+add_kernels,_gpu_overhead,_driver_overhead);
lib/gpu/lal_base_three.cpp:  _gpu_overhead*=hd_balancer.timestep();
lib/gpu/lal_base_three.cpp:                       _gpu_overhead,_driver_overhead,_threads_per_atom,screen);
lib/gpu/lal_base_three.cpp:  double bytes=ans->gpu_bytes()+nbor->gpu_bytes();
lib/gpu/lal_base_three.cpp:  bytes+=ans2->gpu_bytes();
lib/gpu/lal_base_three.cpp:  double bytes=ans->gpu_bytes()+nbor->gpu_bytes();
lib/gpu/lal_base_three.cpp:  bytes+=ans2->gpu_bytes();
lib/gpu/lal_base_three.cpp:// Reneighbor on GPU if necessary and then compute forces, virials, energies
lib/gpu/lal_base_three.cpp:  int inum=hd_balancer.get_gpu_count(ago,inum_full);
lib/gpu/lal_base_three.cpp:  // Build neighbor list on GPU if necessary
lib/gpu/lal_base_three.cpp:  #if defined(USE_OPENCL) && (defined(CL_VERSION_2_1) || defined(CL_VERSION_3_0))
lib/gpu/lal_dpd_coul_slater_long_ext.cpp:int dpd_coul_slater_long_gpu_init(const int ntypes, double **host_cutsq, double **host_a0,
lib/gpu/lal_dpd_coul_slater_long_ext.cpp:                                  const double cell_size, int &gpu_mode, FILE *screen,
lib/gpu/lal_dpd_coul_slater_long_ext.cpp:  gpu_mode=DPDCMF.device->gpu_mode();
lib/gpu/lal_dpd_coul_slater_long_ext.cpp:  double gpu_split=DPDCMF.device->particle_split();
lib/gpu/lal_dpd_coul_slater_long_ext.cpp:  int first_gpu=DPDCMF.device->first_device();
lib/gpu/lal_dpd_coul_slater_long_ext.cpp:  int last_gpu=DPDCMF.device->last_device();
lib/gpu/lal_dpd_coul_slater_long_ext.cpp:  int gpu_rank=DPDCMF.device->gpu_rank();
lib/gpu/lal_dpd_coul_slater_long_ext.cpp:  int procs_per_gpu=DPDCMF.device->procs_per_gpu();
lib/gpu/lal_dpd_coul_slater_long_ext.cpp:  DPDCMF.device->init_message(screen,"dpd",first_gpu,last_gpu);
lib/gpu/lal_dpd_coul_slater_long_ext.cpp:                        max_nbors, maxspecial, cell_size, gpu_split, screen, host_special_coul,
lib/gpu/lal_dpd_coul_slater_long_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_dpd_coul_slater_long_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_dpd_coul_slater_long_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_dpd_coul_slater_long_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_dpd_coul_slater_long_ext.cpp:                last_gpu,i);
lib/gpu/lal_dpd_coul_slater_long_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_dpd_coul_slater_long_ext.cpp:                          max_nbors, maxspecial, cell_size, gpu_split, screen, host_special_coul,
lib/gpu/lal_dpd_coul_slater_long_ext.cpp:    DPDCMF.estimate_gpu_overhead();
lib/gpu/lal_dpd_coul_slater_long_ext.cpp:void dpd_coul_slater_long_gpu_clear() {
lib/gpu/lal_dpd_coul_slater_long_ext.cpp:int ** dpd_coul_slater_long_gpu_compute_n(const int ago, const int inum_full, const int nall,
lib/gpu/lal_dpd_coul_slater_long_ext.cpp:void dpd_coul_slater_long_gpu_compute(const int ago, const int inum_full, const int nall,
lib/gpu/lal_dpd_coul_slater_long_ext.cpp:void dpd_coul_slater_long_gpu_update_coeff(int ntypes, double **host_a0, double **host_gamma,
lib/gpu/lal_dpd_coul_slater_long_ext.cpp:void dpd_coul_slater_long_gpu_get_extra_data(double *host_q) {
lib/gpu/lal_dpd_coul_slater_long_ext.cpp:double dpd_coul_slater_long_gpu_bytes() {
lib/gpu/lal_zbl_ext.cpp:int zbl_gpu_init(const int ntypes, double **cutsq, double **host_sw1,
lib/gpu/lal_zbl_ext.cpp:                 const int maxspecial, const double cell_size, int &gpu_mode, FILE *screen) {
lib/gpu/lal_zbl_ext.cpp:  gpu_mode=ZBLMF.device->gpu_mode();
lib/gpu/lal_zbl_ext.cpp:  double gpu_split=ZBLMF.device->particle_split();
lib/gpu/lal_zbl_ext.cpp:  int first_gpu=ZBLMF.device->first_device();
lib/gpu/lal_zbl_ext.cpp:  int last_gpu=ZBLMF.device->last_device();
lib/gpu/lal_zbl_ext.cpp:  int gpu_rank=ZBLMF.device->gpu_rank();
lib/gpu/lal_zbl_ext.cpp:  int procs_per_gpu=ZBLMF.device->procs_per_gpu();
lib/gpu/lal_zbl_ext.cpp:  ZBLMF.device->init_message(screen,"zbl",first_gpu,last_gpu);
lib/gpu/lal_zbl_ext.cpp:                       inum, nall, max_nbors, maxspecial, cell_size, gpu_split, screen);
lib/gpu/lal_zbl_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_zbl_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_zbl_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_zbl_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_zbl_ext.cpp:                last_gpu,i);
lib/gpu/lal_zbl_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_zbl_ext.cpp:                         inum, nall, max_nbors, maxspecial, cell_size, gpu_split, screen);
lib/gpu/lal_zbl_ext.cpp:    ZBLMF.estimate_gpu_overhead();
lib/gpu/lal_zbl_ext.cpp:void zbl_gpu_clear() {
lib/gpu/lal_zbl_ext.cpp:int ** zbl_gpu_compute_n(const int ago, const int inum_full,
lib/gpu/lal_zbl_ext.cpp:void zbl_gpu_compute(const int ago, const int inum_full, const int nall,
lib/gpu/lal_zbl_ext.cpp:double zbl_gpu_bytes() {
lib/gpu/lal_coul_dsf.cpp:#if defined(USE_OPENCL)
lib/gpu/lal_coul_dsf.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_coul_dsf.cpp:                   const double cell_size, const double gpu_split, FILE *_screen,
lib/gpu/lal_coul_dsf.cpp:  success=this->init_atomic(nlocal,nall,max_nbors,maxspecial,cell_size,gpu_split,
lib/gpu/lal_lj_expand_coul_long_ext.cpp:int ljecl_gpu_init(const int ntypes, double **cutsq, double **host_lj1,
lib/gpu/lal_lj_expand_coul_long_ext.cpp:                  const double cell_size, int &gpu_mode, FILE *screen,
lib/gpu/lal_lj_expand_coul_long_ext.cpp:  gpu_mode=LJECLMF.device->gpu_mode();
lib/gpu/lal_lj_expand_coul_long_ext.cpp:  double gpu_split=LJECLMF.device->particle_split();
lib/gpu/lal_lj_expand_coul_long_ext.cpp:  int first_gpu=LJECLMF.device->first_device();
lib/gpu/lal_lj_expand_coul_long_ext.cpp:  int last_gpu=LJECLMF.device->last_device();
lib/gpu/lal_lj_expand_coul_long_ext.cpp:  int gpu_rank=LJECLMF.device->gpu_rank();
lib/gpu/lal_lj_expand_coul_long_ext.cpp:  int procs_per_gpu=LJECLMF.device->procs_per_gpu();
lib/gpu/lal_lj_expand_coul_long_ext.cpp:  LJECLMF.device->init_message(screen,"lj/expand/coul/long",first_gpu,last_gpu);
lib/gpu/lal_lj_expand_coul_long_ext.cpp:                        cell_size, gpu_split, screen, host_cut_ljsq,
lib/gpu/lal_lj_expand_coul_long_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_lj_expand_coul_long_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_lj_expand_coul_long_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_lj_expand_coul_long_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_lj_expand_coul_long_ext.cpp:                last_gpu,i);
lib/gpu/lal_lj_expand_coul_long_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_lj_expand_coul_long_ext.cpp:                          cell_size, gpu_split, screen, host_cut_ljsq,
lib/gpu/lal_lj_expand_coul_long_ext.cpp:    LJECLMF.estimate_gpu_overhead();
lib/gpu/lal_lj_expand_coul_long_ext.cpp:void ljecl_gpu_reinit(const int ntypes, double **cutsq, double **host_lj1,
lib/gpu/lal_lj_expand_coul_long_ext.cpp:  int gpu_rank=LJECLMF.device->gpu_rank();
lib/gpu/lal_lj_expand_coul_long_ext.cpp:  int procs_per_gpu=LJECLMF.device->procs_per_gpu();
lib/gpu/lal_lj_expand_coul_long_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_lj_expand_coul_long_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_lj_expand_coul_long_ext.cpp:void ljecl_gpu_clear() {
lib/gpu/lal_lj_expand_coul_long_ext.cpp:int** ljecl_gpu_compute_n(const int ago, const int inum_full,
lib/gpu/lal_lj_expand_coul_long_ext.cpp:void ljecl_gpu_compute(const int ago, const int inum_full, const int nall,
lib/gpu/lal_lj_expand_coul_long_ext.cpp:double ljecl_gpu_bytes() {
lib/gpu/lal_buck.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_buck.h:    * - -1 if fix gpu not found
lib/gpu/lal_buck.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_buck.h:           const double gpu_split, FILE *screen);
lib/gpu/lal_yukawa.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_yukawa.h:    * - -1 if fix gpu not found
lib/gpu/lal_yukawa.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_yukawa.h:           const double gpu_split, FILE *screen);
lib/gpu/lal_coul.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_coul.h:    * - -1 if fix gpu not found
lib/gpu/lal_coul.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_coul.h:           const double gpu_split, FILE *screen, const double qqrd2e);
lib/gpu/lal_neighbor_gpu.cu://                               neighbor_gpu.cu
lib/gpu/lal_neighbor_gpu.cu://                              Peng Wang (Nvidia)
lib/gpu/lal_neighbor_gpu.cu://  Device code for handling GPU generated neighbor lists
lib/gpu/lal_neighbor_gpu.cu://    email                : penwang@nvidia.com, brownw@ornl.gov
lib/gpu/lal_neighbor_gpu.cu:#ifdef USE_OPENCL
lib/gpu/lal_neighbor_gpu.cu:#if (__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 2)
lib/gpu/lal_neighbor_gpu.cu:// Issue with incorrect results in CUDA >= 11.2
lib/gpu/lal_tersoff.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_tersoff.h:    * - -1 if fix gpu not found
lib/gpu/lal_tersoff.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_tersoff.h:           const double cell_size, const double gpu_split, FILE *screen,
lib/gpu/Makefile.mpi:#  Generic Linux Makefile for CUDA
lib/gpu/Makefile.mpi:#     - Change CUDA_ARCH for your GPU
lib/gpu/Makefile.mpi:ifeq ($(CUDA_HOME),)
lib/gpu/Makefile.mpi:CUDA_HOME = /usr/local/cuda
lib/gpu/Makefile.mpi:#CUDA_ARCH = -arch=sm_13
lib/gpu/Makefile.mpi:#CUDA_ARCH = -arch=sm_10 -DCUDA_PRE_THREE
lib/gpu/Makefile.mpi:#CUDA_ARCH = -arch=sm_20
lib/gpu/Makefile.mpi:#CUDA_ARCH = -arch=sm_21
lib/gpu/Makefile.mpi:#CUDA_ARCH = -arch=sm_30
lib/gpu/Makefile.mpi:#CUDA_ARCH = -arch=sm_32
lib/gpu/Makefile.mpi:#CUDA_ARCH = -arch=sm_35
lib/gpu/Makefile.mpi:#CUDA_ARCH = -arch=sm_37
lib/gpu/Makefile.mpi:#CUDA_ARCH = -arch=sm_50
lib/gpu/Makefile.mpi:#CUDA_ARCH = -arch=sm_52
lib/gpu/Makefile.mpi:CUDA_ARCH = -arch=sm_60
lib/gpu/Makefile.mpi:#CUDA_ARCH = -arch=sm_61
lib/gpu/Makefile.mpi:#CUDA_ARCH = -arch=sm_70
lib/gpu/Makefile.mpi:#CUDA_ARCH = -arch=sm_75
lib/gpu/Makefile.mpi:#CUDA_ARCH = -arch=sm_80
lib/gpu/Makefile.mpi:#CUDA_ARCH = -arch=sm_86
lib/gpu/Makefile.mpi:#CUDA_ARCH = -arch=sm_89
lib/gpu/Makefile.mpi:#CUDA_ARCH = -arch=sm_90
lib/gpu/Makefile.mpi:# precision for GPU calculations
lib/gpu/Makefile.mpi:CUDA_PRECISION = -D_SINGLE_DOUBLE
lib/gpu/Makefile.mpi:CUDA_INCLUDE = -I$(CUDA_HOME)/include
lib/gpu/Makefile.mpi:CUDA_LIB = -L$(CUDA_HOME)/lib64 -L$(CUDA_HOME)/lib64/stubs
lib/gpu/Makefile.mpi:CUDA_OPTS = -DUNIX -O3 --use_fast_math $(LMP_INC) -Xcompiler -fPIC
lib/gpu/Makefile.mpi:# GPU binning not recommended for most modern GPUs
lib/gpu/Makefile.mpi:include Nvidia.makefile
lib/gpu/lal_beck.cpp:#ifdef USE_OPENCL
lib/gpu/lal_beck.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_beck.cpp:                const double gpu_split, FILE *_screen) {
lib/gpu/lal_beck.cpp:  success=this->init_atomic(nlocal,nall,max_nbors,maxspecial,cell_size,gpu_split,
lib/gpu/lal_lj_coul_debye.cpp:#ifdef USE_OPENCL
lib/gpu/lal_lj_coul_debye.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_lj_coul_debye.cpp:                       const double gpu_split, FILE *_screen,
lib/gpu/lal_lj_coul_debye.cpp:  success=this->init_atomic(nlocal,nall,max_nbors,maxspecial,cell_size,gpu_split,
lib/gpu/lal_born_ext.cpp:int born_gpu_init(const int ntypes, double **cutsq, double **host_rhoinv,
lib/gpu/lal_born_ext.cpp:                  const double cell_size, int &gpu_mode, FILE *screen) {
lib/gpu/lal_born_ext.cpp:  gpu_mode=BORNMF.device->gpu_mode();
lib/gpu/lal_born_ext.cpp:  double gpu_split=BORNMF.device->particle_split();
lib/gpu/lal_born_ext.cpp:  int first_gpu=BORNMF.device->first_device();
lib/gpu/lal_born_ext.cpp:  int last_gpu=BORNMF.device->last_device();
lib/gpu/lal_born_ext.cpp:  int gpu_rank=BORNMF.device->gpu_rank();
lib/gpu/lal_born_ext.cpp:  int procs_per_gpu=BORNMF.device->procs_per_gpu();
lib/gpu/lal_born_ext.cpp:  BORNMF.device->init_message(screen,"born",first_gpu,last_gpu);
lib/gpu/lal_born_ext.cpp:                        maxspecial, cell_size, gpu_split, screen);
lib/gpu/lal_born_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_born_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_born_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_born_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_born_ext.cpp:                last_gpu,i);
lib/gpu/lal_born_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_born_ext.cpp:                          maxspecial, cell_size, gpu_split, screen);
lib/gpu/lal_born_ext.cpp:    BORNMF.estimate_gpu_overhead();
lib/gpu/lal_born_ext.cpp:void born_gpu_reinit(const int ntypes, double **host_rhoinv,
lib/gpu/lal_born_ext.cpp:  int gpu_rank=BORNMF.device->gpu_rank();
lib/gpu/lal_born_ext.cpp:  int procs_per_gpu=BORNMF.device->procs_per_gpu();
lib/gpu/lal_born_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_born_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_born_ext.cpp:void born_gpu_clear() {
lib/gpu/lal_born_ext.cpp:int ** born_gpu_compute_n(const int ago, const int inum_full,
lib/gpu/lal_born_ext.cpp:void born_gpu_compute(const int ago, const int inum_full, const int nall,
lib/gpu/lal_born_ext.cpp:double born_gpu_bytes() {
lib/gpu/lal_yukawa.cpp:#if defined(USE_OPENCL)
lib/gpu/lal_yukawa.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_yukawa.cpp:                  const double gpu_split, FILE *_screen) {
lib/gpu/lal_yukawa.cpp:  success=this->init_atomic(nlocal,nall,max_nbors,maxspecial,cell_size,gpu_split,
lib/gpu/lal_tersoff_mod.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_tersoff_mod.h:    * - -1 if fix gpu not found
lib/gpu/lal_tersoff_mod.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_tersoff_mod.h:           const double cell_size, const double gpu_split, FILE *screen,
lib/gpu/lal_answer.h:#if defined(USE_OPENCL)
lib/gpu/lal_answer.h:using namespace ucl_opencl;
lib/gpu/lal_answer.h:#elif defined(USE_CUDART)
lib/gpu/lal_answer.h:using namespace ucl_cudart;
lib/gpu/lal_answer.h:using namespace ucl_cudadr;
lib/gpu/lal_answer.h:      _gpu_bytes=engv.device.row_bytes()+force.device.row_bytes();
lib/gpu/lal_answer.h:  inline double gpu_bytes() { return _gpu_bytes; }
lib/gpu/lal_answer.h:  // -------------------------COPY FROM GPU -------------------------------
lib/gpu/lal_answer.h:  /// Add forces and torques from the GPU into a LAMMPS pointer
lib/gpu/lal_answer.h:  /// Return the time the CPU was idle waiting for GPU
lib/gpu/lal_answer.h:  double _gpu_bytes;
lib/gpu/lal_born_coul_wolf_cs.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_born_coul_wolf_cs.h:    * - -1 if fix gpu not found
lib/gpu/lal_born_coul_wolf_cs.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_born_coul_wolf_cs.h:           const double gpu_split, FILE *screen, double **host_cut_ljsq,
lib/gpu/lal_answer.cpp:  _gpu_bytes=engv.device.row_bytes()+force.device.row_bytes();
lib/gpu/lal_answer.cpp:  _gpu_bytes=0;
lib/gpu/lal_buck_coul_long.cpp:#if defined(USE_OPENCL)
lib/gpu/lal_buck_coul_long.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_buck_coul_long.cpp:                       const double gpu_split, FILE *_screen,
lib/gpu/lal_buck_coul_long.cpp:  success=this->init_atomic(nlocal,nall,max_nbors,maxspecial,cell_size,gpu_split,
lib/gpu/lal_gayberne.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_gayberne.h:    * - -1 if fix gpu not found
lib/gpu/lal_gayberne.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_gayberne.h:           const double gpu_split, FILE *screen);
lib/gpu/lal_lj_tip4p_long_ext.cpp:int ljtip4p_long_gpu_init(const int ntypes, double **cutsq, double **host_lj1,
lib/gpu/lal_lj_tip4p_long_ext.cpp:    const double cell_size, int &gpu_mode, FILE *screen,
lib/gpu/lal_lj_tip4p_long_ext.cpp:  gpu_mode=LJTIP4PLMF.device->gpu_mode();
lib/gpu/lal_lj_tip4p_long_ext.cpp:  double gpu_split=LJTIP4PLMF.device->particle_split();
lib/gpu/lal_lj_tip4p_long_ext.cpp:  int first_gpu=LJTIP4PLMF.device->first_device();
lib/gpu/lal_lj_tip4p_long_ext.cpp:  int last_gpu=LJTIP4PLMF.device->last_device();
lib/gpu/lal_lj_tip4p_long_ext.cpp:  int gpu_rank=LJTIP4PLMF.device->gpu_rank();
lib/gpu/lal_lj_tip4p_long_ext.cpp:  int procs_per_gpu=LJTIP4PLMF.device->procs_per_gpu();
lib/gpu/lal_lj_tip4p_long_ext.cpp:  LJTIP4PLMF.device->init_message(screen,"lj/cut/tip4p/long/gpu",first_gpu,last_gpu);
lib/gpu/lal_lj_tip4p_long_ext.cpp:        maxspecial, cell_size, gpu_split, screen,
lib/gpu/lal_lj_tip4p_long_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_lj_tip4p_long_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_lj_tip4p_long_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_lj_tip4p_long_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_lj_tip4p_long_ext.cpp:                last_gpu,i);
lib/gpu/lal_lj_tip4p_long_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_lj_tip4p_long_ext.cpp:          cell_size, gpu_split, screen, host_cut_ljsq,
lib/gpu/lal_lj_tip4p_long_ext.cpp:    LJTIP4PLMF.estimate_gpu_overhead(2);
lib/gpu/lal_lj_tip4p_long_ext.cpp:void ljtip4p_long_gpu_clear() {
lib/gpu/lal_lj_tip4p_long_ext.cpp:int ** ljtip4p_long_gpu_compute_n(const int ago, const int inum_full,
lib/gpu/lal_lj_tip4p_long_ext.cpp:void ljtip4p_long_gpu_compute(const int ago, const int inum_full, const int nall,
lib/gpu/lal_lj_tip4p_long_ext.cpp:double ljtip4p_long_gpu_bytes() {
lib/gpu/lal_pppm_ext.cpp:static PPPM<PRECISION,ACC_PRECISION,float,_lgpu_float4> PPPMF;
lib/gpu/lal_pppm_ext.cpp:static PPPM<PRECISION,ACC_PRECISION,double,_lgpu_double4> PPPMD;
lib/gpu/lal_pppm_ext.cpp:grdtyp * pppm_gpu_init(memtyp &pppm, const int nlocal, const int nall,
lib/gpu/lal_pppm_ext.cpp:  int first_gpu=pppm.device->first_device();
lib/gpu/lal_pppm_ext.cpp:  int last_gpu=pppm.device->last_device();
lib/gpu/lal_pppm_ext.cpp:  int gpu_rank=pppm.device->gpu_rank();
lib/gpu/lal_pppm_ext.cpp:  int procs_per_gpu=pppm.device->procs_per_gpu();
lib/gpu/lal_pppm_ext.cpp:  pppm.device->init_message(screen,"pppm",first_gpu,last_gpu);
lib/gpu/lal_pppm_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_pppm_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_pppm_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_pppm_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_pppm_ext.cpp:                last_gpu,i);
lib/gpu/lal_pppm_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_pppm_ext.cpp:float * pppm_gpu_init_f(const int nlocal, const int nall, FILE *screen,
lib/gpu/lal_pppm_ext.cpp:  float *b=pppm_gpu_init(PPPMF,nlocal,nall,screen,order,nxlo_out,nylo_out,
lib/gpu/lal_pppm_ext.cpp:void pppm_gpu_clear_f(const double cpu_time) {
lib/gpu/lal_pppm_ext.cpp:int pppm_gpu_spread_f(const int ago, const int nlocal, const int nall,
lib/gpu/lal_pppm_ext.cpp:void pppm_gpu_interp_f(const float qqrd2e_scale) {
lib/gpu/lal_pppm_ext.cpp:double pppm_gpu_bytes_f() {
lib/gpu/lal_pppm_ext.cpp:void pppm_gpu_forces_f(double **f) {
lib/gpu/lal_pppm_ext.cpp:double * pppm_gpu_init_d(const int nlocal, const int nall, FILE *screen,
lib/gpu/lal_pppm_ext.cpp:  double *b=pppm_gpu_init(PPPMD,nlocal,nall,screen,order,nxlo_out,nylo_out,
lib/gpu/lal_pppm_ext.cpp:void pppm_gpu_clear_d(const double cpu_time) {
lib/gpu/lal_pppm_ext.cpp:int pppm_gpu_spread_d(const int ago, const int nlocal, const int nall,
lib/gpu/lal_pppm_ext.cpp:void pppm_gpu_interp_d(const double qqrd2e_scale) {
lib/gpu/lal_pppm_ext.cpp:double pppm_gpu_bytes_d() {
lib/gpu/lal_pppm_ext.cpp:void pppm_gpu_forces_d(double **f) {
lib/gpu/lal_dpd.cpp:#if defined(USE_OPENCL)
lib/gpu/lal_dpd.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_dpd.cpp:               const double gpu_split, FILE *_screen) {
lib/gpu/lal_dpd.cpp:  #ifdef USE_OPENCL
lib/gpu/lal_dpd.cpp:                            gpu_split,_screen,dpd,"k_dpd",onetype);
lib/gpu/Makefile.lammps.mac_ocl:gpu_SYSINC = -DFFT_SINGLE
lib/gpu/Makefile.lammps.mac_ocl:gpu_SYSLIB = -framework OpenCL
lib/gpu/Makefile.lammps.mac_ocl:gpu_SYSPATH =
lib/gpu/lal_sph_taitwater_ext.cpp:int sph_taitwater_gpu_init(const int ntypes, double **cutsq, double** host_cut,
lib/gpu/lal_sph_taitwater_ext.cpp:                           const double cell_size, int &gpu_mode, FILE *screen) {
lib/gpu/lal_sph_taitwater_ext.cpp:  gpu_mode=SPHTaitwaterMF.device->gpu_mode();
lib/gpu/lal_sph_taitwater_ext.cpp:  double gpu_split=SPHTaitwaterMF.device->particle_split();
lib/gpu/lal_sph_taitwater_ext.cpp:  int first_gpu=SPHTaitwaterMF.device->first_device();
lib/gpu/lal_sph_taitwater_ext.cpp:  int last_gpu=SPHTaitwaterMF.device->last_device();
lib/gpu/lal_sph_taitwater_ext.cpp:  int gpu_rank=SPHTaitwaterMF.device->gpu_rank();
lib/gpu/lal_sph_taitwater_ext.cpp:  int procs_per_gpu=SPHTaitwaterMF.device->procs_per_gpu();
lib/gpu/lal_sph_taitwater_ext.cpp:  SPHTaitwaterMF.device->init_message(screen,"sph_taitwater",first_gpu,last_gpu);
lib/gpu/lal_sph_taitwater_ext.cpp:                                cell_size, gpu_split, screen);
lib/gpu/lal_sph_taitwater_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_sph_taitwater_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_sph_taitwater_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_sph_taitwater_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_sph_taitwater_ext.cpp:                last_gpu,i);
lib/gpu/lal_sph_taitwater_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_sph_taitwater_ext.cpp:                                  cell_size, gpu_split, screen);
lib/gpu/lal_sph_taitwater_ext.cpp:    SPHTaitwaterMF.estimate_gpu_overhead();
lib/gpu/lal_sph_taitwater_ext.cpp:void sph_taitwater_gpu_clear() {
lib/gpu/lal_sph_taitwater_ext.cpp:int ** sph_taitwater_gpu_compute_n(const int ago, const int inum_full, const int nall,
lib/gpu/lal_sph_taitwater_ext.cpp:void sph_taitwater_gpu_compute(const int ago, const int inum_full, const int nall,
lib/gpu/lal_sph_taitwater_ext.cpp:void sph_taitwater_gpu_get_extra_data(double *host_rho) {
lib/gpu/lal_sph_taitwater_ext.cpp:void sph_taitwater_gpu_update_drhoE(void **drhoE_ptr) {
lib/gpu/lal_sph_taitwater_ext.cpp:double sph_taitwater_gpu_bytes() {
lib/gpu/lal_coul_slater_long.cpp:#if defined(USE_OPENCL)
lib/gpu/lal_coul_slater_long.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_coul_slater_long.cpp:extern Device<PRECISION,ACC_PRECISION> pair_gpu_device;
lib/gpu/lal_coul_slater_long.cpp:                    const double gpu_split, FILE *_screen,
lib/gpu/lal_coul_slater_long.cpp:                            gpu_split,_screen,coul_slater_long,"k_coul_slater_long");
lib/gpu/lal_lj_cubic_ext.cpp:int ljcb_gpu_init(const int ntypes, double **cutsq, double **cut_inner_sq,
lib/gpu/lal_lj_cubic_ext.cpp:                  int &gpu_mode, FILE *screen) {
lib/gpu/lal_lj_cubic_ext.cpp:  gpu_mode=LJCubicLMF.device->gpu_mode();
lib/gpu/lal_lj_cubic_ext.cpp:  double gpu_split=LJCubicLMF.device->particle_split();
lib/gpu/lal_lj_cubic_ext.cpp:  int first_gpu=LJCubicLMF.device->first_device();
lib/gpu/lal_lj_cubic_ext.cpp:  int last_gpu=LJCubicLMF.device->last_device();
lib/gpu/lal_lj_cubic_ext.cpp:  int gpu_rank=LJCubicLMF.device->gpu_rank();
lib/gpu/lal_lj_cubic_ext.cpp:  int procs_per_gpu=LJCubicLMF.device->procs_per_gpu();
lib/gpu/lal_lj_cubic_ext.cpp:  LJCubicLMF.device->init_message(screen,"lj/cubic",first_gpu,last_gpu);
lib/gpu/lal_lj_cubic_ext.cpp:                            cell_size, gpu_split, screen);
lib/gpu/lal_lj_cubic_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_lj_cubic_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_lj_cubic_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_lj_cubic_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_lj_cubic_ext.cpp:                last_gpu,i);
lib/gpu/lal_lj_cubic_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_lj_cubic_ext.cpp:                              cell_size, gpu_split, screen);
lib/gpu/lal_lj_cubic_ext.cpp:    LJCubicLMF.estimate_gpu_overhead();
lib/gpu/lal_lj_cubic_ext.cpp:void ljcb_gpu_clear() {
lib/gpu/lal_lj_cubic_ext.cpp:int ** ljcb_gpu_compute_n(const int ago, const int inum_full,
lib/gpu/lal_lj_cubic_ext.cpp:void ljcb_gpu_compute(const int ago, const int inum_full, const int nall,
lib/gpu/lal_lj_cubic_ext.cpp:double ljcb_gpu_bytes() {
lib/gpu/README:                          Peng Wang (NVIDIA)
lib/gpu/README:                        Inderaj Bains (NVIDIA)
lib/gpu/README:when using the GPU package.
lib/gpu/README:This library must be built with a C++ compiler along with CUDA, HIP, or OpenCL
lib/gpu/README:This library, libgpu.a, provides routines for acceleration of certain
lib/gpu/README:LAMMPS styles and neighbor list builds using CUDA, OpenCL, or ROCm HIP.
lib/gpu/README:The gpu library includes binaries to check for available GPUs and their
lib/gpu/README:system and build is setup properly. Additionally, the GPU numbering for
lib/gpu/README:specific selection of devices should be taking from this output. The GPU
lib/gpu/README:After building the GPU library, for OpenCL:
lib/gpu/README:for CUDA:
lib/gpu/README:and for ROCm HIP:
lib/gpu/README:OpenCL: Mac without MPI:
lib/gpu/README:  make -f Makefile.mac_opencl -j; cd ../../src/; make mpi-stubs
lib/gpu/README:  ./lmp_g++_serial -in ../bench/in.lj -log none -sf gpu
lib/gpu/README:OpenCL: Mac with MPI:
lib/gpu/README:  make -f Makefile.mac_opencl_mpi -j; cd ../../src/; make g++_openmpi -j
lib/gpu/README:  mpirun -np $NUM_MPI ./lmp_g++_openmpi -in ../bench/in.lj -log none -sf gpu
lib/gpu/README:OpenCL: Linux with Intel oneAPI:
lib/gpu/README:  mpirun -np $NUM_MPI ./lmp_oneapi -in ../bench/in.lj -log none -sf gpu
lib/gpu/README:OpenCL: Linux with MPI:
lib/gpu/README:  make -f Makefile.linux_opencl -j; cd ../../src; make omp -j
lib/gpu/README:  mpirun -np $NUM_MPI ./lmp_omp -in ../bench/in.lj -log none -sf gpu
lib/gpu/README:NVIDIA CUDA:
lib/gpu/README:  make -f Makefile.cuda_mps -j; cd ../../src; make omp -j
lib/gpu/README:  export CUDA_MPS_LOG_DIRECTORY=/tmp; export CUDA_MPS_PIPE_DIRECTORY=/tmp
lib/gpu/README:  nvidia-smi -i 0 -c EXCLUSIVE_PROCESS
lib/gpu/README:  mpirun -np $NUM_MPI ./lmp_omp -in ../bench/in.lj -log none -sf gpu
lib/gpu/README:  echo quit | /usr/bin/nvidia-cuda-mps-control
lib/gpu/README:  mpirun -np $NUM_MPI ./lmp_omp -in ../bench/in.lj -log none -sf gpu
lib/gpu/README:                 Installing oneAPI, OpenCl, CUDA, or ROCm
lib/gpu/README:installation from Intel, NVIDIA, etc. repositories. All are available for
lib/gpu/README:along with many libraries. Alternatively, Intel OpenCL can also be installed
lib/gpu/README:NOTE: Installation of the CUDA SDK is not required, only the CUDA toolkit.
lib/gpu/README:https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html
lib/gpu/README:https://github.com/RadeonOpenCompute/ROCm
lib/gpu/README:You can type "make lib-gpu" from the src directory to see help on how
lib/gpu/README:make -f Makefile.linux_opencl
lib/gpu/README:libgpu.a                the library LAMMPS will link against
lib/gpu/README:      when building libgpu.a (i.e. by LMP_INC in the lib/gpu/Makefile.bar).
lib/gpu/README:The GPU library supports 3 precision modes: single, double, and mixed, with
lib/gpu/README:Makefiles due to the more restrictive nature of the Apple OpenCL for some
lib/gpu/README:verification), set either CUDA_PRECISION, OCL_PREC, or HIP_PRECISION to one
lib/gpu/README:Some accelerators or OpenCL implementations only support single precision.
lib/gpu/README:LAMMPS must be compiled with -DFFT_SINGLE to use PPPM with GPU acceleration
lib/gpu/README:or GPU acceleration should be disabled for PPPM (e.g. suffix off or pair/only
lib/gpu/README:                             CUDA BUILD NOTES
lib/gpu/README:compilation of the gpu library is required. Also this will build in support
lib/gpu/README:for all compute architecture that are supported by the CUDA toolkit version
lib/gpu/README:used to build the gpu library.  A similar setup is possible using
lib/gpu/README:Makefile.linux_multi after adjusting the settings for the CUDA toolkit in use.
lib/gpu/README:Only CUDA toolkit version 8.0 and later and only GPU architecture 3.0
lib/gpu/README:to use older hard- or software you have to compile for OpenCL or use an older
lib/gpu/README:If you do not want to use a fat binary, that supports multiple CUDA
lib/gpu/README:architectures, the CUDA_ARCH must be set to match the GPU architecture. This
lib/gpu/README:a detailed list of GPU architectures and CUDA compatible GPUs can be found
lib/gpu/README:e.g. here: https://en.wikipedia.org/wiki/CUDA#GPUs_supported
lib/gpu/README:The CUDA_HOME variable should be set to the location of the CUDA toolkit.
lib/gpu/README:To build, edit the CUDA_ARCH, CUDA_PRECISION, CUDA_HOME variables in one of
lib/gpu/README:the Makefiles. CUDA_ARCH should be set based on the compute capability of
lib/gpu/README:your GPU. This can be verified by running the nvc_get_devices executable after
lib/gpu/README:the build is complete. Additionally, the GPU package must be installed and
lib/gpu/README:compiled for LAMMPS. This may require editing the gpu_SYSPATH variable in the
lib/gpu/README:Please note that the GPU library accesses the CUDA driver library directly,
lib/gpu/README:so it needs to be linked with the CUDA driver library (libcuda.so) that ships
lib/gpu/README:with the Nvidia driver. If you are compiling LAMMPS on the head node of a GPU
lib/gpu/README:from one of the compute nodes (best into this directory). Recent CUDA toolkits
lib/gpu/README:starting from CUDA 9 provide a dummy libcuda.so library (typically under
lib/gpu/README:$(CUDA_HOME)/lib64/stubs), that can be used for linking.
lib/gpu/README:Best performance with the GPU library is typically with multiple MPI processes
lib/gpu/README:sharing the same GPU cards. For NVIDIA, this is most efficient with CUDA
lib/gpu/README:MPS enabled. To prevent runtime errors for GPUs configured in exclusive process
lib/gpu/README:mode with MPS, the GPU library should be build with the -DCUDA_MPS_SUPPORT flag.
lib/gpu/README:1. GPU sorting requires installing hipcub
lib/gpu/README:(https://github.com/ROCmSoftwarePlatform/hipCUB). The HIP CUDA-backend
lib/gpu/README:extract the cub directory to lammps/lib/gpu/ or specify an appropriate
lib/gpu/README:path in lammps/lib/gpu/Makefile.hip.
lib/gpu/README:export HIP_PLATFORM=amd (ROCm >= 4.1), HIP_PLATFORM=hcc (ROCm <= 4.0)
lib/gpu/README:in lammps/lib/gpu/Makefile.hip and in lammps/src/MAKE/OPTIONS/Makefile.hip.
lib/gpu/README:                             OPENCL BUILD NOTES
lib/gpu/README:NUMA nodes on GPUs or accelerators as separate devices. For example, a 2-socket
lib/gpu/README:CPU would appear as two separate devices for OpenCL (and LAMMPS would require
lib/gpu/README:two MPI processes to use both sockets with the GPU library - each with its
lib/gpu/README:own device ID as output by ocl_get_devices). OpenCL version 1.2 or later is
lib/gpu/README:CUDA_MPS_SUPPORT        Do not generate errors for exclusive mode for CUDA
lib/gpu/README:GERYON_OCL_FLUSH        For OpenCL, flush queue after every enqueue
lib/gpu/README:LAL_NO_OCL_EV_JIT       Turn off JIT specialization for kernels in OpenCL
lib/gpu/README:USE_CUDPP               Enable GPU binning in neighbor builds (not recommended)
lib/gpu/README:USE_HIP_DEVICE_SORT     Enable GPU binning for HIP builds
lib/gpu/README:LAL_OCL_EXTRA_ARGS      Supply extra args for OpenCL compiler delimited with :
lib/gpu/README:GERYON_KERNEL_DUMP      Dump all compiled OpenCL programs with compiler
lib/gpu/README:GPU_CAST                Casting performed on GPU, untested recently
lib/gpu/README:                                     GPU or CPU device). Default behavior is to
lib/gpu/README:                                     auto-detect. Impacts OpenCL only.
lib/gpu/README:                                     GPU card). Default behavior is to
lib/gpu/README:                                     auto-detect. Impacts OpenCL only.
lib/gpu/lal_born_coul_long_cs_ext.cpp:int bornclcs_gpu_init(const int ntypes, double **cutsq, double **host_rhoinv,
lib/gpu/lal_born_coul_long_cs_ext.cpp:                    const int maxspecial, const double cell_size, int &gpu_mode,
lib/gpu/lal_born_coul_long_cs_ext.cpp:  gpu_mode=BCLCSMF.device->gpu_mode();
lib/gpu/lal_born_coul_long_cs_ext.cpp:  double gpu_split=BCLCSMF.device->particle_split();
lib/gpu/lal_born_coul_long_cs_ext.cpp:  int first_gpu=BCLCSMF.device->first_device();
lib/gpu/lal_born_coul_long_cs_ext.cpp:  int last_gpu=BCLCSMF.device->last_device();
lib/gpu/lal_born_coul_long_cs_ext.cpp:  int gpu_rank=BCLCSMF.device->gpu_rank();
lib/gpu/lal_born_coul_long_cs_ext.cpp:  int procs_per_gpu=BCLCSMF.device->procs_per_gpu();
lib/gpu/lal_born_coul_long_cs_ext.cpp:  BCLCSMF.device->init_message(screen,"born/coul/long/cs",first_gpu,last_gpu);
lib/gpu/lal_born_coul_long_cs_ext.cpp:                          gpu_split, screen, host_cut_ljsq, host_cut_coulsq,
lib/gpu/lal_born_coul_long_cs_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_born_coul_long_cs_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_born_coul_long_cs_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_born_coul_long_cs_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_born_coul_long_cs_ext.cpp:                last_gpu,i);
lib/gpu/lal_born_coul_long_cs_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_born_coul_long_cs_ext.cpp:                            gpu_split, screen, host_cut_ljsq, host_cut_coulsq,
lib/gpu/lal_born_coul_long_cs_ext.cpp:    BCLCSMF.estimate_gpu_overhead();
lib/gpu/lal_born_coul_long_cs_ext.cpp:void bornclcs_gpu_clear() {
lib/gpu/lal_born_coul_long_cs_ext.cpp:int** bornclcs_gpu_compute_n(const int ago, const int inum_full,
lib/gpu/lal_born_coul_long_cs_ext.cpp:void bornclcs_gpu_compute(const int ago, const int inum_full, const int nall,
lib/gpu/lal_born_coul_long_cs_ext.cpp:double bornclcs_gpu_bytes() {
lib/gpu/lal_morse.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_morse.h:    * - -1 if fix gpu not found
lib/gpu/lal_morse.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_morse.h:           const double gpu_split, FILE *screen);
lib/gpu/lal_base_ellipsoid.h:#if defined(USE_OPENCL)
lib/gpu/lal_base_ellipsoid.h:#elif defined(USE_CUDART)
lib/gpu/lal_base_ellipsoid.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_base_ellipsoid.h:    * - -1 if fix gpu not found
lib/gpu/lal_base_ellipsoid.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_base_ellipsoid.h:                const double gpu_split, FILE *screen, const int ntypes,
lib/gpu/lal_base_ellipsoid.h:  /// Estimate the overhead for GPU context changes and CPU driver
lib/gpu/lal_base_ellipsoid.h:  void estimate_gpu_overhead();
lib/gpu/lal_base_ellipsoid.h:    * \note if GPU is neighboring nlocal+host_inum=total number local particles
lib/gpu/lal_base_ellipsoid.h:    * \note if GPU is neighboring olist_size=0 **/
lib/gpu/lal_base_ellipsoid.h:    double bytes=ans->gpu_bytes()+nbor->gpu_bytes();
lib/gpu/lal_base_ellipsoid.h:  double _gpu_overhead, _driver_overhead;
lib/gpu/lal_sph_heatconduction_ext.cpp:int sph_heatconduction_gpu_init(const int ntypes, double **cutsq, double** host_cut,
lib/gpu/lal_sph_heatconduction_ext.cpp:                    const double cell_size, int &gpu_mode, FILE *screen) {
lib/gpu/lal_sph_heatconduction_ext.cpp:  gpu_mode=SPHHeatConductionMF.device->gpu_mode();
lib/gpu/lal_sph_heatconduction_ext.cpp:  double gpu_split=SPHHeatConductionMF.device->particle_split();
lib/gpu/lal_sph_heatconduction_ext.cpp:  int first_gpu=SPHHeatConductionMF.device->first_device();
lib/gpu/lal_sph_heatconduction_ext.cpp:  int last_gpu=SPHHeatConductionMF.device->last_device();
lib/gpu/lal_sph_heatconduction_ext.cpp:  int gpu_rank=SPHHeatConductionMF.device->gpu_rank();
lib/gpu/lal_sph_heatconduction_ext.cpp:  int procs_per_gpu=SPHHeatConductionMF.device->procs_per_gpu();
lib/gpu/lal_sph_heatconduction_ext.cpp:  SPHHeatConductionMF.device->init_message(screen,"sph_heatconduction",first_gpu,last_gpu);
lib/gpu/lal_sph_heatconduction_ext.cpp:                         cell_size, gpu_split, screen);
lib/gpu/lal_sph_heatconduction_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_sph_heatconduction_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_sph_heatconduction_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_sph_heatconduction_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_sph_heatconduction_ext.cpp:                last_gpu,i);
lib/gpu/lal_sph_heatconduction_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_sph_heatconduction_ext.cpp:                           cell_size, gpu_split, screen);
lib/gpu/lal_sph_heatconduction_ext.cpp:    SPHHeatConductionMF.estimate_gpu_overhead();
lib/gpu/lal_sph_heatconduction_ext.cpp:void sph_heatconduction_gpu_clear() {
lib/gpu/lal_sph_heatconduction_ext.cpp:int ** sph_heatconduction_gpu_compute_n(const int ago, const int inum_full, const int nall,
lib/gpu/lal_sph_heatconduction_ext.cpp:void sph_heatconduction_gpu_compute(const int ago, const int inum_full, const int nall,
lib/gpu/lal_sph_heatconduction_ext.cpp:void sph_heatconduction_gpu_get_extra_data(double *host_rho, double *host_esph) {
lib/gpu/lal_sph_heatconduction_ext.cpp:void sph_heatconduction_gpu_update_dE(void **dE_ptr) {
lib/gpu/lal_sph_heatconduction_ext.cpp:double sph_heatconduction_gpu_bytes() {
lib/gpu/lal_device.h:    * the device (ngpu starting at first_gpu_id) that this proc will be using
lib/gpu/lal_device.h:    * - -2 if GPU not found
lib/gpu/lal_device.h:    * - -4 if GPU library not compiled for GPU
lib/gpu/lal_device.h:    * - -6 if GPU could not be initialized for use
lib/gpu/lal_device.h:  int init_device(MPI_Comm world, MPI_Comm replica, const int ngpu,
lib/gpu/lal_device.h:                  const int first_gpu_id, const int gpu_mode,
lib/gpu/lal_device.h:    * - -1 if fix gpu not found
lib/gpu/lal_device.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_device.h:    * - -1 if fix gpu not found
lib/gpu/lal_device.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_device.h:    * \param gpu_host 0 if host will not perform force calculations,
lib/gpu/lal_device.h:    *                 1 if gpu_nbor is true, and host needs a half nbor list,
lib/gpu/lal_device.h:    *                 2 if gpu_nbor is true, and host needs a full nbor list
lib/gpu/lal_device.h:    * - -1 if fix gpu not found
lib/gpu/lal_device.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_device.h:                const int maxspecial, const int gpu_host,
lib/gpu/lal_device.h:                    const int first_gpu, const int last_gpu);
lib/gpu/lal_device.h:                             float,_lgpu_float4> *pppm);
lib/gpu/lal_device.h:                             double,_lgpu_double4> *pppm);
lib/gpu/lal_device.h:  /// Esimate the overhead from GPU calls from multiple procs
lib/gpu/lal_device.h:    * \param gpu_overhead Estimated gpu overhead per timestep (sec)
lib/gpu/lal_device.h:  void estimate_gpu_overhead(const int kernel_calls, double &gpu_overhead,
lib/gpu/lal_device.h:                             double &gpu_driver_overhead);
lib/gpu/lal_device.h:  inline bool double_precision() { return gpu->double_precision(); }
lib/gpu/lal_device.h:                    const double max_bytes, const double gpu_overhead,
lib/gpu/lal_device.h:  /// Add an answer object for putting forces, energies, etc from GPU to LAMMPS
lib/gpu/lal_device.h:  inline double fix_gpu(double **f, double **tor, double *eatom, double **vatom,
lib/gpu/lal_device.h:  inline int procs_per_gpu() const { return _procs_per_gpu; }
lib/gpu/lal_device.h:  /// Return the per-GPU MPI communicator
lib/gpu/lal_device.h:  inline MPI_Comm & gpu_comm() { return _comm_gpu; }
lib/gpu/lal_device.h:  inline int gpu_rank() const { return _gpu_rank; }
lib/gpu/lal_device.h:  /// MPI Barrier for gpu
lib/gpu/lal_device.h:  inline void gpu_barrier() { MPI_Barrier(_comm_gpu); }
lib/gpu/lal_device.h:  /// Serialize GPU initialization and JIT for unsafe platforms
lib/gpu/lal_device.h:    gpu_barrier();
lib/gpu/lal_device.h:  /// Return the 'mode' for acceleration: GPU_FORCE, GPU_NEIGH or GPU_HYB_NEIGH
lib/gpu/lal_device.h:  inline int gpu_mode() const { return _gpu_mode; }
lib/gpu/lal_device.h:  /// For OpenCL, 0 if fast-math options disabled, 1 enabled
lib/gpu/lal_device.h:  /// Architecture gpu code compiled for (returns 0 for OpenCL)
lib/gpu/lal_device.h:  UCL_Device *gpu;
lib/gpu/lal_device.h:  enum{GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH};
lib/gpu/lal_device.h:  PPPM<numtyp,acctyp,float,_lgpu_float4> *pppm_single;
lib/gpu/lal_device.h:  PPPM<numtyp,acctyp,double,_lgpu_double4> *pppm_double;
lib/gpu/lal_device.h:  MPI_Comm _comm_world, _comm_replica, _comm_gpu;
lib/gpu/lal_device.h:  int _procs_per_gpu, _gpu_rank, _world_me, _world_size, _replica_me,
lib/gpu/lal_device.h:  int _gpu_mode, _first_device, _last_device, _platform_id;
lib/gpu/lal_pre_ocl_config.h:   "nvidiagpu",
lib/gpu/lal_pre_ocl_config.h:   "amdgpu",
lib/gpu/lal_pre_ocl_config.h:   "intelgpu",
lib/gpu/lal_pre_ocl_config.h:   "applegpu",
lib/gpu/lal_pre_ocl_config.h:   "NVIDIA_GPU,203,32,32,1,1,4,8,2,256,256,128,64,128,8,128,11,128,8,0",
lib/gpu/lal_pre_ocl_config.h:   "AMD_GPU,403,64,64,0,1,4,8,2,256,256,128,64,128,8,128,11,128,8,0",
lib/gpu/lal_pre_ocl_config.h:   "INTEL_GPU,500,8,32,1,1,4,8,2,128,128,128,128,64,8,128,8,128,8,0",
lib/gpu/lal_pre_ocl_config.h:   "APPLE_GPU,600,16,16,0,1,4,8,1,64,64,64,64,64,8,128,8,128,8,0",
lib/gpu/lal_pre_ocl_config.h:   "INTEL_GPU,500,8,32,1,1,2,8,2,128,128,128,128,64,8,128,8,128,8,2",
lib/gpu/lal_pre_ocl_config.h:   "APPLE_GPU,600,16,16,0,1,2,8,1,64,64,64,64,64,8,128,8,128,8,0",
lib/gpu/lal_base_dipole.h:#ifdef USE_OPENCL
lib/gpu/lal_base_dipole.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_base_dipole.h:    * - -1 if fix gpu not found
lib/gpu/lal_base_dipole.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_base_dipole.h:                  const double gpu_split, FILE *screen,
lib/gpu/lal_base_dipole.h:  /// Estimate the overhead for GPU context changes and CPU driver
lib/gpu/lal_base_dipole.h:  void estimate_gpu_overhead();
lib/gpu/lal_base_dipole.h:  double _gpu_overhead, _driver_overhead;
lib/gpu/lal_gauss.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_gauss.h:    * - -1 if fix gpu not found
lib/gpu/lal_gauss.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_gauss.h:           const double gpu_split, FILE *screen);
lib/gpu/lal_lj.cpp:#if defined(USE_OPENCL)
lib/gpu/lal_lj.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_lj.cpp:                          const double gpu_split, FILE *_screen) {
lib/gpu/lal_lj.cpp:  #ifdef USE_OPENCL
lib/gpu/lal_lj.cpp:                            gpu_split,_screen,lj,"k_lj",onetype);
lib/gpu/lal_lj_smooth_ext.cpp:int ljsmt_gpu_init(const int ntypes, double **cutsq, double **host_lj1,
lib/gpu/lal_lj_smooth_ext.cpp:                 const double cell_size, int &gpu_mode, FILE *screen,
lib/gpu/lal_lj_smooth_ext.cpp:  gpu_mode=LJSMTMF.device->gpu_mode();
lib/gpu/lal_lj_smooth_ext.cpp:  double gpu_split=LJSMTMF.device->particle_split();
lib/gpu/lal_lj_smooth_ext.cpp:  int first_gpu=LJSMTMF.device->first_device();
lib/gpu/lal_lj_smooth_ext.cpp:  int last_gpu=LJSMTMF.device->last_device();
lib/gpu/lal_lj_smooth_ext.cpp:  int gpu_rank=LJSMTMF.device->gpu_rank();
lib/gpu/lal_lj_smooth_ext.cpp:  int procs_per_gpu=LJSMTMF.device->procs_per_gpu();
lib/gpu/lal_lj_smooth_ext.cpp:  LJSMTMF.device->init_message(screen,"lj/smooth",first_gpu,last_gpu);
lib/gpu/lal_lj_smooth_ext.cpp:                       maxspecial, cell_size, gpu_split, screen,
lib/gpu/lal_lj_smooth_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_lj_smooth_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_lj_smooth_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_lj_smooth_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_lj_smooth_ext.cpp:                last_gpu,i);
lib/gpu/lal_lj_smooth_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_lj_smooth_ext.cpp:                         cell_size, gpu_split, screen, host_ljsw0, host_ljsw1, host_ljsw2, host_ljsw3,
lib/gpu/lal_lj_smooth_ext.cpp:    LJSMTMF.estimate_gpu_overhead();
lib/gpu/lal_lj_smooth_ext.cpp:void ljsmt_gpu_reinit(const int ntypes, double **cutsq, double **host_lj1,
lib/gpu/lal_lj_smooth_ext.cpp:  int gpu_rank=LJSMTMF.device->gpu_rank();
lib/gpu/lal_lj_smooth_ext.cpp:  int procs_per_gpu=LJSMTMF.device->procs_per_gpu();
lib/gpu/lal_lj_smooth_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_lj_smooth_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_lj_smooth_ext.cpp:void ljsmt_gpu_clear() {
lib/gpu/lal_lj_smooth_ext.cpp:int ** ljsmt_gpu_compute_n(const int ago, const int inum_full,
lib/gpu/lal_lj_smooth_ext.cpp:void ljsmt_gpu_compute(const int ago, const int inum_full, const int nall,
lib/gpu/lal_lj_smooth_ext.cpp:double ljsmt_gpu_bytes() {
lib/gpu/lal_born_coul_wolf.cpp:#ifdef USE_OPENCL
lib/gpu/lal_born_coul_wolf.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_born_coul_wolf.cpp:                        const double gpu_split, FILE *_screen,
lib/gpu/lal_born_coul_wolf.cpp:  success=this->init_atomic(nlocal,nall,max_nbors,maxspecial,cell_size,gpu_split,
lib/gpu/Makefile.lammps.opencl:gpu_SYSINC =
lib/gpu/Makefile.lammps.opencl:gpu_SYSLIB = -lOpenCL
lib/gpu/Makefile.lammps.opencl:gpu_SYSPATH =
lib/gpu/lal_lj_dsf.cpp:#if defined(USE_OPENCL)
lib/gpu/lal_lj_dsf.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_lj_dsf.cpp:                 const double gpu_split, FILE *_screen,
lib/gpu/lal_lj_dsf.cpp:  success=this->init_atomic(nlocal,nall,max_nbors,maxspecial,cell_size,gpu_split,
lib/gpu/lal_born_coul_wolf_ext.cpp:int borncw_gpu_init(const int ntypes, double **cutsq, double **host_rhoinv,
lib/gpu/lal_born_coul_wolf_ext.cpp:                    const double cell_size, int &gpu_mode, FILE *screen,
lib/gpu/lal_born_coul_wolf_ext.cpp:  gpu_mode=BORNCWMF.device->gpu_mode();
lib/gpu/lal_born_coul_wolf_ext.cpp:  double gpu_split=BORNCWMF.device->particle_split();
lib/gpu/lal_born_coul_wolf_ext.cpp:  int first_gpu=BORNCWMF.device->first_device();
lib/gpu/lal_born_coul_wolf_ext.cpp:  int last_gpu=BORNCWMF.device->last_device();
lib/gpu/lal_born_coul_wolf_ext.cpp:  int gpu_rank=BORNCWMF.device->gpu_rank();
lib/gpu/lal_born_coul_wolf_ext.cpp:  int procs_per_gpu=BORNCWMF.device->procs_per_gpu();
lib/gpu/lal_born_coul_wolf_ext.cpp:  BORNCWMF.device->init_message(screen,"born/coul/wolf",first_gpu,last_gpu);
lib/gpu/lal_born_coul_wolf_ext.cpp:                          maxspecial, cell_size, gpu_split, screen, host_cut_ljsq,
lib/gpu/lal_born_coul_wolf_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_born_coul_wolf_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_born_coul_wolf_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_born_coul_wolf_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_born_coul_wolf_ext.cpp:                last_gpu,i);
lib/gpu/lal_born_coul_wolf_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_born_coul_wolf_ext.cpp:                            maxspecial, cell_size, gpu_split, screen, host_cut_ljsq,
lib/gpu/lal_born_coul_wolf_ext.cpp:    BORNCWMF.estimate_gpu_overhead();
lib/gpu/lal_born_coul_wolf_ext.cpp:void borncw_gpu_clear() {
lib/gpu/lal_born_coul_wolf_ext.cpp:int** borncw_gpu_compute_n(const int ago, const int inum_full,
lib/gpu/lal_born_coul_wolf_ext.cpp:void borncw_gpu_compute(const int ago, const int inum_full, const int nall,
lib/gpu/lal_born_coul_wolf_ext.cpp:double borncw_gpu_bytes() {
lib/gpu/lal_lj_coul_ext.cpp:int ljc_gpu_init(const int ntypes, double **cutsq, double **host_lj1,
lib/gpu/lal_lj_coul_ext.cpp:                 const double cell_size, int &gpu_mode, FILE *screen,
lib/gpu/lal_lj_coul_ext.cpp:  gpu_mode=LJCMF.device->gpu_mode();
lib/gpu/lal_lj_coul_ext.cpp:  double gpu_split=LJCMF.device->particle_split();
lib/gpu/lal_lj_coul_ext.cpp:  int first_gpu=LJCMF.device->first_device();
lib/gpu/lal_lj_coul_ext.cpp:  int last_gpu=LJCMF.device->last_device();
lib/gpu/lal_lj_coul_ext.cpp:  int gpu_rank=LJCMF.device->gpu_rank();
lib/gpu/lal_lj_coul_ext.cpp:  int procs_per_gpu=LJCMF.device->procs_per_gpu();
lib/gpu/lal_lj_coul_ext.cpp:  LJCMF.device->init_message(screen,"lj/cut/coul/cut",first_gpu,last_gpu);
lib/gpu/lal_lj_coul_ext.cpp:                       maxspecial, cell_size, gpu_split, screen, host_cut_ljsq,
lib/gpu/lal_lj_coul_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_lj_coul_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_lj_coul_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_lj_coul_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_lj_coul_ext.cpp:                last_gpu,i);
lib/gpu/lal_lj_coul_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_lj_coul_ext.cpp:                         cell_size, gpu_split, screen, host_cut_ljsq,
lib/gpu/lal_lj_coul_ext.cpp:    LJCMF.estimate_gpu_overhead();
lib/gpu/lal_lj_coul_ext.cpp:void ljc_gpu_clear() {
lib/gpu/lal_lj_coul_ext.cpp:int** ljc_gpu_compute_n(const int ago, const int inum_full,
lib/gpu/lal_lj_coul_ext.cpp:void ljc_gpu_compute(const int ago, const int inum_full, const int nall,
lib/gpu/lal_lj_coul_ext.cpp:double ljc_gpu_bytes() {
lib/gpu/lal_lj96.cpp:#if defined(USE_OPENCL)
lib/gpu/lal_lj96.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_lj96.cpp:                           const double gpu_split, FILE *_screen) {
lib/gpu/lal_lj96.cpp:  success=this->init_atomic(nlocal,nall,max_nbors,maxspecial,cell_size,gpu_split,
lib/gpu/lal_base_sph.h:#ifdef USE_OPENCL
lib/gpu/lal_base_sph.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_base_sph.h:    * - -1 if fix gpu not found
lib/gpu/lal_base_sph.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_base_sph.h:                  const double gpu_split, FILE *screen,
lib/gpu/lal_base_sph.h:  /// Estimate the overhead for GPU context changes and CPU driver
lib/gpu/lal_base_sph.h:  void estimate_gpu_overhead();
lib/gpu/lal_base_sph.h:  double _gpu_overhead, _driver_overhead;
lib/gpu/lal_sw_ext.cpp:int sw_gpu_init(const int ntypes, const int inum, const int nall,
lib/gpu/lal_sw_ext.cpp:                const int max_nbors, const double cell_size, int &gpu_mode,
lib/gpu/lal_sw_ext.cpp:  gpu_mode=SWMF.device->gpu_mode();
lib/gpu/lal_sw_ext.cpp:  double gpu_split=SWMF.device->particle_split();
lib/gpu/lal_sw_ext.cpp:  int first_gpu=SWMF.device->first_device();
lib/gpu/lal_sw_ext.cpp:  int last_gpu=SWMF.device->last_device();
lib/gpu/lal_sw_ext.cpp:  int gpu_rank=SWMF.device->gpu_rank();
lib/gpu/lal_sw_ext.cpp:  int procs_per_gpu=SWMF.device->procs_per_gpu();
lib/gpu/lal_sw_ext.cpp:  if (gpu_split != 1.0)
lib/gpu/lal_sw_ext.cpp:  SWMF.device->init_message(screen,"sw/gpu",first_gpu,last_gpu);
lib/gpu/lal_sw_ext.cpp:    init_ok=SWMF.init(ntypes, inum, nall, max_nbors, cell_size, gpu_split,
lib/gpu/lal_sw_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_sw_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_sw_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_sw_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_sw_ext.cpp:                last_gpu,i);
lib/gpu/lal_sw_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_sw_ext.cpp:      init_ok=SWMF.init(ntypes, inum, nall, max_nbors, cell_size, gpu_split,
lib/gpu/lal_sw_ext.cpp:    SWMF.estimate_gpu_overhead();
lib/gpu/lal_sw_ext.cpp:void sw_gpu_clear() {
lib/gpu/lal_sw_ext.cpp:int ** sw_gpu_compute_n(const int ago, const int inum_full,
lib/gpu/lal_sw_ext.cpp:void sw_gpu_compute(const int ago, const int nlocal, const int nall,
lib/gpu/lal_sw_ext.cpp:double sw_gpu_bytes() {
lib/gpu/lal_lj_cubic.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_lj_cubic.h:    * - -1 if fix gpu not found
lib/gpu/lal_lj_cubic.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_lj_cubic.h:           const double cell_size, const double gpu_split, FILE *screen);
lib/gpu/lal_lj_gromacs.cpp:#if defined(USE_OPENCL)
lib/gpu/lal_lj_gromacs.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_lj_gromacs.cpp:                     const double gpu_split, FILE *_screen,
lib/gpu/lal_lj_gromacs.cpp:  success=this->init_atomic(nlocal,nall,max_nbors,maxspecial,cell_size,gpu_split,
lib/gpu/lal_coul_slater_long_ext.cpp:int csl_gpu_init(const int ntypes, double **host_scale,
lib/gpu/lal_coul_slater_long_ext.cpp:                const int maxspecial, const double cell_size, int &gpu_mode,
lib/gpu/lal_coul_slater_long_ext.cpp:  gpu_mode=CSLMF.device->gpu_mode();
lib/gpu/lal_coul_slater_long_ext.cpp:  double gpu_split=CSLMF.device->particle_split();
lib/gpu/lal_coul_slater_long_ext.cpp:  int first_gpu=CSLMF.device->first_device();
lib/gpu/lal_coul_slater_long_ext.cpp:  int last_gpu=CSLMF.device->last_device();
lib/gpu/lal_coul_slater_long_ext.cpp:  int gpu_rank=CSLMF.device->gpu_rank();
lib/gpu/lal_coul_slater_long_ext.cpp:  int procs_per_gpu=CSLMF.device->procs_per_gpu();
lib/gpu/lal_coul_slater_long_ext.cpp:  CSLMF.device->init_message(screen,"coul/slater/long",first_gpu,last_gpu);
lib/gpu/lal_coul_slater_long_ext.cpp:                      cell_size, gpu_split, screen, host_cut_coulsq,
lib/gpu/lal_coul_slater_long_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_coul_slater_long_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_coul_slater_long_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_coul_slater_long_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_coul_slater_long_ext.cpp:                last_gpu,i);
lib/gpu/lal_coul_slater_long_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_coul_slater_long_ext.cpp:                        cell_size, gpu_split, screen, host_cut_coulsq,
lib/gpu/lal_coul_slater_long_ext.cpp:    CSLMF.estimate_gpu_overhead();
lib/gpu/lal_coul_slater_long_ext.cpp:void csl_gpu_reinit(const int ntypes, double **host_scale) {
lib/gpu/lal_coul_slater_long_ext.cpp:  int gpu_rank=CSLMF.device->gpu_rank();
lib/gpu/lal_coul_slater_long_ext.cpp:  int procs_per_gpu=CSLMF.device->procs_per_gpu();
lib/gpu/lal_coul_slater_long_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_coul_slater_long_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_coul_slater_long_ext.cpp:void csl_gpu_clear() {
lib/gpu/lal_coul_slater_long_ext.cpp:int** csl_gpu_compute_n(const int ago, const int inum_full,
lib/gpu/lal_coul_slater_long_ext.cpp:void csl_gpu_compute(const int ago, const int inum_full, const int nall,
lib/gpu/lal_coul_slater_long_ext.cpp:double csl_gpu_bytes() {
lib/gpu/lal_hippo.cpp:#if defined(USE_OPENCL)
lib/gpu/lal_hippo.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_hippo.cpp:                 const double cell_size, const double gpu_split, FILE *_screen,
lib/gpu/lal_hippo.cpp:                            cell_size,gpu_split,_screen,hippo,
lib/gpu/lal_hippo.cpp:// Reneighbor on GPU if necessary, and then compute polar real-space
lib/gpu/lal_dpd.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_dpd.h:    * - -1 if fix gpu not found
lib/gpu/lal_dpd.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_dpd.h:           const double cell_size, const double gpu_split, FILE *screen);
lib/gpu/lal_sph_lj_ext.cpp:int sph_lj_gpu_init(const int ntypes, double **cutsq, double** host_cut,
lib/gpu/lal_sph_lj_ext.cpp:                    const double cell_size, int &gpu_mode, FILE *screen) {
lib/gpu/lal_sph_lj_ext.cpp:  gpu_mode=SPHLJMF.device->gpu_mode();
lib/gpu/lal_sph_lj_ext.cpp:  double gpu_split=SPHLJMF.device->particle_split();
lib/gpu/lal_sph_lj_ext.cpp:  int first_gpu=SPHLJMF.device->first_device();
lib/gpu/lal_sph_lj_ext.cpp:  int last_gpu=SPHLJMF.device->last_device();
lib/gpu/lal_sph_lj_ext.cpp:  int gpu_rank=SPHLJMF.device->gpu_rank();
lib/gpu/lal_sph_lj_ext.cpp:  int procs_per_gpu=SPHLJMF.device->procs_per_gpu();
lib/gpu/lal_sph_lj_ext.cpp:  SPHLJMF.device->init_message(screen,"sph_lj",first_gpu,last_gpu);
lib/gpu/lal_sph_lj_ext.cpp:                         cell_size, gpu_split, screen);
lib/gpu/lal_sph_lj_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_sph_lj_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_sph_lj_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_sph_lj_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_sph_lj_ext.cpp:                last_gpu,i);
lib/gpu/lal_sph_lj_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_sph_lj_ext.cpp:                           cell_size, gpu_split, screen);
lib/gpu/lal_sph_lj_ext.cpp:    SPHLJMF.estimate_gpu_overhead();
lib/gpu/lal_sph_lj_ext.cpp:void sph_lj_gpu_clear() {
lib/gpu/lal_sph_lj_ext.cpp:int ** sph_lj_gpu_compute_n(const int ago, const int inum_full, const int nall,
lib/gpu/lal_sph_lj_ext.cpp:void sph_lj_gpu_compute(const int ago, const int inum_full, const int nall,
lib/gpu/lal_sph_lj_ext.cpp:void sph_lj_gpu_get_extra_data(double *host_rho, double *host_esph, double *host_cv) {
lib/gpu/lal_sph_lj_ext.cpp:void sph_lj_gpu_update_drhoE(void **drhoE_ptr) {
lib/gpu/lal_sph_lj_ext.cpp:double sph_lj_gpu_bytes() {
lib/gpu/lal_ufm.cpp:#if defined(USE_OPENCL)
lib/gpu/lal_ufm.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_ufm.cpp:                          const double gpu_split, FILE *_screen) {
lib/gpu/lal_ufm.cpp:  success=this->init_atomic(nlocal,nall,max_nbors,maxspecial,cell_size,gpu_split,
lib/gpu/lal_lj_coul_soft.cpp:#if defined(USE_OPENCL)
lib/gpu/lal_lj_coul_soft.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_lj_coul_soft.cpp:                          const double gpu_split, FILE *_screen,
lib/gpu/lal_lj_coul_soft.cpp:  success=this->init_atomic(nlocal,nall,max_nbors,maxspecial,cell_size,gpu_split,
lib/gpu/lal_yukawa_ext.cpp:int yukawa_gpu_init(const int ntypes, double **cutsq, double kappa,
lib/gpu/lal_yukawa_ext.cpp:                 int &gpu_mode, FILE *screen) {
lib/gpu/lal_yukawa_ext.cpp:  gpu_mode=YKMF.device->gpu_mode();
lib/gpu/lal_yukawa_ext.cpp:  double gpu_split=YKMF.device->particle_split();
lib/gpu/lal_yukawa_ext.cpp:  int first_gpu=YKMF.device->first_device();
lib/gpu/lal_yukawa_ext.cpp:  int last_gpu=YKMF.device->last_device();
lib/gpu/lal_yukawa_ext.cpp:  int gpu_rank=YKMF.device->gpu_rank();
lib/gpu/lal_yukawa_ext.cpp:  int procs_per_gpu=YKMF.device->procs_per_gpu();
lib/gpu/lal_yukawa_ext.cpp:  YKMF.device->init_message(screen,"yukawa",first_gpu,last_gpu);
lib/gpu/lal_yukawa_ext.cpp:                      gpu_split, screen);
lib/gpu/lal_yukawa_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_yukawa_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_yukawa_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_yukawa_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_yukawa_ext.cpp:                last_gpu,i);
lib/gpu/lal_yukawa_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_yukawa_ext.cpp:                      gpu_split, screen);
lib/gpu/lal_yukawa_ext.cpp:    YKMF.estimate_gpu_overhead();
lib/gpu/lal_yukawa_ext.cpp:void yukawa_gpu_clear() {
lib/gpu/lal_yukawa_ext.cpp:int ** yukawa_gpu_compute_n(const int ago, const int inum_full,
lib/gpu/lal_yukawa_ext.cpp:void yukawa_gpu_compute(const int ago, const int inum_full, const int nall,
lib/gpu/lal_yukawa_ext.cpp:double yukawa_gpu_bytes() {
lib/gpu/lal_neighbor_shared.h:#if defined(USE_OPENCL)
lib/gpu/lal_neighbor_shared.h:using namespace ucl_opencl;
lib/gpu/lal_neighbor_shared.h:#elif defined(USE_CUDART)
lib/gpu/lal_neighbor_shared.h:using namespace ucl_cudart;
lib/gpu/lal_neighbor_shared.h:using namespace ucl_cudadr;
lib/gpu/lal_neighbor_shared.h:  /// Texture for cached position/type access with CUDA
lib/gpu/lal_neighbor_shared.h:  void compile_kernels(UCL_Device &dev, const int gpu_nbor,
lib/gpu/lal_neighbor_shared.h:  int _gpu_nbor;
lib/gpu/lal_dpd_coul_slater_long.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_dpd_coul_slater_long.h:    * - -1 if fix gpu not found
lib/gpu/lal_dpd_coul_slater_long.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_dpd_coul_slater_long.h:           const double gpu_split, FILE *screen, double *host_special_coul, const double qqrd2e,
lib/gpu/lal_lj_spica_long.cpp:#if defined(USE_OPENCL)
lib/gpu/lal_lj_spica_long.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_lj_spica_long.cpp:                           const double gpu_split, FILE *_screen,
lib/gpu/lal_lj_spica_long.cpp:  success=this->init_atomic(nlocal,nall,max_nbors,maxspecial,cell_size,gpu_split,
lib/gpu/lal_lj_class2_long_ext.cpp:int c2cl_gpu_init(const int ntypes, double **cutsq, double **host_lj1,
lib/gpu/lal_lj_class2_long_ext.cpp:                  const double cell_size, int &gpu_mode, FILE *screen,
lib/gpu/lal_lj_class2_long_ext.cpp:  gpu_mode=C2CLMF.device->gpu_mode();
lib/gpu/lal_lj_class2_long_ext.cpp:  double gpu_split=C2CLMF.device->particle_split();
lib/gpu/lal_lj_class2_long_ext.cpp:  int first_gpu=C2CLMF.device->first_device();
lib/gpu/lal_lj_class2_long_ext.cpp:  int last_gpu=C2CLMF.device->last_device();
lib/gpu/lal_lj_class2_long_ext.cpp:  int gpu_rank=C2CLMF.device->gpu_rank();
lib/gpu/lal_lj_class2_long_ext.cpp:  int procs_per_gpu=C2CLMF.device->procs_per_gpu();
lib/gpu/lal_lj_class2_long_ext.cpp:  C2CLMF.device->init_message(screen,"lj/class2/coul/long",first_gpu,last_gpu);
lib/gpu/lal_lj_class2_long_ext.cpp:                        cell_size, gpu_split, screen, host_cut_ljsq,
lib/gpu/lal_lj_class2_long_ext.cpp:  for (int i=0; i<procs_per_gpu; i++) {
lib/gpu/lal_lj_class2_long_ext.cpp:      if (last_gpu-first_gpu==0)
lib/gpu/lal_lj_class2_long_ext.cpp:        fprintf(screen,"Initializing Device %d on core %d...",first_gpu,i);
lib/gpu/lal_lj_class2_long_ext.cpp:        fprintf(screen,"Initializing Devices %d-%d on core %d...",first_gpu,
lib/gpu/lal_lj_class2_long_ext.cpp:                last_gpu,i);
lib/gpu/lal_lj_class2_long_ext.cpp:    if (gpu_rank==i && world_me!=0)
lib/gpu/lal_lj_class2_long_ext.cpp:                          cell_size, gpu_split, screen, host_cut_ljsq,
lib/gpu/lal_lj_class2_long_ext.cpp:    C2CLMF.estimate_gpu_overhead();
lib/gpu/lal_lj_class2_long_ext.cpp:void c2cl_gpu_clear() {
lib/gpu/lal_lj_class2_long_ext.cpp:int** c2cl_gpu_compute_n(const int ago, const int inum_full,
lib/gpu/lal_lj_class2_long_ext.cpp:void c2cl_gpu_compute(const int ago, const int inum_full, const int nall,
lib/gpu/lal_lj_class2_long_ext.cpp:double c2cl_gpu_bytes() {
lib/gpu/lal_lj_spica.h:    * \param gpu_split fraction of particles handled by device
lib/gpu/lal_lj_spica.h:    * - -1 if fix gpu not found
lib/gpu/lal_lj_spica.h:    * - -4 if the GPU library was not compiled for GPU
lib/gpu/lal_lj_spica.h:           const double gpu_split, FILE *screen);
lib/gpu/lal_coul_long_cs.cpp:#if defined(USE_OPENCL)
lib/gpu/lal_coul_long_cs.cpp:#elif defined(USE_CUDART)
lib/gpu/lal_coul_long_cs.cpp:extern Device<PRECISION,ACC_PRECISION> pair_gpu_device;
lib/gpu/lal_coul_long_cs.cpp:                    const double gpu_split, FILE *_screen,
lib/gpu/lal_coul_long_cs.cpp:                            gpu_split,_screen,coul_long_cs,"k_coul_long_cs");
lib/gpu/Opencl.makefile:GPU_LIB = $(LIB_DIR)/libgpu.a
lib/gpu/Opencl.makefile:all: $(OBJ_DIR) $(KERS) $(GPU_LIB) $(EXECS)
lib/gpu/Opencl.makefile:OCL  = $(OCL_CPP) $(OCL_PREC) $(OCL_TUNE) -DUSE_OPENCL
lib/gpu/Opencl.makefile:$(OBJ_DIR)/neighbor_gpu_cl.h: lal_neighbor_gpu.cu lal_preprocessor.h
lib/gpu/Opencl.makefile:	$(BSH) ./geryon/file_to_cstr.sh neighbor_gpu lal_preprocessor.h lal_neighbor_gpu.cu $(OBJ_DIR)/neighbor_gpu_cl.h
lib/gpu/Opencl.makefile:	$(OCL) -o $@ ./geryon/ucl_get_devices.cpp -DUCL_OPENCL $(OCL_LINK)
lib/gpu/Opencl.makefile:$(GPU_LIB): $(OBJS)
lib/gpu/Opencl.makefile:	$(AR) -crusv $(GPU_LIB) $(OBJS)
lib/gpu/Opencl.makefile:	-rm -f $(EXECS) $(GPU_LIB) $(OBJS) $(KERS) *.linkinfo
lib/gpu/Opencl.makefile:	-rm -f $(EXECS) $(GPU_LIB) $(OBJS) $(KERS) *.linkinfo
lib/colvars/colvaratoms.h: *          branch prediction is broken (or further migration to GPU code).
lib/colvars/colvarmodule_refs.h:    "  title = {Scalable molecular dynamics on {CPU} and {GPU} architectures with {NAMD}},\n"
lib/README:gpu           general GPU routines, GPU package
lib/README:kokkos        Kokkos package for GPU and many-core acceleration
cmake/presets/pgi.cmake:# preset that will enable PGI (Nvidia) compilers with support for MPI and OpenMP (on Linux boxes)
cmake/presets/nolib.cmake:# library or special compiler (fortran or cuda) or equivalent.
cmake/presets/nolib.cmake:  GPU
cmake/presets/kokkos-sycl-nvidia.cmake:set(Kokkos_ENABLE_CUDA   OFF CACHE BOOL "" FORCE)
cmake/presets/kokkos-sycl-nvidia.cmake:set(CMAKE_TUNE_FLAGS "-fgpu-inline-threshold=100000 -Xsycl-target-frontend -O3  -Xsycl-target-frontend -ffp-contract=on -Wno-unknown-cuda-version" CACHE STRING "" FORCE)
cmake/presets/all_on.cmake:  GPU
cmake/presets/kokkos-hip.cmake:set(Kokkos_ENABLE_CUDA   OFF CACHE BOOL "" FORCE)
cmake/presets/kokkos-serial.cmake:set(Kokkos_ENABLE_CUDA   OFF CACHE BOOL "" FORCE)
cmake/presets/gpu-cuda.cmake:# preset that enables GPU and selects CUDA API
cmake/presets/gpu-cuda.cmake:set(PKG_GPU ON CACHE BOOL "Build GPU package" FORCE)
cmake/presets/gpu-cuda.cmake:set(GPU_API "cuda" CACHE STRING "APU used by GPU package" FORCE)
cmake/presets/gpu-cuda.cmake:set(GPU_PREC "mixed" CACHE STRING "" FORCE)
cmake/presets/gpu-cuda.cmake:set(CUDA_NVCC_FLAGS "-allow-unsupported-compiler" CACHE STRING "" FORCE)
cmake/presets/gpu-cuda.cmake:set(CUDA_NVCC_FLAGS_DEBUG "-allow-unsupported-compiler" CACHE STRING "" FORCE)
cmake/presets/gpu-cuda.cmake:set(CUDA_NVCC_FLAGS_MINSIZEREL "-allow-unsupported-compiler" CACHE STRING "" FORCE)
cmake/presets/gpu-cuda.cmake:set(CUDA_NVCC_FLAGS_RELWITHDEBINFO "-allow-unsupported-compiler" CACHE STRING "" FORCE)
cmake/presets/gpu-cuda.cmake:set(CUDA_NVCC_FLAGS_RELEASE "-allow-unsupported-compiler" CACHE STRING "" FORCE)
cmake/presets/kokkos-sycl-intel.cmake:set(Kokkos_ENABLE_CUDA   OFF CACHE BOOL "" FORCE)
cmake/presets/kokkos-sycl-intel.cmake:set(FFT_KOKKOS "MKL_GPU" CACHE STRING "" FORCE)
cmake/presets/kokkos-openmp.cmake:set(Kokkos_ENABLE_CUDA   OFF CACHE BOOL "" FORCE)
cmake/presets/all_off.cmake:  GPU
cmake/presets/mingw-cross.cmake:  GPU
cmake/presets/nvhpc.cmake:# preset that will enable Nvidia HPC SDK compilers with support for MPI and OpenMP (on Linux boxes)
cmake/presets/kokkos-cuda.cmake:# preset that enables KOKKOS and selects CUDA compilation with OpenMP
cmake/presets/kokkos-cuda.cmake:# enabled as well. This preselects CC 5.0 as default GPU arch, since
cmake/presets/kokkos-cuda.cmake:set(Kokkos_ENABLE_CUDA   ON CACHE BOOL "" FORCE)
cmake/CMakeLists.txt:if((PKG_KOKKOS) AND (Kokkos_ENABLE_CUDA) AND NOT (CMAKE_CXX_COMPILER_ID STREQUAL "Clang"))
cmake/CMakeLists.txt:  set(CMAKE_TUNE_DEFAULT "${CMAKE_TUNE_DEFAULT} -Xcudafe --diag_suppress=unrecognized_pragma -Xcudafe --diag_suppress=128")
cmake/CMakeLists.txt:set(SUFFIX_PACKAGES CORESHELL GPU KOKKOS OPT INTEL OPENMP)
cmake/CMakeLists.txt:  set(CUDA_REQUEST_PIC "-Xcompiler ${CMAKE_SHARED_LIBRARY_CXX_FLAGS}")
cmake/CMakeLists.txt:  set(CUDA_REQUEST_PIC)
cmake/CMakeLists.txt:  if(NOT (("${_FLAG}" STREQUAL "-Xcudafe") OR (("${_FLAG}" STREQUAL "--diag_suppress=unrecognized_pragma"))))
cmake/CMakeLists.txt:foreach(PKG_WITH_INCL CORESHELL DPD-BASIC DPD-SMOOTH MC MISC PHONON QEQ OPENMP KOKKOS OPT INTEL GPU)
cmake/CMakeLists.txt:    string(REGEX MATCH "Kokkos_ENABLE_(SERIAL|THREADS|OPENMP|CUDA|HIP|SYCL|OPENMPTARGET|HPX)" _match ${_var})
cmake/CMakeLists.txt:      string(REGEX REPLACE "Kokkos_ENABLE_(OPENMP|SERIAL|CUDA|HIP|SYCL)" "\\1" _match ${_var})
cmake/CMakeLists.txt:if(PKG_GPU)
cmake/CMakeLists.txt:  message(STATUS "<<< GPU package settings >>>
cmake/CMakeLists.txt:-- GPU API:                  ${GPU_API}")
cmake/CMakeLists.txt:  if(GPU_API STREQUAL "CUDA")
cmake/CMakeLists.txt:    message(STATUS "CUDA Compiler:            ${CUDA_NVCC_EXECUTABLE}")
cmake/CMakeLists.txt:    message(STATUS "GPU default architecture: ${GPU_ARCH}")
cmake/CMakeLists.txt:    message(STATUS "GPU binning with CUDPP:   ${CUDPP_OPT}")
cmake/CMakeLists.txt:    message(STATUS "CUDA MPS support:         ${CUDA_MPS_SUPPORT}")
cmake/CMakeLists.txt:  elseif(GPU_API STREQUAL "HIP")
cmake/CMakeLists.txt:      message(STATUS "HIP GPU sorting: on")
cmake/CMakeLists.txt:      message(STATUS "HIP GPU sorting: off")
cmake/CMakeLists.txt:  message(STATUS "GPU precision:            ${GPU_PREC}")
cmake/Modules/OpenCLLoader.cmake:message(STATUS "Downloading and building OpenCL loader library")
cmake/Modules/OpenCLLoader.cmake:set(OPENCL_LOADER_URL "${LAMMPS_THIRDPARTY_URL}/opencl-loader-2024.05.09.tar.gz" CACHE STRING "URL for OpenCL loader tarball")
cmake/Modules/OpenCLLoader.cmake:set(OPENCL_LOADER_MD5 "e7796826b21c059224fabe997e0f2075" CACHE STRING "MD5 checksum of OpenCL loader tarball")
cmake/Modules/OpenCLLoader.cmake:mark_as_advanced(OPENCL_LOADER_URL)
cmake/Modules/OpenCLLoader.cmake:mark_as_advanced(OPENCL_LOADER_MD5)
cmake/Modules/OpenCLLoader.cmake:set(INSTALL_LIBOPENCL OFF CACHE BOOL "" FORCE)
cmake/Modules/OpenCLLoader.cmake:ExternalCMakeProject(opencl_loader ${OPENCL_LOADER_URL} ${OPENCL_LOADER_MD5} opencl-loader . "")
cmake/Modules/OpenCLUtils.cmake:function(WriteOpenCLHeader varname outfile files)
cmake/Modules/OpenCLUtils.cmake:endfunction(WriteOpenCLHeader)
cmake/Modules/OpenCLUtils.cmake:function(GenerateOpenCLHeader varname outfile files)
cmake/Modules/OpenCLUtils.cmake:                               -P ${CMAKE_CURRENT_SOURCE_DIR}/Modules/GenerateOpenCLHeader.cmake
cmake/Modules/OpenCLUtils.cmake:endfunction(GenerateOpenCLHeader)
cmake/Modules/Packages/GPU.cmake:# Silence CMake warnings about FindCUDA being obsolete.
cmake/Modules/Packages/GPU.cmake:# We may need to eventually rewrite this section to use enable_language(CUDA)
cmake/Modules/Packages/GPU.cmake:set(GPU_SOURCES_DIR ${LAMMPS_SOURCE_DIR}/GPU)
cmake/Modules/Packages/GPU.cmake:set(GPU_SOURCES ${GPU_SOURCES_DIR}/gpu_extra.h
cmake/Modules/Packages/GPU.cmake:                ${GPU_SOURCES_DIR}/fix_gpu.h
cmake/Modules/Packages/GPU.cmake:                ${GPU_SOURCES_DIR}/fix_gpu.cpp
cmake/Modules/Packages/GPU.cmake:                ${GPU_SOURCES_DIR}/fix_nh_gpu.h
cmake/Modules/Packages/GPU.cmake:                ${GPU_SOURCES_DIR}/fix_nh_gpu.cpp)
cmake/Modules/Packages/GPU.cmake:target_compile_definitions(lammps PRIVATE -DLMP_GPU)
cmake/Modules/Packages/GPU.cmake:set(GPU_API "opencl" CACHE STRING "API used by GPU package")
cmake/Modules/Packages/GPU.cmake:set(GPU_API_VALUES opencl cuda hip)
cmake/Modules/Packages/GPU.cmake:set_property(CACHE GPU_API PROPERTY STRINGS ${GPU_API_VALUES})
cmake/Modules/Packages/GPU.cmake:validate_option(GPU_API GPU_API_VALUES)
cmake/Modules/Packages/GPU.cmake:string(TOUPPER ${GPU_API} GPU_API)
cmake/Modules/Packages/GPU.cmake:set(GPU_PREC "mixed" CACHE STRING "LAMMPS GPU precision")
cmake/Modules/Packages/GPU.cmake:set(GPU_PREC_VALUES double mixed single)
cmake/Modules/Packages/GPU.cmake:set_property(CACHE GPU_PREC PROPERTY STRINGS ${GPU_PREC_VALUES})
cmake/Modules/Packages/GPU.cmake:validate_option(GPU_PREC GPU_PREC_VALUES)
cmake/Modules/Packages/GPU.cmake:string(TOUPPER ${GPU_PREC} GPU_PREC)
cmake/Modules/Packages/GPU.cmake:if(GPU_PREC STREQUAL "DOUBLE")
cmake/Modules/Packages/GPU.cmake:  set(GPU_PREC_SETTING "DOUBLE_DOUBLE")
cmake/Modules/Packages/GPU.cmake:elseif(GPU_PREC STREQUAL "MIXED")
cmake/Modules/Packages/GPU.cmake:  set(GPU_PREC_SETTING "SINGLE_DOUBLE")
cmake/Modules/Packages/GPU.cmake:elseif(GPU_PREC STREQUAL "SINGLE")
cmake/Modules/Packages/GPU.cmake:  set(GPU_PREC_SETTING "SINGLE_SINGLE")
cmake/Modules/Packages/GPU.cmake:option(GPU_DEBUG "Enable debugging code of the GPU package" OFF)
cmake/Modules/Packages/GPU.cmake:mark_as_advanced(GPU_DEBUG)
cmake/Modules/Packages/GPU.cmake:  message(FATAL_ERROR "GPU acceleration of AMOEBA is not (yet) compatible with single precision FFT")
cmake/Modules/Packages/GPU.cmake:  list(APPEND GPU_SOURCES
cmake/Modules/Packages/GPU.cmake:              ${GPU_SOURCES_DIR}/amoeba_convolution_gpu.h
cmake/Modules/Packages/GPU.cmake:              ${GPU_SOURCES_DIR}/amoeba_convolution_gpu.cpp)
cmake/Modules/Packages/GPU.cmake:file(GLOB GPU_LIB_SOURCES CONFIGURE_DEPENDS ${LAMMPS_LIB_SOURCE_DIR}/gpu/[^.]*.cpp)
cmake/Modules/Packages/GPU.cmake:file(MAKE_DIRECTORY ${LAMMPS_LIB_BINARY_DIR}/gpu)
cmake/Modules/Packages/GPU.cmake:if(GPU_API STREQUAL "CUDA")
cmake/Modules/Packages/GPU.cmake:  find_package(CUDA QUIET)
cmake/Modules/Packages/GPU.cmake:  # augment search path for CUDA toolkit libraries to include the stub versions. Needed to find libcuda.so on machines without a CUDA driver installation
cmake/Modules/Packages/GPU.cmake:  if(CUDA_FOUND)
cmake/Modules/Packages/GPU.cmake:    set(CMAKE_LIBRARY_PATH "${CUDA_TOOLKIT_ROOT_DIR}/lib64/stubs;${CUDA_TOOLKIT_ROOT_DIR}/lib/stubs;${CUDA_TOOLKIT_ROOT_DIR}/lib64;${CUDA_TOOLKIT_ROOT_DIR}/lib;${CMAKE_LIBRARY_PATH}")
cmake/Modules/Packages/GPU.cmake:    find_package(CUDA REQUIRED)
cmake/Modules/Packages/GPU.cmake:    message(FATAL_ERROR "CUDA Toolkit not found")
cmake/Modules/Packages/GPU.cmake:  option(CUDPP_OPT "Enable GPU binning via CUDAPP (should be off for modern GPUs)" OFF)
cmake/Modules/Packages/GPU.cmake:  option(CUDA_MPS_SUPPORT "Enable tweaks to support CUDA Multi-process service (MPS)" OFF)
cmake/Modules/Packages/GPU.cmake:  if(CUDA_MPS_SUPPORT)
cmake/Modules/Packages/GPU.cmake:      message(FATAL_ERROR "Must use -DCUDPP_OPT=OFF with -DCUDA_MPS_SUPPORT=ON")
cmake/Modules/Packages/GPU.cmake:    set(GPU_CUDA_MPS_FLAGS "-DCUDA_MPS_SUPPORT")
cmake/Modules/Packages/GPU.cmake:  option(CUDA_BUILD_MULTIARCH "Enable building CUDA kernels for all supported GPU architectures" ON)
cmake/Modules/Packages/GPU.cmake:  mark_as_advanced(GPU_BUILD_MULTIARCH)
cmake/Modules/Packages/GPU.cmake:  set(GPU_ARCH "sm_50" CACHE STRING "LAMMPS GPU CUDA SM primary architecture (e.g. sm_60)")
cmake/Modules/Packages/GPU.cmake:  # ensure that no *cubin.h files exist from a compile in the lib/gpu folder
cmake/Modules/Packages/GPU.cmake:  file(GLOB GPU_LIB_OLD_CUBIN_HEADERS CONFIGURE_DEPENDS ${LAMMPS_LIB_SOURCE_DIR}/gpu/*_cubin.h)
cmake/Modules/Packages/GPU.cmake:  if(GPU_LIB_OLD_CUBIN_HEADERS)
cmake/Modules/Packages/GPU.cmake:      "Found file(s) generated by the make-based build system in lib/gpu\n"
cmake/Modules/Packages/GPU.cmake:      "  make -C ${LAMMPS_LIB_SOURCE_DIR}/gpu -f Makefile.serial clean\n"
cmake/Modules/Packages/GPU.cmake:  file(GLOB GPU_LIB_CU CONFIGURE_DEPENDS ${LAMMPS_LIB_SOURCE_DIR}/gpu/[^.]*.cu ${CMAKE_CURRENT_SOURCE_DIR}/gpu/[^.]*.cu)
cmake/Modules/Packages/GPU.cmake:  list(REMOVE_ITEM GPU_LIB_CU ${LAMMPS_LIB_SOURCE_DIR}/gpu/lal_pppm.cu)
cmake/Modules/Packages/GPU.cmake:  cuda_include_directories(${LAMMPS_LIB_SOURCE_DIR}/gpu ${LAMMPS_LIB_BINARY_DIR}/gpu)
cmake/Modules/Packages/GPU.cmake:    cuda_include_directories(${LAMMPS_LIB_SOURCE_DIR}/gpu/cudpp_mini)
cmake/Modules/Packages/GPU.cmake:    file(GLOB GPU_LIB_CUDPP_SOURCES CONFIGURE_DEPENDS ${LAMMPS_LIB_SOURCE_DIR}/gpu/cudpp_mini/[^.]*.cpp)
cmake/Modules/Packages/GPU.cmake:    file(GLOB GPU_LIB_CUDPP_CU CONFIGURE_DEPENDS ${LAMMPS_LIB_SOURCE_DIR}/gpu/cudpp_mini/[^.]*.cu)
cmake/Modules/Packages/GPU.cmake:  # build arch/gencode commands for nvcc based on CUDA toolkit version and use choice
cmake/Modules/Packages/GPU.cmake:  set(GPU_CUDA_GENCODE "-arch=${GPU_ARCH}")
cmake/Modules/Packages/GPU.cmake:  if(CUDA_BUILD_MULTIARCH)
cmake/Modules/Packages/GPU.cmake:    # apply the following to build "fat" CUDA binaries only for known CUDA toolkits since version 8.0
cmake/Modules/Packages/GPU.cmake:    # comparison chart according to: https://en.wikipedia.org/wiki/CUDA#GPUs_supported
cmake/Modules/Packages/GPU.cmake:    if(CUDA_VERSION VERSION_LESS 8.0)
cmake/Modules/Packages/GPU.cmake:      message(FATAL_ERROR "CUDA Toolkit version 8.0 or later is required")
cmake/Modules/Packages/GPU.cmake:    elseif(CUDA_VERSION VERSION_GREATER_EQUAL "13.0")
cmake/Modules/Packages/GPU.cmake:      message(WARNING "Untested CUDA Toolkit version ${CUDA_VERSION}. Use at your own risk")
cmake/Modules/Packages/GPU.cmake:      set(GPU_CUDA_GENCODE "-arch=all")
cmake/Modules/Packages/GPU.cmake:    elseif(CUDA_VERSION VERSION_GREATER_EQUAL "12.0")
cmake/Modules/Packages/GPU.cmake:      set(GPU_CUDA_GENCODE "-arch=all")
cmake/Modules/Packages/GPU.cmake:      # Kepler (GPU Arch 3.0) is supported by CUDA 5 to CUDA 10.2
cmake/Modules/Packages/GPU.cmake:      if((CUDA_VERSION VERSION_GREATER_EQUAL "5.0") AND (CUDA_VERSION VERSION_LESS "11.0"))
cmake/Modules/Packages/GPU.cmake:        string(APPEND GPU_CUDA_GENCODE " -gencode arch=compute_30,code=[sm_30,compute_30] ")
cmake/Modules/Packages/GPU.cmake:      # Kepler (GPU Arch 3.5) is supported by CUDA 5 to CUDA 11
cmake/Modules/Packages/GPU.cmake:      if((CUDA_VERSION VERSION_GREATER_EQUAL "5.0") AND (CUDA_VERSION VERSION_LESS "12.0"))
cmake/Modules/Packages/GPU.cmake:        string(APPEND GPU_CUDA_GENCODE " -gencode arch=compute_35,code=[sm_35,compute_35]")
cmake/Modules/Packages/GPU.cmake:      # Maxwell (GPU Arch 5.x) is supported by CUDA 6 and later
cmake/Modules/Packages/GPU.cmake:      if(CUDA_VERSION VERSION_GREATER_EQUAL "6.0")
cmake/Modules/Packages/GPU.cmake:        string(APPEND GPU_CUDA_GENCODE " -gencode arch=compute_50,code=[sm_50,compute_50] -gencode arch=compute_52,code=[sm_52,compute_52]")
cmake/Modules/Packages/GPU.cmake:      # Pascal (GPU Arch 6.x) is supported by CUDA 8 and later
cmake/Modules/Packages/GPU.cmake:      if(CUDA_VERSION VERSION_GREATER_EQUAL "8.0")
cmake/Modules/Packages/GPU.cmake:        string(APPEND GPU_CUDA_GENCODE " -gencode arch=compute_60,code=[sm_60,compute_60] -gencode arch=compute_61,code=[sm_61,compute_61]")
cmake/Modules/Packages/GPU.cmake:      # Volta (GPU Arch 7.0) is supported by CUDA 9 and later
cmake/Modules/Packages/GPU.cmake:      if(CUDA_VERSION VERSION_GREATER_EQUAL "9.0")
cmake/Modules/Packages/GPU.cmake:        string(APPEND GPU_CUDA_GENCODE " -gencode arch=compute_70,code=[sm_70,compute_70]")
cmake/Modules/Packages/GPU.cmake:      # Turing (GPU Arch 7.5) is supported by CUDA 10 and later
cmake/Modules/Packages/GPU.cmake:      if(CUDA_VERSION VERSION_GREATER_EQUAL "10.0")
cmake/Modules/Packages/GPU.cmake:        string(APPEND GPU_CUDA_GENCODE " -gencode arch=compute_75,code=[sm_75,compute_75]")
cmake/Modules/Packages/GPU.cmake:      # Ampere (GPU Arch 8.0) is supported by CUDA 11 and later
cmake/Modules/Packages/GPU.cmake:      if(CUDA_VERSION VERSION_GREATER_EQUAL "11.0")
cmake/Modules/Packages/GPU.cmake:        string(APPEND GPU_CUDA_GENCODE " -gencode arch=compute_80,code=[sm_80,compute_80]")
cmake/Modules/Packages/GPU.cmake:      # Ampere (GPU Arch 8.6) is supported by CUDA 11.1 and later
cmake/Modules/Packages/GPU.cmake:      if(CUDA_VERSION VERSION_GREATER_EQUAL "11.1")
cmake/Modules/Packages/GPU.cmake:        string(APPEND GPU_CUDA_GENCODE " -gencode arch=compute_86,code=[sm_86,compute_86]")
cmake/Modules/Packages/GPU.cmake:      # Lovelace (GPU Arch 8.9) is supported by CUDA 11.8 and later
cmake/Modules/Packages/GPU.cmake:      if(CUDA_VERSION VERSION_GREATER_EQUAL "11.8")
cmake/Modules/Packages/GPU.cmake:        string(APPEND GPU_CUDA_GENCODE " -gencode arch=compute_90,code=[sm_90,compute_90]")
cmake/Modules/Packages/GPU.cmake:      # Hopper (GPU Arch 9.0) is supported by CUDA 12.0 and later
cmake/Modules/Packages/GPU.cmake:      if(CUDA_VERSION VERSION_GREATER_EQUAL "12.0")
cmake/Modules/Packages/GPU.cmake:        string(APPEND GPU_CUDA_GENCODE " -gencode arch=compute_90,code=[sm_90,compute_90]")
cmake/Modules/Packages/GPU.cmake:  cuda_compile_fatbin(GPU_GEN_OBJS ${GPU_LIB_CU} OPTIONS ${CUDA_REQUEST_PIC}
cmake/Modules/Packages/GPU.cmake:          -DUNIX -O3 --use_fast_math -Wno-deprecated-gpu-targets -allow-unsupported-compiler -DNV_KERNEL -DUCL_CUDADR ${GPU_CUDA_GENCODE} -D_${GPU_PREC_SETTING} -DLAMMPS_${LAMMPS_SIZES})
cmake/Modules/Packages/GPU.cmake:  cuda_compile(GPU_OBJS ${GPU_LIB_CUDPP_CU} OPTIONS ${CUDA_REQUEST_PIC}
cmake/Modules/Packages/GPU.cmake:          -DUNIX -O3 --use_fast_math -Wno-deprecated-gpu-targets -allow-unsupported-compiler -DUCL_CUDADR ${GPU_CUDA_GENCODE} -D_${GPU_PREC_SETTING} -DLAMMPS_${LAMMPS_SIZES})
cmake/Modules/Packages/GPU.cmake:  foreach(CU_OBJ ${GPU_GEN_OBJS})
cmake/Modules/Packages/GPU.cmake:    add_custom_command(OUTPUT ${LAMMPS_LIB_BINARY_DIR}/gpu/${CU_NAME}_cubin.h
cmake/Modules/Packages/GPU.cmake:      COMMAND ${BIN2C} -c -n ${CU_NAME} ${CU_OBJ} > ${LAMMPS_LIB_BINARY_DIR}/gpu/${CU_NAME}_cubin.h
cmake/Modules/Packages/GPU.cmake:    list(APPEND GPU_LIB_SOURCES ${LAMMPS_LIB_BINARY_DIR}/gpu/${CU_NAME}_cubin.h)
cmake/Modules/Packages/GPU.cmake:  set_directory_properties(PROPERTIES ADDITIONAL_MAKE_CLEAN_FILES "${LAMMPS_LIB_BINARY_DIR}/gpu/*_cubin.h")
cmake/Modules/Packages/GPU.cmake:  add_library(gpu STATIC ${GPU_LIB_SOURCES} ${GPU_LIB_CUDPP_SOURCES} ${GPU_OBJS})
cmake/Modules/Packages/GPU.cmake:  target_link_libraries(gpu PRIVATE ${CUDA_LIBRARIES} ${CUDA_CUDA_LIBRARY})
cmake/Modules/Packages/GPU.cmake:  target_include_directories(gpu PRIVATE ${LAMMPS_LIB_BINARY_DIR}/gpu ${CUDA_INCLUDE_DIRS})
cmake/Modules/Packages/GPU.cmake:  target_compile_definitions(gpu PRIVATE -DUSE_CUDA -D_${GPU_PREC_SETTING} ${GPU_CUDA_MPS_FLAGS})
cmake/Modules/Packages/GPU.cmake:  if(GPU_DEBUG)
cmake/Modules/Packages/GPU.cmake:    target_compile_definitions(gpu PRIVATE -DUCL_DEBUG -DGERYON_KERNEL_DUMP)
cmake/Modules/Packages/GPU.cmake:    target_compile_definitions(gpu PRIVATE -DMPI_GERYON -DUCL_NO_EXIT)
cmake/Modules/Packages/GPU.cmake:    target_include_directories(gpu PRIVATE ${LAMMPS_LIB_SOURCE_DIR}/gpu/cudpp_mini)
cmake/Modules/Packages/GPU.cmake:    target_compile_definitions(gpu PRIVATE -DUSE_CUDPP)
cmake/Modules/Packages/GPU.cmake:  add_executable(nvc_get_devices ${LAMMPS_LIB_SOURCE_DIR}/gpu/geryon/ucl_get_devices.cpp)
cmake/Modules/Packages/GPU.cmake:  target_compile_definitions(nvc_get_devices PRIVATE -DUCL_CUDADR)
cmake/Modules/Packages/GPU.cmake:  target_link_libraries(nvc_get_devices PRIVATE ${CUDA_LIBRARIES} ${CUDA_CUDA_LIBRARY})
cmake/Modules/Packages/GPU.cmake:  target_include_directories(nvc_get_devices PRIVATE ${CUDA_INCLUDE_DIRS})
cmake/Modules/Packages/GPU.cmake:elseif(GPU_API STREQUAL "OPENCL")
cmake/Modules/Packages/GPU.cmake:  # the static OpenCL loader doesn't seem to work on macOS. use the system provided
cmake/Modules/Packages/GPU.cmake:    set(_opencl_static_default OFF)
cmake/Modules/Packages/GPU.cmake:    set(_opencl_static_default ON)
cmake/Modules/Packages/GPU.cmake:  option(USE_STATIC_OPENCL_LOADER "Download and include a static OpenCL ICD loader" ${_opencl_static_default})
cmake/Modules/Packages/GPU.cmake:  mark_as_advanced(USE_STATIC_OPENCL_LOADER)
cmake/Modules/Packages/GPU.cmake:  if(USE_STATIC_OPENCL_LOADER)
cmake/Modules/Packages/GPU.cmake:    include(OpenCLLoader)
cmake/Modules/Packages/GPU.cmake:    find_package(OpenCL REQUIRED)
cmake/Modules/Packages/GPU.cmake:  include(OpenCLUtils)
cmake/Modules/Packages/GPU.cmake:  set(OCL_COMMON_HEADERS ${LAMMPS_LIB_SOURCE_DIR}/gpu/lal_preprocessor.h ${LAMMPS_LIB_SOURCE_DIR}/gpu/lal_aux_fun1.h)
cmake/Modules/Packages/GPU.cmake:  file(GLOB GPU_LIB_CU CONFIGURE_DEPENDS ${LAMMPS_LIB_SOURCE_DIR}/gpu/[^.]*.cu)
cmake/Modules/Packages/GPU.cmake:  list(REMOVE_ITEM GPU_LIB_CU
cmake/Modules/Packages/GPU.cmake:    ${LAMMPS_LIB_SOURCE_DIR}/gpu/lal_gayberne.cu
cmake/Modules/Packages/GPU.cmake:    ${LAMMPS_LIB_SOURCE_DIR}/gpu/lal_gayberne_lj.cu
cmake/Modules/Packages/GPU.cmake:    ${LAMMPS_LIB_SOURCE_DIR}/gpu/lal_re_squared.cu
cmake/Modules/Packages/GPU.cmake:    ${LAMMPS_LIB_SOURCE_DIR}/gpu/lal_re_squared_lj.cu
cmake/Modules/Packages/GPU.cmake:    ${LAMMPS_LIB_SOURCE_DIR}/gpu/lal_tersoff.cu
cmake/Modules/Packages/GPU.cmake:    ${LAMMPS_LIB_SOURCE_DIR}/gpu/lal_tersoff_zbl.cu
cmake/Modules/Packages/GPU.cmake:    ${LAMMPS_LIB_SOURCE_DIR}/gpu/lal_tersoff_mod.cu
cmake/Modules/Packages/GPU.cmake:    ${LAMMPS_LIB_SOURCE_DIR}/gpu/lal_hippo.cu
cmake/Modules/Packages/GPU.cmake:  foreach(GPU_KERNEL ${GPU_LIB_CU})
cmake/Modules/Packages/GPU.cmake:      get_filename_component(basename ${GPU_KERNEL} NAME_WE)
cmake/Modules/Packages/GPU.cmake:      GenerateOpenCLHeader(${KERNEL_NAME} ${CMAKE_CURRENT_BINARY_DIR}/gpu/${KERNEL_NAME}_cl.h ${OCL_COMMON_HEADERS} ${GPU_KERNEL})
cmake/Modules/Packages/GPU.cmake:      list(APPEND GPU_LIB_SOURCES ${CMAKE_CURRENT_BINARY_DIR}/gpu/${KERNEL_NAME}_cl.h)
cmake/Modules/Packages/GPU.cmake:  GenerateOpenCLHeader(gayberne ${CMAKE_CURRENT_BINARY_DIR}/gpu/gayberne_cl.h ${OCL_COMMON_HEADERS} ${LAMMPS_LIB_SOURCE_DIR}/gpu/lal_ellipsoid_extra.h ${LAMMPS_LIB_SOURCE_DIR}/gpu/lal_gayberne.cu)
cmake/Modules/Packages/GPU.cmake:  GenerateOpenCLHeader(gayberne_lj ${CMAKE_CURRENT_BINARY_DIR}/gpu/gayberne_lj_cl.h ${OCL_COMMON_HEADERS} ${LAMMPS_LIB_SOURCE_DIR}/gpu/lal_ellipsoid_extra.h ${LAMMPS_LIB_SOURCE_DIR}/gpu/lal_gayberne_lj.cu)
cmake/Modules/Packages/GPU.cmake:  GenerateOpenCLHeader(re_squared ${CMAKE_CURRENT_BINARY_DIR}/gpu/re_squared_cl.h ${OCL_COMMON_HEADERS} ${LAMMPS_LIB_SOURCE_DIR}/gpu/lal_ellipsoid_extra.h ${LAMMPS_LIB_SOURCE_DIR}/gpu/lal_re_squared.cu)
cmake/Modules/Packages/GPU.cmake:  GenerateOpenCLHeader(re_squared_lj ${CMAKE_CURRENT_BINARY_DIR}/gpu/re_squared_lj_cl.h ${OCL_COMMON_HEADERS} ${LAMMPS_LIB_SOURCE_DIR}/gpu/lal_ellipsoid_extra.h ${LAMMPS_LIB_SOURCE_DIR}/gpu/lal_re_squared_lj.cu)
cmake/Modules/Packages/GPU.cmake:  GenerateOpenCLHeader(tersoff ${CMAKE_CURRENT_BINARY_DIR}/gpu/tersoff_cl.h ${OCL_COMMON_HEADERS} ${LAMMPS_LIB_SOURCE_DIR}/gpu/lal_tersoff_extra.h ${LAMMPS_LIB_SOURCE_DIR}/gpu/lal_tersoff.cu)
cmake/Modules/Packages/GPU.cmake:  GenerateOpenCLHeader(tersoff_zbl ${CMAKE_CURRENT_BINARY_DIR}/gpu/tersoff_zbl_cl.h ${OCL_COMMON_HEADERS} ${LAMMPS_LIB_SOURCE_DIR}/gpu/lal_tersoff_zbl_extra.h ${LAMMPS_LIB_SOURCE_DIR}/gpu/lal_tersoff_zbl.cu)
cmake/Modules/Packages/GPU.cmake:  GenerateOpenCLHeader(tersoff_mod ${CMAKE_CURRENT_BINARY_DIR}/gpu/tersoff_mod_cl.h ${OCL_COMMON_HEADERS} ${LAMMPS_LIB_SOURCE_DIR}/gpu/lal_tersoff_mod_extra.h ${LAMMPS_LIB_SOURCE_DIR}/gpu/lal_tersoff_mod.cu)
cmake/Modules/Packages/GPU.cmake:  GenerateOpenCLHeader(hippo ${CMAKE_CURRENT_BINARY_DIR}/gpu/hippo_cl.h ${OCL_COMMON_HEADERS} ${LAMMPS_LIB_SOURCE_DIR}/gpu/lal_hippo_extra.h ${LAMMPS_LIB_SOURCE_DIR}/gpu/lal_hippo.cu)
cmake/Modules/Packages/GPU.cmake:  list(APPEND GPU_LIB_SOURCES
cmake/Modules/Packages/GPU.cmake:    ${CMAKE_CURRENT_BINARY_DIR}/gpu/gayberne_cl.h
cmake/Modules/Packages/GPU.cmake:    ${CMAKE_CURRENT_BINARY_DIR}/gpu/gayberne_lj_cl.h
cmake/Modules/Packages/GPU.cmake:    ${CMAKE_CURRENT_BINARY_DIR}/gpu/re_squared_cl.h
cmake/Modules/Packages/GPU.cmake:    ${CMAKE_CURRENT_BINARY_DIR}/gpu/re_squared_lj_cl.h
cmake/Modules/Packages/GPU.cmake:    ${CMAKE_CURRENT_BINARY_DIR}/gpu/tersoff_cl.h
cmake/Modules/Packages/GPU.cmake:    ${CMAKE_CURRENT_BINARY_DIR}/gpu/tersoff_zbl_cl.h
cmake/Modules/Packages/GPU.cmake:    ${CMAKE_CURRENT_BINARY_DIR}/gpu/tersoff_mod_cl.h
cmake/Modules/Packages/GPU.cmake:    ${CMAKE_CURRENT_BINARY_DIR}/gpu/hippo_cl.h
cmake/Modules/Packages/GPU.cmake:  add_library(gpu STATIC ${GPU_LIB_SOURCES})
cmake/Modules/Packages/GPU.cmake:  target_link_libraries(gpu PRIVATE OpenCL::OpenCL)
cmake/Modules/Packages/GPU.cmake:  target_include_directories(gpu PRIVATE ${CMAKE_CURRENT_BINARY_DIR}/gpu)
cmake/Modules/Packages/GPU.cmake:  target_compile_definitions(gpu PRIVATE -DUSE_OPENCL -D_${GPU_PREC_SETTING})
cmake/Modules/Packages/GPU.cmake:  if(GPU_DEBUG)
cmake/Modules/Packages/GPU.cmake:    target_compile_definitions(gpu PRIVATE -DUCL_DEBUG -DGERYON_KERNEL_DUMP)
cmake/Modules/Packages/GPU.cmake:    target_compile_definitions(gpu PRIVATE -DMPI_GERYON -DGERYON_NUMA_FISSION -DUCL_NO_EXIT)
cmake/Modules/Packages/GPU.cmake:  add_executable(ocl_get_devices ${LAMMPS_LIB_SOURCE_DIR}/gpu/geryon/ucl_get_devices.cpp)
cmake/Modules/Packages/GPU.cmake:  target_compile_definitions(ocl_get_devices PRIVATE -DUCL_OPENCL)
cmake/Modules/Packages/GPU.cmake:  target_link_libraries(ocl_get_devices PRIVATE OpenCL::OpenCL)
cmake/Modules/Packages/GPU.cmake:  add_dependencies(ocl_get_devices OpenCL::OpenCL)
cmake/Modules/Packages/GPU.cmake:elseif(GPU_API STREQUAL "HIP")
cmake/Modules/Packages/GPU.cmake:  option(HIP_USE_DEVICE_SORT "Use GPU sorting" ON)
cmake/Modules/Packages/GPU.cmake:    find_package(CUDA REQUIRED)
cmake/Modules/Packages/GPU.cmake:    set(HIP_ARCH "sm_50" CACHE STRING "HIP primary CUDA architecture (e.g. sm_60)")
cmake/Modules/Packages/GPU.cmake:    if(CUDA_VERSION VERSION_LESS 8.0)
cmake/Modules/Packages/GPU.cmake:      message(FATAL_ERROR "CUDA Toolkit version 8.0 or later is required")
cmake/Modules/Packages/GPU.cmake:    elseif(CUDA_VERSION VERSION_GREATER_EQUAL "12.0")
cmake/Modules/Packages/GPU.cmake:      message(WARNING "Untested CUDA Toolkit version ${CUDA_VERSION}. Use at your own risk")
cmake/Modules/Packages/GPU.cmake:      set(HIP_CUDA_GENCODE "-arch=all")
cmake/Modules/Packages/GPU.cmake:      # build arch/gencode commands for nvcc based on CUDA toolkit version and use choice
cmake/Modules/Packages/GPU.cmake:      # comparison chart according to: https://en.wikipedia.org/wiki/CUDA#GPUs_supported
cmake/Modules/Packages/GPU.cmake:      set(HIP_CUDA_GENCODE "-arch=${HIP_ARCH}")
cmake/Modules/Packages/GPU.cmake:      # Kepler (GPU Arch 3.0) is supported by CUDA 5 to CUDA 10.2
cmake/Modules/Packages/GPU.cmake:      if((CUDA_VERSION VERSION_GREATER_EQUAL "5.0") AND (CUDA_VERSION VERSION_LESS "11.0"))
cmake/Modules/Packages/GPU.cmake:        string(APPEND HIP_CUDA_GENCODE " -gencode arch=compute_30,code=[sm_30,compute_30]")
cmake/Modules/Packages/GPU.cmake:      # Kepler (GPU Arch 3.5) is supported by CUDA 5 to CUDA 11.0
cmake/Modules/Packages/GPU.cmake:      if((CUDA_VERSION VERSION_GREATER_EQUAL "5.0") AND (CUDA_VERSION VERSION_LESS "12.0"))
cmake/Modules/Packages/GPU.cmake:        string(APPEND HIP_CUDA_GENCODE " -gencode arch=compute_35,code=[sm_35,compute_35]")
cmake/Modules/Packages/GPU.cmake:      # Maxwell (GPU Arch 5.x) is supported by CUDA 6 and later
cmake/Modules/Packages/GPU.cmake:      if(CUDA_VERSION VERSION_GREATER_EQUAL "6.0")
cmake/Modules/Packages/GPU.cmake:        string(APPEND HIP_CUDA_GENCODE " -gencode arch=compute_50,code=[sm_50,compute_50] -gencode arch=compute_52,code=[sm_52,compute_52]")
cmake/Modules/Packages/GPU.cmake:      # Pascal (GPU Arch 6.x) is supported by CUDA 8 and later
cmake/Modules/Packages/GPU.cmake:      if(CUDA_VERSION VERSION_GREATER_EQUAL "8.0")
cmake/Modules/Packages/GPU.cmake:        string(APPEND HIP_CUDA_GENCODE " -gencode arch=compute_60,code=[sm_60,compute_60] -gencode arch=compute_61,code=[sm_61,compute_61]")
cmake/Modules/Packages/GPU.cmake:      # Volta (GPU Arch 7.0) is supported by CUDA 9 and later
cmake/Modules/Packages/GPU.cmake:      if(CUDA_VERSION VERSION_GREATER_EQUAL "9.0")
cmake/Modules/Packages/GPU.cmake:        string(APPEND HIP_CUDA_GENCODE " -gencode arch=compute_70,code=[sm_70,compute_70]")
cmake/Modules/Packages/GPU.cmake:      # Turing (GPU Arch 7.5) is supported by CUDA 10 and later
cmake/Modules/Packages/GPU.cmake:      if(CUDA_VERSION VERSION_GREATER_EQUAL "10.0")
cmake/Modules/Packages/GPU.cmake:        string(APPEND HIP_CUDA_GENCODE " -gencode arch=compute_75,code=[sm_75,compute_75]")
cmake/Modules/Packages/GPU.cmake:      # Ampere (GPU Arch 8.0) is supported by CUDA 11 and later
cmake/Modules/Packages/GPU.cmake:      if(CUDA_VERSION VERSION_GREATER_EQUAL "11.0")
cmake/Modules/Packages/GPU.cmake:        string(APPEND HIP_CUDA_GENCODE " -gencode arch=compute_80,code=[sm_80,compute_80]")
cmake/Modules/Packages/GPU.cmake:      # Ampere (GPU Arch 8.6) is supported by CUDA 11.1 and later
cmake/Modules/Packages/GPU.cmake:      if(CUDA_VERSION VERSION_GREATER_EQUAL "11.1")
cmake/Modules/Packages/GPU.cmake:        string(APPEND HIP_CUDA_GENCODE " -gencode arch=compute_86,code=[sm_86,compute_86]")
cmake/Modules/Packages/GPU.cmake:      # Lovelace (GPU Arch 8.9) is supported by CUDA 11.8 and later
cmake/Modules/Packages/GPU.cmake:      if(CUDA_VERSION VERSION_GREATER_EQUAL "11.8")
cmake/Modules/Packages/GPU.cmake:        string(APPEND HIP_CUDA_GENCODE " -gencode arch=compute_90,code=[sm_90,compute_90]")
cmake/Modules/Packages/GPU.cmake:      # Hopper (GPU Arch 9.0) is supported by CUDA 12.0 and later
cmake/Modules/Packages/GPU.cmake:      if(CUDA_VERSION VERSION_GREATER_EQUAL "12.0")
cmake/Modules/Packages/GPU.cmake:        string(APPEND HIP_CUDA_GENCODE " -gencode arch=compute_90,code=[sm_90,compute_90]")
cmake/Modules/Packages/GPU.cmake:  file(GLOB GPU_LIB_CU CONFIGURE_DEPENDS ${LAMMPS_LIB_SOURCE_DIR}/gpu/[^.]*.cu ${CMAKE_CURRENT_SOURCE_DIR}/gpu/[^.]*.cu)
cmake/Modules/Packages/GPU.cmake:  list(REMOVE_ITEM GPU_LIB_CU ${LAMMPS_LIB_SOURCE_DIR}/gpu/lal_pppm.cu)
cmake/Modules/Packages/GPU.cmake:  set(GPU_LIB_CU_HIP "")
cmake/Modules/Packages/GPU.cmake:  foreach(CU_FILE ${GPU_LIB_CU})
cmake/Modules/Packages/GPU.cmake:    set(CU_CPP_FILE  "${LAMMPS_LIB_BINARY_DIR}/gpu/${CU_NAME}.cu.cpp")
cmake/Modules/Packages/GPU.cmake:    set(CUBIN_FILE   "${LAMMPS_LIB_BINARY_DIR}/gpu/${CU_NAME}.cubin")
cmake/Modules/Packages/GPU.cmake:    set(CUBIN_H_FILE "${LAMMPS_LIB_BINARY_DIR}/gpu/${CU_NAME}_cubin.h")
cmake/Modules/Packages/GPU.cmake:            VERBATIM COMMAND ${HIP_HIPCC_EXECUTABLE} --genco --offload-arch=${HIP_ARCH} -O3 -DUSE_HIP -D_${GPU_PREC_SETTING} -DLAMMPS_${LAMMPS_SIZES} -I${LAMMPS_LIB_SOURCE_DIR}/gpu -o ${CUBIN_FILE} ${CU_CPP_FILE}
cmake/Modules/Packages/GPU.cmake:            VERBATIM COMMAND ${HIP_HIPCC_EXECUTABLE} --genco -t="${HIP_ARCH}" -f=\"-O3 -DUSE_HIP -D_${GPU_PREC_SETTING} -DLAMMPS_${LAMMPS_SIZES} -I${LAMMPS_LIB_SOURCE_DIR}/gpu\" -o ${CUBIN_FILE} ${CU_CPP_FILE}
cmake/Modules/Packages/GPU.cmake:          VERBATIM COMMAND ${HIP_HIPCC_EXECUTABLE} --fatbin --use_fast_math -DUSE_HIP -D_${GPU_PREC_SETTING} -DLAMMPS_${LAMMPS_SIZES} ${HIP_CUDA_GENCODE} -I${LAMMPS_LIB_SOURCE_DIR}/gpu -o ${CUBIN_FILE} ${CU_FILE}
cmake/Modules/Packages/GPU.cmake:        VERBATIM COMMAND ${HIP_HIPCC_EXECUTABLE} -c -O3 -DUSE_HIP -D_${GPU_PREC_SETTING} -DLAMMPS_${LAMMPS_SIZES} -I${LAMMPS_LIB_SOURCE_DIR}/gpu -o ${CUBIN_FILE} ${CU_CPP_FILE}
cmake/Modules/Packages/GPU.cmake:    list(APPEND GPU_LIB_SOURCES ${CUBIN_H_FILE})
cmake/Modules/Packages/GPU.cmake:  set_directory_properties(PROPERTIES ADDITIONAL_MAKE_CLEAN_FILES "${LAMMPS_LIB_BINARY_DIR}/gpu/*_cubin.h ${LAMMPS_LIB_BINARY_DIR}/gpu/*.cu.cpp")
cmake/Modules/Packages/GPU.cmake:  add_library(gpu STATIC ${GPU_LIB_SOURCES})
cmake/Modules/Packages/GPU.cmake:  target_include_directories(gpu PRIVATE ${LAMMPS_LIB_BINARY_DIR}/gpu)
cmake/Modules/Packages/GPU.cmake:  target_compile_definitions(gpu PRIVATE -DUSE_HIP -D_${GPU_PREC_SETTING})
cmake/Modules/Packages/GPU.cmake:  if(GPU_DEBUG)
cmake/Modules/Packages/GPU.cmake:    target_compile_definitions(gpu PRIVATE -DUCL_DEBUG -DGERYON_KERNEL_DUMP)
cmake/Modules/Packages/GPU.cmake:    target_compile_definitions(gpu PRIVATE -DMPI_GERYON -DUCL_NO_EXIT)
cmake/Modules/Packages/GPU.cmake:  target_link_libraries(gpu PRIVATE hip::host)
cmake/Modules/Packages/GPU.cmake:      # newer version of ROCm (5.1+) require c++14 for rocprim
cmake/Modules/Packages/GPU.cmake:      set_property(TARGET gpu PROPERTY CXX_STANDARD 14)
cmake/Modules/Packages/GPU.cmake:    target_link_libraries(gpu PRIVATE hip::hipcub)
cmake/Modules/Packages/GPU.cmake:    target_compile_definitions(gpu PRIVATE -DUSE_HIP_DEVICE_SORT)
cmake/Modules/Packages/GPU.cmake:        set(CUB_URL "https://github.com/nvidia/cub/archive/1.12.0.tar.gz" CACHE STRING "URL for CUB tarball")
cmake/Modules/Packages/GPU.cmake:      target_include_directories(gpu PRIVATE ${CUB_INCLUDE_DIR})
cmake/Modules/Packages/GPU.cmake:  add_executable(hip_get_devices ${LAMMPS_LIB_SOURCE_DIR}/gpu/geryon/ucl_get_devices.cpp)
cmake/Modules/Packages/GPU.cmake:    target_compile_definitions(gpu PRIVATE -D__HIP_PLATFORM_NVCC__)
cmake/Modules/Packages/GPU.cmake:    target_include_directories(gpu PRIVATE ${CUDA_INCLUDE_DIRS})
cmake/Modules/Packages/GPU.cmake:    target_link_libraries(gpu PRIVATE ${CUDA_LIBRARIES} ${CUDA_CUDA_LIBRARY})
cmake/Modules/Packages/GPU.cmake:    target_include_directories(hip_get_devices PRIVATE ${CUDA_INCLUDE_DIRS})
cmake/Modules/Packages/GPU.cmake:    target_link_libraries(hip_get_devices PRIVATE ${CUDA_LIBRARIES} ${CUDA_CUDA_LIBRARY})
cmake/Modules/Packages/GPU.cmake:  target_link_libraries(gpu PRIVATE OpenMP::OpenMP_CXX)
cmake/Modules/Packages/GPU.cmake:target_link_libraries(lammps PRIVATE gpu)
cmake/Modules/Packages/GPU.cmake:set_property(GLOBAL PROPERTY "GPU_SOURCES" "${GPU_SOURCES}")
cmake/Modules/Packages/GPU.cmake:# detect styles which have a GPU version
cmake/Modules/Packages/GPU.cmake:RegisterStylesExt(${GPU_SOURCES_DIR} gpu GPU_SOURCES)
cmake/Modules/Packages/GPU.cmake:RegisterFixStyle(${GPU_SOURCES_DIR}/fix_gpu.h)
cmake/Modules/Packages/GPU.cmake:get_property(GPU_SOURCES GLOBAL PROPERTY GPU_SOURCES)
cmake/Modules/Packages/GPU.cmake:  target_link_libraries(gpu PRIVATE MPI::MPI_CXX)
cmake/Modules/Packages/GPU.cmake:  target_link_libraries(gpu PRIVATE mpi_stubs)
cmake/Modules/Packages/GPU.cmake:target_compile_definitions(gpu PRIVATE -DLAMMPS_${LAMMPS_SIZES})
cmake/Modules/Packages/GPU.cmake:set_target_properties(gpu PROPERTIES OUTPUT_NAME lammps_gpu${LAMMPS_MACHINE})
cmake/Modules/Packages/GPU.cmake:target_sources(lammps PRIVATE ${GPU_SOURCES})
cmake/Modules/Packages/GPU.cmake:target_include_directories(lammps PRIVATE ${GPU_SOURCES_DIR})
cmake/Modules/Packages/KOKKOS.cmake:if(Kokkos_ENABLE_CUDA)
cmake/Modules/Packages/KOKKOS.cmake:  option(Kokkos_ENABLE_IMPL_CUDA_MALLOC_ASYNC "CUDA asynchronous malloc support" OFF)
cmake/Modules/Packages/KOKKOS.cmake:  mark_as_advanced(Kokkos_ENABLE_IMPL_CUDA_MALLOC_ASYNC)
cmake/Modules/Packages/KOKKOS.cmake:  if(Kokkos_ENABLE_IMPL_CUDA_MALLOC_ASYNC)
cmake/Modules/Packages/KOKKOS.cmake:    message(STATUS "KOKKOS: CUDA malloc async support enabled")
cmake/Modules/Packages/KOKKOS.cmake:    message(STATUS "KOKKOS: CUDA malloc async support disabled")
cmake/Modules/Packages/KOKKOS.cmake:    Kokkos_ENABLE_CUDA OR Kokkos_ENABLE_HIP OR Kokkos_ENABLE_SYCL
cmake/Modules/Packages/KOKKOS.cmake:  set(FFT_KOKKOS_VALUES KISS FFTW3 MKL NVPL HIPFFT CUFFT MKL_GPU)
cmake/Modules/Packages/KOKKOS.cmake:  if(Kokkos_ENABLE_CUDA)
cmake/Modules/Packages/KOKKOS.cmake:      message(FATAL_ERROR "The CUDA backend of Kokkos requires either KISS FFT or CUFFT.")
cmake/Modules/Packages/KOKKOS.cmake:      message(WARNING "Using KISS FFT with the CUDA backend of Kokkos may be sub-optimal.")
cmake/Modules/Packages/KOKKOS.cmake:      find_package(CUDAToolkit REQUIRED)
cmake/Modules/Packages/KOKKOS.cmake:      target_link_libraries(lammps PRIVATE CUDA::cufft)
cmake/Modules/Packages/KOKKOS.cmake:  elseif(FFT_KOKKOS STREQUAL "MKL_GPU")
cmake/Modules/Packages/KOKKOS.cmake:      message(FATAL_ERROR "Using MKL_GPU FFT currently requires the SYCL backend of Kokkos.")
cmake/Modules/Packages/MACHDYN.cmake:# PGI/Nvidia compiler internals collide with vector intrinsics support in Eigen3
cmake/Modules/Packages/MACHDYN.cmake:target_compile_definitions(lammps PRIVATE -DEIGEN_NO_CUDA)
cmake/Modules/GenerateOpenCLHeader.cmake:# utility script to call WriteOpenCLHeader function
cmake/Modules/GenerateOpenCLHeader.cmake:include(${SOURCE_DIR}/Modules/OpenCLUtils.cmake)
cmake/Modules/GenerateOpenCLHeader.cmake:WriteOpenCLHeader(${VARNAME} ${HEADER_FILE} ${SOURCE_FILES})
cmake/Modules/DetectHIPInstallation.cmake:if(NOT DEFINED ROCM_PATH)
cmake/Modules/DetectHIPInstallation.cmake:    if(NOT DEFINED ENV{ROCM_PATH})
cmake/Modules/DetectHIPInstallation.cmake:        set(ROCM_PATH "/opt/rocm" CACHE PATH "Path to ROCm installation")
cmake/Modules/DetectHIPInstallation.cmake:        set(ROCM_PATH $ENV{ROCM_PATH} CACHE PATH "Path to ROCm installation")
cmake/Modules/DetectHIPInstallation.cmake:list(APPEND CMAKE_PREFIX_PATH ${ROCM_PATH})
examples/mliap/jax/README.md:        python -m pip install cupy-cuda12x
examples/mliap/jax/README.md:2. Install JAX for GPU/CUDA:
examples/mliap/jax/README.md:        python -m pip install --trusted-host storage.googleapis.com --upgrade "jax[cuda12_local]" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html
examples/mliap/jax/README.md:3. Install cudNN: https://developer.nvidia.com/cudnn
examples/mliap/jax/README.md:                  -DKokkos_ENABLE_CUDA=yes \
examples/mliap/jax/write_unified.py:        :param model_device: the device to send torch data to (cpu or cuda)
examples/mliap/jax/mliap_unified_jax_kokkos.py:# Does not fix GPU problem with larger num. atoms.
examples/python/log.18Mar22.pair_python_harmonic.opencl.1:package gpu 0
examples/python/log.18Mar22.pair_python_harmonic.opencl.1:- GPU package (short-range, long-range and three-body potentials):
examples/python/log.18Mar22.pair_python_harmonic.opencl.1: title = {GPU-accelerated Tersoff potentials for massively parallel Molecular Dynamics simulations},
examples/python/log.18Mar22.pair_python_harmonic.opencl.1: title = {GPU acceleration of four-site water models in LAMMPS},
examples/python/log.18Mar22.pair_python_harmonic.opencl.1:package gpu 0
examples/python/log.18Mar22.pair_python_harmonic.opencl.1:package gpu 0
examples/python/log.18Mar22.pair_python_harmonic.opencl.1:package gpu 0
examples/QUANTUM/LATTE/latte.in.uo2:  NGPU= 2
examples/COUPLE/plugin/liblammpsplugin.h:  int (*has_gpu_device)();
examples/COUPLE/plugin/liblammpsplugin.h:  void (*get_gpu_device_info)(char *, int);
examples/COUPLE/plugin/liblammpsplugin.c:  ADDSYM(has_gpu_device);
examples/COUPLE/plugin/liblammpsplugin.c:  ADDSYM(get_gpu_device_info);
examples/PACKAGES/fep/ta/log.spce:package gpu 0
examples/PACKAGES/fep/ta/log.spce:- GPU package (short-range, long-range and three-body potentials):
examples/PACKAGES/fep/ta/log.spce: title = {GPU-accelerated Tersoff potentials for massively parallel Molecular Dynamics simulations},
examples/PACKAGES/fep/ta/log.spce: title = {GPU acceleration of four-site water models in LAMMPS},
examples/PACKAGES/imd/in.bucky-plus-cnt-gpu:pair_style  lj/cut/gpu  10.0
examples/PACKAGES/imd/in.bucky-plus-cnt-gpu:# required for GPU acceleration
examples/PACKAGES/imd/in.bucky-plus-cnt-gpu:fix   gpu  all      gpu  force 0 0 1.0
examples/PACKAGES/imd/in.melt_imd-gpu:pair_style	lj/cut/gpu 2.5
examples/PACKAGES/imd/in.melt_imd-gpu:fix     0 all gpu force/neigh 0 0 1.0
examples/PACKAGES/imd/in.deca-ala_imd-gpu:pair_style      lj/charmm/coul/long/gpu 8 10
examples/PACKAGES/imd/in.deca-ala_imd-gpu:fix             0 all gpu force/neigh 0 0 1.0    
examples/PACKAGES/imd/in.deca-ala_imd-gpu:kspace_style    pppm/gpu 1e-5
examples/PACKAGES/pod/README.md:    cmake -C ../cmake/presets/basic.cmake -C ../cmake/presets/kokkos-cuda.cmake -D PKG_ML-POD=on ../cmake
examples/wall/log.7Feb24.wall.flow.g++.4: title = {GPU-based molecular dynamics of fluid flows: Reaching for turbulence},
examples/wall/log.7Feb24.wall.flow.g++.1: title = {GPU-based molecular dynamics of fluid flows: Reaching for turbulence},
examples/README:GPU.
src/modify.cpp:    {"GPU", "OMP", "INTEL", "property/atom", "cmap", "cmap3", "rx", "deprecated", "STORE/KIM",
src/modify.cpp:     GPU, INTEL, OPENMP packages, which have their own fixes
src/KOKKOS/pair_eam_kokkos.cpp:  // The rho array is duplicated for OpenMP, atomic for GPU, and neither for Serial
src/KOKKOS/pair_eam_kokkos.cpp:  // The f array is duplicated for OpenMP, atomic for GPU, and neither for Serial
src/KOKKOS/pair_eam_kokkos.cpp:  // The f array is duplicated for OpenMP, atomic for GPU, and neither for Serial
src/KOKKOS/pair_eam_kokkos.cpp:  // The eatom and vatom arrays are duplicated for OpenMP, atomic for GPU, and neither for Serial
src/KOKKOS/pair_eam_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/fix_nph_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/compute_coord_atom_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_table_rx_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_exp6_rx_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_exp6_rx_kokkos.cpp:#ifdef LMP_KOKKOS_GPU  // Use atomics
src/KOKKOS/pair_exp6_rx_kokkos.cpp:#ifndef LMP_KOKKOS_GPU
src/KOKKOS/pair_exp6_rx_kokkos.cpp:#ifndef LMP_KOKKOS_GPU
src/KOKKOS/pair_exp6_rx_kokkos.cpp:#ifndef LMP_KOKKOS_GPU
src/KOKKOS/pair_exp6_rx_kokkos.cpp:#ifndef LMP_KOKKOS_GPU
src/KOKKOS/pair_exp6_rx_kokkos.cpp:#ifndef LMP_KOKKOS_GPU
src/KOKKOS/pair_exp6_rx_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/compute_reaxff_atom_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_lj_expand_coul_long_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/angle_harmonic_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/compute_erotate_sphere_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/comm_kokkos.cpp:      if (lmp->kokkos->gpu_aware_flag) {
src/KOKKOS/comm_kokkos.cpp:      if (!lmp->kokkos->gpu_aware_flag) {
src/KOKKOS/comm_kokkos.cpp:      if (lmp->kokkos->gpu_aware_flag) {
src/KOKKOS/comm_kokkos.cpp:      if (!lmp->kokkos->gpu_aware_flag) {
src/KOKKOS/comm_kokkos.cpp:    if (lmp->kokkos->gpu_aware_flag) {
src/KOKKOS/comm_kokkos.cpp:      if (!lmp->kokkos->gpu_aware_flag) {
src/KOKKOS/angle_charmm_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/mliap_model_linear_kokkos.h:   Contributing author: Matt Bettencourt (NVIDIA)
src/KOKKOS/fix_npt_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_kokkos.h:    if (fpair->lmp->kokkos->ngpus && inum <= 16000)
src/KOKKOS/pair_kokkos.h:#if defined(LMP_KOKKOS_GPU)
src/KOKKOS/fix_spring_self_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_mliap_kokkos.cpp:   Contributing author: Matt Bettencourt (NVIDIA)
src/KOKKOS/pair_mliap_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_yukawa_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/npair_trim_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/angle_cosine_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/meam_funcs_kokkos.h:   Contributing author: Naga Vydyanathan (NVIDIA)
src/KOKKOS/pair_dpd_ext_tstat_kokkos.cpp:   Contributing author: Matt Bettencourt (NVIDIA)
src/KOKKOS/pair_dpd_ext_tstat_kokkos.cpp:  // The f array is duplicated for OpenMP, atomic for GPU, and neither for Serial
src/KOKKOS/pair_dpd_ext_tstat_kokkos.cpp:  // The vatom array is duplicated for OpenMP, atomic for GPU, and neither for Serial
src/KOKKOS/pair_dpd_ext_tstat_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_snap_kokkos.h:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_snap_kokkos.h:// Routines for both the CPU and GPU backend
src/KOKKOS/pair_snap_kokkos.h:// GPU backend only
src/KOKKOS/pair_snap_kokkos.h:  // GPU backend only
src/KOKKOS/pair_snap_kokkos.h:  Kokkos::View<real_type***, Kokkos::LayoutLeft, DeviceType> d_beta_pack;          // betas for all atoms in list, GPU
src/KOKKOS/pair_snap_kokkos.h:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_lj_cut_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/dihedral_opls_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_sw_kokkos.cpp:  // The f array is duplicated for OpenMP, atomic for GPU, and neither for Serial
src/KOKKOS/pair_sw_kokkos.cpp:  // The eatom and vatom arrays are duplicated for OpenMP, atomic for GPU, and neither for Serial
src/KOKKOS/pair_sw_kokkos.cpp:  // The eatom and vatom arrays are duplicated for OpenMP, atomic for GPU, and neither for Serial
src/KOKKOS/pair_sw_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_yukawa_colloid_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_lj_charmm_coul_charmm_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/mliap_model_linear_kokkos.cpp:   Contributing author: Matt Bettencourt (NVIDIA)
src/KOKKOS/mliap_model_linear_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/fix_rx_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_meam_kokkos.cpp:   Contributing authors: Naga Vydyanathan (NVIDIA), Stan Moore (SNL)
src/KOKKOS/pair_meam_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/kokkos_type.h:#if defined(KOKKOS_ENABLE_CUDA) || defined(KOKKOS_ENABLE_HIP) || defined(KOKKOS_ENABLE_SYCL) || defined(KOKKOS_ENABLE_OPENMPTARGET)
src/KOKKOS/kokkos_type.h:#define LMP_KOKKOS_GPU
src/KOKKOS/kokkos_type.h:#if defined(LMP_KOKKOS_GPU)
src/KOKKOS/kokkos_type.h:#define KOKKOS_GPU_ARG(x) x
src/KOKKOS/kokkos_type.h:#define KOKKOS_GPU_ARG(x)
src/KOKKOS/kokkos_type.h:#if ((defined(KOKKOS_ENABLE_CUDA) && defined(KOKKOS_ENABLE_CUDA_UVM)) || \
src/KOKKOS/kokkos_type.h:#ifdef KOKKOS_ENABLE_CUDA
src/KOKKOS/kokkos_type.h:struct ExecutionSpaceFromDevice<Kokkos::Cuda> {
src/KOKKOS/kokkos_type.h:#if defined(KOKKOS_ENABLE_CUDA)
src/KOKKOS/kokkos_type.h:typedef Kokkos::CudaHostPinnedSpace LMPPinnedHostType;
src/KOKKOS/kokkos_type.h:// create simple LMPDeviceSpace typedef for non CUDA-, HIP-, or SYCL-specific
src/KOKKOS/kokkos_type.h:#if defined(KOKKOS_ENABLE_CUDA)
src/KOKKOS/kokkos_type.h:typedef Kokkos::Cuda LMPDeviceSpace;
src/KOKKOS/kokkos_type.h:// Do atomic trait when running HALFTHREAD neighbor list style with CUDA
src/KOKKOS/kokkos_type.h:#ifdef KOKKOS_ENABLE_CUDA
src/KOKKOS/kokkos_type.h:struct AtomicDup<HALFTHREAD,Kokkos::Cuda> {
src/KOKKOS/kokkos_type.h:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/kokkos_type.h:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/kokkos_type.h:#if defined(KOKKOS_ENABLE_CUDA) || defined(KOKKOS_ENABLE_HIP)
src/KOKKOS/kokkos_type.h:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/kokkos_type.h:#if defined(__CUDA_ARCH__) || defined(__HIP_DEVICE_COMPILE__) || defined(__SYCL_DEVICE_ONLY__)
src/KOKKOS/pair_uf3_kokkos.cpp:  // The f array is duplicated for OpenMP, atomic for CUDA, and neither for Serial
src/KOKKOS/pair_uf3_kokkos.cpp:  // The eatom and vatom arrays are duplicated for OpenMP, atomic for CUDA,
src/KOKKOS/pair_uf3_kokkos.cpp:  // The eatom and vatom arrays are duplicated for OpenMP, atomic for CUDA,
src/KOKKOS/pair_uf3_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/fix_nve_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/domain_kokkos.cpp:   // reduce GPU data movement
src/KOKKOS/dihedral_charmmfsw_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/mliap_model_python_couple_kokkos.pyx:        mem = cupy.cuda.UnownedMemory(ptr=int( <uintptr_t> pointer), owner=None, size=size)
src/KOKKOS/mliap_model_python_couple_kokkos.pyx:        memptr = cupy.cuda.MemoryPointer(mem, 0)
src/KOKKOS/mliap_model_python_couple_kokkos.pyx:    torch.cuda.nvtx.range_push("set data fields")
src/KOKKOS/mliap_model_python_couple_kokkos.pyx:    torch.cuda.nvtx.range_pop()
src/KOKKOS/mliap_model_python_couple_kokkos.pyx:    torch.cuda.nvtx.range_push("call model")
src/KOKKOS/mliap_model_python_couple_kokkos.pyx:    torch.cuda.nvtx.range_pop()
src/KOKKOS/mliap_data_kokkos.cpp:   Contributing author: Matt Bettencourt (NVIDIA)
src/KOKKOS/mliap_data_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/kokkos.cpp:int KokkosLMP::init_ngpus = 0;
src/KOKKOS/kokkos.cpp:  ngpus = 0;
src/KOKKOS/kokkos.cpp:               strcmp(arg[iarg],"gpus") == 0) {
src/KOKKOS/kokkos.cpp:      ngpus = utils::inumeric(FLERR, arg[iarg+1], false, lmp);
src/KOKKOS/kokkos.cpp:#ifndef LMP_KOKKOS_GPU
src/KOKKOS/kokkos.cpp:      if (ngpus > 0)
src/KOKKOS/kokkos.cpp:        error->all(FLERR,"GPUs are requested but Kokkos has not been compiled using a GPU-enabled backend");
src/KOKKOS/kokkos.cpp:      int skip_gpu = 9999;
src/KOKKOS/kokkos.cpp:        skip_gpu = utils::inumeric(FLERR, arg[iarg+2], false, lmp);
src/KOKKOS/kokkos.cpp:        device = local_rank % ngpus;
src/KOKKOS/kokkos.cpp:        if (device >= skip_gpu) device++;
src/KOKKOS/kokkos.cpp:        if (ngpus > 0) {
src/KOKKOS/kokkos.cpp:          device = local_rank % ngpus;
src/KOKKOS/kokkos.cpp:          if (device >= skip_gpu) device++;
src/KOKKOS/kokkos.cpp:        if (ngpus > 0) {
src/KOKKOS/kokkos.cpp:          device = local_rank % ngpus;
src/KOKKOS/kokkos.cpp:          if (device >= skip_gpu) device++;
src/KOKKOS/kokkos.cpp:        if (ngpus > 0) {
src/KOKKOS/kokkos.cpp:          device = local_rank % ngpus;
src/KOKKOS/kokkos.cpp:          if (device >= skip_gpu) device++;
src/KOKKOS/kokkos.cpp:        if (ngpus > 0) {
src/KOKKOS/kokkos.cpp:          device = local_rank % ngpus;
src/KOKKOS/kokkos.cpp:          if (device >= skip_gpu) device++;
src/KOKKOS/kokkos.cpp:        if (ngpus > 0) {
src/KOKKOS/kokkos.cpp:          device = local_rank % ngpus;
src/KOKKOS/kokkos.cpp:          if (device >= skip_gpu) device++;
src/KOKKOS/kokkos.cpp:        if (ngpus > 0) {
src/KOKKOS/kokkos.cpp:          device = local_rank % ngpus;
src/KOKKOS/kokkos.cpp:          if (device >= skip_gpu) device++;
src/KOKKOS/kokkos.cpp:      if (ngpus > 1 && !set_flag)
src/KOKKOS/kokkos.cpp:                           "GPUs with Kokkos because MPI library not recognized");
src/KOKKOS/kokkos.cpp:    ngpus = init_ngpus;
src/KOKKOS/kokkos.cpp:    init_ngpus = ngpus;
src/KOKKOS/kokkos.cpp:  if ((me == 0) && (ngpus > 0))
src/KOKKOS/kokkos.cpp:    utils::logmesg(lmp, "  will use up to {} GPU(s) per node\n", ngpus);
src/KOKKOS/kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/kokkos.cpp:  if (ngpus <= 0)
src/KOKKOS/kokkos.cpp:    error->all(FLERR,"Kokkos has been compiled with GPU-enabled backend but no GPUs are requested");
src/KOKKOS/kokkos.cpp:#if defined(LMP_KOKKOS_GPU)
src/KOKKOS/kokkos.cpp:  gpu_aware_flag = 1;
src/KOKKOS/kokkos.cpp:  gpu_aware_flag = 0;
src/KOKKOS/kokkos.cpp:  if (ngpus > 0) {
src/KOKKOS/kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/kokkos.cpp:  // check and warn about GPU-aware MPI availability when using multiple MPI tasks
src/KOKKOS/kokkos.cpp:  // change default only if we can detect that GPU-aware MPI is not available
src/KOKKOS/kokkos.cpp:    // for detecting GPU-aware MPI support:
src/KOKKOS/kokkos.cpp:    // the variable int have_gpu_aware
src/KOKKOS/kokkos.cpp:    // - is  1 if GPU-aware MPI support is available
src/KOKKOS/kokkos.cpp:    // - is  0 if GPU-aware MPI support is unavailable
src/KOKKOS/kokkos.cpp:    // - is -1 if GPU-aware MPI support is unknown
src/KOKKOS/kokkos.cpp:    int have_gpu_aware = -1;
src/KOKKOS/kokkos.cpp:#if defined(KOKKOS_ENABLE_CUDA)
src/KOKKOS/kokkos.cpp:#if defined(OMPI_HAVE_MPI_EXT_CUDA) && OMPI_HAVE_MPI_EXT_CUDA
src/KOKKOS/kokkos.cpp:    have_gpu_aware = MPIX_Query_cuda_support();
src/KOKKOS/kokkos.cpp:#if defined(OMPI_HAVE_MPI_EXT_ROCM) && OMPI_HAVE_MPI_EXT_ROCM
src/KOKKOS/kokkos.cpp:    have_gpu_aware = MPIX_Query_rocm_support();
src/KOKKOS/kokkos.cpp:    have_gpu_aware = 0;
src/KOKKOS/kokkos.cpp:    have_gpu_aware = 0;
src/KOKKOS/kokkos.cpp:    if (gpu_aware_flag == 1 && have_gpu_aware == 0) {
src/KOKKOS/kokkos.cpp:        error->warning(FLERR,"Turning off GPU-aware MPI since it is not detected, "
src/KOKKOS/kokkos.cpp:                       "use '-pk kokkos gpu/aware on' to override");
src/KOKKOS/kokkos.cpp:      gpu_aware_flag = 0;
src/KOKKOS/kokkos.cpp:      have_gpu_aware = 0;
src/KOKKOS/kokkos.cpp:      if ((str = getenv("OMPI_MCA_pml_pami_enable_cuda")))
src/KOKKOS/kokkos.cpp:          have_gpu_aware = 1;
src/KOKKOS/kokkos.cpp:      if (!have_gpu_aware) {
src/KOKKOS/kokkos.cpp:          error->warning(FLERR,"The Spectrum MPI '-gpu' flag is not set. Disabling GPU-aware MPI");
src/KOKKOS/kokkos.cpp:        gpu_aware_flag = 0;
src/KOKKOS/kokkos.cpp:    if (have_gpu_aware == -1) {
src/KOKKOS/kokkos.cpp:      have_gpu_aware = 0;
src/KOKKOS/kokkos.cpp:      if ((str = getenv("MV2_USE_CUDA")))
src/KOKKOS/kokkos.cpp:          have_gpu_aware = 1;
src/KOKKOS/kokkos.cpp:      if (!have_gpu_aware) {
src/KOKKOS/kokkos.cpp:          error->warning(FLERR,"MVAPICH2 'MV2_USE_CUDA' environment variable is not set. Disabling GPU-aware MPI");
src/KOKKOS/kokkos.cpp:        gpu_aware_flag = 0;
src/KOKKOS/kokkos.cpp:      // check for Cray MPICH which has GPU-aware support
src/KOKKOS/kokkos.cpp:      have_gpu_aware = 0;
src/KOKKOS/kokkos.cpp:      if ((str = getenv("MPICH_GPU_SUPPORT_ENABLED")))
src/KOKKOS/kokkos.cpp:          have_gpu_aware = 1;
src/KOKKOS/kokkos.cpp:      if (!have_gpu_aware) {
src/KOKKOS/kokkos.cpp:          error->warning(FLERR,"Detected MPICH. Disabling GPU-aware MPI");
src/KOKKOS/kokkos.cpp:        gpu_aware_flag = 0;
src/KOKKOS/kokkos.cpp:        error->warning(FLERR,"Kokkos with GPU-enabled backend assumes GPU-aware MPI is available,"
src/KOKKOS/kokkos.cpp:                       " '-pk kokkos gpu/aware off' if getting segmentation faults");
src/KOKKOS/kokkos.cpp:#endif // LMP_KOKKOS_GPU
src/KOKKOS/kokkos.cpp:        if (nthreads > 1 || ngpus > 0)
src/KOKKOS/kokkos.cpp:        if (nthreads > 1 || ngpus > 0)
src/KOKKOS/kokkos.cpp:    } else if ((strcmp(arg[iarg],"gpu/aware") == 0)
src/KOKKOS/kokkos.cpp:               || (strcmp(arg[iarg],"cuda/aware") == 0)) {
src/KOKKOS/kokkos.cpp:      gpu_aware_flag = utils::logical(FLERR,arg[iarg+1],false,lmp);
src/KOKKOS/kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/kokkos.cpp:  // if "gpu/aware off" or "pair/only on", and "comm device", change to "comm no"
src/KOKKOS/kokkos.cpp:  if ((!gpu_aware_flag && nmpi > 1) || lmp->pair_only_flag) {
src/KOKKOS/kokkos.cpp:  // if "gpu/aware on" and "pair/only off", and comm flags were changed previously, change them back
src/KOKKOS/kokkos.cpp:  if (gpu_aware_flag && !lmp->pair_only_flag) {
src/KOKKOS/npair_kokkos.h:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/npair_kokkos.h:  void build_ItemGPU(typename Kokkos::TeamPolicy<DeviceType>::member_type dev,
src/KOKKOS/npair_kokkos.h:  void build_ItemGhostGPU(typename Kokkos::TeamPolicy<DeviceType>::member_type dev,
src/KOKKOS/npair_kokkos.h:  void build_ItemSizeGPU(typename Kokkos::TeamPolicy<DeviceType>::member_type dev,
src/KOKKOS/npair_kokkos.h:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/npair_kokkos.h:    c.template build_ItemGPU<HALF_NEIGH,NEWTON,TRI>(dev, sharedsize);
src/KOKKOS/npair_kokkos.h:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/npair_kokkos.h:    c.template build_ItemGhostGPU<HALF_NEIGH>(dev, sharedsize);
src/KOKKOS/npair_kokkos.h:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/npair_kokkos.h:    c.template build_ItemSizeGPU<HALF_NEIGH,NEWTON,TRI>(dev, sharedsize);
src/KOKKOS/pair_tersoff_mod_kokkos.cpp:// on CUDA due to the way Kokkos handles cache carveout preferences. This is
src/KOKKOS/pair_tersoff_mod_kokkos.cpp:  // The f array is duplicated for OpenMP, atomic for GPU, and neither for Serial
src/KOKKOS/pair_tersoff_mod_kokkos.cpp:  // The eatom and vatom arrays are duplicated for OpenMP, atomic for GPU, and neither for Serial
src/KOKKOS/pair_tersoff_mod_kokkos.cpp:  // The vatom array is duplicated for OpenMP, atomic for GPU, and neither for Serial
src/KOKKOS/pair_tersoff_mod_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/angle_class2_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/npair_skip_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/meam_force_kokkos.h:  // The f, etc. arrays are duplicated for OpenMP, atomic for GPU, and neither for Serial
src/KOKKOS/min_linesearch_kokkos.h: //protected: // won't compile with CUDA
src/KOKKOS/fix_qeq_reaxff_kokkos.cpp:                          Kamesh Arumugam (NVIDIA)
src/KOKKOS/fix_qeq_reaxff_kokkos.cpp:         parallelism on GPUs
src/KOKKOS/fix_qeq_reaxff_kokkos.cpp:  } else { // GPU, use teams
src/KOKKOS/fix_qeq_reaxff_kokkos.cpp:    // The q array is duplicated for OpenMP, atomic for GPU, and neither for Serial
src/KOKKOS/fix_qeq_reaxff_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/bond_fene_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/fft3d_kokkos.h:#if defined(FFT_KOKKOS_MKL_GPU)
src/KOKKOS/fft3d_kokkos.h:#if defined(FFT_KOKKOS_MKL_GPU)
src/KOKKOS/rand_pool_wrap_kokkos.h:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/rand_pool_wrap_kokkos.h:    error->all(FLERR,"Cannot use Marsaglia RNG with GPUs");
src/KOKKOS/mliap_unified_couple_kokkos.pyx:        mem = cupy.cuda.UnownedMemory(ptr=int( <uintptr_t> pointer), owner=None, size=size)
src/KOKKOS/mliap_unified_couple_kokkos.pyx:        memptr = cupy.cuda.MemoryPointer(mem, 0)
src/KOKKOS/mliap_unified_couple_kokkos.pyx:    def update_pair_energy_gpu(self, eij):
src/KOKKOS/mliap_unified_couple_kokkos.pyx:            self.update_pair_energy_gpu(eij)
src/KOKKOS/mliap_unified_couple_kokkos.pyx:    def update_pair_forces_gpu(self, fij):
src/KOKKOS/mliap_unified_couple_kokkos.pyx:            self.update_pair_forces_gpu(fij)
src/KOKKOS/mliap_unified_couple_kokkos.pyx:        print("This code has not been tested or optimized for the GPU, if you are getting this warning optimize gradforce")
src/KOKKOS/mliap_unified_couple_kokkos.pyx:        print("This code has not been tested or optimized for the GPU, if you are getting this warning optimize ")
src/KOKKOS/mliap_unified_couple_kokkos.pyx:        print("This code has not been tested or optimized for the GPU, if you are getting this warning optimize descriptors")
src/KOKKOS/mliap_descriptor_so3_kokkos.cpp:   Contributing author: Matt Bettencourt (NVIDIA)
src/KOKKOS/mliap_descriptor_so3_kokkos.cpp:  error->all(FLERR,"This has not been tested in cuda/kokkos");
src/KOKKOS/mliap_descriptor_so3_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_reaxff_kokkos.cpp:     Evan Weinberg (NVIDIA)
src/KOKKOS/pair_reaxff_kokkos.cpp:         reduce thread divergence on GPUs
src/KOKKOS/pair_reaxff_kokkos.cpp:  if (execution_space != Host) // GPU
src/KOKKOS/pair_reaxff_kokkos.cpp:  // The f array is duplicated for OpenMP, atomic for GPU, and neither for Serial
src/KOKKOS/pair_reaxff_kokkos.cpp:  // The f array is duplicated for OpenMP, atomic for GPU, and neither for Serial
src/KOKKOS/pair_reaxff_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_reaxff_kokkos.cpp:  // The eatom and vatom arrays are duplicated for OpenMP, atomic for GPU, and neither for Serial
src/KOKKOS/pair_reaxff_kokkos.cpp:  // The eatom array is duplicated for OpenMP, atomic for GPU, and neither for Serial
src/KOKKOS/pair_reaxff_kokkos.cpp:  // The eatom array is duplicated for OpenMP, atomic for GPU, and neither for Serial
src/KOKKOS/pair_reaxff_kokkos.cpp:  // The eatom and vatom arrays are duplicated for OpenMP, atomic for GPU, and neither for Serial
src/KOKKOS/pair_reaxff_kokkos.cpp:  // The vatom array is duplicated for OpenMP, atomic for GPU, and neither for Serial
src/KOKKOS/pair_reaxff_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_zbl_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/npair_halffull_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/meam_impl_kokkos.h:   Contributing author: Naga Vydyanathan (NVIDIA), Stan Moore (SNL)
src/KOKKOS/angle_spica_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_coul_cut_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_hybrid_kokkos.h:  friend class FixGPU;
src/KOKKOS/sna_kokkos.h:  // functions for bispectrum coefficients, GPU only
src/KOKKOS/sna_kokkos.h:  // functions for derivatives, GPU only
src/KOKKOS/sna_kokkos.h:  // Modified structures for GPU backend
src/KOKKOS/pair_multi_lucy_rx_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_lj_spica_coul_long_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/improper_class2_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_snap_kokkos_impl.h:                         Evan Weinberg (NVIDIA)
src/KOKKOS/pair_snap_kokkos_impl.h:    } else { // GPU
src/KOKKOS/pair_snap_kokkos_impl.h:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_snap_kokkos_impl.h:#endif // LMP_KOKKOS_GPU
src/KOKKOS/pair_snap_kokkos_impl.h:   Begin routines that are unique to the GPU codepath. These take advantage
src/KOKKOS/pair_snap_kokkos_impl.h:   different arithmetic intensity requirements for the CPU vs GPU.
src/KOKKOS/pair_snap_kokkos_impl.h:   Also used for both CPU and GPU codepaths. Could maybe benefit from a
src/KOKKOS/pair_snap_kokkos_impl.h:   separate GPU/CPU codepath, but this kernel takes so little time it's
src/KOKKOS/pair_snap_kokkos_impl.h:  // The f array is duplicated for OpenMP, atomic for GPU, and neither for Serial
src/KOKKOS/pair_snap_kokkos_impl.h:  // The vatom array is duplicated for OpenMP, atomic for GPU, and neither for Serial
src/KOKKOS/pair_snap_kokkos_impl.h:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_lj_charmm_coul_long_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/fix_qeq_reaxff_kokkos.h:#if defined(KOKKOS_ENABLE_CUDA)
src/KOKKOS/fix_dt_reset_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_tersoff_kokkos.cpp:// on CUDA due to the way Kokkos handles cache carveout preferences. This is
src/KOKKOS/pair_tersoff_kokkos.cpp:  // The f array is duplicated for OpenMP, atomic for GPU, and neither for Serial
src/KOKKOS/pair_tersoff_kokkos.cpp:  // The eatom and vatom arrays are duplicated for OpenMP, atomic for GPU, and neither for Serial
src/KOKKOS/pair_tersoff_kokkos.cpp:  // The vatom array is duplicated for OpenMP, atomic for GPU, and neither for Serial
src/KOKKOS/pair_tersoff_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_lj_class2_coul_cut_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/transpose_helper_kokkos.h:   Contributing author: Evan Weinberg (NVIDIA)
src/KOKKOS/pair_lj_charmmfsw_coul_long_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_adp_kokkos.cpp:  // The rho array is duplicated for OpenMP, atomic for GPU, and neither for Serial
src/KOKKOS/pair_adp_kokkos.cpp:  // The f array is duplicated for OpenMP, atomic for GPU, and neither for Serial
src/KOKKOS/pair_adp_kokkos.cpp:  // The eatom and vatom arrays are duplicated for OpenMP, atomic for GPU, and neither for Serial
src/KOKKOS/pair_adp_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/mliap_model_python_kokkos.h:   Contributing author: Matt Bettencourt (NVIDIA)
src/KOKKOS/kokkos.h:  int nthreads,ngpus;
src/KOKKOS/kokkos.h:  int gpu_aware_flag;
src/KOKKOS/kokkos.h:  static int init_ngpus;
src/KOKKOS/mliap_unified_kokkos.cpp:   Contributing author: Matt Bettencourt (NVIDIA)
src/KOKKOS/mliap_unified_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/improper_harmonic_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_soft_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/region_block_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_dpd_ext_kokkos.cpp:   Contributing author: Matt Bettencourt (NVIDIA)
src/KOKKOS/pair_dpd_ext_kokkos.cpp:  // The f array is duplicated for OpenMP, atomic for GPU, and neither for Serial
src/KOKKOS/pair_dpd_ext_kokkos.cpp:  // The eatom and vatom arrays are duplicated for OpenMP, atomic for GPU, and neither for Serial
src/KOKKOS/pair_dpd_ext_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/fftdata_kokkos.h:// with KOKKOS in CUDA, HIP, or SYCL mode we can only have
src/KOKKOS/fftdata_kokkos.h://  CUFFT/HIPFFT/MKL_GPU or KISS, thus undefine all other
src/KOKKOS/fftdata_kokkos.h:#ifdef KOKKOS_ENABLE_CUDA
src/KOKKOS/fftdata_kokkos.h:# if !defined(FFT_KOKKOS_MKL_GPU) && !defined(FFT_KOKKOS_KISS)
src/KOKKOS/fftdata_kokkos.h:#  error "Must enable CUDA with KOKKOS to use -DFFT_KOKKOS_CUFFT"
src/KOKKOS/fftdata_kokkos.h:# if defined(FFT_KOKKOS_MKL_GPU)
src/KOKKOS/fftdata_kokkos.h:#  error "Must enable SYCL with KOKKOS to use -DFFT_KOKKOS_MKL_GPU"
src/KOKKOS/fftdata_kokkos.h:#elif defined(FFT_KOKKOS_MKL_GPU)
src/KOKKOS/fftdata_kokkos.h:#define LMP_FFT_KOKKOS_LIB "MKL_GPU FFT"
src/KOKKOS/fftdata_kokkos.h:#if defined(FFT_KOKKOS_MKL_GPU)
src/KOKKOS/fftdata_kokkos.h:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_tersoff_zbl_kokkos.cpp:// on CUDA due to the way Kokkos handles cache carveout preferences. This is
src/KOKKOS/pair_tersoff_zbl_kokkos.cpp:  // The f array is duplicated for OpenMP, atomic for GPU, and neither for Serial
src/KOKKOS/pair_tersoff_zbl_kokkos.cpp:  // The eatom and vatom arrays are duplicated for OpenMP, atomic for GPU, and neither for Serial
src/KOKKOS/pair_tersoff_zbl_kokkos.cpp:  // The vatom array is duplicated for OpenMP, atomic for GPU, and neither for Serial
src/KOKKOS/pair_tersoff_zbl_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/fix_viscous_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/mliap_data_kokkos.h:   Contributing author: Matt Bettencourt (NVIDIA)
src/KOKKOS/mliap_data_kokkos.h:#if defined(KOKKOS_ENABLE_CUDA)
src/KOKKOS/mliap_data_kokkos.h:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/fix_setforce_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_eam_fs_kokkos.h:// Cannot use virtual inheritance on the GPU
src/KOKKOS/compute_orientorder_atom_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_mliap_kokkos.h:   Contributing author: Matt Bettencourt (NVIDIA)
src/KOKKOS/dihedral_charmm_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/grid3d_kokkos.cpp:  if (lmp->kokkos->gpu_aware_flag) {
src/KOKKOS/grid3d_kokkos.cpp:      if (!lmp->kokkos->gpu_aware_flag) {
src/KOKKOS/grid3d_kokkos.cpp:      if (!lmp->kokkos->gpu_aware_flag) {
src/KOKKOS/grid3d_kokkos.cpp:  if (lmp->kokkos->gpu_aware_flag) {
src/KOKKOS/grid3d_kokkos.cpp:    if (!lmp->kokkos->gpu_aware_flag) {
src/KOKKOS/grid3d_kokkos.cpp:    if (!lmp->kokkos->gpu_aware_flag) {
src/KOKKOS/grid3d_kokkos.cpp:  if (lmp->kokkos->gpu_aware_flag) {
src/KOKKOS/grid3d_kokkos.cpp:      if (!lmp->kokkos->gpu_aware_flag) {
src/KOKKOS/grid3d_kokkos.cpp:      if (!lmp->kokkos->gpu_aware_flag) {
src/KOKKOS/grid3d_kokkos.cpp:  if (lmp->kokkos->gpu_aware_flag) {
src/KOKKOS/grid3d_kokkos.cpp:    if (!lmp->kokkos->gpu_aware_flag) {
src/KOKKOS/grid3d_kokkos.cpp:    if (!lmp->kokkos->gpu_aware_flag) {
src/KOKKOS/grid3d_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/fix_efield_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_lj_cut_coul_dsf_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/fix_nve_sphere_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/npair_kokkos.cpp:  // use 1D view for scalars to reduce GPU memory operations
src/KOKKOS/npair_kokkos.cpp:#if defined(KOKKOS_ENABLE_CUDA) || defined(KOKKOS_ENABLE_HIP)
src/KOKKOS/npair_kokkos.cpp://#ifdef LMP_KOKKOS_GPU
src/KOKKOS/npair_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/npair_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/npair_kokkos.cpp:// ROCm versions < 3.7 are missing __syncthreads_count, so we define a functional
src/KOKKOS/npair_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/npair_kokkos.cpp:void NeighborKokkosExecute<DeviceType>::build_ItemGPU(typename Kokkos::TeamPolicy<DeviceType>::member_type dev,
src/KOKKOS/npair_kokkos.cpp:#if defined(KOKKOS_ENABLE_CUDA) || defined(KOKKOS_ENABLE_HIP)
src/KOKKOS/npair_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/npair_kokkos.cpp:void NeighborKokkosExecute<DeviceType>::build_ItemGhostGPU(typename Kokkos::TeamPolicy<DeviceType>::member_type dev,
src/KOKKOS/npair_kokkos.cpp:#if defined(KOKKOS_ENABLE_CUDA) || defined(KOKKOS_ENABLE_HIP)
src/KOKKOS/npair_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/npair_kokkos.cpp:void NeighborKokkosExecute<DeviceType>::build_ItemSizeGPU(typename Kokkos::TeamPolicy<DeviceType>::member_type dev,
src/KOKKOS/npair_kokkos.cpp:#if defined(KOKKOS_ENABLE_CUDA) || defined(KOKKOS_ENABLE_HIP)
src/KOKKOS/npair_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/neigh_bond_kokkos.cpp:  // use 1D view for scalars to reduce GPU memory operations
src/KOKKOS/neigh_bond_kokkos.cpp:   normally built with pair lists, but CUDA separates them
src/KOKKOS/neigh_bond_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/fix_freeze_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/dihedral_harmonic_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_coul_dsf_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/mliap_so3_kokkos.h:   Contributing author: Matt Bettencourt (NVIDIA)
src/KOKKOS/fix_shake_kokkos.cpp:  // use 1D view for scalars to reduce GPU memory operations
src/KOKKOS/fix_shake_kokkos.cpp:    if (lmp->kokkos->nthreads > 1 || lmp->kokkos->ngpus > 0)
src/KOKKOS/fix_shake_kokkos.cpp:  // The f array is duplicated for OpenMP, atomic for GPU, and neither for Serial
src/KOKKOS/fix_shake_kokkos.cpp:  // The f array is duplicated for OpenMP, atomic for GPU, and neither for Serial
src/KOKKOS/fix_shake_kokkos.cpp:  // The f array is duplicated for OpenMP, atomic for GPU, and neither for Serial
src/KOKKOS/fix_shake_kokkos.cpp:  // The f array is duplicated for OpenMP, atomic for GPU, and neither for Serial
src/KOKKOS/fix_shake_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_pod_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_buck_coul_long_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/remap_kokkos.h:  int usegpu_aware;                 // use GPU-Aware MPI or not
src/KOKKOS/fft3d_kokkos.cpp:             int usegpu_aware) :
src/KOKKOS/fft3d_kokkos.cpp:#if defined(LMP_KOKKOS_GPU)
src/KOKKOS/fft3d_kokkos.cpp:  int ngpus = lmp->kokkos->ngpus;
src/KOKKOS/fft3d_kokkos.cpp:#if defined(FFT_KOKKOS_MKL_GPU)
src/KOKKOS/fft3d_kokkos.cpp:  if (ngpus > 0 && execution_space == Host)
src/KOKKOS/fft3d_kokkos.cpp:    lmp->error->all(FLERR,"Cannot use the MKL library with Kokkos on the host CPUs in a GPU build");
src/KOKKOS/fft3d_kokkos.cpp:  if (ngpus > 0 && execution_space == Device)
src/KOKKOS/fft3d_kokkos.cpp:    lmp->error->all(FLERR,"Cannot use the MKL library with Kokkos on GPUs");
src/KOKKOS/fft3d_kokkos.cpp:  if (ngpus > 0 && execution_space == Device)
src/KOKKOS/fft3d_kokkos.cpp:    lmp->error->all(FLERR,"Cannot use the FFTW library with Kokkos on GPUs");
src/KOKKOS/fft3d_kokkos.cpp:  if (ngpus > 0 && execution_space == Device)
src/KOKKOS/fft3d_kokkos.cpp:    lmp->error->all(FLERR,"Cannot use the NVPL FFT library with Kokkos on GPUs");
src/KOKKOS/fft3d_kokkos.cpp:  if (ngpus > 0 && execution_space == Host)
src/KOKKOS/fft3d_kokkos.cpp:  if (ngpus > 0 && execution_space == Host)
src/KOKKOS/fft3d_kokkos.cpp:  //  stack size on GPUs needs to be increased to prevent stack overflows
src/KOKKOS/fft3d_kokkos.cpp:  #if defined (KOKKOS_ENABLE_CUDA)
src/KOKKOS/fft3d_kokkos.cpp:    cudaDeviceGetLimit(&stack_size,cudaLimitStackSize);
src/KOKKOS/fft3d_kokkos.cpp:      cudaDeviceSetLimit(cudaLimitStackSize,2048);
src/KOKKOS/fft3d_kokkos.cpp:                            scaled,permute,nbuf,usecollective,nthreads,usegpu_aware);
src/KOKKOS/fft3d_kokkos.cpp:#if defined(FFT_KOKKOS_FFTW3) || defined(FFT_KOKKOS_CUFFT) || defined(FFT_KOKKOS_HIPFFT) || defined(FFT_KOKKOS_MKL_GPU) || defined(FFT_KOKKOS_NVPL)
src/KOKKOS/fft3d_kokkos.cpp:  #if defined(FFT_KOKKOS_MKL_GPU)
src/KOKKOS/fft3d_kokkos.cpp:  #if defined(FFT_KOKKOS_MKL_GPU)
src/KOKKOS/fft3d_kokkos.cpp:  #if defined(FFT_KOKKOS_MKL_GPU)
src/KOKKOS/fft3d_kokkos.cpp:   usegpu_aware         use GPU-Aware MPI or not
src/KOKKOS/fft3d_kokkos.cpp:       int nthreads, int usegpu_aware)
src/KOKKOS/fft3d_kokkos.cpp:                           usecollective,usegpu_aware);
src/KOKKOS/fft3d_kokkos.cpp:                           usecollective,usegpu_aware);
src/KOKKOS/fft3d_kokkos.cpp:                         usecollective,usegpu_aware);
src/KOKKOS/fft3d_kokkos.cpp:                           usecollective,usegpu_aware);
src/KOKKOS/fft3d_kokkos.cpp:#if defined(FFT_KOKKOS_MKL_GPU)
src/KOKKOS/fft3d_kokkos.cpp:#if defined(FFT_KOKKOS_MKL_GPU)
src/KOKKOS/fft3d_kokkos.cpp:#if defined(FFT_KOKKOS_MKL_GPU) || defined(FFT_KOKKOS_MKL) || defined(FFT_KOKKOS_FFTW3) || defined(FFT_KOKKOS_NVPL)
src/KOKKOS/fft3d_kokkos.cpp:#if defined(FFT_KOKKOS_MKL_GPU)
src/KOKKOS/fft3d_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_pace_kokkos.cpp:    error->all(FLERR,"Must use 'product' algorithm with pair pace/kk on the GPU");
src/KOKKOS/pair_pace_kokkos.cpp:  // The f array is duplicated for OpenMP, atomic for GPU, and neither for Serial
src/KOKKOS/pair_pace_kokkos.cpp:  // The vatom array is duplicated for OpenMP, atomic for GPU, and neither for Serial
src/KOKKOS/pair_pace_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/fix_wall_reflect_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/atom_vec_kokkos.h:  #ifdef LMP_KOKKOS_GPU
src/KOKKOS/npair_copy_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_eam_alloy_kokkos.cpp:// Cannot use virtual inheritance on the GPU, so must duplicate code
src/KOKKOS/pair_eam_alloy_kokkos.cpp:  // The rho array is duplicated for OpenMP, atomic for GPU, and neither for Serial
src/KOKKOS/pair_eam_alloy_kokkos.cpp:  // The f array is duplicated for OpenMP, atomic for GPU, and neither for Serial
src/KOKKOS/pair_eam_alloy_kokkos.cpp:  // The f array is duplicated for OpenMP, atomic for GPU, and neither for Serial
src/KOKKOS/pair_eam_alloy_kokkos.cpp:  // The eatom and vatom arrays are duplicated for OpenMP, atomic for GPU, and neither for Serial
src/KOKKOS/pair_eam_alloy_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_lj_class2_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/mliap_model_python_kokkos.cpp:   Contributing author: Matt Bettencourt (NVIDIA)
src/KOKKOS/mliap_model_python_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/mliap_so3_kokkos.cpp:   Contributing author: Matt Bettencourt (NVIDIA)
src/KOKKOS/mliap_so3_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/bond_harmonic_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/mliap_descriptor_so3_kokkos.h:   Contributing author: Matt Bettencourt (NVIDIA)
src/KOKKOS/fix_wall_flow_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_dpd_tstat_kokkos.cpp:   Contributing author: Matt Bettencourt (NVIDIA)
src/KOKKOS/pair_dpd_tstat_kokkos.cpp:  // The f array is duplicated for OpenMP, atomic for GPU, and neither for Serial
src/KOKKOS/pair_dpd_tstat_kokkos.cpp:  // The vatom array is duplicated for OpenMP, atomic for GPU, and neither for Serial
src/KOKKOS/pair_dpd_tstat_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/nbin_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/compute_temp_deform_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_coul_debye_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pppm_kokkos.cpp:#if defined (LMP_KOKKOS_GPU)
src/KOKKOS/pppm_kokkos.cpp:      error->warning(FLERR,"Using default KISS FFT with Kokkos GPU backends may give suboptimal performance");
src/KOKKOS/pppm_kokkos.cpp:  int gpu_aware_flag = lmp->kokkos->gpu_aware_flag;
src/KOKKOS/pppm_kokkos.cpp:                         0,0,&tmp,collective_flag,gpu_aware_flag);
src/KOKKOS/pppm_kokkos.cpp:                         0,0,&tmp,collective_flag,gpu_aware_flag);
src/KOKKOS/pppm_kokkos.cpp:                          1,0,0,FFT_PRECISION,collective_flag,gpu_aware_flag);
src/KOKKOS/pppm_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pppm_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/fix_nvt_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/fix_nh_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/compute_temp_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_table_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/dihedral_class2_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_lj_class2_coul_long_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/fix_eos_table_rx_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_lj_charmm_coul_charmm_implicit_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_meam_ms_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_lj_gromacs_coul_gromacs_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_dpd_kokkos.cpp:   Contributing author: Matt Bettencourt (NVIDIA)
src/KOKKOS/pair_dpd_kokkos.cpp:  // The f array is duplicated for OpenMP, atomic for GPU, and neither for Serial
src/KOKKOS/pair_dpd_kokkos.cpp:  // The eatom and vatom arrays are duplicated for OpenMP, atomic for GPU, and neither for Serial
src/KOKKOS/pair_dpd_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/fix_temp_berendsen_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_coul_long_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_snap_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/fix_nvt_sllod_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/nbin_ssa_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_buck_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/neigh_list_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/min_kokkos.h: //protected: // won't compile with CUDA
src/KOKKOS/fix_shardlow_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_lj_spica_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/bond_class2_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/meam_dens_init_kokkos.h:  // The rho0, etc. arrays are duplicated for OpenMP, atomic for GPU, and neither for Serial
src/KOKKOS/mliap_descriptor_kokkos.h:   Contributing author: Matt Bettencourt (NVIDIA)
src/KOKKOS/pair_lj_gromacs_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_lj_cut_coul_long_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/fix_wall_gran_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_eam_alloy_kokkos.h:// Cannot use virtual inheritance on the GPU
src/KOKKOS/npair_ssa_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_vashishta_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/comm_tiled_kokkos.cpp:        for (int jj = 0; jj < nprocmax[ii]; jj++)
src/KOKKOS/compute_ave_sphere_atom_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_dpd_fdt_energy_kokkos.cpp:#if (defined(KOKKOS_ENABLE_CUDA) && defined(__CUDACC__)) || defined(KOKKOS_ENABLE_HIP) || defined(KOKKOS_ENABLE_SYCL)
src/KOKKOS/pair_dpd_fdt_energy_kokkos.cpp:// CUDA specialization of init_style to properly call rand_pool.init()
src/KOKKOS/pair_dpd_fdt_energy_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/fix_temp_rescale_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/fix_acks2_reaxff_kokkos.cpp:  } else { // GPU, use teams
src/KOKKOS/fix_acks2_reaxff_kokkos.cpp:  } else { // GPU, use teams
src/KOKKOS/fix_acks2_reaxff_kokkos.cpp:  // The X_diag array is duplicated for OpenMP, atomic for GPU, and neither for Serial
src/KOKKOS/fix_acks2_reaxff_kokkos.cpp:  // The X_diag array is duplicated for OpenMP, atomic for GPU, and neither for Serial
src/KOKKOS/fix_acks2_reaxff_kokkos.cpp:  // The bb array is duplicated for OpenMP, atomic for GPU, and neither for Serial
src/KOKKOS/fix_acks2_reaxff_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_lj_cut_coul_cut_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_eam_fs_kokkos.cpp:// Cannot use virtual inheritance on the GPU, so must duplicate code
src/KOKKOS/pair_eam_fs_kokkos.cpp:  // The rho array is duplicated for OpenMP, atomic for GPU, and neither for Serial
src/KOKKOS/pair_eam_fs_kokkos.cpp:  // The f array is duplicated for OpenMP, atomic for GPU, and neither for Serial
src/KOKKOS/pair_eam_fs_kokkos.cpp:  // The f array is duplicated for OpenMP, atomic for GPU, and neither for Serial
src/KOKKOS/pair_eam_fs_kokkos.cpp:  // The eatom and vatom arrays are duplicated for OpenMP, atomic for GPU, and neither for Serial
src/KOKKOS/pair_eam_fs_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/fix_langevin_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/fix_momentum_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_coul_wolf_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/sna_kokkos_impl.h:  // this is the GPU scratch memory requirements
src/KOKKOS/sna_kokkos_impl.h:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/sna_kokkos_impl.h:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/sna_kokkos_impl.h: * GPU routines
src/KOKKOS/sna_kokkos_impl.h:   This routine better exploits parallelism than the GPU ComputeUi and
src/KOKKOS/sna_kokkos_impl.h:   accumulating to the total. GPU only.
src/KOKKOS/sna_kokkos_impl.h:   divergence. GPU version
src/KOKKOS/sna_kokkos_impl.h:   divergence. GPU version.
src/KOKKOS/sna_kokkos_impl.h:   divergence. GPU version.
src/KOKKOS/sna_kokkos_impl.h:   and accumulation into dEidRj. GPU only.
src/KOKKOS/sna_kokkos_impl.h:   shared memory layout for the GPU routines, which is efficient for
src/KOKKOS/sna_kokkos_impl.h:   information stored between layers via scratch memory on the GPU path
src/KOKKOS/sna_kokkos_impl.h:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/sna_kokkos_impl.h:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/mliap_model_kokkos.h:   Contributing author: Matt Bettencourt (NVIDIA)
src/KOKKOS/fix_dpd_energy_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_morse_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/fix_gravity_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/remap_kokkos.cpp:             int usegpu_aware) : Pointers(lmp)
src/KOKKOS/remap_kokkos.cpp:                              usegpu_aware);
src/KOKKOS/remap_kokkos.cpp:  // If not using GPU-aware MPI, mirror data to host
src/KOKKOS/remap_kokkos.cpp:  if (!plan->usegpu_aware) {
src/KOKKOS/remap_kokkos.cpp:      if (!plan->usegpu_aware)
src/KOKKOS/remap_kokkos.cpp:      if (!plan->usegpu_aware)
src/KOKKOS/remap_kokkos.cpp:      if (!plan->usegpu_aware)
src/KOKKOS/remap_kokkos.cpp:      if (!plan->usegpu_aware)
src/KOKKOS/remap_kokkos.cpp:   usegpu_aware         whether to use GPU-Aware MPI or not
src/KOKKOS/remap_kokkos.cpp:  int usecollective, int usegpu_aware)
src/KOKKOS/remap_kokkos.cpp:  plan->usegpu_aware = usegpu_aware;
src/KOKKOS/remap_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_lj_cut_dipole_cut_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_lj_expand_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_pace_extrapolation_kokkos.cpp:        h_ASI(mu,i,j) = A_as_inv(j,i); // transpose back for better performance on GPU
src/KOKKOS/pair_pace_extrapolation_kokkos.cpp:  // The f array is duplicated for OpenMP, atomic for GPU, and neither for Serial
src/KOKKOS/pair_pace_extrapolation_kokkos.cpp:  // The vatom array is duplicated for OpenMP, atomic for GPU, and neither for Serial
src/KOKKOS/pair_pace_extrapolation_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/README:hardware, such as a GPU, Intel Phi, or many-core chip.
src/KOKKOS/neighbor_kokkos.cpp:   topology lists also built if topoflag = 1, CUDA calls with topoflag = 0
src/KOKKOS/neighbor_kokkos.cpp:  if (!binsizeflag && lmp->kokkos->ngpus > 0) {
src/KOKKOS/neighbor_kokkos.cpp:   normally built with pair lists, but CUDA separates them
src/KOKKOS/fix_neigh_history_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_gran_hooke_history_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/fix_enforce2d_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_brownian_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/compute_composition_atom_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_buck_coul_cut_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/fix_wall_lj93_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/KOKKOS/pair_lj_cut_coul_debye_kokkos.cpp:#ifdef LMP_KOKKOS_GPU
src/comm_tiled.h:  int *nprocmax;                 // current max # of send procs per swap
src/comm_tiled.h:  int *nexchprocmax;    // current max # of exch procs for each dim
src/MAKE/OPTIONS/Makefile.oneapi:# oneapi = For Intel oneAPI builds with GPU package
src/MAKE/OPTIONS/Makefile.gpu:# gpu = GPU package, MPI with its default compiler
src/MAKE/OPTIONS/Makefile.hip:HIP_PATH ?= $(wildcard /opt/rocm/hip)
src/MAKE/OPTIONS/Makefile.kokkos_cuda_mpi:# kokkos_cuda_mpi = KOKKOS/CUDA package, MPICH or OpenMPI with nvcc compiler, Kepler GPU
src/MAKE/OPTIONS/Makefile.kokkos_cuda_mpi:CCFLAGS =	-g -O3 -DNDEBUG -Xcudafe --diag_suppress=unrecognized_pragma -Xcudafe --diag_suppress=128
src/MAKE/OPTIONS/Makefile.kokkos_cuda_mpi:KOKKOS_DEVICES = Cuda
src/MAKE/MACHINES/Makefile.frontier_kokkos:# frontier_kokkos = KOKKOS/HIP, AMD MI250X GPU and AMD EPYC 7A53 "Optimized 3rd Gen EPYC" CPU, Cray MPICH, hipcc compiler, hipFFT
src/MAKE/MACHINES/Makefile.aurora:# aurora = Intel Sapphire Rapids CPU, mpicxx compiler (compatible w/ GPU package)
src/MAKE/MACHINES/Makefile.aurora_kokkos:# aurora_kokkos = KOKKOS/SYCL, Intel Data Center Max (Ponte Vecchio) GPU, Intel Sapphire Rapids CPU, mpicxx compiler
src/MAKE/MACHINES/Makefile.aurora_kokkos:FFT_INC = -DFFT_KOKKOS_MKL_GPU -DFFT_SINGLE -I${MKL_ROOT}/include
src/MAKE/MACHINES/Makefile.summit_kokkos:# summit_kokkos = KOKKOS/CUDA, V100 GPU and Power9, IBM Spectrum MPI, nvcc compiler with gcc
src/MAKE/MACHINES/Makefile.summit_kokkos:CCFLAGS =	-g -O3 -DNDEBUG -Xcudafe --diag_suppress=unrecognized_pragma -Xcudafe --diag_suppress=128
src/MAKE/MACHINES/Makefile.summit_kokkos:KOKKOS_DEVICES = Cuda
src/MAKE/MACHINES/Makefile.perlmutter_kokkos:# perlmutter_kokkos = KOKKOS/CUDA, NVIDIA A100 GPU and AMD EPYC 7763 (Milan) CPU, Cray MPICH, nvcc compiler with gcc
src/MAKE/MACHINES/Makefile.perlmutter_kokkos:CCFLAGS =	-g -O3 -DNDEBUG -Xcudafe --diag_suppress=unrecognized_pragma -Xcudafe --diag_suppress=128
src/MAKE/MACHINES/Makefile.perlmutter_kokkos:KOKKOS_DEVICES = Cuda
src/MAKE/MACHINES/Makefile.perlmutter_kokkos:MPI_LIB = -L${MPICH_DIR}/lib -lmpi -L${CRAY_MPICH_ROOTDIR}/gtl/lib -lmpi_gtl_cuda
src/MAKE/MACHINES/Makefile.perlmutter_kokkos:FFT_LIB = ${CRAY_CUDATOOLKIT_POST_LINK_OPTS} -lcufft
src/MAKE/README:The KOKKOS/CUDA variants require use of the NVIDIA nvcc compiler as
src/MAKE/README:Makefile.gpu                 GPU package, using default MPI
src/MAKE/README:Makefile.kokkos_cuda_mpich   KOKKOS package with GPU support for MPICH
src/MAKE/README:Makefile.kokkos_cuda_openmpi KOKKOS package with GPU support for OpenMPI
src/GPU/pair_lj_cut_coul_long_gpu.cpp:#include "pair_lj_cut_coul_long_gpu.h"
src/GPU/pair_lj_cut_coul_long_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_lj_cut_coul_long_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_lj_cut_coul_long_gpu.cpp:int ljcl_gpu_init(const int ntypes, double **cutsq, double **host_lj1, double **host_lj2,
src/GPU/pair_lj_cut_coul_long_gpu.cpp:                  const double cell_size, int &gpu_mode, FILE *screen, double **host_cut_ljsq,
src/GPU/pair_lj_cut_coul_long_gpu.cpp:void ljcl_gpu_reinit(const int ntypes, double **cutsq, double **host_lj1, double **host_lj2,
src/GPU/pair_lj_cut_coul_long_gpu.cpp:void ljcl_gpu_clear();
src/GPU/pair_lj_cut_coul_long_gpu.cpp:int **ljcl_gpu_compute_n(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_lj_cut_coul_long_gpu.cpp:void ljcl_gpu_compute(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_lj_cut_coul_long_gpu.cpp:double ljcl_gpu_bytes();
src/GPU/pair_lj_cut_coul_long_gpu.cpp:PairLJCutCoulLongGPU::PairLJCutCoulLongGPU(LAMMPS *lmp) :
src/GPU/pair_lj_cut_coul_long_gpu.cpp:    PairLJCutCoulLong(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_lj_cut_coul_long_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_lj_cut_coul_long_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_lj_cut_coul_long_gpu.cpp:PairLJCutCoulLongGPU::~PairLJCutCoulLongGPU()
src/GPU/pair_lj_cut_coul_long_gpu.cpp:  ljcl_gpu_clear();
src/GPU/pair_lj_cut_coul_long_gpu.cpp:void PairLJCutCoulLongGPU::compute(int eflag, int vflag)
src/GPU/pair_lj_cut_coul_long_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_lj_cut_coul_long_gpu.cpp:    firstneigh = ljcl_gpu_compute_n(neighbor->ago, inum, nall, atom->x, atom->type, sublo, subhi,
src/GPU/pair_lj_cut_coul_long_gpu.cpp:    ljcl_gpu_compute(neighbor->ago, inum, nall, atom->x, atom->type, ilist, numneigh, firstneigh,
src/GPU/pair_lj_cut_coul_long_gpu.cpp:void PairLJCutCoulLongGPU::init_style()
src/GPU/pair_lj_cut_coul_long_gpu.cpp:  if (!atom->q_flag) error->all(FLERR, "Pair style lj/cut/coul/long/gpu requires atom attribute q");
src/GPU/pair_lj_cut_coul_long_gpu.cpp:      ljcl_gpu_init(atom->ntypes + 1, cutsq, lj1, lj2, lj3, lj4, offset, force->special_lj,
src/GPU/pair_lj_cut_coul_long_gpu.cpp:                    atom->nlocal, atom->nlocal + atom->nghost, mnf, maxspecial, cell_size, gpu_mode,
src/GPU/pair_lj_cut_coul_long_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_lj_cut_coul_long_gpu.cpp:  if (gpu_mode == GPU_FORCE) neighbor->add_request(this, NeighConst::REQ_FULL);
src/GPU/pair_lj_cut_coul_long_gpu.cpp:void PairLJCutCoulLongGPU::reinit()
src/GPU/pair_lj_cut_coul_long_gpu.cpp:  ljcl_gpu_reinit(atom->ntypes + 1, cutsq, lj1, lj2, lj3, lj4, offset, cut_ljsq);
src/GPU/pair_lj_cut_coul_long_gpu.cpp:double PairLJCutCoulLongGPU::memory_usage()
src/GPU/pair_lj_cut_coul_long_gpu.cpp:  return bytes + ljcl_gpu_bytes();
src/GPU/pair_lj_cut_coul_long_gpu.cpp:void PairLJCutCoulLongGPU::cpu_compute(int start, int inum, int eflag, int /* vflag */, int *ilist,
src/GPU/pair_lj_cut_coul_cut_soft_gpu.cpp:#include "pair_lj_cut_coul_cut_soft_gpu.h"
src/GPU/pair_lj_cut_coul_cut_soft_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_lj_cut_coul_cut_soft_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_lj_cut_coul_cut_soft_gpu.cpp:int ljcs_gpu_init(const int ntypes, double **cutsq, double **host_lj1, double **host_lj2,
src/GPU/pair_lj_cut_coul_cut_soft_gpu.cpp:                 const double cell_size, int &gpu_mode, FILE *screen, double **host_cut_ljsq,
src/GPU/pair_lj_cut_coul_cut_soft_gpu.cpp:void ljcs_gpu_clear();
src/GPU/pair_lj_cut_coul_cut_soft_gpu.cpp:int **ljcs_gpu_compute_n(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_lj_cut_coul_cut_soft_gpu.cpp:void ljcs_gpu_compute(const int ago, const int inum, const int nall, double **host_x, int *host_type,
src/GPU/pair_lj_cut_coul_cut_soft_gpu.cpp:double ljcs_gpu_bytes();
src/GPU/pair_lj_cut_coul_cut_soft_gpu.cpp:PairLJCutCoulCutSoftGPU::PairLJCutCoulCutSoftGPU(LAMMPS *lmp) :
src/GPU/pair_lj_cut_coul_cut_soft_gpu.cpp:    PairLJCutCoulCutSoft(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_lj_cut_coul_cut_soft_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_lj_cut_coul_cut_soft_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_lj_cut_coul_cut_soft_gpu.cpp:PairLJCutCoulCutSoftGPU::~PairLJCutCoulCutSoftGPU()
src/GPU/pair_lj_cut_coul_cut_soft_gpu.cpp:  ljcs_gpu_clear();
src/GPU/pair_lj_cut_coul_cut_soft_gpu.cpp:void PairLJCutCoulCutSoftGPU::compute(int eflag, int vflag)
src/GPU/pair_lj_cut_coul_cut_soft_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_lj_cut_coul_cut_soft_gpu.cpp:    firstneigh = ljcs_gpu_compute_n(neighbor->ago, inum, nall, atom->x, atom->type, sublo, subhi,
src/GPU/pair_lj_cut_coul_cut_soft_gpu.cpp:    ljcs_gpu_compute(neighbor->ago, inum, nall, atom->x, atom->type, ilist, numneigh, firstneigh,
src/GPU/pair_lj_cut_coul_cut_soft_gpu.cpp:void PairLJCutCoulCutSoftGPU::init_style()
src/GPU/pair_lj_cut_coul_cut_soft_gpu.cpp:  if (!atom->q_flag) error->all(FLERR, "Pair style lj/cut/coul/cut/soft/gpu requires atom attribute q");
src/GPU/pair_lj_cut_coul_cut_soft_gpu.cpp:      ljcs_gpu_init(atom->ntypes + 1, cutsq, lj1, lj2, lj3, lj4, offset, epsilon, force->special_lj,
src/GPU/pair_lj_cut_coul_cut_soft_gpu.cpp:                   atom->nlocal, atom->nlocal + atom->nghost, mnf, maxspecial, cell_size, gpu_mode,
src/GPU/pair_lj_cut_coul_cut_soft_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_lj_cut_coul_cut_soft_gpu.cpp:  if (gpu_mode == GPU_FORCE) neighbor->add_request(this, NeighConst::REQ_FULL);
src/GPU/pair_lj_cut_coul_cut_soft_gpu.cpp:double PairLJCutCoulCutSoftGPU::memory_usage()
src/GPU/pair_lj_cut_coul_cut_soft_gpu.cpp:  return bytes + ljcs_gpu_bytes();
src/GPU/pair_lj_cut_coul_cut_soft_gpu.cpp:void PairLJCutCoulCutSoftGPU::cpu_compute(int start, int inum, int eflag, int /* vflag */, int *ilist,
src/GPU/pair_coul_slater_long_gpu.cpp:#include "pair_coul_slater_long_gpu.h"
src/GPU/pair_coul_slater_long_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_coul_slater_long_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_coul_slater_long_gpu.cpp:int csl_gpu_init(const int ntypes, double **scale, const int nlocal, const int nall,
src/GPU/pair_coul_slater_long_gpu.cpp:                const int max_nbors, const int maxspecial, const double cell_size, int &gpu_mode,
src/GPU/pair_coul_slater_long_gpu.cpp:void csl_gpu_reinit(const int ntypes, double **scale);
src/GPU/pair_coul_slater_long_gpu.cpp:void csl_gpu_clear();
src/GPU/pair_coul_slater_long_gpu.cpp:int **csl_gpu_compute_n(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_coul_slater_long_gpu.cpp:void csl_gpu_compute(const int ago, const int inum, const int nall, double **host_x, int *host_type,
src/GPU/pair_coul_slater_long_gpu.cpp:double csl_gpu_bytes();
src/GPU/pair_coul_slater_long_gpu.cpp:PairCoulSlaterLongGPU::PairCoulSlaterLongGPU(LAMMPS *lmp) : PairCoulSlaterLong(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_coul_slater_long_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_coul_slater_long_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_coul_slater_long_gpu.cpp:PairCoulSlaterLongGPU::~PairCoulSlaterLongGPU()
src/GPU/pair_coul_slater_long_gpu.cpp:  csl_gpu_clear();
src/GPU/pair_coul_slater_long_gpu.cpp:void PairCoulSlaterLongGPU::compute(int eflag, int vflag)
src/GPU/pair_coul_slater_long_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_coul_slater_long_gpu.cpp:    firstneigh = csl_gpu_compute_n(neighbor->ago, inum, nall, atom->x, atom->type, sublo, subhi,
src/GPU/pair_coul_slater_long_gpu.cpp:    csl_gpu_compute(neighbor->ago, inum, nall, atom->x, atom->type, ilist, numneigh, firstneigh,
src/GPU/pair_coul_slater_long_gpu.cpp:void PairCoulSlaterLongGPU::init_style()
src/GPU/pair_coul_slater_long_gpu.cpp:  if (!atom->q_flag) error->all(FLERR, "Pair style coul/slater/long/gpu requires atom attribute q");
src/GPU/pair_coul_slater_long_gpu.cpp:  int success = csl_gpu_init(atom->ntypes + 1, scale, atom->nlocal, atom->nlocal + atom->nghost, mnf,
src/GPU/pair_coul_slater_long_gpu.cpp:                            maxspecial, cell_size, gpu_mode, screen, cut_coulsq,
src/GPU/pair_coul_slater_long_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_coul_slater_long_gpu.cpp:  if (gpu_mode == GPU_FORCE) neighbor->add_request(this, NeighConst::REQ_FULL);
src/GPU/pair_coul_slater_long_gpu.cpp:void PairCoulSlaterLongGPU::reinit()
src/GPU/pair_coul_slater_long_gpu.cpp:  csl_gpu_reinit(atom->ntypes + 1, scale);
src/GPU/pair_coul_slater_long_gpu.cpp:double PairCoulSlaterLongGPU::memory_usage()
src/GPU/pair_coul_slater_long_gpu.cpp:  return bytes + csl_gpu_bytes();
src/GPU/pair_coul_slater_long_gpu.cpp:void PairCoulSlaterLongGPU::cpu_compute(int start, int inum, int eflag, int /* vflag */, int *ilist,
src/GPU/pair_eam_alloy_gpu.h:PairStyle(eam/alloy/gpu,PairEAMAlloyGPU);
src/GPU/pair_eam_alloy_gpu.h:#ifndef LMP_PAIR_EAM_ALLOY_GPU_H
src/GPU/pair_eam_alloy_gpu.h:#define LMP_PAIR_EAM_ALLOY_GPU_H
src/GPU/pair_eam_alloy_gpu.h:class PairEAMAlloyGPU : public PairEAM {
src/GPU/pair_eam_alloy_gpu.h:  PairEAMAlloyGPU(class LAMMPS *);
src/GPU/pair_eam_alloy_gpu.h:  ~PairEAMAlloyGPU() override;
src/GPU/pair_eam_alloy_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_eam_alloy_gpu.h:  int gpu_mode;
src/GPU/pair_lj_charmm_coul_long_gpu.h:PairStyle(lj/charmm/coul/long/gpu,PairLJCharmmCoulLongGPU);
src/GPU/pair_lj_charmm_coul_long_gpu.h:#ifndef LMP_PAIR_LJ_CHARMM_COUL_LONG_GPU_H
src/GPU/pair_lj_charmm_coul_long_gpu.h:#define LMP_PAIR_LJ_CHARMM_COUL_LONG_GPU_H
src/GPU/pair_lj_charmm_coul_long_gpu.h:class PairLJCharmmCoulLongGPU : public PairLJCharmmCoulLong {
src/GPU/pair_lj_charmm_coul_long_gpu.h:  PairLJCharmmCoulLongGPU(LAMMPS *lmp);
src/GPU/pair_lj_charmm_coul_long_gpu.h:  ~PairLJCharmmCoulLongGPU() override;
src/GPU/pair_lj_charmm_coul_long_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_lj_charmm_coul_long_gpu.h:  int gpu_mode;
src/GPU/pair_tersoff_mod_gpu.cpp:#include "pair_tersoff_mod_gpu.h"
src/GPU/pair_tersoff_mod_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_tersoff_mod_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_tersoff_mod_gpu.cpp:int tersoff_mod_gpu_init(const int ntypes, const int inum, const int nall, const int max_nbors,
src/GPU/pair_tersoff_mod_gpu.cpp:                         const double cell_size, int &gpu_mode, FILE *screen, int *host_map,
src/GPU/pair_tersoff_mod_gpu.cpp:void tersoff_mod_gpu_clear();
src/GPU/pair_tersoff_mod_gpu.cpp:int **tersoff_mod_gpu_compute_n(const int ago, const int inum_full, const int nall, double **host_x,
src/GPU/pair_tersoff_mod_gpu.cpp:void tersoff_mod_gpu_compute(const int ago, const int nlocal, const int nall, const int nlist,
src/GPU/pair_tersoff_mod_gpu.cpp:double tersoff_mod_gpu_bytes();
src/GPU/pair_tersoff_mod_gpu.cpp:PairTersoffMODGPU::PairTersoffMODGPU(LAMMPS *lmp) : PairTersoffMOD(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_tersoff_mod_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_tersoff_mod_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_tersoff_mod_gpu.cpp:PairTersoffMODGPU::~PairTersoffMODGPU()
src/GPU/pair_tersoff_mod_gpu.cpp:  tersoff_mod_gpu_clear();
src/GPU/pair_tersoff_mod_gpu.cpp:void PairTersoffMODGPU::compute(int eflag, int vflag)
src/GPU/pair_tersoff_mod_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_tersoff_mod_gpu.cpp:    firstneigh = tersoff_mod_gpu_compute_n(neighbor->ago, inum, nall, atom->x, atom->type, sublo,
src/GPU/pair_tersoff_mod_gpu.cpp:    tersoff_mod_gpu_compute(neighbor->ago, inum, nall, inum + list->gnum, atom->x, atom->type,
src/GPU/pair_tersoff_mod_gpu.cpp:void PairTersoffMODGPU::allocate()
src/GPU/pair_tersoff_mod_gpu.cpp:void PairTersoffMODGPU::init_style()
src/GPU/pair_tersoff_mod_gpu.cpp:  if (atom->tag_enable == 0) error->all(FLERR, "Pair style tersoff/mod/gpu requires atom IDs");
src/GPU/pair_tersoff_mod_gpu.cpp:  int success = tersoff_mod_gpu_init(atom->ntypes + 1, atom->nlocal, atom->nlocal + atom->nghost,
src/GPU/pair_tersoff_mod_gpu.cpp:                                     mnf, cell_size, gpu_mode, screen, map, nelements, elem3param,
src/GPU/pair_tersoff_mod_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_tersoff_mod_gpu.cpp:  if (gpu_mode == GPU_FORCE)
src/GPU/pair_tersoff_mod_gpu.cpp:      error->warning(FLERR, "Increasing communication cutoff to {:.8} for GPU pair style",
src/GPU/pair_tersoff_mod_gpu.cpp:double PairTersoffMODGPU::init_one(int i, int j)
src/GPU/pair_lj_cut_coul_msm_gpu.h:PairStyle(lj/cut/coul/msm/gpu,PairLJCutCoulMSMGPU);
src/GPU/pair_lj_cut_coul_msm_gpu.h:#ifndef LMP_PAIR_LJ_CUT_COUL_MSM_GPU_H
src/GPU/pair_lj_cut_coul_msm_gpu.h:#define LMP_PAIR_LJ_CUT_COUL_MSM_GPU_H
src/GPU/pair_lj_cut_coul_msm_gpu.h:class PairLJCutCoulMSMGPU : public PairLJCutCoulMSM {
src/GPU/pair_lj_cut_coul_msm_gpu.h:  PairLJCutCoulMSMGPU(LAMMPS *lmp);
src/GPU/pair_lj_cut_coul_msm_gpu.h:  ~PairLJCutCoulMSMGPU() override;
src/GPU/pair_lj_cut_coul_msm_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_lj_cut_coul_msm_gpu.h:  int gpu_mode;
src/GPU/pair_coul_dsf_gpu.h:PairStyle(coul/dsf/gpu,PairCoulDSFGPU);
src/GPU/pair_coul_dsf_gpu.h:#ifndef LMP_PAIR_COUL_DSF_GPU_H
src/GPU/pair_coul_dsf_gpu.h:#define LMP_PAIR_COUL_DSF_GPU_H
src/GPU/pair_coul_dsf_gpu.h:class PairCoulDSFGPU : public PairCoulDSF {
src/GPU/pair_coul_dsf_gpu.h:  PairCoulDSFGPU(LAMMPS *lmp);
src/GPU/pair_coul_dsf_gpu.h:  ~PairCoulDSFGPU() override;
src/GPU/pair_coul_dsf_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_coul_dsf_gpu.h:  int gpu_mode;
src/GPU/pair_born_coul_long_cs_gpu.cpp:#include "pair_born_coul_long_cs_gpu.h"
src/GPU/pair_born_coul_long_cs_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_born_coul_long_cs_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_born_coul_long_cs_gpu.cpp:int bornclcs_gpu_init(const int ntypes, double **cutsq, double **host_rhoinv, double **host_born1,
src/GPU/pair_born_coul_long_cs_gpu.cpp:                      const double cell_size, int &gpu_mode, FILE *screen, double **host_cut_ljsq,
src/GPU/pair_born_coul_long_cs_gpu.cpp:void bornclcs_gpu_clear();
src/GPU/pair_born_coul_long_cs_gpu.cpp:int **bornclcs_gpu_compute_n(const int ago, const int inum_full, const int nall, double **host_x,
src/GPU/pair_born_coul_long_cs_gpu.cpp:void bornclcs_gpu_compute(const int ago, const int inum_full, const int nall, double **host_x,
src/GPU/pair_born_coul_long_cs_gpu.cpp:double bornclcs_gpu_bytes();
src/GPU/pair_born_coul_long_cs_gpu.cpp:PairBornCoulLongCSGPU::PairBornCoulLongCSGPU(LAMMPS *lmp) :
src/GPU/pair_born_coul_long_cs_gpu.cpp:    PairBornCoulLongCS(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_born_coul_long_cs_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_born_coul_long_cs_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_born_coul_long_cs_gpu.cpp:PairBornCoulLongCSGPU::~PairBornCoulLongCSGPU()
src/GPU/pair_born_coul_long_cs_gpu.cpp:  bornclcs_gpu_clear();
src/GPU/pair_born_coul_long_cs_gpu.cpp:void PairBornCoulLongCSGPU::compute(int eflag, int vflag)
src/GPU/pair_born_coul_long_cs_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_born_coul_long_cs_gpu.cpp:    firstneigh = bornclcs_gpu_compute_n(
src/GPU/pair_born_coul_long_cs_gpu.cpp:    bornclcs_gpu_compute(neighbor->ago, inum, nall, atom->x, atom->type, ilist, numneigh,
src/GPU/pair_born_coul_long_cs_gpu.cpp:void PairBornCoulLongCSGPU::init_style()
src/GPU/pair_born_coul_long_cs_gpu.cpp:    error->all(FLERR, "Pair style born/coul/long/cs/gpu requires atom attribute q");
src/GPU/pair_born_coul_long_cs_gpu.cpp:  int success = bornclcs_gpu_init(
src/GPU/pair_born_coul_long_cs_gpu.cpp:      gpu_mode, screen, cut_ljsq, cut_coulsq, force->special_coul, force->qqrd2e, g_ewald);
src/GPU/pair_born_coul_long_cs_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_born_coul_long_cs_gpu.cpp:  if (gpu_mode == GPU_FORCE) neighbor->add_request(this, NeighConst::REQ_FULL);
src/GPU/pair_born_coul_long_cs_gpu.cpp:double PairBornCoulLongCSGPU::memory_usage()
src/GPU/pair_born_coul_long_cs_gpu.cpp:  return bytes + bornclcs_gpu_bytes();
src/GPU/pair_born_coul_long_cs_gpu.cpp:void PairBornCoulLongCSGPU::cpu_compute(int start, int inum, int eflag, int /* vflag */, int *ilist,
src/GPU/fix_nve_gpu.cpp:#include "fix_nve_gpu.h"
src/GPU/fix_nve_gpu.cpp:#include "gpu_extra.h"
src/GPU/fix_nve_gpu.cpp:FixNVEGPU::FixNVEGPU(LAMMPS *lmp, int narg, char **arg) :
src/GPU/fix_nve_gpu.cpp:FixNVEGPU::~FixNVEGPU()
src/GPU/fix_nve_gpu.cpp:void FixNVEGPU::setup(int vflag)
src/GPU/fix_nve_gpu.cpp:void FixNVEGPU::initial_integrate(int vflag)
src/GPU/fix_nve_gpu.cpp:void FixNVEGPU::final_integrate()
src/GPU/fix_nve_gpu.cpp:        memory->create(_dtfm, _nlocal_max * 3, "fix_nve_gpu:dtfm");
src/GPU/fix_nve_gpu.cpp:void FixNVEGPU::reset_dt() {
src/GPU/fix_nve_gpu.cpp:      memory->create(_dtfm, _nlocal_max * 3, "fix_nve_gpu:dtfm");
src/GPU/fix_nve_gpu.cpp:void FixNVEGPU::reset_dt_omp(const int ifrom, const int ito, const int tid) {
src/GPU/fix_nve_gpu.cpp:double FixNVEGPU::memory_usage()
src/GPU/pair_lj_expand_coul_long_gpu.h:PairStyle(lj/expand/coul/long/gpu,PairLJExpandCoulLongGPU);
src/GPU/pair_lj_expand_coul_long_gpu.h:#ifndef LMP_PAIR_LJ_EXPAND_COUL_LONG_GPU_H
src/GPU/pair_lj_expand_coul_long_gpu.h:#define LMP_PAIR_LJ_EXPAND_COUL_LONG_GPU_H
src/GPU/pair_lj_expand_coul_long_gpu.h:class PairLJExpandCoulLongGPU : public PairLJExpandCoulLong {
src/GPU/pair_lj_expand_coul_long_gpu.h:  PairLJExpandCoulLongGPU(LAMMPS *lmp);
src/GPU/pair_lj_expand_coul_long_gpu.h:  ~PairLJExpandCoulLongGPU() override;
src/GPU/pair_lj_expand_coul_long_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_lj_expand_coul_long_gpu.h:  int gpu_mode;
src/GPU/pair_born_gpu.h:PairStyle(born/gpu,PairBornGPU);
src/GPU/pair_born_gpu.h:#ifndef LMP_PAIR_BORN_GPU_H
src/GPU/pair_born_gpu.h:#define LMP_PAIR_BORN_GPU_H
src/GPU/pair_born_gpu.h:class PairBornGPU : public PairBorn {
src/GPU/pair_born_gpu.h:  PairBornGPU(LAMMPS *lmp);
src/GPU/pair_born_gpu.h:  ~PairBornGPU() override;
src/GPU/pair_born_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_born_gpu.h:  int gpu_mode;
src/GPU/pair_colloid_gpu.h:PairStyle(colloid/gpu,PairColloidGPU);
src/GPU/pair_colloid_gpu.h:#ifndef LMP_PAIR_COLLOID_GPU_H
src/GPU/pair_colloid_gpu.h:#define LMP_PAIR_COLLOID_GPU_H
src/GPU/pair_colloid_gpu.h:class PairColloidGPU : public PairColloid {
src/GPU/pair_colloid_gpu.h:  PairColloidGPU(LAMMPS *lmp);
src/GPU/pair_colloid_gpu.h:  ~PairColloidGPU() override;
src/GPU/pair_colloid_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_colloid_gpu.h:  int gpu_mode;
src/GPU/pppm_gpu.h:KSpaceStyle(pppm/gpu,PPPMGPU);
src/GPU/pppm_gpu.h:#ifndef LMP_PPPM_GPU_H
src/GPU/pppm_gpu.h:#define LMP_PPPM_GPU_H
src/GPU/pppm_gpu.h:class PPPMGPU : public PPPM {
src/GPU/pppm_gpu.h:  PPPMGPU(class LAMMPS *);
src/GPU/pppm_gpu.h:  ~PPPMGPU() override;
src/GPU/pppm_gpu.h:  FFT_SCALAR ***density_brick_gpu, ***vd_brick;
src/GPU/pppm_gpu.h:  void brick2fft_gpu();
src/GPU/pair_lj_cut_dipole_cut_gpu.cpp:#include "pair_lj_cut_dipole_cut_gpu.h"
src/GPU/pair_lj_cut_dipole_cut_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_lj_cut_dipole_cut_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_lj_cut_dipole_cut_gpu.cpp:int dpl_gpu_init(const int ntypes, double **cutsq, double **host_lj1, double **host_lj2,
src/GPU/pair_lj_cut_dipole_cut_gpu.cpp:                 const double cell_size, int &gpu_mode, FILE *screen, double **host_cut_ljsq,
src/GPU/pair_lj_cut_dipole_cut_gpu.cpp:void dpl_gpu_clear();
src/GPU/pair_lj_cut_dipole_cut_gpu.cpp:int **dpl_gpu_compute_n(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_lj_cut_dipole_cut_gpu.cpp:void dpl_gpu_compute(const int ago, const int inum, const int nall, double **host_x, int *host_type,
src/GPU/pair_lj_cut_dipole_cut_gpu.cpp:double dpl_gpu_bytes();
src/GPU/pair_lj_cut_dipole_cut_gpu.cpp:PairLJCutDipoleCutGPU::PairLJCutDipoleCutGPU(LAMMPS *lmp) :
src/GPU/pair_lj_cut_dipole_cut_gpu.cpp:    PairLJCutDipoleCut(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_lj_cut_dipole_cut_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_lj_cut_dipole_cut_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_lj_cut_dipole_cut_gpu.cpp:PairLJCutDipoleCutGPU::~PairLJCutDipoleCutGPU()
src/GPU/pair_lj_cut_dipole_cut_gpu.cpp:  dpl_gpu_clear();
src/GPU/pair_lj_cut_dipole_cut_gpu.cpp:void PairLJCutDipoleCutGPU::compute(int eflag, int vflag)
src/GPU/pair_lj_cut_dipole_cut_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_lj_cut_dipole_cut_gpu.cpp:    firstneigh = dpl_gpu_compute_n(neighbor->ago, inum, nall, atom->x, atom->type, sublo, subhi,
src/GPU/pair_lj_cut_dipole_cut_gpu.cpp:    dpl_gpu_compute(neighbor->ago, inum, nall, atom->x, atom->type, ilist, numneigh, firstneigh,
src/GPU/pair_lj_cut_dipole_cut_gpu.cpp:void PairLJCutDipoleCutGPU::init_style()
src/GPU/pair_lj_cut_dipole_cut_gpu.cpp:    error->all(FLERR, "Pair dipole/cut/gpu requires atom attributes q, mu, torque");
src/GPU/pair_lj_cut_dipole_cut_gpu.cpp:      dpl_gpu_init(atom->ntypes + 1, cutsq, lj1, lj2, lj3, lj4, offset, force->special_lj,
src/GPU/pair_lj_cut_dipole_cut_gpu.cpp:                   atom->nlocal, atom->nlocal + atom->nghost, mnf, maxspecial, cell_size, gpu_mode,
src/GPU/pair_lj_cut_dipole_cut_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_lj_cut_dipole_cut_gpu.cpp:  if (gpu_mode == GPU_FORCE) neighbor->add_request(this, NeighConst::REQ_FULL);
src/GPU/pair_lj_cut_dipole_cut_gpu.cpp:double PairLJCutDipoleCutGPU::memory_usage()
src/GPU/pair_lj_cut_dipole_cut_gpu.cpp:  return bytes + dpl_gpu_bytes();
src/GPU/pair_lj_cut_dipole_cut_gpu.cpp:void PairLJCutDipoleCutGPU::cpu_compute(int start, int inum, int eflag, int vflag, int *ilist,
src/GPU/fix_gpu.cpp:#include "fix_gpu.h"
src/GPU/fix_gpu.cpp:#include "gpu_extra.h"
src/GPU/fix_gpu.cpp:enum{GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH};
src/GPU/fix_gpu.cpp:extern int lmp_init_device(MPI_Comm world, MPI_Comm replica, const int ngpu,
src/GPU/fix_gpu.cpp:                           const int first_gpu_id, const int gpu_mode,
src/GPU/fix_gpu.cpp:                           const double cell_size, char *opencl_args,
src/GPU/fix_gpu.cpp:extern double lmp_gpu_forces(double **f, double **tor, double *eatom, double **vatom,
src/GPU/fix_gpu.cpp:extern double lmp_gpu_update_bin_size(const double subx, const double suby, const double subz,
src/GPU/fix_gpu.cpp:static const char cite_gpu_package[] =
src/GPU/fix_gpu.cpp:  "GPU package (short-range, long-range and three-body potentials): doi:10.1016/j.cpc.2010.12.021, doi:10.1016/j.cpc.2011.10.012, doi:10.1016/j.cpc.2013.08.002, doi:10.1016/j.commatsci.2014.10.068, doi:10.1016/j.cpc.2016.10.020, doi:10.3233/APC200086\n\n"
src/GPU/fix_gpu.cpp:  " title = {{GPU}-Accelerated {T}ersoff Potentials for Massively Parallel\n"
src/GPU/fix_gpu.cpp:  " title = {{GPU} Acceleration of Four-Site Water Models in {LAMMPS}},\n"
src/GPU/fix_gpu.cpp:FixGPU::FixGPU(LAMMPS *lmp, int narg, char **arg) :
src/GPU/fix_gpu.cpp:  if (lmp->citeme) lmp->citeme->add(cite_gpu_package);
src/GPU/fix_gpu.cpp:  if (narg < 4) error->all(FLERR,"Illegal package gpu command");
src/GPU/fix_gpu.cpp:  // If ngpu is 0, autoset ngpu to the number of devices per node matching
src/GPU/fix_gpu.cpp:  int ngpu = utils::inumeric(FLERR, arg[3], false, lmp);
src/GPU/fix_gpu.cpp:  if (ngpu < 0) error->all(FLERR,"Illegal number of GPUs ({}) in package gpu command", ngpu);
src/GPU/fix_gpu.cpp:  // Negative value indicate GPU package should find the best device ID
src/GPU/fix_gpu.cpp:  int first_gpu_id = -1;
src/GPU/fix_gpu.cpp:  _gpu_mode = GPU_NEIGH;
src/GPU/fix_gpu.cpp:  char *opencl_args = nullptr;
src/GPU/fix_gpu.cpp:      if (iarg+2 > narg) error->all(FLERR,"Illegal package gpu command");
src/GPU/fix_gpu.cpp:        _gpu_mode = GPU_NEIGH;
src/GPU/fix_gpu.cpp:        _gpu_mode = GPU_FORCE;
src/GPU/fix_gpu.cpp:      else if (modearg == "hybrid") _gpu_mode = GPU_HYB_NEIGH;
src/GPU/fix_gpu.cpp:      else error->all(FLERR,"Illegal package gpu command");
src/GPU/fix_gpu.cpp:      if (iarg+2 > narg) error->all(FLERR,"Illegal package gpu command");
src/GPU/fix_gpu.cpp:      if (iarg+2 > narg) error->all(FLERR,"Illegal package gpu command");
src/GPU/fix_gpu.cpp:      if (binsize <= 0.0) error->all(FLERR,"Illegal fix GPU command");
src/GPU/fix_gpu.cpp:      if (iarg+2 > narg) error->all(FLERR,"Illegal package gpu command");
src/GPU/fix_gpu.cpp:        error->all(FLERR,"Illegal package GPU command");
src/GPU/fix_gpu.cpp:    } else if (strcmp(arg[iarg],"gpuID") == 0) {
src/GPU/fix_gpu.cpp:      if (iarg+2 > narg) error->all(FLERR,"Illegal package gpu command");
src/GPU/fix_gpu.cpp:      first_gpu_id = utils::inumeric(FLERR,arg[iarg+1],false,lmp);
src/GPU/fix_gpu.cpp:      if (iarg+2 > narg) error->all(FLERR,"Illegal package gpu command");
src/GPU/fix_gpu.cpp:      if (iarg+2 > narg) error->all(FLERR,"Illegal package gpu command");
src/GPU/fix_gpu.cpp:      if (nthreads < 0) error->all(FLERR,"Illegal fix GPU command");
src/GPU/fix_gpu.cpp:      if (iarg+2 > narg) error->all(FLERR,"Illegal package gpu command");
src/GPU/fix_gpu.cpp:      if (iarg+2 > narg) error->all(FLERR,"Illegal package gpu command");
src/GPU/fix_gpu.cpp:      if (iarg+2 > narg) error->all(FLERR,"Illegal package gpu command");
src/GPU/fix_gpu.cpp:      if (iarg+2 > narg) error->all(FLERR,"Illegal package gpu command");
src/GPU/fix_gpu.cpp:      if (iarg+2 > narg) error->all(FLERR,"Illegal package gpu command");
src/GPU/fix_gpu.cpp:      opencl_args = arg[iarg+1];
src/GPU/fix_gpu.cpp:    } else error->all(FLERR,"Illegal package gpu command");
src/GPU/fix_gpu.cpp:    error->all(FLERR,"No OpenMP support compiled into the GPU package");
src/GPU/fix_gpu.cpp:  // pass params to GPU library
src/GPU/fix_gpu.cpp:  // change binsize default (0.0) to -1.0 used by GPU lib
src/GPU/fix_gpu.cpp:  int gpu_flag = lmp_init_device(universe->uworld, world, ngpu, first_gpu_id,
src/GPU/fix_gpu.cpp:                                 _gpu_mode, _particle_split, threads_per_atom,
src/GPU/fix_gpu.cpp:                                 binsize, opencl_args, ocl_platform,
src/GPU/fix_gpu.cpp:  GPU_EXTRA::check_flag(gpu_flag,error,world);
src/GPU/fix_gpu.cpp:FixGPU::~FixGPU()
src/GPU/fix_gpu.cpp:int FixGPU::setmask()
src/GPU/fix_gpu.cpp:void FixGPU::init()
src/GPU/fix_gpu.cpp:  // GPU package cannot be used with atom_style template
src/GPU/fix_gpu.cpp:    error->all(FLERR,"GPU package does not (yet) work with "
src/GPU/fix_gpu.cpp:    error->warning(FLERR,"Using package gpu without any pair style defined");
src/GPU/fix_gpu.cpp:  // also disallow GPU neighbor lists for hybrid styles
src/GPU/fix_gpu.cpp:      if (!utils::strmatch(hybrid->keywords[i],"/gpu$"))
src/GPU/fix_gpu.cpp:    if (_gpu_mode != GPU_FORCE)
src/GPU/fix_gpu.cpp:      error->all(FLERR, "Must not use GPU neighbor lists with hybrid pair style");
src/GPU/fix_gpu.cpp:void FixGPU::setup(int vflag)
src/GPU/fix_gpu.cpp:  // See if we should overlap topology list builds on CPU with work on GPU
src/GPU/fix_gpu.cpp:        if (force->pair_match("gpu",0,isub)) overlap_topo = 1;
src/GPU/fix_gpu.cpp:      if (force->pair_match("gpu",0)) overlap_topo = 1;
src/GPU/fix_gpu.cpp:  if (_gpu_mode == GPU_NEIGH || _gpu_mode == GPU_HYB_NEIGH)
src/GPU/fix_gpu.cpp:      error->all(FLERR, "Cannot use neigh_modify exclude with GPU neighbor builds");
src/GPU/fix_gpu.cpp:    // In setup only, all forces calculated on GPU are put in the outer level
src/GPU/fix_gpu.cpp:void FixGPU::min_setup(int vflag)
src/GPU/fix_gpu.cpp:void FixGPU::post_force(int /* vflag */)
src/GPU/fix_gpu.cpp:  double my_eng = lmp_gpu_forces(atom->f, atom->torque, force->pair->eatom, force->pair->vatom,
src/GPU/fix_gpu.cpp:    error->one(FLERR,"Neighbor list problem on the GPU. Try increasing the value of 'neigh_modify one' "
src/GPU/fix_gpu.cpp:               "or the GPU neighbor list 'binsize'.");
src/GPU/fix_gpu.cpp:void FixGPU::min_post_force(int vflag)
src/GPU/fix_gpu.cpp:void FixGPU::post_force_respa(int vflag, int /* ilevel */, int /* iloop */)
src/GPU/fix_gpu.cpp:double FixGPU::memory_usage()
src/GPU/fix_gpu.cpp:double FixGPU::binsize(const double subx, const double suby,
src/GPU/fix_gpu.cpp:  else if (_gpu_mode == GPU_FORCE || comm->cutghostuser)
src/GPU/fix_gpu.cpp:    return lmp_gpu_update_bin_size(subx, suby, subz, nlocal, cut);
src/GPU/pair_dpd_coul_slater_long_gpu.h:PairStyle(dpd/coul/slater/long/gpu,PairDPDCoulSlaterLongGPU);
src/GPU/pair_dpd_coul_slater_long_gpu.h:#ifndef LMP_PAIR_DPD_COUL_SLATER_LONG_GPU_H
src/GPU/pair_dpd_coul_slater_long_gpu.h:#define LMP_PAIR_DPD_COUL_SLATER_LONG_GPU_H
src/GPU/pair_dpd_coul_slater_long_gpu.h:class PairDPDCoulSlaterLongGPU : public PairDPDCoulSlaterLong {
src/GPU/pair_dpd_coul_slater_long_gpu.h:  PairDPDCoulSlaterLongGPU(LAMMPS *lmp);
src/GPU/pair_dpd_coul_slater_long_gpu.h:  ~PairDPDCoulSlaterLongGPU() override;
src/GPU/pair_dpd_coul_slater_long_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_dpd_coul_slater_long_gpu.h:  int gpu_mode;
src/GPU/pair_mie_cut_gpu.cpp:#include "pair_mie_cut_gpu.h"
src/GPU/pair_mie_cut_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_mie_cut_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_mie_cut_gpu.cpp:int mie_gpu_init(const int ntypes, double **cutsq, double **host_mie1, double **host_mie2,
src/GPU/pair_mie_cut_gpu.cpp:                 const int max_nbors, const int maxspecial, const double cell_size, int &gpu_mode,
src/GPU/pair_mie_cut_gpu.cpp:void mie_gpu_clear();
src/GPU/pair_mie_cut_gpu.cpp:int **mie_gpu_compute_n(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_mie_cut_gpu.cpp:void mie_gpu_compute(const int ago, const int inum, const int nall, double **host_x, int *host_type,
src/GPU/pair_mie_cut_gpu.cpp:double mie_gpu_bytes();
src/GPU/pair_mie_cut_gpu.cpp:PairMIECutGPU::PairMIECutGPU(LAMMPS *lmp) : PairMIECut(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_mie_cut_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_mie_cut_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_mie_cut_gpu.cpp:PairMIECutGPU::~PairMIECutGPU()
src/GPU/pair_mie_cut_gpu.cpp:  mie_gpu_clear();
src/GPU/pair_mie_cut_gpu.cpp:void PairMIECutGPU::compute(int eflag, int vflag)
src/GPU/pair_mie_cut_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_mie_cut_gpu.cpp:        mie_gpu_compute_n(neighbor->ago, inum, nall, atom->x, atom->type, sublo, subhi, atom->tag,
src/GPU/pair_mie_cut_gpu.cpp:    mie_gpu_compute(neighbor->ago, inum, nall, atom->x, atom->type, ilist, numneigh, firstneigh,
src/GPU/pair_mie_cut_gpu.cpp:void PairMIECutGPU::init_style()
src/GPU/pair_mie_cut_gpu.cpp:  int success = mie_gpu_init(atom->ntypes + 1, cutsq, mie1, mie2, mie3, mie4, gamA, gamR, offset,
src/GPU/pair_mie_cut_gpu.cpp:                             maxspecial, cell_size, gpu_mode, screen);
src/GPU/pair_mie_cut_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_mie_cut_gpu.cpp:  if (gpu_mode == GPU_FORCE) neighbor->add_request(this, NeighConst::REQ_FULL);
src/GPU/pair_mie_cut_gpu.cpp:double PairMIECutGPU::memory_usage()
src/GPU/pair_mie_cut_gpu.cpp:  return bytes + mie_gpu_bytes();
src/GPU/pair_mie_cut_gpu.cpp:void PairMIECutGPU::cpu_compute(int start, int inum, int eflag, int /* vflag */, int *ilist,
src/GPU/pair_lj_cut_coul_long_gpu.h:PairStyle(lj/cut/coul/long/gpu,PairLJCutCoulLongGPU);
src/GPU/pair_lj_cut_coul_long_gpu.h:#ifndef LMP_PAIR_LJ_CUT_COUL_LONG_GPU_H
src/GPU/pair_lj_cut_coul_long_gpu.h:#define LMP_PAIR_LJ_CUT_COUL_LONG_GPU_H
src/GPU/pair_lj_cut_coul_long_gpu.h:class PairLJCutCoulLongGPU : public PairLJCutCoulLong {
src/GPU/pair_lj_cut_coul_long_gpu.h:  PairLJCutCoulLongGPU(LAMMPS *lmp);
src/GPU/pair_lj_cut_coul_long_gpu.h:  ~PairLJCutCoulLongGPU() override;
src/GPU/pair_lj_cut_coul_long_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_lj_cut_coul_long_gpu.h:  int gpu_mode;
src/GPU/pair_lj_expand_gpu.cpp:   Contributing author: Inderaj Bains (NVIDIA), ibains@nvidia.com
src/GPU/pair_lj_expand_gpu.cpp:#include "pair_lj_expand_gpu.h"
src/GPU/pair_lj_expand_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_lj_expand_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_lj_expand_gpu.cpp:int lje_gpu_init(const int ntypes, double **cutsq, double **host_lj1, double **host_lj2,
src/GPU/pair_lj_expand_gpu.cpp:                 const int maxspecial, const double cell_size, int &gpu_mode, FILE *screen);
src/GPU/pair_lj_expand_gpu.cpp:void lje_gpu_reinit(const int ntypes, double **cutsq, double **host_lj1, double **host_lj2,
src/GPU/pair_lj_expand_gpu.cpp:void lje_gpu_clear();
src/GPU/pair_lj_expand_gpu.cpp:int **lje_gpu_compute_n(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_lj_expand_gpu.cpp:void lje_gpu_compute(const int ago, const int inum, const int nall, double **host_x, int *host_type,
src/GPU/pair_lj_expand_gpu.cpp:double lje_gpu_bytes();
src/GPU/pair_lj_expand_gpu.cpp:PairLJExpandGPU::PairLJExpandGPU(LAMMPS *lmp) : PairLJExpand(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_lj_expand_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_lj_expand_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_lj_expand_gpu.cpp:PairLJExpandGPU::~PairLJExpandGPU()
src/GPU/pair_lj_expand_gpu.cpp:  lje_gpu_clear();
src/GPU/pair_lj_expand_gpu.cpp:void PairLJExpandGPU::compute(int eflag, int vflag)
src/GPU/pair_lj_expand_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_lj_expand_gpu.cpp:        lje_gpu_compute_n(neighbor->ago, inum, nall, atom->x, atom->type, sublo, subhi, atom->tag,
src/GPU/pair_lj_expand_gpu.cpp:    lje_gpu_compute(neighbor->ago, inum, nall, atom->x, atom->type, ilist, numneigh, firstneigh,
src/GPU/pair_lj_expand_gpu.cpp:void PairLJExpandGPU::init_style()
src/GPU/pair_lj_expand_gpu.cpp:  int success = lje_gpu_init(atom->ntypes + 1, cutsq, lj1, lj2, lj3, lj4, offset, shift,
src/GPU/pair_lj_expand_gpu.cpp:                             maxspecial, cell_size, gpu_mode, screen);
src/GPU/pair_lj_expand_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_lj_expand_gpu.cpp:  if (gpu_mode == GPU_FORCE) neighbor->add_request(this, NeighConst::REQ_FULL);
src/GPU/pair_lj_expand_gpu.cpp:void PairLJExpandGPU::reinit()
src/GPU/pair_lj_expand_gpu.cpp:  lje_gpu_reinit(atom->ntypes + 1, cutsq, lj1, lj2, lj3, lj4, offset, shift);
src/GPU/pair_lj_expand_gpu.cpp:double PairLJExpandGPU::memory_usage()
src/GPU/pair_lj_expand_gpu.cpp:  return bytes + lje_gpu_bytes();
src/GPU/pair_lj_expand_gpu.cpp:void PairLJExpandGPU::cpu_compute(int start, int inum, int eflag, int /* vflag */, int *ilist,
src/GPU/pair_lj_class2_gpu.cpp:#include "pair_lj_class2_gpu.h"
src/GPU/pair_lj_class2_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_lj_class2_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_lj_class2_gpu.cpp:int lj96_gpu_init(const int ntypes, double **cutsq, double **host_lj1, double **host_lj2,
src/GPU/pair_lj_class2_gpu.cpp:                  const double cell_size, int &gpu_mode, FILE *screen);
src/GPU/pair_lj_class2_gpu.cpp:void lj96_gpu_clear();
src/GPU/pair_lj_class2_gpu.cpp:int **lj96_gpu_compute_n(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_lj_class2_gpu.cpp:void lj96_gpu_compute(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_lj_class2_gpu.cpp:double lj96_gpu_bytes();
src/GPU/pair_lj_class2_gpu.cpp:PairLJClass2GPU::PairLJClass2GPU(LAMMPS *lmp) : PairLJClass2(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_lj_class2_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_lj_class2_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_lj_class2_gpu.cpp:PairLJClass2GPU::~PairLJClass2GPU()
src/GPU/pair_lj_class2_gpu.cpp:  lj96_gpu_clear();
src/GPU/pair_lj_class2_gpu.cpp:void PairLJClass2GPU::compute(int eflag, int vflag)
src/GPU/pair_lj_class2_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_lj_class2_gpu.cpp:        lj96_gpu_compute_n(neighbor->ago, inum, nall, atom->x, atom->type, sublo, subhi, atom->tag,
src/GPU/pair_lj_class2_gpu.cpp:    lj96_gpu_compute(neighbor->ago, inum, nall, atom->x, atom->type, ilist, numneigh, firstneigh,
src/GPU/pair_lj_class2_gpu.cpp:void PairLJClass2GPU::init_style()
src/GPU/pair_lj_class2_gpu.cpp:  int success = lj96_gpu_init(atom->ntypes + 1, cutsq, lj1, lj2, lj3, lj4, offset,
src/GPU/pair_lj_class2_gpu.cpp:                              maxspecial, cell_size, gpu_mode, screen);
src/GPU/pair_lj_class2_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_lj_class2_gpu.cpp:  if (gpu_mode == GPU_FORCE) neighbor->add_request(this, NeighConst::REQ_FULL);
src/GPU/pair_lj_class2_gpu.cpp:double PairLJClass2GPU::memory_usage()
src/GPU/pair_lj_class2_gpu.cpp:  return bytes + lj96_gpu_bytes();
src/GPU/pair_lj_class2_gpu.cpp:void PairLJClass2GPU::cpu_compute(int start, int inum, int eflag, int /* vflag */, int *ilist,
src/GPU/pair_buck_coul_cut_gpu.h:PairStyle(buck/coul/cut/gpu,PairBuckCoulCutGPU);
src/GPU/pair_buck_coul_cut_gpu.h:#ifndef LMP_PAIR_BUCK_COUL_CUT_GPU_H
src/GPU/pair_buck_coul_cut_gpu.h:#define LMP_PAIR_BUCK_COUL_CUT_GPU_H
src/GPU/pair_buck_coul_cut_gpu.h:class PairBuckCoulCutGPU : public PairBuckCoulCut {
src/GPU/pair_buck_coul_cut_gpu.h:  PairBuckCoulCutGPU(LAMMPS *lmp);
src/GPU/pair_buck_coul_cut_gpu.h:  ~PairBuckCoulCutGPU() override;
src/GPU/pair_buck_coul_cut_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_buck_coul_cut_gpu.h:  int gpu_mode;
src/GPU/pppm_gpu.cpp:#include "pppm_gpu.h"
src/GPU/pppm_gpu.cpp:#include "gpu_extra.h"
src/GPU/pppm_gpu.cpp:// external functions from cuda library for atom decomposition
src/GPU/pppm_gpu.cpp:#define PPPM_GPU_API(api)  pppm_gpu_ ## api ## _f
src/GPU/pppm_gpu.cpp:#define PPPM_GPU_API(api)  pppm_gpu_ ## api ## _d
src/GPU/pppm_gpu.cpp:FFT_SCALAR* PPPM_GPU_API(init)(const int nlocal, const int nall, FILE *screen,
src/GPU/pppm_gpu.cpp:void PPPM_GPU_API(clear)(const double poisson_time);
src/GPU/pppm_gpu.cpp:int PPPM_GPU_API(spread)(const int ago, const int nlocal, const int nall,
src/GPU/pppm_gpu.cpp:void PPPM_GPU_API(interp)(const FFT_SCALAR qqrd2e_scale);
src/GPU/pppm_gpu.cpp:double PPPM_GPU_API(bytes)();
src/GPU/pppm_gpu.cpp:void PPPM_GPU_API(forces)(double **f);
src/GPU/pppm_gpu.cpp:PPPMGPU::PPPMGPU(LAMMPS *lmp) : PPPM(lmp)
src/GPU/pppm_gpu.cpp:  density_brick_gpu = vd_brick = nullptr;
src/GPU/pppm_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pppm_gpu.cpp:PPPMGPU::~PPPMGPU()
src/GPU/pppm_gpu.cpp:  PPPM_GPU_API(clear)(poisson_time);
src/GPU/pppm_gpu.cpp:  destroy_3d_offset(density_brick_gpu,nzlo_out,nylo_out);
src/GPU/pppm_gpu.cpp:void PPPMGPU::init()
src/GPU/pppm_gpu.cpp:  // PPPM init manages all arrays except density_brick_gpu and vd_brick
src/GPU/pppm_gpu.cpp:  //       before allocating db_gpu and vd_brick down below, if don't need,
src/GPU/pppm_gpu.cpp:  destroy_3d_offset(density_brick_gpu,nzlo_out,nylo_out);
src/GPU/pppm_gpu.cpp:  density_brick_gpu = vd_brick = nullptr;
src/GPU/pppm_gpu.cpp:      error->all(FLERR,"Cannot currently use pppm/gpu with fix balance.");
src/GPU/pppm_gpu.cpp:    error->all(FLERR,"Cannot (yet) do analytic differentiation with pppm/gpu");
src/GPU/pppm_gpu.cpp:  // GPU precision specific init
src/GPU/pppm_gpu.cpp:    error->all(FLERR,"Cannot use order greater than 8 with pppm/gpu.");
src/GPU/pppm_gpu.cpp:  PPPM_GPU_API(clear)(poisson_time);
src/GPU/pppm_gpu.cpp:  h_brick = PPPM_GPU_API(init)(atom->nlocal, atom->nlocal+atom->nghost, screen,
src/GPU/pppm_gpu.cpp:  GPU_EXTRA::check_flag(success,error,world);
src/GPU/pppm_gpu.cpp:  // allocate density_brick_gpu and vd_brick
src/GPU/pppm_gpu.cpp:  density_brick_gpu =
src/GPU/pppm_gpu.cpp:                     nxlo_out,nxhi_out,"pppm:density_brick_gpu",h_brick,1);
src/GPU/pppm_gpu.cpp:   compute the PPPMGPU long-range force, energy, virial
src/GPU/pppm_gpu.cpp:void PPPMGPU::compute(int eflag, int vflag)
src/GPU/pppm_gpu.cpp:  // so that particle map on host can be done concurrently with GPU calculations
src/GPU/pppm_gpu.cpp:    int flag=PPPM_GPU_API(spread)(nago, atom->nlocal, atom->nlocal +
src/GPU/pppm_gpu.cpp:  // concurrently with GPU calculations,
src/GPU/pppm_gpu.cpp:    gc->reverse_comm(Grid3d::KSPACE,this,REVERSE_RHO_GPU,1,sizeof(FFT_SCALAR),
src/GPU/pppm_gpu.cpp:    brick2fft_gpu();
src/GPU/pppm_gpu.cpp:  if (triclinic == 0) PPPM_GPU_API(interp)(qscale);
src/GPU/pppm_gpu.cpp:  if (kspace_split) PPPM_GPU_API(forces)(atom->f);
src/GPU/pppm_gpu.cpp:void PPPMGPU::brick2fft_gpu()
src/GPU/pppm_gpu.cpp:        density_fft[n++] = density_brick_gpu[iz][iy][ix];
src/GPU/pppm_gpu.cpp:void PPPMGPU::poisson_ik()
src/GPU/pppm_gpu.cpp:void PPPMGPU::pack_forward_grid(int flag, void *vbuf, int nlist, int *list)
src/GPU/pppm_gpu.cpp:void PPPMGPU::unpack_forward_grid(int flag, void *vbuf, int nlist, int *list)
src/GPU/pppm_gpu.cpp:void PPPMGPU::pack_reverse_grid(int flag, void *vbuf, int nlist, int *list)
src/GPU/pppm_gpu.cpp:  if (flag == REVERSE_RHO_GPU) {
src/GPU/pppm_gpu.cpp:    FFT_SCALAR *src = &density_brick_gpu[nzlo_out][nylo_out][nxlo_out];
src/GPU/pppm_gpu.cpp:void PPPMGPU::unpack_reverse_grid(int flag, void *vbuf, int nlist, int *list)
src/GPU/pppm_gpu.cpp:  if (flag == REVERSE_RHO_GPU) {
src/GPU/pppm_gpu.cpp:    FFT_SCALAR *dest = &density_brick_gpu[nzlo_out][nylo_out][nxlo_out];
src/GPU/pppm_gpu.cpp:FFT_SCALAR ***PPPMGPU::create_3d_offset(int n1lo, int n1hi, int n2lo, int n2hi,
src/GPU/pppm_gpu.cpp:void PPPMGPU::destroy_3d_offset(FFT_SCALAR ***array, int n1_offset,
src/GPU/pppm_gpu.cpp:double PPPMGPU::memory_usage()
src/GPU/pppm_gpu.cpp:  // NOTE: add tallying here for density_brick_gpu and vd_brick
src/GPU/pppm_gpu.cpp:  return bytes + PPPM_GPU_API(bytes)();
src/GPU/pppm_gpu.cpp:int PPPMGPU::timing_1d(int n, double &time1d)
src/GPU/pppm_gpu.cpp:int PPPMGPU::timing_3d(int n, double &time3d)
src/GPU/pppm_gpu.cpp:void PPPMGPU::setup()
src/GPU/pppm_gpu.cpp:void PPPMGPU::compute_group_group(int groupbit_A, int groupbit_B, int AA_flag)
src/GPU/pair_lj_cut_coul_long_soft_gpu.cpp:#include "pair_lj_cut_coul_long_soft_gpu.h"
src/GPU/pair_lj_cut_coul_long_soft_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_lj_cut_coul_long_soft_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_lj_cut_coul_long_soft_gpu.cpp:int ljcls_gpu_init(const int ntypes, double **cutsq, double **host_lj1, double **host_lj2,
src/GPU/pair_lj_cut_coul_long_soft_gpu.cpp:                  const double cell_size, int &gpu_mode, FILE *screen, double **host_cut_ljsq,
src/GPU/pair_lj_cut_coul_long_soft_gpu.cpp:void ljcls_gpu_reinit(const int ntypes, double **cutsq, double **host_lj1, double **host_lj2,
src/GPU/pair_lj_cut_coul_long_soft_gpu.cpp:void ljcls_gpu_clear();
src/GPU/pair_lj_cut_coul_long_soft_gpu.cpp:int **ljcls_gpu_compute_n(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_lj_cut_coul_long_soft_gpu.cpp:void ljcls_gpu_compute(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_lj_cut_coul_long_soft_gpu.cpp:double ljcls_gpu_bytes();
src/GPU/pair_lj_cut_coul_long_soft_gpu.cpp:PairLJCutCoulLongSoftGPU::PairLJCutCoulLongSoftGPU(LAMMPS *lmp) :
src/GPU/pair_lj_cut_coul_long_soft_gpu.cpp:    PairLJCutCoulLongSoft(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_lj_cut_coul_long_soft_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_lj_cut_coul_long_soft_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_lj_cut_coul_long_soft_gpu.cpp:PairLJCutCoulLongSoftGPU::~PairLJCutCoulLongSoftGPU()
src/GPU/pair_lj_cut_coul_long_soft_gpu.cpp:  ljcls_gpu_clear();
src/GPU/pair_lj_cut_coul_long_soft_gpu.cpp:void PairLJCutCoulLongSoftGPU::compute(int eflag, int vflag)
src/GPU/pair_lj_cut_coul_long_soft_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_lj_cut_coul_long_soft_gpu.cpp:    firstneigh = ljcls_gpu_compute_n(neighbor->ago, inum, nall, atom->x, atom->type, sublo, subhi,
src/GPU/pair_lj_cut_coul_long_soft_gpu.cpp:    ljcls_gpu_compute(neighbor->ago, inum, nall, atom->x, atom->type, ilist, numneigh, firstneigh,
src/GPU/pair_lj_cut_coul_long_soft_gpu.cpp:void PairLJCutCoulLongSoftGPU::init_style()
src/GPU/pair_lj_cut_coul_long_soft_gpu.cpp:  if (!atom->q_flag) error->all(FLERR, "Pair style lj/cut/coul/long/soft/gpu requires atom attribute q");
src/GPU/pair_lj_cut_coul_long_soft_gpu.cpp:      ljcls_gpu_init(atom->ntypes + 1, cutsq, lj1, lj2, lj3, lj4, offset, epsilon, force->special_lj,
src/GPU/pair_lj_cut_coul_long_soft_gpu.cpp:                    atom->nlocal, atom->nlocal + atom->nghost, mnf, maxspecial, cell_size, gpu_mode,
src/GPU/pair_lj_cut_coul_long_soft_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_lj_cut_coul_long_soft_gpu.cpp:  if (gpu_mode == GPU_FORCE) neighbor->add_request(this, NeighConst::REQ_FULL);
src/GPU/pair_lj_cut_coul_long_soft_gpu.cpp:void PairLJCutCoulLongSoftGPU::reinit()
src/GPU/pair_lj_cut_coul_long_soft_gpu.cpp:  ljcls_gpu_reinit(atom->ntypes + 1, cutsq, lj1, lj2, lj3, lj4, offset, epsilon, cut_ljsq);
src/GPU/pair_lj_cut_coul_long_soft_gpu.cpp:double PairLJCutCoulLongSoftGPU::memory_usage()
src/GPU/pair_lj_cut_coul_long_soft_gpu.cpp:  return bytes + ljcls_gpu_bytes();
src/GPU/pair_lj_cut_coul_long_soft_gpu.cpp:void PairLJCutCoulLongSoftGPU::cpu_compute(int start, int inum, int eflag, int /* vflag */, int *ilist,
src/GPU/pair_beck_gpu.cpp:#include "pair_beck_gpu.h"
src/GPU/pair_beck_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_beck_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_beck_gpu.cpp:int beck_gpu_init(const int ntypes, double **cutsq, double **host_aa, double **alpha, double **beta,
src/GPU/pair_beck_gpu.cpp:                  const int max_nbors, const int maxspecial, const double cell_size, int &gpu_mode,
src/GPU/pair_beck_gpu.cpp:void beck_gpu_clear();
src/GPU/pair_beck_gpu.cpp:int **beck_gpu_compute_n(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_beck_gpu.cpp:void beck_gpu_compute(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_beck_gpu.cpp:double beck_gpu_bytes();
src/GPU/pair_beck_gpu.cpp:PairBeckGPU::PairBeckGPU(LAMMPS *lmp) : PairBeck(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_beck_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_beck_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_beck_gpu.cpp:PairBeckGPU::~PairBeckGPU()
src/GPU/pair_beck_gpu.cpp:  beck_gpu_clear();
src/GPU/pair_beck_gpu.cpp:void PairBeckGPU::compute(int eflag, int vflag)
src/GPU/pair_beck_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_beck_gpu.cpp:        beck_gpu_compute_n(neighbor->ago, inum, nall, atom->x, atom->type, sublo, subhi, atom->tag,
src/GPU/pair_beck_gpu.cpp:    beck_gpu_compute(neighbor->ago, inum, nall, atom->x, atom->type, ilist, numneigh, firstneigh,
src/GPU/pair_beck_gpu.cpp:void PairBeckGPU::init_style()
src/GPU/pair_beck_gpu.cpp:  int success = beck_gpu_init(atom->ntypes + 1, cutsq, aa, alpha, beta, AA, BB, force->special_lj,
src/GPU/pair_beck_gpu.cpp:                              gpu_mode, screen);
src/GPU/pair_beck_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_beck_gpu.cpp:  if (gpu_mode == GPU_FORCE) neighbor->add_request(this, NeighConst::REQ_FULL);
src/GPU/pair_beck_gpu.cpp:double PairBeckGPU::memory_usage()
src/GPU/pair_beck_gpu.cpp:  return bytes + beck_gpu_bytes();
src/GPU/pair_beck_gpu.cpp:void PairBeckGPU::cpu_compute(int start, int inum, int eflag, int /* vflag */, int *ilist,
src/GPU/pair_lj_cut_coul_dsf_gpu.cpp:#include "pair_lj_cut_coul_dsf_gpu.h"
src/GPU/pair_lj_cut_coul_dsf_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_lj_cut_coul_dsf_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_lj_cut_coul_dsf_gpu.cpp:int ljd_gpu_init(const int ntypes, double **cutsq, double **host_lj1, double **host_lj2,
src/GPU/pair_lj_cut_coul_dsf_gpu.cpp:                 const double cell_size, int &gpu_mode, FILE *screen, double **host_cut_ljsq,
src/GPU/pair_lj_cut_coul_dsf_gpu.cpp:void ljd_gpu_clear();
src/GPU/pair_lj_cut_coul_dsf_gpu.cpp:int **ljd_gpu_compute_n(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_lj_cut_coul_dsf_gpu.cpp:void ljd_gpu_compute(const int ago, const int inum, const int nall, double **host_x, int *host_type,
src/GPU/pair_lj_cut_coul_dsf_gpu.cpp:double ljd_gpu_bytes();
src/GPU/pair_lj_cut_coul_dsf_gpu.cpp:PairLJCutCoulDSFGPU::PairLJCutCoulDSFGPU(LAMMPS *lmp) : PairLJCutCoulDSF(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_lj_cut_coul_dsf_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_lj_cut_coul_dsf_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_lj_cut_coul_dsf_gpu.cpp:PairLJCutCoulDSFGPU::~PairLJCutCoulDSFGPU()
src/GPU/pair_lj_cut_coul_dsf_gpu.cpp:  ljd_gpu_clear();
src/GPU/pair_lj_cut_coul_dsf_gpu.cpp:void PairLJCutCoulDSFGPU::compute(int eflag, int vflag)
src/GPU/pair_lj_cut_coul_dsf_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_lj_cut_coul_dsf_gpu.cpp:    firstneigh = ljd_gpu_compute_n(neighbor->ago, inum, nall, atom->x, atom->type, sublo, subhi,
src/GPU/pair_lj_cut_coul_dsf_gpu.cpp:    ljd_gpu_compute(neighbor->ago, inum, nall, atom->x, atom->type, ilist, numneigh, firstneigh,
src/GPU/pair_lj_cut_coul_dsf_gpu.cpp:void PairLJCutCoulDSFGPU::init_style()
src/GPU/pair_lj_cut_coul_dsf_gpu.cpp:  if (!atom->q_flag) error->all(FLERR, "Pair style lj/cut/coul/dsf/gpu requires atom attribute q");
src/GPU/pair_lj_cut_coul_dsf_gpu.cpp:  int success = ljd_gpu_init(atom->ntypes + 1, cutsq, lj1, lj2, lj3, lj4, offset, force->special_lj,
src/GPU/pair_lj_cut_coul_dsf_gpu.cpp:                             gpu_mode, screen, cut_ljsq, cut_coulsq, force->special_coul,
src/GPU/pair_lj_cut_coul_dsf_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_lj_cut_coul_dsf_gpu.cpp:  if (gpu_mode == GPU_FORCE) neighbor->add_request(this, NeighConst::REQ_FULL);
src/GPU/pair_lj_cut_coul_dsf_gpu.cpp:double PairLJCutCoulDSFGPU::memory_usage()
src/GPU/pair_lj_cut_coul_dsf_gpu.cpp:  return bytes + ljd_gpu_bytes();
src/GPU/pair_lj_cut_coul_dsf_gpu.cpp:void PairLJCutCoulDSFGPU::cpu_compute(int start, int inum, int eflag, int /* vflag */, int *ilist,
src/GPU/pair_edpd_gpu.h:PairStyle(edpd/gpu,PairEDPDGPU);
src/GPU/pair_edpd_gpu.h:#ifndef LMP_PAIR_EDPD_GPU_H
src/GPU/pair_edpd_gpu.h:#define LMP_PAIR_EDPD_GPU_H
src/GPU/pair_edpd_gpu.h:class PairEDPDGPU : public PairEDPD {
src/GPU/pair_edpd_gpu.h:  PairEDPDGPU(LAMMPS *lmp);
src/GPU/pair_edpd_gpu.h:  ~PairEDPDGPU() override;
src/GPU/pair_edpd_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_edpd_gpu.h:  int gpu_mode;
src/GPU/pair_lj_cut_coul_debye_gpu.h:PairStyle(lj/cut/coul/debye/gpu,PairLJCutCoulDebyeGPU);
src/GPU/pair_lj_cut_coul_debye_gpu.h:#ifndef LMP_PAIR_LJ_CUT_COUL_DEBYE_GPU_H
src/GPU/pair_lj_cut_coul_debye_gpu.h:#define LMP_PAIR_LJ_CUT_COUL_DEBYE_GPU_H
src/GPU/pair_lj_cut_coul_debye_gpu.h:class PairLJCutCoulDebyeGPU : public PairLJCutCoulDebye {
src/GPU/pair_lj_cut_coul_debye_gpu.h:  PairLJCutCoulDebyeGPU(LAMMPS *lmp);
src/GPU/pair_lj_cut_coul_debye_gpu.h:  ~PairLJCutCoulDebyeGPU() override;
src/GPU/pair_lj_cut_coul_debye_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_lj_cut_coul_debye_gpu.h:  int gpu_mode;
src/GPU/pair_coul_long_cs_gpu.cpp:#include "pair_coul_long_cs_gpu.h"
src/GPU/pair_coul_long_cs_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_coul_long_cs_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_coul_long_cs_gpu.cpp:int clcs_gpu_init(const int ntypes, double **scale, const int nlocal, const int nall,
src/GPU/pair_coul_long_cs_gpu.cpp:                  const int max_nbors, const int maxspecial, const double cell_size, int &gpu_mode,
src/GPU/pair_coul_long_cs_gpu.cpp:void clcs_gpu_reinit(const int ntypes, double **scale);
src/GPU/pair_coul_long_cs_gpu.cpp:void clcs_gpu_clear();
src/GPU/pair_coul_long_cs_gpu.cpp:int **clcs_gpu_compute_n(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_coul_long_cs_gpu.cpp:void clcs_gpu_compute(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_coul_long_cs_gpu.cpp:double clcs_gpu_bytes();
src/GPU/pair_coul_long_cs_gpu.cpp:PairCoulLongCSGPU::PairCoulLongCSGPU(LAMMPS *lmp) : PairCoulLongCS(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_coul_long_cs_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_coul_long_cs_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_coul_long_cs_gpu.cpp:PairCoulLongCSGPU::~PairCoulLongCSGPU()
src/GPU/pair_coul_long_cs_gpu.cpp:  clcs_gpu_clear();
src/GPU/pair_coul_long_cs_gpu.cpp:void PairCoulLongCSGPU::compute(int eflag, int vflag)
src/GPU/pair_coul_long_cs_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_coul_long_cs_gpu.cpp:    firstneigh = clcs_gpu_compute_n(neighbor->ago, inum, nall, atom->x, atom->type, sublo, subhi,
src/GPU/pair_coul_long_cs_gpu.cpp:    clcs_gpu_compute(neighbor->ago, inum, nall, atom->x, atom->type, ilist, numneigh, firstneigh,
src/GPU/pair_coul_long_cs_gpu.cpp:void PairCoulLongCSGPU::init_style()
src/GPU/pair_coul_long_cs_gpu.cpp:  if (!atom->q_flag) error->all(FLERR, "Pair style coul/long/cs/gpu requires atom attribute q");
src/GPU/pair_coul_long_cs_gpu.cpp:  int success = clcs_gpu_init(atom->ntypes + 1, scale, atom->nlocal, atom->nlocal + atom->nghost,
src/GPU/pair_coul_long_cs_gpu.cpp:                              mnf, maxspecial, cell_size, gpu_mode, screen, cut_coulsq,
src/GPU/pair_coul_long_cs_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_coul_long_cs_gpu.cpp:  if (gpu_mode == GPU_FORCE) neighbor->add_request(this, NeighConst::REQ_FULL);
src/GPU/pair_coul_long_cs_gpu.cpp:void PairCoulLongCSGPU::reinit()
src/GPU/pair_coul_long_cs_gpu.cpp:  clcs_gpu_reinit(atom->ntypes + 1, scale);
src/GPU/pair_coul_long_cs_gpu.cpp:double PairCoulLongCSGPU::memory_usage()
src/GPU/pair_coul_long_cs_gpu.cpp:  return bytes + clcs_gpu_bytes();
src/GPU/pair_coul_long_cs_gpu.cpp:void PairCoulLongCSGPU::cpu_compute(int start, int inum, int eflag, int /* vflag */, int *ilist,
src/GPU/pair_lj_sf_dipole_sf_gpu.h:PairStyle(lj/sf/dipole/sf/gpu,PairLJSFDipoleSFGPU);
src/GPU/pair_lj_sf_dipole_sf_gpu.h:#ifndef LMP_PAIR_LJ_SF_DIPOLE_SF_GPU_H
src/GPU/pair_lj_sf_dipole_sf_gpu.h:#define LMP_PAIR_LJ_SF_DIPOLE_SF_GPU_H
src/GPU/pair_lj_sf_dipole_sf_gpu.h:class PairLJSFDipoleSFGPU : public PairLJSFDipoleSF {
src/GPU/pair_lj_sf_dipole_sf_gpu.h:  PairLJSFDipoleSFGPU(LAMMPS *lmp);
src/GPU/pair_lj_sf_dipole_sf_gpu.h:  ~PairLJSFDipoleSFGPU() override;
src/GPU/pair_lj_sf_dipole_sf_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_lj_sf_dipole_sf_gpu.h:  int gpu_mode;
src/GPU/fix_npt_gpu.cpp:#include "fix_npt_gpu.h"
src/GPU/fix_npt_gpu.cpp:FixNPTGPU::FixNPTGPU(LAMMPS *lmp, int narg, char **arg) :
src/GPU/fix_npt_gpu.cpp:  FixNHGPU(lmp, narg, arg)
src/GPU/fix_npt_gpu.cpp:    error->all(FLERR,"Temperature control must be used with fix npt/gpu");
src/GPU/fix_npt_gpu.cpp:    error->all(FLERR,"Pressure control must be used with fix npt/gpu");
src/GPU/pair_mdpd_gpu.h:PairStyle(mdpd/gpu,PairMDPDGPU);
src/GPU/pair_mdpd_gpu.h:#ifndef LMP_PAIR_MDPD_GPU_H
src/GPU/pair_mdpd_gpu.h:#define LMP_PAIR_MDPD_GPU_H
src/GPU/pair_mdpd_gpu.h:class PairMDPDGPU : public PairMDPD {
src/GPU/pair_mdpd_gpu.h:  PairMDPDGPU(LAMMPS *lmp);
src/GPU/pair_mdpd_gpu.h:  ~PairMDPDGPU() override;
src/GPU/pair_mdpd_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_mdpd_gpu.h:  int gpu_mode;
src/GPU/pair_ufm_gpu.cpp:#include "pair_ufm_gpu.h"
src/GPU/pair_ufm_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_ufm_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_ufm_gpu.cpp:int ufml_gpu_init(const int ntypes, double **cutsq, double **host_uf1, double **host_uf2,
src/GPU/pair_ufm_gpu.cpp:                  int &gpu_mode, FILE *screen);
src/GPU/pair_ufm_gpu.cpp:void ufml_gpu_reinit(const int ntypes, double **cutsq, double **host_uf1, double **host_uf2,
src/GPU/pair_ufm_gpu.cpp:void ufml_gpu_clear();
src/GPU/pair_ufm_gpu.cpp:int **ufml_gpu_compute_n(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_ufm_gpu.cpp:void ufml_gpu_compute(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_ufm_gpu.cpp:double ufml_gpu_bytes();
src/GPU/pair_ufm_gpu.cpp:PairUFMGPU::PairUFMGPU(LAMMPS *lmp) : PairUFM(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_ufm_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_ufm_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_ufm_gpu.cpp:PairUFMGPU::~PairUFMGPU()
src/GPU/pair_ufm_gpu.cpp:  ufml_gpu_clear();
src/GPU/pair_ufm_gpu.cpp:void PairUFMGPU::compute(int eflag, int vflag)
src/GPU/pair_ufm_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_ufm_gpu.cpp:        ufml_gpu_compute_n(neighbor->ago, inum, nall, atom->x, atom->type, sublo, subhi, atom->tag,
src/GPU/pair_ufm_gpu.cpp:    ufml_gpu_compute(neighbor->ago, inum, nall, atom->x, atom->type, ilist, numneigh, firstneigh,
src/GPU/pair_ufm_gpu.cpp:void PairUFMGPU::init_style()
src/GPU/pair_ufm_gpu.cpp:      ufml_gpu_init(atom->ntypes + 1, cutsq, uf1, uf2, uf3, offset, force->special_lj, atom->nlocal,
src/GPU/pair_ufm_gpu.cpp:                    atom->nlocal + atom->nghost, mnf, maxspecial, cell_size, gpu_mode, screen);
src/GPU/pair_ufm_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_ufm_gpu.cpp:  if (gpu_mode == GPU_FORCE) neighbor->add_request(this, NeighConst::REQ_FULL);
src/GPU/pair_ufm_gpu.cpp:void PairUFMGPU::reinit()
src/GPU/pair_ufm_gpu.cpp:  ufml_gpu_reinit(atom->ntypes + 1, cutsq, uf1, uf2, uf3, offset);
src/GPU/pair_ufm_gpu.cpp:double PairUFMGPU::memory_usage()
src/GPU/pair_ufm_gpu.cpp:  return bytes + ufml_gpu_bytes();
src/GPU/pair_ufm_gpu.cpp:void PairUFMGPU::cpu_compute(int start, int inum, int eflag, int /* vflag */, int *ilist,
src/GPU/pair_lj_charmm_coul_charmm_gpu.cpp:#include "pair_lj_charmm_coul_charmm_gpu.h"
src/GPU/pair_lj_charmm_coul_charmm_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_lj_charmm_coul_charmm_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_lj_charmm_coul_charmm_gpu.cpp:int crm_gpu_init(const int ntypes, double cut_bothsq, double **host_lj1, double **host_lj2,
src/GPU/pair_lj_charmm_coul_charmm_gpu.cpp:                 int &gpu_mode, FILE *screen, double host_cut_ljsq, double host_cut_coulsq,
src/GPU/pair_lj_charmm_coul_charmm_gpu.cpp:void crm_gpu_clear();
src/GPU/pair_lj_charmm_coul_charmm_gpu.cpp:int **crm_gpu_compute_n(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_lj_charmm_coul_charmm_gpu.cpp:void crm_gpu_compute(const int ago, const int inum, const int nall, double **host_x, int *host_type,
src/GPU/pair_lj_charmm_coul_charmm_gpu.cpp:double crm_gpu_bytes();
src/GPU/pair_lj_charmm_coul_charmm_gpu.cpp:PairLJCharmmCoulCharmmGPU::PairLJCharmmCoulCharmmGPU(LAMMPS *lmp) :
src/GPU/pair_lj_charmm_coul_charmm_gpu.cpp:    PairLJCharmmCoulCharmm(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_lj_charmm_coul_charmm_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_lj_charmm_coul_charmm_gpu.cpp:PairLJCharmmCoulCharmmGPU::~PairLJCharmmCoulCharmmGPU()
src/GPU/pair_lj_charmm_coul_charmm_gpu.cpp:  crm_gpu_clear();
src/GPU/pair_lj_charmm_coul_charmm_gpu.cpp:void PairLJCharmmCoulCharmmGPU::compute(int eflag, int vflag)
src/GPU/pair_lj_charmm_coul_charmm_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_lj_charmm_coul_charmm_gpu.cpp:    firstneigh = crm_gpu_compute_n(neighbor->ago, inum, nall, atom->x, atom->type, domain->sublo,
src/GPU/pair_lj_charmm_coul_charmm_gpu.cpp:    crm_gpu_compute(neighbor->ago, inum, nall, atom->x, atom->type, ilist, numneigh, firstneigh,
src/GPU/pair_lj_charmm_coul_charmm_gpu.cpp:void PairLJCharmmCoulCharmmGPU::init_style()
src/GPU/pair_lj_charmm_coul_charmm_gpu.cpp:    error->all(FLERR, "Pair style lj/charmm/coul/long/gpu requires atom attribute q");
src/GPU/pair_lj_charmm_coul_charmm_gpu.cpp:  // Repeated cutsq calculation in init_one() is required for GPU package
src/GPU/pair_lj_charmm_coul_charmm_gpu.cpp:      crm_gpu_init(atom->ntypes + 1, cut_bothsq, lj1, lj2, lj3, lj4, force->special_lj,
src/GPU/pair_lj_charmm_coul_charmm_gpu.cpp:                   atom->nlocal, atom->nlocal + atom->nghost, mnf, maxspecial, cell_size, gpu_mode,
src/GPU/pair_lj_charmm_coul_charmm_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_lj_charmm_coul_charmm_gpu.cpp:  if (gpu_mode == GPU_FORCE) neighbor->add_request(this, NeighConst::REQ_FULL);
src/GPU/pair_lj_charmm_coul_charmm_gpu.cpp:double PairLJCharmmCoulCharmmGPU::memory_usage()
src/GPU/pair_lj_charmm_coul_charmm_gpu.cpp:  return bytes + crm_gpu_bytes();
src/GPU/pair_lj_charmm_coul_charmm_gpu.cpp:void PairLJCharmmCoulCharmmGPU::cpu_compute(int start, int inum, int eflag, int /* vflag */,
src/GPU/pair_tersoff_zbl_gpu.cpp:#include "pair_tersoff_zbl_gpu.h"
src/GPU/pair_tersoff_zbl_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_tersoff_zbl_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_tersoff_zbl_gpu.cpp:int tersoff_zbl_gpu_init(const int ntypes, const int inum, const int nall, const int max_nbors,
src/GPU/pair_tersoff_zbl_gpu.cpp:                         const double cell_size, int &gpu_mode, FILE *screen, int *host_map,
src/GPU/pair_tersoff_zbl_gpu.cpp:void tersoff_zbl_gpu_clear();
src/GPU/pair_tersoff_zbl_gpu.cpp:int **tersoff_zbl_gpu_compute_n(const int ago, const int inum_full, const int nall, double **host_x,
src/GPU/pair_tersoff_zbl_gpu.cpp:void tersoff_zbl_gpu_compute(const int ago, const int nlocal, const int nall, const int nlist,
src/GPU/pair_tersoff_zbl_gpu.cpp:double tersoff_zbl_gpu_bytes();
src/GPU/pair_tersoff_zbl_gpu.cpp:PairTersoffZBLGPU::PairTersoffZBLGPU(LAMMPS *lmp) : PairTersoffZBL(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_tersoff_zbl_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_tersoff_zbl_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_tersoff_zbl_gpu.cpp:PairTersoffZBLGPU::~PairTersoffZBLGPU()
src/GPU/pair_tersoff_zbl_gpu.cpp:  tersoff_zbl_gpu_clear();
src/GPU/pair_tersoff_zbl_gpu.cpp:void PairTersoffZBLGPU::compute(int eflag, int vflag)
src/GPU/pair_tersoff_zbl_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_tersoff_zbl_gpu.cpp:    firstneigh = tersoff_zbl_gpu_compute_n(neighbor->ago, inum, nall, atom->x, atom->type, sublo,
src/GPU/pair_tersoff_zbl_gpu.cpp:    tersoff_zbl_gpu_compute(neighbor->ago, inum, nall, inum + list->gnum, atom->x, atom->type,
src/GPU/pair_tersoff_zbl_gpu.cpp:void PairTersoffZBLGPU::allocate()
src/GPU/pair_tersoff_zbl_gpu.cpp:void PairTersoffZBLGPU::init_style()
src/GPU/pair_tersoff_zbl_gpu.cpp:  if (atom->tag_enable == 0) error->all(FLERR, "Pair style tersoff/zbl/gpu requires atom IDs");
src/GPU/pair_tersoff_zbl_gpu.cpp:  int success = tersoff_zbl_gpu_init(atom->ntypes + 1, atom->nlocal, atom->nlocal + atom->nghost,
src/GPU/pair_tersoff_zbl_gpu.cpp:                                     mnf, cell_size, gpu_mode, screen, map, nelements, elem3param,
src/GPU/pair_tersoff_zbl_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_tersoff_zbl_gpu.cpp:  if (gpu_mode == GPU_FORCE)
src/GPU/pair_tersoff_zbl_gpu.cpp:      error->warning(FLERR, "Increasing communication cutoff to {:.8} for GPU pair style",
src/GPU/pair_tersoff_zbl_gpu.cpp:double PairTersoffZBLGPU::init_one(int i, int j)
src/GPU/pair_edpd_gpu.cpp:#include "pair_edpd_gpu.h"
src/GPU/pair_edpd_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_edpd_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_edpd_gpu.cpp:int edpd_gpu_init(const int ntypes, double **cutsq, double **host_a0, double **host_gamma,
src/GPU/pair_edpd_gpu.cpp:                  const int maxspecial, const double cell_size, int &gpu_mode, FILE *screen);
src/GPU/pair_edpd_gpu.cpp:void edpd_gpu_clear();
src/GPU/pair_edpd_gpu.cpp:int **edpd_gpu_compute_n(const int ago, const int inum_full, const int nall, double **host_x,
src/GPU/pair_edpd_gpu.cpp:void edpd_gpu_compute(const int ago, const int inum_full, const int nall, double **host_x,
src/GPU/pair_edpd_gpu.cpp:void edpd_gpu_get_extra_data(double *host_T, double *host_cv);
src/GPU/pair_edpd_gpu.cpp:void edpd_gpu_update_flux(void **flux_ptr);
src/GPU/pair_edpd_gpu.cpp:double edpd_gpu_bytes();
src/GPU/pair_edpd_gpu.cpp:PairEDPDGPU::PairEDPDGPU(LAMMPS *lmp) : PairEDPD(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_edpd_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_edpd_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_edpd_gpu.cpp:PairEDPDGPU::~PairEDPDGPU()
src/GPU/pair_edpd_gpu.cpp:  edpd_gpu_clear();
src/GPU/pair_edpd_gpu.cpp:void PairEDPDGPU::compute(int eflag, int vflag)
src/GPU/pair_edpd_gpu.cpp:  edpd_gpu_get_extra_data(T, cv);
src/GPU/pair_edpd_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_edpd_gpu.cpp:    firstneigh = edpd_gpu_compute_n(
src/GPU/pair_edpd_gpu.cpp:    edpd_gpu_compute(neighbor->ago, inum, nall, atom->x, atom->type, ilist, numneigh, firstneigh,
src/GPU/pair_edpd_gpu.cpp:  edpd_gpu_update_flux(&flux_pinned);
src/GPU/pair_edpd_gpu.cpp:void PairEDPDGPU::init_style()
src/GPU/pair_edpd_gpu.cpp:      edpd_gpu_init(atom->ntypes + 1, cutsq, a0, gamma, cut, power, kappa,
src/GPU/pair_edpd_gpu.cpp:                    mnf, maxspecial, cell_size, gpu_mode, screen);
src/GPU/pair_edpd_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_edpd_gpu.cpp:  acc_float = Info::has_accelerator_feature("GPU", "precision", "single");
src/GPU/pair_edpd_gpu.cpp:  if (gpu_mode == GPU_FORCE) neighbor->add_request(this, NeighConst::REQ_FULL);
src/GPU/pair_edpd_gpu.cpp:double PairEDPDGPU::memory_usage()
src/GPU/pair_edpd_gpu.cpp:  return bytes + edpd_gpu_bytes();
src/GPU/pair_lj_cubic_gpu.h:PairStyle(lj/cubic/gpu,PairLJCubicGPU);
src/GPU/pair_lj_cubic_gpu.h:#ifndef LMP_PAIR_LJ_CUBIC_GPU_H
src/GPU/pair_lj_cubic_gpu.h:#define LMP_PAIR_LJ_CUBIC_GPU_H
src/GPU/pair_lj_cubic_gpu.h:class PairLJCubicGPU : public PairLJCubic {
src/GPU/pair_lj_cubic_gpu.h:  PairLJCubicGPU(LAMMPS *lmp);
src/GPU/pair_lj_cubic_gpu.h:  ~PairLJCubicGPU() override;
src/GPU/pair_lj_cubic_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_lj_cubic_gpu.h:  int gpu_mode;
src/GPU/pair_hippo_gpu.cpp:#include "pair_hippo_gpu.h"
src/GPU/pair_hippo_gpu.cpp:#include "amoeba_convolution_gpu.h"
src/GPU/pair_hippo_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_hippo_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_hippo_gpu.cpp:int hippo_gpu_init(const int ntypes, const int max_amtype, const int max_amclass,
src/GPU/pair_hippo_gpu.cpp:                    const double cell_size, int &gpu_mode, FILE *screen,
src/GPU/pair_hippo_gpu.cpp:void hippo_gpu_clear();
src/GPU/pair_hippo_gpu.cpp:int** hippo_gpu_precompute(const int ago, const int inum_full, const int nall,
src/GPU/pair_hippo_gpu.cpp:void hippo_gpu_compute_repulsion(const int ago, const int inum_full,
src/GPU/pair_hippo_gpu.cpp:void hippo_gpu_compute_dispersion_real(int *host_amtype, int *host_amgroup, double **host_rpole,
src/GPU/pair_hippo_gpu.cpp:void hippo_gpu_compute_multipole_real(const int ago, const int inum, const int nall,
src/GPU/pair_hippo_gpu.cpp:void hippo_gpu_compute_udirect2b(int *host_amtype, int *host_amgroup,
src/GPU/pair_hippo_gpu.cpp:void hippo_gpu_compute_umutual2b(int *host_amtype, int *host_amgroup,
src/GPU/pair_hippo_gpu.cpp:void hippo_gpu_update_fieldp(void **fieldp_ptr);
src/GPU/pair_hippo_gpu.cpp:void hippo_gpu_precompute_kspace(const int inum_full, const int bsorder,
src/GPU/pair_hippo_gpu.cpp:void hippo_gpu_fphi_uind(double ****host_grid_brick, void **host_fdip_phi1,
src/GPU/pair_hippo_gpu.cpp:void hippo_gpu_compute_polar_real(int *host_amtype, int *host_amgroup,
src/GPU/pair_hippo_gpu.cpp:double hippo_gpu_bytes();
src/GPU/pair_hippo_gpu.cpp:PairHippoGPU::PairHippoGPU(LAMMPS *lmp) : PairAmoeba(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_hippo_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_hippo_gpu.cpp:  gpu_hal_ready = false;              // always false for HIPPO
src/GPU/pair_hippo_gpu.cpp:  gpu_repulsion_ready = true;
src/GPU/pair_hippo_gpu.cpp:  gpu_dispersion_real_ready = true;
src/GPU/pair_hippo_gpu.cpp:  gpu_multipole_real_ready = true;
src/GPU/pair_hippo_gpu.cpp:  gpu_udirect2b_ready = true;
src/GPU/pair_hippo_gpu.cpp:  gpu_umutual1_ready = true;
src/GPU/pair_hippo_gpu.cpp:  gpu_fphi_uind_ready = true;
src/GPU/pair_hippo_gpu.cpp:  gpu_umutual2b_ready = true;
src/GPU/pair_hippo_gpu.cpp:  gpu_polar_real_ready = true;         // need to be true for copying data from device back to host
src/GPU/pair_hippo_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_hippo_gpu.cpp:PairHippoGPU::~PairHippoGPU()
src/GPU/pair_hippo_gpu.cpp:  hippo_gpu_clear();
src/GPU/pair_hippo_gpu.cpp:void PairHippoGPU::compute(int eflag, int vflag)
src/GPU/pair_hippo_gpu.cpp:void PairHippoGPU::init_style()
src/GPU/pair_hippo_gpu.cpp:  int success = hippo_gpu_init(atom->ntypes+1, max_amtype, max_amclass,
src/GPU/pair_hippo_gpu.cpp:                               maxspecial, maxspecial15, cell_size, gpu_mode,
src/GPU/pair_hippo_gpu.cpp:  GPU_EXTRA::check_flag(success,error,world);
src/GPU/pair_hippo_gpu.cpp:  if (gpu_mode == GPU_FORCE)
src/GPU/pair_hippo_gpu.cpp:    error->all(FLERR,"Pair style hippo/gpu does not support neigh no for now");
src/GPU/pair_hippo_gpu.cpp:  acc_float = Info::has_accelerator_feature("GPU", "precision", "single");
src/GPU/pair_hippo_gpu.cpp:  // replace with the gpu counterpart
src/GPU/pair_hippo_gpu.cpp:  if (gpu_umutual1_ready) {
src/GPU/pair_hippo_gpu.cpp:        new AmoebaConvolutionGPU(lmp,this,nefft1,nefft2,nefft3,bsporder,INDUCE_GRIDC);
src/GPU/pair_hippo_gpu.cpp:void PairHippoGPU::repulsion()
src/GPU/pair_hippo_gpu.cpp:  if (!gpu_repulsion_ready) {
src/GPU/pair_hippo_gpu.cpp:  hippo_gpu_precompute(neighbor->ago, inum, nall, atom->x,
src/GPU/pair_hippo_gpu.cpp:  hippo_gpu_compute_repulsion(neighbor->ago, inum, nall, atom->x,
src/GPU/pair_hippo_gpu.cpp:  // reference to the tep array from GPU lib
src/GPU/pair_hippo_gpu.cpp:void PairHippoGPU::dispersion_real()
src/GPU/pair_hippo_gpu.cpp:  if (!gpu_dispersion_real_ready) {
src/GPU/pair_hippo_gpu.cpp:  hippo_gpu_compute_dispersion_real(amtype, amgroup, rpole, aewald, off2);
src/GPU/pair_hippo_gpu.cpp:void PairHippoGPU::multipole_real()
src/GPU/pair_hippo_gpu.cpp:  if (!gpu_multipole_real_ready) {
src/GPU/pair_hippo_gpu.cpp:  hippo_gpu_compute_multipole_real(neighbor->ago, inum, nall, atom->x,
src/GPU/pair_hippo_gpu.cpp:  // reference to the tep array from GPU lib
src/GPU/pair_hippo_gpu.cpp:void PairHippoGPU::induce()
src/GPU/pair_hippo_gpu.cpp:  if (!gpu_udirect2b_ready) {
src/GPU/pair_hippo_gpu.cpp:    hippo_gpu_precompute_kspace(atom->nlocal, bsorder, thetai1, thetai2,
src/GPU/pair_hippo_gpu.cpp:      if (!gpu_umutual2b_ready) {
src/GPU/pair_hippo_gpu.cpp:    if (!gpu_umutual2b_ready) {
src/GPU/pair_hippo_gpu.cpp:      if (!gpu_umutual2b_ready) {
src/GPU/pair_hippo_gpu.cpp:void PairHippoGPU::udirect2b(double **field, double **fieldp)
src/GPU/pair_hippo_gpu.cpp:  if (!gpu_udirect2b_ready) {
src/GPU/pair_hippo_gpu.cpp:  hippo_gpu_compute_udirect2b(amtype, amgroup, rpole, uind, uinp, pval,
src/GPU/pair_hippo_gpu.cpp:  // accumulate the field and fieldp values from the GPU lib
src/GPU/pair_hippo_gpu.cpp:void PairHippoGPU::udirect2b_cpu()
src/GPU/pair_hippo_gpu.cpp:void PairHippoGPU::ufield0c(double **field, double **fieldp)
src/GPU/pair_hippo_gpu.cpp:  // accumulate the field and fieldp values from the real-space portion from umutual2b() on the GPU
src/GPU/pair_hippo_gpu.cpp:  hippo_gpu_update_fieldp(&fieldp_pinned);
src/GPU/pair_hippo_gpu.cpp:void PairHippoGPU::umutual1(double **field, double **fieldp)
src/GPU/pair_hippo_gpu.cpp:void PairHippoGPU::fphi_uind(FFT_SCALAR ****grid, double **fdip_phi1,
src/GPU/pair_hippo_gpu.cpp:  if (!gpu_fphi_uind_ready) {
src/GPU/pair_hippo_gpu.cpp:  hippo_gpu_fphi_uind(grid, &fdip_phi1_pinned, &fdip_phi2_pinned,
src/GPU/pair_hippo_gpu.cpp:void PairHippoGPU::umutual2b(double **field, double **fieldp)
src/GPU/pair_hippo_gpu.cpp:  if (!gpu_umutual2b_ready) {
src/GPU/pair_hippo_gpu.cpp:  hippo_gpu_compute_umutual2b(amtype, amgroup, rpole, uind, uinp, pval,
src/GPU/pair_hippo_gpu.cpp:void PairHippoGPU::polar_real()
src/GPU/pair_hippo_gpu.cpp:  if (!gpu_polar_real_ready) {
src/GPU/pair_hippo_gpu.cpp:  hippo_gpu_compute_polar_real(amtype, amgroup, rpole, uind, uinp, pval,
src/GPU/pair_hippo_gpu.cpp:  // reference to the tep array from GPU lib
src/GPU/pair_hippo_gpu.cpp:void PairHippoGPU::compute_force_from_torque(const numtyp* tq_ptr,
src/GPU/pair_hippo_gpu.cpp:double PairHippoGPU::memory_usage()
src/GPU/pair_hippo_gpu.cpp:  return bytes + hippo_gpu_bytes();
src/GPU/pair_sph_lj_gpu.cpp:#include "pair_sph_lj_gpu.h"
src/GPU/pair_sph_lj_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_sph_lj_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_sph_lj_gpu.cpp:int sph_lj_gpu_init(const int ntypes, double **cutsq, double** host_cut,
src/GPU/pair_sph_lj_gpu.cpp:                    const double cell_size, int &gpu_mode, FILE *screen);
src/GPU/pair_sph_lj_gpu.cpp:void sph_lj_gpu_clear();
src/GPU/pair_sph_lj_gpu.cpp:int **sph_lj_gpu_compute_n(const int ago, const int inum_full, const int nall,
src/GPU/pair_sph_lj_gpu.cpp:void sph_lj_gpu_compute(const int ago, const int inum_full, const int nall,
src/GPU/pair_sph_lj_gpu.cpp:void sph_lj_gpu_get_extra_data(double *host_rho, double *host_esph,
src/GPU/pair_sph_lj_gpu.cpp:void sph_lj_gpu_update_drhoE(void **drhoE_ptr);
src/GPU/pair_sph_lj_gpu.cpp:double sph_lj_gpu_bytes();
src/GPU/pair_sph_lj_gpu.cpp:PairSPHLJGPU::PairSPHLJGPU(LAMMPS *lmp) : PairSPHLJ(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_sph_lj_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_sph_lj_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_sph_lj_gpu.cpp:PairSPHLJGPU::~PairSPHLJGPU()
src/GPU/pair_sph_lj_gpu.cpp:  sph_lj_gpu_clear();
src/GPU/pair_sph_lj_gpu.cpp:void PairSPHLJGPU::compute(int eflag, int vflag)
src/GPU/pair_sph_lj_gpu.cpp:  sph_lj_gpu_get_extra_data(rho, esph, cv);
src/GPU/pair_sph_lj_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_sph_lj_gpu.cpp:    firstneigh = sph_lj_gpu_compute_n(
src/GPU/pair_sph_lj_gpu.cpp:    sph_lj_gpu_compute(neighbor->ago, inum, nall, atom->x, atom->type,
src/GPU/pair_sph_lj_gpu.cpp:  sph_lj_gpu_update_drhoE(&drhoE_pinned);
src/GPU/pair_sph_lj_gpu.cpp:void PairSPHLJGPU::init_style()
src/GPU/pair_sph_lj_gpu.cpp:      sph_lj_gpu_init(atom->ntypes + 1, cutsq, cut, viscosity, atom->mass,
src/GPU/pair_sph_lj_gpu.cpp:                      mnf, maxspecial, cell_size, gpu_mode, screen);
src/GPU/pair_sph_lj_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_sph_lj_gpu.cpp:  acc_float = Info::has_accelerator_feature("GPU", "precision", "single");
src/GPU/pair_sph_lj_gpu.cpp:  if (gpu_mode == GPU_FORCE) neighbor->add_request(this, NeighConst::REQ_FULL);
src/GPU/pair_sph_lj_gpu.cpp:double PairSPHLJGPU::memory_usage()
src/GPU/pair_sph_lj_gpu.cpp:  return bytes + sph_lj_gpu_bytes();
src/GPU/pair_vashishta_gpu.h:PairStyle(vashishta/gpu,PairVashishtaGPU);
src/GPU/pair_vashishta_gpu.h:#ifndef LMP_PAIR_VASHISHTA_GPU_H
src/GPU/pair_vashishta_gpu.h:#define LMP_PAIR_VASHISHTA_GPU_H
src/GPU/pair_vashishta_gpu.h:class PairVashishtaGPU : public PairVashishta {
src/GPU/pair_vashishta_gpu.h:  PairVashishtaGPU(class LAMMPS *);
src/GPU/pair_vashishta_gpu.h:  ~PairVashishtaGPU() override;
src/GPU/pair_vashishta_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_vashishta_gpu.h:  int gpu_allocated;
src/GPU/pair_vashishta_gpu.h:  int gpu_mode;
src/GPU/pair_lj_cut_dipole_cut_gpu.h:PairStyle(lj/cut/dipole/cut/gpu,PairLJCutDipoleCutGPU);
src/GPU/pair_lj_cut_dipole_cut_gpu.h:#ifndef LMP_PAIR_LJ_CUT_DIPOLE_CUT_GPU_H
src/GPU/pair_lj_cut_dipole_cut_gpu.h:#define LMP_PAIR_LJ_CUT_DIPOLE_CUT_GPU_H
src/GPU/pair_lj_cut_dipole_cut_gpu.h:class PairLJCutDipoleCutGPU : public PairLJCutDipoleCut {
src/GPU/pair_lj_cut_dipole_cut_gpu.h:  PairLJCutDipoleCutGPU(LAMMPS *lmp);
src/GPU/pair_lj_cut_dipole_cut_gpu.h:  ~PairLJCutDipoleCutGPU() override;
src/GPU/pair_lj_cut_dipole_cut_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_lj_cut_dipole_cut_gpu.h:  int gpu_mode;
src/GPU/pair_sph_taitwater_gpu.h:PairStyle(sph/taitwater/gpu,PairSPHTaitwaterGPU);
src/GPU/pair_sph_taitwater_gpu.h:#ifndef LMP_PAIR_SPH_TAITWATER_GPU_H
src/GPU/pair_sph_taitwater_gpu.h:#define LMP_PAIR_SPH_TAITWATER_GPU_H
src/GPU/pair_sph_taitwater_gpu.h:class PairSPHTaitwaterGPU : public PairSPHTaitwater {
src/GPU/pair_sph_taitwater_gpu.h:  PairSPHTaitwaterGPU(LAMMPS *lmp);
src/GPU/pair_sph_taitwater_gpu.h:  ~PairSPHTaitwaterGPU() override;
src/GPU/pair_sph_taitwater_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_sph_taitwater_gpu.h:  int gpu_mode;
src/GPU/gpu_extra.h:#ifndef LMP_GPU_EXTRA_H
src/GPU/gpu_extra.h:#define LMP_GPU_EXTRA_H
src/GPU/gpu_extra.h:namespace GPU_EXTRA {
src/GPU/gpu_extra.h:      error->all(FLERR, "The package gpu command is required for gpu styles");
src/GPU/gpu_extra.h:      error->all(FLERR, "GPU library not compiled for this accelerator");
src/GPU/gpu_extra.h:      error->all(FLERR, "GPU particle split must be set to 1 for this pair style.");
src/GPU/gpu_extra.h:      error->all(FLERR, "Invalid custom OpenCL parameter string.");
src/GPU/gpu_extra.h:      error->all(FLERR, "Invalid OpenCL platform ID.");
src/GPU/gpu_extra.h:                 "but GPU device supports single precision only.");
src/GPU/gpu_extra.h:                 "GPU library was compiled for double or mixed precision "
src/GPU/gpu_extra.h:                 "floating point but GPU device supports single precision only.");
src/GPU/gpu_extra.h:      error->all(FLERR, "Unknown error in GPU library");
src/GPU/gpu_extra.h:inline void gpu_ready(LAMMPS_NS::Modify *modify, LAMMPS_NS::Error *error)
src/GPU/gpu_extra.h:  int ifix = modify->find_fix("package_gpu");
src/GPU/gpu_extra.h:  if (ifix < 0) error->all(FLERR, "The package gpu command is required for gpu styles");
src/GPU/gpu_extra.h:}    // namespace GPU_EXTRA
src/GPU/fix_nve_asphere_gpu.cpp:#include "fix_nve_asphere_gpu.h"
src/GPU/fix_nve_asphere_gpu.cpp:#include "gpu_extra.h"
src/GPU/fix_nve_asphere_gpu.cpp:FixNVEAsphereGPU::FixNVEAsphereGPU(LAMMPS *lmp, int narg, char **arg) :
src/GPU/fix_nve_asphere_gpu.cpp:void FixNVEAsphereGPU::init()
src/GPU/fix_nve_asphere_gpu.cpp:void FixNVEAsphereGPU::setup(int vflag)
src/GPU/fix_nve_asphere_gpu.cpp:void FixNVEAsphereGPU::initial_integrate(int /*vflag*/)
src/GPU/fix_nve_asphere_gpu.cpp:void FixNVEAsphereGPU::final_integrate()
src/GPU/fix_nve_asphere_gpu.cpp:      memory->create(_dtfm, _nlocal_max * 3, "fix_nve_gpu:dtfm");
src/GPU/fix_nve_asphere_gpu.cpp:      memory->create(_inertia0, _nlocal_max * 3, "fix_nve_gpu:inertia0");
src/GPU/fix_nve_asphere_gpu.cpp:      memory->create(_inertia1, _nlocal_max * 3, "fix_nve_gpu:inertia1");
src/GPU/fix_nve_asphere_gpu.cpp:      memory->create(_inertia2, _nlocal_max * 3, "fix_nve_gpu:inertia2");
src/GPU/fix_nve_asphere_gpu.cpp:void FixNVEAsphereGPU::reset_dt() {
src/GPU/fix_nve_asphere_gpu.cpp:    memory->create(_dtfm, _nlocal_max * 3, "fix_nve_gpu:dtfm");
src/GPU/fix_nve_asphere_gpu.cpp:    memory->create(_inertia0, _nlocal_max * 3, "fix_nve_gpu:inertia0");
src/GPU/fix_nve_asphere_gpu.cpp:    memory->create(_inertia1, _nlocal_max * 3, "fix_nve_gpu:inertia1");
src/GPU/fix_nve_asphere_gpu.cpp:    memory->create(_inertia2, _nlocal_max * 3, "fix_nve_gpu:inertia2");
src/GPU/fix_nve_asphere_gpu.cpp:double FixNVEAsphereGPU::reset_dt_omp(const int ifrom, const int ito,
src/GPU/fix_nve_asphere_gpu.cpp:double FixNVEAsphereGPU::memory_usage()
src/GPU/pair_lj_smooth_gpu.h:PairStyle(lj/smooth/gpu, PairLJSmoothGPU);
src/GPU/pair_lj_smooth_gpu.h:#ifndef LMP_PAIR_LJ_SMOOTH_GPU_H
src/GPU/pair_lj_smooth_gpu.h:#define LMP_PAIR_LJ_SMOOTH_GPU_H
src/GPU/pair_lj_smooth_gpu.h:class PairLJSmoothGPU : public PairLJSmooth {
src/GPU/pair_lj_smooth_gpu.h:  PairLJSmoothGPU(LAMMPS *lmp);
src/GPU/pair_lj_smooth_gpu.h:  ~PairLJSmoothGPU() override;
src/GPU/pair_lj_smooth_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_lj_smooth_gpu.h:  int gpu_mode;
src/GPU/pair_lj_gromacs_gpu.h:PairStyle(lj/gromacs/gpu,PairLJGromacsGPU);
src/GPU/pair_lj_gromacs_gpu.h:#ifndef LMP_PAIR_LJ_GROMACS_GPU_H
src/GPU/pair_lj_gromacs_gpu.h:#define LMP_PAIR_LJ_GROMACS_GPU_H
src/GPU/pair_lj_gromacs_gpu.h:class PairLJGromacsGPU : public PairLJGromacs {
src/GPU/pair_lj_gromacs_gpu.h:  PairLJGromacsGPU(LAMMPS *lmp);
src/GPU/pair_lj_gromacs_gpu.h:  ~PairLJGromacsGPU() override;
src/GPU/pair_lj_gromacs_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_lj_gromacs_gpu.h:  int gpu_mode;
src/GPU/pair_lj_expand_coul_long_gpu.cpp:#include "pair_lj_expand_coul_long_gpu.h"
src/GPU/pair_lj_expand_coul_long_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_lj_expand_coul_long_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_lj_expand_coul_long_gpu.cpp:int ljecl_gpu_init(const int ntypes, double **cutsq, double **host_lj1, double **host_lj2,
src/GPU/pair_lj_expand_coul_long_gpu.cpp:                   const int maxspecial, const double cell_size, int &gpu_mode, FILE *screen,
src/GPU/pair_lj_expand_coul_long_gpu.cpp:void ljecl_gpu_reinit(const int ntypes, double **cutsq, double **host_lj1, double **host_lj2,
src/GPU/pair_lj_expand_coul_long_gpu.cpp:void ljecl_gpu_clear();
src/GPU/pair_lj_expand_coul_long_gpu.cpp:int **ljecl_gpu_compute_n(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_lj_expand_coul_long_gpu.cpp:void ljecl_gpu_compute(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_lj_expand_coul_long_gpu.cpp:double ljecl_gpu_bytes();
src/GPU/pair_lj_expand_coul_long_gpu.cpp:PairLJExpandCoulLongGPU::PairLJExpandCoulLongGPU(LAMMPS *lmp) :
src/GPU/pair_lj_expand_coul_long_gpu.cpp:    PairLJExpandCoulLong(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_lj_expand_coul_long_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_lj_expand_coul_long_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_lj_expand_coul_long_gpu.cpp:PairLJExpandCoulLongGPU::~PairLJExpandCoulLongGPU()
src/GPU/pair_lj_expand_coul_long_gpu.cpp:  ljecl_gpu_clear();
src/GPU/pair_lj_expand_coul_long_gpu.cpp:void PairLJExpandCoulLongGPU::compute(int eflag, int vflag)
src/GPU/pair_lj_expand_coul_long_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_lj_expand_coul_long_gpu.cpp:    firstneigh = ljecl_gpu_compute_n(neighbor->ago, inum, nall, atom->x, atom->type, sublo, subhi,
src/GPU/pair_lj_expand_coul_long_gpu.cpp:    ljecl_gpu_compute(neighbor->ago, inum, nall, atom->x, atom->type, ilist, numneigh, firstneigh,
src/GPU/pair_lj_expand_coul_long_gpu.cpp:void PairLJExpandCoulLongGPU::init_style()
src/GPU/pair_lj_expand_coul_long_gpu.cpp:  if (!atom->q_flag) error->all(FLERR, "Pair style lj/cut/coul/long/gpu requires atom attribute q");
src/GPU/pair_lj_expand_coul_long_gpu.cpp:  int success = ljecl_gpu_init(atom->ntypes + 1, cutsq, lj1, lj2, lj3, lj4, offset, shift,
src/GPU/pair_lj_expand_coul_long_gpu.cpp:                               maxspecial, cell_size, gpu_mode, screen, cut_ljsq, cut_coulsq,
src/GPU/pair_lj_expand_coul_long_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_lj_expand_coul_long_gpu.cpp:  if (gpu_mode == GPU_FORCE) neighbor->add_request(this, NeighConst::REQ_FULL);
src/GPU/pair_lj_expand_coul_long_gpu.cpp:void PairLJExpandCoulLongGPU::reinit()
src/GPU/pair_lj_expand_coul_long_gpu.cpp:  ljecl_gpu_reinit(atom->ntypes + 1, cutsq, lj1, lj2, lj3, lj4, offset, shift, cut_ljsq);
src/GPU/pair_lj_expand_coul_long_gpu.cpp:double PairLJExpandCoulLongGPU::memory_usage()
src/GPU/pair_lj_expand_coul_long_gpu.cpp:  return bytes + ljecl_gpu_bytes();
src/GPU/pair_lj_expand_coul_long_gpu.cpp:void PairLJExpandCoulLongGPU::cpu_compute(int start, int inum, int eflag, int /* vflag */,
src/GPU/pair_lj_cut_dipole_long_gpu.h:PairStyle(lj/cut/dipole/long/gpu,PairLJCutDipoleLongGPU);
src/GPU/pair_lj_cut_dipole_long_gpu.h:#ifndef LMP_PAIR_LJ_CUT_DIPOLE_LONG_GPU_H
src/GPU/pair_lj_cut_dipole_long_gpu.h:#define LMP_PAIR_LJ_CUT_DIPOLE_LONG_GPU_H
src/GPU/pair_lj_cut_dipole_long_gpu.h:class PairLJCutDipoleLongGPU : public PairLJCutDipoleLong {
src/GPU/pair_lj_cut_dipole_long_gpu.h:  PairLJCutDipoleLongGPU(LAMMPS *lmp);
src/GPU/pair_lj_cut_dipole_long_gpu.h:  ~PairLJCutDipoleLongGPU() override;
src/GPU/pair_lj_cut_dipole_long_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_lj_cut_dipole_long_gpu.h:  int gpu_mode;
src/GPU/pair_lj_charmm_coul_long_gpu.cpp:#include "pair_lj_charmm_coul_long_gpu.h"
src/GPU/pair_lj_charmm_coul_long_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_lj_charmm_coul_long_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_lj_charmm_coul_long_gpu.cpp:int crml_gpu_init(const int ntypes, double cut_bothsq, double **host_lj1, double **host_lj2,
src/GPU/pair_lj_charmm_coul_long_gpu.cpp:                  const double cell_size, int &gpu_mode, FILE *screen, double host_cut_ljsq,
src/GPU/pair_lj_charmm_coul_long_gpu.cpp:void crml_gpu_clear();
src/GPU/pair_lj_charmm_coul_long_gpu.cpp:int **crml_gpu_compute_n(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_lj_charmm_coul_long_gpu.cpp:void crml_gpu_compute(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_lj_charmm_coul_long_gpu.cpp:double crml_gpu_bytes();
src/GPU/pair_lj_charmm_coul_long_gpu.cpp:PairLJCharmmCoulLongGPU::PairLJCharmmCoulLongGPU(LAMMPS *lmp) :
src/GPU/pair_lj_charmm_coul_long_gpu.cpp:    PairLJCharmmCoulLong(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_lj_charmm_coul_long_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_lj_charmm_coul_long_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_lj_charmm_coul_long_gpu.cpp:PairLJCharmmCoulLongGPU::~PairLJCharmmCoulLongGPU()
src/GPU/pair_lj_charmm_coul_long_gpu.cpp:  crml_gpu_clear();
src/GPU/pair_lj_charmm_coul_long_gpu.cpp:void PairLJCharmmCoulLongGPU::compute(int eflag, int vflag)
src/GPU/pair_lj_charmm_coul_long_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_lj_charmm_coul_long_gpu.cpp:    firstneigh = crml_gpu_compute_n(neighbor->ago, inum, nall, atom->x, atom->type, sublo, subhi,
src/GPU/pair_lj_charmm_coul_long_gpu.cpp:    crml_gpu_compute(neighbor->ago, inum, nall, atom->x, atom->type, ilist, numneigh, firstneigh,
src/GPU/pair_lj_charmm_coul_long_gpu.cpp:void PairLJCharmmCoulLongGPU::init_style()
src/GPU/pair_lj_charmm_coul_long_gpu.cpp:    error->all(FLERR, "Pair style lj/charmm/coul/long/gpu requires atom attribute q");
src/GPU/pair_lj_charmm_coul_long_gpu.cpp:      crml_gpu_init(atom->ntypes + 1, cut_bothsq, lj1, lj2, lj3, lj4, offset, force->special_lj,
src/GPU/pair_lj_charmm_coul_long_gpu.cpp:                    atom->nlocal, atom->nlocal + atom->nghost, mnf, maxspecial, cell_size, gpu_mode,
src/GPU/pair_lj_charmm_coul_long_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_lj_charmm_coul_long_gpu.cpp:  if (gpu_mode == GPU_FORCE) neighbor->add_request(this, NeighConst::REQ_FULL);
src/GPU/pair_lj_charmm_coul_long_gpu.cpp:double PairLJCharmmCoulLongGPU::memory_usage()
src/GPU/pair_lj_charmm_coul_long_gpu.cpp:  return bytes + crml_gpu_bytes();
src/GPU/pair_lj_charmm_coul_long_gpu.cpp:void PairLJCharmmCoulLongGPU::cpu_compute(int start, int inum, int eflag, int /* vflag */,
src/GPU/pair_coul_long_gpu.cpp:#include "pair_coul_long_gpu.h"
src/GPU/pair_coul_long_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_coul_long_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_coul_long_gpu.cpp:int cl_gpu_init(const int ntypes, double **scale, const int nlocal, const int nall,
src/GPU/pair_coul_long_gpu.cpp:                const int max_nbors, const int maxspecial, const double cell_size, int &gpu_mode,
src/GPU/pair_coul_long_gpu.cpp:void cl_gpu_reinit(const int ntypes, double **scale);
src/GPU/pair_coul_long_gpu.cpp:void cl_gpu_clear();
src/GPU/pair_coul_long_gpu.cpp:int **cl_gpu_compute_n(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_coul_long_gpu.cpp:void cl_gpu_compute(const int ago, const int inum, const int nall, double **host_x, int *host_type,
src/GPU/pair_coul_long_gpu.cpp:double cl_gpu_bytes();
src/GPU/pair_coul_long_gpu.cpp:PairCoulLongGPU::PairCoulLongGPU(LAMMPS *lmp) : PairCoulLong(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_coul_long_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_coul_long_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_coul_long_gpu.cpp:PairCoulLongGPU::~PairCoulLongGPU()
src/GPU/pair_coul_long_gpu.cpp:  cl_gpu_clear();
src/GPU/pair_coul_long_gpu.cpp:void PairCoulLongGPU::compute(int eflag, int vflag)
src/GPU/pair_coul_long_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_coul_long_gpu.cpp:    firstneigh = cl_gpu_compute_n(neighbor->ago, inum, nall, atom->x, atom->type, sublo, subhi,
src/GPU/pair_coul_long_gpu.cpp:    cl_gpu_compute(neighbor->ago, inum, nall, atom->x, atom->type, ilist, numneigh, firstneigh,
src/GPU/pair_coul_long_gpu.cpp:void PairCoulLongGPU::init_style()
src/GPU/pair_coul_long_gpu.cpp:  if (!atom->q_flag) error->all(FLERR, "Pair style coul/long/gpu requires atom attribute q");
src/GPU/pair_coul_long_gpu.cpp:  int success = cl_gpu_init(atom->ntypes + 1, scale, atom->nlocal, atom->nlocal + atom->nghost, mnf,
src/GPU/pair_coul_long_gpu.cpp:                            maxspecial, cell_size, gpu_mode, screen, cut_coulsq,
src/GPU/pair_coul_long_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_coul_long_gpu.cpp:  if (gpu_mode == GPU_FORCE) neighbor->add_request(this, NeighConst::REQ_FULL);
src/GPU/pair_coul_long_gpu.cpp:void PairCoulLongGPU::reinit()
src/GPU/pair_coul_long_gpu.cpp:  cl_gpu_reinit(atom->ntypes + 1, scale);
src/GPU/pair_coul_long_gpu.cpp:double PairCoulLongGPU::memory_usage()
src/GPU/pair_coul_long_gpu.cpp:  return bytes + cl_gpu_bytes();
src/GPU/pair_coul_long_gpu.cpp:void PairCoulLongGPU::cpu_compute(int start, int inum, int eflag, int /* vflag */, int *ilist,
src/GPU/pair_born_coul_long_gpu.cpp:#include "pair_born_coul_long_gpu.h"
src/GPU/pair_born_coul_long_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_born_coul_long_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_born_coul_long_gpu.cpp:int borncl_gpu_init(const int ntypes, double **cutsq, double **host_rhoinv, double **host_born1,
src/GPU/pair_born_coul_long_gpu.cpp:                    const double cell_size, int &gpu_mode, FILE *screen, double **host_cut_ljsq,
src/GPU/pair_born_coul_long_gpu.cpp:void borncl_gpu_clear();
src/GPU/pair_born_coul_long_gpu.cpp:int **borncl_gpu_compute_n(const int ago, const int inum_full, const int nall, double **host_x,
src/GPU/pair_born_coul_long_gpu.cpp:void borncl_gpu_compute(const int ago, const int inum_full, const int nall, double **host_x,
src/GPU/pair_born_coul_long_gpu.cpp:double borncl_gpu_bytes();
src/GPU/pair_born_coul_long_gpu.cpp:PairBornCoulLongGPU::PairBornCoulLongGPU(LAMMPS *lmp) : PairBornCoulLong(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_born_coul_long_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_born_coul_long_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_born_coul_long_gpu.cpp:PairBornCoulLongGPU::~PairBornCoulLongGPU()
src/GPU/pair_born_coul_long_gpu.cpp:  borncl_gpu_clear();
src/GPU/pair_born_coul_long_gpu.cpp:void PairBornCoulLongGPU::compute(int eflag, int vflag)
src/GPU/pair_born_coul_long_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_born_coul_long_gpu.cpp:    firstneigh = borncl_gpu_compute_n(neighbor->ago, inum, nall, atom->x, atom->type, sublo, subhi,
src/GPU/pair_born_coul_long_gpu.cpp:    borncl_gpu_compute(neighbor->ago, inum, nall, atom->x, atom->type, ilist, numneigh, firstneigh,
src/GPU/pair_born_coul_long_gpu.cpp:void PairBornCoulLongGPU::init_style()
src/GPU/pair_born_coul_long_gpu.cpp:  if (!atom->q_flag) error->all(FLERR, "Pair style born/coul/long/gpu requires atom attribute q");
src/GPU/pair_born_coul_long_gpu.cpp:  int success = borncl_gpu_init(
src/GPU/pair_born_coul_long_gpu.cpp:      gpu_mode, screen, cut_ljsq, cut_coulsq, force->special_coul, force->qqrd2e, g_ewald);
src/GPU/pair_born_coul_long_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_born_coul_long_gpu.cpp:  if (gpu_mode == GPU_FORCE) neighbor->add_request(this, NeighConst::REQ_FULL);
src/GPU/pair_born_coul_long_gpu.cpp:double PairBornCoulLongGPU::memory_usage()
src/GPU/pair_born_coul_long_gpu.cpp:  return bytes + borncl_gpu_bytes();
src/GPU/pair_born_coul_long_gpu.cpp:void PairBornCoulLongGPU::cpu_compute(int start, int inum, int eflag, int /* vflag */, int *ilist,
src/GPU/pair_table_gpu.h:PairStyle(table/gpu,PairTableGPU);
src/GPU/pair_table_gpu.h:#ifndef LMP_PAIR_TABLE_GPU_H
src/GPU/pair_table_gpu.h:#define LMP_PAIR_TABLE_GPU_H
src/GPU/pair_table_gpu.h:class PairTableGPU : public PairTable {
src/GPU/pair_table_gpu.h:  PairTableGPU(LAMMPS *lmp);
src/GPU/pair_table_gpu.h:  ~PairTableGPU() override;
src/GPU/pair_table_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_table_gpu.h:  int gpu_mode;
src/GPU/pair_morse_gpu.h:PairStyle(morse/gpu,PairMorseGPU);
src/GPU/pair_morse_gpu.h:#ifndef LMP_PAIR_MORSE_GPU_H
src/GPU/pair_morse_gpu.h:#define LMP_PAIR_MORSE_GPU_H
src/GPU/pair_morse_gpu.h:class PairMorseGPU : public PairMorse {
src/GPU/pair_morse_gpu.h:  PairMorseGPU(LAMMPS *lmp);
src/GPU/pair_morse_gpu.h:  ~PairMorseGPU() override;
src/GPU/pair_morse_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_morse_gpu.h:  int gpu_mode;
src/GPU/pair_coul_dsf_gpu.cpp:#include "pair_coul_dsf_gpu.h"
src/GPU/pair_coul_dsf_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_coul_dsf_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_coul_dsf_gpu.cpp:int cdsf_gpu_init(const int ntypes, const int nlocal, const int nall, const int max_nbors,
src/GPU/pair_coul_dsf_gpu.cpp:                  const int maxspecial, const double cell_size, int &gpu_mode, FILE *screen,
src/GPU/pair_coul_dsf_gpu.cpp:void cdsf_gpu_clear();
src/GPU/pair_coul_dsf_gpu.cpp:int **cdsf_gpu_compute_n(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_coul_dsf_gpu.cpp:void cdsf_gpu_compute(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_coul_dsf_gpu.cpp:double cdsf_gpu_bytes();
src/GPU/pair_coul_dsf_gpu.cpp:PairCoulDSFGPU::PairCoulDSFGPU(LAMMPS *lmp) : PairCoulDSF(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_coul_dsf_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_coul_dsf_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_coul_dsf_gpu.cpp:PairCoulDSFGPU::~PairCoulDSFGPU()
src/GPU/pair_coul_dsf_gpu.cpp:  cdsf_gpu_clear();
src/GPU/pair_coul_dsf_gpu.cpp:void PairCoulDSFGPU::compute(int eflag, int vflag)
src/GPU/pair_coul_dsf_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_coul_dsf_gpu.cpp:    firstneigh = cdsf_gpu_compute_n(neighbor->ago, inum, nall, atom->x, atom->type, sublo, subhi,
src/GPU/pair_coul_dsf_gpu.cpp:    cdsf_gpu_compute(neighbor->ago, inum, nall, atom->x, atom->type, ilist, numneigh, firstneigh,
src/GPU/pair_coul_dsf_gpu.cpp:void PairCoulDSFGPU::init_style()
src/GPU/pair_coul_dsf_gpu.cpp:  if (!atom->q_flag) error->all(FLERR, "Pair style coul/dsf/gpu requires atom attribute q");
src/GPU/pair_coul_dsf_gpu.cpp:  int success = cdsf_gpu_init(atom->ntypes + 1, atom->nlocal, atom->nlocal + atom->nghost, mnf,
src/GPU/pair_coul_dsf_gpu.cpp:                              maxspecial, cell_size, gpu_mode, screen, cut_coulsq,
src/GPU/pair_coul_dsf_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_coul_dsf_gpu.cpp:  if (gpu_mode == GPU_FORCE) neighbor->add_request(this, NeighConst::REQ_FULL);
src/GPU/pair_coul_dsf_gpu.cpp:double PairCoulDSFGPU::memory_usage()
src/GPU/pair_coul_dsf_gpu.cpp:  return bytes + cdsf_gpu_bytes();
src/GPU/pair_coul_dsf_gpu.cpp:void PairCoulDSFGPU::cpu_compute(int start, int inum, int eflag, int /* vflag */, int *ilist,
src/GPU/pair_lj_cut_coul_cut_soft_gpu.h:PairStyle(lj/cut/coul/cut/soft/gpu,PairLJCutCoulCutSoftGPU);
src/GPU/pair_lj_cut_coul_cut_soft_gpu.h:#ifndef LMP_PAIR_LJ_CUT_COUL_CUT_SOFT_GPU_H
src/GPU/pair_lj_cut_coul_cut_soft_gpu.h:#define LMP_PAIR_LJ_CUT_COUL_CUT_SOFT_GPU_H
src/GPU/pair_lj_cut_coul_cut_soft_gpu.h:class PairLJCutCoulCutSoftGPU : public PairLJCutCoulCutSoft {
src/GPU/pair_lj_cut_coul_cut_soft_gpu.h:  PairLJCutCoulCutSoftGPU(LAMMPS *lmp);
src/GPU/pair_lj_cut_coul_cut_soft_gpu.h:  ~PairLJCutCoulCutSoftGPU() override;
src/GPU/pair_lj_cut_coul_cut_soft_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_lj_cut_coul_cut_soft_gpu.h:  int gpu_mode;
src/GPU/pair_mie_cut_gpu.h:PairStyle(mie/cut/gpu,PairMIECutGPU);
src/GPU/pair_mie_cut_gpu.h:#ifndef LMP_PAIR_MIE_CUT_GPU_H
src/GPU/pair_mie_cut_gpu.h:#define LMP_PAIR_MIE_CUT_GPU_H
src/GPU/pair_mie_cut_gpu.h:class PairMIECutGPU : public PairMIECut {
src/GPU/pair_mie_cut_gpu.h:  PairMIECutGPU(LAMMPS *lmp);
src/GPU/pair_mie_cut_gpu.h:  ~PairMIECutGPU() override;
src/GPU/pair_mie_cut_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_mie_cut_gpu.h:  int gpu_mode;
src/GPU/pair_lj_cut_tip4p_long_gpu.h:PairStyle(lj/cut/tip4p/long/gpu,PairLJCutTIP4PLongGPU);
src/GPU/pair_lj_cut_tip4p_long_gpu.h:#ifndef LMP_PAIR_LJ_TIP4P_LONG_GPU_H
src/GPU/pair_lj_cut_tip4p_long_gpu.h:#define LMP_PAIR_LJ_TIP4P_LONG_GPU_H
src/GPU/pair_lj_cut_tip4p_long_gpu.h:class PairLJCutTIP4PLongGPU : public PairLJCutTIP4PLong {
src/GPU/pair_lj_cut_tip4p_long_gpu.h:  PairLJCutTIP4PLongGPU(LAMMPS *lmp);
src/GPU/pair_lj_cut_tip4p_long_gpu.h:  ~PairLJCutTIP4PLongGPU() override;
src/GPU/pair_lj_cut_tip4p_long_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_lj_cut_tip4p_long_gpu.h:  int gpu_mode;
src/GPU/pair_dpd_gpu.cpp:#include "pair_dpd_gpu.h"
src/GPU/pair_dpd_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_dpd_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_dpd_gpu.cpp:int dpd_gpu_init(const int ntypes, double **cutsq, double **host_a0, double **host_gamma,
src/GPU/pair_dpd_gpu.cpp:                 int &gpu_mode, FILE *screen);
src/GPU/pair_dpd_gpu.cpp:void dpd_gpu_clear();
src/GPU/pair_dpd_gpu.cpp:int **dpd_gpu_compute_n(const int ago, const int inum_full, const int nall, double **host_x,
src/GPU/pair_dpd_gpu.cpp:void dpd_gpu_compute(const int ago, const int inum_full, const int nall, double **host_x,
src/GPU/pair_dpd_gpu.cpp:double dpd_gpu_bytes();
src/GPU/pair_dpd_gpu.cpp:PairDPDGPU::PairDPDGPU(LAMMPS *lmp) : PairDPD(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_dpd_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_dpd_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_dpd_gpu.cpp:PairDPDGPU::~PairDPDGPU()
src/GPU/pair_dpd_gpu.cpp:  dpd_gpu_clear();
src/GPU/pair_dpd_gpu.cpp:void PairDPDGPU::compute(int eflag, int vflag)
src/GPU/pair_dpd_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_dpd_gpu.cpp:    firstneigh = dpd_gpu_compute_n(
src/GPU/pair_dpd_gpu.cpp:    dpd_gpu_compute(neighbor->ago, inum, nall, atom->x, atom->type, ilist, numneigh, firstneigh,
src/GPU/pair_dpd_gpu.cpp:void PairDPDGPU::init_style()
src/GPU/pair_dpd_gpu.cpp:      dpd_gpu_init(atom->ntypes + 1, cutsq, a0, gamma, sigma, cut, force->special_lj, atom->nlocal,
src/GPU/pair_dpd_gpu.cpp:                   atom->nlocal + atom->nghost, mnf, maxspecial, cell_size, gpu_mode, screen);
src/GPU/pair_dpd_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_dpd_gpu.cpp:  if (gpu_mode == GPU_FORCE) neighbor->add_request(this, NeighConst::REQ_FULL);
src/GPU/pair_dpd_gpu.cpp:double PairDPDGPU::memory_usage()
src/GPU/pair_dpd_gpu.cpp:  return bytes + dpd_gpu_bytes();
src/GPU/pair_dpd_gpu.cpp:void PairDPDGPU::cpu_compute(int start, int inum, int eflag, int /* vflag */, int *ilist,
src/GPU/pair_sph_heatconduction_gpu.h:PairStyle(sph/heatconduction/gpu,PairSPHHeatConductionGPU);
src/GPU/pair_sph_heatconduction_gpu.h:#ifndef LMP_PAIR_SPH_HEATCONDUCTION_GPU_H
src/GPU/pair_sph_heatconduction_gpu.h:#define LMP_PAIR_SPH_HEATCONDUCTION_GPU_H
src/GPU/pair_sph_heatconduction_gpu.h:class PairSPHHeatConductionGPU : public PairSPHHeatConduction {
src/GPU/pair_sph_heatconduction_gpu.h:  PairSPHHeatConductionGPU(LAMMPS *lmp);
src/GPU/pair_sph_heatconduction_gpu.h:  ~PairSPHHeatConductionGPU() override;
src/GPU/pair_sph_heatconduction_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_sph_heatconduction_gpu.h:  int gpu_mode;
src/GPU/pair_coul_slater_long_gpu.h:PairStyle(coul/slater/long/gpu,PairCoulSlaterLongGPU);
src/GPU/pair_coul_slater_long_gpu.h:#ifndef LMP_PAIR_COUL_SLATER_LONG_GPU_H
src/GPU/pair_coul_slater_long_gpu.h:#define LMP_PAIR_COUL_SLATER_LONG_GPU_H
src/GPU/pair_coul_slater_long_gpu.h:class PairCoulSlaterLongGPU : public PairCoulSlaterLong {
src/GPU/pair_coul_slater_long_gpu.h:  PairCoulSlaterLongGPU(LAMMPS *lmp);
src/GPU/pair_coul_slater_long_gpu.h:  ~PairCoulSlaterLongGPU() override;
src/GPU/pair_coul_slater_long_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_coul_slater_long_gpu.h:  int gpu_mode;
src/GPU/pair_soft_gpu.h:PairStyle(soft/gpu,PairSoftGPU);
src/GPU/pair_soft_gpu.h:#ifndef LMP_PAIR_SOFT_GPU_H
src/GPU/pair_soft_gpu.h:#define LMP_PAIR_SOFT_GPU_H
src/GPU/pair_soft_gpu.h:class PairSoftGPU : public PairSoft {
src/GPU/pair_soft_gpu.h:  PairSoftGPU(LAMMPS *lmp);
src/GPU/pair_soft_gpu.h:  ~PairSoftGPU() override;
src/GPU/pair_soft_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_soft_gpu.h:  int gpu_mode;
src/GPU/pair_eam_alloy_gpu.cpp:#include "pair_eam_alloy_gpu.h"
src/GPU/pair_eam_alloy_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_eam_alloy_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_eam_alloy_gpu.cpp:int eam_alloy_gpu_init(const int ntypes, double host_cutforcesq, int **host_type2rhor,
src/GPU/pair_eam_alloy_gpu.cpp:                       const int maxspecial, const double cell_size, int &gpu_mode, FILE *screen,
src/GPU/pair_eam_alloy_gpu.cpp:void eam_alloy_gpu_clear();
src/GPU/pair_eam_alloy_gpu.cpp:int **eam_alloy_gpu_compute_n(const int ago, const int inum_full, const int nall, double **host_x,
src/GPU/pair_eam_alloy_gpu.cpp:void eam_alloy_gpu_compute(const int ago, const int inum_full, const int nlocal, const int nall,
src/GPU/pair_eam_alloy_gpu.cpp:void eam_alloy_gpu_compute_force(int *ilist, const bool eflag, const bool vflag, const bool eatom,
src/GPU/pair_eam_alloy_gpu.cpp:double eam_alloy_gpu_bytes();
src/GPU/pair_eam_alloy_gpu.cpp:PairEAMAlloyGPU::PairEAMAlloyGPU(LAMMPS *lmp) : PairEAM(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_eam_alloy_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_eam_alloy_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_eam_alloy_gpu.cpp:PairEAMAlloyGPU::~PairEAMAlloyGPU()
src/GPU/pair_eam_alloy_gpu.cpp:  eam_alloy_gpu_clear();
src/GPU/pair_eam_alloy_gpu.cpp:double PairEAMAlloyGPU::memory_usage()
src/GPU/pair_eam_alloy_gpu.cpp:  return bytes + eam_alloy_gpu_bytes();
src/GPU/pair_eam_alloy_gpu.cpp:void PairEAMAlloyGPU::compute(int eflag, int vflag)
src/GPU/pair_eam_alloy_gpu.cpp:  // compute density on each atom on GPU
src/GPU/pair_eam_alloy_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_eam_alloy_gpu.cpp:    firstneigh = eam_alloy_gpu_compute_n(neighbor->ago, inum, nall, atom->x, atom->type, sublo,
src/GPU/pair_eam_alloy_gpu.cpp:  } else {    // gpu_mode == GPU_FORCE
src/GPU/pair_eam_alloy_gpu.cpp:    eam_alloy_gpu_compute(neighbor->ago, inum, nlocal, nall, atom->x, atom->type, ilist, numneigh,
src/GPU/pair_eam_alloy_gpu.cpp:  // compute forces on each atom on GPU
src/GPU/pair_eam_alloy_gpu.cpp:  if (gpu_mode != GPU_FORCE)
src/GPU/pair_eam_alloy_gpu.cpp:    eam_alloy_gpu_compute_force(nullptr, eflag, vflag, eflag_atom, vflag_atom);
src/GPU/pair_eam_alloy_gpu.cpp:    eam_alloy_gpu_compute_force(ilist, eflag, vflag, eflag_atom, vflag_atom);
src/GPU/pair_eam_alloy_gpu.cpp:void PairEAMAlloyGPU::init_style()
src/GPU/pair_eam_alloy_gpu.cpp:  int success = eam_alloy_gpu_init(
src/GPU/pair_eam_alloy_gpu.cpp:      atom->nlocal + atom->nghost, mnf, maxspecial, cell_size, gpu_mode, screen, fp_size);
src/GPU/pair_eam_alloy_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_eam_alloy_gpu.cpp:  if (gpu_mode == GPU_FORCE) neighbor->add_request(this, NeighConst::REQ_FULL);
src/GPU/pair_eam_alloy_gpu.cpp:double PairEAMAlloyGPU::single(int i, int j, int itype, int jtype, double rsq,
src/GPU/pair_eam_alloy_gpu.cpp:int PairEAMAlloyGPU::pack_forward_comm(int n, int *list, double *buf, int /* pbc_flag */,
src/GPU/pair_eam_alloy_gpu.cpp:void PairEAMAlloyGPU::unpack_forward_comm(int n, int first, double *buf)
src/GPU/pair_eam_alloy_gpu.cpp:void PairEAMAlloyGPU::coeff(int narg, char **arg)
src/GPU/pair_eam_alloy_gpu.cpp:void PairEAMAlloyGPU::read_file(char *filename)
src/GPU/pair_eam_alloy_gpu.cpp:void PairEAMAlloyGPU::file2array()
src/GPU/pair_sph_taitwater_gpu.cpp:#include "pair_sph_taitwater_gpu.h"
src/GPU/pair_sph_taitwater_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_sph_taitwater_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_sph_taitwater_gpu.cpp:int sph_taitwater_gpu_init(const int ntypes, double **cutsq, double** host_cut,
src/GPU/pair_sph_taitwater_gpu.cpp:                           const double cell_size, int &gpu_mode, FILE *screen);
src/GPU/pair_sph_taitwater_gpu.cpp:void sph_taitwater_gpu_clear();
src/GPU/pair_sph_taitwater_gpu.cpp:int **sph_taitwater_gpu_compute_n(const int ago, const int inum_full, const int nall,
src/GPU/pair_sph_taitwater_gpu.cpp:void sph_taitwater_gpu_compute(const int ago, const int inum_full, const int nall,
src/GPU/pair_sph_taitwater_gpu.cpp:void sph_taitwater_gpu_get_extra_data(double *host_rho);
src/GPU/pair_sph_taitwater_gpu.cpp:void sph_taitwater_gpu_update_drhoE(void **drhoE_ptr);
src/GPU/pair_sph_taitwater_gpu.cpp:double sph_taitwater_gpu_bytes();
src/GPU/pair_sph_taitwater_gpu.cpp:PairSPHTaitwaterGPU::PairSPHTaitwaterGPU(LAMMPS *lmp) : PairSPHTaitwater(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_sph_taitwater_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_sph_taitwater_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_sph_taitwater_gpu.cpp:PairSPHTaitwaterGPU::~PairSPHTaitwaterGPU()
src/GPU/pair_sph_taitwater_gpu.cpp:  sph_taitwater_gpu_clear();
src/GPU/pair_sph_taitwater_gpu.cpp:void PairSPHTaitwaterGPU::compute(int eflag, int vflag)
src/GPU/pair_sph_taitwater_gpu.cpp:  sph_taitwater_gpu_get_extra_data(rho);
src/GPU/pair_sph_taitwater_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_sph_taitwater_gpu.cpp:    firstneigh = sph_taitwater_gpu_compute_n(
src/GPU/pair_sph_taitwater_gpu.cpp:    sph_taitwater_gpu_compute(neighbor->ago, inum, nall, atom->x, atom->type, ilist, numneigh, firstneigh,
src/GPU/pair_sph_taitwater_gpu.cpp:  sph_taitwater_gpu_update_drhoE(&drhoE_pinned);
src/GPU/pair_sph_taitwater_gpu.cpp:void PairSPHTaitwaterGPU::init_style()
src/GPU/pair_sph_taitwater_gpu.cpp:      sph_taitwater_gpu_init(atom->ntypes + 1, cutsq, cut, viscosity, atom->mass,
src/GPU/pair_sph_taitwater_gpu.cpp:                             mnf, maxspecial, cell_size, gpu_mode, screen);
src/GPU/pair_sph_taitwater_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_sph_taitwater_gpu.cpp:  acc_float = Info::has_accelerator_feature("GPU", "precision", "single");
src/GPU/pair_sph_taitwater_gpu.cpp:  if (gpu_mode == GPU_FORCE) neighbor->add_request(this, NeighConst::REQ_FULL);
src/GPU/pair_sph_taitwater_gpu.cpp:double PairSPHTaitwaterGPU::memory_usage()
src/GPU/pair_sph_taitwater_gpu.cpp:  return bytes + sph_taitwater_gpu_bytes();
src/GPU/fix_nh_gpu.cpp:#include "fix_nh_gpu.h"
src/GPU/fix_nh_gpu.cpp:#include "gpu_extra.h"
src/GPU/fix_nh_gpu.cpp:FixNHGPU::FixNHGPU(LAMMPS *lmp, int narg, char **arg) :
src/GPU/fix_nh_gpu.cpp:void FixNHGPU::setup(int vflag)
src/GPU/fix_nh_gpu.cpp:void FixNHGPU::remap()
src/GPU/fix_nh_gpu.cpp:void FixNHGPU::final_integrate() {
src/GPU/fix_nh_gpu.cpp:void FixNHGPU::reset_dt()
src/GPU/fix_nh_gpu.cpp:    memory->create(_dtfm, _nlocal_max * 3, "fix_nh_gpu:dtfm");
src/GPU/fix_nh_gpu.cpp:void FixNHGPU::nh_v_press()
src/GPU/fix_nh_gpu.cpp:void FixNHGPU::nve_v()
src/GPU/fix_nh_gpu.cpp:void FixNHGPU::nve_x()
src/GPU/fix_nh_gpu.cpp:void FixNHGPU::nh_v_temp()
src/GPU/fix_nh_gpu.cpp:double FixNHGPU::memory_usage()
src/GPU/pair_lj_cut_coul_msm_gpu.cpp:#include "pair_lj_cut_coul_msm_gpu.h"
src/GPU/pair_lj_cut_coul_msm_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_lj_cut_coul_msm_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_lj_cut_coul_msm_gpu.cpp:int ljcm_gpu_init(const int ntypes, double **cutsq, double **host_lj1, double **host_lj2,
src/GPU/pair_lj_cut_coul_msm_gpu.cpp:                  const int max_nbors, const int maxspecial, const double cell_size, int &gpu_mode,
src/GPU/pair_lj_cut_coul_msm_gpu.cpp:void ljcm_gpu_clear();
src/GPU/pair_lj_cut_coul_msm_gpu.cpp:int **ljcm_gpu_compute_n(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_lj_cut_coul_msm_gpu.cpp:void ljcm_gpu_compute(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_lj_cut_coul_msm_gpu.cpp:double ljcm_gpu_bytes();
src/GPU/pair_lj_cut_coul_msm_gpu.cpp:PairLJCutCoulMSMGPU::PairLJCutCoulMSMGPU(LAMMPS *lmp) : PairLJCutCoulMSM(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_lj_cut_coul_msm_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_lj_cut_coul_msm_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_lj_cut_coul_msm_gpu.cpp:PairLJCutCoulMSMGPU::~PairLJCutCoulMSMGPU()
src/GPU/pair_lj_cut_coul_msm_gpu.cpp:  ljcm_gpu_clear();
src/GPU/pair_lj_cut_coul_msm_gpu.cpp:void PairLJCutCoulMSMGPU::compute(int eflag, int vflag)
src/GPU/pair_lj_cut_coul_msm_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_lj_cut_coul_msm_gpu.cpp:    firstneigh = ljcm_gpu_compute_n(neighbor->ago, inum, nall, atom->x, atom->type, sublo, subhi,
src/GPU/pair_lj_cut_coul_msm_gpu.cpp:    ljcm_gpu_compute(neighbor->ago, inum, nall, atom->x, atom->type, ilist, numneigh, firstneigh,
src/GPU/pair_lj_cut_coul_msm_gpu.cpp:void PairLJCutCoulMSMGPU::init_style()
src/GPU/pair_lj_cut_coul_msm_gpu.cpp:  if (!atom->q_flag) error->all(FLERR, "Pair style lj/cut/coul/cut/gpu requires atom attribute q");
src/GPU/pair_lj_cut_coul_msm_gpu.cpp:    error->all(FLERR, "Must use 'kspace_modify pressure/scalar no' with GPU MSM Pair styles");
src/GPU/pair_lj_cut_coul_msm_gpu.cpp:      ljcm_gpu_init(atom->ntypes + 1, cutsq, lj1, lj2, lj3, lj4, force->kspace->get_gcons(),
src/GPU/pair_lj_cut_coul_msm_gpu.cpp:                    atom->nlocal + atom->nghost, mnf, maxspecial, cell_size, gpu_mode, screen,
src/GPU/pair_lj_cut_coul_msm_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_lj_cut_coul_msm_gpu.cpp:  if (gpu_mode == GPU_FORCE) neighbor->add_request(this, NeighConst::REQ_FULL);
src/GPU/pair_lj_cut_coul_msm_gpu.cpp:double PairLJCutCoulMSMGPU::memory_usage()
src/GPU/pair_lj_cut_coul_msm_gpu.cpp:  return bytes + ljcm_gpu_bytes();
src/GPU/pair_lj_cut_coul_msm_gpu.cpp:void PairLJCutCoulMSMGPU::cpu_compute(int start, int inum, int eflag, int /* vflag */, int *ilist,
src/GPU/pair_lj_charmm_coul_charmm_gpu.h:PairStyle(lj/charmm/coul/charmm/gpu,PairLJCharmmCoulCharmmGPU);
src/GPU/pair_lj_charmm_coul_charmm_gpu.h:#ifndef LMP_PAIR_LJ_CHARMM_COUL_CHARMM_GPU_H
src/GPU/pair_lj_charmm_coul_charmm_gpu.h:#define LMP_PAIR_LJ_CHARMM_COUL_CHARMM_GPU_H
src/GPU/pair_lj_charmm_coul_charmm_gpu.h:class PairLJCharmmCoulCharmmGPU : public PairLJCharmmCoulCharmm {
src/GPU/pair_lj_charmm_coul_charmm_gpu.h:  PairLJCharmmCoulCharmmGPU(LAMMPS *lmp);
src/GPU/pair_lj_charmm_coul_charmm_gpu.h:  ~PairLJCharmmCoulCharmmGPU() override;
src/GPU/pair_lj_charmm_coul_charmm_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_lj_charmm_coul_charmm_gpu.h:  int gpu_mode;
src/GPU/pair_colloid_gpu.cpp:#include "pair_colloid_gpu.h"
src/GPU/pair_colloid_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_colloid_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_colloid_gpu.cpp:int colloid_gpu_init(const int ntypes, double **cutsq, double **host_lj1, double **host_lj2,
src/GPU/pair_colloid_gpu.cpp:                     const double cell_size, int &gpu_mode, FILE *screen);
src/GPU/pair_colloid_gpu.cpp:void colloid_gpu_clear();
src/GPU/pair_colloid_gpu.cpp:int **colloid_gpu_compute_n(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_colloid_gpu.cpp:void colloid_gpu_compute(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_colloid_gpu.cpp:double colloid_gpu_bytes();
src/GPU/pair_colloid_gpu.cpp:PairColloidGPU::PairColloidGPU(LAMMPS *lmp) : PairColloid(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_colloid_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_colloid_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_colloid_gpu.cpp:PairColloidGPU::~PairColloidGPU()
src/GPU/pair_colloid_gpu.cpp:  colloid_gpu_clear();
src/GPU/pair_colloid_gpu.cpp:void PairColloidGPU::compute(int eflag, int vflag)
src/GPU/pair_colloid_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_colloid_gpu.cpp:        colloid_gpu_compute_n(neighbor->ago, inum, nall, atom->x, atom->type, sublo, subhi,
src/GPU/pair_colloid_gpu.cpp:    colloid_gpu_compute(neighbor->ago, inum, nall, atom->x, atom->type, ilist, numneigh, firstneigh,
src/GPU/pair_colloid_gpu.cpp:void PairColloidGPU::init_style()
src/GPU/pair_colloid_gpu.cpp:  memory->create(_form, n + 1, n + 1, "colloid/gpu:_form");
src/GPU/pair_colloid_gpu.cpp:      colloid_gpu_init(atom->ntypes + 1, cutsq, lj1, lj2, lj3, lj4, offset, force->special_lj, a12,
src/GPU/pair_colloid_gpu.cpp:                       atom->nlocal + atom->nghost, mnf, maxspecial, cell_size, gpu_mode, screen);
src/GPU/pair_colloid_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_colloid_gpu.cpp:  if (gpu_mode == GPU_FORCE) neighbor->add_request(this, NeighConst::REQ_FULL);
src/GPU/pair_colloid_gpu.cpp:double PairColloidGPU::memory_usage()
src/GPU/pair_colloid_gpu.cpp:  return bytes + colloid_gpu_bytes();
src/GPU/pair_colloid_gpu.cpp:void PairColloidGPU::cpu_compute(int start, int inum, int eflag, int /* vflag */, int *ilist,
src/GPU/pair_tersoff_zbl_gpu.h:PairStyle(tersoff/zbl/gpu,PairTersoffZBLGPU);
src/GPU/pair_tersoff_zbl_gpu.h:#ifndef LMP_PAIR_TERSOFF_ZBL_GPU_H
src/GPU/pair_tersoff_zbl_gpu.h:#define LMP_PAIR_TERSOFF_ZBL_GPU_H
src/GPU/pair_tersoff_zbl_gpu.h:class PairTersoffZBLGPU : public PairTersoffZBL {
src/GPU/pair_tersoff_zbl_gpu.h:  PairTersoffZBLGPU(class LAMMPS *);
src/GPU/pair_tersoff_zbl_gpu.h:  ~PairTersoffZBLGPU() override;
src/GPU/pair_tersoff_zbl_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_tersoff_zbl_gpu.h:  int gpu_mode;
src/GPU/pair_tersoff_zbl_gpu.h:  int *gpulist;
src/GPU/pair_lj_gromacs_gpu.cpp:#include "pair_lj_gromacs_gpu.h"
src/GPU/pair_lj_gromacs_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_lj_gromacs_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_lj_gromacs_gpu.cpp:int ljgrm_gpu_init(const int ntypes, double **cutsq, double **host_lj1, double **host_lj2,
src/GPU/pair_lj_gromacs_gpu.cpp:                   const double cell_size, int &gpu_mode, FILE *screen, double **host_ljsw1,
src/GPU/pair_lj_gromacs_gpu.cpp:void ljgrm_gpu_clear();
src/GPU/pair_lj_gromacs_gpu.cpp:int **ljgrm_gpu_compute_n(const int ago, const int inum_full, const int nall, double **host_x,
src/GPU/pair_lj_gromacs_gpu.cpp:void ljgrm_gpu_compute(const int ago, const int inum_full, const int nall, double **host_x,
src/GPU/pair_lj_gromacs_gpu.cpp:double ljgrm_gpu_bytes();
src/GPU/pair_lj_gromacs_gpu.cpp:PairLJGromacsGPU::PairLJGromacsGPU(LAMMPS *lmp) : PairLJGromacs(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_lj_gromacs_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_lj_gromacs_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_lj_gromacs_gpu.cpp:PairLJGromacsGPU::~PairLJGromacsGPU()
src/GPU/pair_lj_gromacs_gpu.cpp:  ljgrm_gpu_clear();
src/GPU/pair_lj_gromacs_gpu.cpp:void PairLJGromacsGPU::compute(int eflag, int vflag)
src/GPU/pair_lj_gromacs_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_lj_gromacs_gpu.cpp:        ljgrm_gpu_compute_n(neighbor->ago, inum, nall, atom->x, atom->type, sublo, subhi, atom->tag,
src/GPU/pair_lj_gromacs_gpu.cpp:    ljgrm_gpu_compute(neighbor->ago, inum, nall, atom->x, atom->type, ilist, numneigh, firstneigh,
src/GPU/pair_lj_gromacs_gpu.cpp:void PairLJGromacsGPU::init_style()
src/GPU/pair_lj_gromacs_gpu.cpp:      ljgrm_gpu_init(atom->ntypes + 1, cutsq, lj1, lj2, lj3, lj4, force->special_lj, atom->nlocal,
src/GPU/pair_lj_gromacs_gpu.cpp:                     atom->nlocal + atom->nghost, mnf, maxspecial, cell_size, gpu_mode, screen,
src/GPU/pair_lj_gromacs_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_lj_gromacs_gpu.cpp:  if (gpu_mode == GPU_FORCE) neighbor->add_request(this, NeighConst::REQ_FULL);
src/GPU/pair_lj_gromacs_gpu.cpp:double PairLJGromacsGPU::memory_usage()
src/GPU/pair_lj_gromacs_gpu.cpp:  return bytes + ljgrm_gpu_bytes();
src/GPU/pair_lj_gromacs_gpu.cpp:void PairLJGromacsGPU::cpu_compute(int start, int inum, int eflag, int /* vflag */, int *ilist,
src/GPU/fix_nh_gpu.h:#ifndef LMP_FIX_NH_GPU_H
src/GPU/fix_nh_gpu.h:#define LMP_FIX_NH_GPU_H
src/GPU/fix_nh_gpu.h:class FixNHGPU : public FixNH {
src/GPU/fix_nh_gpu.h:  FixNHGPU(class LAMMPS *, int, char **);
src/GPU/pair_eam_gpu.cpp:#include "pair_eam_gpu.h"
src/GPU/pair_eam_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_eam_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_eam_gpu.cpp:int eam_gpu_init(const int ntypes, double host_cutforcesq, int **host_type2rhor,
src/GPU/pair_eam_gpu.cpp:                 const int maxspecial, const double cell_size, int &gpu_mode, FILE *screen,
src/GPU/pair_eam_gpu.cpp:void eam_gpu_clear();
src/GPU/pair_eam_gpu.cpp:int **eam_gpu_compute_n(const int ago, const int inum_full, const int nall, double **host_x,
src/GPU/pair_eam_gpu.cpp:void eam_gpu_compute(const int ago, const int inum_full, const int nlocal, const int nall,
src/GPU/pair_eam_gpu.cpp:void eam_gpu_compute_force(int *ilist, const bool eflag, const bool vflag, const bool eatom,
src/GPU/pair_eam_gpu.cpp:double eam_gpu_bytes();
src/GPU/pair_eam_gpu.cpp:PairEAMGPU::PairEAMGPU(LAMMPS *lmp) : PairEAM(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_eam_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_eam_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_eam_gpu.cpp:PairEAMGPU::~PairEAMGPU()
src/GPU/pair_eam_gpu.cpp:  eam_gpu_clear();
src/GPU/pair_eam_gpu.cpp:double PairEAMGPU::memory_usage()
src/GPU/pair_eam_gpu.cpp:  return bytes + eam_gpu_bytes();
src/GPU/pair_eam_gpu.cpp:void PairEAMGPU::compute(int eflag, int vflag)
src/GPU/pair_eam_gpu.cpp:  // compute density on each atom on GPU
src/GPU/pair_eam_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_eam_gpu.cpp:        eam_gpu_compute_n(neighbor->ago, inum, nall, atom->x, atom->type, sublo, subhi, atom->tag,
src/GPU/pair_eam_gpu.cpp:  } else {    // gpu_mode == GPU_FORCE
src/GPU/pair_eam_gpu.cpp:    eam_gpu_compute(neighbor->ago, inum, nlocal, nall, atom->x, atom->type, ilist, numneigh,
src/GPU/pair_eam_gpu.cpp:  // compute forces on each atom on GPU
src/GPU/pair_eam_gpu.cpp:  if (gpu_mode != GPU_FORCE)
src/GPU/pair_eam_gpu.cpp:    eam_gpu_compute_force(nullptr, eflag, vflag, eflag_atom, vflag_atom);
src/GPU/pair_eam_gpu.cpp:    eam_gpu_compute_force(ilist, eflag, vflag, eflag_atom, vflag_atom);
src/GPU/pair_eam_gpu.cpp:void PairEAMGPU::init_style()
src/GPU/pair_eam_gpu.cpp:  int success = eam_gpu_init(atom->ntypes + 1, cutforcesq, type2rhor, type2z2r, type2frho,
src/GPU/pair_eam_gpu.cpp:                             maxspecial, cell_size, gpu_mode, screen, fp_size);
src/GPU/pair_eam_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_eam_gpu.cpp:  if (gpu_mode == GPU_FORCE) neighbor->add_request(this, NeighConst::REQ_FULL);
src/GPU/pair_eam_gpu.cpp:double PairEAMGPU::single(int i, int j, int itype, int jtype, double rsq, double /* factor_coul */,
src/GPU/pair_eam_gpu.cpp:int PairEAMGPU::pack_forward_comm(int n, int *list, double *buf, int /* pbc_flag */,
src/GPU/pair_eam_gpu.cpp:void PairEAMGPU::unpack_forward_comm(int n, int first, double *buf)
src/GPU/Install.sh:action amoeba_convolution_gpu.cpp amoeba_convolution.cpp
src/GPU/Install.sh:action amoeba_convolution_gpu.h amoeba_convolution.cpp
src/GPU/Install.sh:action fix_gpu.cpp
src/GPU/Install.sh:action fix_gpu.h
src/GPU/Install.sh:action fix_nve_gpu.h
src/GPU/Install.sh:action fix_nve_gpu.cpp
src/GPU/Install.sh:action fix_nh_gpu.h
src/GPU/Install.sh:action fix_nh_gpu.cpp
src/GPU/Install.sh:action fix_nvt_gpu.h
src/GPU/Install.sh:action fix_nvt_gpu.cpp
src/GPU/Install.sh:action fix_npt_gpu.h
src/GPU/Install.sh:action fix_npt_gpu.cpp
src/GPU/Install.sh:action fix_nve_asphere_gpu.h fix_nve_asphere.h
src/GPU/Install.sh:action fix_nve_asphere_gpu.cpp fix_nve_asphere.cpp
src/GPU/Install.sh:action gpu_extra.h
src/GPU/Install.sh:action pair_amoeba_gpu.cpp pair_amoeba.cpp
src/GPU/Install.sh:action pair_amoeba_gpu.h pair_amoeba.h
src/GPU/Install.sh:action pair_beck_gpu.cpp pair_beck.cpp
src/GPU/Install.sh:action pair_beck_gpu.h pair_beck.h
src/GPU/Install.sh:action pair_born_coul_long_gpu.cpp pair_born_coul_long.cpp
src/GPU/Install.sh:action pair_born_coul_long_gpu.h pair_born_coul_long.cpp
src/GPU/Install.sh:action pair_born_coul_long_cs_gpu.cpp pair_born_coul_long_cs.cpp
src/GPU/Install.sh:action pair_born_coul_long_cs_gpu.h pair_born_coul_long_cs.cpp
src/GPU/Install.sh:action pair_born_coul_wolf_gpu.cpp pair_born_coul_wolf.cpp
src/GPU/Install.sh:action pair_born_coul_wolf_gpu.h pair_born_coul_wolf.h
src/GPU/Install.sh:action pair_born_coul_wolf_cs_gpu.cpp pair_born_coul_wolf_cs.cpp
src/GPU/Install.sh:action pair_born_coul_wolf_cs_gpu.h pair_born_coul_wolf_cs.cpp
src/GPU/Install.sh:action pair_born_gpu.cpp
src/GPU/Install.sh:action pair_born_gpu.h
src/GPU/Install.sh:action pair_buck_coul_cut_gpu.cpp pair_buck_coul_cut.cpp
src/GPU/Install.sh:action pair_buck_coul_cut_gpu.h pair_buck_coul_cut.cpp
src/GPU/Install.sh:action pair_buck_coul_long_gpu.cpp pair_buck_coul_long.cpp
src/GPU/Install.sh:action pair_buck_coul_long_gpu.h pair_buck_coul_long.cpp
src/GPU/Install.sh:action pair_buck_gpu.cpp pair_buck.cpp
src/GPU/Install.sh:action pair_buck_gpu.h pair_buck.cpp
src/GPU/Install.sh:action pair_colloid_gpu.cpp pair_colloid.cpp
src/GPU/Install.sh:action pair_colloid_gpu.h pair_colloid.cpp
src/GPU/Install.sh:action pair_coul_cut_gpu.cpp
src/GPU/Install.sh:action pair_coul_cut_gpu.h
src/GPU/Install.sh:action pair_coul_debye_gpu.cpp
src/GPU/Install.sh:action pair_coul_debye_gpu.h
src/GPU/Install.sh:action pair_coul_dsf_gpu.cpp
src/GPU/Install.sh:action pair_coul_dsf_gpu.h
src/GPU/Install.sh:action pair_coul_long_gpu.cpp pair_coul_long.cpp
src/GPU/Install.sh:action pair_coul_long_gpu.h pair_coul_long.cpp
src/GPU/Install.sh:action pair_coul_long_cs_gpu.cpp pair_coul_long_cs.cpp
src/GPU/Install.sh:action pair_coul_long_cs_gpu.h pair_coul_long_cs.cpp
src/GPU/Install.sh:action pair_dpd_gpu.cpp pair_dpd.cpp
src/GPU/Install.sh:action pair_dpd_gpu.h pair_dpd.h
src/GPU/Install.sh:action pair_dpd_tstat_gpu.cpp pair_dpd_tstat.cpp
src/GPU/Install.sh:action pair_dpd_tstat_gpu.h pair_dpd_tstat.h
src/GPU/Install.sh:action pair_lj_cut_dipole_cut_gpu.cpp pair_lj_cut_dipole_cut.cpp
src/GPU/Install.sh:action pair_lj_cut_dipole_cut_gpu.h pair_lj_cut_dipole_cut.cpp
src/GPU/Install.sh:action pair_lj_sf_dipole_sf_gpu.cpp pair_lj_sf_dipole_sf.cpp
src/GPU/Install.sh:action pair_lj_sf_dipole_sf_gpu.h pair_lj_sf_dipole_sf.cpp
src/GPU/Install.sh:action pair_eam_alloy_gpu.cpp pair_eam.cpp
src/GPU/Install.sh:action pair_eam_alloy_gpu.h pair_eam.cpp
src/GPU/Install.sh:action pair_eam_fs_gpu.cpp pair_eam.cpp
src/GPU/Install.sh:action pair_eam_fs_gpu.h pair_eam.cpp
src/GPU/Install.sh:action pair_eam_gpu.cpp pair_eam.cpp
src/GPU/Install.sh:action pair_eam_gpu.h pair_eam.cpp
src/GPU/Install.sh:action pair_gauss_gpu.cpp pair_gauss.cpp
src/GPU/Install.sh:action pair_gauss_gpu.h pair_gauss.h
src/GPU/Install.sh:action pair_gayberne_gpu.cpp pair_gayberne.cpp
src/GPU/Install.sh:action pair_gayberne_gpu.h pair_gayberne.cpp
src/GPU/Install.sh:action pair_hippo_gpu.cpp pair_hippo.cpp
src/GPU/Install.sh:action pair_hippo_gpu.h pair_hippo.cpp
src/GPU/Install.sh:action pair_lj96_cut_gpu.cpp pair_lj96_cut.cpp
src/GPU/Install.sh:action pair_lj96_cut_gpu.h pair_lj96_cut.h
src/GPU/Install.sh:action pair_lj_charmm_coul_long_gpu.cpp pair_lj_charmm_coul_long.cpp
src/GPU/Install.sh:action pair_lj_charmm_coul_long_gpu.h pair_lj_charmm_coul_long.cpp
src/GPU/Install.sh:action pair_lj_charmm_coul_charmm_gpu.cpp pair_lj_charmm_coul_charmm.cpp
src/GPU/Install.sh:action pair_lj_charmm_coul_charmm_gpu.h pair_lj_charmm_coul_charmm.cpp
src/GPU/Install.sh:action pair_lj_class2_coul_long_gpu.cpp pair_lj_class2_coul_long.cpp
src/GPU/Install.sh:action pair_lj_class2_coul_long_gpu.h pair_lj_class2_coul_long.cpp
src/GPU/Install.sh:action pair_lj_class2_gpu.cpp pair_lj_class2.cpp
src/GPU/Install.sh:action pair_lj_class2_gpu.h pair_lj_class2.cpp
src/GPU/Install.sh:action pair_lj_cubic_gpu.cpp pair_lj_cubic.cpp
src/GPU/Install.sh:action pair_lj_cubic_gpu.h pair_lj_cubic.h
src/GPU/Install.sh:action pair_lj_cut_coul_cut_gpu.cpp
src/GPU/Install.sh:action pair_lj_cut_coul_cut_gpu.h
src/GPU/Install.sh:action pair_lj_cut_coul_debye_gpu.cpp pair_lj_cut_coul_debye.cpp
src/GPU/Install.sh:action pair_lj_cut_coul_debye_gpu.h pair_lj_cut_coul_debye.h
src/GPU/Install.sh:action pair_lj_cut_coul_dsf_gpu.cpp pair_lj_cut_coul_dsf.cpp
src/GPU/Install.sh:action pair_lj_cut_coul_dsf_gpu.h pair_lj_cut_coul_dsf.h
src/GPU/Install.sh:action pair_lj_cut_coul_long_gpu.cpp pair_lj_cut_coul_long.cpp
src/GPU/Install.sh:action pair_lj_cut_coul_long_gpu.h pair_lj_cut_coul_long.cpp
src/GPU/Install.sh:action pair_lj_cut_coul_msm_gpu.cpp pair_lj_cut_coul_msm.cpp
src/GPU/Install.sh:action pair_lj_cut_coul_msm_gpu.h pair_lj_cut_coul_msm.h
src/GPU/Install.sh:action pair_lj_cut_gpu.cpp
src/GPU/Install.sh:action pair_lj_cut_gpu.h
src/GPU/Install.sh:action pair_lj_cut_dipole_long_gpu.cpp pair_lj_cut_dipole_long.cpp
src/GPU/Install.sh:action pair_lj_cut_dipole_long_gpu.h pair_lj_cut_dipole_long.cpp
src/GPU/Install.sh:action pair_lj_cut_tip4p_long_gpu.h pair_lj_cut_tip4p_long.cpp
src/GPU/Install.sh:action pair_lj_cut_tip4p_long_gpu.cpp pair_lj_cut_tip4p_long.cpp
src/GPU/Install.sh:action pair_lj_smooth_gpu.cpp pair_lj_smooth.cpp
src/GPU/Install.sh:action pair_lj_smooth_gpu.h pair_lj_smooth.cpp
src/GPU/Install.sh:action pair_lj_expand_gpu.cpp
src/GPU/Install.sh:action pair_lj_expand_gpu.h
src/GPU/Install.sh:action pair_lj_expand_coul_long_gpu.cpp pair_lj_expand_coul_long.cpp
src/GPU/Install.sh:action pair_lj_expand_coul_long_gpu.h pair_lj_expand_coul_long.cpp
src/GPU/Install.sh:action pair_lj_gromacs_gpu.cpp pair_lj_gromacs.cpp
src/GPU/Install.sh:action pair_lj_gromacs_gpu.h pair_lj_gromacs.h
src/GPU/Install.sh:action pair_lj_spica_coul_long_gpu.cpp pair_lj_spica_coul_long.cpp
src/GPU/Install.sh:action pair_lj_spica_coul_long_gpu.h pair_lj_spica_coul_long.cpp
src/GPU/Install.sh:action pair_lj_spica_gpu.cpp pair_lj_spica.cpp
src/GPU/Install.sh:action pair_lj_spica_gpu.h pair_lj_spica.cpp
src/GPU/Install.sh:action pair_mie_cut_gpu.cpp pair_mie_cut.cpp
src/GPU/Install.sh:action pair_mie_cut_gpu.h pair_mie_cut.h
src/GPU/Install.sh:action pair_morse_gpu.cpp
src/GPU/Install.sh:action pair_morse_gpu.h
src/GPU/Install.sh:action pair_resquared_gpu.cpp pair_resquared.cpp
src/GPU/Install.sh:action pair_resquared_gpu.h pair_resquared.cpp
src/GPU/Install.sh:action pair_soft_gpu.cpp
src/GPU/Install.sh:action pair_soft_gpu.h
src/GPU/Install.sh:action pair_sw_gpu.cpp pair_sw.cpp
src/GPU/Install.sh:action pair_sw_gpu.h pair_sw.h
src/GPU/Install.sh:action pair_vashishta_gpu.cpp pair_vashishta.cpp
src/GPU/Install.sh:action pair_vashishta_gpu.h pair_vashishta.h
src/GPU/Install.sh:action pair_table_gpu.cpp pair_table.cpp
src/GPU/Install.sh:action pair_table_gpu.h pair_table.cpp
src/GPU/Install.sh:action pair_tersoff_gpu.cpp pair_tersoff.cpp
src/GPU/Install.sh:action pair_tersoff_gpu.h pair_tersoff.cpp
src/GPU/Install.sh:action pair_tersoff_mod_gpu.cpp pair_tersoff_mod.cpp
src/GPU/Install.sh:action pair_tersoff_mod_gpu.h pair_tersoff_mod.cpp
src/GPU/Install.sh:action pair_tersoff_zbl_gpu.cpp pair_tersoff_zbl.cpp
src/GPU/Install.sh:action pair_tersoff_zbl_gpu.h pair_tersoff_zbl.cpp
src/GPU/Install.sh:action pair_yukawa_colloid_gpu.cpp pair_yukawa_colloid.cpp
src/GPU/Install.sh:action pair_yukawa_colloid_gpu.h pair_yukawa_colloid.cpp
src/GPU/Install.sh:action pair_yukawa_gpu.cpp pair_yukawa.cpp
src/GPU/Install.sh:action pair_yukawa_gpu.h pair_yukawa.cpp
src/GPU/Install.sh:action pair_zbl_gpu.cpp
src/GPU/Install.sh:action pair_zbl_gpu.h
src/GPU/Install.sh:action pppm_gpu.cpp pppm.cpp
src/GPU/Install.sh:action pppm_gpu.h pppm.cpp
src/GPU/Install.sh:action pair_ufm_gpu.cpp pair_ufm.cpp
src/GPU/Install.sh:action pair_ufm_gpu.h pair_ufm.h
src/GPU/Install.sh:    sed -i -e 's/[^ \t]*gpu[^ \t]* //' ../Makefile.package
src/GPU/Install.sh:    sed -i -e 's/[^ \t]*GPU[^ \t]* //' ../Makefile.package
src/GPU/Install.sh:    sed -i -e 's|^PKG_INC =[ \t]*|&-DLMP_GPU |' ../Makefile.package
src/GPU/Install.sh:    sed -i -e 's|^PKG_PATH =[ \t]*|&-L../../lib/gpu |' ../Makefile.package
src/GPU/Install.sh:    sed -i -e 's|^PKG_LIB =[ \t]*|&-lgpu |' ../Makefile.package
src/GPU/Install.sh:    sed -i -e 's|^PKG_SYSINC =[ \t]*|&$(gpu_SYSINC) |' ../Makefile.package
src/GPU/Install.sh:    sed -i -e 's|^PKG_SYSLIB =[ \t]*|&$(gpu_SYSLIB) |' ../Makefile.package
src/GPU/Install.sh:    sed -i -e 's|^PKG_SYSPATH =[ \t]*|&$(gpu_SYSPATH) |' ../Makefile.package
src/GPU/Install.sh:    sed -i -e '/^[ \t]*include.*gpu.*$/d' ../Makefile.package.settings
src/GPU/Install.sh:include ..\/..\/lib\/gpu\/Makefile.lammps
src/GPU/Install.sh:    sed -i -e 's/[^ \t]*gpu[^ \t]* //' ../Makefile.package
src/GPU/Install.sh:    sed -i -e 's/[^ \t]*GPU[^ \t]* //' ../Makefile.package
src/GPU/Install.sh:    sed -i -e '/^[ \t]*include.*gpu.*$/d' ../Makefile.package.settings
src/GPU/pair_lj_cut_dipole_long_gpu.cpp:#include "pair_lj_cut_dipole_long_gpu.h"
src/GPU/pair_lj_cut_dipole_long_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_lj_cut_dipole_long_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_lj_cut_dipole_long_gpu.cpp:int dplj_gpu_init(const int ntypes, double **cutsq, double **host_lj1, double **host_lj2,
src/GPU/pair_lj_cut_dipole_long_gpu.cpp:                  const double cell_size, int &gpu_mode, FILE *screen, double **host_cut_ljsq,
src/GPU/pair_lj_cut_dipole_long_gpu.cpp:void dplj_gpu_clear();
src/GPU/pair_lj_cut_dipole_long_gpu.cpp:int **dplj_gpu_compute_n(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_lj_cut_dipole_long_gpu.cpp:void dplj_gpu_compute(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_lj_cut_dipole_long_gpu.cpp:double dplj_gpu_bytes();
src/GPU/pair_lj_cut_dipole_long_gpu.cpp:PairLJCutDipoleLongGPU::PairLJCutDipoleLongGPU(LAMMPS *lmp) :
src/GPU/pair_lj_cut_dipole_long_gpu.cpp:    PairLJCutDipoleLong(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_lj_cut_dipole_long_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_lj_cut_dipole_long_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_lj_cut_dipole_long_gpu.cpp:PairLJCutDipoleLongGPU::~PairLJCutDipoleLongGPU()
src/GPU/pair_lj_cut_dipole_long_gpu.cpp:  dplj_gpu_clear();
src/GPU/pair_lj_cut_dipole_long_gpu.cpp:void PairLJCutDipoleLongGPU::compute(int eflag, int vflag)
src/GPU/pair_lj_cut_dipole_long_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_lj_cut_dipole_long_gpu.cpp:    firstneigh = dplj_gpu_compute_n(neighbor->ago, inum, nall, atom->x, atom->type, sublo, subhi,
src/GPU/pair_lj_cut_dipole_long_gpu.cpp:    dplj_gpu_compute(neighbor->ago, inum, nall, atom->x, atom->type, ilist, numneigh, firstneigh,
src/GPU/pair_lj_cut_dipole_long_gpu.cpp:void PairLJCutDipoleLongGPU::init_style()
src/GPU/pair_lj_cut_dipole_long_gpu.cpp:    error->all(FLERR, "Pair dipole/cut/gpu requires atom attributes q, mu, torque");
src/GPU/pair_lj_cut_dipole_long_gpu.cpp:      dplj_gpu_init(atom->ntypes + 1, cutsq, lj1, lj2, lj3, lj4, offset, force->special_lj,
src/GPU/pair_lj_cut_dipole_long_gpu.cpp:                    atom->nlocal, atom->nlocal + atom->nghost, mnf, maxspecial, cell_size, gpu_mode,
src/GPU/pair_lj_cut_dipole_long_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_lj_cut_dipole_long_gpu.cpp:  if (gpu_mode == GPU_FORCE) neighbor->add_request(this, NeighConst::REQ_FULL);
src/GPU/pair_lj_cut_dipole_long_gpu.cpp:double PairLJCutDipoleLongGPU::memory_usage()
src/GPU/pair_lj_cut_dipole_long_gpu.cpp:  return bytes + dplj_gpu_bytes();
src/GPU/pair_lj_cut_dipole_long_gpu.cpp:void PairLJCutDipoleLongGPU::cpu_compute(int start, int inum, int eflag, int vflag, int *ilist,
src/GPU/pair_lj_class2_coul_long_gpu.h:PairStyle(lj/class2/coul/long/gpu,PairLJClass2CoulLongGPU);
src/GPU/pair_lj_class2_coul_long_gpu.h:#ifndef LMP_PAIR_LJ_CLASS2_COUL_LONG_GPU_H
src/GPU/pair_lj_class2_coul_long_gpu.h:#define LMP_PAIR_LJ_CLASS2_COUL_LONG_GPU_H
src/GPU/pair_lj_class2_coul_long_gpu.h:class PairLJClass2CoulLongGPU : public PairLJClass2CoulLong {
src/GPU/pair_lj_class2_coul_long_gpu.h:  PairLJClass2CoulLongGPU(LAMMPS *lmp);
src/GPU/pair_lj_class2_coul_long_gpu.h:  ~PairLJClass2CoulLongGPU() override;
src/GPU/pair_lj_class2_coul_long_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_lj_class2_coul_long_gpu.h:  int gpu_mode;
src/GPU/pair_sw_gpu.cpp:#include "pair_sw_gpu.h"
src/GPU/pair_sw_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_sw_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_sw_gpu.cpp:int sw_gpu_init(const int ntypes, const int inum, const int nall, const int max_nbors,
src/GPU/pair_sw_gpu.cpp:                const double cell_size, int &gpu_mode, FILE *screen, double **ncutsq, double **ncut,
src/GPU/pair_sw_gpu.cpp:void sw_gpu_clear();
src/GPU/pair_sw_gpu.cpp:int **sw_gpu_compute_n(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_sw_gpu.cpp:void sw_gpu_compute(const int ago, const int nloc, const int nall, const int ln, double **host_x,
src/GPU/pair_sw_gpu.cpp:double sw_gpu_bytes();
src/GPU/pair_sw_gpu.cpp:PairSWGPU::PairSWGPU(LAMMPS *lmp) : PairSW(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_sw_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_sw_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_sw_gpu.cpp:PairSWGPU::~PairSWGPU()
src/GPU/pair_sw_gpu.cpp:  sw_gpu_clear();
src/GPU/pair_sw_gpu.cpp:void PairSWGPU::compute(int eflag, int vflag)
src/GPU/pair_sw_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_sw_gpu.cpp:        sw_gpu_compute_n(neighbor->ago, inum, nall, atom->x, atom->type, sublo, subhi, atom->tag,
src/GPU/pair_sw_gpu.cpp:    sw_gpu_compute(neighbor->ago, inum, nall, inum + list->gnum, atom->x, atom->type, ilist,
src/GPU/pair_sw_gpu.cpp:void PairSWGPU::allocate()
src/GPU/pair_sw_gpu.cpp:void PairSWGPU::init_style()
src/GPU/pair_sw_gpu.cpp:  if (atom->tag_enable == 0) error->all(FLERR, "Pair style sw/gpu requires atom IDs");
src/GPU/pair_sw_gpu.cpp:  int success = sw_gpu_init(tp1, atom->nlocal, atom->nlocal + atom->nghost, mnf, cell_size,
src/GPU/pair_sw_gpu.cpp:                            gpu_mode, screen, ncutsq, ncut, sigma, powerp, powerq, sigma_gamma, c1,
src/GPU/pair_sw_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_sw_gpu.cpp:  if (gpu_mode == GPU_FORCE)
src/GPU/pair_sw_gpu.cpp:      error->warning(FLERR, "Increasing communication cutoff to {:.8} for GPU pair style",
src/GPU/pair_sw_gpu.cpp:double PairSWGPU::init_one(int i, int j)
src/GPU/pair_lj_smooth_gpu.cpp:#include "pair_lj_smooth_gpu.h"
src/GPU/pair_lj_smooth_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_lj_smooth_gpu.cpp:// External functions from gpu library for atom decomposition
src/GPU/pair_lj_smooth_gpu.cpp:int ljsmt_gpu_init(const int ntypes, double **cutsq, double **host_lj1, double **host_lj2,
src/GPU/pair_lj_smooth_gpu.cpp:                   const double cell_size, int &gpu_mode, FILE *screen, double **host_ljsw0,
src/GPU/pair_lj_smooth_gpu.cpp:void ljsmt_gpu_reinit(const int ntypes, double **cutsq, double **host_lj1, double **host_lj2,
src/GPU/pair_lj_smooth_gpu.cpp:void ljsmt_gpu_clear();
src/GPU/pair_lj_smooth_gpu.cpp:int **ljsmt_gpu_compute_n(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_lj_smooth_gpu.cpp:void ljsmt_gpu_compute(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_lj_smooth_gpu.cpp:double ljsmt_gpu_bytes();
src/GPU/pair_lj_smooth_gpu.cpp:PairLJSmoothGPU::PairLJSmoothGPU(LAMMPS *lmp) : PairLJSmooth(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_lj_smooth_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_lj_smooth_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_lj_smooth_gpu.cpp:PairLJSmoothGPU::~PairLJSmoothGPU()
src/GPU/pair_lj_smooth_gpu.cpp:  ljsmt_gpu_clear();
src/GPU/pair_lj_smooth_gpu.cpp:void PairLJSmoothGPU::compute(int eflag, int vflag)
src/GPU/pair_lj_smooth_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_lj_smooth_gpu.cpp:        ljsmt_gpu_compute_n(neighbor->ago, inum, nall, atom->x, atom->type, sublo, subhi, atom->tag,
src/GPU/pair_lj_smooth_gpu.cpp:    ljsmt_gpu_compute(neighbor->ago, inum, nall, atom->x, atom->type, ilist, numneigh, firstneigh,
src/GPU/pair_lj_smooth_gpu.cpp:  //fprintf("LJ_SMOOTH_GPU");
src/GPU/pair_lj_smooth_gpu.cpp:void PairLJSmoothGPU::init_style()
src/GPU/pair_lj_smooth_gpu.cpp:      ljsmt_gpu_init(atom->ntypes + 1, cutsq, lj1, lj2, lj3, lj4, offset, force->special_lj,
src/GPU/pair_lj_smooth_gpu.cpp:                     gpu_mode, screen, ljsw0, ljsw1, ljsw2, ljsw3, ljsw4, cut_inner, cut_inner_sq);
src/GPU/pair_lj_smooth_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_lj_smooth_gpu.cpp:  if (gpu_mode == GPU_FORCE) neighbor->add_request(this, NeighConst::REQ_FULL);
src/GPU/pair_lj_smooth_gpu.cpp:void PairLJSmoothGPU::reinit()
src/GPU/pair_lj_smooth_gpu.cpp:  ljsmt_gpu_reinit(atom->ntypes + 1, cutsq, lj1, lj2, lj3, lj4, offset, ljsw0, ljsw1, ljsw2, ljsw3,
src/GPU/pair_lj_smooth_gpu.cpp:double PairLJSmoothGPU::memory_usage()
src/GPU/pair_lj_smooth_gpu.cpp:  return bytes + ljsmt_gpu_bytes();
src/GPU/pair_lj_smooth_gpu.cpp:void PairLJSmoothGPU::cpu_compute(int start, int inum, int eflag, int /* vflag */, int *ilist,
src/GPU/pair_yukawa_colloid_gpu.h:PairStyle(yukawa/colloid/gpu,PairYukawaColloidGPU);
src/GPU/pair_yukawa_colloid_gpu.h:#ifndef LMP_PAIR_YUKAWA_COLLOID_GPU_H
src/GPU/pair_yukawa_colloid_gpu.h:#define LMP_PAIR_YUKAWA_COLLOID_GPU_H
src/GPU/pair_yukawa_colloid_gpu.h:class PairYukawaColloidGPU : public PairYukawaColloid {
src/GPU/pair_yukawa_colloid_gpu.h:  PairYukawaColloidGPU(LAMMPS *lmp);
src/GPU/pair_yukawa_colloid_gpu.h:  ~PairYukawaColloidGPU() override;
src/GPU/pair_yukawa_colloid_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_yukawa_colloid_gpu.h:  int gpu_mode;
src/GPU/pair_lj_cubic_gpu.cpp:#include "pair_lj_cubic_gpu.h"
src/GPU/pair_lj_cubic_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_lj_cubic_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_lj_cubic_gpu.cpp:int ljcb_gpu_init(const int ntypes, double **cutsq, double **cut_inner_sq, double **cut_inner,
src/GPU/pair_lj_cubic_gpu.cpp:                  int &gpu_mode, FILE *screen);
src/GPU/pair_lj_cubic_gpu.cpp:void ljcb_gpu_clear();
src/GPU/pair_lj_cubic_gpu.cpp:int **ljcb_gpu_compute_n(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_lj_cubic_gpu.cpp:void ljcb_gpu_compute(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_lj_cubic_gpu.cpp:double ljcb_gpu_bytes();
src/GPU/pair_lj_cubic_gpu.cpp:PairLJCubicGPU::PairLJCubicGPU(LAMMPS *lmp) : PairLJCubic(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_lj_cubic_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_lj_cubic_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_lj_cubic_gpu.cpp:PairLJCubicGPU::~PairLJCubicGPU()
src/GPU/pair_lj_cubic_gpu.cpp:  ljcb_gpu_clear();
src/GPU/pair_lj_cubic_gpu.cpp:void PairLJCubicGPU::compute(int eflag, int vflag)
src/GPU/pair_lj_cubic_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_lj_cubic_gpu.cpp:        ljcb_gpu_compute_n(neighbor->ago, inum, nall, atom->x, atom->type, sublo, subhi, atom->tag,
src/GPU/pair_lj_cubic_gpu.cpp:    ljcb_gpu_compute(neighbor->ago, inum, nall, atom->x, atom->type, ilist, numneigh, firstneigh,
src/GPU/pair_lj_cubic_gpu.cpp:void PairLJCubicGPU::init_style()
src/GPU/pair_lj_cubic_gpu.cpp:      ljcb_gpu_init(atom->ntypes + 1, cutsq, cut_inner_sq, cut_inner, sigma, epsilon, lj1, lj2, lj3,
src/GPU/pair_lj_cubic_gpu.cpp:                    maxspecial, cell_size, gpu_mode, screen);
src/GPU/pair_lj_cubic_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_lj_cubic_gpu.cpp:  if (gpu_mode == GPU_FORCE) neighbor->add_request(this, NeighConst::REQ_FULL);
src/GPU/pair_lj_cubic_gpu.cpp:double PairLJCubicGPU::memory_usage()
src/GPU/pair_lj_cubic_gpu.cpp:  return bytes + ljcb_gpu_bytes();
src/GPU/pair_lj_cubic_gpu.cpp:void PairLJCubicGPU::cpu_compute(int start, int inum, int eflag, int /* vflag */, int *ilist,
src/GPU/pair_coul_long_cs_gpu.h:PairStyle(coul/long/cs/gpu,PairCoulLongCSGPU);
src/GPU/pair_coul_long_cs_gpu.h:#ifndef LMP_PAIR_COUL_LONG_CS_GPU_H
src/GPU/pair_coul_long_cs_gpu.h:#define LMP_PAIR_COUL_LONG_CS_GPU_H
src/GPU/pair_coul_long_cs_gpu.h:class PairCoulLongCSGPU : public PairCoulLongCS {
src/GPU/pair_coul_long_cs_gpu.h:  PairCoulLongCSGPU(LAMMPS *lmp);
src/GPU/pair_coul_long_cs_gpu.h:  ~PairCoulLongCSGPU() override;
src/GPU/pair_coul_long_cs_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_coul_long_cs_gpu.h:  int gpu_mode;
src/GPU/pair_dpd_tstat_gpu.cpp:#include "pair_dpd_tstat_gpu.h"
src/GPU/pair_dpd_tstat_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_dpd_tstat_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_dpd_tstat_gpu.cpp:int dpd_tstat_gpu_init(const int ntypes, double **cutsq, double **host_a0, double **host_gamma,
src/GPU/pair_dpd_tstat_gpu.cpp:                       const double cell_size, int &gpu_mode, FILE *screen);
src/GPU/pair_dpd_tstat_gpu.cpp:void dpd_tstat_gpu_clear();
src/GPU/pair_dpd_tstat_gpu.cpp:int **dpd_tstat_gpu_compute_n(const int ago, const int inum_full, const int nall, double **host_x,
src/GPU/pair_dpd_tstat_gpu.cpp:void dpd_tstat_gpu_compute(const int ago, const int inum_full, const int nall, double **host_x,
src/GPU/pair_dpd_tstat_gpu.cpp:void dpd_tstat_gpu_update_coeff(int ntypes, double **host_a0, double **host_gamma,
src/GPU/pair_dpd_tstat_gpu.cpp:double dpd_tstat_gpu_bytes();
src/GPU/pair_dpd_tstat_gpu.cpp:PairDPDTstatGPU::PairDPDTstatGPU(LAMMPS *lmp) : PairDPDTstat(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_dpd_tstat_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_dpd_tstat_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_dpd_tstat_gpu.cpp:PairDPDTstatGPU::~PairDPDTstatGPU()
src/GPU/pair_dpd_tstat_gpu.cpp:  dpd_tstat_gpu_clear();
src/GPU/pair_dpd_tstat_gpu.cpp:void PairDPDTstatGPU::compute(int eflag, int vflag)
src/GPU/pair_dpd_tstat_gpu.cpp:    dpd_tstat_gpu_update_coeff(atom->ntypes + 1, a0, gamma, sigma, cut);
src/GPU/pair_dpd_tstat_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_dpd_tstat_gpu.cpp:    firstneigh = dpd_tstat_gpu_compute_n(
src/GPU/pair_dpd_tstat_gpu.cpp:    dpd_tstat_gpu_compute(neighbor->ago, inum, nall, atom->x, atom->type, ilist, numneigh,
src/GPU/pair_dpd_tstat_gpu.cpp:void PairDPDTstatGPU::init_style()
src/GPU/pair_dpd_tstat_gpu.cpp:  int success = dpd_tstat_gpu_init(atom->ntypes + 1, cutsq, a0, gamma, sigma, cut,
src/GPU/pair_dpd_tstat_gpu.cpp:                                   mnf, maxspecial, cell_size, gpu_mode, screen);
src/GPU/pair_dpd_tstat_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_dpd_tstat_gpu.cpp:  if (gpu_mode == GPU_FORCE) neighbor->add_request(this, NeighConst::REQ_FULL);
src/GPU/pair_dpd_tstat_gpu.cpp:double PairDPDTstatGPU::memory_usage()
src/GPU/pair_dpd_tstat_gpu.cpp:  return bytes + dpd_tstat_gpu_bytes();
src/GPU/pair_dpd_tstat_gpu.cpp:void PairDPDTstatGPU::cpu_compute(int start, int inum, int /* eflag */, int /* vflag */, int *ilist,
src/GPU/pair_born_gpu.cpp:#include "pair_born_gpu.h"
src/GPU/pair_born_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_born_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_born_gpu.cpp:int born_gpu_init(const int ntypes, double **cutsq, double **host_rhoinv, double **host_born1,
src/GPU/pair_born_gpu.cpp:                  const double cell_size, int &gpu_mode, FILE *screen);
src/GPU/pair_born_gpu.cpp:void born_gpu_reinit(const int ntypes, double **host_rhoinv, double **host_born1,
src/GPU/pair_born_gpu.cpp:void born_gpu_clear();
src/GPU/pair_born_gpu.cpp:int **born_gpu_compute_n(const int ago, const int inum_full, const int nall, double **host_x,
src/GPU/pair_born_gpu.cpp:void born_gpu_compute(const int ago, const int inum_full, const int nall, double **host_x,
src/GPU/pair_born_gpu.cpp:double born_gpu_bytes();
src/GPU/pair_born_gpu.cpp:PairBornGPU::PairBornGPU(LAMMPS *lmp) : PairBorn(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_born_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_born_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_born_gpu.cpp:PairBornGPU::~PairBornGPU()
src/GPU/pair_born_gpu.cpp:  born_gpu_clear();
src/GPU/pair_born_gpu.cpp:void PairBornGPU::compute(int eflag, int vflag)
src/GPU/pair_born_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_born_gpu.cpp:        born_gpu_compute_n(neighbor->ago, inum, nall, atom->x, atom->type, sublo, subhi, atom->tag,
src/GPU/pair_born_gpu.cpp:    born_gpu_compute(neighbor->ago, inum, nall, atom->x, atom->type, ilist, numneigh, firstneigh,
src/GPU/pair_born_gpu.cpp:void PairBornGPU::init_style()
src/GPU/pair_born_gpu.cpp:  int success = born_gpu_init(atom->ntypes + 1, cutsq, rhoinv, born1, born2, born3, a, c, d, sigma,
src/GPU/pair_born_gpu.cpp:                              mnf, maxspecial, cell_size, gpu_mode, screen);
src/GPU/pair_born_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_born_gpu.cpp:  if (gpu_mode == GPU_FORCE) neighbor->add_request(this, NeighConst::REQ_FULL);
src/GPU/pair_born_gpu.cpp:void PairBornGPU::reinit()
src/GPU/pair_born_gpu.cpp:  born_gpu_reinit(atom->ntypes + 1, rhoinv, born1, born2, born3, a, c, d, offset);
src/GPU/pair_born_gpu.cpp:double PairBornGPU::memory_usage()
src/GPU/pair_born_gpu.cpp:  return bytes + born_gpu_bytes();
src/GPU/pair_born_gpu.cpp:void PairBornGPU::cpu_compute(int start, int inum, int eflag, int /* vflag */, int *ilist,
src/GPU/pair_lj_sf_dipole_sf_gpu.cpp:#include "pair_lj_sf_dipole_sf_gpu.h"
src/GPU/pair_lj_sf_dipole_sf_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_lj_sf_dipole_sf_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_lj_sf_dipole_sf_gpu.cpp:int dplsf_gpu_init(const int ntypes, double **cutsq, double **host_lj1, double **host_lj2,
src/GPU/pair_lj_sf_dipole_sf_gpu.cpp:                   const double cell_size, int &gpu_mode, FILE *screen, double **host_cut_ljsq,
src/GPU/pair_lj_sf_dipole_sf_gpu.cpp:void dplsf_gpu_clear();
src/GPU/pair_lj_sf_dipole_sf_gpu.cpp:int **dplsf_gpu_compute_n(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_lj_sf_dipole_sf_gpu.cpp:void dplsf_gpu_compute(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_lj_sf_dipole_sf_gpu.cpp:double dplsf_gpu_bytes();
src/GPU/pair_lj_sf_dipole_sf_gpu.cpp:PairLJSFDipoleSFGPU::PairLJSFDipoleSFGPU(LAMMPS *lmp) : PairLJSFDipoleSF(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_lj_sf_dipole_sf_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_lj_sf_dipole_sf_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_lj_sf_dipole_sf_gpu.cpp:PairLJSFDipoleSFGPU::~PairLJSFDipoleSFGPU()
src/GPU/pair_lj_sf_dipole_sf_gpu.cpp:  dplsf_gpu_clear();
src/GPU/pair_lj_sf_dipole_sf_gpu.cpp:void PairLJSFDipoleSFGPU::compute(int eflag, int vflag)
src/GPU/pair_lj_sf_dipole_sf_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_lj_sf_dipole_sf_gpu.cpp:    firstneigh = dplsf_gpu_compute_n(
src/GPU/pair_lj_sf_dipole_sf_gpu.cpp:    dplsf_gpu_compute(neighbor->ago, inum, nall, atom->x, atom->type, ilist, numneigh, firstneigh,
src/GPU/pair_lj_sf_dipole_sf_gpu.cpp:void PairLJSFDipoleSFGPU::init_style()
src/GPU/pair_lj_sf_dipole_sf_gpu.cpp:    error->all(FLERR, "Pair dipole/sf/gpu requires atom attributes q, mu, torque");
src/GPU/pair_lj_sf_dipole_sf_gpu.cpp:      dplsf_gpu_init(atom->ntypes + 1, cutsq, lj1, lj2, lj3, lj4, force->special_lj, atom->nlocal,
src/GPU/pair_lj_sf_dipole_sf_gpu.cpp:                     atom->nlocal + atom->nghost, mnf, maxspecial, cell_size, gpu_mode, screen,
src/GPU/pair_lj_sf_dipole_sf_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_lj_sf_dipole_sf_gpu.cpp:  if (gpu_mode == GPU_FORCE) neighbor->add_request(this, NeighConst::REQ_FULL);
src/GPU/pair_lj_sf_dipole_sf_gpu.cpp:double PairLJSFDipoleSFGPU::memory_usage()
src/GPU/pair_lj_sf_dipole_sf_gpu.cpp:  return bytes + dplsf_gpu_bytes();
src/GPU/pair_lj_sf_dipole_sf_gpu.cpp:void PairLJSFDipoleSFGPU::cpu_compute(int start, int inum, int eflag, int vflag, int *ilist,
src/GPU/pair_born_coul_long_gpu.h:PairStyle(born/coul/long/gpu,PairBornCoulLongGPU);
src/GPU/pair_born_coul_long_gpu.h:#ifndef LMP_PAIR_BORN_COUL_LONG_GPU_H
src/GPU/pair_born_coul_long_gpu.h:#define LMP_PAIR_BORN_COUL_LONG_GPU_H
src/GPU/pair_born_coul_long_gpu.h:class PairBornCoulLongGPU : public PairBornCoulLong {
src/GPU/pair_born_coul_long_gpu.h:  PairBornCoulLongGPU(LAMMPS *lmp);
src/GPU/pair_born_coul_long_gpu.h:  ~PairBornCoulLongGPU() override;
src/GPU/pair_born_coul_long_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_born_coul_long_gpu.h:  int gpu_mode;
src/GPU/pair_tersoff_mod_gpu.h:PairStyle(tersoff/mod/gpu,PairTersoffMODGPU);
src/GPU/pair_tersoff_mod_gpu.h:#ifndef LMP_PAIR_TERSOFF_MOD_GPU_H
src/GPU/pair_tersoff_mod_gpu.h:#define LMP_PAIR_TERSOFF_MOD_GPU_H
src/GPU/pair_tersoff_mod_gpu.h:class PairTersoffMODGPU : public PairTersoffMOD {
src/GPU/pair_tersoff_mod_gpu.h:  PairTersoffMODGPU(class LAMMPS *);
src/GPU/pair_tersoff_mod_gpu.h:  ~PairTersoffMODGPU() override;
src/GPU/pair_tersoff_mod_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_tersoff_mod_gpu.h:  int gpu_mode;
src/GPU/pair_tersoff_mod_gpu.h:  int *gpulist;
src/GPU/pair_gauss_gpu.h:PairStyle(gauss/gpu,PairGaussGPU);
src/GPU/pair_gauss_gpu.h:#ifndef LMP_PAIR_GAUSS_GPU_H
src/GPU/pair_gauss_gpu.h:#define LMP_PAIR_GAUSS_GPU_H
src/GPU/pair_gauss_gpu.h:class PairGaussGPU : public PairGauss {
src/GPU/pair_gauss_gpu.h:  PairGaussGPU(LAMMPS *lmp);
src/GPU/pair_gauss_gpu.h:  ~PairGaussGPU() override;
src/GPU/pair_gauss_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_gauss_gpu.h:  int gpu_mode;
src/GPU/pair_buck_coul_long_gpu.cpp:#include "pair_buck_coul_long_gpu.h"
src/GPU/pair_buck_coul_long_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_buck_coul_long_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_buck_coul_long_gpu.cpp:int buckcl_gpu_init(const int ntypes, double **cutsq, double **host_rhoinv, double **host_buck1,
src/GPU/pair_buck_coul_long_gpu.cpp:                    const int maxspecial, const double cell_size, int &gpu_mode, FILE *screen,
src/GPU/pair_buck_coul_long_gpu.cpp:void buckcl_gpu_clear();
src/GPU/pair_buck_coul_long_gpu.cpp:int **buckcl_gpu_compute_n(const int ago, const int inum_full, const int nall, double **host_x,
src/GPU/pair_buck_coul_long_gpu.cpp:void buckcl_gpu_compute(const int ago, const int inum_full, const int nall, double **host_x,
src/GPU/pair_buck_coul_long_gpu.cpp:double buckcl_gpu_bytes();
src/GPU/pair_buck_coul_long_gpu.cpp:PairBuckCoulLongGPU::PairBuckCoulLongGPU(LAMMPS *lmp) : PairBuckCoulLong(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_buck_coul_long_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_buck_coul_long_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_buck_coul_long_gpu.cpp:PairBuckCoulLongGPU::~PairBuckCoulLongGPU()
src/GPU/pair_buck_coul_long_gpu.cpp:  buckcl_gpu_clear();
src/GPU/pair_buck_coul_long_gpu.cpp:void PairBuckCoulLongGPU::compute(int eflag, int vflag)
src/GPU/pair_buck_coul_long_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_buck_coul_long_gpu.cpp:    firstneigh = buckcl_gpu_compute_n(neighbor->ago, inum, nall, atom->x, atom->type, sublo, subhi,
src/GPU/pair_buck_coul_long_gpu.cpp:    buckcl_gpu_compute(neighbor->ago, inum, nall, atom->x, atom->type, ilist, numneigh, firstneigh,
src/GPU/pair_buck_coul_long_gpu.cpp:void PairBuckCoulLongGPU::init_style()
src/GPU/pair_buck_coul_long_gpu.cpp:  if (!atom->q_flag) error->all(FLERR, "Pair style buck/coul/long/gpu requires atom attribute q");
src/GPU/pair_buck_coul_long_gpu.cpp:  int success = buckcl_gpu_init(atom->ntypes + 1, cutsq, rhoinv, buck1, buck2, a, c, offset,
src/GPU/pair_buck_coul_long_gpu.cpp:                                maxspecial, cell_size, gpu_mode, screen, cut_ljsq, cut_coulsq,
src/GPU/pair_buck_coul_long_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_buck_coul_long_gpu.cpp:  if (gpu_mode == GPU_FORCE) neighbor->add_request(this, NeighConst::REQ_FULL);
src/GPU/pair_buck_coul_long_gpu.cpp:double PairBuckCoulLongGPU::memory_usage()
src/GPU/pair_buck_coul_long_gpu.cpp:  return bytes + buckcl_gpu_bytes();
src/GPU/pair_buck_coul_long_gpu.cpp:void PairBuckCoulLongGPU::cpu_compute(int start, int inum, int eflag, int /* vflag */, int *ilist,
src/GPU/pair_lj_expand_gpu.h:PairStyle(lj/expand/gpu,PairLJExpandGPU);
src/GPU/pair_lj_expand_gpu.h:#ifndef LMP_PAIR_LJE_LIGHT_GPU_H
src/GPU/pair_lj_expand_gpu.h:#define LMP_PAIR_LJE_LIGHT_GPU_H
src/GPU/pair_lj_expand_gpu.h:class PairLJExpandGPU : public PairLJExpand {
src/GPU/pair_lj_expand_gpu.h:  PairLJExpandGPU(LAMMPS *lmp);
src/GPU/pair_lj_expand_gpu.h:  ~PairLJExpandGPU() override;
src/GPU/pair_lj_expand_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_lj_expand_gpu.h:  int gpu_mode;
src/GPU/pair_coul_debye_gpu.cpp:#include "pair_coul_debye_gpu.h"
src/GPU/pair_coul_debye_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_coul_debye_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_coul_debye_gpu.cpp:int cdebye_gpu_init(const int ntypes, double **host_scale, double **cutsq, double *special_coul,
src/GPU/pair_coul_debye_gpu.cpp:                    const double cell_size, int &gpu_mode, FILE *screen, const double qqrd2e,
src/GPU/pair_coul_debye_gpu.cpp:void cdebye_gpu_reinit(const int ntypes, double **host_scale);
src/GPU/pair_coul_debye_gpu.cpp:void cdebye_gpu_clear();
src/GPU/pair_coul_debye_gpu.cpp:int **cdebye_gpu_compute_n(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_coul_debye_gpu.cpp:void cdebye_gpu_compute(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_coul_debye_gpu.cpp:double cdebye_gpu_bytes();
src/GPU/pair_coul_debye_gpu.cpp:PairCoulDebyeGPU::PairCoulDebyeGPU(LAMMPS *lmp) : PairCoulDebye(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_coul_debye_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_coul_debye_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_coul_debye_gpu.cpp:PairCoulDebyeGPU::~PairCoulDebyeGPU()
src/GPU/pair_coul_debye_gpu.cpp:  cdebye_gpu_clear();
src/GPU/pair_coul_debye_gpu.cpp:void PairCoulDebyeGPU::compute(int eflag, int vflag)
src/GPU/pair_coul_debye_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_coul_debye_gpu.cpp:    firstneigh = cdebye_gpu_compute_n(neighbor->ago, inum, nall, atom->x, atom->type, sublo, subhi,
src/GPU/pair_coul_debye_gpu.cpp:    cdebye_gpu_compute(neighbor->ago, inum, nall, atom->x, atom->type, ilist, numneigh, firstneigh,
src/GPU/pair_coul_debye_gpu.cpp:void PairCoulDebyeGPU::init_style()
src/GPU/pair_coul_debye_gpu.cpp:  if (!atom->q_flag) error->all(FLERR, "Pair style coul/debye/gpu requires atom attribute q");
src/GPU/pair_coul_debye_gpu.cpp:  int success = cdebye_gpu_init(atom->ntypes + 1, scale, cutsq, force->special_coul, atom->nlocal,
src/GPU/pair_coul_debye_gpu.cpp:                                atom->nlocal + atom->nghost, mnf, maxspecial, cell_size, gpu_mode,
src/GPU/pair_coul_debye_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_coul_debye_gpu.cpp:  if (gpu_mode == GPU_FORCE) neighbor->add_request(this, NeighConst::REQ_FULL);
src/GPU/pair_coul_debye_gpu.cpp:void PairCoulDebyeGPU::reinit()
src/GPU/pair_coul_debye_gpu.cpp:  cdebye_gpu_reinit(atom->ntypes + 1, scale);
src/GPU/pair_coul_debye_gpu.cpp:double PairCoulDebyeGPU::memory_usage()
src/GPU/pair_coul_debye_gpu.cpp:  return bytes + cdebye_gpu_bytes();
src/GPU/pair_coul_debye_gpu.cpp:void PairCoulDebyeGPU::cpu_compute(int start, int inum, int eflag, int /* vflag */, int *ilist,
src/GPU/pair_born_coul_wolf_gpu.cpp:#include "pair_born_coul_wolf_gpu.h"
src/GPU/pair_born_coul_wolf_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_born_coul_wolf_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_born_coul_wolf_gpu.cpp:int borncw_gpu_init(const int ntypes, double **cutsq, double **host_rhoinv, double **host_born1,
src/GPU/pair_born_coul_wolf_gpu.cpp:                    const double cell_size, int &gpu_mode, FILE *screen, double **host_cut_ljsq,
src/GPU/pair_born_coul_wolf_gpu.cpp:void borncw_gpu_clear();
src/GPU/pair_born_coul_wolf_gpu.cpp:int **borncw_gpu_compute_n(const int ago, const int inum_full, const int nall, double **host_x,
src/GPU/pair_born_coul_wolf_gpu.cpp:void borncw_gpu_compute(const int ago, const int inum_full, const int nall, double **host_x,
src/GPU/pair_born_coul_wolf_gpu.cpp:double borncw_gpu_bytes();
src/GPU/pair_born_coul_wolf_gpu.cpp:PairBornCoulWolfGPU::PairBornCoulWolfGPU(LAMMPS *lmp) : PairBornCoulWolf(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_born_coul_wolf_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_born_coul_wolf_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_born_coul_wolf_gpu.cpp:PairBornCoulWolfGPU::~PairBornCoulWolfGPU()
src/GPU/pair_born_coul_wolf_gpu.cpp:  borncw_gpu_clear();
src/GPU/pair_born_coul_wolf_gpu.cpp:void PairBornCoulWolfGPU::compute(int eflag, int vflag)
src/GPU/pair_born_coul_wolf_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_born_coul_wolf_gpu.cpp:    firstneigh = borncw_gpu_compute_n(neighbor->ago, inum, nall, atom->x, atom->type, sublo, subhi,
src/GPU/pair_born_coul_wolf_gpu.cpp:    borncw_gpu_compute(neighbor->ago, inum, nall, atom->x, atom->type, ilist, numneigh, firstneigh,
src/GPU/pair_born_coul_wolf_gpu.cpp:void PairBornCoulWolfGPU::init_style()
src/GPU/pair_born_coul_wolf_gpu.cpp:  if (!atom->q_flag) error->all(FLERR, "Pair style born/coul/wolf/gpu requires atom attribute q");
src/GPU/pair_born_coul_wolf_gpu.cpp:      borncw_gpu_init(atom->ntypes + 1, cutsq, rhoinv, born1, born2, born3, a, c, d, sigma, offset,
src/GPU/pair_born_coul_wolf_gpu.cpp:                      cell_size, gpu_mode, screen, cut_ljsq, cut_coulsq, force->special_coul,
src/GPU/pair_born_coul_wolf_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_born_coul_wolf_gpu.cpp:  if (gpu_mode == GPU_FORCE) neighbor->add_request(this, NeighConst::REQ_FULL);
src/GPU/pair_born_coul_wolf_gpu.cpp:double PairBornCoulWolfGPU::memory_usage()
src/GPU/pair_born_coul_wolf_gpu.cpp:  return bytes + borncw_gpu_bytes();
src/GPU/pair_born_coul_wolf_gpu.cpp:void PairBornCoulWolfGPU::cpu_compute(int start, int inum, int eflag, int /* vflag */, int *ilist,
src/GPU/pair_soft_gpu.cpp:#include "pair_soft_gpu.h"
src/GPU/pair_soft_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_soft_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_soft_gpu.cpp:int soft_gpu_init(const int ntypes, double **cutsq, double **prefactor, double **cut,
src/GPU/pair_soft_gpu.cpp:                  const int maxspecial, const double cell_size, int &gpu_mode, FILE *screen);
src/GPU/pair_soft_gpu.cpp:void soft_gpu_reinit(const int ntypes, double **cutsq, double **host_prefactor, double **host_cut);
src/GPU/pair_soft_gpu.cpp:void soft_gpu_clear();
src/GPU/pair_soft_gpu.cpp:int **soft_gpu_compute_n(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_soft_gpu.cpp:void soft_gpu_compute(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_soft_gpu.cpp:double soft_gpu_bytes();
src/GPU/pair_soft_gpu.cpp:PairSoftGPU::PairSoftGPU(LAMMPS *lmp) : PairSoft(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_soft_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_soft_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_soft_gpu.cpp:PairSoftGPU::~PairSoftGPU()
src/GPU/pair_soft_gpu.cpp:  soft_gpu_clear();
src/GPU/pair_soft_gpu.cpp:void PairSoftGPU::compute(int eflag, int vflag)
src/GPU/pair_soft_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_soft_gpu.cpp:        soft_gpu_compute_n(neighbor->ago, inum, nall, atom->x, atom->type, sublo, subhi, atom->tag,
src/GPU/pair_soft_gpu.cpp:    soft_gpu_compute(neighbor->ago, inum, nall, atom->x, atom->type, ilist, numneigh, firstneigh,
src/GPU/pair_soft_gpu.cpp:void PairSoftGPU::init_style()
src/GPU/pair_soft_gpu.cpp:      soft_gpu_init(atom->ntypes + 1, cutsq, prefactor, cut, force->special_lj, atom->nlocal,
src/GPU/pair_soft_gpu.cpp:                    atom->nlocal + atom->nghost, mnf, maxspecial, cell_size, gpu_mode, screen);
src/GPU/pair_soft_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_soft_gpu.cpp:  if (gpu_mode == GPU_FORCE) neighbor->add_request(this, NeighConst::REQ_FULL);
src/GPU/pair_soft_gpu.cpp:void PairSoftGPU::reinit()
src/GPU/pair_soft_gpu.cpp:  soft_gpu_reinit(atom->ntypes + 1, cutsq, prefactor, cut);
src/GPU/pair_soft_gpu.cpp:double PairSoftGPU::memory_usage()
src/GPU/pair_soft_gpu.cpp:  return bytes + soft_gpu_bytes();
src/GPU/pair_soft_gpu.cpp:void PairSoftGPU::cpu_compute(int start, int inum, int eflag, int /* vflag */, int *ilist,
src/GPU/fix_nvt_gpu.h:FixStyle(nvt/gpu,FixNVTGPU);
src/GPU/fix_nvt_gpu.h:#ifndef LMP_FIX_NVT_GPU_H
src/GPU/fix_nvt_gpu.h:#define LMP_FIX_NVT_GPU_H
src/GPU/fix_nvt_gpu.h:#include "fix_nh_gpu.h"
src/GPU/fix_nvt_gpu.h:class FixNVTGPU : public FixNHGPU {
src/GPU/fix_nvt_gpu.h:  FixNVTGPU(class LAMMPS *, int, char **);
src/GPU/pair_resquared_gpu.cpp:#include "pair_resquared_gpu.h"
src/GPU/pair_resquared_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_resquared_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_resquared_gpu.cpp:int re_gpu_init(const int ntypes, double **shape, double **well, double **cutsq,
src/GPU/pair_resquared_gpu.cpp:                const int maxspecial, const double cell_size, int &gpu_mode,
src/GPU/pair_resquared_gpu.cpp:void re_gpu_clear();
src/GPU/pair_resquared_gpu.cpp:int **re_gpu_compute_n(const int ago, const int inum, const int nall,
src/GPU/pair_resquared_gpu.cpp:int *re_gpu_compute(const int ago, const int inum, const int nall,
src/GPU/pair_resquared_gpu.cpp:double re_gpu_bytes();
src/GPU/pair_resquared_gpu.cpp:PairRESquaredGPU::PairRESquaredGPU(LAMMPS *lmp) : PairRESquared(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_resquared_gpu.cpp:  if (!avec) error->all(FLERR, "Pair resquared/gpu requires atom style ellipsoid");
src/GPU/pair_resquared_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_resquared_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_resquared_gpu.cpp:PairRESquaredGPU::~PairRESquaredGPU()
src/GPU/pair_resquared_gpu.cpp:  re_gpu_clear();
src/GPU/pair_resquared_gpu.cpp:void PairRESquaredGPU::compute(int eflag, int vflag)
src/GPU/pair_resquared_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_resquared_gpu.cpp:        re_gpu_compute_n(neighbor->ago, inum, nall, atom->x, atom->type, sublo,
src/GPU/pair_resquared_gpu.cpp:    ilist = re_gpu_compute(neighbor->ago, inum, nall, atom->x, atom->type,
src/GPU/pair_resquared_gpu.cpp:void PairRESquaredGPU::init_style()
src/GPU/pair_resquared_gpu.cpp:  if (!atom->ellipsoid_flag) error->all(FLERR, "Pair resquared/gpu requires atom style ellipsoid");
src/GPU/pair_resquared_gpu.cpp:      error->all(FLERR, "Pair resquared/gpu requires atoms with same type have same shape");
src/GPU/pair_resquared_gpu.cpp:      re_gpu_init(atom->ntypes + 1, shape1, well, cutsq, sigma, epsilon, form,
src/GPU/pair_resquared_gpu.cpp:                  gpu_mode, screen);
src/GPU/pair_resquared_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_resquared_gpu.cpp:  if (gpu_mode == GPU_FORCE) neighbor->add_request(this, NeighConst::REQ_FULL);
src/GPU/pair_resquared_gpu.cpp:double PairRESquaredGPU::memory_usage()
src/GPU/pair_resquared_gpu.cpp:  return bytes + re_gpu_bytes();
src/GPU/pair_resquared_gpu.cpp:void PairRESquaredGPU::cpu_compute(int start, int inum, int eflag, int /* vflag */, int *ilist,
src/GPU/pair_table_gpu.cpp:#include "pair_table_gpu.h"
src/GPU/pair_table_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_table_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_table_gpu.cpp:int table_gpu_init(const int ntypes, double **cutsq, double ***host_table_coeffs,
src/GPU/pair_table_gpu.cpp:                   const int max_nbors, const int maxspecial, const double cell_size, int &gpu_mode,
src/GPU/pair_table_gpu.cpp:void table_gpu_clear();
src/GPU/pair_table_gpu.cpp:int **table_gpu_compute_n(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_table_gpu.cpp:void table_gpu_compute(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_table_gpu.cpp:double table_gpu_bytes();
src/GPU/pair_table_gpu.cpp:PairTableGPU::PairTableGPU(LAMMPS *lmp) : PairTable(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_table_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_table_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_table_gpu.cpp:PairTableGPU::~PairTableGPU()
src/GPU/pair_table_gpu.cpp:  table_gpu_clear();
src/GPU/pair_table_gpu.cpp:void PairTableGPU::compute(int eflag, int vflag)
src/GPU/pair_table_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_table_gpu.cpp:        table_gpu_compute_n(neighbor->ago, inum, nall, atom->x, atom->type, sublo, subhi, atom->tag,
src/GPU/pair_table_gpu.cpp:    table_gpu_compute(neighbor->ago, inum, nall, atom->x, atom->type, ilist, numneigh, firstneigh,
src/GPU/pair_table_gpu.cpp:void PairTableGPU::init_style()
src/GPU/pair_table_gpu.cpp:  int success = table_gpu_init(atom->ntypes + 1, cutsq, table_coeffs, table_data, force->special_lj,
src/GPU/pair_table_gpu.cpp:                               cell_size, gpu_mode, screen, tabstyle, ntables, tablength);
src/GPU/pair_table_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_table_gpu.cpp:  if (gpu_mode == GPU_FORCE) neighbor->add_request(this, NeighConst::REQ_FULL);
src/GPU/pair_table_gpu.cpp:double PairTableGPU::memory_usage()
src/GPU/pair_table_gpu.cpp:  return bytes + table_gpu_bytes();
src/GPU/pair_table_gpu.cpp:void PairTableGPU::cpu_compute(int start, int inum, int eflag, int /* vflag */, int *ilist,
src/GPU/pair_dpd_tstat_gpu.h:PairStyle(dpd/tstat/gpu,PairDPDTstatGPU);
src/GPU/pair_dpd_tstat_gpu.h:#ifndef LMP_PAIR_DPD_TSTAT_GPU_H
src/GPU/pair_dpd_tstat_gpu.h:#define LMP_PAIR_DPD_TSTAT_GPU_H
src/GPU/pair_dpd_tstat_gpu.h:class PairDPDTstatGPU : public PairDPDTstat {
src/GPU/pair_dpd_tstat_gpu.h:  PairDPDTstatGPU(LAMMPS *lmp);
src/GPU/pair_dpd_tstat_gpu.h:  ~PairDPDTstatGPU() override;
src/GPU/pair_dpd_tstat_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_dpd_tstat_gpu.h:  int gpu_mode;
src/GPU/pair_yukawa_gpu.h:PairStyle(yukawa/gpu,PairYukawaGPU);
src/GPU/pair_yukawa_gpu.h:#ifndef LMP_PAIR_YUKAWA_GPU_H
src/GPU/pair_yukawa_gpu.h:#define LMP_PAIR_YUKAWA_GPU_H
src/GPU/pair_yukawa_gpu.h:class PairYukawaGPU : public PairYukawa {
src/GPU/pair_yukawa_gpu.h:  PairYukawaGPU(LAMMPS *lmp);
src/GPU/pair_yukawa_gpu.h:  ~PairYukawaGPU() override;
src/GPU/pair_yukawa_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_yukawa_gpu.h:  int gpu_mode;
src/GPU/pair_lj_class2_gpu.h:PairStyle(lj/class2/gpu,PairLJClass2GPU);
src/GPU/pair_lj_class2_gpu.h:#ifndef LMP_PAIR_LJ_CLASS2_GPU_H
src/GPU/pair_lj_class2_gpu.h:#define LMP_PAIR_LJ_CLASS2_GPU_H
src/GPU/pair_lj_class2_gpu.h:class PairLJClass2GPU : public PairLJClass2 {
src/GPU/pair_lj_class2_gpu.h:  PairLJClass2GPU(LAMMPS *lmp);
src/GPU/pair_lj_class2_gpu.h:  ~PairLJClass2GPU() override;
src/GPU/pair_lj_class2_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_lj_class2_gpu.h:  int gpu_mode;
src/GPU/pair_lj_cut_gpu.cpp:#include "pair_lj_cut_gpu.h"
src/GPU/pair_lj_cut_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_lj_cut_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_lj_cut_gpu.cpp:int ljl_gpu_init(const int ntypes, double **cutsq, double **host_lj1, double **host_lj2,
src/GPU/pair_lj_cut_gpu.cpp:                 const double cell_size, int &gpu_mode, FILE *screen);
src/GPU/pair_lj_cut_gpu.cpp:void ljl_gpu_reinit(const int ntypes, double **cutsq, double **host_lj1, double **host_lj2,
src/GPU/pair_lj_cut_gpu.cpp:void ljl_gpu_clear();
src/GPU/pair_lj_cut_gpu.cpp:int **ljl_gpu_compute_n(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_lj_cut_gpu.cpp:void ljl_gpu_compute(const int ago, const int inum, const int nall, double **host_x, int *host_type,
src/GPU/pair_lj_cut_gpu.cpp:double ljl_gpu_bytes();
src/GPU/pair_lj_cut_gpu.cpp:PairLJCutGPU::PairLJCutGPU(LAMMPS *lmp) : PairLJCut(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_lj_cut_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_lj_cut_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_lj_cut_gpu.cpp:PairLJCutGPU::~PairLJCutGPU()
src/GPU/pair_lj_cut_gpu.cpp:  ljl_gpu_clear();
src/GPU/pair_lj_cut_gpu.cpp:void PairLJCutGPU::compute(int eflag, int vflag)
src/GPU/pair_lj_cut_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_lj_cut_gpu.cpp:        ljl_gpu_compute_n(neighbor->ago, inum, nall, atom->x, atom->type, sublo, subhi, atom->tag,
src/GPU/pair_lj_cut_gpu.cpp:    ljl_gpu_compute(neighbor->ago, inum, nall, atom->x, atom->type, ilist, numneigh, firstneigh,
src/GPU/pair_lj_cut_gpu.cpp:void PairLJCutGPU::init_style()
src/GPU/pair_lj_cut_gpu.cpp:  int success = ljl_gpu_init(atom->ntypes + 1, cutsq, lj1, lj2, lj3, lj4, offset, force->special_lj,
src/GPU/pair_lj_cut_gpu.cpp:                             gpu_mode, screen);
src/GPU/pair_lj_cut_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_lj_cut_gpu.cpp:  if (gpu_mode == GPU_FORCE) neighbor->add_request(this, NeighConst::REQ_FULL);
src/GPU/pair_lj_cut_gpu.cpp:void PairLJCutGPU::reinit()
src/GPU/pair_lj_cut_gpu.cpp:  ljl_gpu_reinit(atom->ntypes + 1, cutsq, lj1, lj2, lj3, lj4, offset);
src/GPU/pair_lj_cut_gpu.cpp:double PairLJCutGPU::memory_usage()
src/GPU/pair_lj_cut_gpu.cpp:  return bytes + ljl_gpu_bytes();
src/GPU/pair_lj_cut_gpu.cpp:void PairLJCutGPU::cpu_compute(int start, int inum, int eflag, int /* vflag */, int *ilist,
src/GPU/pair_zbl_gpu.cpp:#include "pair_zbl_gpu.h"
src/GPU/pair_zbl_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_zbl_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_zbl_gpu.cpp:int zbl_gpu_init(const int ntypes, double **cutsq, double **host_sw1, double **host_sw2,
src/GPU/pair_zbl_gpu.cpp:                 int &gpu_mode, FILE *screen);
src/GPU/pair_zbl_gpu.cpp:void zbl_gpu_clear();
src/GPU/pair_zbl_gpu.cpp:int **zbl_gpu_compute_n(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_zbl_gpu.cpp:void zbl_gpu_compute(const int ago, const int inum, const int nall, double **host_x, int *host_type,
src/GPU/pair_zbl_gpu.cpp:double zbl_gpu_bytes();
src/GPU/pair_zbl_gpu.cpp:PairZBLGPU::PairZBLGPU(LAMMPS *lmp) : PairZBL(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_zbl_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_zbl_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_zbl_gpu.cpp:PairZBLGPU::~PairZBLGPU()
src/GPU/pair_zbl_gpu.cpp:  zbl_gpu_clear();
src/GPU/pair_zbl_gpu.cpp:void PairZBLGPU::compute(int eflag, int vflag)
src/GPU/pair_zbl_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_zbl_gpu.cpp:        zbl_gpu_compute_n(neighbor->ago, inum, nall, atom->x, atom->type, sublo, subhi, atom->tag,
src/GPU/pair_zbl_gpu.cpp:    zbl_gpu_compute(neighbor->ago, inum, nall, atom->x, atom->type, ilist, numneigh, firstneigh,
src/GPU/pair_zbl_gpu.cpp:void PairZBLGPU::init_style()
src/GPU/pair_zbl_gpu.cpp:      zbl_gpu_init(atom->ntypes + 1, cutsq, sw1, sw2, sw3, sw4, sw5, d1a, d2a, d3a, d4a, zze,
src/GPU/pair_zbl_gpu.cpp:                   mnf, maxspecial, cell_size, gpu_mode, screen);
src/GPU/pair_zbl_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_zbl_gpu.cpp:  if (gpu_mode == GPU_FORCE) neighbor->add_request(this, NeighConst::REQ_FULL);
src/GPU/pair_zbl_gpu.cpp:double PairZBLGPU::memory_usage()
src/GPU/pair_zbl_gpu.cpp:  return bytes + zbl_gpu_bytes();
src/GPU/pair_zbl_gpu.cpp:void PairZBLGPU::cpu_compute(int start, int inum, int eflag, int /* vflag */, int *ilist,
src/GPU/pair_lj_cut_coul_cut_gpu.h:PairStyle(lj/cut/coul/cut/gpu,PairLJCutCoulCutGPU);
src/GPU/pair_lj_cut_coul_cut_gpu.h:#ifndef LMP_PAIR_LJ_CUT_COUL_CUT_GPU_H
src/GPU/pair_lj_cut_coul_cut_gpu.h:#define LMP_PAIR_LJ_CUT_COUL_CUT_GPU_H
src/GPU/pair_lj_cut_coul_cut_gpu.h:class PairLJCutCoulCutGPU : public PairLJCutCoulCut {
src/GPU/pair_lj_cut_coul_cut_gpu.h:  PairLJCutCoulCutGPU(LAMMPS *lmp);
src/GPU/pair_lj_cut_coul_cut_gpu.h:  ~PairLJCutCoulCutGPU() override;
src/GPU/pair_lj_cut_coul_cut_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_lj_cut_coul_cut_gpu.h:  int gpu_mode;
src/GPU/pair_vashishta_gpu.cpp:#include "pair_vashishta_gpu.h"
src/GPU/pair_vashishta_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_vashishta_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_vashishta_gpu.cpp:int vashishta_gpu_init(const int ntypes, const int inum, const int nall, const int max_nbors,
src/GPU/pair_vashishta_gpu.cpp:                       const double cell_size, int &gpu_mode, FILE *screen, int *host_map,
src/GPU/pair_vashishta_gpu.cpp:void vashishta_gpu_clear();
src/GPU/pair_vashishta_gpu.cpp:int **vashishta_gpu_compute_n(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_vashishta_gpu.cpp:void vashishta_gpu_compute(const int ago, const int nloc, const int nall, const int ln,
src/GPU/pair_vashishta_gpu.cpp:double vashishta_gpu_bytes();
src/GPU/pair_vashishta_gpu.cpp:PairVashishtaGPU::PairVashishtaGPU(LAMMPS *lmp) : PairVashishta(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_vashishta_gpu.cpp:  gpu_allocated = false;
src/GPU/pair_vashishta_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_vashishta_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_vashishta_gpu.cpp:PairVashishtaGPU::~PairVashishtaGPU()
src/GPU/pair_vashishta_gpu.cpp:  vashishta_gpu_clear();
src/GPU/pair_vashishta_gpu.cpp:void PairVashishtaGPU::compute(int eflag, int vflag)
src/GPU/pair_vashishta_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_vashishta_gpu.cpp:        vashishta_gpu_compute_n(neighbor->ago, inum, nall, atom->x, atom->type, sublo, subhi,
src/GPU/pair_vashishta_gpu.cpp:    vashishta_gpu_compute(neighbor->ago, inum, nall, inum + list->gnum, atom->x, atom->type, ilist,
src/GPU/pair_vashishta_gpu.cpp:void PairVashishtaGPU::allocate()
src/GPU/pair_vashishta_gpu.cpp:  gpu_allocated = true;
src/GPU/pair_vashishta_gpu.cpp:void PairVashishtaGPU::init_style()
src/GPU/pair_vashishta_gpu.cpp:  if (atom->tag_enable == 0) error->all(FLERR, "Pair style vashishta/gpu requires atom IDs");
src/GPU/pair_vashishta_gpu.cpp:  int success = vashishta_gpu_init(atom->ntypes + 1, atom->nlocal, atom->nlocal + atom->nghost, mnf,
src/GPU/pair_vashishta_gpu.cpp:                                   cell_size, gpu_mode, screen, map, nelements, elem3param, nparams,
src/GPU/pair_vashishta_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_vashishta_gpu.cpp:  if (gpu_mode == GPU_FORCE)
src/GPU/pair_vashishta_gpu.cpp:      error->warning(FLERR, "Increasing communication cutoff to {:.8} for GPU pair style",
src/GPU/pair_vashishta_gpu.cpp:double PairVashishtaGPU::init_one(int i, int j)
src/GPU/pair_vashishta_gpu.cpp:  if (!gpu_allocated) { allocate(); }
src/GPU/pair_resquared_gpu.h:PairStyle(resquared/gpu,PairRESquaredGPU);
src/GPU/pair_resquared_gpu.h:#ifndef LMP_PAIR_RESQUARED_GPU_H
src/GPU/pair_resquared_gpu.h:#define LMP_PAIR_RESQUARED_GPU_H
src/GPU/pair_resquared_gpu.h:class PairRESquaredGPU : public PairRESquared {
src/GPU/pair_resquared_gpu.h:  PairRESquaredGPU(LAMMPS *lmp);
src/GPU/pair_resquared_gpu.h:  ~PairRESquaredGPU() override;
src/GPU/pair_resquared_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_resquared_gpu.h:  int gpu_mode;
src/GPU/pair_buck_gpu.cpp:#include "pair_buck_gpu.h"
src/GPU/pair_buck_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_buck_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_buck_gpu.cpp:int buck_gpu_init(const int ntypes, double **cutsq, double **host_rhoinv, double **host_buck1,
src/GPU/pair_buck_gpu.cpp:                  const int maxspecial, const double cell_size, int &gpu_mode, FILE *screen);
src/GPU/pair_buck_gpu.cpp:void buck_gpu_reinit(const int ntypes, double **cutsq, double **host_rhoinv, double **host_buck1,
src/GPU/pair_buck_gpu.cpp:void buck_gpu_clear();
src/GPU/pair_buck_gpu.cpp:int **buck_gpu_compute_n(const int ago, const int inum_full, const int nall, double **host_x,
src/GPU/pair_buck_gpu.cpp:void buck_gpu_compute(const int ago, const int inum_full, const int nall, double **host_x,
src/GPU/pair_buck_gpu.cpp:double buck_gpu_bytes();
src/GPU/pair_buck_gpu.cpp:PairBuckGPU::PairBuckGPU(LAMMPS *lmp) : PairBuck(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_buck_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_buck_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_buck_gpu.cpp:PairBuckGPU::~PairBuckGPU()
src/GPU/pair_buck_gpu.cpp:  buck_gpu_clear();
src/GPU/pair_buck_gpu.cpp:void PairBuckGPU::compute(int eflag, int vflag)
src/GPU/pair_buck_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_buck_gpu.cpp:        buck_gpu_compute_n(neighbor->ago, inum, nall, atom->x, atom->type, sublo, subhi, atom->tag,
src/GPU/pair_buck_gpu.cpp:    buck_gpu_compute(neighbor->ago, inum, nall, atom->x, atom->type, ilist, numneigh, firstneigh,
src/GPU/pair_buck_gpu.cpp:void PairBuckGPU::init_style()
src/GPU/pair_buck_gpu.cpp:  int success = buck_gpu_init(atom->ntypes + 1, cutsq, rhoinv, buck1, buck2, a, c, offset,
src/GPU/pair_buck_gpu.cpp:                              maxspecial, cell_size, gpu_mode, screen);
src/GPU/pair_buck_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_buck_gpu.cpp:  if (gpu_mode == GPU_FORCE) neighbor->add_request(this, NeighConst::REQ_FULL);
src/GPU/pair_buck_gpu.cpp:void PairBuckGPU::reinit()
src/GPU/pair_buck_gpu.cpp:  buck_gpu_reinit(atom->ntypes + 1, cutsq, rhoinv, buck1, buck2, a, c, offset);
src/GPU/pair_buck_gpu.cpp:double PairBuckGPU::memory_usage()
src/GPU/pair_buck_gpu.cpp:  return bytes + buck_gpu_bytes();
src/GPU/pair_buck_gpu.cpp:void PairBuckGPU::cpu_compute(int start, int inum, int eflag, int /* vflag */, int *ilist,
src/GPU/fix_nve_asphere_gpu.h:FixStyle(nve/asphere/gpu,FixNVEAsphereGPU);
src/GPU/fix_nve_asphere_gpu.h:#ifndef LMP_FIX_NVE_ASPHERE_GPU_H
src/GPU/fix_nve_asphere_gpu.h:#define LMP_FIX_NVE_ASPHERE_GPU_H
src/GPU/fix_nve_asphere_gpu.h:class FixNVEAsphereGPU : public FixNVE {
src/GPU/fix_nve_asphere_gpu.h:  FixNVEAsphereGPU(class LAMMPS *, int, char **);
src/GPU/pair_gayberne_gpu.cpp:#include "pair_gayberne_gpu.h"
src/GPU/pair_gayberne_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_gayberne_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_gayberne_gpu.cpp:int gb_gpu_init(const int ntypes, const double gamma, const double upsilon,
src/GPU/pair_gayberne_gpu.cpp:                const double cell_size, int &gpu_mode, FILE *screen);
src/GPU/pair_gayberne_gpu.cpp:void gb_gpu_clear();
src/GPU/pair_gayberne_gpu.cpp:int **gb_gpu_compute_n(const int ago, const int inum, const int nall,
src/GPU/pair_gayberne_gpu.cpp:int *gb_gpu_compute(const int ago, const int inum, const int nall,
src/GPU/pair_gayberne_gpu.cpp:double gb_gpu_bytes();
src/GPU/pair_gayberne_gpu.cpp:PairGayBerneGPU::PairGayBerneGPU(LAMMPS *lmp) : PairGayBerne(lmp),
src/GPU/pair_gayberne_gpu.cpp:                                                gpu_mode(GPU_FORCE)
src/GPU/pair_gayberne_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_gayberne_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_gayberne_gpu.cpp:PairGayBerneGPU::~PairGayBerneGPU()
src/GPU/pair_gayberne_gpu.cpp:  gb_gpu_clear();
src/GPU/pair_gayberne_gpu.cpp:void PairGayBerneGPU::compute(int eflag, int vflag)
src/GPU/pair_gayberne_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_gayberne_gpu.cpp:        gb_gpu_compute_n(neighbor->ago, inum, nall, atom->x, atom->type, sublo,
src/GPU/pair_gayberne_gpu.cpp:    ilist = gb_gpu_compute(neighbor->ago, inum, nall, atom->x, atom->type,
src/GPU/pair_gayberne_gpu.cpp:void PairGayBerneGPU::init_style()
src/GPU/pair_gayberne_gpu.cpp:  if (!avec) error->all(FLERR, "Pair gayberne/gpu requires atom style ellipsoid");
src/GPU/pair_gayberne_gpu.cpp:  if (!atom->ellipsoid_flag) error->all(FLERR, "Pair gayberne/gpu requires atom style ellipsoid");
src/GPU/pair_gayberne_gpu.cpp:      error->all(FLERR, "Pair gayberne/gpu requires atoms with same type have same shape");
src/GPU/pair_gayberne_gpu.cpp:      gb_gpu_init(atom->ntypes + 1, gamma, upsilon, mu, shape2, well, cutsq,
src/GPU/pair_gayberne_gpu.cpp:                  mnf, maxspecial, cell_size, gpu_mode, screen);
src/GPU/pair_gayberne_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_gayberne_gpu.cpp:  if (gpu_mode == GPU_FORCE) neighbor->add_request(this, NeighConst::REQ_FULL);
src/GPU/pair_gayberne_gpu.cpp:double PairGayBerneGPU::memory_usage()
src/GPU/pair_gayberne_gpu.cpp:  return bytes + gb_gpu_bytes();
src/GPU/pair_gayberne_gpu.cpp:void PairGayBerneGPU::cpu_compute(int start, int inum, int eflag,
src/GPU/pair_coul_long_gpu.h:PairStyle(coul/long/gpu,PairCoulLongGPU);
src/GPU/pair_coul_long_gpu.h:#ifndef LMP_PAIR_COUL_LONG_GPU_H
src/GPU/pair_coul_long_gpu.h:#define LMP_PAIR_COUL_LONG_GPU_H
src/GPU/pair_coul_long_gpu.h:class PairCoulLongGPU : public PairCoulLong {
src/GPU/pair_coul_long_gpu.h:  PairCoulLongGPU(LAMMPS *lmp);
src/GPU/pair_coul_long_gpu.h:  ~PairCoulLongGPU() override;
src/GPU/pair_coul_long_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_coul_long_gpu.h:  int gpu_mode;
src/GPU/pair_lj_spica_gpu.cpp:#include "pair_lj_spica_gpu.h"
src/GPU/pair_lj_spica_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_lj_spica_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_lj_spica_gpu.cpp:int spica_gpu_init(const int ntypes, double **cutsq, int **cg_types, double **host_lj1,
src/GPU/pair_lj_spica_gpu.cpp:                   const int maxspecial, const double cell_size, int &gpu_mode, FILE *screen);
src/GPU/pair_lj_spica_gpu.cpp:void spica_gpu_clear();
src/GPU/pair_lj_spica_gpu.cpp:int **spica_gpu_compute_n(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_lj_spica_gpu.cpp:void spica_gpu_compute(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_lj_spica_gpu.cpp:double spica_gpu_bytes();
src/GPU/pair_lj_spica_gpu.cpp:PairLJSPICAGPU::PairLJSPICAGPU(LAMMPS *lmp) : PairLJSPICA(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_lj_spica_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_lj_spica_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_lj_spica_gpu.cpp:PairLJSPICAGPU::~PairLJSPICAGPU()
src/GPU/pair_lj_spica_gpu.cpp:  spica_gpu_clear();
src/GPU/pair_lj_spica_gpu.cpp:void PairLJSPICAGPU::compute(int eflag, int vflag)
src/GPU/pair_lj_spica_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_lj_spica_gpu.cpp:        spica_gpu_compute_n(neighbor->ago, inum, nall, atom->x, atom->type, sublo, subhi, atom->tag,
src/GPU/pair_lj_spica_gpu.cpp:    spica_gpu_compute(neighbor->ago, inum, nall, atom->x, atom->type, ilist, numneigh, firstneigh,
src/GPU/pair_lj_spica_gpu.cpp:void PairLJSPICAGPU::init_style()
src/GPU/pair_lj_spica_gpu.cpp:  int success = spica_gpu_init(atom->ntypes + 1, cutsq, lj_type, lj1, lj2, lj3, lj4, offset,
src/GPU/pair_lj_spica_gpu.cpp:                               maxspecial, cell_size, gpu_mode, screen);
src/GPU/pair_lj_spica_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_lj_spica_gpu.cpp:  if (gpu_mode == GPU_FORCE) neighbor->add_request(this, NeighConst::REQ_FULL);
src/GPU/pair_lj_spica_gpu.cpp:double PairLJSPICAGPU::memory_usage()
src/GPU/pair_lj_spica_gpu.cpp:  return bytes + spica_gpu_bytes();
src/GPU/pair_lj_spica_gpu.cpp:void PairLJSPICAGPU::cpu_compute(int start, int inum, int *ilist, int *numneigh, int **firstneigh)
src/GPU/pair_sph_heatconduction_gpu.cpp:#include "pair_sph_heatconduction_gpu.h"
src/GPU/pair_sph_heatconduction_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_sph_heatconduction_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_sph_heatconduction_gpu.cpp:int sph_heatconduction_gpu_init(const int ntypes, double **cutsq, double** host_cut,
src/GPU/pair_sph_heatconduction_gpu.cpp:                                const double cell_size, int &gpu_mode, FILE *screen);
src/GPU/pair_sph_heatconduction_gpu.cpp:void sph_heatconduction_gpu_clear();
src/GPU/pair_sph_heatconduction_gpu.cpp:int **sph_heatconduction_gpu_compute_n(const int ago, const int inum_full, const int nall,
src/GPU/pair_sph_heatconduction_gpu.cpp:void sph_heatconduction_gpu_compute(const int ago, const int inum_full, const int nall,
src/GPU/pair_sph_heatconduction_gpu.cpp:void sph_heatconduction_gpu_get_extra_data(double *host_rho, double *host_esph);
src/GPU/pair_sph_heatconduction_gpu.cpp:void sph_heatconduction_gpu_update_dE(void **dE_ptr);
src/GPU/pair_sph_heatconduction_gpu.cpp:double sph_heatconduction_gpu_bytes();
src/GPU/pair_sph_heatconduction_gpu.cpp:PairSPHHeatConductionGPU::PairSPHHeatConductionGPU(LAMMPS *lmp) :
src/GPU/pair_sph_heatconduction_gpu.cpp:  PairSPHHeatConduction(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_sph_heatconduction_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_sph_heatconduction_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_sph_heatconduction_gpu.cpp:PairSPHHeatConductionGPU::~PairSPHHeatConductionGPU()
src/GPU/pair_sph_heatconduction_gpu.cpp:  sph_heatconduction_gpu_clear();
src/GPU/pair_sph_heatconduction_gpu.cpp:void PairSPHHeatConductionGPU::compute(int eflag, int vflag)
src/GPU/pair_sph_heatconduction_gpu.cpp:  sph_heatconduction_gpu_get_extra_data(rho, esph);
src/GPU/pair_sph_heatconduction_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_sph_heatconduction_gpu.cpp:    firstneigh = sph_heatconduction_gpu_compute_n(
src/GPU/pair_sph_heatconduction_gpu.cpp:    sph_heatconduction_gpu_compute(neighbor->ago, inum, nall, atom->x, atom->type,
src/GPU/pair_sph_heatconduction_gpu.cpp:  sph_heatconduction_gpu_update_dE(&dE_pinned);
src/GPU/pair_sph_heatconduction_gpu.cpp:void PairSPHHeatConductionGPU::init_style()
src/GPU/pair_sph_heatconduction_gpu.cpp:      sph_heatconduction_gpu_init(atom->ntypes + 1, cutsq, cut, alpha, atom->mass,
src/GPU/pair_sph_heatconduction_gpu.cpp:                      mnf, maxspecial, cell_size, gpu_mode, screen);
src/GPU/pair_sph_heatconduction_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_sph_heatconduction_gpu.cpp:  acc_float = Info::has_accelerator_feature("GPU", "precision", "single");
src/GPU/pair_sph_heatconduction_gpu.cpp:  if (gpu_mode == GPU_FORCE) neighbor->add_request(this, NeighConst::REQ_FULL);
src/GPU/pair_sph_heatconduction_gpu.cpp:double PairSPHHeatConductionGPU::memory_usage()
src/GPU/pair_sph_heatconduction_gpu.cpp:  return bytes + sph_heatconduction_gpu_bytes();
src/GPU/pair_born_coul_wolf_cs_gpu.h:PairStyle(born/coul/wolf/cs/gpu,PairBornCoulWolfCSGPU);
src/GPU/pair_born_coul_wolf_cs_gpu.h:#ifndef LMP_PAIR_BORN_COUL_WOLF_CS_GPU_H
src/GPU/pair_born_coul_wolf_cs_gpu.h:#define LMP_PAIR_BORN_COUL_WOLF_CS_GPU_H
src/GPU/pair_born_coul_wolf_cs_gpu.h:class PairBornCoulWolfCSGPU : public PairBornCoulWolfCS {
src/GPU/pair_born_coul_wolf_cs_gpu.h:  PairBornCoulWolfCSGPU(LAMMPS *lmp);
src/GPU/pair_born_coul_wolf_cs_gpu.h:  ~PairBornCoulWolfCSGPU() override;
src/GPU/pair_born_coul_wolf_cs_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_born_coul_wolf_cs_gpu.h:  int gpu_mode;
src/GPU/pair_dpd_coul_slater_long_gpu.cpp:#include "pair_dpd_coul_slater_long_gpu.h"
src/GPU/pair_dpd_coul_slater_long_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_dpd_coul_slater_long_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_dpd_coul_slater_long_gpu.cpp:int dpd_coul_slater_long_gpu_init(const int ntypes, double **cutsq, double **host_a0,
src/GPU/pair_dpd_coul_slater_long_gpu.cpp:                                  const double cell_size, int &gpu_mode, FILE *screen,
src/GPU/pair_dpd_coul_slater_long_gpu.cpp:void dpd_coul_slater_long_gpu_clear();
src/GPU/pair_dpd_coul_slater_long_gpu.cpp:int **dpd_coul_slater_long_gpu_compute_n(const int ago, const int inum_full, const int nall,
src/GPU/pair_dpd_coul_slater_long_gpu.cpp:void dpd_coul_slater_long_gpu_compute(const int ago, const int inum_full, const int nall,
src/GPU/pair_dpd_coul_slater_long_gpu.cpp:void dpd_coul_slater_long_gpu_get_extra_data(double *host_q);
src/GPU/pair_dpd_coul_slater_long_gpu.cpp:double dpd_coul_slater_long_gpu_bytes();
src/GPU/pair_dpd_coul_slater_long_gpu.cpp:PairDPDCoulSlaterLongGPU::PairDPDCoulSlaterLongGPU(LAMMPS *lmp) : PairDPDCoulSlaterLong(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_dpd_coul_slater_long_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_dpd_coul_slater_long_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_dpd_coul_slater_long_gpu.cpp:PairDPDCoulSlaterLongGPU::~PairDPDCoulSlaterLongGPU()
src/GPU/pair_dpd_coul_slater_long_gpu.cpp:  dpd_coul_slater_long_gpu_clear();
src/GPU/pair_dpd_coul_slater_long_gpu.cpp:void PairDPDCoulSlaterLongGPU::compute(int eflag, int vflag)
src/GPU/pair_dpd_coul_slater_long_gpu.cpp:  dpd_coul_slater_long_gpu_get_extra_data(q);
src/GPU/pair_dpd_coul_slater_long_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_dpd_coul_slater_long_gpu.cpp:    firstneigh = dpd_coul_slater_long_gpu_compute_n(
src/GPU/pair_dpd_coul_slater_long_gpu.cpp:    dpd_coul_slater_long_gpu_compute(neighbor->ago, inum, nall, atom->x, atom->type, ilist, numneigh, firstneigh,
src/GPU/pair_dpd_coul_slater_long_gpu.cpp:void PairDPDCoulSlaterLongGPU::init_style()
src/GPU/pair_dpd_coul_slater_long_gpu.cpp:      dpd_coul_slater_long_gpu_init(atom->ntypes + 1, cutsq, a0, gamma, sigma,
src/GPU/pair_dpd_coul_slater_long_gpu.cpp:                   atom->nlocal + atom->nghost, mnf, maxspecial, cell_size, gpu_mode, screen,
src/GPU/pair_dpd_coul_slater_long_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_dpd_coul_slater_long_gpu.cpp:  if (gpu_mode == GPU_FORCE) neighbor->add_request(this, NeighConst::REQ_FULL);
src/GPU/pair_dpd_coul_slater_long_gpu.cpp:double PairDPDCoulSlaterLongGPU::memory_usage()
src/GPU/pair_dpd_coul_slater_long_gpu.cpp:  return bytes + dpd_coul_slater_long_gpu_bytes();
src/GPU/pair_dpd_coul_slater_long_gpu.cpp:void PairDPDCoulSlaterLongGPU::cpu_compute(int start, int inum, int eflag, int /* vflag */, int *ilist,
src/GPU/fix_gpu.h:FixStyle(GPU,FixGPU);
src/GPU/fix_gpu.h:#ifndef LMP_FIX_GPU_H
src/GPU/fix_gpu.h:#define LMP_FIX_GPU_H
src/GPU/fix_gpu.h:class FixGPU : public Fix {
src/GPU/fix_gpu.h:  FixGPU(class LAMMPS *, int, char **);
src/GPU/fix_gpu.h:  ~FixGPU() override;
src/GPU/fix_gpu.h:  int _gpu_mode;
src/GPU/pair_amoeba_gpu.cpp:#include "pair_amoeba_gpu.h"
src/GPU/pair_amoeba_gpu.cpp:#include "amoeba_convolution_gpu.h"
src/GPU/pair_amoeba_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_amoeba_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_amoeba_gpu.cpp:int amoeba_gpu_init(const int ntypes, const int max_amtype, const int max_amclass,
src/GPU/pair_amoeba_gpu.cpp:                    const double cell_size, int &gpu_mode, FILE *screen,
src/GPU/pair_amoeba_gpu.cpp:void amoeba_gpu_clear();
src/GPU/pair_amoeba_gpu.cpp:int** amoeba_gpu_precompute(const int ago, const int inum_full, const int nall,
src/GPU/pair_amoeba_gpu.cpp:void amoeba_gpu_compute_multipole_real(const int ago, const int inum, const int nall,
src/GPU/pair_amoeba_gpu.cpp:void amoeba_gpu_compute_udirect2b(int *host_amtype, int *host_amgroup,
src/GPU/pair_amoeba_gpu.cpp:void amoeba_gpu_compute_umutual2b(int *host_amtype, int *host_amgroup,
src/GPU/pair_amoeba_gpu.cpp:void amoeba_gpu_update_fieldp(void **fieldp_ptr);
src/GPU/pair_amoeba_gpu.cpp:void amoeba_gpu_precompute_kspace(const int inum_full, const int bsorder,
src/GPU/pair_amoeba_gpu.cpp:void amoeba_gpu_fphi_uind(double ****host_grid_brick, void **host_fdip_phi1,
src/GPU/pair_amoeba_gpu.cpp:void amoeba_gpu_fphi_mpole(double ***host_grid_brick, void **host_fdip_sum_phi,
src/GPU/pair_amoeba_gpu.cpp:void amoeba_gpu_compute_polar_real(int *host_amtype, int *host_amgroup,
src/GPU/pair_amoeba_gpu.cpp:double amoeba_gpu_bytes();
src/GPU/pair_amoeba_gpu.cpp:PairAmoebaGPU::PairAmoebaGPU(LAMMPS *lmp) : PairAmoeba(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_amoeba_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_amoeba_gpu.cpp:  gpu_hal_ready = false;               // true for AMOEBA when ready
src/GPU/pair_amoeba_gpu.cpp:  gpu_repulsion_ready = false;         // always false for AMOEBA
src/GPU/pair_amoeba_gpu.cpp:  gpu_dispersion_real_ready = false;   // always false for AMOEBA
src/GPU/pair_amoeba_gpu.cpp:  gpu_multipole_real_ready = true;     // need to be true for precompute()
src/GPU/pair_amoeba_gpu.cpp:  gpu_udirect2b_ready = true;
src/GPU/pair_amoeba_gpu.cpp:  gpu_umutual1_ready = true;
src/GPU/pair_amoeba_gpu.cpp:  gpu_fphi_uind_ready = true;
src/GPU/pair_amoeba_gpu.cpp:  gpu_umutual2b_ready = true;
src/GPU/pair_amoeba_gpu.cpp:  gpu_polar_real_ready = true;         // need to be true for copying data from device back to host
src/GPU/pair_amoeba_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_amoeba_gpu.cpp:PairAmoebaGPU::~PairAmoebaGPU()
src/GPU/pair_amoeba_gpu.cpp:  amoeba_gpu_clear();
src/GPU/pair_amoeba_gpu.cpp:void PairAmoebaGPU::compute(int eflag, int vflag)
src/GPU/pair_amoeba_gpu.cpp:void PairAmoebaGPU::init_style()
src/GPU/pair_amoeba_gpu.cpp:  int success = amoeba_gpu_init(atom->ntypes+1, max_amtype, max_amclass,
src/GPU/pair_amoeba_gpu.cpp:                                maxspecial15, cell_size, gpu_mode, screen,
src/GPU/pair_amoeba_gpu.cpp:  GPU_EXTRA::check_flag(success,error,world);
src/GPU/pair_amoeba_gpu.cpp:  if (gpu_mode == GPU_FORCE)
src/GPU/pair_amoeba_gpu.cpp:    error->all(FLERR,"Pair style amoeba/gpu does not support neigh no for now");
src/GPU/pair_amoeba_gpu.cpp:  acc_float = Info::has_accelerator_feature("GPU", "precision", "single");
src/GPU/pair_amoeba_gpu.cpp:  // replace with the gpu counterpart
src/GPU/pair_amoeba_gpu.cpp:  if (gpu_umutual1_ready) {
src/GPU/pair_amoeba_gpu.cpp:        new AmoebaConvolutionGPU(lmp,this,nefft1,nefft2,nefft3,bsporder,INDUCE_GRIDC);
src/GPU/pair_amoeba_gpu.cpp:void PairAmoebaGPU::multipole_real()
src/GPU/pair_amoeba_gpu.cpp:  if (!gpu_multipole_real_ready) {
src/GPU/pair_amoeba_gpu.cpp:  amoeba_gpu_precompute(neighbor->ago, inum, nall, atom->x,
src/GPU/pair_amoeba_gpu.cpp:  amoeba_gpu_compute_multipole_real(neighbor->ago, inum, nall, atom->x,
src/GPU/pair_amoeba_gpu.cpp:  // reference to the tep array from GPU lib
src/GPU/pair_amoeba_gpu.cpp:void PairAmoebaGPU::induce()
src/GPU/pair_amoeba_gpu.cpp:  if (!gpu_udirect2b_ready) {
src/GPU/pair_amoeba_gpu.cpp:    amoeba_gpu_precompute_kspace(atom->nlocal, bsorder, thetai1, thetai2,
src/GPU/pair_amoeba_gpu.cpp:      if (!gpu_umutual2b_ready) {
src/GPU/pair_amoeba_gpu.cpp:    if (!gpu_umutual2b_ready) {
src/GPU/pair_amoeba_gpu.cpp:      if (!gpu_umutual2b_ready) {
src/GPU/pair_amoeba_gpu.cpp:void PairAmoebaGPU::udirect2b(double **field, double **fieldp)
src/GPU/pair_amoeba_gpu.cpp:  if (!gpu_udirect2b_ready) {
src/GPU/pair_amoeba_gpu.cpp:  amoeba_gpu_compute_udirect2b(amtype, amgroup, rpole, uind, uinp,
src/GPU/pair_amoeba_gpu.cpp:  // accumulate the field and fieldp values from the GPU lib
src/GPU/pair_amoeba_gpu.cpp:void PairAmoebaGPU::udirect2b_cpu()
src/GPU/pair_amoeba_gpu.cpp:void PairAmoebaGPU::ufield0c(double **field, double **fieldp)
src/GPU/pair_amoeba_gpu.cpp:  // accumulate the field and fieldp values from the real-space portion from umutual2b() on the GPU
src/GPU/pair_amoeba_gpu.cpp:  amoeba_gpu_update_fieldp(&fieldp_pinned);
src/GPU/pair_amoeba_gpu.cpp:void PairAmoebaGPU::umutual1(double **field, double **fieldp)
src/GPU/pair_amoeba_gpu.cpp:void PairAmoebaGPU::fphi_uind(FFT_SCALAR ****grid, double **fdip_phi1,
src/GPU/pair_amoeba_gpu.cpp:  if (!gpu_fphi_uind_ready) {
src/GPU/pair_amoeba_gpu.cpp:  amoeba_gpu_fphi_uind(grid, &fdip_phi1_pinned, &fdip_phi2_pinned,
src/GPU/pair_amoeba_gpu.cpp:void PairAmoebaGPU::umutual2b(double **field, double **fieldp)
src/GPU/pair_amoeba_gpu.cpp:  if (!gpu_umutual2b_ready) {
src/GPU/pair_amoeba_gpu.cpp:  amoeba_gpu_compute_umutual2b(amtype, amgroup, rpole, uind, uinp,
src/GPU/pair_amoeba_gpu.cpp:void PairAmoebaGPU::polar_real()
src/GPU/pair_amoeba_gpu.cpp:  if (!gpu_polar_real_ready) {
src/GPU/pair_amoeba_gpu.cpp:  amoeba_gpu_compute_polar_real(amtype, amgroup, rpole, uind, uinp,
src/GPU/pair_amoeba_gpu.cpp:  // reference to the tep array from GPU lib
src/GPU/pair_amoeba_gpu.cpp:void PairAmoebaGPU::polar_kspace()
src/GPU/pair_amoeba_gpu.cpp:  bool gpu_fphi_mpole_ready = true;
src/GPU/pair_amoeba_gpu.cpp:    if (gpu_fphi_mpole_ready) {
src/GPU/pair_amoeba_gpu.cpp:       amoeba_gpu_precompute_kspace(atom->nlocal, bsorder,
src/GPU/pair_amoeba_gpu.cpp:    if (!gpu_fphi_mpole_ready) {
src/GPU/pair_amoeba_gpu.cpp:      amoeba_gpu_fphi_mpole(gridpost, &fphi_pinned, felec);
src/GPU/pair_amoeba_gpu.cpp:  // TODO: port the remaining loops to the GPU
src/GPU/pair_amoeba_gpu.cpp:void PairAmoebaGPU::compute_force_from_torque(const numtyp* tq_ptr,
src/GPU/pair_amoeba_gpu.cpp:double PairAmoebaGPU::memory_usage()
src/GPU/pair_amoeba_gpu.cpp:  return bytes + amoeba_gpu_bytes();
src/GPU/pair_eam_fs_gpu.h:PairStyle(eam/fs/gpu,PairEAMFSGPU);
src/GPU/pair_eam_fs_gpu.h:#ifndef LMP_PAIR_EAM_FS_GPU_H
src/GPU/pair_eam_fs_gpu.h:#define LMP_PAIR_EAM_FS_GPU_H
src/GPU/pair_eam_fs_gpu.h:class PairEAMFSGPU : public PairEAM {
src/GPU/pair_eam_fs_gpu.h:  PairEAMFSGPU(class LAMMPS *);
src/GPU/pair_eam_fs_gpu.h:  ~PairEAMFSGPU() override;
src/GPU/pair_eam_fs_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_eam_fs_gpu.h:  int gpu_mode;
src/GPU/pair_born_coul_wolf_gpu.h:PairStyle(born/coul/wolf/gpu,PairBornCoulWolfGPU);
src/GPU/pair_born_coul_wolf_gpu.h:#ifndef LMP_PAIR_BORN_COUL_WOLF_GPU_H
src/GPU/pair_born_coul_wolf_gpu.h:#define LMP_PAIR_BORN_COUL_WOLF_GPU_H
src/GPU/pair_born_coul_wolf_gpu.h:class PairBornCoulWolfGPU : public PairBornCoulWolf {
src/GPU/pair_born_coul_wolf_gpu.h:  PairBornCoulWolfGPU(LAMMPS *lmp);
src/GPU/pair_born_coul_wolf_gpu.h:  ~PairBornCoulWolfGPU() override;
src/GPU/pair_born_coul_wolf_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_born_coul_wolf_gpu.h:  int gpu_mode;
src/GPU/pair_amoeba_gpu.h:PairStyle(amoeba/gpu,PairAmoebaGPU);
src/GPU/pair_amoeba_gpu.h:#ifndef LMP_PAIR_AMOEBA_GPU_H
src/GPU/pair_amoeba_gpu.h:#define LMP_PAIR_AMOEBA_GPU_H
src/GPU/pair_amoeba_gpu.h:class PairAmoebaGPU : public PairAmoeba {
src/GPU/pair_amoeba_gpu.h:  PairAmoebaGPU(LAMMPS *lmp);
src/GPU/pair_amoeba_gpu.h:  ~PairAmoebaGPU() override;
src/GPU/pair_amoeba_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_amoeba_gpu.h:  int gpu_mode;
src/GPU/pair_amoeba_gpu.h:  bool gpu_hal_ready;
src/GPU/pair_amoeba_gpu.h:  bool gpu_repulsion_ready;
src/GPU/pair_amoeba_gpu.h:  bool gpu_dispersion_real_ready;
src/GPU/pair_amoeba_gpu.h:  bool gpu_multipole_real_ready;
src/GPU/pair_amoeba_gpu.h:  bool gpu_udirect2b_ready;
src/GPU/pair_amoeba_gpu.h:  bool gpu_umutual1_ready;
src/GPU/pair_amoeba_gpu.h:  bool gpu_fphi_uind_ready;
src/GPU/pair_amoeba_gpu.h:  bool gpu_umutual2b_ready;
src/GPU/pair_amoeba_gpu.h:  bool gpu_polar_real_ready;
src/GPU/pair_tersoff_gpu.cpp:#include "pair_tersoff_gpu.h"
src/GPU/pair_tersoff_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_tersoff_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_tersoff_gpu.cpp:int tersoff_gpu_init(const int ntypes, const int inum, const int nall, const int max_nbors,
src/GPU/pair_tersoff_gpu.cpp:                     const double cell_size, int &gpu_mode, FILE *screen, int *host_map,
src/GPU/pair_tersoff_gpu.cpp:void tersoff_gpu_clear();
src/GPU/pair_tersoff_gpu.cpp:int **tersoff_gpu_compute_n(const int ago, const int inum_full, const int nall, double **host_x,
src/GPU/pair_tersoff_gpu.cpp:void tersoff_gpu_compute(const int ago, const int nlocal, const int nall, const int nlist,
src/GPU/pair_tersoff_gpu.cpp:double tersoff_gpu_bytes();
src/GPU/pair_tersoff_gpu.cpp:PairTersoffGPU::PairTersoffGPU(LAMMPS *lmp) : PairTersoff(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_tersoff_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_tersoff_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_tersoff_gpu.cpp:PairTersoffGPU::~PairTersoffGPU()
src/GPU/pair_tersoff_gpu.cpp:  tersoff_gpu_clear();
src/GPU/pair_tersoff_gpu.cpp:void PairTersoffGPU::compute(int eflag, int vflag)
src/GPU/pair_tersoff_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_tersoff_gpu.cpp:        tersoff_gpu_compute_n(neighbor->ago, inum, nall, atom->x, atom->type, sublo, subhi,
src/GPU/pair_tersoff_gpu.cpp:    tersoff_gpu_compute(neighbor->ago, inum, nall, inum + list->gnum, atom->x, atom->type, ilist,
src/GPU/pair_tersoff_gpu.cpp:void PairTersoffGPU::allocate()
src/GPU/pair_tersoff_gpu.cpp:void PairTersoffGPU::init_style()
src/GPU/pair_tersoff_gpu.cpp:  if (atom->tag_enable == 0) error->all(FLERR, "Pair style tersoff/gpu requires atom IDs");
src/GPU/pair_tersoff_gpu.cpp:  int success = tersoff_gpu_init(atom->ntypes + 1, atom->nlocal, atom->nlocal + atom->nghost, mnf,
src/GPU/pair_tersoff_gpu.cpp:                                 cell_size, gpu_mode, screen, map, nelements, elem3param, nparams,
src/GPU/pair_tersoff_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_tersoff_gpu.cpp:  if (gpu_mode == GPU_FORCE)
src/GPU/pair_tersoff_gpu.cpp:      error->warning(FLERR, "Increasing communication cutoff to {:.8} for GPU pair style",
src/GPU/pair_tersoff_gpu.cpp:double PairTersoffGPU::init_one(int i, int j)
src/GPU/pair_yukawa_gpu.cpp:#include "pair_yukawa_gpu.h"
src/GPU/pair_yukawa_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_yukawa_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_yukawa_gpu.cpp:int yukawa_gpu_init(const int ntypes, double **cutsq, double kappa, double **host_a,
src/GPU/pair_yukawa_gpu.cpp:                    int &gpu_mode, FILE *screen);
src/GPU/pair_yukawa_gpu.cpp:void yukawa_gpu_clear();
src/GPU/pair_yukawa_gpu.cpp:int **yukawa_gpu_compute_n(const int ago, const int inum_full, const int nall, double **host_x,
src/GPU/pair_yukawa_gpu.cpp:void yukawa_gpu_compute(const int ago, const int inum_full, const int nall, double **host_x,
src/GPU/pair_yukawa_gpu.cpp:double yukawa_gpu_bytes();
src/GPU/pair_yukawa_gpu.cpp:PairYukawaGPU::PairYukawaGPU(LAMMPS *lmp) : PairYukawa(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_yukawa_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_yukawa_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_yukawa_gpu.cpp:PairYukawaGPU::~PairYukawaGPU()
src/GPU/pair_yukawa_gpu.cpp:  yukawa_gpu_clear();
src/GPU/pair_yukawa_gpu.cpp:void PairYukawaGPU::compute(int eflag, int vflag)
src/GPU/pair_yukawa_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_yukawa_gpu.cpp:        yukawa_gpu_compute_n(neighbor->ago, inum, nall, atom->x, atom->type, sublo, subhi,
src/GPU/pair_yukawa_gpu.cpp:    yukawa_gpu_compute(neighbor->ago, inum, nall, atom->x, atom->type, ilist, numneigh, firstneigh,
src/GPU/pair_yukawa_gpu.cpp:void PairYukawaGPU::init_style()
src/GPU/pair_yukawa_gpu.cpp:      yukawa_gpu_init(atom->ntypes + 1, cutsq, kappa, a, offset, force->special_lj, atom->nlocal,
src/GPU/pair_yukawa_gpu.cpp:                      atom->nlocal + atom->nghost, mnf, maxspecial, cell_size, gpu_mode, screen);
src/GPU/pair_yukawa_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_yukawa_gpu.cpp:  if (gpu_mode == GPU_FORCE) neighbor->add_request(this, NeighConst::REQ_FULL);
src/GPU/pair_yukawa_gpu.cpp:double PairYukawaGPU::memory_usage()
src/GPU/pair_yukawa_gpu.cpp:  return bytes + yukawa_gpu_bytes();
src/GPU/pair_yukawa_gpu.cpp:void PairYukawaGPU::cpu_compute(int start, int inum, int eflag, int /* vflag */, int *ilist,
src/GPU/pair_buck_gpu.h:PairStyle(buck/gpu,PairBuckGPU);
src/GPU/pair_buck_gpu.h:#ifndef LMP_PAIR_BUCK_GPU_H
src/GPU/pair_buck_gpu.h:#define LMP_PAIR_BUCK_GPU_H
src/GPU/pair_buck_gpu.h:class PairBuckGPU : public PairBuck {
src/GPU/pair_buck_gpu.h:  PairBuckGPU(LAMMPS *lmp);
src/GPU/pair_buck_gpu.h:  ~PairBuckGPU() override;
src/GPU/pair_buck_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_buck_gpu.h:  int gpu_mode;
src/GPU/pair_hippo_gpu.h:PairStyle(hippo/gpu,PairHippoGPU);
src/GPU/pair_hippo_gpu.h:#ifndef LMP_PAIR_HIPPO_GPU_H
src/GPU/pair_hippo_gpu.h:#define LMP_PAIR_HIPPO_GPU_H
src/GPU/pair_hippo_gpu.h:class PairHippoGPU : public PairAmoeba {
src/GPU/pair_hippo_gpu.h:  PairHippoGPU(LAMMPS *lmp);
src/GPU/pair_hippo_gpu.h:  ~PairHippoGPU() override;
src/GPU/pair_hippo_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_hippo_gpu.h:  int gpu_mode;
src/GPU/pair_hippo_gpu.h:  bool gpu_hal_ready;
src/GPU/pair_hippo_gpu.h:  bool gpu_repulsion_ready;
src/GPU/pair_hippo_gpu.h:  bool gpu_dispersion_real_ready;
src/GPU/pair_hippo_gpu.h:  bool gpu_multipole_real_ready;
src/GPU/pair_hippo_gpu.h:  bool gpu_udirect2b_ready;
src/GPU/pair_hippo_gpu.h:  bool gpu_umutual1_ready;
src/GPU/pair_hippo_gpu.h:  bool gpu_fphi_uind_ready;
src/GPU/pair_hippo_gpu.h:  bool gpu_umutual2b_ready;
src/GPU/pair_hippo_gpu.h:  bool gpu_polar_real_ready;
src/GPU/pair_beck_gpu.h:PairStyle(beck/gpu,PairBeckGPU);
src/GPU/pair_beck_gpu.h:#ifndef LMP_PAIR_BECK_GPU_H
src/GPU/pair_beck_gpu.h:#define LMP_PAIR_BECK_GPU_H
src/GPU/pair_beck_gpu.h:class PairBeckGPU : public PairBeck {
src/GPU/pair_beck_gpu.h:  PairBeckGPU(LAMMPS *lmp);
src/GPU/pair_beck_gpu.h:  ~PairBeckGPU() override;
src/GPU/pair_beck_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_beck_gpu.h:  int gpu_mode;
src/GPU/pair_sph_lj_gpu.h:PairStyle(sph/lj/gpu,PairSPHLJGPU);
src/GPU/pair_sph_lj_gpu.h:#ifndef LMP_PAIR_SPH_LJ_GPU_H
src/GPU/pair_sph_lj_gpu.h:#define LMP_PAIR_SPH_LJ_GPU_H
src/GPU/pair_sph_lj_gpu.h:class PairSPHLJGPU : public PairSPHLJ {
src/GPU/pair_sph_lj_gpu.h:  PairSPHLJGPU(LAMMPS *lmp);
src/GPU/pair_sph_lj_gpu.h:  ~PairSPHLJGPU() override;
src/GPU/pair_sph_lj_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_sph_lj_gpu.h:  int gpu_mode;
src/GPU/pair_lj_cut_coul_long_soft_gpu.h:PairStyle(lj/cut/coul/long/soft/gpu,PairLJCutCoulLongSoftGPU);
src/GPU/pair_lj_cut_coul_long_soft_gpu.h:#ifndef LMP_PAIR_LJ_CUT_COUL_LONG_SOFT_GPU_H
src/GPU/pair_lj_cut_coul_long_soft_gpu.h:#define LMP_PAIR_LJ_CUT_COUL_LONG_SOFT_GPU_H
src/GPU/pair_lj_cut_coul_long_soft_gpu.h:class PairLJCutCoulLongSoftGPU : public PairLJCutCoulLongSoft {
src/GPU/pair_lj_cut_coul_long_soft_gpu.h:  PairLJCutCoulLongSoftGPU(LAMMPS *lmp);
src/GPU/pair_lj_cut_coul_long_soft_gpu.h:  ~PairLJCutCoulLongSoftGPU() override;
src/GPU/pair_lj_cut_coul_long_soft_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_lj_cut_coul_long_soft_gpu.h:  int gpu_mode;
src/GPU/pair_coul_debye_gpu.h:PairStyle(coul/debye/gpu,PairCoulDebyeGPU);
src/GPU/pair_coul_debye_gpu.h:#ifndef LMP_PAIR_COUL_DEBYE_GPU_H
src/GPU/pair_coul_debye_gpu.h:#define LMP_PAIR_COUL_DEBYE_GPU_H
src/GPU/pair_coul_debye_gpu.h:class PairCoulDebyeGPU : public PairCoulDebye {
src/GPU/pair_coul_debye_gpu.h:  PairCoulDebyeGPU(LAMMPS *lmp);
src/GPU/pair_coul_debye_gpu.h:  ~PairCoulDebyeGPU() override;
src/GPU/pair_coul_debye_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_coul_debye_gpu.h:  int gpu_mode;
src/GPU/pair_lj_spica_gpu.h:PairStyle(lj/spica/gpu,PairLJSPICAGPU);
src/GPU/pair_lj_spica_gpu.h:PairStyle(lj/sdk/gpu,PairLJSPICAGPU);
src/GPU/pair_lj_spica_gpu.h:#ifndef LMP_PAIR_LJ_SPICA_GPU_H
src/GPU/pair_lj_spica_gpu.h:#define LMP_PAIR_LJ_SPICA_GPU_H
src/GPU/pair_lj_spica_gpu.h:class PairLJSPICAGPU : public PairLJSPICA {
src/GPU/pair_lj_spica_gpu.h:  PairLJSPICAGPU(LAMMPS *lmp);
src/GPU/pair_lj_spica_gpu.h:  ~PairLJSPICAGPU() override;
src/GPU/pair_lj_spica_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_lj_spica_gpu.h:  int gpu_mode;
src/GPU/amoeba_convolution_gpu.cpp:#include "amoeba_convolution_gpu.h"
src/GPU/amoeba_convolution_gpu.cpp:// External functions from GPU library
src/GPU/amoeba_convolution_gpu.cpp:AmoebaConvolutionGPU::AmoebaConvolutionGPU(LAMMPS *lmp, Pair *pair,
src/GPU/amoeba_convolution_gpu.cpp:FFT_SCALAR *AmoebaConvolutionGPU::pre_convolution_4d()
src/GPU/amoeba_convolution_gpu.cpp:void *AmoebaConvolutionGPU::post_convolution_4d()
src/GPU/pair_yukawa_colloid_gpu.cpp:#include "pair_yukawa_colloid_gpu.h"
src/GPU/pair_yukawa_colloid_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_yukawa_colloid_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_yukawa_colloid_gpu.cpp:int ykcolloid_gpu_init(const int ntypes, double **cutsq, double **host_a, double **host_offset,
src/GPU/pair_yukawa_colloid_gpu.cpp:                       const int maxspecial, const double cell_size, int &gpu_mode, FILE *screen,
src/GPU/pair_yukawa_colloid_gpu.cpp:void ykcolloid_gpu_clear();
src/GPU/pair_yukawa_colloid_gpu.cpp:int **ykcolloid_gpu_compute_n(const int ago, const int inum_full, const int nall, double **host_x,
src/GPU/pair_yukawa_colloid_gpu.cpp:void ykcolloid_gpu_compute(const int ago, const int inum_full, const int nall, double **host_x,
src/GPU/pair_yukawa_colloid_gpu.cpp:double ykcolloid_gpu_bytes();
src/GPU/pair_yukawa_colloid_gpu.cpp:PairYukawaColloidGPU::PairYukawaColloidGPU(LAMMPS *lmp) :
src/GPU/pair_yukawa_colloid_gpu.cpp:    PairYukawaColloid(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_yukawa_colloid_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_yukawa_colloid_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_yukawa_colloid_gpu.cpp:PairYukawaColloidGPU::~PairYukawaColloidGPU()
src/GPU/pair_yukawa_colloid_gpu.cpp:  ykcolloid_gpu_clear();
src/GPU/pair_yukawa_colloid_gpu.cpp:void PairYukawaColloidGPU::compute(int eflag, int vflag)
src/GPU/pair_yukawa_colloid_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_yukawa_colloid_gpu.cpp:    firstneigh = ykcolloid_gpu_compute_n(neighbor->ago, inum, nall, atom->x, atom->type, sublo,
src/GPU/pair_yukawa_colloid_gpu.cpp:    ykcolloid_gpu_compute(neighbor->ago, inum, nall, atom->x, atom->type, ilist, numneigh,
src/GPU/pair_yukawa_colloid_gpu.cpp:void PairYukawaColloidGPU::init_style()
src/GPU/pair_yukawa_colloid_gpu.cpp:  if (!atom->radius_flag) error->all(FLERR, "Pair style yukawa/colloid/gpu requires atom attribute radius");
src/GPU/pair_yukawa_colloid_gpu.cpp:  int success = ykcolloid_gpu_init(atom->ntypes + 1, cutsq, a, offset, force->special_lj,
src/GPU/pair_yukawa_colloid_gpu.cpp:                                   cell_size, gpu_mode, screen, kappa);
src/GPU/pair_yukawa_colloid_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_yukawa_colloid_gpu.cpp:  if (gpu_mode == GPU_FORCE) neighbor->add_request(this, NeighConst::REQ_FULL);
src/GPU/pair_yukawa_colloid_gpu.cpp:double PairYukawaColloidGPU::memory_usage()
src/GPU/pair_yukawa_colloid_gpu.cpp:  return bytes + ykcolloid_gpu_bytes();
src/GPU/pair_yukawa_colloid_gpu.cpp:void PairYukawaColloidGPU::cpu_compute(int start, int inum, int eflag, int /* vflag */, int *ilist,
src/GPU/pair_mdpd_gpu.cpp:#include "pair_mdpd_gpu.h"
src/GPU/pair_mdpd_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_mdpd_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_mdpd_gpu.cpp:int mdpd_gpu_init(const int ntypes, double **cutsq, double **host_A_att, double **host_B_rep,
src/GPU/pair_mdpd_gpu.cpp:                  const int maxspecial, const double cell_size, int &gpu_mode, FILE *screen);
src/GPU/pair_mdpd_gpu.cpp:void mdpd_gpu_clear();
src/GPU/pair_mdpd_gpu.cpp:int **mdpd_gpu_compute_n(const int ago, const int inum_full, const int nall, double **host_x,
src/GPU/pair_mdpd_gpu.cpp:void mdpd_gpu_compute(const int ago, const int inum_full, const int nall, double **host_x,
src/GPU/pair_mdpd_gpu.cpp:void mdpd_gpu_get_extra_data(double *host_rho);
src/GPU/pair_mdpd_gpu.cpp:double mdpd_gpu_bytes();
src/GPU/pair_mdpd_gpu.cpp:PairMDPDGPU::PairMDPDGPU(LAMMPS *lmp) : PairMDPD(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_mdpd_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_mdpd_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_mdpd_gpu.cpp:PairMDPDGPU::~PairMDPDGPU()
src/GPU/pair_mdpd_gpu.cpp:  mdpd_gpu_clear();
src/GPU/pair_mdpd_gpu.cpp:void PairMDPDGPU::compute(int eflag, int vflag)
src/GPU/pair_mdpd_gpu.cpp:  mdpd_gpu_get_extra_data(rho);
src/GPU/pair_mdpd_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_mdpd_gpu.cpp:    firstneigh = mdpd_gpu_compute_n(
src/GPU/pair_mdpd_gpu.cpp:    mdpd_gpu_compute(neighbor->ago, inum, nall, atom->x, atom->type, ilist, numneigh, firstneigh,
src/GPU/pair_mdpd_gpu.cpp:void PairMDPDGPU::init_style()
src/GPU/pair_mdpd_gpu.cpp:      mdpd_gpu_init(atom->ntypes + 1, cutsq, A_att, B_rep, gamma, sigma,
src/GPU/pair_mdpd_gpu.cpp:                    mnf, maxspecial, cell_size, gpu_mode, screen);
src/GPU/pair_mdpd_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_mdpd_gpu.cpp:  if (gpu_mode == GPU_FORCE) neighbor->add_request(this, NeighConst::REQ_FULL);
src/GPU/pair_mdpd_gpu.cpp:double PairMDPDGPU::memory_usage()
src/GPU/pair_mdpd_gpu.cpp:  return bytes + mdpd_gpu_bytes();
src/GPU/pair_lj96_cut_gpu.cpp:#include "pair_lj96_cut_gpu.h"
src/GPU/pair_lj96_cut_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_lj96_cut_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_lj96_cut_gpu.cpp:int lj96_gpu_init(const int ntypes, double **cutsq, double **host_lj1, double **host_lj2,
src/GPU/pair_lj96_cut_gpu.cpp:                  const double cell_size, int &gpu_mode, FILE *screen);
src/GPU/pair_lj96_cut_gpu.cpp:void lj96_gpu_clear();
src/GPU/pair_lj96_cut_gpu.cpp:int **lj96_gpu_compute_n(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_lj96_cut_gpu.cpp:void lj96_gpu_compute(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_lj96_cut_gpu.cpp:double lj96_gpu_bytes();
src/GPU/pair_lj96_cut_gpu.cpp:PairLJ96CutGPU::PairLJ96CutGPU(LAMMPS *lmp) : PairLJ96Cut(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_lj96_cut_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_lj96_cut_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_lj96_cut_gpu.cpp:PairLJ96CutGPU::~PairLJ96CutGPU()
src/GPU/pair_lj96_cut_gpu.cpp:  lj96_gpu_clear();
src/GPU/pair_lj96_cut_gpu.cpp:void PairLJ96CutGPU::compute(int eflag, int vflag)
src/GPU/pair_lj96_cut_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_lj96_cut_gpu.cpp:        lj96_gpu_compute_n(neighbor->ago, inum, nall, atom->x, atom->type, sublo, subhi, atom->tag,
src/GPU/pair_lj96_cut_gpu.cpp:    lj96_gpu_compute(neighbor->ago, inum, nall, atom->x, atom->type, ilist, numneigh, firstneigh,
src/GPU/pair_lj96_cut_gpu.cpp:void PairLJ96CutGPU::init_style()
src/GPU/pair_lj96_cut_gpu.cpp:  int success = lj96_gpu_init(atom->ntypes + 1, cutsq, lj1, lj2, lj3, lj4, offset,
src/GPU/pair_lj96_cut_gpu.cpp:                              maxspecial, cell_size, gpu_mode, screen);
src/GPU/pair_lj96_cut_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_lj96_cut_gpu.cpp:  if (gpu_mode == GPU_FORCE) neighbor->add_request(this, NeighConst::REQ_FULL);
src/GPU/pair_lj96_cut_gpu.cpp:double PairLJ96CutGPU::memory_usage()
src/GPU/pair_lj96_cut_gpu.cpp:  return bytes + lj96_gpu_bytes();
src/GPU/pair_lj96_cut_gpu.cpp:void PairLJ96CutGPU::cpu_compute(int start, int inum, int eflag, int /* vflag */, int *ilist,
src/GPU/pair_morse_gpu.cpp:#include "pair_morse_gpu.h"
src/GPU/pair_morse_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_morse_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_morse_gpu.cpp:int mor_gpu_init(const int ntypes, double **cutsq, double **host_morse1, double **host_r0,
src/GPU/pair_morse_gpu.cpp:                 const double cell_size, int &gpu_mode, FILE *screen);
src/GPU/pair_morse_gpu.cpp:void mor_gpu_clear();
src/GPU/pair_morse_gpu.cpp:int **mor_gpu_compute_n(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_morse_gpu.cpp:void mor_gpu_compute(const int ago, const int inum, const int nall, double **host_x, int *host_type,
src/GPU/pair_morse_gpu.cpp:double mor_gpu_bytes();
src/GPU/pair_morse_gpu.cpp:PairMorseGPU::PairMorseGPU(LAMMPS *lmp) : PairMorse(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_morse_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_morse_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_morse_gpu.cpp:PairMorseGPU::~PairMorseGPU()
src/GPU/pair_morse_gpu.cpp:  mor_gpu_clear();
src/GPU/pair_morse_gpu.cpp:void PairMorseGPU::compute(int eflag, int vflag)
src/GPU/pair_morse_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_morse_gpu.cpp:        mor_gpu_compute_n(neighbor->ago, inum, nall, atom->x, atom->type, sublo, subhi, atom->tag,
src/GPU/pair_morse_gpu.cpp:    mor_gpu_compute(neighbor->ago, inum, nall, atom->x, atom->type, ilist, numneigh, firstneigh,
src/GPU/pair_morse_gpu.cpp:void PairMorseGPU::init_style()
src/GPU/pair_morse_gpu.cpp:  int success = mor_gpu_init(atom->ntypes + 1, cutsq, morse1, r0, alpha, d0, offset,
src/GPU/pair_morse_gpu.cpp:                             maxspecial, cell_size, gpu_mode, screen);
src/GPU/pair_morse_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_morse_gpu.cpp:  if (gpu_mode == GPU_FORCE) neighbor->add_request(this, NeighConst::REQ_FULL);
src/GPU/pair_morse_gpu.cpp:double PairMorseGPU::memory_usage()
src/GPU/pair_morse_gpu.cpp:  return bytes + mor_gpu_bytes();
src/GPU/pair_morse_gpu.cpp:void PairMorseGPU::cpu_compute(int start, int inum, int eflag, int /* vflag */, int *ilist,
src/GPU/fix_nvt_gpu.cpp:#include "fix_nvt_gpu.h"
src/GPU/fix_nvt_gpu.cpp:FixNVTGPU::FixNVTGPU(LAMMPS *lmp, int narg, char **arg) :
src/GPU/fix_nvt_gpu.cpp:  FixNHGPU(lmp, narg, arg)
src/GPU/fix_nvt_gpu.cpp:    error->all(FLERR,"Temperature control must be used with fix nvt/gpu");
src/GPU/fix_nvt_gpu.cpp:    error->all(FLERR,"Pressure control can not be used with fix nvt/gpu");
src/GPU/fix_npt_gpu.h:FixStyle(npt/gpu,FixNPTGPU);
src/GPU/fix_npt_gpu.h:#ifndef LMP_FIX_NPT_GPU_H
src/GPU/fix_npt_gpu.h:#define LMP_FIX_NPT_GPU_H
src/GPU/fix_npt_gpu.h:#include "fix_nh_gpu.h"
src/GPU/fix_npt_gpu.h:class FixNPTGPU : public FixNHGPU {
src/GPU/fix_npt_gpu.h:  FixNPTGPU(class LAMMPS *, int, char **);
src/GPU/pair_sw_gpu.h:PairStyle(sw/gpu,PairSWGPU);
src/GPU/pair_sw_gpu.h:#ifndef LMP_PAIR_SW_GPU_H
src/GPU/pair_sw_gpu.h:#define LMP_PAIR_SW_GPU_H
src/GPU/pair_sw_gpu.h:class PairSWGPU : public PairSW {
src/GPU/pair_sw_gpu.h:  PairSWGPU(class LAMMPS *);
src/GPU/pair_sw_gpu.h:  ~PairSWGPU() override;
src/GPU/pair_sw_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_sw_gpu.h:  int gpu_mode;
src/GPU/pair_tersoff_gpu.h:PairStyle(tersoff/gpu,PairTersoffGPU);
src/GPU/pair_tersoff_gpu.h:#ifndef LMP_PAIR_TERSOFF_GPU_H
src/GPU/pair_tersoff_gpu.h:#define LMP_PAIR_TERSOFF_GPU_H
src/GPU/pair_tersoff_gpu.h:class PairTersoffGPU : public PairTersoff {
src/GPU/pair_tersoff_gpu.h:  PairTersoffGPU(class LAMMPS *);
src/GPU/pair_tersoff_gpu.h:  ~PairTersoffGPU() override;
src/GPU/pair_tersoff_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_tersoff_gpu.h:  int gpu_mode;
src/GPU/pair_tersoff_gpu.h:  int *gpulist;
src/GPU/pair_eam_gpu.h:PairStyle(eam/gpu,PairEAMGPU);
src/GPU/pair_eam_gpu.h:#ifndef LMP_PAIR_EAM_GPU_H
src/GPU/pair_eam_gpu.h:#define LMP_PAIR_EAM_GPU_H
src/GPU/pair_eam_gpu.h:class PairEAMGPU : public PairEAM {
src/GPU/pair_eam_gpu.h:  PairEAMGPU(class LAMMPS *);
src/GPU/pair_eam_gpu.h:  ~PairEAMGPU() override;
src/GPU/pair_eam_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_eam_gpu.h:  int gpu_mode;
src/GPU/pair_dpd_gpu.h:PairStyle(dpd/gpu,PairDPDGPU);
src/GPU/pair_dpd_gpu.h:#ifndef LMP_PAIR_DPD_GPU_H
src/GPU/pair_dpd_gpu.h:#define LMP_PAIR_DPD_GPU_H
src/GPU/pair_dpd_gpu.h:class PairDPDGPU : public PairDPD {
src/GPU/pair_dpd_gpu.h:  PairDPDGPU(LAMMPS *lmp);
src/GPU/pair_dpd_gpu.h:  ~PairDPDGPU() override;
src/GPU/pair_dpd_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_dpd_gpu.h:  int gpu_mode;
src/GPU/fix_nve_gpu.h:FixStyle(nve/gpu,FixNVEGPU);
src/GPU/fix_nve_gpu.h:#ifndef LMP_FIX_NVE_GPU_H
src/GPU/fix_nve_gpu.h:#define LMP_FIX_NVE_GPU_H
src/GPU/fix_nve_gpu.h:class FixNVEGPU : public FixNVE {
src/GPU/fix_nve_gpu.h:  FixNVEGPU(class LAMMPS *, int, char **);
src/GPU/fix_nve_gpu.h:  ~FixNVEGPU() override;
src/GPU/pair_zbl_gpu.h:PairStyle(zbl/gpu,PairZBLGPU);
src/GPU/pair_zbl_gpu.h:#ifndef LMP_PAIR_ZBL_GPU_H
src/GPU/pair_zbl_gpu.h:#define LMP_PAIR_ZBL_GPU_H
src/GPU/pair_zbl_gpu.h:class PairZBLGPU : public PairZBL {
src/GPU/pair_zbl_gpu.h:  PairZBLGPU(LAMMPS *lmp);
src/GPU/pair_zbl_gpu.h:  ~PairZBLGPU() override;
src/GPU/pair_zbl_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_zbl_gpu.h:  int gpu_mode;
src/GPU/pair_born_coul_long_cs_gpu.h:PairStyle(born/coul/long/cs/gpu,PairBornCoulLongCSGPU);
src/GPU/pair_born_coul_long_cs_gpu.h:#ifndef LMP_PAIR_BORN_COUL_LONG_CS_GPU_H
src/GPU/pair_born_coul_long_cs_gpu.h:#define LMP_PAIR_BORN_COUL_LONG_CS_GPU_H
src/GPU/pair_born_coul_long_cs_gpu.h:class PairBornCoulLongCSGPU : public PairBornCoulLongCS {
src/GPU/pair_born_coul_long_cs_gpu.h:  PairBornCoulLongCSGPU(LAMMPS *lmp);
src/GPU/pair_born_coul_long_cs_gpu.h:  ~PairBornCoulLongCSGPU() override;
src/GPU/pair_born_coul_long_cs_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_born_coul_long_cs_gpu.h:  int gpu_mode;
src/GPU/pair_lj_cut_coul_dsf_gpu.h:PairStyle(lj/cut/coul/dsf/gpu,PairLJCutCoulDSFGPU);
src/GPU/pair_lj_cut_coul_dsf_gpu.h:#ifndef LMP_PAIR_LJ_CUT_COUL_DSF_GPU_H
src/GPU/pair_lj_cut_coul_dsf_gpu.h:#define LMP_PAIR_LJ_CUT_COUL_DSF_GPU_H
src/GPU/pair_lj_cut_coul_dsf_gpu.h:class PairLJCutCoulDSFGPU : public PairLJCutCoulDSF {
src/GPU/pair_lj_cut_coul_dsf_gpu.h:  PairLJCutCoulDSFGPU(LAMMPS *lmp);
src/GPU/pair_lj_cut_coul_dsf_gpu.h:  ~PairLJCutCoulDSFGPU() override;
src/GPU/pair_lj_cut_coul_dsf_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_lj_cut_coul_dsf_gpu.h:  int gpu_mode;
src/GPU/amoeba_convolution_gpu.h:#ifndef LMP_AMOEBA_CONVOLUTION_GPU_H
src/GPU/amoeba_convolution_gpu.h:#define LMP_AMOEBA_CONVOLUTION_GPU_H
src/GPU/amoeba_convolution_gpu.h:class AmoebaConvolutionGPU : public AmoebaConvolution {
src/GPU/amoeba_convolution_gpu.h:  AmoebaConvolutionGPU(class LAMMPS *, class Pair *, int, int, int, int, int);
src/GPU/pair_born_coul_wolf_cs_gpu.cpp:#include "pair_born_coul_wolf_cs_gpu.h"
src/GPU/pair_born_coul_wolf_cs_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_born_coul_wolf_cs_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_born_coul_wolf_cs_gpu.cpp:int borncwcs_gpu_init(const int ntypes, double **cutsq, double **host_rhoinv, double **host_born1,
src/GPU/pair_born_coul_wolf_cs_gpu.cpp:                      const double cell_size, int &gpu_mode, FILE *screen, double **host_cut_ljsq,
src/GPU/pair_born_coul_wolf_cs_gpu.cpp:void borncwcs_gpu_clear();
src/GPU/pair_born_coul_wolf_cs_gpu.cpp:int **borncwcs_gpu_compute_n(const int ago, const int inum_full, const int nall, double **host_x,
src/GPU/pair_born_coul_wolf_cs_gpu.cpp:void borncwcs_gpu_compute(const int ago, const int inum_full, const int nall, double **host_x,
src/GPU/pair_born_coul_wolf_cs_gpu.cpp:double borncwcs_gpu_bytes();
src/GPU/pair_born_coul_wolf_cs_gpu.cpp:PairBornCoulWolfCSGPU::PairBornCoulWolfCSGPU(LAMMPS *lmp) :
src/GPU/pair_born_coul_wolf_cs_gpu.cpp:    PairBornCoulWolfCS(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_born_coul_wolf_cs_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_born_coul_wolf_cs_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_born_coul_wolf_cs_gpu.cpp:PairBornCoulWolfCSGPU::~PairBornCoulWolfCSGPU()
src/GPU/pair_born_coul_wolf_cs_gpu.cpp:  borncwcs_gpu_clear();
src/GPU/pair_born_coul_wolf_cs_gpu.cpp:void PairBornCoulWolfCSGPU::compute(int eflag, int vflag)
src/GPU/pair_born_coul_wolf_cs_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_born_coul_wolf_cs_gpu.cpp:    firstneigh = borncwcs_gpu_compute_n(
src/GPU/pair_born_coul_wolf_cs_gpu.cpp:    borncwcs_gpu_compute(neighbor->ago, inum, nall, atom->x, atom->type, ilist, numneigh,
src/GPU/pair_born_coul_wolf_cs_gpu.cpp:void PairBornCoulWolfCSGPU::init_style()
src/GPU/pair_born_coul_wolf_cs_gpu.cpp:    error->all(FLERR, "Pair style born/coul/wolf/cs/gpu requires atom attribute q");
src/GPU/pair_born_coul_wolf_cs_gpu.cpp:      borncwcs_gpu_init(atom->ntypes + 1, cutsq, rhoinv, born1, born2, born3, a, c, d, sigma,
src/GPU/pair_born_coul_wolf_cs_gpu.cpp:                        maxspecial, cell_size, gpu_mode, screen, cut_ljsq, cut_coulsq,
src/GPU/pair_born_coul_wolf_cs_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_born_coul_wolf_cs_gpu.cpp:  if (gpu_mode == GPU_FORCE) neighbor->add_request(this, NeighConst::REQ_FULL);
src/GPU/pair_born_coul_wolf_cs_gpu.cpp:double PairBornCoulWolfCSGPU::memory_usage()
src/GPU/pair_born_coul_wolf_cs_gpu.cpp:  return bytes + borncwcs_gpu_bytes();
src/GPU/pair_born_coul_wolf_cs_gpu.cpp:void PairBornCoulWolfCSGPU::cpu_compute(int start, int inum, int eflag, int /* vflag */, int *ilist,
src/GPU/README:This package implements GPU optimizations of various LAMMPS styles.
src/GPU/README:Section 3.7 of the manual gives details of what hardware and Cuda
src/GPU/README:and use this package.  The KOKKOS package also has GPU-enabled styles.
src/GPU/README:This package uses an external library provided in lib/gpu which must
src/GPU/README:be compiled before making LAMMPS.  See the lib/gpu/README file and the
src/GPU/README:The GPU package and its associated library was created by Mike Brown
src/GPU/pair_lj_cut_coul_debye_gpu.cpp:#include "pair_lj_cut_coul_debye_gpu.h"
src/GPU/pair_lj_cut_coul_debye_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_lj_cut_coul_debye_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_lj_cut_coul_debye_gpu.cpp:int ljcd_gpu_init(const int ntypes, double **cutsq, double **host_lj1, double **host_lj2,
src/GPU/pair_lj_cut_coul_debye_gpu.cpp:                  const double cell_size, int &gpu_mode, FILE *screen, double **host_cut_ljsq,
src/GPU/pair_lj_cut_coul_debye_gpu.cpp:void ljcd_gpu_clear();
src/GPU/pair_lj_cut_coul_debye_gpu.cpp:int **ljcd_gpu_compute_n(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_lj_cut_coul_debye_gpu.cpp:void ljcd_gpu_compute(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_lj_cut_coul_debye_gpu.cpp:double ljcd_gpu_bytes();
src/GPU/pair_lj_cut_coul_debye_gpu.cpp:PairLJCutCoulDebyeGPU::PairLJCutCoulDebyeGPU(LAMMPS *lmp) :
src/GPU/pair_lj_cut_coul_debye_gpu.cpp:    PairLJCutCoulDebye(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_lj_cut_coul_debye_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_lj_cut_coul_debye_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_lj_cut_coul_debye_gpu.cpp:PairLJCutCoulDebyeGPU::~PairLJCutCoulDebyeGPU()
src/GPU/pair_lj_cut_coul_debye_gpu.cpp:  ljcd_gpu_clear();
src/GPU/pair_lj_cut_coul_debye_gpu.cpp:void PairLJCutCoulDebyeGPU::compute(int eflag, int vflag)
src/GPU/pair_lj_cut_coul_debye_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_lj_cut_coul_debye_gpu.cpp:    firstneigh = ljcd_gpu_compute_n(neighbor->ago, inum, nall, atom->x, atom->type, sublo, subhi,
src/GPU/pair_lj_cut_coul_debye_gpu.cpp:    ljcd_gpu_compute(neighbor->ago, inum, nall, atom->x, atom->type, ilist, numneigh, firstneigh,
src/GPU/pair_lj_cut_coul_debye_gpu.cpp:void PairLJCutCoulDebyeGPU::init_style()
src/GPU/pair_lj_cut_coul_debye_gpu.cpp:    error->all(FLERR, "Pair style lj/cut/coul/debye/gpu requires atom attribute q");
src/GPU/pair_lj_cut_coul_debye_gpu.cpp:      ljcd_gpu_init(atom->ntypes + 1, cutsq, lj1, lj2, lj3, lj4, offset, force->special_lj,
src/GPU/pair_lj_cut_coul_debye_gpu.cpp:                    atom->nlocal, atom->nlocal + atom->nghost, mnf, maxspecial, cell_size, gpu_mode,
src/GPU/pair_lj_cut_coul_debye_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_lj_cut_coul_debye_gpu.cpp:  if (gpu_mode == GPU_FORCE) neighbor->add_request(this, NeighConst::REQ_FULL);
src/GPU/pair_lj_cut_coul_debye_gpu.cpp:double PairLJCutCoulDebyeGPU::memory_usage()
src/GPU/pair_lj_cut_coul_debye_gpu.cpp:  return bytes + ljcd_gpu_bytes();
src/GPU/pair_lj_cut_coul_debye_gpu.cpp:void PairLJCutCoulDebyeGPU::cpu_compute(int start, int inum, int eflag, int /* vflag */, int *ilist,
src/GPU/pair_lj96_cut_gpu.h:PairStyle(lj96/cut/gpu,PairLJ96CutGPU);
src/GPU/pair_lj96_cut_gpu.h:#ifndef LMP_PAIR_LJ_96_GPU_H
src/GPU/pair_lj96_cut_gpu.h:#define LMP_PAIR_LJ_96_GPU_H
src/GPU/pair_lj96_cut_gpu.h:class PairLJ96CutGPU : public PairLJ96Cut {
src/GPU/pair_lj96_cut_gpu.h:  PairLJ96CutGPU(LAMMPS *lmp);
src/GPU/pair_lj96_cut_gpu.h:  ~PairLJ96CutGPU() override;
src/GPU/pair_lj96_cut_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_lj96_cut_gpu.h:  int gpu_mode;
src/GPU/pair_lj_class2_coul_long_gpu.cpp:#include "pair_lj_class2_coul_long_gpu.h"
src/GPU/pair_lj_class2_coul_long_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_lj_class2_coul_long_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_lj_class2_coul_long_gpu.cpp:int c2cl_gpu_init(const int ntypes, double **cutsq, double **host_lj1, double **host_lj2,
src/GPU/pair_lj_class2_coul_long_gpu.cpp:                  const double cell_size, int &gpu_mode, FILE *screen, double **host_cut_ljsq,
src/GPU/pair_lj_class2_coul_long_gpu.cpp:void c2cl_gpu_clear();
src/GPU/pair_lj_class2_coul_long_gpu.cpp:int **c2cl_gpu_compute_n(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_lj_class2_coul_long_gpu.cpp:void c2cl_gpu_compute(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_lj_class2_coul_long_gpu.cpp:double c2cl_gpu_bytes();
src/GPU/pair_lj_class2_coul_long_gpu.cpp:PairLJClass2CoulLongGPU::PairLJClass2CoulLongGPU(LAMMPS *lmp) :
src/GPU/pair_lj_class2_coul_long_gpu.cpp:    PairLJClass2CoulLong(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_lj_class2_coul_long_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_lj_class2_coul_long_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_lj_class2_coul_long_gpu.cpp:PairLJClass2CoulLongGPU::~PairLJClass2CoulLongGPU()
src/GPU/pair_lj_class2_coul_long_gpu.cpp:  c2cl_gpu_clear();
src/GPU/pair_lj_class2_coul_long_gpu.cpp:void PairLJClass2CoulLongGPU::compute(int eflag, int vflag)
src/GPU/pair_lj_class2_coul_long_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_lj_class2_coul_long_gpu.cpp:    firstneigh = c2cl_gpu_compute_n(neighbor->ago, inum, nall, atom->x, atom->type, sublo, subhi,
src/GPU/pair_lj_class2_coul_long_gpu.cpp:    c2cl_gpu_compute(neighbor->ago, inum, nall, atom->x, atom->type, ilist, numneigh, firstneigh,
src/GPU/pair_lj_class2_coul_long_gpu.cpp:void PairLJClass2CoulLongGPU::init_style()
src/GPU/pair_lj_class2_coul_long_gpu.cpp:    error->all(FLERR, "Pair style lj/class2/coul/long/gpu requires atom attribute q");
src/GPU/pair_lj_class2_coul_long_gpu.cpp:      c2cl_gpu_init(atom->ntypes + 1, cutsq, lj1, lj2, lj3, lj4, offset, force->special_lj,
src/GPU/pair_lj_class2_coul_long_gpu.cpp:                    atom->nlocal, atom->nlocal + atom->nghost, mnf, maxspecial, cell_size, gpu_mode,
src/GPU/pair_lj_class2_coul_long_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_lj_class2_coul_long_gpu.cpp:  if (gpu_mode == GPU_FORCE) neighbor->add_request(this, NeighConst::REQ_FULL);
src/GPU/pair_lj_class2_coul_long_gpu.cpp:double PairLJClass2CoulLongGPU::memory_usage()
src/GPU/pair_lj_class2_coul_long_gpu.cpp:  return bytes + c2cl_gpu_bytes();
src/GPU/pair_lj_class2_coul_long_gpu.cpp:void PairLJClass2CoulLongGPU::cpu_compute(int start, int inum, int eflag, int /* vflag */,
src/GPU/pair_gauss_gpu.cpp:#include "pair_gauss_gpu.h"
src/GPU/pair_gauss_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_gauss_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_gauss_gpu.cpp:int gauss_gpu_init(const int ntypes, double **cutsq, double **host_a, double **b, double **offset,
src/GPU/pair_gauss_gpu.cpp:                   const int maxspecial, const double cell_size, int &gpu_mode, FILE *screen);
src/GPU/pair_gauss_gpu.cpp:void gauss_gpu_reinit(const int ntypes, double **cutsq, double **host_a, double **b,
src/GPU/pair_gauss_gpu.cpp:void gauss_gpu_clear();
src/GPU/pair_gauss_gpu.cpp:int **gauss_gpu_compute_n(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_gauss_gpu.cpp:void gauss_gpu_compute(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_gauss_gpu.cpp:double gauss_gpu_bytes();
src/GPU/pair_gauss_gpu.cpp:PairGaussGPU::PairGaussGPU(LAMMPS *lmp) : PairGauss(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_gauss_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_gauss_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_gauss_gpu.cpp:PairGaussGPU::~PairGaussGPU()
src/GPU/pair_gauss_gpu.cpp:  gauss_gpu_clear();
src/GPU/pair_gauss_gpu.cpp:void PairGaussGPU::compute(int eflag, int vflag)
src/GPU/pair_gauss_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_gauss_gpu.cpp:        gauss_gpu_compute_n(neighbor->ago, inum, nall, atom->x, atom->type, sublo, subhi, atom->tag,
src/GPU/pair_gauss_gpu.cpp:    gauss_gpu_compute(neighbor->ago, inum, nall, atom->x, atom->type, ilist, numneigh, firstneigh,
src/GPU/pair_gauss_gpu.cpp:void PairGaussGPU::init_style()
src/GPU/pair_gauss_gpu.cpp:      gauss_gpu_init(atom->ntypes + 1, cutsq, a, b, offset, force->special_lj, atom->nlocal,
src/GPU/pair_gauss_gpu.cpp:                     atom->nlocal + atom->nghost, mnf, maxspecial, cell_size, gpu_mode, screen);
src/GPU/pair_gauss_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_gauss_gpu.cpp:  if (gpu_mode == GPU_FORCE) neighbor->add_request(this, NeighConst::REQ_FULL);
src/GPU/pair_gauss_gpu.cpp:void PairGaussGPU::reinit()
src/GPU/pair_gauss_gpu.cpp:  gauss_gpu_reinit(atom->ntypes + 1, cutsq, a, b, offset);
src/GPU/pair_gauss_gpu.cpp:double PairGaussGPU::memory_usage()
src/GPU/pair_gauss_gpu.cpp:  return bytes + gauss_gpu_bytes();
src/GPU/pair_gauss_gpu.cpp:void PairGaussGPU::cpu_compute(int start, int inum, int eflag, int /* vflag */, int *ilist,
src/GPU/pair_lj_spica_coul_long_gpu.h:PairStyle(lj/spica/coul/long/gpu,PairLJSPICACoulLongGPU);
src/GPU/pair_lj_spica_coul_long_gpu.h:PairStyle(lj/sdk/coul/long/gpu,PairLJSPICACoulLongGPU);
src/GPU/pair_lj_spica_coul_long_gpu.h:#ifndef LMP_PAIR_LJ_SPICA_COUL_LONG_GPU_H
src/GPU/pair_lj_spica_coul_long_gpu.h:#define LMP_PAIR_LJ_SPICA_COUL_LONG_GPU_H
src/GPU/pair_lj_spica_coul_long_gpu.h:class PairLJSPICACoulLongGPU : public PairLJSPICACoulLong {
src/GPU/pair_lj_spica_coul_long_gpu.h:  PairLJSPICACoulLongGPU(LAMMPS *lmp);
src/GPU/pair_lj_spica_coul_long_gpu.h:  ~PairLJSPICACoulLongGPU() override;
src/GPU/pair_lj_spica_coul_long_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_lj_spica_coul_long_gpu.h:  int gpu_mode;
src/GPU/pair_ufm_gpu.h:PairStyle(ufm/gpu,PairUFMGPU);
src/GPU/pair_ufm_gpu.h:#ifndef LMP_PAIR_UFM_GPU_H
src/GPU/pair_ufm_gpu.h:#define LMP_PAIR_UFM_GPU_H
src/GPU/pair_ufm_gpu.h:class PairUFMGPU : public PairUFM {
src/GPU/pair_ufm_gpu.h:  PairUFMGPU(LAMMPS *lmp);
src/GPU/pair_ufm_gpu.h:  ~PairUFMGPU() override;
src/GPU/pair_ufm_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_ufm_gpu.h:  int gpu_mode;
src/GPU/pair_lj_cut_coul_cut_gpu.cpp:#include "pair_lj_cut_coul_cut_gpu.h"
src/GPU/pair_lj_cut_coul_cut_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_lj_cut_coul_cut_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_lj_cut_coul_cut_gpu.cpp:int ljc_gpu_init(const int ntypes, double **cutsq, double **host_lj1, double **host_lj2,
src/GPU/pair_lj_cut_coul_cut_gpu.cpp:                 const double cell_size, int &gpu_mode, FILE *screen, double **host_cut_ljsq,
src/GPU/pair_lj_cut_coul_cut_gpu.cpp:void ljc_gpu_clear();
src/GPU/pair_lj_cut_coul_cut_gpu.cpp:int **ljc_gpu_compute_n(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_lj_cut_coul_cut_gpu.cpp:void ljc_gpu_compute(const int ago, const int inum, const int nall, double **host_x, int *host_type,
src/GPU/pair_lj_cut_coul_cut_gpu.cpp:double ljc_gpu_bytes();
src/GPU/pair_lj_cut_coul_cut_gpu.cpp:PairLJCutCoulCutGPU::PairLJCutCoulCutGPU(LAMMPS *lmp) : PairLJCutCoulCut(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_lj_cut_coul_cut_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_lj_cut_coul_cut_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_lj_cut_coul_cut_gpu.cpp:PairLJCutCoulCutGPU::~PairLJCutCoulCutGPU()
src/GPU/pair_lj_cut_coul_cut_gpu.cpp:  ljc_gpu_clear();
src/GPU/pair_lj_cut_coul_cut_gpu.cpp:void PairLJCutCoulCutGPU::compute(int eflag, int vflag)
src/GPU/pair_lj_cut_coul_cut_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_lj_cut_coul_cut_gpu.cpp:    firstneigh = ljc_gpu_compute_n(neighbor->ago, inum, nall, atom->x, atom->type, sublo, subhi,
src/GPU/pair_lj_cut_coul_cut_gpu.cpp:    ljc_gpu_compute(neighbor->ago, inum, nall, atom->x, atom->type, ilist, numneigh, firstneigh,
src/GPU/pair_lj_cut_coul_cut_gpu.cpp:void PairLJCutCoulCutGPU::init_style()
src/GPU/pair_lj_cut_coul_cut_gpu.cpp:  if (!atom->q_flag) error->all(FLERR, "Pair style lj/cut/coul/cut/gpu requires atom attribute q");
src/GPU/pair_lj_cut_coul_cut_gpu.cpp:      ljc_gpu_init(atom->ntypes + 1, cutsq, lj1, lj2, lj3, lj4, offset, force->special_lj,
src/GPU/pair_lj_cut_coul_cut_gpu.cpp:                   atom->nlocal, atom->nlocal + atom->nghost, mnf, maxspecial, cell_size, gpu_mode,
src/GPU/pair_lj_cut_coul_cut_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_lj_cut_coul_cut_gpu.cpp:  if (gpu_mode == GPU_FORCE) neighbor->add_request(this, NeighConst::REQ_FULL);
src/GPU/pair_lj_cut_coul_cut_gpu.cpp:double PairLJCutCoulCutGPU::memory_usage()
src/GPU/pair_lj_cut_coul_cut_gpu.cpp:  return bytes + ljc_gpu_bytes();
src/GPU/pair_lj_cut_coul_cut_gpu.cpp:void PairLJCutCoulCutGPU::cpu_compute(int start, int inum, int eflag, int /* vflag */, int *ilist,
src/GPU/pair_coul_cut_gpu.cpp:#include "pair_coul_cut_gpu.h"
src/GPU/pair_coul_cut_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_coul_cut_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_coul_cut_gpu.cpp:int coul_gpu_init(const int ntypes, double **host_scale, double **cutsq, double *special_coul,
src/GPU/pair_coul_cut_gpu.cpp:                  const double cell_size, int &gpu_mode, FILE *screen, const double qqrd2e);
src/GPU/pair_coul_cut_gpu.cpp:void coul_gpu_reinit(const int ntypes, double **host_scale);
src/GPU/pair_coul_cut_gpu.cpp:void coul_gpu_clear();
src/GPU/pair_coul_cut_gpu.cpp:int **coul_gpu_compute_n(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_coul_cut_gpu.cpp:void coul_gpu_compute(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_coul_cut_gpu.cpp:double coul_gpu_bytes();
src/GPU/pair_coul_cut_gpu.cpp:PairCoulCutGPU::PairCoulCutGPU(LAMMPS *lmp) : PairCoulCut(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_coul_cut_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_coul_cut_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_coul_cut_gpu.cpp:PairCoulCutGPU::~PairCoulCutGPU()
src/GPU/pair_coul_cut_gpu.cpp:  coul_gpu_clear();
src/GPU/pair_coul_cut_gpu.cpp:void PairCoulCutGPU::compute(int eflag, int vflag)
src/GPU/pair_coul_cut_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_coul_cut_gpu.cpp:    firstneigh = coul_gpu_compute_n(neighbor->ago, inum, nall, atom->x, atom->type, sublo, subhi,
src/GPU/pair_coul_cut_gpu.cpp:    coul_gpu_compute(neighbor->ago, inum, nall, atom->x, atom->type, ilist, numneigh, firstneigh,
src/GPU/pair_coul_cut_gpu.cpp:void PairCoulCutGPU::init_style()
src/GPU/pair_coul_cut_gpu.cpp:  if (!atom->q_flag) error->all(FLERR, "Pair style coul/cut/gpu requires atom attribute q");
src/GPU/pair_coul_cut_gpu.cpp:  int success = coul_gpu_init(atom->ntypes + 1, scale, cutsq, force->special_coul, atom->nlocal,
src/GPU/pair_coul_cut_gpu.cpp:                              atom->nlocal + atom->nghost, mnf, maxspecial, cell_size, gpu_mode,
src/GPU/pair_coul_cut_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_coul_cut_gpu.cpp:  if (gpu_mode == GPU_FORCE) neighbor->add_request(this, NeighConst::REQ_FULL);
src/GPU/pair_coul_cut_gpu.cpp:void PairCoulCutGPU::reinit()
src/GPU/pair_coul_cut_gpu.cpp:  coul_gpu_reinit(atom->ntypes + 1, scale);
src/GPU/pair_coul_cut_gpu.cpp:double PairCoulCutGPU::memory_usage()
src/GPU/pair_coul_cut_gpu.cpp:  return bytes + coul_gpu_bytes();
src/GPU/pair_coul_cut_gpu.cpp:void PairCoulCutGPU::cpu_compute(int start, int inum, int eflag, int /* vflag */, int *ilist,
src/GPU/pair_buck_coul_long_gpu.h:PairStyle(buck/coul/long/gpu,PairBuckCoulLongGPU);
src/GPU/pair_buck_coul_long_gpu.h:#ifndef LMP_PAIR_BUCK_COUL_LONG_GPU_H
src/GPU/pair_buck_coul_long_gpu.h:#define LMP_PAIR_BUCK_COUL_LONG_GPU_H
src/GPU/pair_buck_coul_long_gpu.h:class PairBuckCoulLongGPU : public PairBuckCoulLong {
src/GPU/pair_buck_coul_long_gpu.h:  PairBuckCoulLongGPU(LAMMPS *lmp);
src/GPU/pair_buck_coul_long_gpu.h:  ~PairBuckCoulLongGPU() override;
src/GPU/pair_buck_coul_long_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_buck_coul_long_gpu.h:  int gpu_mode;
src/GPU/pair_eam_fs_gpu.cpp:#include "pair_eam_fs_gpu.h"
src/GPU/pair_eam_fs_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_eam_fs_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_eam_fs_gpu.cpp:int eam_fs_gpu_init(const int ntypes, double host_cutforcesq, int **host_type2rhor,
src/GPU/pair_eam_fs_gpu.cpp:                    const int maxspecial, const double cell_size, int &gpu_mode, FILE *screen,
src/GPU/pair_eam_fs_gpu.cpp:void eam_fs_gpu_clear();
src/GPU/pair_eam_fs_gpu.cpp:int **eam_fs_gpu_compute_n(const int ago, const int inum_full, const int nall, double **host_x,
src/GPU/pair_eam_fs_gpu.cpp:void eam_fs_gpu_compute(const int ago, const int inum_full, const int nlocal, const int nall,
src/GPU/pair_eam_fs_gpu.cpp:void eam_fs_gpu_compute_force(int *ilist, const bool eflag, const bool vflag, const bool eatom,
src/GPU/pair_eam_fs_gpu.cpp:double eam_fs_gpu_bytes();
src/GPU/pair_eam_fs_gpu.cpp:PairEAMFSGPU::PairEAMFSGPU(LAMMPS *lmp) : PairEAM(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_eam_fs_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_eam_fs_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_eam_fs_gpu.cpp:PairEAMFSGPU::~PairEAMFSGPU()
src/GPU/pair_eam_fs_gpu.cpp:  eam_fs_gpu_clear();
src/GPU/pair_eam_fs_gpu.cpp:double PairEAMFSGPU::memory_usage()
src/GPU/pair_eam_fs_gpu.cpp:  return bytes + eam_fs_gpu_bytes();
src/GPU/pair_eam_fs_gpu.cpp:void PairEAMFSGPU::compute(int eflag, int vflag)
src/GPU/pair_eam_fs_gpu.cpp:  // compute density on each atom on GPU
src/GPU/pair_eam_fs_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_eam_fs_gpu.cpp:    firstneigh = eam_fs_gpu_compute_n(neighbor->ago, inum, nall, atom->x, atom->type, sublo, subhi,
src/GPU/pair_eam_fs_gpu.cpp:  } else {    // gpu_mode == GPU_FORCE
src/GPU/pair_eam_fs_gpu.cpp:    eam_fs_gpu_compute(neighbor->ago, inum, nlocal, nall, atom->x, atom->type, ilist, numneigh,
src/GPU/pair_eam_fs_gpu.cpp:  // compute forces on each atom on GPU
src/GPU/pair_eam_fs_gpu.cpp:  if (gpu_mode != GPU_FORCE)
src/GPU/pair_eam_fs_gpu.cpp:    eam_fs_gpu_compute_force(nullptr, eflag, vflag, eflag_atom, vflag_atom);
src/GPU/pair_eam_fs_gpu.cpp:    eam_fs_gpu_compute_force(ilist, eflag, vflag, eflag_atom, vflag_atom);
src/GPU/pair_eam_fs_gpu.cpp:void PairEAMFSGPU::init_style()
src/GPU/pair_eam_fs_gpu.cpp:  int success = eam_fs_gpu_init(
src/GPU/pair_eam_fs_gpu.cpp:      atom->nlocal + atom->nghost, mnf, maxspecial, cell_size, gpu_mode, screen, fp_size);
src/GPU/pair_eam_fs_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_eam_fs_gpu.cpp:  if (gpu_mode == GPU_FORCE) neighbor->add_request(this, NeighConst::REQ_FULL);
src/GPU/pair_eam_fs_gpu.cpp:double PairEAMFSGPU::single(int i, int j, int itype, int jtype, double rsq,
src/GPU/pair_eam_fs_gpu.cpp:int PairEAMFSGPU::pack_forward_comm(int n, int *list, double *buf, int /* pbc_flag */,
src/GPU/pair_eam_fs_gpu.cpp:void PairEAMFSGPU::unpack_forward_comm(int n, int first, double *buf)
src/GPU/pair_eam_fs_gpu.cpp:void PairEAMFSGPU::coeff(int narg, char **arg)
src/GPU/pair_eam_fs_gpu.cpp:void PairEAMFSGPU::read_file(char *filename)
src/GPU/pair_eam_fs_gpu.cpp:void PairEAMFSGPU::file2array()
src/GPU/pair_coul_cut_gpu.h:PairStyle(coul/cut/gpu,PairCoulCutGPU);
src/GPU/pair_coul_cut_gpu.h:#ifndef LMP_PAIR_COUL_CUT_GPU_H
src/GPU/pair_coul_cut_gpu.h:#define LMP_PAIR_COUL_CUT_GPU_H
src/GPU/pair_coul_cut_gpu.h:class PairCoulCutGPU : public PairCoulCut {
src/GPU/pair_coul_cut_gpu.h:  PairCoulCutGPU(LAMMPS *lmp);
src/GPU/pair_coul_cut_gpu.h:  ~PairCoulCutGPU() override;
src/GPU/pair_coul_cut_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_coul_cut_gpu.h:  int gpu_mode;
src/GPU/pair_buck_coul_cut_gpu.cpp:#include "pair_buck_coul_cut_gpu.h"
src/GPU/pair_buck_coul_cut_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_buck_coul_cut_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_buck_coul_cut_gpu.cpp:int buckc_gpu_init(const int ntypes, double **cutsq, double **host_rhoinv, double **host_buck1,
src/GPU/pair_buck_coul_cut_gpu.cpp:                   const int maxspecial, const double cell_size, int &gpu_mode, FILE *screen,
src/GPU/pair_buck_coul_cut_gpu.cpp:void buckc_gpu_clear();
src/GPU/pair_buck_coul_cut_gpu.cpp:int **buckc_gpu_compute_n(const int ago, const int inum_full, const int nall, double **host_x,
src/GPU/pair_buck_coul_cut_gpu.cpp:void buckc_gpu_compute(const int ago, const int inum_full, const int nall, double **host_x,
src/GPU/pair_buck_coul_cut_gpu.cpp:double buckc_gpu_bytes();
src/GPU/pair_buck_coul_cut_gpu.cpp:PairBuckCoulCutGPU::PairBuckCoulCutGPU(LAMMPS *lmp) : PairBuckCoulCut(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_buck_coul_cut_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_buck_coul_cut_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_buck_coul_cut_gpu.cpp:PairBuckCoulCutGPU::~PairBuckCoulCutGPU()
src/GPU/pair_buck_coul_cut_gpu.cpp:  buckc_gpu_clear();
src/GPU/pair_buck_coul_cut_gpu.cpp:void PairBuckCoulCutGPU::compute(int eflag, int vflag)
src/GPU/pair_buck_coul_cut_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_buck_coul_cut_gpu.cpp:    firstneigh = buckc_gpu_compute_n(neighbor->ago, inum, nall, atom->x, atom->type, sublo, subhi,
src/GPU/pair_buck_coul_cut_gpu.cpp:    buckc_gpu_compute(neighbor->ago, inum, nall, atom->x, atom->type, ilist, numneigh, firstneigh,
src/GPU/pair_buck_coul_cut_gpu.cpp:void PairBuckCoulCutGPU::init_style()
src/GPU/pair_buck_coul_cut_gpu.cpp:  if (!atom->q_flag) error->all(FLERR, "Pair style buck/coul/cut/gpu requires atom attribute q");
src/GPU/pair_buck_coul_cut_gpu.cpp:      buckc_gpu_init(atom->ntypes + 1, cutsq, rhoinv, buck1, buck2, a, c, offset, force->special_lj,
src/GPU/pair_buck_coul_cut_gpu.cpp:                     gpu_mode, screen, cut_ljsq, cut_coulsq, force->special_coul, force->qqrd2e);
src/GPU/pair_buck_coul_cut_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_buck_coul_cut_gpu.cpp:  if (gpu_mode == GPU_FORCE) neighbor->add_request(this, NeighConst::REQ_FULL);
src/GPU/pair_buck_coul_cut_gpu.cpp:double PairBuckCoulCutGPU::memory_usage()
src/GPU/pair_buck_coul_cut_gpu.cpp:  return bytes + buckc_gpu_bytes();
src/GPU/pair_buck_coul_cut_gpu.cpp:void PairBuckCoulCutGPU::cpu_compute(int start, int inum, int eflag, int /* vflag */, int *ilist,
src/GPU/pair_lj_cut_tip4p_long_gpu.cpp:#include "pair_lj_cut_tip4p_long_gpu.h"
src/GPU/pair_lj_cut_tip4p_long_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_lj_cut_tip4p_long_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_lj_cut_tip4p_long_gpu.cpp:int ljtip4p_long_gpu_init(const int ntypes, double **cutsq, double **host_lj1, double **host_lj2,
src/GPU/pair_lj_cut_tip4p_long_gpu.cpp:                          const int maxspecial, const double cell_size, int &gpu_mode, FILE *screen,
src/GPU/pair_lj_cut_tip4p_long_gpu.cpp:void ljtip4p_long_gpu_clear();
src/GPU/pair_lj_cut_tip4p_long_gpu.cpp:int **ljtip4p_long_gpu_compute_n(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_lj_cut_tip4p_long_gpu.cpp:void ljtip4p_long_gpu_compute(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_lj_cut_tip4p_long_gpu.cpp:double ljtip4p_long_gpu_bytes();
src/GPU/pair_lj_cut_tip4p_long_gpu.cpp:PairLJCutTIP4PLongGPU::PairLJCutTIP4PLongGPU(LAMMPS *lmp) :
src/GPU/pair_lj_cut_tip4p_long_gpu.cpp:    PairLJCutTIP4PLong(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_lj_cut_tip4p_long_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_lj_cut_tip4p_long_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_lj_cut_tip4p_long_gpu.cpp:PairLJCutTIP4PLongGPU::~PairLJCutTIP4PLongGPU()
src/GPU/pair_lj_cut_tip4p_long_gpu.cpp:  ljtip4p_long_gpu_clear();
src/GPU/pair_lj_cut_tip4p_long_gpu.cpp:void PairLJCutTIP4PLongGPU::compute(int eflag, int vflag)
src/GPU/pair_lj_cut_tip4p_long_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_lj_cut_tip4p_long_gpu.cpp:    firstneigh = ljtip4p_long_gpu_compute_n(
src/GPU/pair_lj_cut_tip4p_long_gpu.cpp:    ljtip4p_long_gpu_compute(neighbor->ago, inum, nall, atom->x, atom->type, ilist, numneigh,
src/GPU/pair_lj_cut_tip4p_long_gpu.cpp:void PairLJCutTIP4PLongGPU::init_style()
src/GPU/pair_lj_cut_tip4p_long_gpu.cpp:    error->all(FLERR, "Pair style lj/cut/tip4p/long/gpu requires atom IDs");
src/GPU/pair_lj_cut_tip4p_long_gpu.cpp:    error->all(FLERR, "Pair style lj/cut/tip4p/long/gpu requires atom attribute q");
src/GPU/pair_lj_cut_tip4p_long_gpu.cpp:               "GPU-accelerated pair style lj/cut/tip4p/long currently"
src/GPU/pair_lj_cut_tip4p_long_gpu.cpp:      error->warning(FLERR, "Increasing communication cutoff to {:.8} for TIP4P GPU style",
src/GPU/pair_lj_cut_tip4p_long_gpu.cpp:  int success = ljtip4p_long_gpu_init(
src/GPU/pair_lj_cut_tip4p_long_gpu.cpp:      typeO, alpha, qdist, atom->nlocal + atom->nghost, mnf, maxspecial, cell_size, gpu_mode,
src/GPU/pair_lj_cut_tip4p_long_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_lj_cut_tip4p_long_gpu.cpp:  if (gpu_mode == GPU_FORCE) {
src/GPU/pair_lj_cut_tip4p_long_gpu.cpp:double PairLJCutTIP4PLongGPU::memory_usage()
src/GPU/pair_lj_cut_tip4p_long_gpu.cpp:  return bytes + ljtip4p_long_gpu_bytes();
src/GPU/pair_gayberne_gpu.h:PairStyle(gayberne/gpu,PairGayBerneGPU);
src/GPU/pair_gayberne_gpu.h:#ifndef LMP_PAIR_GAYBERNE_GPU_H
src/GPU/pair_gayberne_gpu.h:#define LMP_PAIR_GAYBERNE_GPU_H
src/GPU/pair_gayberne_gpu.h:class PairGayBerneGPU : public PairGayBerne {
src/GPU/pair_gayberne_gpu.h:  PairGayBerneGPU(LAMMPS *lmp);
src/GPU/pair_gayberne_gpu.h:  ~PairGayBerneGPU() override;
src/GPU/pair_gayberne_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_gayberne_gpu.h:  int gpu_mode;
src/GPU/pair_lj_cut_gpu.h:PairStyle(lj/cut/gpu,PairLJCutGPU);
src/GPU/pair_lj_cut_gpu.h:#ifndef LMP_PAIR_LJ_LIGHT_GPU_H
src/GPU/pair_lj_cut_gpu.h:#define LMP_PAIR_LJ_LIGHT_GPU_H
src/GPU/pair_lj_cut_gpu.h:class PairLJCutGPU : public PairLJCut {
src/GPU/pair_lj_cut_gpu.h:  PairLJCutGPU(LAMMPS *lmp);
src/GPU/pair_lj_cut_gpu.h:  ~PairLJCutGPU() override;
src/GPU/pair_lj_cut_gpu.h:  enum { GPU_FORCE, GPU_NEIGH, GPU_HYB_NEIGH };
src/GPU/pair_lj_cut_gpu.h:  int gpu_mode;
src/GPU/pair_lj_spica_coul_long_gpu.cpp:#include "pair_lj_spica_coul_long_gpu.h"
src/GPU/pair_lj_spica_coul_long_gpu.cpp:#include "gpu_extra.h"
src/GPU/pair_lj_spica_coul_long_gpu.cpp:// External functions from cuda library for atom decomposition
src/GPU/pair_lj_spica_coul_long_gpu.cpp:int spical_gpu_init(const int ntypes, double **cutsq, int **lj_type, double **host_lj1,
src/GPU/pair_lj_spica_coul_long_gpu.cpp:                    const int maxspecial, const double cell_size, int &gpu_mode, FILE *screen,
src/GPU/pair_lj_spica_coul_long_gpu.cpp:void spical_gpu_clear();
src/GPU/pair_lj_spica_coul_long_gpu.cpp:int **spical_gpu_compute_n(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_lj_spica_coul_long_gpu.cpp:void spical_gpu_compute(const int ago, const int inum, const int nall, double **host_x,
src/GPU/pair_lj_spica_coul_long_gpu.cpp:double spical_gpu_bytes();
src/GPU/pair_lj_spica_coul_long_gpu.cpp:PairLJSPICACoulLongGPU::PairLJSPICACoulLongGPU(LAMMPS *lmp) :
src/GPU/pair_lj_spica_coul_long_gpu.cpp:    PairLJSPICACoulLong(lmp), gpu_mode(GPU_FORCE)
src/GPU/pair_lj_spica_coul_long_gpu.cpp:  suffix_flag |= Suffix::GPU;
src/GPU/pair_lj_spica_coul_long_gpu.cpp:  GPU_EXTRA::gpu_ready(lmp->modify, lmp->error);
src/GPU/pair_lj_spica_coul_long_gpu.cpp:PairLJSPICACoulLongGPU::~PairLJSPICACoulLongGPU()
src/GPU/pair_lj_spica_coul_long_gpu.cpp:  spical_gpu_clear();
src/GPU/pair_lj_spica_coul_long_gpu.cpp:void PairLJSPICACoulLongGPU::compute(int eflag, int vflag)
src/GPU/pair_lj_spica_coul_long_gpu.cpp:  if (gpu_mode != GPU_FORCE) {
src/GPU/pair_lj_spica_coul_long_gpu.cpp:    firstneigh = spical_gpu_compute_n(neighbor->ago, inum, nall, atom->x, atom->type, sublo, subhi,
src/GPU/pair_lj_spica_coul_long_gpu.cpp:    spical_gpu_compute(neighbor->ago, inum, nall, atom->x, atom->type, ilist, numneigh, firstneigh,
src/GPU/pair_lj_spica_coul_long_gpu.cpp:void PairLJSPICACoulLongGPU::init_style()
src/GPU/pair_lj_spica_coul_long_gpu.cpp:    error->all(FLERR, "Pair style lj/spica/coul/long/gpu requires atom attribute q");
src/GPU/pair_lj_spica_coul_long_gpu.cpp:  int success = spical_gpu_init(atom->ntypes + 1, cutsq, lj_type, lj1, lj2, lj3, lj4, offset,
src/GPU/pair_lj_spica_coul_long_gpu.cpp:                                maxspecial, cell_size, gpu_mode, screen, cut_ljsq, cut_coulsq,
src/GPU/pair_lj_spica_coul_long_gpu.cpp:  GPU_EXTRA::check_flag(success, error, world);
src/GPU/pair_lj_spica_coul_long_gpu.cpp:  if (gpu_mode == GPU_FORCE) neighbor->add_request(this, NeighConst::REQ_FULL);
src/GPU/pair_lj_spica_coul_long_gpu.cpp:double PairLJSPICACoulLongGPU::memory_usage()
src/GPU/pair_lj_spica_coul_long_gpu.cpp:  return bytes + spical_gpu_bytes();
src/GPU/pair_lj_spica_coul_long_gpu.cpp:void PairLJSPICACoulLongGPU::cpu_compute(int start, int inum, int *ilist, int *numneigh,
src/Purge.list:pair_lj_sdk_gpu.cpp
src/Purge.list:pair_lj_sdk_gpu.h
src/Purge.list:pair_lj_sdk_coul_long_gpu.cpp
src/Purge.list:pair_lj_sdk_coul_long_gpu.h
src/Purge.list:atom_vec_angle_cuda.cpp
src/Purge.list:atom_vec_angle_cuda.h
src/Purge.list:atom_vec_atomic_cuda.cpp
src/Purge.list:atom_vec_atomic_cuda.h
src/Purge.list:atom_vec_charge_cuda.cpp
src/Purge.list:atom_vec_charge_cuda.h
src/Purge.list:atom_vec_full_cuda.cpp
src/Purge.list:atom_vec_full_cuda.h
src/Purge.list:comm_cuda.cpp
src/Purge.list:comm_cuda.h
src/Purge.list:compute_pe_cuda.cpp
src/Purge.list:compute_pe_cuda.h
src/Purge.list:compute_pressure_cuda.cpp
src/Purge.list:compute_pressure_cuda.h
src/Purge.list:compute_temp_cuda.cpp
src/Purge.list:compute_temp_cuda.h
src/Purge.list:compute_temp_partial_cuda.cpp
src/Purge.list:compute_temp_partial_cuda.h
src/Purge.list:cuda.cpp
src/Purge.list:cuda_data.h
src/Purge.list:cuda_modify_flags.h
src/Purge.list:cuda_neigh_list.cpp
src/Purge.list:cuda_neigh_list.h
src/Purge.list:domain_cuda.cpp
src/Purge.list:domain_cuda.h
src/Purge.list:fft3d_cuda.cpp
src/Purge.list:fft3d_cuda.h
src/Purge.list:fft3d_wrap_cuda.cpp
src/Purge.list:fft3d_wrap_cuda.h
src/Purge.list:fix_addforce_cuda.cpp
src/Purge.list:fix_addforce_cuda.h
src/Purge.list:fix_aveforce_cuda.cpp
src/Purge.list:fix_aveforce_cuda.h
src/Purge.list:fix_enforce2d_cuda.cpp
src/Purge.list:fix_enforce2d_cuda.h
src/Purge.list:fix_freeze_cuda.cpp
src/Purge.list:fix_freeze_cuda.h
src/Purge.list:fix_gravity_cuda.cpp
src/Purge.list:fix_gravity_cuda.h
src/Purge.list:fix_nh_cuda.cpp
src/Purge.list:fix_nh_cuda.h
src/Purge.list:fix_npt_cuda.cpp
src/Purge.list:fix_npt_cuda.h
src/Purge.list:fix_nve_cuda.cpp
src/Purge.list:fix_nve_cuda.h
src/Purge.list:fix_nvt_cuda.cpp
src/Purge.list:fix_nvt_cuda.h
src/Purge.list:fix_set_force_cuda.cpp
src/Purge.list:fix_set_force_cuda.h
src/Purge.list:fix_shake_cuda.cpp
src/Purge.list:fix_shake_cuda.h
src/Purge.list:fix_temp_berendsen_cuda.cpp
src/Purge.list:fix_temp_berendsen_cuda.h
src/Purge.list:fix_temp_rescale_cuda.cpp
src/Purge.list:fix_temp_rescale_cuda.h
src/Purge.list:fix_temp_rescale_limit_cuda.cpp
src/Purge.list:fix_temp_rescale_limit_cuda.h
src/Purge.list:fix_viscous_cuda.cpp
src/Purge.list:fix_viscous_cuda.h
src/Purge.list:modify_cuda.cpp
src/Purge.list:modify_cuda.h
src/Purge.list:neighbor_cuda.cpp
src/Purge.list:neighbor_cuda.h
src/Purge.list:neigh_full_cuda.cpp
src/Purge.list:pair_born_coul_long_cuda.cpp
src/Purge.list:pair_born_coul_long_cuda.h
src/Purge.list:pair_buck_coul_cut_cuda.cpp
src/Purge.list:pair_buck_coul_cut_cuda.h
src/Purge.list:pair_buck_coul_long_cuda.cpp
src/Purge.list:pair_buck_coul_long_cuda.h
src/Purge.list:pair_buck_cuda.cpp
src/Purge.list:pair_buck_cuda.h
src/Purge.list:pair_eam_alloy_cuda.cpp
src/Purge.list:pair_eam_alloy_cuda.h
src/Purge.list:pair_eam_cuda.cpp
src/Purge.list:pair_eam_cuda.h
src/Purge.list:pair_eam_fs_cuda.cpp
src/Purge.list:pair_eam_fs_cuda.h
src/Purge.list:pair_gran_hooke_cuda.cpp
src/Purge.list:pair_gran_hooke_cuda.h
src/Purge.list:pair_lj96_cut_cuda.cpp
src/Purge.list:pair_lj96_cut_cuda.h
src/Purge.list:pair_lj_charmm_coul_charmm_cuda.cpp
src/Purge.list:pair_lj_charmm_coul_charmm_cuda.h
src/Purge.list:pair_lj_charmm_coul_charmm_implicit_cuda.cpp
src/Purge.list:pair_lj_charmm_coul_charmm_implicit_cuda.h
src/Purge.list:pair_lj_charmm_coul_long_cuda.cpp
src/Purge.list:pair_lj_charmm_coul_long_cuda.h
src/Purge.list:pair_lj_class2_coul_cut_cuda.cpp
src/Purge.list:pair_lj_class2_coul_cut_cuda.h
src/Purge.list:pair_lj_class2_coul_long_cuda.cpp
src/Purge.list:pair_lj_class2_coul_long_cuda.h
src/Purge.list:pair_lj_class2_cuda.cpp
src/Purge.list:pair_lj_class2_cuda.h
src/Purge.list:pair_lj_cut_coul_cut_cuda.cpp
src/Purge.list:pair_lj_cut_coul_cut_cuda.h
src/Purge.list:pair_lj_cut_coul_debye_cuda.cpp
src/Purge.list:pair_lj_cut_coul_debye_cuda.h
src/Purge.list:pair_lj_cut_coul_long_cuda.cpp
src/Purge.list:pair_lj_cut_coul_long_cuda.h
src/Purge.list:pair_lj_cut_cuda.cpp
src/Purge.list:pair_lj_cut_cuda.h
src/Purge.list:pair_lj_cut_experimental_cuda.cpp
src/Purge.list:pair_lj_cut_experimental_cuda.h
src/Purge.list:pair_lj_expand_cuda.cpp
src/Purge.list:pair_lj_expand_cuda.h
src/Purge.list:pair_lj_gromacs_coul_gromacs_cuda.cpp
src/Purge.list:pair_lj_gromacs_coul_gromacs_cuda.h
src/Purge.list:pair_lj_gromacs_cuda.cpp
src/Purge.list:pair_lj_gromacs_cuda.h
src/Purge.list:pair_lj_sdk_coul_long_cuda.cpp
src/Purge.list:pair_lj_sdk_coul_long_cuda.h
src/Purge.list:pair_lj_sdk_cuda.cpp
src/Purge.list:pair_lj_sdk_cuda.h
src/Purge.list:pair_lj_smooth_cuda.cpp
src/Purge.list:pair_lj_smooth_cuda.h
src/Purge.list:pair_morse_cuda.cpp
src/Purge.list:pair_morse_cuda.h
src/Purge.list:pair_sw_cuda.cpp
src/Purge.list:pair_sw_cuda.h
src/Purge.list:pair_tersoff_cuda.cpp
src/Purge.list:pair_tersoff_cuda.h
src/Purge.list:pair_tersoff_zbl_cuda.cpp
src/Purge.list:pair_tersoff_zbl_cuda.h
src/Purge.list:pppm_cuda.cpp
src/Purge.list:pppm_cuda.h
src/Purge.list:user_cuda.h
src/Purge.list:verlet_cuda.cpp
src/Purge.list:verlet_cuda.h
src/Purge.list:pair_dipole_cut_gpu.h
src/Purge.list:pair_dipole_cut_gpu.cpp
src/Purge.list:pair_dipole_sf_gpu.h
src/Purge.list:pair_dipole_sf_gpu.cpp
src/Purge.list:pair_lj_sdk_coul_cut_cuda.cpp
src/Purge.list:pair_lj_sdk_coul_cut_cuda.h
src/Purge.list:pair_lj_sdk_coul_debye_cuda.cpp
src/Purge.list:pair_lj_sdk_coul_debye_cuda.h
src/Purge.list:pair_cg_cmm_coul_cut_cuda.cpp
src/Purge.list:pair_cg_cmm_coul_cut_cuda.h
src/Purge.list:pair_cg_cmm_coul_debye_cuda.cpp
src/Purge.list:pair_cg_cmm_coul_debye_cuda.h
src/Purge.list:pair_cg_cmm_coul_long_cuda.cpp
src/Purge.list:pair_cg_cmm_coul_long_cuda.h
src/Purge.list:pair_cg_cmm_cuda.cpp
src/Purge.list:pair_cg_cmm_cuda.h
src/Purge.list:pair_cg_cmm_coul_long_gpu.cpp
src/Purge.list:pair_cg_cmm_coul_long_gpu.h
src/Purge.list:pair_cg_cmm_gpu.cpp
src/Purge.list:pair_cg_cmm_gpu.h
src/Purge.list:pair_cg_cmm_coul_long_gpu.cpp
src/Purge.list:pair_cg_cmm_coul_long_gpu.h
src/Purge.list:pair_cg_cmm_gpu.cpp
src/Purge.list:pair_cg_cmm_gpu.h
src/Purge.list:pppm_gpu_double.cpp
src/Purge.list:pppm_gpu_double.h
src/Purge.list:pppm_gpu_single.cpp
src/Purge.list:pppm_gpu_single.h
src/Purge.list:pair_lj_charmm_coul_long_gpu2.cpp
src/Purge.list:pair_lj_charmm_coul_long_gpu2.h
src/pair.cpp:  // when a hybrid pair style uses both a gpu and non-gpu pair style
src/pair.cpp:  // or when respa is used with gpu pair styles
src/lmpfftsettings.h:#elif defined(FFT_MKL_GPU)
src/lmpfftsettings.h:#define LMP_FFT_LIB "MKL GPU FFT"
src/neighbor.h:  // option to call build_topology (e.g. from gpu styles instead for overlapped computation)
src/procmap.h:#ifndef LMP_PROCMAP_H
src/procmap.h:#define LMP_PROCMAP_H
src/procmap.h:class ProcMap : protected Pointers {
src/procmap.h:  ProcMap(class LAMMPS *);
src/atom.cpp:#ifdef LMP_GPU
src/atom.cpp:#include "fix_gpu.h"
src/atom.cpp:#ifdef LMP_GPU
src/atom.cpp:    auto ifix = dynamic_cast<FixGPU *>(modify->get_fix_by_id("package_gpu"));
src/atom.cpp:#ifdef LMP_GPU
src/atom.cpp:    FixGPU *fix = dynamic_cast<FixGPU *>(modify->get_fix_by_id("package_gpu"));
src/imbalance_neigh.cpp:  // cannot use neighbor list weight with KOKKOS using GPUs
src/imbalance_neigh.cpp:    if (lmp->kokkos->ngpus > 0) {
src/imbalance_neigh.cpp:        error->warning(FLERR, "Balance weight neigh skipped with KOKKOS using GPUs");
src/EXTRA-FIX/fix_wall_flow.cpp:    " title = {GPU-based molecular dynamics of fluid flows: Reaching for turbulence},\n"
src/info.h:  static bool has_gpu_device();
src/info.h:  static std::string get_gpu_device_info();
src/Makefile:	gpu \
src/Makefile:	gpu \
src/Makefile:PACKINT = atc awpmd colvars electrode gpu kokkos lepton ml-pod poems
src/pair_hybrid.cpp:  // The GPU library uses global data for each pair style, so the
src/pair_hybrid.cpp:    bool is_gpu = styles[istyle]->suffix_flag & Suffix::GPU;
src/pair_hybrid.cpp:    if (multiple[istyle] && is_gpu)
src/pair_hybrid.cpp:      error->all(FLERR,"GPU package styles must not be used multiple times");
src/pair_hybrid.cpp:  if (styles[m]->suffix_flag & (Suffix::INTEL|Suffix::GPU))
src/grid3d.cpp:  // procmid = 1st processor in upper half of partition
src/grid3d.cpp:  int procmid = proclower + (procupper - proclower) / 2 + 1;
src/grid3d.cpp:  int dim = rcbinfo[procmid].dim;
src/grid3d.cpp:  int cut = rcbinfo[procmid].cut;
src/grid3d.cpp:  if (box[2*dim] < cut) box_drop_grid(box,proclower,procmid-1,np,plist);
src/grid3d.cpp:  if (box[2*dim+1] >= cut) box_drop_grid(box,procmid,procupper,np,plist);
src/grid3d.cpp:  // procmid = 1st processor in upper half of partition
src/grid3d.cpp:  int procmid = proclower + (procupper - proclower) / 2 + 1;
src/grid3d.cpp:  int dim = rcbinfo[procmid].dim;
src/grid3d.cpp:  int cut = rcbinfo[procmid].cut;
src/grid3d.cpp:  if (proc < procmid) {
src/grid3d.cpp:    partition_tiled(proc,proclower,procmid-1,box);
src/grid3d.cpp:    partition_tiled(proc,procmid,procupper,box);
src/pair_hybrid_scaled.cpp:    if ((styles[nstyles]->suffix_flag & (Suffix::INTEL | Suffix::GPU | Suffix::OMP)) != 0)
src/procmap.cpp:#include "procmap.h"
src/procmap.cpp:ProcMap::ProcMap(LAMMPS *lmp) : Pointers(lmp) {}
src/procmap.cpp:void ProcMap::onelevel_grid(int nprocs, int *user_procgrid, int *procgrid,
src/procmap.cpp:  memory->create(factors,npossible,3,"procmap:factors");
src/procmap.cpp:void ProcMap::twolevel_grid(int nprocs, int *user_procgrid, int *procgrid,
src/procmap.cpp:  memory->create(nfactors,nnpossible,3,"procmap:nfactors");
src/procmap.cpp:  memory->create(cfactors,ncpossible,3,"procmap:cfactors");
src/procmap.cpp:  memory->create(factors,npossible,4,"procmap:factors");
src/procmap.cpp:void ProcMap::numa_grid(int numa_nodes, int nprocs, int *user_procgrid,
src/procmap.cpp:  memory->create(nodefactors,nodepossible,3,"procmap:nodefactors");
src/procmap.cpp:  memory->create(numafactors,numapossible,3,"procmap:numafactors");
src/procmap.cpp:void ProcMap::custom_grid(char *cfile, int nprocs,
src/procmap.cpp:  memory->create(cmap,nprocs,4,"procmap:cmap");
src/procmap.cpp:void ProcMap::cart_map(int reorder, int *procgrid,
src/procmap.cpp:void ProcMap::cart_map(int reorder, int *procgrid, int ncores, int *coregrid,
src/procmap.cpp:void ProcMap::xyz_map(char *xyz, int *procgrid,
src/procmap.cpp:void ProcMap::xyz_map(char *xyz, int *procgrid, int ncores, int *coregrid,
src/procmap.cpp:void ProcMap::numa_map(int reorder, int *numagrid,
src/procmap.cpp:void ProcMap::custom_map(int *procgrid,
src/procmap.cpp:void ProcMap::output(char *file, int *procgrid, int ***grid2proc)
src/procmap.cpp:int ProcMap::factor(int n, int **factors)
src/procmap.cpp:int ProcMap::combine_factors(int n1, int **factors1, int n2, int **factors2,
src/procmap.cpp:int ProcMap::cull_2d(int n, int **factors, int m)
src/procmap.cpp:int ProcMap::cull_user(int n, int **factors, int m, int *user_factors)
src/procmap.cpp:int ProcMap::cull_other(int n, int **factors, int m,
src/procmap.cpp:int ProcMap::best_factors(int npossible, int **factors, int *best,
src/procmap.cpp:void ProcMap::grid_shift(int myloc, int nprocs, int &minus, int &plus)
src/fmt/base.h:// LAMMPS customization: exclude __NVCC__ in addition to __NVCOMPILER for nvcc from CUDA toolkit
src/fmt/base.h:// TODO: check if __CUDACC__ is a sufficient condition
src/fmt/base.h:#if !defined(__OPTIMIZE__) && !defined(__CUDACC__) && !defined(__NVCOMPILER) && !defined(__NVCC__)
src/fmt/format-inl.h:  // Chosen instead of std::abort to satisfy Clang in CUDA mode during device
src/fmt/format.h:// EDG based compilers (Intel, NVIDIA, Elbrus, etc), GCC and MSVC support UDLs.
src/pair.h:  friend class FixGPU;
src/MANYBODY/pair_tersoff.cpp:      if (suffix_flag & (Suffix::INTEL|Suffix::GPU|Suffix::KOKKOS))
src/comm.cpp:#include "procmap.h"
src/comm.cpp:  // create ProcMap class to create 3d grid and map procs to it
src/comm.cpp:  auto pmap = new ProcMap(lmp);
src/comm.cpp:  // should not be necessary due to ProcMap
src/comm.cpp:  // free ProcMap class
src/finish.cpp:  if ((comm->me == 0) && lmp->kokkos && (lmp->kokkos->ngpus > 0))
src/finish.cpp:    if (const char* env_clb = getenv("CUDA_LAUNCH_BLOCKING"))
src/finish.cpp:                       "since GPU/CPU overlap is enabled\n"
src/finish.cpp:                       "Using 'export CUDA_LAUNCH_BLOCKING=1' will give an "
src/grid2d.cpp:  // procmid = 1st processor in upper half of partition
src/grid2d.cpp:  int procmid = proclower + (procupper - proclower) / 2 + 1;
src/grid2d.cpp:  int dim = rcbinfo[procmid].dim;
src/grid2d.cpp:  int cut = rcbinfo[procmid].cut;
src/grid2d.cpp:  if (box[2*dim] < cut) box_drop_grid(box,proclower,procmid-1,np,plist);
src/grid2d.cpp:  if (box[2*dim+1] >= cut) box_drop_grid(box,procmid,procupper,np,plist);
src/grid2d.cpp:  // procmid = 1st processor in upper half of partition
src/grid2d.cpp:  int procmid = proclower + (procupper - proclower) / 2 + 1;
src/grid2d.cpp:  int dim = rcbinfo[procmid].dim;
src/grid2d.cpp:  int cut = rcbinfo[procmid].cut;
src/grid2d.cpp:  if (proc < procmid) {
src/grid2d.cpp:    partition_tiled(proc,proclower,procmid-1,box);
src/grid2d.cpp:    partition_tiled(proc,procmid,procupper,box);
src/kspace.h:    REVERSE_RHO_GPU,
src/library.cpp:   * - kokkos_ngpus
src/library.cpp:     - Number of Kokkos gpus per physical node, 0 if Kokkos is not active or no GPU support.
src/library.cpp:  if (strcmp(keyword,"kokkos_ngpus") == 0) return (lmp->kokkos) ? lmp->kokkos->ngpus : 0;
src/library.cpp:Supported packages names are "GPU", "KOKKOS", "INTEL", and "OPENMP".
src/library.cpp:Supported categories are "api" with possible settings "cuda", "hip", "phi",
src/library.cpp:"pthreads", "opencl", "openmp", and "serial", and "precision" with
src/library.cpp:/** Check for presence of a viable GPU package device
src/library.cpp:The :cpp:func:`lammps_has_gpu_device` function checks at runtime if
src/library.cpp::doc:`GPU package <Speed_gpu>`. If at least one suitable device is
src/library.cpp::cpp:func:`lammps_get_gpu_device_info` function.
src/library.cpp:int lammps_has_gpu_device()
src/library.cpp:  return Info::has_gpu_device() ? 1: 0;
src/library.cpp:/** Get GPU package device information
src/library.cpp:The :cpp:func:`lammps_get_gpu_device_info` function can be used to retrieve
src/library.cpp:with the :doc:`GPU package <Speed_gpu>`.  It will produce a string that is
src/library.cpp:``hip_get_device`` tools that are compiled alongside LAMMPS if the GPU
src/library.cpp:void lammps_get_gpu_device_info(char *buffer, int buf_size)
src/library.cpp:  std::string devinfo = Info::get_gpu_device_info();
src/info.cpp:    if (Info::has_gpu_device())
src/info.cpp:      fmt::print(out,"\nAvailable GPU devices:\n{}\n",get_gpu_device_info());
src/info.cpp:    if (strcmp(name,"gpu") == 0) {
src/info.cpp:      return modify->get_fix_by_id("package_gpu") != nullptr;
src/info.cpp:#if defined(LMP_GPU)
src/info.cpp:extern bool lmp_gpu_config(const std::string &, const std::string &);
src/info.cpp:extern bool lmp_has_compatible_gpu_device();
src/info.cpp:extern std::string lmp_gpu_device_info();
src/info.cpp:// we will only report compatible GPUs, i.e. when a GPU device is
src/info.cpp:bool Info::has_gpu_device()
src/info.cpp:  return lmp_has_compatible_gpu_device();
src/info.cpp:std::string Info::get_gpu_device_info()
src/info.cpp:  return lmp_gpu_device_info();
src/info.cpp:bool Info::has_gpu_device()
src/info.cpp:std::string Info::get_gpu_device_info()
src/info.cpp:#if defined(KOKKOS_ENABLE_CUDA)
src/info.cpp:      if (setting == "cuda") return true;
src/info.cpp:#if defined(LMP_GPU)
src/info.cpp:  if (package == "GPU") {
src/info.cpp:    return lmp_gpu_config(category,setting);
src/info.cpp:  if ((package.empty() || (package == "GPU")) && has_package("GPU")) {
src/info.cpp:    mesg += "GPU package API:";
src/info.cpp:    if (has_accelerator_feature("GPU","api","cuda"))   mesg += " CUDA";
src/info.cpp:    if (has_accelerator_feature("GPU","api","hip"))    mesg += " HIP";
src/info.cpp:    if (has_accelerator_feature("GPU","api","opencl")) mesg += " OpenCL";
src/info.cpp:    mesg +=  "\nGPU package precision:";
src/info.cpp:    if (has_accelerator_feature("GPU","precision","single")) mesg += " single";
src/info.cpp:    if (has_accelerator_feature("GPU","precision","mixed"))  mesg += " mixed";
src/info.cpp:    if (has_accelerator_feature("GPU","precision","double")) mesg += " double";
src/info.cpp:    if (has_accelerator_feature("KOKKOS","api","cuda"))     mesg += " CUDA";
src/info.cpp:#elif defined(FFT_MKL_GPU)
src/info.cpp:  fft_info += "FFT library = MKL GPU\n";
src/info.cpp:#elif defined(FFT_KOKKOS_MKL_GPU)
src/info.cpp:  fft_info += "KOKKOS FFT library = MKL GPU\n";
src/accelerator_kokkos.h:  int ngpus;
src/comm_tiled.cpp:  nprocmax = nullptr;
src/comm_tiled.cpp:  nexchprocmax = nullptr;
src/comm_tiled.cpp:  // resets nprocmax
src/comm_tiled.cpp:      if (noverlap > nprocmax[iswap]) {
src/comm_tiled.cpp:        int oldmax = nprocmax[iswap];
src/comm_tiled.cpp:        while (nprocmax[iswap] < noverlap) nprocmax[iswap] += DELTA_PROCS;
src/comm_tiled.cpp:        grow_swap_send(iswap,nprocmax[iswap],oldmax);
src/comm_tiled.cpp:        if (idir == 0) grow_swap_recv(iswap+1,nprocmax[iswap]);
src/comm_tiled.cpp:        else grow_swap_recv(iswap-1,nprocmax[iswap]);
src/comm_tiled.cpp:  // sets nexchproc & exchproc, resets nexchprocmax
src/comm_tiled.cpp:    if (noverlap > nexchprocmax[idim]) {
src/comm_tiled.cpp:      while (nexchprocmax[idim] < noverlap) nexchprocmax[idim] += DELTA_PROCS;
src/comm_tiled.cpp:      exchproc[idim] = new int[nexchprocmax[idim]];
src/comm_tiled.cpp:      exchnum[idim] = new int[nexchprocmax[idim]];
src/comm_tiled.cpp:  for (i = 0; i < nswap; i++) nmax = MAX(nmax,nprocmax[i]);
src/comm_tiled.cpp:  for (i = 0; i < dimension; i++) nmax = MAX(nmax,nexchprocmax[i]);
src/comm_tiled.cpp:  // procmid = 1st processor in upper half of partition
src/comm_tiled.cpp:  int procmid = proclower + (procupper - proclower) / 2 + 1;
src/comm_tiled.cpp:  int idim = rcbinfo[procmid].dim;
src/comm_tiled.cpp:  double cut = boxlo[idim] + prd[idim]*rcbinfo[procmid].cutfrac;
src/comm_tiled.cpp:    box_drop_tiled_recurse(lo,hi,proclower,procmid-1,indexme);
src/comm_tiled.cpp:    box_drop_tiled_recurse(lo,hi,procmid,procupper,indexme);
src/comm_tiled.cpp:  // procmid = 1st processor in upper half of partition
src/comm_tiled.cpp:  int procmid = proclower + (procupper - proclower) / 2 + 1;
src/comm_tiled.cpp:  int idim = rcbinfo[procmid].dim;
src/comm_tiled.cpp:  double cut = boxlo[idim] + prd[idim]*rcbinfo[procmid].cutfrac;
src/comm_tiled.cpp:  if (x[idim] < cut) return point_drop_tiled_recurse(x,proclower,procmid-1);
src/comm_tiled.cpp:  else return point_drop_tiled_recurse(x,procmid,procupper);
src/comm_tiled.cpp:  nprocmax = new int[n];
src/comm_tiled.cpp:    nprocmax[i] = DELTA_PROCS;
src/comm_tiled.cpp:  nexchprocmax = new int[n/2];
src/comm_tiled.cpp:    nexchprocmax[i] = DELTA_PROCS;
src/comm_tiled.cpp:      for (int j = 0; j < nprocmax[i]; j++) memory->destroy(sendlist[i][j]);
src/comm_tiled.cpp:  delete [] nprocmax;
src/comm_tiled.cpp:  delete [] nexchprocmax;
src/read_data.cpp:static const char *suffixes[] = {"/cuda", "/gpu", "/opt", "/omp", "/kk", "/coul/cut", "/coul/long",
src/rcb.cpp:  // procmid = 1st proc in upper half of partition
src/rcb.cpp:  int procmid;
src/rcb.cpp:    procmid = proclower + (procupper - proclower) / 2 + 1;
src/rcb.cpp:    if (me < procmid)
src/rcb.cpp:      procpartner = me + (procmid - proclower);
src/rcb.cpp:      procpartner = me - (procmid - proclower);
src/rcb.cpp:    if (me == procupper && procpartner != procmid - 1) {
src/rcb.cpp:    targetlo = wttot * (procmid - proclower) / (procupper + 1 - proclower);
src/rcb.cpp:        if (first_iteration && reuse && dim == tree[procmid].dim) {
src/rcb.cpp:          valuehalf = tree[procmid].cut;
src/rcb.cpp:    // store cut info only if I am procmid
src/rcb.cpp:    if (me == procmid) {
src/rcb.cpp:    if (me < procmid) hi[dim] = valuehalf;
src/rcb.cpp:    if (me < procmid) {
src/rcb.cpp:      procupper = procmid - 1;
src/rcb.cpp:      proclower = procmid;
src/rcb.cpp:  // procmid = 1st proc in upper half of partition
src/rcb.cpp:  int procmid;
src/rcb.cpp:    procmid = proclower + (procupper - proclower) / 2 + 1;
src/rcb.cpp:    if (me < procmid)
src/rcb.cpp:      procpartner = me + (procmid - proclower);
src/rcb.cpp:      procpartner = me - (procmid - proclower);
src/rcb.cpp:    if (me == procupper && procpartner != procmid - 1) {
src/rcb.cpp:    targetlo = wttot * (procmid - proclower) / (procupper + 1 - proclower);
src/rcb.cpp:      if (first_iteration && reuse && dim == tree[procmid].dim) {
src/rcb.cpp:        valuehalf = tree[procmid].cut;
src/rcb.cpp:    // store cut info only if I am procmid
src/rcb.cpp:    if (me == procmid) {
src/rcb.cpp:    if (me < procmid) hi[dim] = valuehalf;
src/rcb.cpp:    if (me < procmid) {
src/rcb.cpp:      procupper = procmid - 1;
src/rcb.cpp:      proclower = procmid;
src/lammps.cpp:        if (strcmp("gpu", pkg_name) == 0) package_issued |= Suffix::GPU;
src/lammps.cpp:  // check that GPU, INTEL, OPENMP fixes were compiled with LAMMPS
src/lammps.cpp:    if (strcmp(suffix,"gpu") == 0 && !modify->check_package("GPU"))
src/lammps.cpp:      error->all(FLERR,"Using suffix gpu without GPU package installed");
src/lammps.cpp:    if (strcmp(suffix,"gpu") == 0 && !(package_issued & Suffix::GPU))
src/lammps.cpp:      input->one("package gpu 0");
src/lammps.cpp:      if (strcmp(suffix2,"gpu") == 0 && !(package_issued & Suffix::GPU))
src/lammps.cpp:        input->one("package gpu 0");
src/lammps.cpp:#ifdef LMP_KOKKOS_GPU
src/lammps.cpp:          "-suffix gpu/intel/kk/opt/omp: style suffix to apply (-sf)\n"
src/lammps.cpp:#if defined(LMP_GPU)
src/lammps.cpp:  fmt::print(fp,"Compatible GPU present: {}\n\n",Info::has_gpu_device() ? "yes" : "no");
src/suffix.h:  enum { NONE = 0, OPT = 1 << 0, GPU = 1 << 1, OMP = 1 << 2, INTEL = 1 << 3, KOKKOS = 1 << 4 };
src/neighbor.cpp:  // GPU setting
src/neighbor.cpp:  // skip if GPU package styles will call it explicitly to overlap with GPU computation.
src/neighbor.cpp:   return the value of exclude - used to check compatibility with GPU
src/neighbor.cpp:   If nonzero, call build_topology from GPU styles instead to overlap comp
src/library.h:int lammps_has_gpu_device();
src/library.h:void lammps_get_gpu_device_info(char *buffer, int buf_size);
src/.gitignore:/*_gpu.h
src/.gitignore:/*_gpu.cpp
src/.gitignore:/fix_gpu.cpp
src/.gitignore:/fix_gpu.h
src/.gitignore:/gpu_extra.h
src/.gitignore:/pair_lj_cut_tgpu.cpp
src/.gitignore:/pair_lj_cut_tgpu.h
src/input.cpp:  if (strcmp(arg[0],"gpu") == 0) {
src/input.cpp:    if (!modify->check_package("GPU"))
src/input.cpp:      error->all(FLERR,"Package gpu command without GPU package installed");
src/input.cpp:    std::string fixcmd = "package_gpu all GPU";
src/Depend.sh:  depend GPU
src/Depend.sh:  depend GPU
src/Depend.sh:  depend GPU
src/Depend.sh:  depend GPU
src/Depend.sh:  depend GPU
src/Depend.sh:  depend GPU
src/Depend.sh:  depend GPU
src/Depend.sh:  depend GPU
src/Depend.sh:  depend GPU
src/Depend.sh:  depend GPU
src/Depend.sh:  depend GPU
src/pair_hybrid.h:  friend class FixGPU;
src/FEP/compute_fep.h:  class Fix *fixgpu;
src/FEP/compute_fep.cpp:  fixgpu = nullptr;
src/FEP/compute_fep.cpp:  // detect if package gpu is present
src/FEP/compute_fep.cpp:  fixgpu = modify->get_fix_by_id("package_gpu");
src/FEP/compute_fep.cpp:  // accumulate force/energy/virial from /gpu pair styles
src/FEP/compute_fep.cpp:  if (fixgpu) fixgpu->post_force(vflag);
src/FEP/compute_fep.cpp:  // accumulate force/energy/virial from /gpu pair styles
src/FEP/compute_fep.cpp:  // otherwise the force compute on the GPU in the next step would be incorrect
src/FEP/compute_fep.cpp:  if (fixgpu) fixgpu->post_force(vflag);
src/FEP/compute_fep_ta.cpp:  fixgpu = nullptr;
src/FEP/compute_fep_ta.cpp:  // detect if package gpu is present
src/FEP/compute_fep_ta.cpp:  fixgpu = modify->get_fix_by_id("package_gpu");
src/FEP/compute_fep_ta.cpp:  // accumulate force/energy/virial from /gpu pair styles
src/FEP/compute_fep_ta.cpp:  // otherwise the force compute on the GPU in the next step would be incorrect
src/FEP/compute_fep_ta.cpp:  if (fixgpu) fixgpu->post_force(vflag);
src/FEP/compute_fep_ta.cpp:  // accumulate force/energy/virial from /gpu pair styles
src/FEP/compute_fep_ta.cpp:  // otherwise the force compute on the GPU in the next step would be incorrect
src/FEP/compute_fep_ta.cpp:  if (fixgpu) fixgpu->post_force(vflag);
src/FEP/compute_fep_ta.h:  class Fix *fixgpu;

```
