# https://github.com/TinkerTools/tinker-hp

```console
GPU/plumed2/configure.ac:PLUMED_CONFIG_ENABLE([af_cuda],[search for arrayfire_cuda],[no])
GPU/plumed2/configure.ac:  PLUMED_CHECK_PACKAGE([arrayfire.h],[af_is_double],[__PLUMED_HAS_ARRAYFIRE],[afopencl])
GPU/plumed2/configure.ac:if test "$af_cuda" == true ; then
GPU/plumed2/configure.ac:  PLUMED_CHECK_PACKAGE([arrayfire.h],[af_is_double],[__PLUMED_HAS_ARRAYFIRE],[afcuda])
GPU/plumed2/patches/amber18.config:This patch is compatible with the MPI, CUDA and CUDA + MPI versions of pmemd.
GPU/plumed2/patches/namd-2.12.diff: 	$(CUDALIB) \
GPU/plumed2/patches/namd-2.12.diff: #include "DeviceCUDA.h"
GPU/plumed2/patches/namd-2.12.diff: #ifdef NAMD_CUDA
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:#include "gromacs/ewald/pme-gpu-program.h"
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:#include "gromacs/gpu_utils/clfftinitializer.h"
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:#include "gromacs/gpu_utils/gpu_utils.h"
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:#include "gromacs/listed-forces/gpubonded.h"
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:#include "gromacs/mdlib/nbnxn_gpu_data_mgmt.h"
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:#include "gromacs/taskassignment/decidegpuusage.h"
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:#include "gromacs/taskassignment/usergpuids.h"
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:                                  bool                            makeGpuPairList,
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:        ListSetupType      listType  = (makeGpuPairList ? ListSetupType::Gpu : ListSetupType::CpuSimdWhenSupported);
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:        increaseNstlist(fplog, cr, ir, nstlist_cmdline, mtop, box, makeGpuPairList, cpuinfo);
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:/*! \brief Return whether GPU acceleration of nonbondeds is supported with the given settings.
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:static bool gpuAccelerationOfNonbondedIsUseful(const MDLogger   &mdlog,
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:        /* The GPU code does not support more than one energy group.
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:         * If the user requested GPUs explicitly, a fatal error is given later.
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:                .appendText("Multiple energy groups is not implemented for GPUs, falling back to the CPU. "
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:                            "For better performance, run on the GPU without energy groups and then do "
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:    else if (strncmp(optionString, "gpu", 3) == 0)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:        returnValue = TaskTarget::Gpu;
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:    EmulateGpuNonbonded emulateGpuNonbonded = (getenv("GMX_EMULATE_GPU") != nullptr ?
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:                                               EmulateGpuNonbonded::Yes : EmulateGpuNonbonded::No);
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:    std::vector<int>    gpuIdsAvailable;
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:        gpuIdsAvailable = parseUserGpuIds(hw_opt.gpuIdsAvailable);
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:        // TODO We could put the GPU IDs into a std::map to find
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:        for (size_t i = 0; i != gpuIdsAvailable.size(); ++i)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:            for (size_t j = i+1; j != gpuIdsAvailable.size(); ++j)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:                if (gpuIdsAvailable[i] == gpuIdsAvailable[j])
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:                    GMX_THROW(InvalidInputError(formatString("The string of available GPU device IDs '%s' may not contain duplicate device IDs", hw_opt.gpuIdsAvailable.c_str())));
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:    std::vector<int> userGpuTaskAssignment;
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:        userGpuTaskAssignment = parseUserGpuIds(hw_opt.userGpuTaskAssignment);
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:    std::vector<int> gpuIdsToUse;
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:    auto             compatibleGpus = getCompatibleGpus(hwinfo->gpu_info);
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:    if (gpuIdsAvailable.empty())
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:        gpuIdsToUse = compatibleGpus;
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:        for (const auto &availableGpuId : gpuIdsAvailable)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:            bool availableGpuIsCompatible = false;
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:            for (const auto &compatibleGpuId : compatibleGpus)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:                if (availableGpuId == compatibleGpuId)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:                    availableGpuIsCompatible = true;
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:            if (!availableGpuIsCompatible)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:                gmx_fatal(FARGS, "You limited the set of compatible GPUs to a set that included ID #%d, but that ID is not for a compatible GPU. List only compatible GPUs.", availableGpuId);
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:            gpuIdsToUse.push_back(availableGpuId);
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:            if (!compatibleGpus.empty())
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:                        "NOTE: GPU(s) found, but the current simulation can not use GPUs\n"
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:                        "      To use a GPU, set the mdp option: cutoff-scheme = Verlet");
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:        bool useGpuForNonbonded = false;
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:        bool useGpuForPme       = false;
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:            // the number of GPUs to choose the number of ranks.
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:            auto canUseGpuForNonbonded = buildSupportsNonbondedOnGpu(nullptr);
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:            useGpuForNonbonded = decideWhetherToUseGpusForNonbondedWithThreadMpi
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:                    (nonbondedTarget, gpuIdsToUse, userGpuTaskAssignment, emulateGpuNonbonded,
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:                    canUseGpuForNonbonded,
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:                    gpuAccelerationOfNonbondedIsUseful(mdlog, inputrec, GMX_THREAD_MPI),
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:            useGpuForPme = decideWhetherToUseGpusForPmeWithThreadMpi
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:                    (useGpuForNonbonded, pmeTarget, gpuIdsToUse, userGpuTaskAssignment,
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:                                                gpuIdsToUse,
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:                                                useGpuForNonbonded,
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:                                                useGpuForPme,
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:    // Note that when bonded interactions run on a GPU they always run
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:    bool useGpuForNonbonded = false;
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:    bool useGpuForPme       = false;
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:    bool useGpuForBonded    = false;
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:        // It's possible that there are different numbers of GPUs on
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:        bool gpusWereDetected      = hwinfo->ngpu_compatible_tot > 0;
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:        auto canUseGpuForNonbonded = buildSupportsNonbondedOnGpu(nullptr);
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:        useGpuForNonbonded = decideWhetherToUseGpusForNonbonded(nonbondedTarget, userGpuTaskAssignment,
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:                                                                emulateGpuNonbonded,
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:                                                                canUseGpuForNonbonded,
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:                                                                gpuAccelerationOfNonbondedIsUseful(mdlog, inputrec, !GMX_THREAD_MPI),
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:                                                                gpusWereDetected);
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:        useGpuForPme = decideWhetherToUseGpusForPme(useGpuForNonbonded, pmeTarget, userGpuTaskAssignment,
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:                                                    gpusWereDetected);
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:        auto canUseGpuForBonded = buildSupportsGpuBondeds(nullptr) && inputSupportsGpuBondeds(*inputrec, mtop, nullptr);
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:        useGpuForBonded =
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:            decideWhetherToUseGpusForBonded(useGpuForNonbonded, useGpuForPme, usingVerletScheme,
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:                                            bondedTarget, canUseGpuForBonded,
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:                                            domdecOptions.numPmeRanks, gpusWereDetected);
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:        pmeRunMode   = (useGpuForPme ? PmeRunMode::GPU : PmeRunMode::CPU);
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:        if (pmeRunMode == PmeRunMode::GPU)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:        else if (pmeFftTarget == TaskTarget::Gpu)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:            gmx_fatal(FARGS, "Assigning FFTs to GPU requires PME to be assigned to GPU as well. With PME on CPU you should not be using -pmefft.");
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:    if (useGpuForNonbonded && domdecOptions.numPmeRanks < 0)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:        /* With NB GPUs we don't automatically use PME-only CPU ranks. PME ranks can
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:         * improve performance with many threads per GPU, since our OpenMP
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:    if (useGpuForPme)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:            // TODO possibly print a note that one can opt-in for a separate PME GPU rank?
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:            GMX_RELEASE_ASSERT(domdecOptions.numPmeRanks <= 1, "PME GPU decomposition is not supported");
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:                              useGpuForNonbonded || (emulateGpuNonbonded == EmulateGpuNonbonded::Yes), *hwinfo->cpuInfo);
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:    // Note that in general useGpuForNonbonded, etc. can have a value
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:    // that is inconsistent with the presence of actual GPUs on any
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:    // from which we construct gpuTasksOnThisRank.
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:    // GPU, if present and compatible.  This has to be coordinated
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:    auto                 haveGpus = !gpuIdsToUse.empty();
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:    std::vector<GpuTask> gpuTasksOnThisRank;
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:        if (useGpuForNonbonded)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:            // Note that any bonded tasks on a GPU always accompany a
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:            if (haveGpus)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:                gpuTasksOnThisRank.push_back(GpuTask::Nonbonded);
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:            else if (nonbondedTarget == TaskTarget::Gpu)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:                gmx_fatal(FARGS, "Cannot run short-ranged nonbonded interactions on a GPU because there is none detected.");
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:            else if (bondedTarget == TaskTarget::Gpu)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:                gmx_fatal(FARGS, "Cannot run bonded interactions on a GPU because there is none detected.");
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:        if (useGpuForPme)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:            if (haveGpus)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:                gpuTasksOnThisRank.push_back(GpuTask::Pme);
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:            else if (pmeTarget == TaskTarget::Gpu)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:                gmx_fatal(FARGS, "Cannot run PME on a GPU because there is none detected.");
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:    GpuTaskAssignment gpuTaskAssignment;
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:        gpuTaskAssignment = runTaskAssignment(gpuIdsToUse, userGpuTaskAssignment, *hwinfo,
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:                                              mdlog, cr, ms, physicalNodeComm, gpuTasksOnThisRank,
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:                                              useGpuForBonded, pmeRunMode);
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:    check_resource_division_efficiency(hwinfo, !gpuTaskAssignment.empty(), mdrunOptions.ntompOptionIsSet,
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:        auto nbGpuTaskMapping = std::find_if(gpuTaskAssignment.begin(), gpuTaskAssignment.end(),
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:                                             hasTaskType<GpuTask::Nonbonded>);
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:        if (nbGpuTaskMapping != gpuTaskAssignment.end())
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:            int nonbondedDeviceId = nbGpuTaskMapping->deviceId_;
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:            nonbondedDeviceInfo = getDeviceInfo(hwinfo->gpu_info, nonbondedDeviceId);
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:            init_gpu(nonbondedDeviceInfo);
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:                /* When we share GPUs over ranks, we need to know this for the DLB */
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:    PmeGpuProgramStorage pmeGpuProgram;
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:    auto                 pmeGpuTaskMapping     = std::find_if(gpuTaskAssignment.begin(), gpuTaskAssignment.end(), hasTaskType<GpuTask::Pme>);
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:    const bool           thisRankHasPmeGpuTask = (pmeGpuTaskMapping != gpuTaskAssignment.end());
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:    if (thisRankHasPmeGpuTask)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:        pmeDeviceInfo = getDeviceInfo(hwinfo->gpu_info, pmeGpuTaskMapping->deviceId_);
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:        init_gpu(pmeDeviceInfo);
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:        pmeGpuProgram = buildPmeGpuProgram(pmeDeviceInfo);
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:        if (pmeRunMode == PmeRunMode::GPU && !initializedClfftLibrary && isMasterThread)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:                      useGpuForBonded,
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:        mdAtoms = makeMDAtoms(fplog, mtop, *inputrec, thisRankHasPmeGpuTask);
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:        if (globalState && thisRankHasPmeGpuTask)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:            // PME on GPU without DD or on a separate PME rank, and because the local state pointer
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:                                       pmeDeviceInfo, pmeGpuProgram.get(), mdlog);
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:    // before we destroy the GPU context(s) in free_gpu_resources().
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:    // Pinned buffers are associated with contexts in CUDA.
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:    // As soon as we destroy GPU contexts after mdrunner() exits, these lines should go.
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:    mdModules.reset(nullptr);   // destruct force providers here as they might also use the GPU
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:    /* Free GPU memory and set a physical node tMPI barrier (which should eventually go away) */
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:    free_gpu_resources(fr, physicalNodeComm);
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:    free_gpu(nonbondedDeviceInfo);
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp:    free_gpu(pmeDeviceInfo);
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/legacymdrunoptions.cpp:    // which compatible GPUs are available for use, or to select a GPU
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/legacymdrunoptions.cpp:        hw_opt.gpuIdsAvailable       = gpuIdsAvailable;
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/legacymdrunoptions.cpp:        hw_opt.userGpuTaskAssignment = userGpuTaskAssignment;
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/legacymdrunoptions.cpp:        const char *env = getenv("GMX_GPU_ID");
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/legacymdrunoptions.cpp:            if (!hw_opt.gpuIdsAvailable.empty())
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/legacymdrunoptions.cpp:                gmx_fatal(FARGS, "GMX_GPU_ID and -gpu_id can not be used at the same time");
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/legacymdrunoptions.cpp:            hw_opt.gpuIdsAvailable = env;
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/legacymdrunoptions.cpp:        env = getenv("GMX_GPUTASKS");
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/legacymdrunoptions.cpp:            if (!hw_opt.userGpuTaskAssignment.empty())
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/legacymdrunoptions.cpp:                gmx_fatal(FARGS, "GMX_GPUTASKS and -gputasks can not be used at the same time");
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/legacymdrunoptions.cpp:            hw_opt.userGpuTaskAssignment = env;
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/legacymdrunoptions.cpp:        if (!hw_opt.gpuIdsAvailable.empty() && !hw_opt.userGpuTaskAssignment.empty())
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/legacymdrunoptions.cpp:            gmx_fatal(FARGS, "-gpu_id and -gputasks cannot be used at the same time");
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/legacymdrunoptions.cpp.preplumed:    // which compatible GPUs are available for use, or to select a GPU
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/legacymdrunoptions.cpp.preplumed:        hw_opt.gpuIdsAvailable       = gpuIdsAvailable;
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/legacymdrunoptions.cpp.preplumed:        hw_opt.userGpuTaskAssignment = userGpuTaskAssignment;
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/legacymdrunoptions.cpp.preplumed:        const char *env = getenv("GMX_GPU_ID");
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/legacymdrunoptions.cpp.preplumed:            if (!hw_opt.gpuIdsAvailable.empty())
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/legacymdrunoptions.cpp.preplumed:                gmx_fatal(FARGS, "GMX_GPU_ID and -gpu_id can not be used at the same time");
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/legacymdrunoptions.cpp.preplumed:            hw_opt.gpuIdsAvailable = env;
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/legacymdrunoptions.cpp.preplumed:        env = getenv("GMX_GPUTASKS");
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/legacymdrunoptions.cpp.preplumed:            if (!hw_opt.userGpuTaskAssignment.empty())
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/legacymdrunoptions.cpp.preplumed:                gmx_fatal(FARGS, "GMX_GPUTASKS and -gputasks can not be used at the same time");
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/legacymdrunoptions.cpp.preplumed:            hw_opt.userGpuTaskAssignment = env;
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/legacymdrunoptions.cpp.preplumed:        if (!hw_opt.gpuIdsAvailable.empty() && !hw_opt.userGpuTaskAssignment.empty())
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/legacymdrunoptions.cpp.preplumed:            gmx_fatal(FARGS, "-gpu_id and -gputasks cannot be used at the same time");
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:        { nullptr, "auto", "cpu", "gpu", nullptr };
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:        { nullptr, "auto", "cpu", "gpu", nullptr };
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:        { nullptr, "auto", "cpu", "gpu", nullptr };
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:        { nullptr, "auto", "cpu", "gpu", nullptr };
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:        const char       *gpuIdsAvailable       = "";
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:        const char       *userGpuTaskAssignment = "";
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:            { "-gpu_id",  FALSE, etSTR, {&gpuIdsAvailable},
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:              "List of unique GPU device IDs available to use" },
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:            { "-gputasks",  FALSE, etSTR, {&userGpuTaskAssignment},
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:              "List of GPU device IDs, mapping each PP task on each node to a device" },
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:              "Optimize PME load between PP/PME ranks or GPU/CPU (only with the Verlet cut-off scheme)" },
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/legacymdrunoptions.h:        { nullptr, "auto", "cpu", "gpu", nullptr };
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/legacymdrunoptions.h:        { nullptr, "auto", "cpu", "gpu", nullptr };
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/legacymdrunoptions.h:        { nullptr, "auto", "cpu", "gpu", nullptr };
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/legacymdrunoptions.h:        { nullptr, "auto", "cpu", "gpu", nullptr };
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/legacymdrunoptions.h:        const char       *gpuIdsAvailable       = "";
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/legacymdrunoptions.h:        const char       *userGpuTaskAssignment = "";
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/legacymdrunoptions.h:            { "-gpu_id",  FALSE, etSTR, {&gpuIdsAvailable},
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/legacymdrunoptions.h:              "List of unique GPU device IDs available to use" },
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/legacymdrunoptions.h:            { "-gputasks",  FALSE, etSTR, {&userGpuTaskAssignment},
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/legacymdrunoptions.h:              "List of GPU device IDs, mapping each PP task on each node to a device" },
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/legacymdrunoptions.h:              "Optimize PME load between PP/PME ranks or GPU/CPU (only with the Verlet cut-off scheme)" },
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:#include "gromacs/ewald/pme-gpu-program.h"
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:#include "gromacs/gpu_utils/clfftinitializer.h"
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:#include "gromacs/gpu_utils/gpu_utils.h"
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:#include "gromacs/listed-forces/gpubonded.h"
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:#include "gromacs/mdlib/nbnxn_gpu_data_mgmt.h"
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:#include "gromacs/taskassignment/decidegpuusage.h"
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:#include "gromacs/taskassignment/usergpuids.h"
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                  bool                            makeGpuPairList,
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:        ListSetupType      listType  = (makeGpuPairList ? ListSetupType::Gpu : ListSetupType::CpuSimdWhenSupported);
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:        increaseNstlist(fplog, cr, ir, nstlist_cmdline, mtop, box, makeGpuPairList, cpuinfo);
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:/*! \brief Return whether GPU acceleration of nonbondeds is supported with the given settings.
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:static bool gpuAccelerationOfNonbondedIsUseful(const MDLogger   &mdlog,
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:        /* The GPU code does not support more than one energy group.
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:         * If the user requested GPUs explicitly, a fatal error is given later.
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:                .appendText("Multiple energy groups is not implemented for GPUs, falling back to the CPU. "
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:                            "For better performance, run on the GPU without energy groups and then do "
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:    else if (strncmp(optionString, "gpu", 3) == 0)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:        returnValue = TaskTarget::Gpu;
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:    EmulateGpuNonbonded emulateGpuNonbonded = (getenv("GMX_EMULATE_GPU") != nullptr ?
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                               EmulateGpuNonbonded::Yes : EmulateGpuNonbonded::No);
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:    std::vector<int>    gpuIdsAvailable;
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:        gpuIdsAvailable = parseUserGpuIds(hw_opt.gpuIdsAvailable);
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:        // TODO We could put the GPU IDs into a std::map to find
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:        for (size_t i = 0; i != gpuIdsAvailable.size(); ++i)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:            for (size_t j = i+1; j != gpuIdsAvailable.size(); ++j)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:                if (gpuIdsAvailable[i] == gpuIdsAvailable[j])
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:                    GMX_THROW(InvalidInputError(formatString("The string of available GPU device IDs '%s' may not contain duplicate device IDs", hw_opt.gpuIdsAvailable.c_str())));
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:    std::vector<int> userGpuTaskAssignment;
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:        userGpuTaskAssignment = parseUserGpuIds(hw_opt.userGpuTaskAssignment);
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:    std::vector<int> gpuIdsToUse;
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:    auto             compatibleGpus = getCompatibleGpus(hwinfo->gpu_info);
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:    if (gpuIdsAvailable.empty())
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:        gpuIdsToUse = compatibleGpus;
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:        for (const auto &availableGpuId : gpuIdsAvailable)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:            bool availableGpuIsCompatible = false;
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:            for (const auto &compatibleGpuId : compatibleGpus)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:                if (availableGpuId == compatibleGpuId)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:                    availableGpuIsCompatible = true;
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:            if (!availableGpuIsCompatible)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:                gmx_fatal(FARGS, "You limited the set of compatible GPUs to a set that included ID #%d, but that ID is not for a compatible GPU. List only compatible GPUs.", availableGpuId);
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:            gpuIdsToUse.push_back(availableGpuId);
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:            if (!compatibleGpus.empty())
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:                        "NOTE: GPU(s) found, but the current simulation can not use GPUs\n"
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:                        "      To use a GPU, set the mdp option: cutoff-scheme = Verlet");
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:        bool useGpuForNonbonded = false;
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:        bool useGpuForPme       = false;
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:            // the number of GPUs to choose the number of ranks.
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:            auto canUseGpuForNonbonded = buildSupportsNonbondedOnGpu(nullptr);
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:            useGpuForNonbonded = decideWhetherToUseGpusForNonbondedWithThreadMpi
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:                    (nonbondedTarget, gpuIdsToUse, userGpuTaskAssignment, emulateGpuNonbonded,
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:                    canUseGpuForNonbonded,
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:                    gpuAccelerationOfNonbondedIsUseful(mdlog, inputrec, GMX_THREAD_MPI),
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:            useGpuForPme = decideWhetherToUseGpusForPmeWithThreadMpi
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:                    (useGpuForNonbonded, pmeTarget, gpuIdsToUse, userGpuTaskAssignment,
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                                gpuIdsToUse,
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                                useGpuForNonbonded,
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                                useGpuForPme,
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:    // Note that when bonded interactions run on a GPU they always run
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:    bool useGpuForNonbonded = false;
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:    bool useGpuForPme       = false;
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:    bool useGpuForBonded    = false;
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:        // It's possible that there are different numbers of GPUs on
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:        bool gpusWereDetected      = hwinfo->ngpu_compatible_tot > 0;
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:        auto canUseGpuForNonbonded = buildSupportsNonbondedOnGpu(nullptr);
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:        useGpuForNonbonded = decideWhetherToUseGpusForNonbonded(nonbondedTarget, userGpuTaskAssignment,
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                                                emulateGpuNonbonded,
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                                                canUseGpuForNonbonded,
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                                                gpuAccelerationOfNonbondedIsUseful(mdlog, inputrec, !GMX_THREAD_MPI),
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                                                gpusWereDetected);
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:        useGpuForPme = decideWhetherToUseGpusForPme(useGpuForNonbonded, pmeTarget, userGpuTaskAssignment,
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                                    gpusWereDetected);
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:        auto canUseGpuForBonded = buildSupportsGpuBondeds(nullptr) && inputSupportsGpuBondeds(*inputrec, mtop, nullptr);
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:        useGpuForBonded =
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:            decideWhetherToUseGpusForBonded(useGpuForNonbonded, useGpuForPme, usingVerletScheme,
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                            bondedTarget, canUseGpuForBonded,
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                            domdecOptions.numPmeRanks, gpusWereDetected);
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:        pmeRunMode   = (useGpuForPme ? PmeRunMode::GPU : PmeRunMode::CPU);
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:        if (pmeRunMode == PmeRunMode::GPU)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:        else if (pmeFftTarget == TaskTarget::Gpu)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:            gmx_fatal(FARGS, "Assigning FFTs to GPU requires PME to be assigned to GPU as well. With PME on CPU you should not be using -pmefft.");
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:    if (useGpuForNonbonded && domdecOptions.numPmeRanks < 0)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:        /* With NB GPUs we don't automatically use PME-only CPU ranks. PME ranks can
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:         * improve performance with many threads per GPU, since our OpenMP
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:    if (useGpuForPme)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:            // TODO possibly print a note that one can opt-in for a separate PME GPU rank?
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:            GMX_RELEASE_ASSERT(domdecOptions.numPmeRanks <= 1, "PME GPU decomposition is not supported");
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:                              useGpuForNonbonded || (emulateGpuNonbonded == EmulateGpuNonbonded::Yes), *hwinfo->cpuInfo);
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:    // Note that in general useGpuForNonbonded, etc. can have a value
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:    // that is inconsistent with the presence of actual GPUs on any
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:    // from which we construct gpuTasksOnThisRank.
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:    // GPU, if present and compatible.  This has to be coordinated
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:    auto                 haveGpus = !gpuIdsToUse.empty();
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:    std::vector<GpuTask> gpuTasksOnThisRank;
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:        if (useGpuForNonbonded)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:            // Note that any bonded tasks on a GPU always accompany a
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:            if (haveGpus)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:                gpuTasksOnThisRank.push_back(GpuTask::Nonbonded);
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:            else if (nonbondedTarget == TaskTarget::Gpu)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:                gmx_fatal(FARGS, "Cannot run short-ranged nonbonded interactions on a GPU because there is none detected.");
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:            else if (bondedTarget == TaskTarget::Gpu)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:                gmx_fatal(FARGS, "Cannot run bonded interactions on a GPU because there is none detected.");
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:        if (useGpuForPme)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:            if (haveGpus)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:                gpuTasksOnThisRank.push_back(GpuTask::Pme);
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:            else if (pmeTarget == TaskTarget::Gpu)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:                gmx_fatal(FARGS, "Cannot run PME on a GPU because there is none detected.");
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:    GpuTaskAssignment gpuTaskAssignment;
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:        gpuTaskAssignment = runTaskAssignment(gpuIdsToUse, userGpuTaskAssignment, *hwinfo,
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                              mdlog, cr, ms, physicalNodeComm, gpuTasksOnThisRank,
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                              useGpuForBonded, pmeRunMode);
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:    check_resource_division_efficiency(hwinfo, !gpuTaskAssignment.empty(), mdrunOptions.ntompOptionIsSet,
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:        auto nbGpuTaskMapping = std::find_if(gpuTaskAssignment.begin(), gpuTaskAssignment.end(),
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                             hasTaskType<GpuTask::Nonbonded>);
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:        if (nbGpuTaskMapping != gpuTaskAssignment.end())
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:            int nonbondedDeviceId = nbGpuTaskMapping->deviceId_;
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:            nonbondedDeviceInfo = getDeviceInfo(hwinfo->gpu_info, nonbondedDeviceId);
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:            init_gpu(nonbondedDeviceInfo);
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:                /* When we share GPUs over ranks, we need to know this for the DLB */
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:    PmeGpuProgramStorage pmeGpuProgram;
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:    auto                 pmeGpuTaskMapping     = std::find_if(gpuTaskAssignment.begin(), gpuTaskAssignment.end(), hasTaskType<GpuTask::Pme>);
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:    const bool           thisRankHasPmeGpuTask = (pmeGpuTaskMapping != gpuTaskAssignment.end());
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:    if (thisRankHasPmeGpuTask)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:        pmeDeviceInfo = getDeviceInfo(hwinfo->gpu_info, pmeGpuTaskMapping->deviceId_);
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:        init_gpu(pmeDeviceInfo);
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:        pmeGpuProgram = buildPmeGpuProgram(pmeDeviceInfo);
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:        if (pmeRunMode == PmeRunMode::GPU && !initializedClfftLibrary && isMasterThread)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:                      useGpuForBonded,
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:        mdAtoms = makeMDAtoms(fplog, mtop, *inputrec, thisRankHasPmeGpuTask);
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:        if (globalState && thisRankHasPmeGpuTask)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:            // PME on GPU without DD or on a separate PME rank, and because the local state pointer
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                       pmeDeviceInfo, pmeGpuProgram.get(), mdlog);
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:    // before we destroy the GPU context(s) in free_gpu_resources().
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:    // Pinned buffers are associated with contexts in CUDA.
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:    // As soon as we destroy GPU contexts after mdrunner() exits, these lines should go.
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:    mdModules.reset(nullptr);   // destruct force providers here as they might also use the GPU
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:    /* Free GPU memory and set a physical node tMPI barrier (which should eventually go away) */
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:    free_gpu_resources(fr, physicalNodeComm);
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:    free_gpu(nonbondedDeviceInfo);
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/runner.cpp.preplumed:    free_gpu(pmeDeviceInfo);
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/md.cpp.preplumed:#include "gromacs/gpu_utils/gpu_utils.h"
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/md.cpp.preplumed:#include "gromacs/mdlib/nbnxn_gpu_data_mgmt.h"
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/md.cpp.preplumed:    /* PME load balancing data for GPU kernels */
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/md.cpp.preplumed:                         *fr->ic, *fr->nbv->listParams, fr->pmedata, use_GPU(fr->nbv),
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/md.cpp.preplumed:            /* PME grid + cut-off optimization with GPUs or PME nodes */
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/md.cpp.preplumed:                step, step_rel, mdlog, fplog, cr, (use_GPU(fr->nbv) ? fr->nbv : nullptr),
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/md.cpp.preplumed:        pme_loadbal_done(pme_loadbal, fplog, mdlog, use_GPU(fr->nbv));
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/md.cpp:#include "gromacs/gpu_utils/gpu_utils.h"
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/md.cpp:#include "gromacs/mdlib/nbnxn_gpu_data_mgmt.h"
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/md.cpp:    /* PME load balancing data for GPU kernels */
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/md.cpp:                         *fr->ic, *fr->nbv->listParams, fr->pmedata, use_GPU(fr->nbv),
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/md.cpp:            /* PME grid + cut-off optimization with GPUs or PME nodes */
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/md.cpp:                step, step_rel, mdlog, fplog, cr, (use_GPU(fr->nbv) ? fr->nbv : nullptr),
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/mdrun/md.cpp:        pme_loadbal_done(pme_loadbal, fplog, mdlog, use_GPU(fr->nbv));
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt:if (GMX_CLANG_CUDA)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt:    include(gmxClangCudaUtils)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt:set_property(GLOBAL PROPERTY GMX_LIBGROMACS_GPU_IMPL_SOURCES)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt:# TODO Reconsider this, as the CUDA driver API is probably a simpler
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt:function (gmx_compile_cpp_as_cuda)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt:    _gmx_add_files_to_property(GMX_LIBGROMACS_GPU_IMPL_SOURCES ${ARGN})
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt:add_subdirectory(gpu_utils)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt:# Mark some shared GPU implementation files to compile with CUDA if needed
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt:if (GMX_USE_CUDA)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt:    get_property(LIBGROMACS_GPU_IMPL_SOURCES GLOBAL PROPERTY GMX_LIBGROMACS_GPU_IMPL_SOURCES)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt:    set_source_files_properties(${LIBGROMACS_GPU_IMPL_SOURCES} PROPERTIES CUDA_SOURCE_PROPERTY_FORMAT OBJ)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt:# set up CUDA compilation with clang
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt:if (GMX_CLANG_CUDA)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt:        get_source_file_property(_cuda_source_format ${_file} CUDA_SOURCE_PROPERTY_FORMAT)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt:        if ("${_ext}" STREQUAL ".cu" OR _cuda_source_format)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt:            gmx_compile_cuda_file_with_clang(${_file})
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt:if (GMX_USE_CUDA)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt:    # Work around FindCUDA that prevents using target_link_libraries()
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt:    set(CUDA_LIBRARIES PRIVATE ${CUDA_LIBRARIES})
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt:    if (NOT GMX_CLANG_CUDA)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt:        cuda_add_library(libgromacs ${LIBGROMACS_SOURCES})
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt:    target_link_libraries(libgromacs PRIVATE ${CUDA_CUFFT_LIBRARIES})
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt:if (GMX_USE_OPENCL)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt:"An OpenCL build was requested with Visual Studio compiler, but GROMACS
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt:clFFT to help with building for OpenCL, but that clFFT has not yet been
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt:requires. Thus for now, OpenCL is not available with MSVC and the internal
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt:                      ${OpenCL_LIBRARIES}
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt:# Technically, the user could want to do this for an OpenCL build
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt:# using the CUDA runtime, but currently there's no reason to want to
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt:if (INSTALL_CUDART_LIB) #can be set manual by user
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt:    if (GMX_USE_CUDA)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt:        foreach(CUDA_LIB ${CUDA_LIBRARIES})
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt:            string(REGEX MATCH "cudart" IS_CUDART ${CUDA_LIB})
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt:            if(IS_CUDART) #libcuda should not be installed
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt:                file(GLOB CUDA_LIBS ${CUDA_LIB}*)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt:                install(FILES ${CUDA_LIBS} DESTINATION
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt:        message(WARNING "INSTALL_CUDART_LIB only makes sense when configuring for CUDA support")
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt:if(GMX_USE_OPENCL)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt:    file(GLOB OPENCL_INSTALLED_FILES
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt:        gpu_utils/vectype_ops.clh
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt:        gpu_utils/device_utils.clh
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt:    install(FILES ${OPENCL_INSTALLED_FILES}
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt:        DESTINATION ${GMX_INSTALL_OCLDIR}/gromacs/gpu_utils
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt:    file(GLOB OPENCL_INSTALLED_FILES
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt:    install(FILES ${OPENCL_INSTALLED_FILES}
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt:    file(GLOB OPENCL_INSTALLED_FILES
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt:    install(FILES ${OPENCL_INSTALLED_FILES}
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt:    file(GLOB OPENCL_INSTALLED_FILES
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt:    install(FILES ${OPENCL_INSTALLED_FILES}
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt:    file(GLOB OPENCL_INSTALLED_FILES
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt:        ewald/pme-gpu-utils.clh
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt:        ewald/pme-gpu-types.h
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt:    install(FILES ${OPENCL_INSTALLED_FILES}
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt.preplumed:if (GMX_CLANG_CUDA)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt.preplumed:    include(gmxClangCudaUtils)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt.preplumed:set_property(GLOBAL PROPERTY GMX_LIBGROMACS_GPU_IMPL_SOURCES)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt.preplumed:# TODO Reconsider this, as the CUDA driver API is probably a simpler
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt.preplumed:function (gmx_compile_cpp_as_cuda)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt.preplumed:    _gmx_add_files_to_property(GMX_LIBGROMACS_GPU_IMPL_SOURCES ${ARGN})
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt.preplumed:add_subdirectory(gpu_utils)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt.preplumed:# Mark some shared GPU implementation files to compile with CUDA if needed
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt.preplumed:if (GMX_USE_CUDA)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt.preplumed:    get_property(LIBGROMACS_GPU_IMPL_SOURCES GLOBAL PROPERTY GMX_LIBGROMACS_GPU_IMPL_SOURCES)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt.preplumed:    set_source_files_properties(${LIBGROMACS_GPU_IMPL_SOURCES} PROPERTIES CUDA_SOURCE_PROPERTY_FORMAT OBJ)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt.preplumed:# set up CUDA compilation with clang
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt.preplumed:if (GMX_CLANG_CUDA)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt.preplumed:        get_source_file_property(_cuda_source_format ${_file} CUDA_SOURCE_PROPERTY_FORMAT)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt.preplumed:        if ("${_ext}" STREQUAL ".cu" OR _cuda_source_format)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt.preplumed:            gmx_compile_cuda_file_with_clang(${_file})
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt.preplumed:if (GMX_USE_CUDA)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt.preplumed:    # Work around FindCUDA that prevents using target_link_libraries()
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt.preplumed:    set(CUDA_LIBRARIES PRIVATE ${CUDA_LIBRARIES})
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt.preplumed:    if (NOT GMX_CLANG_CUDA)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt.preplumed:        cuda_add_library(libgromacs ${LIBGROMACS_SOURCES})
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt.preplumed:    target_link_libraries(libgromacs PRIVATE ${CUDA_CUFFT_LIBRARIES})
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt.preplumed:if (GMX_USE_OPENCL)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt.preplumed:"An OpenCL build was requested with Visual Studio compiler, but GROMACS
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt.preplumed:clFFT to help with building for OpenCL, but that clFFT has not yet been
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt.preplumed:requires. Thus for now, OpenCL is not available with MSVC and the internal
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt.preplumed:                      ${OpenCL_LIBRARIES}
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt.preplumed:# Technically, the user could want to do this for an OpenCL build
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt.preplumed:# using the CUDA runtime, but currently there's no reason to want to
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt.preplumed:if (INSTALL_CUDART_LIB) #can be set manual by user
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt.preplumed:    if (GMX_USE_CUDA)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt.preplumed:        foreach(CUDA_LIB ${CUDA_LIBRARIES})
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt.preplumed:            string(REGEX MATCH "cudart" IS_CUDART ${CUDA_LIB})
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt.preplumed:            if(IS_CUDART) #libcuda should not be installed
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt.preplumed:                file(GLOB CUDA_LIBS ${CUDA_LIB}*)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt.preplumed:                install(FILES ${CUDA_LIBS} DESTINATION
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt.preplumed:        message(WARNING "INSTALL_CUDART_LIB only makes sense when configuring for CUDA support")
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt.preplumed:if(GMX_USE_OPENCL)
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt.preplumed:    file(GLOB OPENCL_INSTALLED_FILES
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt.preplumed:        gpu_utils/vectype_ops.clh
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt.preplumed:        gpu_utils/device_utils.clh
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt.preplumed:    install(FILES ${OPENCL_INSTALLED_FILES}
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt.preplumed:        DESTINATION ${GMX_INSTALL_OCLDIR}/gromacs/gpu_utils
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt.preplumed:    file(GLOB OPENCL_INSTALLED_FILES
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt.preplumed:    install(FILES ${OPENCL_INSTALLED_FILES}
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt.preplumed:    file(GLOB OPENCL_INSTALLED_FILES
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt.preplumed:    install(FILES ${OPENCL_INSTALLED_FILES}
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt.preplumed:    file(GLOB OPENCL_INSTALLED_FILES
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt.preplumed:    install(FILES ${OPENCL_INSTALLED_FILES}
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt.preplumed:    file(GLOB OPENCL_INSTALLED_FILES
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt.preplumed:        ewald/pme-gpu-utils.clh
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt.preplumed:        ewald/pme-gpu-types.h
GPU/plumed2/patches/gromacs-2019.6.diff/src/gromacs/CMakeLists.txt.preplumed:    install(FILES ${OPENCL_INSTALLED_FILES}
GPU/plumed2/patches/amber18.diff: .NOTPARALLEL: cuda_SPFP cuda_DPFP pmemd.cuda_SPFP$(SFX) pmemd.cuda_DPFP$(SFX)
GPU/plumed2/patches/amber18.diff:-$(BINDIR)/pmemd.cuda_SPFP$(SFX): $(OBJS) cuda_spfp_libs $(EMIL) $(NFE_OBJECTS)
GPU/plumed2/patches/amber18.diff:+$(BINDIR)/pmemd.cuda_SPFP$(SFX): $(OBJS) cuda_spfp_libs $(EMIL) $(NFE_OBJECTS) $(PLUMED_DEPENDENCIES)
GPU/plumed2/patches/amber18.diff:-$(BINDIR)/pmemd.cuda_SPFP.MPI$(SFX): $(OBJS) cuda_spfp_libs $(EMIL) $(NFE_OBJECTS)
GPU/plumed2/patches/amber18.diff:+$(BINDIR)/pmemd.cuda_SPFP.MPI$(SFX): $(OBJS) cuda_spfp_libs $(EMIL) $(NFE_OBJECTS) $(PLUMED_DEPENDENCIES)
GPU/plumed2/patches/amber18.diff:-$(BINDIR)/pmemd.cuda_DPFP$(SFX): $(OBJS) cuda_dpfp_libs $(EMIL) $(NFE_OBJECTS)
GPU/plumed2/patches/amber18.diff:+$(BINDIR)/pmemd.cuda_DPFP$(SFX): $(OBJS) cuda_dpfp_libs $(EMIL) $(NFE_OBJECTS) $(PLUMED_DEPENDENCIES)
GPU/plumed2/patches/amber18.diff:-$(BINDIR)/pmemd.cuda_DPFP.MPI$(SFX): $(OBJS) cuda_dpfp_libs $(EMIL) $(NFE_OBJECTS)
GPU/plumed2/patches/amber18.diff:+$(BINDIR)/pmemd.cuda_DPFP.MPI$(SFX): $(OBJS) cuda_dpfp_libs $(EMIL) $(NFE_OBJECTS) $(PLUMED_DEPENDENCIES)
GPU/plumed2/patches/amber18.diff:-$(BINDIR)/pmemd.cuda_SPXP$(SFX): $(OBJS) cuda_spxp_libs $(EMIL) $(NFE_OBJECTS)
GPU/plumed2/patches/amber18.diff:+$(BINDIR)/pmemd.cuda_SPXP$(SFX): $(OBJS) cuda_spxp_libs $(EMIL) $(NFE_OBJECTS) $(PLUMED_DEPENDENCIES)
GPU/plumed2/patches/amber18.diff:-$(BINDIR)/pmemd.cuda_SPXP.MPI$(SFX): $(OBJS) cuda_spxp_libs $(EMIL) $(NFE_OBJECTS)
GPU/plumed2/patches/amber18.diff:+$(BINDIR)/pmemd.cuda_SPXP.MPI$(SFX): $(OBJS) cuda_spxp_libs $(EMIL) $(NFE_OBJECTS) $(PLUMED_DEPENDENCIES)
GPU/plumed2/patches/amber18.diff:+#ifdef CUDA
GPU/plumed2/patches/amber18.diff:+#ifdef CUDA
GPU/plumed2/patches/amber18.diff:+      call gpu_download_crd(crd)
GPU/plumed2/patches/amber18.diff:+        call gpu_download_frc(frc)
GPU/plumed2/patches/amber18.diff:+#ifdef CUDA
GPU/plumed2/patches/amber18.diff:+        call gpu_upload_frc(frc)
GPU/plumed2/patches/amber18.diff:+        call gpu_upload_frc_add(plumed_frc)
GPU/plumed2/patches/amber18.diff:+#ifdef CUDA
GPU/plumed2/patches/amber18.diff:+        call gpu_download_crd(crd)
GPU/plumed2/patches/amber18.diff:+          call gpu_download_frc(frc)
GPU/plumed2/patches/amber18.diff:+#ifdef CUDA
GPU/plumed2/patches/amber18.diff:+          call gpu_upload_frc(frc)
GPU/plumed2/patches/amber18.diff:+          call gpu_upload_frc_add(plumed_frc)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:#include "gromacs/domdec/gpuhaloexchange.h"
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:#include "gromacs/ewald/pme_gpu_program.h"
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:#include "gromacs/ewald/pme_pp_comm_gpu.h"
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:#include "gromacs/gpu_utils/gpu_utils.h"
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:#include "gromacs/listed_forces/gpubonded.h"
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:#include "gromacs/mdtypes/state_propagator_data_gpu.h"
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:#include "gromacs/nbnxm/gpu_data_mgmt.h"
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:#include "gromacs/taskassignment/decidegpuusage.h"
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:#include "gromacs/taskassignment/usergpuids.h"
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:#include "gromacs/timing/gpu_timing.h"
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp: * the GPU communication flags are set to false in non-tMPI and non-CUDA builds.
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp: * \param[in]  useGpuForNonbonded   True if the nonbonded task is offloaded in this run.
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:                                                         const bool           useGpuForNonbonded,
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:    devFlags.enableGpuBufferOps = (getenv("GMX_USE_GPU_BUFFER_OPS") != nullptr)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:                                  && (GMX_GPU == GMX_GPU_CUDA) && useGpuForNonbonded;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:    devFlags.forceGpuUpdateDefault = (getenv("GMX_FORCE_UPDATE_DEFAULT_GPU") != nullptr);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:    devFlags.enableGpuHaloExchange =
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:            (getenv("GMX_GPU_DD_COMMS") != nullptr && GMX_THREAD_MPI && (GMX_GPU == GMX_GPU_CUDA));
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:    devFlags.enableGpuPmePPComm =
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:            (getenv("GMX_GPU_PME_PP_COMMS") != nullptr && GMX_THREAD_MPI && (GMX_GPU == GMX_GPU_CUDA));
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:    if (devFlags.enableGpuBufferOps)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:                        "This run uses the 'GPU buffer ops' feature, enabled by the "
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:                        "GMX_USE_GPU_BUFFER_OPS environment variable.");
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:    if (devFlags.forceGpuUpdateDefault)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:                        "This run will default to '-update gpu' as requested by the "
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:                        "GMX_FORCE_UPDATE_DEFAULT_GPU environment variable. GPU update with domain "
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:    if (devFlags.enableGpuHaloExchange)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:        if (useGpuForNonbonded)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:            if (!devFlags.enableGpuBufferOps)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:                                "Enabling GPU buffer operations required by GMX_GPU_DD_COMMS "
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:                                "(equivalent with GMX_USE_GPU_BUFFER_OPS=1).");
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:                devFlags.enableGpuBufferOps = true;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:                            "This run uses the 'GPU halo exchange' feature, enabled by the "
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:                            "GMX_GPU_DD_COMMS environment variable.");
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:                            "GMX_GPU_DD_COMMS environment variable detected, but the 'GPU "
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:            devFlags.enableGpuHaloExchange = false;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:    if (devFlags.enableGpuPmePPComm)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:        if (pmeRunMode == PmeRunMode::GPU)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:                            "This run uses the 'GPU PME-PP communications' feature, enabled "
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:                            "by the GMX_GPU_PME_PP_COMMS environment variable.");
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:                        "PME FFT and gather are not offloaded to the GPU (PME is running in mixed "
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:                clarification = "PME is not offloaded to the GPU.";
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:                            "GMX_GPU_PME_PP_COMMS environment variable detected, but the "
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:                            "'GPU PME-PP communications' feature was not enabled as "
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:            devFlags.enableGpuPmePPComm = false;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:                                  bool                makeGpuPairList,
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:                (makeGpuPairList ? ListSetupType::Gpu : ListSetupType::CpuSimdWhenSupported);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:        increaseNstlist(fplog, cr, ir, nstlist_cmdline, mtop, box, makeGpuPairList, cpuinfo);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:/*! \brief Return whether GPU acceleration of nonbondeds is supported with the given settings.
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:static bool gpuAccelerationOfNonbondedIsUseful(const MDLogger& mdlog, const t_inputrec& ir, bool issueWarning)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:    bool        gpuIsUseful = true;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:        /* The GPU code does not support more than one energy group.
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:         * If the user requested GPUs explicitly, a fatal error is given later.
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:        gpuIsUseful = false;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:                "Multiple energy groups is not implemented for GPUs, falling back to the CPU. "
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:                "For better performance, run on the GPU without energy groups and then do "
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:        gpuIsUseful = false;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:        warning     = "TPI is not implemented for GPUs.";
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:    if (!gpuIsUseful && issueWarning)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:    return gpuIsUseful;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:    else if (strncmp(optionString, "gpu", 3) == 0)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:        returnValue = TaskTarget::Gpu;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:        auto nbnxn_gpu_timings =
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:                (nbv != nullptr && nbv->useGpu()) ? Nbnxm::gpu_get_timings(nbv->gpu_nbv) : nullptr;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:        gmx_wallclock_gpu_pme_t pme_gpu_timings = {};
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:        if (pme_gpu_task_enabled(pme))
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:            pme_gpu_get_timings(pme, &pme_gpu_timings);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:                        elapsed_time_over_all_ranks, wcycle, cycle_sum, nbnxn_gpu_timings,
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:                        &pme_gpu_timings);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:    EmulateGpuNonbonded emulateGpuNonbonded =
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:            (getenv("GMX_EMULATE_GPU") != nullptr ? EmulateGpuNonbonded::Yes : EmulateGpuNonbonded::No);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:    std::vector<int> userGpuTaskAssignment;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:        userGpuTaskAssignment = parseUserTaskAssignmentString(hw_opt.userGpuTaskAssignment);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:    std::vector<int> gpuIdsToUse = makeGpuIdsToUse(hwinfo->gpu_info, hw_opt.gpuIdsAvailable);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:        bool useGpuForNonbonded = false;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:        bool useGpuForPme       = false;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:            // the number of GPUs to choose the number of ranks.
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:            auto canUseGpuForNonbonded = buildSupportsNonbondedOnGpu(nullptr);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:            useGpuForNonbonded         = decideWhetherToUseGpusForNonbondedWithThreadMpi(
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:                    nonbondedTarget, gpuIdsToUse, userGpuTaskAssignment, emulateGpuNonbonded,
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:                    canUseGpuForNonbonded,
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:                    gpuAccelerationOfNonbondedIsUseful(mdlog, *inputrec, GMX_THREAD_MPI),
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:            useGpuForPme = decideWhetherToUseGpusForPmeWithThreadMpi(
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:                    useGpuForNonbonded, pmeTarget, gpuIdsToUse, userGpuTaskAssignment, *hwinfo,
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:        hw_opt.nthreads_tmpi = get_nthreads_mpi(hwinfo, &hw_opt, gpuIdsToUse, useGpuForNonbonded,
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:                                                useGpuForPme, inputrec, &mtop, mdlog, doMembed);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:    // Note that when bonded interactions run on a GPU they always run
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:    bool useGpuForNonbonded = false;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:    bool useGpuForPme       = false;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:    bool useGpuForBonded    = false;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:    bool useGpuForUpdate    = false;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:    bool gpusWereDetected   = hwinfo->ngpu_compatible_tot > 0;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:        // It's possible that there are different numbers of GPUs on
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:        auto canUseGpuForNonbonded = buildSupportsNonbondedOnGpu(nullptr);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:        useGpuForNonbonded         = decideWhetherToUseGpusForNonbonded(
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:                nonbondedTarget, userGpuTaskAssignment, emulateGpuNonbonded, canUseGpuForNonbonded,
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:                gpuAccelerationOfNonbondedIsUseful(mdlog, *inputrec, !GMX_THREAD_MPI), gpusWereDetected);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:        useGpuForPme = decideWhetherToUseGpusForPme(
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:                useGpuForNonbonded, pmeTarget, userGpuTaskAssignment, *hwinfo, *inputrec, mtop,
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:                cr->nnodes, domdecOptions.numPmeRanks, gpusWereDetected);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:        auto canUseGpuForBonded = buildSupportsGpuBondeds(nullptr)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:                                  && inputSupportsGpuBondeds(*inputrec, mtop, nullptr);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:        useGpuForBonded = decideWhetherToUseGpusForBonded(
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:                useGpuForNonbonded, useGpuForPme, bondedTarget, canUseGpuForBonded,
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:                domdecOptions.numPmeRanks, gpusWereDetected);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:    const PmeRunMode pmeRunMode = determinePmeRunMode(useGpuForPme, pmeFftTarget, *inputrec);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:            manageDevelopmentFeatures(mdlog, useGpuForNonbonded, pmeRunMode);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:    if (useGpuForNonbonded && domdecOptions.numPmeRanks < 0)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:        /* With NB GPUs we don't automatically use PME-only CPU ranks. PME ranks can
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:         * improve performance with many threads per GPU, since our OpenMP
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:    if (useGpuForPme)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:            // TODO possibly print a note that one can opt-in for a separate PME GPU rank?
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:                               "PME GPU decomposition is not supported");
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:                          useGpuForNonbonded || (emulateGpuNonbonded == EmulateGpuNonbonded::Yes),
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:    const bool prefer1DAnd1PulseDD = (devFlags.enableGpuHaloExchange && useGpuForNonbonded);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:    GpuTaskAssignmentsBuilder gpuTaskAssignmentsBuilder;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:    GpuTaskAssignments        gpuTaskAssignments = gpuTaskAssignmentsBuilder.build(
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:            gpuIdsToUse, userGpuTaskAssignment, *hwinfo, communicator, physicalNodeComm,
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:            nonbondedTarget, pmeTarget, bondedTarget, updateTarget, useGpuForNonbonded,
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:            useGpuForPme, thisRankHasDuty(cr, DUTY_PP),
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:    gmx_device_info_t* nonbondedDeviceInfo = gpuTaskAssignments.initNonbondedDevice(cr);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:    gmx_device_info_t* pmeDeviceInfo       = gpuTaskAssignments.initPmeDevice();
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:    // TODO Initialize GPU streams here.
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:        // TODO Pass the GPU streams to ddBuilder to use in buffer
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:    // The GPU update is decided here because we need to know whether the constraints or
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:    // defined). This is only known after DD is initialized, hence decision on using GPU
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:        useGpuForUpdate = decideWhetherToUseGpuForUpdate(
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:                useGpuForNonbonded, updateTarget, gpusWereDetected, *inputrec, mtop,
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:    gpuTaskAssignments.reportGpuUsage(mdlog, printHostName, useGpuForBonded, pmeRunMode, useGpuForUpdate);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:    if (!userGpuTaskAssignment.empty())
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:        gpuTaskAssignments.logPerformanceHints(mdlog, ssize(gpuIdsToUse));
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:    check_resource_division_efficiency(hwinfo, gpuTaskAssignments.thisRankHasAnyGpuTask(),
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:    // Enable Peer access between GPUs where available
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:    // any of the GPU communication features are active.
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:        && (devFlags.enableGpuHaloExchange || devFlags.enableGpuPmePPComm))
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:        setupGpuDevicePeerAccess(gpuIdsToUse, mdlog);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:    const bool                   thisRankHasPmeGpuTask = gpuTaskAssignments.thisRankHasPmeGpuTask();
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:                      nonbondedDeviceInfo, useGpuForBonded,
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:                      pmeRunMode == PmeRunMode::GPU && !thisRankHasDuty(cr, DUTY_PME), pforce, wcycle);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:        // TODO remove need to pass local stream into GPU halo exchange - Redmine #3093
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:            GMX_RELEASE_ASSERT(devFlags.enableGpuBufferOps,
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:                               "Must use GMX_USE_GPU_BUFFER_OPS=1 to use GMX_GPU_DD_COMMS=1");
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:                    Nbnxm::gpu_get_command_stream(fr->nbv->gpu_nbv, InteractionLocality::Local);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:                    Nbnxm::gpu_get_command_stream(fr->nbv->gpu_nbv, InteractionLocality::NonLocal);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:                            "NOTE: This run uses the 'GPU halo exchange' feature, enabled by the "
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:                            "GMX_GPU_DD_COMMS environment variable.");
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:            cr->dd->gpuHaloExchange = std::make_unique<GpuHaloExchange>(
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:        mdAtoms = makeMDAtoms(fplog, mtop, *inputrec, thisRankHasPmeGpuTask);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:        if (globalState && thisRankHasPmeGpuTask)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:            // PME on GPU without DD or on a separate PME rank, and because the local state pointer
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:    PmeGpuProgramStorage pmeGpuProgram;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:    if (thisRankHasPmeGpuTask)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:        pmeGpuProgram = buildPmeGpuProgram(pmeDeviceInfo);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:                                       nullptr, pmeDeviceInfo, pmeGpuProgram.get(), mdlog);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:                useGpuForNonbonded, pmeRunMode, useGpuForBonded, useGpuForUpdate,
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:                devFlags.enableGpuBufferOps, devFlags.enableGpuHaloExchange,
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:                devFlags.enableGpuPmePPComm, haveEwaldSurfaceContribution(*inputrec));
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:        std::unique_ptr<gmx::StatePropagatorDataGpu> stateGpu;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:        if (gpusWereDetected
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:            && ((useGpuForPme && thisRankHasDuty(cr, DUTY_PME))
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:                || runScheduleWork.simulationWork.useGpuBufferOps))
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:            const void* pmeStream = pme_gpu_get_device_stream(fr->pmedata);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:                    fr->nbv->gpu_nbv != nullptr
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:                            ? Nbnxm::gpu_get_command_stream(fr->nbv->gpu_nbv, InteractionLocality::Local)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:                    fr->nbv->gpu_nbv != nullptr
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:                            ? Nbnxm::gpu_get_command_stream(fr->nbv->gpu_nbv, InteractionLocality::NonLocal)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:            const void*        deviceContext = pme_gpu_get_device_context(fr->pmedata);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:            const int          paddingSize   = pme_gpu_get_padding_size(fr->pmedata);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:            GpuApiCallBehavior transferKind = (inputrec->eI == eiMD && !doRerun && !useModularSimulator)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:                                                      ? GpuApiCallBehavior::Async
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:                                                      : GpuApiCallBehavior::Sync;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:            stateGpu = std::make_unique<gmx::StatePropagatorDataGpu>(
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:            fr->stateGpu = stateGpu.get();
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:        if (fr->pmePpCommGpu)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:            // destroy object since it is no longer required. (This needs to be done while the GPU context still exists.)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:            fr->pmePpCommGpu.reset();
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:    // before we destroy the GPU context(s) in free_gpu_resources().
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:    // Pinned buffers are associated with contexts in CUDA.
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:    // As soon as we destroy GPU contexts after mdrunner() exits, these lines should go.
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:    mdModules_.reset(nullptr); // destruct force providers here as they might also use the GPU
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:    /* Free GPU memory and set a physical node tMPI barrier (which should eventually go away) */
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:    free_gpu_resources(fr, physicalNodeComm, hwinfo->gpu_info);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:    free_gpu(nonbondedDeviceInfo);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp:    free_gpu(pmeDeviceInfo);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/legacymdrunoptions.cpp:    // which compatible GPUs are available for use, or to select a GPU
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/legacymdrunoptions.cpp:        hw_opt.gpuIdsAvailable       = gpuIdsAvailable;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/legacymdrunoptions.cpp:        hw_opt.userGpuTaskAssignment = userGpuTaskAssignment;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/legacymdrunoptions.cpp:        const char* env = getenv("GMX_GPU_ID");
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/legacymdrunoptions.cpp:            if (!hw_opt.gpuIdsAvailable.empty())
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/legacymdrunoptions.cpp:                gmx_fatal(FARGS, "GMX_GPU_ID and -gpu_id can not be used at the same time");
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/legacymdrunoptions.cpp:            hw_opt.gpuIdsAvailable = env;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/legacymdrunoptions.cpp:        env = getenv("GMX_GPUTASKS");
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/legacymdrunoptions.cpp:            if (!hw_opt.userGpuTaskAssignment.empty())
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/legacymdrunoptions.cpp:                gmx_fatal(FARGS, "GMX_GPUTASKS and -gputasks can not be used at the same time");
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/legacymdrunoptions.cpp:            hw_opt.userGpuTaskAssignment = env;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/legacymdrunoptions.cpp:        if (!hw_opt.gpuIdsAvailable.empty() && !hw_opt.userGpuTaskAssignment.empty())
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/legacymdrunoptions.cpp:            gmx_fatal(FARGS, "-gpu_id and -gputasks cannot be used at the same time");
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/legacymdrunoptions.cpp.preplumed:    // which compatible GPUs are available for use, or to select a GPU
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/legacymdrunoptions.cpp.preplumed:        hw_opt.gpuIdsAvailable       = gpuIdsAvailable;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/legacymdrunoptions.cpp.preplumed:        hw_opt.userGpuTaskAssignment = userGpuTaskAssignment;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/legacymdrunoptions.cpp.preplumed:        const char* env = getenv("GMX_GPU_ID");
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/legacymdrunoptions.cpp.preplumed:            if (!hw_opt.gpuIdsAvailable.empty())
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/legacymdrunoptions.cpp.preplumed:                gmx_fatal(FARGS, "GMX_GPU_ID and -gpu_id can not be used at the same time");
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/legacymdrunoptions.cpp.preplumed:            hw_opt.gpuIdsAvailable = env;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/legacymdrunoptions.cpp.preplumed:        env = getenv("GMX_GPUTASKS");
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/legacymdrunoptions.cpp.preplumed:            if (!hw_opt.userGpuTaskAssignment.empty())
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/legacymdrunoptions.cpp.preplumed:                gmx_fatal(FARGS, "GMX_GPUTASKS and -gputasks can not be used at the same time");
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/legacymdrunoptions.cpp.preplumed:            hw_opt.userGpuTaskAssignment = env;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/legacymdrunoptions.cpp.preplumed:        if (!hw_opt.gpuIdsAvailable.empty() && !hw_opt.userGpuTaskAssignment.empty())
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/legacymdrunoptions.cpp.preplumed:            gmx_fatal(FARGS, "-gpu_id and -gputasks cannot be used at the same time");
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:    const char* nbpu_opt_choices[5]    = { nullptr, "auto", "cpu", "gpu", nullptr };
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:    const char* pme_opt_choices[5]     = { nullptr, "auto", "cpu", "gpu", nullptr };
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:    const char* pme_fft_opt_choices[5] = { nullptr, "auto", "cpu", "gpu", nullptr };
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:    const char* bonded_opt_choices[5]  = { nullptr, "auto", "cpu", "gpu", nullptr };
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:    const char* update_opt_choices[5]  = { nullptr, "auto", "cpu", "gpu", nullptr };
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:    const char* gpuIdsAvailable        = "";
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:    const char* userGpuTaskAssignment  = "";
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:        { "-gpu_id",
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:          { &gpuIdsAvailable },
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:          "List of unique GPU device IDs available to use" },
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:        { "-gputasks",
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:          { &userGpuTaskAssignment },
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:          "List of GPU device IDs, mapping each PP task on each node to a device" },
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:          "Optimize PME load between PP/PME ranks or GPU/CPU" },
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/legacymdrunoptions.h:    const char* nbpu_opt_choices[5]    = { nullptr, "auto", "cpu", "gpu", nullptr };
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/legacymdrunoptions.h:    const char* pme_opt_choices[5]     = { nullptr, "auto", "cpu", "gpu", nullptr };
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/legacymdrunoptions.h:    const char* pme_fft_opt_choices[5] = { nullptr, "auto", "cpu", "gpu", nullptr };
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/legacymdrunoptions.h:    const char* bonded_opt_choices[5]  = { nullptr, "auto", "cpu", "gpu", nullptr };
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/legacymdrunoptions.h:    const char* update_opt_choices[5]  = { nullptr, "auto", "cpu", "gpu", nullptr };
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/legacymdrunoptions.h:    const char* gpuIdsAvailable        = "";
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/legacymdrunoptions.h:    const char* userGpuTaskAssignment  = "";
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/legacymdrunoptions.h:        { "-gpu_id",
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/legacymdrunoptions.h:          { &gpuIdsAvailable },
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/legacymdrunoptions.h:          "List of unique GPU device IDs available to use" },
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/legacymdrunoptions.h:        { "-gputasks",
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/legacymdrunoptions.h:          { &userGpuTaskAssignment },
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/legacymdrunoptions.h:          "List of GPU device IDs, mapping each PP task on each node to a device" },
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/legacymdrunoptions.h:          "Optimize PME load between PP/PME ranks or GPU/CPU" },
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:#include "gromacs/domdec/gpuhaloexchange.h"
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:#include "gromacs/ewald/pme_gpu_program.h"
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:#include "gromacs/ewald/pme_pp_comm_gpu.h"
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:#include "gromacs/gpu_utils/gpu_utils.h"
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:#include "gromacs/listed_forces/gpubonded.h"
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:#include "gromacs/mdtypes/state_propagator_data_gpu.h"
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:#include "gromacs/nbnxm/gpu_data_mgmt.h"
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:#include "gromacs/taskassignment/decidegpuusage.h"
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:#include "gromacs/taskassignment/usergpuids.h"
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:#include "gromacs/timing/gpu_timing.h"
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed: * the GPU communication flags are set to false in non-tMPI and non-CUDA builds.
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed: * \param[in]  useGpuForNonbonded   True if the nonbonded task is offloaded in this run.
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                                         const bool           useGpuForNonbonded,
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:    devFlags.enableGpuBufferOps = (getenv("GMX_USE_GPU_BUFFER_OPS") != nullptr)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                  && (GMX_GPU == GMX_GPU_CUDA) && useGpuForNonbonded;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:    devFlags.forceGpuUpdateDefault = (getenv("GMX_FORCE_UPDATE_DEFAULT_GPU") != nullptr);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:    devFlags.enableGpuHaloExchange =
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:            (getenv("GMX_GPU_DD_COMMS") != nullptr && GMX_THREAD_MPI && (GMX_GPU == GMX_GPU_CUDA));
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:    devFlags.enableGpuPmePPComm =
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:            (getenv("GMX_GPU_PME_PP_COMMS") != nullptr && GMX_THREAD_MPI && (GMX_GPU == GMX_GPU_CUDA));
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:    if (devFlags.enableGpuBufferOps)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:                        "This run uses the 'GPU buffer ops' feature, enabled by the "
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:                        "GMX_USE_GPU_BUFFER_OPS environment variable.");
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:    if (devFlags.forceGpuUpdateDefault)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:                        "This run will default to '-update gpu' as requested by the "
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:                        "GMX_FORCE_UPDATE_DEFAULT_GPU environment variable. GPU update with domain "
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:    if (devFlags.enableGpuHaloExchange)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:        if (useGpuForNonbonded)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:            if (!devFlags.enableGpuBufferOps)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                "Enabling GPU buffer operations required by GMX_GPU_DD_COMMS "
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                "(equivalent with GMX_USE_GPU_BUFFER_OPS=1).");
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:                devFlags.enableGpuBufferOps = true;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:                            "This run uses the 'GPU halo exchange' feature, enabled by the "
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:                            "GMX_GPU_DD_COMMS environment variable.");
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:                            "GMX_GPU_DD_COMMS environment variable detected, but the 'GPU "
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:            devFlags.enableGpuHaloExchange = false;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:    if (devFlags.enableGpuPmePPComm)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:        if (pmeRunMode == PmeRunMode::GPU)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:                            "This run uses the 'GPU PME-PP communications' feature, enabled "
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:                            "by the GMX_GPU_PME_PP_COMMS environment variable.");
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:                        "PME FFT and gather are not offloaded to the GPU (PME is running in mixed "
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:                clarification = "PME is not offloaded to the GPU.";
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:                            "GMX_GPU_PME_PP_COMMS environment variable detected, but the "
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:                            "'GPU PME-PP communications' feature was not enabled as "
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:            devFlags.enableGpuPmePPComm = false;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                  bool                makeGpuPairList,
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:                (makeGpuPairList ? ListSetupType::Gpu : ListSetupType::CpuSimdWhenSupported);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:        increaseNstlist(fplog, cr, ir, nstlist_cmdline, mtop, box, makeGpuPairList, cpuinfo);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:/*! \brief Return whether GPU acceleration of nonbondeds is supported with the given settings.
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:static bool gpuAccelerationOfNonbondedIsUseful(const MDLogger& mdlog, const t_inputrec& ir, bool issueWarning)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:    bool        gpuIsUseful = true;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:        /* The GPU code does not support more than one energy group.
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:         * If the user requested GPUs explicitly, a fatal error is given later.
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:        gpuIsUseful = false;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:                "Multiple energy groups is not implemented for GPUs, falling back to the CPU. "
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:                "For better performance, run on the GPU without energy groups and then do "
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:        gpuIsUseful = false;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:        warning     = "TPI is not implemented for GPUs.";
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:    if (!gpuIsUseful && issueWarning)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:    return gpuIsUseful;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:    else if (strncmp(optionString, "gpu", 3) == 0)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:        returnValue = TaskTarget::Gpu;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:        auto nbnxn_gpu_timings =
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:                (nbv != nullptr && nbv->useGpu()) ? Nbnxm::gpu_get_timings(nbv->gpu_nbv) : nullptr;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:        gmx_wallclock_gpu_pme_t pme_gpu_timings = {};
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:        if (pme_gpu_task_enabled(pme))
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:            pme_gpu_get_timings(pme, &pme_gpu_timings);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:                        elapsed_time_over_all_ranks, wcycle, cycle_sum, nbnxn_gpu_timings,
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:                        &pme_gpu_timings);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:    EmulateGpuNonbonded emulateGpuNonbonded =
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:            (getenv("GMX_EMULATE_GPU") != nullptr ? EmulateGpuNonbonded::Yes : EmulateGpuNonbonded::No);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:    std::vector<int> userGpuTaskAssignment;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:        userGpuTaskAssignment = parseUserTaskAssignmentString(hw_opt.userGpuTaskAssignment);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:    std::vector<int> gpuIdsToUse = makeGpuIdsToUse(hwinfo->gpu_info, hw_opt.gpuIdsAvailable);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:        bool useGpuForNonbonded = false;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:        bool useGpuForPme       = false;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:            // the number of GPUs to choose the number of ranks.
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:            auto canUseGpuForNonbonded = buildSupportsNonbondedOnGpu(nullptr);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:            useGpuForNonbonded         = decideWhetherToUseGpusForNonbondedWithThreadMpi(
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:                    nonbondedTarget, gpuIdsToUse, userGpuTaskAssignment, emulateGpuNonbonded,
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:                    canUseGpuForNonbonded,
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:                    gpuAccelerationOfNonbondedIsUseful(mdlog, *inputrec, GMX_THREAD_MPI),
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:            useGpuForPme = decideWhetherToUseGpusForPmeWithThreadMpi(
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:                    useGpuForNonbonded, pmeTarget, gpuIdsToUse, userGpuTaskAssignment, *hwinfo,
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:        hw_opt.nthreads_tmpi = get_nthreads_mpi(hwinfo, &hw_opt, gpuIdsToUse, useGpuForNonbonded,
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                                useGpuForPme, inputrec, &mtop, mdlog, doMembed);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:    // Note that when bonded interactions run on a GPU they always run
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:    bool useGpuForNonbonded = false;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:    bool useGpuForPme       = false;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:    bool useGpuForBonded    = false;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:    bool useGpuForUpdate    = false;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:    bool gpusWereDetected   = hwinfo->ngpu_compatible_tot > 0;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:        // It's possible that there are different numbers of GPUs on
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:        auto canUseGpuForNonbonded = buildSupportsNonbondedOnGpu(nullptr);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:        useGpuForNonbonded         = decideWhetherToUseGpusForNonbonded(
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:                nonbondedTarget, userGpuTaskAssignment, emulateGpuNonbonded, canUseGpuForNonbonded,
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:                gpuAccelerationOfNonbondedIsUseful(mdlog, *inputrec, !GMX_THREAD_MPI), gpusWereDetected);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:        useGpuForPme = decideWhetherToUseGpusForPme(
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:                useGpuForNonbonded, pmeTarget, userGpuTaskAssignment, *hwinfo, *inputrec, mtop,
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:                cr->nnodes, domdecOptions.numPmeRanks, gpusWereDetected);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:        auto canUseGpuForBonded = buildSupportsGpuBondeds(nullptr)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                  && inputSupportsGpuBondeds(*inputrec, mtop, nullptr);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:        useGpuForBonded = decideWhetherToUseGpusForBonded(
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:                useGpuForNonbonded, useGpuForPme, bondedTarget, canUseGpuForBonded,
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:                domdecOptions.numPmeRanks, gpusWereDetected);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:    const PmeRunMode pmeRunMode = determinePmeRunMode(useGpuForPme, pmeFftTarget, *inputrec);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:            manageDevelopmentFeatures(mdlog, useGpuForNonbonded, pmeRunMode);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:    if (useGpuForNonbonded && domdecOptions.numPmeRanks < 0)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:        /* With NB GPUs we don't automatically use PME-only CPU ranks. PME ranks can
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:         * improve performance with many threads per GPU, since our OpenMP
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:    if (useGpuForPme)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:            // TODO possibly print a note that one can opt-in for a separate PME GPU rank?
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:                               "PME GPU decomposition is not supported");
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:                          useGpuForNonbonded || (emulateGpuNonbonded == EmulateGpuNonbonded::Yes),
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:    const bool prefer1DAnd1PulseDD = (devFlags.enableGpuHaloExchange && useGpuForNonbonded);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:    GpuTaskAssignmentsBuilder gpuTaskAssignmentsBuilder;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:    GpuTaskAssignments        gpuTaskAssignments = gpuTaskAssignmentsBuilder.build(
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:            gpuIdsToUse, userGpuTaskAssignment, *hwinfo, communicator, physicalNodeComm,
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:            nonbondedTarget, pmeTarget, bondedTarget, updateTarget, useGpuForNonbonded,
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:            useGpuForPme, thisRankHasDuty(cr, DUTY_PP),
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:    gmx_device_info_t* nonbondedDeviceInfo = gpuTaskAssignments.initNonbondedDevice(cr);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:    gmx_device_info_t* pmeDeviceInfo       = gpuTaskAssignments.initPmeDevice();
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:    // TODO Initialize GPU streams here.
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:        // TODO Pass the GPU streams to ddBuilder to use in buffer
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:    // The GPU update is decided here because we need to know whether the constraints or
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:    // defined). This is only known after DD is initialized, hence decision on using GPU
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:        useGpuForUpdate = decideWhetherToUseGpuForUpdate(
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:                useGpuForNonbonded, updateTarget, gpusWereDetected, *inputrec, mtop,
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:    gpuTaskAssignments.reportGpuUsage(mdlog, printHostName, useGpuForBonded, pmeRunMode, useGpuForUpdate);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:    if (!userGpuTaskAssignment.empty())
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:        gpuTaskAssignments.logPerformanceHints(mdlog, ssize(gpuIdsToUse));
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:    check_resource_division_efficiency(hwinfo, gpuTaskAssignments.thisRankHasAnyGpuTask(),
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:    // Enable Peer access between GPUs where available
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:    // any of the GPU communication features are active.
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:        && (devFlags.enableGpuHaloExchange || devFlags.enableGpuPmePPComm))
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:        setupGpuDevicePeerAccess(gpuIdsToUse, mdlog);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:    const bool                   thisRankHasPmeGpuTask = gpuTaskAssignments.thisRankHasPmeGpuTask();
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:                      nonbondedDeviceInfo, useGpuForBonded,
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:                      pmeRunMode == PmeRunMode::GPU && !thisRankHasDuty(cr, DUTY_PME), pforce, wcycle);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:        // TODO remove need to pass local stream into GPU halo exchange - Redmine #3093
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:            GMX_RELEASE_ASSERT(devFlags.enableGpuBufferOps,
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:                               "Must use GMX_USE_GPU_BUFFER_OPS=1 to use GMX_GPU_DD_COMMS=1");
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:                    Nbnxm::gpu_get_command_stream(fr->nbv->gpu_nbv, InteractionLocality::Local);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:                    Nbnxm::gpu_get_command_stream(fr->nbv->gpu_nbv, InteractionLocality::NonLocal);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:                            "NOTE: This run uses the 'GPU halo exchange' feature, enabled by the "
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:                            "GMX_GPU_DD_COMMS environment variable.");
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:            cr->dd->gpuHaloExchange = std::make_unique<GpuHaloExchange>(
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:        mdAtoms = makeMDAtoms(fplog, mtop, *inputrec, thisRankHasPmeGpuTask);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:        if (globalState && thisRankHasPmeGpuTask)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:            // PME on GPU without DD or on a separate PME rank, and because the local state pointer
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:    PmeGpuProgramStorage pmeGpuProgram;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:    if (thisRankHasPmeGpuTask)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:        pmeGpuProgram = buildPmeGpuProgram(pmeDeviceInfo);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                       nullptr, pmeDeviceInfo, pmeGpuProgram.get(), mdlog);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:                useGpuForNonbonded, pmeRunMode, useGpuForBonded, useGpuForUpdate,
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:                devFlags.enableGpuBufferOps, devFlags.enableGpuHaloExchange,
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:                devFlags.enableGpuPmePPComm, haveEwaldSurfaceContribution(*inputrec));
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:        std::unique_ptr<gmx::StatePropagatorDataGpu> stateGpu;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:        if (gpusWereDetected
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:            && ((useGpuForPme && thisRankHasDuty(cr, DUTY_PME))
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:                || runScheduleWork.simulationWork.useGpuBufferOps))
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:            const void* pmeStream = pme_gpu_get_device_stream(fr->pmedata);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:                    fr->nbv->gpu_nbv != nullptr
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:                            ? Nbnxm::gpu_get_command_stream(fr->nbv->gpu_nbv, InteractionLocality::Local)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:                    fr->nbv->gpu_nbv != nullptr
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:                            ? Nbnxm::gpu_get_command_stream(fr->nbv->gpu_nbv, InteractionLocality::NonLocal)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:            const void*        deviceContext = pme_gpu_get_device_context(fr->pmedata);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:            const int          paddingSize   = pme_gpu_get_padding_size(fr->pmedata);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:            GpuApiCallBehavior transferKind = (inputrec->eI == eiMD && !doRerun && !useModularSimulator)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                                      ? GpuApiCallBehavior::Async
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                                      : GpuApiCallBehavior::Sync;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:            stateGpu = std::make_unique<gmx::StatePropagatorDataGpu>(
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:            fr->stateGpu = stateGpu.get();
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:        if (fr->pmePpCommGpu)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:            // destroy object since it is no longer required. (This needs to be done while the GPU context still exists.)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:            fr->pmePpCommGpu.reset();
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:    // before we destroy the GPU context(s) in free_gpu_resources().
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:    // Pinned buffers are associated with contexts in CUDA.
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:    // As soon as we destroy GPU contexts after mdrunner() exits, these lines should go.
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:    mdModules_.reset(nullptr); // destruct force providers here as they might also use the GPU
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:    /* Free GPU memory and set a physical node tMPI barrier (which should eventually go away) */
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:    free_gpu_resources(fr, physicalNodeComm, hwinfo->gpu_info);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:    free_gpu(nonbondedDeviceInfo);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/runner.cpp.preplumed:    free_gpu(pmeDeviceInfo);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:#include "gromacs/gpu_utils/gpu_utils.h"
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:#include "gromacs/mdlib/update_constrain_cuda.h"
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:#include "gromacs/mdtypes/state_propagator_data_gpu.h"
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:#include "gromacs/nbnxm/gpu_data_mgmt.h"
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:    /* PME load balancing data for GPU kernels */
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:    std::unique_ptr<UpdateConstrainCuda> integrator;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:    const bool  useGpuForPme       = simulationWork.useGpuPme;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:    const bool  useGpuForNonbonded = simulationWork.useGpuNonbonded;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:    const bool  useGpuForBufferOps = simulationWork.useGpuBufferOps;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:    const bool  useGpuForUpdate    = simulationWork.useGpuUpdate;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:    StatePropagatorDataGpu* stateGpu = fr->stateGpu;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:    if (useGpuForUpdate)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:                           "groups if using GPU update.\n");
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:                           "SHAKE is not supported with GPU update.");
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:        GMX_RELEASE_ASSERT(useGpuForPme || (useGpuForNonbonded && simulationWork.useGpuBufferOps),
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:                           "the GPU to use GPU update.\n");
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:                           "Only the md integrator is supported with the GPU update.\n");
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:                "Nose-Hoover temperature coupling is not supported with the GPU update.\n");
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:                           "with the GPU update.\n");
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:                           "Virtual sites are not supported with the GPU update.\n");
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:                           "Essential dynamics is not supported with the GPU update.\n");
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:                           "Constraints pulling is not supported with the GPU update.\n");
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:                           "Orientation restraints are not supported with the GPU update.\n");
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:                           "Free energy perturbations are not supported with the GPU update.");
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:        GMX_RELEASE_ASSERT(graph == nullptr, "The graph is not supported with GPU update.");
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:                    .appendText("Updating coordinates and applying constraints on the GPU.");
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:            GMX_LOG(mdlog.info).asParagraph().appendText("Updating coordinates on the GPU.");
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:        integrator = std::make_unique<UpdateConstrainCuda>(
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:                *ir, *top_global, stateGpu->getUpdateStream(), stateGpu->xUpdatedOnDevice());
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:    if (useGpuForPme || (useGpuForNonbonded && useGpuForBufferOps) || useGpuForUpdate)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:    if ((useGpuForNonbonded && useGpuForBufferOps) || useGpuForUpdate)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:    if (useGpuForUpdate)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:                         fr->nbv->useGpu());
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:            if (useGpuForUpdate && !bFirstStep)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:                stateGpu->copyCoordinatesFromGpu(state->x, AtomLocality::Local);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:                stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:            /* PME grid + cut-off optimization with GPUs or PME nodes */
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:                           &bPMETunePrinting, simulationWork.useGpuPmePpCommunication);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:        if (useGpuForUpdate && !bFirstStep && bNS)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:            // Copy velocities from the GPU on search steps to keep a copy on host (device buffers are reinitialized).
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:            stateGpu->copyVelocitiesFromGpu(state->v, AtomLocality::Local);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:            stateGpu->waitVelocitiesReadyOnHost(AtomLocality::Local);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:            // Copy coordinate from the GPU when needed at the search step.
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:            stateGpu->copyCoordinatesFromGpu(state->x, AtomLocality::Local);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:            stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:                    if (useGpuForUpdate)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:        // Copy coordinate from the GPU for the output/checkpointing if the update is offloaded and
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:        if (useGpuForUpdate && !bNS && !runScheduleWork->domainWork.haveCpuLocalForceWork
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:            stateGpu->copyCoordinatesFromGpu(state->x, AtomLocality::Local);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:            stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:        if (useGpuForUpdate && !bNS
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:            stateGpu->copyVelocitiesFromGpu(state->v, AtomLocality::Local);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:            stateGpu->waitVelocitiesReadyOnHost(AtomLocality::Local);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:        // Copy forces for the output if the forces were reduced on the GPU (not the case on virial steps)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:        // and update is offloaded hence forces are kept on the GPU for update and have not been
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:        //       when the forces are ready on the GPU -- the same synchronizer should be used as the one
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:        //       prior to GPU update.
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:        if (runScheduleWork->stepWork.useGpuFBufferOps && (simulationWork.useGpuUpdate && !vsite)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:            stateGpu->copyForcesFromGpu(ArrayRef<RVec>(f), AtomLocality::Local);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:            stateGpu->waitForcesReadyOnHost(AtomLocality::Local);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:        if (useGpuForUpdate)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:                integrator->set(stateGpu->getCoordinates(), stateGpu->getVelocities(),
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:                                stateGpu->getForces(), top.idef, *mdatoms, ekind->ngtc);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:                // Copy data to the GPU after buffers might have being reinitialized
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:                stateGpu->copyVelocitiesToGpu(state->v, AtomLocality::Local);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:                stateGpu->copyCoordinatesToGpu(state->x, AtomLocality::Local);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:            if (!runScheduleWork->stepWork.useGpuFBufferOps)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:                stateGpu->copyForcesToGpu(ArrayRef<RVec>(f), AtomLocality::Local);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:            integrator->integrate(stateGpu->getForcesReadyOnDeviceEvent(
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:                                          AtomLocality::Local, runScheduleWork->stepWork.useGpuFBufferOps),
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:                stateGpu->copyVelocitiesFromGpu(state->v, AtomLocality::Local);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:                stateGpu->waitVelocitiesReadyOnHost(AtomLocality::Local);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:                if (useGpuForUpdate && !EI_VV(ir->eI) && bStopCM)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:                    stateGpu->copyCoordinatesFromGpu(state->x, AtomLocality::Local);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:                    stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:                    if (useGpuForUpdate)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:                        stateGpu->copyCoordinatesToGpu(state->x, AtomLocality::Local);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:                        stateGpu->waitCoordinatesCopiedToDevice(AtomLocality::Local);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:                                         pressureCouplingMu, state, nrnb, &upd, !useGpuForUpdate);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:        if (useGpuForUpdate && (doBerendsenPressureCoupling || doParrinelloRahman))
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp.preplumed:        pme_loadbal_done(pme_loadbal, fplog, mdlog, fr->nbv->useGpu());
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:#include "gromacs/gpu_utils/gpu_utils.h"
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:#include "gromacs/mdlib/update_constrain_cuda.h"
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:#include "gromacs/mdtypes/state_propagator_data_gpu.h"
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:#include "gromacs/nbnxm/gpu_data_mgmt.h"
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:    /* PME load balancing data for GPU kernels */
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:    std::unique_ptr<UpdateConstrainCuda> integrator;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:    const bool  useGpuForPme       = simulationWork.useGpuPme;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:    const bool  useGpuForNonbonded = simulationWork.useGpuNonbonded;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:    const bool  useGpuForBufferOps = simulationWork.useGpuBufferOps;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:    const bool  useGpuForUpdate    = simulationWork.useGpuUpdate;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:    StatePropagatorDataGpu* stateGpu = fr->stateGpu;
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:    if (useGpuForUpdate)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:                           "groups if using GPU update.\n");
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:                           "SHAKE is not supported with GPU update.");
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:        GMX_RELEASE_ASSERT(useGpuForPme || (useGpuForNonbonded && simulationWork.useGpuBufferOps),
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:                           "the GPU to use GPU update.\n");
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:                           "Only the md integrator is supported with the GPU update.\n");
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:                "Nose-Hoover temperature coupling is not supported with the GPU update.\n");
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:                           "with the GPU update.\n");
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:                           "Virtual sites are not supported with the GPU update.\n");
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:                           "Essential dynamics is not supported with the GPU update.\n");
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:                           "Constraints pulling is not supported with the GPU update.\n");
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:                           "Orientation restraints are not supported with the GPU update.\n");
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:                           "Free energy perturbations are not supported with the GPU update.");
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:        GMX_RELEASE_ASSERT(graph == nullptr, "The graph is not supported with GPU update.");
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:                    .appendText("Updating coordinates and applying constraints on the GPU.");
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:            GMX_LOG(mdlog.info).asParagraph().appendText("Updating coordinates on the GPU.");
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:        integrator = std::make_unique<UpdateConstrainCuda>(
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:                *ir, *top_global, stateGpu->getUpdateStream(), stateGpu->xUpdatedOnDevice());
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:    if (useGpuForPme || (useGpuForNonbonded && useGpuForBufferOps) || useGpuForUpdate)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:    if ((useGpuForNonbonded && useGpuForBufferOps) || useGpuForUpdate)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:    if (useGpuForUpdate)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:                         fr->nbv->useGpu());
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:            if (useGpuForUpdate && !bFirstStep)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:                stateGpu->copyCoordinatesFromGpu(state->x, AtomLocality::Local);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:                stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:            /* PME grid + cut-off optimization with GPUs or PME nodes */
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:                           &bPMETunePrinting, simulationWork.useGpuPmePpCommunication);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:        if (useGpuForUpdate && !bFirstStep && bNS)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:            // Copy velocities from the GPU on search steps to keep a copy on host (device buffers are reinitialized).
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:            stateGpu->copyVelocitiesFromGpu(state->v, AtomLocality::Local);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:            stateGpu->waitVelocitiesReadyOnHost(AtomLocality::Local);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:            // Copy coordinate from the GPU when needed at the search step.
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:            stateGpu->copyCoordinatesFromGpu(state->x, AtomLocality::Local);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:            stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:                    if (useGpuForUpdate)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:        // Copy coordinate from the GPU for the output/checkpointing if the update is offloaded and
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:        if (useGpuForUpdate && !bNS && !runScheduleWork->domainWork.haveCpuLocalForceWork
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:            stateGpu->copyCoordinatesFromGpu(state->x, AtomLocality::Local);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:            stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:        if (useGpuForUpdate && !bNS
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:            stateGpu->copyVelocitiesFromGpu(state->v, AtomLocality::Local);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:            stateGpu->waitVelocitiesReadyOnHost(AtomLocality::Local);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:        // Copy forces for the output if the forces were reduced on the GPU (not the case on virial steps)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:        // and update is offloaded hence forces are kept on the GPU for update and have not been
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:        //       when the forces are ready on the GPU -- the same synchronizer should be used as the one
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:        //       prior to GPU update.
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:        if (runScheduleWork->stepWork.useGpuFBufferOps && (simulationWork.useGpuUpdate && !vsite)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:            stateGpu->copyForcesFromGpu(ArrayRef<RVec>(f), AtomLocality::Local);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:            stateGpu->waitForcesReadyOnHost(AtomLocality::Local);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:        if (useGpuForUpdate)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:                integrator->set(stateGpu->getCoordinates(), stateGpu->getVelocities(),
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:                                stateGpu->getForces(), top.idef, *mdatoms, ekind->ngtc);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:                // Copy data to the GPU after buffers might have being reinitialized
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:                stateGpu->copyVelocitiesToGpu(state->v, AtomLocality::Local);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:                stateGpu->copyCoordinatesToGpu(state->x, AtomLocality::Local);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:            if (!runScheduleWork->stepWork.useGpuFBufferOps)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:                stateGpu->copyForcesToGpu(ArrayRef<RVec>(f), AtomLocality::Local);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:            integrator->integrate(stateGpu->getForcesReadyOnDeviceEvent(
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:                                          AtomLocality::Local, runScheduleWork->stepWork.useGpuFBufferOps),
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:                stateGpu->copyVelocitiesFromGpu(state->v, AtomLocality::Local);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:                stateGpu->waitVelocitiesReadyOnHost(AtomLocality::Local);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:                if (useGpuForUpdate && !EI_VV(ir->eI) && bStopCM)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:                    stateGpu->copyCoordinatesFromGpu(state->x, AtomLocality::Local);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:                    stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:                    if (useGpuForUpdate)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:                        stateGpu->copyCoordinatesToGpu(state->x, AtomLocality::Local);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:                        stateGpu->waitCoordinatesCopiedToDevice(AtomLocality::Local);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:                                         pressureCouplingMu, state, nrnb, &upd, !useGpuForUpdate);
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:        if (useGpuForUpdate && (doBerendsenPressureCoupling || doParrinelloRahman))
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/mdrun/md.cpp:        pme_loadbal_done(pme_loadbal, fplog, mdlog, fr->nbv->useGpu());
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt:if (GMX_CLANG_CUDA)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt:    include(gmxClangCudaUtils)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt:set_property(GLOBAL PROPERTY GMX_LIBGROMACS_GPU_IMPL_SOURCES)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt:# TODO Reconsider this, as the CUDA driver API is probably a simpler
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt:function (gmx_compile_cpp_as_cuda)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt:    _gmx_add_files_to_property(GMX_LIBGROMACS_GPU_IMPL_SOURCES ${ARGN})
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt:add_subdirectory(gpu_utils)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt:# Mark some shared GPU implementation files to compile with CUDA if needed
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt:if (GMX_USE_CUDA)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt:    get_property(LIBGROMACS_GPU_IMPL_SOURCES GLOBAL PROPERTY GMX_LIBGROMACS_GPU_IMPL_SOURCES)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt:    set_source_files_properties(${LIBGROMACS_GPU_IMPL_SOURCES} PROPERTIES CUDA_SOURCE_PROPERTY_FORMAT OBJ)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt:# set up CUDA compilation with clang
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt:if (GMX_CLANG_CUDA)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt:        get_source_file_property(_cuda_source_format ${_file} CUDA_SOURCE_PROPERTY_FORMAT)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt:        if ("${_ext}" STREQUAL ".cu" OR _cuda_source_format)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt:            gmx_compile_cuda_file_with_clang(${_file})
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt:if (GMX_USE_CUDA)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt:    # Work around FindCUDA that prevents using target_link_libraries()
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt:    set(CUDA_LIBRARIES PRIVATE ${CUDA_LIBRARIES})
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt:    if (NOT GMX_CLANG_CUDA)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt:        gmx_cuda_add_library(libgromacs ${LIBGROMACS_SOURCES})
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt:    target_link_libraries(libgromacs PRIVATE ${CUDA_CUFFT_LIBRARIES})
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt:if (GMX_USE_OPENCL)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt:"An OpenCL build was requested with Visual Studio compiler, but GROMACS
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt:clFFT to help with building for OpenCL, but that clFFT has not yet been
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt:requires. Thus for now, OpenCL is not available with MSVC and the internal
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt:                      ${OpenCL_LIBRARIES}
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt:# Technically, the user could want to do this for an OpenCL build
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt:# using the CUDA runtime, but currently there's no reason to want to
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt:if (INSTALL_CUDART_LIB) #can be set manual by user
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt:    if (GMX_USE_CUDA)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt:        foreach(CUDA_LIB ${CUDA_LIBRARIES})
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt:            string(REGEX MATCH "cudart" IS_CUDART ${CUDA_LIB})
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt:            if(IS_CUDART) #libcuda should not be installed
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt:                file(GLOB CUDA_LIBS ${CUDA_LIB}*)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt:                install(FILES ${CUDA_LIBS} DESTINATION
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt:        message(WARNING "INSTALL_CUDART_LIB only makes sense when configuring for CUDA support")
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt:if(GMX_USE_OPENCL)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt:    file(GLOB OPENCL_INSTALLED_FILES
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt:        gpu_utils/vectype_ops.clh
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt:        gpu_utils/device_utils.clh
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt:    install(FILES ${OPENCL_INSTALLED_FILES}
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt:        DESTINATION ${GMX_INSTALL_OCLDIR}/gromacs/gpu_utils
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt:    file(GLOB OPENCL_INSTALLED_FILES
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt:    install(FILES ${OPENCL_INSTALLED_FILES}
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt:    file(GLOB OPENCL_INSTALLED_FILES
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt:    install(FILES ${OPENCL_INSTALLED_FILES}
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt:    file(GLOB OPENCL_INSTALLED_FILES
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt:        nbnxm/opencl/nbnxm_ocl_kernels.cl
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt:        nbnxm/opencl/nbnxm_ocl_kernel.clh
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt:        nbnxm/opencl/nbnxm_ocl_kernel_pruneonly.clh
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt:        nbnxm/opencl/nbnxm_ocl_kernels.clh
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt:        nbnxm/opencl/nbnxm_ocl_kernels_fastgen.clh
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt:        nbnxm/opencl/nbnxm_ocl_kernels_fastgen_add_twincut.clh
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt:        nbnxm/opencl/nbnxm_ocl_kernel_utils.clh
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt:        nbnxm/opencl/nbnxm_ocl_consts.h
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt:    install(FILES ${OPENCL_INSTALLED_FILES}
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt:        DESTINATION ${GMX_INSTALL_OCLDIR}/gromacs/nbnxm/opencl
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt:    file(GLOB OPENCL_INSTALLED_FILES
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt:        ewald/pme_gpu_utils.clh
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt:        ewald/pme_gpu_types.h
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt:    install(FILES ${OPENCL_INSTALLED_FILES}
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt.preplumed:if (GMX_CLANG_CUDA)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt.preplumed:    include(gmxClangCudaUtils)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt.preplumed:set_property(GLOBAL PROPERTY GMX_LIBGROMACS_GPU_IMPL_SOURCES)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt.preplumed:# TODO Reconsider this, as the CUDA driver API is probably a simpler
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt.preplumed:function (gmx_compile_cpp_as_cuda)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt.preplumed:    _gmx_add_files_to_property(GMX_LIBGROMACS_GPU_IMPL_SOURCES ${ARGN})
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt.preplumed:add_subdirectory(gpu_utils)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt.preplumed:# Mark some shared GPU implementation files to compile with CUDA if needed
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt.preplumed:if (GMX_USE_CUDA)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt.preplumed:    get_property(LIBGROMACS_GPU_IMPL_SOURCES GLOBAL PROPERTY GMX_LIBGROMACS_GPU_IMPL_SOURCES)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt.preplumed:    set_source_files_properties(${LIBGROMACS_GPU_IMPL_SOURCES} PROPERTIES CUDA_SOURCE_PROPERTY_FORMAT OBJ)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt.preplumed:# set up CUDA compilation with clang
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt.preplumed:if (GMX_CLANG_CUDA)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt.preplumed:        get_source_file_property(_cuda_source_format ${_file} CUDA_SOURCE_PROPERTY_FORMAT)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt.preplumed:        if ("${_ext}" STREQUAL ".cu" OR _cuda_source_format)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt.preplumed:            gmx_compile_cuda_file_with_clang(${_file})
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt.preplumed:if (GMX_USE_CUDA)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt.preplumed:    # Work around FindCUDA that prevents using target_link_libraries()
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt.preplumed:    set(CUDA_LIBRARIES PRIVATE ${CUDA_LIBRARIES})
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt.preplumed:    if (NOT GMX_CLANG_CUDA)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt.preplumed:        gmx_cuda_add_library(libgromacs ${LIBGROMACS_SOURCES})
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt.preplumed:    target_link_libraries(libgromacs PRIVATE ${CUDA_CUFFT_LIBRARIES})
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt.preplumed:if (GMX_USE_OPENCL)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt.preplumed:"An OpenCL build was requested with Visual Studio compiler, but GROMACS
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt.preplumed:clFFT to help with building for OpenCL, but that clFFT has not yet been
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt.preplumed:requires. Thus for now, OpenCL is not available with MSVC and the internal
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt.preplumed:                      ${OpenCL_LIBRARIES}
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt.preplumed:# Technically, the user could want to do this for an OpenCL build
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt.preplumed:# using the CUDA runtime, but currently there's no reason to want to
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt.preplumed:if (INSTALL_CUDART_LIB) #can be set manual by user
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt.preplumed:    if (GMX_USE_CUDA)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt.preplumed:        foreach(CUDA_LIB ${CUDA_LIBRARIES})
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt.preplumed:            string(REGEX MATCH "cudart" IS_CUDART ${CUDA_LIB})
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt.preplumed:            if(IS_CUDART) #libcuda should not be installed
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt.preplumed:                file(GLOB CUDA_LIBS ${CUDA_LIB}*)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt.preplumed:                install(FILES ${CUDA_LIBS} DESTINATION
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt.preplumed:        message(WARNING "INSTALL_CUDART_LIB only makes sense when configuring for CUDA support")
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt.preplumed:if(GMX_USE_OPENCL)
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt.preplumed:    file(GLOB OPENCL_INSTALLED_FILES
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt.preplumed:        gpu_utils/vectype_ops.clh
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt.preplumed:        gpu_utils/device_utils.clh
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt.preplumed:    install(FILES ${OPENCL_INSTALLED_FILES}
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt.preplumed:        DESTINATION ${GMX_INSTALL_OCLDIR}/gromacs/gpu_utils
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt.preplumed:    file(GLOB OPENCL_INSTALLED_FILES
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt.preplumed:    install(FILES ${OPENCL_INSTALLED_FILES}
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt.preplumed:    file(GLOB OPENCL_INSTALLED_FILES
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt.preplumed:    install(FILES ${OPENCL_INSTALLED_FILES}
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt.preplumed:    file(GLOB OPENCL_INSTALLED_FILES
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt.preplumed:        nbnxm/opencl/nbnxm_ocl_kernels.cl
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt.preplumed:        nbnxm/opencl/nbnxm_ocl_kernel.clh
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt.preplumed:        nbnxm/opencl/nbnxm_ocl_kernel_pruneonly.clh
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt.preplumed:        nbnxm/opencl/nbnxm_ocl_kernels.clh
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt.preplumed:        nbnxm/opencl/nbnxm_ocl_kernels_fastgen.clh
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt.preplumed:        nbnxm/opencl/nbnxm_ocl_kernels_fastgen_add_twincut.clh
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt.preplumed:        nbnxm/opencl/nbnxm_ocl_kernel_utils.clh
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt.preplumed:        nbnxm/opencl/nbnxm_ocl_consts.h
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt.preplumed:    install(FILES ${OPENCL_INSTALLED_FILES}
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt.preplumed:        DESTINATION ${GMX_INSTALL_OCLDIR}/gromacs/nbnxm/opencl
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt.preplumed:    file(GLOB OPENCL_INSTALLED_FILES
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt.preplumed:        ewald/pme_gpu_utils.clh
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt.preplumed:        ewald/pme_gpu_types.h
GPU/plumed2/patches/gromacs-2020.1.diff/src/gromacs/CMakeLists.txt.preplumed:    install(FILES ${OPENCL_INSTALLED_FILES}
GPU/plumed2/patches/namd-2.13.diff: 	$(CUDALIB) \
GPU/plumed2/patches/namd-2.13.diff: #include "DeviceCUDA.h"
GPU/plumed2/patches/namd-2.13.diff: #ifdef NAMD_CUDA
GPU/plumed2/configure:enable_af_cuda
GPU/plumed2/configure:  --enable-af_cuda        enable search for arrayfire_cuda, default: no
GPU/plumed2/configure:af_cuda=
GPU/plumed2/configure:# Check whether --enable-af_cuda was given.
GPU/plumed2/configure:if test "${enable_af_cuda+set}" = set; then :
GPU/plumed2/configure:  enableval=$enable_af_cuda; case "${enableval}" in
GPU/plumed2/configure:             (yes) af_cuda=true ;;
GPU/plumed2/configure:             (no)  af_cuda=false ;;
GPU/plumed2/configure:             (*)   as_fn_error $? "wrong argument to --enable-af_cuda" "$LINENO" 5 ;;
GPU/plumed2/configure:             (yes) af_cuda=true ;;
GPU/plumed2/configure:             (no)  af_cuda=false ;;
GPU/plumed2/configure:for ac_lib in '' afopencl; do
GPU/plumed2/configure:if test "$af_cuda" == true ; then
GPU/plumed2/configure:for ac_lib in '' afcuda; do
GPU/plumed2/CHANGES/v2.5.md:  - \ref SAXS includes a GPU implementation based on ArrayFire (need to be linked at compile time) that can be activated with GPU
GPU/plumed2/CHANGES/v2.5.md:- plumed can be compiled with ArrayFire to enable for gpu code. \ref SAXS collective variable is available as part of the isdb module to provide an example of a gpu implementation for a CV
GPU/plumed2/CHANGES/v2.0.md:  using GPUs.
GPU/plumed2/src/isdb/SAXS.cpp:By default SAXS is calculated using Debye on CPU, by adding the GPU flag it is possible to solve the equation on a GPU
GPU/plumed2/src/isdb/SAXS.cpp:  bool                       gpu;
GPU/plumed2/src/isdb/SAXS.cpp:  void calculate_gpu(vector<Vector> &deriv);
GPU/plumed2/src/isdb/SAXS.cpp:  keys.add("compulsory","DEVICEID","0","Identifier of the GPU to be used");
GPU/plumed2/src/isdb/SAXS.cpp:  keys.addFlag("GPU",false,"calculate SAXS using ARRAYFIRE on an accelerator device");
GPU/plumed2/src/isdb/SAXS.cpp:  gpu(false),
GPU/plumed2/src/isdb/SAXS.cpp:  parseFlag("GPU",gpu);
GPU/plumed2/src/isdb/SAXS.cpp:  if(gpu) error("To use the GPU mode PLUMED must be compiled with ARRAYFIRE");
GPU/plumed2/src/isdb/SAXS.cpp:  if(gpu) {
GPU/plumed2/src/isdb/SAXS.cpp:  if(!gpu) {
GPU/plumed2/src/isdb/SAXS.cpp:void SAXS::calculate_gpu(vector<Vector> &deriv)
GPU/plumed2/src/isdb/SAXS.cpp:  // on gpu only the master rank run the calculation
GPU/plumed2/src/isdb/SAXS.cpp:  if(gpu) calculate_gpu(deriv);
GPU/2decomp_fft/src/fft_fftw3.f90:#ifndef _OPENACC
GPU/2decomp_fft/src/Makefile.inc.pgi:OPTIM=-g -fast -Mdalign -Minline -ta=tesla:cc60,cc70,cuda10.1
GPU/2decomp_fft/src/transpose_z_to_y.f90:#ifdef _OPENACC
GPU/2decomp_fft/src/fft_common.f90:  ! ***TODO: once FFT fully integrated on GPU
GPU/2decomp_fft/src/fft_generic.f90:#ifndef _OPENACC
GPU/2decomp_fft/src/transpose_x_to_y.f90:#ifdef _OPENACC
GPU/2decomp_fft/src/transpose_x_to_y.f90:#ifdef _OPENACC
GPU/2decomp_fft/src/fft_cufft.f90:! This file is a modified file from 2DECOMP&FFT that uses cuda FFT for GPU
GPU/2decomp_fft/src/fft_cufft.f90:! Define generic names for cudaFFT subroutines/variables for simple/double precision
GPU/2decomp_fft/src/fft_cufft.f90:#ifdef _OPENACC
GPU/2decomp_fft/src/fft_cufft.f90:  use openacc
GPU/2decomp_fft/src/fft_cufft.f90:  use cufft      ! cuda_fft library module
GPU/2decomp_fft/src/fft_cufft.f90:  ! Return a cudaFFT plan for multiple 1D c2c FFTs in Z direction
GPU/2decomp_fft/src/fft_cufft.f90:  ! Return a cudaFFT plan for multiple 2D c2c FFTs in xy direction
GPU/2decomp_fft/src/fft_cufft.f90:  ! Return a cudaFFT plan for 3D c2c FFTs in xyz direction
GPU/2decomp_fft/src/fft_cufft.f90:       ierr = ierr + cufftSetStream(cuplan_xyz,acc_get_cuda_stream(rec_queue))
GPU/2decomp_fft/src/fft_cufft.f90:       ierr = ierr + cufftSetStream(cuplan_z, acc_get_cuda_stream(rec_queue))
GPU/2decomp_fft/src/fft_cufft.f90:       ierr = ierr + cufftSetStream(cuplan_xy,acc_get_cuda_stream(rec_queue))
GPU/2decomp_fft/src/fft_cufft.f90:      call decomp_2d_abort(CUFFT_ERROR, "Assigning cuda stream to plan failed (cufftSetStream)"  )
GPU/2decomp_fft/src/fft_ffte.f90:#ifndef _OPENACC
GPU/2decomp_fft/src/transpose_y_to_z.f90:#ifdef _OPENACC
GPU/2decomp_fft/src/fft_mkl.f90:#ifndef _OPENACC
GPU/2decomp_fft/src/decomp_2d.f90:#ifdef _OPENACC
GPU/2decomp_fft/src/decomp_2d.f90:  use openacc
GPU/2decomp_fft/src/decomp_2d.f90:  integer, save, public :: dir_queue,rec_queue ! async queue fo GPU
GPU/2decomp_fft/src/decomp_2d.f90:  subroutine tmatxb_pmegpu(nrhs,dodiag,mu,ef)
GPU/2decomp_fft/src/decomp_2d.f90:  procedure(tmatxb_pmegpu),pointer,public:: decomp2d_WhileWait1
GPU/2decomp_fft/src/decomp_2d.f90:#ifdef _OPENACC
GPU/2decomp_fft/src/decomp_2d.f90:#ifdef _OPENACC
GPU/2decomp_fft/src/decomp_2d.f90:  ! Send decomp_main to the GPU
GPU/2decomp_fft/src/decomp_2d.f90:#ifdef _OPENACC
GPU/2decomp_fft/src/decomp_2d.f90:#ifndef _OPENACC
GPU/2decomp_fft/src/decomp_2d.f90:#ifdef _OPENACC
GPU/2decomp_fft/src/decomp_2d.f90:    ! TODO Add auto-tuning mode with cudaFFT
GPU/2decomp_fft/src/decomp_2d.f90:#ifdef _OPENACC
GPU/2decomp_fft/src/decomp_2d.f90:  use cudafor
GPU/2decomp_fft/src/decomp_2d.f90:#ifdef _OPENACC
GPU/2decomp_fft/src/decomp_2d.f90:  ierr = cudamallochost(cptr0,n*sizeof(cplx0))
GPU/2decomp_fft/src/decomp_2d.f90:  if (ierr.ne.cudasuccess) then
GPU/2decomp_fft/src/decomp_2d.f90:     call decomp_2d_abort(ierr,cudaGetErrorString(ierr))
GPU/2decomp_fft/src/decomp_2d.f90:#ifdef _OPENACC
GPU/2decomp_fft/src/decomp_2d.f90:  use cudafor
GPU/2decomp_fft/src/decomp_2d.f90:#ifdef _OPENACC
GPU/2decomp_fft/src/decomp_2d.f90:     ierr = cudafreehost(c_loc(array))
GPU/2decomp_fft/src/decomp_2d.f90:     if (ierr.ne.cudasuccess) then
GPU/2decomp_fft/src/decomp_2d.f90:        call decomp_2d_abort(ierr,cudaGetErrorString(ierr))
GPU/2decomp_fft/src/transpose_y_to_x.f90:#ifdef _OPENACC
GPU/ci/install.sh:# Tinker-HP [GPU] Quick Installation script
GPU/ci/install.sh:target_arch='gpu'             #   [gpu]|cpu
GPU/ci/install.sh:c_c=60,70,80,86               #   Target GPU compute capability  [https://en.wikipedia.org/wiki/CUDA]
GPU/ci/install.sh:cuda_ver=11.7                 #   Cuda Version to be used by OpenACC compiler  (not the CUDA C/C++ compiler)
GPU/ci/install.sh:#add_host_f='-Mx,231,0x1'      #   Uncomment this when building Nvidia HPC-SDK package version 21.[3-9]
GPU/ci/install.sh:current_config="arch=$target_arch compute_capability=$c_c cuda_version=$cuda_ver PLUMED_SUPPORT=$build_plumed COLVARS_SUPPORT=$build_colvars NN_SUPPORT=$NN"
GPU/ci/install.sh:in_notif 'TINKER-HP GPU installation completes successfully'
GPU/Quantum-HP_tutorial.md:Quantum-HP is a multi-CPU and multi-GPU massively parallel platform designed to incorporate nuclear quantum effects in Tinker-HP MD simulations. It utilizes two primary simulation strategies:
GPU/Quantum-HP_tutorial.md:- Efficient implementation with parallelization for multi-CPU and multi-GPU systems.
GPU/Quantum-HP_tutorial.md:4. **Parallelization**: Quantum-HP is optimized for parallel computing. Make sure to take advantage of multi-CPU and (multi-)GPU systems to accelerate your simulations.
GPU/Quantum-HP_tutorial.md:   - `QTB_BATCH_SIZE` (int): number of atoms on which to generate the colored noise   simultaneously (only for the GPU version). Allows to reduce memory requirements   for large systems but might be slower when using small batch sizes. Defaults to   all the local atoms (i.e. no batches).
GPU/Prerequisites.md:A relatively recent Nvidia GPU is mandatory for the GPU code. One with at least a compute capability above 5.0.
GPU/Prerequisites.md:   - The nvidia CUDA C/C++ compiler for CUDA Files in GPU code
GPU/Prerequisites.md:   - MPI Fortran wrapper on the selected compiler and compiled to be CUDA-Aware for device communications during multi-GPUs execution.
GPU/Prerequisites.md:     __Note :__ PGI compiler does not support the GPU code on macOS.
GPU/Prerequisites.md:   - [Nvidia HPC Package](https://developer.nvidia.com/nvidia-hpc-sdk-releases) Any version under __22.7 !__ is operational.  
GPU/Prerequisites.md:     Starting from version 22.XX, the previously described issue has been solved by the developers, and the OpenACC compiler bug lately detected within 22.XX version has also been fixed. It involves an ambiguous OpenACC compilation when calling an acc device subroutine compiled in an other translation unit.  
GPU/Prerequisites.md:     This package contains everything you need to build Tinker-HP (GPU code). Previously listed items are already available within it. However, we need to make sure the installation has been correctly done and the environment variables (PATH; CPATH & NVHPC) are set and updated. Follow instructions described in `$PREFIX/modulefiles` with `$PREFIX` the installation directory of your package.
GPU/Prerequisites.md:   - HPC-SDK 22.7 + cuda11.7 + GNU-11.x.x
GPU/Prerequisites.md:   - HPC-SDK 22.7 + cuda11.0 + GNU-9.x.x
GPU/Prerequisites.md:   - HPC-SDK 22.2 + cuda11.6 + GNU-9.x.x
GPU/Prerequisites.md:   - HPC-SDK 22.2 + cuda11.0 + GNU-9.x.x
GPU/Prerequisites.md:   - HPC-SDK 21.9 + cuda11.4 + GNU-8.x.x
GPU/Prerequisites.md:   - HPC-SDK 21.2 + cuda11.2 + GNU-8.x.x
GPU/Prerequisites.md:   If you have a multi-CUDA HPC-SDK installation, you can reconfigure the compilers to use the desired version of CUDA by editing/creating the `localrc` file inside the `nvfortran` compiler directory. Fill or complete the file with the instruction.
GPU/Prerequisites.md:   set CUDAVERSION=11.x;
GPU/Prerequisites.md:If you plan to combine machine learning with MD, we provide environment files `tinker-hp/GPU/tinkerml.[torch|tensorflow].yaml` which installs python modules for machine learning. First one install the pytorch module and TorchAni model whereas the second focuses on DeepMD-kit over Tensorflow module. Those specific python environments require [Anaconda](https://www.anaconda.com/products/distribution) or [Miniconda](https://docs.conda.io/en/latest/miniconda.html) to be installed on the host machine. Install the environment of your choice according to your ML Model by following the next instructions. Of course this step has to be carried out before moving to Tinker-HP.
GPU/Prerequisites.md:   - `CUDA` (Toolkit and libraries)
GPU/Prerequisites.md:      - Cuda(C/C++) compiler `nvcc`
GPU/Prerequisites.md:      - Recent CUDA libraries are required by the GPU code. Every CUDA version since 9.1 has been tested.
GPU/Prerequisites.md:        From now on, CUDA Toolkit is a part of NVIDIA HPC-SDK package.
GPU/Prerequisites.md:If you are building with machine learning interface, please ensure the consistency of your environment between the one of Tinker-HP (HPC-SDK) and python ; especially the installed version of CUDA. Default python environment comes with cuda11.3 version `(tinkerml.yaml)`. Therefore, it is mandatory to use an NVIDIA HPC-SDK package which contains cuda11.3?  
GPU/Prerequisites.md:It is necessary to make sure that your environments (both build and run) are version consistent. In most cases, we have to pay attention, our native CUDA version does not differ from PGI provided CUDA and, is compatible with NVIDIA installed driver. Naturally, this issue will not occur with Nvidia HPC-SDK Package.
GPU/Deep-HP.md:# Deep-HP: Multi-GPUs platform for hybrid Machine Learning Polarizable Potential
GPU/Deep-HP.md:Deep-HP is a multi-GPU deep learning potential platform, part of the Tinker-HP molecular dynamics package and aims to couple deep learning with force fields for biological simulations. 
GPU/Deep-HP.md:* **Performing highly efficient classical molecular dynamics with your deep learning model** thanks to the highly optimized GPU-resident Tinker-HP code. Tinker-HP and Tinker9 are the fastest code for many-body polarizable force fields.
GPU/Deep-HP.md:But more importantly, when you are saving a `model` in `jit` format you are saving the whole code and you will not be able to use Tinker-HP's full capabilities for example its highly efficient neighbor list and thus use multi-GPU, Particle Mesh Ewald,... <br />
GPU/Deep-HP.md:We provide 6 examples that encompass the basics of Deep-HP inputs with which you can cover a wide range of applications. They are located in `/home/user/.../tinker-hp/GPU/examples/`. Some toy machine learning potential models are located in `/home/user/.../tinker-hp/GPU/ml_models/`.
GPU/Deep-HP.md::green_circle: *Objective:* Perform machine learning potential simulation - on the full system (100000 atoms) with 2 GPUs.<br />
GPU/source/ker_strtor.inc.f:#ifdef   USE_NVSHMEM_CUDA
GPU/source/ker_strtor.inc.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/ker_strtor.inc.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/ker_strtor.inc.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/ker_strtor.inc.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/elj1gpu.f:      module elj1gpu_inl
GPU/source/elj1gpu.f:      subroutine elj1gpu
GPU/source/elj1gpu.f:      use utilgpu
GPU/source/elj1gpu.f:         call evcorr1gpu (mode,elrc,vlrc)
GPU/source/elj1gpu.f:      use elj1gpu_inl
GPU/source/elj1gpu.f:      use utilgpu
GPU/source/elj1gpu.f:      ! set elj1cgpu Configuration
GPU/source/elj1gpu.f:#ifdef _CUDA
GPU/source/elj1gpu.f:c            CUDA Routine for Lennard-Jones 
GPU/source/elj1gpu.f:      use cudafor   ,only: dim3
GPU/source/elj1gpu.f:      use elj1gpu_inl,only: enr2en,mdr2md
GPU/source/elj1gpu.f:      use utilgpu   ,only: def_queue,dir_queue,rec_queue,dir_stream
GPU/source/elj1gpu.f:c     Call Lennard-Jones kernel in CUDA using C2 nblist
GPU/source/kmpole.f:      use utilgpu
GPU/source/kmpole.f:c       Translate polaxe to integer to acc GPU comput
GPU/source/kmpole.f:#ifdef _OPENACC
GPU/source/readxyz.f:#ifdef _OPENACC
GPU/source/tinker_cudart.h:#ifndef TINKER_CUDART_H
GPU/source/tinker_cudart.h:#define TINKER_CUDART_H
GPU/source/nblistcu.f:#ifdef _CUDA
GPU/source/nblistcu.f:      use cudadevice
GPU/source/nblistcu.f:      use utilgpu   ,only: BLOCK_SIZE,WARP_SIZE,WARP_SHFT,inf
GPU/source/kstrbnd.f:      use utilgpu
GPU/source/kstrbnd.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/kstrbnd.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/kstrbnd.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/kstrbnd.f:#ifdef _OPENACC
GPU/source/MOD_nvtx.f90:! if USE_NVTX is defined, executable must be linked with ${CUDAROOT}/lib64/libnvToolsExt.so
GPU/source/MOD_nvtx.f90:! Module nvtx adapted from https://devblogs.nvidia.com/customize-cuda-fortran-profiling-nvtx/
GPU/source/kgeom.f:      use utilgpu   ,only: prmem_request
GPU/source/kgeom.f:#ifdef _OPENACC
GPU/source/bbk.f:      use utilgpu
GPU/source/bbk.f:#ifdef _OPENACC
GPU/source/bbk.f:      call normalgpu(Rn,3*nloc)
GPU/source/bbk.f:      call mdrestgpu (istep)
GPU/source/eangle1gpu.f:      module eangle1gpu_inl
GPU/source/eangle1gpu.f:      subroutine eangle1gpu_(dea,deW1aMD,atmType,afld,s)
GPU/source/eangle1gpu.f:      use eangle1gpu_inl
GPU/source/eangle1gpu.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/eangle1gpu.f:      if(deb_Path) write(*,*) 'eangle1gpu'
GPU/source/eangle1gpu.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/eangle1gpu.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/eangle1gpu.f:      subroutine eangle1gpu
GPU/source/eangle1gpu.f:      subroutine eangle1gpu_(dea,deW1aMD,atmType,afld,s)
GPU/source/eangle1gpu.f:      call eangle1gpu_(dea,deW1aMD,type,afld,max(1,size(deW1aMD,2)))
GPU/source/estrtor1gpu.f:      module estrtor1gpu_inl
GPU/source/estrtor1gpu.f:      subroutine estrtor1gpu_(itors,x,y,z,tors1,tors2,tors3,ist,kst,bl
GPU/source/estrtor1gpu.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/estrtor1gpu.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/estrtor1gpu.f:      subroutine estrtor1gpu
GPU/source/estrtor1gpu.f:      use estrtor1gpu_inl
GPU/source/estrtor1gpu.f:      call estrtor1gpu_(itors,x,y,z,tors1,tors2,tors3,ist,kst,bl,debt)
GPU/source/eurey1gpu.f:      module eurey1gpu_inl
GPU/source/eurey1gpu.f:      subroutine eurey1gpu
GPU/source/eurey1gpu.f:      use eurey1gpu_inl
GPU/source/eurey1gpu.f:      if(deb_Path) write(*,*) 'eurey1gpu'
GPU/source/newinduce_shortreal.f:#ifdef _OPENACC
GPU/source/pair_polar.inc.f:#include "tinker_cudart.h"
GPU/source/pair_polar.inc.f:      use utilgpu   ,only: rpole_elt,real3,real6,real3_red
GPU/source/respa.f:      use utilgpu  ,only: rec_queue,openacc_abort
GPU/source/respa.f:            ! call openacc_abort("Rattle not tested with openacc")
GPU/source/respa.f:            ! call openacc_abort("Rattle2 not tested with openacc")
GPU/source/respa.f:c           call rotpolegpu
GPU/source/respa.f:        !  call openacc_abort("Rattle2 not tested with openacc")
GPU/source/respa.f:      call mdrestgpu  (istep)
GPU/source/epolar1gpu.f:      module epolar1gpu_inl
GPU/source/epolar1gpu.f:      subroutine epolar1gpu
GPU/source/epolar1gpu.f:          !call epolar1tcggpu !FIXME
GPU/source/epolar1gpu.f:          call epolar1cgpu
GPU/source/epolar1gpu.f:          !call epolar1tcggpu
GPU/source/epolar1gpu.f:          call epolar1cgpu
GPU/source/epolar1gpu.f:      subroutine epolar1cgpu
GPU/source/epolar1gpu.f:      use epolar1gpu_inl
GPU/source/epolar1gpu.f:      use interfaces    ,only:torquegpu,epreal1c_p
GPU/source/epolar1gpu.f:      use utilgpu
GPU/source/epolar1gpu.f:      if(deb_Path)write(*,*) 'epolar1cgpu'
GPU/source/epolar1gpu.f:#ifdef _OPENACC
GPU/source/epolar1gpu.f:          call dcinduce_shortrealgpu
GPU/source/epolar1gpu.f:          call newinduce_shortrealgpu
GPU/source/epolar1gpu.f:#ifdef _OPENACC
GPU/source/epolar1gpu.f:          call newinduce_pmegpu
GPU/source/epolar1gpu.f:          call dcinduce_pme2gpu
GPU/source/epolar1gpu.f:          call newinduce_pme2gpu
GPU/source/epolar1gpu.f:#ifdef _OPENACC
GPU/source/epolar1gpu.f:            call epreal1cgpu
GPU/source/epolar1gpu.f:#ifdef _OPENACC
GPU/source/epolar1gpu.f:           call torquegpu(npoleloc,nbloc,poleglob,loc,trq,dep,def_queue)
GPU/source/epolar1gpu.f:              call torquegpu(npoleloc,nbloc
GPU/source/epolar1gpu.f:#ifdef _OPENACC
GPU/source/epolar1gpu.f:            call eprecip1gpu
GPU/source/epolar1gpu.f:#ifdef _OPENACC
GPU/source/epolar1gpu.f:      subroutine epreal1cgpu
GPU/source/epolar1gpu.f:      use epolar1gpu_inl
GPU/source/epolar1gpu.f:      use interfaces ,only: epreal1c_core_p,torquegpu
GPU/source/epolar1gpu.f:      use utilgpu   ,pot=>ug_workS_r
GPU/source/epolar1gpu.f:      if(deb_Path)write(*,'(2x,a)') 'epreal1cgpu'
GPU/source/epolar1gpu.f:#ifdef _OPENACC
GPU/source/epolar1gpu.f:      call torquegpu(trqvec,fix,fiy,fiz,dep,extract)
GPU/source/epolar1gpu.f:      use epolar1gpu_inl
GPU/source/epolar1gpu.f:      use utilgpu
GPU/source/epolar1gpu.f:      use epolar1gpu_inl
GPU/source/epolar1gpu.f:      use utilgpu ,only: def_queue, pot=>ug_workS_r
GPU/source/epolar1gpu.f:#ifdef _CUDA
GPU/source/epolar1gpu.f:      use cudafor
GPU/source/epolar1gpu.f:      use utilgpu   ,only: def_queue
GPU/source/epolar1gpu.f:         call cudaMaxGridSize("epreal1c_core_cu",gS)
GPU/source/epolar1gpu.f:      use cudafor
GPU/source/epolar1gpu.f:      use utilgpu ,only: def_queue
GPU/source/epolar1gpu.f:         call cudaMaxGridSize("mpreal1c_core_cu",gS)
GPU/source/epolar1gpu.f:      use epolar1gpu_inl
GPU/source/epolar1gpu.f:      use utilgpu ,only: def_queue, pot=>ug_workS_r
GPU/source/epolar1gpu.f:      module eprecip1gpu_mod
GPU/source/epolar1gpu.f:      subroutine eprecip1gpu
GPU/source/epolar1gpu.f:      use eprecip1gpu_mod
GPU/source/epolar1gpu.f:      use interfaces,only: fphi_uind_site1_p,torquegpu
GPU/source/epolar1gpu.f:      use utilgpu   ,pot=>ug_workS_r,potrec=>ug_workS_r1
GPU/source/epolar1gpu.f:      if (deb_Path) write(*,'(2x,a)') 'eprecip1gpu'
GPU/source/epolar1gpu.f:      !call frac_to_cartgpu
GPU/source/epolar1gpu.f:      call cmp_to_fmp_sitegpu(cmp,fmp)
GPU/source/epolar1gpu.f:#ifdef _OPENACC
GPU/source/epolar1gpu.f:#ifdef _OPENACC
GPU/source/epolar1gpu.f:#ifdef _OPENACC
GPU/source/epolar1gpu.f:#ifdef _OPENACC
GPU/source/epolar1gpu.f:      call fphi_to_cphi_sitegpu(fphidp,cphirec)
GPU/source/epolar1gpu.f:      call torquegpu(npolerecloc,nlocrec2,polerecglob,locrec
GPU/source/epolar1gpu.f:      call cmp_to_fmp_sitegpu(cmp,fmp)
GPU/source/epolar1gpu.f:#ifdef _OPENACC
GPU/source/epolar1gpu.f:      call cmp_to_fmp_sitegpu(cmp,fmp)
GPU/source/epolar1gpu.f:#ifdef _OPENACC
GPU/source/ktors.f:#ifdef _OPENACC
GPU/source/ktors.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/ktors.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/eurey3gpu.f:      module eurey3gpu_inl
GPU/source/eurey3gpu.f:      subroutine eurey3gpu
GPU/source/eurey3gpu.f:      use eurey3gpu_inl
GPU/source/eurey3gpu.f:      if(deb_Path) write(*,*) 'eurey3gpu'
GPU/source/newinduce_group.f:      use interfaces,only: inducepcg_pme2gpu,tmatxb_p
GPU/source/newinduce_group.f:     &              , inducestepcg_pme2gpu, efld0_group
GPU/source/newinduce_group.f:     &              , efld0_directgpu2, efld0_directgpu_p
GPU/source/newinduce_group.f:      use utilgpu
GPU/source/newinduce_group.f:      use utilgpu
GPU/source/newinduce_group.f:      use utilgpu ,only: dir_queue,rec_queue,def_queue
GPU/source/newinduce_group.f:#ifdef _OPENACC
GPU/source/newinduce_group.f:      use utilgpu  ,only: dir_queue,def_queue
GPU/source/newinduce_group.f:      use utilgpu , only : maxscaling1, def_queue, dir_queue,real3,real6
GPU/source/newinduce_group.f:      use utilgpu , only : def_queue, dir_queue,real3,real6
GPU/source/newinduce_group.f:      use utilgpu ,only: prmem_request,dir_queue
GPU/source/rotpolegpu.f:      subroutine rotpolegpu
GPU/source/rotpolegpu.f:      use utilgpu,only: dir_queue,rec_queue,def_queue
GPU/source/rotpolegpu.f:      use utilgpu
GPU/source/rotpolegpu.f:#ifdef _OPENACC
GPU/source/rotpolegpu.f:            call randomgpu(samplevec(1),3*nZ_Onlyloc)
GPU/source/rotpolegpu.f:#ifdef _OPENACC
GPU/source/eliaison1cu.f:#include  "tinker_cudart.h"
GPU/source/eliaison1cu.f:        use cudafor  ,only: atomicAdd
GPU/source/eliaison1cu.f:        use utilgpu  ,only: RED_BUFF_SIZE
GPU/source/eliaison1cu.f:#ifdef   USE_NVSHMEM_CUDA
GPU/source/eliaison1cu.f:#ifdef   USE_NVSHMEM_CUDA
GPU/source/eliaison1cu.f:#ifdef   USE_NVSHMEM_CUDA
GPU/source/eliaison1cu.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/eliaison1cu.f:#ifdef   USE_NVSHMEM_CUDA
GPU/source/eliaison1cu.f:#ifdef   USE_NVSHMEM_CUDA
GPU/source/eliaison1cu.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/eliaison1cu.f:#ifdef   USE_NVSHMEM_CUDA
GPU/source/eliaison1cu.f:#ifdef   USE_NVSHMEM_CUDA
GPU/source/eliaison1cu.f:#ifdef   USE_NVSHMEM_CUDA
GPU/source/eliaison1cu.f:#ifdef   USE_NVSHMEM_CUDA
GPU/source/attach.f:      use utilgpu
GPU/source/attach.f:      ! Remove openacc declaration before shmem openacc configuration
GPU/source/ehal1gpu.f:c     ##  subroutine ehal1gpu  --  buffered 14-7 energy & derivatives  ##
GPU/source/ehal1gpu.f:      module ehal1gpu_inl
GPU/source/ehal1gpu.f:      subroutine ehal1gpu
GPU/source/ehal1gpu.f:      use ehal1gpu_inl,only: elrc,vlrc
GPU/source/ehal1gpu.f:      use utilgpu    ,only: def_queue,dir_queue
GPU/source/ehal1gpu.f:         call evcorr1gpu (mode,elrc,vlrc)
GPU/source/ehal1gpu.f:      use ehal1gpu_inl
GPU/source/ehal1gpu.f:      use utilgpu   ,only: def_queue,dir_queue,rec_queue
GPU/source/ehal1gpu.f:#ifdef _OPENACC
GPU/source/ehal1gpu.f:#ifdef _OPENACC
GPU/source/ehal1gpu.f:c      use utilgpu   ,only: inf,def_queue
GPU/source/ehal1gpu.f:      use ehal1gpu_inl
GPU/source/ehal1gpu.f:      use utilgpu   ,only: inf,def_queue
GPU/source/ehal1gpu.f:      use ehal1gpu_inl ,only: atomic_add
GPU/source/ehal1gpu.f:      use utilgpu, only: def_queue
GPU/source/ehal1gpu.f:#ifdef _CUDA
GPU/source/ehal1gpu.f:      use utilgpu   ,only: def_queue,dir_queue,rec_queue,dir_stream
GPU/source/ehal1gpu.f:     &              ,CUDAOCCUPANCYMAXACTIVEBLOCKSPERMULTIPROCESSOR
GPU/source/ehal1gpu.f:      i       = CUDAOCCUPANCYMAXACTIVEBLOCKSPERMULTIPROCESSOR
GPU/source/ehal1gpu.f:      i       = CUDAOCCUPANCYMAXACTIVEBLOCKSPERMULTIPROCESSOR
GPU/source/ehal1gpu.f:      i       = CUDAOCCUPANCYMAXACTIVEBLOCKSPERMULTIPROCESSOR
GPU/source/ehal1gpu.f:      if (i.ne.0) print*,'Issue detected with ehal1:cudaoccupancy',i
GPU/source/ehal1gpu.f:c     Call Vdw kernel in CUDA using C2 nblist
GPU/source/ehal1gpu.f:      use ehal1gpu_inl
GPU/source/ehal1gpu.f:      use ehal1gpu_inl
GPU/source/ehal1gpu.f:#if defined(_OPENACC) && 1
GPU/source/ehal1gpu.f:      use utilgpu   ,only: def_queue,dir_queue,rec_queue
GPU/source/ehal1gpu.f:#ifdef _OPENACC
GPU/source/ehal1gpu.f:#ifdef _OPENACC
GPU/source/ehal1gpu.f:#ifdef _OPENACC
GPU/source/ehal1gpu.f:c     Call Vdw kernel in CUDA using C2 nblist
GPU/source/ehal1gpu.f:      use ehal1gpu_inl
GPU/source/ehal1gpu.f:      use utilgpu   ,only: def_queue
GPU/source/analyze_beads.f:      use utilgpu
GPU/source/egeom1gpu.f:      module egeom1gpu_inl
GPU/source/egeom1gpu.f:      subroutine egeom1gpu
GPU/source/egeom1gpu.f:      use egeom1gpu_inl
GPU/source/egeom1gpu.f:      use utilgpu   ,only:def_queue,rec_queue
GPU/source/egeom1gpu.f:      if(deb_Path) write(*,*) 'egeom1gpu'
GPU/source/MOD_interfaces.f:#ifdef _OPENACC
GPU/source/MOD_interfaces.f:      use cudafor ,only:cuda_stream_kind
GPU/source/MOD_interfaces.f:         enumerator conf_elj1gpu
GPU/source/MOD_interfaces.f:         enumerator conf_ecreal1dgpu
GPU/source/MOD_interfaces.f:         enumerator conf_emreal1c_1     !1  ( OpenACC )
GPU/source/MOD_interfaces.f:         enumerator conf_emreal1c_2     !2  ( CUDA Fortran )
GPU/source/MOD_interfaces.f:         enumerator conf_emreal1c_3     !4  ( CUDA C )
GPU/source/MOD_interfaces.f:         enumerator conf_efld0_directgpu1
GPU/source/MOD_interfaces.f:         enumerator conf_efld0_directgpu2
GPU/source/MOD_interfaces.f:         enumerator conf_efld0_directgpu3
GPU/source/MOD_interfaces.f:         enumerator conf_fphi_cuda
GPU/source/MOD_interfaces.f:         enumerator conf_grid_cuda
GPU/source/MOD_interfaces.f:         enumerator conf_conv_cuda
GPU/source/MOD_interfaces.f:        subroutine tmatxb_pmegpu(nrhs,dodiag,mu,ef)
GPU/source/MOD_interfaces.f:        subroutine tmatxb_pmegpu1(nrhs,dodiag,mu,ef)
GPU/source/MOD_interfaces.f:      procedure(tmatxb_pmegpu)   ,pointer:: tmatxb_p,tmatxb_p1,tmatxb_cp
GPU/source/MOD_interfaces.f:      interface fphi_uind_sitegpu
GPU/source/MOD_interfaces.f:         subroutine fphi_uind_sitegpu1(fphi1,fphi2,fphi_sum)
GPU/source/MOD_interfaces.f:         subroutine fphi_uind_sitegpu2(fphi1,fphi2)
GPU/source/MOD_interfaces.f:         subroutine grid_uind_sitegpu(fuindvec,fuinpvec,qgrid2loc)
GPU/source/MOD_interfaces.f:         subroutine grid_mpole_sitegpu(fmpvec)
GPU/source/MOD_interfaces.f:         subroutine grid_pchg_sitegpu
GPU/source/MOD_interfaces.f:         subroutine grid_disp_sitegpu
GPU/source/MOD_interfaces.f:         subroutine bspline_fill_sitegpu(config)
GPU/source/MOD_interfaces.f:         subroutine pme_conv_gpu(e,vxx,vxy,vxz,vyy,vyz,vzz)
GPU/source/MOD_interfaces.f:      procedure(tinker_void_sub) :: fphi_mpole_sitegpu,fphi_mpole_sitecu
GPU/source/MOD_interfaces.f:     &                             ,fphi_chg_sitecu,fphi_chg_sitegpu
GPU/source/MOD_interfaces.f:      procedure(fphi_uind_sitegpu2),pointer:: fphi_uind_site2_p
GPU/source/MOD_interfaces.f:      procedure(fphi_uind_sitegpu1),pointer:: fphi_uind_site1_p
GPU/source/MOD_interfaces.f:      procedure(grid_uind_sitegpu) ,pointer:: grid_uind_site_p
GPU/source/MOD_interfaces.f:      procedure(grid_mpole_sitegpu),pointer:: grid_mpole_site_p
GPU/source/MOD_interfaces.f:      procedure(grid_pchg_sitegpu) ,pointer:: grid_pchg_site_p
GPU/source/MOD_interfaces.f:      procedure(pme_conv_gpu)      ,pointer:: pme_conv_p
GPU/source/MOD_interfaces.f:        subroutine ecreal1dgpu
GPU/source/MOD_interfaces.f:#ifdef _CUDA
GPU/source/MOD_interfaces.f:        subroutine ecrealshortlong1dgpu
GPU/source/MOD_interfaces.f:        subroutine ecreal3dgpu
GPU/source/MOD_interfaces.f:#ifdef _CUDA
GPU/source/MOD_interfaces.f:      procedure(ecreal1dgpu),pointer:: ecreal1d_p,ecreal1d_cp
GPU/source/MOD_interfaces.f:      procedure(ecreal3dgpu),pointer:: ecreal3d_p
GPU/source/MOD_interfaces.f:        subroutine efld0_directgpu(nrhs,ef)
GPU/source/MOD_interfaces.f:        subroutine efld0_directgpu2(nrhs,ef)
GPU/source/MOD_interfaces.f:        subroutine otf_dc_efld0_directgpu2(nrhs,ef)
GPU/source/MOD_interfaces.f:        subroutine otf_dc_efld0_directgpu3(nrhs,ef)
GPU/source/MOD_interfaces.f:      procedure(efld0_directgpu2):: efld0_directgpu3
GPU/source/MOD_interfaces.f:      procedure(otf_dc_efld0_directgpu2)::oft_dc_efld0_directgpu3
GPU/source/MOD_interfaces.f:      procedure(efld0_directgpu2),pointer:: efld0_directgpu_p
GPU/source/MOD_interfaces.f:     &                           ,efld0_directgpu_p1,efld0_direct_cp
GPU/source/MOD_interfaces.f:      procedure(otf_dc_efld0_directgpu2),pointer:: 
GPU/source/MOD_interfaces.f:     &          otf_dc_efld0_directgpu_p
GPU/source/MOD_interfaces.f:        subroutine inducepcg_pme2gpu(matvec,nrhs,precnd,ef,mu,murec)
GPU/source/MOD_interfaces.f:           import tmatxb_pmegpu
GPU/source/MOD_interfaces.f:           procedure(tmatxb_pmegpu)::matvec
GPU/source/MOD_interfaces.f:        subroutine inducepcg_pmegpu(matvec,nrhs,precnd,ef,mu,murec)
GPU/source/MOD_interfaces.f:           import tmatxb_pmegpu
GPU/source/MOD_interfaces.f:           procedure(tmatxb_pmegpu)::matvec
GPU/source/MOD_interfaces.f:        subroutine inducestepcg_pme2gpu(matvec,nrhs,precnd,ef,mu,murec)
GPU/source/MOD_interfaces.f:           import tmatxb_pmegpu
GPU/source/MOD_interfaces.f:           procedure(tmatxb_pmegpu)::matvec
GPU/source/MOD_interfaces.f:        subroutine inducejac_pme2gpu(matvec,nrhs,dodiis,ef,mu,murec)
GPU/source/MOD_interfaces.f:           import tmatxb_pmegpu
GPU/source/MOD_interfaces.f:           procedure(tmatxb_pmegpu)::matvec
GPU/source/MOD_interfaces.f:        subroutine inducejac_pmegpu(matvec,nrhs,dodiis,ef,mu,murec)
GPU/source/MOD_interfaces.f:           import tmatxb_pmegpu
GPU/source/MOD_interfaces.f:           procedure(tmatxb_pmegpu)::matvec
GPU/source/MOD_interfaces.f:        subroutine inducepcg_shortrealgpu(matvec,nrhs,precnd,ef,mu)
GPU/source/MOD_interfaces.f:           import tmatxb_pmegpu
GPU/source/MOD_interfaces.f:           procedure(tmatxb_pmegpu)::matvec
GPU/source/MOD_interfaces.f:        subroutine epreal3dgpu
GPU/source/MOD_interfaces.f:        subroutine epreal1cgpu
GPU/source/MOD_interfaces.f:      procedure(epreal3dgpu)   ,pointer:: epreal3d_p
GPU/source/MOD_interfaces.f:      procedure(epreal1cgpu)   ,pointer:: epreal1c_p,epreal1c_cp
GPU/source/MOD_interfaces.f:!              Tinker CUDA(C/C++) Wrapper routines Interfaces
GPU/source/MOD_interfaces.f:#ifdef _OPENACC
GPU/source/MOD_interfaces.f:           import cuda_stream_kind
GPU/source/MOD_interfaces.f:           integer(cuda_stream_kind),value:: rec_stream
GPU/source/MOD_interfaces.f:        import cuda_stream_kind
GPU/source/MOD_interfaces.f:        integer(cuda_stream_kind),value::stream
GPU/source/MOD_interfaces.f:        import cuda_stream_kind
GPU/source/MOD_interfaces.f:        integer(cuda_stream_kind),value::stream
GPU/source/MOD_interfaces.f:        import cuda_stream_kind
GPU/source/MOD_interfaces.f:        integer(cuda_stream_kind),value::stream
GPU/source/MOD_interfaces.f:        import cuda_stream_kind
GPU/source/MOD_interfaces.f:        integer(cuda_stream_kind),value::stream
GPU/source/MOD_interfaces.f:        import cuda_stream_kind
GPU/source/MOD_interfaces.f:        integer(cuda_stream_kind),value:: stream
GPU/source/MOD_interfaces.f:        import cuda_stream_kind,c_int
GPU/source/MOD_interfaces.f:        integer(cuda_stream_kind),value:: stream
GPU/source/MOD_interfaces.f:        import cuda_stream_kind,c_int
GPU/source/MOD_interfaces.f:        integer(cuda_stream_kind),value:: stream
GPU/source/MOD_interfaces.f:        import cuda_stream_kind,c_int
GPU/source/MOD_interfaces.f:        integer(cuda_stream_kind),value:: stream
GPU/source/MOD_interfaces.f:        import cuda_stream_kind,c_int
GPU/source/MOD_interfaces.f:        integer(cuda_stream_kind),value:: stream
GPU/source/MOD_interfaces.f:        import cuda_stream_kind,c_int
GPU/source/MOD_interfaces.f:        integer(cuda_stream_kind),value:: stream
GPU/source/MOD_interfaces.f:        import cuda_stream_kind,c_int
GPU/source/MOD_interfaces.f:        integer(cuda_stream_kind),value:: stream
GPU/source/MOD_interfaces.f:      interface torquegpu
GPU/source/MOD_interfaces.f:         subroutine torquedgpu(trqvec,frx,fry,frz,de,extract)
GPU/source/MOD_interfaces.f:         subroutine torquefgpu(trqvec,frx,fry,frz,de,extract)
GPU/source/MOD_interfaces.f:         subroutine torquerfgpu(n,nb,pole,poleloc,trqvec,de,queue)
GPU/source/MOD_interfaces.f:         subroutine torquergpu(n,nb,pole,poleloc,trqvec,de,queue)
GPU/source/MOD_interfaces.f:      subroutine torquegpu_group (trqvec,frcx,frcy,frcz,de)
GPU/source/MOD_interfaces.f:      if (associated(otf_dc_efld0_directgpu_p))
GPU/source/MOD_interfaces.f:     &       nullify(otf_dc_efld0_directgpu_p)
GPU/source/MOD_interfaces.f:      if (associated(efld0_directgpu_p)) nullify(efld0_directgpu_p)
GPU/source/MOD_interfaces.f:      if (associated(efld0_directgpu_p1))nullify(efld0_directgpu_p1)
GPU/source/kangle.f:#ifdef _OPENACC
GPU/source/torquegpu.f:c     ##  subroutine torquegpu  --  convert single site torque to force  ##
GPU/source/torquegpu.f:c     "torque2gpu" takes the torque values on multiple sites defined by
GPU/source/torquegpu.f:      module torquegpu_inl
GPU/source/torquegpu.f:      use utilgpu
GPU/source/torquegpu.f:      subroutine torquedgpu (trqvec,frcx,frcy,frcz,de,extract)
GPU/source/torquegpu.f:      use torquegpu_inl
GPU/source/torquegpu.f:      if (deb_Path) write(*,'(3x,a)') 'torquedgpu'
GPU/source/torquegpu.f:           if(ca.le.10) print*,'WARNING!!! torquedgpu Outside loc'
GPU/source/torquegpu.f:           if(ca.le.10) print*,'WARNING!!! torquedgpu Outside loc'
GPU/source/torquegpu.f:      subroutine torquefgpu (trqvec,frcx,frcy,frcz,de,extract)
GPU/source/torquegpu.f:      use torquegpu_inl
GPU/source/torquegpu.f:      if (deb_Path) write(*,'(3x,a)') 'torquefgpu'
GPU/source/torquegpu.f:      subroutine torquerfgpu (natom,natombloc,polevec,ilocvec,
GPU/source/torquegpu.f:      use torquegpu_inl
GPU/source/torquegpu.f:      if (deb_Path) write(*,'(3x,a)') 'torquerfgpu'
GPU/source/torquegpu.f:      subroutine torquergpu (natom,natombloc,polevec,ilocvec,
GPU/source/torquegpu.f:      use torquegpu_inl
GPU/source/torquegpu.f:      if (deb_Path) write(*,'(3x,a)') 'torquergpu'
GPU/source/torquegpu.f:      subroutine torquegpu_group (trqvec,frcx,frcy,frcz,de)
GPU/source/torquegpu.f:      use torquegpu_inl
GPU/source/torquegpu.f:      if (deb_Path) write(*,'(3x,a)') 'torquegpu_group'
GPU/source/torquegpu.f:           if(ca.le.10) print*,'WARNING!!! torquegpu_group Outside loc'
GPU/source/torquegpu.f:           if(ca.le.10) print*,'WARNING!!! torquegpu_group Outside loc'
GPU/source/echargecu.tpl.f:c     Hal CUDA Kernel Template 
GPU/source/cluster.f:#ifdef _OPENACC
GPU/source/cluster.f:#ifdef _OPENACC
GPU/source/cluster.f:      use utilgpu ,only: rec_queue
GPU/source/cluster.f:#ifdef _OPENACC
GPU/source/cluster.f:#ifdef _OPENACC
GPU/source/cluster.f:#ifdef _OPENACC
GPU/source/cluster.f:#ifdef _OPENACC
GPU/source/cluster.f:#ifdef _OPENACC
GPU/source/promo.f:     &        /,1x,'##',23x,'Version 1.2(GPU) Octobre 2020',22x,'##',
GPU/source/epitors3.f:#ifndef _OPENACC
GPU/source/pair_charge.f.inc:#include "tinker_cudart.h"
GPU/source/dcinduce_pme.f:#ifdef _OPENACC
GPU/source/dcinduce_pme.f:#ifdef _OPENACC
GPU/source/dcinduce_pme.f:#ifdef _OPENACC
GPU/source/alterchg.gpu.f:      use utilgpu
GPU/source/alterchg.gpu.f:#ifndef _OPENACC
GPU/source/alterchg.gpu.f:#ifndef _OPENACC
GPU/source/alterchg.gpu.f:      use utilgpu ,only: def_queue
GPU/source/alterchg.gpu.f:      use utilgpu ,only: def_queue
GPU/source/MOD_orthogonalize.f:#ifdef _OPENACC
GPU/source/MOD_orthogonalize.f:#ifdef _OPENACC
GPU/source/eangle.f:      module eanglegpu_inl
GPU/source/eangle.f:      use eanglegpu_inl
GPU/source/eangle.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/eangle.f:      if(deb_Path) write(*,*) 'eanglegpu'
GPU/source/eangle.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/eangle.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/ker_tors.inc.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/library_host.make:MOD_precompute_polegpu.o \
GPU/source/library_host.make:MOD_utilgpu.o \
GPU/source/library_host.make:chkpolegpu.o \
GPU/source/library_host.make:dcinduce_pmegpu.o \
GPU/source/library_host.make:dcinduce_pme2gpu.o \
GPU/source/library_host.make:dcinduce_shortrealgpu.o \
GPU/source/library_host.make:eangle1gpu.o \
GPU/source/library_host.make:eangle3gpu.o  \
GPU/source/library_host.make:ebond1gpu.o \
GPU/source/library_host.make:ebond3gpu.o \
GPU/source/library_host.make:echarge1gpu.o \
GPU/source/library_host.make:echarge3gpu.o \
GPU/source/library_host.make:efld0_directgpu.o  \
GPU/source/library_host.make:egeom1gpu.o \
GPU/source/library_host.make:egeom3gpu.o \
GPU/source/library_host.make:ehal1gpu.o \
GPU/source/library_host.make:ehal3gpu.o \
GPU/source/library_host.make:eimprop1gpu.o \
GPU/source/library_host.make:eimptor1gpu.o \
GPU/source/library_host.make:elj1gpu.o \
GPU/source/library_host.make:elj3gpu.o \
GPU/source/library_host.make:empole1gpu.o \
GPU/source/library_host.make:empole3gpu.o \
GPU/source/library_host.make:eopbend1gpu.o \
GPU/source/library_host.make:eopbend3gpu.o \
GPU/source/library_host.make:epitors1gpu.o \
GPU/source/library_host.make:epolar1gpu.o \
GPU/source/library_host.make:epolar3gpu.o \
GPU/source/library_host.make:estrbnd1gpu.o \
GPU/source/library_host.make:estrtor1gpu.o \
GPU/source/library_host.make:etors1gpu.o \
GPU/source/library_host.make:etortor1gpu.o \
GPU/source/library_host.make:eurey1gpu.o  \
GPU/source/library_host.make:eurey3gpu.o \
GPU/source/library_host.make:nblistgpu.o \
GPU/source/library_host.make:newinduce_pmegpu.o \
GPU/source/library_host.make:newinduce_pme2gpu.o \
GPU/source/library_host.make:newinduce_shortrealgpu.o \
GPU/source/library_host.make:pmestuffgpu.o \
GPU/source/library_host.make:rotpolegpu.o  \
GPU/source/library_host.make:tmatxb_pmegpu.o \
GPU/source/library_host.make:torquegpu.o \
GPU/source/eopbend3gpu.f:      module eopbend3gpu_inl
GPU/source/eopbend3gpu.f:      subroutine eopbend3gpu_(deopb)
GPU/source/eopbend3gpu.f:      use eopbend3gpu_inl
GPU/source/eopbend3gpu.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/eopbend3gpu.f:      if(deb_Path) write(*,*) 'eopbend3gpu'
GPU/source/eopbend3gpu.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/eopbend3gpu.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/eopbend3gpu.f:      subroutine eopbend3gpu
GPU/source/eopbend3gpu.f:      call eopbend3gpu_(deopb)
GPU/source/initial.f:      use utilgpu
GPU/source/initial.f:      integer ::cuda_success=0
GPU/source/initial.f:c     Initialize GPU
GPU/source/initial.f:      use utilgpu
GPU/source/initial.f:#ifdef _OPENACC
GPU/source/initial.f:      use utilcu    ,only: copy_data_to_cuda_env
GPU/source/initial.f:c     Inititialize gpu stuff
GPU/source/initial.f:      ngpus         = 0
GPU/source/initial.f:#ifdef _OPENACC
GPU/source/initial.f:#ifdef _OPENACC
GPU/source/initial.f:            write(*,*) "*** Enabled Debug with [cuda-]Gdb ***"
GPU/source/initial.f:c     mapping parallelism for gpu execution
GPU/source/initial.f:#ifdef _OPENACC
GPU/source/initial.f:      call copy_data_to_cuda_env(nproc)
GPU/source/initial.f:      gpu_gangs   = 80
GPU/source/initial.f:      gpu_workers = 1
GPU/source/initial.f:      gpu_vector  = 32
GPU/source/initial.f:!$acc update device(rank,ngpus,devicenum,gpu_gangs
GPU/source/initial.f:!$acc&  ,gpu_workers,gpu_vector)
GPU/source/MOD_uprior.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/MOD_uprior.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/MOD_precompute_polegpu.f:      use utilgpu,only: dir_queue
GPU/source/MOD_precompute_polegpu.f:      use utilgpu , only : def_queue
GPU/source/MOD_precompute_polegpu.f:      use utilgpu , only : maxscaling1,maxscaling,dir_queue,warning
GPU/source/MOD_precompute_polegpu.f:      use utilgpu , only : def_queue
GPU/source/nblist_build.gpu.f:      use utilgpu
GPU/source/nblist_build.gpu.f:      use utilgpu
GPU/source/nblist_build.gpu.f:      use utilgpu ,only: BLOCK_SIZE
GPU/source/nblist_build.gpu.f:#ifdef _OPENACC
GPU/source/nblist_build.gpu.f:      use cudafor
GPU/source/nblist_build.gpu.f:      use utilgpu ,only: rec_queue,BLOCK_SIZE,inf,mem_set
GPU/source/nblist_build.gpu.f:#ifdef _OPENACC
GPU/source/nblist_build.gpu.f:#ifdef _CUDA
GPU/source/nblist_build.gpu.f:      !TODO Add an OpenACC version of this kernel
GPU/source/nblist_build.gpu.f:      use utilgpu ,only: mem_set,rec_stream
GPU/source/nblist_build.gpu.f:#ifdef _OPENACC
GPU/source/nblist_build.gpu.f:      use utilgpu ,only: rec_stream
GPU/source/nblist_build.gpu.f:      use cudafor
GPU/source/nblist_build.gpu.f:      use utilgpu ,only: rec_queue,BLOCK_SIZE,inf
GPU/source/nblist_build.gpu.f:#ifdef _CUDA
GPU/source/nblist_build.gpu.f:      !TODO Add an OpenACC version of this kernel
GPU/source/nblist_build.gpu.f:#ifdef _OPENACC
GPU/source/nblist_build.gpu.f:      use utilgpu ,only: rec_stream
GPU/source/nblist_build.gpu.f:      use cudafor
GPU/source/nblist_build.gpu.f:      use utilgpu ,only: rec_queue,BLOCK_SIZE,inf
GPU/source/nblist_build.gpu.f:#ifdef _CUDA
GPU/source/nblist_build.gpu.f:      !TODO Add an OpenACC version of this kernel
GPU/source/nblist_build.gpu.f:      subroutine vlistcellgpu
GPU/source/nblist_build.gpu.f:      use utilgpu
GPU/source/nblist_build.gpu.f:      if(deb_Path) write(*,*) 'vlistcellgpu'
GPU/source/nblist_build.gpu.f:c    "vlistcell2gpu" performs a complete rebuild of the
GPU/source/nblist_build.gpu.f:      subroutine vlistcell2gpu
GPU/source/nblist_build.gpu.f:      use utilgpu
GPU/source/nblist_build.gpu.f:      if(deb_Path) write(*,*) 'vlistcell2gpu'
GPU/source/nblist_build.gpu.f:c    "mlistcellgpu" rebuilds the regular electrostatic neighbor lists
GPU/source/nblist_build.gpu.f:      subroutine mlistcellgpu
GPU/source/nblist_build.gpu.f:      use utilgpu
GPU/source/nblist_build.gpu.f:      if(deb_Path) write(*,*) 'mlistcellgpu'
GPU/source/nblist_build.gpu.f:c    "mlistcell2gpu" performs a complete rebuild of the
GPU/source/nblist_build.gpu.f:      subroutine mlistcell2gpu
GPU/source/nblist_build.gpu.f:      use utilgpu
GPU/source/nblist_build.gpu.f:      if(deb_Path) write(*,*) 'mlistcell2gpu'
GPU/source/nblist_build.gpu.f:      subroutine clistcellgpu
GPU/source/nblist_build.gpu.f:      use utilgpu
GPU/source/nblist_build.gpu.f:      if(deb_Path) write(*,*) 'clistcellgpu'
GPU/source/nblist_build.gpu.f:c    "clistcell2gpu" performs a complete rebuild of the
GPU/source/nblist_build.gpu.f:      subroutine clistcell2gpu
GPU/source/nblist_build.gpu.f:      use utilgpu
GPU/source/nblist_build.gpu.f:      if(deb_Path) write(*,*) 'clistcell2gpu'
GPU/source/rattle.f:c#ifdef _OPENACC
GPU/source/rattle.f:c#ifdef _OPENACC
GPU/source/energy.f:      use utilgpu,only: inf_r
GPU/source/energy.f:         if (vdwtyp .eq. 'LENNARD-JONES')  call elj3gpu
GPU/source/energy.f:         if (vdwtyp .eq. 'BUFFERED-14-7')  call ehal3gpu
GPU/source/energy.f:      !if (use_repuls)  call erepel3gpu  !TODO Amoebap Uncomment
GPU/source/energy.f:      !if (use_disp  )  call edisp3gpu
GPU/source/energy.f:      if (use_charge) call echarge3gpu
GPU/source/energy.f:      if (use_mpole)  call empole3gpu
GPU/source/energy.f:      if (use_polar)  call epolar3gpu
GPU/source/energy.f:      if (use_chgtrn) call echgtrn3gpu
GPU/source/kvdw.f:      use utilgpu,only : prmem_request,rec_queue
GPU/source/kvdw.f:#ifdef _OPENACC
GPU/source/kvdw.f:#ifdef _OPENACC
GPU/source/kvdw.f:#ifdef _OPENACC
GPU/source/kvdw.f:#ifdef _OPENACC
GPU/source/kimprop.f:#ifdef _OPENACC
GPU/source/kimprop.f:      use utilgpu  ,only:rec_stream
GPU/source/kimprop.f:#ifdef _OPENACC
GPU/source/epolar3gpu.f:      module epolar3gpu_inl
GPU/source/epolar3gpu.f:      subroutine epolar3gpu
GPU/source/epolar3gpu.f:        call elambdapolar3cgpu
GPU/source/epolar3gpu.f:        call epolar3cgpu
GPU/source/epolar3gpu.f:c     ##  subroutine epolar3cgpu  --  Ewald polarization analysis; list  ##
GPU/source/epolar3gpu.f:      subroutine epolar3cgpu
GPU/source/epolar3gpu.f:      use epolar3gpu_inl
GPU/source/epolar3gpu.f:      use utilgpu
GPU/source/epolar3gpu.f:      if (deb_Path) write(*,*) 'epolar3cgpu'
GPU/source/epolar3gpu.f:         call newinduce_shortrealgpu
GPU/source/epolar3gpu.f:            call newinduce_pmegpu
GPU/source/epolar3gpu.f:            call dcinduce_pme2gpu
GPU/source/epolar3gpu.f:            call newinduce_pme2gpu
GPU/source/epolar3gpu.f:#ifdef _OPENACC
GPU/source/epolar3gpu.f:            call eprecipgpu
GPU/source/epolar3gpu.f:      subroutine elambdapolar3cgpu
GPU/source/epolar3gpu.f:      use epolar1gpu_inl
GPU/source/epolar3gpu.f:      use utilgpu
GPU/source/epolar3gpu.f:#ifdef _OPENACC
GPU/source/epolar3gpu.f:         call rotpolegpu
GPU/source/epolar3gpu.f:         call epolar3cgpu
GPU/source/epolar3gpu.f:      call rotpolegpu
GPU/source/epolar3gpu.f:      call epolar3cgpu
GPU/source/epolar3gpu.f:      call rotpolegpu
GPU/source/epolar3gpu.f:c     ##  subroutine epreal3dgpu  --  real space polar analysis via list  ##
GPU/source/epolar3gpu.f:      subroutine epreal3dgpu
GPU/source/epolar3gpu.f:      use epolar3gpu_inl
GPU/source/epolar3gpu.f:      use utilgpu
GPU/source/epolar3gpu.f:     &   write(*,'(2x,a)') 'epreal3dgpu'
GPU/source/epolar3gpu.f:#ifdef _CUDA
GPU/source/epolar3gpu.f:#ifdef _CUDA
GPU/source/epolar3gpu.f:      use cudafor
GPU/source/epolar3gpu.f:      use utilgpu ,only: def_queue,dir_queue,rec_queue
GPU/source/epolar3gpu.f:#ifdef _CUDA
GPU/source/epolar3gpu.f:         call cudaMaxGridSize("epreal3_cu",gS)
GPU/source/epolar3gpu.f:      use epolar3gpu_inl
GPU/source/epolar3gpu.f:      use utilgpu ,only: def_queue,real3,real6,real3_red,rpole_elt
GPU/source/epolar3gpu.f:c     ##  subroutine eprecipgpu  --  PME recip space polarization energy  ##
GPU/source/epolar3gpu.f:      subroutine eprecipgpu
GPU/source/epolar3gpu.f:      use utilgpu,only:rec_queue
GPU/source/epolar3gpu.f:      if (deb_Path) write(*,'(2x,a)') 'eprecipgpu'
GPU/source/kcharge.f:#ifdef _OPENACC
GPU/source/kcharge.f:      use utilgpu
GPU/source/kcharge.f:#ifdef _OPENACC
GPU/source/cholesky.f:      use utilgpu,only:def_queue
GPU/source/kdisp.f:      use utilgpu
GPU/source/kopbend.f:      use utilgpu
GPU/source/kopbend.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/kopbend.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/kopbend.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/kopbend.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/kopbend.f:#ifdef _OPENACC
GPU/source/kopbend.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/estrbnd1gpu.f:      module estrbnd1gpu_inl
GPU/source/estrbnd1gpu.f:      subroutine estrbnd1gpu_(isb,typeA,anat,bl,deba,deaMD,s)
GPU/source/estrbnd1gpu.f:      use estrbnd1gpu_inl
GPU/source/estrbnd1gpu.f:      if(deb_Path) write(*,*) 'estrbnd1gpu'
GPU/source/estrbnd1gpu.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/estrbnd1gpu.f:      subroutine estrbnd1gpu
GPU/source/estrbnd1gpu.f:      subroutine estrbnd1gpu_(isb,typeA,anat,bl,deba,deW1aMD,s)
GPU/source/estrbnd1gpu.f:      call estrbnd1gpu_(isb,type,anat,bl,deba,deW1aMD,max(s,1))
GPU/source/pmestuffgpu.f90:subroutine bspline_fill_sitegpu(config)
GPU/source/pmestuffgpu.f90:   use utilgpu  ,only:rec_queue,nSMP
GPU/source/pmestuffgpu.f90:#ifdef _OPENACC
GPU/source/pmestuffgpu.f90:#ifdef _OPENACC
GPU/source/pmestuffgpu.f90:#ifdef _OPENACC
GPU/source/pmestuffgpu.f90:subroutine grid_mpole_sitegpu(fmpvec)
GPU/source/pmestuffgpu.f90:   use utilgpu,only:rec_queue,ngangs_rec
GPU/source/pmestuffgpu.f90:   if (deb_Path) write(*,'(4X,A)') "grid_mpole_sitegpu"
GPU/source/pmestuffgpu.f90:!     "grid_pchg_sitegpu" places the i-th fractional atomic charge onto
GPU/source/pmestuffgpu.f90:subroutine grid_pchg_sitegpu
GPU/source/pmestuffgpu.f90:!     "grid_disp_sitegpu" places the i-th damped dispersion coefficients onto
GPU/source/pmestuffgpu.f90:subroutine grid_disp_sitegpu
GPU/source/pmestuffgpu.f90:   use utilgpu,only: rec_queue
GPU/source/pmestuffgpu.f90:subroutine grid_uind_sitegpu(fuindvec,fuinpvec,qgrid2loc)
GPU/source/pmestuffgpu.f90:   use utilgpu,only:rec_queue,ngangs_rec
GPU/source/pmestuffgpu.f90:   if (deb_Path)  print '(5x,a)','grid_uind_sitegpu'
GPU/source/pmestuffgpu.f90:subroutine fphi_mpole_sitegpu
GPU/source/pmestuffgpu.f90:   use utilgpu
GPU/source/pmestuffgpu.f90:   if (deb_Path) write(*,'(5x,a)') "fphi_mpole_sitegpu"
GPU/source/pmestuffgpu.f90:subroutine fphi_chg_sitegpu
GPU/source/pmestuffgpu.f90:   use utilgpu
GPU/source/pmestuffgpu.f90:   if (deb_Path) write(*,'(5x,a)') "fphi_chg_sitegpu"
GPU/source/pmestuffgpu.f90:   use utilgpu,only: ngangs_rec,rec_queue
GPU/source/pmestuffgpu.f90:subroutine fphi_uind_sitegpu1(fdip_phi1,fdip_phi2,fdip_sum_phi)
GPU/source/pmestuffgpu.f90:   use utilgpu,only: rec_queue,ngangs_rec
GPU/source/pmestuffgpu.f90:   if (deb_Path) write(*,'(4X,A)') "fphi_uind_sitegpu1"
GPU/source/pmestuffgpu.f90:!     "fphi_uind_sitegpu2" extracts the induced dipole potential at the i-th site from
GPU/source/pmestuffgpu.f90:subroutine fphi_uind_sitegpu2(fdip_phi1,fdip_phi2)
GPU/source/pmestuffgpu.f90:   use utilgpu,only: rec_queue,ngangs_rec
GPU/source/pmestuffgpu.f90:   if (deb_Path) write(*,'(4X,A)') "fphi_uind_sitegpu2"
GPU/source/pmestuffgpu.f90:subroutine pme_conv_gpu(e,vxx,vxy,vxz,vyy,vyz,vzz)
GPU/source/pmestuffgpu.f90:   use utilgpu
GPU/source/pmestuffgpu.f90:   if(deb_Path) write(*,'(4x,A)') 'pme_conv_gpu'
GPU/source/pmestuffgpu.f90:subroutine pme_convd_gpu(e,vxx,vxy,vxz,vyy,vyz,vzz)
GPU/source/pmestuffgpu.f90:   use utilgpu
GPU/source/pmestuffgpu.f90:   if(deb_Path) write(*,'(4x,A)') 'pme_convd_gpu'
GPU/source/pmestuffgpu.f90:#ifdef _CUDA
GPU/source/pmestuffgpu.f90:   use utilgpu   ,only:rec_stream,rec_queue,nSMP
GPU/source/pmestuffgpu.f90:      call cudaMaxgridsize("grid_mpole_core",gS)
GPU/source/pmestuffgpu.f90:!     the particle mesh Ewald grid (CUDA Routine)
GPU/source/pmestuffgpu.f90:   use utilgpu ,only: rec_stream
GPU/source/pmestuffgpu.f90:      call cudaMaxgridsize("grid_put_site_kcu1",gS)
GPU/source/pmestuffgpu.f90:   use utilgpu   ,only:rec_stream,rec_queue,nSMP
GPU/source/pmestuffgpu.f90:      call cudaMaxgridsize("grid_uind_core",gS)
GPU/source/pmestuffgpu.f90:   use utilgpu   ,only:rec_stream,rec_queue,nSMP
GPU/source/pmestuffgpu.f90:      call cudaMaxgridsize("fphi_mpole_core",gS)
GPU/source/pmestuffgpu.f90:   use utilgpu,only: rec_queue,rec_stream
GPU/source/pmestuffgpu.f90:      call cudaMaxgridsize("grid_calc_frc_kcu",gS)
GPU/source/pmestuffgpu.f90:      call cudaMaxgridsize("grid_calc_frc_kcu1",gS1)
GPU/source/pmestuffgpu.f90:   use utilgpu,only: rec_queue,rec_stream
GPU/source/pmestuffgpu.f90:      call cudaMaxgridsize("grid_calc_frc_kcu",gS)
GPU/source/pmestuffgpu.f90:      call cudaMaxgridsize("grid_calc_frc_kcu1",gS1)
GPU/source/pmestuffgpu.f90:!       the particle mesh Ewald grid ( CUDA Fortran wrapper )
GPU/source/pmestuffgpu.f90:   use utilgpu
GPU/source/pmestuffgpu.f90:      call cudaMaxgridsize("fphi_chg_site_kcu",gS)
GPU/source/pmestuffgpu.f90:   use utilgpu   ,only:rec_stream,rec_queue,nSMP
GPU/source/pmestuffgpu.f90:         call cudamaxgridSize("fphi_uind_sitecu2_core_1p",gS)
GPU/source/pmestuffgpu.f90:         call cudamaxgridSize("fphi_uind_sitecu2",gS)
GPU/source/pmestuffgpu.f90:   use utilgpu   ,only:rec_stream,rec_queue,nSMP
GPU/source/pmestuffgpu.f90:      call cudamaxgridSize("fphi_uind_sitecu1",gS)
GPU/source/pmestuffgpu.f90:!     "pme_conv_cu" apply convolution on cuda device
GPU/source/pmestuffgpu.f90:   use utilgpu   ,only: reduce_energy_virial,ered_buff,vred_buf1&
GPU/source/pmestuffgpu.f90:   use utilgpu   ,only: reduce_energy_virial,ered_buff,vred_buf1&
GPU/source/pmestuffgpu.f90:subroutine cudaMaxGridSize(kernelname,gS)
GPU/source/pmestuffgpu.f90:   use cudafor
GPU/source/pmestuffgpu.f90:   use utilgpu
GPU/source/pmestuffgpu.f90:200 format("error",I5," returned when calling Cuda0ccupancy"//&
GPU/source/pmestuffgpu.f90:      &"MaxActiveBlocksPerMulti in cudamaxGridSize subroutine",&
GPU/source/pmestuffgpu.f90:201 format("cudaMaxGridSize argument ( ", A,&
GPU/source/pmestuffgpu.f90:      &" ) does not match any known Cuda Kernel", /,&
GPU/source/pmestuffgpu.f90:      ierr = CUDAOCCUPANCYMAXACTIVEBLOCKSPERMULTIPROCESSOR&
GPU/source/pmestuffgpu.f90:      if (ierr.ne.cudaSuccess)&
GPU/source/pmestuffgpu.f90:         &print 200, ierr,cudageterrorstring(ierr)
GPU/source/pmestuffgpu.f90:      ierr = CUDAOCCUPANCYMAXACTIVEBLOCKSPERMULTIPROCESSOR&
GPU/source/pmestuffgpu.f90:      if (ierr.ne.cudaSuccess)&
GPU/source/pmestuffgpu.f90:         &print 200, ierr,cudageterrorstring(ierr)
GPU/source/pmestuffgpu.f90:      ierr = CUDAOCCUPANCYMAXACTIVEBLOCKSPERMULTIPROCESSOR&
GPU/source/pmestuffgpu.f90:      if (ierr.ne.cudaSuccess)&
GPU/source/pmestuffgpu.f90:         &print 200, ierr,cudageterrorstring(ierr)
GPU/source/pmestuffgpu.f90:      ierr = CUDAOCCUPANCYMAXACTIVEBLOCKSPERMULTIPROCESSOR&
GPU/source/pmestuffgpu.f90:      if (ierr.ne.cudaSuccess)&
GPU/source/pmestuffgpu.f90:         &print 200, ierr,cudageterrorstring(ierr)
GPU/source/pmestuffgpu.f90:      ierr = CUDAOCCUPANCYMAXACTIVEBLOCKSPERMULTIPROCESSOR&
GPU/source/pmestuffgpu.f90:      if (ierr.ne.cudaSuccess)&
GPU/source/pmestuffgpu.f90:         &print 200, ierr,cudageterrorstring(ierr)
GPU/source/pmestuffgpu.f90:      ierr = CUDAOCCUPANCYMAXACTIVEBLOCKSPERMULTIPROCESSOR&
GPU/source/pmestuffgpu.f90:      if (ierr.ne.cudaSuccess)&
GPU/source/pmestuffgpu.f90:         &print 200, ierr,cudageterrorstring(ierr)
GPU/source/pmestuffgpu.f90:      ierr = CUDAOCCUPANCYMAXACTIVEBLOCKSPERMULTIPROCESSOR&
GPU/source/pmestuffgpu.f90:      if (ierr.ne.cudaSuccess)&
GPU/source/pmestuffgpu.f90:         &print 200, ierr,cudageterrorstring(ierr)
GPU/source/pmestuffgpu.f90:      ierr = CUDAOCCUPANCYMAXACTIVEBLOCKSPERMULTIPROCESSOR&
GPU/source/pmestuffgpu.f90:      if (ierr.ne.cudaSuccess)&
GPU/source/pmestuffgpu.f90:         &print 200, ierr,cudageterrorstring(ierr)
GPU/source/pmestuffgpu.f90:      ierr = CUDAOCCUPANCYMAXACTIVEBLOCKSPERMULTIPROCESSOR&
GPU/source/pmestuffgpu.f90:      if (ierr.ne.cudaSuccess)&
GPU/source/pmestuffgpu.f90:         &print 200, ierr,cudageterrorstring(ierr)
GPU/source/pmestuffgpu.f90:      ierr = CUDAOCCUPANCYMAXACTIVEBLOCKSPERMULTIPROCESSOR&
GPU/source/pmestuffgpu.f90:      if (ierr.ne.cudaSuccess)&
GPU/source/pmestuffgpu.f90:         &print 200, ierr,cudageterrorstring(ierr)
GPU/source/pmestuffgpu.f90:      ierr = CUDAOCCUPANCYMAXACTIVEBLOCKSPERMULTIPROCESSOR&
GPU/source/pmestuffgpu.f90:      if (ierr.ne.cudaSuccess)&
GPU/source/pmestuffgpu.f90:         &print 200, ierr,cudageterrorstring(ierr)
GPU/source/pmestuffgpu.f90:      ierr = CUDAOCCUPANCYMAXACTIVEBLOCKSPERMULTIPROCESSOR&
GPU/source/pmestuffgpu.f90:      if (ierr.ne.cudaSuccess)&
GPU/source/pmestuffgpu.f90:         &print 200, ierr,cudageterrorstring(ierr)
GPU/source/pmestuffgpu.f90:      ierr = CUDAOCCUPANCYMAXACTIVEBLOCKSPERMULTIPROCESSOR&
GPU/source/pmestuffgpu.f90:      if (ierr.ne.cudaSuccess)&
GPU/source/pmestuffgpu.f90:         &print 200, ierr,cudageterrorstring(ierr)
GPU/source/pmestuffgpu.f90:      ierr = CUDAOCCUPANCYMAXACTIVEBLOCKSPERMULTIPROCESSOR&
GPU/source/pmestuffgpu.f90:      if (ierr.ne.cudaSuccess)&
GPU/source/pmestuffgpu.f90:         &print 200, ierr,cudageterrorstring(ierr)
GPU/source/pmestuffgpu.f90:      ierr = CUDAOCCUPANCYMAXACTIVEBLOCKSPERMULTIPROCESSOR&
GPU/source/pmestuffgpu.f90:      if (ierr.ne.cudaSuccess)&
GPU/source/pmestuffgpu.f90:         &print 200, ierr,cudageterrorstring(ierr)
GPU/source/pmestuffgpu.f90:      ierr = CUDAOCCUPANCYMAXACTIVEBLOCKSPERMULTIPROCESSOR&
GPU/source/pmestuffgpu.f90:      if (ierr.ne.cudaSuccess)&
GPU/source/pmestuffgpu.f90:         &print 200, ierr,cudageterrorstring(ierr)
GPU/source/pmestuffgpu.f90:      ierr = CUDAOCCUPANCYMAXACTIVEBLOCKSPERMULTIPROCESSOR&
GPU/source/pmestuffgpu.f90:      if (ierr.ne.cudaSuccess)&
GPU/source/pmestuffgpu.f90:         &print 200, ierr,cudageterrorstring(ierr)
GPU/source/pmestuffgpu.f90:      ierr = CUDAOCCUPANCYMAXACTIVEBLOCKSPERMULTIPROCESSOR&
GPU/source/pmestuffgpu.f90:      if (ierr.ne.cudaSuccess)&
GPU/source/pmestuffgpu.f90:         &print 200, ierr,cudageterrorstring(ierr)
GPU/source/pmestuffgpu.f90:      ierr = CUDAOCCUPANCYMAXACTIVEBLOCKSPERMULTIPROCESSOR&
GPU/source/pmestuffgpu.f90:      if (ierr.ne.cudaSuccess)&
GPU/source/pmestuffgpu.f90:         &print 200, ierr,cudageterrorstring(ierr)
GPU/source/pmestuffgpu.f90:      ierr = CUDAOCCUPANCYMAXACTIVEBLOCKSPERMULTIPROCESSOR&
GPU/source/pmestuffgpu.f90:      if (ierr.ne.cudaSuccess)&
GPU/source/pmestuffgpu.f90:         &print 200, ierr,cudageterrorstring(ierr)
GPU/source/pmestuffgpu.f90:#ifdef _CUDA
GPU/source/pmestuffgpu.f90:# ifdef _CUDA
GPU/source/pmestuffgpu.f90:subroutine cmp_to_fmp_sitegpu(cmp,fmp)
GPU/source/pmestuffgpu.f90:   use utilgpu   ,only:rec_queue,ctf
GPU/source/pmestuffgpu.f90:      call cart_to_fracgpu
GPU/source/pmestuffgpu.f90:subroutine fphi_to_cphi_sitegpu(fphi,cphi)
GPU/source/pmestuffgpu.f90:   use utilgpu   ,only:rec_queue,ftc
GPU/source/pmestuffgpu.f90:      call frac_to_cartgpu
GPU/source/cu_tmatxb_pme.cu:cudaDeviceProp devProp;
GPU/source/cu_tmatxb_pme.cu:   cudaGetDeviceProperties(&devProp,devicenum_);
GPU/source/cu_tmatxb_pme.cu:void cu_efld0_direct(EFLD0_PARAMS,cudaStream_t st){
GPU/source/cu_tmatxb_pme.cu:   cudaError_t ierrSync;
GPU/source/cu_tmatxb_pme.cu:      cudaKernelMaxGridSize(gS_efld,cu_efld0_direct_core,BLOCK_DIM,0)  /* This a Macro Function */
GPU/source/cu_tmatxb_pme.cu:      if (aewald!=(real)0.0) { cudaKernelMaxGridSize(gS_efld1,efld0_direct_kcu<DIRDAMP+EWALD>,BLOCK_DIM,0) }  /* This a Macro Function */
GPU/source/cu_tmatxb_pme.cu:      else                   { cudaKernelMaxGridSize(gS_efld1,efld0_direct_kcu<DIRDAMP      >,BLOCK_DIM,0) }
GPU/source/cu_tmatxb_pme.cu:         cudaKernelMaxGridSize(gS_loc,check_loc,BLOCK_DIM,0)  /* This a Macro Function */
GPU/source/cu_tmatxb_pme.cu:         ierrSync = tinkerdebug>0 ? cudaDeviceSynchronize() : cudaGetLastError();
GPU/source/cu_tmatxb_pme.cu:         if (ierrSync != cudaSuccess) printf("check_loc kernel error: %d ( %s )\n",ierrSync,cudaGetErrorString(ierrSync));
GPU/source/cu_tmatxb_pme.cu:      if (useLambdaDyn){ printf(" Lambda Dynamic is unavailable with DIRECT DAMPING model on CUDA Kernel !\n  - Add `run-mode   legacy` into keyfile"); exit(1); } 
GPU/source/cu_tmatxb_pme.cu:   ierrSync = tinkerdebug>0 ? cudaDeviceSynchronize() : cudaGetLastError();
GPU/source/cu_tmatxb_pme.cu:   if (ierrSync != cudaSuccess) printf("%s C kernel error: %d ( %s )\n",rname,ierrSync,cudaGetErrorString(ierrSync));
GPU/source/cu_tmatxb_pme.cu:void cu_otfdc_efld0_direct(OTFDC_EFLD0_PARAMS,cudaStream_t st){
GPU/source/cu_tmatxb_pme.cu:      cudaKernelMaxGridSize(gS_otfdc_efld,cu_otfdc_efld0_direct_core,BLOCK_DIM,0)  /* This a Macro Function */
GPU/source/cu_tmatxb_pme.cu:   if  (tinkerdebug>0) gpuErrchk( cudaDeviceSynchronize() )
GPU/source/cu_tmatxb_pme.cu:   else                gpuErrchk( cudaGetLastError() )
GPU/source/cu_tmatxb_pme.cu:void cu_tmatxb_pme(TMATXB_PARAMS,cudaStream_t st){
GPU/source/cu_tmatxb_pme.cu:      cudaKernelMaxGridSize(gS_tmat,cu_tmatxb_pme_core,BLOCK_DIM,0)  /* This a Macro Function */
GPU/source/cu_tmatxb_pme.cu:   cudaError_t ierrSync;
GPU/source/cu_tmatxb_pme.cu:   if(tinkerdebug>0) ierrSync = cudaDeviceSynchronize();
GPU/source/cu_tmatxb_pme.cu:   else              ierrSync = cudaGetLastError();
GPU/source/cu_tmatxb_pme.cu:   if (ierrSync != cudaSuccess)
GPU/source/cu_tmatxb_pme.cu:      printf("tmatxb_pme_core C kernel error: %d \n  %s",ierrSync, cudaGetErrorString(ierrSync));
GPU/source/pair_lj.inc.f:#include "tinker_cudart.h"
GPU/source/kstrtor.f:#ifdef _OPENACC
GPU/source/kstrtor.f:      use utilgpu  ,only:rec_stream
GPU/source/kstrtor.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/kstrtor.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/kstrtor.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/kstrtor.f:#ifdef _OPENACC
GPU/source/echarge1gpu.f:      module echarge1gpu_inl
GPU/source/echarge1gpu.f:        use utilgpu , only: real3,real3_red,rpole_elt
GPU/source/echarge1gpu.f:      subroutine echarge1gpu
GPU/source/echarge1gpu.f:c        call elambdacharge1cgpu
GPU/source/echarge1gpu.f:        call echarge1cgpu
GPU/source/echarge1gpu.f:      subroutine echarge1cgpu
GPU/source/echarge1gpu.f:      use echarge1gpu_inl
GPU/source/echarge1gpu.f:      use utilgpu
GPU/source/echarge1gpu.f:      if (deb_Path)  write(*,'(1x,a)') 'echarge1cgpu'
GPU/source/echarge1gpu.f:            call ecrecip1gpu
GPU/source/echarge1gpu.f:c      subroutine elambdacharge1cgpu
GPU/source/echarge1gpu.f:c      use echarge1gpu_inl
GPU/source/echarge1gpu.f:c      use utilgpu
GPU/source/echarge1gpu.f:c#ifdef _OPENACC
GPU/source/echarge1gpu.f:c      if (deb_Path)  write(*,'(1x,a)') 'elambdacharge1cgpu'
GPU/source/echarge1gpu.f:c#ifdef _OPENACC
GPU/source/echarge1gpu.f:c            call ecrecip1gpu
GPU/source/echarge1gpu.f:c            call ecrecip1gpu
GPU/source/echarge1gpu.f:c     "ecreal1dgpu" evaluates the real space portion of the Ewald sum
GPU/source/echarge1gpu.f:      subroutine ecreal1dgpu
GPU/source/echarge1gpu.f:      use echarge1gpu_inl
GPU/source/echarge1gpu.f:      use utilgpu
GPU/source/echarge1gpu.f:      if (deb_Path) write(*,'(2x,a)') 'ecreal1dgpu'
GPU/source/echarge1gpu.f:#ifdef _CUDA
GPU/source/echarge1gpu.f:      use utilgpu ,only: def_queue,dir_queue,dir_stream,def_stream
GPU/source/echarge1gpu.f:         call cudaMaxGridSize("ecreal1_kcu",gS)
GPU/source/echarge1gpu.f:      use cudafor, only: dim3
GPU/source/echarge1gpu.f:      use utilgpu ,only: def_queue,dir_queue,vred_buff
GPU/source/echarge1gpu.f:         call cudaMaxGridSize("echg_lj1_kcu_v0",gS)
GPU/source/echarge1gpu.f:c     ##  subroutine ecrecip1gpu  --  PME recip charge energy & derivs  ##
GPU/source/echarge1gpu.f:      subroutine ecrecip1gpu
GPU/source/echarge1gpu.f:#ifdef _OPENACC
GPU/source/echarge1gpu.f:      use utilgpu
GPU/source/echarge1gpu.f:      if (deb_Path) write(*,'(2x,A)') 'ecrecip1gpu'
GPU/source/echarge1gpu.f:#ifdef _OPENACC
GPU/source/echarge1gpu.f:     &   call bspline_fill_sitegpu(1)
GPU/source/echarge1gpu.f:      call bspline_fill_sitegpu(1)
GPU/source/echarge1gpu.f:#ifdef _OPENACC
GPU/source/echarge1gpu.f:#ifdef _OPENACC
GPU/source/echarge1gpu.f:#ifdef _OPENACC
GPU/source/echarge1gpu.f:c         call fphi_to_cphi_chg_sitegpu (fphirec,cphirec)
GPU/source/echarge1gpu.f:#ifdef _OPENACC
GPU/source/echgtrncu.f:#include "tinker_cudart.h"
GPU/source/echgtrncu.f:      use utilgpu  ,only: RED_BUFF_SIZE,WARP_SIZE
GPU/source/Makefile.host:   MOD_vdw.o MOD_utilbaoab.o MOD_utilbaoabpi.o MOD_utils.o MOD_utilcomm.o MOD_utilvec.o MOD_utilgpu.o\
GPU/source/Makefile.host:   MOD_precompute_polegpu.o MOD_USampling.o
GPU/source/Makefile.host:   chkpole.o chkpolegpu.o chkring.o chkxyz.o cholesky.o cluster.o\
GPU/source/Makefile.host:   dcinduce_pme.o dcinduce_pmegpu.o dcinduce_pme2.o dcinduce_pme2gpu.o dcinduce_shortreal.o dcinduce_shortrealgpu.o\
GPU/source/Makefile.host:   eangle.o eangle1.o eangle1gpu.o eangle3.o eangle3gpu.o\
GPU/source/Makefile.host:   ebond.o ebond1.o ebond1gpu.o ebond3.o ebond3gpu.o\
GPU/source/Makefile.host:   echarge.o echarge1.o echarge1vec.o echarge1gpu.o echarge3.o echarge3vec.o echarge3gpu.o\
GPU/source/Makefile.host:   efld0_direct.o efld0_directvec.o efld0_directgpu.o\
GPU/source/Makefile.host:   egeom.o egeom1.o egeom1gpu.o egeom3.o egeom3gpu.o\
GPU/source/Makefile.host:   ehal.o ehal1.o ehal1vec.o ehal1gpu.o ehal3.o ehal3vec.o ehal3gpu.o\
GPU/source/Makefile.host:   elj.o elj1.o elj1vec.o eimprop1gpu.o elj1gpu.o elj3.o elj3vec.o elj3gpu.o\
GPU/source/Makefile.host:   empole.o empole0.o empole1.o empole1vec.o empole1gpu.o empole1_group.o\
GPU/source/Makefile.host:   empole3.o  empole3vec.o empole3gpu.o empole3_group.o\
GPU/source/Makefile.host:   energy.o eopbend.o eopbend1.o eopbend1gpu.o  eopbend3.o eopbend3gpu.o\
GPU/source/Makefile.host:   epitors.o epitors1.o epitors1gpu.o epitors3.o\
GPU/source/Makefile.host:   epolar.o epolarvec.o epolar1.o epolar1vec.o epolar1gpu.o epolar1_group.o\
GPU/source/Makefile.host:   epolar3.o epolar3vec.o epolar3gpu.o epolar3_group.o\
GPU/source/Makefile.host:   estrbnd.o estrbnd1.o estrbnd1gpu.o estrbnd3.o\
GPU/source/Makefile.host:   estrtor.o estrtor1.o estrtor1gpu.o estrtor3.o\
GPU/source/Makefile.host:   etors.o etors1.o etors1gpu.o etors3.o\
GPU/source/Makefile.host:   etortor.o etortor1.o etortor1gpu.o etortor3.o\
GPU/source/Makefile.host:   eurey.o eurey1.o eurey1gpu.o eurey3.o  eurey3gpu.o\
GPU/source/Makefile.host:OBJSNZ = nblist.o nblistvec.o nblistgpu.o newinduce_pme.o newinduce_pmevec.o newinduce_pmegpu.o\
GPU/source/Makefile.host:   newinduce_pme2.o newinduce_pme2vec.o newinduce_pme2gpu.o newinduce_group.o\
GPU/source/Makefile.host:   newinduce_shortreal.o newinduce_shortrealgpu.o\
GPU/source/Makefile.host:   numeral.o orthogonalize.o openend.o optsave.o pimd.o plumed.o pmestuff.o pmestuffgpu.o\
GPU/source/Makefile.host:   rings.o rotpole.o rotpolegpu.o search.o sort.o\
GPU/source/Makefile.host:   suffix.o switch.o switch_group.o temper.o testgrad.o tmatxb_pme.o tmatxb_pmevec.o tmatxb_pmegpu.o\
GPU/source/Makefile.host:   torphase.o torque.o torquegpu.o torquevec2.o torsions.o trimtext.o\
GPU/source/MOD_random.f:#ifdef _OPENACC
GPU/source/MOD_random.f:      use openacc
GPU/source/MOD_random.f:      !use cudafor
GPU/source/MOD_random.f:      !use openacc_curand
GPU/source/MOD_random.f:#ifdef _OPENACC
GPU/source/MOD_random.f:#ifdef _OPENACC
GPU/source/MOD_random.f:     &         ,init_curand_engine,randomgpu,normalgpu,rand_unitgpu
GPU/source/MOD_random.f:     &         ,normalgpuR4,reset_curand_seed,disp_ranvec
GPU/source/MOD_random.f:      interface normalgpu
GPU/source/MOD_random.f:         module procedure normalgpuR4
GPU/source/MOD_random.f:         module procedure normalgpuR8
GPU/source/MOD_random.f:#ifdef _OPENACC
GPU/source/MOD_random.f:          call normalgpuR4(array,n,mean_,stddev_,stream_)
GPU/source/MOD_random.f:#ifdef _OPENACC
GPU/source/MOD_random.f:           call normalgpuR8(array,n,mean_,stddev_,stream_)
GPU/source/MOD_random.f:#ifdef _OPENACC
GPU/source/MOD_random.f:c      All functions within this scope are specific to the GPU
GPU/source/MOD_random.f:c     Set cuda stream if necessary
GPU/source/MOD_random.f:     &                       acc_get_cuda_stream(stream))
GPU/source/MOD_random.f:c     get a sample following uniform distribution on GPU
GPU/source/MOD_random.f:      subroutine randomgpu(array,n,stream_)
GPU/source/MOD_random.f:     &                       acc_get_cuda_stream(stream_))
GPU/source/MOD_random.f:c     get a sample following normal distribution on GPU
GPU/source/MOD_random.f:      subroutine normalgpuR8(array,n,mean_,stddev_,stream_)
GPU/source/MOD_random.f:     &                       acc_get_cuda_stream(stream_))
GPU/source/MOD_random.f:c     get a sample following normal distribution on GPU with forced simple precision
GPU/source/MOD_random.f:      subroutine normalgpuR4(array,n,mean_,stddev_,stream_)
GPU/source/MOD_random.f:     &                       acc_get_cuda_stream(stream_))
GPU/source/MOD_random.f:      subroutine rand_unitgpu (vector,n)
GPU/source/MOD_random.f:      call randomgpu(samplevec,2*n+2)
GPU/source/MOD_random.f:      !call randomgpu(samplevec(1),20)
GPU/source/MOD_random.f:      !call normalgpu(samplevec(1),10)
GPU/source/MOD_random.f:      !call randomgpu(samplevec(11),10)
GPU/source/MOD_random.f:      !call normalgpu(samplevec(21),20)
GPU/source/tmatxb_pmegpu.f:      subroutine tmatxb_pmegpu(nrhs,dodiag,mu,efi)
GPU/source/tmatxb_pmegpu.f:        use utilgpu , only : def_queue, dir_queue, rec_queue
GPU/source/tmatxb_pmegpu.f:#ifdef _OPENACC
GPU/source/tmatxb_pmegpu.f:        use timestat, only : timer_tmatxb_pmegpu, timer_enter,timer_exit
GPU/source/tmatxb_pmegpu.f:#ifdef _OPENACC
GPU/source/tmatxb_pmegpu.f:        if(deb_Path) write(*,'(4x,a)') 'tmatxb_pmegpu'
GPU/source/tmatxb_pmegpu.f:        call timer_enter( timer_tmatxb_pmegpu )
GPU/source/tmatxb_pmegpu.f:#ifdef _OPENACC
GPU/source/tmatxb_pmegpu.f:        call timer_exit( timer_tmatxb_pmegpu )
GPU/source/tmatxb_pmegpu.f:      subroutine tmatxb_pmegpu1(nrhs,dodiag,mu,efi)
GPU/source/tmatxb_pmegpu.f:      use utilgpu , only : maxscaling1, def_queue, dir_queue
GPU/source/tmatxb_pmegpu.f:      use timestat, only : timer_tmatxb_pmegpu, timer_enter, timer_exit
GPU/source/tmatxb_pmegpu.f:      if(deb_Path) write(*,'(4x,a)') 'tmatxb_pmegpu1'
GPU/source/tmatxb_pmegpu.f:      call timer_enter( timer_tmatxb_pmegpu )
GPU/source/tmatxb_pmegpu.f:      call timer_exit( timer_tmatxb_pmegpu )
GPU/source/tmatxb_pmegpu.f:      subroutine otf_dc_tmatxb_pmegpu(nrhs,dodiag,mu,efi)
GPU/source/tmatxb_pmegpu.f:      use utilgpu
GPU/source/tmatxb_pmegpu.f:     &   write(*,'(4x,a)') 'otf_dc_tmatxb_pmegpu'
GPU/source/tmatxb_pmegpu.f:#ifdef _OPENACC
GPU/source/tmatxb_pmegpu.f:      use utilgpu , only : maxscaling1, def_queue, dir_queue,real3,real6
GPU/source/tmatxb_pmegpu.f:      use utilgpu , only : def_queue, dir_queue,real3,real6
GPU/source/tmatxb_pmegpu.f:      use utilgpu , only : def_queue, dir_queue,real3,real6
GPU/source/tmatxb_pmegpu.f:#ifdef _CUDA
GPU/source/tmatxb_pmegpu.f:      use utilgpu , only : def_queue, dir_queue,real3,real6
GPU/source/tmatxb_pmegpu.f:      use cudafor
GPU/source/tmatxb_pmegpu.f:      use utilgpu   , only : def_stream,rec_stream,rec_queue,nSMP
GPU/source/tmatxb_pmegpu.f:         call cudaMaxGridSize("tmatxb_pme_core_cu",gS)
GPU/source/tmatxb_pmegpu.f:      use utilgpu , only : def_queue, dir_queue,real3,real6
GPU/source/tmatxb_pmegpu.f:#ifdef _OPENACC
GPU/source/tmatxb_pmegpu.f:      use cudafor
GPU/source/tmatxb_pmegpu.f:      use utilgpu   , only : def_stream,rec_stream, nSMP
GPU/source/tmatxb_pmegpu.f:#ifdef _CUDA
GPU/source/tmatxb_pmegpu.f:         call cudaMaxGridSize("tmatxb_pme_core_cu",gS)
GPU/source/tmatxb_pmegpu.f:      use utilgpu , only : def_queue, dir_queue,real3,real6
GPU/source/tmatxb_pmegpu.f:#ifdef _OPENACC
GPU/source/tmatxb_pmegpu.f:      use cudafor
GPU/source/tmatxb_pmegpu.f:      use utilgpu   , only : def_stream, nSMP
GPU/source/tmatxb_pmegpu.f:#ifdef _CUDA
GPU/source/tmatxb_pmegpu.f:         call cudaMaxGridSize("otf_dc_tmatxb_pme_core_cu",gS)
GPU/source/tmatxb_pmegpu.f:      use utilgpu , only : def_queue, dir_queue,real3,real6
GPU/source/tmatxb_pmegpu.f:      use utilgpu , only : def_queue, dir_queue,real3,real6
GPU/source/eimprop1gpu.f:      module eimprop1gpu_inl
GPU/source/eimprop1gpu.f:      subroutine eimprop1gpu
GPU/source/eimprop1gpu.f:      use eimprop1gpu_inl
GPU/source/eimprop1gpu.f:      if (deb_Path) write(*,*) 'eimprop1gpu'
GPU/source/diis.f:        if (n.gt.0) call sprodgpu(n,e(1,1),e(1,1),b(2,2))
GPU/source/diis.f:          if (n.gt.0) call sprodgpu(n,e(1,j),e(1,nvec),b(nvec+1,j+1))
GPU/source/diis.f:          if (n.gt.0) call sprodgpu(n,e(1,j),e(1,nvec),b(j+1,nvec+1))
GPU/source/diis.f:        if(n.gt.0) call sprodgpu(n,e(1,nvec),e(1,nvec),b(nvec+1,nvec+1))
GPU/source/diis.f:        if (n.gt.0) call sprodgpu(n,e(1,1),e(1,1),b(2,2))
GPU/source/diis.f:          if (n.gt.0) call sprodgpu(n,e(1,j),e(1,nvec),b(nvec+1,j+1))
GPU/source/diis.f:          if (n.gt.0) call sprodgpu(n,e(1,j),e(1,nvec),b(j+1,nvec+1))
GPU/source/diis.f:        if (n.gt.0)call sprodgpu(n,e(1,nvec),e(1,nvec),b(nvec+1,nvec+1))
GPU/source/empole1cu.f:#ifdef _CUDA
GPU/source/empole1cu.f:        use utilgpu  ,only: BLOCK_SIZE,RED_BUFF_SIZE,WARP_SIZE
GPU/source/eopbend1gpu.f:      module eopbend1gpu_inl
GPU/source/eopbend1gpu.f:      subroutine eopbend1gpu_(deopb)
GPU/source/eopbend1gpu.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/eopbend1gpu.f:      if(deb_Path) write(*,*) 'eopbend1gpu'
GPU/source/eopbend1gpu.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/eopbend1gpu.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/eopbend1gpu.f:      subroutine eopbend1gpu
GPU/source/eopbend1gpu.f:      use eopbend1gpu_inl
GPU/source/eopbend1gpu.f:      call eopbend1gpu_(deopb)
GPU/source/kurey.f:      use utilgpu  
GPU/source/kurey.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/kurey.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/kurey.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/kurey.f:#ifdef _OPENACC
GPU/source/mechanic.f:#ifdef _OPENACC
GPU/source/mechanic.f:      use utilgpu,only:rec_queue,thrust_cache_init
GPU/source/mechanic.f:      use openacc
GPU/source/mechanic.f:#ifdef _OPENACC
GPU/source/mechanic.f:      call initmpipmegpu
GPU/source/mechanic.f:#ifdef _OPENACC
GPU/source/mechanic.f:c     init cuda rand engine
GPU/source/mechanic.f:      call initmpipmegpu
GPU/source/mechanic.f:         call initmpipmegpu
GPU/source/mechanic.f:        call initmpipmegpu
GPU/source/mechanic.f:      use utilgpu
GPU/source/mechanic.f:#ifdef _OPENACC
GPU/source/mechanic.f:#ifdef _OPENACC
GPU/source/mechanic.f:      use utilgpu,only: allow_precomp
GPU/source/mechanic.f:#if _OPENACC
GPU/source/mechanic.f:#ifdef _OPENACC
GPU/source/mechanic.f:#ifdef _OPENACC
GPU/source/mechanic.f:         else if (configure.eq.conf_elj1gpu) then
GPU/source/mechanic.f:#ifdef _OPENACC
GPU/source/mechanic.f:#ifdef _OPENACC
GPU/source/mechanic.f:            ecreal3d_p          => ecreal3dgpu
GPU/source/mechanic.f:         else if (configure.eq.conf_ecreal1dgpu) then
GPU/source/mechanic.f:            ecreal1d_p          => ecreal1dgpu
GPU/source/mechanic.f:            ecreal3d_p          => ecreal3dgpu
GPU/source/mechanic.f:#ifdef _OPENACC
GPU/source/mechanic.f:            ecreal1d_p          => ecreal1dgpu
GPU/source/mechanic.f:            ecreal3d_p          => ecreal3dgpu
GPU/source/mechanic.f:#ifdef _OPENACC
GPU/source/mechanic.f:         otf_dc_efld0_directgpu_p => otf_dc_efld0_directgpu2
GPU/source/mechanic.f:         if      (configure.eq.conf_efld0_directgpu1) then
GPU/source/mechanic.f:            efld0_directgpu_p => efld0_directgpu2
GPU/source/mechanic.f:         else if (configure.eq.conf_efld0_directgpu2) then
GPU/source/mechanic.f:            efld0_directgpu_p => efld0_directgpu2
GPU/source/mechanic.f:#ifdef _OPENACC
GPU/source/mechanic.f:         else if (configure.eq.conf_efld0_directgpu3) then
GPU/source/mechanic.f:            efld0_directgpu_p => efld0_directgpu3
GPU/source/mechanic.f:        otf_dc_efld0_directgpu_p => otf_dc_efld0_directgpu3
GPU/source/mechanic.f:            efld0_directgpu_p => efld0_directgpu2
GPU/source/mechanic.f:         otf_dc_efld0_directgpu_p => otf_dc_efld0_directgpu2
GPU/source/mechanic.f:         efld0_directgpu_p1  => efld0_directgpu_p
GPU/source/mechanic.f:         tmatxb_p              => tmatxb_pmegpu
GPU/source/mechanic.f:            tmatxb_p          => tmatxb_pmegpu1
GPU/source/mechanic.f:#ifdef _OPENACC
GPU/source/mechanic.f:#ifdef _OPENACC
GPU/source/mechanic.f:         epreal1c_p => epreal1cgpu
GPU/source/mechanic.f:            epreal3d_p      => epreal3dgpu
GPU/source/mechanic.f:            epreal3d_p      => epreal3dgpu
GPU/source/mechanic.f:#ifdef _OPENACC
GPU/source/mechanic.f:            epreal3d_p      => epreal3dgpu
GPU/source/mechanic.f:#ifdef _OPENACC
GPU/source/mechanic.f:#ifdef _OPENACC
GPU/source/mechanic.f:         fphi_uind_site1_p => fphi_uind_sitegpu1
GPU/source/mechanic.f:         fphi_uind_site2_p => fphi_uind_sitegpu2
GPU/source/mechanic.f:         fphi_mpole_site_p => fphi_mpole_sitegpu
GPU/source/mechanic.f:         fphi_chg_site_p   => fphi_chg_sitegpu
GPU/source/mechanic.f:#ifdef _OPENACC
GPU/source/mechanic.f:      else if (configure.eq.conf_fphi_cuda) then
GPU/source/mechanic.f:         fphi_uind_site1_p => fphi_uind_sitegpu1
GPU/source/mechanic.f:         fphi_uind_site2_p => fphi_uind_sitegpu2
GPU/source/mechanic.f:         fphi_mpole_site_p => fphi_mpole_sitegpu
GPU/source/mechanic.f:         fphi_chg_site_p   => fphi_chg_sitegpu
GPU/source/mechanic.f:         grid_uind_site_p  => grid_uind_sitegpu
GPU/source/mechanic.f:         grid_mpole_site_p => grid_mpole_sitegpu
GPU/source/mechanic.f:         grid_pchg_site_p  => grid_pchg_sitegpu
GPU/source/mechanic.f:         grid_disp_site_p  => grid_disp_sitegpu
GPU/source/mechanic.f:#ifdef _OPENACC
GPU/source/mechanic.f:      else if (configure.eq.conf_grid_cuda) then
GPU/source/mechanic.f:            grid_uind_site_p  => grid_uind_sitegpu
GPU/source/mechanic.f:            grid_mpole_site_p => grid_mpole_sitegpu
GPU/source/mechanic.f:            grid_pchg_site_p  => grid_pchg_sitegpu
GPU/source/mechanic.f:            grid_disp_site_p  => grid_disp_sitegpu
GPU/source/mechanic.f:         grid_uind_site_p  => grid_uind_sitegpu
GPU/source/mechanic.f:         grid_mpole_site_p => grid_mpole_sitegpu
GPU/source/mechanic.f:         grid_pchg_site_p  => grid_pchg_sitegpu
GPU/source/mechanic.f:         grid_disp_site_p  => grid_disp_sitegpu
GPU/source/mechanic.f:         pme_conv_p => pme_conv_gpu
GPU/source/mechanic.f:#ifdef _OPENACC
GPU/source/mechanic.f:      else if (configure.eq.conf_conv_cuda) then
GPU/source/mechanic.f:      use utilgpu
GPU/source/mechanic.f:#ifdef _OPENACC
GPU/source/mechanic.f:#ifdef _OPENACC
GPU/source/mechanic.f:         efld0_direct_cp   => efld0_directgpu_p
GPU/source/mechanic.f:         efld0_directgpu_p => efld0_direct_void
GPU/source/mechanic.f:#ifdef _OPENACC
GPU/source/mechanic.f:#ifdef _OPENACC
GPU/source/mechanic.f:#ifdef _OPENACC
GPU/source/mechanic.f:     &   (associated(efld0_directgpu_p,efld0_directgpu3).or.
GPU/source/mechanic.f:     &    associated(efld0_direct_cp,efld0_directgpu3)).and.
GPU/source/mechanic.f:#ifdef _OPENACC
GPU/source/mutate.f:#ifdef _OPENACC
GPU/source/mutate.f:#ifdef _OPENACC
GPU/source/mutate.f:#ifdef _OPENACC
GPU/source/mutate.f:#ifdef _OPENACC
GPU/source/mdinitbead.f:#ifdef _OPENACC
GPU/source/mdinitbead.f:              call rand_unitgpu(samplevec(1),nloc)
GPU/source/mdinitbead.f:              call maxwellgpu(mass,nbeads*kelvin,nloc,speedvec)
GPU/source/empolecu.tpl.f:c     "empole1cu" : CUDA Template for calculation of the multipole and dipole polarization
GPU/source/mlinterface.py:        global pcd, GPUArray
GPU/source/mlinterface.py:        import pycuda.driver   as pcd
GPU/source/mlinterface.py:        from pycuda.gpuarray import GPUArray
GPU/source/mlinterface.py:        print('Exception: Fail to load pycuda with exception:', exp, flush=True)
GPU/source/mlinterface.py:            torch.cuda.init()
GPU/source/mlinterface.py:            device_name     = 'cuda:'+str(devID) if torch.cuda.is_available() else 'cpu'
GPU/source/mlinterface.py:            default_tf_session_config.device_count['GPU']=1
GPU/source/mlinterface.py:            default_tf_session_config.gpu_options.visible_device_list=str(devID)
GPU/source/mlinterface.py:            physical_devices = tf.config.list_physical_devices('GPU')
GPU/source/mlinterface.py:            # Only use the GPU corresponding to devID
GPU/source/mlinterface.py:            tf.config.set_visible_devices(physical_devices[devID], 'GPU')
GPU/source/mlinterface.py:            logical_devices = tf.config.list_logical_devices('GPU')
GPU/source/mlinterface.py:            self.device = tf.device('/GPU:0')
GPU/source/mlinterface.py:          torch.cuda.synchronize()
GPU/source/mlinterface.py:          print(' ML input GPU processes', flush=True)
GPU/source/mlinterface.py:          print(torch.cuda.list_gpu_processes(), flush=True)
GPU/source/mlinterface.py:            torch.cuda.synchronize()
GPU/source/mlinterface.py:            torch.cuda.synchronize()
GPU/source/mlinterface.py:              torch.cuda.synchronize()
GPU/source/mlinterface.py:            torch.cuda.synchronize()
GPU/source/mlinterface.py:              torch.cuda.synchronize()
GPU/source/mlinterface.py:        atomic_energies    = asGPUarray(atm_ener_ptr, 'float',[1,nb_species])
GPU/source/mlinterface.py:        if dograd: gradient= asGPUarray(gradient_ptr, 'float',[nb_species,3])
GPU/source/mlinterface.py:        torch.cuda.synchronize()
GPU/source/mlinterface.py:            torch.cuda.synchronize()
GPU/source/mlinterface.py:            print('getmcache',torch.cuda.list_gpu_processes(), flush=True)
GPU/source/mlinterface.py:            torch.cuda.synchronize()
GPU/source/mlinterface.py:              , self.device, torch.cuda.list_gpu_processes(), flush=True)
GPU/source/mlinterface.py:  def asGPUarray(ptr, ctype, shape, **kwargs):
GPU/source/mlinterface.py:    return GPUArray(gpudata=ptr,shape=shape,dtype=_ctype2dtype[ctype], **kwargs)
GPU/source/mlinterface.py:    ga     = asGPUarray(ptr, ctype, shape, **kwargs)
GPU/source/mlinterface.py:    ga     = asGPUarray(ptr, ctype, shape, **kwargs)
GPU/source/mlinterface.py:     if(ar.use_mod==ar.mod_torch): torch.cuda.synchronize()
GPU/source/mlinterface.py:  #      if ar.verbose: print('ML potential pop CUDA Context', flush=True)
GPU/source/gradient.f:      use utilgpu   ,only:rec_queue,def_queue,inf_r,zero_mdred_buffers
GPU/source/gradient.f:#ifdef _OPENACC
GPU/source/gradient.f:      if (use_bond)     call ebond1gpu
GPU/source/gradient.f:      if (use_urey)     call eurey1gpu
GPU/source/gradient.f:      if (use_opbend)   call eopbend1gpu
GPU/source/gradient.f:      if (use_strbnd)   call estrbnd1gpu
GPU/source/gradient.f:      if (use_angle)    call eangle1gpu
GPU/source/gradient.f:      if (use_tors)     call etors1gpu
GPU/source/gradient.f:      if (use_pitors)   call epitors1gpu
GPU/source/gradient.f:      if (use_tortor)   call etortor1gpu
GPU/source/gradient.f:      if (use_improp)   call eimprop1gpu
GPU/source/gradient.f:      if (use_imptor)   call eimptor1gpu
GPU/source/gradient.f:      if (use_strtor)   call estrtor1gpu
GPU/source/gradient.f:      if (use_geom)     call egeom1gpu
GPU/source/gradient.f:         if (vdwtyp .eq. 'LENNARD-JONES')  call elj1gpu
GPU/source/gradient.f:         if (vdwtyp .eq. 'BUFFERED-14-7')  call ehal1gpu
GPU/source/gradient.f:      if (use_charge)   call echarge1gpu
GPU/source/gradient.f:      if (use_mpole)    call empole1gpu
GPU/source/gradient.f:                        call epolar1gpu
GPU/source/gradient.f:      if (use_chgtrn)   call echgtrn1gpu
GPU/source/eljcu.f:        use cudafor
GPU/source/eljcu.f:        use utilgpu  ,only: BLOCK_SIZE,RED_BUFF_SIZE
GPU/source/ebond3gpu.f:      module ebond3gpu_inl
GPU/source/ebond3gpu.f:      subroutine ebond3gpu
GPU/source/ebond3gpu.f:      use ebond3gpu_inl
GPU/source/ebond3gpu.f:      if (deb_Path) write(*,*) 'ebond3gpu'
GPU/source/ebond3gpu.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/ebond3gpu.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/pair_chgtrn.inc.f:#include "tinker_cudart.h"
GPU/source/library.make:MOD_precompute_polegpu.o \
GPU/source/library.make:MOD_utilgpu.o \
GPU/source/library.make:chkpolegpu.o \
GPU/source/library.make:dcinduce_pmegpu.o \
GPU/source/library.make:dcinduce_pme2gpu.o \
GPU/source/library.make:dcinduce_shortrealgpu.o \
GPU/source/library.make:eangle1gpu.o \
GPU/source/library.make:eangle3gpu.o  \
GPU/source/library.make:ebond1gpu.o \
GPU/source/library.make:ebond3gpu.o \
GPU/source/library.make:eliaison1gpu.o \
GPU/source/library.make:echarge1gpu.o \
GPU/source/library.make:echarge3gpu.o \
GPU/source/library.make:efld0_directgpu.o  \
GPU/source/library.make:egeom1gpu.o \
GPU/source/library.make:egeom3gpu.o \
GPU/source/library.make:ehal1gpu.o \
GPU/source/library.make:ehal3gpu.o \
GPU/source/library.make:eimprop1gpu.o \
GPU/source/library.make:eimptor1gpu.o \
GPU/source/library.make:elj1gpu.o \
GPU/source/library.make:elj3gpu.o \
GPU/source/library.make:empole1gpu.o \
GPU/source/library.make:empole3gpu.o \
GPU/source/library.make:eopbend1gpu.o \
GPU/source/library.make:eopbend3gpu.o \
GPU/source/library.make:epitors1gpu.o \
GPU/source/library.make:epolar1gpu.o \
GPU/source/library.make:epolar3gpu.o \
GPU/source/library.make:estrbnd1gpu.o \
GPU/source/library.make:estrtor1gpu.o \
GPU/source/library.make:etors1gpu.o \
GPU/source/library.make:etortor1gpu.o \
GPU/source/library.make:eurey1gpu.o  \
GPU/source/library.make:eurey3gpu.o \
GPU/source/library.make:nblistgpu.o \
GPU/source/library.make:newinduce_pmegpu.o \
GPU/source/library.make:newinduce_pme2gpu.o \
GPU/source/library.make:newinduce_shortrealgpu.o \
GPU/source/library.make:pmestuffgpu.o \
GPU/source/library.make:rotpolegpu.o  \
GPU/source/library.make:tmatxb_pmegpu.o \
GPU/source/library.make:torquegpu.o \
GPU/source/echgljcu.f:        use cudafor
GPU/source/echgljcu.f:        use utilgpu  ,only: inf,BLOCK_SIZE,RED_BUFF_SIZE
GPU/source/newinduce_pme.f:#ifdef _OPENACC
GPU/source/maxwell.f:#ifdef _OPENACC
GPU/source/maxwell.f:      subroutine maxwellgpu (mass,temper,nloc,max_result)
GPU/source/maxwell.f:      call randomgpu(Rn(1,1),3*nloc)
GPU/source/baoabrespapi.f:      use utilgpu
GPU/source/baoabrespapi.f:      use utilgpu
GPU/source/MOD_utilcu.f:c     ##  module utilcu  -- CUDA utility functions and params           ##
GPU/source/MOD_utilcu.f:#ifdef _CUDA
GPU/source/MOD_utilcu.f:        use cudadevice ,only: f_inv=>__frcp_rn
GPU/source/MOD_utilcu.f:        use cudadevice ,only: f_inv=>__drcp_rn
GPU/source/MOD_utilcu.f:        use cudadevice ,only: __ll_as_tp=>__longlong_as_double
GPU/source/MOD_utilcu.f:        use cudafor
GPU/source/MOD_utilcu.f:        !sets the default stream for all subsequent high-level CUDA Fortran operations
GPU/source/MOD_utilcu.f:           ierrSync = cudaDeviceSynchronize()
GPU/source/MOD_utilcu.f:           ierrSync = cudaGetLastError()
GPU/source/MOD_utilcu.f:        if (ierrSync.ne.cudaSuccess) then
GPU/source/MOD_utilcu.f:              write(*,126) ierrSync,msg,cudaGetErrorString(ierrSync)
GPU/source/MOD_utilcu.f:              write(*,125) ierrSync,cudaGetErrorString(ierrSync)
GPU/source/MOD_utilcu.f:        if (i.ne.cudaSuccess)
GPU/source/MOD_utilcu.f:     &     write(0,126) msg,i,cudaGetErrorString(ierrSync)
GPU/source/MOD_utilcu.f:        if (tinkerdebug.gt.0) ierrSync = cudaDeviceSynchronize()
GPU/source/MOD_utilcu.f:        subroutine copy_data_to_cuda_env(data,flag)
GPU/source/MOD_utilcu.f:      integer(cuda_stream_kind),intent(in):: stream
GPU/source/MOD_utilcu.f:      ierr = cudaMemCpyAsync(dst,src,1,cudaMemcpyDeviceToHost,stream)
GPU/source/MOD_utilcu.f:      if (ierr.ne.cudasuccess) then
GPU/source/MOD_utilcu.f:         write(*,*) cudageterrorstring(ierr)
GPU/source/MOD_utilcu.f:      integer(cuda_stream_kind) :: stream
GPU/source/MOD_utilcu.f:         !ierr = cudamalloc(d_array,n)
GPU/source/MOD_utilcu.f:         call chk_cuAPI_O(ierr,'cudaMalloc')
GPU/source/MOD_utilcu.f:         !call chk_cuAPI_O(cudaFree(d_array),'cudaFree')
GPU/source/MOD_utilcu.f:         !ierr = cudamalloc(d_array,n)
GPU/source/MOD_utilcu.f:         call chk_cuAPI_O(ierr,'cudaMalloc')
GPU/source/MOD_utilcu.f:      integer(cuda_stream_kind),intent(in):: n
GPU/source/MOD_utilcu.f:      integer(cuda_stream_kind),intent(in):: stream
GPU/source/MOD_utilcu.f:      integer(cuda_stream_kind),optional:: offset
GPU/source/MOD_utilcu.f:      integer(cuda_stream_kind) offset_
GPU/source/MOD_utilcu.f:      ierr = cudaMemsetAsync(dst(offset_),val,n,stream)
GPU/source/efld0_directgpu.f:      module efld0_directgpu_inl
GPU/source/efld0_directgpu.f:      subroutine efld0_directgpu(nrhs,ef)
GPU/source/efld0_directgpu.f:      use efld0_directgpu_inl
GPU/source/efld0_directgpu.f:      use utilgpu  ,only: dir_queue,rec_queue,def_queue,
GPU/source/efld0_directgpu.f:#ifdef _OPENACC
GPU/source/efld0_directgpu.f:     &   write(*,'(3x,a)') 'efld0_directgpu'
GPU/source/efld0_directgpu.f:      subroutine efld0_directgpu2(nrhs,ef)
GPU/source/efld0_directgpu.f:      use efld0_directgpu_inl
GPU/source/efld0_directgpu.f:      use utilgpu ,only: dir_queue,rec_queue,def_queue
GPU/source/efld0_directgpu.f:#ifdef _OPENACC
GPU/source/efld0_directgpu.f:#ifdef _OPENACC
GPU/source/efld0_directgpu.f:     &   write(*,'(3x,a,L3)') 'efld0_directgpu2',use_polarshortreal
GPU/source/efld0_directgpu.f:      subroutine efld0_directgpu3(nrhs,ef)
GPU/source/efld0_directgpu.f:#ifdef _OPENACC
GPU/source/efld0_directgpu.f:      use efld0_directgpu_inl
GPU/source/efld0_directgpu.f:      use utilgpu ,only: dir_queue,rec_queue,def_queue,get_GridDim
GPU/source/efld0_directgpu.f:         rtami = 'efld0_directgpu3'//merge(' THOLE','',use_thole)
GPU/source/efld0_directgpu.f: 100  format('eld0_directgpu3 is a specific device routine',/,
GPU/source/efld0_directgpu.f:      subroutine otf_dc_efld0_directgpu2(nrhs,ef)
GPU/source/efld0_directgpu.f:      use efld0_directgpu_inl
GPU/source/efld0_directgpu.f:      use utilgpu ,only: dir_queue,rec_queue,def_queue
GPU/source/efld0_directgpu.f:#ifdef _OPENACC
GPU/source/efld0_directgpu.f:     &   write(*,'(3x,a)') 'oft_dc_efld0_directgpu2'
GPU/source/efld0_directgpu.f:#ifdef _OPENACC
GPU/source/efld0_directgpu.f:      subroutine otf_dc_efld0_directgpu3(nrhs,ef)
GPU/source/efld0_directgpu.f:      use efld0_directgpu_inl
GPU/source/efld0_directgpu.f:#ifdef _CUDA
GPU/source/efld0_directgpu.f:      use utilgpu ,only: dir_queue,rec_queue,def_queue
GPU/source/efld0_directgpu.f:#ifdef _OPENACC
GPU/source/efld0_directgpu.f:      if (deb_Path) write(*,'(3x,a)') 'otf_dc_efld0_directgpu3'
GPU/source/efld0_directgpu.f:#ifdef _OPENACC
GPU/source/efld0_directgpu.f:#ifdef _CUDA
GPU/source/efld0_directgpu.f: 100  format('eld0_directgpu3 is a specific device routine',/,
GPU/source/efld0_directgpu.f:      use efld0_directgpu_inl
GPU/source/efld0_directgpu.f:      use utilgpu  ,only: dir_queue,def_queue
GPU/source/MOD_utilgpu.f:c     ##  module utilgpu  -- GPU utility functions and params           ##
GPU/source/MOD_utilgpu.f:c     ngpus        numbers of gpus available 
GPU/source/MOD_utilgpu.f:c     gpu_gangs    attribute #gangs to create inside a kernels
GPU/source/MOD_utilgpu.f:c     gpu_workers  attribute #workers for parallelism mapping
GPU/source/MOD_utilgpu.f:c     gpu_vector   attribute #vector_length in kernels or parallel clauses
GPU/source/MOD_utilgpu.f:      module utilgpu
GPU/source/MOD_utilgpu.f:#ifdef _OPENACC
GPU/source/MOD_utilgpu.f:      use openacc
GPU/source/MOD_utilgpu.f:      use cudafor
GPU/source/MOD_utilgpu.f:#define cuda_stream_kind 8
GPU/source/MOD_utilgpu.f:      integer ngpus
GPU/source/MOD_utilgpu.f:      integer gpu_gangs,ngangs_rec
GPU/source/MOD_utilgpu.f:      integer gpu_workers,gpu_vector
GPU/source/MOD_utilgpu.f:#ifdef _OPENACC
GPU/source/MOD_utilgpu.f:      type(cudaDeviceProp) devProp
GPU/source/MOD_utilgpu.f:      integer(cuda_stream_kind) dir_stream,rec_stream
GPU/source/MOD_utilgpu.f:      integer(cuda_stream_kind) def_stream
GPU/source/MOD_utilgpu.f:      type(cudaEvent) dir_event,rec_event,def_event
GPU/source/MOD_utilgpu.f:      !              - empole1cgpu, epreal1cgpu, emreal3d_cu, epreal3d_cu, emrecip1gpu, eprecip1gpu
GPU/source/MOD_utilgpu.f:      !              - emrecip1gpu, eprecip1gpu
GPU/source/MOD_utilgpu.f:#ifdef _OPENACC
GPU/source/MOD_utilgpu.f:      function acc_get_cuda_stream_c( async ) result(queue)
GPU/source/MOD_utilgpu.f:     &     bind(C,name='acc_get_cuda_stream')
GPU/source/MOD_utilgpu.f:!$acc declare create(gpu_gangs,gpu_workers,gpu_vector,devicenum,
GPU/source/MOD_utilgpu.f:!$acc& ngpus,warning,sugest_vdw,ftc,ctf)
GPU/source/MOD_utilgpu.f:c     updating the number of gangs to launch on the gpu if authorized
GPU/source/MOD_utilgpu.f:      if (mod_gangs.and.ngpus>0) gpu_gangs = gang_
GPU/source/MOD_utilgpu.f:      subroutine openacc_abort(message)
GPU/source/MOD_utilgpu.f:#ifdef _OPENACC
GPU/source/MOD_utilgpu.f:      print *, "OpenACC abort : ",message
GPU/source/MOD_utilgpu.f:#ifdef _OPENACC
GPU/source/MOD_utilgpu.f:      subroutine bind_gpu
GPU/source/MOD_utilgpu.f:      !Initialisation OpenAcc
GPU/source/MOD_utilgpu.f:      ! Look for CUDA environment
GPU/source/MOD_utilgpu.f:      call get_environment_variable(name="CUDA_VISIBLE_DEVICE",
GPU/source/MOD_utilgpu.f: 34   format( "Warning : Could not bind GPU device before MPI_Init",
GPU/source/MOD_utilgpu.f:      ! Look for device ID to bind with using OpenACC
GPU/source/MOD_utilgpu.f:      ngpus = acc_get_num_devices(device_type)
GPU/source/MOD_utilgpu.f:      if (local_rank.ge.ngpus) local_rank = mod(local_rank,ngpus)
GPU/source/MOD_utilgpu.f:      end subroutine bind_gpu
GPU/source/MOD_utilgpu.f:      integer ::i,cuda_success=0
GPU/source/MOD_utilgpu.f:            ngpus = 1
GPU/source/MOD_utilgpu.f:            ngpus = 1
GPU/source/MOD_utilgpu.f:            ngpus     = acc_get_num_devices(device_type)
GPU/source/MOD_utilgpu.f:            devicenum = mod( device_start+host_rank,ngpus )
GPU/source/MOD_utilgpu.f:      integer ::i,cuda_success=0
GPU/source/MOD_utilgpu.f:      rec_stream = acc_get_cuda_stream(acc_async_noval)
GPU/source/MOD_utilgpu.f:     &'| WARNING !! cannot recover CUDA stream from OpenACC async',
GPU/source/MOD_utilgpu.f:      cuda_success = cudaDeviceGetStreamPriorityRange(low_priority,
GPU/source/MOD_utilgpu.f:      cuda_success = cuda_success +
GPU/source/MOD_utilgpu.f:     &  cudastreamcreate(rec_stream,cudaStreamNonBlocking,high_priority)
GPU/source/MOD_utilgpu.f:      call acc_set_cuda_stream(rec_queue,rec_stream)
GPU/source/MOD_utilgpu.f:         cuda_success = cuda_success + 
GPU/source/MOD_utilgpu.f:     &   cudastreamcreate(dir_stream,cudaStreamNonBlocking,low_priority)
GPU/source/MOD_utilgpu.f:         call acc_set_cuda_stream(dir_queue,dir_stream)
GPU/source/MOD_utilgpu.f:      if (cuda_success.ne.0)
GPU/source/MOD_utilgpu.f:     &   print*,'error creating cuda async queue >',cuda_success
GPU/source/MOD_utilgpu.f:      cuda_success = cuda_success + 
GPU/source/MOD_utilgpu.f:     &       cudaEventCreateWithFlags(dir_event,cudaEventDisableTiming)
GPU/source/MOD_utilgpu.f:      cuda_success = cuda_success + 
GPU/source/MOD_utilgpu.f:     &       cudaEventCreateWithFlags(rec_event,cudaEventDisableTiming)
GPU/source/MOD_utilgpu.f:      cuda_success = cuda_success + 
GPU/source/MOD_utilgpu.f:     &       cudaEventCreateWithFlags(def_event,cudaEventDisableTiming)
GPU/source/MOD_utilgpu.f:      if (cuda_success.ne.0)
GPU/source/MOD_utilgpu.f:     &   print*,'error creating cuda Events  >', cuda_success
GPU/source/MOD_utilgpu.f:      istat  = cudaStreamSynchronize(stream_)
GPU/source/MOD_utilgpu.f:      integer(cuda_stream_kind)::stream
GPU/source/MOD_utilgpu.f:      stream = acc_get_cuda_stream(queue)
GPU/source/MOD_utilgpu.f:      integer cuda_success
GPU/source/MOD_utilgpu.f:         !cuda_success = cudaStreamDestroy(dir_stream)
GPU/source/MOD_utilgpu.f:c12      format('CUDA Error destroying rec/direct streams',I10)
GPU/source/MOD_utilgpu.f:c        if (cuda_success.ne.0) then
GPU/source/MOD_utilgpu.f:c           print 12, cuda_success
GPU/source/MOD_utilgpu.f:c     A cuda implementation of _pgi_acc_wait_async
GPU/source/MOD_utilgpu.f:c     Synchonisation within CUDA streams doesn't seem to work with OpenACC
GPU/source/MOD_utilgpu.f:      integer(cuda_stream_kind),intent(in)::wait_s,async_s
GPU/source/MOD_utilgpu.f:      type(cudaEvent),optional::event_
GPU/source/MOD_utilgpu.f:      type(cudaEvent) :: event
GPU/source/MOD_utilgpu.f:      integer cuda_success
GPU/source/MOD_utilgpu.f:      cuda_success = 0
GPU/source/MOD_utilgpu.f:      cuda_success = cuda_success +
GPU/source/MOD_utilgpu.f:     &     cudaEventRecord(event,wait_s)
GPU/source/MOD_utilgpu.f:      cuda_success = cuda_success +
GPU/source/MOD_utilgpu.f:     &     cudaStreamWaitEvent(async_s,event,zero_flag)
GPU/source/MOD_utilgpu.f:      if (cuda_success.ne.0)
GPU/source/MOD_utilgpu.f:     &   write(*,*) 'Error in stream_wait_async ! ',cuda_success
GPU/source/MOD_utilgpu.f:      integer cuda_success
GPU/source/MOD_utilgpu.f:      cuda_success = 0
GPU/source/MOD_utilgpu.f:      cuda_success = cuda_success +
GPU/source/MOD_utilgpu.f:     &     cudaEventRecord(dir_event,rec_stream)
GPU/source/MOD_utilgpu.f:      cuda_success = cuda_success +
GPU/source/MOD_utilgpu.f:     &     cudaStreamWaitEvent(dir_stream,dir_event,zero_flag)
GPU/source/MOD_utilgpu.f:      if (cuda_success.ne.0)
GPU/source/MOD_utilgpu.f:     &   write(*,*) 'Error in start_dir_stream_cover!',cuda_success
GPU/source/MOD_utilgpu.f:      integer cuda_success
GPU/source/MOD_utilgpu.f:      cuda_success = 0
GPU/source/MOD_utilgpu.f:      cuda_success = cuda_success +
GPU/source/MOD_utilgpu.f:     &     cudaEventRecord(rec_event,dir_stream)
GPU/source/MOD_utilgpu.f:      cuda_success = cuda_success +
GPU/source/MOD_utilgpu.f:     &     cudaStreamWaitEvent(rec_stream,rec_event,zero_flag)
GPU/source/MOD_utilgpu.f:      if (cuda_success.ne.0)
GPU/source/MOD_utilgpu.f:     &   write(*,*) 'Error in start_dir_stream_cover!',cuda_success
GPU/source/MOD_utilgpu.f:      ierr = CUDAGETDEVICEPROPERTIES(devProp,devicenum)
GPU/source/MOD_utilgpu.f:#ifdef _OPENACC
GPU/source/MOD_utilgpu.f:      ierr = cudaMemGetinfo(free,total)
GPU/source/MOD_utilgpu.f:      integer(cuda_stream_kind),intent(in) :: stream
GPU/source/MOD_utilgpu.f:      !ierr = cudaStreamSynchronize(stream)
GPU/source/MOD_utilgpu.f:      integer(cuda_stream_kind),intent(in):: stream
GPU/source/MOD_utilgpu.f:#ifdef _OPENACC
GPU/source/MOD_utilgpu.f:         ierr = cudaMemset(dst(offset_+1),val,n)
GPU/source/MOD_utilgpu.f:         ierr = ierr + cudaStreamSynchronize(stream)
GPU/source/MOD_utilgpu.f:         ierr = cudaMemsetAsync(dst(offset_+1),val,n,stream)
GPU/source/MOD_utilgpu.f:      if (ierr.ne.cudasuccess) then
GPU/source/MOD_utilgpu.f:         write(*,14) 'error',ierr,'detected in utilgpu_mem_set_i4'
GPU/source/MOD_utilgpu.f:     &              ,cudageterrorstring(ierr)
GPU/source/MOD_utilgpu.f:      integer(cuda_stream_kind),intent(in):: stream
GPU/source/MOD_utilgpu.f:#ifdef _OPENACC
GPU/source/MOD_utilgpu.f:         ierr = cudaMemset(dst(offset_+1),val,n)
GPU/source/MOD_utilgpu.f:         ierr = ierr + cudaStreamSynchronize(stream)
GPU/source/MOD_utilgpu.f:         ierr = cudaMemsetAsync(dst(offset_+1),val,n,stream)
GPU/source/MOD_utilgpu.f:      if (ierr.ne.cudasuccess) then
GPU/source/MOD_utilgpu.f:         write(*,14) 'error',ierr,'detected in utilgpu_mem_set_i8'
GPU/source/MOD_utilgpu.f:     &              ,cudageterrorstring(ierr)
GPU/source/MOD_utilgpu.f:      integer(cuda_stream_kind),intent(in):: stream
GPU/source/MOD_utilgpu.f:#ifdef _OPENACC
GPU/source/MOD_utilgpu.f:         ierr = cudaMemset(dst(offset_+1),val,n)
GPU/source/MOD_utilgpu.f:         ierr = ierr + cudaStreamSynchronize(stream)
GPU/source/MOD_utilgpu.f:         ierr = cudaMemsetAsync(dst(offset_+1),val,n,stream)
GPU/source/MOD_utilgpu.f:      if (ierr.ne.cudasuccess) then
GPU/source/MOD_utilgpu.f:         write(*,14) 'error',ierr,'detected in utilgpu_mem_set_r4'
GPU/source/MOD_utilgpu.f:     &              ,cudageterrorstring(ierr)
GPU/source/MOD_utilgpu.f:      integer(cuda_stream_kind),intent(in):: stream
GPU/source/MOD_utilgpu.f:#ifdef _OPENACC
GPU/source/MOD_utilgpu.f:         ierr = cudaMemset(dst(offset_+1,1),val,n)
GPU/source/MOD_utilgpu.f:         ierr = ierr + cudaStreamSynchronize(stream)
GPU/source/MOD_utilgpu.f:         ierr = cudaMemsetAsync(dst(offset_+1,1),val,n,stream)
GPU/source/MOD_utilgpu.f:      if (ierr.ne.cudasuccess) then
GPU/source/MOD_utilgpu.f:         write(*,14) 'error',ierr,'detected in utilgpu_mem_set_r4'
GPU/source/MOD_utilgpu.f:     &               ,cudageterrorstring(ierr)
GPU/source/MOD_utilgpu.f:      integer(cuda_stream_kind),intent(in):: stream
GPU/source/MOD_utilgpu.f:#ifdef _OPENACC
GPU/source/MOD_utilgpu.f:         ierr = cudaMemset(dst(offset_+1,1,1),val,n)
GPU/source/MOD_utilgpu.f:         ierr = ierr + cudaStreamSynchronize(stream)
GPU/source/MOD_utilgpu.f:         ierr = cudaMemsetAsync(dst(offset_+1,1,1),val,n,stream)
GPU/source/MOD_utilgpu.f:      if (ierr.ne.cudasuccess) then
GPU/source/MOD_utilgpu.f:         write(*,14) 'error',ierr,'detected in utilgpu_mem_set_r4'
GPU/source/MOD_utilgpu.f:     &               ,cudageterrorstring(ierr)
GPU/source/MOD_utilgpu.f:      integer(cuda_stream_kind),intent(in):: stream
GPU/source/MOD_utilgpu.f:#ifdef _OPENACC
GPU/source/MOD_utilgpu.f:         ierr = cudaMemset(dst(offset_+1,1,1,1,1),val,n)
GPU/source/MOD_utilgpu.f:         ierr = ierr + cudaStreamSynchronize(stream)
GPU/source/MOD_utilgpu.f:         ierr = cudaMemsetAsync(dst(offset_+1,1,1,1,1),val,n,stream)
GPU/source/MOD_utilgpu.f:      if (ierr.ne.cudasuccess) then
GPU/source/MOD_utilgpu.f:         write(*,14) 'error',ierr,'detected in utilgpu_mem_set_r4'
GPU/source/MOD_utilgpu.f:     &               ,cudageterrorstring(ierr)
GPU/source/MOD_utilgpu.f:      integer(cuda_stream_kind),intent(in):: stream
GPU/source/MOD_utilgpu.f:#ifdef _OPENACC
GPU/source/MOD_utilgpu.f:         ierr = cudaMemset(dst(offset_+1),val,n)
GPU/source/MOD_utilgpu.f:         ierr = ierr + cudaStreamSynchronize(stream)
GPU/source/MOD_utilgpu.f:         ierr = cudaMemsetAsync(dst(offset_+1),val,n,stream)
GPU/source/MOD_utilgpu.f:      if (ierr.ne.cudasuccess) then
GPU/source/MOD_utilgpu.f:         write(*,14) 'error',ierr,'detected in utilgpu_mem_set_r8'
GPU/source/MOD_utilgpu.f:     &              ,cudageterrorstring(ierr)
GPU/source/MOD_utilgpu.f:      integer(cuda_stream_kind),intent(in):: stream
GPU/source/MOD_utilgpu.f:#ifdef _OPENACC
GPU/source/MOD_utilgpu.f:         ierr = cudaMemset(dst(offset_+1,1),val,n)
GPU/source/MOD_utilgpu.f:         ierr = ierr + cudaStreamSynchronize(stream)
GPU/source/MOD_utilgpu.f:         ierr = cudaMemsetAsync(dst(offset_+1,1),val,n,stream)
GPU/source/MOD_utilgpu.f:      if (ierr.ne.cudasuccess) then
GPU/source/MOD_utilgpu.f:         write(*,14) 'error',ierr,'detected in utilgpu_mem_set_r8'
GPU/source/MOD_utilgpu.f:     &               ,cudageterrorstring(ierr)
GPU/source/MOD_utilgpu.f:      integer(cuda_stream_kind),intent(in):: stream
GPU/source/MOD_utilgpu.f:#ifdef _OPENACC
GPU/source/MOD_utilgpu.f:         ierr = cudaMemset(dst(offset_+1,1,1),val,n)
GPU/source/MOD_utilgpu.f:         ierr = ierr + cudaStreamSynchronize(stream)
GPU/source/MOD_utilgpu.f:         ierr = cudaMemsetAsync(dst(offset_+1,1,1),val,n,stream)
GPU/source/MOD_utilgpu.f:      if (ierr.ne.cudasuccess) then
GPU/source/MOD_utilgpu.f:         write(*,14) 'error',ierr,'detected in utilgpu_mem_set_r8'
GPU/source/MOD_utilgpu.f:     &               ,cudageterrorstring(ierr)
GPU/source/MOD_utilgpu.f:      integer(cuda_stream_kind),intent(in):: stream
GPU/source/MOD_utilgpu.f:#ifdef _OPENACC
GPU/source/MOD_utilgpu.f:         ierr = cudaMemset(dst(offset_+1,1,1,1,1),val,n)
GPU/source/MOD_utilgpu.f:         ierr = ierr + cudaStreamSynchronize(stream)
GPU/source/MOD_utilgpu.f:         ierr = cudaMemsetAsync(dst(offset_+1,1,1,1,1),val,n,stream)
GPU/source/MOD_utilgpu.f:      if (ierr.ne.cudasuccess) then
GPU/source/MOD_utilgpu.f:         write(*,14) 'error',ierr,'detected in utilgpu_mem_set_r4'
GPU/source/MOD_utilgpu.f:     &               ,cudageterrorstring(ierr)
GPU/source/MOD_utilgpu.f:      integer(cuda_stream_kind),intent(in):: stream
GPU/source/MOD_utilgpu.f:#ifdef _OPENACC
GPU/source/MOD_utilgpu.f:      ierr = cudaMemCpyAsync(dst,src,n,cudaMemcpyDeviceToDevice,stream)
GPU/source/MOD_utilgpu.f:      if(stream.eq.zero8) ierr = ierr + cudaStreamSynchronize(stream)
GPU/source/MOD_utilgpu.f:      if (ierr.ne.cudasuccess) then
GPU/source/MOD_utilgpu.f:         write(*,*) 'error',ierr,'detected in utilgpu_mem_move_i4'
GPU/source/MOD_utilgpu.f:         write(*,*) cudageterrorstring(ierr)
GPU/source/MOD_utilgpu.f:      integer(cuda_stream_kind),intent(in):: stream
GPU/source/MOD_utilgpu.f:#ifdef _OPENACC
GPU/source/MOD_utilgpu.f:      ierr = cudamemcpyasync(dst,src,n,cudaMemcpyDeviceToDevice,stream)
GPU/source/MOD_utilgpu.f:      if(stream.eq.zero8) ierr = ierr + cudaStreamSynchronize(stream)
GPU/source/MOD_utilgpu.f:      if (ierr.ne.cudasuccess) then
GPU/source/MOD_utilgpu.f:         write(*,*) 'error',ierr,'detected in utilgpu_mem_move_i8'
GPU/source/MOD_utilgpu.f:         write(*,*) cudageterrorstring(ierr)
GPU/source/MOD_utilgpu.f:      integer(cuda_stream_kind),intent(in):: stream
GPU/source/MOD_utilgpu.f:#ifdef _OPENACC
GPU/source/MOD_utilgpu.f:      ierr = cudamemcpyasync(dst,src,n,cudaMemcpyDeviceToDevice,stream)
GPU/source/MOD_utilgpu.f:      if(stream.eq.zero8) ierr = ierr + cudaStreamSynchronize(stream)
GPU/source/MOD_utilgpu.f:      if (ierr.ne.cudasuccess) then
GPU/source/MOD_utilgpu.f:         write(*,*) 'error',ierr,'detected in utilgpu_mem_move_r4'
GPU/source/MOD_utilgpu.f:         write(*,*) cudageterrorstring(ierr)
GPU/source/MOD_utilgpu.f:      integer(cuda_stream_kind),intent(in):: stream
GPU/source/MOD_utilgpu.f:#ifdef _OPENACC
GPU/source/MOD_utilgpu.f:      ierr = cudamemcpyasync(dst,src,n,cudaMemcpyDeviceToDevice,stream)
GPU/source/MOD_utilgpu.f:      if(stream.eq.zero8) ierr = ierr + cudaStreamSynchronize(stream)
GPU/source/MOD_utilgpu.f:      if (ierr.ne.cudasuccess) then
GPU/source/MOD_utilgpu.f:         write(*,*) 'error',ierr,'detected in utilgpu_mem_move_r4'
GPU/source/MOD_utilgpu.f:         write(*,*) cudageterrorstring(ierr)
GPU/source/MOD_utilgpu.f:      integer(cuda_stream_kind),intent(in):: stream
GPU/source/MOD_utilgpu.f:#ifdef _OPENACC
GPU/source/MOD_utilgpu.f:      ierr = cudamemcpyasync(dst,src,n,cudaMemcpyDeviceToDevice,stream)
GPU/source/MOD_utilgpu.f:      if(stream.eq.zero8) ierr = ierr + cudaStreamSynchronize(stream)
GPU/source/MOD_utilgpu.f:      if (ierr.ne.cudasuccess) then
GPU/source/MOD_utilgpu.f:         write(*,*) 'error',ierr,'detected in utilgpu_mem_move_r8'
GPU/source/MOD_utilgpu.f:         write(*,*) cudageterrorstring(ierr)
GPU/source/MOD_utilgpu.f:      integer(cuda_stream_kind),intent(in):: stream
GPU/source/MOD_utilgpu.f:#ifdef _OPENACC
GPU/source/MOD_utilgpu.f:      ierr = cudamemcpyasync(dst,src,n,cudaMemcpyDeviceToDevice,stream)
GPU/source/MOD_utilgpu.f:      if(stream.eq.zero8) ierr = ierr + cudaStreamSynchronize(stream)
GPU/source/MOD_utilgpu.f:      if (ierr.ne.cudasuccess) then
GPU/source/MOD_utilgpu.f:         write(*,*) 'error',ierr,'detected in utilgpu_mem_move_r8'
GPU/source/MOD_utilgpu.f:         write(*,*) cudageterrorstring(ierr)
GPU/source/MOD_utilgpu.f:      end module utilgpu
GPU/source/alterchg.f:      use utilgpu
GPU/source/alterchg.f:#ifndef _OPENACC
GPU/source/alterchg.f:#ifndef _OPENACC
GPU/source/katom.f:#ifdef _OPENACC
GPU/source/dynamic_rep.f:#ifdef _OPENACC
GPU/source/dynamic_rep.f:      use utilgpu,only: bind_gpu
GPU/source/dynamic_rep.f:#ifdef _OPENACC
GPU/source/dynamic_rep.f:      call bind_gpu
GPU/source/dynamic_rep.f:      use utilgpu ,only: rec_queue
GPU/source/empole3gpu.f:      module empole3gpu_inl
GPU/source/empole3gpu.f:      subroutine empole3gpu
GPU/source/empole3gpu.f:        call elambdampole3cgpu
GPU/source/empole3gpu.f:        call empole3cgpu
GPU/source/empole3gpu.f:c     ##  subroutine empole3cgpu     --  Ewald multipole analysis via list  ##
GPU/source/empole3gpu.f:      subroutine empole3cgpu
GPU/source/empole3gpu.f:      use empole3gpu_inl
GPU/source/empole3gpu.f:      use utilgpu
GPU/source/empole3gpu.f:      if(deb_Path) write(*,*) 'empole3cgpu'
GPU/source/empole3gpu.f:#ifdef _OPENACC
GPU/source/empole3gpu.f:         call emrecipgpu
GPU/source/empole3gpu.f:#ifdef _OPENACC
GPU/source/empole3gpu.f:c     ##  subroutine elambdampole3cgpu     --  Ewald multipole analysis via list  ##
GPU/source/empole3gpu.f:      subroutine elambdampole3cgpu
GPU/source/empole3gpu.f:      use empole3gpu_inl
GPU/source/empole3gpu.f:      use utilgpu
GPU/source/empole3gpu.f:#ifdef _OPENACC
GPU/source/empole3gpu.f:      if(deb_Path) write(*,*) 'empole3cgpu'
GPU/source/empole3gpu.f:#ifdef _OPENACC
GPU/source/empole3gpu.f:      call chkpolegpu(.false.)
GPU/source/empole3gpu.f:      call rotpolegpu
GPU/source/empole3gpu.f:#ifdef _OPENACC
GPU/source/empole3gpu.f:         call rotpolegpu
GPU/source/empole3gpu.f:         call emrecipgpu
GPU/source/empole3gpu.f:         call rotpolegpu
GPU/source/empole3gpu.f:         call emrecipgpu
GPU/source/empole3gpu.f:         call rotpolegpu
GPU/source/empole3gpu.f:#ifdef _OPENACC
GPU/source/empole3gpu.f:c     ##  subroutine emreal3dgpu  --  real space mpole analysis via list  ##
GPU/source/empole3gpu.f:      use empole3gpu_inl
GPU/source/empole3gpu.f:      use utilgpu,only:dir_queue,rec_queue,def_queue,warning
GPU/source/empole3gpu.f:#ifdef _CUDA
GPU/source/empole3gpu.f:      ! CUDA Fortran version of emreal1c using C2 nblist
GPU/source/empole3gpu.f:      use utilgpu,only:dir_queue,rec_queue,def_queue,warning
GPU/source/empole3gpu.f:         call cudaMaxGridSize("emreal3_kcu",gS)
GPU/source/empole3gpu.f:c     ##  subroutine emrecipgpu  --  PME recip space multipole energy  ##
GPU/source/empole3gpu.f:c     "emrecipgpu" evaluates on device the reciprocal space portion of the particle
GPU/source/empole3gpu.f:      subroutine emrecipgpu
GPU/source/empole3gpu.f:      use interfaces,only: torquegpu,fphi_mpole_site_p
GPU/source/empole3gpu.f:      use utilgpu ,only:dir_queue,rec_queue
GPU/source/empole3gpu.f:     &   write(*,'(2x,a)') 'emrecipgpu'
GPU/source/empole3gpu.f:      call cmp_to_fmp_sitegpu(cmp,fmp)
GPU/source/empole3gpu.f:         call aaddgpuAsync(2*n1mpimax*n2mpimax*n3mpimax,
GPU/source/empole3gpu.f:#ifdef _OPENACC
GPU/source/empole3gpu.f:#ifdef _OPENACC
GPU/source/kewald.f:      call frac_to_cartgpu
GPU/source/kewald.f:      call cart_to_fracgpu
GPU/source/pair_efld_cp.inc.f:#include "tinker_cudart.h"
GPU/source/analysis.f:      if (use_bond)    call ebond3gpu
GPU/source/analysis.f:      if (use_angle)   call eangle3gpu
GPU/source/analysis.f:      if (use_urey)    call eurey3gpu
GPU/source/analysis.f:      if (use_opbend)  call eopbend3gpu
GPU/source/analysis.f:         if (vdwtyp .eq. 'LENNARD-JONES')  call elj3gpu
GPU/source/analysis.f:         if (vdwtyp .eq. 'BUFFERED-14-7')  call ehal3gpu
GPU/source/analysis.f:      if (use_charge)  call echarge3gpu
GPU/source/analysis.f:      if (use_mpole)   call empole3gpu
GPU/source/analysis.f:      if (use_polar)   call epolar3gpu
GPU/source/analysis.f:      if (use_chgtrn)  call echgtrn3gpu
GPU/source/analysis.f:      if (use_geom)    call egeom3gpu
GPU/source/MOD_tinheader.f:#ifdef _CUDA
GPU/source/linalg.f:      subroutine sprodgpu(n,v,w,r)
GPU/source/linalg.f:      subroutine aaddgpu(n,a,b,c)
GPU/source/linalg.f:      subroutine aaddgpuAsync(n,a,b,c)
GPU/source/linalg.f:      use utilgpu, only : rec_queue
GPU/source/dcinduce_pmegpu.f:      subroutine dcinduce_pmegpu
GPU/source/dcinduce_pmegpu.f:      subroutine inducedc_pmegpu(matvec,nrhs,dodiis,ef,mu,murec)
GPU/source/dcinduce_pmegpu.f:#ifdef _OPENACC
GPU/source/dcinduce_pmegpu.f:            call fatal_device("inducedc_pmegpu")
GPU/source/dcinduce_pmegpu.f:#ifdef _OPENACC
GPU/source/dcinduce_pmegpu.f:            call fatal_device("inducedc_pmegpu")
GPU/source/dcinduce_pmegpu.f:      subroutine dc_factorgpu
GPU/source/dcinduce_pmegpu.f:#ifdef _OPENACC
GPU/source/dcinduce_pmegpu.f:      use utilgpu    ,only: rec_stream
GPU/source/dcinduce_pmegpu.f:      use utilgpu    ,only: prmem_request
GPU/source/dcinduce_pmegpu.f:#ifdef _OPENACC
GPU/source/dcinduce_pmegpu.f:#ifdef _OPENACC
GPU/source/dcinduce_pmegpu.f:      subroutine pc_dc_tmatxb_pmegpu(nrhs,dodiag,mu,efi)
GPU/source/dcinduce_pmegpu.f:     &   write(*,'(4x,a)') 'pc_dc_tmatxb_pmegpu'
GPU/source/dcinduce_pmegpu.f:      subroutine commfield2gpu(nrhs,ef)
GPU/source/dcinduce_pmegpu.f:      use utilgpu ,only: prmem_request,dir_queue
GPU/source/dcinduce_pmegpu.f:      call dc_factorgpu
GPU/source/dcinduce_pmegpu.f:      subroutine commfield2shortgpu(nrhs,ef)
GPU/source/dcinduce_pmegpu.f:      use utilgpu ,only: prmem_request,dir_queue
GPU/source/dcinduce_pmegpu.f:      call dc_factorgpu
GPU/source/etors3.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/etors3.f:#ifndef _OPENACC
GPU/source/etors3.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/etors3.f:      use utilgpu ,only: lam_buff
GPU/source/dynamic.f:#ifdef _OPENACC
GPU/source/dynamic.f:      use utilgpu,only: bind_gpu
GPU/source/dynamic.f:#ifdef _OPENACC
GPU/source/dynamic.f:      call bind_gpu
GPU/source/dynamic.f:      use utilgpu ,only: rec_queue,ti_p,re_p
GPU/source/dcinduce_pme2gpu.f:      subroutine dcinduce_pme2gpu
GPU/source/dcinduce_pme2gpu.f:      use interfaces,only: otf_dc_efld0_directgpu_p
GPU/source/dcinduce_pme2gpu.f:      use utilgpu
GPU/source/dcinduce_pme2gpu.f:      external pc_dc_tmatxb_pmegpu
GPU/source/dcinduce_pme2gpu.f:      external otf_dc_tmatxb_pmegpu
GPU/source/dcinduce_pme2gpu.f:     &   write(*,'(2x,a)') 'dcinduce_pme2gpu'
GPU/source/dcinduce_pme2gpu.f:#ifdef _OPENACC
GPU/source/dcinduce_pme2gpu.f:        call openacc_abort("kmean clustering not yet supported with"
GPU/source/dcinduce_pme2gpu.f:     &       //" OpenAcc !!")
GPU/source/dcinduce_pme2gpu.f:#ifdef _OPENACC
GPU/source/dcinduce_pme2gpu.f:#ifdef _OPENACC
GPU/source/dcinduce_pme2gpu.f:         call openacc_abort("dcinduce pc_dc not supported with OpenAcc")
GPU/source/dcinduce_pme2gpu.f:         call otf_dc_efld0_directgpu_p(nrhs,ef)
GPU/source/dcinduce_pme2gpu.f:      call commfield2gpu(nrhs,ef)
GPU/source/dcinduce_pme2gpu.f:      call efld0_recipgpu(cphi,ef)
GPU/source/dcinduce_pme2gpu.f:      call commrecdirfieldsgpu(0,cphirec,cphi,buffermpi1,buffermpi2,
GPU/source/dcinduce_pme2gpu.f:      call commrecdirfieldsgpu(1,cphirec,cphi,buffermpi1,buffermpi2,
GPU/source/dcinduce_pme2gpu.f:      call commrecdirfieldsgpu(2,cphirec,cphi,buffermpi1,buffermpi2,
GPU/source/dcinduce_pme2gpu.f:      call commdirdirgpu(nrhs,0,mu,reqrec,reqsend)
GPU/source/dcinduce_pme2gpu.f:#ifdef _OPENACC
GPU/source/dcinduce_pme2gpu.f:      call commdirdirgpu(nrhs,1,mu,reqrec,reqsend)
GPU/source/dcinduce_pme2gpu.f:      call commdirdirgpu(nrhs,2,mu,reqrec,reqsend)
GPU/source/dcinduce_pme2gpu.f:      call commrecdirdipgpu(nrhs,0,murec,mu,buffermpimu1,buffermpimu2,
GPU/source/dcinduce_pme2gpu.f:      call commrecdirdipgpu(nrhs,1,murec,mu,buffermpimu1,buffermpimu2,
GPU/source/dcinduce_pme2gpu.f:      call commrecdirdipgpu(nrhs,2,murec,mu,buffermpimu1,buffermpimu2,
GPU/source/dcinduce_pme2gpu.f:        call inducedc_pme2gpu( pc_dc_tmatxb_pmegpu,nrhs,.true.,ef,mu,
GPU/source/dcinduce_pme2gpu.f:        call inducedc_pme2gpu(otf_dc_tmatxb_pmegpu,nrhs,.true.,ef,mu,
GPU/source/dcinduce_pme2gpu.f:      subroutine inducedc_pme2gpu(matvec,nrhs,dodiis,ef,mu,murec)
GPU/source/dcinduce_pme2gpu.f:      use utilgpu
GPU/source/dcinduce_pme2gpu.f:     &   write(*,'(3x,a)') 'inducedc_pme2gpu'
GPU/source/dcinduce_pme2gpu.f:#ifdef _OPENACC
GPU/source/dcinduce_pme2gpu.f:#ifdef _OPENACC
GPU/source/dcinduce_pme2gpu.f:         call tmatxbrecipgpu(mu,murec,nrhs,dipfield,dipfieldbis)
GPU/source/dcinduce_pme2gpu.f:         call commfieldgpu(nrhs,h)
GPU/source/dcinduce_pme2gpu.f:         call commrecdirsolvgpu(nrhs,0,dipfieldbis,dipfield,buffermpi1,
GPU/source/dcinduce_pme2gpu.f:         call commdirdirgpu(nrhs,0,mu,reqrec,reqsend)
GPU/source/dcinduce_pme2gpu.f:         call commrecdirsolvgpu(nrhs,1,dipfieldbis,dipfield,buffermpi1,
GPU/source/dcinduce_pme2gpu.f:         call commrecdirdipgpu(nrhs,0,murec,munew,buffermpimu1,
GPU/source/dcinduce_pme2gpu.f:         call commrecdirsolvgpu(nrhs,2,dipfieldbis,dipfield,buffermpi1,
GPU/source/dcinduce_pme2gpu.f:#ifdef _OPENACC
GPU/source/dcinduce_pme2gpu.f:             !TODO This loop is no longer useful; MPI_IAllReduce is not CUDA-Aware
GPU/source/dcinduce_pme2gpu.f:#ifdef _OPENACC
GPU/source/dcinduce_pme2gpu.f:         call commdirdirgpu(nrhs,1,munew,reqrec,reqsend)
GPU/source/dcinduce_pme2gpu.f:         call commrecdirdipgpu(nrhs,1,murec,munew,buffermpimu1,
GPU/source/dcinduce_pme2gpu.f:         call commdirdirgpu(nrhs,2,mu,reqrec,reqsend)
GPU/source/dcinduce_pme2gpu.f:         call commrecdirdipgpu(nrhs,2,murec,munew,buffermpimu1,
GPU/source/MOD_strbnd.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/MOD_strbnd.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/eliaison1gpu.f:      module eliaison1gpu_inl
GPU/source/eliaison1gpu.f:      use utilgpu ,only: WARP_SHFT
GPU/source/eliaison1gpu.f:#ifdef _OPENACC
GPU/source/eliaison1gpu.f:      use eliaison1gpu_inl
GPU/source/eliaison1gpu.f:      use cudafor   ,only: dim3
GPU/source/eliaison1gpu.f:      use utilgpu
GPU/source/eliaison1gpu.f:      if(deb_Path) write(*,*) 'eliaison1gpu'
GPU/source/eliaison1gpu.f:     &       /,' eliaison1gpu routine is not made for host execution ')
GPU/source/bitors.f:      use utilgpu
GPU/source/bitors.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/bitors.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/bitors.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/bitors.f:#ifdef _OPENACC
GPU/source/baoab.f:      use utilgpu  ,only: rec_queue,def_queue
GPU/source/baoab.f:      call mdrestgpu (istep)
GPU/source/nblist.f:      use utilgpu  ,only: rec_queue
GPU/source/nblist.f:         call build_cell_listgpu(istep)
GPU/source/nblist.f:         if (clst_en) call clistcell2gpu
GPU/source/nblist.f:         if (clst_en) call clistcellgpu
GPU/source/nblist.f:         if (vlst_en) call vlistcell2gpu
GPU/source/nblist.f:         if (vlst_en) call vlistcellgpu
GPU/source/nblist.f:         if (mlst_en) call mlistcell2gpu
GPU/source/nblist.f:         if (mlst_en) call mlistcellgpu
GPU/source/nblist.f:      use utilgpu   ,only:rec_queue
GPU/source/nblist.f:#ifdef _OPENACC
GPU/source/nblist.f:#ifdef _OPENACC
GPU/source/nblist.f:#ifdef _OPENACC
GPU/source/nblist.f:#ifdef _OPENACC
GPU/source/nblist.f:#ifdef _OPENACC
GPU/source/nblist.f:#ifdef _OPENACC
GPU/source/nblist.f:      use utilgpu ,only: rec_queue
GPU/source/ebond.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/ebond.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/eimptor3.f:      if (deb_Path) write(*,*) 'eimptor3gpu'
GPU/source/eimptor3.f:#ifndef _OPENACC
GPU/source/baoabrespa.f:      use utilgpu
GPU/source/baoabrespa.f:c          call rotpolegpu
GPU/source/baoabrespa.f:      call mdrestgpu (istep)
GPU/source/MOD_angle.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/MOD_angle.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/baoabrespa1.f:      use utilgpu
GPU/source/baoabrespa1.f:      call mdrestgpu (istep)
GPU/source/baoabrespa1.f:      use utilgpu
GPU/source/baoabrespa1.f:      use utilgpu
GPU/source/empole1_group.f:      use utilgpu
GPU/source/empole1_group.f:      use interfaces ,only: torquegpu_group
GPU/source/empole1_group.f:      use utilgpu
GPU/source/empole1_group.f:      call torquegpu_group(trqvec,fix,fiy,fiz,demgroup)
GPU/source/empole1_group.f:      use utilgpu ,only: def_queue
GPU/source/nblistgpu.f:      module nblistgpu_inl
GPU/source/nblistgpu.f:      use nblistgpu_inl
GPU/source/nblistgpu.f:      use utilgpu,only:rank,dir_queue
GPU/source/nblistgpu.f:      subroutine initmpipmegpu
GPU/source/nblistgpu.f:      use utilgpu ,only: rec_queue
GPU/source/nblistgpu.f:#ifdef _OPENACC
GPU/source/nblistgpu.f:#ifdef _OPENACC
GPU/source/nblistgpu.f:      use utilgpu,only: rec_queue
GPU/source/nblistgpu.f:32       format( ' build_cell_listgpu : Atoms are missing from',
GPU/source/nblistgpu.f:      subroutine build_cell_listgpu(istep)
GPU/source/nblistgpu.f:      use nblistgpu_inl
GPU/source/nblistgpu.f:      use utilgpu   ,only: nSMP, cores_SMP
GPU/source/nblistgpu.f:     &              , openacc_abort,rec_queue
GPU/source/nblistgpu.f:      if (deb_Path) write(*,'(2x,a)') 'build_cell_listgpu'
GPU/source/nblistgpu.f:24          format ('build_cell_listgpu -- Improper cell number',/,
GPU/source/nblistgpu.f:                     print*,':build_cell_listgpu'
GPU/source/nblistgpu.f:            print*,'build_cell_listgpu: max neighbor cell reached'
GPU/source/nblistgpu.f:      !TODO Integrate ii cell to neigcell and update [vcm]list2?gpu
GPU/source/nblistgpu.f:      use nblistgpu_inl
GPU/source/nblistgpu.f:#ifdef _OPENACC
GPU/source/nblistgpu.f:      use utilgpu  ,only: nSMP,cores_SMP
GPU/source/nblistgpu.f:#ifdef _OPENACC
GPU/source/nblistgpu.f:      if (deb_Path) write(*,'(2x,a)') 'build_cell_listgpu2'
GPU/source/nblistgpu.f:#ifdef _OPENACC
GPU/source/nblistgpu.f:      use utilgpu
GPU/source/nblistgpu.f:      use utilgpu
GPU/source/nblistgpu.f:      use utilgpu
GPU/source/nblistgpu.f:      use nblistgpu_inl ,only: midpointimage_inl
GPU/source/nblistgpu.f:      use utilgpu ,only: prmem_request,rec_queue,BLOCK_SIZE
GPU/source/nblistgpu.f:      use nblistgpu_inl
GPU/source/nblistgpu.f:      use utilgpu ,only: prmem_request,rec_queue,BLOCK_SIZE
GPU/source/nblistgpu.f:                     print*,':build_cell_listgpu'
GPU/source/nblistgpu.f:      use utilgpu ,only: rec_queue,prmem_request
GPU/source/nblistgpu.f:#ifdef _OPENACC
GPU/source/nblistgpu.f:      use utilgpu ,only: rec_stream
GPU/source/nblistgpu.f:#ifdef _OPENACC
GPU/source/nblistgpu.f:      use utilgpu ,only: rec_queue,prmem_request
GPU/source/nblistgpu.f:#ifdef _OPENACC
GPU/source/nblistgpu.f:      use utilgpu ,only: rec_stream
GPU/source/nblistgpu.f:      use utilgpu,only: BLOCK_SIZE,rec_queue
GPU/source/nblistgpu.f:      use utilgpu,only: BLOCK_SIZE,rec_queue
GPU/source/nblistgpu.f:      use utilgpu,only: BLOCK_SIZE,rec_queue
GPU/source/nblistgpu.f:      use utilgpu,only: BLOCK_SIZE,rec_queue,prmem_request
GPU/source/nblistgpu.f:#ifdef _OPENACC
GPU/source/nblistgpu.f:#ifdef _OPENACC
GPU/source/nblistgpu.f:      use utilgpu,only: BLOCK_SIZE,rec_queue,prmem_request
GPU/source/nblistgpu.f:#ifdef _OPENACC
GPU/source/nblistgpu.f:#ifdef _OPENACC
GPU/source/nblistgpu.f:#ifdef _OPENACC
GPU/source/nblistgpu.f:      use utilgpu
GPU/source/nblistgpu.f:#ifndef _OPENACC
GPU/source/nblistgpu.f:#ifdef _OPENACC
GPU/source/nblistgpu.f:#ifdef _OPENACC
GPU/source/nblistgpu.f:#ifdef _OPENACC
GPU/source/nblistgpu.f:#ifdef _OPENACC
GPU/source/nblistgpu.f:#ifdef _OPENACC
GPU/source/nblistgpu.f:      use utilgpu ,only: rec_stream
GPU/source/nblistgpu.f:      use cudafor
GPU/source/nblistgpu.f:      use utilgpu ,only: rec_queue,BLOCK_SIZE,inf
GPU/source/nblistgpu.f:#ifdef _CUDA
GPU/source/nblistgpu.f:      !TODO Add an OpenACC version of this kernel
GPU/source/nblistgpu.f:#ifdef _OPENACC
GPU/source/kimptor.f:#ifdef _OPENACC
GPU/source/radial.f:#ifdef _OPENACC
GPU/source/radial.f:      use utilgpu,only: bind_gpu
GPU/source/radial.f:#ifdef _OPENACC
GPU/source/radial.f:      call bind_gpu
GPU/source/switch_respa.f.inc:#include "tinker_cudart.h"
GPU/source/empole3_group.f:      use utilgpu
GPU/source/empole3_group.f:      use utilgpu
GPU/source/empole3_group.f:      use utilgpu ,only: def_queue,real3,real6,real3_red,rpole_elt
GPU/source/pmestuff.f:      subroutine cart_to_fracgpu
GPU/source/pmestuff.f:      use utilgpu,only:rec_queue,ctf,qi1,qi2
GPU/source/pmestuff.f:      subroutine frac_to_cartgpu
GPU/source/pmestuff.f:      use utilgpu,only:rec_queue,ftc,qi1,qi2
GPU/source/pmestuff.f:      use utilgpu   ,only: rec_queue
GPU/source/pmestuff.f:#ifdef _OPENACC
GPU/source/pmestuff.f:#ifdef _OPENACC
GPU/source/pmestuff.f:#ifdef _OPENACC
GPU/source/pmestuff.f:          ! Attach CUDA grid device pointers
GPU/source/pmestuff.f:#ifdef _OPENACC
GPU/source/pmestuff.f:#ifdef _OPENACC
GPU/source/pmestuff.f:        if(nproc.gt.1) call orderbuffer_gpu(.false.)
GPU/source/pmestuff.f:      call orderbufferrec_gpu
GPU/source/dcinduce_pme2.f:#ifdef _OPENACC
GPU/source/dcinduce_pme2.f:#ifdef _OPENACC
GPU/source/efld0_cpencu.f:#ifdef _CUDA
GPU/source/efld0_cpencu.f:#include "tinker_cudart.h"
GPU/source/efld0_cpencu.f:      use utilgpu ,only: BLOCK_SIZE
GPU/source/ebond1gpu.f:      module ebond1gpu_inl
GPU/source/ebond1gpu.f:      subroutine ebond1gpu
GPU/source/ebond1gpu.f:      use ebond1gpu_inl
GPU/source/ebond1gpu.f:      if (deb_Path) write(*,*) 'ebond1gpu'
GPU/source/ebond1gpu.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/ebond1gpu.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/etors.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/etors.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/newinduce_shortrealgpu.f:      subroutine newinduce_shortrealgpu
GPU/source/newinduce_shortrealgpu.f:      use interfaces,only:inducepcg_shortrealgpu
GPU/source/newinduce_shortrealgpu.f:     &              ,inducestepcg_pme2gpu
GPU/source/newinduce_shortrealgpu.f:     &              ,tmatxb_p1,efld0_directgpu2
GPU/source/newinduce_shortrealgpu.f:     &              ,efld0_directgpu_p1
GPU/source/newinduce_shortrealgpu.f:      use utilgpu
GPU/source/newinduce_shortrealgpu.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/newinduce_shortrealgpu.f:#ifdef _OPENACC
GPU/source/newinduce_shortrealgpu.f:      call efld0_directgpu_p1(nrhs,ef)
GPU/source/newinduce_shortrealgpu.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/newinduce_shortrealgpu.f:         call inducepcg_shortrealgpu(tmatxb_p1,nrhs,.true.,ef,mu)
GPU/source/newinduce_shortrealgpu.f:         call inducejac_shortrealgpu(tmatxb_p1,nrhs,.true.,ef,mu)
GPU/source/newinduce_shortrealgpu.f:         call inducestepcg_pme2gpu(tmatxb_p1,nrhs,.true.,ef,mu,murec)
GPU/source/newinduce_shortrealgpu.f:#ifdef _OPENACC
GPU/source/newinduce_shortrealgpu.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/newinduce_shortrealgpu.f:      subroutine inducepcg_shortrealgpu(matvec,nrhs,precnd,ef,mu)
GPU/source/newinduce_shortrealgpu.f:      use interfaces ,only: tmatxb_pmegpu,pcg_a,pcg_b,pcg_newDirection
GPU/source/newinduce_shortrealgpu.f:      use utilgpu
GPU/source/newinduce_shortrealgpu.f:      procedure(tmatxb_pmegpu) :: matvec
GPU/source/newinduce_shortrealgpu.f:     &   write(*,'(3x,a)') 'inducepcg_shortrealgpu'
GPU/source/newinduce_shortrealgpu.f:#ifdef _OPENACC
GPU/source/newinduce_shortrealgpu.f:      use utilgpu  ,only: def_queue
GPU/source/newinduce_shortrealgpu.f:      use utilgpu  ,only: def_queue
GPU/source/newinduce_shortrealgpu.f:      use utilgpu  ,only: def_queue
GPU/source/newinduce_shortrealgpu.f:      use utilgpu ,only: def_queue
GPU/source/newinduce_shortrealgpu.f:      subroutine inducejac_shortrealgpu(matvec,nrhs,dodiis,ef,mu)
GPU/source/newinduce_shortrealgpu.f:#ifdef _OPENACC
GPU/source/newinduce_shortrealgpu.f:          call fatal_device("inducejac_shortrealgpu")
GPU/source/eamd1.f:      use utilgpu,only: rec_queue
GPU/source/ehalcu.tpl.f:c     Hal CUDA Kernel Template 
GPU/source/echgtrncu.tpl.f:c     Chgtrn CUDA Kernel Template 
GPU/source/pair_mpole1.f.inc:#include "tinker_cudart.h"
GPU/source/pair_mpole1.f.inc:      use utilgpu   ,only: rpole_elt,real3,real6,mdyn3_r
GPU/source/pair_mpole1.f.inc:      use utilgpu   ,only: rpole_elt,real3,real6,mdyn3_r
GPU/source/pair_mpole1.f.inc:      use utilgpu   ,only: rpole_elt,real3,real6
GPU/source/pair_mpole1.f.inc:      use utilgpu   ,only: rpole_elt,real3,real6
GPU/source/mdstate.gpu.f:#ifdef _OPENACC
GPU/source/mdstate.gpu.f:     &               ,randomgpu,normalgpu
GPU/source/mdstate.gpu.f:      use utilgpu
GPU/source/mdstate.gpu.f:#ifdef _OPENACC
GPU/source/mdstate.gpu.f:         do ii = 1,pack_; call randomgpu(samplevec,s); end do
GPU/source/mdstate.gpu.f:      do i = 1,pack_/s; call randomgpu(samplevec,s); end do
GPU/source/mdstate.gpu.f:      if (s.gt.0) call randomgpu(samplevec,s)
GPU/source/mdstate.gpu.f:         do ii = 1,pack_; call normalgpu(samplevec,s); end do
GPU/source/mdstate.gpu.f:      do i = 1,pack_/s; call normalgpu(samplevec,s); end do
GPU/source/mdstate.gpu.f:      if (s.gt.0) call normalgpu(samplevec,s)
GPU/source/lattice.f:#ifdef _OPENACC
GPU/source/lattice.f:#ifdef _OPENACC
GPU/source/lattice.f:#ifdef _OPENACC
GPU/source/lattice.f:         call frac_to_cartgpu
GPU/source/lattice.f:         call cart_to_fracgpu
GPU/source/mdinit.f:      use mdstuf1 ,only: gpuAllocMdstuf1Data
GPU/source/mdinit.f:#ifdef _OPENACC
GPU/source/mdinit.f:      use utilgpu ,only: prmem_requestm,rec_queue,rec_stream,mem_set
GPU/source/mdinit.f:      subroutine maxwellgpu (mass,temper,nloc,max_result)
GPU/source/mdinit.f:#ifdef _OPENACC
GPU/source/mdinit.f:      call gpuAllocMdstuf1Data
GPU/source/mdinit.f:#ifdef _OPENACC
GPU/source/mdinit.f:            call rand_unitgpu(samplevec(1),nloc)
GPU/source/mdinit.f:            call maxwellgpu(mass,kelvin,nloc,speedvec)
GPU/source/mdinit.f:         if (nuse .eq. n)  call mdrestgpu (0)
GPU/source/mdinit.f:#ifdef _OPENACC
GPU/source/mdinit.f:            call rand_unitgpu(samplevec(1),nloc)
GPU/source/mdinit.f:            call maxwellgpu(mass,kelvin,nloc,speedvec)
GPU/source/mdinit.f:         if (nuse .eq. n)  call mdrestgpu (0)
GPU/source/mdinit.f:#ifdef _OPENACC
GPU/source/mdinit.f:            call rand_unitgpu(samplevec(1),nloc)
GPU/source/mdinit.f:            call maxwellgpu(mass,kelvin,nloc,speedvec)
GPU/source/mdinit.f:         if (nuse .eq. n)  call mdrestgpu (0)
GPU/source/mdinit.f:#ifdef _OPENACC
GPU/source/mdinit.f:#ifdef _OPENACC
GPU/source/newinduce_pme2gpu.f:      subroutine newinduce_pme2gpu
GPU/source/newinduce_pme2gpu.f:      use interfaces,only: inducepcg_pme2gpu,tmatxb_p
GPU/source/newinduce_pme2gpu.f:     &                   , inducestepcg_pme2gpu
GPU/source/newinduce_pme2gpu.f:     &                   , efld0_directgpu2, efld0_directgpu_p
GPU/source/newinduce_pme2gpu.f:      use utilgpu
GPU/source/newinduce_pme2gpu.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/newinduce_pme2gpu.f:      if (deb_Path) write(*,'(2x,a)') 'newinduce_pme2gpu'
GPU/source/newinduce_pme2gpu.f:      call efld0_directgpu_p(nrhs,ef)
GPU/source/newinduce_pme2gpu.f:      call efld0_recipgpu(cphi,ef)
GPU/source/newinduce_pme2gpu.f:      call commrecdirfieldsgpu(0,cphirec,cphi,buffermpi1,buffermpi2,
GPU/source/newinduce_pme2gpu.f:      call commrecdirfieldsgpu(1,cphirec,cphi,buffermpi1,buffermpi2,
GPU/source/newinduce_pme2gpu.f:      call commrecdirfieldsgpu(2,cphirec,cphi,buffermpi1,buffermpi2,
GPU/source/newinduce_pme2gpu.f:      call commfieldgpu(nrhs,ef)
GPU/source/newinduce_pme2gpu.f:        call commfieldgpu(nrhs,deflambda)
GPU/source/newinduce_pme2gpu.f:#ifdef _OPENACC
GPU/source/newinduce_pme2gpu.f:      call commdirdirgpu(nrhs,0,mu,reqrec,reqsend)
GPU/source/newinduce_pme2gpu.f:      call commdirdirgpu(nrhs,1,mu,reqrec,reqsend)
GPU/source/newinduce_pme2gpu.f:      call commdirdirgpu(nrhs,2,mu,reqrec,reqsend)
GPU/source/newinduce_pme2gpu.f:      call commrecdirdipgpu(nrhs,0,murec,mu,buffermpimu1,buffermpimu2,
GPU/source/newinduce_pme2gpu.f:      call commrecdirdipgpu(nrhs,1,murec,mu,buffermpimu1,buffermpimu2,
GPU/source/newinduce_pme2gpu.f:      call commrecdirdipgpu(nrhs,2,murec,mu,buffermpimu1,buffermpimu2,
GPU/source/newinduce_pme2gpu.f:         call inducepcg_pme2gpu(tmatxb_p,nrhs,.true.,ef,mu,murec)
GPU/source/newinduce_pme2gpu.f:         call inducejac_pme2gpu(tmatxb_p,nrhs,.true.,ef,mu,murec)
GPU/source/newinduce_pme2gpu.f:         call inducestepcg_pme2gpu(tmatxb_p,nrhs,.true.,ef,mu,murec)
GPU/source/newinduce_pme2gpu.f:            call inducepcg_pme2gpu(tmatxb_p,nrhs,.true.,ef,mu,murec)
GPU/source/newinduce_pme2gpu.f:            call inducestepcg_pme2gpu(tmatxb_p,nrhs,.true.,ef,mu,murec)
GPU/source/newinduce_pme2gpu.f:      use utilgpu
GPU/source/newinduce_pme2gpu.f:      use utilgpu  ,only: def_queue
GPU/source/newinduce_pme2gpu.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/newinduce_pme2gpu.f:#ifndef USE_NVSHMEM_CUDA
GPU/source/newinduce_pme2gpu.f:      call ulspredgpu
GPU/source/newinduce_pme2gpu.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/newinduce_pme2gpu.f:      use utilgpu   ,only: def_queue
GPU/source/newinduce_pme2gpu.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/newinduce_pme2gpu.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/newinduce_pme2gpu.f:      subroutine inducepcg_pme2gpu(matvec,nrhs,precnd,ef,mu,murec)
GPU/source/newinduce_pme2gpu.f:      use interfaces,only: tmatxb_pmegpu,pcg_a,pcg_aRec,pcg_b
GPU/source/newinduce_pme2gpu.f:      use utilgpu
GPU/source/newinduce_pme2gpu.f:      procedure(tmatxb_pmegpu) :: matvec
GPU/source/newinduce_pme2gpu.f:     &   write(*,'(3x,a)') 'inducepcg_pme2gpu'
GPU/source/newinduce_pme2gpu.f:#ifdef _OPENACC
GPU/source/newinduce_pme2gpu.f:      call tmatxbrecipgpu(mu,murec,nrhs,dipfield,dipfieldbis)
GPU/source/newinduce_pme2gpu.f:      call commfieldgpu(nrhs,h)
GPU/source/newinduce_pme2gpu.f:      call commrecdirsolvgpu(nrhs,0,dipfieldbis,dipfield,buffermpi1,
GPU/source/newinduce_pme2gpu.f:      call commdirdirgpu(nrhs,0,pp,reqrec,reqsend)
GPU/source/newinduce_pme2gpu.f:      call commrecdirsolvgpu(nrhs,1,dipfieldbis,dipfield,buffermpi1,
GPU/source/newinduce_pme2gpu.f:      call commrecdirdipgpu(nrhs,0,murec,pp,buffermpimu1,
GPU/source/newinduce_pme2gpu.f:      call commrecdirsolvgpu(nrhs,2,dipfieldbis,dipfield,buffermpi1,
GPU/source/newinduce_pme2gpu.f:      call commdirdirgpu(nrhs,1,pp,reqrec,reqsend)
GPU/source/newinduce_pme2gpu.f:      call commdirdirgpu(nrhs,2,pp,reqrec,reqsend)
GPU/source/newinduce_pme2gpu.f:      call commrecdirdipgpu(nrhs,1,murec,pp,buffermpimu1,
GPU/source/newinduce_pme2gpu.f:      call commrecdirdipgpu(nrhs,2,murec,pp,buffermpimu1,
GPU/source/newinduce_pme2gpu.f:         call tmatxbrecipgpu(pp,murec,nrhs,dipfield,dipfieldbis)
GPU/source/newinduce_pme2gpu.f:         call commfieldgpu(nrhs,h)
GPU/source/newinduce_pme2gpu.f:         call commrecdirsolvgpu(nrhs,0,dipfieldbis,dipfield,buffermpi1,
GPU/source/newinduce_pme2gpu.f:         call commrecdirsolvgpu(nrhs,1,dipfieldbis,dipfield,buffermpi1,
GPU/source/newinduce_pme2gpu.f:         call commrecdirsolvgpu(nrhs,2,dipfieldbis,dipfield,buffermpi1,
GPU/source/newinduce_pme2gpu.f:         call commdirdirgpu(nrhs,1,pp,reqrec,reqsend)
GPU/source/newinduce_pme2gpu.f:         call commdirdirgpu(nrhs,0,pp,reqrec,reqsend)
GPU/source/newinduce_pme2gpu.f:         call commrecdirdipgpu(nrhs,0,murec,pp,buffermpimu1,
GPU/source/newinduce_pme2gpu.f:         call commdirdirgpu(nrhs,2,pp,reqrec,reqsend)
GPU/source/newinduce_pme2gpu.f:         call commrecdirdipgpu(nrhs,1,murec,pp,buffermpimu1,
GPU/source/newinduce_pme2gpu.f:         call commrecdirdipgpu(nrhs,2,murec,pp,buffermpimu1,
GPU/source/newinduce_pme2gpu.f:      call commrecdirdipgpu(nrhs,0,murec,mu,buffermpimu1,
GPU/source/newinduce_pme2gpu.f:      call commdirdirgpu(nrhs,0,mu,reqendrec,reqendsend)
GPU/source/newinduce_pme2gpu.f:      call commdirdirgpu(nrhs,1,mu,reqendrec,reqendsend)
GPU/source/newinduce_pme2gpu.f:      call commdirdirgpu(nrhs,2,mu,reqendrec,reqendsend)
GPU/source/newinduce_pme2gpu.f:      call commrecdirdipgpu(nrhs,1,murec,mu,buffermpimu1,
GPU/source/newinduce_pme2gpu.f:      call commrecdirdipgpu(nrhs,2,murec,mu,buffermpimu1,
GPU/source/newinduce_pme2gpu.f:      subroutine inducejac_pme2gpu(matvec,nrhs,dodiis,ef,mu,murec)
GPU/source/newinduce_pme2gpu.f:      use utilgpu
GPU/source/newinduce_pme2gpu.f:     &   write(*,'(3x,a)') 'inducejac_pme2gpu'
GPU/source/newinduce_pme2gpu.f:      call openacc_abort("induce jac unported use polar-alg 1")
GPU/source/newinduce_pme2gpu.f:        call commdirdirgpu(nrhs,0,mu,reqrec,reqsend)
GPU/source/newinduce_pme2gpu.f:#ifdef _OPENACC
GPU/source/newinduce_pme2gpu.f:          call fatal_device("inducejac_pme2gpu")
GPU/source/newinduce_pme2gpu.f:        call commdirdirgpu(nrhs,1,munew,reqrec,reqsend)
GPU/source/newinduce_pme2gpu.f:        call commdirdirgpu(nrhs,2,mu,reqrec,reqsend)
GPU/source/newinduce_pme2gpu.f:      subroutine inducestepcg_pme2gpu(matvec,nrhs,precnd,ef,mu,murec)
GPU/source/newinduce_pme2gpu.f:      use interfaces,only: tmatxb_pmegpu
GPU/source/newinduce_pme2gpu.f:#ifdef _OPENACC
GPU/source/newinduce_pme2gpu.f:      use utilgpu
GPU/source/newinduce_pme2gpu.f:      procedure(tmatxb_pmegpu) :: matvec
GPU/source/newinduce_pme2gpu.f:     &   write(*,'(3x,a)') 'inducestepcg_pme2gpu'
GPU/source/newinduce_pme2gpu.f:#ifdef _OPENACC
GPU/source/newinduce_pme2gpu.f:#ifdef _OPENACC
GPU/source/newinduce_pme2gpu.f:      call tmatxbrecipgpu(mu,murec,nrhs,dipfield,dipfieldbis)
GPU/source/newinduce_pme2gpu.f:         call commfieldgpu(nrhs,h)
GPU/source/newinduce_pme2gpu.f:         call commrecdirsolvgpu(nrhs,0,dipfieldbis,dipfield,buffermpi1,
GPU/source/newinduce_pme2gpu.f:         call commrecdirsolvgpu(nrhs,1,dipfieldbis,dipfield,buffermpi1,
GPU/source/newinduce_pme2gpu.f:         call commrecdirsolvgpu(nrhs,2,dipfieldbis,dipfield,buffermpi1,
GPU/source/newinduce_pme2gpu.f:#ifdef _OPENACC
GPU/source/newinduce_pme2gpu.f:#ifdef _OPENACC
GPU/source/newinduce_pme2gpu.f:            call commdirdirgpu(nrhs,0,res,reqrec,reqsend)
GPU/source/newinduce_pme2gpu.f:            call commdirdirgpu(nrhs,1,res,reqrec,reqsend)
GPU/source/newinduce_pme2gpu.f:            call commdirdirgpu(nrhs,2,res,reqrec,reqsend)
GPU/source/newinduce_pme2gpu.f:            call commrecdirdipgpu(nrhs,0,murec,res,buffermpimu1,
GPU/source/newinduce_pme2gpu.f:            call commrecdirdipgpu(nrhs,1,murec,res,buffermpimu1,
GPU/source/newinduce_pme2gpu.f:            call commrecdirdipgpu(nrhs,2,murec,res,buffermpimu1,
GPU/source/newinduce_pme2gpu.f:         call tmatxbrecipgpu(res,murec,nrhs,dipfield,dipfieldbis)
GPU/source/newinduce_pme2gpu.f:            call commfieldgpu(nrhs,h)
GPU/source/newinduce_pme2gpu.f:         call commrecdirsolvgpu(nrhs,0,dipfieldbis,dipfield,buffermpi1,
GPU/source/newinduce_pme2gpu.f:         call commrecdirsolvgpu(nrhs,1,dipfieldbis,dipfield,buffermpi1,
GPU/source/newinduce_pme2gpu.f:         call commrecdirsolvgpu(nrhs,2,dipfieldbis,dipfield,buffermpi1,
GPU/source/newinduce_pme2gpu.f:#ifdef _OPENACC
GPU/source/newinduce_pme2gpu.f:#ifdef _OPENACC
GPU/source/newinduce_pme2gpu.f:#ifdef _OPENACC
GPU/source/newinduce_pme2gpu.f:#ifdef _OPENACC
GPU/source/newinduce_pme2gpu.f:            call commdirdirgpu(nrhs,0,mu,reqendrec,reqendsend)
GPU/source/newinduce_pme2gpu.f:            call commdirdirgpu(nrhs,1,mu,reqendrec,reqendsend)
GPU/source/newinduce_pme2gpu.f:            call commdirdirgpu(nrhs,2,mu,reqendrec,reqendsend)
GPU/source/newinduce_pme2gpu.f:            call commrecdirdipgpu(nrhs,0,murec,mu,buffermpimu1,
GPU/source/newinduce_pme2gpu.f:            call commrecdirdipgpu(nrhs,1,murec,mu,buffermpimu1,
GPU/source/newinduce_pme2gpu.f:            call commrecdirdipgpu(nrhs,2,murec,mu,buffermpimu1,
GPU/source/newinduce_pme2gpu.f:         call tmatxbrecipgpu(mu,murec,nrhs,dipfield,dipfieldbis)
GPU/source/newinduce_pme2gpu.f:#ifdef _OPENACC
GPU/source/newinduce_pme2gpu.f:            call commfieldgpu(nrhs,h)
GPU/source/newinduce_pme2gpu.f:         call commrecdirsolvgpu(nrhs,0,dipfieldbis,dipfield,buffermpi1,
GPU/source/newinduce_pme2gpu.f:         call commrecdirsolvgpu(nrhs,1,dipfieldbis,dipfield,buffermpi1,
GPU/source/newinduce_pme2gpu.f:         call commrecdirsolvgpu(nrhs,2,dipfieldbis,dipfield,buffermpi1,
GPU/source/newinduce_pme2gpu.f:      subroutine ulspredgpu
GPU/source/newinduce_pme2gpu.f:      use utilgpu
GPU/source/newinduce_pme2gpu.f:      if (deb_Path) write(*,'(4x,a)') 'ulspredgpu'
GPU/source/newinduce_pme2gpu.f:      subroutine diagvecgpu(nrhs, A, B)
GPU/source/egeom3gpu.f:      module egeom3gpu_inl
GPU/source/egeom3gpu.f:      subroutine egeom3gpu
GPU/source/egeom3gpu.f:      use egeom3gpu_inl
GPU/source/egeom3gpu.f:      if (deb_Path) write(*,*) 'egeom3gpu'
GPU/source/egeom3gpu.f:#ifndef _OPENACC
GPU/source/egeom3gpu.f:#ifndef _OPENACC
GPU/source/egeom3gpu.f:#ifndef _OPENACC
GPU/source/egeom3gpu.f:#ifndef _OPENACC
GPU/source/egeom3gpu.f:#ifndef _OPENACC
GPU/source/egeom3gpu.f:#ifndef _OPENACC
GPU/source/egeom3gpu.f:#ifndef _OPENACC
GPU/source/egeom3gpu.f:#ifndef _OPENACC
GPU/source/eangtor3.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/eangtor3.f:#ifndef _OPENACC
GPU/source/eangtor3.f:      use utilgpu ,only: lam_buff
GPU/source/newinduce_pmegpu.f:      subroutine newinduce_pmegpu
GPU/source/newinduce_pmegpu.f:      use interfaces,only:inducepcg_pmegpu,tmatxb_p,
GPU/source/newinduce_pmegpu.f:     &    efld0_directgpu_p,
GPU/source/newinduce_pmegpu.f:     &    inducejac_pmegpu
GPU/source/newinduce_pmegpu.f:      use utilgpu
GPU/source/newinduce_pmegpu.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/newinduce_pmegpu.f:     &   write(*,'(2a,x)') 'newinduce_pmegpu'
GPU/source/newinduce_pmegpu.f:        call commdirdirgpu(nrhs,0,mu,reqrec,reqsend)
GPU/source/newinduce_pmegpu.f:        call commrecdirfieldsgpu(0,cphirec,cphi,buffermpi,buffermpi,
GPU/source/newinduce_pmegpu.f:        call efld0_recipgpu(cphi,ef)
GPU/source/newinduce_pmegpu.f:        call commrecdirfieldsgpu(1,cphirec,cphi,buffermpi,buffermpi,
GPU/source/newinduce_pmegpu.f:        call commrecdirfieldsgpu(2,cphirec,cphi,buffermpi,buffermpi,
GPU/source/newinduce_pmegpu.f:        call efld0_directgpu_p(nrhs,ef)
GPU/source/newinduce_pmegpu.f:        call commfieldgpu(nrhs,ef)
GPU/source/newinduce_pmegpu.f:        call commrecdirfieldsgpu(2,cphirec,cphi,buffermpi,buffermpi,
GPU/source/newinduce_pmegpu.f:        call commdirdirgpu(nrhs,1,mu,reqrec,reqsend)
GPU/source/newinduce_pmegpu.f:        call commrecdirdipgpu(nrhs,1,murec,mu,buffermpimu,buffermpimu,
GPU/source/newinduce_pmegpu.f:        call commdirdirgpu(nrhs,2,mu,reqrec,reqsend)
GPU/source/newinduce_pmegpu.f:        call commrecdirdipgpu(nrhs,2,murec,mu,buffermpimu,buffermpimu,
GPU/source/newinduce_pmegpu.f:        call commrecdirdipgpu(nrhs,0,murec,mu,buffermpimu,buffermpimu,
GPU/source/newinduce_pmegpu.f:        call commrecdirdipgpu(nrhs,2,murec,mu,buffermpimu,buffermpimu,
GPU/source/newinduce_pmegpu.f:          call inducepcg_pmegpu(tmatxb_p,nrhs,.true.,ef,mu,murec)
GPU/source/newinduce_pmegpu.f:          call inducepcg_pmegpu(tmatxb_p,nrhs,.true.,ef,mu,murec)
GPU/source/newinduce_pmegpu.f:          call inducejac_pmegpu(tmatxb_p,nrhs,.true.,ef,mu,murec)
GPU/source/newinduce_pmegpu.f:          call inducejac_pmegpu(tmatxb_p,nrhs,.true.,ef,mu,murec)
GPU/source/newinduce_pmegpu.f:      subroutine inducepcg_pmegpu(matvec,nrhs,precnd,ef,mu,murec)
GPU/source/newinduce_pmegpu.f:      use interfaces,only: tmatxb_pmegpu
GPU/source/newinduce_pmegpu.f:      use utilgpu
GPU/source/newinduce_pmegpu.f:      procedure(tmatxb_pmegpu)::matvec
GPU/source/newinduce_pmegpu.f:     &   write(*,'(3x,a)') 'inducepcg_pmegpu'
GPU/source/newinduce_pmegpu.f:        call commdirdirgpu(nrhs,0,pp,reqrec,reqsend)
GPU/source/newinduce_pmegpu.f:        call commrecdirsolvgpu(nrhs,0,dipfieldbis,dipfield,buffermpi1,
GPU/source/newinduce_pmegpu.f:        call commrecdirdipgpu(nrhs,0,murec,pp,buffermpimu,
GPU/source/newinduce_pmegpu.f:        call tmatxbrecipgpu(mu,murec,nrhs,dipfield,dipfieldbis)
GPU/source/newinduce_pmegpu.f:        call commrecdirsolvgpu(nrhs,1,dipfieldbis,dipfield,buffermpi1,
GPU/source/newinduce_pmegpu.f:        call commrecdirsolvgpu(nrhs,2,dipfieldbis,dipfield,buffermpi1,
GPU/source/newinduce_pmegpu.f:         call commfieldgpu(nrhs,h)
GPU/source/newinduce_pmegpu.f:         call commrecdirsolvgpu(nrhs,2,dipfieldbis,dipfield,buffermpi1,
GPU/source/newinduce_pmegpu.f:         call commdirdirgpu(nrhs,1,pp,reqrec,reqsend)
GPU/source/newinduce_pmegpu.f:         call commrecdirdipgpu(nrhs,1,murec,pp,buffermpimu,
GPU/source/newinduce_pmegpu.f:         call commdirdirgpu(nrhs,2,mu,reqrec,reqsend)
GPU/source/newinduce_pmegpu.f:         call commrecdirdipgpu(nrhs,2,murec,pp,buffermpimu,
GPU/source/newinduce_pmegpu.f:         call commrecdirdipgpu(nrhs,2,murec,pp,buffermpimu,
GPU/source/newinduce_pmegpu.f:            call commdirdirgpu(nrhs,0,pp,reqrec,reqsend)
GPU/source/newinduce_pmegpu.f:            call commrecdirdipgpu(nrhs,0,murec,pp,buffermpimu,
GPU/source/newinduce_pmegpu.f:            call commrecdirsolvgpu(nrhs,0,dipfieldbis,dipfield
GPU/source/newinduce_pmegpu.f:            call tmatxbrecipgpu(mu,murec,nrhs,dipfield,dipfieldbis)
GPU/source/newinduce_pmegpu.f:            call commrecdirsolvgpu(nrhs,1,dipfieldbis,dipfield
GPU/source/newinduce_pmegpu.f:            call commrecdirsolvgpu(nrhs,2,dipfieldbis,dipfield
GPU/source/newinduce_pmegpu.f:            call commfieldgpu(nrhs,h)
GPU/source/newinduce_pmegpu.f:            call commrecdirsolvgpu(nrhs,2,dipfieldbis,dipfield
GPU/source/newinduce_pmegpu.f:            call commdirdirgpu(nrhs,1,pp,reqrec,reqsend)
GPU/source/newinduce_pmegpu.f:            call commrecdirdipgpu(nrhs,1,murec,pp,buffermpimu,
GPU/source/newinduce_pmegpu.f:            call commdirdirgpu(nrhs,2,pp,reqrec,reqsend)
GPU/source/newinduce_pmegpu.f:            call commrecdirdipgpu(nrhs,2,murec,pp,buffermpimu,
GPU/source/newinduce_pmegpu.f:            call commrecdirdipgpu(nrhs,2,murec,pp,buffermpimu,
GPU/source/newinduce_pmegpu.f:         call commdirdirgpu(nrhs,0,mu,reqendrec,reqendsend)
GPU/source/newinduce_pmegpu.f:         call commdirdirgpu(nrhs,1,mu,reqendrec,reqendsend)
GPU/source/newinduce_pmegpu.f:         call commrecdirdipgpu(nrhs,1,murec,mu,buffermpimu,
GPU/source/newinduce_pmegpu.f:         call commdirdirgpu(nrhs,2,mu,reqendrec,reqendsend)
GPU/source/newinduce_pmegpu.f:         call commrecdirdipgpu(nrhs,2,murec,mu,buffermpimu,
GPU/source/newinduce_pmegpu.f:         call commrecdirdipgpu(nrhs,0,murec,mu,buffermpimu,
GPU/source/newinduce_pmegpu.f:         call commrecdirdipgpu(nrhs,2,murec,mu,buffermpimu,
GPU/source/newinduce_pmegpu.f:      subroutine inducejac_pmegpu(matvec,nrhs,dodiis,ef,mu,murec)
GPU/source/newinduce_pmegpu.f:      use interfaces ,only: tmatxb_pmegpu
GPU/source/newinduce_pmegpu.f:      use utilgpu
GPU/source/newinduce_pmegpu.f:      procedure(tmatxb_pmegpu):: matvec
GPU/source/newinduce_pmegpu.f:          call tmatxbrecipgpu(mu,murec,nrhs,dipfield,dipfieldbis)
GPU/source/newinduce_pmegpu.f:#ifdef _OPENACC
GPU/source/newinduce_pmegpu.f:            call fatal_device("inducejac_pmegpu")
GPU/source/newinduce_pmegpu.f:      subroutine efld0_recipgpu(cphi,ef)
GPU/source/newinduce_pmegpu.f:      use utilgpu
GPU/source/newinduce_pmegpu.f:     &   write(*,'(3x,a)') 'efld0_recipgpu'
GPU/source/newinduce_pmegpu.f:      call cmp_to_fmp_sitegpu(cmp,fmp)
GPU/source/newinduce_pmegpu.f:#ifdef _OPENACC
GPU/source/newinduce_pmegpu.f:#ifdef _OPENACC
GPU/source/newinduce_pmegpu.f:#ifdef _OPENACC
GPU/source/newinduce_pmegpu.f:#ifdef _OPENACC
GPU/source/newinduce_pmegpu.f:      call fphi_to_cphi_sitegpu(fphirec,cphirec)
GPU/source/newinduce_pmegpu.f:      subroutine tmatxbrecipgpu(mu,murec,nrhs,dipfield,dipfieldbis)
GPU/source/newinduce_pmegpu.f:      use utilgpu   ,only: rec_queue,dir_queue,rec_stream,prmem_request
GPU/source/newinduce_pmegpu.f:#ifdef _OPENACC
GPU/source/newinduce_pmegpu.f:      call timer_enter( timer_tmatxrecipgpu )
GPU/source/newinduce_pmegpu.f:     &   write(*,'(4x,a)') 'tmatxbrecipgpu'
GPU/source/newinduce_pmegpu.f:#ifdef _OPENACC
GPU/source/newinduce_pmegpu.f:#ifdef _OPENACC
GPU/source/newinduce_pmegpu.f:#ifdef _OPENACC
GPU/source/newinduce_pmegpu.f:#ifdef _OPENACC
GPU/source/newinduce_pmegpu.f:#ifdef _OPENACC
GPU/source/newinduce_pmegpu.f:#ifdef _OPENACC
GPU/source/newinduce_pmegpu.f:      call timer_exit( timer_tmatxrecipgpu )
GPU/source/kscalfactor.f:      use utilgpu,only:maxscaling
GPU/source/kscalfactor.f:c#ifdef _OPENACC
GPU/source/kscalfactor.f:c#ifdef _OPENACC
GPU/source/kscalfactor.f:      use utilgpu,only:maxscaling1
GPU/source/kscalfactor.f:c#ifdef _OPENACC
GPU/source/kscalfactor.f:c#ifdef _OPENACC
GPU/source/kscalfactor.f:#ifdef _OPENACC
GPU/source/kscalfactor.f:      use utilgpu,only: maxscaling
GPU/source/kscalfactor.f:#ifdef _OPENACC
GPU/source/kscalfactor.f:#ifdef _OPENACC
GPU/source/kscalfactor.f:#ifdef _OPENACC
GPU/source/kscalfactor.f:#ifdef _OPENACC
GPU/source/kscalfactor.f:      use utilgpu,only: rec_stream
GPU/source/kscalfactor.f:#ifdef _OPENACC
GPU/source/kscalfactor.f:#ifdef _OPENACC
GPU/source/kscalfactor.f:#ifdef _OPENACC
GPU/source/kscalfactor.f:      use utilgpu,only: rec_stream
GPU/source/kscalfactor.f:#ifdef _OPENACC
GPU/source/kscalfactor.f:#ifdef _OPENACC
GPU/source/kscalfactor.f:      use utilgpu,only: rec_queue
GPU/source/kscalfactor.f:#ifdef _OPENACC
GPU/source/kscalfactor.f:#ifdef _OPENACC
GPU/source/kscalfactor.f:#ifdef _OPENACC
GPU/source/kscalfactor.f:      use utilgpu,only: rec_queue
GPU/source/kscalfactor.f:#ifdef _OPENACC
GPU/source/kscalfactor.f:#ifdef _OPENACC
GPU/source/kscalfactor.f:#ifdef _OPENACC
GPU/source/kscalfactor.f:      use utilgpu,only: rec_queue,nSMP
GPU/source/kscalfactor.f:#ifdef _OPENACC
GPU/source/kscalfactor.f:#ifdef _OPENACC
GPU/source/kscalfactor.f:#ifdef _OPENACC
GPU/source/kscalfactor.f:      use utilgpu,only: rec_stream
GPU/source/kscalfactor.f:#ifdef _OPENACC
GPU/source/kscalfactor.f:#ifdef _OPENACC
GPU/source/kscalfactor.f:      use utilgpu,only: rec_queue
GPU/source/kscalfactor.f:#ifdef _OPENACC
GPU/source/kscalfactor.f:#ifdef _OPENACC
GPU/source/kscalfactor.f:#ifdef _OPENACC
GPU/source/kscalfactor.f:      use utilgpu,only: rec_queue
GPU/source/kscalfactor.f:#ifdef _OPENACC
GPU/source/kscalfactor.f:#ifdef _OPENACC
GPU/source/kscalfactor.f:#ifdef _OPENACC
GPU/source/kscalfactor.f:      use utilgpu,only: rec_stream
GPU/source/kscalfactor.f:#ifdef _OPENACC
GPU/source/kscalfactor.f:#ifdef _OPENACC
GPU/source/kscalfactor.f:      use utilgpu,only: rec_stream
GPU/source/kscalfactor.f:#ifdef _OPENACC
GPU/source/kscalfactor.f:      use utilgpu,only: warning,sugest_vdw
GPU/source/estrtor.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/estrtor.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/MOD_subAtoms.f:      use utilgpu   ,only: mem_set,rec_stream
GPU/source/kopdist.f:#ifdef _OPENACC
GPU/source/cu_mpole1.cu:extern const cudaDeviceProp devProp;
GPU/source/cu_mpole1.cu:void cu_emreal1c(MPOLE1_PARAMS1,cudaStream_t st,GROUPS_PARAMS){
GPU/source/cu_mpole1.cu:   cudaError_t ierrSync;
GPU/source/cu_mpole1.cu:      cudaKernelMaxGridSize(gS_emreal,cu_emreal1c_core,BLOCK_DIM,0)  /* This a Macro Function */
GPU/source/cu_mpole1.cu:   if (tinkerdebug) ierrSync = cudaDeviceSynchronize();
GPU/source/cu_mpole1.cu:   else             ierrSync = cudaGetLastError();
GPU/source/cu_mpole1.cu:   if (ierrSync != cudaSuccess)
GPU/source/cu_mpole1.cu:      printf("emreal1c_core C kernel error: %d \n  %s",ierrSync, cudaGetErrorString(ierrSync));
GPU/source/evcorr.f:      use utilgpu   ,only: def_queue
GPU/source/evcorr.f:      subroutine evcorr1gpu (mode,elrc,vlrc)
GPU/source/evcorr.f:      use utilgpu   ,only: def_queue
GPU/source/evcorr.f: 12   format(3x,'evcorr1gpu',3x,'(nvt:',I8,')')
GPU/source/evcorr.f:     &      ,' ~evcorr1gpu~')
GPU/source/baoabpiston.f:      use utilgpu  ,only: rec_queue
GPU/source/baoabpiston.f:#ifdef _OPENACC
GPU/source/baoabpiston.f:      call normalgpu(Rn(1,1),3*nloc)
GPU/source/baoabpiston.f:      call mdrestgpu (istep)
GPU/source/eangtor.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/eangtor.f:      use utilgpu ,only: lam_buff
GPU/source/fft_mpi.f90:#ifdef _OPENACC
GPU/source/fft_mpi.f90:use utilgpu  ,only: rec_q=>rec_queue,dir_q=>dir_queue
GPU/source/fft_mpi.f90:#ifdef _OPENACC
GPU/source/fft_mpi.f90:#ifdef _OPENACC
GPU/source/fft_mpi.f90:  use utilgpu  ,only: rec_q=>rec_queue
GPU/source/fft_mpi.f90:#ifdef _OPENACC
GPU/source/fft_mpi.f90:  use utilgpu  ,only: rec_q=>rec_queue
GPU/source/fft_mpi.f90:#ifdef _OPENACC
GPU/source/fft_mpi.f90:  use interfaces ,only: tmatxb_pmegpu
GPU/source/fft_mpi.f90:  procedure(tmatxb_pmegpu) :: fftTrC_tmat
GPU/source/fft_mpi.f90:  use utilgpu
GPU/source/fft_mpi.f90:#ifdef _OPENACC
GPU/source/fft_mpi.f90:#ifdef _OPENACC
GPU/source/cu_nblist.cu:void filter_lst_sparse (FILTER_PARAMS,cudaStream_t st){
GPU/source/cu_nblist.cu:   cudaError_t ierrSync = cudaGetLastError();
GPU/source/cu_nblist.cu:   //cudaError_t ierrSync = cudaDeviceSynchronize();
GPU/source/cu_nblist.cu:   if (ierrSync != cudaSuccess) {
GPU/source/cu_nblist.cu:             cudaGetErrorString(ierrSync));
GPU/source/ktortor.f:#ifdef _OPENACC
GPU/source/ktortor.f:      use utilgpu
GPU/source/ktortor.f:#ifdef _OPENACC
GPU/source/eimprop3.f:#ifndef _OPENACC
GPU/source/Makefile.pgi:config_file__  ?= config.nvidia.mk
GPU/source/Makefile.pgi:#  librarie's directory (MKL, FFT, 2DECOMP, CUDA, std C++, Thrust Wrapper)
GPU/source/Makefile.pgi:CUDA_HOME     ?= /usr/local/cuda
GPU/source/Makefile.pgi:INC_CUDA       = $(CUDA_HOME)/include
GPU/source/Makefile.pgi:LIB_CUDA       = $(CUDA_HOME)/lib64
GPU/source/Makefile.pgi:LIB_CUDA_DRIVER= $(CUDA_HOME)/lib64/stubs -lcuda
GPU/source/Makefile.pgi:MODSNZ += MOD_utils.o MOD_utilcomm.o MOD_utilvec.o MOD_utilgpu.o MOD_utilbaoab.o
GPU/source/Makefile.pgi:MODSNZ += MOD_precompute_polegpu.o MOD_USampling.o
GPU/source/Makefile.pgi:# Cuda-Fortran object Files
GPU/source/Makefile.pgi:   nblistgpu.o nblist_build.gpu.o\
GPU/source/Makefile.pgi:   alterchg.o alterchg.gpu.o ani.o attach.o basefile.o beeman.o bicubic.o\
GPU/source/Makefile.pgi:   chkpole.o chkpolegpu.o chkring.o chkxyz.o cholesky.o cluster.o\
GPU/source/Makefile.pgi:   damping.o dcdio.o dcflux.o dcflux.gpu.o diis.o domdecstuff.o\
GPU/source/Makefile.pgi:   dcinduce_pme.o dcinduce_pmegpu.o dcinduce_pme2.o dcinduce_pme2gpu.o dcinduce_shortreal.o dcinduce_shortrealgpu.o\
GPU/source/Makefile.pgi:   eangle.o eangle1.o eangle1gpu.o eangle3.o eangle3gpu.o\
GPU/source/Makefile.pgi:   ebond.o ebond1.o ebond1gpu.o ebond3.o ebond3gpu.o\
GPU/source/Makefile.pgi:   echarge.o echarge1.o echarge1gpu.o echarge3.o echarge3gpu.o\
GPU/source/Makefile.pgi:   echgtrn.o echgtrn1.o echgtrn1gpu.o echgtrn3.o echgtrn3gpu.o\
GPU/source/Makefile.pgi:   efld0_direct.o efld0_directgpu.o\
GPU/source/Makefile.pgi:   egeom.o egeom1.o egeom1gpu.o egeom3.o egeom3gpu.o\
GPU/source/Makefile.pgi:   ehal.o ehal1.o ehal1gpu.o ehal3.o ehal3gpu.o\
GPU/source/Makefile.pgi:   eimprop.o eimprop1.o eimprop1gpu.o eimprop3.o\
GPU/source/Makefile.pgi:   eimptor.o eimptor1.o eimptor1gpu.o eimptor3.o\
GPU/source/Makefile.pgi:   elj.o elj1.o elj1gpu.o elj3.o elj3gpu.o\
GPU/source/Makefile.pgi:   eliaison1gpu.o\
GPU/source/Makefile.pgi:   empole.o empole1.o empole1gpu.o empole1_group.o empole3.o empole3gpu.o empole3_group.o\
GPU/source/Makefile.pgi:   eopbend.o eopbend1.o eopbend1gpu.o eopbend3.o eopbend3gpu.o\
GPU/source/Makefile.pgi:   epitors.o epitors1.o epitors1gpu.o epitors3.o\
GPU/source/Makefile.pgi:   epolar.o epolar1.o epolar1gpu.o epolar1_group.o epolar3.o epolar3gpu.o epolar3_group.o\
GPU/source/Makefile.pgi:   estrbnd.o estrbnd1.o estrbnd1gpu.o estrbnd3.o\
GPU/source/Makefile.pgi:   estrtor.o estrtor1.o estrtor1gpu.o estrtor3.o\
GPU/source/Makefile.pgi:   etors.o etors1.o etors1gpu.o etors3.o\
GPU/source/Makefile.pgi:   etortor.o etortor1.o etortor1gpu.o etortor3.o\
GPU/source/Makefile.pgi:   eurey.o eurey1.o eurey1gpu.o eurey3.o  eurey3gpu.o\
GPU/source/Makefile.pgi:   mdsave.o mdsavebeads.o mdstat.o mdstatpi.o mdstate.gpu.o mechanic.o\
GPU/source/Makefile.pgi:   molecule.o mpistuff.o mpiUtils.gpu.o mutate.o
GPU/source/Makefile.pgi:# Cuda-C object Files
GPU/source/Makefile.pgi:OBJSNZ += newinduce_pme.o newinduce_pmegpu.o newinduce_pme2.o newinduce_pme2gpu.o newinduce_group.o newinduce_shortreal.o newinduce_shortrealgpu.o
GPU/source/Makefile.pgi:OBJSNZ += pmestuff.o pmestuffgpu.o
GPU/source/Makefile.pgi:OBJSNZ += rotpole.o rotpolegpu.o
GPU/source/Makefile.pgi:OBJSNZ += tmatxb_pme.o tmatxb_pmegpu.o
GPU/source/Makefile.pgi:OBJSNZ += torque.o torquegpu.o
GPU/source/Makefile.pgi:ifeq ($(arch),$(filter $(arch),device gpu))
GPU/source/Makefile.pgi:.PHONY: dynamic.gpu
GPU/source/Makefile.pgi:dynamic.gpu:
GPU/source/Makefile.pgi:	@$(MAKE) prog_suffix=.gpu dynamic
GPU/source/Makefile.pgi:build_mod: FFLAGS=$(GPUFLAGS)
GPU/source/Makefile.pgi:	$(RunF77) $(GPUFLAGS) -c $<
GPU/source/Makefile.pgi:	$(RunF77) $(GPUFLAGS) -c $<
GPU/source/Makefile.pgi:	$(RunF77) $(GPUFLAGS) -c $<
GPU/source/Makefile.pgi:	$(RunF77) $(GPUFLAGS) -c $<
GPU/source/Makefile.pgi:	$(RunF77) $(GPUFLAGS) -c $<
GPU/source/Makefile.pgi:	$(RunF77) $(GPUFLAGS) -c $<
GPU/source/Makefile.pgi:	$(RunF77) $(GPUFLAGS) -c $<
GPU/source/Makefile.pgi:	$(RunF77) $(GPUFLAGS) -c $<
GPU/source/Makefile.pgi:	$(RunF77) $(GPUFLAGS) -c $<
GPU/source/Makefile.pgi:%gpu.o: %gpu.f
GPU/source/Makefile.pgi:	$(RunF77) $(GPUFLAGS) -c $*gpu.f
GPU/source/Makefile.pgi:	$(RunF77) $(GPUFLAGS) -c e$*.f
GPU/source/Makefile.pgi:	$(RunF77) $(GPUFLAGS) -c k$*.f
GPU/source/Makefile.pgi:	$(RunF77) $(GPUFLAGS) -I$(INC_FFTDECOMP) -c $*.f90
GPU/source/Makefile.pgi:	@echo "CUDA fortan flags  " $(cuda_flags_f_)
GPU/source/Makefile.pgi:	@echo "CUDA config        " $(cuda_flags_c_)
GPU/source/Makefile.pgi:	@echo "CUDA libraries     " $(dev_ldlibs)
GPU/source/Makefile.pgi:	@echo "GPUFLAGS   " $(GPUFLAGS)
GPU/source/Makefile.pgi:ifeq ($(arch), $(filter $(arch),device gpu))
GPU/source/Makefile.pgi:	@cd ../wrappers; $(MAKE) "CUDACC=$(RunCUCC)" "FCFLAGS=$(CUFFLAGS)" "CUFLAGS=$(CUCFLAGS)" "AT=$(AT)"
GPU/source/Makefile.pgi:	@cd ../wrappers; $(MAKE) clean; $(MAKE) "CUDACC=$(RunCUCC)" "FCFLAGS=$(CUFFLAGS)" "CUFLAGS=$(CUCFLAGS)" "AT=$(AT)"
GPU/source/pair_tmatxb.f.inc:#include "tinker_cudart.h"
GPU/source/pair_tmatxb.f.inc:           use utilgpu ,only: real3,real6
GPU/source/pair_tmatxb.f.inc:              bn0     = f_erfc(ralpha)   ! Attention to Macro f_erfc in single prec with hastings method (check tinker_cudart.h)
GPU/source/pair_tmatxb.f.inc:        use utilgpu ,only: real3,real6
GPU/source/pair_tmatxb.f.inc:           bn0      = f_erfc(ralpha)   ! Attention to Macro f_erfc in single prec with hastings method (check tinker_cudart.h)
GPU/source/pair_tmatxb.f.inc:      use utilgpu ,only: real3,real6
GPU/source/pair_tmatxb.f.inc:      use utilgpu ,only: real3,real6
GPU/source/pair_tmatxb.f.inc:      use utilgpu ,only: real3,real6
GPU/source/pair_tmatxb.f.inc:           use utilgpu ,only: real3,real6
GPU/source/pair_tmatxb.f.inc:              bn0     = f_erfc(ralpha)   ! Attention to Macro f_erfc in single prec with hastings method (check tinker_cudart.h)
GPU/source/MOD_atmlst.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/MOD_atmlst.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/torsions.f:      use utilgpu
GPU/source/torsions.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/torsions.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/torsions.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/torsions.f:#ifdef _OPENACC
GPU/source/torsions.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/torsions.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/torsions.f:      ! self association (OpenAcc visibility)
GPU/source/echargecu.f:#include "tinker_cudart.h"
GPU/source/echargecu.f:      use utilgpu ,only: real3,real6,mdyn3_r,rpole_elt
GPU/source/echarge3gpu.f:      module echarge3gpu_inl
GPU/source/echarge3gpu.f:      subroutine echarge3gpu
GPU/source/echarge3gpu.f:        call elambdacharge3cgpu
GPU/source/echarge3gpu.f:        call echarge3cgpu
GPU/source/echarge3gpu.f:c     ##  subroutine echarge3cgpu  --  Ewald charge analysis via list  ##
GPU/source/echarge3gpu.f:c     "echarge3cgpu" calculates the charge-charge interaction energy
GPU/source/echarge3gpu.f:      subroutine echarge3cgpu
GPU/source/echarge3gpu.f:      use echarge3gpu_inl
GPU/source/echarge3gpu.f:      use utilgpu
GPU/source/echarge3gpu.f:      if(deb_Path) write (*,*) 'echarge3cgpu'
GPU/source/echarge3gpu.f:c     update the number of gangs required for gpu
GPU/source/echarge3gpu.f:         call ecrecipgpu
GPU/source/echarge3gpu.f:      !print*, 'echarge3gpu',ec, ec_r, ecrec
GPU/source/echarge3gpu.f:c     ##  subroutine elambdacharge3cgpu  --  Ewald charge analysis via list  ##
GPU/source/echarge3gpu.f:c     "echarge3cgpu" calculates the charge-charge interaction energy
GPU/source/echarge3gpu.f:      subroutine elambdacharge3cgpu
GPU/source/echarge3gpu.f:      use echarge3gpu_inl
GPU/source/echarge3gpu.f:      use utilgpu
GPU/source/echarge3gpu.f:#ifdef _OPENACC
GPU/source/echarge3gpu.f:      if(rank.eq.0.and.tinkerdebug) write (*,*) 'echarge3cgpu'
GPU/source/echarge3gpu.f:c     update the number of gangs required for gpu
GPU/source/echarge3gpu.f:            call ecrecipgpu
GPU/source/echarge3gpu.f:            call ecrecipgpu
GPU/source/echarge3gpu.f:      !print*,'echarge3gpu',nec_,nec,nionloc
GPU/source/echarge3gpu.f:      subroutine ecreal3dgpu
GPU/source/echarge3gpu.f:      use echarge3gpu_inl
GPU/source/echarge3gpu.f:      use utilgpu
GPU/source/echarge3gpu.f:      if (deb_Path) write(*,'(2x,a)') 'ecreal3dgpu'
GPU/source/echarge3gpu.f:#ifdef _CUDA
GPU/source/echarge3gpu.f:      use utilgpu ,only: def_queue,dir_queue,dir_stream,def_stream
GPU/source/echarge3gpu.f:         call cudaMaxGridSize("ecreal3_kcu",gS)
GPU/source/echarge3gpu.f:c     GPU version of ecrecip
GPU/source/echarge3gpu.f:      subroutine ecrecipgpu
GPU/source/echarge3gpu.f:      use utilgpu   ,only: rec_queue
GPU/source/echarge3gpu.f:     &   write(*,'(2x,a)') 'ecrecipgpu'
GPU/source/echarge3gpu.f:      call bspline_fill_sitegpu(1)
GPU/source/echarge3gpu.f:#ifdef _OPENACC
GPU/source/beeman.f:      use utilgpu
GPU/source/beeman.f:      call mdrestgpu (istep)
GPU/source/MOD_timestat.f:     &           timer_tmatxb_pmevec,timer_tmatxb_pmegpu,
GPU/source/MOD_timestat.f:     &           timer_tmatxrecipgpu,
GPU/source/MOD_timestat.f:        call timer_init( "emreal1cgpu"     , timer_emreal)
GPU/source/MOD_timestat.f:        call timer_init( "emrecip1gpu"     , timer_emrecip)
GPU/source/MOD_timestat.f:        call timer_init( "tmatxb_pmegpu"   , timer_tmatxb_pmegpu)
GPU/source/MOD_timestat.f:        call timer_init( "tmatxrecipgpu"   , timer_tmatxrecipgpu)
GPU/source/MOD_timestat.f:        call timer_init( "eprecip1gpu"     , timer_eprecip)
GPU/source/MOD_timestat.f:        call timer_init( "torquegpu"       , timer_torque)
GPU/source/elj3gpu.f:c     ## subroutine elj3gpu -- Lennard-Jones vdw energy & analysis  ##
GPU/source/elj3gpu.f:      module elj3gpu_inl
GPU/source/elj3gpu.f:      subroutine elj3gpu
GPU/source/elj3gpu.f:      use elj3gpu_inl
GPU/source/elj3gpu.f:      use utilgpu
GPU/source/elj3gpu.f:#ifdef _CUDA
GPU/source/elj3gpu.f:      use cudafor   ,only: dim3
GPU/source/elj3gpu.f:      use elj1gpu_inl,only: enr2en,mdr2md
GPU/source/elj3gpu.f:      use utilgpu   ,only: def_queue,dir_queue,rec_queue,dir_stream
GPU/source/elj3gpu.f:c     Call Lennard-Jones kernel in CUDA using C2 nblist
GPU/source/angles.f:#ifdef _OPENACC
GPU/source/angles.f:      use utilgpu
GPU/source/angles.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/angles.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/angles.f:#ifdef _OPENACC
GPU/source/angles.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/angles.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/angles.f:      ! self association (OpenAcc visibility)
GPU/source/angles.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/angles.f:      ! self association (OpenAcc visibility)
GPU/source/midpointimage.f.inc:#include "tinker_cudart.h"
GPU/source/midpointimage.f.inc:c     Much efficient on GPU
GPU/source/ehal3gpu.f:      module ehal3gpu_inl
GPU/source/ehal3gpu.f:      subroutine ehal3gpu
GPU/source/ehal3gpu.f:c     ##  subroutine ehal3cgpu  --  buffered 14-7 analysis via list  ##
GPU/source/ehal3gpu.f:      use ehal3gpu_inl
GPU/source/ehal3gpu.f:      use utilgpu   ,only: def_queue,dir_queue,rec_queue
GPU/source/ehal3gpu.f:#ifdef _OPENACC
GPU/source/ehal3gpu.f:#ifdef _OPENACC
GPU/source/ehal3gpu.f:#ifdef _CUDA
GPU/source/ehal3gpu.f:      use utilgpu   ,only: def_queue,dir_queue,rec_queue,dir_stream
GPU/source/ehal3gpu.f:     &              ,CUDAOCCUPANCYMAXACTIVEBLOCKSPERMULTIPROCESSOR
GPU/source/ehal3gpu.f:      i       = CUDAOCCUPANCYMAXACTIVEBLOCKSPERMULTIPROCESSOR
GPU/source/ehal3gpu.f:      if (i.ne.0) print*,'Issue detected with ehal1:cudaoccupancy',i
GPU/source/ehal3gpu.f:c     Call Vdw kernel in CUDA using C2 nblist
GPU/source/pmestuffcu.f:#include "tinker_cudart.h"
GPU/source/pmestuffcu.f:        use utilgpu,only: RED_BUFF_SIZE
GPU/source/pmestuffcu.f:        logical function pme_check_cuda_bsorder(tinker_bsorder)
GPU/source/pmestuffcu.f:c       CUDA Fortran written kernel
GPU/source/pmestuffcu.f:c       CUDA Fortran written kernel
GPU/source/pmestuffcu.f:c       CUDA Fortran written kernel
GPU/source/pmestuffcu.f:c     "fphi_uind_sitegpu2" extracts the induced dipole potential at the i-th site from
GPU/source/epolar1cu.f:c     and derivatives with respect to Cartesian coordinates in a CUDA Fortran kernel
GPU/source/epolar1cu.f:#ifdef _CUDA
GPU/source/epolar1cu.f:#include "tinker_cudart.h"
GPU/source/epolar1cu.f:        use utilgpu ,only: BLOCK_SIZE,RED_BUFF_SIZE
GPU/source/prmkey.f:      use utilgpu
GPU/source/prmkey.f:      !GPU Tools
GPU/source/prmkey.f:      else if (keyword(1:10) .eq. 'GPU-GANGS ')       then
GPU/source/prmkey.f:         read (string,*,err=10,end=10)   gpu_gangs
GPU/source/prmkey.f:!$acc update device(gpu_gangs)
GPU/source/prmkey.f:      else if (keyword(1:12) .eq. 'GPU-WORKERS ')     then
GPU/source/prmkey.f:         read (string,*,err=10,end=10)   gpu_workers
GPU/source/prmkey.f:!$acc update device(gpu_workers)
GPU/source/prmkey.f:      else if (keyword(1:11) .eq. 'GPU-VECTOR ')      then
GPU/source/prmkey.f:         read (string,*,err=10,end=10)   gpu_vector
GPU/source/prmkey.f:!$acc update device(gpu_vector)
GPU/source/prmkey.f:#ifdef _OPENACC
GPU/source/prmkey.f:#ifdef _OPENACC
GPU/source/cu_CholeskySolver.cu://#   if (CUDART_VERSION>10010)
GPU/source/cu_CholeskySolver.cu://#   if (CUDART_VERSION>10010)
GPU/source/cu_CholeskySolver.cu://#   if (CUDART_VERSION>10010)
GPU/source/cu_CholeskySolver.cu:void initcuSolverHandle(cudaStream_t stream){
GPU/source/cu_CholeskySolver.cu:   gpuErrchkSolver( cusolverDnCreate(&cuCholHandle) )
GPU/source/cu_CholeskySolver.cu:   gpuErrchkSolver( cusolverDnSetStream(cuCholHandle, stream) )
GPU/source/cu_CholeskySolver.cu:   gpuErrchk( cudaMalloc (&d_info, sizeof(int)) )
GPU/source/cu_CholeskySolver.cu:   Reallocation procedure based on MOD_utilgpu.f reallocate_acc
GPU/source/cu_CholeskySolver.cu:void device_reallocate(void** array, const size_t bytesSize, size_t& PrevSize, cudaStream_t stream){
GPU/source/cu_CholeskySolver.cu:      gpuErrchk( cudaMalloc(array,bytesSize) )
GPU/source/cu_CholeskySolver.cu:         gpuErrchk( cudaStreamSynchronize( stream ) )
GPU/source/cu_CholeskySolver.cu:         gpuErrchk( cudaFree( *array ) )
GPU/source/cu_CholeskySolver.cu:         gpuErrchk( cudaMalloc(array,bytesSize) )
GPU/source/cu_CholeskySolver.cu:void cuPOTRF_Wrapper(const int n, real* A, const int lda, cudaStream_t stream){
GPU/source/cu_CholeskySolver.cu:   gpuErrchk( cuPOTRF_buffSize(cuCholHandle, uplo, n, A, lda, &Lwork) )
GPU/source/cu_CholeskySolver.cu:      gpuErrchk( cudaGetLastError() )
GPU/source/cu_CholeskySolver.cu:void cuPOTRFm_Wrapper(const int n, realm* A, const int lda, cudaStream_t stream){
GPU/source/cu_CholeskySolver.cu:   gpuErrchk( cuPOTRFm_buffSize(cuCholHandle, uplo, n, A, lda, &Lwork) )
GPU/source/cu_CholeskySolver.cu:      gpuErrchk( cudaGetLastError() )
GPU/source/cu_CholeskySolver.cu:void cuPOTRI_Wrapper(const int n, real* A, const int lda, cudaStream_t stream){
GPU/source/cu_CholeskySolver.cu:   gpuErrchk( cuPOTRI_buffSize(cuCholHandle, uplo, n, A, lda, &Lwork) )
GPU/source/cu_CholeskySolver.cu:      gpuErrchk( cudaGetLastError() )
GPU/source/cu_CholeskySolver.cu:void cuPOTRIm_Wrapper(const int n, realm* A, const int lda, cudaStream_t stream){
GPU/source/cu_CholeskySolver.cu:   gpuErrchk( cuPOTRIm_buffSize(cuCholHandle, uplo, n, A, lda, &Lwork) )
GPU/source/cu_CholeskySolver.cu:      gpuErrchk( cudaGetLastError() )
GPU/source/cu_CholeskySolver.cu:void cuPOTRS_Wrapper(const int n, real* A, const int lda, real* B, const int ldb, cudaStream_t stream){
GPU/source/cu_CholeskySolver.cu:      gpuErrchk( cudaGetLastError() )
GPU/source/cu_CholeskySolver.cu:void cuGESV_Wrapper(const int n, const int nrhs, real* A, const int lda, int* Ipiv, real* B, const int ldb, cudaStream_t stream){
GPU/source/cu_CholeskySolver.cu://#   if (CUDART_VERSION>10010)
GPU/source/cu_CholeskySolver.cu:   gpuErrchkSolver( cuGESV_buffSize(cuCholHandle, n, nrhs, A, lda, Ipiv, d_inB, ldb, B, ldb, d_workSpace, &lwork_bytes) )
GPU/source/cu_CholeskySolver.cu:   gpuErrchk( cudaMemcpyAsync( d_inB,B,Bsize,cudaMemcpyDeviceToDevice,stream ) )
GPU/source/cu_CholeskySolver.cu:   gpuErrchkSolver( cuGESV(cuCholHandle, n, nrhs, A, lda, Ipiv, d_inB, ldb, B, ldb, d_workSpace, lwork_bytes, &iter, d_info) )
GPU/source/cu_CholeskySolver.cu:   gpuErrchkSolver( cuGETRF_buffsize(cuCholHandle, n, n, A, lda, &Lwork) )
GPU/source/cu_CholeskySolver.cu:   gpuErrchkSolver( cuGETRF(cuCholHandle, n, n, A, lda, d_workSpace, Ipiv, d_info) )
GPU/source/cu_CholeskySolver.cu:   gpuErrchkSolver( cuGETRS(cuCholHandle, CUBLAS_OP_N, n, nrhs, A, lda, Ipiv, B, ldb, d_info) )
GPU/source/cu_CholeskySolver.cu:      gpuErrchk( cudaGetLastError() )
GPU/source/cu_CholeskySolver.cu:void cuGESVm_Wrapper(const int n, const int nrhs, realm* A, const int lda, int* Ipiv, realm* B, const int ldb, cudaStream_t stream){
GPU/source/cu_CholeskySolver.cu://#   if (CUDART_VERSION>10010)
GPU/source/cu_CholeskySolver.cu:   gpuErrchkSolver( cuGESVm_buffSize(cuCholHandle, n, nrhs, A, lda, Ipiv, dm_inB, ldb, B, ldb, dm_workSpace, &lwork_bytes) )
GPU/source/cu_CholeskySolver.cu:   gpuErrchk( cudaMemcpyAsync( dm_inB,B,Bsize,cudaMemcpyDeviceToDevice,stream ) )
GPU/source/cu_CholeskySolver.cu:   gpuErrchkSolver( cuGESVm(cuCholHandle, n, nrhs, A, lda, Ipiv, dm_inB, ldb, B, ldb, dm_workSpace, lwork_bytes, &iter, d_info) )
GPU/source/cu_CholeskySolver.cu:   gpuErrchkSolver( cuGETRFm_buffsize(cuCholHandle, n, n, A, lda, &Lwork) )
GPU/source/cu_CholeskySolver.cu:   gpuErrchkSolver( cuGETRFm(cuCholHandle, n, n, A, lda, dm_workSpace, Ipiv, d_info) )
GPU/source/cu_CholeskySolver.cu:   gpuErrchkSolver( cuGETRSm(cuCholHandle, CUBLAS_OP_N, n, nrhs, A, lda, Ipiv, B, ldb, d_info) )
GPU/source/cu_CholeskySolver.cu:      gpuErrchk( cudaGetLastError() )
GPU/source/cu_CholeskySolver.cu:   gpuErrchkSolver( cusolverDnDestroy(cuCholHandle) )
GPU/source/ker_tortor.inc.f:#ifdef  _CUDA_ONLY
GPU/source/ker_tortor.inc.f:#ifdef _CUDA_ONLY
GPU/source/ker_tortor.inc.f:#ifdef _CUDA_ONLY
GPU/source/ker_tortor.inc.f:#ifdef _CUDA_ONLY
GPU/source/ker_tortor.inc.f:#ifdef _CUDA_ONLY
GPU/source/atomicOp.inc.f:      !  CUDA Section
GPU/source/atomicOp.inc.f:      !   OpenACC Section
GPU/source/epolar_cpencu.tpl.f:c     "epolar_cpencu" : CUDA Template for calculation of the real space portion of the Ewald
GPU/source/utils.h:#include <cuda_runtime.h>
GPU/source/utils.h:inline static void gpuAssert(cudaError_t code, const char* file, int line, int abort=1, const cudaError_t success=cudaSuccess){
GPU/source/utils.h:      fprintf(stderr,"GPUAssert: %s %s %d\n", cudaGetErrorString(code), file, line);
GPU/source/utils.h:inline static void gpuAssert(cusolverStatus_t code, const char* file, int line, int abort=1){
GPU/source/utils.h:      fprintf(stderr,"GPUAssert: cuSolver Error : %d %s %d\n", code, file, line);
GPU/source/utils.h:#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__, cudaSuccess); }
GPU/source/utils.h:#define gpuErrchkSolver(ans) { gpuAssert((ans), __FILE__, __LINE__); }
GPU/source/utils.h:#define cudaKernelMaxGridSize(gS,kernel,bS,dynSMem)                                               \
GPU/source/utils.h:   gpuErrchk(cudaOccupancyMaxActiveBlocksPerMultiprocessor(&gS,kernel,bS,dynSMem))                \
GPU/source/ani.f:      use utilgpu   ,only: devicenum
GPU/source/pair_efld.inc.f:#include "tinker_cudart.h"
GPU/source/MOD_utilbaoabpi.f:      use utilgpu
GPU/source/MOD_utilbaoabpi.f:      use utilgpu
GPU/source/MOD_utilbaoabpi.f:              !if(save_mpole .and. full_dipole) call rotpolegpu
GPU/source/MOD_utilbaoabpi.f:      use utilgpu
GPU/source/MOD_utilbaoabpi.f:      use utilgpu
GPU/source/epitors1gpu.f:      module epitors1gpu_inl
GPU/source/epitors1gpu.f:      subroutine epitors1gpu
GPU/source/epitors1gpu.f:      use epitors1gpu_inl
GPU/source/epitors1gpu.f:      if (deb_Path) write(*,*) 'epitors1gpu'
GPU/source/echgtrn3gpu.f:      subroutine echgtrn3gpu
GPU/source/echgtrn3gpu.f:#ifdef _OPENACC
GPU/source/echgtrn3gpu.f:c     ##  subroutine echgtrn1cgpu  --  charge transfer derivs via list  ##
GPU/source/echgtrn3gpu.f:c     "echgtrn1cgpu" calculates the charge transfer energy and first
GPU/source/echgtrn3gpu.f:#ifdef _OPENACC
GPU/source/echgtrn3gpu.f:      use utilgpu
GPU/source/etortor1gpu.f:#if defined(_CUDA) && !defined(_OPENACC)
GPU/source/etortor1gpu.f:#define _CUDA_ONLY
GPU/source/etortor1gpu.f:      module etortor1gpu_inl
GPU/source/etortor1gpu.f:      subroutine etortor1gpu_(i12,n12,typAtm,atomic,dett)
GPU/source/etortor1gpu.f:      if (deb_Path) write(*,*) 'etortor1gpu'
GPU/source/etortor1gpu.f:      subroutine etortor1gpu
GPU/source/etortor1gpu.f:      use etortor1gpu_inl
GPU/source/etortor1gpu.f:      call etortor1gpu_(i12,n12,type,atomic,dett)
GPU/source/echgtrn1gpu.f:      subroutine echgtrn1gpu
GPU/source/echgtrn1gpu.f:#ifdef _OPENACC
GPU/source/echgtrn1gpu.f:c     ##  subroutine echgtrn1cgpu  --  charge transfer derivs via list  ##
GPU/source/echgtrn1gpu.f:c     "echgtrn1cgpu" calculates the charge transfer energy and first
GPU/source/echgtrn1gpu.f:#ifdef _OPENACC
GPU/source/echgtrn1gpu.f:      use utilgpu
GPU/source/kpolar.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/kpolar.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/kpolar.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/kpolar.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/kpolar.f:#ifdef _OPENACC
GPU/source/kpolar.f:      use utilgpu
GPU/source/kpolar.f:#ifndef _OPENACC
GPU/source/kpolar.f:      use utilgpu
GPU/source/kpolar.f:! -- to be used in ~rotpolegpu~ which is 2 calls away
GPU/source/kpolar.f:      use utilgpu ,only : rec_queue,dir_queue
GPU/source/kpolar.f:#ifdef _OPENACC
GPU/source/mpistuff.f:#ifdef _OPENACC
GPU/source/mpistuff.f:      call orderbuffer_gpu(.true.)
GPU/source/mpistuff.f:        call orderbufferrec_gpu
GPU/source/mpistuff.f:20      call orderbuffer_gpu(.false.)
GPU/source/mpistuff.f:      use utilgpu
GPU/source/mpistuff.f:      use utilgpu,only: rec_queue
GPU/source/mpistuff.f:      use utilgpu  ,only: mem_set,rec_stream
GPU/source/mpistuff.f:c     Orderbuffer with openacc directives
GPU/source/mpistuff.f:      subroutine orderbuffer_gpu(init)
GPU/source/mpistuff.f:      use utilgpu, only : openacc_abort
GPU/source/mpistuff.f:      if (deb_Path) write(*,*) '   >> orderbuffer_gpu',init
GPU/source/mpistuff.f:      if (deb_Path) write(*,*) '   << orderbuffer_gpu'
GPU/source/mpistuff.f:      subroutine orderbufferrec_gpu
GPU/source/mpistuff.f:      if (deb_Path) write(*,*) '   >> orderbufferrec_gpu'
GPU/source/mpistuff.f: 48   format('orderbufferrec_gpu::Error',/
GPU/source/mpistuff.f:      if (deb_Path) write(*,*) '   << orderbufferrec_gpu'
GPU/source/mpistuff.f:      subroutine commdirdirgpu(nrhs,rule,mu,reqrec1,reqsend1)
GPU/source/mpistuff.f:      use utilgpu ,only: rec_queue
GPU/source/mpistuff.f: 1000 format(' illegal rule in commdirdirgpu.')
GPU/source/mpistuff.f:         if (deb_Path) write(*,41) 'commdirdirgpu       '
GPU/source/mpistuff.f:         if (deb_Path) write(*,42) 'commdirdirgpu       '
GPU/source/mpistuff.f:         if (deb_Path) write(*,43) 'commdirdirgpu       '
GPU/source/mpistuff.f:      use utilgpu ,only:def_queue
GPU/source/mpistuff.f:      subroutine commrecdirfieldsgpu(rule,efrec,ef,buffermpi1,buffermpi2
GPU/source/mpistuff.f: 1000 format(' illegal rule in commrecdirfieldsgpu.')
GPU/source/mpistuff.f:      subroutine commrecdirsolvgpu(nrhs,rule,dipfieldbis,dipfield,
GPU/source/mpistuff.f: 1000 format(' illegal rule in commrecdirsolvgpu.')
GPU/source/mpistuff.f:        if (deb_Path) write(*,42) 'commrecdirsolvgpu   '
GPU/source/mpistuff.f:        if (deb_Path) write(*,43) 'commrecdirsolvgpu   '
GPU/source/mpistuff.f:      subroutine commrecdirdipgpu(nrhs,rule,diprec,dip,buffermpimu1,
GPU/source/mpistuff.f: 1000 format(' illegal rule in commrecdirdipgpu.')
GPU/source/mpistuff.f:        if (deb_Path) write(*,42) 'commrecdirdipgpu    '
GPU/source/mpistuff.f:        if (deb_Path) write(*,43) 'commrecdirdipgpu    '
GPU/source/mpistuff.f:      subroutine commfieldgpu(nrhs,ef)
GPU/source/mpistuff.f:      use utilgpu ,only:dir_queue,prmem_request
GPU/source/mpistuff.f:      if (deb_Path) write(*,41) 'commfieldgpu        '
GPU/source/mpistuff.f:      if (deb_Path) write(*,43) 'commfieldgpu        '
GPU/source/mpistuff.f:      use utilgpu  ,only: prmem_request,def_queue
GPU/source/mpistuff.f:      use utilgpu ,only: rec_queue
GPU/source/mpistuff.f:            call aaddgpuAsync(MSG_sz
GPU/source/mpistuff.f:      use utilgpu ,only: rec_queue
GPU/source/kbond.f:#ifdef _OPENACC
GPU/source/kbond.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/baoab_util.f:#ifdef _OPENACC
GPU/source/baoab_util.f:     &               ,normalgpu
GPU/source/baoab_util.f:#ifdef _OPENACC
GPU/source/baoab_util.f:          call normalgpu(Rn,3*nloc)
GPU/source/baoab_util.f:#ifdef _OPENACC
GPU/source/baoab_util.f:         call normalgpu(Rn,3*nloc)
GPU/source/empole_cpencu.f:#ifdef _CUDA
GPU/source/empole_cpencu.f:#include "tinker_cudart.h"
GPU/source/empole_cpencu.f:      use utilgpu  ,only: BLOCK_SIZE,RED_BUFF_SIZE,WARP_SIZE
GPU/source/kmlpot.f:#ifdef _OPENACC
GPU/source/kmlpot.f:      use utilgpu  ,only: rec_stream
GPU/source/kmlpot.f:#ifdef _OPENACC
GPU/source/etors1gpu.f:      module etors1gpu_inl
GPU/source/etors1gpu.f:      subroutine etors1gpu
GPU/source/etors1gpu.f:      call etors1agpu
GPU/source/etors1gpu.f:      subroutine etors1agpu_(tors1,tors2,tors3,tors4,tors5,tors6,det)
GPU/source/etors1gpu.f:      use etors1gpu_inl
GPU/source/etors1gpu.f:      use utilgpu   ,only: mem_move,rec_stream
GPU/source/etors1gpu.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/etors1gpu.f:      if (deb_Path) write(*,*) 'etors1gpu'
GPU/source/etors1gpu.f:#ifndef USE_NVSHMEM_CUDA
GPU/source/etors1gpu.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/etors1gpu.f:      subroutine etors1agpu
GPU/source/etors1gpu.f:      subroutine etors1agpu_(tors1,tors2,tors3,tors4,tors5,tors6,det)
GPU/source/etors1gpu.f:      call etors1agpu_(tors1,tors2,tors3,tors4,tors5,tors6,det)
GPU/source/MOD_pme.f:#ifdef _OPENACC
GPU/source/kpitors.f:      use utilgpu
GPU/source/kpitors.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/kpitors.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/kpitors.f:#ifdef _OPENACC
GPU/source/kpitors.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/MOD_qtb.f:#ifdef _OPENACC
GPU/source/MOD_qtb.f:#ifdef _OPENACC
GPU/source/MOD_qtb.f:          call normalgpuR4(qtbdata(iglob)%noise(1,1,1),3*2*nseg*nbeads)
GPU/source/MOD_qtb.f:#ifdef _OPENACC
GPU/source/MOD_qtb.f:      use openacc
GPU/source/MOD_qtb.f:      use utilgpu, only: rec_queue
GPU/source/MOD_qtb.f:c#ifdef _OPENACC
GPU/source/MOD_qtb.f:#ifdef _OPENACC
GPU/source/MOD_qtb.f:      use utilgpu  ,only: rec_queue
GPU/source/MOD_qtb.f:#ifdef _OPENACC
GPU/source/MOD_qtb.f:        call normalgpu(r,3*nseg*nbatch_*nbeads
GPU/source/MOD_qtb.f:#ifdef _OPENACC
GPU/source/MOD_qtb.f:#ifdef _OPENACC
GPU/source/MOD_qtb.f:#ifdef _OPENACC
GPU/source/MOD_qtb.f:!#ifdef _OPENACC
GPU/source/MOD_qtb.f:#ifdef _OPENACC
GPU/source/MOD_qtb.f:#ifndef _OPENACC
GPU/source/MOD_qtb.f:#ifdef _OPENACC
GPU/source/MOD_qtb.f:#ifdef _OPENACC
GPU/source/MOD_qtb.f:        use utilgpu, only: rec_queue
GPU/source/MOD_qtb.f:        use openacc
GPU/source/MOD_qtb.f:#ifdef _OPENACC
GPU/source/MOD_qtb.f:     &     ,acc_get_cuda_stream(rec_queue))  
GPU/source/MOD_memory.f:c     s_driver  CUDA Driver memory size
GPU/source/MOD_memory.f:#ifdef _CUDA
GPU/source/MOD_memory.f:      use cudafor
GPU/source/MOD_memory.f:        enumerator :: memcuda
GPU/source/MOD_memory.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/MOD_memory.f:#if defined(_CUDA) && defined(USE_NVSHMEM)
GPU/source/MOD_memory.f:#if defined(_CUDA) && defined(USE_NVSHMEM)
GPU/source/MOD_memory.f:#if defined(_CUDA) && defined(USE_NVSHMEM)
GPU/source/MOD_memory.f:#if defined(_CUDA) && defined(USE_NVSHMEM)
GPU/source/MOD_memory.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/MOD_memory.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/MOD_memory.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/MOD_memory.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/MOD_memory.f:#if defined(_CUDA) && defined(USE_NVSHMEM)
GPU/source/MOD_memory.f:#if defined(_CUDA) && defined(USE_NVSHMEM)
GPU/source/MOD_memory.f:#if defined(_CUDA) && defined(USE_NVSHMEM)
GPU/source/MOD_memory.f:#if defined(_CUDA) && defined(USE_NVSHMEM)
GPU/source/MOD_memory.f:#if defined(_CUDA) && defined(USE_NVSHMEM)
GPU/source/MOD_memory.f:#if defined(_CUDA) && defined(USE_NVSHMEM)
GPU/source/MOD_memory.f:#if defined(_CUDA) && defined(USE_NVSHMEM)
GPU/source/MOD_memory.f:#if defined(_CUDA) && defined(USE_NVSHMEM)
GPU/source/MOD_memory.f:#if defined(_CUDA) && defined(USE_NVSHMEM)
GPU/source/MOD_memory.f:#if defined(_CUDA) && defined(USE_NVSHMEM)
GPU/source/MOD_memory.f:#if defined(_CUDA) && defined(USE_NVSHMEM)
GPU/source/MOD_memory.f:#if defined(_CUDA) && defined(USE_NVSHMEM)
GPU/source/MOD_memory.f:#if defined(_CUDA) && defined(USE_NVSHMEM)
GPU/source/MOD_memory.f:#if defined(_CUDA) && defined(USE_NVSHMEM)
GPU/source/MOD_memory.f:#ifdef _CUDA
GPU/source/MOD_memory.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/MOD_memory.f:#ifdef _OPENACC
GPU/source/MOD_memory.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/domdecstuff.f:#ifdef _OPENACC
GPU/source/domdecstuff.f:      use utilcu,only: copy_data_to_cuda_env
GPU/source/domdecstuff.f:#ifdef _OPENACC
GPU/source/domdecstuff.f:      call copy_data_to_cuda_env(ndir,0)
GPU/source/domdecstuff.f: 80   call orderbuffer_gpu(.false.)
GPU/source/domdecstuff.f:      call orderbufferrec_gpu
GPU/source/orthogonalize.f:      use interfaces,only: tmatxb_pmegpu,tmatxb_p
GPU/source/orthogonalize.f:      use utilgpu
GPU/source/orthogonalize.f:         call commdirdirgpu(nrhs,0,Rin,reqrec,reqsend)
GPU/source/orthogonalize.f:         call commdirdirgpu(nrhs,1,Rin,reqrec,reqsend)
GPU/source/orthogonalize.f:         call commdirdirgpu(nrhs,2,Rin,reqrec,reqsend)
GPU/source/orthogonalize.f:         call commrecdirdipgpu(nrhs,0,murec,Rin,buffermpimu1,
GPU/source/orthogonalize.f:         call commrecdirdipgpu(nrhs,1,murec,Rin,buffermpimu1,
GPU/source/orthogonalize.f:         call commrecdirdipgpu(nrhs,2,murec,Rin,buffermpimu1,
GPU/source/orthogonalize.f:      call tmatxbrecipgpu(Rin,murec,nrhs,dipfield,dipfieldbis)
GPU/source/orthogonalize.f:         call commfieldgpu(nrhs,Rout)
GPU/source/orthogonalize.f:         call commrecdirsolvgpu(nrhs,0,dipfieldbis,dipfield,buffermpi1,
GPU/source/orthogonalize.f:         call commrecdirsolvgpu(nrhs,1,dipfieldbis,dipfield,buffermpi1,
GPU/source/orthogonalize.f:         call commrecdirsolvgpu(nrhs,2,dipfieldbis,dipfield,buffermpi1,
GPU/source/orthogonalize.f:      use utilgpu ,only: rec_queue
GPU/source/orthogonalize.f:#ifdef _OPENACC
GPU/source/orthogonalize.f:      use utilgpu
GPU/source/orthogonalize.f:#ifdef _OPENACC
GPU/source/orthogonalize.f:      use utilgpu ,only: rec_queue,Tinker_shellEnv
GPU/source/orthogonalize.f:#ifdef _OPENACC
GPU/source/orthogonalize.f:      use utilgpu
GPU/source/orthogonalize.f:#ifdef _OPENACC
GPU/source/orthogonalize.f:#if TINKER_MIXED_PREC && defined(_OPENACC)
GPU/source/orthogonalize.f:#ifdef _OPENACC
GPU/source/orthogonalize.f:      use utilgpu    ,only: rec_stream
GPU/source/orthogonalize.f:#if _OPENACC
GPU/source/orthogonalize.f:#if defined(_OPENACC)
GPU/source/orthogonalize.f:#ifdef _OPENACC
GPU/source/orthogonalize.f:      use utilgpu    ,only: rec_stream
GPU/source/orthogonalize.f:#ifdef _OPENACC
GPU/source/orthogonalize.f:#ifdef _OPENACC
GPU/source/orthogonalize.f:#ifdef _OPENACC
GPU/source/orthogonalize.f:#ifdef _OPENACC
GPU/source/orthogonalize.f:      use utilgpu  ,only: rec_queue
GPU/source/orthogonalize.f:      use utilgpu    ,only: Tinker_shellEnv
GPU/source/orthogonalize.f:      use utilgpu    ,only: Tinker_shellEnv
GPU/source/epolar_cpencu.f:c     and derivatives with respect to Cartesian coordinates in a CUDA Fortran kernel
GPU/source/epolar_cpencu.f:#ifdef _CUDA
GPU/source/epolar_cpencu.f:#include "tinker_cudart.h"
GPU/source/epolar_cpencu.f:        use utilgpu ,only: BLOCK_SIZE,RED_BUFF_SIZE
GPU/source/respa1.f:      use utilgpu ,only: prmem_requestm,rec_queue
GPU/source/respa1.f:      call mdrestgpu (istep)
GPU/source/respa1.f:      use utilgpu ,only:prmem_requestm,rec_queue
GPU/source/respa1.f:      use utilgpu ,only: prmem_requestm,rec_queue
GPU/source/epolar3_group.f:      use utilgpu
GPU/source/epolar3_group.f:      use utilgpu
GPU/source/epolar3_group.f:      use utilgpu ,only: def_queue,real3,real6,real3_red,rpole_elt
GPU/source/pair_polar_cpen.inc.f:#include "tinker_cudart.h"
GPU/source/image.f.inc:#include "tinker_cudart.h"
GPU/source/bounds.f:      ! Wrap each molecules individually (OpenACC)
GPU/source/empole_cpencu.tpl.f:c     "empole1cu" : CUDA Template for calculation of the multipole and dipole polarization
GPU/source/dcinduce_shortrealgpu.f:      subroutine dcinduce_shortrealgpu
GPU/source/dcinduce_shortrealgpu.f:      subroutine inducedc_shortrealgpu(matvec,nrhs,dodiis,ef,mu)
GPU/source/dcinduce_shortrealgpu.f:      subroutine pc_dc_tmatxb_shortrealgpu(nrhs,dodiag,mu,efi)
GPU/source/dcinduce_shortrealgpu.f:      subroutine otf_dc_tmatxb_shortrealgpu(nrhs,dodiag,mu,efi)
GPU/source/MOD_spectra.f:#ifdef _OPENACC
GPU/source/MOD_spectra.f:      use utilgpu, only: prmem_requestm,prmem_request,rec_queue
GPU/source/MOD_spectra.f:#ifdef _OPENACC
GPU/source/MOD_spectra.f:#ifdef _OPENACC
GPU/source/MOD_spectra.f:#ifdef _OPENACC
GPU/source/MOD_spectra.f:      use utilgpu, only: rec_queue
GPU/source/MOD_spectra.f:#ifndef _OPENACC
GPU/source/MOD_spectra.f:#ifdef _OPENACC
GPU/source/kangtor.f:      use utilgpu,only: rec_queue
GPU/source/kangtor.f:#ifdef _OPENACC
GPU/source/kangtor.f:#ifdef _OPENACC
GPU/source/kangtor.f:#ifdef _OPENACC
GPU/source/final.f:      use utilgpu
GPU/source/final.f:#ifdef _OPENACC
GPU/source/final.f:      use cudafor
GPU/source/final.f:c#ifdef _OPENACC
GPU/source/final.f:c      if (cudaStreamDestroy(dir_stream).ne.0)
GPU/source/final.f:c      if (cudaStreamDestroy(rec_stream).ne.0)
GPU/source/final.f:      call gpuFreeMdstuf1Data
GPU/source/dcflux.gpu.f:      use utilgpu
GPU/source/dcflux.gpu.f:      use utilgpu
GPU/source/dcflux.gpu.f:      use utilgpu
GPU/source/dcflux.gpu.f:      use utilgpu
GPU/source/baoabpi.f:      use utilgpu
GPU/source/pimd.f:#ifdef _OPENACC
GPU/source/pimd.f:      use utilgpu,only: bind_gpu
GPU/source/pimd.f:#ifdef _OPENACC
GPU/source/pimd.f:      call bind_gpu
GPU/source/pimd.f:      use utilgpu ,only: rec_queue
GPU/source/eopbend.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/eopbend.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/eopbend.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/minimize.f:      use utilgpu   ,only: rec_queue,ti_p,re_p
GPU/source/minimize.f:      use utilgpu,only:rec_queue,rec_stream,mem_set,ti_p,re_p
GPU/source/ehal1cu.f:#ifdef _CUDA
GPU/source/ehal1cu.f:        use cudafor
GPU/source/ehal1cu.f:        use utilgpu  ,only: BLOCK_SIZE,RED_BUFF_SIZE
GPU/source/MOD_commstuffpi.f:        use utilgpu
GPU/source/MOD_commstuffpi.f:#ifdef _OPENACC
GPU/source/MOD_commstuffpi.f:20      call orderbuffer_gpu(.false.)
GPU/source/switch_group.f:      use utilgpu
GPU/source/switch_group.f:        !  call chkpolegpu_group
GPU/source/config.nvidia.mk:   ifeq ($(arch),$(filter $(arch),device gpu))
GPU/source/config.nvidia.mk:      bin_suffix_ := $(bin_suffix_).gpu
GPU/source/config.nvidia.mk:pp_flags_cuda_c_    :=
GPU/source/config.nvidia.mk:pp_flags_cuda_f_    := -cpp
GPU/source/config.nvidia.mk:   pp_flags_cuda_c_ += -DUSE_ERFC_HASTINGS
GPU/source/config.nvidia.mk:   pp_flags_cuda_c_ += -DUSE_ERFC_HASTINGS
GPU/source/config.nvidia.mk:ifeq ($(arch),$(filter $(arch),device gpu))
GPU/source/config.nvidia.mk:   pp_flags_cuda_f_ += $(add_options_cuf)
GPU/source/config.nvidia.mk:   pp_flags_cuda_c_ += $(add_options_cu)
GPU/source/config.nvidia.mk:pp_flags_cuda_c_    += $(pp_flags_common_)
GPU/source/config.nvidia.mk:pp_flags_cuda_f_    += $(pp_flags_common_)
GPU/source/config.nvidia.mk:cuda_version        := 11.7
GPU/source/config.nvidia.mk:# -- Nvidia Device's compute capability
GPU/source/config.nvidia.mk:nvidia_cc_f__       := $(foreach var,$(cc_list_),cc$(var))
GPU/source/config.nvidia.mk:nvidia_cc_f__       := $(subst $(space__),$(comma__),$(nvidia_cc_f__))
GPU/source/config.nvidia.mk:nvidia_cc_cxx__     := $(foreach var,$(cc_list_),-gencode arch=compute_$(var)$(comma__)code=sm_$(var))
GPU/source/config.nvidia.mk:# -- GPU Device property
GPU/source/config.nvidia.mk:   nvidia_prop_sufx__  := ,fastmath
GPU/source/config.nvidia.mk:   nvidia_prop_prfx__  := -gpu=
GPU/source/config.nvidia.mk:nvidia_prop_f_      := $(nvidia_cc_f__),cuda$(cuda_version),unroll$(nvidia_prop_sufx__)
GPU/source/config.nvidia.mk:nvidia_prop_cxx_    := $(nvidia_cc_f__),cuda$(cuda_version),unroll$(nvidia_prop_sufx__)
GPU/source/config.nvidia.mk:   oacc_flags_f_      := -ta=tesla:$(nvidia_prop_f_)
GPU/source/config.nvidia.mk:   oacc_flags_cxx_    := -ta=tesla:$(nvidia_prop_cxx_)
GPU/source/config.nvidia.mk:   cuda_flags_f_      := -Mcuda=$(nvidia_prop_f_)
GPU/source/config.nvidia.mk:   cuda_flags_c_      := $(device_c_comp) $(nvidia_cc_cxx__)
GPU/source/config.nvidia.mk:   device_flags_f_    := $(nvidia_prop_prfx__)$(nvidia_prop_f_)
GPU/source/config.nvidia.mk:   device_flags_cxx_  := $(nvidia_prop_prfx__)$(nvidia_prop_cxx_)
GPU/source/config.nvidia.mk:   omp_flags_f_       := -mp=gpu
GPU/source/config.nvidia.mk:   omp_flags_cxx_     := -mp=gpu
GPU/source/config.nvidia.mk:   cuda_flags_f_      := -cuda
GPU/source/config.nvidia.mk:   cuda_flags_c_      := $(device_c_comp) $(nvidia_cc_cxx__)
GPU/source/config.nvidia.mk:   cuda_flags_c_      += $(cuda_c_compiler) -rdc=true -I$(INC_NVSHMEM)
GPU/source/config.nvidia.mk:   cuda_flags_c_      += --use_fast_math
GPU/source/config.nvidia.mk:   host_flags_cuda_c_ := -std=c++$(cxx_std) -g -O3
GPU/source/config.nvidia.mk:   host_flags_cuda_f_ := -traceback -g -fast -Mdalign $(inline_f__)
GPU/source/config.nvidia.mk:   host_flags_cuda_c_ := -std=c++$(cxx_std) -g -O0
GPU/source/config.nvidia.mk:   host_flags_cuda_f_ := -traceback -g -O0 -Mbounds
GPU/source/config.nvidia.mk:   host_flags_cuda_c_ := -std=c++$(cxx_std) -g -O0
GPU/source/config.nvidia.mk:   host_flags_cuda_f_ := -traceback -g -O0 -C -Mbounds
GPU/source/config.nvidia.mk:   host_flags_cuda_c_ += --compiler-options -fPIC
GPU/source/config.nvidia.mk:   host_flags_cuda_f_ += -r4
GPU/source/config.nvidia.mk:   host_flags_cuda_f_ += -r8
GPU/source/config.nvidia.mk:   host_flags_cuda_c_ += $(add_host)
GPU/source/config.nvidia.mk:   host_flags_cuda_f_ += $(add_host)
GPU/source/config.nvidia.mk:   host_flags_cuda_c_ += $(add_host_cu)
GPU/source/config.nvidia.mk:   host_flags_cuda_f_ += $(add_host_cuf)
GPU/source/config.nvidia.mk:ifeq ($(arch),$(filter $(arch),device gpu))
GPU/source/config.nvidia.mk:ifeq ($(arch),$(filter $(arch),device gpu))
GPU/source/config.nvidia.mk:   comp_flags_mod_f_     += $(cuda_flags_f_)
GPU/source/config.nvidia.mk:comp_flags_gpu_f_     := $(comp_flags_mod_f_)
GPU/source/config.nvidia.mk:ifeq ($(arch),$(filter $(arch),device gpu))
GPU/source/config.nvidia.mk:comp_flags_cuda_f_    := $(pp_flags_cuda_f_) $(host_flags_cuda_f_) $(cuda_flags_f_)
GPU/source/config.nvidia.mk:comp_flags_cuda_f_    += $(device_flags_f_)
GPU/source/config.nvidia.mk:comp_flags_cuda_c_    := $(pp_flags_cuda_c_) $(host_flags_cuda_c_) $(cuda_flags_c_)
GPU/source/config.nvidia.mk:comp_flags_hip_c_     := $(pp_flags_cuda_c_) $(host_flags_hip_c_)
GPU/source/config.nvidia.mk:GPUFLAGS               = $(comp_flags_gpu_f_)
GPU/source/config.nvidia.mk:CUFFLAGS               = $(comp_flags_cuda_f_)
GPU/source/config.nvidia.mk:   CUCFLAGS            = $(comp_flags_cuda_c_)
GPU/source/config.nvidia.mk:FFLAGS2                = $(comp_flags_gpu_f_)
GPU/source/config.nvidia.mk:      dev_ldlibs      += -Mcudalib=curand,cufft,cublas -Mcuda=$(acc_cc_flag__) -lcusolver
GPU/source/config.nvidia.mk:      dev_ldlibs      += -cudalib=curand,cufft,cublas -lcusolver
GPU/source/config.nvidia.mk:   dev_ldlibs         += -L$(LIB_NVSHMEM) -L/$(LIB_CUDA_DRIVER) -lmpi_cxx
GPU/source/config.nvidia.mk:ifeq ($(arch),$(filter $(arch),device gpu))
GPU/source/mdrest.f:      if (deb_Path) write(*,*) ' mdrestgpu'
GPU/source/mdrest.f:      subroutine mdrestgpu (istep)
GPU/source/mdrest.f:      if (deb_Path) write(*,*) ' mdrestgpu'
GPU/source/convert.f.inc:      use cudadevice,only: __longlong_as_double
GPU/source/convert.f.inc:      use cudadevice,only: __longlong_as_double
GPU/source/convert.f.inc:      !tp2mdr = nint(input*frac_bits,8) !Not Recognize intrinsic with OpenACC
GPU/source/kinetic.f:      subroutine kineticgpu (eksum,ekin,temp)
GPU/source/eangtor1.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/bonds.f:      use utilgpu
GPU/source/bonds.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/bonds.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/bonds.f:#ifdef _OPENACC
GPU/source/bonds.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/bonds.f:      use utilgpu
GPU/source/bonds.f:#if _OPENACC
GPU/source/bonds.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/bonds.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/bonds.f:      ! Explicit Reassociate to make device pointer data accessible by OpenACC
GPU/source/bonds.f:      ! self association for openAcc visibility
GPU/source/chkpolegpu.f:      subroutine chkpolegpu(init)
GPU/source/chkpolegpu.f:      use utilgpu,only:openacc_abort,rec_queue
GPU/source/chkpolegpu.f:      if (init) call openacc_abort(" init is not supposed to be true"//
GPU/source/chkpolegpu.f:     &   " in chkpolegpu routine")
GPU/source/chkpolegpu.f:      subroutine chkpolegpu_group()
GPU/source/chkpolegpu.f:      use utilgpu  ,only:openacc_abort,rec_queue
GPU/source/MOD_bond.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/MOD_bond.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/tmatxb_pme_cpen.cu.f:#ifdef _CUDA
GPU/source/tmatxb_pme_cpen.cu.f:#include "tinker_cudart.h"
GPU/source/tmatxb_pme_cpen.cu.f:        use utilgpu ,only: BLOCK_SIZE
GPU/source/MOD_tors.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/MOD_tors.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/molecule.f:#ifdef _OPENACC
GPU/source/molecule.f:      use utilgpu
GPU/source/molecule.f:#ifdef _OPENACC
GPU/source/pair_polar.f.inc:#include "tinker_cudart.h"
GPU/source/pair_polar.f.inc:      use utilgpu   ,only: rpole_elt,real3,real6,real3_red
GPU/source/pair_ehal.inc.f:#include "tinker_cudart.h"
GPU/source/MOD_pitors.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/MOD_pitors.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/MOD_subDeriv.f:      use utilgpu  ,only:mem_move,mem_set,rec_stream,rec_queue
GPU/source/newinduce_pme2.f:#ifdef _OPENACC
GPU/source/epolar1_group.f:      use utilgpu
GPU/source/epolar1_group.f:      use interfaces ,only: torquegpu_group
GPU/source/epolar1_group.f:      use utilgpu
GPU/source/epolar1_group.f:      call torquegpu_group(trqvec,fix,fiy,fiz,depgroup)
GPU/source/epolar1_group.f:      use utilgpu ,only: def_queue
GPU/source/eangle3gpu.f:      module eangle3gpu_inl
GPU/source/eangle3gpu.f:      subroutine eangle3gpu_(dea,deW1aMD,atmType,afld)
GPU/source/eangle3gpu.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/eangle3gpu.f:      if(deb_Path) write(*,*) 'eangle3gpu'
GPU/source/eangle3gpu.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/eangle3gpu.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/eangle3gpu.f:      subroutine eangle3gpu
GPU/source/eangle3gpu.f:      use eangle3gpu_inl,only: eangle3gpu_
GPU/source/eangle3gpu.f:      use utilgpu       ,only: lam_buff
GPU/source/eangle3gpu.f:      call eangle3gpu_(lam_buff,lam_buff,type,afld)
GPU/source/eimptor1gpu.f:c     "eimptor1gpu" calculates on device improper torsion energy and its
GPU/source/eimptor1gpu.f:      module eimptor1gpu_inl
GPU/source/eimptor1gpu.f:      subroutine eimptor1gpu
GPU/source/eimptor1gpu.f:      use eimptor1gpu_inl
GPU/source/eimptor1gpu.f:      if (deb_Path) write(*,*) "eimptor1gpu",nitorsloc
GPU/source/MOD_utils.f:      subroutine amove2gpu(src1,src2,dst1,dst2,n,queue_)
GPU/source/MOD_utils.f:#ifdef _OPENACC
GPU/source/MOD_utils.f:      use openacc ,only: acc_async_sync
GPU/source/MOD_utils.f:#ifdef _OPENACC
GPU/source/MOD_subInform.f:      use utilgpu,only: Tinker_shellEnv
GPU/source/ker_strbnd.inc.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/mpiUtils.gpu.f:      use utilgpu
GPU/source/MOD_subMemory.f:#ifdef _OPENACC
GPU/source/MOD_subMemory.f:      use openacc
GPU/source/MOD_subMemory.f:      use utilgpu,only: mem_set,rec_stream,rec_queue
GPU/source/MOD_subMemory.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/MOD_subMemory.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/MOD_subMemory.f:#ifdef _CUDA
GPU/source/MOD_subMemory.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/MOD_subMemory.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/MOD_subMemory.f:         istat = cudamemcpy(c_devloc(d_nshArray(0)),c_loc(nshArray(0)),
GPU/source/MOD_subMemory.f:         CALL CUDA_ERROR_CHECK(istat,__FILE__,__LINE__,"cudamemcpy")
GPU/source/MOD_subMemory.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/MOD_subMemory.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/MOD_subMemory.f:#ifdef _CUDA
GPU/source/MOD_subMemory.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/MOD_subMemory.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/MOD_subMemory.f:         istat = cudamemcpy(c_devloc(d_nshArray(0)),c_loc(nshArray(0)),
GPU/source/MOD_subMemory.f:         CALL CUDA_ERROR_CHECK(istat,__FILE__,__LINE__,"cudamemcpy")
GPU/source/MOD_subMemory.f:#ifdef _CUDA
GPU/source/MOD_subMemory.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/MOD_subMemory.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/MOD_subMemory.f:#ifdef _CUDA
GPU/source/MOD_subMemory.f:#if defined(_CUDA) && defined(USE_NVSHMEM)
GPU/source/MOD_subMemory.f:#if defined(_CUDA) && defined(USE_NVSHMEM)
GPU/source/MOD_subMemory.f:         istat = cudamemcpy(c_devloc(d_nshArray(0)),c_loc(nshArray(0)),
GPU/source/MOD_subMemory.f:         CALL CUDA_ERROR_CHECK(istat,__FILE__,__LINE__,"cudamemcpy")
GPU/source/MOD_subMemory.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/MOD_subMemory.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/MOD_subMemory.f:#ifdef _CUDA
GPU/source/MOD_subMemory.f:#if defined(_CUDA) && defined(USE_NVSHMEM)
GPU/source/MOD_subMemory.f:#if defined(_CUDA) && defined(USE_NVSHMEM)
GPU/source/MOD_subMemory.f:         istat = cudamemcpy(c_devloc(d_nshArray(0)),c_loc(nshArray(0)),
GPU/source/MOD_subMemory.f:         CALL CUDA_ERROR_CHECK(istat,__FILE__,__LINE__,"cudamemcpy")
GPU/source/MOD_subMemory.f:#if defined(_CUDA) && defined(USE_NVSHMEM)
GPU/source/MOD_subMemory.f:#if defined(_CUDA) && defined(USE_NVSHMEM)
GPU/source/MOD_subMemory.f:#ifdef _CUDA
GPU/source/MOD_subMemory.f:#if defined(_CUDA) && defined(USE_NVSHMEM)
GPU/source/MOD_subMemory.f:#if defined(USE_NVSHMEM) && defined(_CUDA)
GPU/source/MOD_subMemory.f:         istat = cudamemcpy(c_devloc(d_nshArray(0)),c_loc(nshArray(0)),
GPU/source/MOD_subMemory.f:         CALL CUDA_ERROR_CHECK(istat,__FILE__,__LINE__,"cudamemcpy")
GPU/source/MOD_subMemory.f:#if defined(_CUDA) && defined(USE_NVSHMEM)
GPU/source/MOD_subMemory.f:#if defined(_CUDA) && defined(USE_NVSHMEM)
GPU/source/MOD_subMemory.f:#ifdef _CUDA
GPU/source/MOD_subMemory.f:#if defined(_CUDA) && defined(USE_NVSHMEM)
GPU/source/MOD_subMemory.f:#if defined(USE_NVSHMEM) && defined(_CUDA)
GPU/source/MOD_subMemory.f:         istat = cudamemcpy(c_devloc(d_nshArray(0)),c_loc(nshArray(0)),
GPU/source/MOD_subMemory.f:         CALL CUDA_ERROR_CHECK(istat,__FILE__,__LINE__,
GPU/source/MOD_subMemory.f:     &        "cudamemcpy")
GPU/source/MOD_subMemory.f:#if defined(_CUDA) && defined(USE_NVSHMEM)
GPU/source/MOD_subMemory.f:#if defined(_CUDA) && defined(USE_NVSHMEM)
GPU/source/MOD_subMemory.f:#ifdef _CUDA
GPU/source/MOD_subMemory.f:#if defined(_CUDA) && defined(USE_NVSHMEM)
GPU/source/MOD_subMemory.f:#if defined(USE_NVSHMEM) && defined(_CUDA)
GPU/source/MOD_subMemory.f:         istat = cudamemcpy(c_devloc(d_nshArray(0)),c_loc(nshArray(0)),
GPU/source/MOD_subMemory.f:         CALL CUDA_ERROR_CHECK(istat,__FILE__,__LINE__,
GPU/source/MOD_subMemory.f:     &        "cudamemcpy")
GPU/source/MOD_subMemory.f:#if defined(_CUDA) && defined(USE_NVSHMEM)
GPU/source/MOD_subMemory.f:#if defined(_CUDA) && defined(USE_NVSHMEM)
GPU/source/MOD_subMemory.f:#ifdef _CUDA
GPU/source/MOD_subMemory.f:#if defined(_CUDA) && defined(USE_NVSHMEM)
GPU/source/MOD_subMemory.f:#if defined(USE_NVSHMEM) && defined(_CUDA)
GPU/source/MOD_subMemory.f:         istat = cudamemcpy(c_devloc(d_nshArray(0)),c_loc(nshArray(0)),
GPU/source/MOD_subMemory.f:         CALL CUDA_ERROR_CHECK(istat,__FILE__,__LINE__,
GPU/source/MOD_subMemory.f:     &        "cudamemcpy")
GPU/source/MOD_subMemory.f:#if defined(_CUDA) && defined(USE_NVSHMEM)
GPU/source/MOD_subMemory.f:#if defined(_CUDA) && defined(USE_NVSHMEM)
GPU/source/MOD_subMemory.f:#ifdef _CUDA
GPU/source/MOD_subMemory.f:#if defined(_CUDA) && defined(USE_NVSHMEM)
GPU/source/MOD_subMemory.f:#if defined(USE_NVSHMEM) && defined(_CUDA)
GPU/source/MOD_subMemory.f:         istat = cudamemcpy(c_devloc(d_nshArray(0)),c_loc(nshArray(0)),
GPU/source/MOD_subMemory.f:         CALL CUDA_ERROR_CHECK(istat,__FILE__,__LINE__,
GPU/source/MOD_subMemory.f:     &        "cudamemcpy")
GPU/source/MOD_subMemory.f:#if defined(_CUDA) && defined(USE_NVSHMEM)
GPU/source/MOD_subMemory.f:#if defined(_CUDA) && defined(USE_NVSHMEM)
GPU/source/MOD_subMemory.f:#if defined(_CUDA) && defined(USE_NVSHMEM)
GPU/source/MOD_subMemory.f:            istat  = cudamemcpyasync(dst,src(l_beg),length)
GPU/source/MOD_subMemory.f:            CALL CUDA_ERROR_CHECK(istat,__FILE__,__LINE__,
GPU/source/MOD_subMemory.f:     &           "cudamemcpy")
GPU/source/MOD_subMemory.f:#if defined(_CUDA) && defined(USE_NVSHMEM)
GPU/source/MOD_subMemory.f:#if defined(_CUDA) && defined(USE_NVSHMEM)
GPU/source/MOD_subMemory.f:#if defined(_CUDA) && defined(USE_NVSHMEM)
GPU/source/MOD_subMemory.f:            istat  = cudamemcpyasync(dst,src(l_beg),length)
GPU/source/MOD_subMemory.f:            CALL CUDA_ERROR_CHECK(istat,__FILE__,__LINE__,
GPU/source/MOD_subMemory.f:     &           "cudamemcpy")
GPU/source/MOD_subMemory.f:#ifdef _CUDA
GPU/source/MOD_subMemory.f:      ! Check for CUDA Error
GPU/source/MOD_subMemory.f:      subroutine CUDA_ERROR_CHECK(istat,filename,line,error_type)
GPU/source/MOD_subMemory.f:65    format ("CUDA ASSERT: ",A,A,1x,6I,A," at",2x,"line",5I,/,15x,A)
GPU/source/MOD_subMemory.f:      if (istat.ne.cudasuccess) then
GPU/source/MOD_subMemory.f:     &               cudageterrorstring(istat)
GPU/source/MOD_subMemory.f:#ifdef _OPENACC
GPU/source/MOD_subMemory.f:#ifdef _OPENACC
GPU/source/MOD_subMemory.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/MOD_subMemory.f:      ! Array is also reallocated on GPU using `exit/enter data` directives
GPU/source/MOD_subMemory.f:#ifdef _OPENACC
GPU/source/MOD_subMemory.f:#ifdef _OPENACC
GPU/source/MOD_subMemory.f:#ifdef _OPENACC
GPU/source/MOD_subMemory.f:#ifdef _OPENACC
GPU/source/MOD_subMemory.f:#ifdef _OPENACC
GPU/source/MOD_subMemory.f:#ifdef _OPENACC
GPU/source/MOD_subMemory.f:#ifdef _OPENACC
GPU/source/MOD_subMemory.f:#ifdef _OPENACC
GPU/source/MOD_subMemory.f:#ifdef _OPENACC
GPU/source/MOD_subMemory.f:#ifdef _OPENACC
GPU/source/MOD_subMemory.f:#ifdef _OPENACC
GPU/source/MOD_subMemory.f:#ifdef _OPENACC
GPU/source/MOD_subMemory.f:#ifdef _OPENACC
GPU/source/MOD_subMemory.f:#ifdef _OPENACC
GPU/source/MOD_subMemory.f:#ifdef _OPENACC
GPU/source/MOD_subMemory.f:#ifdef _OPENACC
GPU/source/MOD_subMemory.f:#ifdef _OPENACC
GPU/source/MOD_subMemory.f:#ifdef _OPENACC
GPU/source/MOD_subMemory.f:#ifdef _OPENACC
GPU/source/MOD_subMemory.f:#ifdef _OPENACC
GPU/source/MOD_subMemory.f:#ifdef _OPENACC
GPU/source/MOD_subMemory.f:#ifdef _OPENACC
GPU/source/MOD_subMemory.f:#ifdef _OPENACC
GPU/source/MOD_subMemory.f:#ifdef _OPENACC
GPU/source/MOD_subMemory.f:#ifdef _OPENACC
GPU/source/MOD_subMemory.f:#ifdef _OPENACC
GPU/source/MOD_subMemory.f:#ifdef _OPENACC
GPU/source/MOD_subMemory.f:#ifdef _OPENACC
GPU/source/MOD_subMemory.f:#ifdef _OPENACC
GPU/source/MOD_subMemory.f:#ifdef _OPENACC
GPU/source/MOD_subMemory.f:#ifdef _OPENACC
GPU/source/MOD_subMemory.f:#ifdef _OPENACC
GPU/source/MOD_subMemory.f:#ifdef _OPENACC
GPU/source/MOD_subMemory.f:#ifdef _OPENACC
GPU/source/MOD_subMemory.f:#ifdef _OPENACC
GPU/source/MOD_subMemory.f:#ifdef _OPENACC
GPU/source/MOD_subMemory.f:#ifdef _OPENACC
GPU/source/MOD_subMemory.f:11      format(" OpenACC ERROR : prmem_int_mvreq",/,
GPU/source/MOD_subMemory.f:     &  " OpenACC options",/,
GPU/source/MOD_erf.f:c     the error function via a Chebyshev approximation on a gpu
GPU/source/MOD_erf.f:c     complementary error function via a Chebyshev approximation a gpu
GPU/source/MOD_erf.f:c     complementary error function via a Chebyshev approximation a gpu
GPU/source/tinker_precision.h:#  ifdef TINKER_CUDART_H
GPU/source/tinker_precision.h:#    warning( "tinker_cudart.h  should not be included before tinker_precision.h")
GPU/source/tinker_precision.h:#if defined(_CUDA) && defined(USE_NVSHMEM)
GPU/source/tinker_precision.h:#  ifndef USE_NVSHMEM_CUDA
GPU/source/tinker_precision.h:#    define USE_NVSHMEM_CUDA
GPU/source/empole1gpu.f:      module empole1gpu_inl
GPU/source/empole1gpu.f:      subroutine empole1gpu
GPU/source/empole1gpu.f:      call empole1cgpu
GPU/source/empole1gpu.f:      use interfaces,only: reorder_nblist,bspline_fill_sitegpu
GPU/source/empole1gpu.f:      use utilgpu
GPU/source/empole1gpu.f:#ifdef _OPENACC
GPU/source/empole1gpu.f:      call chkpolegpu(.false.)
GPU/source/empole1gpu.f:      call rotpolegpu
GPU/source/empole1gpu.f:#ifdef _OPENACC
GPU/source/empole1gpu.f:         call bspline_fill_sitegpu
GPU/source/empole1gpu.f:#ifdef _OPENACC
GPU/source/empole1gpu.f:      subroutine empole1cgpu
GPU/source/empole1gpu.f:      use empole1gpu_inl
GPU/source/empole1gpu.f:      use interfaces ,only: torquegpu,emreal1c_p
GPU/source/empole1gpu.f:      use utilgpu    ,pot=>ug_workS_r
GPU/source/empole1gpu.f:      if (deb_Path) write(*,*) 'empole1cgpu'
GPU/source/empole1gpu.f:            !call torquegpu(npoleloc,nbloc,poleglob,loc,trq,dem,def_queue)
GPU/source/empole1gpu.f:         call emrecip1gpu
GPU/source/empole1gpu.f:#ifdef _OPENACC
GPU/source/empole1gpu.f:      use empole1gpu_inl
GPU/source/empole1gpu.f:      use interfaces ,only: emreal1ca_p,torquegpu
GPU/source/empole1gpu.f:      use utilgpu    ,only: dir_queue,def_queue,rec_queue,mem_set
GPU/source/empole1gpu.f:#ifdef _OPENACC
GPU/source/empole1gpu.f:#ifdef _OPENACC
GPU/source/empole1gpu.f:      call torquegpu(tem,fix,fiy,fiz,dem,extract)
GPU/source/empole1gpu.f:      use utilgpu ,only: def_queue
GPU/source/empole1gpu.f:      use utilgpu ,only: def_queue
GPU/source/empole1gpu.f:      use empole1gpu_inl ,only: image_inl,duo_mpole,groups2_inl
GPU/source/empole1gpu.f:      use utilgpu,only:dir_queue,rec_queue,def_queue,warning
GPU/source/empole1gpu.f:     &   'emreal1c_gpu',use_mpoleshortreal,use_mpolelong
GPU/source/empole1gpu.f:      ! CUDA Fortran routine
GPU/source/empole1gpu.f:#ifdef _CUDA
GPU/source/empole1gpu.f:      use utilgpu,only:dir_queue,rec_queue,def_queue,warning
GPU/source/empole1gpu.f:         call cudaMaxGridSize("emreal1_kcu",gS)
GPU/source/empole1gpu.f:      !call CUDA kernel to compute the real space portion of the Ewald summation
GPU/source/empole1gpu.f:      ! CUDA C wrapper on emreal1c (check file cu_mpole1.cu)
GPU/source/empole1gpu.f:      use utilgpu,only:dir_queue,rec_queue,def_queue,warning
GPU/source/empole1gpu.f: 13   format(3x,"ERROR : empole CUDA-C Routine is not to be with"
GPU/source/empole1gpu.f:c     ##  subroutine emrecip1gpu  --  PME recip multipole energy & derivs  ##
GPU/source/empole1gpu.f:      subroutine emrecip1gpu
GPU/source/empole1gpu.f:      use interfaces,only: torquegpu,fphi_mpole_site_p
GPU/source/empole1gpu.f:      use utilgpu   ,pot=>ug_workS_r,potrec=>ug_workS_r1
GPU/source/empole1gpu.f:      if (deb_Path) write(*,'(2x,a)') 'emrecip1gpu'
GPU/source/empole1gpu.f:      call cmp_to_fmp_sitegpu(cmp,fmp)
GPU/source/empole1gpu.f:#ifdef _OPENACC
GPU/source/empole1gpu.f:#ifdef _OPENACC
GPU/source/empole1gpu.f:#ifdef _OPENACC
GPU/source/empole1gpu.f:#ifdef _OPENACC
GPU/source/empole1gpu.f:      call fphi_to_cphi_sitegpu(fphirec,cphirec)
GPU/source/empole1gpu.f:      call torquegpu(npolerecloc,nlocrec2,polerecglob,locrec
GPU/source/temper.f:         call kineticgpu (eksum,ekin,temp)
GPU/source/temper.f:#ifdef _OPENACC
GPU/source/temper.f:            call normalgpu(samplevec,nfree)
GPU/source/temper.f:#ifdef _OPENACC
GPU/source/temper.f:#ifdef _OPENACC
GPU/source/temper.f:      if (calc_e.or.use_virial) call kineticgpu ( eksum,ekin,temp )
GPU/source/temper.f:      call kineticgpu (eksum,ekin,temp)
GPU/source/verlet.f:      use utilgpu,only:prmem_requestm,rec_queue
GPU/source/verlet.f:      call mdrestgpu (istep)
GPU/source/MOD_mdstuf.f:      subroutine gpuAllocMdstuf1Data
GPU/source/MOD_mdstuf.f:      subroutine gpuFreeMdstuf1Data
GPU/source/tmatxb_pmecu.f:#ifdef _CUDA
GPU/source/tmatxb_pmecu.f:#include "tinker_cudart.h"
GPU/source/tmatxb_pmecu.f:        use utilgpu ,only: BLOCK_SIZE
GPU/source/estrtor3.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/estrtor3.f:#ifdef USE_NVSHMEM_CUDA
GPU/source/depend.mk:           MOD_utilgpu.o MOD_subAtoms.o MOD_mdstuf.o MOD_spectra.o MOD_beads.o
GPU/source/depend.mk:           MOD_utilgpu.o MOD_subAtoms.o MOD_mdstuf.o MOD_boxes.o MOD_charge.o MOD_mpole.o MOD_atmlst.o\
GPU/source/depend.mk:					 MOD_utilgpu.o MOD_couple.o MOD_commstuffpi.o MOD_spectra.o MOD_qtb.o
GPU/source/depend.mk:MOD_utilgpu.o: MOD_couple.o MOD_cell.o MOD_polgrp.o MOD_polpot.o MOD_sizes.o MOD_vdwpot.o
GPU/source/depend.mk:ifeq ($(arch),$(filter $(arch),device gpu))
GPU/source/depend.mk:echargecu.o: MOD_utilcu.o MOD_utilgpu.o pair_charge.f.inc echargecu.tpl.f
GPU/source/depend.mk:echgljcu.o: MOD_utilcu.o MOD_utilgpu.o MOD_vdw.o echgljcu.tpl.f pair_lj.inc.f pair_charge.f.inc
GPU/source/depend.mk:echgtrncu.o: MOD_utilcu.o MOD_utilgpu.o echgtrncu.tpl.f pair_chgtrn.inc.f
GPU/source/depend.mk:efld0_cpencu.o: MOD_utilcu.o MOD_utilgpu.o pair_efld_cp.inc.f
GPU/source/depend.mk:eljcu.o: MOD_utilcu.o MOD_utilgpu.o pair_lj.inc.f eljcu.tpl.f
GPU/source/depend.mk:eliaison1cu.o: MOD_utilcu.o MOD_utilgpu.o MOD_vdw.o
GPU/source/depend.mk:ehal1cu.o: MOD_utilcu.o MOD_utilgpu.o MOD_vdw.o ehalcu.tpl.f pair_ehal.inc.f
GPU/source/depend.mk:nblistcu.o: MOD_utilcu.o MOD_utilgpu.o
GPU/source/depend.mk:tmatxb_pmecu.o: MOD_utilcu.o MOD_utilgpu.o pair_tmatxb.f.inc
GPU/source/depend.mk:tmatxb_pme_cpen.cu.o: MOD_utilcu.o MOD_utilgpu.o pair_tmatxb.f.inc
GPU/source/depend.mk:empole1cu.o: MOD_utilcu.o MOD_utilgpu.o empolecu.tpl.f pair_mpole1.f.inc
GPU/source/depend.mk:empole_cpencu.o: MOD_utilcu.o MOD_utilgpu.o empole_cpencu.tpl.f pair_mpole1.f.inc
GPU/source/depend.mk:epolar1cu.o: MOD_utilcu.o MOD_utilgpu.o pair_polar.inc.f
GPU/source/depend.mk:epolar_cpencu.o: MOD_utilcu.o MOD_utilgpu.o pair_polar_cpen.inc.f epolar_cpencu.tpl.f
GPU/source/depend.mk:pmestuffcu.o: MOD_utilcu.o MOD_utilgpu.o
GPU/source/depend.mk:elj1gpu.o: eljcu.o
GPU/source/depend.mk:elj3gpu.o: eljcu.o
GPU/source/depend.mk:eliaison1gpu.o: MOD_utilcu.o eliaison1cu.o
GPU/source/depend.mk:ehal1gpu.o: ehal1cu.o
GPU/source/depend.mk:ehal3gpu.o: ehal1cu.o
GPU/source/depend.mk:echarge1gpu.o: echargecu.o echgljcu.o
GPU/source/depend.mk:echarge3gpu.o: echargecu.o
GPU/source/depend.mk:echgtrn1gpu.o: echgtrncu.o
GPU/source/depend.mk:echgtrn3gpu.o: echgtrncu.o
GPU/source/depend.mk:empole1gpu.o: empole1cu.o
GPU/source/depend.mk:empole3gpu.o: empole1cu.o
GPU/source/depend.mk:epolar1gpu.o: epolar1cu.o
GPU/source/depend.mk:epolar3gpu.o: epolar1cu.o
GPU/source/depend.mk:nblistgpu.o: nblistcu.o
GPU/source/depend.mk:nblist_build.gpu.o: nblistcu.o
GPU/source/depend.mk:efld0_directgpu.o: tmatxb_pmecu.o efld0_cpencu.o
GPU/source/depend.mk:tmatxb_pmegpu.o: MOD_utilcomm.o tmatxb_pmecu.o tmatxb_pme_cpen.cu.o
GPU/source/depend.mk:pmestuffgpu.o: pmestuffcu.o
GPU/source/depend.mk:MOD_subAtoms.o: MOD_atoms.o MOD_utilgpu.o
GPU/source/depend.mk:MOD_subDeriv.o: MOD_deriv.o MOD_atoms.o MOD_domdec.o MOD_neigh.o MOD_utilgpu.o MOD_beads.o
GPU/source/depend.mk:MOD_subInform.o: MOD_inform.o MOD_neigh.o MOD_usage.o MOD_utilgpu.o MOD_vdw.o
GPU/source/depend.mk:MOD_precompute_polegpu.o: MOD_atoms.o MOD_atmlst.o MOD_couple.o MOD_chgpot.o MOD_domdec.o MOD_inform.o MOD_mpole.o MOD_mplpot.o MOD_neigh.o MOD_polar.o MOD_utilgpu.o MOD_sizes.o MOD_vdwpot.o
GPU/source/depend.mk:alterchg.gpu.o: image.f.inc MOD_atoms.o MOD_atmlst.o MOD_atmlst.o MOD_inform.o MOD_mplpot.o MOD_potent.o MOD_utilcomm.o
GPU/source/depend.mk:angles.o: MOD_utilgpu.o
GPU/source/depend.mk:attach.o: MOD_utilgpu.o
GPU/source/depend.mk:baoabpiston.o: MOD_utilgpu.o
GPU/source/depend.mk:baoabrespa.o: MOD_ani.o MOD_utilgpu.o MOD_utilbaoab.o
GPU/source/depend.mk:baoabrespa1.o: MOD_ani.o MOD_utilgpu.o MOD_utilbaoab.o
GPU/source/depend.mk:					MOD_strtor.o MOD_timestat.o MOD_tortor.o MOD_units.o MOD_uprior.o MOD_urey.o MOD_usage.o MOD_utilgpu.o MOD_utils.o MOD_vdw.o MOD_virial.o
GPU/source/depend.mk:bitors.o: MOD_utilgpu.o
GPU/source/depend.mk:bonds.o: MOD_neigh.o MOD_vdw.o MOD_utilgpu.o
GPU/source/depend.mk:chkpolegpu.o: MOD_utilgpu.o
GPU/source/depend.mk:cholesky.o: MOD_utilgpu.o
GPU/source/depend.mk:cluster.o: MOD_neigh.o MOD_utilgpu.o
GPU/source/depend.mk:dcflux.gpu.o: MOD_cflux.o
GPU/source/depend.mk:dcinduce_pmegpu.o: MOD_neigh.o MOD_pme.o MOD_utilgpu.o
GPU/source/depend.mk:dcinduce_pme2gpu.o: MOD_pme.o MOD_utilgpu.o
GPU/source/depend.mk:dcinduce_shortrealgpu.o: MOD_neigh.o MOD_pme.o
GPU/source/depend.mk:eamd1.o: MOD_mamd.o MOD_utilgpu.o
GPU/source/depend.mk:eangle1gpu.o: ker_angle.inc.f
GPU/source/depend.mk:eangle3gpu.o: ker_angle.inc.f
GPU/source/depend.mk:ebond1gpu.o: ker_bond.inc.f
GPU/source/depend.mk:ebond3gpu.o: ker_bond.inc.f
GPU/source/depend.mk:eliaison1gpu.o:
GPU/source/depend.mk:echarge1gpu.o: MOD_neigh.o MOD_pme.o MOD_utilgpu.o MOD_vdw.o
GPU/source/depend.mk:echarge3gpu.o: MOD_neigh.o MOD_pme.o MOD_utilgpu.o
GPU/source/depend.mk:efld0_directgpu.o: MOD_neigh.o MOD_utilgpu.o pair_efld.inc.f
GPU/source/depend.mk:egeom1gpu.o: MOD_utilgpu.o
GPU/source/depend.mk:egeom3gpu.o:
GPU/source/depend.mk:ehal1gpu.o: MOD_neigh.o MOD_utilgpu.o MOD_vdw.o
GPU/source/depend.mk:ehal3gpu.o: MOD_neigh.o MOD_vdw.o
GPU/source/depend.mk:eimprop1gpu.o:
GPU/source/depend.mk:eimptor1gpu.o:
GPU/source/depend.mk:elj1gpu.o: MOD_neigh.o MOD_utilgpu.o MOD_vdw.o
GPU/source/depend.mk:elj3gpu.o: MOD_neigh.o MOD_utilgpu.o MOD_vdw.o
GPU/source/depend.mk:empole1gpu.o: MOD_neigh.o MOD_pme.o MOD_utilgpu.o
GPU/source/depend.mk:empole3gpu.o: MOD_neigh.o MOD_pme.o MOD_utilgpu.o
GPU/source/depend.mk:energy.o: MOD_ani.o MOD_utilgpu.o
GPU/source/depend.mk:eopbend1gpu.o: ker_opbend.inc.f
GPU/source/depend.mk:eopbend3gpu.o: ker_opbend.inc.f
GPU/source/depend.mk:epolar1gpu.o: MOD_neigh.o MOD_pme.o MOD_utilgpu.o
GPU/source/depend.mk:epolar3gpu.o: MOD_neigh.o MOD_pme.o MOD_utilgpu.o
GPU/source/depend.mk:estrbnd1gpu.o: ker_strbnd.inc.f
GPU/source/depend.mk:estrtor1gpu.o:
GPU/source/depend.mk:etors1gpu.o: ker_tors.inc.f
GPU/source/depend.mk:etortor1gpu.o:
GPU/source/depend.mk:eurey1gpu.o: ker_urey.inc.f
GPU/source/depend.mk:eurey3gpu.o: ker_urey.inc.f
GPU/source/depend.mk:evcorr.o: MOD_vdw.o MOD_utilgpu.o
GPU/source/depend.mk:final.o: MOD_ani.o MOD_neigh.o MOD_pme.o MOD_utilgpu.o MOD_vdw.o
GPU/source/depend.mk:gradient.o: MOD_ani.o MOD_utilgpu.o
GPU/source/depend.mk:initial.o: MOD_beads.o MOD_neigh.o MOD_utilgpu.o
GPU/source/depend.mk:kangtor.o: MOD_utilgpu.o
GPU/source/depend.mk:kcharge.o: MOD_neigh.o MOD_utilgpu.o
GPU/source/depend.mk:kgeom.o: MOD_vdw.o MOD_utilgpu.o
GPU/source/depend.mk:kimprop.o: MOD_utilgpu.o
GPU/source/depend.mk:kmpole.o: MOD_neigh.o MOD_pme.o MOD_utilgpu.o
GPU/source/depend.mk:kopbend.o: MOD_utilgpu.o
GPU/source/depend.mk:kpitors.o: MOD_utilgpu.o
GPU/source/depend.mk:kpolar.o: MOD_neigh.o MOD_pme.o MOD_utilgpu.o
GPU/source/depend.mk:kscalfactor.o: MOD_neigh.o MOD_utilgpu.o MOD_vdw.o
GPU/source/depend.mk:kstrbnd.o: MOD_utilgpu.o
GPU/source/depend.mk:kstrtor.o: MOD_utilgpu.o
GPU/source/depend.mk:ktortor.o: MOD_utilgpu.o
GPU/source/depend.mk:kurey.o: MOD_utilgpu.o MOD_urey.o MOD_urypot.o MOD_kurybr.o
GPU/source/depend.mk:kvdw.o: MOD_neigh.o MOD_utilgpu.o MOD_vdw.o
GPU/source/depend.mk:linalg.o: MOD_utilgpu.o
GPU/source/depend.mk:mdinit.o: MOD_beads.o MOD_neigh.o MOD_qtb.o MOD_utilgpu.o
GPU/source/depend.mk:mechanic.o: MOD_neigh.o MOD_sizes.o MOD_utilgpu.o MOD_vdw.o
GPU/source/depend.mk:minimize.o: MOD_utilgpu.o
GPU/source/depend.mk:mpistuff.o: MOD_neigh.o MOD_pme.o MOD_sizes.o MOD_utilcomm.o MOD_utilgpu.o
GPU/source/depend.mk:mpiUtils.gpu.o: MOD_domdec.o MOD_utilcomm.o
GPU/source/depend.mk:nblist.o: MOD_ani.o MOD_neigh.o MOD_pme.o MOD_utilgpu.o MOD_vdw.o
GPU/source/depend.mk:nblistgpu.o: MOD_neigh.o MOD_pme.o MOD_sizes.o MOD_utilgpu.o MOD_vdw.o
GPU/source/depend.mk:nblist_vdw.gpu.o: MOD_neigh.o MOD_utilgpu.o
GPU/source/depend.mk:newinduce_pmegpu.o: MOD_pme.o MOD_utilgpu.o
GPU/source/depend.mk:newinduce_pme2gpu.o: MOD_pme.o MOD_sizes.o MOD_utilgpu.o
GPU/source/depend.mk:newinduce_shortrealgpu.o: MOD_pme.o MOD_sizes.o MOD_utilcomm.o MOD_utilgpu.o
GPU/source/depend.mk:orthogonalize.o: MOD_orthogonalize.o MOD_utilgpu.o
GPU/source/depend.mk:pmestuff.o: MOD_pme.o MOD_sizes.o MOD_utilgpu.o
GPU/source/depend.mk:pmestuffgpu.o: MOD_neigh.o MOD_sizes.o MOD_pme.o MOD_utilgpu.o
GPU/source/depend.mk:respa.o: MOD_ani.o MOD_utilgpu.o
GPU/source/depend.mk:respa1.o: MOD_ani.o MOD_utilgpu.o
GPU/source/depend.mk:rotpolegpu.o: MOD_pme.o MOD_utilgpu.o
GPU/source/depend.mk:tmatxb_pmegpu.o: MOD_neigh.o MOD_utilgpu.o
GPU/source/depend.mk:torquegpu.o: MOD_utilgpu.o
GPU/source/depend.mk:torsions.o: MOD_utilgpu.o
GPU/source/depend.mk:verlet.o: MOD_ani.o MOD_utilgpu.o
GPU/README.md:Tinker-HP: High Performance Multi-GPUs Massively Parallel Evolution of Tinker
GPU/README.md:<b>This phase-advance GPU version (1.2 ++) is not (yet) an official release of Tinker-HP but is made freely available in link with the COVID-19 HPC community effort.</b>
GPU/README.md:In addition to GitHub, a [GPU container](https://ngc.nvidia.com/catalog/containers/hpc:tinkerhp) (quick install!) is available thanks to NVIDIA on the NVIDIA NGC's website: 
GPU/README.md:   -  [Build Tinker-HP (GPU version)](build.md)
GPU/README.md:## Run Tinker-HP (CPU/GPU)
GPU/README.md:There is no difference between the use of Tinker-HP and Tinker-HP (GPU version) as long as the feature you are looking for is available on the GPU version. The present version is optimized to accelerate simulations using the AMOEBA polarizable force field. Some minimal non-polarizable capabilities are present (enhanced support will be available in 2021). The code has been extensively tested on 1080, 2080, 3090, P100, V100 and A100 NVIDIA GPU cards and support multi-GPUs computations. It will be part of the major Tinker-HP 1.3 2022 release but this present version will continue to evolve. 
GPU/README.md:### GPU available features
GPU/README.md:@article{2021tinkerhpGPU,
GPU/README.md:  title={Tinker-HP: Accelerating molecular dynamics simulations of large complex systems with advanced point dipole polarizable force fields using GPUs and multi-GPU systems},
GPU/tcl8.6.13/ChangeLog:	* generic/tclOOMethod.c (procMethodType): Now that introspection can
GPU/tcl8.6.13/tests/socket.test:    tcltest::DebugPuts 2 "== test \[$::localhost\]:$port $testmode =="
GPU/tcl8.6.13/tests/socket.test:      tcltest::DebugPuts 2 "** trma / $::count ** $args **"
GPU/tcl8.6.13/tests/socket.test:      tcltest::DebugPuts 2 "** iter / $::count **"
GPU/tcl8.6.13/tests/socket.test:    tcltest::DebugPuts 2 "== stop / $::count =="
GPU/tcl8.6.13/tests/winPipe.test:	    tcltest::DebugPuts 4 "  ## test exec [file extension [lindex $cmd 0]] ($cmd) for\n  ##   $args"
GPU/tcl8.6.13/unix/tclUnixFile.c:	while (TclIsSpaceProcM(*p)) {
GPU/tcl8.6.13/unix/dltest/Makefile:Makedir =  /home/user/tinkerhp/GPU/tcl8.6.13/unix/
GPU/tcl8.6.13/library/tcltest/tcltest.tcl:# tcltest::DebugPuts --
GPU/tcl8.6.13/library/tcltest/tcltest.tcl:proc tcltest::DebugPuts {level string} {
GPU/tcl8.6.13/library/tcltest/tcltest.tcl:    DebugPuts 3 "entering testConstraint $constraint $value"
GPU/tcl8.6.13/library/tcltest/tcltest.tcl:    DebugPuts 3 "entering SafeFetch $n1 $n2 $op"
GPU/tcl8.6.13/library/tcltest/tcltest.tcl:    DebugPuts 3 "entering ConstraintInitializer $constraint $script"
GPU/tcl8.6.13/library/tcltest/tcltest.tcl:    DebugPuts 2 "Flags passed into tcltest:"
GPU/tcl8.6.13/library/tcltest/tcltest.tcl:	DebugPuts 2 \
GPU/tcl8.6.13/library/tcltest/tcltest.tcl:	DebugPuts 2 "    argv: $::argv"
GPU/tcl8.6.13/library/tcltest/tcltest.tcl:    DebugPuts    2 "tcltest::debug              = [debug]"
GPU/tcl8.6.13/library/tcltest/tcltest.tcl:    DebugPuts    2 "tcltest::testsDirectory     = [testsDirectory]"
GPU/tcl8.6.13/library/tcltest/tcltest.tcl:    DebugPuts    2 "tcltest::workingDirectory   = [workingDirectory]"
GPU/tcl8.6.13/library/tcltest/tcltest.tcl:    DebugPuts    2 "tcltest::temporaryDirectory = [temporaryDirectory]"
GPU/tcl8.6.13/library/tcltest/tcltest.tcl:    DebugPuts    2 "tcltest::outputChannel      = [outputChannel]"
GPU/tcl8.6.13/library/tcltest/tcltest.tcl:    DebugPuts    2 "tcltest::errorChannel       = [errorChannel]"
GPU/tcl8.6.13/library/tcltest/tcltest.tcl:    DebugPuts    2 "Original environment (tcltest::originalEnv):"
GPU/tcl8.6.13/library/tcltest/tcltest.tcl:    DebugPuts    2 "Constraints:"
GPU/tcl8.6.13/library/tcltest/tcltest.tcl:    DebugPuts 3 "[lindex [info level 0] 0] called"
GPU/tcl8.6.13/library/tcltest/tcltest.tcl:    DebugPuts 3 "test $name $args"
GPU/tcl8.6.13/library/tcltest/tcltest.tcl:    DebugPuts 3 "Running $name {$script}"
GPU/tcl8.6.13/library/tcltest/tcltest.tcl:	DebugPuts 1 "No test directories remain after applying match\
GPU/tcl8.6.13/library/tcltest/tcltest.tcl:    DebugPuts  2 "[lindex [info level 0] 0]: $saveState"
GPU/tcl8.6.13/library/tcltest/tcltest.tcl:	    DebugPuts 2 "[lindex [info level 0] 0]: Removing proc $p"
GPU/tcl8.6.13/library/tcltest/tcltest.tcl:	    DebugPuts 2 "[lindex [info level 0] 0]:\
GPU/tcl8.6.13/library/tcltest/tcltest.tcl:    DebugPuts 3 "[lindex [info level 0] 0]:\
GPU/tcl8.6.13/library/tcltest/tcltest.tcl:    DebugPuts 3 "[lindex [info level 0] 0]: removing $fullName"
GPU/tcl8.6.13/library/tcltest/tcltest.tcl:    DebugPuts 3 "[lindex [info level 0] 0]: creating $fullName"
GPU/tcl8.6.13/library/tcltest/tcltest.tcl:    DebugPuts 3 "[lindex [info level 0] 0]: deleting $fullName"
GPU/tcl8.6.13/pkgs/itcl4.2.3/generic/itcl2TclOO.h:MODULE_SCOPE Tcl_Method Itcl_NewProcMethod(Tcl_Interp *interp, Tcl_Object oPtr,
GPU/tcl8.6.13/pkgs/itcl4.2.3/generic/itclInfo.c:struct NameProcMap2 {
GPU/tcl8.6.13/pkgs/itcl4.2.3/generic/itclInfo.c:static const struct NameProcMap2 infoCmdsDelegated2[] = {
GPU/tcl8.6.13/pkgs/itcl4.2.3/generic/itclBase.c:    Tcl_InitHashTable(&infoPtr->procMethods, TCL_ONE_WORD_KEYS);
GPU/tcl8.6.13/pkgs/itcl4.2.3/generic/itclBase.c:    Tcl_DeleteHashTable(&infoPtr->procMethods);
GPU/tcl8.6.13/pkgs/itcl4.2.3/generic/itclInt.h:    Tcl_HashTable procMethods;      /* maps from procPtr to mFunc */
GPU/tcl8.6.13/pkgs/itcl4.2.3/generic/itclMethod.c:	hPtr = Tcl_CreateHashEntry(&imPtr->iclsPtr->infoPtr->procMethods,
GPU/tcl8.6.13/pkgs/itcl4.2.3/generic/itclMethod.c:	    hPtr = Tcl_FindHashEntry(&imPtr->iclsPtr->infoPtr->procMethods,
GPU/tcl8.6.13/pkgs/itcl4.2.3/generic/itclClass.c:    hPtr = Tcl_FindHashEntry(&imPtr->infoPtr->procMethods,
GPU/tcl8.6.13/pkgs/itcl4.2.3/generic/itclObject.c:		            &imPtr->iclsPtr->infoPtr->procMethods,
GPU/tcl8.6.13/pkgs/itcl4.2.3/generic/itclParse.c:	    hPtr2 = Tcl_CreateHashEntry(&iclsPtr->infoPtr->procMethods,
GPU/tcl8.6.13/pkgs/itcl4.2.3/generic/itclParse.c:		 * entry in the procMethods map based on the old one.
GPU/tcl8.6.13/pkgs/itcl4.2.3/generic/itclParse.c:	        imPtr->tmPtr = Itcl_NewProcMethod(interp,
GPU/tcl8.6.13/pkgs/itcl4.2.3/generic/itcl2TclOO.c:    result = TclOONewProcMethodEx(interp, clsPtr, preCallPtr, postCallPtr,
GPU/tcl8.6.13/pkgs/itcl4.2.3/generic/itcl2TclOO.c: * Itcl_NewProcMethod --
GPU/tcl8.6.13/pkgs/itcl4.2.3/generic/itcl2TclOO.c:Itcl_NewProcMethod(
GPU/tcl8.6.13/pkgs/thread2.8.8/tests/French.txt:envy	envidia[Noun]
GPU/tcl8.6.13/pkgs/thread2.8.8/tests/French.txt:envy	envidiar[Verb]
GPU/tcl8.6.13/generic/tclParse.c:		if (openBrace && TclIsSpaceProcM(src[-1])) {
GPU/tcl8.6.13/generic/tclUtil.c:    count += 1 - TclIsSpaceProcM(*bytes);
GPU/tcl8.6.13/generic/tclUtil.c:	if (TclIsSpaceProcM(*bytes)) {
GPU/tcl8.6.13/generic/tclUtil.c:	    } while (numBytes && TclIsSpaceProcM(*bytes));
GPU/tcl8.6.13/generic/tclUtil.c:    count -= TclIsSpaceProcM(bytes[-1]);
GPU/tcl8.6.13/generic/tclUtil.c:    while ((p < limit) && (TclIsSpaceProcM(*p))) {
GPU/tcl8.6.13/generic/tclUtil.c:		if ((p >= limit) || TclIsSpaceProcM(*p)) {
GPU/tcl8.6.13/generic/tclUtil.c:		    while ((p2 < limit) && (!TclIsSpaceProcM(*p2))
GPU/tcl8.6.13/generic/tclUtil.c:		if ((p >= limit) || TclIsSpaceProcM(*p)) {
GPU/tcl8.6.13/generic/tclUtil.c:		    while ((p2 < limit) && (!TclIsSpaceProcM(*p2))
GPU/tcl8.6.13/generic/tclUtil.c:	    if (TclIsSpaceProcM(*p)) {
GPU/tcl8.6.13/generic/tclUtil.c:    while ((p < limit) && (TclIsSpaceProcM(*p))) {
GPU/tcl8.6.13/generic/tclUtil.c:	    if (TclIsSpaceProcM(*p)) {
GPU/tcl8.6.13/generic/tclUtil.c:	while ((--dst >= dsPtr->string) && TclIsSpaceProcM(*dst)) {
GPU/tcl8.6.13/generic/tclUtil.c:    if (TclIsSpaceProcM(*end)) {
GPU/tcl8.6.13/generic/tclUtil.c:    while (length && TclIsSpaceProcM(*bytes)) {
GPU/tcl8.6.13/generic/tclUtil.c:	if (TclIsSpaceProcM(opPtr[1])) {
GPU/tcl8.6.13/generic/tclUtil.c:	if (TclIsSpaceProcM(bytes[4])) {
GPU/tcl8.6.13/generic/tclUtil.c:    while (TclIsSpaceProcM(*p)) {
GPU/tcl8.6.13/generic/tclUtil.c:	while (TclIsSpaceProcM(*p)) {
GPU/tcl8.6.13/generic/tclGetDate.y:	while (TclIsSpaceProcM(*yyInput)) {
GPU/tcl8.6.13/generic/tclOOMethod.c:static const Tcl_MethodType procMethodType = {
GPU/tcl8.6.13/generic/tclOOMethod.c:	    argsObj, bodyObj, &procMethodType, pmPtr, &pmPtr->procPtr);
GPU/tcl8.6.13/generic/tclOOMethod.c: * TclOONewProcMethod --
GPU/tcl8.6.13/generic/tclOOMethod.c:TclOONewProcMethod(
GPU/tcl8.6.13/generic/tclOOMethod.c:    method = TclOOMakeProcMethod(interp, clsPtr, flags, nameObj, procName,
GPU/tcl8.6.13/generic/tclOOMethod.c:	    argsObj, bodyObj, &procMethodType, pmPtr, &pmPtr->procPtr);
GPU/tcl8.6.13/generic/tclOOMethod.c: * TclOOMakeProcMethod --
GPU/tcl8.6.13/generic/tclOOMethod.c:TclOOMakeProcMethod(
GPU/tcl8.6.13/generic/tclOOMethod.c:    if (mPtr->typePtr == &procMethodType) {
GPU/tcl8.6.13/generic/tclOOMethod.c:    if (mPtr->typePtr == &procMethodType) {
GPU/tcl8.6.13/generic/tclOOMethod.c:TclOONewProcMethodEx(
GPU/tcl8.6.13/generic/tclOOMethod.c:    Tcl_Method method = (Tcl_Method) TclOONewProcMethod(interp,
GPU/tcl8.6.13/generic/tclOO.decls:    Tcl_Method TclOOMakeProcMethod(Tcl_Interp *interp, Class *clsPtr,
GPU/tcl8.6.13/generic/tclOO.decls:    Method *TclOONewProcMethod(Tcl_Interp *interp, Class *clsPtr,
GPU/tcl8.6.13/generic/tclOO.decls:    Tcl_Method TclOONewProcMethodEx(Tcl_Interp *interp, Tcl_Class clsPtr,
GPU/tcl8.6.13/generic/tclOOIntDecls.h:TCLAPI Tcl_Method	TclOOMakeProcMethod(Tcl_Interp *interp,
GPU/tcl8.6.13/generic/tclOOIntDecls.h:TCLAPI Method *		TclOONewProcMethod(Tcl_Interp *interp, Class *clsPtr,
GPU/tcl8.6.13/generic/tclOOIntDecls.h:TCLAPI Tcl_Method	TclOONewProcMethodEx(Tcl_Interp *interp,
GPU/tcl8.6.13/generic/tclOOIntDecls.h:    Tcl_Method (*tclOOMakeProcMethod) (Tcl_Interp *interp, Class *clsPtr, int flags, Tcl_Obj *nameObj, const char *namePtr, Tcl_Obj *argsObj, Tcl_Obj *bodyObj, const Tcl_MethodType *typePtr, void *clientData, Proc **procPtrPtr); /* 2 */
GPU/tcl8.6.13/generic/tclOOIntDecls.h:    Method * (*tclOONewProcMethod) (Tcl_Interp *interp, Class *clsPtr, int flags, Tcl_Obj *nameObj, Tcl_Obj *argsObj, Tcl_Obj *bodyObj, ProcedureMethod **pmPtrPtr); /* 4 */
GPU/tcl8.6.13/generic/tclOOIntDecls.h:    Tcl_Method (*tclOONewProcMethodEx) (Tcl_Interp *interp, Tcl_Class clsPtr, TclOO_PreCallProc *preCallPtr, TclOO_PostCallProc *postCallPtr, ProcErrorProc *errProc, void *clientData, Tcl_Obj *nameObj, Tcl_Obj *argsObj, Tcl_Obj *bodyObj, int flags, void **internalTokenPtr); /* 10 */
GPU/tcl8.6.13/generic/tclOOIntDecls.h:#define TclOOMakeProcMethod \
GPU/tcl8.6.13/generic/tclOOIntDecls.h:	(tclOOIntStubsPtr->tclOOMakeProcMethod) /* 2 */
GPU/tcl8.6.13/generic/tclOOIntDecls.h:#define TclOONewProcMethod \
GPU/tcl8.6.13/generic/tclOOIntDecls.h:	(tclOOIntStubsPtr->tclOONewProcMethod) /* 4 */
GPU/tcl8.6.13/generic/tclOOIntDecls.h:#define TclOONewProcMethodEx \
GPU/tcl8.6.13/generic/tclOOIntDecls.h:	(tclOOIntStubsPtr->tclOONewProcMethodEx) /* 10 */
GPU/tcl8.6.13/generic/tclOO.c:    TclOONewProcMethod(interp, fPtr->objectCls, 0, fPtr->clonedName, argsPtr,
GPU/tcl8.6.13/generic/tclOOStubInit.c:    TclOOMakeProcMethod, /* 2 */
GPU/tcl8.6.13/generic/tclOOStubInit.c:    TclOONewProcMethod, /* 4 */
GPU/tcl8.6.13/generic/tclOOStubInit.c:    TclOONewProcMethodEx, /* 10 */
GPU/tcl8.6.13/generic/tclResult.c:	while ((--dst >= iPtr->appendResult) && TclIsSpaceProcM(*dst)) {
GPU/tcl8.6.13/generic/tclCmdMZ.c:		    while (TclIsSpaceProcM(*p)) {
GPU/tcl8.6.13/generic/tclInt.h:#	define TclIsSpaceProcM(byte) \
GPU/tcl8.6.13/generic/tclDate.c:	while (TclIsSpaceProcM(*yyInput)) {
GPU/tcl8.6.13/generic/tclOODefineCmds.c:	method = (Tcl_Method) TclOONewProcMethod(interp, clsPtr,
GPU/tcl8.6.13/generic/tclOODefineCmds.c:	method = (Tcl_Method) TclOONewProcMethod(interp, clsPtr,
GPU/tcl8.6.13/generic/tclOODefineCmds.c:	if (TclOONewProcMethod(interp, oPtr->classPtr, isPublic, objv[1],
GPU/tcl8.6.13/generic/tclStrToD.c:	    if (TclIsSpaceProcM(c)) {
GPU/tcl8.6.13/generic/tclStrToD.c:	    if (TclIsSpaceProcM(c)) {
GPU/tcl8.6.13/generic/tclStrToD.c:	    while (len != 0 && TclIsSpaceProcM(*p)) {
GPU/tcl8.6.13/generic/tclCmdAH.c:	    if (TclIsSpaceProcM(*p) || (*p == '\\')) {
GPU/tcl8.6.13/generic/tclUtf.c:	return TclIsSpaceProcM((char) ch);
GPU/tinkerml.tensorflow.yaml:  - pycuda
GPU/tinkerml.tensorflow.yaml:  - deepmd-kit=*=*gpu
GPU/tinkerml.tensorflow.yaml:  - libdeepmd=*=*gpu
GPU/tinkerml.torch.yaml:  - nvidia
GPU/tinkerml.torch.yaml:  - pycuda
GPU/tinkerml.torch.yaml:  - pytorch-cuda=11.8
GPU/wrappers/csort.cu:   void sort_int_async_wrapper ( int* data, int N, cudaStream_t s)
GPU/wrappers/csort.cu:      thrust::sort(thrust::cuda::par(thrust_cache).on(s), dev_ptr, dev_ptr+N);
GPU/wrappers/csort.cu:   void exclusive_scan_int_async_wrapper ( int* in, int N, int* out, cudaStream_t s)
GPU/wrappers/csort.cu:      thrust::exclusive_scan (thrust::cuda::par(thrust_cache).on(s),dev_in,dev_in+N,dev_out);
GPU/wrappers/csort.cu:   void stable_sort_by_key_int_async_wrapper ( int* in, int N, int* values, cudaStream_t s )
GPU/wrappers/csort.cu:      thrust::stable_sort_by_key (thrust::cuda::par(thrust_cache).on(s),dev_in,dev_in+N,dev_values);
GPU/wrappers/csort.cu:   void remove_zero_async_wrapper (int* in, int N, cudaStream_t s)
GPU/wrappers/csort.cu:      thrust::remove(thrust::cuda::par(thrust_cache).on(s),dev_in,dev_in+N,0);
GPU/wrappers/csort.cu:   void remove_async_wrapper (int* in, int N, int value, int* new_last, cudaStream_t s)
GPU/wrappers/csort.cu:            thrust::remove(thrust::cuda::par(thrust_cache).on(s),dev_in,dev_in+N,value));
GPU/wrappers/Makefile:CUDACC   = nvcc
GPU/wrappers/Makefile:#CUDA_DIR  = /usr/local/cuda-10.0
GPU/wrappers/Makefile:#CUDA_DIR  = $(CUDAROOT)
GPU/wrappers/Makefile:#CUDA_DIR  = $(PGI)/linux86-64-llvm/2019/cuda/10.0
GPU/wrappers/Makefile:FCFLAGS   = -O3 -Mcuda=cc60,cc70#-rc=rc4.0
GPU/wrappers/Makefile:#INCLUDE       = -I$(CUDA_DIR)/include
GPU/wrappers/Makefile:	$(CUDACC) $(CUFLAGS) -c $< $(INCLUDE)
GPU/wrappers/thrust_module.f90:    use cudafor
GPU/wrappers/thrust_module.f90:    integer(cuda_count_kind),private :: numbytes=0
GPU/wrappers/thrust_module.f90:         import  c_int,cuda_stream_kind
GPU/wrappers/thrust_module.f90:         integer(cuda_stream_kind),value :: stream
GPU/wrappers/thrust_module.f90:         import  c_int,cuda_stream_kind
GPU/wrappers/thrust_module.f90:         integer(cuda_stream_kind),value  :: s
GPU/wrappers/thrust_module.f90:         import  c_int,cuda_stream_kind
GPU/wrappers/thrust_module.f90:         integer(cuda_stream_kind),value  :: s
GPU/wrappers/thrust_module.f90:         import c_int,cuda_stream_kind
GPU/wrappers/thrust_module.f90:         integer(cuda_stream_kind),value  :: s
GPU/wrappers/thrust_module.f90:          import c_int,cuda_stream_kind
GPU/wrappers/thrust_module.f90:          integer(cuda_stream_kind),value :: s
GPU/wrappers/thrust_module.f90:        import cuda_count_kind,c_devptr
GPU/wrappers/thrust_module.f90:        integer(cuda_count_kind),value::n
GPU/wrappers/thrust_module.f90:    ierr     = cudaMalloc(thrust_cache_ptr, numbytes)
GPU/wrappers/thrust_module.f90:    if (ierr.ne.cudasuccess) print 30, ierr,cudageterrorstring(ierr)
GPU/wrappers/thrust_module.f90:    if (mem_alloc) ierr = cudaFree(thrust_cache_ptr)
GPU/wrappers/thrust_module.f90:    if (ierr.ne.cudasuccess) print 20, ierr,cudageterrorstring(ierr)
GPU/wrappers/thrust_cache.h:#include <thrust/system/cuda/vector.h>
GPU/wrappers/thrust_cache.h:            // create a new one with cuda::malloc
GPU/wrappers/thrust_cache.h:            // try if cuda::malloc can't satisfy the request
GPU/wrappers/thrust_cache.h:            try { ptr  = thrust::cuda::malloc<char>(num_bytes).get(); }
GPU/wrappers/thrust_cache.h:            // transform the pointer to cuda::pointer before calling cuda::free
GPU/wrappers/thrust_cache.h:            thrust::cuda::free(thrust::cuda::pointer<char>(ptr)); 
GPU/wrappers/thrust_cache.h:            try { ptr  = thrust::cuda::malloc<char>(num_bytes).get(); }
GPU/build.md:# Build Tinker-HP (GPU)
GPU/build.md:For most users, we recommend this relatively accessible method which lies on a bash installation script named `/tinker-hp/GPU/ci/install/sh`. After setting your environment according to the [Prerequisites](Prerequisites.md), change directory to the GPU folder, edit the configuration section of the installation script to select your build and proceed with the commands below.
GPU/build.md:#> /home/$user/.../tinker-hp/GPU
GPU/build.md:  - `target_arch`  control the target platform architecture. Decide whether the build should target CPUs or GPUs
GPU/build.md:  - `c_c`   Stand for compute capability. It is specific to the GPU device. Default option target every Nvidia GPU from Pascal architecture to Ampere.
GPU/build.md:  Requires a CUDA python environment consistent with the installation one before building.
GPU/build.md:This part concerns every developers or anyone familiar enough with source code compilation. `GPU/source/Makefile.pgi` allows every type of build. By editing configurations variables you can change default compilers, adapts flags, rename libraries, and so on. Here is the description of the useful variables need to configure your own build's.
GPU/build.md:  - `NVSHMEM_SUPPORT` enables build with nvshmem library if set to 1. This option requires `NVSHMEM_HOME` variable to be set on nvshmem install directory. It has not been tested with recent Nvidia HPC package.
GPU/build.md:  - `arch=(host|[device]) .or. (cpu|[gpu])` allows you to select target architecture
GPU/build.md:  For instance `analyze` or `analyze.gpu`
GPU/build.md:  - `cuda_version=([10.1])` contains the cuda version to be used for construct.  
GPU/build.md:    _PGI 19.10_ compilers only supports _cuda 9.2 10.0 and 10.1_ for OpenACC. To be consistent with the construct, we strongly recommend to use the same version of CUDA compiler as your OpenACC compiler.
GPU/build.md:    Build the wrapper on CUDA thrust library
GPU/build.md:    [Re]Build the wrapper on CUDA thrust library
GPU/build.md:#> /home/$user/.../tinker-hp/GPU
GPU/build.md:make FPA_SUPPORT=1 COLVARS_SUPPORT=1 prec=mixed arch=device compute_capability=60 cuda_version=11.3 prog_suffix=.gmix $@
GPU/examples/Deep-HP_example5.key:## Parallel setup (to be used only with multiple GPUs)
GPU/examples/dhfr2.key:## Parallel setup (to be used only with multiple GPUs)
GPU/examples/Deep-HP_example2.key:## Parallel setup (to be used only with multiple GPUs)
GPU/examples/cox.key:polar-alg             1 #use of PCG with GPUs is mandatory
GPU/examples/Deep-HP_example4.key:## Parallel setup (to be used only with multiple GPUs)
GPU/examples/Deep-HP_example3.key:## Parallel setup (to be used only with multiple GPUs)
GPU/examples/Deep-HP_example1.key:## Parallel setup (to be used only with multiple GPUs)
GPU/colvars/colvaratoms.h: *          branch prediction is broken (or further migration to GPU code).
GPU/colvars/colvarmodule_refs.h:    "  title = {Scalable molecular dynamics on {CPU} and {GPU} architectures with {NAMD}},\n"
README.md:Tinker-HP: High-Performance Massively Parallel Evolution of Tinker on CPUs & GPUs
README.md:- **Update 02/2021:** **PLUMED** Support for version 1.2 GPUs
README.md:Current Github version: 1.1v (enhanced AVX512 vectorized CPUs version), 1.2 (CPUs) + 1.2 (multi)-GPUs  
README.md:Current Development version: 1.3 (CPUs + multi-GPUs)
README.md:In addition to GitHub, a GPUs container (quick install!) is available thanks to NVIDIA on the [*NVIDIA NGC's website.*](https://ngc.nvidia.com/catalog/containers/hpc:tinkerhp)
README.md:**Tinker-HP** is a **CPUs and GPUs** based, multi-precision, **MPI** massively parallel package dedicated to long **polarizable molecular dynamics** simulations and to polarizable **QM/MM**. Tinker-HP is an evolution of the popular Tinker package that conserves it simplicity of use but brings new capabilities allowing performing very long molecular dynamics simulations on modern supercomputers that use thousands of cores. The Tinker-HP approach offers various strategies using domain decomposition techniques for periodic boundary conditions in the framework of the *(n)log(n) Smooth Particle Mesh Ewald*. Tinker-HP proposes a high performance scalable computing environment for polarizable **(AMOEBA, Amberpol...)** and classical **(Amber, Charmm, OPLS...) force fields** giving access to large systems up to **millions of atoms**. It can be used on supercomputers as well as on lab clusters. Tinker-HP supports **Intel** (**AVX-512** enhanced version) and AMD CPUs platforms as well as **NVIDIA GPUs** *(GTX-10xx, RTX-20xx, 30xx, 40xx, P100, V100, A100)*. 
README.md:- If you use the **GPUs version**, please also cite:  
README.md:[*Tinker-HP : Accelerating Molecular Dynamics Simulations of Large Complex Systems with Advanced Point Dipole Polarizable Force Fields using GPUs and Multi-GPUs systems Olivier Adjoua, Louis Lagardre, Luc-Henri Jolly, Arnaud Durocher, Thibaut Very, Isabelle Dupays, Zhi Wang, Tho Jaffrelot Inizan, Frdric Clerse, Pengyu Ren, Jay W. Ponder, Jean-Philip Piquemal, J. Chem. Theory. Comput., 2021, 17 (4), 20342053 (Open Access)*](https://doi.org/10.1021/acs.jctc.0c01164)
LiveCOMS/lipsum.dtx:  \seq_gput_right:Nn{\g_lipsum_paragraph_seq}{#1}
LiveCOMS/lipsum.dtx:\NewLipsumPar{Quam autem ego dicam voluptatem, iam videtis, ne invidia verbi
LiveCOMS/lipsum.dtx:\NewLipsumPar{Atque ut odia, invidiae, despicationes adversantur voluptatibus,
LiveCOMS/lipsum.dtx:  Iam contemni non poteris. Odium autem et invidiam facile vitabis. Ad eas enim
LiveCOMS/Tinkerperf.bib:abstract = {GENeralized-Ensemble SImulation System (GENESIS) is a software package for molecular dynamics (MD) simulation of biological systems. It is designed to extend limitations in system size and accessible time scale by adopting highly parallelized schemes and enhanced conformational sampling algorithms. In this new version, GENESIS 1.1, new functions and advanced algorithms have been added. The all-atom and coarse-grained potential energy functions used in AMBER and GROMACS packages now become available in addition to CHARMM energy functions. The performance of MD simulations has been greatly improved by further optimization, multiple time-step integration, and hybrid (CPU+GPU) computing. The string method and replica-exchange umbrella sampling with flexible collective variable choice are used for finding the minimum free-energy pathway and obtaining free-energy profiles for conformational changes of a macromolecule. These new features increase the usefulness and power of GENESIS for modeling and simulation in biological research.  2017 Wiley Periodicals, Inc.},
LiveCOMS/Tinkerperf.bib:keywords = "Molecular dynamics, GPU, SIMD, Free energy",
LiveCOMS/Tinkerperf.bib:abstract = "GROMACS is one of the most widely used open-source and free software codes in chemistry, used primarily for dynamical simulations of biomolecules. It provides a rich set of calculation types, preparation and analysis tools. Several advanced techniques for free-energy calculations are supported. In version 5, it reaches new performance heights, through several new and enhanced parallelization algorithms. These work on every level; SIMD registers inside cores, multithreading, heterogeneous CPUGPU acceleration, state-of-the-art 3D domain decomposition, and ensemble-level parallelization through built-in replica exchange and the separate Copernicus framework. The latest best-in-class compressed trajectory storage format is supported."
v1.2/plumed2/configure.ac:PLUMED_CONFIG_ENABLE([af_cuda],[search for arrayfire_cuda],[no])
v1.2/plumed2/configure.ac:  PLUMED_CHECK_PACKAGE([arrayfire.h],[af_is_double],[__PLUMED_HAS_ARRAYFIRE],[afopencl])
v1.2/plumed2/configure.ac:  PLUMED_CHECK_PACKAGE([arrayfire.h],[af_is_double],[__PLUMED_HAS_ARRAYFIRE_OCL],[afopencl])
v1.2/plumed2/configure.ac:if test "$af_cuda" = true ; then
v1.2/plumed2/configure.ac:  PLUMED_CHECK_PACKAGE([arrayfire.h],[af_is_double],[__PLUMED_HAS_ARRAYFIRE],[afcuda])
v1.2/plumed2/configure.ac:  PLUMED_CHECK_PACKAGE([arrayfire.h],[af_is_double],[__PLUMED_HAS_ARRAYFIRE_CUDA],[afcuda])
v1.2/plumed2/patches/namd-2.12.diff: 	$(CUDALIB) \
v1.2/plumed2/patches/namd-2.12.diff: #include "DeviceCUDA.h"
v1.2/plumed2/patches/namd-2.12.diff: #ifdef NAMD_CUDA
v1.2/plumed2/patches/qespresso-7.0.diff/PW/src/forces.f90.preplumed:  USE control_flags,     ONLY : use_gpu
v1.2/plumed2/patches/qespresso-7.0.diff/PW/src/forces.f90.preplumed:#if defined(__CUDA)
v1.2/plumed2/patches/qespresso-7.0.diff/PW/src/forces.f90.preplumed:  IF (ierr .ne. 0) CALL infomsg('forces', 'Cannot reset GPU buffers! Some buffers still locked.')
v1.2/plumed2/patches/qespresso-7.0.diff/PW/src/forces.f90.preplumed:  IF (.not. use_gpu) CALL force_us( forcenl )
v1.2/plumed2/patches/qespresso-7.0.diff/PW/src/forces.f90.preplumed:  IF (      use_gpu) CALL force_us_gpu( forcenl )
v1.2/plumed2/patches/qespresso-7.0.diff/PW/src/forces.f90.preplumed:  IF (.not. use_gpu) & ! On the CPU
v1.2/plumed2/patches/qespresso-7.0.diff/PW/src/forces.f90.preplumed:  IF (      use_gpu) THEN ! On the GPU
v1.2/plumed2/patches/qespresso-7.0.diff/PW/src/forces.f90.preplumed:     ! move these data to the GPU
v1.2/plumed2/patches/qespresso-7.0.diff/PW/src/forces.f90.preplumed:     CALL force_lc_gpu( nat, tau, ityp, alat, omega, ngm, ngl, igtongl_d, &
v1.2/plumed2/patches/qespresso-7.0.diff/PW/src/forces.f90.preplumed:  IF (.not. use_gpu) CALL force_cc( forcecc )
v1.2/plumed2/patches/qespresso-7.0.diff/PW/src/forces.f90.preplumed:  IF (      use_gpu) CALL force_cc_gpu( forcecc )
v1.2/plumed2/patches/qespresso-7.0.diff/PW/src/forces.f90.preplumed:  IF (.not. use_gpu) THEN
v1.2/plumed2/patches/qespresso-7.0.diff/PW/src/forces.f90.preplumed:     IF ( lda_plus_u .AND. U_projection.NE.'pseudo' ) CALL force_hub_gpu( forceh )
v1.2/plumed2/patches/qespresso-7.0.diff/PW/src/forces.f90.preplumed:  IF (ierr .ne. 0) CALL errore('forces', 'Cannot reset GPU buffers! Buffers still locked: ', abs(ierr))
v1.2/plumed2/patches/qespresso-7.0.diff/PW/src/forces.f90.preplumed:  IF ( .not. use_gpu ) CALL force_corr( forcescc )
v1.2/plumed2/patches/qespresso-7.0.diff/PW/src/forces.f90.preplumed:  IF (       use_gpu ) CALL force_corr_gpu( forcescc )
v1.2/plumed2/patches/qespresso-7.0.diff/PW/src/run_pwscf.f90:     IF ( ierr .ne. 0 ) CALL infomsg( 'run_pwscf', 'Cannot reset GPU buffers! Some buffers still locked.' )
v1.2/plumed2/patches/qespresso-7.0.diff/PW/src/forces.f90:  USE control_flags,     ONLY : use_gpu
v1.2/plumed2/patches/qespresso-7.0.diff/PW/src/forces.f90:#if defined(__CUDA)
v1.2/plumed2/patches/qespresso-7.0.diff/PW/src/forces.f90:  IF (ierr .ne. 0) CALL infomsg('forces', 'Cannot reset GPU buffers! Some buffers still locked.')
v1.2/plumed2/patches/qespresso-7.0.diff/PW/src/forces.f90:  IF (.not. use_gpu) CALL force_us( forcenl )
v1.2/plumed2/patches/qespresso-7.0.diff/PW/src/forces.f90:  IF (      use_gpu) CALL force_us_gpu( forcenl )
v1.2/plumed2/patches/qespresso-7.0.diff/PW/src/forces.f90:  IF (.not. use_gpu) & ! On the CPU
v1.2/plumed2/patches/qespresso-7.0.diff/PW/src/forces.f90:  IF (      use_gpu) THEN ! On the GPU
v1.2/plumed2/patches/qespresso-7.0.diff/PW/src/forces.f90:     ! move these data to the GPU
v1.2/plumed2/patches/qespresso-7.0.diff/PW/src/forces.f90:     CALL force_lc_gpu( nat, tau, ityp, alat, omega, ngm, ngl, igtongl_d, &
v1.2/plumed2/patches/qespresso-7.0.diff/PW/src/forces.f90:  IF (.not. use_gpu) CALL force_cc( forcecc )
v1.2/plumed2/patches/qespresso-7.0.diff/PW/src/forces.f90:  IF (      use_gpu) CALL force_cc_gpu( forcecc )
v1.2/plumed2/patches/qespresso-7.0.diff/PW/src/forces.f90:  IF (.not. use_gpu) THEN
v1.2/plumed2/patches/qespresso-7.0.diff/PW/src/forces.f90:     IF ( lda_plus_u .AND. U_projection.NE.'pseudo' ) CALL force_hub_gpu( forceh )
v1.2/plumed2/patches/qespresso-7.0.diff/PW/src/forces.f90:  IF (ierr .ne. 0) CALL errore('forces', 'Cannot reset GPU buffers! Buffers still locked: ', abs(ierr))
v1.2/plumed2/patches/qespresso-7.0.diff/PW/src/forces.f90:  IF ( .not. use_gpu ) CALL force_corr( forcescc )
v1.2/plumed2/patches/qespresso-7.0.diff/PW/src/forces.f90:  IF (       use_gpu ) CALL force_corr_gpu( forcescc )
v1.2/plumed2/patches/qespresso-7.0.diff/PW/src/run_pwscf.f90.preplumed:     IF ( ierr .ne. 0 ) CALL infomsg( 'run_pwscf', 'Cannot reset GPU buffers! Some buffers still locked.' )
v1.2/plumed2/patches/qespresso-7.0.diff/Modules/Makefile.preplumed:# GPU versions of modules
v1.2/plumed2/patches/qespresso-7.0.diff/Modules/Makefile.preplumed:  wavefunctions_gpu.o \
v1.2/plumed2/patches/qespresso-7.0.diff/Modules/Makefile.preplumed:  becmod_gpu.o \
v1.2/plumed2/patches/qespresso-7.0.diff/Modules/Makefile.preplumed:  becmod_subs_gpu.o \
v1.2/plumed2/patches/qespresso-7.0.diff/Modules/Makefile.preplumed:  cuda_subroutines.o \
v1.2/plumed2/patches/qespresso-7.0.diff/Modules/Makefile.preplumed:  random_numbers_gpu.o
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:#include "gromacs/domdec/gpuhaloexchange.h"
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:#include "gromacs/ewald/pme_coordinate_receiver_gpu.h"
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:#include "gromacs/ewald/pme_pp_comm_gpu.h"
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:#include "gromacs/gpu_utils/gpu_utils.h"
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:#include "gromacs/listed_forces/listed_forces_gpu.h"
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:#include "gromacs/mdtypes/state_propagator_data_gpu.h"
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:#include "gromacs/nbnxm/gpu_data_mgmt.h"
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:#include "gromacs/nbnxm/nbnxm_gpu.h"
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:#include "gromacs/timing/gpu_timing.h"
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:#include "gpuforcereduction.h"
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:static const bool c_disableAlternatingWait = (getenv("GMX_DISABLE_ALTERNATING_GPU_WAIT") != nullptr);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                   bool                  useGpuPmePpComms,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                   bool                  receivePmeForceToGpu,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    gmx_pme_receive_f(fr->pmePpCommGpu.get(),
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                      useGpuPmePpComms,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                      receivePmeForceToGpu,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    /* GPU kernel launch overhead is already timed separately */
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (!nbv->useGpu())
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:/*! \brief Launch the prepare_step and spread stages of PME GPU.
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:static inline void launchPmeGpuSpread(gmx_pme_t*            pmedata,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                      GpuEventSynchronizer* xReadyOnDevice,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    pme_gpu_prepare_computation(pmedata, box, wcycle, stepWork);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    bool                           useGpuDirectComm         = false;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    gmx::PmeCoordinateReceiverGpu* pmeCoordinateReceiverGpu = nullptr;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    pme_gpu_launch_spread(
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            pmedata, xReadyOnDevice, wcycle, lambdaQ, useGpuDirectComm, pmeCoordinateReceiverGpu);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:/*! \brief Launch the FFT and gather stages of PME GPU
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:static void launchPmeGpuFftAndGather(gmx_pme_t*               pmedata,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    pme_gpu_launch_complex_transforms(pmedata, wcycle, stepWork);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    pme_gpu_launch_gather(pmedata, wcycle, lambdaQ);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed: *  Polling wait for either of the PME or nonbonded GPU tasks.
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed: * Instead of a static order in waiting for GPU tasks, this function
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed: * one of the reductions, regardless of the GPU task completion order.
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:static void alternatePmeNbGpuWaitReduce(nonbonded_verlet_t* nbv,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    bool isPmeGpuDone = false;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    bool isNbGpuDone  = false;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    gmx::ArrayRef<const gmx::RVec> pmeGpuForces;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    while (!isPmeGpuDone || !isNbGpuDone)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        if (!isPmeGpuDone)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            GpuTaskCompletion completionType =
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                    (isNbGpuDone) ? GpuTaskCompletion::Wait : GpuTaskCompletion::Check;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            isPmeGpuDone = pme_gpu_try_finish_task(
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        if (!isNbGpuDone)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            GpuTaskCompletion completionType =
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                    (isPmeGpuDone) ? GpuTaskCompletion::Wait : GpuTaskCompletion::Check;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            isNbGpuDone = Nbnxm::gpu_try_finish_task(
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                    nbv->gpu_nbv,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            if (isNbGpuDone)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        && (domainWork.haveCpuLocalForceWork || !stepWork.useGpuFBufferOps
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            || (havePpDomainDecomposition && !stepWork.useGpuFHalo)))
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    domainWork.haveGpuBondedWork =
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            ((fr.listedForcesGpu != nullptr) && fr.listedForcesGpu->haveInteractions());
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (simulationWork.useGpuXBufferOps || simulationWork.useGpuFBufferOps)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        GMX_ASSERT(simulationWork.useGpuNonbonded,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    flags.useGpuXBufferOps = simulationWork.useGpuXBufferOps && !flags.doNeighborSearch;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    flags.useGpuFBufferOps       = simulationWork.useGpuFBufferOps && !flags.computeVirial;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    const bool rankHasGpuPmeTask = simulationWork.useGpuPme && !simulationWork.haveSeparatePmeRank;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    flags.useGpuPmeFReduction    = flags.computeSlowForces && flags.useGpuFBufferOps
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                && (rankHasGpuPmeTask || simulationWork.useGpuPmePpCommunication);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    flags.useGpuXHalo              = simulationWork.useGpuHaloExchange && !flags.doNeighborSearch;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    flags.useGpuFHalo              = simulationWork.useGpuHaloExchange && flags.useGpuFBufferOps;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    flags.haveGpuPmeOnThisRank     = rankHasGpuPmeTask && flags.computeSlowForces;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:             && !(flags.computeVirial || simulationWork.useGpuNonbonded || flags.haveGpuPmeOnThisRank));
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:/* \brief Launch end-of-step GPU tasks: buffer clearing and rolling pruning.
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:static void launchGpuEndOfStepTasks(nonbonded_verlet_t*               nbv,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                    gmx::ListedForcesGpu*             listedForcesGpu,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (runScheduleWork.simulationWork.useGpuNonbonded && runScheduleWork.stepWork.computeNonbondedForces)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:         * clear kernel launches can leave the GPU idle while it could be running
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        if (nbv->isDynamicPruningStepGpu(step))
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            nbv->dispatchPruneKernelGpu(step);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        /* now clear the GPU outputs while we finish the step on the CPU */
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        wallcycle_start_nocount(wcycle, WallCycleCounter::LaunchGpu);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        wallcycle_sub_start_nocount(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        Nbnxm::gpu_clear_outputs(nbv->gpu_nbv, runScheduleWork.stepWork.computeVirial);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        wallcycle_sub_stop(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        wallcycle_stop(wcycle, WallCycleCounter::LaunchGpu);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (runScheduleWork.stepWork.haveGpuPmeOnThisRank)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        pme_gpu_reinit_computation(pmedata, wcycle);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (runScheduleWork.domainWork.haveGpuBondedWork && runScheduleWork.stepWork.computeEnergy)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        listedForcesGpu->waitAccumulateEnergyTerms(enerd);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        listedForcesGpu->clearEnergies();
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:/*! \brief Compute the number of times the "local coordinates ready on device" GPU event will be used as a synchronization point.
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed: * When some work is offloaded to GPU, force calculation should wait for the atom coordinates to
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed: * or from the GPU integration at the end of the previous step.
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed: * \param pmeSendCoordinatesFromGpu Whether peer-to-peer communication is used for PME coordinates.
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                                          bool pmeSendCoordinatesFromGpu)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        if (pmeSendCoordinatesFromGpu)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                       "GPU PME PP communications require having a separate PME rank");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            // Event is consumed by gmx_pme_send_coordinates for GPU PME PP Communications
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        if (stepWork.haveGpuPmeOnThisRank)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            // Event is consumed by launchPmeGpuSpread
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        if (stepWork.computeNonbondedForces && stepWork.useGpuXBufferOps)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            // Event is consumed by convertCoordinatesGpu
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (stepWork.useGpuXHalo)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        // Event is consumed by communicateGpuHaloCoordinates
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:/*! \brief Setup for the local GPU force reduction:
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed: * \param [in] stateGpu            GPU state propagator object
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed: * \param [in] gpuForceReduction   GPU force reduction object
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed: * \param [in] pmePpCommGpu        PME-PP GPU communication object
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:static void setupLocalGpuForceReduction(const gmx::MdrunScheduleWorkload* runScheduleWork,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                        gmx::StatePropagatorDataGpu*      stateGpu,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                        gmx::GpuForceReduction*           gpuForceReduction,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                        gmx::PmePpCommGpu*                pmePpCommGpu,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:               "GPU force reduction is not compatible with MTS");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    // (re-)initialize local GPU force reduction
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    gpuForceReduction->reinit(stateGpu->getForces(),
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                              stateGpu->fReducedOnDevice(AtomLocality::Local));
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    gpuForceReduction->registerNbnxmForce(Nbnxm::gpu_get_f(nbv->gpu_nbv));
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    GpuEventSynchronizer*   pmeSynchronizer     = nullptr;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (runScheduleWork->simulationWork.useGpuPme && !runScheduleWork->simulationWork.haveSeparatePmeRank)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        pmeForcePtr = pme_gpu_get_device_f(pmedata);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            pmeSynchronizer     = pme_gpu_get_f_ready_synchronizer(pmedata);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    else if (runScheduleWork->simulationWork.useGpuPmePpCommunication)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        pmeForcePtr = pmePpCommGpu->getGpuForceStagingPtr();
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                pmeSynchronizer = pmePpCommGpu->getForcesReadySynchronizer();
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        gpuForceReduction->registerRvecForce(pmeForcePtr);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        if (!runScheduleWork->simulationWork.useGpuPmePpCommunication || GMX_THREAD_MPI)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            GMX_ASSERT(pmeSynchronizer != nullptr, "PME force ready cuda event should not be NULL");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            gpuForceReduction->addDependency(pmeSynchronizer);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            && !runScheduleWork->simulationWork.useGpuHaloExchange))
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        gpuForceReduction->addDependency(stateGpu->fReadyOnDevice(AtomLocality::Local));
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (runScheduleWork->simulationWork.useGpuHaloExchange)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        gpuForceReduction->addDependency(dd->gpuHaloExchange[0][0]->getForcesReadyOnDeviceEvent());
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:/*! \brief Setup for the non-local GPU force reduction:
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed: * \param [in] stateGpu            GPU state propagator object
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed: * \param [in] gpuForceReduction   GPU force reduction object
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:static void setupNonLocalGpuForceReduction(const gmx::MdrunScheduleWorkload* runScheduleWork,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                           gmx::StatePropagatorDataGpu*      stateGpu,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                           gmx::GpuForceReduction*           gpuForceReduction,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    // (re-)initialize non-local GPU force reduction
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    gpuForceReduction->reinit(stateGpu->getForces(),
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                              stateGpu->fReducedOnDevice(AtomLocality::NonLocal));
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    gpuForceReduction->registerNbnxmForce(Nbnxm::gpu_get_f(nbv->gpu_nbv));
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        gpuForceReduction->addDependency(stateGpu->fReadyOnDevice(AtomLocality::NonLocal));
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    gmx::StatePropagatorDataGpu* stateGpu = fr->stateGpu;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (stepWork.doNeighborSearch && gmx::needStateGpu(simulationWork))
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        stateGpu->reinit(mdatoms->homenr,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        if (stepWork.haveGpuPmeOnThisRank)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            // TODO: This should be moved into PME setup function ( pme_gpu_prepare_computation(...) )
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            pme_gpu_set_device_x(fr->pmedata, stateGpu->getCoordinates());
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (stepWork.useGpuFHalo && !runScheduleWork->domainWork.haveCpuLocalForceWork && !stepWork.doNeighborSearch)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        // GPU Force halo exchange will set a subset of local atoms with remote non-local data
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        // On NS steps, the buffer could have already cleared in stateGpu->reinit.
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        stateGpu->clearForcesOnGpu(AtomLocality::Local,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                   stateGpu->getCoordinatesReadyOnDeviceEvent(
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    const bool pmeSendCoordinatesFromGpu =
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            simulationWork.useGpuPmePpCommunication && !(stepWork.doNeighborSearch);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    const bool reinitGpuPmePpComms =
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            simulationWork.useGpuPmePpCommunication && (stepWork.doNeighborSearch);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    auto* localXReadyOnDevice = (stepWork.haveGpuPmeOnThisRank || simulationWork.useGpuXBufferOps)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                        ? stateGpu->getCoordinatesReadyOnDeviceEvent(
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    GMX_ASSERT(simulationWork.useGpuHaloExchange
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                       == ((cr->dd != nullptr) && (!cr->dd->gpuHaloExchange[0].empty())),
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:               "The GPU halo exchange is active, but it has not been constructed.");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    bool gmx_used_in_debug haveCopiedXFromGpu = false;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    // Copy coordinate from the GPU if update is on the GPU and there
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (simulationWork.useGpuUpdate && !stepWork.doNeighborSearch
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        stateGpu->copyCoordinatesFromGpu(x.unpaddedArrayRef(), AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        haveCopiedXFromGpu = true;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (stepWork.haveGpuPmeOnThisRank || stepWork.useGpuXBufferOps)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        GMX_ASSERT(stateGpu != nullptr, "stateGpu should not be null");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                        simulationWork, stepWork, pmeSendCoordinatesFromGpu);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        if (!simulationWork.useGpuUpdate || stepWork.doNeighborSearch)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            stateGpu->copyCoordinatesToGpu(x.unpaddedArrayRef(),
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        else if (simulationWork.useGpuUpdate)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            stateGpu->setXUpdatedOnDeviceEventExpectedConsumptionCount(
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        if (!pmeSendCoordinatesFromGpu && !stepWork.doNeighborSearch && simulationWork.useGpuUpdate)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            GMX_ASSERT(haveCopiedXFromGpu,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                 simulationWork.useGpuPmePpCommunication,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                 reinitGpuPmePpComms,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                 pmeSendCoordinatesFromGpu,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                 stepWork.useGpuPmeFReduction,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (stepWork.haveGpuPmeOnThisRank)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        launchPmeGpuSpread(fr->pmedata,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        /* initialize the GPU nbnxm atom data and bonded data structures */
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        if (simulationWork.useGpuNonbonded)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            // Note: cycle counting only nononbondeds, GPU listed forces counts internally
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            wallcycle_start_nocount(wcycle, WallCycleCounter::LaunchGpu);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            wallcycle_sub_start_nocount(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            Nbnxm::gpu_init_atomdata(nbv->gpu_nbv, nbv->nbat.get());
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            wallcycle_sub_stop(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            wallcycle_stop(wcycle, WallCycleCounter::LaunchGpu);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            if (fr->listedForcesGpu)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                 * interactions to the GPU, where the grid order is
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                fr->listedForcesGpu->updateInteractionListsAndDeviceBuffers(
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                        Nbnxm::gpu_get_xq(nbv->gpu_nbv),
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                        Nbnxm::gpu_get_f(nbv->gpu_nbv),
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                        Nbnxm::gpu_get_fshift(nbv->gpu_nbv));
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        // Need to run after the GPU-offload bonded interaction lists
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        /* Note that with a GPU the launch overhead of the list transfer is not timed separately */
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        nbv->setupGpuShortRangeWork(fr->listedForcesGpu.get(), InteractionLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        if (simulationWork.useGpuXBufferOps)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            nbv->atomdata_init_copy_x_to_nbat_x_gpu();
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        if (simulationWork.useGpuFBufferOps)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            setupLocalGpuForceReduction(runScheduleWork,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                        stateGpu,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                        fr->gpuForceReduction[gmx::AtomLocality::Local].get(),
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                        fr->pmePpCommGpu.get(),
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                setupNonLocalGpuForceReduction(runScheduleWork,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                               stateGpu,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                               fr->gpuForceReduction[gmx::AtomLocality::NonLocal].get(),
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        if (stepWork.useGpuXBufferOps)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            GMX_ASSERT(stateGpu, "stateGpu should be valid when buffer ops are offloaded");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            nbv->convertCoordinatesGpu(AtomLocality::Local, stateGpu->getCoordinates(), localXReadyOnDevice);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            if (simulationWork.useGpuUpdate)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                GMX_ASSERT(stateGpu, "need a valid stateGpu object");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                GMX_ASSERT(haveCopiedXFromGpu,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (simulationWork.useGpuNonbonded && (stepWork.computeNonbondedForces || domainWork.haveGpuBondedWork))
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        ddBalanceRegionHandler.openBeforeForceComputationGpu();
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        wallcycle_start(wcycle, WallCycleCounter::LaunchGpu);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        wallcycle_sub_start(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        Nbnxm::gpu_upload_shiftvec(nbv->gpu_nbv, nbv->nbat.get());
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        if (!stepWork.useGpuXBufferOps)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            Nbnxm::gpu_copy_xq_to_gpu(nbv->gpu_nbv, nbv->nbat.get(), AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        wallcycle_sub_stop(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        wallcycle_stop(wcycle, WallCycleCounter::LaunchGpu);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        // with X buffer ops offloaded to the GPU on all but the search steps
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        if (domainWork.haveGpuBondedWork && !simulationWork.havePpDomainDecomposition)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            fr->listedForcesGpu->setPbcAndlaunchKernel(fr->pbcType, box, fr->bMolPBC, stepWork);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        /* launch local nonbonded work on GPU */
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        wallcycle_start_nocount(wcycle, WallCycleCounter::LaunchGpu);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        wallcycle_sub_start_nocount(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        wallcycle_sub_stop(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        wallcycle_stop(wcycle, WallCycleCounter::LaunchGpu);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (stepWork.haveGpuPmeOnThisRank)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        // In PME GPU and mixed mode we launch FFT / gather after the
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        // X copy/transform to allow overlap as well as after the GPU NB
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        launchPmeGpuFftAndGather(fr->pmedata,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            /* Note that with a GPU the launch overhead of the list transfer is not timed separately */
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            nbv->setupGpuShortRangeWork(fr->listedForcesGpu.get(), InteractionLocality::NonLocal);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            // TODO refactor this GPU halo exchange re-initialisation
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            // to location in do_md where GPU halo exchange is
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            // constructed at partitioning, after above stateGpu
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            if (simulationWork.useGpuHaloExchange)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                reinitGpuHaloExchange(*cr, stateGpu->getCoordinates(), stateGpu->getForces());
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            GpuEventSynchronizer* gpuCoordinateHaloLaunched = nullptr;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            if (stepWork.useGpuXHalo)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                gpuCoordinateHaloLaunched = communicateGpuHaloCoordinates(*cr, box, localXReadyOnDevice);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                    stateGpu->copyCoordinatesFromGpu(
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                            x.unpaddedArrayRef(), AtomLocality::NonLocal, gpuCoordinateHaloLaunched);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                if (simulationWork.useGpuUpdate)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                    GMX_ASSERT(haveCopiedXFromGpu,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                            (stepWork.computePmeOnSeparateRank && !pmeSendCoordinatesFromGpu);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                        stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            if (stepWork.useGpuXBufferOps)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                if (!stepWork.useGpuXHalo)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                    stateGpu->copyCoordinatesToGpu(x.unpaddedArrayRef(), AtomLocality::NonLocal);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                nbv->convertCoordinatesGpu(
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                        stateGpu->getCoordinates(),
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                        stateGpu->getCoordinatesReadyOnDeviceEvent(
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                AtomLocality::NonLocal, simulationWork, stepWork, gpuCoordinateHaloLaunched));
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        if (simulationWork.useGpuNonbonded)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            if (!stepWork.useGpuXBufferOps)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                wallcycle_start(wcycle, WallCycleCounter::LaunchGpu);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                wallcycle_sub_start(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                Nbnxm::gpu_copy_xq_to_gpu(nbv->gpu_nbv, nbv->nbat.get(), AtomLocality::NonLocal);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                wallcycle_sub_stop(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                wallcycle_stop(wcycle, WallCycleCounter::LaunchGpu);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            if (domainWork.haveGpuBondedWork)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                fr->listedForcesGpu->setPbcAndlaunchKernel(fr->pbcType, box, fr->bMolPBC, stepWork);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            /* launch non-local nonbonded tasks on GPU */
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            wallcycle_start_nocount(wcycle, WallCycleCounter::LaunchGpu);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            wallcycle_sub_start(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            wallcycle_sub_stop(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            wallcycle_stop(wcycle, WallCycleCounter::LaunchGpu);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (simulationWork.useGpuNonbonded && stepWork.computeNonbondedForces)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        wallcycle_start_nocount(wcycle, WallCycleCounter::LaunchGpu);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        wallcycle_sub_start_nocount(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            Nbnxm::gpu_launch_cpyback(nbv->gpu_nbv, nbv->nbat.get(), stepWork, AtomLocality::NonLocal);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        Nbnxm::gpu_launch_cpyback(nbv->gpu_nbv, nbv->nbat.get(), stepWork, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        wallcycle_sub_stop(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        if (domainWork.haveGpuBondedWork && stepWork.computeEnergy)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            fr->listedForcesGpu->launchEnergyTransfer();
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        wallcycle_stop(wcycle, WallCycleCounter::LaunchGpu);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    // For the rest of the CPU tasks that depend on GPU-update produced coordinates,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (simulationWork.useGpuUpdate && !stepWork.doNeighborSearch)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                || (stepWork.computePmeOnSeparateRank && !pmeSendCoordinatesFromGpu);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            GMX_ASSERT(haveCopiedXFromGpu,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    const bool useOrEmulateGpuNb = simulationWork.useGpuNonbonded || fr->nbv->emulateGpu();
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (!useOrEmulateGpuNb)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (stepWork.useGpuXHalo && (domainWork.haveCpuBondedWork || domainWork.haveFreeEnergyWork))
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        stateGpu->waitCoordinatesReadyOnHost(AtomLocality::NonLocal);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:         * Happens here on the CPU both with and without GPU.
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (stepWork.computeNonbondedForces && !useOrEmulateGpuNb)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            (stepWork.haveGpuPmeOnThisRank || needToReceivePmeResultsFromSeparateRank);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:     * GPU we must wait for the PME calculation (dhdl) results to finish before sampling the
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        if (stepWork.haveGpuPmeOnThisRank)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            pme_gpu_wait_and_reduce(fr->pmedata,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                   simulationWork.useGpuPmePpCommunication,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                   stepWork.useGpuPmeFReduction,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (simulationWork.havePpDomainDecomposition && stepWork.computeForces && stepWork.useGpuFHalo
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        stateGpu->copyForcesToGpu(forceOutMtsLevel0.forceWithShiftForces().force(), AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    GMX_ASSERT(!(nonbondedAtMtsLevel1 && stepWork.useGpuFBufferOps),
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:               "The schedule below does not allow for nonbonded MTS with GPU buffer ops");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    GMX_ASSERT(!(nonbondedAtMtsLevel1 && stepWork.useGpuFHalo),
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:               "The schedule below does not allow for nonbonded MTS with GPU halo exchange");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    // Will store the amount of cycles spent waiting for the GPU that
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    float cycles_wait_gpu = 0;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (useOrEmulateGpuNb && stepWork.computeNonbondedForces)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            if (simulationWork.useGpuNonbonded)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                cycles_wait_gpu += Nbnxm::gpu_wait_finish_task(
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                        nbv->gpu_nbv,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            if (stepWork.useGpuFBufferOps)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                    stateGpu->copyForcesToGpu(forceOutMtsLevel0.forceWithShiftForces().force(),
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                fr->gpuForceReduction[gmx::AtomLocality::NonLocal]->execute();
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                if (!stepWork.useGpuFHalo)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                    stateGpu->consumeForcesReducedOnDeviceEvent(AtomLocality::NonLocal);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                    // copy from GPU input for dd_move_f()
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                    stateGpu->copyForcesFromGpu(forceOutMtsLevel0.forceWithShiftForces().force(),
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            if (fr->nbv->emulateGpu() && stepWork.computeVirial)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:         * If we use a GPU this will overlap with GPU work, so in that case
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            if (stepWork.useGpuFHalo)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                gmx::FixedCapacityVector<GpuEventSynchronizer*, 2> gpuForceHaloDependencies;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                gpuForceHaloDependencies.push_back(stateGpu->fReadyOnDevice(AtomLocality::Local));
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                gpuForceHaloDependencies.push_back(stateGpu->fReducedOnDevice(AtomLocality::NonLocal));
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                communicateGpuHaloForces(*cr, accumulateForces, &gpuForceHaloDependencies);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                if (stepWork.useGpuFBufferOps)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                    stateGpu->waitForcesReadyOnHost(AtomLocality::NonLocal);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    // With both nonbonded and PME offloaded a GPU on the same rank, we use
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    // When running free energy perturbations steered by AWH and calculating PME on GPU,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    bool alternateGpuWait = (!c_disableAlternatingWait && stepWork.haveGpuPmeOnThisRank
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                             && simulationWork.useGpuNonbonded && !simulationWork.havePpDomainDecomposition
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                             && !stepWork.useGpuFBufferOps && !needEarlyPmeResults);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (alternateGpuWait)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        alternatePmeNbGpuWaitReduce(fr->nbv.get(),
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (!alternateGpuWait && stepWork.haveGpuPmeOnThisRank && !needEarlyPmeResults)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        pme_gpu_wait_and_reduce(fr->pmedata,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    /* Wait for local GPU NB outputs on the non-alternating wait path */
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (!alternateGpuWait && stepWork.computeNonbondedForces && simulationWork.useGpuNonbonded)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        /* Measured overhead on CUDA and OpenCL with(out) GPU sharing
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        const float gpuWaitApiOverheadMargin = 2e6F; /* cycles */
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        const float waitCycles               = Nbnxm::gpu_wait_finish_task(
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                nbv->gpu_nbv,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            DdBalanceRegionWaitedForGpu waitedForGpu = DdBalanceRegionWaitedForGpu::yes;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            if (stepWork.computeForces && waitCycles <= gpuWaitApiOverheadMargin)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                waitedForGpu = DdBalanceRegionWaitedForGpu::no;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            ddBalanceRegionHandler.closeAfterForceComputationGpu(cycles_wait_gpu, waitedForGpu);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (fr->nbv->emulateGpu())
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    // If on GPU PME-PP comms path, receive forces from PME before GPU buffer ops
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    // When running free energy perturbations steered by AWH and calculating PME on GPU,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (needToReceivePmeResultsFromSeparateRank && simulationWork.useGpuPmePpCommunication && !needEarlyPmeResults)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                               simulationWork.useGpuPmePpCommunication,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                               stepWork.useGpuPmeFReduction,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    /* Do the nonbonded GPU (or emulation) force buffer reduction
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    GMX_ASSERT(!(nonbondedAtMtsLevel1 && stepWork.useGpuFBufferOps),
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:               "The schedule below does not allow for nonbonded MTS with GPU buffer ops");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (useOrEmulateGpuNb && !alternateGpuWait)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        if (stepWork.useGpuFBufferOps)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            // - copy is not perfomed if GPU force halo exchange is active, because it would overwrite the result
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            if (domainWork.haveLocalForceContribInCpuBuffer && !stepWork.useGpuFHalo)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                stateGpu->copyForcesToGpu(forceWithShift, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                fr->gpuForceReduction[gmx::AtomLocality::Local]->execute();
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            if (!simulationWork.useGpuUpdate
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                || (simulationWork.useGpuUpdate && haveDDAtomOrdering(*cr) && simulationWork.useCpuPmePpCommunication)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                    /* We have previously issued force reduction on the GPU, but we will
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                    stateGpu->consumeForcesReducedOnDeviceEvent(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                stateGpu->copyForcesFromGpu(forceWithShift, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                stateGpu->waitForcesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    launchGpuEndOfStepTasks(
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            nbv, fr->listedForcesGpu.get(), fr->pmedata, enerd, *runScheduleWork, step, wcycle);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    // TODO refactor this and unify with above GPU PME-PP / GPU update path call to the same function
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    // When running free energy perturbations steered by AWH and calculating PME on GPU,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                               simulationWork.useGpuPmePpCommunication,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    /* In case we don't have constraints and are using GPUs, the next balancing
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:#include "gromacs/domdec/gpuhaloexchange.h"
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:#include "gromacs/ewald/pme_coordinate_receiver_gpu.h"
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:#include "gromacs/ewald/pme_pp_comm_gpu.h"
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:#include "gromacs/gpu_utils/gpu_utils.h"
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:#include "gromacs/listed_forces/listed_forces_gpu.h"
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:#include "gromacs/mdtypes/state_propagator_data_gpu.h"
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:#include "gromacs/nbnxm/gpu_data_mgmt.h"
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:#include "gromacs/nbnxm/nbnxm_gpu.h"
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:#include "gromacs/timing/gpu_timing.h"
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:#include "gpuforcereduction.h"
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:static const bool c_disableAlternatingWait = (getenv("GMX_DISABLE_ALTERNATING_GPU_WAIT") != nullptr);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                                   bool                  useGpuPmePpComms,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                                   bool                  receivePmeForceToGpu,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    gmx_pme_receive_f(fr->pmePpCommGpu.get(),
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                      useGpuPmePpComms,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                      receivePmeForceToGpu,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    /* GPU kernel launch overhead is already timed separately */
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    if (!nbv->useGpu())
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:/*! \brief Launch the prepare_step and spread stages of PME GPU.
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:static inline void launchPmeGpuSpread(gmx_pme_t*            pmedata,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                                      GpuEventSynchronizer* xReadyOnDevice,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    pme_gpu_prepare_computation(pmedata, box, wcycle, stepWork);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    bool                           useGpuDirectComm         = false;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    gmx::PmeCoordinateReceiverGpu* pmeCoordinateReceiverGpu = nullptr;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    pme_gpu_launch_spread(
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            pmedata, xReadyOnDevice, wcycle, lambdaQ, useGpuDirectComm, pmeCoordinateReceiverGpu);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:/*! \brief Launch the FFT and gather stages of PME GPU
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:static void launchPmeGpuFftAndGather(gmx_pme_t*               pmedata,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    pme_gpu_launch_complex_transforms(pmedata, wcycle, stepWork);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    pme_gpu_launch_gather(pmedata, wcycle, lambdaQ);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp: *  Polling wait for either of the PME or nonbonded GPU tasks.
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp: * Instead of a static order in waiting for GPU tasks, this function
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp: * one of the reductions, regardless of the GPU task completion order.
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:static void alternatePmeNbGpuWaitReduce(nonbonded_verlet_t* nbv,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    bool isPmeGpuDone = false;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    bool isNbGpuDone  = false;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    gmx::ArrayRef<const gmx::RVec> pmeGpuForces;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    while (!isPmeGpuDone || !isNbGpuDone)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        if (!isPmeGpuDone)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            GpuTaskCompletion completionType =
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                    (isNbGpuDone) ? GpuTaskCompletion::Wait : GpuTaskCompletion::Check;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            isPmeGpuDone = pme_gpu_try_finish_task(
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        if (!isNbGpuDone)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            GpuTaskCompletion completionType =
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                    (isPmeGpuDone) ? GpuTaskCompletion::Wait : GpuTaskCompletion::Check;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            isNbGpuDone = Nbnxm::gpu_try_finish_task(
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                    nbv->gpu_nbv,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            if (isNbGpuDone)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        && (domainWork.haveCpuLocalForceWork || !stepWork.useGpuFBufferOps
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            || (havePpDomainDecomposition && !stepWork.useGpuFHalo)))
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    domainWork.haveGpuBondedWork =
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            ((fr.listedForcesGpu != nullptr) && fr.listedForcesGpu->haveInteractions());
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    if (simulationWork.useGpuXBufferOps || simulationWork.useGpuFBufferOps)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        GMX_ASSERT(simulationWork.useGpuNonbonded,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    flags.useGpuXBufferOps = simulationWork.useGpuXBufferOps && !flags.doNeighborSearch;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    flags.useGpuFBufferOps       = simulationWork.useGpuFBufferOps && !flags.computeVirial;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    const bool rankHasGpuPmeTask = simulationWork.useGpuPme && !simulationWork.haveSeparatePmeRank;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    flags.useGpuPmeFReduction    = flags.computeSlowForces && flags.useGpuFBufferOps
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                                && (rankHasGpuPmeTask || simulationWork.useGpuPmePpCommunication);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    flags.useGpuXHalo              = simulationWork.useGpuHaloExchange && !flags.doNeighborSearch;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    flags.useGpuFHalo              = simulationWork.useGpuHaloExchange && flags.useGpuFBufferOps;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    flags.haveGpuPmeOnThisRank     = rankHasGpuPmeTask && flags.computeSlowForces;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:             && !(flags.computeVirial || simulationWork.useGpuNonbonded || flags.haveGpuPmeOnThisRank));
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:/* \brief Launch end-of-step GPU tasks: buffer clearing and rolling pruning.
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:static void launchGpuEndOfStepTasks(nonbonded_verlet_t*               nbv,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                                    gmx::ListedForcesGpu*             listedForcesGpu,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    if (runScheduleWork.simulationWork.useGpuNonbonded && runScheduleWork.stepWork.computeNonbondedForces)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:         * clear kernel launches can leave the GPU idle while it could be running
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        if (nbv->isDynamicPruningStepGpu(step))
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            nbv->dispatchPruneKernelGpu(step);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        /* now clear the GPU outputs while we finish the step on the CPU */
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        wallcycle_start_nocount(wcycle, WallCycleCounter::LaunchGpu);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        wallcycle_sub_start_nocount(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        Nbnxm::gpu_clear_outputs(nbv->gpu_nbv, runScheduleWork.stepWork.computeVirial);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        wallcycle_sub_stop(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        wallcycle_stop(wcycle, WallCycleCounter::LaunchGpu);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    if (runScheduleWork.stepWork.haveGpuPmeOnThisRank)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        pme_gpu_reinit_computation(pmedata, wcycle);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    if (runScheduleWork.domainWork.haveGpuBondedWork && runScheduleWork.stepWork.computeEnergy)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        listedForcesGpu->waitAccumulateEnergyTerms(enerd);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        listedForcesGpu->clearEnergies();
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:/*! \brief Compute the number of times the "local coordinates ready on device" GPU event will be used as a synchronization point.
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp: * When some work is offloaded to GPU, force calculation should wait for the atom coordinates to
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp: * or from the GPU integration at the end of the previous step.
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp: * \param pmeSendCoordinatesFromGpu Whether peer-to-peer communication is used for PME coordinates.
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                                                          bool pmeSendCoordinatesFromGpu)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        if (pmeSendCoordinatesFromGpu)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                       "GPU PME PP communications require having a separate PME rank");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            // Event is consumed by gmx_pme_send_coordinates for GPU PME PP Communications
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        if (stepWork.haveGpuPmeOnThisRank)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            // Event is consumed by launchPmeGpuSpread
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        if (stepWork.computeNonbondedForces && stepWork.useGpuXBufferOps)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            // Event is consumed by convertCoordinatesGpu
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    if (stepWork.useGpuXHalo)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        // Event is consumed by communicateGpuHaloCoordinates
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:/*! \brief Setup for the local GPU force reduction:
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp: * \param [in] stateGpu            GPU state propagator object
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp: * \param [in] gpuForceReduction   GPU force reduction object
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp: * \param [in] pmePpCommGpu        PME-PP GPU communication object
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:static void setupLocalGpuForceReduction(const gmx::MdrunScheduleWorkload* runScheduleWork,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                                        gmx::StatePropagatorDataGpu*      stateGpu,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                                        gmx::GpuForceReduction*           gpuForceReduction,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                                        gmx::PmePpCommGpu*                pmePpCommGpu,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:               "GPU force reduction is not compatible with MTS");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    // (re-)initialize local GPU force reduction
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    gpuForceReduction->reinit(stateGpu->getForces(),
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                              stateGpu->fReducedOnDevice(AtomLocality::Local));
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    gpuForceReduction->registerNbnxmForce(Nbnxm::gpu_get_f(nbv->gpu_nbv));
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    GpuEventSynchronizer*   pmeSynchronizer     = nullptr;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    if (runScheduleWork->simulationWork.useGpuPme && !runScheduleWork->simulationWork.haveSeparatePmeRank)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        pmeForcePtr = pme_gpu_get_device_f(pmedata);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            pmeSynchronizer     = pme_gpu_get_f_ready_synchronizer(pmedata);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    else if (runScheduleWork->simulationWork.useGpuPmePpCommunication)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        pmeForcePtr = pmePpCommGpu->getGpuForceStagingPtr();
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                pmeSynchronizer = pmePpCommGpu->getForcesReadySynchronizer();
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        gpuForceReduction->registerRvecForce(pmeForcePtr);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        if (!runScheduleWork->simulationWork.useGpuPmePpCommunication || GMX_THREAD_MPI)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            GMX_ASSERT(pmeSynchronizer != nullptr, "PME force ready cuda event should not be NULL");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            gpuForceReduction->addDependency(pmeSynchronizer);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            && !runScheduleWork->simulationWork.useGpuHaloExchange))
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        gpuForceReduction->addDependency(stateGpu->fReadyOnDevice(AtomLocality::Local));
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    if (runScheduleWork->simulationWork.useGpuHaloExchange)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        gpuForceReduction->addDependency(dd->gpuHaloExchange[0][0]->getForcesReadyOnDeviceEvent());
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:/*! \brief Setup for the non-local GPU force reduction:
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp: * \param [in] stateGpu            GPU state propagator object
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp: * \param [in] gpuForceReduction   GPU force reduction object
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:static void setupNonLocalGpuForceReduction(const gmx::MdrunScheduleWorkload* runScheduleWork,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                                           gmx::StatePropagatorDataGpu*      stateGpu,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                                           gmx::GpuForceReduction*           gpuForceReduction,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    // (re-)initialize non-local GPU force reduction
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    gpuForceReduction->reinit(stateGpu->getForces(),
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                              stateGpu->fReducedOnDevice(AtomLocality::NonLocal));
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    gpuForceReduction->registerNbnxmForce(Nbnxm::gpu_get_f(nbv->gpu_nbv));
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        gpuForceReduction->addDependency(stateGpu->fReadyOnDevice(AtomLocality::NonLocal));
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    gmx::StatePropagatorDataGpu* stateGpu = fr->stateGpu;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    if (stepWork.doNeighborSearch && gmx::needStateGpu(simulationWork))
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        stateGpu->reinit(mdatoms->homenr,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        if (stepWork.haveGpuPmeOnThisRank)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            // TODO: This should be moved into PME setup function ( pme_gpu_prepare_computation(...) )
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            pme_gpu_set_device_x(fr->pmedata, stateGpu->getCoordinates());
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    if (stepWork.useGpuFHalo && !runScheduleWork->domainWork.haveCpuLocalForceWork && !stepWork.doNeighborSearch)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        // GPU Force halo exchange will set a subset of local atoms with remote non-local data
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        // On NS steps, the buffer could have already cleared in stateGpu->reinit.
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        stateGpu->clearForcesOnGpu(AtomLocality::Local,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                                   stateGpu->getCoordinatesReadyOnDeviceEvent(
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    const bool pmeSendCoordinatesFromGpu =
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            simulationWork.useGpuPmePpCommunication && !(stepWork.doNeighborSearch);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    const bool reinitGpuPmePpComms =
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            simulationWork.useGpuPmePpCommunication && (stepWork.doNeighborSearch);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    auto* localXReadyOnDevice = (stepWork.haveGpuPmeOnThisRank || simulationWork.useGpuXBufferOps)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                                        ? stateGpu->getCoordinatesReadyOnDeviceEvent(
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    GMX_ASSERT(simulationWork.useGpuHaloExchange
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                       == ((cr->dd != nullptr) && (!cr->dd->gpuHaloExchange[0].empty())),
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:               "The GPU halo exchange is active, but it has not been constructed.");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    bool gmx_used_in_debug haveCopiedXFromGpu = false;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    // Copy coordinate from the GPU if update is on the GPU and there
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    if (simulationWork.useGpuUpdate && !stepWork.doNeighborSearch
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        stateGpu->copyCoordinatesFromGpu(x.unpaddedArrayRef(), AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        haveCopiedXFromGpu = true;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    if (stepWork.haveGpuPmeOnThisRank || stepWork.useGpuXBufferOps)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        GMX_ASSERT(stateGpu != nullptr, "stateGpu should not be null");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                        simulationWork, stepWork, pmeSendCoordinatesFromGpu);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        if (!simulationWork.useGpuUpdate || stepWork.doNeighborSearch)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            stateGpu->copyCoordinatesToGpu(x.unpaddedArrayRef(),
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        else if (simulationWork.useGpuUpdate)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            stateGpu->setXUpdatedOnDeviceEventExpectedConsumptionCount(
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        if (!pmeSendCoordinatesFromGpu && !stepWork.doNeighborSearch && simulationWork.useGpuUpdate)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            GMX_ASSERT(haveCopiedXFromGpu,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                                 simulationWork.useGpuPmePpCommunication,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                                 reinitGpuPmePpComms,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                                 pmeSendCoordinatesFromGpu,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                                 stepWork.useGpuPmeFReduction,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    if (stepWork.haveGpuPmeOnThisRank)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        launchPmeGpuSpread(fr->pmedata,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        /* initialize the GPU nbnxm atom data and bonded data structures */
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        if (simulationWork.useGpuNonbonded)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            // Note: cycle counting only nononbondeds, GPU listed forces counts internally
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            wallcycle_start_nocount(wcycle, WallCycleCounter::LaunchGpu);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            wallcycle_sub_start_nocount(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            Nbnxm::gpu_init_atomdata(nbv->gpu_nbv, nbv->nbat.get());
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            wallcycle_sub_stop(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            wallcycle_stop(wcycle, WallCycleCounter::LaunchGpu);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            if (fr->listedForcesGpu)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                 * interactions to the GPU, where the grid order is
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                fr->listedForcesGpu->updateInteractionListsAndDeviceBuffers(
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                        Nbnxm::gpu_get_xq(nbv->gpu_nbv),
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                        Nbnxm::gpu_get_f(nbv->gpu_nbv),
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                        Nbnxm::gpu_get_fshift(nbv->gpu_nbv));
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        // Need to run after the GPU-offload bonded interaction lists
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        /* Note that with a GPU the launch overhead of the list transfer is not timed separately */
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        nbv->setupGpuShortRangeWork(fr->listedForcesGpu.get(), InteractionLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        if (simulationWork.useGpuXBufferOps)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            nbv->atomdata_init_copy_x_to_nbat_x_gpu();
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        if (simulationWork.useGpuFBufferOps)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            setupLocalGpuForceReduction(runScheduleWork,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                                        stateGpu,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                                        fr->gpuForceReduction[gmx::AtomLocality::Local].get(),
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                                        fr->pmePpCommGpu.get(),
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                setupNonLocalGpuForceReduction(runScheduleWork,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                                               stateGpu,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                                               fr->gpuForceReduction[gmx::AtomLocality::NonLocal].get(),
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        if (stepWork.useGpuXBufferOps)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            GMX_ASSERT(stateGpu, "stateGpu should be valid when buffer ops are offloaded");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            nbv->convertCoordinatesGpu(AtomLocality::Local, stateGpu->getCoordinates(), localXReadyOnDevice);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            if (simulationWork.useGpuUpdate)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                GMX_ASSERT(stateGpu, "need a valid stateGpu object");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                GMX_ASSERT(haveCopiedXFromGpu,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    if (simulationWork.useGpuNonbonded && (stepWork.computeNonbondedForces || domainWork.haveGpuBondedWork))
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        ddBalanceRegionHandler.openBeforeForceComputationGpu();
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        wallcycle_start(wcycle, WallCycleCounter::LaunchGpu);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        wallcycle_sub_start(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        Nbnxm::gpu_upload_shiftvec(nbv->gpu_nbv, nbv->nbat.get());
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        if (!stepWork.useGpuXBufferOps)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            Nbnxm::gpu_copy_xq_to_gpu(nbv->gpu_nbv, nbv->nbat.get(), AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        wallcycle_sub_stop(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        wallcycle_stop(wcycle, WallCycleCounter::LaunchGpu);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        // with X buffer ops offloaded to the GPU on all but the search steps
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        if (domainWork.haveGpuBondedWork && !simulationWork.havePpDomainDecomposition)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            fr->listedForcesGpu->setPbcAndlaunchKernel(fr->pbcType, box, fr->bMolPBC, stepWork);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        /* launch local nonbonded work on GPU */
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        wallcycle_start_nocount(wcycle, WallCycleCounter::LaunchGpu);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        wallcycle_sub_start_nocount(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        wallcycle_sub_stop(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        wallcycle_stop(wcycle, WallCycleCounter::LaunchGpu);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    if (stepWork.haveGpuPmeOnThisRank)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        // In PME GPU and mixed mode we launch FFT / gather after the
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        // X copy/transform to allow overlap as well as after the GPU NB
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        launchPmeGpuFftAndGather(fr->pmedata,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            /* Note that with a GPU the launch overhead of the list transfer is not timed separately */
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            nbv->setupGpuShortRangeWork(fr->listedForcesGpu.get(), InteractionLocality::NonLocal);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            // TODO refactor this GPU halo exchange re-initialisation
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            // to location in do_md where GPU halo exchange is
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            // constructed at partitioning, after above stateGpu
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            if (simulationWork.useGpuHaloExchange)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                reinitGpuHaloExchange(*cr, stateGpu->getCoordinates(), stateGpu->getForces());
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            GpuEventSynchronizer* gpuCoordinateHaloLaunched = nullptr;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            if (stepWork.useGpuXHalo)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                gpuCoordinateHaloLaunched = communicateGpuHaloCoordinates(*cr, box, localXReadyOnDevice);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                    stateGpu->copyCoordinatesFromGpu(
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                            x.unpaddedArrayRef(), AtomLocality::NonLocal, gpuCoordinateHaloLaunched);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                if (simulationWork.useGpuUpdate)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                    GMX_ASSERT(haveCopiedXFromGpu,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                            (stepWork.computePmeOnSeparateRank && !pmeSendCoordinatesFromGpu);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                        stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            if (stepWork.useGpuXBufferOps)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                if (!stepWork.useGpuXHalo)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                    stateGpu->copyCoordinatesToGpu(x.unpaddedArrayRef(), AtomLocality::NonLocal);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                nbv->convertCoordinatesGpu(
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                        stateGpu->getCoordinates(),
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                        stateGpu->getCoordinatesReadyOnDeviceEvent(
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                                AtomLocality::NonLocal, simulationWork, stepWork, gpuCoordinateHaloLaunched));
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        if (simulationWork.useGpuNonbonded)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            if (!stepWork.useGpuXBufferOps)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                wallcycle_start(wcycle, WallCycleCounter::LaunchGpu);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                wallcycle_sub_start(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                Nbnxm::gpu_copy_xq_to_gpu(nbv->gpu_nbv, nbv->nbat.get(), AtomLocality::NonLocal);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                wallcycle_sub_stop(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                wallcycle_stop(wcycle, WallCycleCounter::LaunchGpu);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            if (domainWork.haveGpuBondedWork)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                fr->listedForcesGpu->setPbcAndlaunchKernel(fr->pbcType, box, fr->bMolPBC, stepWork);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            /* launch non-local nonbonded tasks on GPU */
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            wallcycle_start_nocount(wcycle, WallCycleCounter::LaunchGpu);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            wallcycle_sub_start(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            wallcycle_sub_stop(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            wallcycle_stop(wcycle, WallCycleCounter::LaunchGpu);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    if (simulationWork.useGpuNonbonded && stepWork.computeNonbondedForces)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        wallcycle_start_nocount(wcycle, WallCycleCounter::LaunchGpu);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        wallcycle_sub_start_nocount(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            Nbnxm::gpu_launch_cpyback(nbv->gpu_nbv, nbv->nbat.get(), stepWork, AtomLocality::NonLocal);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        Nbnxm::gpu_launch_cpyback(nbv->gpu_nbv, nbv->nbat.get(), stepWork, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        wallcycle_sub_stop(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        if (domainWork.haveGpuBondedWork && stepWork.computeEnergy)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            fr->listedForcesGpu->launchEnergyTransfer();
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        wallcycle_stop(wcycle, WallCycleCounter::LaunchGpu);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    // For the rest of the CPU tasks that depend on GPU-update produced coordinates,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    if (simulationWork.useGpuUpdate && !stepWork.doNeighborSearch)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                || (stepWork.computePmeOnSeparateRank && !pmeSendCoordinatesFromGpu);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            GMX_ASSERT(haveCopiedXFromGpu,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    const bool useOrEmulateGpuNb = simulationWork.useGpuNonbonded || fr->nbv->emulateGpu();
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    if (!useOrEmulateGpuNb)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    if (stepWork.useGpuXHalo && (domainWork.haveCpuBondedWork || domainWork.haveFreeEnergyWork))
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        stateGpu->waitCoordinatesReadyOnHost(AtomLocality::NonLocal);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:         * Happens here on the CPU both with and without GPU.
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    if (stepWork.computeNonbondedForces && !useOrEmulateGpuNb)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            (stepWork.haveGpuPmeOnThisRank || needToReceivePmeResultsFromSeparateRank);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:     * GPU we must wait for the PME calculation (dhdl) results to finish before sampling the
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        if (stepWork.haveGpuPmeOnThisRank)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            pme_gpu_wait_and_reduce(fr->pmedata,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                                   simulationWork.useGpuPmePpCommunication,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                                   stepWork.useGpuPmeFReduction,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    if (simulationWork.havePpDomainDecomposition && stepWork.computeForces && stepWork.useGpuFHalo
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        stateGpu->copyForcesToGpu(forceOutMtsLevel0.forceWithShiftForces().force(), AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    GMX_ASSERT(!(nonbondedAtMtsLevel1 && stepWork.useGpuFBufferOps),
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:               "The schedule below does not allow for nonbonded MTS with GPU buffer ops");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    GMX_ASSERT(!(nonbondedAtMtsLevel1 && stepWork.useGpuFHalo),
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:               "The schedule below does not allow for nonbonded MTS with GPU halo exchange");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    // Will store the amount of cycles spent waiting for the GPU that
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    float cycles_wait_gpu = 0;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    if (useOrEmulateGpuNb && stepWork.computeNonbondedForces)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            if (simulationWork.useGpuNonbonded)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                cycles_wait_gpu += Nbnxm::gpu_wait_finish_task(
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                        nbv->gpu_nbv,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            if (stepWork.useGpuFBufferOps)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                    stateGpu->copyForcesToGpu(forceOutMtsLevel0.forceWithShiftForces().force(),
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                fr->gpuForceReduction[gmx::AtomLocality::NonLocal]->execute();
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                if (!stepWork.useGpuFHalo)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                    stateGpu->consumeForcesReducedOnDeviceEvent(AtomLocality::NonLocal);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                    // copy from GPU input for dd_move_f()
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                    stateGpu->copyForcesFromGpu(forceOutMtsLevel0.forceWithShiftForces().force(),
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            if (fr->nbv->emulateGpu() && stepWork.computeVirial)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:         * If we use a GPU this will overlap with GPU work, so in that case
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            if (stepWork.useGpuFHalo)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                gmx::FixedCapacityVector<GpuEventSynchronizer*, 2> gpuForceHaloDependencies;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                gpuForceHaloDependencies.push_back(stateGpu->fReadyOnDevice(AtomLocality::Local));
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                gpuForceHaloDependencies.push_back(stateGpu->fReducedOnDevice(AtomLocality::NonLocal));
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                communicateGpuHaloForces(*cr, accumulateForces, &gpuForceHaloDependencies);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                if (stepWork.useGpuFBufferOps)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                    stateGpu->waitForcesReadyOnHost(AtomLocality::NonLocal);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    // With both nonbonded and PME offloaded a GPU on the same rank, we use
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    // When running free energy perturbations steered by AWH and calculating PME on GPU,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    bool alternateGpuWait = (!c_disableAlternatingWait && stepWork.haveGpuPmeOnThisRank
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                             && simulationWork.useGpuNonbonded && !simulationWork.havePpDomainDecomposition
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                             && !stepWork.useGpuFBufferOps && !needEarlyPmeResults);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    if (alternateGpuWait)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        alternatePmeNbGpuWaitReduce(fr->nbv.get(),
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    if (!alternateGpuWait && stepWork.haveGpuPmeOnThisRank && !needEarlyPmeResults)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        pme_gpu_wait_and_reduce(fr->pmedata,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    /* Wait for local GPU NB outputs on the non-alternating wait path */
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    if (!alternateGpuWait && stepWork.computeNonbondedForces && simulationWork.useGpuNonbonded)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        /* Measured overhead on CUDA and OpenCL with(out) GPU sharing
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        const float gpuWaitApiOverheadMargin = 2e6F; /* cycles */
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        const float waitCycles               = Nbnxm::gpu_wait_finish_task(
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                nbv->gpu_nbv,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            DdBalanceRegionWaitedForGpu waitedForGpu = DdBalanceRegionWaitedForGpu::yes;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            if (stepWork.computeForces && waitCycles <= gpuWaitApiOverheadMargin)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                waitedForGpu = DdBalanceRegionWaitedForGpu::no;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            ddBalanceRegionHandler.closeAfterForceComputationGpu(cycles_wait_gpu, waitedForGpu);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    if (fr->nbv->emulateGpu())
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    // If on GPU PME-PP comms path, receive forces from PME before GPU buffer ops
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    // When running free energy perturbations steered by AWH and calculating PME on GPU,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    if (needToReceivePmeResultsFromSeparateRank && simulationWork.useGpuPmePpCommunication && !needEarlyPmeResults)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                               simulationWork.useGpuPmePpCommunication,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                               stepWork.useGpuPmeFReduction,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    /* Do the nonbonded GPU (or emulation) force buffer reduction
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    GMX_ASSERT(!(nonbondedAtMtsLevel1 && stepWork.useGpuFBufferOps),
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:               "The schedule below does not allow for nonbonded MTS with GPU buffer ops");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    if (useOrEmulateGpuNb && !alternateGpuWait)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:        if (stepWork.useGpuFBufferOps)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            // - copy is not perfomed if GPU force halo exchange is active, because it would overwrite the result
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            if (domainWork.haveLocalForceContribInCpuBuffer && !stepWork.useGpuFHalo)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                stateGpu->copyForcesToGpu(forceWithShift, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                fr->gpuForceReduction[gmx::AtomLocality::Local]->execute();
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            if (!simulationWork.useGpuUpdate
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                || (simulationWork.useGpuUpdate && haveDDAtomOrdering(*cr) && simulationWork.useCpuPmePpCommunication)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                    /* We have previously issued force reduction on the GPU, but we will
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                    stateGpu->consumeForcesReducedOnDeviceEvent(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                stateGpu->copyForcesFromGpu(forceWithShift, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                stateGpu->waitForcesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    launchGpuEndOfStepTasks(
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:            nbv, fr->listedForcesGpu.get(), fr->pmedata, enerd, *runScheduleWork, step, wcycle);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    // TODO refactor this and unify with above GPU PME-PP / GPU update path call to the same function
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    // When running free energy perturbations steered by AWH and calculating PME on GPU,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:                               simulationWork.useGpuPmePpCommunication,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdlib/sim_util.cpp:    /* In case we don't have constraints and are using GPUs, the next balancing
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:#include "gromacs/domdec/gpuhaloexchange.h"
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:#include "gromacs/ewald/pme_gpu_program.h"
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:#include "gromacs/ewald/pme_pp_comm_gpu.h"
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:#include "gromacs/gpu_utils/device_stream_manager.h"
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:#include "gromacs/listed_forces/listed_forces_gpu.h"
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:#include "gromacs/mdlib/gpuforcereduction.h"
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:#include "gromacs/mdtypes/state_propagator_data_gpu.h"
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:#include "gromacs/nbnxm/gpu_data_mgmt.h"
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:#include "gromacs/taskassignment/decidegpuusage.h"
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:#include "gromacs/taskassignment/usergpuids.h"
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:#include "gromacs/timing/gpu_timing.h"
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp: * the GPU communication flags are set to false in non-tMPI and non-CUDA builds.
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp: * \param[in]  useGpuForNonbonded   True if the nonbonded task is offloaded in this run.
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                                                         const bool           useGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:    devFlags.enableGpuBufferOps = (GMX_GPU_CUDA || GMX_GPU_SYCL) && useGpuForNonbonded
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                                  && (getenv("GMX_USE_GPU_BUFFER_OPS") != nullptr);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:    devFlags.forceGpuUpdateDefault = (getenv("GMX_FORCE_UPDATE_DEFAULT_GPU") != nullptr) || GMX_FAHCORE;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:    // Flag use to enable GPU-aware MPI depenendent features such PME GPU decomposition
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:    // GPU-aware MPI is marked available if it has been detected by GROMACS or detection fails but
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:    devFlags.canUseGpuAwareMpi = false;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:    // Direct GPU comm path is being used with GPU-aware MPI
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:    // make sure underlying MPI implementation is GPU-aware
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:    if (GMX_LIB_MPI && GMX_GPU_CUDA)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:        // Allow overriding the detection for GPU-aware MPI
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:        GpuAwareMpiStatus gpuAwareMpiStatus = checkMpiCudaAwareSupport();
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:        const bool        forceGpuAwareMpi  = gpuAwareMpiStatus == GpuAwareMpiStatus::Forced;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:        const bool haveDetectedGpuAwareMpi  = gpuAwareMpiStatus == GpuAwareMpiStatus::Supported;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:        if (getenv("GMX_FORCE_CUDA_AWARE_MPI") != nullptr)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                            "GMX_FORCE_CUDA_AWARE_MPI environment variable is inactive. "
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                            "Please use GMX_FORCE_GPU_AWARE_MPI instead.");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:        devFlags.canUseGpuAwareMpi = haveDetectedGpuAwareMpi || forceGpuAwareMpi;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:        if (getenv("GMX_ENABLE_DIRECT_GPU_COMM") != nullptr)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:            if (!haveDetectedGpuAwareMpi && forceGpuAwareMpi)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                // GPU-aware support not detected in MPI library but, user has forced it's use
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                                "This run has forced use of 'GPU-aware MPI', ie. 'CUDA-aware MPI'. "
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                                "However, GROMACS cannot determine if underlying MPI is GPU-aware. "
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                                "GROMACS recommends use of latest OpenMPI version for GPU-aware "
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                                "GMX_FORCE_GPU_AWARE_MPI environment variable.");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:            if (devFlags.canUseGpuAwareMpi)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                                "GMX_ENABLE_DIRECT_GPU_COMM environment variable detected, "
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                                "enabling direct GPU communication using GPU-aware MPI.");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                                "GPU-aware MPI was not detected, will not use direct GPU "
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                                "for GPU-aware support. If you are certain about GPU-aware support "
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                                "GMX_FORCE_GPU_AWARE_MPI environment variable. Note that such "
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                                "support is often called \"CUDA-aware MPI.\"");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:        else if (haveDetectedGpuAwareMpi)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:            // GPU-aware MPI was detected, let the user know that using it may improve performance
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                            "GPU-aware MPI detected, but by default GROMACS will not "
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                            "make use the direct GPU communication capabilities of MPI. "
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                            "the GMX_ENABLE_DIRECT_GPU_COMM environment variable.");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:        if (getenv("GMX_FORCE_GPU_AWARE_MPI") != nullptr)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:            // Cannot force use of GPU-aware MPI in this build configuration
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                            "A CUDA build with an external MPI library is required in order to "
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                            "benefit from GMX_FORCE_GPU_AWARE_MPI. That environment variable is "
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:    if (devFlags.enableGpuBufferOps)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                        "This run uses the 'GPU buffer ops' feature, enabled by the "
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                        "GMX_USE_GPU_BUFFER_OPS environment variable.");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:    if (devFlags.forceGpuUpdateDefault)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                        "This run will default to '-update gpu' as requested by the "
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                        "GMX_FORCE_UPDATE_DEFAULT_GPU environment variable.");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:    // PME decomposition is supported only with CUDA-backend in mixed mode
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:    // CUDA-backend also needs GPU-aware MPI support for decomposition to work
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:    const bool pmeGpuDecompositionRequested =
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:            (pmeRunMode == PmeRunMode::GPU || pmeRunMode == PmeRunMode::Mixed)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:    const bool pmeGpuDecompositionSupported =
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:            (devFlags.canUseGpuAwareMpi && pmeRunMode == PmeRunMode::Mixed);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:    const bool forcePmeGpuDecomposition = getenv("GMX_GPU_PME_DECOMPOSITION") != nullptr;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:    if (pmeGpuDecompositionSupported && pmeGpuDecompositionRequested)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:        // PME decomposition is supported only when it is forced using GMX_GPU_PME_DECOMPOSITION
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:        if (forcePmeGpuDecomposition)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                            "This run has requested the 'GPU PME decomposition' feature, enabled "
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                            "by the GMX_GPU_PME_DECOMPOSITION environment variable. "
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                      "Multiple PME tasks were required to run on GPUs, "
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                      "Use GMX_GPU_PME_DECOMPOSITION environment variable to enable it.");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:    if (!pmeGpuDecompositionSupported && pmeGpuDecompositionRequested)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                  "PME tasks were required to run on GPUs, but that is not implemented with "
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:    devFlags.enableGpuPmeDecomposition =
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:            forcePmeGpuDecomposition && pmeGpuDecompositionRequested && pmeGpuDecompositionSupported;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                                  bool                makeGpuPairList,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                (makeGpuPairList ? ListSetupType::Gpu : ListSetupType::CpuSimdWhenSupported);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:        increaseNstlist(fplog, cr, ir, nstlist_cmdline, &mtop, box, makeGpuPairList, cpuinfo);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:/*! \brief Return whether GPU acceleration of nonbondeds is supported with the given settings.
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:static bool gpuAccelerationOfNonbondedIsUseful(const MDLogger&   mdlog,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:    bool        gpuIsUseful = true;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:        /* The GPU code does not support more than one energy group.
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:         * If the user requested GPUs explicitly, a fatal error is given later.
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:        gpuIsUseful = false;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                    "Multiple energy groups is not implemented for GPUs, falling back to the CPU. "
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                    "For better performance, run on the GPU without energy groups and then do "
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:    /* There are resource handling issues in the GPU code paths with MTS on anything else than only
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:        gpuIsUseful = false;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                "Multiple time stepping is only supported with GPUs when MTS is only applied to %s "
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:        gpuIsUseful = false;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:        warning     = "TPI is not implemented for GPUs.";
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:    if (!gpuIsUseful && issueWarning)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:    return gpuIsUseful;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:    else if (strncmp(optionString, "gpu", 3) == 0)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:        returnValue = TaskTarget::Gpu;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:        auto* nbnxn_gpu_timings =
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                (nbv != nullptr && nbv->useGpu()) ? Nbnxm::gpu_get_timings(nbv->gpu_nbv) : nullptr;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:        gmx_wallclock_gpu_pme_t pme_gpu_timings = {};
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:        if (pme_gpu_task_enabled(pme))
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:            pme_gpu_get_timings(pme, &pme_gpu_timings);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                        nbnxn_gpu_timings,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                        &pme_gpu_timings);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:    EmulateGpuNonbonded emulateGpuNonbonded =
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:            (getenv("GMX_EMULATE_GPU") != nullptr ? EmulateGpuNonbonded::Yes : EmulateGpuNonbonded::No);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:    std::vector<int> userGpuTaskAssignment;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:        userGpuTaskAssignment = parseUserTaskAssignmentString(hw_opt.userGpuTaskAssignment);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:        bool useGpuForNonbonded = false;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:        bool useGpuForPme       = false;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:            // the number of GPUs to choose the number of ranks.
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:            auto canUseGpuForNonbonded = buildSupportsNonbondedOnGpu(nullptr);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:            useGpuForNonbonded         = decideWhetherToUseGpusForNonbondedWithThreadMpi(
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                    userGpuTaskAssignment,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                    emulateGpuNonbonded,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                    canUseGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                    gpuAccelerationOfNonbondedIsUseful(mdlog, *inputrec, GMX_THREAD_MPI, doRerun),
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:            useGpuForPme = decideWhetherToUseGpusForPmeWithThreadMpi(useGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                                                                     userGpuTaskAssignment,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                                                useGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                                                useGpuForPme,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:    // Note that when bonded interactions run on a GPU they always run
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:    bool useGpuForNonbonded = false;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:    bool useGpuForPme       = false;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:    bool useGpuForBonded    = false;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:    bool useGpuForUpdate    = false;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:    bool gpusWereDetected   = hwinfo_->ngpu_compatible_tot > 0;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:        // It's possible that there are different numbers of GPUs on
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:        auto canUseGpuForNonbonded = buildSupportsNonbondedOnGpu(nullptr);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:        useGpuForNonbonded         = decideWhetherToUseGpusForNonbonded(
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                userGpuTaskAssignment,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                emulateGpuNonbonded,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                canUseGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                gpuAccelerationOfNonbondedIsUseful(mdlog, *inputrec, !GMX_THREAD_MPI, doRerun),
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                gpusWereDetected);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:        useGpuForPme    = decideWhetherToUseGpusForPme(useGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                                                    userGpuTaskAssignment,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                                                    gpusWereDetected);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:        useGpuForBonded = decideWhetherToUseGpusForBonded(
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                useGpuForNonbonded, useGpuForPme, bondedTarget, *inputrec, mtop, domdecOptions.numPmeRanks, gpusWereDetected);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:    const PmeRunMode pmeRunMode = determinePmeRunMode(useGpuForPme, pmeFftTarget, *inputrec);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:            mdlog, useGpuForNonbonded, pmeRunMode, cr->sizeOfDefaultCommunicator, domdecOptions.numPmeRanks);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                || (!useGpuForNonbonded && EEL_FULL(inputrec->coulombtype) && useDDWithSingleRank != 0)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                          useGpuForNonbonded || (emulateGpuNonbonded == EmulateGpuNonbonded::Yes),
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:        useGpuForUpdate = decideWhetherToUseGpuForUpdate(useDomainDecomposition,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                                                         useGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                                                         gpusWereDetected,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:    const bool canUseDirectGpuComm = decideWhetherDirectGpuCommunicationCanBeUsed(
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:    bool useGpuDirectHalo = false;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:    if (useGpuForNonbonded)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:        // domdecOptions.numPmeRanks == -1 results in 0 separate PME ranks when useGpuForNonbonded is true.
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:        useGpuDirectHalo = decideWhetherToUseGpuForHalo(havePPDomainDecomposition,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                                                        useGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                                                        canUseDirectGpuComm,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:        // The DD builder will disable useGpuDirectHalo if the Y or Z component of any domain is
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:        // smaller than twice the communication distance, since GPU-direct communication presently
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:        // perform well on multiple GPUs in any case, but it is important that our core functionality
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:        // (in particular for testing) does not break depending on GPU direct communication being enabled.
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                useGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                useGpuForPme,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                useGpuForUpdate,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                &useGpuDirectHalo,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                devFlags.enableGpuPmeDecomposition);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:    GpuTaskAssignments gpuTaskAssignments = GpuTaskAssignmentsBuilder::build(
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:            userGpuTaskAssignment,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:            useGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:            useGpuForPme,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:    DeviceInformation* deviceInfo = gpuTaskAssignments.initDevice(&deviceId);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:        // TODO Pass the GPU streams to ddBuilder to use in buffer
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:    const bool useGpuPmeDecomposition = numPmeDomains.x * numPmeDomains.y > 1 && useGpuForPme;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:    GMX_RELEASE_ASSERT(!useGpuPmeDecomposition || devFlags.enableGpuPmeDecomposition,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                       "GPU PME decomposition works only in the cases where it is supported");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                                                              useGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                                                              useGpuForBonded,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                                                              useGpuForUpdate,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                                                              useGpuDirectHalo,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                                                              canUseDirectGpuComm,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                                                              useGpuPmeDecomposition);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:    if (isSimulationMasterRank && GMX_GPU_SYCL)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:        bool                      haveAnyGpuWork = simWorkload.useGpuPme || simWorkload.useGpuBonded
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                              || simWorkload.useGpuNonbonded || simWorkload.useGpuUpdate;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:        if (haveAnyGpuWork)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                            "\nNOTE: SYCL GPU support in GROMACS is still new and less tested than "
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:    gpuTaskAssignments.reportGpuUsage(mdlog, printHostName, pmeRunMode, runScheduleWork.simulationWork);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:        const bool useGpuTiming = decideGpuTimingsUsage();
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                *deviceInfo, havePPDomainDecomposition(cr), runScheduleWork.simulationWork, useGpuTiming);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:    if (!userGpuTaskAssignment.empty())
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:        gpuTaskAssignments.logPerformanceHints(mdlog, numAvailableDevices);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:    check_resource_division_efficiency(hwinfo_, gpuTaskAssignments.thisRankHasAnyGpuTask(), cr, mdlog);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:    // Enable Peer access between GPUs where available
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:    // any of the GPU communication features are active.
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:        && (runScheduleWork.simulationWork.useGpuHaloExchange
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:            || runScheduleWork.simulationWork.useGpuPmePpCommunication))
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:        setupGpuDevicePeerAccess(gpuTaskAssignments.deviceIdsAssigned(), mdlog);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:    const bool thisRankHasPmeGpuTask = gpuTaskAssignments.thisRankHasPmeGpuTask();
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:        if (runScheduleWork.simulationWork.useGpuPmePpCommunication && !thisRankHasDuty(cr, DUTY_PME))
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                    "GPU device stream manager should be valid in order to use PME-PP direct "
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                    "GPU PP-PME stream should be valid in order to use GPU PME-PP direct "
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:            fr->pmePpCommGpu = std::make_unique<gmx::PmePpCommGpu>(
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                                        runScheduleWork.simulationWork.useGpuNonbonded,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:        // TODO: Move the logic below to a GPU bonded builder
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:        if (runScheduleWork.simulationWork.useGpuBonded)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                               "GPU device stream manager should be valid in order to use GPU "
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:            fr->listedForcesGpu = std::make_unique<ListedForcesGpu>(
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:        mdAtoms = makeMDAtoms(fplog, mtop, *inputrec, thisRankHasPmeGpuTask);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:        if (globalState && thisRankHasPmeGpuTask)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:            // PME on GPU without DD or on a separate PME rank, and because the local state pointer
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:    PmeGpuProgramStorage pmeGpuProgram;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:    if (thisRankHasPmeGpuTask)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                "GPU device stream manager should be initialized in order to use GPU for PME.");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                           "GPU device should be initialized in order to use GPU for PME.");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:        pmeGpuProgram = buildPmeGpuProgram(deviceStreamManager->context());
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                GMX_RELEASE_ASSERT(!runScheduleWork.simulationWork.useGpuPme
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                                   "Device stream manager should be valid in order to use GPU "
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                        !runScheduleWork.simulationWork.useGpuPme
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                        "GPU PME stream should be valid in order to use GPU version of PME.");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                const DeviceContext* deviceContext = runScheduleWork.simulationWork.useGpuPme
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                        runScheduleWork.simulationWork.useGpuPme
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                                       pmeGpuProgram.get(),
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:        if (runScheduleWork.simulationWork.useGpuFBufferOps)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:            fr->gpuForceReduction[gmx::AtomLocality::Local] = std::make_unique<gmx::GpuForceReduction>(
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:            fr->gpuForceReduction[gmx::AtomLocality::NonLocal] = std::make_unique<gmx::GpuForceReduction>(
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:        std::unique_ptr<gmx::StatePropagatorDataGpu> stateGpu;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:        if (gpusWereDetected && gmx::needStateGpu(runScheduleWork.simulationWork))
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:            GpuApiCallBehavior transferKind =
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                            ? GpuApiCallBehavior::Async
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                            : GpuApiCallBehavior::Sync;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                               "GPU device stream manager should be initialized to use GPU.");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:            stateGpu = std::make_unique<gmx::StatePropagatorDataGpu>(
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                    *deviceStreamManager, transferKind, pme_gpu_get_block_size(fr->pmedata), wcycle.get());
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:            fr->stateGpu = stateGpu.get();
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:          /* set GPU device id */
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:             plumed_cmd(plumedmain,"setGpuDeviceId", &deviceId);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:          if(useGpuForUpdate) {
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                        "This simulation is resident on GPU (-update gpu)\n"
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:        if (fr->pmePpCommGpu)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:            // destroy object since it is no longer required. (This needs to be done while the GPU context still exists.)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:            fr->pmePpCommGpu.reset();
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:                    runScheduleWork.simulationWork.useGpuPmePpCommunication,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:    // before we destroy the GPU context(s)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:    // Pinned buffers are associated with contexts in CUDA.
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:    // As soon as we destroy GPU contexts after mdrunner() exits, these lines should go.
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:    mdModules_.reset(nullptr); // destruct force providers here as they might also use the GPU
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:    fr.reset(nullptr);         // destruct forcerec before gpu
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:        /* stop the GPU profiler (only CUDA) */
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:        stopGpuProfiler();
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:     * destroying the CUDA context as some tMPI ranks may be sharing
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:     * GPU and context.
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:     * This is not a concern in OpenCL where we use one context per rank.
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:     * Note: it is safe to not call the barrier on the ranks which do not use GPU,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:     * Note that this function needs to be called even if GPUs are not used
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:     * in this run because the PME ranks have no knowledge of whether GPUs
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:     * that it's not needed anymore (with a shared GPU run).
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:    const bool haveDetectedOrForcedCudaAwareMpi =
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:            (gmx::checkMpiCudaAwareSupport() == gmx::GpuAwareMpiStatus::Supported
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:             || gmx::checkMpiCudaAwareSupport() == gmx::GpuAwareMpiStatus::Forced);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:    if (!haveDetectedOrForcedCudaAwareMpi)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:        // Don't reset GPU in case of GPU-AWARE MPI
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp:        // UCX creates GPU buffers which are cleaned-up as part of MPI_Finalize()
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/legacymdrunoptions.cpp:    // which compatible GPUs are availble for use, or to select a GPU
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/legacymdrunoptions.cpp:        hw_opt.userGpuTaskAssignment = userGpuTaskAssignment;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/legacymdrunoptions.cpp:        const char* env = getenv("GMX_GPU_ID");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/legacymdrunoptions.cpp:                gmx_fatal(FARGS, "GMX_GPU_ID and -gpu_id can not be used at the same time");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/legacymdrunoptions.cpp:        env = getenv("GMX_GPUTASKS");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/legacymdrunoptions.cpp:            if (!hw_opt.userGpuTaskAssignment.empty())
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/legacymdrunoptions.cpp:                gmx_fatal(FARGS, "GMX_GPUTASKS and -gputasks can not be used at the same time");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/legacymdrunoptions.cpp:            hw_opt.userGpuTaskAssignment = env;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/legacymdrunoptions.cpp:        if (!hw_opt.devicesSelectedByUser.empty() && !hw_opt.userGpuTaskAssignment.empty())
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/legacymdrunoptions.cpp:            gmx_fatal(FARGS, "-gpu_id and -gputasks cannot be used at the same time");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/legacymdrunoptions.cpp.preplumed:    // which compatible GPUs are availble for use, or to select a GPU
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/legacymdrunoptions.cpp.preplumed:        hw_opt.userGpuTaskAssignment = userGpuTaskAssignment;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/legacymdrunoptions.cpp.preplumed:        const char* env = getenv("GMX_GPU_ID");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/legacymdrunoptions.cpp.preplumed:                gmx_fatal(FARGS, "GMX_GPU_ID and -gpu_id can not be used at the same time");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/legacymdrunoptions.cpp.preplumed:        env = getenv("GMX_GPUTASKS");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/legacymdrunoptions.cpp.preplumed:            if (!hw_opt.userGpuTaskAssignment.empty())
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/legacymdrunoptions.cpp.preplumed:                gmx_fatal(FARGS, "GMX_GPUTASKS and -gputasks can not be used at the same time");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/legacymdrunoptions.cpp.preplumed:            hw_opt.userGpuTaskAssignment = env;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/legacymdrunoptions.cpp.preplumed:        if (!hw_opt.devicesSelectedByUser.empty() && !hw_opt.userGpuTaskAssignment.empty())
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/legacymdrunoptions.cpp.preplumed:            gmx_fatal(FARGS, "-gpu_id and -gputasks cannot be used at the same time");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:    const char* nbpu_opt_choices[5]    = { nullptr, "auto", "cpu", "gpu", nullptr };
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:    const char* pme_opt_choices[5]     = { nullptr, "auto", "cpu", "gpu", nullptr };
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:    const char* pme_fft_opt_choices[5] = { nullptr, "auto", "cpu", "gpu", nullptr };
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:    const char* bonded_opt_choices[5]  = { nullptr, "auto", "cpu", "gpu", nullptr };
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:    const char* update_opt_choices[5]  = { nullptr, "auto", "cpu", "gpu", nullptr };
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:    const char* userGpuTaskAssignment  = "";
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:        { "-gpu_id",
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:          "List of unique GPU device IDs available to use" },
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:        { "-gputasks",
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:          { &userGpuTaskAssignment },
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:          "List of GPU device IDs, mapping each PP task on each node to a device" },
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:          "Optimize PME load between PP/PME ranks or GPU/CPU" },
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/legacymdrunoptions.h:    const char* nbpu_opt_choices[5]    = { nullptr, "auto", "cpu", "gpu", nullptr };
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/legacymdrunoptions.h:    const char* pme_opt_choices[5]     = { nullptr, "auto", "cpu", "gpu", nullptr };
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/legacymdrunoptions.h:    const char* pme_fft_opt_choices[5] = { nullptr, "auto", "cpu", "gpu", nullptr };
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/legacymdrunoptions.h:    const char* bonded_opt_choices[5]  = { nullptr, "auto", "cpu", "gpu", nullptr };
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/legacymdrunoptions.h:    const char* update_opt_choices[5]  = { nullptr, "auto", "cpu", "gpu", nullptr };
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/legacymdrunoptions.h:    const char* userGpuTaskAssignment  = "";
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/legacymdrunoptions.h:        { "-gpu_id",
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/legacymdrunoptions.h:          "List of unique GPU device IDs available to use" },
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/legacymdrunoptions.h:        { "-gputasks",
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/legacymdrunoptions.h:          { &userGpuTaskAssignment },
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/legacymdrunoptions.h:          "List of GPU device IDs, mapping each PP task on each node to a device" },
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/legacymdrunoptions.h:          "Optimize PME load between PP/PME ranks or GPU/CPU" },
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:#include "gromacs/domdec/gpuhaloexchange.h"
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:#include "gromacs/ewald/pme_gpu_program.h"
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:#include "gromacs/ewald/pme_pp_comm_gpu.h"
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:#include "gromacs/gpu_utils/device_stream_manager.h"
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:#include "gromacs/listed_forces/listed_forces_gpu.h"
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:#include "gromacs/mdlib/gpuforcereduction.h"
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:#include "gromacs/mdtypes/state_propagator_data_gpu.h"
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:#include "gromacs/nbnxm/gpu_data_mgmt.h"
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:#include "gromacs/taskassignment/decidegpuusage.h"
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:#include "gromacs/taskassignment/usergpuids.h"
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:#include "gromacs/timing/gpu_timing.h"
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed: * the GPU communication flags are set to false in non-tMPI and non-CUDA builds.
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed: * \param[in]  useGpuForNonbonded   True if the nonbonded task is offloaded in this run.
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                                         const bool           useGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:    devFlags.enableGpuBufferOps = (GMX_GPU_CUDA || GMX_GPU_SYCL) && useGpuForNonbonded
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                  && (getenv("GMX_USE_GPU_BUFFER_OPS") != nullptr);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:    devFlags.forceGpuUpdateDefault = (getenv("GMX_FORCE_UPDATE_DEFAULT_GPU") != nullptr) || GMX_FAHCORE;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:    // Flag use to enable GPU-aware MPI depenendent features such PME GPU decomposition
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:    // GPU-aware MPI is marked available if it has been detected by GROMACS or detection fails but
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:    devFlags.canUseGpuAwareMpi = false;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:    // Direct GPU comm path is being used with GPU-aware MPI
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:    // make sure underlying MPI implementation is GPU-aware
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:    if (GMX_LIB_MPI && GMX_GPU_CUDA)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:        // Allow overriding the detection for GPU-aware MPI
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:        GpuAwareMpiStatus gpuAwareMpiStatus = checkMpiCudaAwareSupport();
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:        const bool        forceGpuAwareMpi  = gpuAwareMpiStatus == GpuAwareMpiStatus::Forced;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:        const bool haveDetectedGpuAwareMpi  = gpuAwareMpiStatus == GpuAwareMpiStatus::Supported;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:        if (getenv("GMX_FORCE_CUDA_AWARE_MPI") != nullptr)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                            "GMX_FORCE_CUDA_AWARE_MPI environment variable is inactive. "
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                            "Please use GMX_FORCE_GPU_AWARE_MPI instead.");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:        devFlags.canUseGpuAwareMpi = haveDetectedGpuAwareMpi || forceGpuAwareMpi;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:        if (getenv("GMX_ENABLE_DIRECT_GPU_COMM") != nullptr)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:            if (!haveDetectedGpuAwareMpi && forceGpuAwareMpi)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                // GPU-aware support not detected in MPI library but, user has forced it's use
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                "This run has forced use of 'GPU-aware MPI', ie. 'CUDA-aware MPI'. "
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                "However, GROMACS cannot determine if underlying MPI is GPU-aware. "
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                "GROMACS recommends use of latest OpenMPI version for GPU-aware "
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                "GMX_FORCE_GPU_AWARE_MPI environment variable.");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:            if (devFlags.canUseGpuAwareMpi)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                "GMX_ENABLE_DIRECT_GPU_COMM environment variable detected, "
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                "enabling direct GPU communication using GPU-aware MPI.");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                "GPU-aware MPI was not detected, will not use direct GPU "
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                "for GPU-aware support. If you are certain about GPU-aware support "
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                "GMX_FORCE_GPU_AWARE_MPI environment variable. Note that such "
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                "support is often called \"CUDA-aware MPI.\"");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:        else if (haveDetectedGpuAwareMpi)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:            // GPU-aware MPI was detected, let the user know that using it may improve performance
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                            "GPU-aware MPI detected, but by default GROMACS will not "
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                            "make use the direct GPU communication capabilities of MPI. "
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                            "the GMX_ENABLE_DIRECT_GPU_COMM environment variable.");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:        if (getenv("GMX_FORCE_GPU_AWARE_MPI") != nullptr)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:            // Cannot force use of GPU-aware MPI in this build configuration
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                            "A CUDA build with an external MPI library is required in order to "
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                            "benefit from GMX_FORCE_GPU_AWARE_MPI. That environment variable is "
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:    if (devFlags.enableGpuBufferOps)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                        "This run uses the 'GPU buffer ops' feature, enabled by the "
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                        "GMX_USE_GPU_BUFFER_OPS environment variable.");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:    if (devFlags.forceGpuUpdateDefault)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                        "This run will default to '-update gpu' as requested by the "
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                        "GMX_FORCE_UPDATE_DEFAULT_GPU environment variable.");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:    // PME decomposition is supported only with CUDA-backend in mixed mode
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:    // CUDA-backend also needs GPU-aware MPI support for decomposition to work
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:    const bool pmeGpuDecompositionRequested =
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:            (pmeRunMode == PmeRunMode::GPU || pmeRunMode == PmeRunMode::Mixed)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:    const bool pmeGpuDecompositionSupported =
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:            (devFlags.canUseGpuAwareMpi && pmeRunMode == PmeRunMode::Mixed);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:    const bool forcePmeGpuDecomposition = getenv("GMX_GPU_PME_DECOMPOSITION") != nullptr;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:    if (pmeGpuDecompositionSupported && pmeGpuDecompositionRequested)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:        // PME decomposition is supported only when it is forced using GMX_GPU_PME_DECOMPOSITION
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:        if (forcePmeGpuDecomposition)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                            "This run has requested the 'GPU PME decomposition' feature, enabled "
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                            "by the GMX_GPU_PME_DECOMPOSITION environment variable. "
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                      "Multiple PME tasks were required to run on GPUs, "
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                      "Use GMX_GPU_PME_DECOMPOSITION environment variable to enable it.");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:    if (!pmeGpuDecompositionSupported && pmeGpuDecompositionRequested)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                  "PME tasks were required to run on GPUs, but that is not implemented with "
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:    devFlags.enableGpuPmeDecomposition =
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:            forcePmeGpuDecomposition && pmeGpuDecompositionRequested && pmeGpuDecompositionSupported;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                  bool                makeGpuPairList,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                (makeGpuPairList ? ListSetupType::Gpu : ListSetupType::CpuSimdWhenSupported);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:        increaseNstlist(fplog, cr, ir, nstlist_cmdline, &mtop, box, makeGpuPairList, cpuinfo);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:/*! \brief Return whether GPU acceleration of nonbondeds is supported with the given settings.
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:static bool gpuAccelerationOfNonbondedIsUseful(const MDLogger&   mdlog,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:    bool        gpuIsUseful = true;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:        /* The GPU code does not support more than one energy group.
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:         * If the user requested GPUs explicitly, a fatal error is given later.
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:        gpuIsUseful = false;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                    "Multiple energy groups is not implemented for GPUs, falling back to the CPU. "
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                    "For better performance, run on the GPU without energy groups and then do "
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:    /* There are resource handling issues in the GPU code paths with MTS on anything else than only
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:        gpuIsUseful = false;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                "Multiple time stepping is only supported with GPUs when MTS is only applied to %s "
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:        gpuIsUseful = false;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:        warning     = "TPI is not implemented for GPUs.";
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:    if (!gpuIsUseful && issueWarning)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:    return gpuIsUseful;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:    else if (strncmp(optionString, "gpu", 3) == 0)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:        returnValue = TaskTarget::Gpu;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:        auto* nbnxn_gpu_timings =
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                (nbv != nullptr && nbv->useGpu()) ? Nbnxm::gpu_get_timings(nbv->gpu_nbv) : nullptr;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:        gmx_wallclock_gpu_pme_t pme_gpu_timings = {};
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:        if (pme_gpu_task_enabled(pme))
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:            pme_gpu_get_timings(pme, &pme_gpu_timings);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                        nbnxn_gpu_timings,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                        &pme_gpu_timings);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:    EmulateGpuNonbonded emulateGpuNonbonded =
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:            (getenv("GMX_EMULATE_GPU") != nullptr ? EmulateGpuNonbonded::Yes : EmulateGpuNonbonded::No);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:    std::vector<int> userGpuTaskAssignment;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:        userGpuTaskAssignment = parseUserTaskAssignmentString(hw_opt.userGpuTaskAssignment);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:        bool useGpuForNonbonded = false;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:        bool useGpuForPme       = false;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:            // the number of GPUs to choose the number of ranks.
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:            auto canUseGpuForNonbonded = buildSupportsNonbondedOnGpu(nullptr);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:            useGpuForNonbonded         = decideWhetherToUseGpusForNonbondedWithThreadMpi(
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                    userGpuTaskAssignment,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                    emulateGpuNonbonded,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                    canUseGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                    gpuAccelerationOfNonbondedIsUseful(mdlog, *inputrec, GMX_THREAD_MPI, doRerun),
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:            useGpuForPme = decideWhetherToUseGpusForPmeWithThreadMpi(useGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                                                     userGpuTaskAssignment,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                                useGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                                useGpuForPme,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:    // Note that when bonded interactions run on a GPU they always run
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:    bool useGpuForNonbonded = false;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:    bool useGpuForPme       = false;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:    bool useGpuForBonded    = false;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:    bool useGpuForUpdate    = false;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:    bool gpusWereDetected   = hwinfo_->ngpu_compatible_tot > 0;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:        // It's possible that there are different numbers of GPUs on
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:        auto canUseGpuForNonbonded = buildSupportsNonbondedOnGpu(nullptr);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:        useGpuForNonbonded         = decideWhetherToUseGpusForNonbonded(
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                userGpuTaskAssignment,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                emulateGpuNonbonded,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                canUseGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                gpuAccelerationOfNonbondedIsUseful(mdlog, *inputrec, !GMX_THREAD_MPI, doRerun),
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                gpusWereDetected);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:        useGpuForPme    = decideWhetherToUseGpusForPme(useGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                                    userGpuTaskAssignment,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                                    gpusWereDetected);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:        useGpuForBonded = decideWhetherToUseGpusForBonded(
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                useGpuForNonbonded, useGpuForPme, bondedTarget, *inputrec, mtop, domdecOptions.numPmeRanks, gpusWereDetected);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:    const PmeRunMode pmeRunMode = determinePmeRunMode(useGpuForPme, pmeFftTarget, *inputrec);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:            mdlog, useGpuForNonbonded, pmeRunMode, cr->sizeOfDefaultCommunicator, domdecOptions.numPmeRanks);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                || (!useGpuForNonbonded && EEL_FULL(inputrec->coulombtype) && useDDWithSingleRank != 0)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                          useGpuForNonbonded || (emulateGpuNonbonded == EmulateGpuNonbonded::Yes),
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:        useGpuForUpdate = decideWhetherToUseGpuForUpdate(useDomainDecomposition,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                                         useGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                                         gpusWereDetected,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:    const bool canUseDirectGpuComm = decideWhetherDirectGpuCommunicationCanBeUsed(
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:    bool useGpuDirectHalo = false;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:    if (useGpuForNonbonded)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:        // domdecOptions.numPmeRanks == -1 results in 0 separate PME ranks when useGpuForNonbonded is true.
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:        useGpuDirectHalo = decideWhetherToUseGpuForHalo(havePPDomainDecomposition,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                                        useGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                                        canUseDirectGpuComm,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:        // The DD builder will disable useGpuDirectHalo if the Y or Z component of any domain is
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:        // smaller than twice the communication distance, since GPU-direct communication presently
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:        // perform well on multiple GPUs in any case, but it is important that our core functionality
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:        // (in particular for testing) does not break depending on GPU direct communication being enabled.
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                useGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                useGpuForPme,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                useGpuForUpdate,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                &useGpuDirectHalo,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                devFlags.enableGpuPmeDecomposition);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:    GpuTaskAssignments gpuTaskAssignments = GpuTaskAssignmentsBuilder::build(
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:            userGpuTaskAssignment,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:            useGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:            useGpuForPme,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:    DeviceInformation* deviceInfo = gpuTaskAssignments.initDevice(&deviceId);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:        // TODO Pass the GPU streams to ddBuilder to use in buffer
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:    const bool useGpuPmeDecomposition = numPmeDomains.x * numPmeDomains.y > 1 && useGpuForPme;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:    GMX_RELEASE_ASSERT(!useGpuPmeDecomposition || devFlags.enableGpuPmeDecomposition,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                       "GPU PME decomposition works only in the cases where it is supported");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                                              useGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                                              useGpuForBonded,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                                              useGpuForUpdate,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                                              useGpuDirectHalo,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                                              canUseDirectGpuComm,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                                              useGpuPmeDecomposition);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:    if (isSimulationMasterRank && GMX_GPU_SYCL)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:        bool                      haveAnyGpuWork = simWorkload.useGpuPme || simWorkload.useGpuBonded
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                              || simWorkload.useGpuNonbonded || simWorkload.useGpuUpdate;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:        if (haveAnyGpuWork)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                            "\nNOTE: SYCL GPU support in GROMACS is still new and less tested than "
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:    gpuTaskAssignments.reportGpuUsage(mdlog, printHostName, pmeRunMode, runScheduleWork.simulationWork);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:        const bool useGpuTiming = decideGpuTimingsUsage();
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                *deviceInfo, havePPDomainDecomposition(cr), runScheduleWork.simulationWork, useGpuTiming);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:    if (!userGpuTaskAssignment.empty())
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:        gpuTaskAssignments.logPerformanceHints(mdlog, numAvailableDevices);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:    check_resource_division_efficiency(hwinfo_, gpuTaskAssignments.thisRankHasAnyGpuTask(), cr, mdlog);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:    // Enable Peer access between GPUs where available
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:    // any of the GPU communication features are active.
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:        && (runScheduleWork.simulationWork.useGpuHaloExchange
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:            || runScheduleWork.simulationWork.useGpuPmePpCommunication))
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:        setupGpuDevicePeerAccess(gpuTaskAssignments.deviceIdsAssigned(), mdlog);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:    const bool thisRankHasPmeGpuTask = gpuTaskAssignments.thisRankHasPmeGpuTask();
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:        if (runScheduleWork.simulationWork.useGpuPmePpCommunication && !thisRankHasDuty(cr, DUTY_PME))
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                    "GPU device stream manager should be valid in order to use PME-PP direct "
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                    "GPU PP-PME stream should be valid in order to use GPU PME-PP direct "
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:            fr->pmePpCommGpu = std::make_unique<gmx::PmePpCommGpu>(
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                        runScheduleWork.simulationWork.useGpuNonbonded,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:        // TODO: Move the logic below to a GPU bonded builder
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:        if (runScheduleWork.simulationWork.useGpuBonded)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                               "GPU device stream manager should be valid in order to use GPU "
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:            fr->listedForcesGpu = std::make_unique<ListedForcesGpu>(
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:        mdAtoms = makeMDAtoms(fplog, mtop, *inputrec, thisRankHasPmeGpuTask);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:        if (globalState && thisRankHasPmeGpuTask)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:            // PME on GPU without DD or on a separate PME rank, and because the local state pointer
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:    PmeGpuProgramStorage pmeGpuProgram;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:    if (thisRankHasPmeGpuTask)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                "GPU device stream manager should be initialized in order to use GPU for PME.");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                           "GPU device should be initialized in order to use GPU for PME.");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:        pmeGpuProgram = buildPmeGpuProgram(deviceStreamManager->context());
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                GMX_RELEASE_ASSERT(!runScheduleWork.simulationWork.useGpuPme
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                   "Device stream manager should be valid in order to use GPU "
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                        !runScheduleWork.simulationWork.useGpuPme
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                        "GPU PME stream should be valid in order to use GPU version of PME.");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                const DeviceContext* deviceContext = runScheduleWork.simulationWork.useGpuPme
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                        runScheduleWork.simulationWork.useGpuPme
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                       pmeGpuProgram.get(),
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:        if (runScheduleWork.simulationWork.useGpuFBufferOps)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:            fr->gpuForceReduction[gmx::AtomLocality::Local] = std::make_unique<gmx::GpuForceReduction>(
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:            fr->gpuForceReduction[gmx::AtomLocality::NonLocal] = std::make_unique<gmx::GpuForceReduction>(
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:        std::unique_ptr<gmx::StatePropagatorDataGpu> stateGpu;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:        if (gpusWereDetected && gmx::needStateGpu(runScheduleWork.simulationWork))
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:            GpuApiCallBehavior transferKind =
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                            ? GpuApiCallBehavior::Async
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                            : GpuApiCallBehavior::Sync;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                               "GPU device stream manager should be initialized to use GPU.");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:            stateGpu = std::make_unique<gmx::StatePropagatorDataGpu>(
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                    *deviceStreamManager, transferKind, pme_gpu_get_block_size(fr->pmedata), wcycle.get());
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:            fr->stateGpu = stateGpu.get();
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:        if (fr->pmePpCommGpu)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:            // destroy object since it is no longer required. (This needs to be done while the GPU context still exists.)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:            fr->pmePpCommGpu.reset();
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:                    runScheduleWork.simulationWork.useGpuPmePpCommunication,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:    // before we destroy the GPU context(s)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:    // Pinned buffers are associated with contexts in CUDA.
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:    // As soon as we destroy GPU contexts after mdrunner() exits, these lines should go.
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:    mdModules_.reset(nullptr); // destruct force providers here as they might also use the GPU
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:    fr.reset(nullptr);         // destruct forcerec before gpu
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:        /* stop the GPU profiler (only CUDA) */
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:        stopGpuProfiler();
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:     * destroying the CUDA context as some tMPI ranks may be sharing
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:     * GPU and context.
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:     * This is not a concern in OpenCL where we use one context per rank.
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:     * Note: it is safe to not call the barrier on the ranks which do not use GPU,
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:     * Note that this function needs to be called even if GPUs are not used
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:     * in this run because the PME ranks have no knowledge of whether GPUs
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:     * that it's not needed anymore (with a shared GPU run).
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:    const bool haveDetectedOrForcedCudaAwareMpi =
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:            (gmx::checkMpiCudaAwareSupport() == gmx::GpuAwareMpiStatus::Supported
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:             || gmx::checkMpiCudaAwareSupport() == gmx::GpuAwareMpiStatus::Forced);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:    if (!haveDetectedOrForcedCudaAwareMpi)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:        // Don't reset GPU in case of GPU-AWARE MPI
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/runner.cpp.preplumed:        // UCX creates GPU buffers which are cleaned-up as part of MPI_Finalize()
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:#include "gromacs/domdec/gpuhaloexchange.h"
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:#include "gromacs/gpu_utils/device_stream_manager.h"
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:#include "gromacs/gpu_utils/gpu_utils.h"
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:#include "gromacs/mdlib/update_constrain_gpu.h"
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:#include "gromacs/mdtypes/state_propagator_data_gpu.h"
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:#include "gromacs/nbnxm/gpu_data_mgmt.h"
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:    /* PME load balancing data for GPU kernels */
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:    const bool  useGpuForPme       = simulationWork.useGpuPme;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:    const bool  useGpuForNonbonded = simulationWork.useGpuNonbonded;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:    const bool  useGpuForUpdate    = simulationWork.useGpuUpdate;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:                                 useGpuForPme);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:                   (simulationWork.useGpuFBufferOps || useGpuForUpdate) ? PinningPolicy::PinnedIfSupported
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:    std::unique_ptr<UpdateConstrainGpu> integrator;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:    StatePropagatorDataGpu* stateGpu = fr->stateGpu;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:    if (useGpuForUpdate)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:                           "groups if using GPU update.\n");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:                           "SHAKE is not supported with GPU update.");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:        GMX_RELEASE_ASSERT(useGpuForPme || (useGpuForNonbonded && simulationWork.useGpuXBufferOps),
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:                           "the GPU to use GPU update.\n");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:                           "Only the md integrator is supported with the GPU update.\n");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:                "Nose-Hoover temperature coupling is not supported with the GPU update.\n");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:                "with the GPU update.\n");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:                           "Virtual sites are not supported with the GPU update.\n");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:                           "Essential dynamics is not supported with the GPU update.\n");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:                           "Constraints pulling is not supported with the GPU update.\n");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:                           "Orientation restraints are not supported with the GPU update.\n");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:                "Free energy perturbation of masses and constraints are not supported with the GPU "
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:                    .appendText("Updating coordinates and applying constraints on the GPU.");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:            GMX_LOG(mdlog.info).asParagraph().appendText("Updating coordinates on the GPU.");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:                           "Device stream manager should be initialized in order to use GPU "
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:                "Update stream should be initialized in order to use GPU "
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:        integrator = std::make_unique<UpdateConstrainGpu>(
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:        stateGpu->setXUpdatedOnDeviceEvent(integrator->xUpdatedOnDeviceEvent());
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:    if (useGpuForPme || simulationWork.useGpuXBufferOps || useGpuForUpdate)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:    if (useGpuForUpdate)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:     * Disable PME tuning with GPU PME decomposition */
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:                && ir->cutoff_scheme != CutoffScheme::Group && !simulationWork.useGpuPmeDecomposition);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:                &pme_loadbal, cr, mdlog, *ir, state->box, *fr->ic, *fr->nbv, fr->pmedata, fr->nbv->useGpu());
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:            if (useGpuForUpdate && !bFirstStep)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:                stateGpu->copyCoordinatesFromGpu(state->x, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:                stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:            /* PME grid + cut-off optimization with GPUs or PME nodes */
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:                           simulationWork.useGpuPmePpCommunication);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:        // On search steps, when doing the update on the GPU, copy
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:        if (useGpuForUpdate && bNS && !bFirstStep && !bExchanged)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:            stateGpu->copyVelocitiesFromGpu(state->v, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:            stateGpu->copyCoordinatesFromGpu(state->x, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:            stateGpu->waitVelocitiesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:            stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:            // the GPU Update object should be informed
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:            if (useGpuForUpdate && (bMasterState || bExchanged))
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:        // Allocate or re-size GPU halo exchange object, if necessary
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:        if (bNS && simulationWork.havePpDomainDecomposition && simulationWork.useGpuHaloExchange)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:                               "GPU device manager has to be initialized to use GPU "
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:            constructGpuHaloExchange(*cr, *fr->deviceStreamManager, wcycle);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:        // Copy coordinate from the GPU for the output/checkpointing if the update is offloaded and
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:        if (useGpuForUpdate && !bNS && !runScheduleWork->domainWork.haveCpuLocalForceWork
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:            stateGpu->copyCoordinatesFromGpu(state->x, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:            stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:        if (useGpuForUpdate && !bNS
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:            stateGpu->copyVelocitiesFromGpu(state->v, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:            stateGpu->waitVelocitiesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:        // Copy forces for the output if the forces were reduced on the GPU (not the case on virial steps)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:        // and update is offloaded hence forces are kept on the GPU for update and have not been
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:        //       when the forces are ready on the GPU -- the same synchronizer should be used as the one
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:        //       prior to GPU update.
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:        if (runScheduleWork->stepWork.useGpuFBufferOps && (simulationWork.useGpuUpdate && !vsite)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:            stateGpu->copyForcesFromGpu(f.view().force(), AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:            stateGpu->waitForcesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:        if (!useGpuForUpdate)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:            GMX_ASSERT(!useGpuForUpdate, "GPU update is not supported with VVAK integrator.");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:            if (useGpuForUpdate)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:                    integrator->set(stateGpu->getCoordinates(),
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:                                    stateGpu->getVelocities(),
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:                                    stateGpu->getForces(),
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:                    // Copy data to the GPU after buffers might have been reinitialized
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:                    stateGpu->copyVelocitiesToGpu(state->v, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:                // Copy x to the GPU unless we have already transferred in do_force().
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:                // We transfer in do_force() if a GPU force task requires x (PME or x buffer ops).
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:                if (!(runScheduleWork->stepWork.haveGpuPmeOnThisRank
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:                      || runScheduleWork->stepWork.useGpuXBufferOps))
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:                    stateGpu->copyCoordinatesToGpu(state->x, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:                    stateGpu->consumeCoordinatesCopiedToDeviceEvent(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:                if ((simulationWork.useGpuPme && simulationWork.useCpuPmePpCommunication)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:                    || (!runScheduleWork->stepWork.useGpuFBufferOps))
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:                    // rest of the forces computed on the GPU, so the final forces have to be copied
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:                    // back to the GPU. Or the buffer ops were not offloaded this step, so the
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:                    stateGpu->copyForcesToGpu(f.view().force(), AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:                integrator->integrate(stateGpu->getLocalForcesReadyOnDeviceEvent(
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:            if (useGpuForUpdate)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:                    stateGpu->copyCoordinatesFromGpu(state->x, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:                    stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:                    stateGpu->copyVelocitiesFromGpu(state->v, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:                    stateGpu->waitVelocitiesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:                    if (useGpuForUpdate)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:                        stateGpu->resetCoordinatesCopiedToDeviceEvent(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:                        stateGpu->copyCoordinatesToGpu(state->x, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:                        stateGpu->waitCoordinatesCopiedToDevice(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:                            stateGpu->copyVelocitiesToGpu(state->v, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:        bool scaleCoordinates = !useGpuForUpdate || bDoReplEx;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:        if (useGpuForUpdate
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp.preplumed:        pme_loadbal_done(pme_loadbal, fplog, mdlog, fr->nbv->useGpu());
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/rerun.cpp.preplumed:#include "gromacs/gpu_utils/gpu_utils.h"
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/rerun.cpp.preplumed:                                 runScheduleWork->simulationWork.useGpuPme);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/rerun.cpp:#include "gromacs/gpu_utils/gpu_utils.h"
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/rerun.cpp:                                 runScheduleWork->simulationWork.useGpuPme);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:#include "gromacs/domdec/gpuhaloexchange.h"
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:#include "gromacs/gpu_utils/device_stream_manager.h"
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:#include "gromacs/gpu_utils/gpu_utils.h"
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:#include "gromacs/mdlib/update_constrain_gpu.h"
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:#include "gromacs/mdtypes/state_propagator_data_gpu.h"
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:#include "gromacs/nbnxm/gpu_data_mgmt.h"
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:    /* PME load balancing data for GPU kernels */
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:    const bool  useGpuForPme       = simulationWork.useGpuPme;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:    const bool  useGpuForNonbonded = simulationWork.useGpuNonbonded;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:    const bool  useGpuForUpdate    = simulationWork.useGpuUpdate;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:                                 useGpuForPme);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:                   (simulationWork.useGpuFBufferOps || useGpuForUpdate) ? PinningPolicy::PinnedIfSupported
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:    std::unique_ptr<UpdateConstrainGpu> integrator;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:    StatePropagatorDataGpu* stateGpu = fr->stateGpu;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:    if (useGpuForUpdate)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:                           "groups if using GPU update.\n");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:                           "SHAKE is not supported with GPU update.");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:        GMX_RELEASE_ASSERT(useGpuForPme || (useGpuForNonbonded && simulationWork.useGpuXBufferOps),
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:                           "the GPU to use GPU update.\n");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:                           "Only the md integrator is supported with the GPU update.\n");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:                "Nose-Hoover temperature coupling is not supported with the GPU update.\n");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:                "with the GPU update.\n");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:                           "Virtual sites are not supported with the GPU update.\n");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:                           "Essential dynamics is not supported with the GPU update.\n");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:                           "Constraints pulling is not supported with the GPU update.\n");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:                           "Orientation restraints are not supported with the GPU update.\n");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:                "Free energy perturbation of masses and constraints are not supported with the GPU "
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:                    .appendText("Updating coordinates and applying constraints on the GPU.");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:            GMX_LOG(mdlog.info).asParagraph().appendText("Updating coordinates on the GPU.");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:                           "Device stream manager should be initialized in order to use GPU "
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:                "Update stream should be initialized in order to use GPU "
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:        integrator = std::make_unique<UpdateConstrainGpu>(
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:        stateGpu->setXUpdatedOnDeviceEvent(integrator->xUpdatedOnDeviceEvent());
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:    if (useGpuForPme || simulationWork.useGpuXBufferOps || useGpuForUpdate)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:    if (useGpuForUpdate)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:     * Disable PME tuning with GPU PME decomposition */
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:                && ir->cutoff_scheme != CutoffScheme::Group && !simulationWork.useGpuPmeDecomposition);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:                &pme_loadbal, cr, mdlog, *ir, state->box, *fr->ic, *fr->nbv, fr->pmedata, fr->nbv->useGpu());
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:            if (useGpuForUpdate && !bFirstStep)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:                stateGpu->copyCoordinatesFromGpu(state->x, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:                stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:            /* PME grid + cut-off optimization with GPUs or PME nodes */
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:                           simulationWork.useGpuPmePpCommunication);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:        // On search steps, when doing the update on the GPU, copy
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:        if (useGpuForUpdate && bNS && !bFirstStep && !bExchanged)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:            stateGpu->copyVelocitiesFromGpu(state->v, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:            stateGpu->copyCoordinatesFromGpu(state->x, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:            stateGpu->waitVelocitiesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:            stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:            // the GPU Update object should be informed
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:            if (useGpuForUpdate && (bMasterState || bExchanged))
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:        // Allocate or re-size GPU halo exchange object, if necessary
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:        if (bNS && simulationWork.havePpDomainDecomposition && simulationWork.useGpuHaloExchange)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:                               "GPU device manager has to be initialized to use GPU "
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:            constructGpuHaloExchange(*cr, *fr->deviceStreamManager, wcycle);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:        // Copy coordinate from the GPU for the output/checkpointing if the update is offloaded and
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:        if (useGpuForUpdate && !bNS && !runScheduleWork->domainWork.haveCpuLocalForceWork
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:            stateGpu->copyCoordinatesFromGpu(state->x, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:            stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:        if (useGpuForUpdate && !bNS
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:            stateGpu->copyVelocitiesFromGpu(state->v, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:            stateGpu->waitVelocitiesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:        // Copy forces for the output if the forces were reduced on the GPU (not the case on virial steps)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:        // and update is offloaded hence forces are kept on the GPU for update and have not been
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:        //       when the forces are ready on the GPU -- the same synchronizer should be used as the one
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:        //       prior to GPU update.
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:        if (runScheduleWork->stepWork.useGpuFBufferOps && (simulationWork.useGpuUpdate && !vsite)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:            stateGpu->copyForcesFromGpu(f.view().force(), AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:            stateGpu->waitForcesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:        if (!useGpuForUpdate)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:            GMX_ASSERT(!useGpuForUpdate, "GPU update is not supported with VVAK integrator.");
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:            if (useGpuForUpdate)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:                    integrator->set(stateGpu->getCoordinates(),
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:                                    stateGpu->getVelocities(),
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:                                    stateGpu->getForces(),
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:                    // Copy data to the GPU after buffers might have been reinitialized
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:                    stateGpu->copyVelocitiesToGpu(state->v, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:                // Copy x to the GPU unless we have already transferred in do_force().
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:                // We transfer in do_force() if a GPU force task requires x (PME or x buffer ops).
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:                if (!(runScheduleWork->stepWork.haveGpuPmeOnThisRank
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:                      || runScheduleWork->stepWork.useGpuXBufferOps))
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:                    stateGpu->copyCoordinatesToGpu(state->x, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:                    stateGpu->consumeCoordinatesCopiedToDeviceEvent(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:                if ((simulationWork.useGpuPme && simulationWork.useCpuPmePpCommunication)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:                    || (!runScheduleWork->stepWork.useGpuFBufferOps))
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:                    // rest of the forces computed on the GPU, so the final forces have to be copied
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:                    // back to the GPU. Or the buffer ops were not offloaded this step, so the
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:                    stateGpu->copyForcesToGpu(f.view().force(), AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:                integrator->integrate(stateGpu->getLocalForcesReadyOnDeviceEvent(
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:            if (useGpuForUpdate)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:                    stateGpu->copyCoordinatesFromGpu(state->x, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:                    stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:                    stateGpu->copyVelocitiesFromGpu(state->v, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:                    stateGpu->waitVelocitiesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:                    if (useGpuForUpdate)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:                        stateGpu->resetCoordinatesCopiedToDeviceEvent(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:                        stateGpu->copyCoordinatesToGpu(state->x, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:                        stateGpu->waitCoordinatesCopiedToDevice(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:                            stateGpu->copyVelocitiesToGpu(state->v, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:        bool scaleCoordinates = !useGpuForUpdate || bDoReplEx;
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:        if (useGpuForUpdate
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/mdrun/md.cpp:        pme_loadbal_done(pme_loadbal, fplog, mdlog, fr->nbv->useGpu());
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:if (GMX_CLANG_CUDA)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:    include(gmxClangCudaUtils)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:set_property(GLOBAL PROPERTY CUDA_SOURCES)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:add_subdirectory(gpu_utils)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:# Mark some shared GPU implementation files to compile with CUDA if needed
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:if (GMX_GPU_CUDA)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:    get_property(CUDA_SOURCES GLOBAL PROPERTY CUDA_SOURCES)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:    set_source_files_properties(${CUDA_SOURCES} PROPERTIES CUDA_SOURCE_PROPERTY_FORMAT OBJ)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:if (GMX_GPU_CUDA)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:    # Work around FindCUDA that prevents using target_link_libraries()
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:    set(CUDA_LIBRARIES PRIVATE ${CUDA_LIBRARIES})
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:    if (NOT GMX_CLANG_CUDA)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:        gmx_cuda_add_library(libgromacs ${LIBGROMACS_SOURCES})
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:    target_link_libraries(libgromacs PRIVATE ${CUDA_CUFFT_LIBRARIES})
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:if (GMX_GPU_OPENCL)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:"An OpenCL build was requested with Visual Studio compiler, but GROMACS
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:clFFT to help with building for OpenCL, but that clFFT has not yet been
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:requires. Thus for now, OpenCL is not available with MSVC and the internal
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:# CUDA runtime headers
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:if (GMX_GPU_CUDA AND CMAKE_CXX_COMPILER_ID MATCHES "Clang")
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:        set(GMX_CUDA_CLANG_FLAGS "${GMX_CUDA_CLANG_FLAGS} ${_compile_flag}")
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:    if (GMX_CLANG_CUDA)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:            get_source_file_property(_cuda_source_format ${_file} CUDA_SOURCE_PROPERTY_FORMAT)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:            if ("${_ext}" STREQUAL ".cu" OR _cuda_source_format)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:                gmx_compile_cuda_file_with_clang(${_file})
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:        get_property(CUDA_SOURCES GLOBAL PROPERTY CUDA_SOURCES)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:        set_source_files_properties(${CUDA_SOURCES} PROPERTIES COMPILE_FLAGS ${GMX_CUDA_CLANG_FLAGS})
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:if (GMX_GPU_SYCL)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:                      ${OpenCL_LIBRARIES}
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:                      $<BUILD_INTERFACE:gpu_utils>
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:# Technically, the user could want to do this for an OpenCL build
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:# using the CUDA runtime, but currently there's no reason to want to
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:if (INSTALL_CUDART_LIB) #can be set manual by user
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:    if (GMX_GPU_CUDA)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:        foreach(CUDA_LIB ${CUDA_LIBRARIES})
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:            string(REGEX MATCH "cudart" IS_CUDART ${CUDA_LIB})
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:            if(IS_CUDART) #libcuda should not be installed
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:                file(GLOB CUDA_LIBS ${CUDA_LIB}*)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:                install(FILES ${CUDA_LIBS} DESTINATION
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:        message(WARNING "INSTALL_CUDART_LIB only makes sense when configuring for CUDA support")
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:if(GMX_GPU_OPENCL)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:    file(GLOB OPENCL_INSTALLED_FILES
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:        gpu_utils/vectype_ops.clh
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:        gpu_utils/device_utils.clh
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:    install(FILES ${OPENCL_INSTALLED_FILES}
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:        DESTINATION ${GMX_INSTALL_OCLDIR}/gromacs/gpu_utils
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:    file(GLOB OPENCL_INSTALLED_FILES
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:    install(FILES ${OPENCL_INSTALLED_FILES}
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:    file(GLOB OPENCL_INSTALLED_FILES
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:    install(FILES ${OPENCL_INSTALLED_FILES}
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:    file(GLOB OPENCL_INSTALLED_FILES
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:        nbnxm/opencl/nbnxm_ocl_kernels.cl
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:        nbnxm/opencl/nbnxm_ocl_kernel.clh
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:        nbnxm/opencl/nbnxm_ocl_kernel_pruneonly.clh
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:        nbnxm/opencl/nbnxm_ocl_kernels.clh
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:        nbnxm/opencl/nbnxm_ocl_kernels_fastgen.clh
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:        nbnxm/opencl/nbnxm_ocl_kernels_fastgen_add_twincut.clh
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:        nbnxm/opencl/nbnxm_ocl_kernel_utils.clh
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:        nbnxm/opencl/nbnxm_ocl_consts.h
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:    install(FILES ${OPENCL_INSTALLED_FILES}
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:        DESTINATION ${GMX_INSTALL_OCLDIR}/gromacs/nbnxm/opencl
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:    file(GLOB OPENCL_INSTALLED_FILES
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:        ewald/pme_gpu_calculate_splines.clh
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:        ewald/pme_gpu_types.h
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt:    install(FILES ${OPENCL_INSTALLED_FILES}
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:if (GMX_CLANG_CUDA)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:    include(gmxClangCudaUtils)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:set_property(GLOBAL PROPERTY CUDA_SOURCES)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:add_subdirectory(gpu_utils)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:# Mark some shared GPU implementation files to compile with CUDA if needed
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:if (GMX_GPU_CUDA)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:    get_property(CUDA_SOURCES GLOBAL PROPERTY CUDA_SOURCES)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:    set_source_files_properties(${CUDA_SOURCES} PROPERTIES CUDA_SOURCE_PROPERTY_FORMAT OBJ)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:if (GMX_GPU_CUDA)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:    # Work around FindCUDA that prevents using target_link_libraries()
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:    set(CUDA_LIBRARIES PRIVATE ${CUDA_LIBRARIES})
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:    if (NOT GMX_CLANG_CUDA)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:        gmx_cuda_add_library(libgromacs ${LIBGROMACS_SOURCES})
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:    target_link_libraries(libgromacs PRIVATE ${CUDA_CUFFT_LIBRARIES})
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:if (GMX_GPU_OPENCL)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:"An OpenCL build was requested with Visual Studio compiler, but GROMACS
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:clFFT to help with building for OpenCL, but that clFFT has not yet been
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:requires. Thus for now, OpenCL is not available with MSVC and the internal
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:# CUDA runtime headers
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:if (GMX_GPU_CUDA AND CMAKE_CXX_COMPILER_ID MATCHES "Clang")
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:        set(GMX_CUDA_CLANG_FLAGS "${GMX_CUDA_CLANG_FLAGS} ${_compile_flag}")
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:    if (GMX_CLANG_CUDA)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:            get_source_file_property(_cuda_source_format ${_file} CUDA_SOURCE_PROPERTY_FORMAT)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:            if ("${_ext}" STREQUAL ".cu" OR _cuda_source_format)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:                gmx_compile_cuda_file_with_clang(${_file})
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:        get_property(CUDA_SOURCES GLOBAL PROPERTY CUDA_SOURCES)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:        set_source_files_properties(${CUDA_SOURCES} PROPERTIES COMPILE_FLAGS ${GMX_CUDA_CLANG_FLAGS})
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:if (GMX_GPU_SYCL)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:                      ${OpenCL_LIBRARIES}
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:                      $<BUILD_INTERFACE:gpu_utils>
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:# Technically, the user could want to do this for an OpenCL build
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:# using the CUDA runtime, but currently there's no reason to want to
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:if (INSTALL_CUDART_LIB) #can be set manual by user
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:    if (GMX_GPU_CUDA)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:        foreach(CUDA_LIB ${CUDA_LIBRARIES})
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:            string(REGEX MATCH "cudart" IS_CUDART ${CUDA_LIB})
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:            if(IS_CUDART) #libcuda should not be installed
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:                file(GLOB CUDA_LIBS ${CUDA_LIB}*)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:                install(FILES ${CUDA_LIBS} DESTINATION
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:        message(WARNING "INSTALL_CUDART_LIB only makes sense when configuring for CUDA support")
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:if(GMX_GPU_OPENCL)
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:    file(GLOB OPENCL_INSTALLED_FILES
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:        gpu_utils/vectype_ops.clh
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:        gpu_utils/device_utils.clh
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:    install(FILES ${OPENCL_INSTALLED_FILES}
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:        DESTINATION ${GMX_INSTALL_OCLDIR}/gromacs/gpu_utils
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:    file(GLOB OPENCL_INSTALLED_FILES
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:    install(FILES ${OPENCL_INSTALLED_FILES}
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:    file(GLOB OPENCL_INSTALLED_FILES
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:    install(FILES ${OPENCL_INSTALLED_FILES}
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:    file(GLOB OPENCL_INSTALLED_FILES
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:        nbnxm/opencl/nbnxm_ocl_kernels.cl
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:        nbnxm/opencl/nbnxm_ocl_kernel.clh
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:        nbnxm/opencl/nbnxm_ocl_kernel_pruneonly.clh
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:        nbnxm/opencl/nbnxm_ocl_kernels.clh
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:        nbnxm/opencl/nbnxm_ocl_kernels_fastgen.clh
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:        nbnxm/opencl/nbnxm_ocl_kernels_fastgen_add_twincut.clh
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:        nbnxm/opencl/nbnxm_ocl_kernel_utils.clh
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:        nbnxm/opencl/nbnxm_ocl_consts.h
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:    install(FILES ${OPENCL_INSTALLED_FILES}
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:        DESTINATION ${GMX_INSTALL_OCLDIR}/gromacs/nbnxm/opencl
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:    file(GLOB OPENCL_INSTALLED_FILES
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:        ewald/pme_gpu_calculate_splines.clh
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:        ewald/pme_gpu_types.h
v1.2/plumed2/patches/gromacs-2022.5.diff/src/gromacs/CMakeLists.txt.preplumed:    install(FILES ${OPENCL_INSTALLED_FILES}
v1.2/plumed2/patches/namd-2.14.diff: 	$(CUDALIB) \
v1.2/plumed2/patches/namd-2.14.diff: #include "DeviceCUDA.h"
v1.2/plumed2/patches/namd-2.14.diff: #ifdef NAMD_CUDA
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:#include "gromacs/domdec/gpuhaloexchange.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:#include "gromacs/ewald/pme_coordinate_receiver_gpu.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:#include "gromacs/ewald/pme_pp_comm_gpu.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:#include "gromacs/gpu_utils/gpu_utils.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:#include "gromacs/listed_forces/listed_forces_gpu.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:#include "gromacs/mdtypes/state_propagator_data_gpu.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:#include "gromacs/nbnxm/gpu_data_mgmt.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:#include "gromacs/nbnxm/nbnxm_gpu.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:#include "gromacs/timing/gpu_timing.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:#include "gpuforcereduction.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:static const bool c_disableAlternatingWait = (getenv("GMX_DISABLE_ALTERNATING_GPU_WAIT") != nullptr);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                   bool                  useGpuPmePpComms,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                   bool                  receivePmeForceToGpu,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    gmx_pme_receive_f(fr->pmePpCommGpu.get(),
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                      useGpuPmePpComms,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                      receivePmeForceToGpu,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    /* GPU kernel launch overhead is already timed separately */
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (!nbv->useGpu())
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:/*! \brief Launch the prepare_step and spread stages of PME GPU.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:static inline void launchPmeGpuSpread(gmx_pme_t*            pmedata,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                      GpuEventSynchronizer* xReadyOnDevice,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    wallcycle_start(wcycle, WallCycleCounter::PmeGpuMesh);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    pme_gpu_prepare_computation(pmedata, box, wcycle, stepWork);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    bool                           useGpuDirectComm         = false;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    gmx::PmeCoordinateReceiverGpu* pmeCoordinateReceiverGpu = nullptr;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    pme_gpu_launch_spread(
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            pmedata, xReadyOnDevice, wcycle, lambdaQ, useGpuDirectComm, pmeCoordinateReceiverGpu);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    wallcycle_stop(wcycle, WallCycleCounter::PmeGpuMesh);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:/*! \brief Launch the FFT and gather stages of PME GPU
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:static void launchPmeGpuFftAndGather(gmx_pme_t*               pmedata,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    wallcycle_start_nocount(wcycle, WallCycleCounter::PmeGpuMesh);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    pme_gpu_launch_complex_transforms(pmedata, wcycle, stepWork);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    pme_gpu_launch_gather(pmedata, wcycle, lambdaQ);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    wallcycle_stop(wcycle, WallCycleCounter::PmeGpuMesh);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed: * Blocks until PME GPU tasks are completed, and gets the output forces and virial/energy
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:static void pmeGpuWaitAndReduce(gmx_pme_t*               pme,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    wallcycle_start_nocount(wcycle, WallCycleCounter::PmeGpuMesh);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    pme_gpu_wait_and_reduce(pme, stepWork, wcycle, forceWithVirial, enerd, lambdaQ);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    wallcycle_stop(wcycle, WallCycleCounter::PmeGpuMesh);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed: *  Polling wait for either of the PME or nonbonded GPU tasks.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed: * Instead of a static order in waiting for GPU tasks, this function
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed: * one of the reductions, regardless of the GPU task completion order.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:static void alternatePmeNbGpuWaitReduce(nonbonded_verlet_t* nbv,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    bool isPmeGpuDone = false;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    bool isNbGpuDone  = false;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    gmx::ArrayRef<const gmx::RVec> pmeGpuForces;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    while (!isPmeGpuDone || !isNbGpuDone)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        if (!isPmeGpuDone)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            wallcycle_start_nocount(wcycle, WallCycleCounter::PmeGpuMesh);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            GpuTaskCompletion completionType =
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                    (isNbGpuDone) ? GpuTaskCompletion::Wait : GpuTaskCompletion::Check;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            isPmeGpuDone = pme_gpu_try_finish_task(
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            wallcycle_stop(wcycle, WallCycleCounter::PmeGpuMesh);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        if (!isNbGpuDone)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            GpuTaskCompletion completionType =
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                    (isPmeGpuDone) ? GpuTaskCompletion::Wait : GpuTaskCompletion::Check;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            isNbGpuDone = Nbnxm::gpu_try_finish_task(
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                    nbv->gpu_nbv,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            if (isNbGpuDone)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        && (domainWork.haveCpuLocalForceWork || !stepWork.useGpuFBufferOps
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            || (havePpDomainDecomposition && !stepWork.useGpuFHalo)))
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    domainWork.haveGpuBondedWork =
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            ((fr.listedForcesGpu != nullptr) && fr.listedForcesGpu->haveInteractions());
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (simulationWork.useGpuXBufferOps || simulationWork.useGpuFBufferOps)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        GMX_ASSERT(simulationWork.useGpuNonbonded,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    flags.useGpuXBufferOps = simulationWork.useGpuXBufferOps && !flags.doNeighborSearch;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    flags.useGpuFBufferOps = simulationWork.useGpuFBufferOps && !flags.computeVirial;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    flags.useGpuPmeFReduction =
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            flags.computeSlowForces && flags.useGpuFBufferOps
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            && (simulationWork.haveGpuPmeOnPpRank() || simulationWork.useGpuPmePpCommunication);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    flags.useGpuXHalo              = simulationWork.useGpuHaloExchange && !flags.doNeighborSearch;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    flags.useGpuFHalo              = simulationWork.useGpuHaloExchange && flags.useGpuFBufferOps;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    flags.haveGpuPmeOnThisRank     = simulationWork.haveGpuPmeOnPpRank() && flags.computeSlowForces;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:             && !(flags.computeVirial || simulationWork.useGpuNonbonded || flags.haveGpuPmeOnThisRank));
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    // On NS steps, the buffer is cleared in stateGpu->reinit, no need to clear it twice.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    flags.clearGpuFBufferEarly =
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            flags.useGpuFHalo && !domainWork.haveCpuLocalForceWork && !flags.doNeighborSearch;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:/* \brief Launch end-of-step GPU tasks: buffer clearing and rolling pruning.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:static void launchGpuEndOfStepTasks(nonbonded_verlet_t*               nbv,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                    gmx::ListedForcesGpu*             listedForcesGpu,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (runScheduleWork.simulationWork.useGpuNonbonded && runScheduleWork.stepWork.computeNonbondedForces)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:         * clear kernel launches can leave the GPU idle while it could be running
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        if (nbv->isDynamicPruningStepGpu(step))
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            nbv->dispatchPruneKernelGpu(step);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        /* now clear the GPU outputs while we finish the step on the CPU */
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        wallcycle_start_nocount(wcycle, WallCycleCounter::LaunchGpuPp);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        wallcycle_sub_start_nocount(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        Nbnxm::gpu_clear_outputs(nbv->gpu_nbv, runScheduleWork.stepWork.computeVirial);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        wallcycle_sub_stop(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        wallcycle_stop(wcycle, WallCycleCounter::LaunchGpuPp);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (runScheduleWork.stepWork.haveGpuPmeOnThisRank)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        wallcycle_start_nocount(wcycle, WallCycleCounter::PmeGpuMesh);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        pme_gpu_reinit_computation(pmedata, runScheduleWork.simulationWork.useMdGpuGraph, wcycle);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        wallcycle_stop(wcycle, WallCycleCounter::PmeGpuMesh);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (runScheduleWork.domainWork.haveGpuBondedWork && runScheduleWork.stepWork.computeEnergy)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        listedForcesGpu->waitAccumulateEnergyTerms(enerd);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        listedForcesGpu->clearEnergies();
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:/*! \brief Compute the number of times the "local coordinates ready on device" GPU event will be used as a synchronization point.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed: * When some work is offloaded to GPU, force calculation should wait for the atom coordinates to
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed: * or from the GPU integration at the end of the previous step.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed: * \param pmeSendCoordinatesFromGpu Whether peer-to-peer communication is used for PME coordinates.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                                          bool pmeSendCoordinatesFromGpu)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        if (pmeSendCoordinatesFromGpu)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                       "GPU PME PP communications require having a separate PME rank");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            // Event is consumed by gmx_pme_send_coordinates for GPU PME PP Communications
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        if (stepWork.haveGpuPmeOnThisRank)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            // Event is consumed by launchPmeGpuSpread
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        if (stepWork.computeNonbondedForces && stepWork.useGpuXBufferOps)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            // Event is consumed by convertCoordinatesGpu
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (stepWork.useGpuXHalo)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        // Event is consumed by communicateGpuHaloCoordinates
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (stepWork.clearGpuFBufferEarly && simulationWork.useGpuUpdate)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:/*! \brief Compute the number of times the "local forces ready on device" GPU event will be used as a synchronization point.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed: * \param useOrEmulateGpuNb Whether GPU non-bonded calculations are used or emulated.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed: * \param alternateGpuWait Whether alternating wait/reduce scheme is used.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                                          bool useOrEmulateGpuNb,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                                          bool alternateGpuWait)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    bool eventUsedInGpuForceReduction =
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:             || (simulationWork.havePpDomainDecomposition && !simulationWork.useGpuHaloExchange));
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    bool gpuForceReductionUsed = useOrEmulateGpuNb && !alternateGpuWait && stepWork.useGpuFBufferOps
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (gpuForceReductionUsed && eventUsedInGpuForceReduction)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    bool gpuForceHaloUsed = simulationWork.havePpDomainDecomposition && stepWork.computeForces
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                            && stepWork.useGpuFHalo;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (gpuForceHaloUsed)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:/*! \brief Setup for the local GPU force reduction:
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed: * \param [in] stateGpu            GPU state propagator object
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed: * \param [in] gpuForceReduction   GPU force reduction object
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed: * \param [in] pmePpCommGpu        PME-PP GPU communication object
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:static void setupLocalGpuForceReduction(const gmx::MdrunScheduleWorkload* runScheduleWork,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                        gmx::StatePropagatorDataGpu*      stateGpu,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                        gmx::GpuForceReduction*           gpuForceReduction,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                        gmx::PmePpCommGpu*                pmePpCommGpu,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:               "GPU force reduction is not compatible with MTS");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    // (re-)initialize local GPU force reduction
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    gpuForceReduction->reinit(stateGpu->getForces(),
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                              stateGpu->fReducedOnDevice(AtomLocality::Local));
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    gpuForceReduction->registerNbnxmForce(Nbnxm::gpu_get_f(nbv->gpu_nbv));
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    GpuEventSynchronizer*   pmeSynchronizer     = nullptr;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (runScheduleWork->simulationWork.haveGpuPmeOnPpRank())
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        pmeForcePtr = pme_gpu_get_device_f(pmedata);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            pmeSynchronizer     = pme_gpu_get_f_ready_synchronizer(pmedata);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    else if (runScheduleWork->simulationWork.useGpuPmePpCommunication)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        pmeForcePtr = pmePpCommGpu->getGpuForceStagingPtr();
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                pmeSynchronizer = pmePpCommGpu->getForcesReadySynchronizer();
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        gpuForceReduction->registerRvecForce(pmeForcePtr);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        if (!runScheduleWork->simulationWork.useGpuPmePpCommunication || GMX_THREAD_MPI)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            GMX_ASSERT(pmeSynchronizer != nullptr, "PME force ready cuda event should not be NULL");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            gpuForceReduction->addDependency(pmeSynchronizer);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            && !runScheduleWork->simulationWork.useGpuHaloExchange))
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        gpuForceReduction->addDependency(stateGpu->fReadyOnDevice(AtomLocality::Local));
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (runScheduleWork->simulationWork.useGpuHaloExchange)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        gpuForceReduction->addDependency(dd->gpuHaloExchange[0][0]->getForcesReadyOnDeviceEvent());
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:/*! \brief Setup for the non-local GPU force reduction:
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed: * \param [in] stateGpu            GPU state propagator object
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed: * \param [in] gpuForceReduction   GPU force reduction object
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:static void setupNonLocalGpuForceReduction(const gmx::MdrunScheduleWorkload* runScheduleWork,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                           gmx::StatePropagatorDataGpu*      stateGpu,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                           gmx::GpuForceReduction*           gpuForceReduction,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    // (re-)initialize non-local GPU force reduction
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    gpuForceReduction->reinit(stateGpu->getForces(),
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                              stateGpu->fReducedOnDevice(AtomLocality::NonLocal));
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    gpuForceReduction->registerNbnxmForce(Nbnxm::gpu_get_f(nbv->gpu_nbv));
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        gpuForceReduction->addDependency(stateGpu->fReadyOnDevice(AtomLocality::NonLocal));
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    gmx::StatePropagatorDataGpu* stateGpu = fr->stateGpu;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (stepWork.doNeighborSearch && gmx::needStateGpu(simulationWork))
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        stateGpu->reinit(mdatoms->homenr,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (stepWork.doNeighborSearch && simulationWork.haveGpuPmeOnPpRank())
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        GMX_ASSERT(gmx::needStateGpu(simulationWork), "StatePropagatorDataGpu is needed");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        // TODO: This should be moved into PME setup function ( pme_gpu_prepare_computation(...) )
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        pme_gpu_set_device_x(fr->pmedata, stateGpu->getCoordinates());
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            (stepWork.haveGpuPmeOnThisRank || simulationWork.useGpuXBufferOps || simulationWork.useGpuUpdate)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                    ? stateGpu->getCoordinatesReadyOnDeviceEvent(AtomLocality::Local, simulationWork, stepWork)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (stepWork.clearGpuFBufferEarly)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        // GPU Force halo exchange will set a subset of local atoms with remote non-local data.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        // which is satisfied when localXReadyOnDevice has been marked for GPU update case.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        GpuEventSynchronizer* dependency = simulationWork.useGpuUpdate ? localXReadyOnDevice : nullptr;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        stateGpu->clearForcesOnGpu(AtomLocality::Local, dependency);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    const bool pmeSendCoordinatesFromGpu =
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            simulationWork.useGpuPmePpCommunication && !(stepWork.doNeighborSearch);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    const bool reinitGpuPmePpComms =
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            simulationWork.useGpuPmePpCommunication && (stepWork.doNeighborSearch);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    GMX_ASSERT(simulationWork.useGpuHaloExchange
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                       == ((cr->dd != nullptr) && (!cr->dd->gpuHaloExchange[0].empty())),
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:               "The GPU halo exchange is active, but it has not been constructed.");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    bool gmx_used_in_debug haveCopiedXFromGpu = false;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    // Copy coordinate from the GPU if update is on the GPU and there
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (simulationWork.useGpuUpdate && !stepWork.doNeighborSearch
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        stateGpu->copyCoordinatesFromGpu(x.unpaddedArrayRef(), AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        haveCopiedXFromGpu = true;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (stepWork.haveGpuPmeOnThisRank || stepWork.useGpuXBufferOps)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        GMX_ASSERT(stateGpu != nullptr, "stateGpu should not be null");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                        simulationWork, stepWork, pmeSendCoordinatesFromGpu);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        if (!simulationWork.useGpuUpdate || stepWork.doNeighborSearch)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            stateGpu->copyCoordinatesToGpu(x.unpaddedArrayRef(),
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        else if (simulationWork.useGpuUpdate)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            stateGpu->setXUpdatedOnDeviceEventExpectedConsumptionCount(
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        if (!pmeSendCoordinatesFromGpu && !stepWork.doNeighborSearch && simulationWork.useGpuUpdate)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            GMX_ASSERT(haveCopiedXFromGpu,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                 simulationWork.useGpuPmePpCommunication,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                 reinitGpuPmePpComms,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                 pmeSendCoordinatesFromGpu,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                 stepWork.useGpuPmeFReduction,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                 pmeSendCoordinatesFromGpu ? localXReadyOnDevice : nullptr,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                 simulationWork.useMdGpuGraph,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (stepWork.haveGpuPmeOnThisRank)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        launchPmeGpuSpread(fr->pmedata,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        /* initialize the GPU nbnxm atom data and bonded data structures */
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        if (simulationWork.useGpuNonbonded)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            // Note: cycle counting only nononbondeds, GPU listed forces counts internally
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            wallcycle_start_nocount(wcycle, WallCycleCounter::LaunchGpuPp);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            wallcycle_sub_start_nocount(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            Nbnxm::gpu_init_atomdata(nbv->gpu_nbv, nbv->nbat.get());
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            wallcycle_sub_stop(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            wallcycle_stop(wcycle, WallCycleCounter::LaunchGpuPp);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            if (fr->listedForcesGpu)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                 * interactions to the GPU, where the grid order is
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                fr->listedForcesGpu->updateInteractionListsAndDeviceBuffers(
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                        nbv->getGridIndices(), top->idef, Nbnxm::gpuGetNBAtomData(nbv->gpu_nbv));
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        // Need to run after the GPU-offload bonded interaction lists
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        /* Note that with a GPU the launch overhead of the list transfer is not timed separately */
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        nbv->setupGpuShortRangeWork(fr->listedForcesGpu.get(), InteractionLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        if (simulationWork.useGpuXBufferOps)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            nbv->atomdata_init_copy_x_to_nbat_x_gpu();
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        if (simulationWork.useGpuFBufferOps)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            setupLocalGpuForceReduction(runScheduleWork,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                        stateGpu,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                        fr->gpuForceReduction[gmx::AtomLocality::Local].get(),
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                        fr->pmePpCommGpu.get(),
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                setupNonLocalGpuForceReduction(runScheduleWork,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                               stateGpu,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                               fr->gpuForceReduction[gmx::AtomLocality::NonLocal].get(),
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        if (stepWork.useGpuXBufferOps)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            GMX_ASSERT(stateGpu, "stateGpu should be valid when buffer ops are offloaded");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            nbv->convertCoordinatesGpu(AtomLocality::Local, stateGpu->getCoordinates(), localXReadyOnDevice);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            if (simulationWork.useGpuUpdate)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                GMX_ASSERT(stateGpu, "need a valid stateGpu object");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                GMX_ASSERT(haveCopiedXFromGpu,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (simulationWork.useGpuNonbonded && (stepWork.computeNonbondedForces || domainWork.haveGpuBondedWork))
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        ddBalanceRegionHandler.openBeforeForceComputationGpu();
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        wallcycle_start(wcycle, WallCycleCounter::LaunchGpuPp);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        wallcycle_sub_start(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        Nbnxm::gpu_upload_shiftvec(nbv->gpu_nbv, nbv->nbat.get());
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        if (!stepWork.useGpuXBufferOps)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            Nbnxm::gpu_copy_xq_to_gpu(nbv->gpu_nbv, nbv->nbat.get(), AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        wallcycle_sub_stop(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        wallcycle_stop(wcycle, WallCycleCounter::LaunchGpuPp);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        // with X buffer ops offloaded to the GPU on all but the search steps
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        if (domainWork.haveGpuBondedWork && !simulationWork.havePpDomainDecomposition)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            fr->listedForcesGpu->setPbcAndlaunchKernel(fr->pbcType, box, fr->bMolPBC, stepWork);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        /* launch local nonbonded work on GPU */
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        wallcycle_start_nocount(wcycle, WallCycleCounter::LaunchGpuPp);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        wallcycle_sub_start_nocount(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        wallcycle_sub_stop(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        wallcycle_stop(wcycle, WallCycleCounter::LaunchGpuPp);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (stepWork.haveGpuPmeOnThisRank)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        // In PME GPU and mixed mode we launch FFT / gather after the
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        // X copy/transform to allow overlap as well as after the GPU NB
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        launchPmeGpuFftAndGather(fr->pmedata,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            /* Note that with a GPU the launch overhead of the list transfer is not timed separately */
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            nbv->setupGpuShortRangeWork(fr->listedForcesGpu.get(), InteractionLocality::NonLocal);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            // TODO refactor this GPU halo exchange re-initialisation
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            // to location in do_md where GPU halo exchange is
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            // constructed at partitioning, after above stateGpu
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            if (simulationWork.useGpuHaloExchange)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                reinitGpuHaloExchange(*cr, stateGpu->getCoordinates(), stateGpu->getForces());
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            GpuEventSynchronizer* gpuCoordinateHaloLaunched = nullptr;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            if (stepWork.useGpuXHalo)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                gpuCoordinateHaloLaunched = communicateGpuHaloCoordinates(*cr, box, localXReadyOnDevice);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                    stateGpu->copyCoordinatesFromGpu(
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                            x.unpaddedArrayRef(), AtomLocality::NonLocal, gpuCoordinateHaloLaunched);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                if (simulationWork.useGpuUpdate)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                    GMX_ASSERT(haveCopiedXFromGpu,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                            (stepWork.computePmeOnSeparateRank && !pmeSendCoordinatesFromGpu);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                        stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            if (stepWork.useGpuXBufferOps)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                if (!stepWork.useGpuXHalo)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                    stateGpu->copyCoordinatesToGpu(x.unpaddedArrayRef(), AtomLocality::NonLocal);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                GpuEventSynchronizer* xReadyOnDeviceEvent = stateGpu->getCoordinatesReadyOnDeviceEvent(
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                        AtomLocality::NonLocal, simulationWork, stepWork, gpuCoordinateHaloLaunched);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                if (stepWork.useGpuXHalo && domainWork.haveCpuNonLocalForceWork)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                    /* We already enqueued an event for Gpu Halo exchange completion into the
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                nbv->convertCoordinatesGpu(
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                        AtomLocality::NonLocal, stateGpu->getCoordinates(), xReadyOnDeviceEvent);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        if (simulationWork.useGpuNonbonded)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            if (!stepWork.useGpuXBufferOps)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                wallcycle_start(wcycle, WallCycleCounter::LaunchGpuPp);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                wallcycle_sub_start(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                Nbnxm::gpu_copy_xq_to_gpu(nbv->gpu_nbv, nbv->nbat.get(), AtomLocality::NonLocal);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                wallcycle_sub_stop(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                wallcycle_stop(wcycle, WallCycleCounter::LaunchGpuPp);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            if (domainWork.haveGpuBondedWork)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                fr->listedForcesGpu->setPbcAndlaunchKernel(fr->pbcType, box, fr->bMolPBC, stepWork);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            /* launch non-local nonbonded tasks on GPU */
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            wallcycle_start_nocount(wcycle, WallCycleCounter::LaunchGpuPp);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            wallcycle_sub_start(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            wallcycle_sub_stop(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            wallcycle_stop(wcycle, WallCycleCounter::LaunchGpuPp);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (simulationWork.useGpuNonbonded && stepWork.computeNonbondedForces)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        wallcycle_start_nocount(wcycle, WallCycleCounter::LaunchGpuPp);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        wallcycle_sub_start_nocount(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            Nbnxm::gpu_launch_cpyback(nbv->gpu_nbv, nbv->nbat.get(), stepWork, AtomLocality::NonLocal);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        Nbnxm::gpu_launch_cpyback(nbv->gpu_nbv, nbv->nbat.get(), stepWork, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        wallcycle_sub_stop(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        if (domainWork.haveGpuBondedWork && stepWork.computeEnergy)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            fr->listedForcesGpu->launchEnergyTransfer();
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        wallcycle_stop(wcycle, WallCycleCounter::LaunchGpuPp);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    // For the rest of the CPU tasks that depend on GPU-update produced coordinates,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (simulationWork.useGpuUpdate && !stepWork.doNeighborSearch)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                || (stepWork.computePmeOnSeparateRank && !pmeSendCoordinatesFromGpu);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            GMX_ASSERT(haveCopiedXFromGpu,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    const bool useOrEmulateGpuNb = simulationWork.useGpuNonbonded || fr->nbv->emulateGpu();
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (!useOrEmulateGpuNb)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (stepWork.useGpuXHalo && domainWork.haveCpuNonLocalForceWork)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        stateGpu->waitCoordinatesReadyOnHost(AtomLocality::NonLocal);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:         * Happens here on the CPU both with and without GPU.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (stepWork.computeNonbondedForces && !useOrEmulateGpuNb)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            (stepWork.haveGpuPmeOnThisRank || needToReceivePmeResultsFromSeparateRank);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:     * GPU we must wait for the PME calculation (dhdl) results to finish before sampling the
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        if (stepWork.haveGpuPmeOnThisRank)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            pmeGpuWaitAndReduce(fr->pmedata,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                   simulationWork.useGpuPmePpCommunication,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                   stepWork.useGpuPmeFReduction,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (simulationWork.havePpDomainDecomposition && stepWork.computeForces && stepWork.useGpuFHalo
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        stateGpu->copyForcesToGpu(forceOutMtsLevel0.forceWithShiftForces().force(), AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    GMX_ASSERT(!(nonbondedAtMtsLevel1 && stepWork.useGpuFBufferOps),
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:               "The schedule below does not allow for nonbonded MTS with GPU buffer ops");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    GMX_ASSERT(!(nonbondedAtMtsLevel1 && stepWork.useGpuFHalo),
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:               "The schedule below does not allow for nonbonded MTS with GPU halo exchange");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    // Will store the amount of cycles spent waiting for the GPU that
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    float cycles_wait_gpu = 0;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (useOrEmulateGpuNb && stepWork.computeNonbondedForces)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            if (simulationWork.useGpuNonbonded)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                cycles_wait_gpu += Nbnxm::gpu_wait_finish_task(
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                        nbv->gpu_nbv,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            if (stepWork.useGpuFBufferOps)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                    stateGpu->copyForcesToGpu(forceOutMtsLevel0.forceWithShiftForces().force(),
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                fr->gpuForceReduction[gmx::AtomLocality::NonLocal]->execute();
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                if (!stepWork.useGpuFHalo)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                    stateGpu->consumeForcesReducedOnDeviceEvent(AtomLocality::NonLocal);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                    // copy from GPU input for dd_move_f()
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                    stateGpu->copyForcesFromGpu(forceOutMtsLevel0.forceWithShiftForces().force(),
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            if (fr->nbv->emulateGpu() && stepWork.computeVirial)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    // With both nonbonded and PME offloaded a GPU on the same rank, we use
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    // When running free energy perturbations steered by AWH and calculating PME on GPU,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    const bool alternateGpuWait = (!c_disableAlternatingWait && stepWork.haveGpuPmeOnThisRank
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                   && simulationWork.useGpuNonbonded && !simulationWork.havePpDomainDecomposition
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                   && !stepWork.useGpuFBufferOps && !needEarlyPmeResults);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            simulationWork, domainWork, stepWork, useOrEmulateGpuNb, alternateGpuWait);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    // If expectedLocalFReadyOnDeviceConsumptionCount == 0, stateGpu can be uninitialized
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        stateGpu->setFReadyOnDeviceEventExpectedConsumptionCount(
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:         * If we use a GPU this will overlap with GPU work, so in that case
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            if (stepWork.useGpuFHalo)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                gmx::FixedCapacityVector<GpuEventSynchronizer*, 2> gpuForceHaloDependencies;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                if (domainWork.haveCpuLocalForceWork || stepWork.clearGpuFBufferEarly)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                    gpuForceHaloDependencies.push_back(stateGpu->fReadyOnDevice(AtomLocality::Local));
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                gpuForceHaloDependencies.push_back(stateGpu->fReducedOnDevice(AtomLocality::NonLocal));
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                communicateGpuHaloForces(*cr, accumulateForces, &gpuForceHaloDependencies);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                if (stepWork.useGpuFBufferOps)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                    stateGpu->waitForcesReadyOnHost(AtomLocality::NonLocal);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (alternateGpuWait)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        alternatePmeNbGpuWaitReduce(fr->nbv.get(),
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (!alternateGpuWait && stepWork.haveGpuPmeOnThisRank && !needEarlyPmeResults)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        pmeGpuWaitAndReduce(fr->pmedata,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    /* Wait for local GPU NB outputs on the non-alternating wait path */
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (!alternateGpuWait && stepWork.computeNonbondedForces && simulationWork.useGpuNonbonded)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        /* Measured overhead on CUDA and OpenCL with(out) GPU sharing
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        const float gpuWaitApiOverheadMargin = 2e6F; /* cycles */
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        const float waitCycles               = Nbnxm::gpu_wait_finish_task(
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                nbv->gpu_nbv,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            DdBalanceRegionWaitedForGpu waitedForGpu = DdBalanceRegionWaitedForGpu::yes;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            if (stepWork.computeForces && waitCycles <= gpuWaitApiOverheadMargin)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                waitedForGpu = DdBalanceRegionWaitedForGpu::no;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            ddBalanceRegionHandler.closeAfterForceComputationGpu(cycles_wait_gpu, waitedForGpu);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (fr->nbv->emulateGpu())
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    // If on GPU PME-PP comms path, receive forces from PME before GPU buffer ops
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    // When running free energy perturbations steered by AWH and calculating PME on GPU,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (needToReceivePmeResultsFromSeparateRank && simulationWork.useGpuPmePpCommunication && !needEarlyPmeResults)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                               simulationWork.useGpuPmePpCommunication,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                               stepWork.useGpuPmeFReduction,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    /* Do the nonbonded GPU (or emulation) force buffer reduction
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    GMX_ASSERT(!(nonbondedAtMtsLevel1 && stepWork.useGpuFBufferOps),
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:               "The schedule below does not allow for nonbonded MTS with GPU buffer ops");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (useOrEmulateGpuNb && !alternateGpuWait)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        if (stepWork.useGpuFBufferOps)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            // - copy is not perfomed if GPU force halo exchange is active, because it would overwrite the result
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            if (domainWork.haveLocalForceContribInCpuBuffer && !stepWork.useGpuFHalo)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                stateGpu->copyForcesToGpu(forceWithShift, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                fr->gpuForceReduction[gmx::AtomLocality::Local]->execute();
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            if (!simulationWork.useGpuUpdate
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                || (simulationWork.useGpuUpdate && haveDDAtomOrdering(*cr) && simulationWork.useCpuPmePpCommunication)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                    /* We have previously issued force reduction on the GPU, but we will
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                    stateGpu->consumeForcesReducedOnDeviceEvent(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                stateGpu->copyForcesFromGpu(forceWithShift, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                stateGpu->waitForcesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        stateGpu->setFReadyOnDeviceEventExpectedConsumptionCount(AtomLocality::Local, 1);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    launchGpuEndOfStepTasks(
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            nbv, fr->listedForcesGpu.get(), fr->pmedata, enerd, *runScheduleWork, step, wcycle);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    // TODO refactor this and unify with above GPU PME-PP / GPU update path call to the same function
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    // When running free energy perturbations steered by AWH and calculating PME on GPU,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                               simulationWork.useGpuPmePpCommunication,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    /* In case we don't have constraints and are using GPUs, the next balancing
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:#include "gromacs/domdec/gpuhaloexchange.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:#include "gromacs/ewald/pme_coordinate_receiver_gpu.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:#include "gromacs/ewald/pme_pp_comm_gpu.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:#include "gromacs/gpu_utils/gpu_utils.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:#include "gromacs/listed_forces/listed_forces_gpu.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:#include "gromacs/mdtypes/state_propagator_data_gpu.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:#include "gromacs/nbnxm/gpu_data_mgmt.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:#include "gromacs/nbnxm/nbnxm_gpu.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:#include "gromacs/timing/gpu_timing.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:#include "gpuforcereduction.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:static const bool c_disableAlternatingWait = (getenv("GMX_DISABLE_ALTERNATING_GPU_WAIT") != nullptr);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                                   bool                  useGpuPmePpComms,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                                   bool                  receivePmeForceToGpu,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    gmx_pme_receive_f(fr->pmePpCommGpu.get(),
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                      useGpuPmePpComms,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                      receivePmeForceToGpu,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    /* GPU kernel launch overhead is already timed separately */
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    if (!nbv->useGpu())
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:/*! \brief Launch the prepare_step and spread stages of PME GPU.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:static inline void launchPmeGpuSpread(gmx_pme_t*            pmedata,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                                      GpuEventSynchronizer* xReadyOnDevice,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    wallcycle_start(wcycle, WallCycleCounter::PmeGpuMesh);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    pme_gpu_prepare_computation(pmedata, box, wcycle, stepWork);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    bool                           useGpuDirectComm         = false;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    gmx::PmeCoordinateReceiverGpu* pmeCoordinateReceiverGpu = nullptr;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    pme_gpu_launch_spread(
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            pmedata, xReadyOnDevice, wcycle, lambdaQ, useGpuDirectComm, pmeCoordinateReceiverGpu);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    wallcycle_stop(wcycle, WallCycleCounter::PmeGpuMesh);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:/*! \brief Launch the FFT and gather stages of PME GPU
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:static void launchPmeGpuFftAndGather(gmx_pme_t*               pmedata,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    wallcycle_start_nocount(wcycle, WallCycleCounter::PmeGpuMesh);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    pme_gpu_launch_complex_transforms(pmedata, wcycle, stepWork);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    pme_gpu_launch_gather(pmedata, wcycle, lambdaQ);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    wallcycle_stop(wcycle, WallCycleCounter::PmeGpuMesh);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp: * Blocks until PME GPU tasks are completed, and gets the output forces and virial/energy
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:static void pmeGpuWaitAndReduce(gmx_pme_t*               pme,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    wallcycle_start_nocount(wcycle, WallCycleCounter::PmeGpuMesh);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    pme_gpu_wait_and_reduce(pme, stepWork, wcycle, forceWithVirial, enerd, lambdaQ);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    wallcycle_stop(wcycle, WallCycleCounter::PmeGpuMesh);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp: *  Polling wait for either of the PME or nonbonded GPU tasks.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp: * Instead of a static order in waiting for GPU tasks, this function
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp: * one of the reductions, regardless of the GPU task completion order.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:static void alternatePmeNbGpuWaitReduce(nonbonded_verlet_t* nbv,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    bool isPmeGpuDone = false;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    bool isNbGpuDone  = false;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    gmx::ArrayRef<const gmx::RVec> pmeGpuForces;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    while (!isPmeGpuDone || !isNbGpuDone)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        if (!isPmeGpuDone)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            wallcycle_start_nocount(wcycle, WallCycleCounter::PmeGpuMesh);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            GpuTaskCompletion completionType =
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                    (isNbGpuDone) ? GpuTaskCompletion::Wait : GpuTaskCompletion::Check;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            isPmeGpuDone = pme_gpu_try_finish_task(
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            wallcycle_stop(wcycle, WallCycleCounter::PmeGpuMesh);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        if (!isNbGpuDone)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            GpuTaskCompletion completionType =
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                    (isPmeGpuDone) ? GpuTaskCompletion::Wait : GpuTaskCompletion::Check;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            isNbGpuDone = Nbnxm::gpu_try_finish_task(
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                    nbv->gpu_nbv,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            if (isNbGpuDone)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        && (domainWork.haveCpuLocalForceWork || !stepWork.useGpuFBufferOps
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            || (havePpDomainDecomposition && !stepWork.useGpuFHalo)))
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    domainWork.haveGpuBondedWork =
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            ((fr.listedForcesGpu != nullptr) && fr.listedForcesGpu->haveInteractions());
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    if (simulationWork.useGpuXBufferOps || simulationWork.useGpuFBufferOps)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        GMX_ASSERT(simulationWork.useGpuNonbonded,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    flags.useGpuXBufferOps = simulationWork.useGpuXBufferOps && !flags.doNeighborSearch;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    flags.useGpuFBufferOps = simulationWork.useGpuFBufferOps && !flags.computeVirial;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    flags.useGpuPmeFReduction =
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            flags.computeSlowForces && flags.useGpuFBufferOps
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            && (simulationWork.haveGpuPmeOnPpRank() || simulationWork.useGpuPmePpCommunication);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    flags.useGpuXHalo              = simulationWork.useGpuHaloExchange && !flags.doNeighborSearch;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    flags.useGpuFHalo              = simulationWork.useGpuHaloExchange && flags.useGpuFBufferOps;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    flags.haveGpuPmeOnThisRank     = simulationWork.haveGpuPmeOnPpRank() && flags.computeSlowForces;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:             && !(flags.computeVirial || simulationWork.useGpuNonbonded || flags.haveGpuPmeOnThisRank));
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    // On NS steps, the buffer is cleared in stateGpu->reinit, no need to clear it twice.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    flags.clearGpuFBufferEarly =
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            flags.useGpuFHalo && !domainWork.haveCpuLocalForceWork && !flags.doNeighborSearch;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:/* \brief Launch end-of-step GPU tasks: buffer clearing and rolling pruning.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:static void launchGpuEndOfStepTasks(nonbonded_verlet_t*               nbv,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                                    gmx::ListedForcesGpu*             listedForcesGpu,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    if (runScheduleWork.simulationWork.useGpuNonbonded && runScheduleWork.stepWork.computeNonbondedForces)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:         * clear kernel launches can leave the GPU idle while it could be running
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        if (nbv->isDynamicPruningStepGpu(step))
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            nbv->dispatchPruneKernelGpu(step);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        /* now clear the GPU outputs while we finish the step on the CPU */
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        wallcycle_start_nocount(wcycle, WallCycleCounter::LaunchGpuPp);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        wallcycle_sub_start_nocount(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        Nbnxm::gpu_clear_outputs(nbv->gpu_nbv, runScheduleWork.stepWork.computeVirial);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        wallcycle_sub_stop(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        wallcycle_stop(wcycle, WallCycleCounter::LaunchGpuPp);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    if (runScheduleWork.stepWork.haveGpuPmeOnThisRank)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        wallcycle_start_nocount(wcycle, WallCycleCounter::PmeGpuMesh);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        pme_gpu_reinit_computation(pmedata, runScheduleWork.simulationWork.useMdGpuGraph, wcycle);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        wallcycle_stop(wcycle, WallCycleCounter::PmeGpuMesh);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    if (runScheduleWork.domainWork.haveGpuBondedWork && runScheduleWork.stepWork.computeEnergy)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        listedForcesGpu->waitAccumulateEnergyTerms(enerd);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        listedForcesGpu->clearEnergies();
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:/*! \brief Compute the number of times the "local coordinates ready on device" GPU event will be used as a synchronization point.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp: * When some work is offloaded to GPU, force calculation should wait for the atom coordinates to
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp: * or from the GPU integration at the end of the previous step.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp: * \param pmeSendCoordinatesFromGpu Whether peer-to-peer communication is used for PME coordinates.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                                                          bool pmeSendCoordinatesFromGpu)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        if (pmeSendCoordinatesFromGpu)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                       "GPU PME PP communications require having a separate PME rank");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            // Event is consumed by gmx_pme_send_coordinates for GPU PME PP Communications
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        if (stepWork.haveGpuPmeOnThisRank)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            // Event is consumed by launchPmeGpuSpread
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        if (stepWork.computeNonbondedForces && stepWork.useGpuXBufferOps)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            // Event is consumed by convertCoordinatesGpu
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    if (stepWork.useGpuXHalo)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        // Event is consumed by communicateGpuHaloCoordinates
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    if (stepWork.clearGpuFBufferEarly && simulationWork.useGpuUpdate)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:/*! \brief Compute the number of times the "local forces ready on device" GPU event will be used as a synchronization point.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp: * \param useOrEmulateGpuNb Whether GPU non-bonded calculations are used or emulated.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp: * \param alternateGpuWait Whether alternating wait/reduce scheme is used.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                                                          bool useOrEmulateGpuNb,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                                                          bool alternateGpuWait)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    bool eventUsedInGpuForceReduction =
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:             || (simulationWork.havePpDomainDecomposition && !simulationWork.useGpuHaloExchange));
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    bool gpuForceReductionUsed = useOrEmulateGpuNb && !alternateGpuWait && stepWork.useGpuFBufferOps
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    if (gpuForceReductionUsed && eventUsedInGpuForceReduction)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    bool gpuForceHaloUsed = simulationWork.havePpDomainDecomposition && stepWork.computeForces
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                            && stepWork.useGpuFHalo;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    if (gpuForceHaloUsed)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:/*! \brief Setup for the local GPU force reduction:
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp: * \param [in] stateGpu            GPU state propagator object
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp: * \param [in] gpuForceReduction   GPU force reduction object
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp: * \param [in] pmePpCommGpu        PME-PP GPU communication object
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:static void setupLocalGpuForceReduction(const gmx::MdrunScheduleWorkload* runScheduleWork,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                                        gmx::StatePropagatorDataGpu*      stateGpu,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                                        gmx::GpuForceReduction*           gpuForceReduction,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                                        gmx::PmePpCommGpu*                pmePpCommGpu,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:               "GPU force reduction is not compatible with MTS");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    // (re-)initialize local GPU force reduction
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    gpuForceReduction->reinit(stateGpu->getForces(),
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                              stateGpu->fReducedOnDevice(AtomLocality::Local));
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    gpuForceReduction->registerNbnxmForce(Nbnxm::gpu_get_f(nbv->gpu_nbv));
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    GpuEventSynchronizer*   pmeSynchronizer     = nullptr;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    if (runScheduleWork->simulationWork.haveGpuPmeOnPpRank())
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        pmeForcePtr = pme_gpu_get_device_f(pmedata);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            pmeSynchronizer     = pme_gpu_get_f_ready_synchronizer(pmedata);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    else if (runScheduleWork->simulationWork.useGpuPmePpCommunication)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        pmeForcePtr = pmePpCommGpu->getGpuForceStagingPtr();
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                pmeSynchronizer = pmePpCommGpu->getForcesReadySynchronizer();
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        gpuForceReduction->registerRvecForce(pmeForcePtr);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        if (!runScheduleWork->simulationWork.useGpuPmePpCommunication || GMX_THREAD_MPI)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            GMX_ASSERT(pmeSynchronizer != nullptr, "PME force ready cuda event should not be NULL");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            gpuForceReduction->addDependency(pmeSynchronizer);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            && !runScheduleWork->simulationWork.useGpuHaloExchange))
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        gpuForceReduction->addDependency(stateGpu->fReadyOnDevice(AtomLocality::Local));
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    if (runScheduleWork->simulationWork.useGpuHaloExchange)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        gpuForceReduction->addDependency(dd->gpuHaloExchange[0][0]->getForcesReadyOnDeviceEvent());
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:/*! \brief Setup for the non-local GPU force reduction:
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp: * \param [in] stateGpu            GPU state propagator object
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp: * \param [in] gpuForceReduction   GPU force reduction object
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:static void setupNonLocalGpuForceReduction(const gmx::MdrunScheduleWorkload* runScheduleWork,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                                           gmx::StatePropagatorDataGpu*      stateGpu,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                                           gmx::GpuForceReduction*           gpuForceReduction,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    // (re-)initialize non-local GPU force reduction
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    gpuForceReduction->reinit(stateGpu->getForces(),
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                              stateGpu->fReducedOnDevice(AtomLocality::NonLocal));
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    gpuForceReduction->registerNbnxmForce(Nbnxm::gpu_get_f(nbv->gpu_nbv));
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        gpuForceReduction->addDependency(stateGpu->fReadyOnDevice(AtomLocality::NonLocal));
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    gmx::StatePropagatorDataGpu* stateGpu = fr->stateGpu;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    if (stepWork.doNeighborSearch && gmx::needStateGpu(simulationWork))
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        stateGpu->reinit(mdatoms->homenr,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    if (stepWork.doNeighborSearch && simulationWork.haveGpuPmeOnPpRank())
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        GMX_ASSERT(gmx::needStateGpu(simulationWork), "StatePropagatorDataGpu is needed");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        // TODO: This should be moved into PME setup function ( pme_gpu_prepare_computation(...) )
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        pme_gpu_set_device_x(fr->pmedata, stateGpu->getCoordinates());
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            (stepWork.haveGpuPmeOnThisRank || simulationWork.useGpuXBufferOps || simulationWork.useGpuUpdate)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                    ? stateGpu->getCoordinatesReadyOnDeviceEvent(AtomLocality::Local, simulationWork, stepWork)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    if (stepWork.clearGpuFBufferEarly)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        // GPU Force halo exchange will set a subset of local atoms with remote non-local data.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        // which is satisfied when localXReadyOnDevice has been marked for GPU update case.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        GpuEventSynchronizer* dependency = simulationWork.useGpuUpdate ? localXReadyOnDevice : nullptr;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        stateGpu->clearForcesOnGpu(AtomLocality::Local, dependency);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    const bool pmeSendCoordinatesFromGpu =
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            simulationWork.useGpuPmePpCommunication && !(stepWork.doNeighborSearch);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    const bool reinitGpuPmePpComms =
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            simulationWork.useGpuPmePpCommunication && (stepWork.doNeighborSearch);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    GMX_ASSERT(simulationWork.useGpuHaloExchange
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                       == ((cr->dd != nullptr) && (!cr->dd->gpuHaloExchange[0].empty())),
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:               "The GPU halo exchange is active, but it has not been constructed.");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    bool gmx_used_in_debug haveCopiedXFromGpu = false;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    // Copy coordinate from the GPU if update is on the GPU and there
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    if (simulationWork.useGpuUpdate && !stepWork.doNeighborSearch
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        stateGpu->copyCoordinatesFromGpu(x.unpaddedArrayRef(), AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        haveCopiedXFromGpu = true;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    if (stepWork.haveGpuPmeOnThisRank || stepWork.useGpuXBufferOps)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        GMX_ASSERT(stateGpu != nullptr, "stateGpu should not be null");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                        simulationWork, stepWork, pmeSendCoordinatesFromGpu);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        if (!simulationWork.useGpuUpdate || stepWork.doNeighborSearch)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            stateGpu->copyCoordinatesToGpu(x.unpaddedArrayRef(),
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        else if (simulationWork.useGpuUpdate)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            stateGpu->setXUpdatedOnDeviceEventExpectedConsumptionCount(
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        if (!pmeSendCoordinatesFromGpu && !stepWork.doNeighborSearch && simulationWork.useGpuUpdate)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            GMX_ASSERT(haveCopiedXFromGpu,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                                 simulationWork.useGpuPmePpCommunication,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                                 reinitGpuPmePpComms,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                                 pmeSendCoordinatesFromGpu,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                                 stepWork.useGpuPmeFReduction,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                                 pmeSendCoordinatesFromGpu ? localXReadyOnDevice : nullptr,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                                 simulationWork.useMdGpuGraph,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    if (stepWork.haveGpuPmeOnThisRank)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        launchPmeGpuSpread(fr->pmedata,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        /* initialize the GPU nbnxm atom data and bonded data structures */
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        if (simulationWork.useGpuNonbonded)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            // Note: cycle counting only nononbondeds, GPU listed forces counts internally
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            wallcycle_start_nocount(wcycle, WallCycleCounter::LaunchGpuPp);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            wallcycle_sub_start_nocount(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            Nbnxm::gpu_init_atomdata(nbv->gpu_nbv, nbv->nbat.get());
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            wallcycle_sub_stop(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            wallcycle_stop(wcycle, WallCycleCounter::LaunchGpuPp);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            if (fr->listedForcesGpu)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                 * interactions to the GPU, where the grid order is
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                fr->listedForcesGpu->updateInteractionListsAndDeviceBuffers(
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                        nbv->getGridIndices(), top->idef, Nbnxm::gpuGetNBAtomData(nbv->gpu_nbv));
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        // Need to run after the GPU-offload bonded interaction lists
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        /* Note that with a GPU the launch overhead of the list transfer is not timed separately */
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        nbv->setupGpuShortRangeWork(fr->listedForcesGpu.get(), InteractionLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        if (simulationWork.useGpuXBufferOps)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            nbv->atomdata_init_copy_x_to_nbat_x_gpu();
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        if (simulationWork.useGpuFBufferOps)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            setupLocalGpuForceReduction(runScheduleWork,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                                        stateGpu,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                                        fr->gpuForceReduction[gmx::AtomLocality::Local].get(),
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                                        fr->pmePpCommGpu.get(),
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                setupNonLocalGpuForceReduction(runScheduleWork,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                                               stateGpu,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                                               fr->gpuForceReduction[gmx::AtomLocality::NonLocal].get(),
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        if (stepWork.useGpuXBufferOps)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            GMX_ASSERT(stateGpu, "stateGpu should be valid when buffer ops are offloaded");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            nbv->convertCoordinatesGpu(AtomLocality::Local, stateGpu->getCoordinates(), localXReadyOnDevice);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            if (simulationWork.useGpuUpdate)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                GMX_ASSERT(stateGpu, "need a valid stateGpu object");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                GMX_ASSERT(haveCopiedXFromGpu,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    if (simulationWork.useGpuNonbonded && (stepWork.computeNonbondedForces || domainWork.haveGpuBondedWork))
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        ddBalanceRegionHandler.openBeforeForceComputationGpu();
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        wallcycle_start(wcycle, WallCycleCounter::LaunchGpuPp);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        wallcycle_sub_start(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        Nbnxm::gpu_upload_shiftvec(nbv->gpu_nbv, nbv->nbat.get());
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        if (!stepWork.useGpuXBufferOps)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            Nbnxm::gpu_copy_xq_to_gpu(nbv->gpu_nbv, nbv->nbat.get(), AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        wallcycle_sub_stop(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        wallcycle_stop(wcycle, WallCycleCounter::LaunchGpuPp);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        // with X buffer ops offloaded to the GPU on all but the search steps
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        if (domainWork.haveGpuBondedWork && !simulationWork.havePpDomainDecomposition)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            fr->listedForcesGpu->setPbcAndlaunchKernel(fr->pbcType, box, fr->bMolPBC, stepWork);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        /* launch local nonbonded work on GPU */
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        wallcycle_start_nocount(wcycle, WallCycleCounter::LaunchGpuPp);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        wallcycle_sub_start_nocount(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        wallcycle_sub_stop(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        wallcycle_stop(wcycle, WallCycleCounter::LaunchGpuPp);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    if (stepWork.haveGpuPmeOnThisRank)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        // In PME GPU and mixed mode we launch FFT / gather after the
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        // X copy/transform to allow overlap as well as after the GPU NB
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        launchPmeGpuFftAndGather(fr->pmedata,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            /* Note that with a GPU the launch overhead of the list transfer is not timed separately */
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            nbv->setupGpuShortRangeWork(fr->listedForcesGpu.get(), InteractionLocality::NonLocal);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            // TODO refactor this GPU halo exchange re-initialisation
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            // to location in do_md where GPU halo exchange is
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            // constructed at partitioning, after above stateGpu
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            if (simulationWork.useGpuHaloExchange)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                reinitGpuHaloExchange(*cr, stateGpu->getCoordinates(), stateGpu->getForces());
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            GpuEventSynchronizer* gpuCoordinateHaloLaunched = nullptr;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            if (stepWork.useGpuXHalo)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                gpuCoordinateHaloLaunched = communicateGpuHaloCoordinates(*cr, box, localXReadyOnDevice);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                    stateGpu->copyCoordinatesFromGpu(
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                            x.unpaddedArrayRef(), AtomLocality::NonLocal, gpuCoordinateHaloLaunched);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                if (simulationWork.useGpuUpdate)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                    GMX_ASSERT(haveCopiedXFromGpu,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                            (stepWork.computePmeOnSeparateRank && !pmeSendCoordinatesFromGpu);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                        stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            if (stepWork.useGpuXBufferOps)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                if (!stepWork.useGpuXHalo)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                    stateGpu->copyCoordinatesToGpu(x.unpaddedArrayRef(), AtomLocality::NonLocal);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                GpuEventSynchronizer* xReadyOnDeviceEvent = stateGpu->getCoordinatesReadyOnDeviceEvent(
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                        AtomLocality::NonLocal, simulationWork, stepWork, gpuCoordinateHaloLaunched);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                if (stepWork.useGpuXHalo && domainWork.haveCpuNonLocalForceWork)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                    /* We already enqueued an event for Gpu Halo exchange completion into the
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                nbv->convertCoordinatesGpu(
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                        AtomLocality::NonLocal, stateGpu->getCoordinates(), xReadyOnDeviceEvent);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        if (simulationWork.useGpuNonbonded)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            if (!stepWork.useGpuXBufferOps)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                wallcycle_start(wcycle, WallCycleCounter::LaunchGpuPp);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                wallcycle_sub_start(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                Nbnxm::gpu_copy_xq_to_gpu(nbv->gpu_nbv, nbv->nbat.get(), AtomLocality::NonLocal);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                wallcycle_sub_stop(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                wallcycle_stop(wcycle, WallCycleCounter::LaunchGpuPp);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            if (domainWork.haveGpuBondedWork)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                fr->listedForcesGpu->setPbcAndlaunchKernel(fr->pbcType, box, fr->bMolPBC, stepWork);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            /* launch non-local nonbonded tasks on GPU */
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            wallcycle_start_nocount(wcycle, WallCycleCounter::LaunchGpuPp);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            wallcycle_sub_start(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            wallcycle_sub_stop(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            wallcycle_stop(wcycle, WallCycleCounter::LaunchGpuPp);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    if (simulationWork.useGpuNonbonded && stepWork.computeNonbondedForces)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        wallcycle_start_nocount(wcycle, WallCycleCounter::LaunchGpuPp);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        wallcycle_sub_start_nocount(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            Nbnxm::gpu_launch_cpyback(nbv->gpu_nbv, nbv->nbat.get(), stepWork, AtomLocality::NonLocal);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        Nbnxm::gpu_launch_cpyback(nbv->gpu_nbv, nbv->nbat.get(), stepWork, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        wallcycle_sub_stop(wcycle, WallCycleSubCounter::LaunchGpuNonBonded);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        if (domainWork.haveGpuBondedWork && stepWork.computeEnergy)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            fr->listedForcesGpu->launchEnergyTransfer();
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        wallcycle_stop(wcycle, WallCycleCounter::LaunchGpuPp);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    // For the rest of the CPU tasks that depend on GPU-update produced coordinates,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    if (simulationWork.useGpuUpdate && !stepWork.doNeighborSearch)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                || (stepWork.computePmeOnSeparateRank && !pmeSendCoordinatesFromGpu);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            GMX_ASSERT(haveCopiedXFromGpu,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    const bool useOrEmulateGpuNb = simulationWork.useGpuNonbonded || fr->nbv->emulateGpu();
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    if (!useOrEmulateGpuNb)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    if (stepWork.useGpuXHalo && domainWork.haveCpuNonLocalForceWork)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        stateGpu->waitCoordinatesReadyOnHost(AtomLocality::NonLocal);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:         * Happens here on the CPU both with and without GPU.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    if (stepWork.computeNonbondedForces && !useOrEmulateGpuNb)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            (stepWork.haveGpuPmeOnThisRank || needToReceivePmeResultsFromSeparateRank);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:     * GPU we must wait for the PME calculation (dhdl) results to finish before sampling the
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        if (stepWork.haveGpuPmeOnThisRank)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            pmeGpuWaitAndReduce(fr->pmedata,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                                   simulationWork.useGpuPmePpCommunication,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                                   stepWork.useGpuPmeFReduction,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    if (simulationWork.havePpDomainDecomposition && stepWork.computeForces && stepWork.useGpuFHalo
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        stateGpu->copyForcesToGpu(forceOutMtsLevel0.forceWithShiftForces().force(), AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    GMX_ASSERT(!(nonbondedAtMtsLevel1 && stepWork.useGpuFBufferOps),
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:               "The schedule below does not allow for nonbonded MTS with GPU buffer ops");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    GMX_ASSERT(!(nonbondedAtMtsLevel1 && stepWork.useGpuFHalo),
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:               "The schedule below does not allow for nonbonded MTS with GPU halo exchange");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    // Will store the amount of cycles spent waiting for the GPU that
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    float cycles_wait_gpu = 0;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    if (useOrEmulateGpuNb && stepWork.computeNonbondedForces)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            if (simulationWork.useGpuNonbonded)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                cycles_wait_gpu += Nbnxm::gpu_wait_finish_task(
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                        nbv->gpu_nbv,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            if (stepWork.useGpuFBufferOps)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                    stateGpu->copyForcesToGpu(forceOutMtsLevel0.forceWithShiftForces().force(),
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                fr->gpuForceReduction[gmx::AtomLocality::NonLocal]->execute();
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                if (!stepWork.useGpuFHalo)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                    stateGpu->consumeForcesReducedOnDeviceEvent(AtomLocality::NonLocal);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                    // copy from GPU input for dd_move_f()
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                    stateGpu->copyForcesFromGpu(forceOutMtsLevel0.forceWithShiftForces().force(),
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            if (fr->nbv->emulateGpu() && stepWork.computeVirial)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    // With both nonbonded and PME offloaded a GPU on the same rank, we use
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    // When running free energy perturbations steered by AWH and calculating PME on GPU,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    const bool alternateGpuWait = (!c_disableAlternatingWait && stepWork.haveGpuPmeOnThisRank
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                                   && simulationWork.useGpuNonbonded && !simulationWork.havePpDomainDecomposition
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                                   && !stepWork.useGpuFBufferOps && !needEarlyPmeResults);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            simulationWork, domainWork, stepWork, useOrEmulateGpuNb, alternateGpuWait);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    // If expectedLocalFReadyOnDeviceConsumptionCount == 0, stateGpu can be uninitialized
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        stateGpu->setFReadyOnDeviceEventExpectedConsumptionCount(
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:         * If we use a GPU this will overlap with GPU work, so in that case
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            if (stepWork.useGpuFHalo)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                gmx::FixedCapacityVector<GpuEventSynchronizer*, 2> gpuForceHaloDependencies;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                if (domainWork.haveCpuLocalForceWork || stepWork.clearGpuFBufferEarly)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                    gpuForceHaloDependencies.push_back(stateGpu->fReadyOnDevice(AtomLocality::Local));
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                gpuForceHaloDependencies.push_back(stateGpu->fReducedOnDevice(AtomLocality::NonLocal));
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                communicateGpuHaloForces(*cr, accumulateForces, &gpuForceHaloDependencies);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                if (stepWork.useGpuFBufferOps)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                    stateGpu->waitForcesReadyOnHost(AtomLocality::NonLocal);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    if (alternateGpuWait)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        alternatePmeNbGpuWaitReduce(fr->nbv.get(),
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    if (!alternateGpuWait && stepWork.haveGpuPmeOnThisRank && !needEarlyPmeResults)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        pmeGpuWaitAndReduce(fr->pmedata,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    /* Wait for local GPU NB outputs on the non-alternating wait path */
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    if (!alternateGpuWait && stepWork.computeNonbondedForces && simulationWork.useGpuNonbonded)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        /* Measured overhead on CUDA and OpenCL with(out) GPU sharing
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        const float gpuWaitApiOverheadMargin = 2e6F; /* cycles */
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        const float waitCycles               = Nbnxm::gpu_wait_finish_task(
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                nbv->gpu_nbv,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            DdBalanceRegionWaitedForGpu waitedForGpu = DdBalanceRegionWaitedForGpu::yes;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            if (stepWork.computeForces && waitCycles <= gpuWaitApiOverheadMargin)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                waitedForGpu = DdBalanceRegionWaitedForGpu::no;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            ddBalanceRegionHandler.closeAfterForceComputationGpu(cycles_wait_gpu, waitedForGpu);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    if (fr->nbv->emulateGpu())
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    // If on GPU PME-PP comms path, receive forces from PME before GPU buffer ops
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    // When running free energy perturbations steered by AWH and calculating PME on GPU,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    if (needToReceivePmeResultsFromSeparateRank && simulationWork.useGpuPmePpCommunication && !needEarlyPmeResults)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                               simulationWork.useGpuPmePpCommunication,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                               stepWork.useGpuPmeFReduction,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    /* Do the nonbonded GPU (or emulation) force buffer reduction
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    GMX_ASSERT(!(nonbondedAtMtsLevel1 && stepWork.useGpuFBufferOps),
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:               "The schedule below does not allow for nonbonded MTS with GPU buffer ops");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    if (useOrEmulateGpuNb && !alternateGpuWait)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        if (stepWork.useGpuFBufferOps)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            // - copy is not perfomed if GPU force halo exchange is active, because it would overwrite the result
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            if (domainWork.haveLocalForceContribInCpuBuffer && !stepWork.useGpuFHalo)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                stateGpu->copyForcesToGpu(forceWithShift, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                fr->gpuForceReduction[gmx::AtomLocality::Local]->execute();
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            if (!simulationWork.useGpuUpdate
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                || (simulationWork.useGpuUpdate && haveDDAtomOrdering(*cr) && simulationWork.useCpuPmePpCommunication)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                    /* We have previously issued force reduction on the GPU, but we will
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                    stateGpu->consumeForcesReducedOnDeviceEvent(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                stateGpu->copyForcesFromGpu(forceWithShift, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                stateGpu->waitForcesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:        stateGpu->setFReadyOnDeviceEventExpectedConsumptionCount(AtomLocality::Local, 1);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    launchGpuEndOfStepTasks(
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:            nbv, fr->listedForcesGpu.get(), fr->pmedata, enerd, *runScheduleWork, step, wcycle);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    // TODO refactor this and unify with above GPU PME-PP / GPU update path call to the same function
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    // When running free energy perturbations steered by AWH and calculating PME on GPU,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:                               simulationWork.useGpuPmePpCommunication,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdlib/sim_util.cpp:    /* In case we don't have constraints and are using GPUs, the next balancing
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:#include "gromacs/domdec/gpuhaloexchange.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:#include "gromacs/ewald/pme_gpu_program.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:#include "gromacs/ewald/pme_pp_comm_gpu.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:#include "gromacs/gpu_utils/device_stream_manager.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:#include "gromacs/gpu_utils/gpueventsynchronizer_helpers.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:#include "gromacs/listed_forces/listed_forces_gpu.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:#include "gromacs/mdlib/gpuforcereduction.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:#include "gromacs/mdlib/mdgraph_gpu.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:#include "gromacs/mdtypes/state_propagator_data_gpu.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:#include "gromacs/nbnxm/gpu_data_mgmt.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:#include "gromacs/taskassignment/decidegpuusage.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:#include "gromacs/taskassignment/usergpuids.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:#include "gromacs/timing/gpu_timing.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp: * the GPU communication flags are set to false in non-tMPI and non-CUDA builds.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp: * \param[in]  useGpuForNonbonded   True if the nonbonded task is offloaded in this run.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                                                         const bool           useGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:    devFlags.enableGpuBufferOps = (GMX_GPU_CUDA || GMX_GPU_SYCL) && useGpuForNonbonded
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                                  && (getenv("GMX_USE_GPU_BUFFER_OPS") != nullptr);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:    if (getenv("GMX_CUDA_GRAPH") != nullptr)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:        if (GMX_HAVE_CUDA_GRAPH_SUPPORT)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:            devFlags.enableCudaGraphs = true;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                            "GMX_CUDA_GRAPH environment variable is detected. "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                            "The experimental CUDA Graphs feature will be used if run conditions "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:            devFlags.enableCudaGraphs = false;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                            "GMX_CUDA_GRAPH environment variable is detected, "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                            "but the CUDA version in use is below the minumum requirement (11.1). "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                            "CUDA Graphs will be disabled.");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:    // Flag use to enable GPU-aware MPI depenendent features such PME GPU decomposition
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:    // GPU-aware MPI is marked available if it has been detected by GROMACS or detection fails but
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:    devFlags.canUseGpuAwareMpi = false;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:    // Direct GPU comm path is being used with GPU-aware MPI
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:    // make sure underlying MPI implementation is GPU-aware
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:    if (GMX_LIB_MPI && (GMX_GPU_CUDA || GMX_GPU_SYCL))
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:        // Allow overriding the detection for GPU-aware MPI
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:        GpuAwareMpiStatus gpuAwareMpiStatus = checkMpiCudaAwareSupport();
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:        const bool        forceGpuAwareMpi  = gpuAwareMpiStatus == GpuAwareMpiStatus::Forced;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:        const bool haveDetectedGpuAwareMpi  = gpuAwareMpiStatus == GpuAwareMpiStatus::Supported;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:        if (getenv("GMX_FORCE_CUDA_AWARE_MPI") != nullptr)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                            "GMX_FORCE_CUDA_AWARE_MPI environment variable is inactive. "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                            "Please use GMX_FORCE_GPU_AWARE_MPI instead.");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:        devFlags.canUseGpuAwareMpi = haveDetectedGpuAwareMpi || forceGpuAwareMpi;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:        if (getenv("GMX_ENABLE_DIRECT_GPU_COMM") != nullptr)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:            if (!haveDetectedGpuAwareMpi && forceGpuAwareMpi)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                // GPU-aware support not detected in MPI library but, user has forced it's use
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                                "This run has forced use of 'GPU-aware MPI'. "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                                "However, GROMACS cannot determine if underlying MPI is GPU-aware. "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                                "Check the GROMACS install guide for recommendations for GPU-aware "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                                "GMX_FORCE_GPU_AWARE_MPI environment variable.");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:            if (devFlags.canUseGpuAwareMpi)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                                "GMX_ENABLE_DIRECT_GPU_COMM environment variable detected, "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                                "enabling direct GPU communication using GPU-aware MPI.");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                                "GPU-aware MPI was not detected, will not use direct GPU "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                                "for GPU-aware support. If you are certain about GPU-aware support "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                                "GMX_FORCE_GPU_AWARE_MPI environment variable.");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:        else if (haveDetectedGpuAwareMpi)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:            // GPU-aware MPI was detected, let the user know that using it may improve performance
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                            "GPU-aware MPI detected, but by default GROMACS will not "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                            "make use the direct GPU communication capabilities of MPI. "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                            "the GMX_ENABLE_DIRECT_GPU_COMM environment variable.");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:        if (getenv("GMX_FORCE_GPU_AWARE_MPI") != nullptr)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:            // Cannot force use of GPU-aware MPI in this build configuration
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                            "A CUDA or SYCL build with an external MPI library is required in "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                            "order to benefit from GMX_FORCE_GPU_AWARE_MPI. That environment "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:    if (devFlags.enableGpuBufferOps)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                        "This run uses the 'GPU buffer ops' feature, enabled by the "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                        "GMX_USE_GPU_BUFFER_OPS environment variable.");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:    // PME decomposition is supported only with CUDA-backend in mixed mode
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:    // CUDA-backend also needs GPU-aware MPI support for decomposition to work
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:    const bool pmeGpuDecompositionRequested =
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:            (pmeRunMode == PmeRunMode::GPU || pmeRunMode == PmeRunMode::Mixed)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:    const bool pmeGpuDecompositionSupported =
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:            (devFlags.canUseGpuAwareMpi && (GMX_GPU_CUDA || GMX_GPU_SYCL)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:             && ((pmeRunMode == PmeRunMode::GPU && (GMX_USE_Heffte || GMX_USE_cuFFTMp))
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:    const bool forcePmeGpuDecomposition = getenv("GMX_GPU_PME_DECOMPOSITION") != nullptr;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:    if (pmeGpuDecompositionSupported && pmeGpuDecompositionRequested)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:        // PME decomposition is supported only when it is forced using GMX_GPU_PME_DECOMPOSITION
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:        if (forcePmeGpuDecomposition)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                            "This run has requested the 'GPU PME decomposition' feature, enabled "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                            "by the GMX_GPU_PME_DECOMPOSITION environment variable. "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                      "Multiple PME tasks were required to run on GPUs, "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                      "Use GMX_GPU_PME_DECOMPOSITION environment variable to enable it.");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:    if (!pmeGpuDecompositionSupported && pmeGpuDecompositionRequested)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:        if (GMX_GPU_CUDA)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                      "PME tasks were required to run on more than one CUDA-devices. To enable "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                      "use MPI with CUDA-aware support and build GROMACS with cuFFTMp support.");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                    "PME tasks were required to run on GPUs, but that is not implemented with "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:    devFlags.enableGpuPmeDecomposition =
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:            forcePmeGpuDecomposition && pmeGpuDecompositionRequested && pmeGpuDecompositionSupported;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                                  bool                           makeGpuPairList,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                (makeGpuPairList ? ListSetupType::Gpu : ListSetupType::CpuSimdWhenSupported);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                fplog, cr, ir, nstlist_cmdline, &mtop, box, effectiveAtomDensity.value(), makeGpuPairList, cpuinfo);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:/*! \brief Return whether GPU acceleration of nonbondeds is supported with the given settings.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:static bool gpuAccelerationOfNonbondedIsUseful(const MDLogger&   mdlog,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:    bool        gpuIsUseful = true;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:        /* The GPU code does not support more than one energy group.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:         * If the user requested GPUs explicitly, a fatal error is given later.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:        gpuIsUseful = false;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                    "Multiple energy groups is not implemented for GPUs, falling back to the CPU. "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                    "For better performance, run on the GPU without energy groups and then do "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:    /* There are resource handling issues in the GPU code paths with MTS on anything else than only
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:        gpuIsUseful = false;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                "Multiple time stepping is only supported with GPUs when MTS is only applied to %s "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:        gpuIsUseful = false;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:        warning     = "TPI is not implemented for GPUs.";
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:    if (!gpuIsUseful && issueWarning)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:    return gpuIsUseful;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:    else if (strncmp(optionString, "gpu", 3) == 0)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:        returnValue = TaskTarget::Gpu;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:        auto* nbnxn_gpu_timings =
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                (nbv != nullptr && nbv->useGpu()) ? Nbnxm::gpu_get_timings(nbv->gpu_nbv) : nullptr;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:        gmx_wallclock_gpu_pme_t pme_gpu_timings = {};
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:        if (pme_gpu_task_enabled(pme))
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:            pme_gpu_get_timings(pme, &pme_gpu_timings);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                        nbnxn_gpu_timings,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                        &pme_gpu_timings);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:    EmulateGpuNonbonded emulateGpuNonbonded =
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:            (getenv("GMX_EMULATE_GPU") != nullptr ? EmulateGpuNonbonded::Yes : EmulateGpuNonbonded::No);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:    std::vector<int> userGpuTaskAssignment;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:        userGpuTaskAssignment = parseUserTaskAssignmentString(hw_opt.userGpuTaskAssignment);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:        bool useGpuForNonbonded = false;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:        bool useGpuForPme       = false;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:            // the number of GPUs to choose the number of ranks.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:            auto canUseGpuForNonbonded = buildSupportsNonbondedOnGpu(nullptr);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:            useGpuForNonbonded         = decideWhetherToUseGpusForNonbondedWithThreadMpi(
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                    userGpuTaskAssignment,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                    emulateGpuNonbonded,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                    canUseGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                    gpuAccelerationOfNonbondedIsUseful(mdlog, *inputrec, GMX_THREAD_MPI, doRerun),
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:            useGpuForPme = decideWhetherToUseGpusForPmeWithThreadMpi(useGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                                                                     userGpuTaskAssignment,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                                                useGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                                                useGpuForPme,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:    // Note that when bonded interactions run on a GPU they always run
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:    bool useGpuForNonbonded = false;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:    bool useGpuForPme       = false;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:    bool useGpuForBonded    = false;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:    bool useGpuForUpdate    = false;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:    bool gpusWereDetected   = hwinfo_->ngpu_compatible_tot > 0;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:        // It's possible that there are different numbers of GPUs on
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:        auto canUseGpuForNonbonded = buildSupportsNonbondedOnGpu(nullptr);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:        useGpuForNonbonded         = decideWhetherToUseGpusForNonbonded(
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                userGpuTaskAssignment,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                emulateGpuNonbonded,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                canUseGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                gpuAccelerationOfNonbondedIsUseful(mdlog, *inputrec, !GMX_THREAD_MPI, doRerun),
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                gpusWereDetected);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:        useGpuForPme    = decideWhetherToUseGpusForPme(useGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                                                    userGpuTaskAssignment,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                                                    gpusWereDetected);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:        useGpuForBonded = decideWhetherToUseGpusForBonded(
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                useGpuForNonbonded, useGpuForPme, bondedTarget, *inputrec, mtop, domdecOptions.numPmeRanks, gpusWereDetected);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:    const PmeRunMode pmeRunMode = determinePmeRunMode(useGpuForPme, pmeFftTarget, *inputrec);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:            mdlog, useGpuForNonbonded, pmeRunMode, cr->sizeOfDefaultCommunicator, domdecOptions.numPmeRanks);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                                                              updateTarget == TaskTarget::Gpu);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                || (!useGpuForNonbonded && usingFullElectrostatics(inputrec->coulombtype)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                          useGpuForNonbonded || (emulateGpuNonbonded == EmulateGpuNonbonded::Yes),
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:        useGpuForUpdate = decideWhetherToUseGpuForUpdate(useDomainDecomposition,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                                                         useGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                                                         gpusWereDetected,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:    const bool canUseDirectGpuComm = decideWhetherDirectGpuCommunicationCanBeUsed(
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:    bool useGpuDirectHalo = false;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:    if (useGpuForNonbonded)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:        // domdecOptions.numPmeRanks == -1 results in 0 separate PME ranks when useGpuForNonbonded is true.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:        useGpuDirectHalo = decideWhetherToUseGpuForHalo(havePPDomainDecomposition,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                                                        useGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                                                        canUseDirectGpuComm,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:        // The DD builder will disable useGpuDirectHalo if the Y or Z component of any domain is
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:        // smaller than twice the communication distance, since GPU-direct communication presently
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:        // perform well on multiple GPUs in any case, but it is important that our core functionality
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:        // (in particular for testing) does not break depending on GPU direct communication being enabled.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                useGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                useGpuForPme,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                useGpuForUpdate,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                useGpuDirectHalo,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                devFlags.enableGpuPmeDecomposition);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:    GpuTaskAssignments gpuTaskAssignments = GpuTaskAssignmentsBuilder::build(
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:            userGpuTaskAssignment,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:            useGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:            useGpuForPme,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:    DeviceInformation* deviceInfo = gpuTaskAssignments.initDevice(&deviceId);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:        // TODO Pass the GPU streams to ddBuilder to use in buffer
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:    const bool useGpuPmeDecomposition = numPmeDomains.x * numPmeDomains.y > 1 && useGpuForPme;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:    GMX_RELEASE_ASSERT(!useGpuPmeDecomposition || devFlags.enableGpuPmeDecomposition,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                       "GPU PME decomposition works only in the cases where it is supported");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                                                              useGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                                                              useGpuForBonded,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                                                              useGpuForUpdate,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                                                              useGpuDirectHalo,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                                                              canUseDirectGpuComm,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                                                              useGpuPmeDecomposition);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:    if (runScheduleWork.simulationWork.useGpuDirectCommunication && GMX_GPU_CUDA)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:        // Don't enable event counting with GPU Direct comm, see #3988.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:        gmx::internal::disableCudaEventConsumptionCounting();
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:    if (isSimulationMainRank && GMX_GPU_SYCL)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:        bool                      haveAnyGpuWork = simWorkload.useGpuPme || simWorkload.useGpuBonded
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                              || simWorkload.useGpuNonbonded || simWorkload.useGpuUpdate;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:        if (haveAnyGpuWork)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                            "\nNOTE: SYCL GPU support in GROMACS, and the compilers, libraries,\n"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:    gpuTaskAssignments.reportGpuUsage(mdlog, printHostName, pmeRunMode, runScheduleWork.simulationWork);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:        const bool useGpuTiming = decideGpuTimingsUsage();
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                *deviceInfo, runScheduleWork.simulationWork, useGpuTiming);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:    if (!userGpuTaskAssignment.empty())
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:        gpuTaskAssignments.logPerformanceHints(mdlog, numAvailableDevices);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:    check_resource_division_efficiency(hwinfo_, gpuTaskAssignments.thisRankHasAnyGpuTask(), cr, mdlog);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:    // Enable Peer access between GPUs where available
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:    // any of the GPU communication features are active.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:        && (runScheduleWork.simulationWork.useGpuHaloExchange
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:            || runScheduleWork.simulationWork.useGpuPmePpCommunication))
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:        setupGpuDevicePeerAccess(gpuTaskAssignments.deviceIdsAssigned(), mdlog);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:    const bool thisRankHasPmeGpuTask = gpuTaskAssignments.thisRankHasPmeGpuTask();
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:        if (runScheduleWork.simulationWork.useGpuPmePpCommunication && !thisRankHasDuty(cr, DUTY_PME))
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                    "GPU device stream manager should be valid in order to use PME-PP direct "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                    "GPU PP-PME stream should be valid in order to use GPU PME-PP direct "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:            fr->pmePpCommGpu = std::make_unique<gmx::PmePpCommGpu>(
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                runScheduleWork.simulationWork.useGpuNonbonded,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:        // TODO: Move the logic below to a GPU bonded builder
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:        if (runScheduleWork.simulationWork.useGpuBonded)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                               "GPU device stream manager should be valid in order to use GPU "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:            fr->listedForcesGpu = std::make_unique<ListedForcesGpu>(mtop.ffparams,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:        mdAtoms = makeMDAtoms(fplog, mtop, *inputrec, thisRankHasPmeGpuTask);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:        if (globalState && thisRankHasPmeGpuTask)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:            // PME on GPU without DD or on a separate PME rank, and because the local state pointer
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:    PmeGpuProgramStorage pmeGpuProgram;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:    if (thisRankHasPmeGpuTask)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                "GPU device stream manager should be initialized in order to use GPU for PME.");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                           "GPU device should be initialized in order to use GPU for PME.");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:        pmeGpuProgram = buildPmeGpuProgram(deviceStreamManager->context());
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                GMX_RELEASE_ASSERT(!runScheduleWork.simulationWork.useGpuPme
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                                   "Device stream manager should be valid in order to use GPU "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                        !runScheduleWork.simulationWork.useGpuPme
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                        "GPU PME stream should be valid in order to use GPU version of PME.");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                const DeviceContext* deviceContext = runScheduleWork.simulationWork.useGpuPme
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                        runScheduleWork.simulationWork.useGpuPme
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                                       pmeGpuProgram.get(),
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:        if (runScheduleWork.simulationWork.useGpuFBufferOps)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:            fr->gpuForceReduction[gmx::AtomLocality::Local] = std::make_unique<gmx::GpuForceReduction>(
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:            fr->gpuForceReduction[gmx::AtomLocality::NonLocal] = std::make_unique<gmx::GpuForceReduction>(
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:            if (runScheduleWork.simulationWork.useMdGpuGraph)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                        std::make_unique<gmx::MdGpuGraph>(*fr->deviceStreamManager,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                        std::make_unique<gmx::MdGpuGraph>(*fr->deviceStreamManager,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:        std::unique_ptr<gmx::StatePropagatorDataGpu> stateGpu;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:        if (gpusWereDetected && gmx::needStateGpu(runScheduleWork.simulationWork))
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:            GpuApiCallBehavior transferKind =
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                            ? GpuApiCallBehavior::Async
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                            : GpuApiCallBehavior::Sync;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                               "GPU device stream manager should be initialized to use GPU.");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:            stateGpu = std::make_unique<gmx::StatePropagatorDataGpu>(
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                    *deviceStreamManager, transferKind, pme_gpu_get_block_size(fr->pmedata), wcycle.get());
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:            fr->stateGpu = stateGpu.get();
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:          /* set GPU device id */
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:             plumed_cmd(plumedmain,"setGpuDeviceId", &deviceId);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:          if(useGpuForUpdate) {
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                        "This simulation is resident on GPU (-update gpu)\n"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:        if (fr->pmePpCommGpu)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:            // destroy object since it is no longer required. (This needs to be done while the GPU context still exists.)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:            fr->pmePpCommGpu.reset();
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:                    runScheduleWork.simulationWork.useGpuPmePpCommunication,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:    // before we destroy the GPU context(s)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:    // Pinned buffers are associated with contexts in CUDA.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:    // As soon as we destroy GPU contexts after mdrunner() exits, these lines should go.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:    mdModules_.reset(nullptr); // destruct force providers here as they might also use the GPU
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:    fr.reset(nullptr);         // destruct forcerec before gpu
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:        /* stop the GPU profiler (only CUDA) */
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:        stopGpuProfiler();
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:     * destroying the CUDA context as some tMPI ranks may be sharing
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:     * GPU and context.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:     * This is not a concern in OpenCL where we use one context per rank.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:     * Note: it is safe to not call the barrier on the ranks which do not use GPU,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:     * Note that this function needs to be called even if GPUs are not used
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:     * in this run because the PME ranks have no knowledge of whether GPUs
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:     * that it's not needed anymore (with a shared GPU run).
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:    const bool haveDetectedOrForcedCudaAwareMpi =
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:            (gmx::checkMpiCudaAwareSupport() == gmx::GpuAwareMpiStatus::Supported
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:             || gmx::checkMpiCudaAwareSupport() == gmx::GpuAwareMpiStatus::Forced);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:    if (!haveDetectedOrForcedCudaAwareMpi)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:        // Don't reset GPU in case of GPU-AWARE MPI
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp:        // UCX creates GPU buffers which are cleaned-up as part of MPI_Finalize()
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/legacymdrunoptions.cpp:    // which compatible GPUs are availble for use, or to select a GPU
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/legacymdrunoptions.cpp:        hw_opt.userGpuTaskAssignment = userGpuTaskAssignment;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/legacymdrunoptions.cpp:        const char* env = getenv("GMX_GPU_ID");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/legacymdrunoptions.cpp:                gmx_fatal(FARGS, "GMX_GPU_ID and -gpu_id can not be used at the same time");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/legacymdrunoptions.cpp:        env = getenv("GMX_GPUTASKS");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/legacymdrunoptions.cpp:            if (!hw_opt.userGpuTaskAssignment.empty())
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/legacymdrunoptions.cpp:                gmx_fatal(FARGS, "GMX_GPUTASKS and -gputasks can not be used at the same time");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/legacymdrunoptions.cpp:            hw_opt.userGpuTaskAssignment = env;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/legacymdrunoptions.cpp:        if (!hw_opt.devicesSelectedByUser.empty() && !hw_opt.userGpuTaskAssignment.empty())
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/legacymdrunoptions.cpp:            gmx_fatal(FARGS, "-gpu_id and -gputasks cannot be used at the same time");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/legacymdrunoptions.cpp.preplumed:    // which compatible GPUs are availble for use, or to select a GPU
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/legacymdrunoptions.cpp.preplumed:        hw_opt.userGpuTaskAssignment = userGpuTaskAssignment;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/legacymdrunoptions.cpp.preplumed:        const char* env = getenv("GMX_GPU_ID");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/legacymdrunoptions.cpp.preplumed:                gmx_fatal(FARGS, "GMX_GPU_ID and -gpu_id can not be used at the same time");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/legacymdrunoptions.cpp.preplumed:        env = getenv("GMX_GPUTASKS");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/legacymdrunoptions.cpp.preplumed:            if (!hw_opt.userGpuTaskAssignment.empty())
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/legacymdrunoptions.cpp.preplumed:                gmx_fatal(FARGS, "GMX_GPUTASKS and -gputasks can not be used at the same time");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/legacymdrunoptions.cpp.preplumed:            hw_opt.userGpuTaskAssignment = env;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/legacymdrunoptions.cpp.preplumed:        if (!hw_opt.devicesSelectedByUser.empty() && !hw_opt.userGpuTaskAssignment.empty())
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/legacymdrunoptions.cpp.preplumed:            gmx_fatal(FARGS, "-gpu_id and -gputasks cannot be used at the same time");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:    const char* nbpu_opt_choices[5]    = { nullptr, "auto", "cpu", "gpu", nullptr };
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:    const char* pme_opt_choices[5]     = { nullptr, "auto", "cpu", "gpu", nullptr };
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:    const char* pme_fft_opt_choices[5] = { nullptr, "auto", "cpu", "gpu", nullptr };
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:    const char* bonded_opt_choices[5]  = { nullptr, "auto", "cpu", "gpu", nullptr };
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:    const char* update_opt_choices[5]  = { nullptr, "auto", "cpu", "gpu", nullptr };
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:    const char* userGpuTaskAssignment  = "";
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:        { "-gpu_id",
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:          "List of unique GPU device IDs available to use" },
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:        { "-gputasks",
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:          { &userGpuTaskAssignment },
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:          "List of GPU device IDs, mapping each PP task on each node to a device" },
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:          "Optimize PME load between PP/PME ranks or GPU/CPU" },
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/legacymdrunoptions.h:    const char* nbpu_opt_choices[5]    = { nullptr, "auto", "cpu", "gpu", nullptr };
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/legacymdrunoptions.h:    const char* pme_opt_choices[5]     = { nullptr, "auto", "cpu", "gpu", nullptr };
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/legacymdrunoptions.h:    const char* pme_fft_opt_choices[5] = { nullptr, "auto", "cpu", "gpu", nullptr };
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/legacymdrunoptions.h:    const char* bonded_opt_choices[5]  = { nullptr, "auto", "cpu", "gpu", nullptr };
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/legacymdrunoptions.h:    const char* update_opt_choices[5]  = { nullptr, "auto", "cpu", "gpu", nullptr };
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/legacymdrunoptions.h:    const char* userGpuTaskAssignment  = "";
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/legacymdrunoptions.h:        { "-gpu_id",
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/legacymdrunoptions.h:          "List of unique GPU device IDs available to use" },
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/legacymdrunoptions.h:        { "-gputasks",
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/legacymdrunoptions.h:          { &userGpuTaskAssignment },
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/legacymdrunoptions.h:          "List of GPU device IDs, mapping each PP task on each node to a device" },
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/legacymdrunoptions.h:          "Optimize PME load between PP/PME ranks or GPU/CPU" },
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:#include "gromacs/domdec/gpuhaloexchange.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:#include "gromacs/ewald/pme_gpu_program.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:#include "gromacs/ewald/pme_pp_comm_gpu.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:#include "gromacs/gpu_utils/device_stream_manager.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:#include "gromacs/gpu_utils/gpueventsynchronizer_helpers.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:#include "gromacs/listed_forces/listed_forces_gpu.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:#include "gromacs/mdlib/gpuforcereduction.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:#include "gromacs/mdlib/mdgraph_gpu.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:#include "gromacs/mdtypes/state_propagator_data_gpu.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:#include "gromacs/nbnxm/gpu_data_mgmt.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:#include "gromacs/taskassignment/decidegpuusage.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:#include "gromacs/taskassignment/usergpuids.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:#include "gromacs/timing/gpu_timing.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed: * the GPU communication flags are set to false in non-tMPI and non-CUDA builds.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed: * \param[in]  useGpuForNonbonded   True if the nonbonded task is offloaded in this run.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                                         const bool           useGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:    devFlags.enableGpuBufferOps = (GMX_GPU_CUDA || GMX_GPU_SYCL) && useGpuForNonbonded
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                  && (getenv("GMX_USE_GPU_BUFFER_OPS") != nullptr);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:    if (getenv("GMX_CUDA_GRAPH") != nullptr)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:        if (GMX_HAVE_CUDA_GRAPH_SUPPORT)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:            devFlags.enableCudaGraphs = true;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                            "GMX_CUDA_GRAPH environment variable is detected. "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                            "The experimental CUDA Graphs feature will be used if run conditions "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:            devFlags.enableCudaGraphs = false;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                            "GMX_CUDA_GRAPH environment variable is detected, "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                            "but the CUDA version in use is below the minumum requirement (11.1). "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                            "CUDA Graphs will be disabled.");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:    // Flag use to enable GPU-aware MPI depenendent features such PME GPU decomposition
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:    // GPU-aware MPI is marked available if it has been detected by GROMACS or detection fails but
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:    devFlags.canUseGpuAwareMpi = false;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:    // Direct GPU comm path is being used with GPU-aware MPI
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:    // make sure underlying MPI implementation is GPU-aware
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:    if (GMX_LIB_MPI && (GMX_GPU_CUDA || GMX_GPU_SYCL))
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:        // Allow overriding the detection for GPU-aware MPI
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:        GpuAwareMpiStatus gpuAwareMpiStatus = checkMpiCudaAwareSupport();
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:        const bool        forceGpuAwareMpi  = gpuAwareMpiStatus == GpuAwareMpiStatus::Forced;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:        const bool haveDetectedGpuAwareMpi  = gpuAwareMpiStatus == GpuAwareMpiStatus::Supported;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:        if (getenv("GMX_FORCE_CUDA_AWARE_MPI") != nullptr)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                            "GMX_FORCE_CUDA_AWARE_MPI environment variable is inactive. "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                            "Please use GMX_FORCE_GPU_AWARE_MPI instead.");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:        devFlags.canUseGpuAwareMpi = haveDetectedGpuAwareMpi || forceGpuAwareMpi;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:        if (getenv("GMX_ENABLE_DIRECT_GPU_COMM") != nullptr)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:            if (!haveDetectedGpuAwareMpi && forceGpuAwareMpi)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                // GPU-aware support not detected in MPI library but, user has forced it's use
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                "This run has forced use of 'GPU-aware MPI'. "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                "However, GROMACS cannot determine if underlying MPI is GPU-aware. "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                "Check the GROMACS install guide for recommendations for GPU-aware "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                "GMX_FORCE_GPU_AWARE_MPI environment variable.");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:            if (devFlags.canUseGpuAwareMpi)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                "GMX_ENABLE_DIRECT_GPU_COMM environment variable detected, "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                "enabling direct GPU communication using GPU-aware MPI.");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                "GPU-aware MPI was not detected, will not use direct GPU "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                "for GPU-aware support. If you are certain about GPU-aware support "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                "GMX_FORCE_GPU_AWARE_MPI environment variable.");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:        else if (haveDetectedGpuAwareMpi)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:            // GPU-aware MPI was detected, let the user know that using it may improve performance
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                            "GPU-aware MPI detected, but by default GROMACS will not "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                            "make use the direct GPU communication capabilities of MPI. "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                            "the GMX_ENABLE_DIRECT_GPU_COMM environment variable.");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:        if (getenv("GMX_FORCE_GPU_AWARE_MPI") != nullptr)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:            // Cannot force use of GPU-aware MPI in this build configuration
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                            "A CUDA or SYCL build with an external MPI library is required in "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                            "order to benefit from GMX_FORCE_GPU_AWARE_MPI. That environment "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:    if (devFlags.enableGpuBufferOps)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                        "This run uses the 'GPU buffer ops' feature, enabled by the "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                        "GMX_USE_GPU_BUFFER_OPS environment variable.");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:    // PME decomposition is supported only with CUDA-backend in mixed mode
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:    // CUDA-backend also needs GPU-aware MPI support for decomposition to work
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:    const bool pmeGpuDecompositionRequested =
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:            (pmeRunMode == PmeRunMode::GPU || pmeRunMode == PmeRunMode::Mixed)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:    const bool pmeGpuDecompositionSupported =
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:            (devFlags.canUseGpuAwareMpi && (GMX_GPU_CUDA || GMX_GPU_SYCL)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:             && ((pmeRunMode == PmeRunMode::GPU && (GMX_USE_Heffte || GMX_USE_cuFFTMp))
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:    const bool forcePmeGpuDecomposition = getenv("GMX_GPU_PME_DECOMPOSITION") != nullptr;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:    if (pmeGpuDecompositionSupported && pmeGpuDecompositionRequested)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:        // PME decomposition is supported only when it is forced using GMX_GPU_PME_DECOMPOSITION
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:        if (forcePmeGpuDecomposition)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                            "This run has requested the 'GPU PME decomposition' feature, enabled "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                            "by the GMX_GPU_PME_DECOMPOSITION environment variable. "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                      "Multiple PME tasks were required to run on GPUs, "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                      "Use GMX_GPU_PME_DECOMPOSITION environment variable to enable it.");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:    if (!pmeGpuDecompositionSupported && pmeGpuDecompositionRequested)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:        if (GMX_GPU_CUDA)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                      "PME tasks were required to run on more than one CUDA-devices. To enable "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                      "use MPI with CUDA-aware support and build GROMACS with cuFFTMp support.");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                    "PME tasks were required to run on GPUs, but that is not implemented with "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:    devFlags.enableGpuPmeDecomposition =
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:            forcePmeGpuDecomposition && pmeGpuDecompositionRequested && pmeGpuDecompositionSupported;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                  bool                           makeGpuPairList,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                (makeGpuPairList ? ListSetupType::Gpu : ListSetupType::CpuSimdWhenSupported);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                fplog, cr, ir, nstlist_cmdline, &mtop, box, effectiveAtomDensity.value(), makeGpuPairList, cpuinfo);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:/*! \brief Return whether GPU acceleration of nonbondeds is supported with the given settings.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:static bool gpuAccelerationOfNonbondedIsUseful(const MDLogger&   mdlog,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:    bool        gpuIsUseful = true;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:        /* The GPU code does not support more than one energy group.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:         * If the user requested GPUs explicitly, a fatal error is given later.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:        gpuIsUseful = false;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                    "Multiple energy groups is not implemented for GPUs, falling back to the CPU. "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                    "For better performance, run on the GPU without energy groups and then do "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:    /* There are resource handling issues in the GPU code paths with MTS on anything else than only
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:        gpuIsUseful = false;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                "Multiple time stepping is only supported with GPUs when MTS is only applied to %s "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:        gpuIsUseful = false;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:        warning     = "TPI is not implemented for GPUs.";
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:    if (!gpuIsUseful && issueWarning)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:    return gpuIsUseful;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:    else if (strncmp(optionString, "gpu", 3) == 0)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:        returnValue = TaskTarget::Gpu;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:        auto* nbnxn_gpu_timings =
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                (nbv != nullptr && nbv->useGpu()) ? Nbnxm::gpu_get_timings(nbv->gpu_nbv) : nullptr;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:        gmx_wallclock_gpu_pme_t pme_gpu_timings = {};
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:        if (pme_gpu_task_enabled(pme))
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:            pme_gpu_get_timings(pme, &pme_gpu_timings);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                        nbnxn_gpu_timings,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                        &pme_gpu_timings);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:    EmulateGpuNonbonded emulateGpuNonbonded =
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:            (getenv("GMX_EMULATE_GPU") != nullptr ? EmulateGpuNonbonded::Yes : EmulateGpuNonbonded::No);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:    std::vector<int> userGpuTaskAssignment;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:        userGpuTaskAssignment = parseUserTaskAssignmentString(hw_opt.userGpuTaskAssignment);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:        bool useGpuForNonbonded = false;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:        bool useGpuForPme       = false;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:            // the number of GPUs to choose the number of ranks.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:            auto canUseGpuForNonbonded = buildSupportsNonbondedOnGpu(nullptr);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:            useGpuForNonbonded         = decideWhetherToUseGpusForNonbondedWithThreadMpi(
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                    userGpuTaskAssignment,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                    emulateGpuNonbonded,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                    canUseGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                    gpuAccelerationOfNonbondedIsUseful(mdlog, *inputrec, GMX_THREAD_MPI, doRerun),
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:            useGpuForPme = decideWhetherToUseGpusForPmeWithThreadMpi(useGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                                                     userGpuTaskAssignment,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                                useGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                                useGpuForPme,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:    // Note that when bonded interactions run on a GPU they always run
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:    bool useGpuForNonbonded = false;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:    bool useGpuForPme       = false;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:    bool useGpuForBonded    = false;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:    bool useGpuForUpdate    = false;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:    bool gpusWereDetected   = hwinfo_->ngpu_compatible_tot > 0;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:        // It's possible that there are different numbers of GPUs on
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:        auto canUseGpuForNonbonded = buildSupportsNonbondedOnGpu(nullptr);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:        useGpuForNonbonded         = decideWhetherToUseGpusForNonbonded(
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                userGpuTaskAssignment,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                emulateGpuNonbonded,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                canUseGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                gpuAccelerationOfNonbondedIsUseful(mdlog, *inputrec, !GMX_THREAD_MPI, doRerun),
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                gpusWereDetected);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:        useGpuForPme    = decideWhetherToUseGpusForPme(useGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                                    userGpuTaskAssignment,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                                    gpusWereDetected);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:        useGpuForBonded = decideWhetherToUseGpusForBonded(
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                useGpuForNonbonded, useGpuForPme, bondedTarget, *inputrec, mtop, domdecOptions.numPmeRanks, gpusWereDetected);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:    const PmeRunMode pmeRunMode = determinePmeRunMode(useGpuForPme, pmeFftTarget, *inputrec);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:            mdlog, useGpuForNonbonded, pmeRunMode, cr->sizeOfDefaultCommunicator, domdecOptions.numPmeRanks);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                                              updateTarget == TaskTarget::Gpu);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                || (!useGpuForNonbonded && usingFullElectrostatics(inputrec->coulombtype)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                          useGpuForNonbonded || (emulateGpuNonbonded == EmulateGpuNonbonded::Yes),
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:        useGpuForUpdate = decideWhetherToUseGpuForUpdate(useDomainDecomposition,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                                         useGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                                         gpusWereDetected,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:    const bool canUseDirectGpuComm = decideWhetherDirectGpuCommunicationCanBeUsed(
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:    bool useGpuDirectHalo = false;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:    if (useGpuForNonbonded)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:        // domdecOptions.numPmeRanks == -1 results in 0 separate PME ranks when useGpuForNonbonded is true.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:        useGpuDirectHalo = decideWhetherToUseGpuForHalo(havePPDomainDecomposition,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                                        useGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                                        canUseDirectGpuComm,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:        // The DD builder will disable useGpuDirectHalo if the Y or Z component of any domain is
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:        // smaller than twice the communication distance, since GPU-direct communication presently
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:        // perform well on multiple GPUs in any case, but it is important that our core functionality
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:        // (in particular for testing) does not break depending on GPU direct communication being enabled.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                useGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                useGpuForPme,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                useGpuForUpdate,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                useGpuDirectHalo,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                devFlags.enableGpuPmeDecomposition);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:    GpuTaskAssignments gpuTaskAssignments = GpuTaskAssignmentsBuilder::build(
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:            userGpuTaskAssignment,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:            useGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:            useGpuForPme,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:    DeviceInformation* deviceInfo = gpuTaskAssignments.initDevice(&deviceId);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:        // TODO Pass the GPU streams to ddBuilder to use in buffer
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:    const bool useGpuPmeDecomposition = numPmeDomains.x * numPmeDomains.y > 1 && useGpuForPme;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:    GMX_RELEASE_ASSERT(!useGpuPmeDecomposition || devFlags.enableGpuPmeDecomposition,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                       "GPU PME decomposition works only in the cases where it is supported");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                                              useGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                                              useGpuForBonded,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                                              useGpuForUpdate,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                                              useGpuDirectHalo,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                                              canUseDirectGpuComm,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                                              useGpuPmeDecomposition);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:    if (runScheduleWork.simulationWork.useGpuDirectCommunication && GMX_GPU_CUDA)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:        // Don't enable event counting with GPU Direct comm, see #3988.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:        gmx::internal::disableCudaEventConsumptionCounting();
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:    if (isSimulationMainRank && GMX_GPU_SYCL)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:        bool                      haveAnyGpuWork = simWorkload.useGpuPme || simWorkload.useGpuBonded
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                              || simWorkload.useGpuNonbonded || simWorkload.useGpuUpdate;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:        if (haveAnyGpuWork)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                            "\nNOTE: SYCL GPU support in GROMACS, and the compilers, libraries,\n"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:    gpuTaskAssignments.reportGpuUsage(mdlog, printHostName, pmeRunMode, runScheduleWork.simulationWork);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:        const bool useGpuTiming = decideGpuTimingsUsage();
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                *deviceInfo, runScheduleWork.simulationWork, useGpuTiming);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:    if (!userGpuTaskAssignment.empty())
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:        gpuTaskAssignments.logPerformanceHints(mdlog, numAvailableDevices);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:    check_resource_division_efficiency(hwinfo_, gpuTaskAssignments.thisRankHasAnyGpuTask(), cr, mdlog);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:    // Enable Peer access between GPUs where available
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:    // any of the GPU communication features are active.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:        && (runScheduleWork.simulationWork.useGpuHaloExchange
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:            || runScheduleWork.simulationWork.useGpuPmePpCommunication))
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:        setupGpuDevicePeerAccess(gpuTaskAssignments.deviceIdsAssigned(), mdlog);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:    const bool thisRankHasPmeGpuTask = gpuTaskAssignments.thisRankHasPmeGpuTask();
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:        if (runScheduleWork.simulationWork.useGpuPmePpCommunication && !thisRankHasDuty(cr, DUTY_PME))
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                    "GPU device stream manager should be valid in order to use PME-PP direct "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                    "GPU PP-PME stream should be valid in order to use GPU PME-PP direct "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:            fr->pmePpCommGpu = std::make_unique<gmx::PmePpCommGpu>(
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                runScheduleWork.simulationWork.useGpuNonbonded,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:        // TODO: Move the logic below to a GPU bonded builder
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:        if (runScheduleWork.simulationWork.useGpuBonded)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                               "GPU device stream manager should be valid in order to use GPU "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:            fr->listedForcesGpu = std::make_unique<ListedForcesGpu>(mtop.ffparams,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:        mdAtoms = makeMDAtoms(fplog, mtop, *inputrec, thisRankHasPmeGpuTask);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:        if (globalState && thisRankHasPmeGpuTask)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:            // PME on GPU without DD or on a separate PME rank, and because the local state pointer
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:    PmeGpuProgramStorage pmeGpuProgram;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:    if (thisRankHasPmeGpuTask)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                "GPU device stream manager should be initialized in order to use GPU for PME.");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                           "GPU device should be initialized in order to use GPU for PME.");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:        pmeGpuProgram = buildPmeGpuProgram(deviceStreamManager->context());
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                GMX_RELEASE_ASSERT(!runScheduleWork.simulationWork.useGpuPme
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                   "Device stream manager should be valid in order to use GPU "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                        !runScheduleWork.simulationWork.useGpuPme
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                        "GPU PME stream should be valid in order to use GPU version of PME.");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                const DeviceContext* deviceContext = runScheduleWork.simulationWork.useGpuPme
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                        runScheduleWork.simulationWork.useGpuPme
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                       pmeGpuProgram.get(),
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:        if (runScheduleWork.simulationWork.useGpuFBufferOps)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:            fr->gpuForceReduction[gmx::AtomLocality::Local] = std::make_unique<gmx::GpuForceReduction>(
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:            fr->gpuForceReduction[gmx::AtomLocality::NonLocal] = std::make_unique<gmx::GpuForceReduction>(
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:            if (runScheduleWork.simulationWork.useMdGpuGraph)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                        std::make_unique<gmx::MdGpuGraph>(*fr->deviceStreamManager,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                        std::make_unique<gmx::MdGpuGraph>(*fr->deviceStreamManager,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:        std::unique_ptr<gmx::StatePropagatorDataGpu> stateGpu;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:        if (gpusWereDetected && gmx::needStateGpu(runScheduleWork.simulationWork))
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:            GpuApiCallBehavior transferKind =
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                            ? GpuApiCallBehavior::Async
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                            : GpuApiCallBehavior::Sync;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                               "GPU device stream manager should be initialized to use GPU.");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:            stateGpu = std::make_unique<gmx::StatePropagatorDataGpu>(
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                    *deviceStreamManager, transferKind, pme_gpu_get_block_size(fr->pmedata), wcycle.get());
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:            fr->stateGpu = stateGpu.get();
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:        if (fr->pmePpCommGpu)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:            // destroy object since it is no longer required. (This needs to be done while the GPU context still exists.)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:            fr->pmePpCommGpu.reset();
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:                    runScheduleWork.simulationWork.useGpuPmePpCommunication,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:    // before we destroy the GPU context(s)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:    // Pinned buffers are associated with contexts in CUDA.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:    // As soon as we destroy GPU contexts after mdrunner() exits, these lines should go.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:    mdModules_.reset(nullptr); // destruct force providers here as they might also use the GPU
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:    fr.reset(nullptr);         // destruct forcerec before gpu
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:        /* stop the GPU profiler (only CUDA) */
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:        stopGpuProfiler();
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:     * destroying the CUDA context as some tMPI ranks may be sharing
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:     * GPU and context.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:     * This is not a concern in OpenCL where we use one context per rank.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:     * Note: it is safe to not call the barrier on the ranks which do not use GPU,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:     * Note that this function needs to be called even if GPUs are not used
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:     * in this run because the PME ranks have no knowledge of whether GPUs
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:     * that it's not needed anymore (with a shared GPU run).
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:    const bool haveDetectedOrForcedCudaAwareMpi =
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:            (gmx::checkMpiCudaAwareSupport() == gmx::GpuAwareMpiStatus::Supported
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:             || gmx::checkMpiCudaAwareSupport() == gmx::GpuAwareMpiStatus::Forced);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:    if (!haveDetectedOrForcedCudaAwareMpi)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:        // Don't reset GPU in case of GPU-AWARE MPI
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/runner.cpp.preplumed:        // UCX creates GPU buffers which are cleaned-up as part of MPI_Finalize()
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:#include "gromacs/domdec/gpuhaloexchange.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:#include "gromacs/gpu_utils/device_stream_manager.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:#include "gromacs/gpu_utils/gpu_utils.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:#include "gromacs/mdlib/mdgraph_gpu.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:#include "gromacs/mdlib/update_constrain_gpu.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:#include "gromacs/mdtypes/state_propagator_data_gpu.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:#include "gromacs/nbnxm/gpu_data_mgmt.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:    /* PME load balancing data for GPU kernels */
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:    const bool  useGpuForPme       = simulationWork.useGpuPme;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:    const bool  useGpuForNonbonded = simulationWork.useGpuNonbonded;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:    const bool  useGpuForUpdate    = simulationWork.useGpuUpdate;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:                                 useGpuForPme);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:                   (simulationWork.useGpuFBufferOps || useGpuForUpdate) ? PinningPolicy::PinnedIfSupported
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:    std::unique_ptr<UpdateConstrainGpu> integrator;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:    StatePropagatorDataGpu* stateGpu = fr->stateGpu;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:    if (useGpuForUpdate)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:                           "groups if using GPU update.\n");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:                           "SHAKE is not supported with GPU update.");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:        GMX_RELEASE_ASSERT(useGpuForPme || (useGpuForNonbonded && simulationWork.useGpuXBufferOps),
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:                           "the GPU to use GPU update.\n");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:                           "Only the md integrator is supported with the GPU update.\n");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:                "Nose-Hoover temperature coupling is not supported with the GPU update.\n");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:                "with the GPU update.\n");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:                           "Virtual sites are not supported with the GPU update.\n");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:                           "Essential dynamics is not supported with the GPU update.\n");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:                           "Constraints pulling is not supported with the GPU update.\n");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:                           "Orientation restraints are not supported with the GPU update.\n");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:                "Free energy perturbation of masses and constraints are not supported with the GPU "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:                    .appendText("Updating coordinates and applying constraints on the GPU.");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:            GMX_LOG(mdlog.info).asParagraph().appendText("Updating coordinates on the GPU.");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:                           "Device stream manager should be initialized in order to use GPU "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:                "Update stream should be initialized in order to use GPU "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:        integrator = std::make_unique<UpdateConstrainGpu>(
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:        stateGpu->setXUpdatedOnDeviceEvent(integrator->xUpdatedOnDeviceEvent());
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:    if (useGpuForPme || simulationWork.useGpuXBufferOps || useGpuForUpdate)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:    if (useGpuForUpdate)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:     * Disable PME tuning with GPU PME decomposition */
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:                && ir->cutoff_scheme != CutoffScheme::Group && !simulationWork.useGpuPmeDecomposition);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:                &pme_loadbal, cr, mdlog, *ir, state->box, *fr->ic, *fr->nbv, fr->pmedata, fr->nbv->useGpu());
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:    bool usedMdGpuGraphLastStep = false;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:            if (useGpuForUpdate && !bFirstStep)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:                stateGpu->copyCoordinatesFromGpu(state->x, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:                stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:            /* PME grid + cut-off optimization with GPUs or PME nodes */
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:                           simulationWork.useGpuPmePpCommunication);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:        // On search steps, when doing the update on the GPU, copy
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:        if (useGpuForUpdate && bNS && !bFirstStep && !bExchanged)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:            stateGpu->copyVelocitiesFromGpu(state->v, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:            stateGpu->copyCoordinatesFromGpu(state->x, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:            stateGpu->waitVelocitiesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:            stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:            // the GPU Update object should be informed
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:            if (useGpuForUpdate && (bMainState || bExchanged))
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:        // Allocate or re-size GPU halo exchange object, if necessary
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:        if (bNS && simulationWork.havePpDomainDecomposition && simulationWork.useGpuHaloExchange)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:                               "GPU device manager has to be initialized to use GPU "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:            constructGpuHaloExchange(*cr, *fr->deviceStreamManager, wcycle);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:        MdGpuGraph* mdGraph = simulationWork.useMdGpuGraph ? fr->mdGraph[step % 2].get() : nullptr;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:        if (simulationWork.useMdGpuGraph)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:                mdGraph->setUsedGraphLastStep(usedMdGpuGraphLastStep);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:                bool canUseMdGpuGraphThisStep =
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:                if (mdGraph->captureThisStep(canUseMdGpuGraphThisStep))
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:                    mdGraph->startRecord(stateGpu->getCoordinatesReadyOnDeviceEvent(
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:        if (!simulationWork.useMdGpuGraph || mdGraph->graphIsCapturingThisStep()
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:            // Copy coordinate from the GPU for the output/checkpointing if the update is offloaded
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:            if (useGpuForUpdate && !bNS && !runScheduleWork->domainWork.haveCpuLocalForceWork
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:                stateGpu->copyCoordinatesFromGpu(state->x, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:                stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:            if (useGpuForUpdate && !bNS
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:                stateGpu->copyVelocitiesFromGpu(state->v, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:                stateGpu->waitVelocitiesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:            // Copy forces for the output if the forces were reduced on the GPU (not the case on virial steps)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:            // and update is offloaded hence forces are kept on the GPU for update and have not been
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:            //       when the forces are ready on the GPU -- the same synchronizer should be used as the one
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:            //       prior to GPU update.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:            if (runScheduleWork->stepWork.useGpuFBufferOps
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:                && (simulationWork.useGpuUpdate && !vsite) && do_per_step(step, ir->nstfout))
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:                stateGpu->copyForcesFromGpu(f.view().force(), AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:                stateGpu->waitForcesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:            if (!useGpuForUpdate)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:                GMX_ASSERT(!useGpuForUpdate, "GPU update is not supported with VVAK integrator.");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:                if (useGpuForUpdate)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:                        integrator->set(stateGpu->getCoordinates(),
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:                                        stateGpu->getVelocities(),
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:                                        stateGpu->getForces(),
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:                        // Copy data to the GPU after buffers might have been reinitialized
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:                        stateGpu->copyVelocitiesToGpu(state->v, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:                    // Copy x to the GPU unless we have already transferred in do_force().
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:                    // We transfer in do_force() if a GPU force task requires x (PME or x buffer ops).
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:                    if (!(runScheduleWork->stepWork.haveGpuPmeOnThisRank
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:                          || runScheduleWork->stepWork.useGpuXBufferOps))
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:                        stateGpu->copyCoordinatesToGpu(state->x, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:                        stateGpu->consumeCoordinatesCopiedToDeviceEvent(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:                    if ((simulationWork.useGpuPme && simulationWork.useCpuPmePpCommunication)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:                        || (!runScheduleWork->stepWork.useGpuFBufferOps))
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:                        // rest of the forces computed on the GPU, so the final forces have to be
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:                        // copied back to the GPU. Or the buffer ops were not offloaded this step,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:                        stateGpu->copyForcesToGpu(f.view().force(), AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:                    integrator->integrate(stateGpu->getLocalForcesReadyOnDeviceEvent(
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:        if (simulationWork.useMdGpuGraph)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:            GMX_ASSERT((mdGraph != nullptr), "MD GPU graph does not exist.");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:                // since the GPU kernels chosen by the FFT library can vary with grid size
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:            usedMdGpuGraphLastStep = mdGraph->useGraphThisStep();
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:            if (useGpuForUpdate)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:                    stateGpu->copyCoordinatesFromGpu(state->x, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:                    stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:                    stateGpu->copyVelocitiesFromGpu(state->v, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:                    stateGpu->waitVelocitiesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:                    if (useGpuForUpdate)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:                        stateGpu->resetCoordinatesCopiedToDeviceEvent(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:                        stateGpu->copyCoordinatesToGpu(state->x, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:                        stateGpu->waitCoordinatesCopiedToDevice(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:                            stateGpu->copyVelocitiesToGpu(state->v, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:        const bool scaleCoordinates = !useGpuForUpdate || bDoReplEx;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:        if (useGpuForUpdate
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:        // any run that uses GPUs must be at least offloading nonbondeds
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:        const bool usingGpu = simulationWork.useGpuNonbonded;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:        if (usingGpu)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:            // ensure that GPU errors do not propagate between MD steps
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp.preplumed:        pme_loadbal_done(pme_loadbal, fplog, mdlog, fr->nbv->useGpu());
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/rerun.cpp.preplumed:#include "gromacs/gpu_utils/gpu_utils.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/rerun.cpp.preplumed:                                 runScheduleWork->simulationWork.useGpuPme);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/rerun.cpp:#include "gromacs/gpu_utils/gpu_utils.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/rerun.cpp:                                 runScheduleWork->simulationWork.useGpuPme);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:#include "gromacs/domdec/gpuhaloexchange.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:#include "gromacs/gpu_utils/device_stream_manager.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:#include "gromacs/gpu_utils/gpu_utils.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:#include "gromacs/mdlib/mdgraph_gpu.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:#include "gromacs/mdlib/update_constrain_gpu.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:#include "gromacs/mdtypes/state_propagator_data_gpu.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:#include "gromacs/nbnxm/gpu_data_mgmt.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:    /* PME load balancing data for GPU kernels */
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:    const bool  useGpuForPme       = simulationWork.useGpuPme;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:    const bool  useGpuForNonbonded = simulationWork.useGpuNonbonded;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:    const bool  useGpuForUpdate    = simulationWork.useGpuUpdate;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:                                 useGpuForPme);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:                   (simulationWork.useGpuFBufferOps || useGpuForUpdate) ? PinningPolicy::PinnedIfSupported
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:    std::unique_ptr<UpdateConstrainGpu> integrator;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:    StatePropagatorDataGpu* stateGpu = fr->stateGpu;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:    if (useGpuForUpdate)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:                           "groups if using GPU update.\n");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:                           "SHAKE is not supported with GPU update.");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:        GMX_RELEASE_ASSERT(useGpuForPme || (useGpuForNonbonded && simulationWork.useGpuXBufferOps),
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:                           "the GPU to use GPU update.\n");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:                           "Only the md integrator is supported with the GPU update.\n");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:                "Nose-Hoover temperature coupling is not supported with the GPU update.\n");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:                "with the GPU update.\n");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:                           "Virtual sites are not supported with the GPU update.\n");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:                           "Essential dynamics is not supported with the GPU update.\n");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:                           "Constraints pulling is not supported with the GPU update.\n");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:                           "Orientation restraints are not supported with the GPU update.\n");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:                "Free energy perturbation of masses and constraints are not supported with the GPU "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:                    .appendText("Updating coordinates and applying constraints on the GPU.");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:            GMX_LOG(mdlog.info).asParagraph().appendText("Updating coordinates on the GPU.");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:                           "Device stream manager should be initialized in order to use GPU "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:                "Update stream should be initialized in order to use GPU "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:        integrator = std::make_unique<UpdateConstrainGpu>(
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:        stateGpu->setXUpdatedOnDeviceEvent(integrator->xUpdatedOnDeviceEvent());
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:    if (useGpuForPme || simulationWork.useGpuXBufferOps || useGpuForUpdate)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:    if (useGpuForUpdate)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:     * Disable PME tuning with GPU PME decomposition */
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:                && ir->cutoff_scheme != CutoffScheme::Group && !simulationWork.useGpuPmeDecomposition);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:                &pme_loadbal, cr, mdlog, *ir, state->box, *fr->ic, *fr->nbv, fr->pmedata, fr->nbv->useGpu());
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:    bool usedMdGpuGraphLastStep = false;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:            if (useGpuForUpdate && !bFirstStep)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:                stateGpu->copyCoordinatesFromGpu(state->x, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:                stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:            /* PME grid + cut-off optimization with GPUs or PME nodes */
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:                           simulationWork.useGpuPmePpCommunication);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:        // On search steps, when doing the update on the GPU, copy
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:        if (useGpuForUpdate && bNS && !bFirstStep && !bExchanged)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:            stateGpu->copyVelocitiesFromGpu(state->v, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:            stateGpu->copyCoordinatesFromGpu(state->x, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:            stateGpu->waitVelocitiesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:            stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:            // the GPU Update object should be informed
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:            if (useGpuForUpdate && (bMainState || bExchanged))
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:        // Allocate or re-size GPU halo exchange object, if necessary
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:        if (bNS && simulationWork.havePpDomainDecomposition && simulationWork.useGpuHaloExchange)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:                               "GPU device manager has to be initialized to use GPU "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:            constructGpuHaloExchange(*cr, *fr->deviceStreamManager, wcycle);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:        MdGpuGraph* mdGraph = simulationWork.useMdGpuGraph ? fr->mdGraph[step % 2].get() : nullptr;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:        if (simulationWork.useMdGpuGraph)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:                mdGraph->setUsedGraphLastStep(usedMdGpuGraphLastStep);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:                bool canUseMdGpuGraphThisStep =
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:                if (mdGraph->captureThisStep(canUseMdGpuGraphThisStep))
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:                    mdGraph->startRecord(stateGpu->getCoordinatesReadyOnDeviceEvent(
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:        if (!simulationWork.useMdGpuGraph || mdGraph->graphIsCapturingThisStep()
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:            // Copy coordinate from the GPU for the output/checkpointing if the update is offloaded
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:            if (useGpuForUpdate && !bNS && !runScheduleWork->domainWork.haveCpuLocalForceWork
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:                stateGpu->copyCoordinatesFromGpu(state->x, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:                stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:            if (useGpuForUpdate && !bNS
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:                stateGpu->copyVelocitiesFromGpu(state->v, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:                stateGpu->waitVelocitiesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:            // Copy forces for the output if the forces were reduced on the GPU (not the case on virial steps)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:            // and update is offloaded hence forces are kept on the GPU for update and have not been
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:            //       when the forces are ready on the GPU -- the same synchronizer should be used as the one
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:            //       prior to GPU update.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:            if (runScheduleWork->stepWork.useGpuFBufferOps
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:                && (simulationWork.useGpuUpdate && !vsite) && do_per_step(step, ir->nstfout))
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:                stateGpu->copyForcesFromGpu(f.view().force(), AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:                stateGpu->waitForcesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:            if (!useGpuForUpdate)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:                GMX_ASSERT(!useGpuForUpdate, "GPU update is not supported with VVAK integrator.");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:                if (useGpuForUpdate)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:                        integrator->set(stateGpu->getCoordinates(),
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:                                        stateGpu->getVelocities(),
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:                                        stateGpu->getForces(),
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:                        // Copy data to the GPU after buffers might have been reinitialized
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:                        stateGpu->copyVelocitiesToGpu(state->v, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:                    // Copy x to the GPU unless we have already transferred in do_force().
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:                    // We transfer in do_force() if a GPU force task requires x (PME or x buffer ops).
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:                    if (!(runScheduleWork->stepWork.haveGpuPmeOnThisRank
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:                          || runScheduleWork->stepWork.useGpuXBufferOps))
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:                        stateGpu->copyCoordinatesToGpu(state->x, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:                        stateGpu->consumeCoordinatesCopiedToDeviceEvent(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:                    if ((simulationWork.useGpuPme && simulationWork.useCpuPmePpCommunication)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:                        || (!runScheduleWork->stepWork.useGpuFBufferOps))
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:                        // rest of the forces computed on the GPU, so the final forces have to be
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:                        // copied back to the GPU. Or the buffer ops were not offloaded this step,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:                        stateGpu->copyForcesToGpu(f.view().force(), AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:                    integrator->integrate(stateGpu->getLocalForcesReadyOnDeviceEvent(
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:        if (simulationWork.useMdGpuGraph)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:            GMX_ASSERT((mdGraph != nullptr), "MD GPU graph does not exist.");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:                // since the GPU kernels chosen by the FFT library can vary with grid size
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:            usedMdGpuGraphLastStep = mdGraph->useGraphThisStep();
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:            if (useGpuForUpdate)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:                    stateGpu->copyCoordinatesFromGpu(state->x, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:                    stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:                    stateGpu->copyVelocitiesFromGpu(state->v, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:                    stateGpu->waitVelocitiesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:                    if (useGpuForUpdate)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:                        stateGpu->resetCoordinatesCopiedToDeviceEvent(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:                        stateGpu->copyCoordinatesToGpu(state->x, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:                        stateGpu->waitCoordinatesCopiedToDevice(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:                            stateGpu->copyVelocitiesToGpu(state->v, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:        const bool scaleCoordinates = !useGpuForUpdate || bDoReplEx;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:        if (useGpuForUpdate
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:        // any run that uses GPUs must be at least offloading nonbondeds
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:        const bool usingGpu = simulationWork.useGpuNonbonded;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:        if (usingGpu)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:            // ensure that GPU errors do not propagate between MD steps
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/mdrun/md.cpp:        pme_loadbal_done(pme_loadbal, fplog, mdlog, fr->nbv->useGpu());
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:if (GMX_CLANG_CUDA)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:    include(gmxClangCudaUtils)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:set_property(GLOBAL PROPERTY CUDA_SOURCES)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:add_subdirectory(gpu_utils)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:# Mark some shared GPU implementation files to compile with CUDA if needed
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:if (GMX_GPU_CUDA)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:    get_property(CUDA_SOURCES GLOBAL PROPERTY CUDA_SOURCES)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:    set_source_files_properties(${CUDA_SOURCES} PROPERTIES CUDA_SOURCE_PROPERTY_FORMAT OBJ)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:if (GMX_GPU_CUDA)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:        # needed as we need to include cufftmp include path before CUDA include path
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:        cuda_include_directories(${cuFFTMp_INCLUDE_DIR})
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:    # Work around FindCUDA that prevents using target_link_libraries()
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:    set(CUDA_LIBRARIES PRIVATE ${CUDA_LIBRARIES})
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:    if (NOT GMX_CLANG_CUDA)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:        gmx_cuda_add_library(libgromacs ${LIBGROMACS_SOURCES})
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:        target_link_libraries(libgromacs PRIVATE ${CUDA_CUFFT_LIBRARIES})
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:if(GMX_GPU_FFT_VKFFT)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:if(GMX_GPU_FFT_ROCFFT)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:if (GMX_GPU_FFT_CLFFT)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:    if (NOT GMX_GPU_OPENCL)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:        message(FATAL_ERROR "clFFT is only supported in OpenCL builds")
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:"An OpenCL build was requested with Visual Studio compiler, but GROMACS
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:clFFT to help with building for OpenCL, but that clFFT has not yet been
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:requires. Thus for now, OpenCL is not available with MSVC and the internal
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:installing a clFFT package, use VkFFT by setting -DGMX_GPU_FFT_LIBRARY=VkFFT,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:# CUDA runtime headers
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:if (GMX_GPU_CUDA AND CMAKE_CXX_COMPILER_ID MATCHES "Clang")
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:        set(GMX_CUDA_CLANG_FLAGS "${GMX_CUDA_CLANG_FLAGS} ${_compile_flag}")
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:    if (GMX_CLANG_CUDA)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:            get_source_file_property(_cuda_source_format ${_file} CUDA_SOURCE_PROPERTY_FORMAT)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:            if ("${_ext}" STREQUAL ".cu" OR _cuda_source_format)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:                gmx_compile_cuda_file_with_clang(${_file})
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:        get_property(CUDA_SOURCES GLOBAL PROPERTY CUDA_SOURCES)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:        set_source_files_properties(${CUDA_SOURCES} PROPERTIES COMPILE_FLAGS ${GMX_CUDA_CLANG_FLAGS})
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:if (GMX_GPU_SYCL)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:                      ${OpenCL_LIBRARIES}
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:                      $<BUILD_INTERFACE:gpu_utils>
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:# Technically, the user could want to do this for an OpenCL build
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:# using the CUDA runtime, but currently there's no reason to want to
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:if (INSTALL_CUDART_LIB) #can be set manual by user
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:    if (GMX_GPU_CUDA)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:        foreach(CUDA_LIB ${CUDA_LIBRARIES})
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:            string(REGEX MATCH "cudart" IS_CUDART ${CUDA_LIB})
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:            if(IS_CUDART) #libcuda should not be installed
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:                file(GLOB CUDA_LIBS ${CUDA_LIB}*)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:                install(FILES ${CUDA_LIBS} DESTINATION
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:        message(WARNING "INSTALL_CUDART_LIB only makes sense when configuring for CUDA support")
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:if(GMX_GPU_OPENCL)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:    file(GLOB OPENCL_INSTALLED_FILES
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:        gpu_utils/vectype_ops.clh
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:        gpu_utils/device_utils.clh
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:    install(FILES ${OPENCL_INSTALLED_FILES}
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:        DESTINATION ${GMX_INSTALL_OCLDIR}/gromacs/gpu_utils
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:    file(GLOB OPENCL_INSTALLED_FILES
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:    install(FILES ${OPENCL_INSTALLED_FILES}
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:    file(GLOB OPENCL_INSTALLED_FILES
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:    install(FILES ${OPENCL_INSTALLED_FILES}
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:    file(GLOB OPENCL_INSTALLED_FILES
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:        nbnxm/opencl/nbnxm_ocl_kernels.cl
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:        nbnxm/opencl/nbnxm_ocl_kernel.clh
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:        nbnxm/opencl/nbnxm_ocl_kernel_pruneonly.clh
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:        nbnxm/opencl/nbnxm_ocl_kernels.clh
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:        nbnxm/opencl/nbnxm_ocl_kernels_fastgen.clh
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:        nbnxm/opencl/nbnxm_ocl_kernels_fastgen_add_twincut.clh
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:        nbnxm/opencl/nbnxm_ocl_kernel_utils.clh
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:        nbnxm/opencl/nbnxm_ocl_consts.h
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:    install(FILES ${OPENCL_INSTALLED_FILES}
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:        DESTINATION ${GMX_INSTALL_OCLDIR}/gromacs/nbnxm/opencl
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:    file(GLOB OPENCL_INSTALLED_FILES
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:        ewald/pme_gpu_calculate_splines.clh
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:        ewald/pme_gpu_types.h
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt:    install(FILES ${OPENCL_INSTALLED_FILES}
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:if (GMX_CLANG_CUDA)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:    include(gmxClangCudaUtils)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:set_property(GLOBAL PROPERTY CUDA_SOURCES)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:add_subdirectory(gpu_utils)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:# Mark some shared GPU implementation files to compile with CUDA if needed
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:if (GMX_GPU_CUDA)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:    get_property(CUDA_SOURCES GLOBAL PROPERTY CUDA_SOURCES)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:    set_source_files_properties(${CUDA_SOURCES} PROPERTIES CUDA_SOURCE_PROPERTY_FORMAT OBJ)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:if (GMX_GPU_CUDA)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:        # needed as we need to include cufftmp include path before CUDA include path
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:        cuda_include_directories(${cuFFTMp_INCLUDE_DIR})
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:    # Work around FindCUDA that prevents using target_link_libraries()
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:    set(CUDA_LIBRARIES PRIVATE ${CUDA_LIBRARIES})
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:    if (NOT GMX_CLANG_CUDA)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:        gmx_cuda_add_library(libgromacs ${LIBGROMACS_SOURCES})
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:        target_link_libraries(libgromacs PRIVATE ${CUDA_CUFFT_LIBRARIES})
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:if(GMX_GPU_FFT_VKFFT)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:if(GMX_GPU_FFT_ROCFFT)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:if (GMX_GPU_FFT_CLFFT)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:    if (NOT GMX_GPU_OPENCL)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:        message(FATAL_ERROR "clFFT is only supported in OpenCL builds")
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:"An OpenCL build was requested with Visual Studio compiler, but GROMACS
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:clFFT to help with building for OpenCL, but that clFFT has not yet been
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:requires. Thus for now, OpenCL is not available with MSVC and the internal
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:installing a clFFT package, use VkFFT by setting -DGMX_GPU_FFT_LIBRARY=VkFFT,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:# CUDA runtime headers
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:if (GMX_GPU_CUDA AND CMAKE_CXX_COMPILER_ID MATCHES "Clang")
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:        set(GMX_CUDA_CLANG_FLAGS "${GMX_CUDA_CLANG_FLAGS} ${_compile_flag}")
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:    if (GMX_CLANG_CUDA)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:            get_source_file_property(_cuda_source_format ${_file} CUDA_SOURCE_PROPERTY_FORMAT)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:            if ("${_ext}" STREQUAL ".cu" OR _cuda_source_format)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:                gmx_compile_cuda_file_with_clang(${_file})
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:        get_property(CUDA_SOURCES GLOBAL PROPERTY CUDA_SOURCES)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:        set_source_files_properties(${CUDA_SOURCES} PROPERTIES COMPILE_FLAGS ${GMX_CUDA_CLANG_FLAGS})
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:if (GMX_GPU_SYCL)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:                      ${OpenCL_LIBRARIES}
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:                      $<BUILD_INTERFACE:gpu_utils>
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:# Technically, the user could want to do this for an OpenCL build
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:# using the CUDA runtime, but currently there's no reason to want to
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:if (INSTALL_CUDART_LIB) #can be set manual by user
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:    if (GMX_GPU_CUDA)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:        foreach(CUDA_LIB ${CUDA_LIBRARIES})
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:            string(REGEX MATCH "cudart" IS_CUDART ${CUDA_LIB})
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:            if(IS_CUDART) #libcuda should not be installed
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:                file(GLOB CUDA_LIBS ${CUDA_LIB}*)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:                install(FILES ${CUDA_LIBS} DESTINATION
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:        message(WARNING "INSTALL_CUDART_LIB only makes sense when configuring for CUDA support")
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:if(GMX_GPU_OPENCL)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:    file(GLOB OPENCL_INSTALLED_FILES
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:        gpu_utils/vectype_ops.clh
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:        gpu_utils/device_utils.clh
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:    install(FILES ${OPENCL_INSTALLED_FILES}
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:        DESTINATION ${GMX_INSTALL_OCLDIR}/gromacs/gpu_utils
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:    file(GLOB OPENCL_INSTALLED_FILES
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:    install(FILES ${OPENCL_INSTALLED_FILES}
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:    file(GLOB OPENCL_INSTALLED_FILES
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:    install(FILES ${OPENCL_INSTALLED_FILES}
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:    file(GLOB OPENCL_INSTALLED_FILES
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:        nbnxm/opencl/nbnxm_ocl_kernels.cl
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:        nbnxm/opencl/nbnxm_ocl_kernel.clh
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:        nbnxm/opencl/nbnxm_ocl_kernel_pruneonly.clh
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:        nbnxm/opencl/nbnxm_ocl_kernels.clh
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:        nbnxm/opencl/nbnxm_ocl_kernels_fastgen.clh
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:        nbnxm/opencl/nbnxm_ocl_kernels_fastgen_add_twincut.clh
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:        nbnxm/opencl/nbnxm_ocl_kernel_utils.clh
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:        nbnxm/opencl/nbnxm_ocl_consts.h
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:    install(FILES ${OPENCL_INSTALLED_FILES}
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:        DESTINATION ${GMX_INSTALL_OCLDIR}/gromacs/nbnxm/opencl
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:    file(GLOB OPENCL_INSTALLED_FILES
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:        ewald/pme_gpu_calculate_splines.clh
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:        ewald/pme_gpu_types.h
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/CMakeLists.txt.preplumed:    install(FILES ${OPENCL_INSTALLED_FILES}
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed: * \brief Defines functionality for deciding whether tasks will run on GPUs.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:#include "gromacs/taskassignment/decidegpuusage.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:#include "gromacs/listed_forces/listed_forces_gpu.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:#include "gromacs/mdlib/update_constrain_gpu.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:        "When you use mdrun -gputasks, %s must be set to non-default "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:#if GMX_GPU
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:        " If you simply want to restrict which GPUs are used, then it is "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:        "better to use mdrun -gpu_id. Otherwise, setting the "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:#    if GMX_GPU_CUDA
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:        "CUDA_VISIBLE_DEVICES"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:#    elif GMX_GPU_OPENCL
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:        // OpenCL standard, but the only current relevant case for GROMACS
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:        // is AMD OpenCL, which offers this variable.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:        "GPU_DEVICE_ORDINAL"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:#    elif GMX_GPU_SYCL && GMX_SYCL_DPCPP
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:#    elif GMX_GPU_SYCL && GMX_SYCL_HIPSYCL
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:        // Not true if we use hipSYCL over CUDA or IntelLLVM, but in that case the user probably
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:        // https://rocmdocs.amd.com/en/latest/Other_Solutions/Other-Solutions.html#hip-environment-variables
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:constexpr bool c_gpuBuildSyclWithoutGpuFft =
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:        (GMX_GPU_SYCL != 0) && (GMX_GPU_FFT_MKL == 0) && (GMX_GPU_FFT_ROCFFT == 0)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:        && (GMX_GPU_FFT_VKFFT == 0) && (GMX_GPU_FFT_DBFFT == 0); // NOLINT(misc-redundant-expression)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:bool decideWhetherToUseGpusForNonbondedWithThreadMpi(const TaskTarget        nonbondedTarget,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:                                                     const std::vector<int>& userGpuTaskAssignment,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:                                                     const EmulateGpuNonbonded emulateGpuNonbonded,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:                                                     const bool buildSupportsNonbondedOnGpu,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:                                                     const bool nonbondedOnGpuIsUseful,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    // First, exclude all cases where we can't run NB on GPUs.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    if (nonbondedTarget == TaskTarget::Cpu || emulateGpuNonbonded == EmulateGpuNonbonded::Yes
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:        || !nonbondedOnGpuIsUseful || !buildSupportsNonbondedOnGpu)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:        // If the user required NB on GPUs, we issue an error later.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    // We now know that NB on GPUs makes sense, if we have any.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    if (!userGpuTaskAssignment.empty())
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:        // Specifying -gputasks requires specifying everything.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    if (nonbondedTarget == TaskTarget::Gpu)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    // Because this is thread-MPI, we already know about the GPUs that
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    // If we get here, then the user permitted or required GPUs.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:static bool decideWhetherToUseGpusForPmeFft(const TaskTarget pmeFftTarget)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:                           || (pmeFftTarget == TaskTarget::Auto && c_gpuBuildSyclWithoutGpuFft);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:static bool canUseGpusForPme(const bool        useGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    errorReasons.startContext("Cannot compute PME interactions on a GPU, because:");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    errorReasons.appendIf(!useGpuForNonbonded, "Nonbonded interactions must also run on GPUs.");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    errorReasons.appendIf(!pme_gpu_supports_build(&tempString), tempString);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    errorReasons.appendIf(!pme_gpu_supports_input(inputrec, &tempString), tempString);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    if (!decideWhetherToUseGpusForPmeFft(pmeFftTarget))
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:        errorReasons.appendIf(!pme_gpu_mixed_mode_supports_input(inputrec, &tempString), tempString);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:        if (pmeTarget == TaskTarget::Gpu && errorMessage != nullptr)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:bool decideWhetherToUseGpusForPmeWithThreadMpi(const bool              useGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:                                               const std::vector<int>& userGpuTaskAssignment,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    // First, exclude all cases where we can't run PME on GPUs.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    if (!canUseGpusForPme(useGpuForNonbonded, pmeTarget, pmeFftTarget, inputrec, nullptr))
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:        // PME can't run on a GPU. If the user required that, we issue an error later.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    // We now know that PME on GPUs might make sense, if we have any.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    if (pmeTarget == TaskTarget::Gpu)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:                    "PME tasks were required to run on GPUs with multiple ranks "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    if (!userGpuTaskAssignment.empty())
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:        // Follow the user's choice of GPU task assignment, if we
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:        // can. Checking that their IDs are for compatible GPUs comes
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:        // Specifying -gputasks requires specifying everything.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:        // PME on GPUs is only supported in a single case
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:        if (pmeTarget == TaskTarget::Gpu)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:                        "When you run mdrun -pme gpu -gputasks, you must supply a PME-enabled .tpr "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    // Because this is thread-MPI, we already know about the GPUs that
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    if (pmeTarget == TaskTarget::Gpu)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:                    "PME tasks were required to run on GPUs, but that is not implemented with "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:        // PME can run well on a GPU shared with NB, and we permit
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:        // We have a single separate PME rank, that can use a GPU
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:        // run well on a GPU shared with NB, and we permit mdrun to
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:        // default to it if there is only one GPU available.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    // Not enough support for PME on GPUs for anything else
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:bool decideWhetherToUseGpusForNonbonded(const TaskTarget          nonbondedTarget,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:                                        const std::vector<int>&   userGpuTaskAssignment,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:                                        const EmulateGpuNonbonded emulateGpuNonbonded,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:                                        const bool                buildSupportsNonbondedOnGpu,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:                                        const bool                nonbondedOnGpuIsUseful,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:                                        const bool                gpusWereDetected)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:        if (!userGpuTaskAssignment.empty())
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:                    "A GPU task assignment was specified, but nonbonded interactions were "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    if (!buildSupportsNonbondedOnGpu && nonbondedTarget == TaskTarget::Gpu)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:                "Nonbonded interactions on the GPU were requested with -nb gpu, "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:                "but the GROMACS binary has been built without GPU support. "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:                "Either run without selecting GPU options, or recompile GROMACS "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:                "with GPU support enabled"));
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    // TODO refactor all these TaskTarget::Gpu checks into one place?
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    if (emulateGpuNonbonded == EmulateGpuNonbonded::Yes)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:        if (nonbondedTarget == TaskTarget::Gpu)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:                    "Nonbonded interactions on the GPU were required, which is inconsistent "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:        if (!userGpuTaskAssignment.empty())
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:                    InconsistentInputError("GPU ID usage was specified, as was GPU emulation. Make "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    if (!nonbondedOnGpuIsUseful)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:        if (nonbondedTarget == TaskTarget::Gpu)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:                    "Nonbonded interactions on the GPU were required, but not supported for these "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:                    "simulation settings. Change your settings, or do not require using GPUs."));
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    if (!userGpuTaskAssignment.empty())
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:        // Specifying -gputasks requires specifying everything.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    if (nonbondedTarget == TaskTarget::Gpu)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:        // We still don't know whether it is an error if no GPUs are found
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:        // GPUs.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    // If we get here, then the user permitted GPUs, which we should
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    return buildSupportsNonbondedOnGpu && gpusWereDetected;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:bool decideWhetherToUseGpusForPme(const bool              useGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:                                  const std::vector<int>& userGpuTaskAssignment,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:                                  const bool              gpusWereDetected)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    if (!canUseGpusForPme(useGpuForNonbonded, pmeTarget, pmeFftTarget, inputrec, &message))
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:        if (!userGpuTaskAssignment.empty())
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:                    "A GPU task assignment was specified, but PME interactions were "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    if (pmeTarget == TaskTarget::Gpu)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:                    "PME tasks were required to run on GPUs with multiple ranks "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    if (!userGpuTaskAssignment.empty())
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:        // Specifying -gputasks requires specifying everything.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    // We still don't know whether it is an error if no GPUs are found
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    // GPUs.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    if (pmeTarget == TaskTarget::Gpu)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    // If we get here, then the user permitted GPUs.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:        // PME can run well on a single GPU shared with NB when there
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:        // detected GPUs.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:        return gpusWereDetected;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:        // We have a single separate PME rank, that can use a GPU
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:        return gpusWereDetected;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    // Not enough support for PME on GPUs for anything else
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:PmeRunMode determinePmeRunMode(const bool useGpuForPme, const TaskTarget& pmeFftTarget, const t_inputrec& inputrec)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    if (useGpuForPme)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:        if (c_gpuBuildSyclWithoutGpuFft && pmeFftTarget == TaskTarget::Gpu)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:                      "GROMACS is built without SYCL GPU FFT library. Please do not use -pmefft "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:                      "gpu.");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:        if (!decideWhetherToUseGpusForPmeFft(pmeFftTarget))
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:            return PmeRunMode::GPU;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:        if (pmeFftTarget == TaskTarget::Gpu)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:                      "Assigning FFTs to GPU requires PME to be assigned to GPU as well. With PME "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:bool decideWhetherToUseGpusForBonded(bool              useGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:                                     bool              useGpuForPme,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:                                     bool              gpusWereDetected)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    if (!buildSupportsListedForcesGpu(&errorMessage))
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:        if (bondedTarget == TaskTarget::Gpu)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    if (!inputSupportsListedForcesGpu(inputrec, mtop, &errorMessage))
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:        if (bondedTarget == TaskTarget::Gpu)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    if (!useGpuForNonbonded)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:        if (bondedTarget == TaskTarget::Gpu)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:                    "Bonded interactions on the GPU were required, but this requires that "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:                    "short-ranged non-bonded interactions are also run on the GPU. Change "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:                    "your settings, or do not require using GPUs."));
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    if (bondedTarget == TaskTarget::Gpu)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:        // We still don't know whether it is an error if no GPUs are
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    // If we get here, then the user permitted GPUs, which we should
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    // choose separate PME ranks when nonBonded are assigned to the GPU.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:                                     || (usingPmeOrEwald(inputrec.coulombtype) && !useGpuForPme
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    return gpusWereDetected && usingOurCpuForPmeOrEwald;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:bool decideWhetherToUseGpuForUpdate(const bool           isDomainDecomposition,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:                                    const bool           useGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:                                    const bool           gpusWereDetected,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:            errorMessage += "With separate PME rank(s), PME must run on the GPU.\n";
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    // Using the GPU-version of update if:
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    // 1. PME is on the GPU (there should be a copy of coordinates on GPU for PME spread) or inactive, or
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    // 2. Non-bonded interactions are on the GPU.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    if ((pmeRunMode == PmeRunMode::CPU || pmeRunMode == PmeRunMode::None) && !useGpuForNonbonded)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:                "Either PME or short-ranged non-bonded interaction tasks must run on the GPU.\n";
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    if (!gpusWereDetected)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:        errorMessage += "Compatible GPUs must have been found.\n";
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    if (!(GMX_GPU_CUDA || GMX_GPU_SYCL))
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:        errorMessage += "Only CUDA and SYCL builds are supported.\n";
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    // does not support it, the actual CUDA LINCS code does support it
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    if (!UpdateConstrainGpu::isNumCoupledConstraintsSupported(mtop))
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:                "The number of coupled constraints is higher than supported in the GPU LINCS "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    if (hasAnyConstraints && !UpdateConstrainGpu::areConstraintsSupported())
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:        errorMessage += "Chosen GPU implementation does not support constraints.\n";
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:        // There is a known bug with frozen atoms and GPU update, see Issue #3920.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:                            "Update task can not run on the GPU, because the following "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:        else if (updateTarget == TaskTarget::Gpu)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:                    "Update task on the GPU was required,\n"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    return (updateTarget == TaskTarget::Gpu
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:bool decideWhetherDirectGpuCommunicationCanBeUsed(const DevelopmentFeatureFlags& devFlags,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    const bool buildSupportsDirectGpuComm = (GMX_GPU_CUDA || GMX_GPU_SYCL) && GMX_MPI;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    if (!buildSupportsDirectGpuComm)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    // Direct GPU communication is presently turned off due to insufficient testing
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    const bool enableDirectGpuComm = (getenv("GMX_ENABLE_DIRECT_GPU_COMM") != nullptr)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:                                     || (getenv("GMX_GPU_DD_COMMS") != nullptr)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:                                     || (getenv("GMX_GPU_PME_PP_COMMS") != nullptr);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    if (GMX_THREAD_MPI && GMX_GPU_SYCL && enableDirectGpuComm)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:                        "GMX_ENABLE_DIRECT_GPU_COMM environment variable detected, "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    errorReasons.startContext("GPU direct communication can not be activated because:");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    bool runAndGpuSupportDirectGpuComm = (runUsesCompatibleFeatures && enableDirectGpuComm);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    bool canUseDirectGpuCommWithThreadMpi =
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:            (runAndGpuSupportDirectGpuComm && GMX_THREAD_MPI && !GMX_GPU_SYCL);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    // GPU-aware MPI case off by default, can be enabled with dev flag
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    // Note: GMX_DISABLE_DIRECT_GPU_COMM already taken into account in devFlags.enableDirectGpuCommWithMpi
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    bool canUseDirectGpuCommWithMpi = (runAndGpuSupportDirectGpuComm && GMX_LIB_MPI
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:                                       && devFlags.canUseGpuAwareMpi && enableDirectGpuComm);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    return canUseDirectGpuCommWithThreadMpi || canUseDirectGpuCommWithMpi;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:bool decideWhetherToUseGpuForHalo(bool                 havePPDomainDecomposition,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:                                  bool                 useGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:                                  bool                 canUseDirectGpuComm,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    if (!canUseDirectGpuComm || !havePPDomainDecomposition || !useGpuForNonbonded)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp.preplumed:    errorReasons.startContext("GPU halo exchange will not be activated because:");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed: * \brief Declares functionality for deciding whether tasks will run on GPUs.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed:#ifndef GMX_TASKASSIGNMENT_DECIDEGPUUSAGE_H
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed:#define GMX_TASKASSIGNMENT_DECIDEGPUUSAGE_H
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed:    Gpu
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed://! Help pass GPU-emulation parameters with type safety.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed:enum class EmulateGpuNonbonded : bool
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed:    //! Do not emulate GPUs.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed:    //! Do emulate GPUs.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed:    bool enableGpuBufferOps = false;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed:    //! True if the GPU-aware MPI can be used for GPU direct communication feature
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed:    bool canUseGpuAwareMpi = false;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed:    //! True if GPU PME-decomposition is enabled
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed:    bool enableGpuPmeDecomposition = false;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed:    //! True if CUDA Graphs are enabled
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed:    bool enableCudaGraphs = false;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed: * nonbonded tasks on GPUs.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed: * The number of GPU tasks and devices influences both the choice of
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed: * \param[in] userGpuTaskAssignment        The user-specified assignment of GPU tasks to device IDs.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed: * \param[in] emulateGpuNonbonded          Whether we will emulate GPU calculation of nonbonded
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed: * \param[in] buildSupportsNonbondedOnGpu  Whether GROMACS was built with GPU support.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed: * \param[in] nonbondedOnGpuIsUseful       Whether computing nonbonded interactions on a GPU is
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed: * \returns    Whether the simulation will run nonbonded tasks on GPUs.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed:bool decideWhetherToUseGpusForNonbondedWithThreadMpi(TaskTarget              nonbondedTarget,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed:                                                     const std::vector<int>& userGpuTaskAssignment,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed:                                                     EmulateGpuNonbonded     emulateGpuNonbonded,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed:                                                     bool buildSupportsNonbondedOnGpu,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed:                                                     bool nonbondedOnGpuIsUseful,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed: * PME tasks on GPUs.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed: * The number of GPU tasks and devices influences both the choice of
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed: * \param[in]  useGpuForNonbonded        Whether GPUs will be used for nonbonded interactions.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed: * \param[in]  numDevicesToUse           The number of compatible GPUs that the user permitted us to use.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed: * \param[in]  userGpuTaskAssignment     The user-specified assignment of GPU tasks to device IDs.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed: * \returns    Whether the simulation will run PME tasks on GPUs.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed:bool decideWhetherToUseGpusForPmeWithThreadMpi(bool                    useGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed:                                               const std::vector<int>& userGpuTaskAssignment,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed: * tasks on GPUs.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed: * is known. But we need to know if nonbonded will run on GPUs for
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed: * user requires GPUs for the tasks of that duty, then it will be an
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed: * decideWhetherToUseGpusForNonbondedWithThreadMpi() and
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed: * decideWhetherToUseGpusForPmeWithThreadMpi() to help determine
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed: * \param[in]  userGpuTaskAssignment       The user-specified assignment of GPU tasks to device IDs.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed: * \param[in]  emulateGpuNonbonded         Whether we will emulate GPU calculation of nonbonded interactions.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed: * \param[in]  buildSupportsNonbondedOnGpu Whether GROMACS was build with GPU support.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed: * \param[in]  nonbondedOnGpuIsUseful      Whether computing nonbonded interactions on a GPU is useful for this calculation.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed: * \param[in]  gpusWereDetected            Whether compatible GPUs were detected on any node.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed: * \returns    Whether the simulation will run nonbonded and PME tasks, respectively, on GPUs.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed:bool decideWhetherToUseGpusForNonbonded(TaskTarget              nonbondedTarget,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed:                                        const std::vector<int>& userGpuTaskAssignment,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed:                                        EmulateGpuNonbonded     emulateGpuNonbonded,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed:                                        bool                    buildSupportsNonbondedOnGpu,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed:                                        bool                    nonbondedOnGpuIsUseful,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed:                                        bool                    gpusWereDetected);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed: * different types on GPUs.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed: * is known. But we need to know if nonbonded will run on GPUs for
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed: * user requires GPUs for the tasks of that duty, then it will be an
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed: * decideWhetherToUseGpusForNonbondedWithThreadMpi() and
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed: * decideWhetherToUseGpusForPmeWithThreadMpi() to help determine
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed: * \param[in]  useGpuForNonbonded        Whether GPUs will be used for nonbonded interactions.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed: * \param[in]  userGpuTaskAssignment     The user-specified assignment of GPU tasks to device IDs.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed: * \param[in]  gpusWereDetected          Whether compatible GPUs were detected on any node.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed: * \returns    Whether the simulation will run nonbonded and PME tasks, respectively, on GPUs.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed:bool decideWhetherToUseGpusForPme(bool                    useGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed:                                  const std::vector<int>& userGpuTaskAssignment,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed:                                  bool                    gpusWereDetected);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed: * Given the PME task assignment in \p useGpuForPme and the user-provided
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed: * \note Aborts the run upon incompatible values of \p useGpuForPme and \p pmeFftTarget.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed: * \param[in]  useGpuForPme              PME task assignment, true if PME task is mapped to the GPU.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed:PmeRunMode determinePmeRunMode(bool useGpuForPme, const TaskTarget& pmeFftTarget, const t_inputrec& inputrec);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed:/*! \brief Decide whether the simulation will try to run bonded tasks on GPUs.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed: * \param[in]  useGpuForNonbonded        Whether GPUs will be used for nonbonded interactions.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed: * \param[in]  useGpuForPme              Whether GPUs will be used for PME interactions.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed: * \param[in]  gpusWereDetected          Whether compatible GPUs were detected on any node.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed: * \returns    Whether the simulation will run bondeded tasks on GPUs.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed:bool decideWhetherToUseGpusForBonded(bool              useGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed:                                     bool              useGpuForPme,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed:                                     bool              gpusWereDetected);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed:/*! \brief Decide whether to use GPU for update.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed: * \param[in]  pmeRunMode                   PME running mode: CPU, GPU or mixed.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed: * \param[in]  useGpuForNonbonded           Whether GPUs will be used for nonbonded interactions.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed: * \param[in]  updateTarget                 User choice for running simulation on GPU.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed: * \param[in]  gpusWereDetected             Whether compatible GPUs were detected on any node.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed: * \returns    Whether complete simulation can be run on GPU.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed:bool decideWhetherToUseGpuForUpdate(bool                 isDomainDecomposition,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed:                                    bool                 useGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed:                                    bool                 gpusWereDetected,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed:/*! \brief Decide whether direct GPU communication can be used.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed: * Takes into account the build type which determines feature support as well as GPU
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed: * development feature flags, determines whether this run can use direct GPU communication.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed: * \param[in]  devFlags                     GPU development / experimental feature flags.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed: * \returns    Whether the MPI-parallel runs can use direct GPU communication.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed:bool decideWhetherDirectGpuCommunicationCanBeUsed(const DevelopmentFeatureFlags& devFlags,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed:/*! \brief Decide whether to use GPU for halo exchange.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed: * \param[in]  useGpuForNonbonded           Whether GPUs will be used for nonbonded interactions.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed: * \param[in]  canUseDirectGpuComm          Whether direct GPU communication can be used.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed: * \returns    Whether halo exchange can be run on GPU.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed:bool decideWhetherToUseGpuForHalo(bool                 havePPDomainDecomposition,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed:                                  bool                 useGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h.preplumed:                                  bool                 canUseDirectGpuComm,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \brief Declares functionality for deciding whether tasks will run on GPUs.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:#ifndef GMX_TASKASSIGNMENT_DECIDEGPUUSAGE_H
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:#define GMX_TASKASSIGNMENT_DECIDEGPUUSAGE_H
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:    Gpu
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h://! Help pass GPU-emulation parameters with type safety.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:enum class EmulateGpuNonbonded : bool
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:    //! Do not emulate GPUs.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:    //! Do emulate GPUs.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:    bool enableGpuBufferOps = false;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:    //! True if the GPU-aware MPI can be used for GPU direct communication feature
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:    bool canUseGpuAwareMpi = false;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:    //! True if GPU PME-decomposition is enabled
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:    bool enableGpuPmeDecomposition = false;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:    //! True if CUDA Graphs are enabled
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:    bool enableCudaGraphs = false;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * nonbonded tasks on GPUs.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * The number of GPU tasks and devices influences both the choice of
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \param[in] userGpuTaskAssignment        The user-specified assignment of GPU tasks to device IDs.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \param[in] emulateGpuNonbonded          Whether we will emulate GPU calculation of nonbonded
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \param[in] buildSupportsNonbondedOnGpu  Whether GROMACS was built with GPU support.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \param[in] nonbondedOnGpuIsUseful       Whether computing nonbonded interactions on a GPU is
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \returns    Whether the simulation will run nonbonded tasks on GPUs.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:bool decideWhetherToUseGpusForNonbondedWithThreadMpi(TaskTarget              nonbondedTarget,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:                                                     const std::vector<int>& userGpuTaskAssignment,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:                                                     EmulateGpuNonbonded     emulateGpuNonbonded,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:                                                     bool buildSupportsNonbondedOnGpu,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:                                                     bool nonbondedOnGpuIsUseful,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * PME tasks on GPUs.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * The number of GPU tasks and devices influences both the choice of
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \param[in]  useGpuForNonbonded        Whether GPUs will be used for nonbonded interactions.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \param[in]  numDevicesToUse           The number of compatible GPUs that the user permitted us to use.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \param[in]  userGpuTaskAssignment     The user-specified assignment of GPU tasks to device IDs.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \returns    Whether the simulation will run PME tasks on GPUs.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:bool decideWhetherToUseGpusForPmeWithThreadMpi(bool                    useGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:                                               const std::vector<int>& userGpuTaskAssignment,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * tasks on GPUs.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * is known. But we need to know if nonbonded will run on GPUs for
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * user requires GPUs for the tasks of that duty, then it will be an
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * decideWhetherToUseGpusForNonbondedWithThreadMpi() and
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * decideWhetherToUseGpusForPmeWithThreadMpi() to help determine
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \param[in]  userGpuTaskAssignment       The user-specified assignment of GPU tasks to device IDs.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \param[in]  emulateGpuNonbonded         Whether we will emulate GPU calculation of nonbonded interactions.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \param[in]  buildSupportsNonbondedOnGpu Whether GROMACS was build with GPU support.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \param[in]  nonbondedOnGpuIsUseful      Whether computing nonbonded interactions on a GPU is useful for this calculation.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \param[in]  gpusWereDetected            Whether compatible GPUs were detected on any node.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \returns    Whether the simulation will run nonbonded and PME tasks, respectively, on GPUs.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:bool decideWhetherToUseGpusForNonbonded(TaskTarget              nonbondedTarget,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:                                        const std::vector<int>& userGpuTaskAssignment,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:                                        EmulateGpuNonbonded     emulateGpuNonbonded,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:                                        bool                    buildSupportsNonbondedOnGpu,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:                                        bool                    nonbondedOnGpuIsUseful,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:                                        bool                    gpusWereDetected);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * different types on GPUs.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * is known. But we need to know if nonbonded will run on GPUs for
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * user requires GPUs for the tasks of that duty, then it will be an
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * decideWhetherToUseGpusForNonbondedWithThreadMpi() and
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * decideWhetherToUseGpusForPmeWithThreadMpi() to help determine
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \param[in]  useGpuForNonbonded        Whether GPUs will be used for nonbonded interactions.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \param[in]  userGpuTaskAssignment     The user-specified assignment of GPU tasks to device IDs.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \param[in]  gpusWereDetected          Whether compatible GPUs were detected on any node.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \returns    Whether the simulation will run nonbonded and PME tasks, respectively, on GPUs.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:bool decideWhetherToUseGpusForPme(bool                    useGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:                                  const std::vector<int>& userGpuTaskAssignment,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:                                  bool                    gpusWereDetected);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * Given the PME task assignment in \p useGpuForPme and the user-provided
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \note Aborts the run upon incompatible values of \p useGpuForPme and \p pmeFftTarget.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \param[in]  useGpuForPme              PME task assignment, true if PME task is mapped to the GPU.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:PmeRunMode determinePmeRunMode(bool useGpuForPme, const TaskTarget& pmeFftTarget, const t_inputrec& inputrec);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:/*! \brief Decide whether the simulation will try to run bonded tasks on GPUs.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \param[in]  useGpuForNonbonded        Whether GPUs will be used for nonbonded interactions.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \param[in]  useGpuForPme              Whether GPUs will be used for PME interactions.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \param[in]  gpusWereDetected          Whether compatible GPUs were detected on any node.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \returns    Whether the simulation will run bondeded tasks on GPUs.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:bool decideWhetherToUseGpusForBonded(bool              useGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:                                     bool              useGpuForPme,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:                                     bool              gpusWereDetected);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:/*! \brief Decide whether to use GPU for update.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \param[in]  pmeRunMode                   PME running mode: CPU, GPU or mixed.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \param[in]  useGpuForNonbonded           Whether GPUs will be used for nonbonded interactions.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \param[in]  updateTarget                 User choice for running simulation on GPU.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \param[in]  gpusWereDetected             Whether compatible GPUs were detected on any node.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \returns    Whether complete simulation can be run on GPU.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:bool decideWhetherToUseGpuForUpdate(bool                 isDomainDecomposition,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:                                    bool                 useGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:                                    bool                 gpusWereDetected,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:/*! \brief Decide whether direct GPU communication can be used.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * Takes into account the build type which determines feature support as well as GPU
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * development feature flags, determines whether this run can use direct GPU communication.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \param[in]  devFlags                     GPU development / experimental feature flags.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \returns    Whether the MPI-parallel runs can use direct GPU communication.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:bool decideWhetherDirectGpuCommunicationCanBeUsed(const DevelopmentFeatureFlags& devFlags,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:/*! \brief Decide whether to use GPU for halo exchange.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \param[in]  useGpuForNonbonded           Whether GPUs will be used for nonbonded interactions.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \param[in]  canUseDirectGpuComm          Whether direct GPU communication can be used.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h: * \returns    Whether halo exchange can be run on GPU.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:bool decideWhetherToUseGpuForHalo(bool                 havePPDomainDecomposition,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:                                  bool                 useGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/include/gromacs/taskassignment/decidegpuusage.h:                                  bool                 canUseDirectGpuComm,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp: * \brief Defines functionality for deciding whether tasks will run on GPUs.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:#include "gromacs/taskassignment/decidegpuusage.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:#include "gromacs/listed_forces/listed_forces_gpu.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:#include "gromacs/mdlib/update_constrain_gpu.h"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:        "When you use mdrun -gputasks, %s must be set to non-default "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:#if GMX_GPU
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:        " If you simply want to restrict which GPUs are used, then it is "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:        "better to use mdrun -gpu_id. Otherwise, setting the "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:#    if GMX_GPU_CUDA
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:        "CUDA_VISIBLE_DEVICES"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:#    elif GMX_GPU_OPENCL
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:        // OpenCL standard, but the only current relevant case for GROMACS
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:        // is AMD OpenCL, which offers this variable.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:        "GPU_DEVICE_ORDINAL"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:#    elif GMX_GPU_SYCL && GMX_SYCL_DPCPP
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:#    elif GMX_GPU_SYCL && GMX_SYCL_HIPSYCL
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:        // Not true if we use hipSYCL over CUDA or IntelLLVM, but in that case the user probably
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:        // https://rocmdocs.amd.com/en/latest/Other_Solutions/Other-Solutions.html#hip-environment-variables
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:constexpr bool c_gpuBuildSyclWithoutGpuFft =
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:        (GMX_GPU_SYCL != 0) && (GMX_GPU_FFT_MKL == 0) && (GMX_GPU_FFT_ROCFFT == 0)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:        && (GMX_GPU_FFT_VKFFT == 0) && (GMX_GPU_FFT_DBFFT == 0); // NOLINT(misc-redundant-expression)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:bool decideWhetherToUseGpusForNonbondedWithThreadMpi(const TaskTarget        nonbondedTarget,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:                                                     const std::vector<int>& userGpuTaskAssignment,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:                                                     const EmulateGpuNonbonded emulateGpuNonbonded,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:                                                     const bool buildSupportsNonbondedOnGpu,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:                                                     const bool nonbondedOnGpuIsUseful,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    // First, exclude all cases where we can't run NB on GPUs.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    if (nonbondedTarget == TaskTarget::Cpu || emulateGpuNonbonded == EmulateGpuNonbonded::Yes
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:        || !nonbondedOnGpuIsUseful || !buildSupportsNonbondedOnGpu)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:        // If the user required NB on GPUs, we issue an error later.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    // We now know that NB on GPUs makes sense, if we have any.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    if (!userGpuTaskAssignment.empty())
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:        // Specifying -gputasks requires specifying everything.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    if (nonbondedTarget == TaskTarget::Gpu)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    // Because this is thread-MPI, we already know about the GPUs that
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    // If we get here, then the user permitted or required GPUs.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:static bool decideWhetherToUseGpusForPmeFft(const TaskTarget pmeFftTarget)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:                           || (pmeFftTarget == TaskTarget::Auto && c_gpuBuildSyclWithoutGpuFft);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:static bool canUseGpusForPme(const bool        useGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    errorReasons.startContext("Cannot compute PME interactions on a GPU, because:");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    errorReasons.appendIf(!useGpuForNonbonded, "Nonbonded interactions must also run on GPUs.");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    errorReasons.appendIf(!pme_gpu_supports_build(&tempString), tempString);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    errorReasons.appendIf(!pme_gpu_supports_input(inputrec, &tempString), tempString);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    if (!decideWhetherToUseGpusForPmeFft(pmeFftTarget))
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:        errorReasons.appendIf(!pme_gpu_mixed_mode_supports_input(inputrec, &tempString), tempString);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:        if (pmeTarget == TaskTarget::Gpu && errorMessage != nullptr)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:bool decideWhetherToUseGpusForPmeWithThreadMpi(const bool              useGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:                                               const std::vector<int>& userGpuTaskAssignment,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    // First, exclude all cases where we can't run PME on GPUs.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    if (!canUseGpusForPme(useGpuForNonbonded, pmeTarget, pmeFftTarget, inputrec, nullptr))
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:        // PME can't run on a GPU. If the user required that, we issue an error later.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    // We now know that PME on GPUs might make sense, if we have any.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    if (pmeTarget == TaskTarget::Gpu)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:                    "PME tasks were required to run on GPUs with multiple ranks "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    if (!userGpuTaskAssignment.empty())
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:        // Follow the user's choice of GPU task assignment, if we
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:        // can. Checking that their IDs are for compatible GPUs comes
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:        // Specifying -gputasks requires specifying everything.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:        // PME on GPUs is only supported in a single case
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:        if (pmeTarget == TaskTarget::Gpu)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:                        "When you run mdrun -pme gpu -gputasks, you must supply a PME-enabled .tpr "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    // Because this is thread-MPI, we already know about the GPUs that
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    if (pmeTarget == TaskTarget::Gpu)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:                    "PME tasks were required to run on GPUs, but that is not implemented with "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:        // PME can run well on a GPU shared with NB, and we permit
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:        // We have a single separate PME rank, that can use a GPU
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:        // run well on a GPU shared with NB, and we permit mdrun to
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:        // default to it if there is only one GPU available.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    // Not enough support for PME on GPUs for anything else
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:bool decideWhetherToUseGpusForNonbonded(const TaskTarget          nonbondedTarget,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:                                        const std::vector<int>&   userGpuTaskAssignment,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:                                        const EmulateGpuNonbonded emulateGpuNonbonded,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:                                        const bool                buildSupportsNonbondedOnGpu,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:                                        const bool                nonbondedOnGpuIsUseful,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:                                        const bool                gpusWereDetected)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:        if (!userGpuTaskAssignment.empty())
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:                    "A GPU task assignment was specified, but nonbonded interactions were "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    if (!buildSupportsNonbondedOnGpu && nonbondedTarget == TaskTarget::Gpu)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:                "Nonbonded interactions on the GPU were requested with -nb gpu, "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:                "but the GROMACS binary has been built without GPU support. "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:                "Either run without selecting GPU options, or recompile GROMACS "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:                "with GPU support enabled"));
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    // TODO refactor all these TaskTarget::Gpu checks into one place?
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    if (emulateGpuNonbonded == EmulateGpuNonbonded::Yes)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:        if (nonbondedTarget == TaskTarget::Gpu)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:                    "Nonbonded interactions on the GPU were required, which is inconsistent "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:        if (!userGpuTaskAssignment.empty())
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:                    InconsistentInputError("GPU ID usage was specified, as was GPU emulation. Make "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    if (!nonbondedOnGpuIsUseful)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:        if (nonbondedTarget == TaskTarget::Gpu)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:                    "Nonbonded interactions on the GPU were required, but not supported for these "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:                    "simulation settings. Change your settings, or do not require using GPUs."));
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    if (!userGpuTaskAssignment.empty())
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:        // Specifying -gputasks requires specifying everything.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    if (nonbondedTarget == TaskTarget::Gpu)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:        // We still don't know whether it is an error if no GPUs are found
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:        // GPUs.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    // If we get here, then the user permitted GPUs, which we should
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    return buildSupportsNonbondedOnGpu && gpusWereDetected;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:bool decideWhetherToUseGpusForPme(const bool              useGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:                                  const std::vector<int>& userGpuTaskAssignment,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:                                  const bool              gpusWereDetected)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    if (!canUseGpusForPme(useGpuForNonbonded, pmeTarget, pmeFftTarget, inputrec, &message))
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:        if (!userGpuTaskAssignment.empty())
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:                    "A GPU task assignment was specified, but PME interactions were "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    if (pmeTarget == TaskTarget::Gpu)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:                    "PME tasks were required to run on GPUs with multiple ranks "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    if (!userGpuTaskAssignment.empty())
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:        // Specifying -gputasks requires specifying everything.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    // We still don't know whether it is an error if no GPUs are found
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    // GPUs.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    if (pmeTarget == TaskTarget::Gpu)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    // If we get here, then the user permitted GPUs.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:        // PME can run well on a single GPU shared with NB when there
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:        // detected GPUs.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:        return gpusWereDetected;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:        // We have a single separate PME rank, that can use a GPU
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:        return gpusWereDetected;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    // Not enough support for PME on GPUs for anything else
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:PmeRunMode determinePmeRunMode(const bool useGpuForPme, const TaskTarget& pmeFftTarget, const t_inputrec& inputrec)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    if (useGpuForPme)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:        if (c_gpuBuildSyclWithoutGpuFft && pmeFftTarget == TaskTarget::Gpu)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:                      "GROMACS is built without SYCL GPU FFT library. Please do not use -pmefft "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:                      "gpu.");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:        if (!decideWhetherToUseGpusForPmeFft(pmeFftTarget))
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:            return PmeRunMode::GPU;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:        if (pmeFftTarget == TaskTarget::Gpu)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:                      "Assigning FFTs to GPU requires PME to be assigned to GPU as well. With PME "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:bool decideWhetherToUseGpusForBonded(bool              useGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:                                     bool              useGpuForPme,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:                                     bool              gpusWereDetected)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    if (!buildSupportsListedForcesGpu(&errorMessage))
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:        if (bondedTarget == TaskTarget::Gpu)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    if (!inputSupportsListedForcesGpu(inputrec, mtop, &errorMessage))
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:        if (bondedTarget == TaskTarget::Gpu)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    if (!useGpuForNonbonded)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:        if (bondedTarget == TaskTarget::Gpu)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:                    "Bonded interactions on the GPU were required, but this requires that "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:                    "short-ranged non-bonded interactions are also run on the GPU. Change "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:                    "your settings, or do not require using GPUs."));
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    if (bondedTarget == TaskTarget::Gpu)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:        // We still don't know whether it is an error if no GPUs are
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    // If we get here, then the user permitted GPUs, which we should
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    // choose separate PME ranks when nonBonded are assigned to the GPU.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:                                     || (usingPmeOrEwald(inputrec.coulombtype) && !useGpuForPme
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    return gpusWereDetected && usingOurCpuForPmeOrEwald;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:bool decideWhetherToUseGpuForUpdate(const bool           isDomainDecomposition,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:                                    const bool           useGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:                                    const bool           gpusWereDetected,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:            errorMessage += "With separate PME rank(s), PME must run on the GPU.\n";
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    // Using the GPU-version of update if:
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    // 1. PME is on the GPU (there should be a copy of coordinates on GPU for PME spread) or inactive, or
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    // 2. Non-bonded interactions are on the GPU.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    if ((pmeRunMode == PmeRunMode::CPU || pmeRunMode == PmeRunMode::None) && !useGpuForNonbonded)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:                "Either PME or short-ranged non-bonded interaction tasks must run on the GPU.\n";
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    if (!gpusWereDetected)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:        errorMessage += "Compatible GPUs must have been found.\n";
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    if (!(GMX_GPU_CUDA || GMX_GPU_SYCL))
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:        errorMessage += "Only CUDA and SYCL builds are supported.\n";
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    // does not support it, the actual CUDA LINCS code does support it
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    if (!UpdateConstrainGpu::isNumCoupledConstraintsSupported(mtop))
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:                "The number of coupled constraints is higher than supported in the GPU LINCS "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    if (hasAnyConstraints && !UpdateConstrainGpu::areConstraintsSupported())
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:        errorMessage += "Chosen GPU implementation does not support constraints.\n";
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:        // There is a known bug with frozen atoms and GPU update, see Issue #3920.
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:                            "Update task can not run on the GPU, because the following "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:        else if (updateTarget == TaskTarget::Gpu)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:                    "Update task on the GPU was required,\n"
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    return (updateTarget == TaskTarget::Gpu
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:bool decideWhetherDirectGpuCommunicationCanBeUsed(const DevelopmentFeatureFlags& devFlags,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    const bool buildSupportsDirectGpuComm = (GMX_GPU_CUDA || GMX_GPU_SYCL) && GMX_MPI;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    if (!buildSupportsDirectGpuComm)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    // Direct GPU communication is presently turned off due to insufficient testing
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    const bool enableDirectGpuComm = (getenv("GMX_ENABLE_DIRECT_GPU_COMM") != nullptr)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:                                     || (getenv("GMX_GPU_DD_COMMS") != nullptr)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:                                     || (getenv("GMX_GPU_PME_PP_COMMS") != nullptr);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    if (GMX_THREAD_MPI && GMX_GPU_SYCL && enableDirectGpuComm)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:                        "GMX_ENABLE_DIRECT_GPU_COMM environment variable detected, "
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    errorReasons.startContext("GPU direct communication can not be activated because:");
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    bool runAndGpuSupportDirectGpuComm = (runUsesCompatibleFeatures && enableDirectGpuComm);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    bool canUseDirectGpuCommWithThreadMpi =
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:            (runAndGpuSupportDirectGpuComm && GMX_THREAD_MPI && !GMX_GPU_SYCL);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    // GPU-aware MPI case off by default, can be enabled with dev flag
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    // Note: GMX_DISABLE_DIRECT_GPU_COMM already taken into account in devFlags.enableDirectGpuCommWithMpi
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    bool canUseDirectGpuCommWithMpi = (runAndGpuSupportDirectGpuComm && GMX_LIB_MPI
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:                                       && devFlags.canUseGpuAwareMpi && enableDirectGpuComm);
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    return canUseDirectGpuCommWithThreadMpi || canUseDirectGpuCommWithMpi;
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:bool decideWhetherToUseGpuForHalo(bool                 havePPDomainDecomposition,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:                                  bool                 useGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:                                  bool                 canUseDirectGpuComm,
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    if (!canUseDirectGpuComm || !havePPDomainDecomposition || !useGpuForNonbonded)
v1.2/plumed2/patches/gromacs-2023.diff/src/gromacs/taskassignment/decidegpuusage.cpp:    errorReasons.startContext("GPU halo exchange will not be activated because:");
v1.2/plumed2/patches/namd-2.13.diff: 	$(CUDALIB) \
v1.2/plumed2/patches/namd-2.13.diff: #include "DeviceCUDA.h"
v1.2/plumed2/patches/namd-2.13.diff: #ifdef NAMD_CUDA
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:#include "gromacs/domdec/gpuhaloexchange.h"
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:#include "gromacs/ewald/pme_pp_comm_gpu.h"
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:#include "gromacs/gpu_utils/gpu_utils.h"
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:#include "gromacs/listed_forces/gpubonded.h"
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:#include "gromacs/mdtypes/state_propagator_data_gpu.h"
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:#include "gromacs/nbnxm/gpu_data_mgmt.h"
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:#include "gromacs/nbnxm/nbnxm_gpu.h"
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:#include "gromacs/timing/gpu_timing.h"
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:#include "gpuforcereduction.h"
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:static const bool c_disableAlternatingWait = (getenv("GMX_DISABLE_ALTERNATING_GPU_WAIT") != nullptr);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                   bool                  useGpuPmePpComms,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                   bool                  receivePmeForceToGpu,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    gmx_pme_receive_f(fr->pmePpCommGpu.get(), cr, forceWithVirial, &e_q, &e_lj, &dvdl_q, &dvdl_lj,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                      useGpuPmePpComms, receivePmeForceToGpu, &cycles_seppme);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    /* GPU kernel launch overhead is already timed separately */
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (!nbv->useGpu())
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:/*! \brief Launch the prepare_step and spread stages of PME GPU.
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:static inline void launchPmeGpuSpread(gmx_pme_t*            pmedata,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                      GpuEventSynchronizer* xReadyOnDevice,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    pme_gpu_prepare_computation(pmedata, box, wcycle, stepWork);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    pme_gpu_launch_spread(pmedata, xReadyOnDevice, wcycle, lambdaQ);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:/*! \brief Launch the FFT and gather stages of PME GPU
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:static void launchPmeGpuFftAndGather(gmx_pme_t*               pmedata,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    pme_gpu_launch_complex_transforms(pmedata, wcycle, stepWork);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    pme_gpu_launch_gather(pmedata, wcycle, lambdaQ);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed: *  Polling wait for either of the PME or nonbonded GPU tasks.
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed: * Instead of a static order in waiting for GPU tasks, this function
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed: * one of the reductions, regardless of the GPU task completion order.
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:static void alternatePmeNbGpuWaitReduce(nonbonded_verlet_t* nbv,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    bool isPmeGpuDone = false;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    bool isNbGpuDone  = false;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    gmx::ArrayRef<const gmx::RVec> pmeGpuForces;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    while (!isPmeGpuDone || !isNbGpuDone)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        if (!isPmeGpuDone)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            GpuTaskCompletion completionType =
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                    (isNbGpuDone) ? GpuTaskCompletion::Wait : GpuTaskCompletion::Check;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            isPmeGpuDone = pme_gpu_try_finish_task(pmedata, stepWork, wcycle,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        if (!isNbGpuDone)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            GpuTaskCompletion completionType =
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                    (isPmeGpuDone) ? GpuTaskCompletion::Wait : GpuTaskCompletion::Check;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            isNbGpuDone = Nbnxm::gpu_try_finish_task(
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                    nbv->gpu_nbv, stepWork, AtomLocality::Local, enerd->grpp.ener[egLJSR].data(),
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            if (isNbGpuDone)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    domainWork.haveGpuBondedWork = ((fr.gpuBonded != nullptr) && fr.gpuBonded->haveInteractions());
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (simulationWork.useGpuBufferOps)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        GMX_ASSERT(simulationWork.useGpuNonbonded,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    flags.useGpuXBufferOps = simulationWork.useGpuBufferOps;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    flags.useGpuFBufferOps = simulationWork.useGpuBufferOps && !flags.computeVirial;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    flags.useGpuPmeFReduction = flags.computeSlowForces && flags.useGpuFBufferOps && simulationWork.useGpuPme
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                && (rankHasPmeDuty || simulationWork.useGpuPmePpCommunication);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    flags.useGpuXHalo = simulationWork.useGpuHaloExchange;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    flags.useGpuFHalo = simulationWork.useGpuHaloExchange && flags.useGpuFBufferOps;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:/* \brief Launch end-of-step GPU tasks: buffer clearing and rolling pruning.
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed: * TODO: eliminate \p useGpuPmeOnThisRank when this is
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:static void launchGpuEndOfStepTasks(nonbonded_verlet_t*               nbv,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                    gmx::GpuBonded*                   gpuBonded,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                    bool                              useGpuPmeOnThisRank,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (runScheduleWork.simulationWork.useGpuNonbonded && runScheduleWork.stepWork.computeNonbondedForces)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:         * clear kernel launches can leave the GPU idle while it could be running
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        if (nbv->isDynamicPruningStepGpu(step))
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            nbv->dispatchPruneKernelGpu(step);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        /* now clear the GPU outputs while we finish the step on the CPU */
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        wallcycle_start_nocount(wcycle, ewcLAUNCH_GPU);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        wallcycle_sub_start_nocount(wcycle, ewcsLAUNCH_GPU_NONBONDED);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        Nbnxm::gpu_clear_outputs(nbv->gpu_nbv, runScheduleWork.stepWork.computeVirial);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        wallcycle_sub_stop(wcycle, ewcsLAUNCH_GPU_NONBONDED);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        wallcycle_stop(wcycle, ewcLAUNCH_GPU);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (useGpuPmeOnThisRank)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        pme_gpu_reinit_computation(pmedata, wcycle);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (runScheduleWork.domainWork.haveGpuBondedWork && runScheduleWork.stepWork.computeEnergy)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        gpuBonded->waitAccumulateEnergyTerms(enerd);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        gpuBonded->clearEnergies();
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:/*! \brief Setup for the local and non-local GPU force reductions:
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:static void setupGpuForceReductions(gmx::MdrunScheduleWorkload* runScheduleWork,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    gmx::StatePropagatorDataGpu* stateGpu = fr->stateGpu;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    // (re-)initialize local GPU force reduction
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    fr->gpuForceReduction[gmx::AtomLocality::Local]->reinit(
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            stateGpu->getForces(), nbv->getNumAtoms(AtomLocality::Local), nbv->getGridIndices(),
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            atomStart, accumulate, stateGpu->fReducedOnDevice());
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    fr->gpuForceReduction[gmx::AtomLocality::Local]->registerNbnxmForce(nbv->getGpuForces());
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (runScheduleWork->simulationWork.useGpuPme
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        && (thisRankHasDuty(cr, DUTY_PME) || runScheduleWork->simulationWork.useGpuPmePpCommunication))
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        void* forcePtr = thisRankHasDuty(cr, DUTY_PME) ? pme_gpu_get_device_f(fr->pmedata)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                                       : // PME force buffer on same GPU
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                 fr->pmePpCommGpu->getGpuForceStagingPtr(); // buffer received from other GPU
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        fr->gpuForceReduction[gmx::AtomLocality::Local]->registerRvecForce(forcePtr);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        GpuEventSynchronizer* const pmeSynchronizer =
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                (thisRankHasDuty(cr, DUTY_PME) ? pme_gpu_get_f_ready_synchronizer(fr->pmedata)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                               : // PME force buffer on same GPU
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                         fr->pmePpCommGpu->getForcesReadySynchronizer()); // buffer received from other GPU
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        fr->gpuForceReduction[gmx::AtomLocality::Local]->addDependency(pmeSynchronizer);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        && !runScheduleWork->simulationWork.useGpuHaloExchange)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        const bool useGpuForceBufferOps = true;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        fr->gpuForceReduction[gmx::AtomLocality::Local]->addDependency(
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                stateGpu->getForcesReadyOnDeviceEvent(forcesReadyLocality, useGpuForceBufferOps));
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (runScheduleWork->simulationWork.useGpuHaloExchange)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        fr->gpuForceReduction[gmx::AtomLocality::Local]->addDependency(
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                cr->dd->gpuHaloExchange[0][0]->getForcesReadyOnDeviceEvent());
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        // (re-)initialize non-local GPU force reduction
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        fr->gpuForceReduction[gmx::AtomLocality::NonLocal]->reinit(
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                stateGpu->getForces(), nbv->getNumAtoms(AtomLocality::NonLocal),
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        fr->gpuForceReduction[gmx::AtomLocality::NonLocal]->registerNbnxmForce(nbv->getGpuForces());
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            fr->gpuForceReduction[gmx::AtomLocality::NonLocal]->addDependency(
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                    stateGpu->getForcesReadyOnDeviceEvent(AtomLocality::NonLocal, true));
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    gmx::StatePropagatorDataGpu* stateGpu = fr->stateGpu;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    const bool useGpuPmeOnThisRank =
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            simulationWork.useGpuPme && thisRankHasDuty(cr, DUTY_PME) && stepWork.computeSlowForces;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    const bool pmeSendCoordinatesFromGpu =
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            GMX_MPI && simulationWork.useGpuPmePpCommunication && !(stepWork.doNeighborSearch);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    const bool reinitGpuPmePpComms =
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            GMX_MPI && simulationWork.useGpuPmePpCommunication && (stepWork.doNeighborSearch);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    const auto localXReadyOnDevice = (useGpuPmeOnThisRank || simulationWork.useGpuBufferOps)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                             ? stateGpu->getCoordinatesReadyOnDeviceEvent(
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    // Copy coordinate from the GPU if update is on the GPU and there
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            !thisRankHasDuty(cr, DUTY_PME) && !simulationWork.useGpuPmePpCommunication;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    GMX_ASSERT(simulationWork.useGpuHaloExchange
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                       == ((cr->dd != nullptr) && (!cr->dd->gpuHaloExchange[0].empty())),
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:               "The GPU halo exchange is active, but it has not been constructed.");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            havePPDomainDecomposition(cr) && !simulationWork.useGpuHaloExchange;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    bool gmx_used_in_debug haveCopiedXFromGpu = false;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (simulationWork.useGpuUpdate && !stepWork.doNeighborSearch
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        stateGpu->copyCoordinatesFromGpu(x.unpaddedArrayRef(), AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        haveCopiedXFromGpu = true;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (GMX_MPI && !thisRankHasDuty(cr, DUTY_PME) && !pmeSendCoordinatesFromGpu && stepWork.computeSlowForces)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        if (!stepWork.doNeighborSearch && simulationWork.useGpuUpdate)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                 step, simulationWork.useGpuPmePpCommunication, reinitGpuPmePpComms,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                 pmeSendCoordinatesFromGpu, localXReadyOnDevice, wcycle);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (useGpuPmeOnThisRank || stepWork.useGpuXBufferOps)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            stateGpu->reinit(mdatoms->homenr,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            if (useGpuPmeOnThisRank)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                // TODO: This should be moved into PME setup function ( pme_gpu_prepare_computation(...) )
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                pme_gpu_set_device_x(fr->pmedata, stateGpu->getCoordinates());
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        if (!simulationWork.useGpuUpdate || stepWork.doNeighborSearch)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            GMX_ASSERT(stateGpu != nullptr, "stateGpu should not be null");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            stateGpu->copyCoordinatesToGpu(x.unpaddedArrayRef(), AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    // If coordinates are to be sent to PME task from GPU memory, perform that send here.
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (!thisRankHasDuty(cr, DUTY_PME) && pmeSendCoordinatesFromGpu)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                 step, simulationWork.useGpuPmePpCommunication, reinitGpuPmePpComms,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                 pmeSendCoordinatesFromGpu, localXReadyOnDevice, wcycle);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (useGpuPmeOnThisRank)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        launchPmeGpuSpread(fr->pmedata, box, stepWork, localXReadyOnDevice, lambda[efptCOUL], wcycle);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        /* initialize the GPU nbnxm atom data and bonded data structures */
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        if (simulationWork.useGpuNonbonded)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            // Note: cycle counting only nononbondeds, gpuBonded counts internally
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            wallcycle_start_nocount(wcycle, ewcLAUNCH_GPU);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            wallcycle_sub_start_nocount(wcycle, ewcsLAUNCH_GPU_NONBONDED);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            Nbnxm::gpu_init_atomdata(nbv->gpu_nbv, nbv->nbat.get());
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            wallcycle_sub_stop(wcycle, ewcsLAUNCH_GPU_NONBONDED);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            wallcycle_stop(wcycle, ewcLAUNCH_GPU);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            if (fr->gpuBonded)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                 * interactions to the GPU, where the grid order is
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                fr->gpuBonded->updateInteractionListsAndDeviceBuffers(
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                        nbv->getGridIndices(), top->idef, Nbnxm::gpu_get_xq(nbv->gpu_nbv),
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                        Nbnxm::gpu_get_f(nbv->gpu_nbv), Nbnxm::gpu_get_fshift(nbv->gpu_nbv));
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        // Need to run after the GPU-offload bonded interaction lists
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        /* Note that with a GPU the launch overhead of the list transfer is not timed separately */
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        nbv->setupGpuShortRangeWork(fr->gpuBonded, InteractionLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        if (stepWork.useGpuXBufferOps)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            nbv->atomdata_init_copy_x_to_nbat_x_gpu();
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        if (simulationWork.useGpuBufferOps)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            setupGpuForceReductions(runScheduleWork, cr, fr);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        if (stepWork.useGpuXBufferOps)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            GMX_ASSERT(stateGpu, "stateGpu should be valid when buffer ops are offloaded");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            nbv->convertCoordinatesGpu(AtomLocality::Local, false, stateGpu->getCoordinates(),
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            if (simulationWork.useGpuUpdate)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                GMX_ASSERT(stateGpu, "need a valid stateGpu object");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                GMX_ASSERT(haveCopiedXFromGpu,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (simulationWork.useGpuNonbonded && (stepWork.computeNonbondedForces || domainWork.haveGpuBondedWork))
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        ddBalanceRegionHandler.openBeforeForceComputationGpu();
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        wallcycle_start(wcycle, ewcLAUNCH_GPU);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        wallcycle_sub_start(wcycle, ewcsLAUNCH_GPU_NONBONDED);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        Nbnxm::gpu_upload_shiftvec(nbv->gpu_nbv, nbv->nbat.get());
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        if (stepWork.doNeighborSearch || !stepWork.useGpuXBufferOps)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            Nbnxm::gpu_copy_xq_to_gpu(nbv->gpu_nbv, nbv->nbat.get(), AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        wallcycle_sub_stop(wcycle, ewcsLAUNCH_GPU_NONBONDED);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        wallcycle_stop(wcycle, ewcLAUNCH_GPU);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        // with X buffer ops offloaded to the GPU on all but the search steps
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        if (domainWork.haveGpuBondedWork && !havePPDomainDecomposition(cr))
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            fr->gpuBonded->setPbcAndlaunchKernel(fr->pbcType, box, fr->bMolPBC, stepWork);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        /* launch local nonbonded work on GPU */
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        wallcycle_start_nocount(wcycle, ewcLAUNCH_GPU);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        wallcycle_sub_start_nocount(wcycle, ewcsLAUNCH_GPU_NONBONDED);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        wallcycle_sub_stop(wcycle, ewcsLAUNCH_GPU_NONBONDED);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        wallcycle_stop(wcycle, ewcLAUNCH_GPU);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (useGpuPmeOnThisRank)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        // In PME GPU and mixed mode we launch FFT / gather after the
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        // X copy/transform to allow overlap as well as after the GPU NB
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        launchPmeGpuFftAndGather(fr->pmedata, lambda[efptCOUL], wcycle, stepWork);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            /* Note that with a GPU the launch overhead of the list transfer is not timed separately */
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            nbv->setupGpuShortRangeWork(fr->gpuBonded, InteractionLocality::NonLocal);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            // TODO refactor this GPU halo exchange re-initialisation
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            // to location in do_md where GPU halo exchange is
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            // constructed at partitioning, after above stateGpu
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            if (simulationWork.useGpuHaloExchange)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                reinitGpuHaloExchange(*cr, stateGpu->getCoordinates(), stateGpu->getForces());
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            if (stepWork.useGpuXHalo)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                communicateGpuHaloCoordinates(*cr, box, localXReadyOnDevice);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                    stateGpu->copyCoordinatesFromGpu(x.unpaddedArrayRef(), AtomLocality::NonLocal);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                if (simulationWork.useGpuUpdate)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                    GMX_ASSERT(haveCopiedXFromGpu,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                    stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            if (stepWork.useGpuXBufferOps)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                if (!useGpuPmeOnThisRank && !stepWork.useGpuXHalo)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                    stateGpu->copyCoordinatesToGpu(x.unpaddedArrayRef(), AtomLocality::NonLocal);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                nbv->convertCoordinatesGpu(AtomLocality::NonLocal, false, stateGpu->getCoordinates(),
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                           stateGpu->getCoordinatesReadyOnDeviceEvent(
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        if (simulationWork.useGpuNonbonded)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            if (stepWork.doNeighborSearch || !stepWork.useGpuXBufferOps)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                wallcycle_start(wcycle, ewcLAUNCH_GPU);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                wallcycle_sub_start(wcycle, ewcsLAUNCH_GPU_NONBONDED);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                Nbnxm::gpu_copy_xq_to_gpu(nbv->gpu_nbv, nbv->nbat.get(), AtomLocality::NonLocal);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                wallcycle_sub_stop(wcycle, ewcsLAUNCH_GPU_NONBONDED);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                wallcycle_stop(wcycle, ewcLAUNCH_GPU);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            if (domainWork.haveGpuBondedWork)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                fr->gpuBonded->setPbcAndlaunchKernel(fr->pbcType, box, fr->bMolPBC, stepWork);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            /* launch non-local nonbonded tasks on GPU */
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            wallcycle_start_nocount(wcycle, ewcLAUNCH_GPU);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            wallcycle_sub_start(wcycle, ewcsLAUNCH_GPU_NONBONDED);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            wallcycle_sub_stop(wcycle, ewcsLAUNCH_GPU_NONBONDED);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            wallcycle_stop(wcycle, ewcLAUNCH_GPU);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (simulationWork.useGpuNonbonded && stepWork.computeNonbondedForces)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        wallcycle_start_nocount(wcycle, ewcLAUNCH_GPU);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        wallcycle_sub_start_nocount(wcycle, ewcsLAUNCH_GPU_NONBONDED);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            Nbnxm::gpu_launch_cpyback(nbv->gpu_nbv, nbv->nbat.get(), stepWork, AtomLocality::NonLocal);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        Nbnxm::gpu_launch_cpyback(nbv->gpu_nbv, nbv->nbat.get(), stepWork, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        wallcycle_sub_stop(wcycle, ewcsLAUNCH_GPU_NONBONDED);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        if (domainWork.haveGpuBondedWork && stepWork.computeEnergy)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            fr->gpuBonded->launchEnergyTransfer();
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        wallcycle_stop(wcycle, ewcLAUNCH_GPU);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        if (simulationWork.useGpuUpdate && !stepWork.doNeighborSearch)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            GMX_ASSERT(haveCopiedXFromGpu,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    // For the rest of the CPU tasks that depend on GPU-update produced coordinates,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if ((simulationWork.useGpuUpdate)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    const bool useOrEmulateGpuNb = simulationWork.useGpuNonbonded || fr->nbv->emulateGpu();
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (!useOrEmulateGpuNb)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:         * Happens here on the CPU both with and without GPU.
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (stepWork.computeNonbondedForces && !useOrEmulateGpuNb)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (stepWork.useGpuXHalo && (domainWork.haveCpuBondedWork || domainWork.haveFreeEnergyWork))
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        stateGpu->waitCoordinatesReadyOnHost(AtomLocality::NonLocal);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:     * GPU we must wait for the PME calculation (dhdl) results to finish before sampling the
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        if (useGpuPmeOnThisRank)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            pme_gpu_wait_and_reduce(fr->pmedata, stepWork, wcycle,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                   simulationWork.useGpuPmePpCommunication,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                                   stepWork.useGpuPmeFReduction, wcycle);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    GMX_ASSERT(!(nonbondedAtMtsLevel1 && stepWork.useGpuFBufferOps),
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:               "The schedule below does not allow for nonbonded MTS with GPU buffer ops");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    GMX_ASSERT(!(nonbondedAtMtsLevel1 && stepWork.useGpuFHalo),
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:               "The schedule below does not allow for nonbonded MTS with GPU halo exchange");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    // Will store the amount of cycles spent waiting for the GPU that
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    float cycles_wait_gpu = 0;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (useOrEmulateGpuNb && stepWork.computeNonbondedForces)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            if (simulationWork.useGpuNonbonded)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                cycles_wait_gpu += Nbnxm::gpu_wait_finish_task(
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                        nbv->gpu_nbv, stepWork, AtomLocality::NonLocal, enerd->grpp.ener[egLJSR].data(),
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            if (stepWork.useGpuFBufferOps)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                // contributions which are a dependency for the GPU force reduction.
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                    stateGpu->copyForcesToGpu(forceOutMtsLevel0.forceWithShiftForces().force(),
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                fr->gpuForceReduction[gmx::AtomLocality::NonLocal]->execute();
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                if (!stepWork.useGpuFHalo)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                    // copy from GPU input for dd_move_f()
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                    stateGpu->copyForcesFromGpu(forceOutMtsLevel0.forceWithShiftForces().force(),
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            if (fr->nbv->emulateGpu() && stepWork.computeVirial)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:             && !(stepWork.computeVirial || simulationWork.useGpuNonbonded || useGpuPmeOnThisRank));
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:         * If we use a GPU this will overlap with GPU work, so in that case
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            if (stepWork.useGpuFHalo)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                    stateGpu->copyForcesToGpu(forceOutMtsLevel0.forceWithShiftForces().force(),
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                communicateGpuHaloForces(*cr, domainWork.haveCpuLocalForceWork);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                if (stepWork.useGpuFBufferOps)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                    stateGpu->waitForcesReadyOnHost(AtomLocality::NonLocal);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    // With both nonbonded and PME offloaded a GPU on the same rank, we use
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    // When running free energy perturbations steered by AWH and calculating PME on GPU,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    bool alternateGpuWait =
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            (!c_disableAlternatingWait && useGpuPmeOnThisRank && simulationWork.useGpuNonbonded
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:             && !DOMAINDECOMP(cr) && !stepWork.useGpuFBufferOps && !needEarlyPmeResults);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (alternateGpuWait)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        alternatePmeNbGpuWaitReduce(fr->nbv.get(), fr->pmedata, forceOutNonbonded,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (!alternateGpuWait && useGpuPmeOnThisRank && !needEarlyPmeResults)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        pme_gpu_wait_and_reduce(fr->pmedata, stepWork, wcycle,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    /* Wait for local GPU NB outputs on the non-alternating wait path */
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (!alternateGpuWait && stepWork.computeNonbondedForces && simulationWork.useGpuNonbonded)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        /* Measured overhead on CUDA and OpenCL with(out) GPU sharing
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        const float gpuWaitApiOverheadMargin = 2e6F; /* cycles */
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        const float waitCycles               = Nbnxm::gpu_wait_finish_task(
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                nbv->gpu_nbv, stepWork, AtomLocality::Local, enerd->grpp.ener[egLJSR].data(),
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            DdBalanceRegionWaitedForGpu waitedForGpu = DdBalanceRegionWaitedForGpu::yes;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            if (stepWork.computeForces && waitCycles <= gpuWaitApiOverheadMargin)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                waitedForGpu = DdBalanceRegionWaitedForGpu::no;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            ddBalanceRegionHandler.closeAfterForceComputationGpu(cycles_wait_gpu, waitedForGpu);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (fr->nbv->emulateGpu())
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    // If on GPU PME-PP comms path, receive forces from PME before GPU buffer ops
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    // When running free energy perturbations steered by AWH and calculating PME on GPU,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (needToReceivePmeResultsFromSeparateRank && simulationWork.useGpuPmePpCommunication && !needEarlyPmeResults)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                               simulationWork.useGpuPmePpCommunication,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                               stepWork.useGpuPmeFReduction, wcycle);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    /* Do the nonbonded GPU (or emulation) force buffer reduction
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    GMX_ASSERT(!(nonbondedAtMtsLevel1 && stepWork.useGpuFBufferOps),
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:               "The schedule below does not allow for nonbonded MTS with GPU buffer ops");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (useOrEmulateGpuNb && !alternateGpuWait)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:        if (stepWork.useGpuFBufferOps)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            // - copy is not perfomed if GPU force halo exchange is active, because it would overwrite the result
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            if (haveLocalForceContribInCpuBuffer && !stepWork.useGpuFHalo)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                // case copyForcesToGpu() uses a separate stream, it allows overlap of
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                // CPU force H2D with GPU force tasks on all streams including those in the
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                stateGpu->copyForcesToGpu(forceWithShift, locality);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                fr->gpuForceReduction[gmx::AtomLocality::Local]->execute();
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:            if (!simulationWork.useGpuUpdate
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                || (simulationWork.useGpuUpdate && DOMAINDECOMP(cr) && haveHostPmePpComms) || vsite)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                stateGpu->copyForcesFromGpu(forceWithShift, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                stateGpu->waitForcesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    launchGpuEndOfStepTasks(nbv, fr->gpuBonded, fr->pmedata, enerd, *runScheduleWork,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                            useGpuPmeOnThisRank, step, wcycle);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    // TODO refactor this and unify with above GPU PME-PP / GPU update path call to the same function
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    // When running free energy perturbations steered by AWH and calculating PME on GPU,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    if (needToReceivePmeResultsFromSeparateRank && !simulationWork.useGpuPmePpCommunication
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:                               simulationWork.useGpuPmePpCommunication, false, wcycle);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp.preplumed:    /* In case we don't have constraints and are using GPUs, the next balancing
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:#include "gromacs/domdec/gpuhaloexchange.h"
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:#include "gromacs/ewald/pme_pp_comm_gpu.h"
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:#include "gromacs/gpu_utils/gpu_utils.h"
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:#include "gromacs/listed_forces/gpubonded.h"
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:#include "gromacs/mdtypes/state_propagator_data_gpu.h"
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:#include "gromacs/nbnxm/gpu_data_mgmt.h"
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:#include "gromacs/nbnxm/nbnxm_gpu.h"
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:#include "gromacs/timing/gpu_timing.h"
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:#include "gpuforcereduction.h"
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:static const bool c_disableAlternatingWait = (getenv("GMX_DISABLE_ALTERNATING_GPU_WAIT") != nullptr);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                                   bool                  useGpuPmePpComms,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                                   bool                  receivePmeForceToGpu,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    gmx_pme_receive_f(fr->pmePpCommGpu.get(), cr, forceWithVirial, &e_q, &e_lj, &dvdl_q, &dvdl_lj,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                      useGpuPmePpComms, receivePmeForceToGpu, &cycles_seppme);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    /* GPU kernel launch overhead is already timed separately */
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    if (!nbv->useGpu())
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:/*! \brief Launch the prepare_step and spread stages of PME GPU.
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:static inline void launchPmeGpuSpread(gmx_pme_t*            pmedata,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                                      GpuEventSynchronizer* xReadyOnDevice,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    pme_gpu_prepare_computation(pmedata, box, wcycle, stepWork);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    pme_gpu_launch_spread(pmedata, xReadyOnDevice, wcycle, lambdaQ);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:/*! \brief Launch the FFT and gather stages of PME GPU
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:static void launchPmeGpuFftAndGather(gmx_pme_t*               pmedata,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    pme_gpu_launch_complex_transforms(pmedata, wcycle, stepWork);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    pme_gpu_launch_gather(pmedata, wcycle, lambdaQ);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp: *  Polling wait for either of the PME or nonbonded GPU tasks.
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp: * Instead of a static order in waiting for GPU tasks, this function
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp: * one of the reductions, regardless of the GPU task completion order.
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:static void alternatePmeNbGpuWaitReduce(nonbonded_verlet_t* nbv,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    bool isPmeGpuDone = false;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    bool isNbGpuDone  = false;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    gmx::ArrayRef<const gmx::RVec> pmeGpuForces;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    while (!isPmeGpuDone || !isNbGpuDone)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        if (!isPmeGpuDone)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            GpuTaskCompletion completionType =
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                    (isNbGpuDone) ? GpuTaskCompletion::Wait : GpuTaskCompletion::Check;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            isPmeGpuDone = pme_gpu_try_finish_task(pmedata, stepWork, wcycle,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        if (!isNbGpuDone)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            GpuTaskCompletion completionType =
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                    (isPmeGpuDone) ? GpuTaskCompletion::Wait : GpuTaskCompletion::Check;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            isNbGpuDone = Nbnxm::gpu_try_finish_task(
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                    nbv->gpu_nbv, stepWork, AtomLocality::Local, enerd->grpp.ener[egLJSR].data(),
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            if (isNbGpuDone)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    domainWork.haveGpuBondedWork = ((fr.gpuBonded != nullptr) && fr.gpuBonded->haveInteractions());
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    if (simulationWork.useGpuBufferOps)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        GMX_ASSERT(simulationWork.useGpuNonbonded,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    flags.useGpuXBufferOps = simulationWork.useGpuBufferOps;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    flags.useGpuFBufferOps = simulationWork.useGpuBufferOps && !flags.computeVirial;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    flags.useGpuPmeFReduction = flags.computeSlowForces && flags.useGpuFBufferOps && simulationWork.useGpuPme
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                                && (rankHasPmeDuty || simulationWork.useGpuPmePpCommunication);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    flags.useGpuXHalo = simulationWork.useGpuHaloExchange;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    flags.useGpuFHalo = simulationWork.useGpuHaloExchange && flags.useGpuFBufferOps;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:/* \brief Launch end-of-step GPU tasks: buffer clearing and rolling pruning.
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp: * TODO: eliminate \p useGpuPmeOnThisRank when this is
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:static void launchGpuEndOfStepTasks(nonbonded_verlet_t*               nbv,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                                    gmx::GpuBonded*                   gpuBonded,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                                    bool                              useGpuPmeOnThisRank,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    if (runScheduleWork.simulationWork.useGpuNonbonded && runScheduleWork.stepWork.computeNonbondedForces)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:         * clear kernel launches can leave the GPU idle while it could be running
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        if (nbv->isDynamicPruningStepGpu(step))
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            nbv->dispatchPruneKernelGpu(step);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        /* now clear the GPU outputs while we finish the step on the CPU */
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        wallcycle_start_nocount(wcycle, ewcLAUNCH_GPU);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        wallcycle_sub_start_nocount(wcycle, ewcsLAUNCH_GPU_NONBONDED);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        Nbnxm::gpu_clear_outputs(nbv->gpu_nbv, runScheduleWork.stepWork.computeVirial);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        wallcycle_sub_stop(wcycle, ewcsLAUNCH_GPU_NONBONDED);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        wallcycle_stop(wcycle, ewcLAUNCH_GPU);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    if (useGpuPmeOnThisRank)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        pme_gpu_reinit_computation(pmedata, wcycle);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    if (runScheduleWork.domainWork.haveGpuBondedWork && runScheduleWork.stepWork.computeEnergy)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        gpuBonded->waitAccumulateEnergyTerms(enerd);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        gpuBonded->clearEnergies();
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:/*! \brief Setup for the local and non-local GPU force reductions:
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:static void setupGpuForceReductions(gmx::MdrunScheduleWorkload* runScheduleWork,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    gmx::StatePropagatorDataGpu* stateGpu = fr->stateGpu;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    // (re-)initialize local GPU force reduction
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    fr->gpuForceReduction[gmx::AtomLocality::Local]->reinit(
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            stateGpu->getForces(), nbv->getNumAtoms(AtomLocality::Local), nbv->getGridIndices(),
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            atomStart, accumulate, stateGpu->fReducedOnDevice());
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    fr->gpuForceReduction[gmx::AtomLocality::Local]->registerNbnxmForce(nbv->getGpuForces());
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    if (runScheduleWork->simulationWork.useGpuPme
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        && (thisRankHasDuty(cr, DUTY_PME) || runScheduleWork->simulationWork.useGpuPmePpCommunication))
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        void* forcePtr = thisRankHasDuty(cr, DUTY_PME) ? pme_gpu_get_device_f(fr->pmedata)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                                                       : // PME force buffer on same GPU
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                                 fr->pmePpCommGpu->getGpuForceStagingPtr(); // buffer received from other GPU
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        fr->gpuForceReduction[gmx::AtomLocality::Local]->registerRvecForce(forcePtr);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        GpuEventSynchronizer* const pmeSynchronizer =
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                (thisRankHasDuty(cr, DUTY_PME) ? pme_gpu_get_f_ready_synchronizer(fr->pmedata)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                                               : // PME force buffer on same GPU
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                         fr->pmePpCommGpu->getForcesReadySynchronizer()); // buffer received from other GPU
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        fr->gpuForceReduction[gmx::AtomLocality::Local]->addDependency(pmeSynchronizer);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        && !runScheduleWork->simulationWork.useGpuHaloExchange)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        const bool useGpuForceBufferOps = true;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        fr->gpuForceReduction[gmx::AtomLocality::Local]->addDependency(
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                stateGpu->getForcesReadyOnDeviceEvent(forcesReadyLocality, useGpuForceBufferOps));
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    if (runScheduleWork->simulationWork.useGpuHaloExchange)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        fr->gpuForceReduction[gmx::AtomLocality::Local]->addDependency(
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                cr->dd->gpuHaloExchange[0][0]->getForcesReadyOnDeviceEvent());
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        // (re-)initialize non-local GPU force reduction
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        fr->gpuForceReduction[gmx::AtomLocality::NonLocal]->reinit(
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                stateGpu->getForces(), nbv->getNumAtoms(AtomLocality::NonLocal),
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        fr->gpuForceReduction[gmx::AtomLocality::NonLocal]->registerNbnxmForce(nbv->getGpuForces());
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            fr->gpuForceReduction[gmx::AtomLocality::NonLocal]->addDependency(
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                    stateGpu->getForcesReadyOnDeviceEvent(AtomLocality::NonLocal, true));
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    gmx::StatePropagatorDataGpu* stateGpu = fr->stateGpu;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    const bool useGpuPmeOnThisRank =
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            simulationWork.useGpuPme && thisRankHasDuty(cr, DUTY_PME) && stepWork.computeSlowForces;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    const bool pmeSendCoordinatesFromGpu =
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            GMX_MPI && simulationWork.useGpuPmePpCommunication && !(stepWork.doNeighborSearch);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    const bool reinitGpuPmePpComms =
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            GMX_MPI && simulationWork.useGpuPmePpCommunication && (stepWork.doNeighborSearch);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    const auto localXReadyOnDevice = (useGpuPmeOnThisRank || simulationWork.useGpuBufferOps)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                                             ? stateGpu->getCoordinatesReadyOnDeviceEvent(
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    // Copy coordinate from the GPU if update is on the GPU and there
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            !thisRankHasDuty(cr, DUTY_PME) && !simulationWork.useGpuPmePpCommunication;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    GMX_ASSERT(simulationWork.useGpuHaloExchange
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                       == ((cr->dd != nullptr) && (!cr->dd->gpuHaloExchange[0].empty())),
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:               "The GPU halo exchange is active, but it has not been constructed.");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            havePPDomainDecomposition(cr) && !simulationWork.useGpuHaloExchange;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    bool gmx_used_in_debug haveCopiedXFromGpu = false;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    if (simulationWork.useGpuUpdate && !stepWork.doNeighborSearch
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        stateGpu->copyCoordinatesFromGpu(x.unpaddedArrayRef(), AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        haveCopiedXFromGpu = true;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    if (GMX_MPI && !thisRankHasDuty(cr, DUTY_PME) && !pmeSendCoordinatesFromGpu && stepWork.computeSlowForces)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        if (!stepWork.doNeighborSearch && simulationWork.useGpuUpdate)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                                 step, simulationWork.useGpuPmePpCommunication, reinitGpuPmePpComms,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                                 pmeSendCoordinatesFromGpu, localXReadyOnDevice, wcycle);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    if (useGpuPmeOnThisRank || stepWork.useGpuXBufferOps)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            stateGpu->reinit(mdatoms->homenr,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            if (useGpuPmeOnThisRank)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                // TODO: This should be moved into PME setup function ( pme_gpu_prepare_computation(...) )
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                pme_gpu_set_device_x(fr->pmedata, stateGpu->getCoordinates());
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        if (!simulationWork.useGpuUpdate || stepWork.doNeighborSearch)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            GMX_ASSERT(stateGpu != nullptr, "stateGpu should not be null");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            stateGpu->copyCoordinatesToGpu(x.unpaddedArrayRef(), AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    // If coordinates are to be sent to PME task from GPU memory, perform that send here.
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    if (!thisRankHasDuty(cr, DUTY_PME) && pmeSendCoordinatesFromGpu)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                                 step, simulationWork.useGpuPmePpCommunication, reinitGpuPmePpComms,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                                 pmeSendCoordinatesFromGpu, localXReadyOnDevice, wcycle);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    if (useGpuPmeOnThisRank)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        launchPmeGpuSpread(fr->pmedata, box, stepWork, localXReadyOnDevice, lambda[efptCOUL], wcycle);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        /* initialize the GPU nbnxm atom data and bonded data structures */
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        if (simulationWork.useGpuNonbonded)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            // Note: cycle counting only nononbondeds, gpuBonded counts internally
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            wallcycle_start_nocount(wcycle, ewcLAUNCH_GPU);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            wallcycle_sub_start_nocount(wcycle, ewcsLAUNCH_GPU_NONBONDED);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            Nbnxm::gpu_init_atomdata(nbv->gpu_nbv, nbv->nbat.get());
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            wallcycle_sub_stop(wcycle, ewcsLAUNCH_GPU_NONBONDED);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            wallcycle_stop(wcycle, ewcLAUNCH_GPU);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            if (fr->gpuBonded)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                 * interactions to the GPU, where the grid order is
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                fr->gpuBonded->updateInteractionListsAndDeviceBuffers(
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                        nbv->getGridIndices(), top->idef, Nbnxm::gpu_get_xq(nbv->gpu_nbv),
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                        Nbnxm::gpu_get_f(nbv->gpu_nbv), Nbnxm::gpu_get_fshift(nbv->gpu_nbv));
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        // Need to run after the GPU-offload bonded interaction lists
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        /* Note that with a GPU the launch overhead of the list transfer is not timed separately */
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        nbv->setupGpuShortRangeWork(fr->gpuBonded, InteractionLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        if (stepWork.useGpuXBufferOps)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            nbv->atomdata_init_copy_x_to_nbat_x_gpu();
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        if (simulationWork.useGpuBufferOps)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            setupGpuForceReductions(runScheduleWork, cr, fr);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        if (stepWork.useGpuXBufferOps)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            GMX_ASSERT(stateGpu, "stateGpu should be valid when buffer ops are offloaded");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            nbv->convertCoordinatesGpu(AtomLocality::Local, false, stateGpu->getCoordinates(),
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            if (simulationWork.useGpuUpdate)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                GMX_ASSERT(stateGpu, "need a valid stateGpu object");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                GMX_ASSERT(haveCopiedXFromGpu,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    if (simulationWork.useGpuNonbonded && (stepWork.computeNonbondedForces || domainWork.haveGpuBondedWork))
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        ddBalanceRegionHandler.openBeforeForceComputationGpu();
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        wallcycle_start(wcycle, ewcLAUNCH_GPU);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        wallcycle_sub_start(wcycle, ewcsLAUNCH_GPU_NONBONDED);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        Nbnxm::gpu_upload_shiftvec(nbv->gpu_nbv, nbv->nbat.get());
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        if (stepWork.doNeighborSearch || !stepWork.useGpuXBufferOps)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            Nbnxm::gpu_copy_xq_to_gpu(nbv->gpu_nbv, nbv->nbat.get(), AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        wallcycle_sub_stop(wcycle, ewcsLAUNCH_GPU_NONBONDED);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        wallcycle_stop(wcycle, ewcLAUNCH_GPU);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        // with X buffer ops offloaded to the GPU on all but the search steps
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        if (domainWork.haveGpuBondedWork && !havePPDomainDecomposition(cr))
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            fr->gpuBonded->setPbcAndlaunchKernel(fr->pbcType, box, fr->bMolPBC, stepWork);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        /* launch local nonbonded work on GPU */
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        wallcycle_start_nocount(wcycle, ewcLAUNCH_GPU);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        wallcycle_sub_start_nocount(wcycle, ewcsLAUNCH_GPU_NONBONDED);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        wallcycle_sub_stop(wcycle, ewcsLAUNCH_GPU_NONBONDED);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        wallcycle_stop(wcycle, ewcLAUNCH_GPU);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    if (useGpuPmeOnThisRank)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        // In PME GPU and mixed mode we launch FFT / gather after the
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        // X copy/transform to allow overlap as well as after the GPU NB
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        launchPmeGpuFftAndGather(fr->pmedata, lambda[efptCOUL], wcycle, stepWork);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            /* Note that with a GPU the launch overhead of the list transfer is not timed separately */
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            nbv->setupGpuShortRangeWork(fr->gpuBonded, InteractionLocality::NonLocal);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            // TODO refactor this GPU halo exchange re-initialisation
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            // to location in do_md where GPU halo exchange is
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            // constructed at partitioning, after above stateGpu
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            if (simulationWork.useGpuHaloExchange)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                reinitGpuHaloExchange(*cr, stateGpu->getCoordinates(), stateGpu->getForces());
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            if (stepWork.useGpuXHalo)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                communicateGpuHaloCoordinates(*cr, box, localXReadyOnDevice);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                    stateGpu->copyCoordinatesFromGpu(x.unpaddedArrayRef(), AtomLocality::NonLocal);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                if (simulationWork.useGpuUpdate)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                    GMX_ASSERT(haveCopiedXFromGpu,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                    stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            if (stepWork.useGpuXBufferOps)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                if (!useGpuPmeOnThisRank && !stepWork.useGpuXHalo)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                    stateGpu->copyCoordinatesToGpu(x.unpaddedArrayRef(), AtomLocality::NonLocal);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                nbv->convertCoordinatesGpu(AtomLocality::NonLocal, false, stateGpu->getCoordinates(),
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                                           stateGpu->getCoordinatesReadyOnDeviceEvent(
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        if (simulationWork.useGpuNonbonded)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            if (stepWork.doNeighborSearch || !stepWork.useGpuXBufferOps)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                wallcycle_start(wcycle, ewcLAUNCH_GPU);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                wallcycle_sub_start(wcycle, ewcsLAUNCH_GPU_NONBONDED);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                Nbnxm::gpu_copy_xq_to_gpu(nbv->gpu_nbv, nbv->nbat.get(), AtomLocality::NonLocal);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                wallcycle_sub_stop(wcycle, ewcsLAUNCH_GPU_NONBONDED);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                wallcycle_stop(wcycle, ewcLAUNCH_GPU);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            if (domainWork.haveGpuBondedWork)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                fr->gpuBonded->setPbcAndlaunchKernel(fr->pbcType, box, fr->bMolPBC, stepWork);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            /* launch non-local nonbonded tasks on GPU */
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            wallcycle_start_nocount(wcycle, ewcLAUNCH_GPU);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            wallcycle_sub_start(wcycle, ewcsLAUNCH_GPU_NONBONDED);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            wallcycle_sub_stop(wcycle, ewcsLAUNCH_GPU_NONBONDED);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            wallcycle_stop(wcycle, ewcLAUNCH_GPU);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    if (simulationWork.useGpuNonbonded && stepWork.computeNonbondedForces)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        wallcycle_start_nocount(wcycle, ewcLAUNCH_GPU);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        wallcycle_sub_start_nocount(wcycle, ewcsLAUNCH_GPU_NONBONDED);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            Nbnxm::gpu_launch_cpyback(nbv->gpu_nbv, nbv->nbat.get(), stepWork, AtomLocality::NonLocal);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        Nbnxm::gpu_launch_cpyback(nbv->gpu_nbv, nbv->nbat.get(), stepWork, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        wallcycle_sub_stop(wcycle, ewcsLAUNCH_GPU_NONBONDED);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        if (domainWork.haveGpuBondedWork && stepWork.computeEnergy)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            fr->gpuBonded->launchEnergyTransfer();
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        wallcycle_stop(wcycle, ewcLAUNCH_GPU);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        if (simulationWork.useGpuUpdate && !stepWork.doNeighborSearch)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            GMX_ASSERT(haveCopiedXFromGpu,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    // For the rest of the CPU tasks that depend on GPU-update produced coordinates,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    if ((simulationWork.useGpuUpdate)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    const bool useOrEmulateGpuNb = simulationWork.useGpuNonbonded || fr->nbv->emulateGpu();
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    if (!useOrEmulateGpuNb)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:         * Happens here on the CPU both with and without GPU.
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    if (stepWork.computeNonbondedForces && !useOrEmulateGpuNb)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    if (stepWork.useGpuXHalo && (domainWork.haveCpuBondedWork || domainWork.haveFreeEnergyWork))
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        stateGpu->waitCoordinatesReadyOnHost(AtomLocality::NonLocal);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:     * GPU we must wait for the PME calculation (dhdl) results to finish before sampling the
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        if (useGpuPmeOnThisRank)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            pme_gpu_wait_and_reduce(fr->pmedata, stepWork, wcycle,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                                   simulationWork.useGpuPmePpCommunication,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                                   stepWork.useGpuPmeFReduction, wcycle);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    GMX_ASSERT(!(nonbondedAtMtsLevel1 && stepWork.useGpuFBufferOps),
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:               "The schedule below does not allow for nonbonded MTS with GPU buffer ops");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    GMX_ASSERT(!(nonbondedAtMtsLevel1 && stepWork.useGpuFHalo),
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:               "The schedule below does not allow for nonbonded MTS with GPU halo exchange");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    // Will store the amount of cycles spent waiting for the GPU that
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    float cycles_wait_gpu = 0;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    if (useOrEmulateGpuNb && stepWork.computeNonbondedForces)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            if (simulationWork.useGpuNonbonded)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                cycles_wait_gpu += Nbnxm::gpu_wait_finish_task(
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                        nbv->gpu_nbv, stepWork, AtomLocality::NonLocal, enerd->grpp.ener[egLJSR].data(),
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            if (stepWork.useGpuFBufferOps)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                // contributions which are a dependency for the GPU force reduction.
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                    stateGpu->copyForcesToGpu(forceOutMtsLevel0.forceWithShiftForces().force(),
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                fr->gpuForceReduction[gmx::AtomLocality::NonLocal]->execute();
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                if (!stepWork.useGpuFHalo)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                    // copy from GPU input for dd_move_f()
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                    stateGpu->copyForcesFromGpu(forceOutMtsLevel0.forceWithShiftForces().force(),
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            if (fr->nbv->emulateGpu() && stepWork.computeVirial)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:             && !(stepWork.computeVirial || simulationWork.useGpuNonbonded || useGpuPmeOnThisRank));
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:         * If we use a GPU this will overlap with GPU work, so in that case
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            if (stepWork.useGpuFHalo)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                    stateGpu->copyForcesToGpu(forceOutMtsLevel0.forceWithShiftForces().force(),
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                communicateGpuHaloForces(*cr, domainWork.haveCpuLocalForceWork);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                if (stepWork.useGpuFBufferOps)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                    stateGpu->waitForcesReadyOnHost(AtomLocality::NonLocal);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    // With both nonbonded and PME offloaded a GPU on the same rank, we use
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    // When running free energy perturbations steered by AWH and calculating PME on GPU,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    bool alternateGpuWait =
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            (!c_disableAlternatingWait && useGpuPmeOnThisRank && simulationWork.useGpuNonbonded
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:             && !DOMAINDECOMP(cr) && !stepWork.useGpuFBufferOps && !needEarlyPmeResults);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    if (alternateGpuWait)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        alternatePmeNbGpuWaitReduce(fr->nbv.get(), fr->pmedata, forceOutNonbonded,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    if (!alternateGpuWait && useGpuPmeOnThisRank && !needEarlyPmeResults)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        pme_gpu_wait_and_reduce(fr->pmedata, stepWork, wcycle,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    /* Wait for local GPU NB outputs on the non-alternating wait path */
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    if (!alternateGpuWait && stepWork.computeNonbondedForces && simulationWork.useGpuNonbonded)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        /* Measured overhead on CUDA and OpenCL with(out) GPU sharing
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        const float gpuWaitApiOverheadMargin = 2e6F; /* cycles */
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        const float waitCycles               = Nbnxm::gpu_wait_finish_task(
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                nbv->gpu_nbv, stepWork, AtomLocality::Local, enerd->grpp.ener[egLJSR].data(),
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            DdBalanceRegionWaitedForGpu waitedForGpu = DdBalanceRegionWaitedForGpu::yes;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            if (stepWork.computeForces && waitCycles <= gpuWaitApiOverheadMargin)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                waitedForGpu = DdBalanceRegionWaitedForGpu::no;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            ddBalanceRegionHandler.closeAfterForceComputationGpu(cycles_wait_gpu, waitedForGpu);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    if (fr->nbv->emulateGpu())
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    // If on GPU PME-PP comms path, receive forces from PME before GPU buffer ops
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    // When running free energy perturbations steered by AWH and calculating PME on GPU,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    if (needToReceivePmeResultsFromSeparateRank && simulationWork.useGpuPmePpCommunication && !needEarlyPmeResults)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                               simulationWork.useGpuPmePpCommunication,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                               stepWork.useGpuPmeFReduction, wcycle);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    /* Do the nonbonded GPU (or emulation) force buffer reduction
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    GMX_ASSERT(!(nonbondedAtMtsLevel1 && stepWork.useGpuFBufferOps),
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:               "The schedule below does not allow for nonbonded MTS with GPU buffer ops");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    if (useOrEmulateGpuNb && !alternateGpuWait)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:        if (stepWork.useGpuFBufferOps)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            // - copy is not perfomed if GPU force halo exchange is active, because it would overwrite the result
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            if (haveLocalForceContribInCpuBuffer && !stepWork.useGpuFHalo)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                // case copyForcesToGpu() uses a separate stream, it allows overlap of
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                // CPU force H2D with GPU force tasks on all streams including those in the
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                stateGpu->copyForcesToGpu(forceWithShift, locality);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                fr->gpuForceReduction[gmx::AtomLocality::Local]->execute();
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:            if (!simulationWork.useGpuUpdate
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                || (simulationWork.useGpuUpdate && DOMAINDECOMP(cr) && haveHostPmePpComms) || vsite)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                stateGpu->copyForcesFromGpu(forceWithShift, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                stateGpu->waitForcesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    launchGpuEndOfStepTasks(nbv, fr->gpuBonded, fr->pmedata, enerd, *runScheduleWork,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                            useGpuPmeOnThisRank, step, wcycle);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    // TODO refactor this and unify with above GPU PME-PP / GPU update path call to the same function
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    // When running free energy perturbations steered by AWH and calculating PME on GPU,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    if (needToReceivePmeResultsFromSeparateRank && !simulationWork.useGpuPmePpCommunication
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:                               simulationWork.useGpuPmePpCommunication, false, wcycle);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdlib/sim_util.cpp:    /* In case we don't have constraints and are using GPUs, the next balancing
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:#include "gromacs/domdec/gpuhaloexchange.h"
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:#include "gromacs/ewald/pme_gpu_program.h"
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:#include "gromacs/ewald/pme_pp_comm_gpu.h"
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:#include "gromacs/gpu_utils/device_stream_manager.h"
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:#include "gromacs/listed_forces/gpubonded.h"
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:#include "gromacs/mdlib/gpuforcereduction.h"
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:#include "gromacs/mdtypes/state_propagator_data_gpu.h"
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:#include "gromacs/nbnxm/gpu_data_mgmt.h"
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:#include "gromacs/taskassignment/decidegpuusage.h"
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:#include "gromacs/taskassignment/usergpuids.h"
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:#include "gromacs/timing/gpu_timing.h"
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp: * the GPU communication flags are set to false in non-tMPI and non-CUDA builds.
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp: * \param[in]  useGpuForNonbonded   True if the nonbonded task is offloaded in this run.
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:                                                         const bool           useGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:    devFlags.enableGpuBufferOps =
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:            GMX_GPU_CUDA && useGpuForNonbonded && (getenv("GMX_USE_GPU_BUFFER_OPS") != nullptr);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:    devFlags.enableGpuHaloExchange = GMX_GPU_CUDA && GMX_THREAD_MPI && getenv("GMX_GPU_DD_COMMS") != nullptr;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:    devFlags.forceGpuUpdateDefault = (getenv("GMX_FORCE_UPDATE_DEFAULT_GPU") != nullptr) || GMX_FAHCORE;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:    devFlags.enableGpuPmePPComm =
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:            GMX_GPU_CUDA && GMX_THREAD_MPI && getenv("GMX_GPU_PME_PP_COMMS") != nullptr;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:    if (devFlags.enableGpuBufferOps)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:                        "This run uses the 'GPU buffer ops' feature, enabled by the "
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:                        "GMX_USE_GPU_BUFFER_OPS environment variable.");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:    if (devFlags.forceGpuUpdateDefault)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:                        "This run will default to '-update gpu' as requested by the "
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:                        "GMX_FORCE_UPDATE_DEFAULT_GPU environment variable. GPU update with domain "
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:    if (devFlags.enableGpuHaloExchange)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:        if (useGpuForNonbonded)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:            if (!devFlags.enableGpuBufferOps)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:                                "Enabling GPU buffer operations required by GMX_GPU_DD_COMMS "
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:                                "(equivalent with GMX_USE_GPU_BUFFER_OPS=1).");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:                devFlags.enableGpuBufferOps = true;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:                            "This run has requested the 'GPU halo exchange' feature, enabled by "
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:                            "GMX_GPU_DD_COMMS environment variable.");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:                            "GMX_GPU_DD_COMMS environment variable detected, but the 'GPU "
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:            devFlags.enableGpuHaloExchange = false;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:    if (devFlags.enableGpuPmePPComm)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:        if (pmeRunMode == PmeRunMode::GPU)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:            if (!devFlags.enableGpuBufferOps)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:                                "Enabling GPU buffer operations required by GMX_GPU_PME_PP_COMMS "
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:                                "(equivalent with GMX_USE_GPU_BUFFER_OPS=1).");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:                devFlags.enableGpuBufferOps = true;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:                            "This run uses the 'GPU PME-PP communications' feature, enabled "
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:                            "by the GMX_GPU_PME_PP_COMMS environment variable.");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:                        "PME FFT and gather are not offloaded to the GPU (PME is running in mixed "
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:                clarification = "PME is not offloaded to the GPU.";
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:                            "GMX_GPU_PME_PP_COMMS environment variable detected, but the "
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:                            "'GPU PME-PP communications' feature was not enabled as "
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:            devFlags.enableGpuPmePPComm = false;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:                                  bool                makeGpuPairList,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:                (makeGpuPairList ? ListSetupType::Gpu : ListSetupType::CpuSimdWhenSupported);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:        increaseNstlist(fplog, cr, ir, nstlist_cmdline, mtop, box, makeGpuPairList, cpuinfo);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:/*! \brief Return whether GPU acceleration of nonbondeds is supported with the given settings.
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:static bool gpuAccelerationOfNonbondedIsUseful(const MDLogger& mdlog, const t_inputrec& ir, bool issueWarning)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:    bool        gpuIsUseful = true;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:        /* The GPU code does not support more than one energy group.
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:         * If the user requested GPUs explicitly, a fatal error is given later.
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:        gpuIsUseful = false;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:                "Multiple energy groups is not implemented for GPUs, falling back to the CPU. "
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:                "For better performance, run on the GPU without energy groups and then do "
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:        gpuIsUseful = false;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:        warning     = "TPI is not implemented for GPUs.";
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:    if (!gpuIsUseful && issueWarning)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:    return gpuIsUseful;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:    else if (strncmp(optionString, "gpu", 3) == 0)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:        returnValue = TaskTarget::Gpu;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:        auto nbnxn_gpu_timings =
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:                (nbv != nullptr && nbv->useGpu()) ? Nbnxm::gpu_get_timings(nbv->gpu_nbv) : nullptr;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:        gmx_wallclock_gpu_pme_t pme_gpu_timings = {};
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:        if (pme_gpu_task_enabled(pme))
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:            pme_gpu_get_timings(pme, &pme_gpu_timings);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:                        elapsed_time_over_all_ranks, wcycle, cycle_sum, nbnxn_gpu_timings,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:                        &pme_gpu_timings);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:    EmulateGpuNonbonded emulateGpuNonbonded =
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:            (getenv("GMX_EMULATE_GPU") != nullptr ? EmulateGpuNonbonded::Yes : EmulateGpuNonbonded::No);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:    std::vector<int> userGpuTaskAssignment;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:        userGpuTaskAssignment = parseUserTaskAssignmentString(hw_opt.userGpuTaskAssignment);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:    std::vector<int> gpuIdsToUse = makeGpuIdsToUse(hwinfo_->deviceInfoList, hw_opt.gpuIdsAvailable);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:    const int        numDevicesToUse = gmx::ssize(gpuIdsToUse);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:        bool useGpuForNonbonded = false;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:        bool useGpuForPme       = false;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:            // the number of GPUs to choose the number of ranks.
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:            auto canUseGpuForNonbonded = buildSupportsNonbondedOnGpu(nullptr);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:            useGpuForNonbonded         = decideWhetherToUseGpusForNonbondedWithThreadMpi(
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:                    nonbondedTarget, numDevicesToUse, userGpuTaskAssignment, emulateGpuNonbonded,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:                    canUseGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:                    gpuAccelerationOfNonbondedIsUseful(mdlog, *inputrec, GMX_THREAD_MPI),
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:            useGpuForPme = decideWhetherToUseGpusForPmeWithThreadMpi(
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:                    useGpuForNonbonded, pmeTarget, pmeFftTarget, numDevicesToUse, userGpuTaskAssignment,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:                get_nthreads_mpi(hwinfo_, &hw_opt, numDevicesToUse, useGpuForNonbonded, useGpuForPme,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:    // Note that when bonded interactions run on a GPU they always run
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:    bool useGpuForNonbonded = false;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:    bool useGpuForPme       = false;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:    bool useGpuForBonded    = false;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:    bool useGpuForUpdate    = false;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:    bool gpusWereDetected   = hwinfo_->ngpu_compatible_tot > 0;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:        // It's possible that there are different numbers of GPUs on
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:        auto canUseGpuForNonbonded = buildSupportsNonbondedOnGpu(nullptr);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:        useGpuForNonbonded         = decideWhetherToUseGpusForNonbonded(
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:                nonbondedTarget, userGpuTaskAssignment, emulateGpuNonbonded, canUseGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:                gpuAccelerationOfNonbondedIsUseful(mdlog, *inputrec, !GMX_THREAD_MPI), gpusWereDetected);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:        useGpuForPme = decideWhetherToUseGpusForPme(
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:                useGpuForNonbonded, pmeTarget, pmeFftTarget, userGpuTaskAssignment, *hwinfo_,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:                *inputrec, cr->sizeOfDefaultCommunicator, domdecOptions.numPmeRanks, gpusWereDetected);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:        useGpuForBonded = decideWhetherToUseGpusForBonded(useGpuForNonbonded, useGpuForPme,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:                                                          domdecOptions.numPmeRanks, gpusWereDetected);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:    const PmeRunMode pmeRunMode = determinePmeRunMode(useGpuForPme, pmeFftTarget, *inputrec);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:            manageDevelopmentFeatures(mdlog, useGpuForNonbonded, pmeRunMode);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:    if (useGpuForNonbonded && domdecOptions.numPmeRanks < 0)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:        /* With NB GPUs we don't automatically use PME-only CPU ranks. PME ranks can
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:         * improve performance with many threads per GPU, since our OpenMP
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:    if (useGpuForPme)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:            // TODO possibly print a note that one can opt-in for a separate PME GPU rank?
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:                               "PME GPU decomposition is not supported");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:                          useGpuForNonbonded || (emulateGpuNonbonded == EmulateGpuNonbonded::Yes),
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:    GpuTaskAssignments gpuTaskAssignments = GpuTaskAssignmentsBuilder::build(
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:            gpuIdsToUse, userGpuTaskAssignment, *hwinfo_, simulationCommunicator, physicalNodeComm,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:            nonbondedTarget, pmeTarget, bondedTarget, updateTarget, useGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:            useGpuForPme, thisRankHasDuty(cr, DUTY_PP),
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:    DeviceInformation* deviceInfo = gpuTaskAssignments.initDevice(&deviceId);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:    // timing enabling - TODO put this in gpu_utils (even though generally this is just option handling?)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:    if (GMX_GPU_CUDA)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:        /* WARNING: CUDA timings are incorrect with multiple streams.
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:        useTiming = (getenv("GMX_ENABLE_GPU_TIMING") != nullptr);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:    else if (GMX_GPU_OPENCL)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:        useTiming = (getenv("GMX_DISABLE_GPU_TIMING") == nullptr);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:        // TODO Pass the GPU streams to ddBuilder to use in buffer
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:    // The GPU update is decided here because we need to know whether the constraints or
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:    // defined). This is only known after DD is initialized, hence decision on using GPU
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:        useGpuForUpdate = decideWhetherToUseGpuForUpdate(
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:                useGpuForNonbonded, updateTarget, gpusWereDetected, *inputrec, mtop,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:    gpuTaskAssignments.reportGpuUsage(mdlog, printHostName, useGpuForBonded, pmeRunMode, useGpuForUpdate);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:    bool useGpuDirectHalo = decideWhetherToUseGpuForHalo(
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:            devFlags, havePPDomainDecomposition(cr), useGpuForNonbonded, useModularSimulator,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:            *inputrec, disableNonbondedCalculation, devFlags, useGpuForNonbonded, pmeRunMode,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:            useGpuForBonded, useGpuForUpdate, useGpuDirectHalo);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:    if (!userGpuTaskAssignment.empty())
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:        gpuTaskAssignments.logPerformanceHints(mdlog, numDevicesToUse);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:    check_resource_division_efficiency(hwinfo_, gpuTaskAssignments.thisRankHasAnyGpuTask(),
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:    // Enable Peer access between GPUs where available
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:    // any of the GPU communication features are active.
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:        && (runScheduleWork.simulationWork.useGpuHaloExchange
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:            || runScheduleWork.simulationWork.useGpuPmePpCommunication))
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:        setupGpuDevicePeerAccess(gpuIdsToUse, mdlog);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:    const bool               thisRankHasPmeGpuTask = gpuTaskAssignments.thisRankHasPmeGpuTask();
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:    std::unique_ptr<GpuBonded>           gpuBonded;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:        if (runScheduleWork.simulationWork.useGpuPmePpCommunication && !thisRankHasDuty(cr, DUTY_PME))
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:                    "GPU device stream manager should be valid in order to use PME-PP direct "
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:                    "GPU PP-PME stream should be valid in order to use GPU PME-PP direct "
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:            fr->pmePpCommGpu = std::make_unique<gmx::PmePpCommGpu>(
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:                                        runScheduleWork.simulationWork.useGpuNonbonded,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:        // TODO: Move the logic below to a GPU bonded builder
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:        if (runScheduleWork.simulationWork.useGpuBonded)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:                               "GPU device stream manager should be valid in order to use GPU "
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:            gpuBonded = std::make_unique<GpuBonded>(
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:            fr->gpuBonded = gpuBonded.get();
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:        mdAtoms = makeMDAtoms(fplog, mtop, *inputrec, thisRankHasPmeGpuTask);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:        if (globalState && thisRankHasPmeGpuTask)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:            // PME on GPU without DD or on a separate PME rank, and because the local state pointer
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:    PmeGpuProgramStorage pmeGpuProgram;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:    if (thisRankHasPmeGpuTask)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:                "GPU device stream manager should be initialized in order to use GPU for PME.");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:                           "GPU device should be initialized in order to use GPU for PME.");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:        pmeGpuProgram = buildPmeGpuProgram(deviceStreamManager->context());
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:                GMX_RELEASE_ASSERT(!runScheduleWork.simulationWork.useGpuPme
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:                                   "Device stream manager should be valid in order to use GPU "
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:                        !runScheduleWork.simulationWork.useGpuPme
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:                        "GPU PME stream should be valid in order to use GPU version of PME.");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:                const DeviceContext* deviceContext = runScheduleWork.simulationWork.useGpuPme
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:                        runScheduleWork.simulationWork.useGpuPme
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:                                       deviceContext, pmeStream, pmeGpuProgram.get(), mdlog);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:        if (runScheduleWork.simulationWork.useGpuBufferOps)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:            fr->gpuForceReduction[gmx::AtomLocality::Local] = std::make_unique<gmx::GpuForceReduction>(
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:            fr->gpuForceReduction[gmx::AtomLocality::NonLocal] = std::make_unique<gmx::GpuForceReduction>(
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:        std::unique_ptr<gmx::StatePropagatorDataGpu> stateGpu;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:        if (gpusWereDetected
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:            && ((runScheduleWork.simulationWork.useGpuPme && thisRankHasDuty(cr, DUTY_PME))
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:                || runScheduleWork.simulationWork.useGpuBufferOps))
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:            GpuApiCallBehavior transferKind = (inputrec->eI == eiMD && !doRerun && !useModularSimulator)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:                                                      ? GpuApiCallBehavior::Async
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:                                                      : GpuApiCallBehavior::Sync;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:                               "GPU device stream manager should be initialized to use GPU.");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:            stateGpu = std::make_unique<gmx::StatePropagatorDataGpu>(
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:                    *deviceStreamManager, transferKind, pme_gpu_get_block_size(fr->pmedata), wcycle);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:            fr->stateGpu = stateGpu.get();
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:          /* set GPU device id */
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:             plumed_cmd(plumedmain,"setGpuDeviceId", &deviceId);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:          if(useGpuForUpdate) {
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:                        "This simulation is resident on GPU (-update gpu)\n"
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:        if (fr->pmePpCommGpu)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:            // destroy object since it is no longer required. (This needs to be done while the GPU context still exists.)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:            fr->pmePpCommGpu.reset();
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:    // before we destroy the GPU context(s)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:    // Pinned buffers are associated with contexts in CUDA.
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:    // As soon as we destroy GPU contexts after mdrunner() exits, these lines should go.
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:    mdModules_.reset(nullptr); // destruct force providers here as they might also use the GPU
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:    gpuBonded.reset(nullptr);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:        /* stop the GPU profiler (only CUDA) */
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:        stopGpuProfiler();
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:     * destroying the CUDA context as some tMPI ranks may be sharing
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:     * GPU and context.
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:     * This is not a concern in OpenCL where we use one context per rank.
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:     * Note: it is safe to not call the barrier on the ranks which do not use GPU,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:     * Note that this function needs to be called even if GPUs are not used
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:     * in this run because the PME ranks have no knowledge of whether GPUs
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp:     * that it's not needed anymore (with a shared GPU run).
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/legacymdrunoptions.cpp:    // which compatible GPUs are availble for use, or to select a GPU
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/legacymdrunoptions.cpp:        hw_opt.gpuIdsAvailable       = gpuIdsAvailable;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/legacymdrunoptions.cpp:        hw_opt.userGpuTaskAssignment = userGpuTaskAssignment;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/legacymdrunoptions.cpp:        const char* env = getenv("GMX_GPU_ID");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/legacymdrunoptions.cpp:            if (!hw_opt.gpuIdsAvailable.empty())
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/legacymdrunoptions.cpp:                gmx_fatal(FARGS, "GMX_GPU_ID and -gpu_id can not be used at the same time");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/legacymdrunoptions.cpp:            hw_opt.gpuIdsAvailable = env;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/legacymdrunoptions.cpp:        env = getenv("GMX_GPUTASKS");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/legacymdrunoptions.cpp:            if (!hw_opt.userGpuTaskAssignment.empty())
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/legacymdrunoptions.cpp:                gmx_fatal(FARGS, "GMX_GPUTASKS and -gputasks can not be used at the same time");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/legacymdrunoptions.cpp:            hw_opt.userGpuTaskAssignment = env;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/legacymdrunoptions.cpp:        if (!hw_opt.gpuIdsAvailable.empty() && !hw_opt.userGpuTaskAssignment.empty())
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/legacymdrunoptions.cpp:            gmx_fatal(FARGS, "-gpu_id and -gputasks cannot be used at the same time");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/legacymdrunoptions.cpp.preplumed:    // which compatible GPUs are availble for use, or to select a GPU
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/legacymdrunoptions.cpp.preplumed:        hw_opt.gpuIdsAvailable       = gpuIdsAvailable;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/legacymdrunoptions.cpp.preplumed:        hw_opt.userGpuTaskAssignment = userGpuTaskAssignment;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/legacymdrunoptions.cpp.preplumed:        const char* env = getenv("GMX_GPU_ID");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/legacymdrunoptions.cpp.preplumed:            if (!hw_opt.gpuIdsAvailable.empty())
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/legacymdrunoptions.cpp.preplumed:                gmx_fatal(FARGS, "GMX_GPU_ID and -gpu_id can not be used at the same time");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/legacymdrunoptions.cpp.preplumed:            hw_opt.gpuIdsAvailable = env;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/legacymdrunoptions.cpp.preplumed:        env = getenv("GMX_GPUTASKS");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/legacymdrunoptions.cpp.preplumed:            if (!hw_opt.userGpuTaskAssignment.empty())
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/legacymdrunoptions.cpp.preplumed:                gmx_fatal(FARGS, "GMX_GPUTASKS and -gputasks can not be used at the same time");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/legacymdrunoptions.cpp.preplumed:            hw_opt.userGpuTaskAssignment = env;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/legacymdrunoptions.cpp.preplumed:        if (!hw_opt.gpuIdsAvailable.empty() && !hw_opt.userGpuTaskAssignment.empty())
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/legacymdrunoptions.cpp.preplumed:            gmx_fatal(FARGS, "-gpu_id and -gputasks cannot be used at the same time");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:    const char* nbpu_opt_choices[5]    = { nullptr, "auto", "cpu", "gpu", nullptr };
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:    const char* pme_opt_choices[5]     = { nullptr, "auto", "cpu", "gpu", nullptr };
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:    const char* pme_fft_opt_choices[5] = { nullptr, "auto", "cpu", "gpu", nullptr };
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:    const char* bonded_opt_choices[5]  = { nullptr, "auto", "cpu", "gpu", nullptr };
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:    const char* update_opt_choices[5]  = { nullptr, "auto", "cpu", "gpu", nullptr };
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:    const char* gpuIdsAvailable        = "";
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:    const char* userGpuTaskAssignment  = "";
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:        { "-gpu_id",
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:          { &gpuIdsAvailable },
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:          "List of unique GPU device IDs available to use" },
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:        { "-gputasks",
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:          { &userGpuTaskAssignment },
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:          "List of GPU device IDs, mapping each PP task on each node to a device" },
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/legacymdrunoptions.h.preplumed:          "Optimize PME load between PP/PME ranks or GPU/CPU" },
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/legacymdrunoptions.h:    const char* nbpu_opt_choices[5]    = { nullptr, "auto", "cpu", "gpu", nullptr };
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/legacymdrunoptions.h:    const char* pme_opt_choices[5]     = { nullptr, "auto", "cpu", "gpu", nullptr };
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/legacymdrunoptions.h:    const char* pme_fft_opt_choices[5] = { nullptr, "auto", "cpu", "gpu", nullptr };
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/legacymdrunoptions.h:    const char* bonded_opt_choices[5]  = { nullptr, "auto", "cpu", "gpu", nullptr };
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/legacymdrunoptions.h:    const char* update_opt_choices[5]  = { nullptr, "auto", "cpu", "gpu", nullptr };
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/legacymdrunoptions.h:    const char* gpuIdsAvailable        = "";
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/legacymdrunoptions.h:    const char* userGpuTaskAssignment  = "";
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/legacymdrunoptions.h:        { "-gpu_id",
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/legacymdrunoptions.h:          { &gpuIdsAvailable },
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/legacymdrunoptions.h:          "List of unique GPU device IDs available to use" },
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/legacymdrunoptions.h:        { "-gputasks",
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/legacymdrunoptions.h:          { &userGpuTaskAssignment },
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/legacymdrunoptions.h:          "List of GPU device IDs, mapping each PP task on each node to a device" },
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/legacymdrunoptions.h:          "Optimize PME load between PP/PME ranks or GPU/CPU" },
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:#include "gromacs/domdec/gpuhaloexchange.h"
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:#include "gromacs/ewald/pme_gpu_program.h"
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:#include "gromacs/ewald/pme_pp_comm_gpu.h"
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:#include "gromacs/gpu_utils/device_stream_manager.h"
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:#include "gromacs/listed_forces/gpubonded.h"
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:#include "gromacs/mdlib/gpuforcereduction.h"
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:#include "gromacs/mdtypes/state_propagator_data_gpu.h"
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:#include "gromacs/nbnxm/gpu_data_mgmt.h"
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:#include "gromacs/taskassignment/decidegpuusage.h"
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:#include "gromacs/taskassignment/usergpuids.h"
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:#include "gromacs/timing/gpu_timing.h"
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed: * the GPU communication flags are set to false in non-tMPI and non-CUDA builds.
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed: * \param[in]  useGpuForNonbonded   True if the nonbonded task is offloaded in this run.
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                                         const bool           useGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:    devFlags.enableGpuBufferOps =
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:            GMX_GPU_CUDA && useGpuForNonbonded && (getenv("GMX_USE_GPU_BUFFER_OPS") != nullptr);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:    devFlags.enableGpuHaloExchange = GMX_GPU_CUDA && GMX_THREAD_MPI && getenv("GMX_GPU_DD_COMMS") != nullptr;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:    devFlags.forceGpuUpdateDefault = (getenv("GMX_FORCE_UPDATE_DEFAULT_GPU") != nullptr) || GMX_FAHCORE;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:    devFlags.enableGpuPmePPComm =
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:            GMX_GPU_CUDA && GMX_THREAD_MPI && getenv("GMX_GPU_PME_PP_COMMS") != nullptr;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:    if (devFlags.enableGpuBufferOps)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:                        "This run uses the 'GPU buffer ops' feature, enabled by the "
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:                        "GMX_USE_GPU_BUFFER_OPS environment variable.");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:    if (devFlags.forceGpuUpdateDefault)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:                        "This run will default to '-update gpu' as requested by the "
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:                        "GMX_FORCE_UPDATE_DEFAULT_GPU environment variable. GPU update with domain "
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:    if (devFlags.enableGpuHaloExchange)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:        if (useGpuForNonbonded)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:            if (!devFlags.enableGpuBufferOps)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                "Enabling GPU buffer operations required by GMX_GPU_DD_COMMS "
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                "(equivalent with GMX_USE_GPU_BUFFER_OPS=1).");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:                devFlags.enableGpuBufferOps = true;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:                            "This run has requested the 'GPU halo exchange' feature, enabled by "
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:                            "GMX_GPU_DD_COMMS environment variable.");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:                            "GMX_GPU_DD_COMMS environment variable detected, but the 'GPU "
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:            devFlags.enableGpuHaloExchange = false;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:    if (devFlags.enableGpuPmePPComm)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:        if (pmeRunMode == PmeRunMode::GPU)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:            if (!devFlags.enableGpuBufferOps)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                "Enabling GPU buffer operations required by GMX_GPU_PME_PP_COMMS "
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                "(equivalent with GMX_USE_GPU_BUFFER_OPS=1).");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:                devFlags.enableGpuBufferOps = true;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:                            "This run uses the 'GPU PME-PP communications' feature, enabled "
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:                            "by the GMX_GPU_PME_PP_COMMS environment variable.");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:                        "PME FFT and gather are not offloaded to the GPU (PME is running in mixed "
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:                clarification = "PME is not offloaded to the GPU.";
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:                            "GMX_GPU_PME_PP_COMMS environment variable detected, but the "
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:                            "'GPU PME-PP communications' feature was not enabled as "
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:            devFlags.enableGpuPmePPComm = false;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                  bool                makeGpuPairList,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:                (makeGpuPairList ? ListSetupType::Gpu : ListSetupType::CpuSimdWhenSupported);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:        increaseNstlist(fplog, cr, ir, nstlist_cmdline, mtop, box, makeGpuPairList, cpuinfo);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:/*! \brief Return whether GPU acceleration of nonbondeds is supported with the given settings.
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:static bool gpuAccelerationOfNonbondedIsUseful(const MDLogger& mdlog, const t_inputrec& ir, bool issueWarning)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:    bool        gpuIsUseful = true;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:        /* The GPU code does not support more than one energy group.
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:         * If the user requested GPUs explicitly, a fatal error is given later.
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:        gpuIsUseful = false;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:                "Multiple energy groups is not implemented for GPUs, falling back to the CPU. "
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:                "For better performance, run on the GPU without energy groups and then do "
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:        gpuIsUseful = false;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:        warning     = "TPI is not implemented for GPUs.";
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:    if (!gpuIsUseful && issueWarning)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:    return gpuIsUseful;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:    else if (strncmp(optionString, "gpu", 3) == 0)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:        returnValue = TaskTarget::Gpu;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:        auto nbnxn_gpu_timings =
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:                (nbv != nullptr && nbv->useGpu()) ? Nbnxm::gpu_get_timings(nbv->gpu_nbv) : nullptr;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:        gmx_wallclock_gpu_pme_t pme_gpu_timings = {};
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:        if (pme_gpu_task_enabled(pme))
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:            pme_gpu_get_timings(pme, &pme_gpu_timings);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:                        elapsed_time_over_all_ranks, wcycle, cycle_sum, nbnxn_gpu_timings,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:                        &pme_gpu_timings);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:    EmulateGpuNonbonded emulateGpuNonbonded =
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:            (getenv("GMX_EMULATE_GPU") != nullptr ? EmulateGpuNonbonded::Yes : EmulateGpuNonbonded::No);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:    std::vector<int> userGpuTaskAssignment;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:        userGpuTaskAssignment = parseUserTaskAssignmentString(hw_opt.userGpuTaskAssignment);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:    std::vector<int> gpuIdsToUse = makeGpuIdsToUse(hwinfo_->deviceInfoList, hw_opt.gpuIdsAvailable);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:    const int        numDevicesToUse = gmx::ssize(gpuIdsToUse);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:        bool useGpuForNonbonded = false;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:        bool useGpuForPme       = false;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:            // the number of GPUs to choose the number of ranks.
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:            auto canUseGpuForNonbonded = buildSupportsNonbondedOnGpu(nullptr);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:            useGpuForNonbonded         = decideWhetherToUseGpusForNonbondedWithThreadMpi(
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:                    nonbondedTarget, numDevicesToUse, userGpuTaskAssignment, emulateGpuNonbonded,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:                    canUseGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:                    gpuAccelerationOfNonbondedIsUseful(mdlog, *inputrec, GMX_THREAD_MPI),
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:            useGpuForPme = decideWhetherToUseGpusForPmeWithThreadMpi(
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:                    useGpuForNonbonded, pmeTarget, pmeFftTarget, numDevicesToUse, userGpuTaskAssignment,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:                get_nthreads_mpi(hwinfo_, &hw_opt, numDevicesToUse, useGpuForNonbonded, useGpuForPme,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:    // Note that when bonded interactions run on a GPU they always run
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:    bool useGpuForNonbonded = false;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:    bool useGpuForPme       = false;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:    bool useGpuForBonded    = false;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:    bool useGpuForUpdate    = false;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:    bool gpusWereDetected   = hwinfo_->ngpu_compatible_tot > 0;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:        // It's possible that there are different numbers of GPUs on
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:        auto canUseGpuForNonbonded = buildSupportsNonbondedOnGpu(nullptr);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:        useGpuForNonbonded         = decideWhetherToUseGpusForNonbonded(
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:                nonbondedTarget, userGpuTaskAssignment, emulateGpuNonbonded, canUseGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:                gpuAccelerationOfNonbondedIsUseful(mdlog, *inputrec, !GMX_THREAD_MPI), gpusWereDetected);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:        useGpuForPme = decideWhetherToUseGpusForPme(
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:                useGpuForNonbonded, pmeTarget, pmeFftTarget, userGpuTaskAssignment, *hwinfo_,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:                *inputrec, cr->sizeOfDefaultCommunicator, domdecOptions.numPmeRanks, gpusWereDetected);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:        useGpuForBonded = decideWhetherToUseGpusForBonded(useGpuForNonbonded, useGpuForPme,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                                          domdecOptions.numPmeRanks, gpusWereDetected);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:    const PmeRunMode pmeRunMode = determinePmeRunMode(useGpuForPme, pmeFftTarget, *inputrec);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:            manageDevelopmentFeatures(mdlog, useGpuForNonbonded, pmeRunMode);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:    if (useGpuForNonbonded && domdecOptions.numPmeRanks < 0)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:        /* With NB GPUs we don't automatically use PME-only CPU ranks. PME ranks can
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:         * improve performance with many threads per GPU, since our OpenMP
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:    if (useGpuForPme)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:            // TODO possibly print a note that one can opt-in for a separate PME GPU rank?
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:                               "PME GPU decomposition is not supported");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:                          useGpuForNonbonded || (emulateGpuNonbonded == EmulateGpuNonbonded::Yes),
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:    GpuTaskAssignments gpuTaskAssignments = GpuTaskAssignmentsBuilder::build(
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:            gpuIdsToUse, userGpuTaskAssignment, *hwinfo_, simulationCommunicator, physicalNodeComm,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:            nonbondedTarget, pmeTarget, bondedTarget, updateTarget, useGpuForNonbonded,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:            useGpuForPme, thisRankHasDuty(cr, DUTY_PP),
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:    DeviceInformation* deviceInfo = gpuTaskAssignments.initDevice(&deviceId);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:    // timing enabling - TODO put this in gpu_utils (even though generally this is just option handling?)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:    if (GMX_GPU_CUDA)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:        /* WARNING: CUDA timings are incorrect with multiple streams.
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:        useTiming = (getenv("GMX_ENABLE_GPU_TIMING") != nullptr);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:    else if (GMX_GPU_OPENCL)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:        useTiming = (getenv("GMX_DISABLE_GPU_TIMING") == nullptr);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:        // TODO Pass the GPU streams to ddBuilder to use in buffer
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:    // The GPU update is decided here because we need to know whether the constraints or
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:    // defined). This is only known after DD is initialized, hence decision on using GPU
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:        useGpuForUpdate = decideWhetherToUseGpuForUpdate(
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:                useGpuForNonbonded, updateTarget, gpusWereDetected, *inputrec, mtop,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:    gpuTaskAssignments.reportGpuUsage(mdlog, printHostName, useGpuForBonded, pmeRunMode, useGpuForUpdate);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:    bool useGpuDirectHalo = decideWhetherToUseGpuForHalo(
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:            devFlags, havePPDomainDecomposition(cr), useGpuForNonbonded, useModularSimulator,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:            *inputrec, disableNonbondedCalculation, devFlags, useGpuForNonbonded, pmeRunMode,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:            useGpuForBonded, useGpuForUpdate, useGpuDirectHalo);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:    if (!userGpuTaskAssignment.empty())
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:        gpuTaskAssignments.logPerformanceHints(mdlog, numDevicesToUse);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:    check_resource_division_efficiency(hwinfo_, gpuTaskAssignments.thisRankHasAnyGpuTask(),
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:    // Enable Peer access between GPUs where available
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:    // any of the GPU communication features are active.
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:        && (runScheduleWork.simulationWork.useGpuHaloExchange
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:            || runScheduleWork.simulationWork.useGpuPmePpCommunication))
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:        setupGpuDevicePeerAccess(gpuIdsToUse, mdlog);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:    const bool               thisRankHasPmeGpuTask = gpuTaskAssignments.thisRankHasPmeGpuTask();
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:    std::unique_ptr<GpuBonded>           gpuBonded;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:        if (runScheduleWork.simulationWork.useGpuPmePpCommunication && !thisRankHasDuty(cr, DUTY_PME))
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:                    "GPU device stream manager should be valid in order to use PME-PP direct "
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:                    "GPU PP-PME stream should be valid in order to use GPU PME-PP direct "
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:            fr->pmePpCommGpu = std::make_unique<gmx::PmePpCommGpu>(
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                        runScheduleWork.simulationWork.useGpuNonbonded,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:        // TODO: Move the logic below to a GPU bonded builder
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:        if (runScheduleWork.simulationWork.useGpuBonded)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:                               "GPU device stream manager should be valid in order to use GPU "
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:            gpuBonded = std::make_unique<GpuBonded>(
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:            fr->gpuBonded = gpuBonded.get();
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:        mdAtoms = makeMDAtoms(fplog, mtop, *inputrec, thisRankHasPmeGpuTask);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:        if (globalState && thisRankHasPmeGpuTask)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:            // PME on GPU without DD or on a separate PME rank, and because the local state pointer
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:    PmeGpuProgramStorage pmeGpuProgram;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:    if (thisRankHasPmeGpuTask)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:                "GPU device stream manager should be initialized in order to use GPU for PME.");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:                           "GPU device should be initialized in order to use GPU for PME.");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:        pmeGpuProgram = buildPmeGpuProgram(deviceStreamManager->context());
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:                GMX_RELEASE_ASSERT(!runScheduleWork.simulationWork.useGpuPme
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                   "Device stream manager should be valid in order to use GPU "
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:                        !runScheduleWork.simulationWork.useGpuPme
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:                        "GPU PME stream should be valid in order to use GPU version of PME.");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:                const DeviceContext* deviceContext = runScheduleWork.simulationWork.useGpuPme
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:                        runScheduleWork.simulationWork.useGpuPme
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                       deviceContext, pmeStream, pmeGpuProgram.get(), mdlog);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:        if (runScheduleWork.simulationWork.useGpuBufferOps)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:            fr->gpuForceReduction[gmx::AtomLocality::Local] = std::make_unique<gmx::GpuForceReduction>(
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:            fr->gpuForceReduction[gmx::AtomLocality::NonLocal] = std::make_unique<gmx::GpuForceReduction>(
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:        std::unique_ptr<gmx::StatePropagatorDataGpu> stateGpu;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:        if (gpusWereDetected
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:            && ((runScheduleWork.simulationWork.useGpuPme && thisRankHasDuty(cr, DUTY_PME))
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:                || runScheduleWork.simulationWork.useGpuBufferOps))
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:            GpuApiCallBehavior transferKind = (inputrec->eI == eiMD && !doRerun && !useModularSimulator)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                                      ? GpuApiCallBehavior::Async
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:                                                      : GpuApiCallBehavior::Sync;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:                               "GPU device stream manager should be initialized to use GPU.");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:            stateGpu = std::make_unique<gmx::StatePropagatorDataGpu>(
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:                    *deviceStreamManager, transferKind, pme_gpu_get_block_size(fr->pmedata), wcycle);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:            fr->stateGpu = stateGpu.get();
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:        if (fr->pmePpCommGpu)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:            // destroy object since it is no longer required. (This needs to be done while the GPU context still exists.)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:            fr->pmePpCommGpu.reset();
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:    // before we destroy the GPU context(s)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:    // Pinned buffers are associated with contexts in CUDA.
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:    // As soon as we destroy GPU contexts after mdrunner() exits, these lines should go.
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:    mdModules_.reset(nullptr); // destruct force providers here as they might also use the GPU
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:    gpuBonded.reset(nullptr);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:        /* stop the GPU profiler (only CUDA) */
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:        stopGpuProfiler();
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:     * destroying the CUDA context as some tMPI ranks may be sharing
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:     * GPU and context.
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:     * This is not a concern in OpenCL where we use one context per rank.
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:     * Note: it is safe to not call the barrier on the ranks which do not use GPU,
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:     * Note that this function needs to be called even if GPUs are not used
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:     * in this run because the PME ranks have no knowledge of whether GPUs
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/runner.cpp.preplumed:     * that it's not needed anymore (with a shared GPU run).
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:#include "gromacs/domdec/gpuhaloexchange.h"
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:#include "gromacs/gpu_utils/device_stream_manager.h"
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:#include "gromacs/gpu_utils/gpu_utils.h"
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:#include "gromacs/mdlib/update_constrain_gpu.h"
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:#include "gromacs/mdtypes/state_propagator_data_gpu.h"
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:#include "gromacs/nbnxm/gpu_data_mgmt.h"
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:    /* PME load balancing data for GPU kernels */
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:    const bool  useGpuForPme       = simulationWork.useGpuPme;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:    const bool  useGpuForNonbonded = simulationWork.useGpuNonbonded;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:    const bool  useGpuForBufferOps = simulationWork.useGpuBufferOps;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:    const bool  useGpuForUpdate    = simulationWork.useGpuUpdate;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:                                 ir->nstcalcenergy, DOMAINDECOMP(cr), useGpuForPme);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:    ForceBuffers f(fr->useMts, ((useGpuForNonbonded && useGpuForBufferOps) || useGpuForUpdate)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:    std::unique_ptr<UpdateConstrainGpu> integrator;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:    StatePropagatorDataGpu* stateGpu = fr->stateGpu;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:    if (useGpuForUpdate)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:                           "groups if using GPU update.\n");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:                           "SHAKE is not supported with GPU update.");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:        GMX_RELEASE_ASSERT(useGpuForPme || (useGpuForNonbonded && simulationWork.useGpuBufferOps),
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:                           "the GPU to use GPU update.\n");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:                           "Only the md integrator is supported with the GPU update.\n");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:                "Nose-Hoover temperature coupling is not supported with the GPU update.\n");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:                "with the GPU update.\n");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:                           "Virtual sites are not supported with the GPU update.\n");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:                           "Essential dynamics is not supported with the GPU update.\n");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:                           "Constraints pulling is not supported with the GPU update.\n");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:                           "Orientation restraints are not supported with the GPU update.\n");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:                "Free energy perturbation of masses and constraints are not supported with the GPU "
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:                    .appendText("Updating coordinates and applying constraints on the GPU.");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:            GMX_LOG(mdlog.info).asParagraph().appendText("Updating coordinates on the GPU.");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:                           "Device stream manager should be initialized in order to use GPU "
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:                "Update stream should be initialized in order to use GPU "
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:        integrator = std::make_unique<UpdateConstrainGpu>(
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:                stateGpu->xUpdatedOnDevice(), wcycle);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:    if (useGpuForPme || (useGpuForNonbonded && useGpuForBufferOps) || useGpuForUpdate)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:    if (useGpuForUpdate)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:                         fr->nbv->useGpu());
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:            if (useGpuForUpdate && !bFirstStep)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:                stateGpu->copyCoordinatesFromGpu(state->x, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:                stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:            /* PME grid + cut-off optimization with GPUs or PME nodes */
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:                           &bPMETunePrinting, simulationWork.useGpuPmePpCommunication);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:        if (useGpuForUpdate && !bFirstStep && bNS)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:            // Copy velocities from the GPU on search steps to keep a copy on host (device buffers are reinitialized).
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:            stateGpu->copyVelocitiesFromGpu(state->v, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:            stateGpu->waitVelocitiesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:            // Copy coordinate from the GPU when needed at the search step.
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:            stateGpu->copyCoordinatesFromGpu(state->x, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:            stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:                    if (useGpuForUpdate)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:        // Allocate or re-size GPU halo exchange object, if necessary
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:        if (bNS && havePPDomainDecomposition(cr) && simulationWork.useGpuHaloExchange)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:                               "GPU device manager has to be initialized to use GPU "
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:            constructGpuHaloExchange(mdlog, *cr, *fr->deviceStreamManager, wcycle);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:        // Copy coordinate from the GPU for the output/checkpointing if the update is offloaded and
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:        if (useGpuForUpdate && !bNS && !runScheduleWork->domainWork.haveCpuLocalForceWork
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:            stateGpu->copyCoordinatesFromGpu(state->x, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:            stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:        if (useGpuForUpdate && !bNS
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:            stateGpu->copyVelocitiesFromGpu(state->v, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:            stateGpu->waitVelocitiesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:        // Copy forces for the output if the forces were reduced on the GPU (not the case on virial steps)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:        // and update is offloaded hence forces are kept on the GPU for update and have not been
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:        //       when the forces are ready on the GPU -- the same synchronizer should be used as the one
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:        //       prior to GPU update.
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:        if (runScheduleWork->stepWork.useGpuFBufferOps && (simulationWork.useGpuUpdate && !vsite)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:            stateGpu->copyForcesFromGpu(f.view().force(), AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:            stateGpu->waitForcesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:        if (!useGpuForUpdate)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:        if (useGpuForUpdate)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:                integrator->set(stateGpu->getCoordinates(), stateGpu->getVelocities(),
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:                                stateGpu->getForces(), top.idef, *mdatoms, ekind->ngtc);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:                // Copy data to the GPU after buffers might have being reinitialized
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:                stateGpu->copyVelocitiesToGpu(state->v, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:                stateGpu->copyCoordinatesToGpu(state->x, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:            if (simulationWork.useGpuPme && !runScheduleWork->simulationWork.useGpuPmePpCommunication
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:                stateGpu->copyForcesToGpu(f.view().force(), AtomLocality::All);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:            else if (!runScheduleWork->stepWork.useGpuFBufferOps)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:                stateGpu->copyForcesToGpu(f.view().force(), AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:            integrator->integrate(stateGpu->getForcesReadyOnDeviceEvent(
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:                                          AtomLocality::Local, runScheduleWork->stepWork.useGpuFBufferOps),
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:                stateGpu->copyVelocitiesFromGpu(state->v, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:                stateGpu->waitVelocitiesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:                if (useGpuForUpdate && !EI_VV(ir->eI) && bStopCM)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:                    stateGpu->copyCoordinatesFromGpu(state->x, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:                    stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:                    if (useGpuForUpdate)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:                        stateGpu->copyCoordinatesToGpu(state->x, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:                        stateGpu->waitCoordinatesCopiedToDevice(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:                            stateGpu->copyVelocitiesToGpu(state->v, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:                                         pressureCouplingMu, state, nrnb, upd.deform(), !useGpuForUpdate);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:        if (useGpuForUpdate
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp.preplumed:        pme_loadbal_done(pme_loadbal, fplog, mdlog, fr->nbv->useGpu());
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/rerun.cpp.preplumed:#include "gromacs/gpu_utils/gpu_utils.h"
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/rerun.cpp.preplumed:                                 runScheduleWork->simulationWork.useGpuPme);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/rerun.cpp:#include "gromacs/gpu_utils/gpu_utils.h"
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/rerun.cpp:                                 runScheduleWork->simulationWork.useGpuPme);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:#include "gromacs/domdec/gpuhaloexchange.h"
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:#include "gromacs/gpu_utils/device_stream_manager.h"
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:#include "gromacs/gpu_utils/gpu_utils.h"
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:#include "gromacs/mdlib/update_constrain_gpu.h"
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:#include "gromacs/mdtypes/state_propagator_data_gpu.h"
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:#include "gromacs/nbnxm/gpu_data_mgmt.h"
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:    /* PME load balancing data for GPU kernels */
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:    const bool  useGpuForPme       = simulationWork.useGpuPme;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:    const bool  useGpuForNonbonded = simulationWork.useGpuNonbonded;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:    const bool  useGpuForBufferOps = simulationWork.useGpuBufferOps;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:    const bool  useGpuForUpdate    = simulationWork.useGpuUpdate;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:                                 ir->nstcalcenergy, DOMAINDECOMP(cr), useGpuForPme);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:    ForceBuffers f(fr->useMts, ((useGpuForNonbonded && useGpuForBufferOps) || useGpuForUpdate)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:    std::unique_ptr<UpdateConstrainGpu> integrator;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:    StatePropagatorDataGpu* stateGpu = fr->stateGpu;
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:    if (useGpuForUpdate)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:                           "groups if using GPU update.\n");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:                           "SHAKE is not supported with GPU update.");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:        GMX_RELEASE_ASSERT(useGpuForPme || (useGpuForNonbonded && simulationWork.useGpuBufferOps),
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:                           "the GPU to use GPU update.\n");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:                           "Only the md integrator is supported with the GPU update.\n");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:                "Nose-Hoover temperature coupling is not supported with the GPU update.\n");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:                "with the GPU update.\n");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:                           "Virtual sites are not supported with the GPU update.\n");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:                           "Essential dynamics is not supported with the GPU update.\n");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:                           "Constraints pulling is not supported with the GPU update.\n");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:                           "Orientation restraints are not supported with the GPU update.\n");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:                "Free energy perturbation of masses and constraints are not supported with the GPU "
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:                    .appendText("Updating coordinates and applying constraints on the GPU.");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:            GMX_LOG(mdlog.info).asParagraph().appendText("Updating coordinates on the GPU.");
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:                           "Device stream manager should be initialized in order to use GPU "
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:                "Update stream should be initialized in order to use GPU "
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:        integrator = std::make_unique<UpdateConstrainGpu>(
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:                stateGpu->xUpdatedOnDevice(), wcycle);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:    if (useGpuForPme || (useGpuForNonbonded && useGpuForBufferOps) || useGpuForUpdate)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:    if (useGpuForUpdate)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:                         fr->nbv->useGpu());
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:            if (useGpuForUpdate && !bFirstStep)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:                stateGpu->copyCoordinatesFromGpu(state->x, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:                stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:            /* PME grid + cut-off optimization with GPUs or PME nodes */
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:                           &bPMETunePrinting, simulationWork.useGpuPmePpCommunication);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:        if (useGpuForUpdate && !bFirstStep && bNS)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:            // Copy velocities from the GPU on search steps to keep a copy on host (device buffers are reinitialized).
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:            stateGpu->copyVelocitiesFromGpu(state->v, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:            stateGpu->waitVelocitiesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:            // Copy coordinate from the GPU when needed at the search step.
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:            stateGpu->copyCoordinatesFromGpu(state->x, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:            stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:                    if (useGpuForUpdate)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:        // Allocate or re-size GPU halo exchange object, if necessary
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:        if (bNS && havePPDomainDecomposition(cr) && simulationWork.useGpuHaloExchange)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:                               "GPU device manager has to be initialized to use GPU "
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:            constructGpuHaloExchange(mdlog, *cr, *fr->deviceStreamManager, wcycle);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:        // Copy coordinate from the GPU for the output/checkpointing if the update is offloaded and
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:        if (useGpuForUpdate && !bNS && !runScheduleWork->domainWork.haveCpuLocalForceWork
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:            stateGpu->copyCoordinatesFromGpu(state->x, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:            stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:        if (useGpuForUpdate && !bNS
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:            stateGpu->copyVelocitiesFromGpu(state->v, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:            stateGpu->waitVelocitiesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:        // Copy forces for the output if the forces were reduced on the GPU (not the case on virial steps)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:        // and update is offloaded hence forces are kept on the GPU for update and have not been
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:        //       when the forces are ready on the GPU -- the same synchronizer should be used as the one
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:        //       prior to GPU update.
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:        if (runScheduleWork->stepWork.useGpuFBufferOps && (simulationWork.useGpuUpdate && !vsite)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:            stateGpu->copyForcesFromGpu(f.view().force(), AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:            stateGpu->waitForcesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:        if (!useGpuForUpdate)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:        if (useGpuForUpdate)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:                integrator->set(stateGpu->getCoordinates(), stateGpu->getVelocities(),
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:                                stateGpu->getForces(), top.idef, *mdatoms, ekind->ngtc);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:                // Copy data to the GPU after buffers might have being reinitialized
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:                stateGpu->copyVelocitiesToGpu(state->v, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:                stateGpu->copyCoordinatesToGpu(state->x, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:            if (simulationWork.useGpuPme && !runScheduleWork->simulationWork.useGpuPmePpCommunication
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:                stateGpu->copyForcesToGpu(f.view().force(), AtomLocality::All);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:            else if (!runScheduleWork->stepWork.useGpuFBufferOps)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:                stateGpu->copyForcesToGpu(f.view().force(), AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:            integrator->integrate(stateGpu->getForcesReadyOnDeviceEvent(
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:                                          AtomLocality::Local, runScheduleWork->stepWork.useGpuFBufferOps),
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:                stateGpu->copyVelocitiesFromGpu(state->v, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:                stateGpu->waitVelocitiesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:                if (useGpuForUpdate && !EI_VV(ir->eI) && bStopCM)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:                    stateGpu->copyCoordinatesFromGpu(state->x, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:                    stateGpu->waitCoordinatesReadyOnHost(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:                    if (useGpuForUpdate)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:                        stateGpu->copyCoordinatesToGpu(state->x, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:                        stateGpu->waitCoordinatesCopiedToDevice(AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:                            stateGpu->copyVelocitiesToGpu(state->v, AtomLocality::Local);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:                                         pressureCouplingMu, state, nrnb, upd.deform(), !useGpuForUpdate);
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:        if (useGpuForUpdate
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/mdrun/md.cpp:        pme_loadbal_done(pme_loadbal, fplog, mdlog, fr->nbv->useGpu());
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:if (GMX_CLANG_CUDA)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:    include(gmxClangCudaUtils)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:set_property(GLOBAL PROPERTY CUDA_SOURCES)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:add_subdirectory(gpu_utils)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:# Mark some shared GPU implementation files to compile with CUDA if needed
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:if (GMX_GPU_CUDA)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:    get_property(CUDA_SOURCES GLOBAL PROPERTY CUDA_SOURCES)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:    set_source_files_properties(${CUDA_SOURCES} PROPERTIES CUDA_SOURCE_PROPERTY_FORMAT OBJ)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:if (GMX_GPU_CUDA)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:    # Work around FindCUDA that prevents using target_link_libraries()
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:    set(CUDA_LIBRARIES PRIVATE ${CUDA_LIBRARIES})
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:    if (NOT GMX_CLANG_CUDA)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:        gmx_cuda_add_library(libgromacs ${LIBGROMACS_SOURCES})
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:    target_link_libraries(libgromacs PRIVATE ${CUDA_CUFFT_LIBRARIES})
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:if (GMX_GPU_OPENCL)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:"An OpenCL build was requested with Visual Studio compiler, but GROMACS
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:clFFT to help with building for OpenCL, but that clFFT has not yet been
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:requires. Thus for now, OpenCL is not available with MSVC and the internal
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:# CUDA runtime headers
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:if (GMX_GPU_CUDA AND CMAKE_CXX_COMPILER_ID MATCHES "Clang")
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:    # CUDA header cuda_runtime_api.h in at least CUDA 10.1 uses 0
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:        set(GMX_CUDA_CLANG_FLAGS "${GMX_CUDA_CLANG_FLAGS} ${_compile_flag}")
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:    if (GMX_CLANG_CUDA)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:            get_source_file_property(_cuda_source_format ${_file} CUDA_SOURCE_PROPERTY_FORMAT)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:            if ("${_ext}" STREQUAL ".cu" OR _cuda_source_format)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:                gmx_compile_cuda_file_with_clang(${_file})
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:        get_property(CUDA_SOURCES GLOBAL PROPERTY CUDA_SOURCES)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:        set_source_files_properties(${CUDA_SOURCES} PROPERTIES COMPILE_FLAGS ${GMX_CUDA_CLANG_FLAGS})
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:if (GMX_GPU_SYCL)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:                      ${OpenCL_LIBRARIES}
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:# Technically, the user could want to do this for an OpenCL build
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:# using the CUDA runtime, but currently there's no reason to want to
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:if (INSTALL_CUDART_LIB) #can be set manual by user
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:    if (GMX_GPU_CUDA)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:        foreach(CUDA_LIB ${CUDA_LIBRARIES})
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:            string(REGEX MATCH "cudart" IS_CUDART ${CUDA_LIB})
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:            if(IS_CUDART) #libcuda should not be installed
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:                file(GLOB CUDA_LIBS ${CUDA_LIB}*)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:                install(FILES ${CUDA_LIBS} DESTINATION
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:        message(WARNING "INSTALL_CUDART_LIB only makes sense when configuring for CUDA support")
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:if(GMX_GPU_OPENCL)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:    file(GLOB OPENCL_INSTALLED_FILES
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:        gpu_utils/vectype_ops.clh
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:        gpu_utils/device_utils.clh
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:    install(FILES ${OPENCL_INSTALLED_FILES}
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:        DESTINATION ${GMX_INSTALL_OCLDIR}/gromacs/gpu_utils
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:    file(GLOB OPENCL_INSTALLED_FILES
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:    install(FILES ${OPENCL_INSTALLED_FILES}
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:    file(GLOB OPENCL_INSTALLED_FILES
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:    install(FILES ${OPENCL_INSTALLED_FILES}
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:    file(GLOB OPENCL_INSTALLED_FILES
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:        nbnxm/opencl/nbnxm_ocl_kernels.cl
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:        nbnxm/opencl/nbnxm_ocl_kernel.clh
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:        nbnxm/opencl/nbnxm_ocl_kernel_pruneonly.clh
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:        nbnxm/opencl/nbnxm_ocl_kernels.clh
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:        nbnxm/opencl/nbnxm_ocl_kernels_fastgen.clh
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:        nbnxm/opencl/nbnxm_ocl_kernels_fastgen_add_twincut.clh
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:        nbnxm/opencl/nbnxm_ocl_kernel_utils.clh
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:        nbnxm/opencl/nbnxm_ocl_consts.h
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:    install(FILES ${OPENCL_INSTALLED_FILES}
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:        DESTINATION ${GMX_INSTALL_OCLDIR}/gromacs/nbnxm/opencl
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:    file(GLOB OPENCL_INSTALLED_FILES
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:        ewald/pme_gpu_calculate_splines.clh
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:        ewald/pme_gpu_types.h
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt:    install(FILES ${OPENCL_INSTALLED_FILES}
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:if (GMX_CLANG_CUDA)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:    include(gmxClangCudaUtils)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:set_property(GLOBAL PROPERTY CUDA_SOURCES)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:add_subdirectory(gpu_utils)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:# Mark some shared GPU implementation files to compile with CUDA if needed
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:if (GMX_GPU_CUDA)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:    get_property(CUDA_SOURCES GLOBAL PROPERTY CUDA_SOURCES)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:    set_source_files_properties(${CUDA_SOURCES} PROPERTIES CUDA_SOURCE_PROPERTY_FORMAT OBJ)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:if (GMX_GPU_CUDA)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:    # Work around FindCUDA that prevents using target_link_libraries()
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:    set(CUDA_LIBRARIES PRIVATE ${CUDA_LIBRARIES})
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:    if (NOT GMX_CLANG_CUDA)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:        gmx_cuda_add_library(libgromacs ${LIBGROMACS_SOURCES})
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:    target_link_libraries(libgromacs PRIVATE ${CUDA_CUFFT_LIBRARIES})
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:if (GMX_GPU_OPENCL)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:"An OpenCL build was requested with Visual Studio compiler, but GROMACS
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:clFFT to help with building for OpenCL, but that clFFT has not yet been
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:requires. Thus for now, OpenCL is not available with MSVC and the internal
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:# CUDA runtime headers
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:if (GMX_GPU_CUDA AND CMAKE_CXX_COMPILER_ID MATCHES "Clang")
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:    # CUDA header cuda_runtime_api.h in at least CUDA 10.1 uses 0
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:        set(GMX_CUDA_CLANG_FLAGS "${GMX_CUDA_CLANG_FLAGS} ${_compile_flag}")
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:    if (GMX_CLANG_CUDA)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:            get_source_file_property(_cuda_source_format ${_file} CUDA_SOURCE_PROPERTY_FORMAT)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:            if ("${_ext}" STREQUAL ".cu" OR _cuda_source_format)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:                gmx_compile_cuda_file_with_clang(${_file})
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:        get_property(CUDA_SOURCES GLOBAL PROPERTY CUDA_SOURCES)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:        set_source_files_properties(${CUDA_SOURCES} PROPERTIES COMPILE_FLAGS ${GMX_CUDA_CLANG_FLAGS})
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:if (GMX_GPU_SYCL)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:                      ${OpenCL_LIBRARIES}
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:# Technically, the user could want to do this for an OpenCL build
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:# using the CUDA runtime, but currently there's no reason to want to
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:if (INSTALL_CUDART_LIB) #can be set manual by user
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:    if (GMX_GPU_CUDA)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:        foreach(CUDA_LIB ${CUDA_LIBRARIES})
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:            string(REGEX MATCH "cudart" IS_CUDART ${CUDA_LIB})
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:            if(IS_CUDART) #libcuda should not be installed
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:                file(GLOB CUDA_LIBS ${CUDA_LIB}*)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:                install(FILES ${CUDA_LIBS} DESTINATION
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:        message(WARNING "INSTALL_CUDART_LIB only makes sense when configuring for CUDA support")
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:if(GMX_GPU_OPENCL)
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:    file(GLOB OPENCL_INSTALLED_FILES
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:        gpu_utils/vectype_ops.clh
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:        gpu_utils/device_utils.clh
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:    install(FILES ${OPENCL_INSTALLED_FILES}
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:        DESTINATION ${GMX_INSTALL_OCLDIR}/gromacs/gpu_utils
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:    file(GLOB OPENCL_INSTALLED_FILES
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:    install(FILES ${OPENCL_INSTALLED_FILES}
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:    file(GLOB OPENCL_INSTALLED_FILES
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:    install(FILES ${OPENCL_INSTALLED_FILES}
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:    file(GLOB OPENCL_INSTALLED_FILES
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:        nbnxm/opencl/nbnxm_ocl_kernels.cl
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:        nbnxm/opencl/nbnxm_ocl_kernel.clh
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:        nbnxm/opencl/nbnxm_ocl_kernel_pruneonly.clh
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:        nbnxm/opencl/nbnxm_ocl_kernels.clh
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:        nbnxm/opencl/nbnxm_ocl_kernels_fastgen.clh
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:        nbnxm/opencl/nbnxm_ocl_kernels_fastgen_add_twincut.clh
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:        nbnxm/opencl/nbnxm_ocl_kernel_utils.clh
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:        nbnxm/opencl/nbnxm_ocl_consts.h
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:    install(FILES ${OPENCL_INSTALLED_FILES}
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:        DESTINATION ${GMX_INSTALL_OCLDIR}/gromacs/nbnxm/opencl
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:    file(GLOB OPENCL_INSTALLED_FILES
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:        ewald/pme_gpu_calculate_splines.clh
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:        ewald/pme_gpu_types.h
v1.2/plumed2/patches/gromacs-2021.7.diff/src/gromacs/CMakeLists.txt.preplumed:    install(FILES ${OPENCL_INSTALLED_FILES}
v1.2/plumed2/regtest/isdb/rt-saxs-gpu/plumed.dat:GPU
v1.2/plumed2/configure:enable_af_cuda
v1.2/plumed2/configure:  --enable-af_cuda        enable search for arrayfire_cuda, default: no
v1.2/plumed2/configure:af_cuda=
v1.2/plumed2/configure:# Check whether --enable-af_cuda was given.
v1.2/plumed2/configure:if test "${enable_af_cuda+set}" = set; then :
v1.2/plumed2/configure:  enableval=$enable_af_cuda; case "${enableval}" in
v1.2/plumed2/configure:             (yes) af_cuda=true ;;
v1.2/plumed2/configure:             (no)  af_cuda=false ;;
v1.2/plumed2/configure:             (*)   as_fn_error $? "wrong argument to --enable-af_cuda" "$LINENO" 5 ;;
v1.2/plumed2/configure:             (yes) af_cuda=true ;;
v1.2/plumed2/configure:             (no)  af_cuda=false ;;
v1.2/plumed2/configure:for ac_lib in '' afopencl; do
v1.2/plumed2/configure:for ac_lib in '' afopencl; do
v1.2/plumed2/configure:if test "$af_cuda" = true ; then
v1.2/plumed2/configure:for ac_lib in '' afcuda; do
v1.2/plumed2/configure:    __PLUMED_HAS_ARRAYFIRE_CUDA=no
v1.2/plumed2/configure:for ac_lib in '' afcuda; do
v1.2/plumed2/configure:       $as_echo "#define __PLUMED_HAS_ARRAYFIRE_CUDA 1" >>confdefs.h
v1.2/plumed2/configure:       __PLUMED_HAS_ARRAYFIRE_CUDA=yes
v1.2/plumed2/configure:       { $as_echo "$as_me:${as_lineno-$LINENO}: WARNING: cannot enable __PLUMED_HAS_ARRAYFIRE_CUDA" >&5
v1.2/plumed2/configure:$as_echo "$as_me: WARNING: cannot enable __PLUMED_HAS_ARRAYFIRE_CUDA" >&2;}
v1.2/plumed2/CHANGES/v2.5.md:  - \ref SAXS includes a GPU implementation based on ArrayFire (need to be linked at compile time) that can be activated with GPU
v1.2/plumed2/CHANGES/v2.5.md:- plumed can be compiled with ArrayFire to enable for gpu code. \ref SAXS collective variable is available as part of the isdb module to provide an example of a gpu implementation for a CV
v1.2/plumed2/CHANGES/v2.0.md:  using GPUs.
v1.2/plumed2/CHANGES/v2.8.md:- Updated GROMACS patches to warn about the joint use of update gpu and plumed: needs patch to be reapplied 
v1.2/plumed2/src/molfile/molfile_plugin.h:    * kernel-bypass direct I/O or using GPU-Direct Storage APIs,
v1.2/plumed2/src/core/PlumedMain.h:/// GpuDevice Identifier
v1.2/plumed2/src/core/PlumedMain.h:  int gpuDeviceId;
v1.2/plumed2/src/core/PlumedMain.h:/// Get the value of the gpuDeviceId
v1.2/plumed2/src/core/PlumedMain.h:  int getGpuDeviceId() const ;
v1.2/plumed2/src/core/PlumedMain.h:int PlumedMain::getGpuDeviceId() const {
v1.2/plumed2/src/core/PlumedMain.h:  return gpuDeviceId;
v1.2/plumed2/src/core/PlumedMain.cpp:  gpuDeviceId(-1)
v1.2/plumed2/src/core/PlumedMain.cpp:      case cmd_setGpuDeviceId:
v1.2/plumed2/src/core/PlumedMain.cpp:          if(id>=0) gpuDeviceId=id;
v1.2/plumed2/src/isdb/SAXS.cpp:#ifdef __PLUMED_HAS_ARRAYFIRE_CUDA
v1.2/plumed2/src/isdb/SAXS.cpp:#include <cuda_runtime.h>
v1.2/plumed2/src/isdb/SAXS.cpp:#include <af/cuda.h>
v1.2/plumed2/src/isdb/SAXS.cpp:#include <af/opencl.h>
v1.2/plumed2/src/isdb/SAXS.cpp:By default SAXS is calculated using Debye on CPU, by adding the GPU flag it is possible to solve the equation on a
v1.2/plumed2/src/isdb/SAXS.cpp:GPU if the ARRAYFIRE libraries are installed and correctly linked.
v1.2/plumed2/src/isdb/SAXS.cpp:By default SANS is calculated using Debye on CPU, by adding the GPU flag it is possible to solve the equation on a
v1.2/plumed2/src/isdb/SAXS.cpp:GPU if the ARRAYFIRE libraries are installed and correctly linked.
v1.2/plumed2/src/isdb/SAXS.cpp:  bool gpu;
v1.2/plumed2/src/isdb/SAXS.cpp:  void calculate_gpu(std::vector<Vector> &pos, std::vector<Vector> &deriv);
v1.2/plumed2/src/isdb/SAXS.cpp:  keys.add("compulsory","DEVICEID","-1","Identifier of the GPU to be used");
v1.2/plumed2/src/isdb/SAXS.cpp:  keys.addFlag("GPU",false,"calculate SAXS using ARRAYFIRE on an accelerator device");
v1.2/plumed2/src/isdb/SAXS.cpp:  gpu(false),
v1.2/plumed2/src/isdb/SAXS.cpp:  parseFlag("GPU",gpu);
v1.2/plumed2/src/isdb/SAXS.cpp:  if(gpu) error("To use the GPU mode PLUMED must be compiled with ARRAYFIRE");
v1.2/plumed2/src/isdb/SAXS.cpp:  if(gpu&&comm.Get_rank()==0) {
v1.2/plumed2/src/isdb/SAXS.cpp:    if(deviceid==-1) deviceid=plumed.getGpuDeviceId();
v1.2/plumed2/src/isdb/SAXS.cpp:#ifdef  __PLUMED_HAS_ARRAYFIRE_CUDA
v1.2/plumed2/src/isdb/SAXS.cpp:  if(!gpu) {
v1.2/plumed2/src/isdb/SAXS.cpp:void SAXS::calculate_gpu(std::vector<Vector> &pos, std::vector<Vector> &deriv)
v1.2/plumed2/src/isdb/SAXS.cpp:  // on gpu only the master rank run the calculation
v1.2/plumed2/src/isdb/SAXS.cpp:#ifdef  __PLUMED_HAS_ARRAYFIRE_CUDA
v1.2/plumed2/src/isdb/SAXS.cpp:            if(!gpu) {
v1.2/plumed2/src/isdb/SAXS.cpp:        if(!gpu) {
v1.2/plumed2/src/isdb/SAXS.cpp:            if(!gpu) {
v1.2/plumed2/src/isdb/SAXS.cpp:        if(!gpu) {
v1.2/plumed2/src/isdb/SAXS.cpp:  if(gpu) calculate_gpu(beads_pos, bd_deriv);
v1.2/tutorials/Quantum-HP_tutorial.md:Quantum-HP is a multi-CPU and multi-GPU massively parallel platform designed to incorporate nuclear quantum effects in Tinker-HP MD simulations. It utilizes two primary simulation strategies:
v1.2/tutorials/Quantum-HP_tutorial.md:- Efficient implementation with parallelization for multi-CPU and multi-GPU systems.
v1.2/tutorials/Quantum-HP_tutorial.md:4. **Parallelization**: Quantum-HP is optimized for parallel computing. Make sure to take advantage of multi-CPU and (multi-)GPU systems to accelerate your simulations.
v1.2/tutorials/Quantum-HP_tutorial.md:   - `QTB_BATCH_SIZE` (int): number of atoms on which to generate the colored noise   simultaneously (only for the GPU version). Allows to reduce memory requirements   for large systems but might be slower when using small batch sizes. Defaults to   all the local atoms (i.e. no batches).
v1.2/tcl8.6.13/ChangeLog:	* generic/tclOOMethod.c (procMethodType): Now that introspection can
v1.2/tcl8.6.13/tests/socket.test:    tcltest::DebugPuts 2 "== test \[$::localhost\]:$port $testmode =="
v1.2/tcl8.6.13/tests/socket.test:      tcltest::DebugPuts 2 "** trma / $::count ** $args **"
v1.2/tcl8.6.13/tests/socket.test:      tcltest::DebugPuts 2 "** iter / $::count **"
v1.2/tcl8.6.13/tests/socket.test:    tcltest::DebugPuts 2 "== stop / $::count =="
v1.2/tcl8.6.13/tests/winPipe.test:	    tcltest::DebugPuts 4 "  ## test exec [file extension [lindex $cmd 0]] ($cmd) for\n  ##   $args"
v1.2/tcl8.6.13/unix/tclUnixFile.c:	while (TclIsSpaceProcM(*p)) {
v1.2/tcl8.6.13/library/tcltest/tcltest.tcl:# tcltest::DebugPuts --
v1.2/tcl8.6.13/library/tcltest/tcltest.tcl:proc tcltest::DebugPuts {level string} {
v1.2/tcl8.6.13/library/tcltest/tcltest.tcl:    DebugPuts 3 "entering testConstraint $constraint $value"
v1.2/tcl8.6.13/library/tcltest/tcltest.tcl:    DebugPuts 3 "entering SafeFetch $n1 $n2 $op"
v1.2/tcl8.6.13/library/tcltest/tcltest.tcl:    DebugPuts 3 "entering ConstraintInitializer $constraint $script"
v1.2/tcl8.6.13/library/tcltest/tcltest.tcl:    DebugPuts 2 "Flags passed into tcltest:"
v1.2/tcl8.6.13/library/tcltest/tcltest.tcl:	DebugPuts 2 \
v1.2/tcl8.6.13/library/tcltest/tcltest.tcl:	DebugPuts 2 "    argv: $::argv"
v1.2/tcl8.6.13/library/tcltest/tcltest.tcl:    DebugPuts    2 "tcltest::debug              = [debug]"
v1.2/tcl8.6.13/library/tcltest/tcltest.tcl:    DebugPuts    2 "tcltest::testsDirectory     = [testsDirectory]"
v1.2/tcl8.6.13/library/tcltest/tcltest.tcl:    DebugPuts    2 "tcltest::workingDirectory   = [workingDirectory]"
v1.2/tcl8.6.13/library/tcltest/tcltest.tcl:    DebugPuts    2 "tcltest::temporaryDirectory = [temporaryDirectory]"
v1.2/tcl8.6.13/library/tcltest/tcltest.tcl:    DebugPuts    2 "tcltest::outputChannel      = [outputChannel]"
v1.2/tcl8.6.13/library/tcltest/tcltest.tcl:    DebugPuts    2 "tcltest::errorChannel       = [errorChannel]"
v1.2/tcl8.6.13/library/tcltest/tcltest.tcl:    DebugPuts    2 "Original environment (tcltest::originalEnv):"
v1.2/tcl8.6.13/library/tcltest/tcltest.tcl:    DebugPuts    2 "Constraints:"
v1.2/tcl8.6.13/library/tcltest/tcltest.tcl:    DebugPuts 3 "[lindex [info level 0] 0] called"
v1.2/tcl8.6.13/library/tcltest/tcltest.tcl:    DebugPuts 3 "test $name $args"
v1.2/tcl8.6.13/library/tcltest/tcltest.tcl:    DebugPuts 3 "Running $name {$script}"
v1.2/tcl8.6.13/library/tcltest/tcltest.tcl:	DebugPuts 1 "No test directories remain after applying match\
v1.2/tcl8.6.13/library/tcltest/tcltest.tcl:    DebugPuts  2 "[lindex [info level 0] 0]: $saveState"
v1.2/tcl8.6.13/library/tcltest/tcltest.tcl:	    DebugPuts 2 "[lindex [info level 0] 0]: Removing proc $p"
v1.2/tcl8.6.13/library/tcltest/tcltest.tcl:	    DebugPuts 2 "[lindex [info level 0] 0]:\
v1.2/tcl8.6.13/library/tcltest/tcltest.tcl:    DebugPuts 3 "[lindex [info level 0] 0]:\
v1.2/tcl8.6.13/library/tcltest/tcltest.tcl:    DebugPuts 3 "[lindex [info level 0] 0]: removing $fullName"
v1.2/tcl8.6.13/library/tcltest/tcltest.tcl:    DebugPuts 3 "[lindex [info level 0] 0]: creating $fullName"
v1.2/tcl8.6.13/library/tcltest/tcltest.tcl:    DebugPuts 3 "[lindex [info level 0] 0]: deleting $fullName"
v1.2/tcl8.6.13/pkgs/itcl4.2.3/generic/itcl2TclOO.h:MODULE_SCOPE Tcl_Method Itcl_NewProcMethod(Tcl_Interp *interp, Tcl_Object oPtr,
v1.2/tcl8.6.13/pkgs/itcl4.2.3/generic/itclInfo.c:struct NameProcMap2 {
v1.2/tcl8.6.13/pkgs/itcl4.2.3/generic/itclInfo.c:static const struct NameProcMap2 infoCmdsDelegated2[] = {
v1.2/tcl8.6.13/pkgs/itcl4.2.3/generic/itclBase.c:    Tcl_InitHashTable(&infoPtr->procMethods, TCL_ONE_WORD_KEYS);
v1.2/tcl8.6.13/pkgs/itcl4.2.3/generic/itclBase.c:    Tcl_DeleteHashTable(&infoPtr->procMethods);
v1.2/tcl8.6.13/pkgs/itcl4.2.3/generic/itclInt.h:    Tcl_HashTable procMethods;      /* maps from procPtr to mFunc */
v1.2/tcl8.6.13/pkgs/itcl4.2.3/generic/itclMethod.c:	hPtr = Tcl_CreateHashEntry(&imPtr->iclsPtr->infoPtr->procMethods,
v1.2/tcl8.6.13/pkgs/itcl4.2.3/generic/itclMethod.c:	    hPtr = Tcl_FindHashEntry(&imPtr->iclsPtr->infoPtr->procMethods,
v1.2/tcl8.6.13/pkgs/itcl4.2.3/generic/itclClass.c:    hPtr = Tcl_FindHashEntry(&imPtr->infoPtr->procMethods,
v1.2/tcl8.6.13/pkgs/itcl4.2.3/generic/itclObject.c:		            &imPtr->iclsPtr->infoPtr->procMethods,
v1.2/tcl8.6.13/pkgs/itcl4.2.3/generic/itclParse.c:	    hPtr2 = Tcl_CreateHashEntry(&iclsPtr->infoPtr->procMethods,
v1.2/tcl8.6.13/pkgs/itcl4.2.3/generic/itclParse.c:		 * entry in the procMethods map based on the old one.
v1.2/tcl8.6.13/pkgs/itcl4.2.3/generic/itclParse.c:	        imPtr->tmPtr = Itcl_NewProcMethod(interp,
v1.2/tcl8.6.13/pkgs/itcl4.2.3/generic/itcl2TclOO.c:    result = TclOONewProcMethodEx(interp, clsPtr, preCallPtr, postCallPtr,
v1.2/tcl8.6.13/pkgs/itcl4.2.3/generic/itcl2TclOO.c: * Itcl_NewProcMethod --
v1.2/tcl8.6.13/pkgs/itcl4.2.3/generic/itcl2TclOO.c:Itcl_NewProcMethod(
v1.2/tcl8.6.13/pkgs/thread2.8.8/tests/French.txt:envy	envidia[Noun]
v1.2/tcl8.6.13/pkgs/thread2.8.8/tests/French.txt:envy	envidiar[Verb]
v1.2/tcl8.6.13/generic/tclParse.c:		if (openBrace && TclIsSpaceProcM(src[-1])) {
v1.2/tcl8.6.13/generic/tclUtil.c:    count += 1 - TclIsSpaceProcM(*bytes);
v1.2/tcl8.6.13/generic/tclUtil.c:	if (TclIsSpaceProcM(*bytes)) {
v1.2/tcl8.6.13/generic/tclUtil.c:	    } while (numBytes && TclIsSpaceProcM(*bytes));
v1.2/tcl8.6.13/generic/tclUtil.c:    count -= TclIsSpaceProcM(bytes[-1]);
v1.2/tcl8.6.13/generic/tclUtil.c:    while ((p < limit) && (TclIsSpaceProcM(*p))) {
v1.2/tcl8.6.13/generic/tclUtil.c:		if ((p >= limit) || TclIsSpaceProcM(*p)) {
v1.2/tcl8.6.13/generic/tclUtil.c:		    while ((p2 < limit) && (!TclIsSpaceProcM(*p2))
v1.2/tcl8.6.13/generic/tclUtil.c:		if ((p >= limit) || TclIsSpaceProcM(*p)) {
v1.2/tcl8.6.13/generic/tclUtil.c:		    while ((p2 < limit) && (!TclIsSpaceProcM(*p2))
v1.2/tcl8.6.13/generic/tclUtil.c:	    if (TclIsSpaceProcM(*p)) {
v1.2/tcl8.6.13/generic/tclUtil.c:    while ((p < limit) && (TclIsSpaceProcM(*p))) {
v1.2/tcl8.6.13/generic/tclUtil.c:	    if (TclIsSpaceProcM(*p)) {
v1.2/tcl8.6.13/generic/tclUtil.c:	while ((--dst >= dsPtr->string) && TclIsSpaceProcM(*dst)) {
v1.2/tcl8.6.13/generic/tclUtil.c:    if (TclIsSpaceProcM(*end)) {
v1.2/tcl8.6.13/generic/tclUtil.c:    while (length && TclIsSpaceProcM(*bytes)) {
v1.2/tcl8.6.13/generic/tclUtil.c:	if (TclIsSpaceProcM(opPtr[1])) {
v1.2/tcl8.6.13/generic/tclUtil.c:	if (TclIsSpaceProcM(bytes[4])) {
v1.2/tcl8.6.13/generic/tclUtil.c:    while (TclIsSpaceProcM(*p)) {
v1.2/tcl8.6.13/generic/tclUtil.c:	while (TclIsSpaceProcM(*p)) {
v1.2/tcl8.6.13/generic/tclGetDate.y:	while (TclIsSpaceProcM(*yyInput)) {
v1.2/tcl8.6.13/generic/tclOOMethod.c:static const Tcl_MethodType procMethodType = {
v1.2/tcl8.6.13/generic/tclOOMethod.c:	    argsObj, bodyObj, &procMethodType, pmPtr, &pmPtr->procPtr);
v1.2/tcl8.6.13/generic/tclOOMethod.c: * TclOONewProcMethod --
v1.2/tcl8.6.13/generic/tclOOMethod.c:TclOONewProcMethod(
v1.2/tcl8.6.13/generic/tclOOMethod.c:    method = TclOOMakeProcMethod(interp, clsPtr, flags, nameObj, procName,
v1.2/tcl8.6.13/generic/tclOOMethod.c:	    argsObj, bodyObj, &procMethodType, pmPtr, &pmPtr->procPtr);
v1.2/tcl8.6.13/generic/tclOOMethod.c: * TclOOMakeProcMethod --
v1.2/tcl8.6.13/generic/tclOOMethod.c:TclOOMakeProcMethod(
v1.2/tcl8.6.13/generic/tclOOMethod.c:    if (mPtr->typePtr == &procMethodType) {
v1.2/tcl8.6.13/generic/tclOOMethod.c:    if (mPtr->typePtr == &procMethodType) {
v1.2/tcl8.6.13/generic/tclOOMethod.c:TclOONewProcMethodEx(
v1.2/tcl8.6.13/generic/tclOOMethod.c:    Tcl_Method method = (Tcl_Method) TclOONewProcMethod(interp,
v1.2/tcl8.6.13/generic/tclOO.decls:    Tcl_Method TclOOMakeProcMethod(Tcl_Interp *interp, Class *clsPtr,
v1.2/tcl8.6.13/generic/tclOO.decls:    Method *TclOONewProcMethod(Tcl_Interp *interp, Class *clsPtr,
v1.2/tcl8.6.13/generic/tclOO.decls:    Tcl_Method TclOONewProcMethodEx(Tcl_Interp *interp, Tcl_Class clsPtr,
v1.2/tcl8.6.13/generic/tclOOIntDecls.h:TCLAPI Tcl_Method	TclOOMakeProcMethod(Tcl_Interp *interp,
v1.2/tcl8.6.13/generic/tclOOIntDecls.h:TCLAPI Method *		TclOONewProcMethod(Tcl_Interp *interp, Class *clsPtr,
v1.2/tcl8.6.13/generic/tclOOIntDecls.h:TCLAPI Tcl_Method	TclOONewProcMethodEx(Tcl_Interp *interp,
v1.2/tcl8.6.13/generic/tclOOIntDecls.h:    Tcl_Method (*tclOOMakeProcMethod) (Tcl_Interp *interp, Class *clsPtr, int flags, Tcl_Obj *nameObj, const char *namePtr, Tcl_Obj *argsObj, Tcl_Obj *bodyObj, const Tcl_MethodType *typePtr, void *clientData, Proc **procPtrPtr); /* 2 */
v1.2/tcl8.6.13/generic/tclOOIntDecls.h:    Method * (*tclOONewProcMethod) (Tcl_Interp *interp, Class *clsPtr, int flags, Tcl_Obj *nameObj, Tcl_Obj *argsObj, Tcl_Obj *bodyObj, ProcedureMethod **pmPtrPtr); /* 4 */
v1.2/tcl8.6.13/generic/tclOOIntDecls.h:    Tcl_Method (*tclOONewProcMethodEx) (Tcl_Interp *interp, Tcl_Class clsPtr, TclOO_PreCallProc *preCallPtr, TclOO_PostCallProc *postCallPtr, ProcErrorProc *errProc, void *clientData, Tcl_Obj *nameObj, Tcl_Obj *argsObj, Tcl_Obj *bodyObj, int flags, void **internalTokenPtr); /* 10 */
v1.2/tcl8.6.13/generic/tclOOIntDecls.h:#define TclOOMakeProcMethod \
v1.2/tcl8.6.13/generic/tclOOIntDecls.h:	(tclOOIntStubsPtr->tclOOMakeProcMethod) /* 2 */
v1.2/tcl8.6.13/generic/tclOOIntDecls.h:#define TclOONewProcMethod \
v1.2/tcl8.6.13/generic/tclOOIntDecls.h:	(tclOOIntStubsPtr->tclOONewProcMethod) /* 4 */
v1.2/tcl8.6.13/generic/tclOOIntDecls.h:#define TclOONewProcMethodEx \
v1.2/tcl8.6.13/generic/tclOOIntDecls.h:	(tclOOIntStubsPtr->tclOONewProcMethodEx) /* 10 */
v1.2/tcl8.6.13/generic/tclOO.c:    TclOONewProcMethod(interp, fPtr->objectCls, 0, fPtr->clonedName, argsPtr,
v1.2/tcl8.6.13/generic/tclOOStubInit.c:    TclOOMakeProcMethod, /* 2 */
v1.2/tcl8.6.13/generic/tclOOStubInit.c:    TclOONewProcMethod, /* 4 */
v1.2/tcl8.6.13/generic/tclOOStubInit.c:    TclOONewProcMethodEx, /* 10 */
v1.2/tcl8.6.13/generic/tclResult.c:	while ((--dst >= iPtr->appendResult) && TclIsSpaceProcM(*dst)) {
v1.2/tcl8.6.13/generic/tclCmdMZ.c:		    while (TclIsSpaceProcM(*p)) {
v1.2/tcl8.6.13/generic/tclInt.h:#	define TclIsSpaceProcM(byte) \
v1.2/tcl8.6.13/generic/tclDate.c:	while (TclIsSpaceProcM(*yyInput)) {
v1.2/tcl8.6.13/generic/tclOODefineCmds.c:	method = (Tcl_Method) TclOONewProcMethod(interp, clsPtr,
v1.2/tcl8.6.13/generic/tclOODefineCmds.c:	method = (Tcl_Method) TclOONewProcMethod(interp, clsPtr,
v1.2/tcl8.6.13/generic/tclOODefineCmds.c:	if (TclOONewProcMethod(interp, oPtr->classPtr, isPublic, objv[1],
v1.2/tcl8.6.13/generic/tclStrToD.c:	    if (TclIsSpaceProcM(c)) {
v1.2/tcl8.6.13/generic/tclStrToD.c:	    if (TclIsSpaceProcM(c)) {
v1.2/tcl8.6.13/generic/tclStrToD.c:	    while (len != 0 && TclIsSpaceProcM(*p)) {
v1.2/tcl8.6.13/generic/tclCmdAH.c:	    if (TclIsSpaceProcM(*p) || (*p == '\\')) {
v1.2/tcl8.6.13/generic/tclUtf.c:	return TclIsSpaceProcM((char) ch);
v1.2/colvars/colvaratoms.h: *          branch prediction is broken (or further migration to GPU code).
v1.2/colvars/colvarmodule_refs.h:    "  title = {Scalable molecular dynamics on {CPU} and {GPU} architectures with {NAMD}},\n"

```
